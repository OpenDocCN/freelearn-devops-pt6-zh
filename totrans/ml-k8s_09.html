<html><head></head><body>
		<div id="_idContainer288">
			<h1 id="_idParaDest-133"><em class="italic"><a id="_idTextAnchor132"/>Chapter 9</em>: Building Your Data Pipeline</h1>
			<p>In the previous chapter, you understood the example business goal of improving user experience by recommending flights that have a higher on-time probability. You have worked with the business <strong class="bold">subject matter expert</strong> (<strong class="bold">SME</strong>) to understand the available data. In this chapter, you will see how the platform assists you in harvesting and processing data from a variety of sources. You will see how on-demand Spark clusters can be created and how workloads could be isolated in a shared environment using the platform. New flights data may be available on a frequent basis and you will see how the platform enables you to automate the execution of your data pipeline.</p>
			<p>In this chapter, you will learn about the following topics:</p>
			<ul>
				<li>Automated provisioning of a Spark cluster for development</li>
				<li>Writing a Spark data pipeline</li>
				<li>Using the Spark UI to monitor your jobs</li>
				<li>Building and executing a data pipeline using Airflow</li>
			</ul>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor133"/>Technical requirements</h1>
			<p>This chapter includes some hands-on setup and exercises. You will need a running Kubernetes cluster configured with <strong class="bold">Operator Lifecycle Manager</strong> (<strong class="bold">OLM</strong>). Building such a Kubernetes environment is covered in <a href="B18332_03_ePub.xhtml#_idTextAnchor040"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring Kubernetes</em>. Before attempting the technical exercises in this chapter, please make sure that you have a working Kubernetes cluster and <strong class="bold">Open Data Hub</strong> (<strong class="bold">ODH</strong>) is installed on your Kubernetes cluster. Installing ODH is covered in <a href="B18332_04_ePub.xhtml#_idTextAnchor055"><em class="italic">Chapter 4</em></a>, <em class="italic">The Anatomy of a Machine Learning Platform</em>.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor134"/>Automated provisioning of a Spark cluster for development </h1>
			<p>In this section, you<a id="_idIndexMarker712"/> will learn how the platform enables your team to provision an Apache Spark cluster on-demand. This capability of provisioning new Apache Spark clusters on-demand enables your organization to run multiple isolated projects used by multiple teams on a shared Kubernetes cluster without overlapping.</p>
			<p>The heart of this component is the Spark operator that is available within the platform. The Spark Kubernetes Operator allows you to start the Spark cluster declaratively. You can find the necessary configuration files in the book's Git repository under the <strong class="source-inline">manifests/radanalyticsio</strong> folder. The details of this operator are out of scope for this book, but we will show you how the mechanism works.</p>
			<p>The Spark operator defines a<a id="_idIndexMarker713"/> Kubernetes <strong class="bold">custom resource definition</strong> (<strong class="bold">CRD</strong>), which provides the schema of the requests that you can make to the Spark operator. In this schema, you can define many things, such as the number of worker nodes for your cluster and resources allocated to the master and worker nodes for the cluster.</p>
			<p>Through this file, you define the following options. Note that this is not an exhaustive list. For a full list, please look into the documentation of this open source project at <a href="https://github.com/radanalyticsio/spark-operator">https://github.com/radanalyticsio/spark-operator</a>:</p>
			<ul>
				<li>The <strong class="source-inline">customImage</strong> section defines the name of the container that provides the Spark software.</li>
				<li>The <strong class="source-inline">master</strong> section defines the number of Spark master instances and the resources allocated to the master Pod.</li>
				<li>The <strong class="source-inline">worker</strong> section defines the number of Spark worker instances and the resources allocated to the worker Pod.</li>
				<li>The <strong class="source-inline">sparkConfiguration</strong> section enables you to add any specific Spark configuration, such as the broadcast join threshold.</li>
				<li>The <strong class="source-inline">env</strong> section enables you to add variables that Spark entertains, such as <strong class="source-inline">SPARK_WORKER_CORES</strong>.</li>
				<li>The <strong class="source-inline">sparkWebUI</strong> section enables flags and instructs the operator to create a Kubernetes Ingress for the Spark UI. In the following section, you will use this UI to investigate your Spark code.</li>
			</ul>
			<p>You can find one such file at <strong class="source-inline">manifests/radanalyticsio/spark/cluster/base/simple-cluster.yaml</strong>, and it is shown in the following screenshot. <em class="italic">Figure 9.1</em> shows a section of the <strong class="source-inline">simple-cluster.yaml</strong> file:</p>
			<div>
				<div id="_idContainer239" class="IMG---Figure">
					<img src="image/B18332_09_001.jpg" alt="Figure 9.1 – A simple Spark custom resource used by Spark operator&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – A simple Spark custom resource used by Spark operator</p>
			<p>Now, you know <a id="_idIndexMarker714"/>the basic process of provisioning a Spark cluster on the platform. However, you will see in the next section that when you select the <strong class="bold">Elyra Notebook Image with Spark</strong> notebook image, the Spark cluster is provisioned for you. This is because, in the platform, JupyterHub is configured to submit a Spark <a id="_idIndexMarker715"/>cluster <strong class="bold">custom resource</strong> (<strong class="bold">CR</strong>) when you select a specific notebook. This configuration is available through two files.</p>
			<p>The first one is <strong class="source-inline">manifests/jupyterhub/jupyterhub/overlays/spark3/jupyterhub-singleusers-profiles-configmap.yaml</strong>, which defines a profile as <strong class="source-inline">Spark Notebook</strong>. In this section, the platform configures the name of the container images under the <strong class="source-inline">images</strong> key, so whenever JupyterHub spawns a new instance of this image, it will apply these settings. The <strong class="bold">Elyra Notebook Image with Spark</strong> notebook points to an image and it is the same image defined in this part of the configuration. This file contains the configuration parameters under <strong class="source-inline">configuration</strong>, and the <strong class="source-inline">resources</strong> section points to resources that will be created alongside the instance of this image. <em class="italic">Figure 9.2</em> shows a<a id="_idIndexMarker716"/> section of the <strong class="source-inline">jupyterhub-singleusers-profiles-configmap.yaml</strong> file:</p>
			<div>
				<div id="_idContainer240" class="IMG---Figure">
					<img src="image/B18332_09_002.jpg" alt="Figure 9.2 – A section of jupyterhub-singleusers-profiles-configmap.yaml&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – A section of jupyterhub-singleusers-profiles-configmap.yaml</p>
			<p>Note that <strong class="source-inline">resources</strong> has a property with a value of <strong class="source-inline">sparkClusterTemplate</strong>, which brings us to our second file.</p>
			<p>The second file, <strong class="source-inline">manifests/jupyterhub/jupyterhub/base/jupyterhub-spark-operator-configmap.yaml</strong>, contains <strong class="source-inline">sparkClusterTemplate</strong>,<strong class="source-inline"> </strong>which defines the Spark CR. Note that the parameters available in the <strong class="source-inline">jupyterhub-singleusers-profiles-configmap.yaml</strong> file will be utilized here. <em class="italic">Figure 9.3</em> shows a section of the <strong class="source-inline">jupyterhub-spark-operator-configmap.yaml</strong> file:</p>
			<div>
				<div id="_idContainer241" class="IMG---Figure">
					<img src="image/B18332_09_003.jpg" alt="Figure 9.3 – A section of jupyterhub-spark-operator-configmap.yaml&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – A section of jupyterhub-spark-operator-configmap.yaml</p>
			<p>In this section, you have seen how the platform wires different components to make life easier for your <a id="_idIndexMarker717"/>teams and organization, and you can change and configure each of these components as per your needs, which brings on the true power of the open source software.</p>
			<p>Let's write a data pipeline to process our flights data.</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor135"/>Writing a Spark data pipeline</h1>
			<p>In this section, you <a id="_idIndexMarker718"/>will build a real data pipeline for gathering and processing datasets. The objective of the processing is to format, clean, and transform data into a state that is useable for model training. Before writing our data pipeline, let's first understand the data.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor136"/>Preparing the environment</h2>
			<p>In order to perform <a id="_idIndexMarker719"/>the following exercises, we first need to set up a couple of things. You need to set up a PostgreSQL database to hold the historical flights data. And you need to upload files to an S3 bucket in MinIO. We used both a relational database and an S3 bucket to better demonstrate how to gather data from disparate data sources.</p>
			<p>We have prepared a Postgres database container image that you can run on your Kubernetes cluster. The container image is available at <a href="https://quay.io/repository/ml-on-k8s/flights-data">https://quay.io/repository/ml-on-k8s/flights-data</a>. It runs a PostgreSQL database with preloaded flights data in a table called <strong class="source-inline">flights</strong>.</p>
			<p>Go through the following steps to run this container, verify the database table, and upload CSV files onto MinIO:</p>
			<ol>
				<li value="1">Run the Postgres database container by running the following command on the same machine where your minikube is running:<p class="source-code"><strong class="bold">kubectl create -f chapter9/deployment-pg-flights-data.yaml -n ml-workshop</strong></p></li>
			</ol>
			<p>You should see a message telling you the <strong class="source-inline">deployment</strong> object is created.</p>
			<ol>
				<li value="2">Expose the Pods of this deployment through a service by running the following command:<p class="source-code"><strong class="bold">kubectl create -f chapter9/service-pg-flights-data.yaml -n ml-workshop</strong></p></li>
			</ol>
			<p>You should see a message saying that the service object has been created.</p>
			<ol>
				<li value="3">Explore the contents of the database. You can do this by going inside the Pod, running the Postgres client <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>), <strong class="source-inline">psql</strong>, and running SQL scripts. Execute the following command to connect to the Postgres Pod and run the Postgres client interface:<p class="source-code"><strong class="bold">POD_NAME=$(kubectl get pods -n ml-workshop –l app=pg-flights-data)</strong></p></li>
				<li>Connect to the Pod. You can do this by executing the following command:<p class="source-code"><strong class="bold">kubectl exec -it $POD_NAME -n ml-workshop -- bash</strong></p></li>
				<li>Run the Postgres client CLI, <strong class="source-inline">psql</strong>, and verify the tables. Run the following command to log in to the Postgres database from the command line:<p class="source-code"><strong class="bold">psql –U postgres</strong></p></li>
			</ol>
			<p>This will run the<a id="_idIndexMarker720"/> client CLI and connect to the default <a id="_idIndexMarker721"/>database. </p>
			<ol>
				<li value="6">Verify that the tables exist. There should be a table named <strong class="source-inline">flights</strong>. Run the following command from the <strong class="source-inline">psql</strong> shell to verify the correctness of the table:<p class="source-code"><strong class="bold">select count(1) from flights;</strong></p></li>
			</ol>
			<p>This should give you the number of records in the <strong class="source-inline">flights</strong> table, which is more than 5.8 million, as shown in <em class="italic">Figure 9.4</em>:</p>
			<div>
				<div id="_idContainer242" class="IMG---Figure">
					<img src="image/B18332_09_004.jpg" alt="Figure 9.4 – Record count from the flights table&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – Record count from the flights table</p>
			<ol>
				<li value="7">Upload the rest of the data to an S3 bucket in MinIO. Open a browser window on the same machine where minikube is running, and navigate to 	. Use the username <strong class="source-inline">minio</strong> and password <strong class="source-inline">minio123</strong>. Remember to replace <strong class="source-inline">&lt;minikube_ip&gt;</strong> with the IP address of your minikube instance.</li>
				<li>Navigate to <strong class="bold">Buckets</strong> and then hit the <strong class="bold">Create Bucket +</strong> button. Name the bucket <strong class="source-inline">airport-data</strong> and <a id="_idIndexMarker722"/>hit the <strong class="bold">Create Bucket</strong> button, as shown in <em class="italic">Figure 9.5</em>:</li>
			</ol>
			<div>
				<div id="_idContainer243" class="IMG---Figure">
					<img src="image/B18332_09_005.jpg" alt="Figure 9.5 – MinIO Create a Bucket dialog &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – MinIO Create a Bucket dialog </p>
			<ol>
				<li value="9">While inside the bucket, upload two CSV files from the <strong class="source-inline">chapter9/data/</strong> folder onto the <strong class="source-inline">airport-data</strong> bucket, as shown in <em class="italic">Figure 9.6</em>:</li>
			</ol>
			<div>
				<div id="_idContainer244" class="IMG---Figure">
					<img src="image/B18332_09_006.jpg" alt="Figure 9.6 – Airport and airline data files&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – Airport and airline data files</p>
			<p>In the real world, you do not need to take the preceding steps. The data sources should already exist and you need to know where to get them. However, for the purpose of the following exercises, we<a id="_idIndexMarker723"/> had to load this data into our environment to make it available for the next steps.</p>
			<p>You now have the data loaded to the platform. Let's explore and understand the data a little bit more.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor137"/>Understanding data</h2>
			<p>Understanding the<a id="_idIndexMarker724"/> data includes the following activities. It is important to<a id="_idIndexMarker725"/> understand the characteristics of all the datasets involved in order to come up with a strategy and design for the pipeline:</p>
			<ul>
				<li><em class="italic">Know where the data will be collected from</em>. Data may come from a variety of sources. It may come from a relational database, object store, NoSQL database, graph database, data stream, S3 bucket, HDFS, filesystem, or FTP. With this information in hand, you will be able to prepare the connectivity you need for your data pipeline. In your case, you need to collect it from a PostgreSQL database and S3 buckets.</li>
				<li><em class="italic">Understand the format of the data</em>. Data can come in many shapes and forms. Whether it's a CSV file, a SQL table, a Kafka stream, an MQ stream, a Parquet file, an Avro file, or even an Excel file, you need to have the right tools that can read such a format. Understanding the format helps you prepare the tools or libraries you will need to use to read these datasets.</li>
				<li><em class="italic">Clean unimportant or irrelevant data</em>. Understanding what data is important and what is irrelevant helps you design your pipeline in a more efficient way. For example, if you have a dataset with fields for <strong class="source-inline">airline_name</strong> and <strong class="source-inline">airline_id</strong>, you may want to drop <strong class="source-inline">airline_name</strong> in the final output and just use <strong class="source-inline">airline_id</strong> alone. This means one field less to be encoded into numbers, which will improve the performance of model training.</li>
				<li><em class="italic">Understand the relationships between different datasets</em>. Identify the identifier fields or primary keys, and understand the join keys and aggregation levels. You need to know this so that you can flatten the data structure and make it easier for the data scientist to consume your datasets.</li>
				<li><em class="italic">Know where to store the processed data</em>. You need to know where you will write the processed data so you can prepare the connectivity requirements and understand the interface.</li>
			</ul>
			<p>Given the<a id="_idIndexMarker726"/> preceding activities, you need a way to access and explore the data <a id="_idIndexMarker727"/>sources. The next section will show you how to read a database table from within a Jupyter notebook.</p>
			<h3>Reading data from a database</h3>
			<p>Using a Jupyter <a id="_idIndexMarker728"/>notebook, let's look at <a id="_idIndexMarker729"/>the <a id="_idIndexMarker730"/>data. Use the following steps to get started with data exploration, starting with reading data from a PostgreSQL database.</p>
			<p>The entire data exploration notebook can be found in this book's Git repository at <strong class="source-inline">chapter9/explore_data.ipynb</strong>. We recommend that you use this notebook to do additional data exploration. It can be by simply displaying the fields, counting the number of occurrences of the same values in a column, and finding the relationships between the data sources:</p>
			<ol>
				<li value="1">Launch a Jupyter notebook by navigating to <strong class="source-inline">https://jupyterhub.&lt;minikube_ip&gt;.nip.io</strong>. If you are prompted for login credentials, you need to log in with the Keycloak user you've created. The username is <strong class="source-inline">mluser</strong> and the password is <strong class="source-inline">mluser</strong>. Launch the <strong class="bold">Elyra Notebook Image with Spark</strong> notebook, as shown in <em class="italic">Figure 9.7</em>. Because we will be reading a big dataset with 5.8 million records, let's use the <strong class="bold">Large</strong> container size. Make sure that, in your environment, you have enough capacity for running <a id="_idIndexMarker731"/>a large container. If you do not have enough capacity, try running on a<a id="_idIndexMarker732"/> medium <a id="_idIndexMarker733"/>container.</li>
			</ol>
			<div>
				<div id="_idContainer245" class="IMG---Figure">
					<img src="image/B18332_09_007.jpg" alt="Figure 9.7 – JupyterHub launch page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – JupyterHub launch page</p>
			<ol>
				<li value="2">Create a Python 3 notebook. You will use this notebook to explore the data. You can do this by selecting the <strong class="bold">File</strong> | <strong class="bold">New</strong> | <strong class="bold">Notebook</strong> menu option. Then, select <strong class="bold">Python 3</strong> as the kernel, as shown in <em class="italic">Figure 9.8</em>:</li>
			</ol>
			<div>
				<div id="_idContainer246" class="IMG---Figure">
					<img src="image/B18332_09_008.jpg" alt="Figure 9.8 – Elyra notebook's kernel selection dialog&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – Elyra notebook's kernel selection dialog</p>
			<ol>
				<li value="3">You can start by looking at the <strong class="source-inline">flights</strong> table in the database. The most basic way of<a id="_idIndexMarker734"/> accessing the database <a id="_idIndexMarker735"/>is <a id="_idIndexMarker736"/>through a PostgreSQL Python client library. Use <strong class="source-inline">psycopg2</strong> for the exercises. You may also choose a different client library to connect to the PostgreSQL database. The code snippet in <em class="italic">Figure 9.9</em> is the most basic example:</li>
			</ol>
			<div>
				<div id="_idContainer247" class="IMG---Figure">
					<img src="image/B18332_09_009.jpg" alt="Figure 9.9 – Basic connection to PostgreSQL using psycopg2 &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9 – Basic connection to PostgreSQL using psycopg2 </p>
			<ol>
				<li value="4">Another, more<a id="_idIndexMarker737"/> elegant, way <a id="_idIndexMarker738"/>of accessing the data is through <strong class="bold">pandas</strong> or <strong class="bold">PySpark</strong>. Both<a id="_idIndexMarker739"/> pandas and PySpark allow you to <a id="_idIndexMarker740"/>access<a id="_idIndexMarker741"/> data, leveraging the functional programming approach through data frames rather than the procedural approach in <em class="italic">Step 3</em>. The difference between pandas and Spark is that Spark queries can be executed in a distributed manner, using multiple machines or Pods executing your query. This is ideal for huge datasets. However, pandas provides more aesthetically appealing visualizations than Spark, which makes pandas good for exploring smaller datasets. <em class="italic">Figure 9.10</em> shows a snippet of how to access the database through pandas:</li>
			</ol>
			<div>
				<div id="_idContainer248" class="IMG---Figure">
					<img src="image/B18332_09_010.jpg" alt="Figure 9.10 – Basic connection to PostgreSQL using pandas&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.10 – Basic connection to PostgreSQL using pandas</p>
			<ol>
				<li value="5">If you need to transform a huge dataset, PySpark would be the ideal option for this. For example, let's say you need to transform and aggregate a table with 100 million records. You will need to distribute this work to multiple machines to<a id="_idIndexMarker742"/> get <a id="_idIndexMarker743"/>faster results. This is where Spark plays an important role. The code snippet<a id="_idIndexMarker744"/> in <em class="italic">Figure 9.11</em> shows how to read the PostgreSQL table through PySpark:</li>
			</ol>
			<div>
				<div id="_idContainer249" class="IMG---Figure">
					<img src="image/B18332_09_011.jpg" alt="Figure 9.11 – Reading a PostgreSQL table through PySpark&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.11 – Reading a PostgreSQL table through PySpark</p>
			<p>Because of the distributed architecture of Spark, you need to provide the partitioning information, particularly the number of partitions and the partition column(s), when reading a table from any relational database. Each partition will become a task in Spark's vernacular, and each task can be executed independently by a single CPU core. If the partition information is not provided, Spark will try to treat the entire table as a single partition. You do not want to do this, as this table has 5.8 million records and it may not fit in the memory of a single Spark worker node.</p>
			<p>You also need to provide some information about the Spark cluster, such as the master URL and the packages required to run your Spark application. In the example in <em class="italic">Figure 9.12</em>, we included the <strong class="source-inline">org.postgresql:postgresql:42.3.3</strong> package. This is the PostgreSQL JDBC driver that Spark needs to connect to the database. Spark will automatically<a id="_idIndexMarker745"/> download this package<a id="_idIndexMarker746"/> from <a id="_idIndexMarker747"/>Maven at the application startup.</p>
			<h3>Reading data from an S3 bucket</h3>
			<p>Now that you<a id="_idIndexMarker748"/> have learned different <a id="_idIndexMarker749"/>ways <a id="_idIndexMarker750"/>of accessing a PostgreSQL database from a Jupyter notebook, let's explore the rest of the data. While the <strong class="source-inline">flights</strong> table in the database contains the flight information, we also have the <em class="italic">airport</em> and <em class="italic">airline </em>information provided as CSV files and hosted in an S3 bucket in MinIO.</p>
			<p>Spark can communicate with any S3 server through the <strong class="source-inline">hadoop-aws</strong> library. <em class="italic">Figure 9.12</em> shows how to access a CSV file in an S3 bucket from a notebook using Spark:</p>
			<div>
				<div id="_idContainer250" class="IMG---Figure">
					<img src="image/B18332_09_012.jpg" alt="Figure 9.12 – Spark code to read an S3 bucket from a notebook&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.12 – Spark code to read an S3 bucket from a notebook</p>
			<p>Take note that we added a few more Spark submit arguments. This is to tell the Spark engine where the S3 server is and what driver library to use.</p>
			<p>After you have explored the datasets, you should have learned the following facts about the data:</p>
			<ul>
				<li>The <em class="italic">flights</em> table contains 5,819,079 records.</li>
				<li>There are 322 airports in the <strong class="source-inline">airports.csv</strong> file.</li>
				<li>There are 22 airlines in the <strong class="source-inline">airlines.csv</strong> file.</li>
				<li>There is no direct relationship between airports and airlines.</li>
				<li>The <strong class="source-inline">flights</strong> table uses the <strong class="source-inline">IATA_CODE</strong> airport from the <strong class="source-inline">airport</strong> CSV file as the origin and destination airport of a particular flight.</li>
				<li>The <strong class="source-inline">flights</strong> table is using the <strong class="source-inline">IATA_CODE</strong> airline from the <strong class="source-inline">airlines</strong> CSV file to tell which airline is serving a particular flight.</li>
				<li>All the airports are in the United States. This means that the country columns are useless<a id="_idIndexMarker751"/> for <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) training.</li>
				<li>The <strong class="source-inline">flights</strong> table has the <strong class="source-inline">SCHEDULED_DEPARTURE</strong>, <strong class="source-inline">DEPARTURE_TIME</strong>, and <strong class="source-inline">DEPARTURE_DELAY</strong> fields, which tell if a flight has been delayed and we can use to produce a <strong class="source-inline">label</strong> column for our ML training.</li>
			</ul>
			<p>Given these facts, we can say that we can use both the airports and airline data to add additional airport <a id="_idIndexMarker752"/>and airline information to <a id="_idIndexMarker753"/>the <a id="_idIndexMarker754"/>original <strong class="source-inline">flights</strong> data. This process is<a id="_idIndexMarker755"/> usually called <strong class="bold">enrichment</strong> and can be done through data frame joins. We can also use the row count information to optimize our Spark code.</p>
			<p>Now that you understand the data, you can start designing and building your pipeline.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor138"/>Designing and building the pipeline</h2>
			<p>Understanding<a id="_idIndexMarker756"/> the <a id="_idIndexMarker757"/>data is one thing, designing a pipeline is another. From the data you have explored in the previous section, you learned a few facts. We will use these facts to decide how to build our data pipeline.</p>
			<p>The objective is to produce a single, flat dataset containing all the vital information that may be useful for ML training. We said all vital information because we do not know for sure which fields or features are important until we do the actual ML training. As a data engineer, you can take an educated guess, based on your understanding of the data and with the help of an SME, on which fields are important and which ones are not. Along the ML life cycle, the data scientist may get back to you to ask for more fields, drop some fields, or perform some transformation on the data.</p>
			<p>With the objective of producing a single dataset in mind, we need to enrich the flight data with the airport and airline data. To enrich the original flight data with airports and airlines data, we need to do a data frame <strong class="source-inline">join</strong> operation. We also need to take note that the flight <a id="_idIndexMarker758"/>data has millions of records, while the airport and <a id="_idIndexMarker759"/>airline data has less than 50. We can use this information to influence Spark's <strong class="source-inline">join</strong> algorithm for optimization.</p>
			<h3>Preparing a notebook for Data frame joins</h3>
			<p>To start, create<a id="_idIndexMarker760"/> a new<a id="_idIndexMarker761"/> notebook<a id="_idIndexMarker762"/> that performs the join, and then adds this notebook as a stage to the pipeline. The following steps will show you how to do this:</p>
			<ol>
				<li value="1">Create a new notebook. Call it <strong class="source-inline">merge_data.ipynb</strong>.</li>
				<li>Use Spark to gather the data from the Postgres and S3 buckets. Use the knowledge you learned in the preceding section. <em class="italic">Figure 9.13</em> shows the data reading part of the notebook. We have also provided a utility Python file, <strong class="source-inline">chapter9/spark_util.py</strong>. This wraps the creation of Spark context to make your notebook more readable. The code snippet in <em class="italic">Figure 9.13</em> shows you how to use this utility:</li>
			</ol>
			<div>
				<div id="_idContainer251" class="IMG---Figure">
					<img src="image/B18332_09_013.jpg" alt="Figure 9.13 – Spark code for preparing the data frames&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.13 – Spark code for preparing the data frames</p>
			<p>Notice<a id="_idIndexMarker763"/> the new<a id="_idIndexMarker764"/> <strong class="source-inline">import</strong> <a id="_idIndexMarker765"/>statement here for <strong class="source-inline">broadcast()</strong>. You will use this function for optimization in the next step.</p>
			<ol>
				<li value="3">Perform a data frame join in Spark, as shown in <em class="italic">Figure 9.14</em>. You need to join all three data frames that you prepared in <em class="italic">Step 2</em>. From our understanding in the previous section, both the airport and airline data should be merged by <strong class="source-inline">IATA_CODE</strong> as the primary key. But first, let's do the join to the airline data. Notice the resulting schema after the join; there are two additional columns at the bottom <a id="_idIndexMarker766"/>when <a id="_idIndexMarker767"/>compared <a id="_idIndexMarker768"/>to the original schema. These new columns came from the <strong class="source-inline">airlines.csv</strong> file:</li>
			</ol>
			<div>
				<div id="_idContainer252" class="IMG---Figure">
					<img src="image/B18332_09_014.jpg" alt="Figure 9.14 – Spark code for basic data frame join&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.14 – Spark code for basic data frame join</p>
			<ol>
				<li value="4">Joining the airport data is a little tricky because you must join it twice: once to <strong class="source-inline">origin_airport</strong> and another to <strong class="source-inline">destination_airport</strong>. If we just follow the same approach as <em class="italic">Step 3</em>, the join will work, and the columns will be added to the schema. The problem is that it will be difficult to tell which airport fields represent the destination airport and which ones are for the airport of origin. <em class="italic">Figure 9.15</em> shows how the field names are duplicated:</li>
			</ol>
			<div>
				<div id="_idContainer253" class="IMG---Figure">
					<img src="image/B18332_09_015.jpg" alt="Figure 9.15 – Duplicated columns after the join&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.15 – Duplicated columns after the join</p>
			<ol>
				<li value="5">The<a id="_idIndexMarker769"/> simplest<a id="_idIndexMarker770"/> way to solve <a id="_idIndexMarker771"/>this is to create new data frames with prefixed field names (<strong class="source-inline">ORIG_</strong> for origin airports and <strong class="source-inline">DEST_</strong> for destination airports). You can also do the same for the airline fields. <em class="italic">Figure 9.16 </em>shows how to do this:</li>
			</ol>
			<div>
				<div id="_idContainer254" class="IMG---Figure">
					<img src="image/B18332_09_016.jpg" alt="Figure 9.16 – Adding prefixes to the field names&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.16 – Adding prefixes to the field names</p>
			<ol>
				<li value="6">Replace<a id="_idIndexMarker772"/> the <strong class="source-inline">df_airports</strong> data<a id="_idIndexMarker773"/> frame with <strong class="source-inline">df_o_airports</strong> and <strong class="source-inline">df_d_airports</strong> in<a id="_idIndexMarker774"/> your <strong class="source-inline">join</strong> statements, as shown in <em class="italic">Figure 9.17</em>. Now, you have a more readable data frame:</li>
			</ol>
			<div>
				<div id="_idContainer255" class="IMG---Figure">
					<img src="image/B18332_09_017.jpg" alt="Figure 9.17 – Updated join statements with prefixed data frames&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.17 – Updated join statements with prefixed data frames</p>
			<p>One thing to <a id="_idIndexMarker775"/>note<a id="_idIndexMarker776"/> in <a id="_idIndexMarker777"/>the <strong class="source-inline">join</strong> statements is the <strong class="source-inline">broadcast()</strong> function. In the previous section, we talked about the importance of knowing the sizes of your datasets so that you can optimize your code. The <strong class="source-inline">broadcast()</strong> function gives a hint to the Spark engine that the given data frame should be broadcasted and that the <strong class="source-inline">join</strong> operation must use the broadcast <strong class="source-inline">join</strong> algorithm. This means that before execution, Spark will distribute a copy of the <strong class="source-inline">df_airlines</strong>, <strong class="source-inline">df_o_airports</strong>, and <strong class="source-inline">df_d_airports</strong> data frames to each of the Spark executors so that they can be joined to the records of each partition. In order to make the broadcast <strong class="source-inline">join</strong> effective, you need to pick the <em class="italic">smaller data frames</em> to be broadcasted. If you want to know more about this, refer to the performance tuning documentation of Spark in the following URL: <a href="https://spark.apache.org/docs/latest/sql-performance-tuning.html">https://spark.apache.org/docs/latest/sql-performance-tuning.html</a>.</p>
			<p>You have just learned how to join data frames using PySpark. Because PySpark statements are lazily evaluated, the actual execution of the <strong class="source-inline">join</strong> operations hasn't taken place<a id="_idIndexMarker778"/> yet. That is<a id="_idIndexMarker779"/> why <a id="_idIndexMarker780"/>the <strong class="source-inline">printSchema()</strong> execution is fast. Spark only performs the processing when the actual data is required. One such scenario is when you persist the actual data to storage.</p>
			<h3>Persisting the data frames</h3>
			<p>To get the result <a id="_idIndexMarker781"/>of the joins, you need to turn the<a id="_idIndexMarker782"/> data frame into physical data. You will write the data frame to S3 storage so that the next stage of your data pipeline can read it. <em class="italic">Figure 9.18</em> shows a code snippet that writes the joined flights data frame onto a CSV file in MinIO:</p>
			<div>
				<div id="_idContainer256" class="IMG---Figure">
					<img src="image/B18332_09_018.jpg" alt="Figure 9.18 – Writing a data frame to an S3 bucket&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.18 – Writing a data frame to an S3 bucket</p>
			<p>Executing this will take some time because this is where the actual processing of 5.8 million records takes place. While this is running, you can take a look at what is going on in the Spark cluster. When you started the notebook, it created a Spark cluster in Kubernetes that dedicated the user <strong class="source-inline">mluser</strong> to you. The Spark GUI is exposed at https://spark-cluster-mluser.&lt;minikube_ip&gt;.nip.io. Navigate to this URL to monitor the Spark application and to check the status of the application's jobs. You should see one running application named <strong class="bold">Enrich flights data</strong>. Clicking on this application name will take you to a more detailed view of the jobs being processed, as shown in <em class="italic">Figure 9.19</em>:</p>
			<div>
				<div id="_idContainer257" class="IMG---Figure">
					<img src="image/B18332_09_019.jpg" alt="Figure 9.19 – Spark application UI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.19 – Spark application UI</p>
			<p><em class="italic">Figure 9.19</em> shows<a id="_idIndexMarker783"/> the details of the <strong class="bold">Enrich flights data</strong> application. Each<a id="_idIndexMarker784"/> application is made up of jobs, which are operations. At the bottom of the screen, you can see the <strong class="bold">Completed Jobs</strong> section, which includes the broadcast operations. You can also tell that the broadcast operations took around 1 second. Under the <strong class="bold">Active Jobs</strong> section, you see the currently running operations, which, in our case, is the actual processing including the reading of the <strong class="source-inline">flights</strong> data from the database, renaming of columns, joining of the data frames, and writing the output to an S3 bucket. This is performed for each partition of the data frame, which <a id="_idIndexMarker785"/>translates to <strong class="bold">tasks</strong> in Spark. On the right-most column of the <strong class="bold">Active Jobs</strong> section, you see the tasks and their progress. Because we partitioned our <strong class="source-inline">flights</strong> data frame by <strong class="source-inline">day of month</strong>, there are 31 partitions. Spark also <a id="_idIndexMarker786"/>created 31 parallel processing tasks. Each of these tasks is scheduled to run on <strong class="bold">Spark executors</strong>. In <em class="italic">Figure 9.19</em>, the details say that for the last 1.2 minutes of processing, there are 13 successfully completed tasks out of 31, and there are four currently running.</p>
			<p>You may also find tasks that failed in some cases. Failed tasks are automatically rescheduled by Spark to another executor. By default, if the same task fails four times in a row, the whole application will be terminated and marked as failed. There are several reasons task failure happens. Some of them include network interruption or resource congestion, such as out-of-memory exceptions or timeouts. This is why it is important to understand the<a id="_idIndexMarker787"/> data so that you can fine-tune the <a id="_idIndexMarker788"/>partitioning logic. Here is a basic rule to take note of: the bigger the number of partitions, the smaller the partition size. A smaller partition size will have fewer chances of out-of-memory exceptions, but it also adds more CPU overhead to scheduling. The Spark mechanism is a lot more complex than this, but it is a good start to understanding the relationship between partitions, tasks, jobs, and executors.</p>
			<p>Almost half of the data engineering work is actually spent on optimizing data pipelines. There are quite a few techniques to optimize Spark applications, including code optimization, partitioning, and executor sizing. We will not discuss this topic in detail in this book. However, if you want to know more about this topic, you can always refer to the performance tuning documentation of Spark.</p>
			<div>
				<div id="_idContainer258" class="IMG---Figure">
					<img src="image/B18332_09_020.jpg" alt="Figure 9.20 – S3 bucket with Parquet files&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.20 – S3 bucket with Parquet files</p>
			<p>After the Spark application is completed, the data should be written in S3 in multiple files, with one file representing one partition in Parquet format, as shown in <em class="italic">Figure 9.20</em>. The <strong class="bold">Parquet</strong> file <a id="_idIndexMarker789"/>format is a columnar data format, meaning the data is organized by columns rather than by rows as in a typical CSV file. The main advantage of Parquet is that you can cherry-pick columns that you want to read without having to scan the entire dataset. This makes Parquet ideal for analytics, reporting, and also data cleaning, which is <a id="_idIndexMarker790"/>what <a id="_idIndexMarker791"/>you need to do next.</p>
			<p>You can find the full <strong class="source-inline">merge_data.ipynb</strong> notebook in this book's Git repository under the <strong class="source-inline">chapter9</strong> folder. However, we strongly recommend that you create your own notebook from scratch to maximize the learning experience.</p>
			<h3>Cleaning the datasets</h3>
			<p>You now have a<a id="_idIndexMarker792"/> flat and enriched version of the <strong class="source-inline">flights</strong> dataset. The <a id="_idIndexMarker793"/>next step is to clean the data, remove unwanted fields, drop unwanted rows, homogenize the field values, derive new fields, and, perhaps, transform some of the fields.</p>
			<p>To start with, create a new notebook and use this notebook to read the Parquet file we generated, and write it as a cleaned version of the dataset. The following steps will walk you through the process:</p>
			<ol>
				<li value="1">Create a new notebook named <strong class="source-inline">clean_data.ipynb</strong>.</li>
				<li>Load the <strong class="source-inline">flights</strong> data Parquet files from the <strong class="source-inline">flights-data/flights</strong> S3 bucket, as shown in <em class="italic">Figure 9.21</em>. Verify the schema and the row count. The row count should be slightly less than the original dataset. This is because the <strong class="source-inline">join</strong> operations performed in the previous steps are inner joins, and there are records in the <a id="_idIndexMarker794"/>original <strong class="source-inline">flights</strong> data that do not<a id="_idIndexMarker795"/> have airport or airline references.</li>
			</ol>
			<div>
				<div id="_idContainer259" class="IMG---Figure">
					<img src="image/B18332_09_021.jpg" alt="Figure 9.21 – Reading Parquet data from S3&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.21 – Reading Parquet data from S3</p>
			<ol>
				<li value="3">Remove the unwanted or duplicated fields, drop fields that have the same value throughout the entire dataset, and create a derived Boolean field called <strong class="source-inline">DELAYED</strong>, with the value <strong class="source-inline">1</strong> for delayed flights and <strong class="source-inline">0</strong> for non-delayed flights. Let's assume that we only consider a flight as delayed if it is delayed for 15 minutes or more. You can always change this depending on the requirement. Let's do this slowly. Drop the<a id="_idIndexMarker796"/> unwanted columns first, as<a id="_idIndexMarker797"/> shown in <em class="italic">Figure 9.22</em>:</li>
			</ol>
			<div>
				<div id="_idContainer260" class="IMG---Figure">
					<img src="image/B18332_09_022.jpg" alt="Figure 9.22 – Dropping unwanted columns&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.22 – Dropping unwanted columns</p>
			<p>We do not need <strong class="source-inline">AI_IATA_CODE</strong>, <strong class="source-inline">ORIG_IATA_CODE</strong>, and <strong class="source-inline">DEST_IATA_CODE</strong> because they are the same as the <strong class="source-inline">airline</strong>, <strong class="source-inline">origin_airport</strong>, and <strong class="source-inline">destination_airport</strong> columns, respectively.</p>
			<ol>
				<li value="4">Finding the<a id="_idIndexMarker798"/> columns with the same values throughout <a id="_idIndexMarker799"/>the dataset is an expensive operation. This means you need to count the distinct values of each column for 5 million records. Luckily, Spark provides the <strong class="source-inline">approx_count_distinct()</strong> function, which is pretty fast. The code snippet in <em class="italic">Figure 9.23</em> shows how to find the columns with uniform values:</li>
			</ol>
			<div>
				<div id="_idContainer261" class="IMG---Figure">
					<img src="image/B18332_09_023.jpg" alt="Figure 9.23 – Dropping columns that have uniform values in all rows&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.23 – Dropping columns that have uniform values in all rows</p>
			<ol>
				<li value="5">Finally, create <a id="_idIndexMarker800"/>the <strong class="source-inline">label</strong> field that determines whether the flight is delayed or not. The data scientist may use this field as the label for training. However, the data scientist may also use an analog range, such as <strong class="source-inline">departure_delay</strong>, depending on the algorithm chosen. So, let's keep the <strong class="source-inline">departure_delay</strong> field together with the new Boolean field based on the 15-minute threshold on <strong class="source-inline">departure_delay</strong>. Let's call this new field <strong class="source-inline">DELAYED</strong>:</li>
			</ol>
			<div>
				<div id="_idContainer262" class="IMG---Figure">
					<img src="image/B18332_09_024.jpg" alt="Figure 9.24 – Creating the DELAYED column&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.24 – Creating the DELAYED column</p>
			<p><em class="italic">Figure 9.24</em> shows the code snippet for creating a derived column. Test the column creation logic by running a simple query using the <strong class="source-inline">show()</strong> function.</p>
			<ol>
				<li value="6">Now, write the <a id="_idIndexMarker801"/>physical data to the same S3<a id="_idIndexMarker802"/> bucket under the <strong class="source-inline">flights-clean</strong> path. We also want to write the output in Parquet (see <em class="italic">Figure 9.25</em>):</li>
			</ol>
			<div>
				<div id="_idContainer263" class="IMG---Figure">
					<img src="image/B18332_09_025.jpg" alt="Figure 9.25 – Writing the final data frame to S3&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.25 – Writing the final data frame to S3</p>
			<p>As a data engineer, you need to agree with the data scientist on the output format. Some data scientists may want to get a single huge CSV file dataset instead of multiple Parquet files. In our case, let's assume that the data scientist prefers to read multiple Parquet files.</p>
			<ol>
				<li value="7"><em class="italic">Step 6</em> may take quite some time. You can visit the Spark UI to monitor the application execution.</li>
			</ol>
			<p>You can find the <a id="_idIndexMarker803"/>full <strong class="source-inline">clean_data.ipynb</strong> notebook<a id="_idIndexMarker804"/> in this book's Git repository under the <strong class="source-inline">chapter9</strong> folder. However, we strongly recommend that you create your own notebook from scratch to maximize the learning experience.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor139"/>Using the Spark UI to monitor your data pipeline</h2>
			<p>While running <a id="_idIndexMarker805"/>Spark applications, you may <a id="_idIndexMarker806"/>want to look deeper into what Spark is actually doing in order to optimize your pipeline. The Spark UI provides very useful information. The landing page from the master displays the list of worker nodes and applications, as shown in <em class="italic">Figure 9.26</em>:</p>
			<div>
				<div id="_idContainer264" class="IMG---Figure">
					<img src="image/B18332_09_026.jpg" alt="Figure 9.26 – Spark cluster landing page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.26 – Spark cluster landing page</p>
			<p>The landing page also displays the historical application runs. You can see some of the details of the completed application by clicking on one of the completed application IDs. However, we<a id="_idIndexMarker807"/> are more interested in the running <a id="_idIndexMarker808"/>application when monitoring applications. Let's understand the information in the UI a little bit more.</p>
			<h3>Exploring the workers page</h3>
			<p>Workers<a id="_idIndexMarker809"/> are machines<a id="_idIndexMarker810"/> that are part of the Spark cluster. Their main responsibility is to run executors. In our<a id="_idIndexMarker811"/> case, the <strong class="bold">worker nodes</strong> are Kubernetes Pods with a <a id="_idIndexMarker812"/>worker <strong class="bold">Java virtual machine</strong> (<strong class="bold">JVM</strong>) running in them. Each Worker can host one or more executors. However, this is not a good idea when running Spark workers on Kubernetes, so you should configure your executors in a way that only one executor can run in a worker:</p>
			<div>
				<div id="_idContainer265" class="IMG---Figure">
					<img src="image/B18332_09_027.jpg" alt="Figure 9.27 – Spark Worker view&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.27 – Spark Worker view</p>
			<p>Clicking on one of the workers in the UI will take you to the worker UI where you can see all the executors that this worker has run or is currently running. You can also see which application owns <a id="_idIndexMarker813"/>the <a id="_idIndexMarker814"/>executor. You can see how much CPU or memory is allocated to it, and you can even see the logs of each executor.</p>
			<h3>Exploring the Executors page</h3>
			<p>Executors are <a id="_idIndexMarker815"/>processes <a id="_idIndexMarker816"/>that run inside the worker nodes. Their main responsibility is to execute tasks. An executor is nothing but a Java or JVM process running on the worker node. The worker JVM process manages instances of executors within the same host. Going to http://spark-cluster-mluser.&lt;minikube_ip&gt;.nip.io/proxy/&lt;application_id&gt;/executors/ will take you to the <strong class="bold">Executors</strong> page, which will list all the executors belonging to the current application, as shown in <em class="italic">Figure 9.28</em>:</p>
			<div>
				<div id="_idContainer266" class="IMG---Figure">
					<img src="image/B18332_09_028.jpg" alt="Figure 9.28 – Spark Executors page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.28 – Spark Executors page</p>
			<p>On this page, you will find useful metrics that are important in fine-tuning and optimizing your application. For example, you can see the resource usage, garbage collection time, and <a id="_idIndexMarker817"/>shuffles. <strong class="bold">Shuffles</strong> are exchanges of data across multiple executors, which will<a id="_idIndexMarker818"/> happen when<a id="_idIndexMarker819"/> you perform an aggregate function, for example. You want to keep this as small as possible.</p>
			<h3>Exploring the application page</h3>
			<p>Applications<a id="_idIndexMarker820"/> in Spark<a id="_idIndexMarker821"/> are any processes that own a Spark context. It could be a running Java, Scala, or Python application that created a Spark session or Spark context and submitted it to the Spark master URL. The applications may not necessarily run in the Spark cluster. It could be anywhere in the network as long as it can connect to the Spark master. However, there is also a mode whereby the application, also called the driver application, is executed inside one of the Spark executors. In our case, the driver application is the Jupyter notebook that is running outside of the Spark cluster. This is why, in <em class="italic">Figure 9.28</em>, you can see one executor, called <strong class="bold">driver</strong>, and not an actual executor ID.</p>
			<p>Clicking the application name of a running application from the landing page will bring you to the application UI page. This page displays all the jobs that belong to the current application. A job is an operation that alters the data frame. Each job is composed of one or more tasks. Tasks are a pair of an operation and a partition of a data frame. This is the unit of work that is distributed to the executors. In computer science, this is equivalent to a <strong class="bold">closure</strong>. These are shipped over the network as binaries to the worker nodes<a id="_idIndexMarker822"/> for the<a id="_idIndexMarker823"/> executors to execute. <em class="italic">Figure 9.29</em> shows the application UI page:</p>
			<div>
				<div id="_idContainer267" class="IMG---Figure">
					<img src="image/B18332_09_029.jpg" alt="Figure 9.29 – Spark application UI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.29 – Spark application UI</p>
			<p>In the example in <em class="italic">Figure 9.29</em>, you can see that active job <em class="italic">5</em> has five tasks, where four tasks are running. The <strong class="bold">Tasks</strong> level of parallelism is dependent on the number of CPU cores allocated to the application. You can also get even deeper into a particular job. If you go to http://spark-cluster-mluser.&lt;minikube_ip&gt;.nip.io/proxy/&lt;application_id&gt;/jobs/job/?id=&lt;job_id&gt;, you should see the stages of the job and the DAG of each stage.</p>
			<div>
				<div id="_idContainer268" class="IMG---Figure">
					<img src="image/B18332_09_030.jpg" alt="Figure 9.30 – Spark job detail page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.30 – Spark job detail page</p>
			<p>The Spark GUI is<a id="_idIndexMarker824"/> extremely <a id="_idIndexMarker825"/>useful when performing diagnostics and fine-tuning complex data processing applications. Spark is also well documented, and we recommend that you visit Spark's documentation at the following link: <a href="https://spark.apache.org/docs/3.0.0">https://spark.apache.org/docs/3.0.0</a>.</p>
			<p>Now that you have created a notebook for enriching the <strong class="source-inline">flights</strong> data and another notebook for cleaning up the dataset to prepare the dataset for the next stage of the ML project life cycle, let's look at how you can automate the execution of these notebooks.</p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor140"/>Building and executing a data pipeline using Airflow</h1>
			<p>In the<a id="_idIndexMarker826"/> preceding<a id="_idIndexMarker827"/> section, you have built your data <a id="_idIndexMarker828"/>pipeline<a id="_idIndexMarker829"/> to ingest and process data. Imagine that new <strong class="source-inline">flights</strong> data is available once a week and you need to process the new data repeatedly. One way is to run the data pipeline manually; however, this approach may not scale as the number of data pipelines grows. Data engineers' time would be used more efficiently in writing new pipelines instead of repeatedly running the old ones. The second concern is security. You may have written the data pipeline on sample data and your team may not have access to production data to execute the data pipeline.</p>
			<p>Automation provides the solution to both problems. You can schedule your data pipelines to run as required while the data engineer works on more interesting work. Your automated pipeline can connect to production data without any involvement from the development team, which will result in better security.</p>
			<p>The ML platform contains Airflow, which can automate the execution and scheduling of your data pipelines. Refer to <a href="B18332_07_ePub.xhtml#_idTextAnchor098"><em class="italic">Chapter 7</em></a>, <em class="italic">Model Deployment and Automation</em>, for an introduction to Airflow <a id="_idIndexMarker830"/>and how the <strong class="bold">visual editor</strong> allows the data engineers to build the data pipelines from the same IDE they have used for writing data pipelines. The integration provides the capabilities for data engineering teams to work in a self-serving and independent manner, which further improves the efficiency of your teams.</p>
			<p>In the next section, you will automate the data pipeline for the project that you have built in the preceding section.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor141"/>Understanding the data pipeline DAG</h2>
			<p>Let's first understand <a id="_idIndexMarker831"/>what is involved in running the data pipeline that you have built. Once you have the right information, it would be easy to automate the process.</p>
			<p>When you start writing your data pipeline in JupyterHub, you start with the <strong class="bold">Elyra Notebook Image with Spark</strong> notebook from the JupyterHub landing page. In the notebook, you connect to the Apache Spark cluster and start writing the data pipelines. The ML platform <em class="italic">knows</em> that for the <strong class="bold">Elyra Notebook Image with Spark</strong> image, it needs to start a new Spark cluster so that it can be used in the notebook. Once you have finished your work, you shut down your Jupyter environment, which results in shutting down the Apache Spark cluster by the ML platform.</p>
			<p>The following are three major<a id="_idIndexMarker832"/> stages involved in the execution of your data pipeline for the <strong class="source-inline">flights</strong> data:</p>
			<ol>
				<li value="1">Start the Spark cluster.</li>
				<li>Run the data pipeline notebook.</li>
				<li>Stop the Spark cluster.</li>
			</ol>
			<p><em class="italic">Figure 9.31</em> shows the stages of your DAG:</p>
			<div>
				<div id="_idContainer269" class="IMG---Figure">
					<img src="image/B18332_09_031.jpg" alt="Figure 9.31 – Airflow DAG for the flights project&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.31 – Airflow DAG for the flights project</p>
			<p>Each of these stages will be executed by Airflow as a discrete step. Airflow spins a Kubernetes Pod to <a id="_idIndexMarker833"/>run each of these stages while you provide the Pod image required to run each stage. The Pod runs the code defined in the Airflow pipeline for that stage.</p>
			<p>Let's see what each stage in our DAG is responsible for.</p>
			<h3>Starting the Spark cluster</h3>
			<p>In this stage, a<a id="_idIndexMarker834"/> new Spark cluster would be provisioned. This cluster will be dedicated to running one Airflow DAG. The role of automation is to submit the request for a new Spark cluster to Kubernetes as a CR. The Spark operator will then provide the cluster, which can be used for the next step in your DAG.</p>
			<p>Once the Airflow engine submits the request to create a Spark cluster, it will move to run the <a id="_idIndexMarker835"/>second stage.</p>
			<h3>Running the data pipeline</h3>
			<p>In this stage, the<a id="_idIndexMarker836"/> set of notebooks (<strong class="source-inline">merge_data</strong> and <strong class="source-inline">clean_data</strong>) that you have written earlier in this chapter will be executed by the Airflow DAG. Recall from <a href="B18332_07_ePub.xhtml#_idTextAnchor098"><em class="italic">Chapter 7</em></a>, <em class="italic">Model Deployment and Automation</em>, that Airflow uses different operators to run various stages of your automation pipeline (note that Airflow operators are different from Kubernetes Operators). Airflow provides a notebook operator to run the Jupyter notebooks. </p>
			<p>The role of automation is to run your data pipeline notebook using the notebook operator. After the data pipeline has finished executing your code, the Airflow engine will move to the next stage.</p>
			<h3>Stopping the Spark cluster</h3>
			<p>At this stage, a Spark <a id="_idIndexMarker837"/>cluster would be destroyed. The role of automation is to delete the Spark cluster CR created in the first stage of this DAG. The Spark operator will then terminate the cluster that was used to execute the data pipeline in the previous stage.</p>
			<p>Next is to define the container images that will be used by Airflow to execute each of these stages.</p>
			<h3>Registering container images to execute your DAG</h3>
			<p>You have just<a id="_idIndexMarker838"/> built your automation DAG to run your data pipeline, and each stage of this DAG will be executed by running a separate Pod for each stage:</p>
			<ol>
				<li value="1">To register the container images, first, open the JupyterHub IDE and click on the <strong class="bold">Runtime Images</strong> option on the left menu bar. You will see the following screen:</li>
			</ol>
			<div>
				<div id="_idContainer270" class="IMG---Figure">
					<img src="image/B18332_09_032.jpg" alt="Figure 9.32 – Container Runtime Images registration in your JupyterHub IDE&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.32 – Container Runtime Images registration in your JupyterHub IDE</p>
			<ol>
				<li value="2">Click on the <strong class="bold">+</strong> icon on the top right to register a new container. You will see the following <a id="_idIndexMarker839"/>screen:</li>
			</ol>
			<div>
				<div id="_idContainer271" class="IMG---Figure">
					<img src="image/B18332_09_033.jpg" alt="Figure 9.33 – Container Runtime Images registration details in your JupyterHub IDE&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.33 – Container Runtime Images registration details in your JupyterHub IDE</p>
			<p>For the <strong class="source-inline">flights</strong> data pipeline DAG, you will need the following two containers:</p>
			<ol>
				<li>The first container image will enable Airflow to run Python code. Fill the screen (shown in <em class="italic">Figure 9.33</em>) with the following details and click on the button titled <strong class="bold">SAVE &amp; CLOSE</strong>:<ul><li><strong class="bold">Name</strong>: <strong class="source-inline">AirFlow Python Runner</strong></li><li><strong class="bold">Description</strong>: <strong class="source-inline">A container with Python runtime</strong></li><li><strong class="bold">Source</strong>: <strong class="source-inline">quay.io/ml-on-k8s/airflow-python-runner:0.0.11</strong></li><li><strong class="bold">Image Pull Policy</strong>: <strong class="bold">IfNotPresent</strong></li></ul></li>
				<li>The second container image will enable Airflow to run the data pipeline notebook. Fill the screen shown in <em class="italic">Figure 9.33</em> with the following details and click on<a id="_idIndexMarker840"/> the button titled <strong class="bold">SAVE &amp; CLOSE</strong>:</li>
			</ol>
			<ul>
				<li><strong class="bold">Name</strong>: <strong class="source-inline">AirFlow PySpark Runner</strong></li>
				<li><strong class="bold">Description</strong>: <strong class="source-inline">A container with notebook and pyspark to enable execution of PySpark code</strong></li>
				<li><strong class="bold">Source</strong>: <strong class="source-inline">quay.io/ml-on-k8s/elyra-spark:0.0.4</strong></li>
				<li><strong class="bold">Image Pull Policy</strong>: <strong class="bold">IfNotPresent</strong></li>
			</ul>
			<p>In the next section, you will build and execute the three stages using Airflow.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor142"/>Building and running the DAG</h2>
			<p>In this section, you will build and deploy the DAG using the ML platform. You will first build the DAG using the drag-and-drop editor, and then modify the generated code to further customize the DAG.</p>
			<h3>Building an Airflow DAG using the visual editor </h3>
			<p>In this section, you<a id="_idIndexMarker841"/> build the DAG for your data <a id="_idIndexMarker842"/>processing flow. You will see how JupyterHub assists you in building your DAG using drag-and-drop capabilities:</p>
			<ol>
				<li value="1">Start with logging on to JupyterHub on the platform.</li>
				<li>Create a new pipeline by selecting the <strong class="bold">File</strong> | <strong class="bold">New</strong> | <strong class="bold">PipelineEditor</strong> menu option. You will get a new empty pipeline:</li>
			</ol>
			<div>
				<div id="_idContainer272" class="IMG---Figure">
					<img src="image/B18332_09_034.jpg" alt="Figure 9.34 – An empty Airflow DAG&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.34 – An empty Airflow DAG</p>
			<ol>
				<li value="3">As shown in the preceding screenshot, you can start by dragging the files required for your pipeline from the file browser on the left-hand side of the editor. For our <strong class="source-inline">flights</strong> DAG, the first step is to start a new Spark cluster. You will see <a id="_idIndexMarker843"/>a <a id="_idIndexMarker844"/>file named <strong class="source-inline">pipeline-helpers/start-spark-cluster</strong> on the browser. Drag it from the browser and drop it on your pipeline:</li>
			</ol>
			<div>
				<div id="_idContainer273" class="IMG---Figure">
					<img src="image/B18332_09_035.jpg" alt="Figure 9.35 – Building DAG stages using drag and drop&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.35 – Building DAG stages using drag and drop</p>
			<ol>
				<li value="4">Complete your pipeline by adding the files that are required for you. The full DAG for the <strong class="source-inline">flights</strong> data is available in the next step.</li>
				<li>We have added a pre-built one for you to use as a reference. Go to the folder named <strong class="source-inline">Chapter 9/</strong>, and open the <strong class="source-inline">flights.pipeline</strong> file. You can see that there<a id="_idIndexMarker845"/> are<a id="_idIndexMarker846"/> three stages required for processing the <strong class="source-inline">flights</strong> data:</li>
			</ol>
			<div>
				<div id="_idContainer274" class="IMG---Figure">
					<img src="image/B18332_09_036.jpg" alt="Figure 9.36 – DAG view in the JupyterHub IDE&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.36 – DAG view in the JupyterHub IDE</p>
			<ol>
				<li value="6">Click on the first element of the DAG named <strong class="bold">start-spark-cluster</strong>. Right-click on this element and select <strong class="bold">Properties</strong>:</li>
			</ol>
			<div>
				<div id="_idContainer275" class="IMG---Figure">
					<img src="image/B18332_09_037.jpg" alt="Figure 9.37 – Select the properties of the first stage in your DAG&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.37 – Select the properties of the first stage in your DAG</p>
			<ol>
				<li value="7">In the right-hand side window, you can see the properties of this stage: </li>
			</ol>
			<div>
				<div id="_idContainer276" class="IMG---Figure">
					<img src="image/B18332_09_038.jpg" alt="Figure 9.38 – Properties of the start-spark.py stage&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.38 – Properties of the start-spark.py stage</p>
			<p>The following list describes each of the properties:</p>
			<ul>
				<li>The <strong class="bold">Filename</strong> section<a id="_idIndexMarker847"/> defines the<a id="_idIndexMarker848"/> file (<strong class="source-inline">start-spark-cluster.py</strong>) that will be executed by Airflow in this stage. </li>
				<li>The <strong class="bold">Runtime Image</strong> section defines the image that will be used to execute the file mentioned in the previous step. This is the container image that you have registered in the earlier section. For the Python stages, you will use the <strong class="bold">AirFlow Python Runner</strong> container image.</li>
				<li>The <strong class="bold">File Dependencies</strong> section defines files required at this stage. The <strong class="source-inline">spark-cluster.yaml</strong> defines the configuration of the Spark cluster. The <strong class="source-inline">spark_util.py</strong> file is the file we have created as a helper utility to talk to the Spark cluster. Note that the files associated with this stage in the DAG will be packaged in the DAG and are available for your stage when it is being executed by Airflow. All of these files are available in the repository.</li>
				<li>The <strong class="bold">Environment Variables</strong> section defines environment variables. The file, <strong class="source-inline">start-spark-cluster.py</strong> in this case, will have access to these environment variables. Think of these variables as configurations that can be used to manage the behavior of your file. For example, the <strong class="source-inline">SPARK_CLUSTER</strong> variable is used to name the Spark cluster created. <strong class="source-inline">WORKER_NODES</strong> defines how many worker Pods will be created as Spark workers. So, for bigger jobs, you may choose to change this parameter to have more nodes. Open the <strong class="source-inline">start-spark-cluster.py</strong> file, and you will see that <a id="_idIndexMarker849"/>the<a id="_idIndexMarker850"/> two environment variables are being read by it. <em class="italic">Figure 9.39</em> shows the file:</li>
			</ul>
			<div>
				<div id="_idContainer277" class="IMG---Figure">
					<img src="image/B18332_09_039.jpg" alt="Figure 9.39 – The start-spark.py file reading the environment variables&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.39 – The start-spark.py file reading the environment variables</p>
			<p>The <strong class="bold">Output Files</strong> section defines any files created by this stage of the DAG. Airflow will copy this file for all other stages of your DAG. This way you can share the information across multiple stages of your DAG. In this example, the <strong class="source-inline">spark_util.py</strong> file prints the location of the Spark cluster; think of it as the network name at which the cluster is listening. This name can be used by other stages, such as the data pipeline notebook, to connect to the Spark cluster. There are other options available in Airflow to share data between stages that you can explore and decide the best one for your use case.</p>
			<ol>
				<li value="8">Click on the second element of the DAG named <strong class="bold">merge_data.ipynb</strong>. Right-click on this element and select <strong class="bold">Properties</strong>. You will see that for this stage, <strong class="bold">Runtime Image</strong> has been changed to <strong class="bold">AirFlow PySpark Runner</strong>. You will notice that the file associated with this stage is the Jupyter notebook file. This is the same file you have used to develop the data pipeline. This is the true flexibility of this<a id="_idIndexMarker851"/> integration<a id="_idIndexMarker852"/> that will take your code as it is to run in any environment.</li>
			</ol>
			<div>
				<div id="_idContainer278" class="IMG---Figure">
					<img src="image/B18332_09_040.jpg" alt="Figure 9.40 – Spark notebook stage in the DAG&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.40 – Spark notebook stage in the DAG</p>
			<p>Add the second notebook, <strong class="source-inline">clean_data.ipynb</strong>, as the next stage of the DAG with a similar setup as <strong class="source-inline">merge_data.ipynb</strong>. We have broken the data pipeline into multiple notebooks for easier maintenance and code management.</p>
			<ol>
				<li value="9">The last stage of this DAG is stopping the Spark cluster. Notice that <strong class="bold">Runtime Image</strong> for this stage is again <strong class="bold">AirFlow Python Runner</strong>, as the code is Python-based.</li>
			</ol>
			<div>
				<div id="_idContainer279" class="IMG---Figure">
					<img src="image/B18332_09_041.jpg" alt="Figure 9.41 – Properties of the stop-spark-cluster.py stage&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.41 – Properties of the stop-spark-cluster.py stage</p>
			<ol>
				<li value="10">Make sure to save the <strong class="source-inline">flights.pipeline</strong> file if you make any changes to it.</li>
			</ol>
			<p>You have now finished the first DAG. The important thing is that, as a data engineer, you have built the DAG yourself and the data pipeline code you have built is used as it is in the pipeline. This<a id="_idIndexMarker853"/> capability will increase the velocity and<a id="_idIndexMarker854"/> make your data engineering team autonomous and self-sufficient.</p>
			<p>In the next stage, you will run this DAG on the platform.</p>
			<h3>Running and validating the DAG</h3>
			<p>In this section, you <a id="_idIndexMarker855"/>will run the DAG you have built in the preceding section. We have assumed that you have completed the steps mentioned in <a href="B18332_07_ePub.xhtml#_idTextAnchor098"><em class="italic">Chapter 7</em></a>, <em class="italic">Model Deployment and Automation</em>, in the <em class="italic">Introducing Airflow</em> section:</p>
			<ol>
				<li value="1">Load the <strong class="source-inline">flights.pipeline</strong> file in the JupyterHub IDE and hit the <strong class="bold">Run pipeline</strong> icon. The icon is a little <em class="italic">play</em> button on the icon bar. You will get the following <strong class="bold">Run pipeline</strong> screen:</li>
			</ol>
			<div>
				<div id="_idContainer280" class="IMG---Figure">
					<img src="image/B18332_09_042.jpg" alt="Figure 9.42 – Airflow DAG submission dialog&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.42 – Airflow DAG submission dialog</p>
			<p>Give the pipeline a name, select <strong class="bold">Apache Airflow runtime</strong> as the <strong class="bold">Runtime Platform</strong> option, and select the <strong class="bold">Runtime Configuration</strong> option as per your settings. If you have followed the instructions in <a href="B18332_07_ePub.xhtml#_idTextAnchor098"><em class="italic">Chapter 7</em></a>, <em class="italic">Model Deployment and Automation</em>, then the value would be <strong class="source-inline">MyAirflow</strong>.</p>
			<ol>
				<li value="2">Click <strong class="bold">OK</strong> after <a id="_idIndexMarker856"/>you have provided the information.</li>
				<li>You will see the following screen, validating that the pipeline has been submitted to the Airflow engine in the platform:</li>
			</ol>
			<div>
				<div id="_idContainer281" class="IMG---Figure">
					<img src="image/B18332_09_043.jpg" alt="Figure 9.43 – Airflow DAG submission confirmation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.43 – Airflow DAG submission confirmation</p>
			<ol>
				<li value="4">Open the Airflow UI. You can access the UI at <strong class="source-inline">https://airflow.&lt;IP Address&gt;.nip.io</strong>. The IP address is the address of your minikube environment. You will find that the pipeline is displayed in the Airflow GUI:</li>
			</ol>
			<div>
				<div id="_idContainer282" class="IMG---Figure">
					<img src="image/B18332_09_044.jpg" alt="Figure 9.44 – DAG list in the Airflow GUI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.44 – DAG list in the Airflow GUI</p>
			<ol>
				<li value="5">Click on the DAG, and then click on the <strong class="bold">Graph View</strong> link. You will get the details of the <a id="_idIndexMarker857"/>executed DAG. This is the same graph that you have built in the preceding section and has the three stages in it.</li>
			</ol>
			<p>Note that your screen may look different depending on your DAG execution stage:</p>
			<div>
				<div id="_idContainer283" class="IMG---Figure">
					<img src="image/B18332_09_045.jpg" alt="Figure 9.45 – DAG execution status&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.45 – DAG execution status</p>
			<p>In this section, you have seen how a data engineer can build the data pipeline (the <strong class="source-inline">merge_data</strong> notebook) and then is able to package and deploy it using Airflow (<strong class="source-inline">flights.pipeline</strong>) from the JupyterHub IDE. The platform provides an integrated solution to build, test, and run your data pipelines at scale.</p>
			<p>The IDE provides the basics to build the Airflow DAG. What if you want to change the DAG to use the advanced capabilities of the Airflow engine? In the next section, you will see how to change <a id="_idIndexMarker858"/>the DAG code generated by the IDE for advanced use cases.</p>
			<h3>Enhancing the DAG by editing the code</h3>
			<p>You may have <a id="_idIndexMarker859"/>noticed that the DAG that you built ran just once. What if you want to run it on a recurring basis? In this section, you will enhance your DAG by changing its running frequency to run daily: </p>
			<ol>
				<li value="1">Open <strong class="source-inline">flights.pipeline</strong> in the JupyterHub IDE. You will see the following familiar screen:</li>
			</ol>
			<div>
				<div id="_idContainer284" class="IMG---Figure">
					<img src="image/B18332_09_046.jpg" alt="Figure 9.46 – The flights.pipeline file&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.46 – The flights.pipeline file</p>
			<ol>
				<li value="2">Click on the <strong class="bold">Export pipeline</strong> icon on the top bar, and you will be presented with a dialog to export the pipeline. Click on the <strong class="bold">OK</strong> button:</li>
			</ol>
			<div>
				<div id="_idContainer285" class="IMG---Figure">
					<img src="image/B18332_09_047.jpg" alt="Figure 9.47 – Export pipeline dialog&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.47 – Export pipeline dialog</p>
			<ol>
				<li value="3">You will get a message that the pipeline export succeeded and a new file will be created as <strong class="source-inline">flights.py</strong>. Open <a id="_idIndexMarker860"/>this file by selecting it from the left-hand side panel. You should see the full code of the generated DAG:</li>
			</ol>
			<div>
				<div id="_idContainer286" class="IMG---Figure">
					<img src="image/B18332_09_048.jpg" alt="Figure 9.48 – The DAG code after the export&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.48 – The DAG code after the export</p>
			<ol>
				<li value="4">You will see your DAG code in Python. From here, you can change the code as needed. For this<a id="_idIndexMarker861"/> exercise, we want to change the frequency of the DAG execution. Find the DAG object in the code; it will be around <em class="italic">line 11</em>: <p class="source-code">dag = DAG(</p><p class="source-code">    "flights-0310132300",</p><p class="source-code">    default_args=args,</p><p class="source-code">    schedule_interval="@once",</p><p class="source-code">    start_date=days_ago(1),</p><p class="source-code">    description="Created with Elyra 2.2.4 pipeline editor using flights.pipeline.",</p><p class="source-code">    is_paused_upon_creation=False,</p><p class="source-code">)</p></li>
				<li>Change the schedule of the DAG object. Change the value from <strong class="source-inline">schedule_interval="@once"</strong> to <strong class="source-inline">schedule_interval="@daily"</strong>.</li>
				<li>The DAG code will look as follows after the change:<p class="source-code">dag = DAG(</p><p class="source-code">    "flights-0310132300",</p><p class="source-code">    default_args=args,</p><p class="source-code">    schedule_interval="@daily",</p><p class="source-code">    start_date=days_ago(1),</p><p class="source-code">    description="Created with Elyra 2.2.4 pipeline editor using flights.pipeline.",</p><p class="source-code">    is_paused_upon_creation=False,</p><p class="source-code">)</p></li>
				<li>Save the file in the IDE and push the file to the Git repository of your DAGs. This is the Git repository that you configured in <a href="B18332_07_ePub.xhtml#_idTextAnchor098"><em class="italic">Chapter 7</em></a>, <em class="italic">Model Deployment and Automation</em>, while configuring the Airflow.</li>
				<li>Now, load the<a id="_idIndexMarker862"/> Airflow GUI and you will be able to see your new DAG with the <strong class="bold">Schedule</strong> column containing the <strong class="bold">@daily</strong> tag. This means that the job will run daily:</li>
			</ol>
			<div>
				<div id="_idContainer287" class="IMG---Figure">
					<img src="image/B18332_09_049.jpg" alt="Figure 9.49 – Airflow DAG list showing the daily schedule&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.49 – Airflow DAG list showing the daily schedule</p>
			<p>Congratulations! You <a id="_idIndexMarker863"/>have successfully built the data pipeline and automated the execution of the pipeline using the DAG. A big part of this abstraction is the life cycle of the Apache Spark cluster that is managed by the platform. Your team will have a higher velocity because the IDE, automation (Airflow), and data processing engine (Apache Spark) are being managed by the platform.</p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor143"/>Summary</h1>
			<p>Phew! This is another marathon chapter in which you have built the data processing pipeline for predicting flights' on-time performance. You have seen how the platform you have built enables you to write complicated data pipelines using Apache Spark, without worrying about provisioning and maintaining the Spark cluster. In fact, you have completed all the exercises without specific help from the IT group. You have automated the execution of the data pipeline using the technologies provided in the platform and have seen the integration of the Airflow pipelines from your IDE, the same IDE you have used for writing the Spark data pipeline.</p>
			<p>Keeping in mind that the main purpose of this book is to help you provide a platform where data and ML teams can work in a self-serving and independent manner, you have just achieved that. You and your team own the full life cycle of data engineering and scheduling the execution of your pipelines.</p>
			<p>In the next chapter, you will see how the same principles can be applied to the data science life cycle, and how teams can use this platform to build and automate the data science components for this project.</p>
		</div>
	</body></html>