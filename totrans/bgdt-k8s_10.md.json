["```\nkubectl get pods -n spark-operator\n```", "```\nNAME                                READY   STATUS\nspark-operator-74db6fcf98-f86vt     1/1     Running\nspark-operator-webhook-init-5594s   0/1     Completed\n```", "```\nkubectl get pods -n trino\n```", "```\nkubectl get pods -n kafka\nkubectl get pods -n elastic\n```", "```\nhelm delete airflow -n airflow\n```", "```\nkubectl delete svc --all -n airflow\nkubectl delete pvc --all -n airflow\n```", "```\nimages:\n  airflow:\n    repository: \"docker.io/neylsoncrepalde/apache-airflow\"\n    tag: \"2.8.1-cncf7.13.0\"\n    digest: ~\n    pullPolicy: IfNotPresent\n```", "```\nhelm install airflow apache-airflow/airflow --namespace airflow --create-namespace -f custom_values.yaml\n```", "```\nkubectl create serviceaccount spark -n airflow\nkubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=airflow:spark --namespace=airflow\n```", "```\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: spark-cluster-cr\n  labels:\n    rbac.authorization.kubeflow.org/aggregate-to-kubeflow-edit: \"true\"\nrules:\n  - apiGroups:\n      - sparkoperator.k8s.io\n    resources:\n      - sparkapplications\n    verbs:\n      - \"*\"\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: airflow-spark-crb\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: spark-cluster-cr\nsubjects:\n  - kind: ServiceAccount\n    name: airflow-worker\n    namespace: airflow\n```", "```\nkubectl apply -f rolebinding_for_airflow.yaml -n airflow\n```", "```\n    from airflow.decorators import task, dag\n    from airflow.utils.task_group import TaskGroup\n    from airflow.providers.cncf.kubernetes.operators.spark_kubernetes import SparkKubernetesOperator\n    from airflow.providers.cncf.kubernetes.sensors.spark_kubernetes import SparkKubernetesSensor\n    from airflow.providers.amazon.aws.operators.glue_crawler import GlueCrawlerOperator\n    from airflow.models import Variable\n    from datetime import datetime\n    import requests\n    import boto3\n    aws_access_key_id = Variable.get(\"aws_access_key_id\")\n    aws_secret_access_key = Variable.get(\"aws_secret_access_key\")\n    s3 = boto3.client('s3',\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key\n    )\n    default_args = {\n        'owner': 'Ney',\n        'start_date': datetime(2024, 5, 10)\n    }\n    ```", "```\n    @dag(\n            default_args=default_args,\n            schedule_interval=\"@once\",\n            description=\"IMDB Dag\",\n            catchup=False,\n            tags=['IMDB']\n    )\n    def IMDB_batch():\n    ```", "```\n    def IMDB_batch():\n        @task\n        def data_acquisition():\n            urls_dict = {\n                \"names.tsv.gz\": \"https://datasets.imdbws.com/name.basics.tsv.gz\",\n                \"basics.tsv.gz\": \"https://datasets.imdbws.com/title.basics.tsv.gz\",\n                \"crew.tsv.gz\": \"https://datasets.imdbws.com/title.crew.tsv.gz\",\n                \"principals.tsv.gz\": \"https://datasets.imdbws.com/title.principals.tsv.gz\",\n                \"ratings.tsv.gz\": \"https://datasets.imdbws.com/title.ratings.tsv.gz\"\n            }\n            for title, url in urls_dict.items():\n                response = requests.get(url, stream=True)\n                with open(f\"/tmp/{title}\", mode=\"wb\") as file:\n                    file.write(response.content)\n                s3.upload_file(f\"/tmp/{title}\", \"bdok-<YOUR_ACCOUNT_NUMBER>\", f\"landing/imdb/{title}\")\n            return True\n    ```", "```\n    with TaskGroup(\"tsvs_to_parquet\") as tsv_parquet:\n        tsvs_to_parquet = SparkKubernetesOperator(\n            task_id=\"tsvs_to_parquet\",\n            namespace=\"airflow\",\n            #application_file=open(f\"{APP_FILES_PATH}/spark_imdb_tsv_parquet.yaml\").read(),\n            application_file=\"spark_imdb_tsv_parquet.yaml\",\n            kubernetes_conn_id=\"kubernetes_default\",\n            do_xcom_push=True\n        )\n        tsvs_to_parquet_sensor = SparkKubernetesSensor(\n            task_id=\"tsvs_to_parquet_sensor\",\n            namespace=\"airflow\",\n            application_name=\"{{ task_instance.xcom_pull(task_ids='tsvs_to_parquet.tsvs_to_parquet')['metadata']['name'] }}\",\n            kubernetes_conn_id=\"kubernetes_default\"\n        )\n        tsvs_to_parquet >> tsvs_to_parquet_sensor\n    ```", "```\n    with TaskGroup('Transformations') as transformations:\n        consolidated_table = SparkKubernetesOperator(\n            task_id='consolidated_table',\n            namespace=\"airflow\",\n            application_file=\"spark_imdb_consolidated_table.yaml\",\n            kubernetes_conn_id=\"kubernetes_default\",\n            do_xcom_push=True\n        )\n        consolidated_table_sensor = SparkKubernetesSensor(\n            task_id='consolidated_table_sensor',\n            namespace=\"airflow\",\n            application_name=\"{{ task_instance.xcom_pull(task_ids='Transformations.consolidated_table')['metadata']['name'] }}\",\n            kubernetes_conn_id=\"kubernetes_default\"\n        )\n        consolidated_table >> consolidated_table_sensor\n    ```", "```\n    glue_crawler_consolidated = GlueCrawlerOperator(\n        task_id='glue_crawler_consolidated',\n        region_name='us-east-1',\n        aws_conn_id='aws_conn',\n        wait_for_completion=True,\n        config = {'Name': 'imdb_consolidated_crawler'}\n    )\n    ```", "```\n        da = data_acquisition()\n        da >> tsv_parquet >> transformations\n        transformations >> glue_crawler_consolidated\n    execution = IMDB_batch()\n    ```", "```\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\n    conf = (\n        SparkConf()\n            .set(\"spark.cores.max\", \"2\")\n            .set(\"spark.executor.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\")\n            .set(\"spark.driver.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\")\n            .set(\"spark.hadoop.fs.s3a.fast.upload\", True)\n            .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n            .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.EnvironmentVariablesCredentials\")\n            .set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.3\")\n    )\n    sc = SparkContext(conf=conf).getOrCreate()\n    ```", "```\n    if __name__ == \"__main__\":\n        spark = SparkSession.builder.appName(\"SparkApplicationJob\").getOrCreate()\n        spark.sparkContext.setLogLevel(\"WARN\")\n    ```", "```\n    schema_names = \"nconst string, primaryName string, birthYear int, deathYear int, primaryProfession string, knownForTitles string\"\n    ```", "```\n    names = (\n        spark\n        .read\n        .schema(schema_names)\n        .options(header=True, delimiter=\"\\t\")\n        .csv('s3a://bdok-<ACCOUNT_NUMBER>/landing/imdb/names.tsv.gz')\n    )\n    ```", "```\n    names.write.mode(\"overwrite\").parquet(\"s3a://bdok-<ACCOUNT_NUMBER>/bronze/imdb/names\")\n    ```", "```\n    spark.stop()\n    ```", "```\n    from pyspark.sql import functions as f\n    ```", "```\n    names = spark.read.parquet(\"s3a://bdok-<ACCOUNT_NUMBER>/bronze/imdb/names\")\n    basics = spark.read.parquet(\"s3a://bdok-<ACCOUNT_NUMBER>/bronze/imdb/basics\")\n    crew = spark.read.parquet(\"s3a://bdok-<ACCOUNT_NUMBER>/bronze/imdb/crew\")\n    principals = spark.read.parquet(\"s3a://bdok-<ACCOUNT_NUMBER>/bronze/imdb/principals\")\n    ratings = spark.read.parquet(\"s3a://bdok-<ACCOUNT_NUMBER>/bronze/imdb/ratings\")\n    ```", "```\n    names = names.select(\n        'nconst', 'primaryName', 'birthYear', 'deathYear',\n        f.explode(f.split('knownForTitles', ',')).alias('knownForTitles')\n    )\n    crew = crew.select(\n        'tconst', f.explode(f.split('directors', ',')).alias('directors'), 'writers'\n    )\n    ```", "```\n    basics_ratings = basics.join(ratings, on=['tconst'], how='inner')\n    principals_names = (\n        principals.join(names, on=['nconst'], how='inner')\n        .select('nconst', 'tconst','ordering', 'category', 'characters', 'primaryName', 'birthYear', 'deathYear')\n        .dropDuplicates()\n    )\n    directors = (\n        crew\n        .join(names, on=crew.directors == names.nconst, how='inner')\n        .selectExpr('tconst', 'directors', 'primaryName as directorPrimaryName',\n                    'birthYear as directorBirthYear', 'deathYear as directorDeathYear')\n        .dropDuplicates()\n    )\n    ```", "```\n    basics_principals = basics_ratings.join(principals_names, on=['tconst'], how='inner').dropDuplicates()\n    basics_principals_directors = basics_principals.join(directors, on=['tconst'], how='inner').dropDuplicates()\n    ```", "```\n    basics_principals_directors.write.mode(\"overwrite\").parquet(\"s3a://bdok-<ACCOUNT_NUMBER>/silver/imdb/consolidated\")\n    spark.stop()\n    ```", "```\n    python simulations.py --host <YOUR-DATABASE-ENDPOINT> -p <YOUR-PASSWORD>\n    ```", "```\n    kubectl apply -f elastic_cluster.yaml -n kafka\n    kubectl apply -f kibana.yaml -n kafka\n    ```", "```\n    kubectl get secret elastic-es-elastic-user -n kafka -o go-template='{{.data.elastic | base64decode}}'\n    ```", "```\n    kubectl get secret elastic-es-http-certs-public -n kafka --output=go-template='{{index .data \"ca.crt\" | base64decode}}' > ca.crt\n    kubectl get secret elastic-es-http-certs-public -n kafka --output=go-template='{{index .data \"tls.crt\" | base64decode}}' > tls.crt\n    kubectl get secret elastic-es-http-certs-internal -n kafka --output=go-template='{{index .data \"tls.key\" | base64decode}}' > tls.key\n    ```", "```\n    openssl pkcs12 -export -in tls.crt -inkey tls.key -CAfile ca.crt -caname root -out keystore.p12 -password pass:BCoqZy82BhIhHv3C -name es-keystore\n    keytool -importkeystore -srckeystore keystore.p12 -srcstoretype PKCS12 -srcstorepass BCoqZy82BhIhHv3C -deststorepass OfwxynZ8KATfZSZe -destkeypass OfwxynZ8KATfZSZe -destkeystore keystore.jks -alias es-keystore\n    ```", "```\n    kubectl create secret generic es-keystore --from-file=keystore.jks -n kafka\n    ```", "```\n    kubectl get svc -n kafka\n    ```", "```\n    kubectl get secret -n kafka\n    ```", "```\n      externalConfiguration:\n        volumes:\n          - name: es-keystore-volume\n            secret:\n              secretName: es-keystore\n    ```", "```\n    kubectl apply -f connect_cluster.yaml -n kafka\n    ```", "```\n    apiVersion: \"kafka.strimzi.io/v1beta2\"\n    kind: \"KafkaConnector\"\n    metadata:\n      name: \"jdbc-source\"\n      namespace: kafka\n      labels:\n        strimzi.io/cluster: kafka-connect-cluster\n    spec:\n      class: io.confluent.connect.jdbc.JdbcSourceConnector\n      tasksMax: 1\n      config:\n        key.converter: org.apache.kafka.connect.json.JsonConverter\n        value.converter: org.apache.kafka.connect.json.JsonConverter\n        key.converter.schemas.enable: true\n        value.converter.schemas.enable: true\n        connection.url: «jdbc:postgresql://<DATABASE_ENDPOINT>:5432/postgres»\n        connection.user: postgres\n        connection.password: \"<YOUR_PASSWORD>\"\n        connection.attempts: \"2\"\n        query: \"SELECT * FROM public.customers\"\n        mode: \"timestamp\"\n        timestamp.column.name: \"dt_update\"\n        topic.prefix: \"src-customers\"\n        valincrate.non.null: \"false\"\n    ```", "```\n    kubectl apply -f connectors/jdbc_source.yaml -n kafka\n    ```", "```\n    kubectl get kafkaconnector -n kafka\n    kubectl describe kafkaconnector jdbc-source -n kafka\n    ```", "```\n    kubectl exec kafka-cluster-kafka-0 -n kafka -c kafka -it -- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic src-customers\n    ```", "```\n    kubectl create serviceaccount spark -n kafka\n    kubectl create clusterrolebinding spark-role-kafka --clusterrole=edit --serviceaccount=kafka:spark -n kafka\n    ```", "```\n    kubectl get secrets -n kafka\n    ```", "```\n    kubectl create secret generic aws-credentials --from-literal=aws_access_key_id=<YOUR_ACCESS_KEY_ID> --from-literal=aws_secret_access_key=\"<YOUR_SECRET_ACCESS_KEY>\" -n kafka\n    ```", "```\n    query = (\n            newdf\n            .withColumn(\"dt_birthdate\", f.col(\"birthdate\"))\n            .withColumn(\"today\", f.to_date(f.current_timestamp() ) )\n            .withColumn(\"age\", f.round(\n                f.datediff(f.col(\"today\"), f.col(\"dt_birthdate\"))/365.25, 0)\n            )\n            .select(\"name\", \"gender\", \"birthdate\", \"profession\", \"age\", \"dt_update\")\n        )\n    ```", "```\n        json_query = (\n            query\n            .select(\n                f.to_json(f.struct(f.col(\"*\")))\n            )\n            .toDF(\"value\")\n        )\n    ```", "```\n        (\n            json_query\n            .withColumn(\"value\", f.concat(f.lit(write_schema), f.col(\"value\"), f.lit('}')))\n            .selectExpr(\"CAST(value AS STRING)\")\n            .writeStream\n            .format(\"kafka\")\n            .option(\"kafka.bootstrap.servers\", \"kafka-cluster-kafka-bootstrap:9092\")\n            .option(\"topic\", \"customers-transformed\")\n            .option(\"checkpointLocation\", \"s3a://bdok-<ACCOUNT-NUMBER>/spark-checkpoint/customers-processing/\")\n            .start()\n            .awaitTermination()\n        )\n    ```", "```\n    kubectl apply -f spark_streaming_job.yaml -n kafka\n    ```", "```\n    kubectl describe sparkapplication spark-streaming-job -n kafka\n    kubectl get pods -n kafka\n    ```", "```\n    kubectl exec kafka-cluster-kafka-0 -n kafka -c kafka -it -- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic customers-transformed\n    ```", "```\n    apiVersion: \"kafka.strimzi.io/v1beta2\"\n    kind: \"KafkaConnector\"\n    metadata:\n      name: \"es-sink\"\n      namespace: kafka\n      labels:\n        strimzi.io/cluster: kafka-connect-cluster\n    spec:\n      class: io.confluent.connect.elasticsearch.ElasticsearchSinkConnector\n      tasksMax: 1\n      config:\n        topics: \"customers-transformed\"\n        connection.url: \"https://elastic-es-http.kafka:9200\"\n        connection.username: \"elastic\"\n        connection.password: \"w6MR9V0SNLD79b56arB9Q6b6\"\n        batch.size: 1\n        key.ignore: \"true\"\n        elastic.security.protocol: \"SSL\"\n        elastic.https.ssl.keystore.location: \"/opt/kafka/external-configuration/es-keystore-volume/keystore.jks\"\n        elastic.https.ssl.keystore.password: \"OfwxynZ8KATfZSZe\"\n        elastic.https.ssl.key.password: \"OfwxynZ8KATfZSZe\"\n        elastic.https.ssl.keystore.type: \"JKS\"\n        elastic.https.ssl.truststore.location: \"/opt/kafka/external-configuration/es-keystore-volume/keystore.jks\"\n        elastic.https.ssl.truststore.password: \"OfwxynZ8KATfZSZe\"\n        elastic.https.ssl.truststore.type: \"JKS\"\n    ```", "```\n    kubectl apply -f connectors/es_sink.yaml -n kafka\n    ```", "```\n    kubectl describe kafkaconnector es-sink -n kafka\n    ```", "```\n    kubectl get svc -n kafka\n    ```"]