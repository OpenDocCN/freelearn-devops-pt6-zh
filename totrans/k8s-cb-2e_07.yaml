- en: Building Kubernetes on GCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use **Google Cloud Platform** (**GCP**) in the following
    recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Playing with GCP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up managed Kubernetes via **Google Kubernetes Engine** (**GKE**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Kubernetes CloudProvider on GKE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing a Kubernetes cluster on GKE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing with GCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GCP is getting popular in the public cloud industry. It has concepts similar
    to AWS, such as VPC, a compute engine, persistent disks, load balancing, and several
    managed services. The most interesting service is GKE, which is the managed Kubernetes
    cluster. We will explore how to use GCP and GKE.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use GCP, you need to have a Google account such as Gmail ([https://mail.google.com/mail/](https://mail.google.com/mail/))),
    which many people already have. Then sign up to GCP using your Google account
    by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the [https://cloud.google.com](https://cloud.google.com) website then
    click the Try it free button
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to Google using your Google account
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register with GCP and enter your personal information and billing information
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That's it!
  prefs: []
  type: TYPE_NORMAL
- en: Once registration is complete, you'll see the GCP Web Console page. In the beginning,
    it may ask you to create one project; the default name could be My First Project.
    You can keep it, but we will create another project in this chapter, the better
    to help you understand.
  prefs: []
  type: TYPE_NORMAL
- en: The GCP Web Console is enough as a first step. But to keep using the Web Console
    is not recommended for DevOps, because human manual input always causes human
    errors and Google might change the Web Console design in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we will use the CLI. GCP provides a CLI tool called Cloud SDK ([https://cloud.google.com/sdk/](https://cloud.google.com/sdk/)).
    So, let's create one new GCP project and then install Cloud SDK on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a GCP project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will create a new project from scratch by following steps. It will help
    you to understand how does GCP project works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the project page by clicking the My First Project link:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a34389e9-cb29-4c1b-8f1f-4df4957081e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Navigating to the project link
  prefs: []
  type: TYPE_NORMAL
- en: 'You may see your own projects to choose from, but this time click the + button
    to create a new one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/47198482-4866-4daa-9443-182e99c7c608.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a new project
  prefs: []
  type: TYPE_NORMAL
- en: Type the project name as `Kubernetes Cookbook`. Then GCP will generate and assign
    a project ID such as kubernetes-cookbook-12345\. Please remember this project
    ID.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You may notice that your project ID is NOT kubernetes-cookbook, like kubernetes-cookbook-194302 in
    the screenshot as shown in the following screenshot. And even you click Edit to
    attempt to change it to kubernetes-cookbook, it doesn't allow it, because the
    project ID is a unique string for all GCP users. And we already took the kubernetes-cookbook project
    ID.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fd87e3a-8715-4e46-a992-f368a6bf95e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Project name and Project ID
  prefs: []
  type: TYPE_NORMAL
- en: 'After a few minutes, your project is ready to use. Go back to the project selection
    page on the top banner and then select your Kubernetes Cookbook project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/93685e0e-2de7-4b92-bc57-d8240fcb7af6.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting a Kubernetes Cookbook project
  prefs: []
  type: TYPE_NORMAL
- en: Done! You can at any time switch to your project and the Kubernetes Cookbook
    project. That is is isolated environment; any VPC, VM, IAM users and even billing
    methods are independent.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Cloud SDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, install Cloud SDK on your machine. It supports the Windows, Mac, and Linux
    platforms. All of these require a Python interpreter version 2.7, but most macOS
    and Linux installs use the defaults.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Windows does't have the Python interpreter by default. However,
    in the Cloud SDK installer for Windows, it is possible to install Python. Let's
    install Cloud SDK on Windows and macOS step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Cloud SDK on Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cloud SDK provides an installer for Windows. It also include Python interpreter
    for Windows as well. Please follow the following steps to install on your Windows
    machine:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the Cloud SDK installer on Windows ([https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe](https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the Cloud SDK installer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you''ve never installed a Python interpreter on your Windows machine, you
    have to choose the Bundled Python option:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f28b2426-efed-4bf7-8cd6-5c7dd9758275.png)'
  prefs: []
  type: TYPE_IMG
- en: Cloud SDK installer for Windows
  prefs: []
  type: TYPE_NORMAL
- en: Other than that, proceed with the installation with the default options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the installation is done, you can find Google Cloud SDK Shell in the Google
    Cloud SDK program group. Click it to launch a Google Cloud SDK Shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1232e920-4d5f-44d9-91c5-52a00344d03e.png)'
  prefs: []
  type: TYPE_IMG
- en: Google Cloud SDK Shell in the Google Cloud SDK program group
  prefs: []
  type: TYPE_NORMAL
- en: 'Type `gcloud info` to check whether you can see the Cloud SDK version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/443c01fd-9b87-450e-a00e-1f2351558d22.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the gcloud command on Windows
  prefs: []
  type: TYPE_NORMAL
- en: Installing Cloud SDK on Linux and macOS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Installing Cloud SDK on both Linux and macOS follows the steps listed here.
    Let''s install Cloud SDK under your home directory:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type the following command to download and run the Cloud SDK installer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'It asks for your desired installation directory. By default, it is under your
    home directory. So, type `return`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It asks whether to send user usage data; it will send some information when
    it crashes. Based on your privacy policy, if don''t wish to send any data to Google,
    choose `n`. Otherwise choose `Y` to improve their quality:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'It asks whether to update `.bash_profile` by adding the `gcloud` command to
    your command search path; type `y` to proceed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Open another Terminal or type `exec -l $SHELL` to refresh your command search
    path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Type `gcloud info` to check whether you can see the Cloud SDK version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now you can start to configure Cloud SDK!
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Cloud SDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can configure both Cloud SDK for Windows and for Linux/macOS, by using
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch Google Cloud SDK Shell (Windows) or open a Terminal (Linux/macOS).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type `gcloud init`; it asks you to log on to your Google account. Type `y`
    and press return:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: It will open a web browser to navigate to the Google logon page; proceed to
    log on using your Google Account with the GCP account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It asks you whether Cloud SDK can access your Google account information. Click
    the ALLOW button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Back to the Terminal—it asks you which project you want to use. Let''s choose
    the Kubernetes Cookbook project you made:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'It asks you whether to configure `Compute Engine` or not. Let''s type `n` to
    skip it this time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now you can start to use Cloud SDK to control GCP. Let's create VPC, subnet,
    and firewall rules, then launch a VM instance to set up our own GCP infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: If you chose the wrong project or you want to try again, at any time you can
    reconfigure your setup by the `gcloud init` command.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will go through GCP''s basic functionality to set up an infrastructure under
    the Kubernetes Cookbook project. By using the `gcloud` command, we will create
    these components:'
  prefs: []
  type: TYPE_NORMAL
- en: One new VPC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two subnets (`us-central1` and `us-east1`) in the VPC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three firewall rules (`public-ssh`, `public-http`, and `private-ssh`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will add your ssh public key to a project-wide metadata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Overall, your infrastructure will resemble the following. Let''s configure
    the components one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e360bc7c-6a8d-4dc1-83db-41c845ded19b.png)'
  prefs: []
  type: TYPE_IMG
- en: Target infrastructure
  prefs: []
  type: TYPE_NORMAL
- en: Creating a VPC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: VPC in GCP is like AWS, but there's no need to bind a particular region, and
    also no need to set the CIDR address range. This means you can create a VPC that
    covers all regions. By default, your Kubernetes Cookbook project has a default
    VPC.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for a better understanding, let''s create a new VPC by following these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `gcloud compute networks` command to create a new VPC. The name is
    `chap7 `and subnet-mode is `custom`, which means subnets are not created automatically.
    So we will add it manually in the next step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the VPC list; you should have two VPCs, `default` VPC and `chap7` VPC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Creating subnets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create two subnets under the `chap7` VPC (network) by following these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to create a subnet, you have to choose the region. By typing `gcloud
    compute regions list` you will know which regions are available to you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9a46d42e-477c-436b-9b2c-fbef497e1ab9.png)'
  prefs: []
  type: TYPE_IMG
- en: Displaying a GCP region list
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s choose `us-central1` and `us-east1` to create two subnets under the `chap7`
    VPC with the following configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| **Subnet name** | **VPC** | **CIDR range** | **Region** |'
  prefs: []
  type: TYPE_TB
- en: '| `chap7-us-central1` | `chap7` | `192.168.1.0/24` | `us-central1` |'
  prefs: []
  type: TYPE_TB
- en: '| `chap7-us-east1` | `chap7` | `192.168.2.0/24` | `us-east1` |'
  prefs: []
  type: TYPE_TB
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the following command to see whether subnets are configured properly
    or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Creating firewall rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Firewall rules are similar to an AWS Security Group in that you can define incoming
    and outgoing packet filters. They use a network tag, which is a label, to distinguish
    between firewall rules and VM instances. So, VM instances can specify zero or
    some network tags, then the firewall rule will apply to the VM which has the same
    Network Tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we need to set a target network tag while creating the firewall
    rule. Overall, we will create three firewall rules that have these configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Firewall rule name** | **Target VPC** | **Allow port** | **Allow from**
    | **Target network tag** |'
  prefs: []
  type: TYPE_TB
- en: '| `public-ssh` | `chap7` | `ssh` (22/tcp) | All (`0.0.0.0/0`) | public |'
  prefs: []
  type: TYPE_TB
- en: '| `public-http` | `chap7` | `http` (80/tcp) | All (`0.0.0.0/0`) | public |'
  prefs: []
  type: TYPE_TB
- en: '| `private-ssh` | `chap7` | `ssh` (22/tcp) | Host which has a public network
    tag | private |'
  prefs: []
  type: TYPE_TB
- en: 'Create a `public-ssh` rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `public-http` rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `private-ssh` rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Check all firewall rules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Adding your ssh public key to GCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before you launch VM instances, you need to upload your ssh public key in order
    to log on to the VM. If you don't have any ssh keys, you have to run the `ssh-keygen` command
    to generate a key pair (public key and private key). Let's assume you have a public
    key as `~/.ssh/id_rsa.pub` and a private key as `~/.ssh/id_rsa`
  prefs: []
  type: TYPE_NORMAL
- en: 'Check your login user name by using the `whoami` command, then use `gcloud
    compute config-ssh` to upload your key via the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Check your ssh public key is registered as metadata:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: That's all. These are minimal configurations in order to launch a VM instance.
    So, let's launch some VM instances on this infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now you have your own VPC, subnet, and firewall rules. This infrastructure
    will be used by the compute engine (VM instances), Kubernetes Engine, and some
    other GCP products. Let''s deploy two VM instances onto your VPC, as in the following
    diagram, to see how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/061df439-c2c1-48b2-9aeb-9645e75b1deb.png)'
  prefs: []
  type: TYPE_IMG
- en: Final state
  prefs: []
  type: TYPE_NORMAL
- en: Launching VM instances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will launch two VM instances on both `us-central1` and `us-east1` by using
    the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **VM Instance name** | **Target VPC** | **zone (see the following steps)**
    | **Target Subnet** | **Assign Network Tag** |'
  prefs: []
  type: TYPE_TB
- en: '| `chap7-public` | `chap7` | `us-central1-a` | `chap7-us-central1` | public
    |'
  prefs: []
  type: TYPE_TB
- en: '| `chap7-private` | `chap7` | `us-east1-b` | `chap7-us-east1` | private |'
  prefs: []
  type: TYPE_TB
- en: 'Check the available zones in `us-central1` and `us-east1` by using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s choose `us-central1-a` for `chap7-public` and `us-east1-b` for `chap7-private`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type the following command to create two VM instances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the VM instance external IP address via the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Run `ssh-agent` to remember your ssh key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'ssh from your machine to `chap7-public` using the `-A` option (forward authentication)
    and using an external IP address:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/01c661b0-0fb2-4541-a527-1d97a51994cd.png)'
  prefs: []
  type: TYPE_IMG
- en: ssh to the public VM instance
  prefs: []
  type: TYPE_NORMAL
- en: 'ssh from `chap7-public` to `chap7-private` via the internal IP address:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0bdef6f5-f918-4d9f-ab06-2dfdf413ecc4.png)'
  prefs: []
  type: TYPE_IMG
- en: ssh to private VM instance
  prefs: []
  type: TYPE_NORMAL
- en: 'Type the `exit` command to go back to the `chap7-public` host, then install
    `nginx` by using the `apt-get` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c932a618-0356-4c6b-aefc-c96873cd71b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Installing nginx on a public VM instance
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch `nginx` by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Access `chap7-public` (via the external IP) using your web browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e140b946-9757-4ec3-96a3-650eddc6f7ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Accessing a nginx web server on a public VM instance
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have finished setting up a GCP VPC, Subnet, and firewall
    rules, and launch VM instances! These are very basic and common usages of Google
    Compute Engine. You can login and install software in these machines, or even
    build a Kubernetes cluster from scratch. However, GCP also has a managed Kubernetes
    product called Kubernetes Engine. We will explore it in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Playing with Google Kubernetes Engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes was designed by google and widely used internally at Google for years.
    Google Cloud Platform offers the hosted GKE. With GKE, we don't need to build
    a cluster from scratch. Instead, clusters can be launched and turned down on demand.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use the Kubernetes Engine dashboard in the GCP console or the gcloud
    CLI to launched and configure a cluster. Using the console is very straightforward
    and intuitive. However, using CLI is a more flexible way to make the operation
    repeatable or to integrate it with your existing pipeline. In this recipe, we'll
    walk through how to use gcloud to launch and set up a Kubernetes cluster, along
    with some importants concept in GCP.
  prefs: []
  type: TYPE_NORMAL
- en: In GCP, everything is associated with a project. A GCP project is the basic
    unit for using GCP services, billing, and permission control. At first, we'll
    have to create a project from the GCP console [https://console.cloud.google.com](https://console.cloud.google.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'The project ID is globally unique in GCP. After the project is properly created,
    we''ll see there is a unique project number assigned. In the home dashboard, we''ll
    have a clear view of how many resources we''ve used. We can set permissions, storage,
    network, billing, and other resources from here. Before we can move forward, we''ll
    need to install gcloud. gcloud is  part of Google Cloud SDK. Other than gcloud,
    which can do most common operations in GCP, Google Cloud SDK also includes other
    common GCP tools, such as gsutil (to manage Cloud Storage), bq (a command-line
    tool for BigQuery), and core (Cloud SDK libraries). The tools are available at
    the Google cloud SDK download page: [https://cloud.google.com/sdk/docs/#install_the_latest_cloud_tools_version_cloudsdk_current_version](https://cloud.google.com/sdk/docs/#install_the_latest_cloud_tools_version_cloudsdk_current_version).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After gcloud is installed, run gcloud init to log in to set up your identity
    with gcloud and create a project named** k8s-cookbook-2e**. We can use gcloud
    to manipulate almost all the services in Google Cloud; the major command group
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The gcloud container command line set is used to manage our containers and
    clusters in Google Kuberentes Engine. For launching a cluster, the most important
    parameters are network settings. Let''s spend some time understanding network
    terminology in GCP here. Just like AWS, GCP has the VPC concept as well. It''s
    a private and safer way to isolate your compute, storage, and cloud resources
    with the public internet. It can be peered across projects, or established as
    a VPN with on-premise datacenters to create a hybrid cloud environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Instances on this network will not be reachable until firewall rules are created.
    As an example, you can allow all internal traffic between instances as well as
    SSH, RDP, and ICMP by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the VPC is created in auto mode, which will create a one subnet
    per region. We can observe that via the subcommand `describe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In GCP, each subnet is across a zone. A zone is an isolated location in a region,
    which is a similar concept to availability zones in AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you could create a network in custom mode by adding the parameter
    `--subnet-mode=custom`, which allows you to define your desired IP range, region,
    and all the routing rules. For more details, please refer to the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Auto mode also helps you set up all default routing rules. A route serves to
    define the destination for certain IP ranges. For example, this route will direct
    the packet to virtual network `10.158.0.0/20`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fe25562-f068-439c-9e86-1e0873de39b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Default route example
  prefs: []
  type: TYPE_NORMAL
- en: 'There route which is used to direct the packet to the outside world. The next
    hop of this route is the default internet gateway, similar to the igw in AWS.
    In GCP, however, you don''t need to explicitly create an internet gateway:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/532e9bd0-5148-40f5-a51d-a02176ae9ee1.png)'
  prefs: []
  type: TYPE_IMG
- en: Default route for internet access
  prefs: []
  type: TYPE_NORMAL
- en: Another important concept in a GCP network is firewall rules, used to control
    the ingress and egress for your instance. In GCP, the association between firewall
    rules and VM instances is implemented by network tags.
  prefs: []
  type: TYPE_NORMAL
- en: A firewall rule can also be assigned to all instances in the network or a group
    of instances with a specific service account (ingress only). The service account
    is the identity of a VM instance in GCP. One or more roles can be assigned to
    a service account, so it can have access to other GCP resources. This is similar
    to AWS instance profiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'One VM instance can have more than one network tags, which implies multiple
    network routes could be applied. This diagram shows how tags work. In the following
    diagram, the first firewall rule is applied to VM1 and VM2, and VM2 has two firewall
    rules associated with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69c75148-13f4-4596-a4d4-dd6b32705650.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of AWS security groups and GCP firewall rules
  prefs: []
  type: TYPE_NORMAL
- en: In **AWS**, one or more ingress/egress rules are defined in a **Security Group**,
    and one or more Security Groups can be assigned to a **EC2** instance. In **GCP**,
    on the other hand, one or more firewall rules are defined, which are associated
    with one or more tags. One or more tags can be assigned to an instance. By mapping
    network tags, firewall rules can control and limit  access in and out of your
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve learned the basic network concept in GCP. Let''s launch our first GKE
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameter** | **Description** | **Value in example** |'
  prefs: []
  type: TYPE_TB
- en: '| `--cluster-version` | Supported cluster version (Refer to [https://cloud.google.com/kubernetes-engine/release-notes](https://cloud.google.com/kubernetes-engine/release-notes))
    | `1.9.2-gke.1` |'
  prefs: []
  type: TYPE_TB
- en: '| `--machine-type` | Instance type of nodes (Refer to [https://cloud.google.com/compute/docs/machine-types](https://cloud.google.com/compute/docs/machine-types))
    | `f1-micro` |'
  prefs: []
  type: TYPE_TB
- en: '| `--num-nodes` | Number of nodes in the cluster | `3` |'
  prefs: []
  type: TYPE_TB
- en: '| `--network` | Target VPC network | `k8s-network` (the one we just created)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `--zone` | Target zone | `us-central1-a` (you''re free to use any zone) |'
  prefs: []
  type: TYPE_TB
- en: '| `--tags` | Network tags to be attached to the nodes | private |'
  prefs: []
  type: TYPE_TB
- en: '| `--service-account &#124; --scopes` | Node identity (Refer to [https://cloud.google.com/sdk/gcloud/reference/container/clusters/create](https://cloud.google.com/sdk/gcloud/reference/container/clusters/create)
    for more scope value) | `storage-rw`,`compute-ro` |'
  prefs: []
  type: TYPE_TB
- en: 'By referring preceding parameters, let''s launch a three nodes cluster by `gcloud`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'After the cluster is up-and-running, we can start to connect to the cluster
    by configuring `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see if the cluster is healthy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can check the nodes inside the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use `kubectl` to check cluster info:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Under the hood, gcloud creates a Kubernetes cluster with three nodes, along
    with a controller manager, scheduler, and etcd cluster with two members. We can
    also see that the master is launched with some services, including a default backend
    used by the controller, heapster (used for monitoring) KubeDNS for DNS services
    in the cluster, a dashboard for Kubernetes UI, and metrics-server for resource
    usage metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw `Kubernetes-dashboard` has a URL; let''s try and access it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e58c86e-2d4e-4495-a79f-3bdc37c3da1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Forbidden to access Kubernetes dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'We got `HTTP 403 Forbidden`. Where do we get the access and credentials though?
    One way to do it is running a proxy via the `kubectl proxy` command. It will bind
    the master IP to local `127.0.0.1:8001`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: After that, when we access `http://127.0.0.1:8001/ui`, it'll be redirected to
    `http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since Kubernetes 1.7, the dashboard has supported user authentication based
    on a bearer token or `Kubeconfig` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd85ef71-b10b-40b8-af0e-31ba8d287dee.png)'
  prefs: []
  type: TYPE_IMG
- en: Logging in to the Kubernetes dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'You could create a user and bind it to the current context (please refer to
    the *Authentication and authorization* recipe in [Chapter 8](d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml),
    *Advanced Cluster Administration*). Just for convenience, we can check if we have
    any existing users. Firstly, we need to know our current context name. Context
    combines of cluster information, users for authentication, and a namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'After we know the context name, we can describe it via the `kubectl` config
    view `$CONTEXT_NAME`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We may find there is a default user existing in our cluster; using its `$ACCESS_TOKEN`,
    you can glimpse the Kubernetes console.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6825b49d-6934-41d2-83a8-cb3a098d2675.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes dashboard overview
  prefs: []
  type: TYPE_NORMAL
- en: 'Our cluster in GKE is up-and-running! Let''s try and see if we can run a simple
    deployment on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check our Kubernetes dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a069c1f-b7cb-4441-8f32-90576dcdab69.png)'
  prefs: []
  type: TYPE_IMG
- en: Workloads in Kubernetes dashboard
  prefs: []
  type: TYPE_NORMAL
- en: Hurray! The deployment is created and as a result two pods are scheduled and
    created.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Advanced settings in kubeconfig* in [Chapter 8](d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml),
    *Advanced Cluster Administration*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Setting resources in nodes* in Chapter 8, *Advanced Cluster Administration*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Playing with the Web UI* in [Chapter 8](d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml),
    *Advanced Cluster Administration*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Setting up a DNS server in Kubernetes Cluster* in [Chapter 8](d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml),
    *Advanced Cluster Administration*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Authentication and authorization* in [Chapter 8](d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml),
    *Advanced Cluster Administration*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring CloudProvider on GKE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GKE works as a native Kubernetes Cloud Provider, which integrates with resources
    in Kubernetes seamlessly and allows you to provision on demand, for example, VPC
    routes for the network, **Persistent Disk** (**PD**) for StorageClass, L4 load
    balancer for Service, and L4 load balancer for ingress.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, when you create the network and launch a Kubernetes cluster in Google
    Cloud Platform with proper routes, containers can already talk to each other without
    an explicit network being set up.Beyond the resources listed previously, we don't
    need to set any settings explicitly in most cases. GKE will just work.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how convenient GKE offers about storage, network and more.
  prefs: []
  type: TYPE_NORMAL
- en: StorageClass
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml), *Walking through
    Kubernetes Concepts*, we learned how to declare `PersistentVolume` and `PersistentVolumeClaim`.
    With dynamic provisioning, you can define a set of `StorageClass` with different
    physical storage backends and use them in `PersistentVolume` or `PersistentVolumeClaim`.
    Let's see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the current default `StorageClass`, use `kubectl get storageclasses`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We can see we have a default storage class named standard and its provisioner
    is GCE PD.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a `PersistentVolumeClaim` request and use the standard `StorageClass`
    as the backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '`storageClassName` is the place to put the name of the `StorageClass`. If you
    put in something that doesn''t exist, PVC will not be created, since there is
    no proper mapped `StorageClass` to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see volume `pvc-1491b08e-1cfc-11e8-8589-42010a800360` has been created
    and bounded. If we list GCP disks, we''ll find there was a Persistent Disk created;
    the suffix of the disk name indicates the volume name in Kubernetes. That''s the
    magic of dynamic volume provisioning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Besides the default `StorageClass`, you can also create your own. Recap this
    in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml), *Walking through Kubernetes
    Concepts*.
  prefs: []
  type: TYPE_NORMAL
- en: Service (LoadBalancer)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A `LoadBalancer` service type only works in the cloud environment that supports
    external load balancers. This allows outside traffic to be routed into target
    Pods. In GCP, a TCP load balancer will be created by a `LoadBalancer` service
    type:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The firewall rules for allowing traffic between the load balancer and nodes
    will be created automatically:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check the service. The `EXTERNAL-IP` will show `<pending>` if the load
    balancer is still provisioning. Wait a while and the load balancer IP will present
    itself eventually:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s curl `$EXTERNAL-IP:80`, to see if it works properly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'If we check the forwarding rules in GCP, we can find a rule that defines how
    the traffic goes from external IP to the target pool:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'A target pool is a set of instances that receive the traffic from forwarding
    rules. We could inspect the target pool by using the gcloud command as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We can see there are three nodes inside the pool. Those are the same three nodes
    in our Kubernetes cluster. Load balancer will dispatch the traffic to a node based
    on a hash of the source/definition IP and port. A service with `LoadBalancer`
    type looks handy; however, it can't do path-based routing. It's time for ingress
    to come into play. Ingress supports virtual hosts, path-based routing, and TLS
    termination, which is a more flexible approach to your web services.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 5](669edaf0-c274-48fa-81d8-61150fa36df5.xhtml), *Building Continuous
    Delivery Pipelines*, we learned about the concept of ingress , and when and how
    to use it. Ingress  defines a set of rules allowing the inbound connection to
    access Kubernetes cluster services. It routes the traffic into cluster at L7,
    and the controller brings the traffic to the nodes. When GCP is the cloud provider,
    a L7 load balancer will be created if an ingress is created, as well as related
    firewall rules, health checks, backend services, forwarding rules, and a URL map.
    A URL map in GCP is a mechanism that contains a set of rules and forwards requests
    to the corresponding backend services.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll reuse the examples from [Chapter 5](669edaf0-c274-48fa-81d8-61150fa36df5.xhtml),
    *Building Continuous Delivery Pipelines*, `Nodeport-deployment.yaml` and `echoserver.yaml`.
    Next is an illustration of how these two services work from [Chapter 5](669edaf0-c274-48fa-81d8-61150fa36df5.xhtml),
    *Building Continuous Delivery Pipelines*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9396e211-b955-4cd9-bb43-1154a5cc13ae.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Ingress illustration
  prefs: []
  type: TYPE_NORMAL
- en: We will create an ingress for nginx and echoserver, that routes to different
    services. When the traffic comes in, the pod ingress controller will decide with
    service to route to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example for ingress . Please note that you might want to add the
    host name inside the rules section if you want the underlying services to always
    be visited from a certain host name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Please double-check that the underlying service is configured as a `NodePort`
    type. Otherwise you might encounter errors such as `googleapi: Error 400: Invalid
    value for field ''namedPorts[1].port'': ''0''. Must be greater than or equal to
    1, invalid error `from `loadbalancer-controller`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After a few minutes, the L7 load balancer will be created and you''ll be able
    to see it from the GCP console or by using the gcloud command. Let''s use `kubectl`
    to check if the backend service in INGRESS is healthy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the three backends are healthy and the related forwarding rules,
    target proxy, and URL map have been all created. We can get a comprehensive view
    from the GCP console by visiting discovery and load balancing in GKE or the Load
    balancing tab in network services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1509f93-dc69-4e09-b6c6-0c80c63776cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Discovery and Load balancing
  prefs: []
  type: TYPE_NORMAL
- en: 'The backend is illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69386610-e730-4510-94ff-cda84aa27e43.png)'
  prefs: []
  type: TYPE_IMG
- en: Backend services
  prefs: []
  type: TYPE_NORMAL
- en: 'From time to time, your ingress resource might encounter updates. When you
    redeploy it, there is no guarantee that GCP will allocate the same IP address
    to your load balancer. This might introduce a problem when the IP address is associated
    with a DNS name. The target IP address will need to be updated every time the
    IP is changed. This could be resolved by a static external IP address plus `kubernetes.io/INGRESS.global-static-ip-name`
    annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s describe `my-INGRESS` and see if it binds properly with the external
    IP we created :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We're all set. `Nginx` and `echoserver` can be visited via the external static
    IP `130.211.37.61`, and we're able to associate a DNS name with it by using the
    cloud DNS service in GCP.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Kubernetes v.1.9, the Kubernetes cloud controller manager was promoted to
    alpha. Cloud controller manager aims to make the cloud provider release feature
    support via its own release cycles, which could be independent from the Kubernetes
    release cycle. Then it could be independent with Kubernetes core release cycle.
    It provides common interfaces that each cloud provider can implement, which decoupling
    with Kubernetes Core logic. In the near future, we'll see more comprehensive support
    from different cloud providers!
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Working with services* in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml),
    *Walking through Kubernetes Concepts*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Working with volumes* in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml),
    *Walking through Kubernetes Concepts*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Forwarding container ports* in [Chapter 3](51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml),
    *Playing with Containers*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing Kubernetes clusters on GKE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google Kubernetes Engines offers us the seamless experience of running Kubernetes;
    it also makes Kubernetes administration so easy. Depending on the expected peak
    time, we might want to scale the Kubernetes nodes out or in. Alternatively, we
    could use Autoscaler to do auto-scaling for the nodes. Kubernetes is an evolving
    platform. The release pace is fast. We might want to upgrade the cluster version
    from time to time, which is very easy to do. We could also use the Autoupgrade
    feature to upgrade the cluster by enabling automatically schedule feature in GKE.
    Let's see how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before setting up the administration features that GCP offers, we'll have to
    have a cluster up and running. We'll reuse the cluster we created in the Playing
    with the Google Kubernetes Engine recipe in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll introduce how to manage the number of nodes based on usage
    and requirements. Also, we'll learn how to deal with cluster upgrades. Finally,
    we'll see how to provision a multi-zone cluster in GKE, in order to prevent a
    physical zone outage.
  prefs: []
  type: TYPE_NORMAL
- en: Node pool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A node pool is a set of instances in GCP that share the same configuration.
    When we launch a cluster from the `gcloud` command, we pass `--num-node=3` and
    the rest of the arguments. Then three instances will be launched inside the same
    pool, sharing the same configuration, using the same method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Assume there is an expected heavy peak time for your service. As a Kubernetes
    administrator, you might want to resize your node pool inside the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The resize command can help you scale out and in. If the node count after resizing
    is less than before, the scheduler will migrate the pods to run on available nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can set the compute resource boundary for each container in the spec. You
    set requests and limits to a pod container. Assume we have a super nginx which
    requires 1024 MB memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The node size we created is `f1-miro`, which only has 0.6 GM memory per node.
    It means the scheduler will never find a node with sufficient memory to run `super-nginx`.
    In this case, we can add more nodes with higher memory to the cluster by creating
    another node pool. We''ll use `g1-small` as an example, which contains 1.7 GB
    memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Looks like we have two more powerful nodes. Let''s see the status of our super
    nginx:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s running! Kubernetes scheduler will always try to find sufficient resources
    to schedule pods. In this case, there are two new nodes added to the cluster that
    can fulfill the resource requirement, so the pod is scheduled and run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: From the events of the pod, we know what path it ran through. Originally, it
    couldn't find any nodes with sufficient resources and eventually it's scheduled
    to the new node named `gke-my-k8s-cluster-larger-mem-pool-a51c8da3-scw1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For making the user preference on scheduling pods on certain nodes, `nodeSelector`
    was introduced. You could either use built-in node labels, such as `beta.kubernetes.io/instance-type:
    n1-standard-1` in pod spec, or use customized labels to achieve it. For more information,
    please refer to [https://kubernetes.io/docs/concepts/configuration/assign-pod-node](https://kubernetes.io/docs/concepts/configuration/assign-pod-node).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes also supports **cluster autoscaler**, which automatically resizes
    your cluster based on capacity if all nodes have insufficient resources to run
    the requested pods. To do that, we add `–enable-autoscaling` and specify the maximum
    and minimum node count when we create the new node pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few minutes, we can see there is a new node inside our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s change the replica of our super-nginx from 1 to 4 by using `kubectl`
    edit or creating a new deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We find there are two pods with a pending status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few minutes, we see that there are new members in our larger mem pool,
    and all our pods get to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Cluster autoscaler comes in handy and is cost-effective. When the nodes are
    over-provisioned, the additional node in the node pool will be terminated automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-zone and regional clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our `my-k8s-cluster` is currently deployed in the `us-central1-a` zone. While
    a zone is a physically isolated location in a region, it may suffer an outage.
    Google Kubernetes Engine supports multi-zone and regional deployment. Multi-zone
    clusters create a single master in a zone and provision nodes in multiple zones;
    on the other hand, a regional cluster creates multiple masters across three zones
    and provisions nodes in multiple zones.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-zone clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To enable multi-zone cluster, add -`-additional-zones $zone2, $zone3, …` into
    the command when you create the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Just like AWS, GCP has service quota limits as well. You could use `gcloud compute
    project-info describe –project $PROJECT_NAME` to check the quota and request an
    increase from the GCP console if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s launch a two-nodes cluster per zone first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We find we have six nodes now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check if the nodes are spread across the three zones we specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Regional clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Regional clusters are still in the beta phase. To use these, we''ll have to
    enable the gcloud beta command. We can enable it via this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we should be able to use the `gcloud v1beta` command to launch the regional
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The command is quite similar to the one that creates a cluster, just with two
    differences: a beta flag is added before the group name container which indicates
    it''s a `v1beta` command. The second difference is changing `--zone` to `--region`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Cluster upgrades
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes is a fast-release project. GKE also keeps supporting new versions.
    It''s not uncommon to have multiple minor version updates within a month. check
    the GKE console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7474190c-b187-407f-aac8-f92c7e3c9ec3.png)'
  prefs: []
  type: TYPE_IMG
- en: Upgrade available information in the GCP console
  prefs: []
  type: TYPE_NORMAL
- en: 'We see that an upgrade is available. 1.9.3-gke.1 in the screenshot has just
    been released and our cluster is able to upgrade:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd70a627-254a-4882-b45a-7f1c15f9b782.png)'
  prefs: []
  type: TYPE_IMG
- en: Upgrade available to 1.9.3-gke.0
  prefs: []
  type: TYPE_NORMAL
- en: We can upgrade the cluster via the GKE console, or using gcloud command. We'll
    use the single zone (`us-central1-a`) cluster to demonstrate how to upgrade in
    the next example. When upgrading the cluster, the master is always the first citizen
    to do the upgrade. The desired node version cannot be greater than the current
    master version.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check the master''s version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Looks good. The master has been upgraded to `v1.9.3-gke.0`, but our nodes didn''t
    get upgrade yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'For the node upgrade, instead of upgrading them all at once, GKE performs rolling
    upgrade. It will first drain and deregister a node from the node pool, delete
    an old instance, and provision a new instance with the desired version, then add
    it back to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: The node pool can be configured to auto-upgrade via the `--enable-autoupgrade`
    flag during cluster creation, or using the gcloud container `node-pools` update
    command to update existing node pools. For more information, please refer to [https://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-upgrades](https://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-upgrades).
  prefs: []
  type: TYPE_NORMAL
- en: It will take more than 10 minutes. After that, all the nodes in the cluster
    are upgraded to `1.9.3-gke.0`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Advanced settings in kubeconfig* in [Chapter 8](d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml),
    *Advanced Cluster Administration*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Setting resources in nodes* in [Chapter 8](d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml),
    *Advanced Cluster Administration*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
