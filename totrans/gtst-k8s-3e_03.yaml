- en: Working with Networking, Load Balancers, and Ingress
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用网络、负载均衡器和入口
- en: In this chapter, we will discuss Kubernetes' approach to cluster networking
    and how it differs from other approaches. We will describe key requirements for
    Kubernetes networking solutions and explore why these are essential for simplifying
    cluster operations. We will investigate DNS in the Kubernetes cluster, dig into
    the **Container Network Interface** (**CNI**) and plugin ecosystems, and will
    take a deeper dive into services and how the Kubernetes proxy works on each node.
    Finishing up, we will look at a brief overview of some higher level isolation
    features for multitenancy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论Kubernetes对集群网络的方法以及与其他方法的不同之处。我们将描述Kubernetes网络解决方案的关键要求，并探讨为简化集群操作而至关重要的原因。我们将深入探讨Kubernetes集群中的DNS，深入研究**容器网络接口**（**CNI**）和插件生态系统，并深入了解服务以及Kubernetes代理在每个节点上的工作原理。最后，我们将简要概述一些用于多租户高级隔离功能的特性。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Kubernetes networking
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes网络
- en: Advanced services concepts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级服务概念
- en: Service discovery
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务发现
- en: DNS, CNI, and ingress
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS、CNI和入口
- en: Namespace limits and quotas
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名空间限制和配额
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You'll need a running Kubernetes cluster like the one we created in the previous
    chapters. You'll also need access to deploy the cluster through the `kubectl`
    command.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要一个运行中的Kubernetes集群，就像我们在前几章创建的那个一样。您还需要通过`kubectl`命令访问部署该集群。
- en: The GitHub repository for this chapter can be found at [https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter03](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter03).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 此章节的GitHub代码库可以在[https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter03](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter03)找到。
- en: Container networking
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器网络
- en: Networking is a vital concern for production-level operations. At a service
    level, we need a reliable way for our application components to find and communicate
    with each other. Introducing containers and clustering into the mix makes things
    more complex as we now have multiple networking namespaces to bear in mind. Communication
    and discovery now becomes a feat that must navigate container IP space, host networking,
    and sometimes even multiple data center network topologies.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产级运营中，网络是一个重要关注点。在服务级别上，我们需要一种可靠的方式让我们的应用组件找到并与彼此通信。引入容器和集群使事情变得更加复杂，因为现在我们必须考虑多个网络命名空间。现在，通信和发现变成了一个必须要处理容器IP空间、主机网络甚至有时多个数据中心网络拓扑的技能。
- en: Kubernetes benefits here from getting its ancestry from the clustering tools
    used by Google for the past decade. Networking is one area where Google has outpaced
    the competition with one of the largest networks on the planet. Earlier, Google
    built its own hardware switches and **Software-defined Networking** (**SDN**)
    to give them more control, redundancy, and efficiency in their day-to-day network
    operations. Many of the lessons learned from running and networking two billion
    containers per week have been distilled into Kubernetes, and informed how K8s
    networking is done.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes受益于过去十年Google使用的集群工具的传承。在全球最大的网络之一中，网络是Google超越竞争对手的一个领域。此前，Google构建了自己的硬件交换机和**软件定义网络**（**SDN**），以提供更多控制、冗余和效率，用于日常网络操作。从每周运行和联网20亿个容器中获得的许多经验教训已经融入Kubernetes，并影响了K8s网络的实施方式。
- en: The Docker approach
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker的方法
- en: In order to understand the motivation behind the K8s networking model, let's
    review Docker's approach to container networking.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解K8s网络模型背后的动机，让我们回顾一下Docker对容器网络的方法。
- en: Docker default networks
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker默认网络
- en: 'The following are some of Docker''s default networks:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Docker的一些默认网络：
- en: '**Bridge network**: In a nonswarm scenario, Docker will use the bridge network
    driver (called `bridge`) to allow standalone containers to speak to each other.
    You can think of the bridge as a link layer device that forwards network traffic
    between segments. If containers are connected to the same bridge network, they
    can communicate; if they''re not connected, they can''t. The bridged network is
    the default choice unless otherwise specified. In this mode, the container has
    its own networking namespace and is then bridged via virtual interfaces to the
    host (or node, in the case of K8s) network. In the bridged network, two containers
    can use the same IP range because they are completely isolated. Therefore, service
    communication requires some additional port mapping through the host side of network
    interfaces.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**桥接网络**：在非Swarm场景下，Docker将使用桥接网络驱动程序（称为`bridge`）来允许独立的容器相互通信。你可以将桥接视为一个链路层设备，它在不同网络段之间转发网络流量。如果容器连接到同一桥接网络，它们可以相互通信；如果没有连接，它们则无法通信。除非另有指定，桥接网络是默认选择。在这种模式下，容器有自己的网络命名空间，然后通过虚拟接口与主机（或K8s中的节点）网络相连接。在桥接网络中，由于容器之间完全隔离，两个容器可以使用相同的IP地址范围。因此，服务通信需要通过主机侧的网络接口进行额外的端口映射。'
- en: '**Host based**: Docker also offers host-based networking for standalone containers,
    which creates a virtual bridge called `docker0` that allocates private IP address
    space for the containers using that bridge. Each container gets a virtual Ethernet
    (`veth`) device that you can see in the container as `eth0`. Performance is greatly
    benefited since it removes a level of network virtualization; however, you lose
    the security of having an isolated network namespace. Additionally, port usage
    must be managed more carefully since all containers share an IP.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主机模式**：Docker还为独立容器提供基于主机的网络，这会创建一个名为`docker0`的虚拟桥接，分配私有IP地址空间给使用该桥接的容器。每个容器都会获得一个虚拟以太网（`veth`）设备，你可以在容器内看到它作为`eth0`。由于移除了一个网络虚拟化层，性能得到了显著提升；然而，你失去了拥有独立网络命名空间的安全性。此外，由于所有容器共享一个IP地址，端口使用必须更加小心管理。'
- en: There's also a none network, which creates a container with no external interface.
    Only a `loopback` device is shown if you inspect the network interfaces.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种none网络，它会创建一个没有外部接口的容器。如果你检查网络接口，只会看到一个`loopback`设备。
- en: In all of these scenarios, we are still on a single machine, and outside of 
    host mode, the container IP space is not available outside that machine. Connecting
    containers across two machines requires NAT and port mapping for communication.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些场景中，我们仍然处于单个机器上，除非是主机模式，否则容器的IP空间无法在该机器外部访问。要在两台机器之间连接容器，则需要NAT和端口映射来进行通信。
- en: Docker user-defined networks
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker用户定义的网络
- en: 'In order to address the cross-machine communication issue and allow greater
    flexibility, Docker also supports user-defined networks via network plugins. These
    networks exist independent of the containers themselves. In this way, containers
    can join the same existing networks. Through the new plugin architecture, various
    drivers can be provided for different network use cases such as the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决跨机器通信问题并提供更大的灵活性，Docker还通过网络插件支持用户定义的网络。这些网络独立于容器本身存在。通过这种方式，容器可以加入到同一个现有的网络中。通过新的插件架构，可以为不同的网络使用场景提供多种驱动程序，如下所示：
- en: '**Swarm**: In a clustered situation with Swarm, the default behavior is an
    overlay network, which allows you to connect multiple Docker daemons running on
    multiple machines. In order to coordinate across multiple hosts, all containers
    and daemons must all agree on the available networks and their topologies. Overlay
    networking introduces a significant amount of complexity with dynamic port mapping
    that Kubernetes avoids.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Swarm模式**：在Swarm集群环境下，默认行为是覆盖网络，它允许你连接在多个机器上运行的多个Docker守护进程。为了在多个主机之间进行协调，所有容器和守护进程必须对可用的网络及其拓扑达成一致。覆盖网络引入了大量复杂性，包括Kubernetes避免的动态端口映射。'
- en: You can read more about overlay networks here: [https://docs.docker.com/network/overlay/](https://docs.docker.com/network/overlay/).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读更多关于覆盖网络的信息：[https://docs.docker.com/network/overlay/](https://docs.docker.com/network/overlay/)。
- en: '**Macvlan**: Docker also provides macvlan addressing, which is most similar
    to the networking model that Kubernetes provides, as it assigns each Docker container
    a MAC address that makes it appear as a physical device on your network. Macvlan
    offers a more efficient network virtualization and isolation as it bypasses the
    Linux bridge. It is important to note that as of this book''s publishing, Macvlan
    isn''t supported in most cloud providers.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Macvlan**：Docker 还提供 macvlan 地址分配，它与 Kubernetes 提供的网络模型最为相似，因为它为每个 Docker
    容器分配一个 MAC 地址，使其看起来像是网络上的一个物理设备。Macvlan 提供了更高效的网络虚拟化和隔离，因为它绕过了 Linux 桥接。需要注意的是，截至本书出版时，大多数云服务提供商不支持
    Macvlan。'
- en: As a result of these options, Docker must manage complex port allocation on
    a per-machine basis for each host IP, and that information must be maintained
    and propagated to all other machines in the cluster. Docker users a gossip protocol
    to manage the forwarding and proxying of ports to other containers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些选项，Docker 必须在每台主机的基础上管理复杂的端口分配，并且必须维护这些信息，并将其传播到集群中的其他所有机器。Docker 使用 gossip
    协议来管理端口的转发和代理。
- en: The Kubernetes approach
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 的方法
- en: 'Kubernetes'' approach to networking differs from the Docker''s, so let''s see
    how. We can learn about Kubernetes while considering four major topics in cluster
    scheduling and orchestration:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的网络方法与 Docker 的不同，让我们来看看具体有哪些差异。我们可以通过考虑集群调度和编排中的四个主要主题来了解 Kubernetes：
- en: Decoupling container-to-container communication by providing pods, not containers,
    with an IP address space
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过为 Pod 提供 IP 地址空间，而非为容器提供，从而解耦容器间通信
- en: Pod-to-pod communication and service as the dominant communication paradigm
    within the Kubernetes networking model
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 与 Pod 之间的通信以及服务作为 Kubernetes 网络模型中主要的通信范式
- en: Pod-to-service and external-to-service communications, which are provided by
    the `services` object
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 与服务之间以及外部与服务之间的通信，通过 `services` 对象提供
- en: These considerations are a meaningful simplification for the Kubernetes networking
    model, as there's no dynamic port mapping to track. Again, IP addressing is scoped
    at the pod level, which means that networking in Kubernetes requires that each
    pod has its own IP address. This means that all containers in a given pod share
    that IP address, and are considered to be in the same network namespace. We'll
    explore how to manage this shared IP resource when we discuss internal and external
    services later in this chapter. Kubernetes facilitates the pod-to-pod communication
    by not allowing the use of **network address translation** (**NAT**) for container-to-container
    or container-to-node (minion) traffic. Furthermore, the internal container IP
    address must match the IP address that is used to communicate with it. This underlines
    the Kubernetes assumption that all pods are able to communicate with all other
    pods regardless of the host they've landed on, and that communication then informs
    routing within pods to a local IP address space that is provided to containers.
    All containers within a given host can communicate with each other on their reserved
    ports via localhost. This unNATed, flat IP space simplifies networking changes
    when you begin scaling to thousands of pods.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些考虑因素是 Kubernetes 网络模型的一种有意义的简化，因为无需跟踪动态端口映射。同样，IP 地址分配是在 Pod 级别进行的，这意味着 Kubernetes
    的网络通信要求每个 Pod 都有自己的 IP 地址。这意味着一个 Pod 中的所有容器共享该 IP 地址，并且被认为在同一个网络命名空间中。我们将在本章稍后讨论内部和外部服务时，探讨如何管理这一共享的
    IP 资源。Kubernetes 通过不允许容器间或容器与节点（minion）间使用 **网络地址转换**（**NAT**）来促进 Pod 与 Pod 之间的通信。此外，内部容器的
    IP 地址必须与用于与其通信的 IP 地址匹配。这强调了 Kubernetes 的假设：所有 Pod 都能与所有其他 Pod 进行通信，无论它们落在哪个主机上，这种通信随后为
    Pod 内部提供了一个本地 IP 地址空间，从而为容器提供了网络连接。所有在同一主机中的容器可以通过本地主机在其保留端口上相互通信。这种没有 NAT 的扁平
    IP 空间简化了当你开始扩展到数千个 Pod 时的网络变更。
- en: These rules keep much of the complexity out of our networking stack and ease
    the design of the applications. Furthermore, they eliminate the need to redesign
    network communication in legacy applications that are migrated from existing infrastructure.
    In greenfield applications, they allow for a greater scale in handling hundreds,
    or even thousands of services and application communications.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则将大部分复杂性从我们的网络堆栈中剔除，并简化了应用程序的设计。此外，它们消除了重新设计从现有基础设施迁移过来的遗留应用程序网络通信的需求。在绿地应用程序中，它们允许以更大的规模处理数百甚至数千个服务和应用程序的通信。
- en: Astute readers may have also noticed that this creates a model that's backwards
    compatible with VMs and physical hosts that have a similar IP architecture as
    pods, with a single address per VM or physical host. This means you don't have
    to change your approach to service discovery, loadbalancing, application configuration,
    and port management, and can port over your application management workflows when
    working with Kubernetes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 敏锐的读者可能也注意到，这种模型与VM和物理主机的IP架构兼容，VM或物理主机每个都有一个单一地址，类似于pod的结构。这意味着你无需改变服务发现、负载均衡、应用配置和端口管理的方式，在使用Kubernetes时可以迁移你的应用管理工作流。
- en: K8s achieves this pod-wide IP magic using a pod container placeholder. Remember
    that the pause container that we saw in [Chapter 1](https://cdp.packtpub.com/getting_started_with_kubernetes__third_edition/wp-admin/post.php?post=26&action=edit#post_70),
    *Introduction to Kubernetes*, in the *Services running on the master* section,
    is often referred to as a pod infrastructure container, and it has the important
    job of reserving the network resources for our application containers that will
    be started later on. Essentially, the pause container holds the networking namespace
    and IP address for the entire pod, and can be used by all the containers running
    within. The pause container joins first and holds the namespace while the subsequent
    containers in the pod join it when they start up using Docker's `--net=container:%ID%`
    function.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: K8s通过使用一个pod容器占位符来实现这种pod级别的IP魔法。记住，我们在[第1章](https://cdp.packtpub.com/getting_started_with_kubernetes__third_edition/wp-admin/post.php?post=26&action=edit#post_70)的*引言*部分看到的暂停容器，在*主节点上运行的服务*一节中，通常被称为pod基础设施容器，它的一个重要任务是为后续启动的应用容器保留网络资源。实际上，暂停容器持有整个pod的网络命名空间和IP地址，所有在pod中运行的容器都可以使用它。暂停容器首先加入并持有命名空间，而pod中的后续容器在启动时通过Docker的`--net=container:%ID%`功能加入该命名空间。
- en: If you'd like to look over the code in the pause container, it's right here: **[https://github.com/kubernetes/kubernetes/blob/master/build/pause/pause.c](https://github.com/kubernetes/kubernetes/blob/master/build/pause/pause.c)**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看暂停容器中的代码，它就在这里：**[https://github.com/kubernetes/kubernetes/blob/master/build/pause/pause.c](https://github.com/kubernetes/kubernetes/blob/master/build/pause/pause.c)**。
- en: Kubernetes can achieve the preceding feature set using either CNI plugins for
    production workloads or kubenet networking for simplified cluster communication.
    Kubernetes can also be used when your cluster is going to rely on logical partioning
    provided by a cloud service provider's security groups or **network access control
    lists** (**NACLs**). Let's dig into the specific networking options now.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes可以通过使用CNI插件来为生产工作负载提供前述功能，或者使用kubenet网络来简化集群通信。当你的集群依赖于云服务提供商的安全组或**网络访问控制列表**（**NACLs**）提供的逻辑分区时，也可以使用Kubernetes。现在我们来深入了解具体的网络选项。
- en: Networking options
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络选项
- en: There are two approaches to the networking model that we have suggested. First,
    you can use one of the CNI plugins that exist in the ecosystem. This involves
    solutions that work with native networking layers of AWS, GCP, and Azure. There
    are also overlay-friendly plugins, which we'll cover in the next section. CNI
    is meant to be a common plugin architecture for containers. It's currently supported
    by several orchestration tools such as Kubernetes, Mesos, and CloudFoundry.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议的网络模型有两种方法。首先，你可以使用生态系统中现有的其中一个CNI插件。这涉及到与AWS、GCP和Azure的本地网络层配合的解决方案。还有一些支持叠加的插件，我们将在下一节中介绍。CNI旨在成为容器的通用插件架构。目前，多个编排工具，如Kubernetes、Mesos和CloudFoundry，都支持CNI。
- en: Network plugins are considered in alpha and therefore their capabilities, content,
    and configuration will change rapidly.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 网络插件被视为alpha版本，因此它们的功能、内容和配置将快速变化。
- en: If you're looking for a simpler alternative for testing and using smaller clusters,
    you can use the kubenet plugin, which uses `bridge` and `host-local` CNI plugs
    with a straightforward implementation of `cbr0`. This plugin is only available
    on Linux, and doesn't provide any advanced features. As it's often used with the
    supplementation of a cloud provider's networking stance, it does not handle policies
    or cross-node networking.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在寻找一个更简单的替代方案来进行测试并使用较小的集群，你可以使用kubenet插件，它使用`bridge`和`host-local` CNI插件，并简化了`cbr0`的实现。这个插件仅在Linux上可用，并且不提供任何高级功能。由于它通常与云提供商的网络配置一起使用，它不处理策略或跨节点网络通信。
- en: Just as with CPU, memory, and storage, Kubernetes takes advantage of network
    namespaces, each with their own iptables rules, interfaces, and route tables.
     Kubernetes uses iptables and NAT to manage multiple logical addresses that sit
    behind a single physical address, though you have the option to provide your cluster
    with multiple physical interfaces (NICs). Most people will find themselves generating
    multiple logical interfaces and using technologies such as multiplexing, virtual
    bridges, and hardware switching using SR-IOV in order to create multiple devices.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 CPU、内存和存储一样，Kubernetes 利用网络命名空间，每个命名空间都有自己的 iptables 规则、接口和路由表。Kubernetes
    使用 iptables 和 NAT 来管理位于单个物理地址后面的多个逻辑地址，尽管你可以选择为你的集群提供多个物理接口（网卡）。大多数人会发现自己需要生成多个逻辑接口，并使用诸如复用、虚拟桥接和使用
    SR-IOV 的硬件交换等技术来创建多个设备。
- en: You can find out more information at **[https://github.com/containernetworking/cni](https://github.com/containernetworking/cni)**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 **[https://github.com/containernetworking/cni](https://github.com/containernetworking/cni)** 找到更多信息。
- en: Always refer to the Kubernetes documentation for the latest and full list of
    supported networking options.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 始终参考 Kubernetes 文档，了解最新和完整的支持网络选项列表。
- en: Networking comparisons
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络对比
- en: To get a better understanding of networking in containers, it can be instructive
    to look at the popular choices for container networking. The following approaches do
    not make an exhaustive list, but should give a taste of the options available.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解容器中的网络，查看流行的容器网络选择是很有帮助的。以下方法并非详尽无遗，但应该能让你了解可用的选项。
- en: Weave
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Weave
- en: '**Weave** provides an overlay network for Docker containers. It can be used
    as a plugin with the new Docker network plugin interface, and it is also compatible
    with Kubernetes through a CNI plugin. Like many overlay networks, many criticize
    the performance impact of the encapsulation overhead. Note that they have recently
    added a preview release with **Virtual Extensible LAN** (**VXLAN**) encapsulation
    support, which greatly improves performance. For more information, visit [http://blog.weave.works/2015/06/12/weave-fast-datapath/](http://blog.weave.works/2015/06/12/weave-fast-datapath/).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**Weave** 提供了一个 Docker 容器的覆盖网络。它可以作为插件与新的 Docker 网络插件接口一起使用，并且通过 CNI 插件与 Kubernetes
    兼容。像许多覆盖网络一样，很多人批评其封装开销对性能的影响。请注意，他们最近添加了一个预览版本，支持 **虚拟可扩展局域网**（**VXLAN**）封装，这大大提高了性能。更多信息，请访问 [http://blog.weave.works/2015/06/12/weave-fast-datapath/](http://blog.weave.works/2015/06/12/weave-fast-datapath/)。'
- en: '[ ](http://blog.weave.works/2015/06/12/weave-fast-datapath/)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[ ](http://blog.weave.works/2015/06/12/weave-fast-datapath/)'
- en: Flannel
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flannel
- en: '**Flannel** comes from CoreOS and is an etcd-backed overlay. Flannel gives
    a full subnet to each host/node, enabling a similar pattern to the Kubernetes
    practice of a routable IP per pod or group of containers. Flannel includes an
    in-kernel VXLAN encapsulation mode for better performance and has an experimental
    multi-network mode similar to the overlay Docker plugin. For more information,
    visit [https://github.com/coreos/flannel](https://github.com/coreos/flannel).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**Flannel** 来自 CoreOS，是一个基于 etcd 的覆盖网络。Flannel 为每个主机/节点分配一个完整的子网，类似于 Kubernetes
    中每个 Pod 或容器组的可路由 IP 的做法。Flannel 包含内核级的 VXLAN 封装模式，以提供更好的性能，并具有类似于 Docker 覆盖插件的实验性多网络模式。更多信息，请访问 [https://github.com/coreos/flannel](https://github.com/coreos/flannel)。'
- en: Project Calico
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Project Calico
- en: '**Project Calico** is a layer 3-based networking model that uses the built-in
    routing functions of the Linux kernel. Routes are propagated to virtual routers
    on each host via **Border Gateway Protocol** (**BGP**). Calico can be used for
    anything from small-scale deploys to large internet-scale installations. Because
    it works at a lower level on the network stack, there is no need for additional
    NAT, tunneling, or overlays. It can interact directly with the underlying network
    infrastructure. Additionally, it has a support for network-level ACLs to provide
    additional isolation and security. For more information, visit [http://www.projectcalico.org/](http://www.projectcalico.org/).
    [](http://www.projectcalico.org/)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**Project Calico** 是一个基于第3层的网络模型，利用 Linux 内核内置的路由功能。路由通过 **边界网关协议**（**BGP**）传播到每个主机上的虚拟路由器。Calico
    可以用于从小规模部署到大规模互联网级别的安装。由于它在网络栈的较低层次工作，因此无需额外的 NAT、隧道或覆盖。它可以直接与底层网络基础设施进行交互。此外，它还支持网络级别的
    ACL 以提供额外的隔离和安全性。更多信息，请访问 [http://www.projectcalico.org/](http://www.projectcalico.org/)。'
- en: Canal
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Canal
- en: '**Canal** merges both Calico for the network policy and Flannel for the overlay
    into one solution. It supports both Calico and Flannel type overlays and uses
    the Calico policy enforcement logic. Users can choose from overlay and non-overlay
    options with this setup as it combines the features of the preceding two projects.
    For more information, visit [https://github.com/tigera/canal](https://github.com/tigera/canal).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**Canal**将Calico用于网络策略和Flannel用于覆盖网络的功能合并成一个解决方案。它支持Calico和Flannel类型的覆盖网络，并使用Calico的策略执行逻辑。用户可以根据需要选择覆盖网络或非覆盖网络选项，因为该方案结合了前述两个项目的特点。欲了解更多信息，请访问：[https://github.com/tigera/canal](https://github.com/tigera/canal)。'
- en: Kube-router
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kube-router
- en: Kube-router option is a purpose-built networking solution that aims to provide
    high performance that's easy to use. It's based on the Linux LVS/IPVS kernel load
    balancing technologies as proxy. It also uses kernel-based networking and uses
    iptables as a network policy enforcer. Since it doesn't use an overlay technology,
    it's potentially a high-performance option for the future. For more information,
    visit the following URL: [https://github.com/cloudnativelabs/kube-router](https://github.com/cloudnativelabs/kube-router).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Kube-router选项是一个专门构建的网络解决方案，旨在提供易于使用且高效的性能。它基于Linux的LVS/IPVS内核负载均衡技术作为代理。它还使用基于内核的网络，并使用iptables作为网络策略执行器。由于它不使用覆盖技术，因此它有可能成为未来的高性能选择。欲了解更多信息，请访问以下网址：[https://github.com/cloudnativelabs/kube-router](https://github.com/cloudnativelabs/kube-router)。
- en: Balanced design
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡设计
- en: It's important to point out the balance that Kubernetes is trying to achieve
    by placing the IP at the pod level. Using unique IP addresses at the host level
    is problematic as the number of containers grows. Ports must be used to expose
    services on specific containers and allow external communication. In addition
    to this, the complexity of running multiple services that may or may not know
    about each other (and their custom ports) and managing the port space becomes
    a big issue.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 需要指出的是，Kubernetes试图通过将IP地址放置在Pod级别来实现平衡。使用主机级别的唯一IP地址在容器数量增加时会变得有问题。必须使用端口来暴露特定容器上的服务并允许外部通信。除此之外，运行多个服务可能相互了解或不互相了解（以及它们的自定义端口），并且管理端口空间将成为一个大问题。
- en: However, assigning an IP address to each container can be overkill. In cases
    of sizable scale, overlay networks and NATs are needed in order to address each
    container. Overlay networks add latency, and IP addresses would be taken up by
    backend services as well since they need to communicate with their frontend counterparts.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为每个容器分配IP地址可能会显得过于复杂。在大规模应用场景中，需要使用覆盖网络和NAT来为每个容器分配地址。覆盖网络会增加延迟，而且IP地址也会被后端服务占用，因为它们需要与前端服务进行通信。
- en: Here, we really see an advantage in the abstractions that Kubernetes provides
    at the application and service level. If I have a web server and a database, we
    can keep them on the same pod and use a single IP address. The web server and
    database can use the local interface and standard ports to communicate, and no
    custom setup is required. Furthermore, services on the backend are not needlessly
    exposed to other application stacks running elsewhere in the cluster (but possibly
    on the same host). Since the pod sees the same IP address that the applications
    running within it see, service discovery does not require any additional translation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们真正看到了Kubernetes在应用程序和服务级别提供的抽象优势。如果我有一个Web服务器和一个数据库，我们可以将它们放在同一个Pod中，并使用一个IP地址。Web服务器和数据库可以使用本地接口和标准端口进行通信，无需任何自定义设置。此外，后端服务不会无谓地暴露给集群中其他地方运行的应用堆栈（但可能在同一主机上）。由于Pod看到的IP地址与其中运行的应用程序看到的相同，服务发现无需任何额外的转换。
- en: If you need the flexibility of an overlay network, you can still use an overlay
    at the pod level. Weave, Flannel, and Project Calico can be used with Kubernetes
    as well as a plethora of other plugins and overlays that are available.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要覆盖网络的灵活性，仍然可以在Pod级别使用覆盖网络。Weave、Flannel和Project Calico可以与Kubernetes一起使用，还有许多其他可用的插件和覆盖网络。
- en: This is also very helpful in the context of scheduling the workloads. It is
    key to have a simple and standard structure for the scheduler to match constraints
    and understand where space exists on the cluster's network at any given time.
    This is a dynamic environment with a variety of applications and tasks running,
    so any additional complexity here will have rippling effects.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在调度工作负载的上下文中，这也是非常有用的。为调度程序提供一个简单且标准的结构，以便匹配约束并理解集群网络中任何时刻的可用空间，是至关重要的。这是一个动态环境，运行着各种应用程序和任务，因此任何额外的复杂性都会产生连锁反应。
- en: There are also implications for service discovery. New services coming online
    must determine and register an IP address on which the rest of the world, or at
    least a cluster, can reach them. If NAT is used, the services will need an additional
    mechanism to learn their externally facing IP.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于服务发现也有影响。新的服务上线时，必须确定并注册一个 IP 地址，以便其他网络，至少是集群，可以访问它们。如果使用 NAT，服务将需要一个额外的机制来学习它们面向外部的
    IP。
- en: Advanced services
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级服务
- en: Let's explore the IP strategy as it relates to services and communication between
    containers. If you recall, in the *Services* section of Chapter 2, *Pods, Services,
    Replication Controllers, and Labels*, you learned that Kubernetes is using `kube-proxy`
    to determine the proper pod IP address and port serving each request. Behind the
    scenes, `kube-proxy` is actually using virtual IPs and iptables to make all this
    magic work.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索与服务和容器之间的通信相关的 IP 策略。如果你还记得，在第二章《Pods、Services、Replication Controllers
    和 Labels》的*服务*部分中，你了解到 Kubernetes 使用`kube-proxy`来确定为每个请求提供服务的正确 pod IP 地址和端口。在幕后，`kube-proxy`实际上是使用虚拟
    IP 和 iptables 来实现这一切的。
- en: '`kube-proxy` now has two modes—*userspace* and *iptables*. As of now, 1.2 iptables
    is the default mode. In both modes, `kube-proxy` is running on every host. Its
    first duty is to monitor the API from the Kubernetes master. Any updates to services
    will trigger an update to iptables from `kube-proxy`. For example, when a new
    service is created, a virtual IP address is chosen and a rule in iptables is set,
    which will direct its traffic to `kube-proxy` via a random port. Thus, we now
    have a way to capture service-destined traffic on this node. Since `kube-proxy`
    is running on all nodes, we have cluster-wide resolution for the service **VIP**
    (short for **virtual IP**). Additionally, DNS records can point to this VIP as
    well.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-proxy`现在有两种模式——*用户空间*和*iptables*。截至目前，1.2 版本的 iptables 是默认模式。在这两种模式下，`kube-proxy`都在每个主机上运行。它的首要任务是监控来自
    Kubernetes 主控的 API。任何服务的更新都会触发 `kube-proxy` 更新 iptables。例如，当创建一个新服务时，会选择一个虚拟 IP
    地址，并在 iptables 中设置一条规则，将流量通过一个随机端口引导到 `kube-proxy`。因此，我们现在可以在此节点上捕获面向服务的流量。由于`kube-proxy`在所有节点上运行，我们就可以在集群范围内解析服务**VIP**（虚拟
    IP 的简称）。此外，DNS 记录也可以指向这个 VIP。'
- en: In the userspace mode,we have a hook created in iptables, but the proxying of
    traffic is still handled by `kube-proxy`. The iptables rule is only sending traffic
    to the service entry in `kube-proxy` at this point. Once `kube-proxy` receives
    the traffic for a particular service, it must then forward it to a pod in the
    service's pool of candidates. It does this using a random port that was selected
    during service creation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户空间模式下，我们在 iptables 中创建了一个钩子，但流量的代理仍然由`kube-proxy`处理。此时，iptables 规则只是将流量发送到`kube-proxy`中的服务入口。一旦`kube-proxy`收到特定服务的流量，它必须将流量转发到该服务候选池中的一个
    pod。它通过在服务创建时选择的随机端口来实现这一点。
- en: 'Refer to the following diagram for an overview of the flow:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考下面的图示以概览流程：
- en: '![](img/7e34dd71-9107-4e88-9d3d-947eced58681.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e34dd71-9107-4e88-9d3d-947eced58681.png)'
- en: Kube-proxy communication
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Kube-proxy 通信
- en: It is also possible to always forward traffic from the same client IP to the
    same backend pod/container using the `sessionAffinity` element in your service
    definition.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过在服务定义中使用`sessionAffinity`元素，始终将来自同一客户端 IP 的流量转发到同一后台 pod/container。
- en: In the iptables mode, the pods are coded directly in the iptable rules. This
    removes the dependency on `kube-proxy` for actually proxying the traffic. The
    request will go straight to iptables and then on to the pod. This is faster and
    removes a possible point of failure. Readiness probe, as we discussed in the *Health
    Check *section of Chapter 2, *Pods, Services, Replication Controllers, and Labels*, is your
    friend here as this mode also loses the ability to retry pods.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 iptables 模式下，pod 直接在 iptable 规则中编码。这消除了对 `kube-proxy` 的依赖，从而直接将流量传递给 iptables，再到
    pod。这种方式更快，减少了可能的故障点。正如我们在第 2 章 *健康检查* 部分中讨论的那样，*Pod、服务、复制控制器和标签*，`Readiness probe`
    在这种模式下非常有用，因为它弥补了丧失重试 pod 功能的问题。
- en: External services
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部服务
- en: 'In the previous chapter, we saw a few service examples. For testing and demonstration
    purposes, we wanted all the services to be externally accessible. This was configured
    by the `type: LoadBalancer` element in our service definition. The `LoadBalancer`
    type creates an external load balancer on the cloud provider. We should note that
    support for external load balancers varies by provider, as does the implementation.
    In our case, we are using GCE, so integration is pretty smooth. The only additional
    setup needed is to open firewall rules for the external service ports.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '在上一章中，我们看到了一些服务示例。为了测试和演示的目的，我们希望所有的服务都可以被外部访问。这是通过在服务定义中的 `type: LoadBalancer`
    元素配置的。`LoadBalancer` 类型会在云提供商上创建一个外部负载均衡器。我们需要注意的是，不同的云提供商对外部负载均衡器的支持情况和实现方式各不相同。在我们的案例中，我们使用的是
    GCE，因此集成非常顺利。唯一需要额外设置的就是打开防火墙规则，允许外部服务端口的访问。'
- en: 'Let''s dig a little deeper and do a `describe` command on one of the services
    from the *More on labels* section in Chapter 2, *Pods, Services, Replication Controllers,
    and Labels*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解一下，使用 `describe` 命令查看第 2 章 *Pod、服务、复制控制器和标签* 中 *更多关于标签* 部分的一个服务：
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的执行结果：
- en: '![](img/4e9a1c4f-8a04-4ef7-b7cc-41ea5396e833.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e9a1c4f-8a04-4ef7-b7cc-41ea5396e833.png)'
- en: Service description
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 服务描述
- en: In the output of the preceding screenshot, you'll note several key elements.
    Our `Namespace:` is set to `default`, the `Type:` is `LoadBalancer`, and we have
    the external IP listed under `LoadBalancer Ingress:`. Furthermore, we can see
    `Endpoints:`, which shows us the IPs of the pods that are available to answer
    service requests.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图输出中，你会注意到几个关键元素。我们的 `Namespace:` 设置为 `default`，`Type:` 为 `LoadBalancer`，并且我们可以在
    `LoadBalancer Ingress:` 下看到外部 IP。此外，我们还可以看到 `Endpoints:`，它显示了可以响应服务请求的 pod 的 IP
    地址。
- en: Service discovery
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务发现
- en: 'As we discussed earlier, the Kubernetes master keeps track of all service definitions
    and updates. Discovery can occur in one of three ways. The first two methods use
    Linux environment variables. There is support for the Docker link style of environment
    variables, but Kubernetes also has its own naming convention. Here is an example
    of what our `node-js` service example might look like using K8s environment variables
    (note that IPs will vary):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，Kubernetes 主节点会跟踪所有服务定义和更新。发现服务的方式有三种。前两种方法使用 Linux 环境变量。虽然支持 Docker
    链接样式的环境变量，但 Kubernetes 也有自己的命名约定。下面是我们 `node-js` 服务示例使用 K8s 环境变量的样子（注意 IP 会有所不同）：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Another option for discovery is through DNS. While environment variables can
    be useful when DNS is not available, it has drawbacks. The system only creates
    variables at creation time, so services that come online later will not be discovered
    or will require some additional tooling to update all the system environments.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种发现服务的方式是通过 DNS。虽然当 DNS 不可用时，环境变量可以发挥作用，但它也有一些缺点。系统只会在创建时创建变量，因此后续上线的服务将无法被发现，或者需要一些额外的工具来更新所有系统环境变量。
- en: Internal services
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内部服务
- en: 'Let''s explore the other types of services that we can deploy. First, by default,
    services are only internally facing. You can specify a type of `clusterIP` to
    achieve this, but, if no type is defined, `clusterIP` is the assumed type. Let''s
    take a look at an example, `nodejs-service-internal.yaml`; note the lack of the
    `type` element:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索可以部署的其他类型的服务。首先，默认情况下，服务仅面向内部。你可以通过指定 `clusterIP` 类型来实现这一点，但如果没有定义类型，`clusterIP`
    会被视为默认类型。让我们看一个例子，`nodejs-service-internal.yaml`；注意没有 `type` 元素：
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Use this listing to create the service definition file. You''ll need a healthy
    version of the `node-js` RC (Listing `nodejs-health-controller-2.yaml`). As you
    can see, the selector matches on the pods named `node-js` that our RC launched
    in the previous chapter. We will create the service and then list the currently
    running services with a filter as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此清单创建服务定义文件。你将需要一个健康版本的 `node-js` RC（清单 `nodejs-health-controller-2.yaml`）。如你所见，选择器与我们在前一章启动的名为
    `node-js` 的 pods 匹配。我们将创建服务，然后列出当前运行的服务，并按如下过滤：
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前一个命令的结果：
- en: '![](img/2ad58904-bda0-4cc4-955b-6d32a05d3ceb.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ad58904-bda0-4cc4-955b-6d32a05d3ceb.png)'
- en: Internal service listing
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 内部服务清单
- en: 'As you can see, we have a new service, but only one IP. Furthermore, the IP
    address is not externally accessible. We won''t be able to test the service from
    a web browser this time. However, we can use the handy `kubectl exec` command
    and attempt to connect from one of the other pods. You will need `node-js-pod`
    (`nodejs-pod.yaml`) running. Then, you can execute the following command:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们有了一个新服务，但只有一个 IP 地址。此外，该 IP 地址无法从外部访问。这一次我们无法通过 Web 浏览器测试该服务。然而，我们可以使用便捷的
    `kubectl exec` 命令并尝试从其他 pods 连接。你需要运行 `node-js-pod`（`nodejs-pod.yaml`）。然后，你可以执行以下命令：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This allows us to run a `docker exec` command as if we had a shell in the `node-js-pod`
    container. It then hits the internal service URL, which forwards to any pods with
    the `node-js` label.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许我们像在 `node-js-pod` 容器中拥有一个 shell 一样运行 `docker exec` 命令。然后它会访问内部服务 URL，该 URL
    会转发到带有 `node-js` 标签的任何 pods。
- en: If all is well, you should get the raw HTML output back. You have successfully
    created an internal-only service. This can be useful for backend services that
    you want to make available to other containers running in your cluster, but not
    open to the world at large.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该会收到原始的 HTML 输出。你已经成功创建了一个仅限内部的服务。这对于你希望向运行在集群中的其他容器提供服务，但又不对外开放的后台服务非常有用。
- en: Custom load balancing
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义负载均衡
- en: 'A third type of service that K8s allows is the `NodePort` type. This type allows
    us to expose a service through the host or node (minion) on a specific port. In
    this way, we can use the IP address of any node (minion) and access our service
    on the assigned node port. Kubernetes will assign a node port by default in the
    range of `3000`-`32767`, but you can also specify your own custom port. In the
    example in the following listing `nodejs-service-nodeport.yaml`, we choose port
    `30001`, as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: K8s 允许的第三种服务类型是 `NodePort` 类型。此类型允许我们通过主机或节点（minion）在特定端口公开服务。通过这种方式，我们可以使用任何节点（minion）的
    IP 地址并访问在分配的节点端口上的服务。Kubernetes 默认会在 `3000` 到 `32767` 的范围内分配一个节点端口，但你也可以指定自己的自定义端口。在以下清单
    `nodejs-service-nodeport.yaml` 的示例中，我们选择端口 `30001`，如下所示：
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once again, create this YAML definition file and create your service, as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 再次创建此 YAML 定义文件并创建你的服务，如下所示：
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output should have a message like this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该包含类似这样的消息：
- en: '![](img/eb464d3d-f14e-4e0f-b00f-99768befc422.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb464d3d-f14e-4e0f-b00f-99768befc422.png)'
- en: New GCP firewall rule
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 新的 GCP 防火墙规则
- en: Note message about opening firewall ports. Similar to the external load balancer
    type, `NodePort` is exposing your service externally using ports on the nodes.
    This could be useful if, for example, you want to use your own load balancer in
    front of the nodes. Let's make sure that we open those ports on GCP before we
    test our new service.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 关于开启防火墙端口的提示。与外部负载均衡器类型类似，`NodePort` 正在使用节点上的端口将服务暴露到外部。如果你希望在节点前面使用自己的负载均衡器，这将非常有用。在我们测试新服务之前，让我们确保在
    GCP 上打开这些端口。
- en: From the GCE VM instance console, click on the details for any of your nodes
    (minions). Then, click on the network, which is usually the default unless otherwise
    specified during creation. In Firewall rules, we can add a rule by clicking on Add
    firewall rule.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从 GCE VM 实例控制台中，点击你任一节点（minion）的详细信息。然后，点击网络，通常默认情况下它会是创建时指定的网络。进入防火墙规则，我们可以通过点击“添加防火墙规则”来添加规则。
- en: 'Create a rule like the one shown in the following screenshot (`tcp:30001` on
    the `0.0.0.0/0` IP range):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 创建如下所示的规则（`tcp:30001` 在 `0.0.0.0/0` IP 范围内）：
- en: '![](img/18e6ed21-7667-42db-bf30-180f96d3a256.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18e6ed21-7667-42db-bf30-180f96d3a256.png)'
- en: Create a new firewall rule page
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新的防火墙规则页面
- en: 'We can now test our new service by opening a browser and using an IP address
    of any node (minion) in your cluster. The format to test the new service is as
    follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过打开浏览器并使用集群中任何节点（minion）的IP地址来测试我们的新服务。测试新服务的格式如下：
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Finally, the latest version has added an `ExternalName` type, which maps a `CNAME`
    to the service.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最新版本添加了`ExternalName`类型，它将`CNAME`映射到服务。
- en: Cross-node proxy
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨节点代理
- en: 'Remember that `kube-proxy` is running on all the nodes, so even if the pod
    is not running there, the traffic will be given a proxy to the appropriate host.
    Refer to the *Cross-node traffic* screenshot for a visual on how the traffic flows.
    A user makes a request to an external IP or URL. The request is serviced by **Node** in
    this case. However, the pod does not happen to run on this node. This is not a
    problem because the pod IP addresses are routable. So, `kube-proxy` or **iptables**
    simply passes traffic onto the pod IP for this service. The network routing then
    completes on **Node 2**, where the requested application lives:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`kube-proxy`在所有节点上运行，所以即使Pod没有在该节点上运行，流量也会被代理到适当的主机。参见*跨节点流量*截图，了解流量如何流动的可视化效果。用户向外部IP或URL发起请求。此请求由**Node**服务。在这种情况下，Pod并没有运行在此节点上。但这不是问题，因为Pod的IP地址是可路由的。所以，`kube-proxy`或**iptables**会将流量直接传递到该服务的Pod
    IP上。网络路由会在**Node 2**上完成，这里是请求的应用程序所在：
- en: '![](img/d6f8a439-78ca-4975-91c2-0ea105849507.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6f8a439-78ca-4975-91c2-0ea105849507.png)'
- en: Cross-node traffic
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 跨节点流量
- en: Custom ports
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义端口
- en: 'Services also allow you to map your traffic to different ports; then, the containers
    and pods expose themselves. We will create a service that exposes port `90` and
    forwards traffic to port `80` on the pods. We will call the `node-js-90` pod to
    reflect the custom port number. Create the following two definition files, `nodejs-customPort-controller.yaml`
    and `nodejs-customPort-service.yaml`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 服务还允许你将流量映射到不同的端口；然后，容器和Pod会暴露自己。我们将创建一个服务，暴露端口`90`并将流量转发到Pod上的端口`80`。我们将把这个Pod命名为`node-js-90`，以反映自定义的端口号。创建以下两个定义文件，`nodejs-customPort-controller.yaml`和`nodejs-customPort-service.yaml`：
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you are using the free trial for Google Cloud Platform, you may have issues
    with the `LoadBalancer` type services. This type creates multiple external IP
    addresses, but trial accounts are limited to only one static address.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Google Cloud Platform的免费试用版，可能会遇到`LoadBalancer`类型服务的问题。该类型会创建多个外部IP地址，但试用账户仅限于一个静态地址。
- en: You'll note that in the service definition, we have a `targetPort` element.
    This element tells the service the port to use for pods/containers in the pool.
    As we saw in previous examples, if you do not specify `targetPort`, it assumes
    that it's the same port as the service. This port is still used as the service
    port, but, in this case, we are going to expose the service on port `90` while
    the containers serve content on port `80`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，在服务定义中，我们有一个`targetPort`元素。这个元素告诉服务在池中使用哪个端口供Pod/容器使用。正如我们在前面的示例中看到的，如果没有指定`targetPort`，则假定它与服务端口相同。这个端口仍然作为服务端口使用，但在这种情况下，我们将通过端口`90`暴露服务，而容器在端口`80`上提供内容。
- en: 'Create this RC and service and open the appropriate firewall rules, as we did
    in the last example. It may take a moment for the external load balancer IP to
    propagate to the `get service` command. Once it does, you should be able to open
    and see our familiar web application in a browser using the following format:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这个RC和服务并打开相应的防火墙规则，像我们在上一个示例中做的那样。可能需要一些时间，直到外部负载均衡器的IP被传播到`get service`命令中。一旦它传播完毕，你应该能够通过以下格式打开并在浏览器中看到我们熟悉的Web应用程序：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Multiple ports
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多个端口
- en: 'Another custom port use case is that of multiple ports. Many applications expose
    multiple ports, such as HTTP on port `80` and port `8888` for web servers. The
    following example shows our app responding on both ports. Once again, we''ll also
    need to add a firewall rule for this port, as we did for the list `nodejs-service-nodeport.yaml`
    previously. Save the listing as `nodejs-multi-controller.yaml` and `nodejs-multi-service.yaml`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个自定义端口的使用案例是多个端口。许多应用程序会暴露多个端口，比如HTTP在端口`80`和端口`8888`上为Web服务器提供服务。以下示例展示了我们的应用程序在这两个端口上的响应。同样，我们也需要为这个端口添加一个防火墙规则，像我们之前为`nodejs-service-nodeport.yaml`列表做的那样。将列表保存为`nodejs-multi-controller.yaml`和`nodejs-multi-service.yaml`：
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The application and container itself must be listening on both ports for this
    to work. In this example, port `8888` is used to represent a fake admin interface. If,
    for example, you want to listen on port `443`, you would need a proper SSL socket
    listening on the server.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序和容器本身必须监听这两个端口才能使其正常工作。在此示例中，端口 `8888` 用于表示一个虚拟的管理界面。例如，如果你想监听端口 `443`，你需要在服务器上监听一个适当的
    SSL 套接字。
- en: Ingress
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ingress
- en: We previously discussed how Kubernetes uses the service abstract as a means
    to proxy traffic to a backing pod that's distributed throughout our cluster. While
    this is helpful in both scaling and pod recovery, there are more advanced routing
    scenarios that are not addressed by this design.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过，Kubernetes 如何使用服务抽象作为将流量代理到分布在集群中的后端 pod 的方式。虽然这种方式在扩展和 pod 恢复方面非常有用，但有一些更复杂的路由场景是此设计无法解决的。
- en: To that end, Kubernetes has added an ingress resource, which allows for custom
    proxying and load balancing to a back service. Think of it as an extra layer or
    hop in the routing path before traffic hits our service. Just as an application
    has a service and backing pods, the ingress resource needs both an Ingress entry
    point and an ingress controller that perform the custom logic. The entry point
    defines the routes and the controller actually handles the routing. This is helpful
    for picking up traffic that would normally be dropped by an edge router or forwarded
    elsewhere outside of the cluster.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，Kubernetes 添加了一个 ingress 资源，允许进行自定义代理和负载均衡到后端服务。可以将其视为在流量到达我们的服务之前，路由路径中的额外一层或跳点。就像应用程序有服务和后端
    pod 一样，ingress 资源需要一个 Ingress 入口点和一个执行自定义逻辑的 ingress 控制器。入口点定义路由，控制器实际处理路由。这对于捕获通常会被边缘路由器丢弃或转发到集群外部的流量非常有用。
- en: Ingress itself can be configured to offer externally addressable URLs for internal
    services, to terminate SSL, offer name-based virtual hosting as you'd see in a
    traditional web server, or load balance traffic. Ingress on its own cannot service
    requests, but requires an additional ingress controller to fulfill the capabilities
    outlined in the object. You'll see nginx and other load balancing or proxying
    technology involved as part of the controller framework.  In the following examples,
    we'll be using GCE, but you'll need to deploy a controller yourself in order to
    take advantage of this feature. A popular option at the moment is the nginx-based
    ingress-nginx controller.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 本身可以配置为为内部服务提供外部可访问的 URL，终止 SSL，提供基于名称的虚拟主机（就像传统 Web 服务器中的做法），或进行负载均衡。Ingress
    本身无法处理请求，但需要额外的 ingress 控制器来实现对象中列出的功能。你会看到 nginx 和其他负载均衡或代理技术作为控制器框架的一部分参与其中。在以下示例中，我们将使用
    GCE，但你需要自己部署控制器才能利用此功能。目前流行的一个选择是基于 nginx 的 ingress-nginx 控制器。
- en: You can check it out here: **[https://github.com/kubernetes/ingress-gce/blob/master/BETA_LIMITATIONS.md#glbc-beta-limitations](https://github.com/kubernetes/ingress-gce/blob/master/BETA_LIMITATIONS.md#glbc-beta-limitations).**
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里查看：**[https://github.com/kubernetes/ingress-gce/blob/master/BETA_LIMITATIONS.md#glbc-beta-limitations](https://github.com/kubernetes/ingress-gce/blob/master/BETA_LIMITATIONS.md#glbc-beta-limitations)。**
- en: An ingress controller is deployed as a pod which runs a daemon. This pod watches
    the Kubernetes apiserver/ingresses endpoint for changes to the ingress resource.
    For our examples, we will use the default GCE backend.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 控制器作为运行守护进程的 pod 部署。该 pod 监视 Kubernetes apiserver/ingresses 端点，查看 ingress
    资源的变化。对于我们的示例，我们将使用默认的 GCE 后端。
- en: Types of ingress
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ingress 的类型
- en: 'There are a couple different types of ingress, such as the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同类型的 ingress，例如：
- en: '**Single service ingress**: This strategy exposes a single service via creating
    an ingress with a default backend that has no rules. You can alternatively use
    `Service.Type=LoadBalancer` or `Service.Type=NodePort`, or a port proxy to accomplish
    something similar.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单服务 ingress**：此策略通过创建一个没有规则的默认后端的 ingress 来暴露单个服务。你也可以使用 `Service.Type=LoadBalancer`
    或 `Service.Type=NodePort`，或使用端口代理来实现类似的功能。'
- en: '**Fanout**: Given that od IP addressing is only available internally to the
    Kubernetes network, you''ll need to use a simple fanout strategy in order to accommodate
    edge traffic and provide ingress to the correct endpoints in your cluster. This
    will resemble a load balancer in practice.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fanout**：由于 IP 地址仅在 Kubernetes 网络内部可用，因此你需要使用简单的 fanout 策略来适应边缘流量，并为集群中的正确端点提供
    ingress。这在实践中类似于负载均衡器。'
- en: '**Name-based hosting:** This approach is similar to **service name indication**
    (**SNI**), which allows a web server to present multiple HTTPS websites with different
    certificates on the same TCP port and IP address.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于名称的托管：** 这种方法类似于**服务名称指示（SNI）**，它允许Web服务器在同一个TCP端口和IP地址上呈现多个HTTPS网站，并使用不同的证书。'
- en: 'Kubernetes uses host headers to route requests with this approach. The following
    example snippet `ingress-example.yaml` shows what name-based virtual hosting would
    look like:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes使用主机头来路由请求。在以下示例片段`ingress-example.yaml`中，展示了基于名称的虚拟托管的样子：
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As you may recall, in Chapter 1, *Introduction to Kubernetes*, we saw that
    a GCE cluster comes with a default back which provides Layer 7 load balancing
    capability. We can see this controller running if we look at the `kube-system`
    namespace:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能记得的，在第1章*《Kubernetes简介》*中，我们看到GCE集群默认带有提供第7层负载均衡功能的后端。如果我们查看`kube-system`命名空间，就能看到这个控制器正在运行：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We should see an RC listed with the `l7-default-backend-v1.0` name, as shown
    here:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到一个名为`l7-default-backend-v1.0`的RC，如下所示：
- en: '![](img/87c3ec22-3e4d-4a90-94e6-964c53daab36.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87c3ec22-3e4d-4a90-94e6-964c53daab36.png)'
- en: GCE Layer 7 Ingress controller
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: GCE第7层Ingress控制器
- en: This provides the ingress controller piece that actually routes the traffic
    defined in our ingress entry points. Let's create some resources for an Ingress.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了Ingress控制器部分，实际上它会路由我们在Ingress入口点中定义的流量。让我们为Ingress创建一些资源。
- en: 'First, we will create a few new replication controllers with the `httpwhalesay`
    image. This is a remix of the original whalesay that was displayed in a browser.
    The following listing, `whale-rcs.yaml`, shows the YAML. Note the three dashes
    that let us combine several resources into one YAML file:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一些新的复制控制器，使用`httpwhalesay`镜像。这是原始whalesay的重混版，原始whalesay在浏览器中显示。以下列出了`whale-rcs.yaml`中的YAML配置。注意那三个破折号，它们允许我们将多个资源合并到一个YAML文件中：
- en: '[PRE13]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Note that we are creating pods with the same container, but different startup
    parameters. Take note of these parameters for later. We will also create `Service` endpoints
    for each of these RCs as shown in the `whale-svcs.yaml` listing:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在创建具有相同容器但不同启动参数的Pod。记下这些参数，稍后会用到。我们还将为这些RC创建`Service`端点，如`whale-svcs.yaml`列出的那样：
- en: '[PRE14]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Again, create these with the `kubectl create -f` command, as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用`kubectl create -f`命令创建这些，如下所示：
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We should see messages about the successful creation of the RCs and Services.
    Next, we need to define the Ingress entry point. We will use `http://a.whale.hey`
    and `http://b.whale.hey` as our demo entry points as shown in the following listing
    `whale-ingress.yaml`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到关于成功创建RC和服务的消息。接下来，我们需要定义Ingress入口点。我们将使用`http://a.whale.hey`和`http://b.whale.hey`作为我们的演示入口点，如下所示的`whale-ingress.yaml`：
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Again, use `kubectl create -f` to create this ingress. Once this is successfully
    created, we will need to wait a few moments for GCE to give the ingress a static
    IP address. Use the following command to watch the Ingress resource:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用`kubectl create -f`命令创建这个Ingress。创建成功后，我们需要等待一会儿，直到GCE为Ingress分配一个静态IP地址。使用以下命令来监视Ingress资源：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once the Ingress has an IP, we should see an entry in `ADDRESS`, like the one
    shown here:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Ingress获得IP地址，我们应该在`ADDRESS`中看到一条条目，如下所示：
- en: '![](img/60b081cd-ed9c-4c6f-8be9-99915cf54209.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60b081cd-ed9c-4c6f-8be9-99915cf54209.png)'
- en: Ingress description
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress描述
- en: 'Since this is not a registered domain name, we will need to specify the resolution
    in the `curl` command, like this:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这不是一个注册的域名，我们需要在`curl`命令中指定解析，像这样：
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This should display the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该显示以下内容：
- en: '![](img/c71c4741-37d4-4913-890d-6443b1617002.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c71c4741-37d4-4913-890d-6443b1617002.png)'
- en: Whalesay A
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Whalesay A
- en: 'We can also try the second URL. Doing this, we will get our second RC:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以尝试第二个URL。这样做，我们将获得第二个RC：
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/dd5c0e9f-50f9-4809-ac95-33ddd5e5ca4a.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd5c0e9f-50f9-4809-ac95-33ddd5e5ca4a.png)'
- en: Whalesay B
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Whalesay B
- en: Note that the images are almost the same, except that the words from each whale
    reflect the startup parameters from each RC we started earlier. Thus, our two
    Ingress points are directing traffic to different backends.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些镜像几乎相同，只是每个鲸鱼的文字反映了我们之前启动的每个RC的启动参数。因此，我们的两个Ingress点正在将流量引导到不同的后端。
- en: In this example, we used the default GCE backend for an Ingress controller.
    Kubernetes allows us to build our own, and nginx actually has a few versions available
    as well.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了默认的GCE后端作为Ingress控制器。Kubernetes允许我们构建自己的控制器，而nginx也有几个可用版本。
- en: Migrations, multicluster, and more
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移、多集群等
- en: As we've already seen so far, Kubernetes offers a high level of flexibility
    and customization to create a service abstraction around your containers running
    in the cluster. However, there may be times where you want to point to something
    outside your cluster.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们到目前为止所看到的，Kubernetes 提供了高度的灵活性和定制性，用于在集群中创建围绕容器的服务抽象。然而，有时你可能希望指向集群外部的某些内容。
- en: An example of this would be working with legacy systems or even applications
    running on another cluster. In the case of the former, this is a perfectly good
    strategy in order to migrate to Kubernetes and containers in general. We can begin
    by managing the service endpoints in Kubernetes while stitching the stack together
    using the K8s orchestration concepts. Additionally, we can even start bringing
    over pieces of the stack, as the frontend, one at a time as the organization refactors
    applications for microservices and/or containerization.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这类情况的一个例子是与遗留系统或甚至在另一个集群上运行的应用程序进行合作。对于前者来说，这是一个完全有效的策略，用于迁移到 Kubernetes 和容器化。我们可以通过在
    Kubernetes 中管理服务端点，同时利用 K8s 的编排概念将堆栈拼接起来，作为开始。此外，我们甚至可以开始逐步引入堆栈的各个部分，比如前端，当组织重构应用程序为微服务和/或容器化时。
- en: 'To allow access to non pod-based applications, the services construct allows
    you to use endpoints that are outside the cluster. Kubernetes is actually creating
    an endpoint resource every time you create a service that uses selectors. The
    `endpoints` object keeps track of the pod IPs in the load balancing pool. You
    can see this by running the `get endpoints` command, as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了允许访问非 Pod 基础的应用程序，服务构造允许你使用集群外部的端点。实际上，每次你创建一个使用选择器的服务时，Kubernetes 都会创建一个端点资源。`endpoints`
    对象会跟踪负载均衡池中的 Pod IP 地址。你可以通过运行 `get endpoints` 命令看到这一点，如下所示：
- en: '[PRE20]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You should see something similar to the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到类似下面的内容：
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You'll note the entry for all the services we currently have running on our
    cluster. For most services, the endpoints are just the IP of each pod running
    in an RC. As I mentioned previously, Kubernetes does this automatically based
    on the selector. As we scale the replicas in a controller with matching labels,
    Kubernetes will update the endpoints automatically.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到当前集群上运行的所有服务的条目。对于大多数服务，端点只是每个在 RC 中运行的 Pod 的 IP 地址。如前所述，Kubernetes 会根据选择器自动执行此操作。当我们在控制器中扩展具有匹配标签的副本时，Kubernetes
    会自动更新端点。
- en: 'If we want to create a service for something that is not a pod and therefore
    has no labels to select, we can easily do this with both a service definition `nodejs-custom-service.yaml`
    and endpoint definition `nodejs-custom-endpoint.yaml`, as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想为一个不是 Pod 的服务创建一个服务，而该服务没有标签可以选择，我们可以通过定义 `nodejs-custom-service.yaml`
    服务和 `nodejs-custom-endpoint.yaml` 端点，轻松实现，如下所示：
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the preceding example, you'll need to replace `<X.X.X.X>` with a real IP
    address, where the new service can point to. In my case, I used the public load
    balancer IP from the `node-js-multi` service we created earlier in listing `ingress-example.yaml`.
    Go ahead and create these resources now.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，你需要将 `<X.X.X.X>` 替换为一个真实的 IP 地址，新的服务可以指向该地址。在我的案例中，我使用了之前在 `ingress-example.yaml`
    中创建的 `node-js-multi` 服务的公共负载均衡器 IP。现在可以开始创建这些资源了。
- en: 'If we now run a `get endpoints` command, we will see this IP address at port
    `80`, which is associated with the `custom-service` endpoint. Furthermore, if
    we look at the service details, we will see the IP listed in the `Endpoints` section:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在运行 `get endpoints` 命令，我们将在 `80` 端口看到这个 IP 地址，它与 `custom-service` 端点关联。此外，如果我们查看服务详情，我们将在
    `Endpoints` 部分看到该 IP 地址：
- en: '[PRE24]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We can test out this new service by opening the `custom-service` external IP
    from a browser.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过浏览器打开 `custom-service` 的外部 IP 来测试这个新服务。
- en: Custom addressing
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义地址
- en: 'Another option to customize services is with the `clusterIP` element. In our
    examples so far, we''ve not specified an IP address, which means that it chooses
    the internal address of the service for us. However, we can add this element and
    choose the IP address in advance with something like `clusterip: 10.0.125.105`.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '定制服务的另一个选项是使用 `clusterIP` 元素。在到目前为止的示例中，我们没有指定 IP 地址，这意味着它为我们选择了服务的内部地址。然而，我们可以添加这个元素，并提前选择
    IP 地址，像这样：`clusterip: 10.0.125.105`。'
- en: There may be times when you don't want to load balance and would rather have
    DNS with *A* records for each pod. For example, software that needs to replicate
    data evenly to all nodes may rely on *A* records to distribute data. In this case,
    we can use an example like the following one and set `clusterip` to `None`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 有时您可能不希望负载均衡，而是希望为每个pod使用*A*记录的DNS。例如，需要将数据均匀复制到所有节点的软件可能依赖于*A*记录来分发数据。在这种情况下，我们可以使用以下示例，并将`clusterip`设置为`None`。
- en: 'Kubernetes will not assign an IP address and instead only assign *A* records
    in DNS for each of the pods. If you are using DNS, the service should be available
    at `node-js-none` or `node-js-none.default.cluster.local` from within the cluster.
    For this, we  will use the following listing `nodejs-headless-service.yaml`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes不会为每个pod分配IP地址，而是仅为每个pod分配*A*记录。如果使用DNS，则从集群内部可以通过`node-js-none`或`node-js-none.default.cluster.local`访问服务。为此，我们将使用以下清单`nodejs-headless-service.yaml`：
- en: '[PRE25]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Test it out after you create this service with the trusty `exec` command:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 创建此服务后，请使用可靠的`exec`命令测试它：
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: DNS
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNS
- en: DNS solves the issues seen with environment variables by allowing us to reference
    the services by their name. As services restart, scale out, or appear anew, the
    DNS entries will be updating and ensuring that the service name always points
    to the latest infrastructure. DNS is set up by default in most of the supported
    providers. You can add DNS support for your cluster via a cluster add on ([https://kubernetes.io/docs/concepts/cluster-administration/addons/](https://kubernetes.io/docs/concepts/cluster-administration/addons/)).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: DNS通过允许我们按名称引用服务来解决环境变量带来的问题。随着服务的重新启动、扩展或新增，DNS条目会更新，确保服务名称始终指向最新的基础设施。大多数支持的提供商默认设置了DNS。您可以通过添加集群附加组件来为您的集群添加DNS支持（[https://kubernetes.io/docs/concepts/cluster-administration/addons/](https://kubernetes.io/docs/concepts/cluster-administration/addons/)）。
- en: 'If DNS is supported by your provider, but is not set up, you can configure
    the following variables in your default provider config when you create your Kubernetes
    cluster:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的提供商支持DNS但未设置，请在创建Kubernetes集群时在默认提供商配置中配置以下变量：
- en: '`ENABLE_CLUSTER_DNS="${KUBE_ENABLE_CLUSTER_DNS:-true}"` `DNS_SERVER_IP="10.0.0.10"`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`ENABLE_CLUSTER_DNS="${KUBE_ENABLE_CLUSTER_DNS:-true}"` `DNS_SERVER_IP="10.0.0.10"`'
- en: '`DNS_DOMAIN="cluster.local"`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`DNS_DOMAIN="cluster.local"`'
- en: '`DNS_REPLICAS=1`.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`DNS_REPLICAS=1`。'
- en: With DNS active, services can be accessed in one of two forms—either the service
    name itself, `<service-name>`, or a fully qualified name that includes the namespace,
    `<service-name>.<namespace-name>.cluster.local`. In our examples, it would look
    similar to `node-js-90` or `node-js-90.default.cluster.local`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 启用DNS后，服务可以以两种形式访问，即服务名称本身`<service-name>`或包括命名空间的完全限定名称`<service-name>.<namespace-name>.cluster.local`。在我们的示例中，它看起来类似于`node-js-90`或`node-js-90.default.cluster.local`。
- en: The DNS server create DNS records based on new services that are created through
    the API. Pods in shared DNS namespaces will be able to see each other, and can
    use DNS SRV records to record ports as well.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: DNS服务器基于通过API创建的新服务创建DNS记录。共享DNS命名空间中的pod将能够看到彼此，并且可以使用DNS SRV记录记录端口。
- en: Kubernetes DNS is comprised of a DNS pod and Service on the cluster which communicates
    directly with kubelets and containers in order to translate DNS names to IP. Services
    with clusterIPs are given `my-service.my-namespace.svc.cluster.local` addresses.
    If the service does not have a clusterIP (otherwise called headless)  it gets
    the same address format, but this resolves in a round-robin fashion to a number
    of IPs that point to the pods of a service. There a number of DNS policies that
    can also be set.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes DNS由集群中的一个DNS pod和一个Service组成，它直接与kubelets和容器通信，以便将DNS名称转换为IP地址。具有clusterIP的服务被赋予`my-service.my-namespace.svc.cluster.local`地址。如果服务没有clusterIP（也称为headless），它将以轮询方式解析为指向服务pod的多个IP地址。还有一些可以设置的DNS策略。
- en: 'One of the Kubernetes incubator projects, [CoreDNS](https://github.com/coredns/coredns) can
    also be used for service discovery. This replaces the native `kube-dns` DNS services
    and requires Kubernetes v1.9 or later. You''ll need to leverage `kubeadm` during
    the initialization process in order to try CoreDNS out. You can install this on
    your cluster with the following command:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes孵化项目之一，[CoreDNS](https://github.com/coredns/coredns)，也可用于服务发现。它取代了本地的`kube-dns`
    DNS服务，需要Kubernetes v1.9或更高版本。在初始化过程中，您需要使用`kubeadm`来尝试CoreDNS。您可以使用以下命令在集群上安装它：
- en: '[PRE27]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If you'd like more information on an example use case of CoreDNS, check out
    this blog post: [https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/](https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于CoreDNS的示例使用案例，可以查看这篇博客：[https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/](https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/)。
- en: Multitenancy
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多租户
- en: Kubernetes also has an additional construct for isolation at the cluster level.
    In most cases, you can run Kubernetes and never worry about namespaces; everything
    will run in the default namespace if not specified. However, in cases where you
    run multitenancy communities or want broad-scale segregation and isolation of
    the cluster resources, namespaces can be used to this end. True, end-to-end multitenancy
    is not yet feature complete in Kubernetes, but you can get very close using RBAC,
    container permissions, ingress rules, and clear network policing. If you're interested
    in enterprise-strength multitenancy right now, Red Hat's **Openshift Origin**
    (**OO**) would be a good place to learn.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还有一个额外的隔离构件，用于集群级别的隔离。在大多数情况下，你可以运行Kubernetes而无需担心命名空间；如果没有指定，所有内容都会运行在默认命名空间中。不过，在运行多租户社区或想要进行大规模集群资源隔离和分割的情况下，可以使用命名空间来实现这一目标。虽然Kubernetes的端到端多租户功能尚未完全完善，但通过RBAC、容器权限、Ingress规则和清晰的网络管理，可以接近这一目标。如果你现在对企业级多租户感兴趣，Red
    Hat的**Openshift Origin**（**OO**）是学习的好地方。
- en: You can check out OO at [https://github.com/openshift/origin](https://github.com/openshift/origin).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/openshift/origin](https://github.com/openshift/origin)上查看OO。
- en: To start, Kubernetes has two namespaces—`default` and `kube-system`. The `kube-system` namespace
    is used for all the system-level containers we saw in Chapter 1, *Introduction
    to* *Kubernetes*, in the *Services running on the minions* section. UI, logging,
    DNS, and so on are all run in `kube-system`. Everything else the user creates
    runs in the default namespace. However, our resource definition files can optionally
    specify a custom namespace. For the sake of experimenting, let's take a look at
    how to build a new namespace.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 开始时，Kubernetes有两个命名空间——`default`和`kube-system`。`kube-system`命名空间用于我们在第1章《Kubernetes简介》中的*Minion上运行的服务*部分看到的所有系统级容器。UI、日志、DNS等都运行在`kube-system`中。用户创建的其他所有内容都运行在默认命名空间中。不过，我们的资源定义文件可以选择性地指定自定义命名空间。为了实验，我们来看一下如何构建一个新的命名空间。
- en: 'First, we''ll need to create a namespace definition file `test-ns.yaml` like
    the one in the following lines of code:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个命名空间定义文件`test-ns.yaml`，就像下面的代码行一样：
- en: '[PRE28]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can go ahead and create this file with our handy `create` command:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续使用我们方便的`create`命令来创建这个文件：
- en: '[PRE29]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, we can create resources that use the `test` namespace. The following listing, `ns-pod.yaml`, is
    an example of a pod using this new namespace:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建使用`test`命名空间的资源。以下列表，`ns-pod.yaml`，是使用这个新命名空间的一个Pod示例：
- en: '[PRE30]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: While the pod can still access services in other namespaces, it will need to
    use the long DNS form of `<service-name>.<namespace-name>.cluster.local`. For
    example, if you were to run a command from inside the container in listing `ns-pod.yaml`,
    you could use `node-js.default.cluster.local` to access the Node.js example from
    Chapter 2, *Pods, Services, Replication Controllers, and Labels*.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Pod仍然可以访问其他命名空间中的服务，但它需要使用`<service-name>.<namespace-name>.cluster.local`的长DNS格式。例如，如果你在`ns-pod.yaml`中运行命令，可以使用`node-js.default.cluster.local`来访问第2章《Pods、Services、Replication
    Controllers和Labels》中的Node.js示例。
- en: 'Here is a note about resource utilization. At some point in this book, you
    may run out of space on your cluster to create new Kubernetes resources. The timing
    will vary based on cluster size, but it''s good to keep this in mind and do some
    cleanup from time to time. Use the following commands to remove old examples:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个关于资源利用的提示。在本书的某个地方，你可能会遇到集群空间不足以创建新的Kubernetes资源的情况。具体时间取决于集群的大小，但最好记住这一点，并不时进行清理。使用以下命令删除旧的示例：
- en: '` $ kubectl delete pod <pod name> $ kubectl delete svc <service name> $ kubectl
    delete rc <replication controller name> $ kubectl delete rs <replicaset name>`.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '` $ kubectl delete pod <pod name> $ kubectl delete svc <service name> $ kubectl
    delete rc <replication controller name> $ kubectl delete rs <replicaset name>`。'
- en: Limits
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制
- en: 'Let''s inspect our new namespace a bit more. Run the `describe` command as
    follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再检查一下我们的新命名空间。运行`describe`命令，如下所示：
- en: '[PRE31]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前面命令的结果：
- en: '![](img/4093ce78-8d53-416d-8f83-9d6d37f823d8.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4093ce78-8d53-416d-8f83-9d6d37f823d8.png)'
- en: The describe namespace
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 描述命名空间
- en: Kubernetes allows you to both limit the resources used by individual pods or
    containers and the resources used by the overall namespace using quotas. You'll
    note that there are no resource limits or quotas currently set on the `test` namespace.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 允许你通过配额来限制单个 Pod 或容器使用的资源以及整个命名空间使用的资源。你会注意到，当前在 `test` 命名空间上没有设置资源限制或配额。
- en: 'Suppose we want to limit the footprint of this new namespace; we can set quotas
    as shown in the following listing `quota.yaml`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要限制这个新命名空间的占用资源，我们可以像下面这样设置配额 `quota.yaml`：
- en: '[PRE32]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In reality, namespaces would be for larger application communities and would
    probably never have quotas this low. I am using this for ease of illustration
    of the capability in this example.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，命名空间通常用于较大的应用程序社区，并且其配额通常不会这么低。我在此示例中使用它是为了方便展示此功能。
- en: 'Here, we will create a quota of `3` pods, `1` RC, and `1` service for the test
    namespace. As you have probably guessed, this is executed once again by our trusty
    `create` command, as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将为 `test` 命名空间创建一个 `3` 个 Pod、`1` 个 RC 和 `1` 个服务的配额。正如你可能猜到的，这一切都是通过我们可靠的
    `create` 命令执行的，如下所示：
- en: '[PRE33]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now that we have that in place, let''s use `describe` on the namespace, as
    follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经设置好了，那就用 `describe` 查看命名空间，如下所示：
- en: '[PRE34]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的结果：
- en: '![](img/4403fdc8-505c-4214-b705-245a121ad810.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4403fdc8-505c-4214-b705-245a121ad810.png)'
- en: The describe namespace after the quota is set
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 配额设置后的命名空间描述
- en: 'You''ll note that we now have some values listed in the quota section, and
    that the limits section is still blank. We also have a `Used` column, which lets
    us know how close to the limits we are at the moment. Let''s try to spin up a
    few pods using the following definition `busybox-ns.yaml`:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，我们现在在配额部分列出了一些值，而限制部分仍然是空白的。我们还有一个 `Used` 列，它让我们知道当前接近配额限制的程度。现在，尝试使用以下定义的
    `busybox-ns.yaml` 启动一些 Pod：
- en: '[PRE35]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: You'll note that we are creating four replicas of this basic pod. After using
    `create` to build this RC, run the `describe` command on the `test` namespace
    once more. You'll notice that the `Used` values for pods and RCs are at their
    max. However, we asked for four replicas and can only see three pods in use.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们正在创建这个基础 Pod 的四个副本。在用 `create` 创建这个 RC 后，再次运行 `describe` 命令查看 `test`
    命名空间。你会发现 Pod 和 RC 的 `Used` 值已达到最大。但我们请求了四个副本，而只能看到使用中的三个 Pod。
- en: 'Let''s see what''s happening with our RC. You might attempt to do that with
    the following command:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的 RC（复制控制器）发生了什么。你可以尝试用以下命令来查看：
- en: '[PRE36]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: However, if you try, you'll be discouraged by being met with a `not found` message
    from the server. This is because we created this RC in a new namespace and `kubectl`
    assumes the default namespace if not specified. This means that we need to specify
    `--namepsace=test` with every command when we wish to access resources in the
    `test` namespace.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你尝试这样做，你会看到服务器返回 `not found` 的信息。这是因为我们在一个新命名空间中创建了这个 RC，而 `kubectl` 默认假设是默认命名空间，如果没有指定的话。这意味着当我们想要访问
    `test` 命名空间中的资源时，每个命令都需要指定 `--namespace=test`。
- en: 'We can also set the current namespace by working with the context settings.
    First, we need to find our current context, which is found with the following
    command:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过修改上下文设置来设置当前命名空间。首先，我们需要找到当前的上下文，使用以下命令可以找到：
- en: '`**$ kubectl config view | grep current-context**`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`**$ kubectl config view | grep current-context**`'
- en: 'Next, we can take that context and set the namespace variable like in the following
    code:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以用以下代码设置命名空间变量：
- en: '`**$ kubectl config set-context <Current Context> --namespace=test**`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`**$ kubectl config set-context <Current Context> --namespace=test**`'
- en: Now, you can run the `kubectl` command without the need to specify the namespace.
    Just remember to switch back when you want to look at the resources running in
    your default namespace.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以运行 `kubectl` 命令而无需指定命名空间。只需记住在想查看默认命名空间中运行的资源时，切换回去即可。
- en: 'Run the command with the namespace specified as shown in the following command.
    If you''ve set your current namespace as demonstrated in the tip box, you can
    leave off the `--namespace` argument:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下命令运行时，指定命名空间。如果你按照提示框所示设置了当前命名空间，那么可以省略 `--namespace` 参数：
- en: '[PRE37]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的结果：
- en: '![](img/01840943-81ce-496e-a8ea-a2f068d573fd.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01840943-81ce-496e-a8ea-a2f068d573fd.png)'
- en: Namespace quotas
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间配额
- en: As you can see in the preceding image, the first three pods were successfully
    created, but our final one fails with a `Limited to 3 pods` error.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: This is an easy way to set limits for resources partitioned out at a community
    scale. It's worth noting that you can also set quotas for CPU, memory, persistent
    volumes, and secrets. Additionally, limits work in a similar way to quota, but
    they set the limit for each pod or container within the namespace.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: A note on resource usage
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As most of the examples in this book utilize GCP or AWS, it can be costly to
    keep everything running. It's also easy to run out of resources using the default
    cluster size, especially if you keep every example running. Therefore, you may
    want to delete older pods, replication controllers, replica sets, and services
    periodically. You can also destroy the cluster and recreate it using Chapter 1,
    *Introduction to Kubernetes*, as a way to lower your cloud provider bill.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a deeper look into networking and services in Kubernetes.
    You should now understand how networking communications are designed in K8s and
    feel comfortable accessing your services internally and externally. We saw how
    `kube-proxy` balances traffic both locally and across the cluster. Additionally,
    we explored the new Ingress resources that allow us finer control of incoming
    traffic. We also looked briefly at how DNS and service discovery is achieved in
    Kubernetes. We finished off with a quick look at namespaces and isolation for
    multitenancy.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Give two way in which the Docker networking approach is different than the Kubernetes
    networking approach.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does NAT stand for?
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the two major classes of Kubernetes networking models?
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name at least two of the third-party overlay networking options available to
    Kubernetes.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At what level (or alternatively, to what object) does Kubernetes assign IP addresses?
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the available modes for `kube-proxy`?
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the three types of services allowed by Kubernetes?
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What elements are used to define container and service ports?
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name two or more types of ingress available to Kubernetes.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you provide multitenancy for your Kubernetes cluster?
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read more about CoreDNS's entry in the CNCF: [https://coredns.io/2018/03/12/coredns-1.1.0-release/](https://coredns.io/2018/03/12/coredns-1.1.0-release/).
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More details on the current crop of Kubernetes network provider scan be found
    at [https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this](https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this).
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can compare nginx's implementation of an ingress controller ([https://github.com/nginxinc/kubernetes-ingress](https://github.com/nginxinc/kubernetes-ingress))
    and the Kubernetes community approach ([https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx))
    and their differences ([https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/nginx-ingress-controllers.md](https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/nginx-ingress-controllers.md)).
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以对比nginx实现的入口控制器([https://github.com/nginxinc/kubernetes-ingress](https://github.com/nginxinc/kubernetes-ingress))和Kubernetes社区的方法([https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx))及其差异([https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/nginx-ingress-controllers.md](https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/nginx-ingress-controllers.md))。
- en: You can read up about Google Compute Engine's layer 7 load balancer, GLBC at [https://github.com/kubernetes/ingress-gce/](https://github.com/kubernetes/ingress-gce/).
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/kubernetes/ingress-gce/](https://github.com/kubernetes/ingress-gce/)了解有关Google
    Compute Engine第7层负载均衡器GLBC的更多信息。
