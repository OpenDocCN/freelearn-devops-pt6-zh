["```\n$ aws ec2 describe-subnets --filters \"Name=tag:k8s.io/cluster-autoscaler/enabled,Values=true\" | jq -r '.Subnets[].SubnetId'\nsubnet-05d5323d274c6d67e\nsubnet-087e0a21855f08fd3\nsubnet-0dbed7d2f514d8897\n$ aws ec2 describe-subnets --filters \"Name=tag:k8s.io/cluster-autoscaler/myipv4cluster,Values=owned\" | jq -r '.Subnets[].SubnetId'\nsubnet-05d5323d274c6d67e\nsubnet-087e0a21855f08fd3\nsubnet-0dbed7d2f514d8897\n```", "```\n{\"Version\": \"2012-10-17\",\n    \"Statement\": [ {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"autoscaling:SetDesiredCapacity\",\n                \"autoscaling:TerminateInstanceInAutoScalingGroup\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"autoscaling:ResourceTag/k8s.io/cluster-autoscaler/enabled\": \"true\",\n                    \"aws:ResourceTag/k8s.io/cluster-autoscaler/myipv4cluster\": \"owned\"\n                }}},\n        { \"Effect\": \"Allow\",\n            \"Action\": [\n                \"autoscaling:DescribeAutoScalingInstances\",\n                \"autoscaling:DescribeAutoScalingGroups\",\n                \"autoscaling:DescribeScalingActivities\",\n                \"ec2:DescribeLaunchTemplateVersions\",\n                \"autoscaling:DescribeTags\",\n                \"autoscaling:DescribeLaunchConfigurations\",\n                \"ec2:DescribeInstanceTypes\"\n            ],\n            \"Resource\": \"*\"\n        }]}\n```", "```\n$ aws iam create-policy --policy-name AmazonEKSClusterAutoscalerPolicy \\\n--policy-document file://autoscaler_policy.json\n{\n    \"Policy\": {\n        \"…….}}\n$ eksctl create iamserviceaccount  --cluster=myipv4cluster --namespace=kube-system --name=cluster-autoscaler  --attach-policy-arn=arn:aws:iam::112233:policy/AmazonEKSClusterAutoscalerPolicy --override-existing-serviceaccounts --approve\n……\n2023-05-02 19:31:55 []  created serviceaccount \"kube-system/cluster-autoscaler\"\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"22+\", GitVersion:\"v1.22.6-eks-7d68063\", GitCommit:\"f24e667e49fb137336f7b064dba897beed639bad\", GitTreeState:\"clean\", BuildDate:\"2022-02-23T19:32:14Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22+\", GitVersion:\"v1.22.17-eks-ec5523e\", GitCommit:\"49675beb7b1c90389418d067d37024616a313555\", GitTreeState:\"clean\", BuildDate:\"2023-03-20T18:44:58Z\", GoVersion:\"go1.16.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n```", "```\nnode-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/myipv4cluster\n```", "```\n- --balance-similar-node-groups\n- --skip-nodes-with-system-pods=false\n```", "```\nimage: registry.k8s.io/autoscaling/cluster-autoscaler:v1.22.2\n```", "```\n$ kubectl create -f cluster-autoscaler-autodiscover.yaml\n….\ndeployment.apps/cluster-autoscaler created\nError from server (AlreadyExists): error when creating \"cluster-autoscaler-autodiscover.yaml\": serviceaccounts \"cluster-autoscaler\" already exists\n$ kubectl patch deployment cluster-autoscaler -n kube-system -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"false\"}}}}}'\n$ kubectl get po -n kube-system\nNAME                  READY   STATUS    RESTARTS   AGE\naws-node-2wrq6         1/1     Running   0          3d7h\naws-node-blfl2         1/1     Running   0          3d7h\ncluster-autoscaler-577 1/1     Running   0          12s\n……\n```", "```\n$ kubectl get node\nNAME            STATUS   ROLES    AGE     VERSION\nip-192-168-18-136.eu-central-1.compute.internal   Ready    <none>   3d8h    v1.22.17-eks-a59e1f0\nip-192-168-43-219.eu-central-1.compute.internal   Ready    <none>   3d8h    v1.22.17-eks-a59e1f0\n```", "```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 150\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n```", "```\n$ kubectl create -f tester.yaml\n$ kubectl get po\nNAME       READY   STATUS    RESTARTS   AGE\nnginx-deployment-12   0/1     Pending   0          18m\nnginx-deployment-13   0/1     Pending   0          18m\n….\nWAIT 10 MINUTES\n$ kubectl get node\nNAME          STATUS   ROLES    AGE    VERSION\nip-192-168-18-136.eu-central-1.compute.internal   Ready    <none>   3d8h   v1.22.17-eks-a59e1f0\nip-192-168-34-4.eu-central-1.compute.internal     Ready    <none>   18m    v1.22.17-eks-a59e1f0\nip-192-168-43-219.eu-central-1.compute.internal   Ready    <none>   3d8h   v1.22.17-eks-a59e1f0\nip-192-168-77-75.eu-central-1.compute.internal    Ready    <none>   18m    v1.22.17-eks-a59e1f0\nip-192-168-85-131.eu-central-1.compute.internal   Ready    <none>   18m    v1.22.17-eks-a59e1f0\n```", "```\n$ kubectl delete -f tester.yaml\ndeployment.apps \"nginx-deployment\" deleted\nWAIT AT LEAST 10 MINUTES\n$ kubectl get node\nNAME             STATUS   ROLES    AGE    VERSION\nip-192-168-34-4.eu-central-1.compute.internal    Ready    <none>   47m   v1.22.17-eks-a59e1f0\nip-192-168-77-75.eu-central-1.compute.internal   Ready    <none>   47m   v1.22.17-eks-a59e1f0\n```", "```\n$ kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler\nI0502 21:22:59.599092       1 scale_down.go:829] ip-192-168-18-136.eu-central-1.compute.internal was unneeded for 7m2.220392583s\n```", "```\n    $ export CLUSTER_NAME=myipv4cluster\n     export AWS_PARTITION=\"aws\"\n     export AWS_REGION=\"$(aws configure list | grep region | tr -s \" \" | cut -d\" \" -f3)\"\n     export OIDC_ENDPOINT=\"$(aws eks describe-cluster --name ${CLUSTER_NAME} --query \"cluster.identity.oidc.issuer\" --output text)\"\n     export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)\n    $ printenv\n    …..\n    AWS_PARTITION=aws\n    CLUSTER_NAME=myipv4cluster\n    AWS_REGION=eu-central-1\n    AWS_ACCOUNT_ID=111222333\n    OIDC_ENDPOINT=https://oidc.eks.eu-central-1.amazonaws.com/id/123455\n    ```", "```\n    { \"Version\": \"2012-10-17\",\n      \"Statement\": [\n        {\n          \"Sid\": \"\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\n            \"Service\": \"ec2.amazonaws.com\"\n          },\n          \"Action\": \"sts:AssumeRole\"\n        }]}\n    ```", "```\n    $ aws iam create-role --role-name KarpenterInstanceNodeRole     --assume-role-policy-document file://\"trust_policy.json\"\n    ```", "```\n    $ aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy --role-name KarpenterInstanceNodeRole\n    $ aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy --role-name KarpenterInstanceNodeRole\n    $ aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly --role-name KarpenterInstanceNodeRole\n    $ aws iam attach-role-policy --policy-arn  arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore --role-name KarpenterInstanceNodeRole\n    ```", "```\n    $ aws iam create-instance-profile --instance-profile-name \"KarpenterNodeInstanceProfile-${CLUSTER_NAME}\"\n    {\n        \"InstanceProfile\": {\n            \"Path\": \"/\",\n            \"InstanceProfileName\": \"KarpenterNodeInstanceProfile-myipv4cluster\",\n            \"InstanceProfileId\": \"AIPARDV7UN62ZBGEB7AV4\",\n            \"Arn\": \"arn:aws:iam::111222333:instance-profile/KarpenterNodeInstanceProfile-myipv4cluster\",\n            \"CreateDate\": \"2023-05-03T06:55:12+00:00\",\n            \"Roles\": []\n        }\n    }\n    ```", "```\n    $ aws iam add-role-to-instance-profile --instance-profile-name \"KarpenterNodeInstanceProfile-${CLUSTER_NAME}\" --role-name \"KarpenterInstanceNodeRole\"\n    ```", "```\n    $ cat << EOF > controller-trust-policy.json\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Principal\": {\n                    \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_ENDPOINT#*//}\"\n                },\n                \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n                \"Condition\": {\n                    \"StringEquals\": {\n                        \"${OIDC_ENDPOINT#*//}:aud\": \"sts.amazonaws.com\",\n                        \"${OIDC_ENDPOINT#*//}:sub\": \"system:serviceaccount:karpenter:karpenter\"\n                    }\n                }\n            }\n        ]\n    }\n    EOF\n    ```", "```\n    $ aws iam create-role --role-name KarpenterControllerRole-${CLUSTER_NAME} --assume-role-policy-document file://controller-trust-policy.json\n    {\"Role\": {\n            \"Path\": \"/\",\n            \"RoleName\": \"KarpenterControllerRole-myipv4cluster\",\n    …..\n    ```", "```\n    cat << EOF > controller-policy.json\n    {\n        \"Statement\": [\n            {\n                \"Action\": [\n                    \"ssm:GetParameter\",\n                    \"ec2:DescribeImages\",\n                    \"ec2:RunInstances\",\n                    \"ec2:DescribeSubnets\",\n                    \"ec2:DescribeSecurityGroups\",\n                    \"ec2:DescribeLaunchTemplates\",\n                    \"ec2:DescribeInstances\",\n                    \"ec2:DescribeInstanceTypes\",\n                    \"ec2:DescribeInstanceTypeOfferings\",\n                    \"ec2:DescribeAvailabilityZones\",\n                    \"ec2:DeleteLaunchTemplate\",\n                    \"ec2:CreateTags\",\n                    \"ec2:CreateLaunchTemplate\",\n                    \"ec2:CreateFleet\",\n                    \"ec2:DescribeSpotPriceHistory\",\n                    \"pricing:GetProducts\"\n                ],\n                \"Effect\": \"Allow\",\n                \"Resource\": \"*\",\n                \"Sid\": \"Karpenter\"\n            },\n            {\n                \"Action\": \"ec2:TerminateInstances\",\n                \"Condition\": {\n                    \"StringLike\": {\n                        \"ec2:ResourceTag/karpenter.sh/provisioner-name\": \"*\"\n                    }\n                },\n                \"Effect\": \"Allow\",\n                \"Resource\": \"*\",\n                \"Sid\": \"ConditionalEC2Termination\"\n            },\n            {\n                \"Effect\": \"Allow\",\n                \"Action\": \"iam:PassRole\",\n                \"Resource\": \"arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:role/KarpenterInstanceNodeRole\",\n                \"Sid\": \"PassNodeIAMRole\"\n            },\n            {\n                \"Effect\": \"Allow\",\n                \"Action\": \"eks:DescribeCluster\",\n                \"Resource\": \"arn:${AWS_PARTITION}:eks:${AWS_REGION}:${AWS_ACCOUNT_ID}:cluster/${CLUSTER_NAME}\",\n                \"Sid\": \"EKSClusterEndpointLookup\"\n            }\n        ],\n        \"Version\": \"2012-10-17\"\n    }\n    EOF\n    ```", "```\n    $ aws iam put-role-policy --role-name KarpenterControllerRole-${CLUSTER_NAME} --policy-name KarpenterControllerPolicy-${CLUSTER_NAME} --policy-document file://controller-policy.json\n    ```", "```\n    $ aws ec2 describe-subnets --filters \"Name=tag:karpenter.sh\n    /discovery,Values=myipv4cluster\" | jq -r '.Subnets[].SubnetId'\n    subnet-05d5323d274c6d67e\n    subnet-087e0a21855f08fd3\n    subnet-0dbed7d2f514d8897\n    ```", "```\n$ aws ec2 create-tags --tags \"Key=karpenter.sh/discovery,Value=${CLUSTER_NAME}\" --resources sg-0222247264816d807\n$ aws ec2 describe-security-groups --filters \"Name=tag:karpenter.sh/discovery,Values=myipv4cluster\" | jq -r '.SecurityGroups[].GroupId'\nsg-0222247264816d807\n```", "```\n    mapRoles: |\n        - groups:\n          - system:bootstrappers\n          - system:nodes\n          rolearn: arn:aws:iam::123:role/eksctl-myipv4cluster-node\n          username: system:node:{{EC2PrivateDNSName}}\n        - groups:\n          - system:bootstrappers\n          - system:nodes\n          rolearn: arn:aws:iam::123:role/KarpenterInstanceNodeRole\n          username: system:node:{{EC2PrivateDNSName}}\n    ```", "```\n$ export KARPENTER_VERSION=v0.27.3\n$ helm template karpenter oci://public.ecr.aws/karpenter/karpenter --version ${KARPENTER_VERSION} --namespace karpenter     --set settings.aws.defaultInstanceProfile=KarpenterInstanceNodeRole  --set settings.aws.clusterName=${CLUSTER_NAME}     --set serviceAccount.annotations.\"eks\\.amazonaws\\.com/role-arn\"=\"arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:role/KarpenterControllerRole-${CLUSTER_NAME}\"     --set controller.resources.requests.cpu=1     --set controller.resources.requests.memory=1Gi     --set controller.resources.limits.cpu=1     --set controller.resources.limits.memory=1Gi > karpenter.yaml\n```", "```\n- matchExpressions:\n              - key: eks.amazonaws.com/nodegroup\n                operator: In\n                values:\n                - ipv4mng\n```", "```\n$ kubectl create namespace karpenter\nnamespace/karpenter created\n$ kubectl create -f https://raw.githubusercontent.com/aws/karpenter/${KARPENTER_VERSION}/pkg/apis/crds/karpenter.sh_provisioners.yaml\ncustomresourcedefinition.apiextensions.k8s.io/provisioners.karpenter.sh created\n$ kubectl create -f https://raw.githubusercontent.com/aws/karpenter/${KARPENTER_VERSION}/pkg/apis/crds/karpenter.k8s.aws_awsnodetemplates.yaml\ncustomresourcedefinition.apiextensions.k8s.io/awsnodetemplates.karpenter.k8s.aws created\n```", "```\n$ kubectl apply -f karpenter.yaml\npoddisruptionbudget.policy/karpenter created\nserviceaccount/karpenter created\nsecret/karpenter-cert created\nconfigmap/config-logging created\n……\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/validation.webhook.karpenter.k8s.aws created\n$ kubectl get all  -n karpenter\nNAME        READY   STATUS    RESTARTS   AGE\npod/karpenter-fcd8f5df6-l2h6k   1/1     Running   0   43s\npod/karpenter-fcd8f5df6-r2vzw   1/1     Running       43s\nNAME   TYPE   CLUSTER-IP  ..\nservice/karpenter   ClusterIP   10.100.76.202   ..\nNAME   READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/karpenter   2/2     2         2           43s\nNAME          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/karpenter-fcd8f5df6   2  2    2       43s\n```", "```\napiVersion: karpenter.sh/v1alpha5\nkind: Provisioner\nmetadata:\n  name: default\nspec:\n  labels:\n    type: karpenter\n  requirements:\n    - key: karpenter.sh/capacity-type\n      operator: In\n      values: [\"on-demand\"]\n    - key: \"node.kubernetes.io/instance-type\"\n      operator: In\n      values: [\"c5.large\", \"m5.large\", \"m5.xlarge\"]\n  limits:\n    resources:\n      cpu: 1000\n      memory: 1000Gi\n  providerRef:\n    name: default\n  ttlSecondsAfterEmpty: 30\n---\napiVersion: karpenter.k8s.aws/v1alpha1\nkind: AWSNodeTemplate\nmetadata:\n  name: default\nspec:\n  subnetSelector:\n    karpenter.sh/discovery: myipv4cluster\n  securityGroupSelector:\n    karpenter.sh/discovery: myipv4cluster\n```", "```\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: other\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inflate\n  namespace: other\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      app: inflate\n  template:\n    metadata:\n      labels:\n        app: inflate\n    spec:\n      nodeSelector:\n        type: karpenter\n      terminationGracePeriodSeconds: 0\n      containers:\n        - name: inflate\n          image: public.ecr.aws/eks-distro/kubernetes/pause:3.2\n          resources:\n            requests:\n              memory: 1Gi\n```", "```\n$ kubectl create -f deployment.yaml\nnamespace/other created\ndeployment.apps/inflate created\n$ kubectl get all -n other\nNAME       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/inflate   0/0     0       0       11s\nNAME        DESIRED   CURRENT   READY   AGE\nreplicaset.apps/inflate-11   0         0         0       11s\n$ kubectl get node -l type=karpenter\nNo resources found\n```", "```\n$ kubectl scale -n other deployment/inflate --replicas 5\ndeployment.apps/inflate scaled\n$ kubectl get po -n other\nNAME                       READY   STATUS    RESTARTS   AGE\ninflate-6cc55bfc74-4rfts   0/1     Pending   0          10s\ninflate-6cc55bfc74-9kl5n   0/1     Pending   0          10s\ninflate-6cc55bfc74-nspbf   0/1     Pending   0          10s\ninflate-6cc55bfc74-rq4cn   0/1     Pending   0          10s\ninflate-6cc55bfc74-swtmv   0/1     Pending   0          10s\n$ kubectl get node -l type=karpenter\nNAME     STATUS   ROLES    AGE   VERSION\nip-192-168-55-119.eu-central-1.compute.internal   Ready    <none>   99s   v1.22.17-eks-a59e1f0\n$ kubectl get po -n other\nNAME                       READY   STATUS    RESTARTS   AGE\ninflate-6cc55bfc74-4rfts   1/1     Running   0          2m36s\ninflate-6cc55bfc74-9kl5n   1/1     Running   0          2m36s\ninflate-6cc55bfc74-nspbf   1/1     Running   0          2m36s\ninflate-6cc55bfc74-rq4cn   1/1     Running   0          2m36s\ninflate-6cc55bfc74-swtmv   1/1     Running   0          2m36s\n```", "```\n$ kubectl delete -f deployment.yaml\nnamespace \"other\" deleted\ndeployment.apps \"inflate\" deleted\n$ kubectl get node -l type=karpenter\nNAME           STATUS   ROLES    AGE     VERSION\nip-192-168-55-119.eu-central-1.compute.internal   Ready    <none>   5m57s   v1.22.17-eks-a59e1f0\n$ kubectl get node -l type=karpenter\nNo resources found\n```", "```\n$ kubectl -n kube-system get deployment/metrics-server\nNAME             READY   UP-TO-DATE   AVAILABLE   AGE\nmetrics-server   1/1     1            1           34h\n$ kubectl top pod -n kube-system\nNAME                              CPU(cores)   MEMORY(bytes)\naws-node-2tx2g                    3m           38Mi\naws-node-9rhjs                    3m           38Mi\ncoredns-7b6fd76bcb-6h9b6          1m           12Mi\ncoredns-7b6fd76bcb-vsvv8          1m           12Mi\nkube-proxy-rn96m                  1m           11Mi\nkube-proxy-tf8d9                  1m           10Mi\nmetrics-server-68c56b9d8c-24mng   3m           17Mi\n```", "```\n$ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n```", "```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: php-apache\nspec:\n  selector:\n    matchLabels:\n      run: php-apache\n  template:\n    metadata:\n      labels:\n        run: php-apache\n    spec:\n      containers:\n      - name: php-apache\n        image: registry.k8s.io/hpa-example\n        ports:\n        - containerPort: 80\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 200m\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: php-apache\n  labels:\n    run: php-apache\nspec:\n  ports:\n  - port: 80\n  selector:\n    run: php-apache\n```", "```\n$ kubectl create -f hpa-deployment.yaml\ndeployment.apps/php-apache created\nservice/php-apache created\n$ kubectl get all\nNAME      READY   STATUS    RESTARTS   AGE\npod/php-apache-11-22   1/1     Running   0          57s\nNAME     TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE\nservice/php-apache   ClusterIP   10.1.2.1 <none>  80/TCP   57s\nNAME       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/php-apache   1/1     1            1    58s\nNAME           DESIRED   CURRENT   READY   AGE\nreplicaset.apps/php-apache-1122   1         1         1   58s\n$ kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\nhorizontalpodautoscaler.autoscaling/php-apache autoscaled\n$ kubectl get hpa\nNAME    REFERENCE  TARGETS   MINPODS   MAXPODS REPLICAS   AGE\nphp-apache Deployment/php-apache   0%/50% 1  10   1       19s\n```", "```\n$ kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c \"while sleep 0.01; do wget -q -O- http://php-apache; done\"\nIf you don't see a command prompt, try pressing enter.\nOK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!.\n```", "```\n$ kubectl get hpa php-apache --watch\nNAME  REFERENCE   TARGETS  MINPODS   MAXPODS   REPLICAS   AGE\nphp-apache Deployment/php-apache   119%/50%   1   10  1   10m\nphp-apache Deployment/php-apache   119%/50%   1   10  3   10m\nphp-apache Deployment/php-apache   188%/50%   1   10  3   10m\nphp-apache Deployment/php-apache   88%/50%    1   10  4   11m\nphp-apache Deployment/php-apache   77%/50%    1   10  4   11m\nphp-apache Deployment/php-apache   48%/50%    1   10  5   11m\n```", "```\nNAME  REFERENCE  TARGETS    MINPODS   MAXPODS   REPLICAS   AGE\nphp-apache Deployment/php-apache 119%/50%   1  10  1      10m\nphp-apache Deployment/php-apache 119%/50%   1  10  3      10m\n………………………………………\nphp-apache Deployment/php-apache 0%/50%  1    10  5     16m\nphp-apache Deployment/php-apache 0%/50%  1    10  1     16m\n```", "```\n$ kubectl create namespace prometheus\nnamespace/prometheus created\n$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n\"prometheus-community\" has been added to your repositories\n$ helm repo update\nHang tight while we grab the latest from your chart repositories…\n….....\n…Successfully got an update from the \"prometheus-community\" chart repository\n…Successfully got an update from the \"stable\" chart repository\nUpdate Complete. Happy Helming\n```", "```\n$ helm upgrade -i prometheus prometheus-community/prometheus     --namespace prometheus     --set alertmanager.persistentVolume.storageClass=\"gp2\",server.persistentVolume.storageClass=\"gp2\"\nRelease \"prometheus\" does not exist. Installing it now.\nNAME: prometheus\nLAST DEPLOYED: Sun May  7 13:16:28 2023\nNAMESPACE: prometheus\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n……….\nFor more information on running Prometheus, visit:\nhttps://prometheus.io/\n$ kubectl get po -n prometheus\nNAME          READY   STATUS    RESTARTS   AGE\nprometheus-alertmanager-0  1/1     Running   0       19m\nprometheus-kube-state-metrics-12  1/1  Running   0 19m\nprometheus-prometheus-node-exporter-12 1/1 Running   0    19m\nprometheus-prometheus-node-exporter-12  1/1 Running   0   19m\nprometheus-prometheus-pushgateway-13   1/1  Running  0    19m\nprometheus-server-677fbf6f-14       2/2     Running   0   19m\n```", "```\n$ kubectl --namespace=prometheus port-forward deploy/prometheus-server 8080:9090\nForwarding from 127.0.0.1:8080 -> 9090\nForwarding from [::1]:8080 -> 9090\n```", "```\n$ kubectl version help\n…\nServer Version: version.Info{Major:\"1\", Minor:\"23+\", GitVersion:\"v1.23.17-eks-0a21954\", GitCommit:\"cd5c12c51b0899612375453f7a7c2e7b6563f5e9\", GitTreeState:\"clean\", BuildDate:\"2023-04-15T00:32:27Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n$ kubectl create ns podinfo\nnamespace/podinfo created\n$ kubectl apply -k github.com/stefanprodan/podinfo/kustomize -n podinfo\nservice/podinfo created\ndeployment.apps/podinfo created\nhorizontalpodautoscaler.autoscaling/podinfo created\n$ kubectl get hpa -n podinfo\nNAME  REFERENCE  TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\npodinfo   Deployment/podinfo   2%/99%    2   4     2  13m\n$ kubectl get po -n podinfo\nNAME                       READY   STATUS    RESTARTS   AGE\npodinfo-78989955bc-12   1/1     Running   0          35m\npodinfo-78989955bc-13   1/1     Running   0          13m\n```", "```\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"9797\"\n```", "```\n$ helm install prometheus-adapter prometheus-community/prometheus-adapter  --set prometheus.url=\"http://prometheus-server.prometheus.svc\",prometheus.port=\"80\" --set image.tag=\"v0.10.0\" --set rbac.create=\"true\" --namespace prometheusNAME: prometheus-adapter\n…..\n$ kubectl get po -n prometheus\nNAME        READY   STATUS    RESTARTS   AGE\nprometheus-adapter-123-chsmv  1/1     Running   0    92s\n….\n$ kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1 | jq .\n{\n  \"kind\": \"APIResourceList\",\n  \"apiVersion\": \"v1\",\n  \"groupVersion\": \"custom.metrics.k8s.io/v1beta1\",\n  \"resources\": [\n    {\n      \"name\": \"nodes/node_vmstat_pgpgin\",\n…..\n```", "```\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-adapter\n  namespace: prometheus\ndata:\n  config.yaml: |\n    rules:\n    - seriesQuery: 'http_requests_total'\n      resources:\n        overrides:\n          namespace:\n            resource: \"namespace\"\n          pod:\n            resource: \"pod\"\n      name:\n        matches: \"^(.*)_total\"\n        as: \"${1}_per_second\"\n      metricsQuery: 'rate(http_requests_total{namespace=\"podinfo\",app=\"podinfo\"}[2m])'\n```", "```\n$ kubectl replace cm prometheus-adapter -n prometheus -f cm-adapter.yaml\nconfigmap/prometheus-adapter replaced\n$ kubectl rollout restart deployment prometheus-adapter -n prometheus\ndeployment.apps/prometheus-adapter restarted\n$ kubectl get po -n prometheus\nNAME     READY   STATUS    RESTARTS   AGE\nprometheus-adapter-123-h7ztg  1/1     Running   0    60s\n……\n$ kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1 | jq .\n{\n  \"kind\": \"APIResourceList\",\n  \"apiVersion\": \"v1\",\n  \"groupVersion\": \"custom.metrics.k8s.io/v1beta1\",\n  \"resources\": [\n    {\n      \"name\": \"pods/http_requests_per_second\",\n      \"singularName\": \"\",\n      \"namespaced\": true,\n      \"kind\": \"MetricValueList\",\n      \"verbs\": [\n        \"get\"\n      ]\n    },\n    {\n      \"name\": \"namespaces/http_requests_per_second\",\n      \"singularName\": \"\",\n      \"namespaced\": false,\n      \"kind\": \"MetricValueList\",\n      \"verbs\": [\n        \"get\"\n      ]}]}\n$ kubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/podinfo/pods/*/http_requests_per_second\" | jq .\n{\n  \"kind\": \"MetricValueList\",\n  \"apiVersion\": \"custom.metrics.k8s.io/v1beta1\",\n  \"metadata\": {},\n  \"items\": [\n    {\n      \"describedObject\": {\n        \"kind\": \"Pod\",\n        \"namespace\": \"podinfo\",\n        \"name\": \"podinfo-78989955bc-89n8m\",\n        \"apiVersion\": \"/v1\"\n      },\n      \"metricName\": \"http_requests_per_second\",\n      \"timestamp\": \"2023-05-08T22:42:11Z\",\n      \"value\": \"200m\",\n      \"selector\": null\n    },\n    {\n      \"describedObject\": {\n        \"kind\": \"Pod\",\n        \"namespace\": \"podinfo\",\n        \"name\": \"podinfo-78989955bc-rnr9c\",\n        \"apiVersion\": \"/v1\"\n      },\n      \"metricName\": \"http_requests_per_second\",\n      \"timestamp\": \"2023-05-08T22:42:11Z\",\n      \"value\": \"200m\",\n      \"selector\": null\n    }]}\n```", "```\n---\napiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: podinfo\n  namespace: podinfo\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: podinfo\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n    - type: Pods\n      pods:\n        metricName: http_requests_per_second\n        targetAverageValue: 10\n```", "```\n$ kubectl replace -f hpa-requests.yaml\nWarning: autoscaling/v2beta1 HorizontalPodAutoscaler is deprecated in v1.22+, unavailable in v1.25+; use autoscaling/v2 HorizontalPodAutoscaler\nhorizontalpodautoscaler.autoscaling/podinfo replaced\n$ kubectl describe hpa podinfo -n podinfo\nWarning: autoscaling/v2beta2 HorizontalPodAutoscaler is deprecated in v1.23+, unavailable in v1.26+; use autoscaling/v2 HorizontalPodAutoscaler\nName:                                  podinfo\nNamespace:                             podinfo\nLabels:                                <none>\nAnnotations:                           <none>\nCreationTimestamp:                     Sun, 07 May 2023 15:30:19 +0000\nReference:                             Deployment/podinfo\nMetrics:                               ( current / target )\n  \"http_requests_per_second\" on pods:  200m / 10\nMin replicas:                          1\nMax replicas:                          10\nDeployment pods:                       1 current / 1 desired\n…….\n```", "```\n$ kubectl run -it load-test --rm --image=nginx -n prometheus – bash\nroot@load-test:/# curl http://podinfo.podinfo.svc.cluster.local:9898\n{\n\"hostname\": \"podinfo-78989955bc-zfwdj\",\n  \"version\": \"6.3.6\",\n  \"revision\": \"073f1ec5aff930bd3411d33534e91cbe23302324\",\n  \"color\": \"#34577c\",\n  \"logo\": \"https://raw.githubusercontent.com/stefanprodan/podinfo/gh-pages/cuddle_clap.gif\",\n  \"message\": \"greetings from podinfo v6.3.6\",\n  \"goos\": \"linux\",\n  \"goarch\": \"amd64\",\n  \"runtime\": \"go1.20.4\",\n  \"num_goroutine\": \"9\",\n  \"num_cpu\": \"2\"\n}\nroot@load-test:/# while sleep 0.01; do curl http://podinfo.podinfo.svc.cluster.local:9898; done\n……..\n```", "```\n$ kubectl get hpa -n podinfo --watch\nNAME   REFERENCE TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\npodinfo Deployment/podinfo   200m/10       1 10  1   31h\npodinfo   Deployment/podinfo   216m/10     1 10  1   31h\npodinfo   Deployment/podinfo   19200m/10   1 10  1   31h\npodinfo   Deployment/podinfo   19200m/10   1 10  2   31h\npodinfo   Deployment/podinfo   20483m/10   1 10  2   31h\npodinfo   Deployment/podinfo   14868m/10   1 10  2   31h\npodinfo   Deployment/podinfo   15744m/10   1 10  3   31h\n```", "```\n$ helm delete prometheus-adapter  --namespace prometheus\nrelease \"prometheus-adapter\" uninstalled\n```", "```\nhelm repo add kedacore https://kedacore.github.io/charts\nhelm repo update\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda –version 2.9.4\n$ kubectl get po -n keda\nNAME                READY   STATUS    RESTARTS   AGE\nkeda-operator-123-5cv    1/1     Running   0          7m49s\nkeda-operator-metrics-apiserver-123   1/1     Running     7m49s\n```", "```\n---\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: prometheus-scaledobject\n  namespace: podinfo\nspec:\n  scaleTargetRef:\n    name: podinfo\n  minReplicaCount:  2\n  maxReplicaCount:  10\n  pollingInterval: 10\n  cooldownPeriod:  30\n  fallback:\n    failureThreshold: 3\n    replicas: 2\n  triggers:\n  - type: prometheus\n    metadata:\n      serverAddress: http://prometheus-server.prometheus.svc.cluster.local:80\n      metricName: http_requests_total\n      threshold: '10'\n      query: sum(rate(http_requests_total{namespace=\"podinfo\",app=\"podinfo\"}[2m]))\n```", "```\n$ kubectl create -f scaledobject.yaml\nscaledobject.keda.sh/prometheus-scaledobject created\n$ kubectl get ScaledObject -n podinfo\nAME                      SCALETARGETKIND      SCALETARGETNAME    MIN   MAX   TRIGGERS     AUTHENTICATION   READY   ACTIVE   FALLBACK   AGE\nprometheus-scaledobject   apps/v1.Deployment   podinfo           2     10    prometheus                    True    True     True       4m25s                    False   False    False\n$ kubectl get hpa -n podinfo\nNAME    REFERENCE  TARGETS  MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-prometheus-scaledobject   Deployment/podinfo   200m/10 (avg)   2         10        2          4m42s\npodinfo                            Deployment/podinfo   2%/99%  2         4         2          84m\n```", "```\n$ kubectl run -it load-test --rm --image=nginx -n prometheus -- bash\nroot@load-test:/# while sleep 0.01; do curl http://podinfo.podinfo.svc.cluster.local:9898; done\n……..\n```", "```\n$ kubectl get hpa keda-hpa-prometheus-scaledobject -n podinfo --watch\nNAME   REFERENCE  TARGETS  MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-prometheus-scaledobject   Deployment/podinfo   16742m/10 (avg)   2         10        2          16m\nkeda-hpa-prometheus-scaledobject   Deployment/podinfo   8371m/10 (avg)   2         10        4          16m\nkeda-hpa-prometheus-scaledobject   Deployment/podinfo   16742m/10 (avg)   2         10        2          16m\nkeda-hpa-prometheus-scaledobject   Deployment/podinfo   8371m/10 (avg)    2         10        4          16m\nkeda-hpa-prometheus-scaledobject   Deployment/podinfo   4961m/10 (avg)    2         10        3          17m\n```"]