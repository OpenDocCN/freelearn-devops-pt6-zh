- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing and Controlling Kubernetes Clusters with Tanzu Mission Control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section of the book, we covered the tools in the Tanzu portfolio
    that help us run cloud-native applications. We covered how Harbor can provide
    a secure home for your container images and how we can run those images using
    Tanzu Kubernetes Grid, which provides a uniform user experience across public
    and private cloud infrastructure. Finally, we took a deep dive into the area of
    developer productivity to automate and secure the software supply chain from an
    idea to a running application in production.
  prefs: []
  type: TYPE_NORMAL
- en: This section is about managing cloud-native apps and their corresponding Kubernetes
    infrastructure. To begin this section, in this chapter, we will learn about managing,
    securing, and governing a fleet of Kubernetes clusters of any flavor and on any
    infrastructure using **Tanzu Mission Control** (**TMC**). Followed by that, we
    will cover VMware Aria Operations for Applications, a tool to monitor every part
    of your running applications, including distributed tracing between microservices,
    application performance, Kubernetes objects, virtual infrastructure, and other
    services used by the applications. Finally, we will learn how to connect your
    apps running on Kubernetes and deploy them in different clusters and environments
    securely with out-of-the-box mutual TLS configuration using Tanzu Service Mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Sidenote
  prefs: []
  type: TYPE_NORMAL
- en: Henceforth in this chapter, we will refer to a *Kubernetes cluster* just as
    a *cluster* for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the background from the last section and our forward-looking statement
    for the upcoming chapters in this section, let’s begin our journey of understanding
    TMC in depth. We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Why TMC?* – Understand the challenges around managing large Kubernetes environments
    and the solutions offered by Tanzu Mission Control'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Getting started with TMC* – Learn how to start using TMC to manage the cluster
    lifecycle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Protecting cluster data using TMC* – Learn how to back up clusters and namespaces,
    and how to restore them when required to protect running workloads from disasters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Applying and ensuring governance policies on clusters using TMC* – Learn how
    to apply different cluster user governance policies to cluster and Kubernetes
    namespace groups and run inspections to find anomalies in clusters as a preventative
    security measure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TMC is a very powerful SaaS offering under the Tanzu portfolio that provides
    a single pane of control for all your Kubernetes environments. Let’s learn more
    about it.
  prefs: []
  type: TYPE_NORMAL
- en: Why TMC?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With its increasing popularity, Kubernetes is the new infrastructure layer.
    Just as 10 years back, almost every software used to run on the virtual infrastructure,
    in the next few years, almost every new application will probably be deployed
    on Kubernetes by default. In fact, most of the vendor-provided solutions are now
    available to run on Kubernetes. As per a survey done by the **Cloud Native Computing
    Foundation** (**CNCF**) in December 2021, over 5.6 million developers said they
    used Kubernetes to deploy their applications, which is a 67% increase in just
    1 year! Because of the array of business and technical benefits provided by Kubernetes,
    it is here to stay for a long time. Based on this CNCF survey, over 96% of organizations
    have embraced Kubernetes with a different level of maturity to run their cloud-native
    applications! And 73% of them have workloads running in production on Kubernetes
    already! As per a blog post by the CNCF in February 2022, the community is seeing
    its highest-ever adoption of this technology. Learn more about the details published
    by the CNCF here: https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/.'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though Kubernetes is an extremely popular tool to run containerized applications,
    operating Kubernetes is difficult in a production-grade environment. As you may
    know about Kubernetes, it only talks via the `kubectl` command-line interface
    or using its REST APIs. Every configuration in Kubernetes is in YAML, which is
    often long and difficult to understand unless you have a good grasp of different
    Kubernetes constructs.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with very large clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A large cluster, in this context, is a cluster with more than 50 nodes in it.
    Adding to the basic complexity of using Kubernetes, running a production-grade
    platform has several other security and operational challenges. To reduce complexity
    and operational overhead, I have seen enterprises deploy very large clusters to
    maintain, operate, and secure only a few critical clusters. Very large Kubernetes
    deployments host applications from several different **lines of business** (**LOBs**)
    using the logical isolation provided by Kubernetes namespaces. At first, this
    approach may sound logical, but there are several drawbacks to this approach,
    as listed in the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: A large cluster serving multiple distinct LOBs and applications is very difficult
    to maintain, as all the applications and their LOBs will have different preferences
    in terms of the maintenance window and tolerance to any downtime. If an application
    is deployed with two or more Pods in a cluster, then it will mostly not face downtime.
    However, ensuring this level of compliance is a different challenge. Additionally,
    large clusters need a large maintenance window to complete activities such as
    upgrades.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When there are hundreds of apps running in a large cluster, they all get impacted
    together if the cluster faces any issue or downtime. The large clusters have large
    blast radii and large disaster impacts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a large cluster is used by several different applications, the application
    teams do not get any freedom of choice to deploy and run their apps with a specific
    cluster setup and resource requirements such as using a specific operating system
    or using a GPU for compute needs. Although Kubernetes provides a way to deploy
    certain application Pods on certain nodes using constructs such as taints and
    tolerations, implementing, maintaining, and using this kind of setup is practically
    very difficult.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although Kubernetes isolates applications of different teams using namespaces,
    this is only a logical level of isolation. In reality, Pods belonging to different
    namespaces may run in the same node. This may create issues of potential security
    threats posed by a malicious actor running in the same cluster and possible resource
    starvation because of a “noisy neighbor” if the application has not requested
    and reserved required resources as a part of its deployment manifest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering these challenges around large clusters, it is recommended to use
    multiple smaller clusters, especially for heterogeneous workloads. If all the
    apps running on a large cluster have the same requirements, then it should be
    okay; otherwise, the recommended approach is to create smaller clusters for different
    teams, LOBs, or applications.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with many clusters and solutions from TMC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: People generally prefer large clusters to avoid the operational overhead that
    increases in proportion to the number of clusters to be maintained. Hence, on
    one side, we may need several small clusters that belong to different teams, environments,
    and purposes, and on the other side, there are several operational and security
    challenges involved in keeping them functional. That is where TMC comes into the
    picture, which addresses these challenges by managing a fleet of clusters from
    a single pane of glass. Let’s understand briefly what those challenges are and
    how TMC helps to solve them.
  prefs: []
  type: TYPE_NORMAL
- en: Increased overhead of cluster lifecycle management
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Basic lifecycle management operations of several clusters, such as creating,
    scaling, upgrading, and deleting, could be a nightmarish situation without complete
    automation in place. Any manual intervention in this process could lead to configuration
    drifts resulting from basic human errors. This could soon result in a group of
    clusters having different configurations, and the need for cluster upgrades would
    be frequent for all the clusters, as the upstream Kubernetes releases a new version
    every 3 to 4 months and patch versions even more frequently. Regular maintenance
    of clusters via upgrades and patches is highly recommended to stay secure and
    supported. Such frequent maintenance of a large number of clusters requires a
    sophisticated automation setup. On the other hand, building and maintaining full
    end-to-end automation for these lifecycle processes require huge in-house effort.
    In that case, it’s a question of the organizations building this automation asking
    themselves whether they should invest in this much effort for below-value-line
    activities to support a container platform, or whether they would rather invest
    these resources to add new business functionalities to their applications, which
    would bring more revenue.
  prefs: []
  type: TYPE_NORMAL
- en: TMC addresses this challenge with the help of **Tanzu Kubernetes Grid** (**TKG**).
    In the previous chapter, we learned about TKG and its concept of a management
    control plane to manage the lifecycle of several workload clusters under that
    management cluster. TMC provides an out-of-the-box integration experience to link
    TKG management control plane clusters and allows TMC users to perform all TKG
    cluster lifecycle operations using the TMC portal. Such an easy, quick, and user-friendly
    approach to managing cluster lifecycles addresses the challenges involved in keeping
    the Kubernetes versions up to date for security reasons. Additionally, a complete
    out-of-the-box cluster lifecycle automation offering from TKG and its integration
    with TMC reduces the additional toil required to create and maintain in-house
    automation for the same reason.
  prefs: []
  type: TYPE_NORMAL
- en: Distinct configuration requirements for different clusters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Things get even more complex when different clusters have different configuration
    requirements. The security requirements for a group of non-production clusters
    would not be the same as the production clusters. Furthermore, the compliance
    requirements of the environments handling **Payment Card Industry** (**PCI**)
    data are even more stringent. Depending on the environment, the clusters may have
    different needs for user access, container network, workload isolation, and deployment
    policies. A security policy such as preventing the deployment of privileged containers
    that may allow root-level access to the host they are deployed on, or a policy
    ensuring high availability only allowing two more Pods to be deployed for an application
    are just some examples of these kinds of policies. It is difficult to find that
    one size that fits all clusters. Adding more flavors of clusters adds increasingly
    more complexity.
  prefs: []
  type: TYPE_NORMAL
- en: To address this challenge, TMC allows us to create groups of clusters and then
    create different sets of policies and treatments for different groups of clusters.
    That way, the access policy of a development group of clusters could be different
    and more lenient than that of a group of production clusters. Additionally, TMC
    also allows you to create a group of Kubernetes namespaces, called **Workspaces**,
    which can span across the boundaries of clusters and cluster groups. Then, we
    can create policies for these Workspaces that are applicable at the namespace
    level. One such policy in Kubernetes is network policy defining, which applications/services
    may connect to which ones across different namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: Installation and configuration of tools
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Every organization has a set of tools that they want to deploy on their clusters
    for cross-cutting concerns such as logging, monitoring, certificate management,
    **identity and access management** (**IAM**), and more. When different teams are
    in charge of the management of their own small clusters, it can become challenging
    to provide a self-service approach to the cluster owners where they can pick and
    deploy required tools on their clusters as and when required quickly and consistently.
  prefs: []
  type: TYPE_NORMAL
- en: To address this point, TMC has a catalog of some popular open source packages
    that different cluster owners may install based on their requirements with a single
    click. This capability not only reduces the overhead for different teams but also
    allows you to instate a guardrail for other cluster owners so that only certain
    authorized packages can be installed on their clusters. As an additional advantage,
    all these packages can be installed using a common approach, and only for the
    published versions in the catalog.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster configuration and workload data protection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Access to a quick and easy procedure to back up and restore cluster states is
    also an important concern for most of the clusters in an organization. For static
    workloads that do not use persistent storage volumes, backing up the Kubernetes
    configuration YAML files may work to restore these objects by applying those configurations
    again, but what if there was a delta in the running state versus the documented
    state? The restored data, in this case, would not be identical. Backup and restoration
    of the stateful workloads add more complexity, as we need to also back up the
    storage volumes used by those workloads along with their configuration YAML files.
    Additionally, performing these activities for many clusters to make sure this
    happens at the required frequency to lose minimal data asks for huge automation
    efforts. On the other side, if the ownership of backup and restoration is left
    to the teams responsible for each individual cluster, then either the ball will
    be dropped or there will be a standardization challenge in how this is done across
    different teams.
  prefs: []
  type: TYPE_NORMAL
- en: To address this, TMC uses **Velero** (https://velero.io), an open source project
    backed by VMware, to back up and restore Kubernetes clusters. TMC users can either
    schedule cluster backups or do them on demand. These backups are saved in **Simple
    Storage Service** (**S3**)-compatible object storage locations accessible using
    web URLs. Taking, scheduling, and restoring backups using the TMC console is very
    easy and provides a consistent method and storage location to ensure the required
    data protection for everything running on a cluster, including stateless and stateful
    workloads. By means of Velero, TMC also allows you to only back up selected important
    namespaces if desired.
  prefs: []
  type: TYPE_NORMAL
- en: What is S3?
  prefs: []
  type: TYPE_NORMAL
- en: S3 is an object or file storage service offered by AWS. S3 has become a standard
    for object storage that implements the S3 interfaces required. Presently, there
    are several cloud-hosted and on-premises S3-compatible storage options available
    on the market in addition to the one offered by AWS, including MinIO and Dell
    **Elastic Cloud** **Storage** (**ECS**).
  prefs: []
  type: TYPE_NORMAL
- en: Conducting cluster inspections
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When an organization has hundreds of clusters owned by multiple teams, it is
    a mammoth effort to inspect each cluster in terms of its security and operational
    policy compliance. Enterprises operating in highly secure domains such as healthcare
    and finance have formal obligations to audit their application platforms at a
    regular frequency to find compliance and address violations. It is almost impossible
    to audit and inspect several different clusters manually to see whether they follow
    required security compliance practices. It requires a great amount of automation
    to build a scanning engine that can check all the different rules on a checklist
    for a given cluster and then report violations of different severities. There
    is a list of recommendations from the official Kubernetes specification that a
    secure production-grade cluster should follow. Similarly, the **Center for Internet
    Security** (**CIS**) also has a benchmarking list of recommendations that should
    be followed in a cluster for security. Conducting different types of inspections
    for hundreds of clusters is a very difficult endeavor.
  prefs: []
  type: TYPE_NORMAL
- en: However, running inspections for clusters only takes a few clicks using TMC.
    To provide this feature, TMC uses **Sonobuoy** (https://sonobuoy.io), another
    open source project backed by VMware, providing Kubernetes cluster configuration
    validation. TMC allows you to run two different types of inspection – CIS benchmarking
    and Kubernetes specification conformance. Upon the completion of these inspections,
    the TMC user gets a detailed report of the compliance and violation of different
    recommendations. These results can help you take quick preventative measures to
    close vulnerable security loopholes before they are exploited.
  prefs: []
  type: TYPE_NORMAL
- en: 'Along with solving these challenges around managing a large group of clusters,
    TMC also emits critical events related to the clusters under its purview to keep
    their owners fully informed about their health and critical lifecycle stages.
    Additionally, TMC offers a comprehensive set of REST APIs to allow you to perform
    all these operations programmatically. Finally, as a major benefit, TMC can perform
    all these operations for any conformant Kubernetes flavor, including AWS **Elastic
    Kubernetes Service** (**EKS**), **Azure Kubernetes Service** (**AKS**), **Google
    Kubernetes Engine** (**GKE**), OpenShift, Rancher, open source upstream distributions,
    and many other types. This excludes the full cluster lifecycle management that
    we covered in the first point under *Increased overhead of cluster lifecycle management*.
    Upgrading Kubernetes clusters is only supported for TKG clusters at the time of
    writing. This capability makes TMC a great tool to implement a multi-cloud Kubernetes
    strategy for an enterprise. The following screenshot shows the TMC console with
    multiple Kubernetes clusters deployed on different cloud environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – TMC console with multiple Kubernetes clusters managed](img/B18145_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – TMC console with multiple Kubernetes clusters managed
  prefs: []
  type: TYPE_NORMAL
- en: With this, you should have gotten a convincible answer to the question, why
    TMC? We will get into the details of many of these capabilities later in this
    chapter, but for now, let’s see how to get started with TMC.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with TMC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TMC is a **Software-as-a-Service** (**SaaS**) offering under VMware Cloud Services.
    Because of that, there is no installation and setup required to start using TMC,
    which is a big relief that SaaS products provide. In TMC, we can have two types
    of clusters, as described in the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clusters that are under a Kubernetes platform management control plane that
    is registered on TMC** – Presently, TMC only supports TKG clusters under this
    category. Once a TKG management cluster (a TKG platform control plane) running
    on vSphere, AWS, or Azure is registered in TMC, we can use the TMC interface to
    perform all lifecycle operations for all the clusters created under that management
    cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clusters that are attached to TMC** – These can be any conformant Kubernetes
    clusters that are created externally. We can attach these clusters to TMC for
    several management activities discussed previously in this chapter, except for
    full lifecycle operations, such as creating, deleting, and upgrading. TMC offers
    a common management control plane for all Kubernetes clusters irrespective of
    their flavors and vendors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this section, we will perform the following operations to get started with
    TMC:'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the TMC portal via the VMware Cloud Services console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registering an existing TKG management cluster running on AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a TKG workload cluster under the registered management cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attaching a GKE cluster for management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a cluster group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding two associated clusters in the newly created cluster group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Workspace, a group of cross-cluster Kubernetes namespaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding two Kubernetes namespaces from two different clusters to the created
    Workspace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, before you can follow along, the following prerequisites must be fulfilled:'
  prefs: []
  type: TYPE_NORMAL
- en: Administrator-level access to TMC via a VMware Cloud Services account.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An existing TKG management cluster (version 1.4.1 or later) with a production
    plan with at least three control plane nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An existing Kubernetes cluster not managed by the previously mentioned TKG management
    cluster – this could be a GKE, AKS, EKS, OpenShift, Rancher, or even an open source
    Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cluster nodes should contain 4 vCPUs and 8 GB memory for the smooth execution
    of steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A user workstation with an internet browser (preferably Google Chrome) and the
    `kubectl` CLI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full internet connectivity from the Kubernetes clusters without any proxy servers,
    as the procedure to configure TMC for different operations differs with a proxy
    in between, which we are not considering in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to either AWS S3 or any other S3-compatible object store service from
    the clusters linked with TMC.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additionally, if your TKG management cluster is running on AWS and if you have
    configured the IAM permissions defined in the CloudFormation stack used by TKG
    manually, then you must add the following listed permissions to the `nodes.tkg.cloud.vmware.com`
    IAM policy or role:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: However, these permissions are included automatically when you create or update
    the CloudFormation stack by running the `tanzu mc permissions aws set` command.
    Once these prerequisites are addressed, we should be good for the rest of the
    chapter. Let’s start executing our plan to get started with TMC.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the TMC portal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s first open the TMC portal via the VMware Cloud Services portal with the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visit the VMware Cloud Services portal at this URL: https://console.cloud.vmware.com/.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **LAUNCH SERVICE** link on the **VMware Tanzu Mission Control**
    tile as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.2 – VMware Cloud Services console](img/B18145_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – VMware Cloud Services console
  prefs: []
  type: TYPE_NORMAL
- en: 'That should open the TMC portal as shown in the following screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Tanzu Mission Control console](img/B18145_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Tanzu Mission Control console
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are not entitled to TMC in your VMware Cloud Services account, then
    you will not see the TMC tile on the VMware Cloud Services console. You may need
    to request access for a TMC trial by getting in touch with your VMware contact
    point, or you can request access to TMC Starter via this URL: https://tanzu.vmware.com/tmc-starter.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, as we are on the TMC portal, let’s register our first TKG management cluster
    in TMC.
  prefs: []
  type: TYPE_NORMAL
- en: Registering a TKG management cluster on TMC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Take the following steps on the TMC portal to register your existing TKG management
    cluster, running either on top of vSphere, AWS, or Azure cloud environments. The
    management cluster used in this chapter is deployed on top of AWS, which does
    not change any procedure in the following steps, except entering some configuration
    details for the workload cluster that we will create:'
  prefs: []
  type: TYPE_NORMAL
- en: Registering a TKG management cluster on TMC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Take the following steps on the TMC portal to register your existing TKG management
    cluster, running either on top of vSphere, AWS, or Azure cloud environments. The
    management cluster used in this chapter is deployed on top of AWS, which does
    not change any procedure in the following steps, except entering some configuration
    details for the workload cluster that we will create:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Administration** menu option from the left navigation bar and
    open the **Management clusters** tab as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Opening the Management clusters screen](img/B18145_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Opening the Management clusters screen
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Management clusters** tab, click on the **REGISTER MANAGEMENT CLUSTER**
    dropdown, and select the **Tanzu Kubernetes Grid** option as highlighted in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Selecting Tanzu Kubernetes Grid to register as a management
    cluster](img/B18145_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Selecting Tanzu Kubernetes Grid to register as a management cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'On the detail screen to register the management cluster, enter a unique name,
    select **default** from the drop-down list of groups, optionally add a small description,
    and finally, click on the **NEXT** button. We will discuss cluster groups in detail
    later in the chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Entering management cluster details](img/B18145_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Entering management cluster details
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **NEXT** button in the **Proxy Configuration** section, as we
    will not need it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Skipping proxy configuration](img/B18145_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Skipping proxy configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be given a URL of a YAML file that contains the Kubernetes resources
    that you need to create for your management cluster to link it with your TMC account.
    Copy the URL as shown in the following screenshot. You may expand the **View YAML**
    section to see the details of the Kubernetes resources that will be created on
    your management cluster and their configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Copying the registration URL to link to the management cluster](img/B18145_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Copying the registration URL to link to the management cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **VIEW MANAGEMENT CLUSTER** button to inspect the newly registered
    management cluster on TMC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Verifying the creation of the management cluster](img/B18145_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Verifying the creation of the management cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see the page as displayed in the following screenshot. The status
    of the cluster is **Unknown**, as we are yet to apply the registration YAML configuration
    to our management cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Management cluster status unknown](img/B18145_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Management cluster status unknown
  prefs: []
  type: TYPE_NORMAL
- en: Open your console window where you have access to the `kubectl` CLI on your
    workstation with the `kubectl` context pointing to the management cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following `kubectl apply` command using the URL copied in *step 4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should list a bunch of different Kubernetes resources created for your
    management cluster. Once completed, we have created a two-way link between the
    TKG management cluster and the TMC account. We should be able to see the management
    cluster successfully verified on TMC.
  prefs: []
  type: TYPE_NORMAL
- en: 'After 5 to 10 minutes, either click on the **VERIFY CONNECTION** button as
    shown in *step 6*, or navigate to **Administration** | **Management clusters**
    to verify the successful registration of the management cluster. As you can see
    in the following screenshot, the cluster is **Healthy** and in a **Ready** state
    now. Click on the cluster name link as highlighted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Management cluster registration verification](img/B18145_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – Management cluster registration verification
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you click on the name of the management cluster as shown in the previous
    step, you will see something similar to the following screenshot showing the details
    of the management cluster, including the health indicators of different components
    running on the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Management cluster details](img/B18145_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – Management cluster details
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our steps to register a TKG management cluster in a TMC account
    using the TMC portal. In the next section, we will see how to use the TMC portal
    to create a new TKG workload cluster under the newly added management cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new workload cluster under a management cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After setting up the first TKG management cluster on TMC, let’s use this setup
    to create a new TKG workload cluster. As discussed in the previous chapter covering
    TKG in detail, a TKG workload cluster is used to run your containerized apps,
    whereas the management cluster is the control plane for several of these workload
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: All TKG workload clusters are created under a **provisioner**. A provisioner
    is a namespace within the management cluster that owns required workload clusters
    within the management cluster. This way, the provisioners in TKG provide a multi-tenancy
    construct to allow different teams to create and manage their own workload clusters
    without interfering with others. In our case, we will use the **default** namespace
    of the management cluster, which should be present by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the following steps to create a new TKG workload cluster for your TKG
    management cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the **Administration** menu of the left-hand navigation bar, open the
    **Management clusters** tab, and click on the **tkg-aws-mgmt-cluster** link to
    open its detail page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Opening the management cluster page](img/B18145_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – Opening the management cluster page
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the **Clusters** menu on the left-hand navigation bar and click on the
    **CREATE CLUSTER** button as highlighted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.14 – Creating a workload cluster](img/B18145_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – Creating a workload cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Select **tkg-aws-mgmt-cluster** from the list that was registered on TMC earlier
    and click on the **CONTINUE TO CREATE** **CLUSTER** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.15 – Continue creating a workload cluster](img/B18145_09_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 – Continue creating a workload cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Select **default** from the **Provisioner** dropdown and click on **NEXT**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.16 – Selecting the provisioner for the workload cluster](img/B18145_09_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 – Selecting the provisioner for the workload cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the cluster name, optionally add a description, and click on the **NEXT**
    button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.17 – Entering the workload cluster details](img/B18145_09_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.17 – Entering the workload cluster details
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the required infrastructure-specific details in this step and click on
    the **NEXT** button. As the TKG management cluster used in this chapter is running
    on AWS, the following screenshot shows AWS-specific details. For vSphere and Azure,
    some fields will be different. You can get more details about them here: https://docs.vmware.com/en/VMware-Tanzu-Mission-Control/services/tanzumc-using/GUID-42150344-CD4C-43AE-8C39-C059A97EF47C.html:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.18 – Entering the workload cluster configuration](img/B18145_09_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.18 – Entering the workload cluster configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Choose the control plane plan for the workload cluster to be created. You can
    also optionally change the values of the other fields, but this is not required
    and we recommend following the procedure in this chapter. Finally, click on the
    **NEXT** button to select the worker node details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.19 – Selecting the workload cluster type](img/B18145_09_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.19 – Selecting the workload cluster type
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the count of the worker nodes for the workload cluster and click on the
    **CREATE** **CLUSTER** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.20 – Entering the worker node count](img/B18145_09_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.20 – Entering the worker node count
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see a message saying that the workload cluster is being created with
    its specifications as shown in the following screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.21 – Workload cluster being created](img/B18145_09_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.21 – Workload cluster being created
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see a cluster detail screen like the following screenshot once the
    workload cluster is created successfully. It might take 5 to 10 minutes depending
    on the size and infrastructure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.22 – Workload cluster detail page](img/B18145_09_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.22 – Workload cluster detail page
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our third step, creating a TKG workload cluster using a TKG management
    cluster using the TMC portal. Now, we will attach an externally managed Kubernetes
    cluster to TMC so that we can perform various day-2 activities for that cluster
    using TMC.
  prefs: []
  type: TYPE_NORMAL
- en: Attaching an existing Kubernetes cluster with TMC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will attach an existing GKE cluster to the TMC account.
    You may select any other type of Kubernetes cluster, including Rancher, AKS, EKS,
    OpenShift, upstream open source, and more. TMC will allow you to attach a Kubernetes
    cluster as far as it is a conformant Kubernetes cluster with admin-level `kubectl`
    access for the cluster. So, let’s attach an external cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the **Clusters** menu from the left-hand menu bar and click on the **ATTACH
    CLUSTER** button as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 9.23 – Go\uFEFFing to the Attach cluster page](img/B18145_09_23.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.23 – Going to the Attach cluster page
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the cluster name, select the cluster group as **default**, optionally
    enter a description, and finally, click on the **NEXT** button as highlighted
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.24 – Entering the attached cluster details](img/B18145_09_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.24 – Entering the attached cluster details
  prefs: []
  type: TYPE_NORMAL
- en: 'Skip the proxy configuration and click on the **NEXT** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.25 – Skipping the proxy configuration details](img/B18145_09_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.25 – Skipping the proxy configuration details
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as we registered the management cluster using a Kubernetes YAML configuration
    file earlier in this chapter, it is now time to register the external cluster
    using a similar approach. You can view the details of the Kubernetes resources
    that will be created on the targeted cluster to establish two-way communication
    between the cluster and TMC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.26 – Copying the agent configuration command to attach the\
    \ cluster to TMC](img/B18145_09_26.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.26 – Copying the agent configuration command to attach the cluster
    to TMC
  prefs: []
  type: TYPE_NORMAL
- en: Open the command window where you have access to the cluster being attached
    via `kubectl` and change the `kubectl` context to point to the cluster being attached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the `kubectl` command copied in *step 4* on your command window to create
    the required agent deployment resources in your cluster to attach to TMC.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After the successful execution of the `kubectl create` command, click on the
    **VERIFY CONNECTION** button. You should see the recently attached cluster in
    the list as shown in the following screenshot. Then, click on the attached cluster
    name to verify its details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.27 – Verifying an attached cluster in the list](img/B18145_09_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.27 – Verifying an attached cluster in the list
  prefs: []
  type: TYPE_NORMAL
- en: 'Examine the details of the attached cluster. As you can see in the following
    screen, the type of the cluster is **Attached**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.28 – Attached cluster details](img/B18145_09_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.28 – Attached cluster details
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our task of attaching an externally managed Kubernetes cluster
    to TMC. As the next steps, we will create a cluster group and add our TKG workload
    and GKE-attached clusters to that group.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a cluster group on TMC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed earlier in this chapter, TMC allows you to group different clusters
    of similar natures and manage and handle them using common configurations. Configuring
    groups of several clusters makes the operation of large Kubernetes foundations
    very easy and efficient. So, let’s learn how to create cluster groups in TMC in
    this section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Cluster groups** menu item from the left-hand navigation bar
    and click on the **CREATE CLUSTER GROUP** button as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.29 – Creating cluster groups](img/B18145_09_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.29 – Creating cluster groups
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the cluster group name, add an optional description, and click the **CREATE**
    button as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.30 – Entering cluster group details](img/B18145_09_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.30 – Entering cluster group details
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see the newly created cluster group in the list as shown in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.31 – Verifying the presence of the cluster group](img/B18145_09_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.31 – Verifying the presence of the cluster group
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s add our two clusters to this newly created group using the following
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **default** cluster group as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.32 – Selecting the cluster group as default](img/B18145_09_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.32 – Selecting the cluster group as default
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the two clusters we have on TMC, for the TKG workload and GKE, and click
    on the **MOVE** button as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.33 – Selecting clusters for grouping](img/B18145_09_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.33 – Selecting clusters for grouping
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the cluster group we created in *step 2* and click on the **MOVE** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.34 – Selecting the cluster group](img/B18145_09_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.34 – Selecting the cluster group
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the **Cluster groups** page and click on the new cluster group we created
    to examine the presence of the two clusters we added in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.35 – Getting into the new cluster group](img/B18145_09_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.35 – Getting into the new cluster group
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following screenshot, both clusters that we created in
    this section are in the cluster group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.36 – Listed clusters in the cluster group](img/B18145_09_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.36 – Listed clusters in the cluster group
  prefs: []
  type: TYPE_NORMAL
- en: Grouping clusters with similar requirements is a very powerful way to manage
    hundreds of clusters with heterogeneous requirements. Along with groups of clusters,
    TMC also allows you to group Kubernetes namespaces to perform certain namespace-level
    configurations applicable to more than one namespace in different clusters. Let’s
    learn more about it.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Workspaces in TMC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed before in this chapter, TMC allows you to group clusters of
    similar natures. Grouping helps us create a common configuration and policies
    for them for operational efficiency. However, certain policies in Kubernetes can
    only be applied at the namespace level, such as network policies that define which
    Pods from one namespace can talk to which Pods in another namespace. Additionally,
    we can also configure cluster user access policies at the namespace level, and
    TMC also allows us to create image registry access policies at the namespace level.
    While applying cluster-level policies to a cluster group is an easy task, applying
    namespace-level policies for individual namespaces could be very impractical.
    Moreover, a way to apply namespace-level policies to a group of namespaces within
    a cluster would also not help much if we were trying to manage hundreds of Kubernetes
    clusters. For these reasons, TMC offers a construct called **Workspaces**, which
    allows us to create a group of namespaces across different clusters – then, we
    can create a namespace-level policy applicable to the entire Workspace and hence
    for all the namespaces within the same Workspace. A multi-cloud application that
    is deployed in different clusters might need similar configurations for either
    user access or network policy configuration. For a Workspace containing all the
    namespaces of this application and running different clusters, it would come in
    very handy to treat them as a unit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create two namespaces in the two clusters we have added in the previous
    section of this chapter and then create a Workspace to add those two namespaces
    into:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the **Workspaces** menu from the left-hand navigation bar and click on
    the **CREATE WORKSPACE** button as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.37 – Creating a Workspace](img/B18145_09_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.37 – Creating a Workspace
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter a name, optionally add a description, and click on the **CREATE** button
    on the page as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.38 – Entering Workspace details](img/B18145_09_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.38 – Entering Workspace details
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following screenshot, a new Workspace has been created.
    Click on the Workspace name to ensure there are no namespaces listed in there:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.39 – Verifying that the Workspace has been created](img/B18145_09_39.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.39 – Verifying that the Workspace has been created
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the **tmc-demo-clusters** cluster group that we previously created on
    the TMC portal as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.40 – Opening the cluster group](img/B18145_09_40.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.40 – Opening the cluster group
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the TKG workload cluster as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.41 – Opening a cluster detail page](img/B18145_09_41.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.41 – Opening a cluster detail page
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Namespaces** tab of the cluster detail page, select the **default**
    namespace, and click on the **ATTACH 1 NAMESPACE** button as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.42 – Selecting a namespace to attach to a Workspace](img/B18145_09_42.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.42 – Selecting a namespace to attach to a Workspace
  prefs: []
  type: TYPE_NORMAL
- en: 'Select or enter the Workspace name and click on the **ATTACH** button as shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.43 – Selecting the Workspace to attach](img/B18145_09_43.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.43 – Selecting the Workspace to attach
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following screenshot, the **default** namespace is now
    under **tmc-demo-workspace**, which we created in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.44 – Verifying the namespace-Workspace association](img/B18145_09_44.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.44 – Verifying the namespace-Workspace association
  prefs: []
  type: TYPE_NORMAL
- en: Repeat *steps 5* to *8* from this list to add the **default** namespace to this
    new Workspace for the other cluster, **gke-cluster-1**, as per this chapter, which
    we previously attached to TMC and added to the **tmc-demo-clusters** group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the **Workspaces** menu from the left-hand navigation bar, and click on
    the new Workspace, **tmc-demo-workspace**, that we created earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.45 – Opening Workspace details](img/B18145_09_45.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.45 – Opening Workspace details
  prefs: []
  type: TYPE_NORMAL
- en: 'You should be able to see two default namespaces from two different clusters
    listed as a part of the new Workspace we created in this part as shown in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.46 – Verifying namespaces in the Workspace](img/B18145_09_46.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.46 – Verifying namespaces in the Workspace
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have covered how to create Workspaces and how to attach existing
    namespaces to different clusters in the Workspace. In the example that we covered
    earlier, we used the default namespaces with the same name as the clusters that
    were in the same cluster group in TMC. However, you can add any namespaces to
    a Workspace irrespective of the namespace names and the group status of their
    parent clusters.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we conclude all the steps that we planned to cover under this part
    of the chapter – *Getting started with TMC*. We learned how to register a TKG
    management cluster in TMC and created a TKG workload cluster using the TMC interface.
    Then, we also attached an externally managed GKE cluster to TMC. Finally, we learned
    how to group clusters and Kubernetes namespaces to perform common operations on
    them as a unit. In the next part of the chapter, we will learn about making and
    restoring cluster data backups using TMC.
  prefs: []
  type: TYPE_NORMAL
- en: Protecting cluster data using TMC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is widely used to run business-critical applications in production
    environments. In these cases, a reliable disaster recovery mechanism should be
    present to make regular cluster data and configuration backups and restore them
    in the event of data loss for any reason. Although Kubernetes is mostly used to
    run stateless workloads where the persistent data is stored outside the clusters
    in the databases, running stateful software, such as caches, queues, and databases,
    is also being adopted slowly. In [*Chapter 6*](B18145_06.xhtml#_idTextAnchor112),
    *Managing Container Images with Harbor*, the Harbor registry deployment used the
    data stores that were deployed on Kubernetes itself. That makes backing up data
    even more important.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to cover this important topic, we will learn how to make cluster backups
    and restore them using TMC with the following high-level steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Configure an S3-compatible remote backup storage location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure a cluster to use the remote storage location for backup data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy a custom application to the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a backup of the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Delete the cluster namespace with the custom deployment to simulate a disaster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restore the cluster backup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify the presence of the deleted namespace and the custom deployment within
    it post-restoration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s start the work to execute these steps one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the backup target location
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TMC allows you to create separate backup target locations for separate cluster
    groups. Configuring a backup location for a cluster group is a one-time administrative
    activity. Once a target location is associated with a cluster group, we can make
    either on-demand or scheduled backups for any cluster in the cluster group using
    the backup location configuration. Additionally, one configured backup location
    can be used by many cluster groups. In this chapter, we will use the AWS S3 option
    as the storage location.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps describe how to configure a backup target location for
    a cluster group in TMC:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create your AWS account credentials for TMC to use to store backup data in
    an S3 bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the **Administration** menu from the left-hand navigation bar and click
    on the **CREATE ACCOUNT CREDENTIAL** button as shown in the following screenshot:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.47 – Creating AWS account credentials](img/B18145_09_47.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.47 – Creating AWS account credentials
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the **AWS S3** option from the **TMC provisioned** **storage** option:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.48 – Selecting provisioned storage](img/B18145_09_48.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.48 – Selecting provisioned storage
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter a name for the account credentials and click on the **GENERATE** **TEMPLATE**
    button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.49 – Entering a credential name and generating a template](img/B18145_09_49.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.49 – Entering a credential name and generating a template
  prefs: []
  type: TYPE_NORMAL
- en: 'A credential template file should be downloaded. This is an AWS CloudFormation
    template that creates the required S3 buckets and grants required permissions
    to TMC to access that bucket for backup and restoration operations. Save that
    template file on your local workstation and click on the **NEXT** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.50 – Downloading the generated AWS CloudFormation template](img/B18145_09_50.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.50 – Downloading the generated AWS CloudFormation template
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the **QUICKSTART GUIDE** link to apply the downloaded template to your
    AWS account. Once the CloudFormation template is applied, you will get the **Amazon
    Resource Name** (**ARN**) in the output of the template execution, which will
    be required in the next step. You will find these instructions in the quick start
    guide as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.51 – Following the quick start guide to create the required\
    \ AWS S3 objects](img/B18145_09_51.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.51 – Following the quick start guide to create the required AWS S3
    objects
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the copied ARN from AWS after applying the CloudFormation template and
    click the **CREATE** button to finally create the account credentials that we
    can use for the backup and restoration operations for any cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.52 – Applying the copied ARN and creating the credentials](img/B18145_09_52.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.52 – Applying the copied ARN and creating the credentials
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon successful creation of the account credentials, you should see it listed
    under the **Accounts** tab of the **Administration** menu as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.53 – Verifying account credential creation](img/B18145_09_53.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.53 – Verifying account credential creation
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the **Target locations** tab under the **Administration** menu and click
    on **CREATE TARGET LOCATION** for the AWS S3 option as per the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.54 – Creating the target location](img/B18145_09_54.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.54 – Creating the target location
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the account credentials that we created in *step 1* of this section
    and click on the **NEXT** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.55 – Selecting account credentials](img/B18145_09_55.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.55 – Selecting account credentials
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the cluster group that we created earlier in this chapter and click
    on the **NEXT** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.56 – Adding the cluster group to the target location](img/B18145_09_56.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.56 – Adding the cluster group to the target location
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the name of the target location and click on the **CREATE** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.57 – Entering the target location name](img/B18145_09_57.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.57 – Entering the target location name
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see a new target location created under the **Target locations**
    tab of the **Administration** menu along with the account credentials and the
    relevant S3 bucket name as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.58 – Verifying target location creation](img/B18145_09_58.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.58 – Verifying target location creation
  prefs: []
  type: TYPE_NORMAL
- en: Now, as we have set up a backup target location and assigned it to the cluster
    group, we can proceed to perform the backup and restoration operations for a cluster
    in that group.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling data protection for a cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After completing the steps required to perform backup and restoration for a
    cluster group, let’s enable it for one of the clusters in the group. In this example,
    we will use the externally managed GKE cluster that we had attached with TMC.
    Take the following steps to complete this task:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the attached cluster, **gke-cluster-1**, in the **tmc-demo-clusters**
    group. If you have used names other than the ones used in this chapter, you will
    need to select the cluster appropriately:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.59 – Opening the attached cluster details page](img/B18145_09_59.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.59 – Opening the attached cluster details page
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **ENABLE DATA PROTECTION** link as highlighted in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.60 – Clicking on the ENABLE DATA PROTECTION link](img/B18145_09_60.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.60 – Clicking on the ENABLE DATA PROTECTION link
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **ENABLE** button to confirm the operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.61 – Enabling data protection for the cluster](img/B18145_09_61.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.61 – Enabling data protection for the cluster
  prefs: []
  type: TYPE_NORMAL
- en: This operation deploys Velero, an open source Kubernetes cluster backup and
    restoration tool. With this, we have prepared the cluster to take backups with
    the required toolset deployed on it. Now, let’s deploy a test workload in the
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a custom application in the cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Execute the following steps to run an nginx deployment on the cluster we enabled
    for backup and restoration in the previous task:'
  prefs: []
  type: TYPE_NORMAL
- en: 'While in the targeted cluster’s `kubectl` context, run the following command,
    which creates a namespace named `nginx` and a Kubernetes deployment named `nginx-deployment`
    with three Pods running. You can check the deployment manifest file used in the
    following command to get more details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify the deployment using the TMC portal under the **Workloads** tab of the
    cluster details page. As you can see in the following screen, the nginx deployment
    and ReplicaSet are running with a **Healthy** status. To minimize the clutter
    on the screen, you can also filter Tanzu and Kubernetes-specific workloads using
    the switches as highlighted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.62 – Verifying workload deployment for a cluster](img/B18145_09_62.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.62 – Verifying workload deployment for a cluster
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have a workload deployed in its own namespace on a target cluster that
    can be backed up. Let’s make a backup of the cluster using the following steps.
  prefs: []
  type: TYPE_NORMAL
- en: Backing up a cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Take the following steps to take a backup of the cluster where we deployed
    the nginx workload in the previous task:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the cluster’s **Overview** tab, click on the **CREATE BACKUP** link as highlighted
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.63 – Initiating the cluster backup process](img/B18145_09_63.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.63 – Initiating the cluster backup process
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the option to make a backup of the entire cluster and click on the **NEXT**
    button. We can also make a backup only of selected namespaces or selected objects
    identified using a label value, which is a very flexible choice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.64 – Selecting the backup scope](img/B18145_09_64.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.64 – Selecting the backup scope
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the backup target location that we created previously in this chapter
    pointing to an AWS S3 bucket and click on the **NEXT** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.65 – Selecting the backup target location](img/B18145_09_65.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.65 – Selecting the backup target location
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the backup schedule. While we can create a regular backup schedule,
    here, we will select **NOW** to make an on-demand backup to learn about the concept.
    Click on the **NEXT** button to move on after that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.66 – Selecting the backup schedule](img/B18145_09_66.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.66 – Selecting the backup schedule
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the backup **Retention (days)** value and click on the **NEXT** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.67 – Entering the backup retention days](img/B18145_09_67.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.67 – Entering the backup retention days
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, enter the backup’s name and click on the **CREATE** button as shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.68 – Entering the backup name](img/B18145_09_68.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.68 – Entering the backup name
  prefs: []
  type: TYPE_NORMAL
- en: 'This will trigger the backup process and within 2 to 5 minutes, you should
    be able to see the backup completed under the **Data protection** tab of the cluster
    as shown in the following screenshot. The backup process may take more time if
    the cluster has other workloads running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.69 – Verifying backup completion](img/B18145_09_69.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.69 – Verifying backup completion
  prefs: []
  type: TYPE_NORMAL
- en: After successfully making a backup of the cluster, let’s now restore it – but
    before we restore it, let’s *accidentally* delete the **nginx** namespace that
    contains the nginx deployment we created earlier before making the backup.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a custom deployment running on the cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Take the following steps to perform this task:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to delete the **nginx** namespace from the targeted
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify the absence of the nginx deployment in the **Workloads** tab of the
    cluster. As you can see, **nginx-deployment** and its corresponding ReplicaSet
    that we verified previously are now missing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.70 – Verifying the workload deletion](img/B18145_09_70.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.70 – Verifying the workload deletion
  prefs: []
  type: TYPE_NORMAL
- en: Now, as we have *accidentally* deleted the entire namespace and the workloads
    running in that namespace to simulate a disaster situation, let’s use the backup
    we made for the cluster to restore the deleted namespace and get its objects back
    up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Restoring the cluster backup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Take the following steps to restore the backup of the targeted cluster and
    bring back the deleted Kubernetes objects in that cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the **Data protection** tab of the cluster, select the backup that we
    took previously from the **Backups** list, and click on the **RESTORE** link as
    highlighted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.71 – Restoring a backup](img/B18145_09_71.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.71 – Restoring a backup
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we had only one namespace deleted, we will just restore that one by selecting
    the highlighted option to restore specific namespaces. Select the **nginx** namespace
    from the list of what we need to restore. Here, TMC also allows us to restore
    the source namespace from the backup to a different target namespace if that is
    intended. This can be done using the little pencil icon beside the target namespace
    caption. After selecting the namespace, click on the **NEXT** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.72 – Selecting the Restore backup specification](img/B18145_09_72.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.72 – Selecting the Restore backup specification
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter a name for the restored instance and click on the **RESTORE** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.73 – Entering a restore instance name](img/B18145_09_73.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.73 – Entering a restore instance name
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon successful restoration, you will see a restored entry in the **Data protection**
    tab of the cluster under the **Restores** section as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.74 – Verifying that the restoration has completed](img/B18145_09_74.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.74 – Verifying that the restoration has completed
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify the presence of the **nginx** namespace and its objects under the **Workloads**
    tab of the cluster details page. As you can see, the **nginx** namespace has now
    been restored successfully:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.75 – Verifying the presence of the restored objects ](img/B18145_09_75.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.75 – Verifying the presence of the restored objects
  prefs: []
  type: TYPE_NORMAL
- en: With this, we can conclude our very long but important section on backing up
    and restoring Kubernetes clusters. It is worth noting that several configurations
    made in this section were one-time activities. This includes creating AWS S3 account
    credentials, creating a target location of the backup, and associating a target
    location with a cluster group, among other things. Once this setup is done, backing
    up and restoring a cluster or a part of the cluster only takes a few clicks.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now look into another very important capability of TMC – policy management.
  prefs: []
  type: TYPE_NORMAL
- en: Applying governance policies to clusters using TMC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section of the chapter, we learned how to get started with
    TMC by registering a TKG management cluster, creating a TKG workload cluster,
    attaching a GKE cluster, and finally, grouping them – but why do we bring all
    the clusters to TMC? In this section, we will check this out by performing various
    activities with these clusters using the TMC interface. We will cover the following
    activities:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a security policy for a cluster group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring an image registry governance policy for a Workspace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring a deployment governance policy for a cluster group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking policy violation status for clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inspecting a cluster for CIS benchmark compliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a long list of activities to cover in this section. Let’s knock them
    off one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a security policy for a cluster group
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to running containers, several things can be misconfigured from
    a security point of view, which keeps the door open for hackers to leverage these.
    Depending on the nature of the workloads running on the cluster, a Kubernetes
    administrator may need to secure several things. In this world of microservices,
    the Kubernetes platform team often needs to allow different teams to deploy their
    apps with their own configurations required by the apps. However, ensuring that
    all the teams follow the required security practices outlined by the platform
    team can be a very difficult task. That is why Kubernetes administrators need
    to guardrail their clusters so that they do not allow workloads to be deployed
    in unsecured ways. To address this need, Kubernetes offers a construct called
    **PodSecurityPolicy**, which defines what a Pod can and cannot do in the Kubernetes
    cluster with a PodSecurityPolicy in effect. TMC allows you to configure these
    security policies for a group of clusters so that there is no chance of configuration
    drifts between clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create one such security policy for the cluster group that we previously
    created. This policy will prevent any Pod from gaining privileged access to the
    Kubernetes node’s operating system and resources. The following steps will help
    create and test this policy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure that a privileged access Pod can be deployed before applying the policy,
    taking the following substeps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy a privileged Pod in the TKG workload cluster under the `tmc-demo-clusters`
    group using the following command. You can check the Pod definition file used
    in the following command – notice that the security context is defined there to
    enable the Pod to get privileged access:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify whether the Pod has been created and is running successfully. Here,
    it is assumed that there is no security policy in place that would prevent a privileged
    Pod from being created:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After verifying that we can run a privileged Pod in the cluster, let’s now
    create a security policy in TMC for the cluster group using the following substeps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the **Policies** > **Assignments** menu from the left-hand navigation
    bar and the **Security** tab. Then, click on the cluster group, **tmc-demo-clusters**,
    and click on the **CREATE SECURITY POLICY** button as highlighted in the following
    screenshot:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.76 – Creating a security policy for a cluster group](img/B18145_09_76.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.76 – Creating a security policy for a cluster group
  prefs: []
  type: TYPE_NORMAL
- en: 'TMC allows you to fully customize your security requirements by defining a
    custom security policy from scratch. However, it also provides two out-of-the-box
    choices: **Baseline** and **Strict**. Let’s select **Baseline** from the **Security
    template** dropdown. You will notice that it restricts the creation of privileged
    containers as highlighted in the following screenshot. Finally, click on the **CREATE
    POLICY** button to apply the restrictions to the relevant cluster group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.77 – Entering the security policy configuration](img/B18145_09_77.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.77 – Entering the security policy configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see the policy listed for the cluster group as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.78 – Verifying security policy creation](img/B18145_09_78.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.78 – Verifying security policy creation
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now verify whether the policy has been applied to the clusters in the
    group. Take the following substeps to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, delete the Pod we created in the first step in the TKG workload cluster
    by running the following command:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the same Pod again by running the following command:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see an error message as follows, explaining that the Pod could not
    successfully be created because of the security policy in place:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That concludes our learning on how to configure a Kubernetes cluster security
    policy applicable to deploying and running workloads using TMC. We tested one
    of the clusters in the cluster group to which we applied the policy, but you can
    also do the same exercise for another cluster in the group and should see similar
    test results. Like the one explained here for running privileged containers, we
    can create several different types of security policies using TMC for a cluster
    group. Let’s now learn how to apply a Workspace-level policy that defines how
    the workloads running in the specific cluster namespaces can pull container images
    from a container registry.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring an image registry policy for a Workspace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to pulling container images from a container registry, several
    proven practices are recommended. The restrictions applicable to pulling images
    for regulatory environments could be more stringent. The following are the parameters
    that TMC allows you to configure for a container registry policy depending on
    different compliance requirements. A registry policy in TMC may either have all
    or some parameters applicable as required:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pulling images only using a digest (SHA) and not with a tag** – This is an
    important rule to set up for a production environment, as a tag could technically
    have different content for different pull instances, whereas the content of an
    image for the same digest will always be the same. Pulling images using their
    digest will give you the confidence that they will always have the same bits inside.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pulling images only from a certain image repository** – If there is a requirement
    that a container can only pull images from an internally hosted container registry,
    this rule can help.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`myapp/*`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Allowing images with specific tag values** – This is like the previous rule
    but applicable to tag names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to configure a policy that requires image
    pulls only with digest and not with tags. The following are the steps to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the **Policies** | **Assignment** menu from the left-hand navigation
    bar and select the **Image registry** tab. From there, select the Workspace we
    have previously created and click on the **CREATE IMAGE REGISTRY POLICY** button
    as highlighted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.79 – Creating an image registry policy](img/B18145_09_79.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.79 – Creating an image registry policy
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the **Require Digest** option from the **Image registry template** dropdown,
    provide a policy name, and click on the **CREATE** **POLICY** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.80 – Entering image registry policy details](img/B18145_09_80.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.80 – Entering image registry policy details
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the new image registry policy created for the selected Workspace
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.81 – Verifying the creation of the image registry policy](img/B18145_09_81.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.81 – Verifying the creation of the image registry policy
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Pod in the GKE-attached cluster in a new namespace that is not affected
    by the policy. This Pod pulls an image using a tag. The Pod definition YAML file
    is given at https://raw.githubusercontent.com/PacktPublishing/DevSecOps-in-Practice-with-VMware-Tanzu/main/chapter-10/image-tag-using-pod.yaml:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new namespace in the cluster using the following command:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the Pod that uses a tagged image pull:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see that the Pod is being created without any issues:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the same Pod in the **default** namespace where we have applied the
    image registry policy that does not allow you to pull an image with a tag value
    using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see that the Pod cannot be created, with the following error message
    explaining the restrictions in place:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the same Pod with a digest replacing the tag for the image using the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see that the Pod is created successfully this time in the **default**
    namespace where the image policy restrictions are applicable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This concludes the topic of learning how to create a policy that is applicable
    to a Workspace. The previously configured and tested restrictions will also be
    applicable in the other cluster’s **default** namespace, being part of the same
    Workspace.
  prefs: []
  type: TYPE_NORMAL
- en: In the next topic, we will learn how to apply a deployment governance policy
    for a cluster group using TMC.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a deployment governance policy for a cluster group
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the majority of Kubernetes platform deployments, a platform team is responsible
    for ensuring that their internal customers, the application teams, use the platform
    with discipline. This discipline involves the fair usage of the computes available
    and high-availability-prone application deployments. When multiple different Kubernetes
    platform teams are managing a smaller number of clusters, it will be challenging
    to implement a set of standards that are applicable enterprise-wide, as all the
    platform teams may have their own likes and dislikes. On the other hand, if a
    central management team manages all the clusters, it would be too much to ensure
    compliance with the governing policies. To address these challenges, TMC allows
    you to create deployment governance policies. TMC uses an open source project
    named **Open Policy Agent** (**OPA**) **Gatekeeper** (https://github.com/open-policy-agent/gatekeeper)
    to implement these policies. Because of the declarative nature of the Gatekeeper
    policy configuration, TMC also allows you to create custom deployment policies
    for any use case – the sky is the limit!
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the out-of-the-box policies that TMC provides that
    can be applied to different cluster groups as required:'
  prefs: []
  type: TYPE_NORMAL
- en: Blocking the creation of certain Kubernetes resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mandating the assignment of labels to certain Kubernetes resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blocking specific subjects from being used for role binding in clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restricting the usage of certain load balancer IP addresses to be used by Kubernetes
    services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enforcing HTTPS for the ingress resource configuration in clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blocking the creation of `NodePort`-type services in clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s learn how to create one such governing policy for the cluster group that
    we created earlier and test its impact on the cluster operations. In the following
    example, we will create a policy to prevent the creation of a Pod without a label
    named *app*. In other words, creating a Pod in the cluster, where this policy
    is applied, should fail if that Pod does not specify the name of the application
    it belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the following steps to create and test this policy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the **Policies** | **Assignments** menu from the left-hand navigation
    bar and open the **Custom** tab. Then, select the cluster group we created from
    the list and click on the **CREATE CUSTOM POLICY** button as highlighted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.82 – Creating a custom deployment policy](img/B18145_09_82.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.82 – Creating a custom deployment policy
  prefs: []
  type: TYPE_NORMAL
- en: 'Select `app` as a required key under **Labels**. Finally, scroll down and create
    the policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.83 – Entering the custom deployment policy details](img/B18145_09_83.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.83 – Entering the custom deployment policy details
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating the policy, you should be able to see these settings configured
    under the cluster group as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.84 – Verifying the creation of the policy](img/B18145_09_84.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.84 – Verifying the creation of the policy
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Pod in one of the two clusters (the TKG workload or GKE) that we added
    to the cluster group with the policy applied using the following command. You
    can see the Pod definition file used in the following command and verify that
    the Pod does not have any labels in the specification YAML:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see a similar error message as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s create a Pod with the required label using the following command.
    This time, we will use a different Pod specification file where the `app` label
    is specified. Open the file used in the command to see the newly added label –
    `app`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see that, this time, the Pod is created without any issues, as we
    supplied the label as required by the policy we created earlier in this section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This concludes how to create a custom policy that is applicable to all the clusters
    in a group and test its impact. TMC admins can create these policies for any logical
    requirements and create their templates. Later, these templates can be used with
    custom parameters (such as the name of the label key in the previous example)
    to apply the policy for a group of clusters – but how do we keep track of policy
    compliance failures to stay fully informed about these issues popping across hundreds
    of clusters managed by TMC? Let’s find that out in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Checking policy violation statuses across all clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed previously in this chapter, there are several different types
    of policies we can create using TMC at the cluster group and Workspace level.
    In addition to creating guardrails for later Kubernetes platforms, TMC also provides
    a way to monitor them for policy violation insights across all the clusters under
    TMC’s purview. To do so, just open **Policies** > the **Insights** menu from the
    left-hand navigation bar and you will see a detailed report, as shown in the following
    screenshot. As you can see, it shows a couple of records for the policy testing
    we did that violated our policies:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.85 – Getting policy insights](img/B18145_09_85.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.85 – Getting policy insights
  prefs: []
  type: TYPE_NORMAL
- en: Having learned about policies and checking their compliance status, let’s now
    learn how to audit clusters for CIS benchmarking.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting clusters for CIS benchmark compliance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed earlier in this chapter, a security compliance audit of the clusters
    for common loopholes is a proactive stance to prevent cyber-attacks. Scanning
    Kubernetes clusters for a set of best security practices with a long checklist
    is a challenging task unless there is sophisticated automation built to do so.
    TMC provides this capability for all its managed clusters using an open source
    tool, Sonobuoy, as discussed earlier. As of the current version, TMC can perform
    the following two types of cluster inspections:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Conformance* – to check the cluster configuration against the official Kubernetes
    specification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CIS benchmark* – to check whether the Kubernetes cluster deployment is following
    the security best practices outlined in the CIS benchmark for Kubernetes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s learn how to inspect a cluster managed by TMC for CIS Benchmark. Take
    the following steps to perform the inspection and check the inspection results
    using the TMC console:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Inspections** menu in the left-hand navigation bar and click
    on the **RUN INSPECTION** button as highlighted in the screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.86 – Running a cluster inspection](img/B18145_09_86.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.86 – Running a cluster inspection
  prefs: []
  type: TYPE_NORMAL
- en: 'Select an existing cluster, set the inspection type as **CIS benchmark**, and
    click on the **RUN** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.87 – Selecting the inspection to run](img/B18145_09_87.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.87 – Selecting the inspection to run
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see the inspection results in a few minutes as displayed in the following
    screenshot. As you can see, the scan has found some failed test cases that should
    be addressed for a stronger security posture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.88 – Checking the inspection results](img/B18145_09_88.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.88 – Checking the inspection results
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the test result link on the previous screen and you will get a detailed
    report of the failed test cases, warnings, and successful cases, as shown in the
    following screenshot. The report provides details of each test case and recommendations
    for it. You can also download the report to look at it offline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF9.89 – Checking the detailed inspection report](img/B18145_09_89.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.89 – Checking the detailed inspection report
  prefs: []
  type: TYPE_NORMAL
- en: That brings us to the end of the topic of running cluster inspections. We can
    also trigger cluster inspections from the cluster detail sections as well on TMC
    in addition to the main **Inspections** menu.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we have also covered all the planned operations that we wanted to
    cover around applying governance policies for Kubernetes clusters using TMC. First,
    we saw how to create a security policy to restrict the ability to run privileged
    containers. Then, we learned how to create an image registry policy, with the
    example of preventing image pulls without its digest. After that, we created a
    deployment policy to prevent Pods from being created without a specific label
    and saw all the different types of custom policies we can create using TMC. Later,
    we saw how to find policy violations. Finally, we learned how to run conformance
    inspections for clusters and get their detailed reports. In addition to all these
    capabilities of TMC, there are a few other operations that we did not cover in
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Managing user access policies for cluster groups, clusters, Workspaces, and
    namespaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing various tools from a published catalog on a cluster managed by TMC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inspecting various events emitted by the clusters managed by TMC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing several administrative activities for TMC, including configuring
    TMC access permissions, external integrations, and proxy configuration, among
    other things
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To get more details about them, refer to this documentation link: https://docs.vmware.com/en/VMware-Tanzu-Mission-Control/index.html.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered a lot of ground in this chapter considering what TMC can do to minimize
    the effort of managing any distribution of upstream Kubernetes that could be running
    on different cloud environments. The high-level value proposition of this tool
    is to provide management control for multi-cloud, multi-cluster, and multi-team
    usage. First, we understood the different challenges of deployment approaches
    with a small number of large clusters or a large number of small clusters. Then,
    we saw use cases for TMC and the solutions it provides for complex problems when
    managing Kubernetes deployments. We discussed how challenging it can be to operate
    on different clusters deployed in different cloud environments and keep the operator
    and developer experience consistent across them. We saw how TMC makes this easy
    using cluster groups and Workspaces, a group of Kubernetes namespaces across different
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we learned how to get started with TMC, covering the way to integrate
    a TKG management cluster with TMC. Then, we discussed how to perform cluster lifecycle
    operations on that TKG foundation using TMC. After that, we saw how to create
    new workload clusters under the TKG control plane, which become a part of the
    TMC-managed clusters. Then, we also learned how to bring externally created or
    managed clusters such as GKE clusters into TMC and make them part of the cluster
    groups defined in TMC. We also learned how to create groups of Kubernetes namespaces,
    the Workspaces, for clusters that are deployed in two different cloud environments.
    After getting started with TMC, the first thing we learned was how to protect
    cluster data with the backup and restore capabilities of TMC. Then, we covered
    a long section on governing a fleet of clusters using various policies that we
    can apply either at the cluster group or Workspace level. Lastly, we covered how
    to inspect different clusters to take a proactive stance toward security.
  prefs: []
  type: TYPE_NORMAL
- en: As TMC is a single pane of glass for *controlling* large-scale Kubernetes deployments,
    VMware Aria operations for Applications is a single pane of glass for *observing*
    the scale of a Kubernetes deployment with its full-stack observability capabilities,
    starting from the application layer and moving to the infrastructure layer. In
    the next chapter, we will cover this topic in detail.
  prefs: []
  type: TYPE_NORMAL
