- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Managing and Controlling Kubernetes Clusters with Tanzu Mission Control
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Tanzu Mission Control 管理和控制 Kubernetes 集群
- en: In the previous section of the book, we covered the tools in the Tanzu portfolio
    that help us run cloud-native applications. We covered how Harbor can provide
    a secure home for your container images and how we can run those images using
    Tanzu Kubernetes Grid, which provides a uniform user experience across public
    and private cloud infrastructure. Finally, we took a deep dive into the area of
    developer productivity to automate and secure the software supply chain from an
    idea to a running application in production.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的上一节中，我们介绍了 Tanzu 产品组合中的工具，这些工具帮助我们运行云原生应用。我们介绍了 Harbor 如何为您的容器镜像提供安全的存储环境，并展示了如何使用
    Tanzu Kubernetes Grid 运行这些镜像，Tanzu Kubernetes Grid 在公共和私有云基础设施中提供一致的用户体验。最后，我们深入探讨了开发人员生产力领域，旨在自动化并保障软件供应链，从构思到生产环境中的应用程序运行。
- en: This section is about managing cloud-native apps and their corresponding Kubernetes
    infrastructure. To begin this section, in this chapter, we will learn about managing,
    securing, and governing a fleet of Kubernetes clusters of any flavor and on any
    infrastructure using **Tanzu Mission Control** (**TMC**). Followed by that, we
    will cover VMware Aria Operations for Applications, a tool to monitor every part
    of your running applications, including distributed tracing between microservices,
    application performance, Kubernetes objects, virtual infrastructure, and other
    services used by the applications. Finally, we will learn how to connect your
    apps running on Kubernetes and deploy them in different clusters and environments
    securely with out-of-the-box mutual TLS configuration using Tanzu Service Mesh.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容涉及管理云原生应用及其相应的 Kubernetes 基础设施。在本章中，我们将学习如何使用 **Tanzu Mission Control**（**TMC**）管理、保护和治理各种
    Kubernetes 集群，无论它们部署在什么样的基础设施上。接下来，我们将介绍 VMware Aria Operations for Applications，这是一款用于监控应用程序各个部分的工具，包括微服务之间的分布式追踪、应用性能、Kubernetes
    对象、虚拟基础设施以及应用使用的其他服务。最后，我们将学习如何通过 Tanzu Service Mesh 使用开箱即用的双向 TLS 配置，安全地连接运行在
    Kubernetes 上的应用并将其部署到不同的集群和环境中。
- en: Sidenote
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 附注
- en: Henceforth in this chapter, we will refer to a *Kubernetes cluster* just as
    a *cluster* for brevity.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章开始，为了简洁起见，我们将 Kubernetes 集群简称为 *集群*。
- en: 'With the background from the last section and our forward-looking statement
    for the upcoming chapters in this section, let’s begin our journey of understanding
    TMC in depth. We will cover the following topics in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上一节的背景知识和对本节接下来章节的展望，我们将深入了解 TMC。我们将在本章中涵盖以下主题：
- en: '*Why TMC?* – Understand the challenges around managing large Kubernetes environments
    and the solutions offered by Tanzu Mission Control'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为什么选择 TMC？* – 了解管理大规模 Kubernetes 环境面临的挑战以及 Tanzu Mission Control 提供的解决方案'
- en: '*Getting started with TMC* – Learn how to start using TMC to manage the cluster
    lifecycle'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*开始使用 TMC* – 学习如何使用 TMC 来管理集群生命周期'
- en: '*Protecting cluster data using TMC* – Learn how to back up clusters and namespaces,
    and how to restore them when required to protect running workloads from disasters'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 TMC 保护集群数据* – 学习如何备份集群和命名空间，并在需要时恢复它们，以保护正在运行的工作负载免受灾难的影响'
- en: '*Applying and ensuring governance policies on clusters using TMC* – Learn how
    to apply different cluster user governance policies to cluster and Kubernetes
    namespace groups and run inspections to find anomalies in clusters as a preventative
    security measure'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*应用和确保使用 TMC 进行集群治理策略* – 学习如何将不同的集群用户治理策略应用到集群和 Kubernetes 命名空间组，并进行检查以发现集群中的异常，从而采取预防性安全措施'
- en: TMC is a very powerful SaaS offering under the Tanzu portfolio that provides
    a single pane of control for all your Kubernetes environments. Let’s learn more
    about it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: TMC 是 Tanzu 产品组合中的一款非常强大的 SaaS 服务，它为您的所有 Kubernetes 环境提供了统一的控制面板。让我们更深入地了解它。
- en: Why TMC?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择 TMC？
- en: 'With its increasing popularity, Kubernetes is the new infrastructure layer.
    Just as 10 years back, almost every software used to run on the virtual infrastructure,
    in the next few years, almost every new application will probably be deployed
    on Kubernetes by default. In fact, most of the vendor-provided solutions are now
    available to run on Kubernetes. As per a survey done by the **Cloud Native Computing
    Foundation** (**CNCF**) in December 2021, over 5.6 million developers said they
    used Kubernetes to deploy their applications, which is a 67% increase in just
    1 year! Because of the array of business and technical benefits provided by Kubernetes,
    it is here to stay for a long time. Based on this CNCF survey, over 96% of organizations
    have embraced Kubernetes with a different level of maturity to run their cloud-native
    applications! And 73% of them have workloads running in production on Kubernetes
    already! As per a blog post by the CNCF in February 2022, the community is seeing
    its highest-ever adoption of this technology. Learn more about the details published
    by the CNCF here: https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着其受欢迎程度的不断上升，Kubernetes 已成为新的基础设施层。就像 10 年前，几乎所有的软件都运行在虚拟基础设施上一样，在未来几年，几乎所有新的应用程序可能会默认部署在
    Kubernetes 上。事实上，大多数厂商提供的解决方案现在都可以在 Kubernetes 上运行。根据 **Cloud Native Computing
    Foundation**（**CNCF**）2021 年 12 月进行的调查，超过 560 万开发者表示他们使用 Kubernetes 部署应用程序，这仅在
    1 年内就增长了 67%！由于 Kubernetes 提供的各种商业和技术优势，它将长期存在。根据这项 CNCF 调查，超过 96% 的组织已经采用 Kubernetes，并在不同的成熟度水平上运行其云原生应用程序！其中
    73% 的组织已经在生产环境中运行 Kubernetes 负载！根据 CNCF 2022 年 2 月的博客文章，社区正见证着该技术的历史最高采用率。了解更多
    CNCF 发布的详细信息，请点击这里：[https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/](https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/)。
- en: Challenges with Kubernetes
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 面临的挑战
- en: Even though Kubernetes is an extremely popular tool to run containerized applications,
    operating Kubernetes is difficult in a production-grade environment. As you may
    know about Kubernetes, it only talks via the `kubectl` command-line interface
    or using its REST APIs. Every configuration in Kubernetes is in YAML, which is
    often long and difficult to understand unless you have a good grasp of different
    Kubernetes constructs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Kubernetes 是一个极受欢迎的工具，用于运行容器化应用程序，但在生产环境中操作 Kubernetes 仍然非常困难。正如你可能知道的，Kubernetes
    仅通过 `kubectl` 命令行接口或使用其 REST API 进行操作。Kubernetes 中的每个配置都是 YAML 格式，通常很长且难以理解，除非你对
    Kubernetes 的各种构件有深刻的理解。
- en: Challenges with very large clusters
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面对超大集群的挑战
- en: 'A large cluster, in this context, is a cluster with more than 50 nodes in it.
    Adding to the basic complexity of using Kubernetes, running a production-grade
    platform has several other security and operational challenges. To reduce complexity
    and operational overhead, I have seen enterprises deploy very large clusters to
    maintain, operate, and secure only a few critical clusters. Very large Kubernetes
    deployments host applications from several different **lines of business** (**LOBs**)
    using the logical isolation provided by Kubernetes namespaces. At first, this
    approach may sound logical, but there are several drawbacks to this approach,
    as listed in the following points:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，超大集群是指包含超过 50 个节点的集群。除了使用 Kubernetes 的基本复杂性外，运行生产级平台还面临其他一些安全和操作挑战。为了减少复杂性和运营开销，我看到一些企业部署了超大集群，仅维护、操作和保护少数几个关键集群。超大
    Kubernetes 部署托管来自多个不同 **业务线**（**LOBs**）的应用程序，通过 Kubernetes 命名空间提供的逻辑隔离来实现。最初，这种做法可能听起来很合乎逻辑，但这种做法有几个缺点，如下所示：
- en: A large cluster serving multiple distinct LOBs and applications is very difficult
    to maintain, as all the applications and their LOBs will have different preferences
    in terms of the maintenance window and tolerance to any downtime. If an application
    is deployed with two or more Pods in a cluster, then it will mostly not face downtime.
    However, ensuring this level of compliance is a different challenge. Additionally,
    large clusters need a large maintenance window to complete activities such as
    upgrades.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个为多个不同业务线和应用程序服务的大型集群很难维护，因为所有应用程序及其业务线在维护窗口和对停机的容忍度方面都有不同的偏好。如果一个应用程序在一个集群中部署了两个或更多
    Pods，那么它通常不会面临停机。然而，确保达到这种合规性水平是一个不同的挑战。此外，大型集群需要较长的维护窗口来完成诸如升级等活动。
- en: When there are hundreds of apps running in a large cluster, they all get impacted
    together if the cluster faces any issue or downtime. The large clusters have large
    blast radii and large disaster impacts.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当在一个大型集群中运行数百个应用程序时，如果集群遇到任何问题或停机，它们都会一起受到影响。大型集群具有较大的冲击范围和灾难影响。
- en: As a large cluster is used by several different applications, the application
    teams do not get any freedom of choice to deploy and run their apps with a specific
    cluster setup and resource requirements such as using a specific operating system
    or using a GPU for compute needs. Although Kubernetes provides a way to deploy
    certain application Pods on certain nodes using constructs such as taints and
    tolerations, implementing, maintaining, and using this kind of setup is practically
    very difficult.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于一个大型集群被多个不同的应用程序使用，应用程序团队无法自由选择特定的集群设置和资源需求来部署和运行他们的应用程序，例如使用特定的操作系统或为计算需求使用GPU。尽管Kubernetes通过诸如污点（taints）和容忍度（tolerations）等构件提供了一种将某些应用程序Pods部署到特定节点的方法，但实施、维护和使用这种设置在实际操作中非常困难。
- en: Although Kubernetes isolates applications of different teams using namespaces,
    this is only a logical level of isolation. In reality, Pods belonging to different
    namespaces may run in the same node. This may create issues of potential security
    threats posed by a malicious actor running in the same cluster and possible resource
    starvation because of a “noisy neighbor” if the application has not requested
    and reserved required resources as a part of its deployment manifest.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管Kubernetes通过命名空间（namespaces）将不同团队的应用程序隔离开来，但这仅仅是逻辑层面的隔离。实际上，属于不同命名空间的Pods可能会在同一节点上运行。这可能会引发潜在的安全威胁问题，如果恶意行为者在同一个集群中运行，或者如果应用程序没有在其部署清单中请求并预留所需资源，就可能会因“吵闹的邻居”（noisy
    neighbor）问题而导致资源饥饿。
- en: Considering these challenges around large clusters, it is recommended to use
    multiple smaller clusters, especially for heterogeneous workloads. If all the
    apps running on a large cluster have the same requirements, then it should be
    okay; otherwise, the recommended approach is to create smaller clusters for different
    teams, LOBs, or applications.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到大型集群面临的这些挑战，推荐使用多个较小的集群，特别是对于异构工作负载。如果在一个大型集群上运行的所有应用程序具有相同的需求，那么应该是没问题的；否则，推荐的做法是为不同的团队、业务单元（LOB）或应用程序创建较小的集群。
- en: Challenges with many clusters and solutions from TMC
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多个集群的挑战及TMC的解决方案
- en: People generally prefer large clusters to avoid the operational overhead that
    increases in proportion to the number of clusters to be maintained. Hence, on
    one side, we may need several small clusters that belong to different teams, environments,
    and purposes, and on the other side, there are several operational and security
    challenges involved in keeping them functional. That is where TMC comes into the
    picture, which addresses these challenges by managing a fleet of clusters from
    a single pane of glass. Let’s understand briefly what those challenges are and
    how TMC helps to solve them.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 人们通常倾向于使用大型集群，以避免随着维护集群数量的增加而增加的操作开销。因此，一方面，我们可能需要多个属于不同团队、环境和目的的小集群，另一方面，也涉及到多个操作和安全挑战，这些挑战与保持这些集群正常运行有关。这正是TMC的作用所在，它通过从一个统一的控制面板管理多个集群来解决这些挑战。让我们简要了解一下这些挑战是什么，以及TMC如何帮助解决它们。
- en: Increased overhead of cluster lifecycle management
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 集群生命周期管理的开销增加
- en: Basic lifecycle management operations of several clusters, such as creating,
    scaling, upgrading, and deleting, could be a nightmarish situation without complete
    automation in place. Any manual intervention in this process could lead to configuration
    drifts resulting from basic human errors. This could soon result in a group of
    clusters having different configurations, and the need for cluster upgrades would
    be frequent for all the clusters, as the upstream Kubernetes releases a new version
    every 3 to 4 months and patch versions even more frequently. Regular maintenance
    of clusters via upgrades and patches is highly recommended to stay secure and
    supported. Such frequent maintenance of a large number of clusters requires a
    sophisticated automation setup. On the other hand, building and maintaining full
    end-to-end automation for these lifecycle processes require huge in-house effort.
    In that case, it’s a question of the organizations building this automation asking
    themselves whether they should invest in this much effort for below-value-line
    activities to support a container platform, or whether they would rather invest
    these resources to add new business functionalities to their applications, which
    would bring more revenue.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 多个集群的基本生命周期管理操作，如创建、扩展、升级和删除，在没有完全自动化的情况下可能会成为一场噩梦。在这个过程中的任何手动干预都可能导致由基本人为错误引起的配置漂移。这很快可能导致一组集群具有不同的配置，所有集群的升级需求将频繁发生，因为上游
    Kubernetes 每 3 到 4 个月发布一个新版本，而补丁版本则更加频繁。建议定期通过升级和补丁来维护集群，以保持安全性和支持。这种对大量集群的频繁维护需要一个复杂的自动化设置。另一方面，为了这些生命周期过程建立和维护完整的端到端自动化需要大量的内部努力。在这种情况下，组织建立这种自动化需要问自己是否应该投入这么多努力来支持容器平台的低价值线活动，还是宁愿将这些资源投入到为其应用程序添加新业务功能，从而带来更多收入。
- en: TMC addresses this challenge with the help of **Tanzu Kubernetes Grid** (**TKG**).
    In the previous chapter, we learned about TKG and its concept of a management
    control plane to manage the lifecycle of several workload clusters under that
    management cluster. TMC provides an out-of-the-box integration experience to link
    TKG management control plane clusters and allows TMC users to perform all TKG
    cluster lifecycle operations using the TMC portal. Such an easy, quick, and user-friendly
    approach to managing cluster lifecycles addresses the challenges involved in keeping
    the Kubernetes versions up to date for security reasons. Additionally, a complete
    out-of-the-box cluster lifecycle automation offering from TKG and its integration
    with TMC reduces the additional toil required to create and maintain in-house
    automation for the same reason.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: TMC 通过 **Tanzu Kubernetes Grid** (**TKG**) 的帮助来解决这一挑战。在前一章中，我们了解了 TKG 及其管理控制平面的概念，用于管理该管理集群下多个工作负载集群的生命周期。TMC
    提供了一个即插即用的集成体验，将 TKG 管理控制平面集群连接起来，并允许 TMC 用户通过 TMC 门户执行所有 TKG 集群生命周期操作。这种简单、快捷和用户友好的管理集群生命周期方法解决了保持
    Kubernetes 版本更新以确保安全性的挑战。此外，TKG 提供的完整即插即用集群生命周期自动化及其与 TMC 的集成减少了为同样的原因创建和维护内部自动化所需的额外努力。
- en: Distinct configuration requirements for different clusters
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同集群的特定配置要求
- en: Things get even more complex when different clusters have different configuration
    requirements. The security requirements for a group of non-production clusters
    would not be the same as the production clusters. Furthermore, the compliance
    requirements of the environments handling **Payment Card Industry** (**PCI**)
    data are even more stringent. Depending on the environment, the clusters may have
    different needs for user access, container network, workload isolation, and deployment
    policies. A security policy such as preventing the deployment of privileged containers
    that may allow root-level access to the host they are deployed on, or a policy
    ensuring high availability only allowing two more Pods to be deployed for an application
    are just some examples of these kinds of policies. It is difficult to find that
    one size that fits all clusters. Adding more flavors of clusters adds increasingly
    more complexity.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当不同集群具有不同的配置要求时，情况变得更加复杂。非生产集群组的安全要求与生产集群不同。此外，处理**支付卡行业**（**PCI**）数据环境的合规要求更为严格。根据环境不同，集群可能对用户访问、容器网络、工作负载隔离和部署策略有不同需求。例如，安全策略可能包括防止部署特权容器，这些容器可能允许对其部署主机进行根级访问，或者确保高可用性的策略只允许为应用程序部署两个以上的
    Pod 等。找到适合所有集群的一种解决方案很困难。添加更多类型的集群会增加更多的复杂性。
- en: To address this challenge, TMC allows us to create groups of clusters and then
    create different sets of policies and treatments for different groups of clusters.
    That way, the access policy of a development group of clusters could be different
    and more lenient than that of a group of production clusters. Additionally, TMC
    also allows you to create a group of Kubernetes namespaces, called **Workspaces**,
    which can span across the boundaries of clusters and cluster groups. Then, we
    can create policies for these Workspaces that are applicable at the namespace
    level. One such policy in Kubernetes is network policy defining, which applications/services
    may connect to which ones across different namespaces.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个挑战，TMC允许我们创建集群组，并为不同的集群组创建不同的策略和处理方法。这样，开发集群组的访问策略可能与生产集群组的访问策略不同且更为宽松。此外，TMC还允许您创建一组
    Kubernetes 命名空间，称为**工作空间**，可以跨集群和集群组的边界。然后，我们可以为这些工作空间创建适用于命名空间级别的策略。在 Kubernetes
    中，网络策略定义了哪些应用程序/服务可以跨不同命名空间连接到哪些应用程序/服务。
- en: Installation and configuration of tools
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 工具的安装和配置
- en: Every organization has a set of tools that they want to deploy on their clusters
    for cross-cutting concerns such as logging, monitoring, certificate management,
    **identity and access management** (**IAM**), and more. When different teams are
    in charge of the management of their own small clusters, it can become challenging
    to provide a self-service approach to the cluster owners where they can pick and
    deploy required tools on their clusters as and when required quickly and consistently.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 每个组织都有一套他们想要在集群上部署的工具，用于跨切面的关注点，如日志记录、监控、证书管理、**身份和访问管理**（**IAM**）等。当不同团队负责管理自己的小集群时，为集群所有者提供自助服务方法变得具有挑战性，他们可以根据需要快速和一致地选择和部署所需的工具。
- en: To address this point, TMC has a catalog of some popular open source packages
    that different cluster owners may install based on their requirements with a single
    click. This capability not only reduces the overhead for different teams but also
    allows you to instate a guardrail for other cluster owners so that only certain
    authorized packages can be installed on their clusters. As an additional advantage,
    all these packages can be installed using a common approach, and only for the
    published versions in the catalog.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一点，TMC拥有一些流行的开源包目录，不同的集群所有者可以根据其要求一键安装。这种能力不仅减少了不同团队的开销，还允许您为其他集群所有者设立防护栏，只有某些授权的包可以安装在其集群上。作为额外的优势，所有这些包都可以使用通用方法安装，并且仅限于目录中发布的版本。
- en: Cluster configuration and workload data protection
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 集群配置和工作负载数据保护
- en: Access to a quick and easy procedure to back up and restore cluster states is
    also an important concern for most of the clusters in an organization. For static
    workloads that do not use persistent storage volumes, backing up the Kubernetes
    configuration YAML files may work to restore these objects by applying those configurations
    again, but what if there was a delta in the running state versus the documented
    state? The restored data, in this case, would not be identical. Backup and restoration
    of the stateful workloads add more complexity, as we need to also back up the
    storage volumes used by those workloads along with their configuration YAML files.
    Additionally, performing these activities for many clusters to make sure this
    happens at the required frequency to lose minimal data asks for huge automation
    efforts. On the other side, if the ownership of backup and restoration is left
    to the teams responsible for each individual cluster, then either the ball will
    be dropped or there will be a standardization challenge in how this is done across
    different teams.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 快速而简便的集群状态备份和恢复程序，也是大多数组织内集群的重要关注点。对于不使用持久化存储卷的静态工作负载，通过备份Kubernetes配置的YAML文件可以恢复这些对象，方法是重新应用这些配置，但如果运行状态与文档状态存在差异怎么办？在这种情况下，恢复的数据将不完全相同。对于有状态的工作负载，备份和恢复会增加更多复杂性，因为我们不仅需要备份这些工作负载使用的存储卷，还要备份它们的配置YAML文件。此外，对于许多集群执行这些活动，确保按要求的频率进行以最小化数据丢失，需要巨大的自动化努力。另一方面，如果备份和恢复的责任交给负责各个集群的团队，那么要么会出现疏漏，要么会面临如何在不同团队之间标准化执行这一过程的挑战。
- en: To address this, TMC uses **Velero** (https://velero.io), an open source project
    backed by VMware, to back up and restore Kubernetes clusters. TMC users can either
    schedule cluster backups or do them on demand. These backups are saved in **Simple
    Storage Service** (**S3**)-compatible object storage locations accessible using
    web URLs. Taking, scheduling, and restoring backups using the TMC console is very
    easy and provides a consistent method and storage location to ensure the required
    data protection for everything running on a cluster, including stateless and stateful
    workloads. By means of Velero, TMC also allows you to only back up selected important
    namespaces if desired.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一需求，TMC使用**Velero**（https://velero.io），这是一个由VMware支持的开源项目，用于备份和恢复Kubernetes集群。TMC用户可以选择安排集群备份或按需进行备份。这些备份存储在**简单存储服务**（**S3**）兼容的对象存储位置，并通过Web
    URL进行访问。使用TMC控制台进行备份、调度和恢复非常简单，提供了一种一致的方法和存储位置，确保对集群上运行的所有内容（包括无状态和有状态的工作负载）提供所需的数据保护。通过Velero，TMC还允许你根据需要仅备份选定的重要命名空间。
- en: What is S3?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是S3？
- en: S3 is an object or file storage service offered by AWS. S3 has become a standard
    for object storage that implements the S3 interfaces required. Presently, there
    are several cloud-hosted and on-premises S3-compatible storage options available
    on the market in addition to the one offered by AWS, including MinIO and Dell
    **Elastic Cloud** **Storage** (**ECS**).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: S3是AWS提供的一种对象存储或文件存储服务。S3已经成为对象存储的标准，符合所需的S3接口。目前，除了AWS提供的S3服务外，市场上还有多个云托管和本地部署的S3兼容存储选项，包括MinIO和戴尔**弹性云**
    **存储**（**ECS**）。
- en: Conducting cluster inspections
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 执行集群检查
- en: When an organization has hundreds of clusters owned by multiple teams, it is
    a mammoth effort to inspect each cluster in terms of its security and operational
    policy compliance. Enterprises operating in highly secure domains such as healthcare
    and finance have formal obligations to audit their application platforms at a
    regular frequency to find compliance and address violations. It is almost impossible
    to audit and inspect several different clusters manually to see whether they follow
    required security compliance practices. It requires a great amount of automation
    to build a scanning engine that can check all the different rules on a checklist
    for a given cluster and then report violations of different severities. There
    is a list of recommendations from the official Kubernetes specification that a
    secure production-grade cluster should follow. Similarly, the **Center for Internet
    Security** (**CIS**) also has a benchmarking list of recommendations that should
    be followed in a cluster for security. Conducting different types of inspections
    for hundreds of clusters is a very difficult endeavor.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个组织拥有数百个由多个团队管理的集群时，检查每个集群的安全性和操作政策合规性是一项庞大的工作。在医疗保健和金融等高度安全领域运营的企业有正式的义务定期审计其应用平台，以查找合规性并处理违规行为。手动审计和检查多个不同的集群，以查看它们是否遵循必要的安全合规性做法几乎是不可能的。这需要大量的自动化来构建一个扫描引擎，该引擎能够检查给定集群的检查清单上的所有不同规则，然后报告不同严重性的违规行为。官方
    Kubernetes 规范中有一份安全生产级集群应遵循的推荐清单。类似地，**互联网安全中心**（**CIS**）也有一份基准测试推荐清单，应该在集群中遵循以确保安全。对数百个集群进行不同类型的检查是一项非常困难的任务。
- en: However, running inspections for clusters only takes a few clicks using TMC.
    To provide this feature, TMC uses **Sonobuoy** (https://sonobuoy.io), another
    open source project backed by VMware, providing Kubernetes cluster configuration
    validation. TMC allows you to run two different types of inspection – CIS benchmarking
    and Kubernetes specification conformance. Upon the completion of these inspections,
    the TMC user gets a detailed report of the compliance and violation of different
    recommendations. These results can help you take quick preventative measures to
    close vulnerable security loopholes before they are exploited.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用 TMC 进行集群检查只需几次点击。为了提供此功能，TMC 使用了**Sonobuoy**（https://sonobuoy.io），这是 VMware
    支持的另一个开源项目，提供 Kubernetes 集群配置验证。TMC 允许你执行两种不同类型的检查——CIS 基准测试和 Kubernetes 规范合规性检查。检查完成后，TMC
    用户将获得有关不同推荐合规性和违规的详细报告。这些结果可以帮助你采取快速的预防措施，在漏洞被利用之前关闭安全漏洞。
- en: 'Along with solving these challenges around managing a large group of clusters,
    TMC also emits critical events related to the clusters under its purview to keep
    their owners fully informed about their health and critical lifecycle stages.
    Additionally, TMC offers a comprehensive set of REST APIs to allow you to perform
    all these operations programmatically. Finally, as a major benefit, TMC can perform
    all these operations for any conformant Kubernetes flavor, including AWS **Elastic
    Kubernetes Service** (**EKS**), **Azure Kubernetes Service** (**AKS**), **Google
    Kubernetes Engine** (**GKE**), OpenShift, Rancher, open source upstream distributions,
    and many other types. This excludes the full cluster lifecycle management that
    we covered in the first point under *Increased overhead of cluster lifecycle management*.
    Upgrading Kubernetes clusters is only supported for TKG clusters at the time of
    writing. This capability makes TMC a great tool to implement a multi-cloud Kubernetes
    strategy for an enterprise. The following screenshot shows the TMC console with
    multiple Kubernetes clusters deployed on different cloud environments:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解决管理大量集群的挑战外，TMC 还会发出与其管辖下的集群相关的关键事件，以确保集群所有者充分了解它们的健康状况和关键生命周期阶段。此外，TMC 提供了一套全面的
    REST API，允许你以编程方式执行所有这些操作。最后，作为主要的优势，TMC 可以为任何符合规范的 Kubernetes 版本执行所有这些操作，包括 AWS
    **弹性 Kubernetes 服务**（**EKS**）、**Azure Kubernetes 服务**（**AKS**）、**Google Kubernetes
    引擎**（**GKE**）、OpenShift、Rancher、开源上游发行版以及许多其他类型。这不包括我们在*集群生命周期管理开销增加*一节中提到的完整集群生命周期管理。本文写作时，Kubernetes
    集群的升级仅支持 TKG 集群。此功能使 TMC 成为实施企业多云 Kubernetes 策略的优秀工具。下图显示了 TMC 控制台，其中部署了多个 Kubernetes
    集群，分别位于不同的云环境中：
- en: '![Figure 9.1 – TMC console with multiple Kubernetes clusters managed](img/B18145_09_01.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – TMC 控制台，管理多个 Kubernetes 集群](img/B18145_09_01.png)'
- en: Figure 9.1 – TMC console with multiple Kubernetes clusters managed
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 管理多个 Kubernetes 集群的 TMC 控制台
- en: With this, you should have gotten a convincible answer to the question, why
    TMC? We will get into the details of many of these capabilities later in this
    chapter, but for now, let’s see how to get started with TMC.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一点，你应该已经获得了一个令人信服的答案，为什么选择 TMC？我们将在本章稍后详细介绍这些功能的细节，但现在让我们看看如何开始使用 TMC。
- en: Getting started with TMC
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 TMC
- en: 'TMC is a **Software-as-a-Service** (**SaaS**) offering under VMware Cloud Services.
    Because of that, there is no installation and setup required to start using TMC,
    which is a big relief that SaaS products provide. In TMC, we can have two types
    of clusters, as described in the following points:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: TMC 是一个 **软件即服务** (**SaaS**) 产品，隶属于 VMware Cloud Services。由于这一点，开始使用 TMC 无需任何安装和设置，这也是
    SaaS 产品所带来的巨大便利。在 TMC 中，我们可以有两种类型的集群，如下所述：
- en: '**Clusters that are under a Kubernetes platform management control plane that
    is registered on TMC** – Presently, TMC only supports TKG clusters under this
    category. Once a TKG management cluster (a TKG platform control plane) running
    on vSphere, AWS, or Azure is registered in TMC, we can use the TMC interface to
    perform all lifecycle operations for all the clusters created under that management
    cluster.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**由 Kubernetes 平台管理控制平面管理的集群，这些集群已在 TMC 上注册** – 目前，TMC 仅支持此类别下的 TKG 集群。一旦在
    vSphere、AWS 或 Azure 上运行的 TKG 管理集群（TKG 平台控制平面）在 TMC 中注册，我们就可以使用 TMC 界面执行所有生命周期操作，管理该管理集群下创建的所有集群。'
- en: '**Clusters that are attached to TMC** – These can be any conformant Kubernetes
    clusters that are created externally. We can attach these clusters to TMC for
    several management activities discussed previously in this chapter, except for
    full lifecycle operations, such as creating, deleting, and upgrading. TMC offers
    a common management control plane for all Kubernetes clusters irrespective of
    their flavors and vendors.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**已连接到 TMC 的集群** – 这些可以是任何外部创建的符合规范的 Kubernetes 集群。我们可以将这些集群连接到 TMC，以进行本章之前讨论的几项管理活动，但不包括完全生命周期操作，如创建、删除和升级。TMC
    提供了一个公共的管理控制平面，适用于所有 Kubernetes 集群，无论其种类和供应商如何。'
- en: 'In this section, we will perform the following operations to get started with
    TMC:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将执行以下操作以开始使用 TMC：
- en: Accessing the TMC portal via the VMware Cloud Services console
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 VMware Cloud Services 控制台访问 TMC 门户
- en: Registering an existing TKG management cluster running on AWS
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册在 AWS 上运行的现有 TKG 管理集群
- en: Creating a TKG workload cluster under the registered management cluster
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在已注册的管理集群下创建 TKG 工作负载集群
- en: Attaching a GKE cluster for management
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接一个 GKE 集群进行管理
- en: Creating a cluster group
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个集群组
- en: Adding two associated clusters in the newly created cluster group
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在新创建的集群组中添加两个关联的集群
- en: Creating a Workspace, a group of cross-cluster Kubernetes namespaces
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个工作区，一个跨集群的 Kubernetes 命名空间组
- en: Adding two Kubernetes namespaces from two different clusters to the created
    Workspace
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从两个不同集群中添加两个 Kubernetes 命名空间到已创建的工作区
- en: 'However, before you can follow along, the following prerequisites must be fulfilled:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在你可以跟随操作之前，必须满足以下前提条件：
- en: Administrator-level access to TMC via a VMware Cloud Services account.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 VMware Cloud Services 账户访问 TMC 的管理员级别权限。
- en: An existing TKG management cluster (version 1.4.1 or later) with a production
    plan with at least three control plane nodes
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个现有的 TKG 管理集群（版本 1.4.1 或更高版本），并具有至少三个控制平面节点的生产计划
- en: An existing Kubernetes cluster not managed by the previously mentioned TKG management
    cluster – this could be a GKE, AKS, EKS, OpenShift, Rancher, or even an open source
    Kubernetes cluster
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由之前提到的 TKG 管理集群未管理的现有 Kubernetes 集群 – 这可以是 GKE、AKS、EKS、OpenShift、Rancher，甚至是开源
    Kubernetes 集群
- en: The cluster nodes should contain 4 vCPUs and 8 GB memory for the smooth execution
    of steps
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群节点应包含 4 个 vCPU 和 8 GB 内存，以确保顺利执行步骤
- en: A user workstation with an internet browser (preferably Google Chrome) and the
    `kubectl` CLI
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配备互联网浏览器（优选 Google Chrome）和 `kubectl` CLI 的用户工作站
- en: Full internet connectivity from the Kubernetes clusters without any proxy servers,
    as the procedure to configure TMC for different operations differs with a proxy
    in between, which we are not considering in this chapter
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 集群应具有完全的互联网连接，且没有任何代理服务器，因为在不同操作中配置 TMC 的步骤会因为代理的存在而不同，本章中不考虑这一点
- en: Access to either AWS S3 or any other S3-compatible object store service from
    the clusters linked with TMC.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从与 TMC 关联的集群访问 AWS S3 或任何其他 S3 兼容的对象存储服务。
- en: 'Additionally, if your TKG management cluster is running on AWS and if you have
    configured the IAM permissions defined in the CloudFormation stack used by TKG
    manually, then you must add the following listed permissions to the `nodes.tkg.cloud.vmware.com`
    IAM policy or role:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您的 TKG 管理集群运行在 AWS 上，并且您已经手动配置了在 TKG 使用的 CloudFormation 堆栈中定义的 IAM 权限，则必须将以下列出的权限添加到
    `nodes.tkg.cloud.vmware.com` IAM 策略或角色中：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: However, these permissions are included automatically when you create or update
    the CloudFormation stack by running the `tanzu mc permissions aws set` command.
    Once these prerequisites are addressed, we should be good for the rest of the
    chapter. Let’s start executing our plan to get started with TMC.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当您通过运行 `tanzu mc permissions aws set` 命令创建或更新 CloudFormation 堆栈时，这些权限会自动包含在内。一旦这些前提条件得到解决，剩下的步骤应该就没有问题了。让我们开始执行计划，着手启动
    TMC。
- en: Accessing the TMC portal
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问 TMC 门户
- en: 'Let’s first open the TMC portal via the VMware Cloud Services portal with the
    following steps:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先通过 VMware Cloud Services 门户按照以下步骤打开 TMC 门户：
- en: 'Visit the VMware Cloud Services portal at this URL: https://console.cloud.vmware.com/.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 VMware Cloud Services 门户，网址为：https://console.cloud.vmware.com/。
- en: 'Click on the **LAUNCH SERVICE** link on the **VMware Tanzu Mission Control**
    tile as shown in the following screenshot:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击**启动服务**链接，位于**VMware Tanzu Mission Control**磁贴上，如下图所示：
- en: '![Figure 9.2 – VMware Cloud Services console](img/B18145_09_02.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – VMware Cloud Services 控制台](img/B18145_09_02.jpg)'
- en: Figure 9.2 – VMware Cloud Services console
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – VMware Cloud Services 控制台
- en: 'That should open the TMC portal as shown in the following screen:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将打开如下图所示的 TMC 门户：
- en: '![Figure 9.3 – Tanzu Mission Control console](img/B18145_09_03.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – Tanzu Mission Control 控制台](img/B18145_09_03.jpg)'
- en: Figure 9.3 – Tanzu Mission Control console
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – Tanzu Mission Control 控制台
- en: Tip
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'If you are not entitled to TMC in your VMware Cloud Services account, then
    you will not see the TMC tile on the VMware Cloud Services console. You may need
    to request access for a TMC trial by getting in touch with your VMware contact
    point, or you can request access to TMC Starter via this URL: https://tanzu.vmware.com/tmc-starter.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在 VMware Cloud Services 账户中没有 TMC 权限，您将无法在 VMware Cloud Services 控制台中看到 TMC
    磁贴。您可能需要联系您的 VMware 联系人申请 TMC 试用访问，或者您可以通过以下 URL 请求访问 TMC Starter：https://tanzu.vmware.com/tmc-starter。
- en: Now, as we are on the TMC portal, let’s register our first TKG management cluster
    in TMC.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经进入 TMC 门户，接下来让我们在 TMC 中注册我们的第一个 TKG 管理集群。
- en: Registering a TKG management cluster on TMC
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TMC 中注册 TKG 管理集群
- en: 'Take the following steps on the TMC portal to register your existing TKG management
    cluster, running either on top of vSphere, AWS, or Azure cloud environments. The
    management cluster used in this chapter is deployed on top of AWS, which does
    not change any procedure in the following steps, except entering some configuration
    details for the workload cluster that we will create:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TMC 门户上执行以下步骤，以注册您现有的 TKG 管理集群， 无论其运行在 vSphere、AWS 还是 Azure 云环境上。本章中使用的管理集群部署在
    AWS 上，这不会改变以下步骤中的任何过程，只需要为我们将创建的工作负载集群输入一些配置信息：
- en: Registering a TKG management cluster on TMC
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TMC 中注册 TKG 管理集群
- en: 'Take the following steps on the TMC portal to register your existing TKG management
    cluster, running either on top of vSphere, AWS, or Azure cloud environments. The
    management cluster used in this chapter is deployed on top of AWS, which does
    not change any procedure in the following steps, except entering some configuration
    details for the workload cluster that we will create:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TMC 门户上执行以下步骤，以注册您现有的 TKG 管理集群， 无论其运行在 vSphere、AWS 还是 Azure 云环境上。本章中使用的管理集群部署在
    AWS 上，这不会改变以下步骤中的任何过程，只需要为我们将创建的工作负载集群输入一些配置信息：
- en: 'Click on the **Administration** menu option from the left navigation bar and
    open the **Management clusters** tab as shown in the following screenshot:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击左侧导航栏中的**管理**菜单选项，打开**管理集群**选项卡，如下图所示：
- en: '![Figure 9.4 – Opening the Management clusters screen](img/B18145_09_04.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – 打开管理集群屏幕](img/B18145_09_04.jpg)'
- en: Figure 9.4 – Opening the Management clusters screen
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 打开管理集群屏幕
- en: 'On the **Management clusters** tab, click on the **REGISTER MANAGEMENT CLUSTER**
    dropdown, and select the **Tanzu Kubernetes Grid** option as highlighted in the
    following screenshot:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**管理集群**选项卡中，单击**注册管理集群**下拉菜单，选择**Tanzu Kubernetes Grid**选项，如下图所示：
- en: '![Figure 9.5 – Selecting Tanzu Kubernetes Grid to register as a management
    cluster](img/B18145_09_05.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – 选择 Tanzu Kubernetes Grid 注册为管理集群](img/B18145_09_05.jpg)'
- en: Figure 9.5 – Selecting Tanzu Kubernetes Grid to register as a management cluster
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 选择 Tanzu Kubernetes Grid 注册为管理集群
- en: 'On the detail screen to register the management cluster, enter a unique name,
    select **default** from the drop-down list of groups, optionally add a small description,
    and finally, click on the **NEXT** button. We will discuss cluster groups in detail
    later in the chapter:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在注册管理集群的详细信息屏幕中，输入唯一的名称，从下拉列表中选择**默认（default）**，可选择性地添加简短描述，最后点击**下一步（NEXT）**按钮。我们将在本章稍后详细讨论集群组：
- en: '![Figure 9.6 – Entering management cluster details](img/B18145_09_06.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – 输入管理集群详细信息](img/B18145_09_06.jpg)'
- en: Figure 9.6 – Entering management cluster details
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – 输入管理集群详细信息
- en: 'Click on the **NEXT** button in the **Proxy Configuration** section, as we
    will not need it:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步（NEXT）**按钮，进入**代理配置（Proxy Configuration）**部分，因为我们不需要使用它：
- en: '![Figure 9.7 – Skipping proxy configuration](img/B18145_09_07.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7 – 跳过代理配置](img/B18145_09_07.jpg)'
- en: Figure 9.7 – Skipping proxy configuration
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 跳过代理配置
- en: 'You will be given a URL of a YAML file that contains the Kubernetes resources
    that you need to create for your management cluster to link it with your TMC account.
    Copy the URL as shown in the following screenshot. You may expand the **View YAML**
    section to see the details of the Kubernetes resources that will be created on
    your management cluster and their configuration:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将获得一个包含 Kubernetes 资源的 YAML 文件的 URL，这些资源是您需要为管理集群创建的，以便将其与 TMC 帐户连接。请按照以下屏幕截图所示复制该
    URL。您可以展开**查看 YAML**部分，查看将在管理集群上创建的 Kubernetes 资源及其配置详细信息：
- en: '![Figure 9.8 – Copying the registration URL to link to the management cluster](img/B18145_09_08.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8 – 复制注册 URL 以链接到管理集群](img/B18145_09_08.jpg)'
- en: Figure 9.8 – Copying the registration URL to link to the management cluster
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – 复制注册 URL 以链接到管理集群
- en: 'Click on the **VIEW MANAGEMENT CLUSTER** button to inspect the newly registered
    management cluster on TMC:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**查看管理集群（VIEW MANAGEMENT CLUSTER）**按钮，检查在 TMC 上新注册的管理集群：
- en: '![Figure 9.9 – Verifying the creation of the management cluster](img/B18145_09_09.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9 – 验证管理集群创建情况](img/B18145_09_09.jpg)'
- en: Figure 9.9 – Verifying the creation of the management cluster
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – 验证管理集群创建情况
- en: 'You should see the page as displayed in the following screenshot. The status
    of the cluster is **Unknown**, as we are yet to apply the registration YAML configuration
    to our management cluster:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该看到如下屏幕截图中显示的页面。集群的状态为**未知（Unknown）**，因为我们还没有将注册的 YAML 配置应用到我们的管理集群：
- en: '![Figure 9.10 – Management cluster status unknown](img/B18145_09_10.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.10 – 管理集群状态未知](img/B18145_09_10.jpg)'
- en: Figure 9.10 – Management cluster status unknown
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 – 管理集群状态未知
- en: Open your console window where you have access to the `kubectl` CLI on your
    workstation with the `kubectl` context pointing to the management cluster.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开控制台窗口，在您的工作站上使用`kubectl` CLI，并确保`kubectl`上下文指向管理集群。
- en: 'Run the following `kubectl apply` command using the URL copied in *step 4*:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在*步骤 4*中复制的 URL 运行以下`kubectl apply`命令：
- en: '[PRE1]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This should list a bunch of different Kubernetes resources created for your
    management cluster. Once completed, we have created a two-way link between the
    TKG management cluster and the TMC account. We should be able to see the management
    cluster successfully verified on TMC.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该列出为您的管理集群创建的多个不同 Kubernetes 资源。完成后，我们已经在 TKG 管理集群和 TMC 帐户之间建立了双向链接。我们应该能够在
    TMC 上看到成功验证的管理集群。
- en: 'After 5 to 10 minutes, either click on the **VERIFY CONNECTION** button as
    shown in *step 6*, or navigate to **Administration** | **Management clusters**
    to verify the successful registration of the management cluster. As you can see
    in the following screenshot, the cluster is **Healthy** and in a **Ready** state
    now. Click on the cluster name link as highlighted in the following screenshot:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过 5 到 10 分钟后，点击**验证连接（VERIFY CONNECTION）**按钮，如*步骤 6*所示，或导航到**管理** | **管理集群**，以验证管理集群是否已成功注册。如以下屏幕截图所示，集群现在处于**健康（Healthy）**状态，并且**已就绪（Ready）**。点击屏幕截图中高亮显示的集群名称链接：
- en: '![Figure 9.11 – Management cluster registration verification](img/B18145_09_11.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.11 – 管理集群注册验证](img/B18145_09_11.jpg)'
- en: Figure 9.11 – Management cluster registration verification
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 – 管理集群注册验证
- en: 'Once you click on the name of the management cluster as shown in the previous
    step, you will see something similar to the following screenshot showing the details
    of the management cluster, including the health indicators of different components
    running on the cluster:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦点击上一步中显示的管理集群名称，您将看到类似以下截图，显示管理集群的详细信息，包括集群中不同组件的健康指标：
- en: '![Figure 9.12 – Management cluster details](img/B18145_09_12.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.12 – 管理集群详细信息](img/B18145_09_12.jpg)'
- en: Figure 9.12 – Management cluster details
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 – 管理集群详细信息
- en: This concludes our steps to register a TKG management cluster in a TMC account
    using the TMC portal. In the next section, we will see how to use the TMC portal
    to create a new TKG workload cluster under the newly added management cluster.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是通过TMC门户注册TKG管理集群的步骤。在接下来的部分，我们将展示如何使用TMC门户在新添加的管理集群下创建一个新的TKG工作负载集群。
- en: Creating a new workload cluster under a management cluster
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在管理集群下创建新的工作负载集群
- en: After setting up the first TKG management cluster on TMC, let’s use this setup
    to create a new TKG workload cluster. As discussed in the previous chapter covering
    TKG in detail, a TKG workload cluster is used to run your containerized apps,
    whereas the management cluster is the control plane for several of these workload
    clusters.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在TMC上设置好第一个TKG管理集群后，我们将使用此设置来创建一个新的TKG工作负载集群。正如前一章详细讨论的，TKG工作负载集群用于运行容器化应用程序，而管理集群是多个工作负载集群的控制平面。
- en: All TKG workload clusters are created under a **provisioner**. A provisioner
    is a namespace within the management cluster that owns required workload clusters
    within the management cluster. This way, the provisioners in TKG provide a multi-tenancy
    construct to allow different teams to create and manage their own workload clusters
    without interfering with others. In our case, we will use the **default** namespace
    of the management cluster, which should be present by default.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所有TKG工作负载集群都在**provisioner**下创建。Provisioner是管理集群中的一个命名空间，负责管理管理集群中的所需工作负载集群。通过这种方式，TKG中的provisioner提供了一个多租户结构，允许不同的团队创建和管理自己的工作负载集群，而不会干扰其他团队。在我们的案例中，我们将使用管理集群的**default**命名空间，该命名空间应该是默认存在的。
- en: 'Take the following steps to create a new TKG workload cluster for your TKG
    management cluster:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤为您的TKG管理集群创建一个新的TKG工作负载集群：
- en: 'Go to the **Administration** menu of the left-hand navigation bar, open the
    **Management clusters** tab, and click on the **tkg-aws-mgmt-cluster** link to
    open its detail page:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往左侧导航栏的**Administration**菜单，打开**Management clusters**标签页，点击**tkg-aws-mgmt-cluster**链接以打开其详细页面：
- en: '![Figure 9.13 – Opening the management cluster page](img/B18145_09_13.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.13 – 打开管理集群页面](img/B18145_09_13.jpg)'
- en: Figure 9.13 – Opening the management cluster page
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 – 打开管理集群页面
- en: 'Go to the **Clusters** menu on the left-hand navigation bar and click on the
    **CREATE CLUSTER** button as highlighted in the following screenshot:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往左侧导航栏的**Clusters**菜单，点击以下截图中突出显示的**CREATE CLUSTER**按钮：
- en: '![Figure 9.14 – Creating a workload cluster](img/B18145_09_14.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.14 – 创建工作负载集群](img/B18145_09_14.jpg)'
- en: Figure 9.14 – Creating a workload cluster
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 – 创建工作负载集群
- en: 'Select **tkg-aws-mgmt-cluster** from the list that was registered on TMC earlier
    and click on the **CONTINUE TO CREATE** **CLUSTER** button:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从早前在TMC上注册的列表中选择**tkg-aws-mgmt-cluster**，然后点击**CONTINUE TO CREATE** **CLUSTER**按钮：
- en: '![Figure 9.15 – Continue creating a workload cluster](img/B18145_09_15.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.15 – 继续创建工作负载集群](img/B18145_09_15.jpg)'
- en: Figure 9.15 – Continue creating a workload cluster
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15 – 继续创建工作负载集群
- en: 'Select **default** from the **Provisioner** dropdown and click on **NEXT**:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**Provisioner**下拉菜单中选择**default**，然后点击**NEXT**按钮：
- en: '![Figure 9.16 – Selecting the provisioner for the workload cluster](img/B18145_09_16.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.16 – 选择工作负载集群的provisioner](img/B18145_09_16.jpg)'
- en: Figure 9.16 – Selecting the provisioner for the workload cluster
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16 – 选择工作负载集群的provisioner
- en: 'Enter the cluster name, optionally add a description, and click on the **NEXT**
    button:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入集群名称，选择性地添加描述，然后点击**NEXT**按钮：
- en: '![Figure 9.17 – Entering the workload cluster details](img/B18145_09_17.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.17 – 输入工作负载集群的详细信息](img/B18145_09_17.jpg)'
- en: Figure 9.17 – Entering the workload cluster details
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.17 – 输入工作负载集群的详细信息
- en: 'Enter the required infrastructure-specific details in this step and click on
    the **NEXT** button. As the TKG management cluster used in this chapter is running
    on AWS, the following screenshot shows AWS-specific details. For vSphere and Azure,
    some fields will be different. You can get more details about them here: https://docs.vmware.com/en/VMware-Tanzu-Mission-Control/services/tanzumc-using/GUID-42150344-CD4C-43AE-8C39-C059A97EF47C.html:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此步骤中输入所需的基础设施特定详细信息，并点击**下一步**按钮。由于本章中使用的TKG管理集群运行在AWS上，以下截图显示了AWS特定的详细信息。对于vSphere和Azure，某些字段会有所不同。你可以在这里获取更多详细信息：https://docs.vmware.com/en/VMware-Tanzu-Mission-Control/services/tanzumc-using/GUID-42150344-CD4C-43AE-8C39-C059A97EF47C.html：
- en: '![Figure 9.18 – Entering the workload cluster configuration](img/B18145_09_18.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.18 – 输入工作负载集群配置](img/B18145_09_18.jpg)'
- en: Figure 9.18 – Entering the workload cluster configuration
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18 – 输入工作负载集群配置
- en: 'Choose the control plane plan for the workload cluster to be created. You can
    also optionally change the values of the other fields, but this is not required
    and we recommend following the procedure in this chapter. Finally, click on the
    **NEXT** button to select the worker node details:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择要创建的工作负载集群的控制平面计划。你还可以选择性地更改其他字段的值，但这不是必需的，我们建议按照本章中的程序进行操作。最后，点击**下一步**按钮以选择工作节点详细信息：
- en: '![Figure 9.19 – Selecting the workload cluster type](img/B18145_09_19.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.19 – 选择工作负载集群类型](img/B18145_09_19.jpg)'
- en: Figure 9.19 – Selecting the workload cluster type
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19 – 选择工作负载集群类型
- en: 'Enter the count of the worker nodes for the workload cluster and click on the
    **CREATE** **CLUSTER** button:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入工作负载集群的工作节点数量，然后点击**创建集群**按钮：
- en: '![Figure 9.20 – Entering the worker node count](img/B18145_09_20.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.20 – 输入工作节点数量](img/B18145_09_20.jpg)'
- en: Figure 9.20 – Entering the worker node count
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.20 – 输入工作节点数量
- en: 'You will see a message saying that the workload cluster is being created with
    its specifications as shown in the following screen:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到一条消息，表示正在创建工作负载集群，并显示其规格，如下图所示：
- en: '![Figure 9.21 – Workload cluster being created](img/B18145_09_21.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.21 – 正在创建工作负载集群](img/B18145_09_21.jpg)'
- en: Figure 9.21 – Workload cluster being created
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.21 – 正在创建工作负载集群
- en: 'You will see a cluster detail screen like the following screenshot once the
    workload cluster is created successfully. It might take 5 to 10 minutes depending
    on the size and infrastructure:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦工作负载集群成功创建，你将看到如下截图所示的集群详细信息页面。根据集群的规模和基础设施，这可能需要5到10分钟：
- en: '![Figure 9.22 – Workload cluster detail page](img/B18145_09_22.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.22 – 工作负载集群详细页面](img/B18145_09_22.jpg)'
- en: Figure 9.22 – Workload cluster detail page
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.22 – 工作负载集群详细页面
- en: This concludes our third step, creating a TKG workload cluster using a TKG management
    cluster using the TMC portal. Now, we will attach an externally managed Kubernetes
    cluster to TMC so that we can perform various day-2 activities for that cluster
    using TMC.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的第三步，通过TMC门户使用TKG管理集群创建TKG工作负载集群。现在，我们将把一个外部管理的Kubernetes集群附加到TMC，以便使用TMC对该集群执行各种第二天操作。
- en: Attaching an existing Kubernetes cluster with TMC
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TMC附加现有Kubernetes集群
- en: 'In this section, we will attach an existing GKE cluster to the TMC account.
    You may select any other type of Kubernetes cluster, including Rancher, AKS, EKS,
    OpenShift, upstream open source, and more. TMC will allow you to attach a Kubernetes
    cluster as far as it is a conformant Kubernetes cluster with admin-level `kubectl`
    access for the cluster. So, let’s attach an external cluster:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们将把现有的GKE集群附加到TMC账户。你也可以选择其他类型的Kubernetes集群，包括Rancher、AKS、EKS、OpenShift、上游开源等。TMC允许你附加符合规范的Kubernetes集群，只要该集群具有管理员级别的`kubectl`访问权限。因此，让我们附加一个外部集群：
- en: 'Go to the **Clusters** menu from the left-hand menu bar and click on the **ATTACH
    CLUSTER** button as shown in the following screenshot:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从左侧菜单栏进入**集群**菜单，并点击下图所示的**附加集群**按钮：
- en: "![Figure 9.23 – Go\uFEFFing to the Attach cluster page](img/B18145_09_23.jpg)"
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.23 – 进入附加集群页面](img/B18145_09_23.jpg)'
- en: Figure 9.23 – Going to the Attach cluster page
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.23 – 进入附加集群页面
- en: 'Enter the cluster name, select the cluster group as **default**, optionally
    enter a description, and finally, click on the **NEXT** button as highlighted
    in the following screenshot:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入集群名称，选择集群组为**默认**，可选择性地输入描述，最后，点击下图中高亮显示的**下一步**按钮：
- en: '![Figure 9.24 – Entering the attached cluster details](img/B18145_09_24.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.24 – 输入附加集群的详细信息](img/B18145_09_24.jpg)'
- en: Figure 9.24 – Entering the attached cluster details
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.24 – 输入附加集群的详细信息
- en: 'Skip the proxy configuration and click on the **NEXT** button:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳过代理配置并点击**下一步**按钮：
- en: '![Figure 9.25 – Skipping the proxy configuration details](img/B18145_09_25.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.25 – 跳过代理配置详细信息](img/B18145_09_25.jpg)'
- en: Figure 9.25 – Skipping the proxy configuration details
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.25 – 跳过代理配置详细信息
- en: 'Just as we registered the management cluster using a Kubernetes YAML configuration
    file earlier in this chapter, it is now time to register the external cluster
    using a similar approach. You can view the details of the Kubernetes resources
    that will be created on the targeted cluster to establish two-way communication
    between the cluster and TMC:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像我们在本章前面使用 Kubernetes YAML 配置文件注册管理集群一样，现在是时候使用类似的方法注册外部集群了。你可以查看将在目标集群上创建的
    Kubernetes 资源的详细信息，以建立集群与 TMC 之间的双向通信：
- en: "![Figure \uFEFF9.26 – Copying the agent configuration command to attach the\
    \ cluster to TMC](img/B18145_09_26.jpg)"
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.26 – 复制代理配置命令以将集群附加到 TMC](img/B18145_09_26.jpg)'
- en: Figure 9.26 – Copying the agent configuration command to attach the cluster
    to TMC
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.26 – 复制代理配置命令以将集群附加到 TMC
- en: Open the command window where you have access to the cluster being attached
    via `kubectl` and change the `kubectl` context to point to the cluster being attached.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开命令窗口，在该窗口中你可以通过`kubectl`访问正在附加的集群，并将`kubectl`上下文更改为指向正在附加的集群。
- en: Run the `kubectl` command copied in *step 4* on your command window to create
    the required agent deployment resources in your cluster to attach to TMC.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令窗口中运行 *第4步* 中复制的`kubectl`命令，以在集群中创建所需的代理部署资源，进而附加到 TMC。
- en: 'After the successful execution of the `kubectl create` command, click on the
    **VERIFY CONNECTION** button. You should see the recently attached cluster in
    the list as shown in the following screenshot. Then, click on the attached cluster
    name to verify its details:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在成功执行`kubectl create`命令后，点击**验证连接**按钮。你应该能在列表中看到最近附加的集群，如下图所示。然后，点击附加集群的名称以验证其详细信息：
- en: '![Figure 9.27 – Verifying an attached cluster in the list](img/B18145_09_27.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.27 – 在列表中验证附加的集群](img/B18145_09_27.jpg)'
- en: Figure 9.27 – Verifying an attached cluster in the list
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.27 – 在列表中验证附加的集群
- en: 'Examine the details of the attached cluster. As you can see in the following
    screen, the type of the cluster is **Attached**:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查附加集群的详细信息。如以下屏幕所示，集群的类型为**附加**：
- en: '![Figure 9.28 – Attached cluster details](img/B18145_09_28.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.28 – 附加集群详细信息](img/B18145_09_28.jpg)'
- en: Figure 9.28 – Attached cluster details
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.28 – 附加集群详细信息
- en: This concludes our task of attaching an externally managed Kubernetes cluster
    to TMC. As the next steps, we will create a cluster group and add our TKG workload
    and GKE-attached clusters to that group.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们将外部管理的 Kubernetes 集群附加到 TMC 的任务完成。接下来的步骤是创建集群组，并将我们的 TKG 工作负载和 GKE 附加集群添加到该组中。
- en: Creating a cluster group on TMC
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TMC 上创建集群组
- en: 'As discussed earlier in this chapter, TMC allows you to group different clusters
    of similar natures and manage and handle them using common configurations. Configuring
    groups of several clusters makes the operation of large Kubernetes foundations
    very easy and efficient. So, let’s learn how to create cluster groups in TMC in
    this section:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章前面讨论的那样，TMC 允许你将具有相似性质的不同集群进行分组，并使用通用配置进行管理和操作。配置多个集群的组可以使大型 Kubernetes
    基础设施的操作变得非常简便和高效。那么，接下来我们将学习如何在 TMC 中创建集群组：
- en: 'Click on the **Cluster groups** menu item from the left-hand navigation bar
    and click on the **CREATE CLUSTER GROUP** button as shown in the following screenshot:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击左侧导航栏中的**集群组**菜单项，然后点击**创建集群组**按钮，如下图所示：
- en: '![Figure 9.29 – Creating cluster groups](img/B18145_09_29.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.29 – 创建集群组](img/B18145_09_29.jpg)'
- en: Figure 9.29 – Creating cluster groups
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.29 – 创建集群组
- en: 'Enter the cluster group name, add an optional description, and click the **CREATE**
    button as shown in the following screenshot:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入集群组名称，添加可选描述，然后点击**创建**按钮，如下图所示：
- en: '![Figure 9.30 – Entering cluster group details](img/B18145_09_30.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.30 – 输入集群组详细信息](img/B18145_09_30.jpg)'
- en: Figure 9.30 – Entering cluster group details
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.30 – 输入集群组详细信息
- en: 'You should see the newly created cluster group in the list as shown in the
    following screenshot:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该能在列表中看到新创建的集群组，如下图所示：
- en: '![Figure 9.31 – Verifying the presence of the cluster group](img/B18145_09_31.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.31 – 验证集群组的存在](img/B18145_09_31.jpg)'
- en: Figure 9.31 – Verifying the presence of the cluster group
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.31 – 验证集群组的存在
- en: Now, let’s add our two clusters to this newly created group using the following
    steps.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下步骤将这两个集群添加到这个新创建的集群组中。
- en: 'Click on the **default** cluster group as shown in the following screenshot:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击如下截图所示的**默认**集群组：
- en: '![Figure 9.32 – Selecting the cluster group as default](img/B18145_09_32.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.32 – 选择默认集群组](img/B18145_09_32.jpg)'
- en: Figure 9.32 – Selecting the cluster group as default
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.32 – 选择默认集群组
- en: 'Select the two clusters we have on TMC, for the TKG workload and GKE, and click
    on the **MOVE** button as shown in the following screenshot:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择我们在 TMC 上的两个集群，分别用于 TKG 工作负载和 GKE，然后点击如以下截图所示的**移动**按钮：
- en: '![Figure 9.33 – Selecting clusters for grouping](img/B18145_09_33.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.33 – 选择用于分组的集群](img/B18145_09_33.jpg)'
- en: Figure 9.33 – Selecting clusters for grouping
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.33 – 选择用于分组的集群
- en: 'Select the cluster group we created in *step 2* and click on the **MOVE** button:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择我们在*步骤 2* 中创建的集群组并点击**移动**按钮：
- en: '![Figure 9.34 – Selecting the cluster group](img/B18145_09_34.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.34 – 选择集群组](img/B18145_09_34.jpg)'
- en: Figure 9.34 – Selecting the cluster group
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.34 – 选择集群组
- en: 'Go to the **Cluster groups** page and click on the new cluster group we created
    to examine the presence of the two clusters we added in the previous step:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入**集群组**页面，点击我们创建的新集群组，检查我们在上一步中添加的两个集群的存在：
- en: '![Figure 9.35 – Getting into the new cluster group](img/B18145_09_35.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.35 – 进入新集群组](img/B18145_09_35.jpg)'
- en: Figure 9.35 – Getting into the new cluster group
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.35 – 进入新的集群组
- en: 'As you can see in the following screenshot, both clusters that we created in
    this section are in the cluster group:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下截图所示， 在本节中我们创建的两个集群都在集群组中：
- en: '![Figure 9.36 – Listed clusters in the cluster group](img/B18145_09_36.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.36 – 集群组中的列出集群](img/B18145_09_36.jpg)'
- en: Figure 9.36 – Listed clusters in the cluster group
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.36 – 集群组中的列出集群
- en: Grouping clusters with similar requirements is a very powerful way to manage
    hundreds of clusters with heterogeneous requirements. Along with groups of clusters,
    TMC also allows you to group Kubernetes namespaces to perform certain namespace-level
    configurations applicable to more than one namespace in different clusters. Let’s
    learn more about it.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 将具有相似需求的集群进行分组是一种非常强大的方法，能够管理数百个具有异构需求的集群。除了集群组，TMC 还允许你将 Kubernetes 命名空间进行分组，以执行适用于多个集群中不同命名空间的某些命名空间级配置。让我们深入了解一下。
- en: Understanding Workspaces in TMC
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 TMC 中的工作区
- en: As we discussed before in this chapter, TMC allows you to group clusters of
    similar natures. Grouping helps us create a common configuration and policies
    for them for operational efficiency. However, certain policies in Kubernetes can
    only be applied at the namespace level, such as network policies that define which
    Pods from one namespace can talk to which Pods in another namespace. Additionally,
    we can also configure cluster user access policies at the namespace level, and
    TMC also allows us to create image registry access policies at the namespace level.
    While applying cluster-level policies to a cluster group is an easy task, applying
    namespace-level policies for individual namespaces could be very impractical.
    Moreover, a way to apply namespace-level policies to a group of namespaces within
    a cluster would also not help much if we were trying to manage hundreds of Kubernetes
    clusters. For these reasons, TMC offers a construct called **Workspaces**, which
    allows us to create a group of namespaces across different clusters – then, we
    can create a namespace-level policy applicable to the entire Workspace and hence
    for all the namespaces within the same Workspace. A multi-cloud application that
    is deployed in different clusters might need similar configurations for either
    user access or network policy configuration. For a Workspace containing all the
    namespaces of this application and running different clusters, it would come in
    very handy to treat them as a unit.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面所讨论的，TMC 允许你将具有相似性质的集群进行分组。分组有助于我们为它们创建共同的配置和策略，从而提高运营效率。然而，Kubernetes
    中的某些策略只能在命名空间级别应用，例如定义一个命名空间中的 Pod 可以与另一个命名空间中的 Pod 通信的网络策略。此外，我们还可以在命名空间级别配置集群用户访问策略，TMC
    也允许我们在命名空间级别创建镜像仓库访问策略。虽然将集群级别的策略应用于集群组是一个简单的任务，但为单个命名空间应用命名空间级别的策略可能非常不实用。此外，如果我们试图管理数百个
    Kubernetes 集群，尝试将命名空间级别的策略应用于集群中一组命名空间也不会有太大帮助。基于这些原因，TMC 提供了一种名为**工作区**的构造，它允许我们在不同的集群中创建一组命名空间——然后，我们可以创建一个适用于整个工作区的命名空间级别的策略，从而适用于同一工作区中的所有命名空间。在不同集群中部署的多云应用程序可能需要类似的用户访问或网络策略配置。对于包含此应用程序所有命名空间并运行不同集群的工作区，将它们作为一个单位进行管理会非常方便。
- en: 'Let’s create two namespaces in the two clusters we have added in the previous
    section of this chapter and then create a Workspace to add those two namespaces
    into:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在本章前一节中添加的两个集群中创建两个命名空间，然后创建一个工作区，将这两个命名空间添加到其中：
- en: 'Open the **Workspaces** menu from the left-hand navigation bar and click on
    the **CREATE WORKSPACE** button as shown in the following screenshot:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从左侧导航栏中打开**工作区**菜单，并点击**创建工作区**按钮，如下截图所示：
- en: '![Figure 9.37 – Creating a Workspace](img/B18145_09_37.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.37 – 创建工作区](img/B18145_09_37.jpg)'
- en: Figure 9.37 – Creating a Workspace
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.37 – 创建工作区
- en: 'Enter a name, optionally add a description, and click on the **CREATE** button
    on the page as shown in the following screenshot:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入名称，任选地添加描述，并点击页面上的**创建**按钮，如下截图所示：
- en: '![Figure 9.38 – Entering Workspace details](img/B18145_09_38.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.38 – 输入工作区详情](img/B18145_09_38.jpg)'
- en: Figure 9.38 – Entering Workspace details
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.38 – 输入工作区详情
- en: 'As you can see in the following screenshot, a new Workspace has been created.
    Click on the Workspace name to ensure there are no namespaces listed in there:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下截图所示，一个新的工作区已经创建。点击工作区名称，确保其中没有列出任何命名空间：
- en: "![Figure \uFEFF9.39 – Verifying that the Workspace has been created](img/B18145_09_39.jpg)"
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.39 – 验证工作区是否已创建](img/B18145_09_39.jpg)'
- en: Figure 9.39 – Verifying that the Workspace has been created
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.39 – 验证工作区是否已创建
- en: 'Open the **tmc-demo-clusters** cluster group that we previously created on
    the TMC portal as shown in the following screenshot:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开我们之前在 TMC 门户上创建的**tmc-demo-clusters**集群组，如下截图所示：
- en: "![Figure \uFEFF9.40 – Opening the cluster group](img/B18145_09_40.jpg)"
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.40 – 打开集群组](img/B18145_09_40.jpg)'
- en: Figure 9.40 – Opening the cluster group
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.40 – 打开集群组
- en: 'Click on the TKG workload cluster as shown in the following screenshot:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 TKG 工作负载集群，如下截图所示：
- en: "![Figure \uFEFF9.41 – Opening a cluster detail page](img/B18145_09_41.jpg)"
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.41 – 打开集群详情页面](img/B18145_09_41.jpg)'
- en: Figure 9.41 – Opening a cluster detail page
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.41 – 打开集群详情页面
- en: 'Click on the **Namespaces** tab of the cluster detail page, select the **default**
    namespace, and click on the **ATTACH 1 NAMESPACE** button as shown in the following
    screenshot:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击集群详情页面的**命名空间**选项卡，选择**default**命名空间，然后点击**ATTACH 1 NAMESPACE**按钮，如下图所示：
- en: "![Figure \uFEFF9.42 – Selecting a namespace to attach to a Workspace](img/B18145_09_42.jpg)"
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.42 – 选择要附加到工作区的命名空间](img/B18145_09_42.jpg)'
- en: Figure 9.42 – Selecting a namespace to attach to a Workspace
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.42 – 选择要附加到工作区的命名空间
- en: 'Select or enter the Workspace name and click on the **ATTACH** button as shown
    in the following screenshot:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择或输入工作区名称，然后点击**ATTACH**按钮，如下图所示：
- en: "![Figure \uFEFF9.43 – Selecting the Workspace to attach](img/B18145_09_43.jpg)"
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.43 – 选择要附加的工作区](img/B18145_09_43.jpg)'
- en: Figure 9.43 – Selecting the Workspace to attach
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.43 – 选择要附加的工作区
- en: 'As you can see in the following screenshot, the **default** namespace is now
    under **tmc-demo-workspace**, which we created in the previous step:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下图所示，你可以看到**default**命名空间现在位于**tmc-demo-workspace**下，我们在之前的步骤中创建了这个工作区：
- en: "![Figure \uFEFF9.44 – Verifying the namespace-Workspace association](img/B18145_09_44.jpg)"
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.44 – 验证命名空间与工作区的关联](img/B18145_09_44.jpg)'
- en: Figure 9.44 – Verifying the namespace-Workspace association
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.44 – 验证命名空间与工作区的关联
- en: Repeat *steps 5* to *8* from this list to add the **default** namespace to this
    new Workspace for the other cluster, **gke-cluster-1**, as per this chapter, which
    we previously attached to TMC and added to the **tmc-demo-clusters** group.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照本章节的说明，重复*步骤 5* 到 *8*，将**default**命名空间添加到此新工作区中，适用于另一个集群**gke-cluster-1**，我们之前已将其附加到
    TMC 并加入了**tmc-demo-clusters**组。
- en: 'Open the **Workspaces** menu from the left-hand navigation bar, and click on
    the new Workspace, **tmc-demo-workspace**, that we created earlier:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从左侧导航栏打开**工作区**菜单，点击我们之前创建的新的工作区**tmc-demo-workspace**：
- en: "![Figure \uFEFF9.45 – Opening Workspace details](img/B18145_09_45.jpg)"
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.45 – 打开工作区详情](img/B18145_09_45.jpg)'
- en: Figure 9.45 – Opening Workspace details
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.45 – 打开工作区详情
- en: 'You should be able to see two default namespaces from two different clusters
    listed as a part of the new Workspace we created in this part as shown in the
    following screenshot:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该能够看到来自两个不同集群的两个默认命名空间，它们作为我们在本部分创建的新工作区的一部分列出，如下图所示：
- en: "![Figure \uFEFF9.46 – Verifying namespaces in the Workspace](img/B18145_09_46.jpg)"
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.46 – 验证工作区中的命名空间](img/B18145_09_46.jpg)'
- en: Figure 9.46 – Verifying namespaces in the Workspace
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.46 – 验证工作区中的命名空间
- en: With this, we have covered how to create Workspaces and how to attach existing
    namespaces to different clusters in the Workspace. In the example that we covered
    earlier, we used the default namespaces with the same name as the clusters that
    were in the same cluster group in TMC. However, you can add any namespaces to
    a Workspace irrespective of the namespace names and the group status of their
    parent clusters.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经涵盖了如何创建工作区以及如何将现有命名空间附加到工作区中的不同集群。在我们之前讨论的示例中，我们使用了与 TMC 中同一集群组内集群名称相同的默认命名空间。然而，你可以将任何命名空间添加到工作区，无论命名空间名称和其父集群的组状态如何。
- en: With this, we conclude all the steps that we planned to cover under this part
    of the chapter – *Getting started with TMC*. We learned how to register a TKG
    management cluster in TMC and created a TKG workload cluster using the TMC interface.
    Then, we also attached an externally managed GKE cluster to TMC. Finally, we learned
    how to group clusters and Kubernetes namespaces to perform common operations on
    them as a unit. In the next part of the chapter, we will learn about making and
    restoring cluster data backups using TMC.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经完成了本章节这一部分的所有步骤——*开始使用 TMC*。我们学习了如何在 TMC 中注册 TKG 管理集群，并使用 TMC 界面创建了一个
    TKG 工作负载集群。然后，我们还将一个外部管理的 GKE 集群附加到 TMC。最后，我们学习了如何将集群和 Kubernetes 命名空间分组，以便将它们作为一个单位执行常见操作。在章节的下一部分，我们将学习如何使用
    TMC 进行集群数据的备份和恢复。
- en: Protecting cluster data using TMC
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TMC 保护集群数据
- en: Kubernetes is widely used to run business-critical applications in production
    environments. In these cases, a reliable disaster recovery mechanism should be
    present to make regular cluster data and configuration backups and restore them
    in the event of data loss for any reason. Although Kubernetes is mostly used to
    run stateless workloads where the persistent data is stored outside the clusters
    in the databases, running stateful software, such as caches, queues, and databases,
    is also being adopted slowly. In [*Chapter 6*](B18145_06.xhtml#_idTextAnchor112),
    *Managing Container Images with Harbor*, the Harbor registry deployment used the
    data stores that were deployed on Kubernetes itself. That makes backing up data
    even more important.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 广泛用于在生产环境中运行业务关键应用程序。在这些情况下，应该有可靠的灾难恢复机制来定期进行集群数据和配置备份，并在任何原因导致数据丢失时恢复它们。尽管
    Kubernetes 主要用于运行无状态工作负载，其中持久化数据存储在集群外部的数据库中，但运行有状态的软件，如缓存、队列和数据库，正逐渐被采纳。在[*第
    6 章*](B18145_06.xhtml#_idTextAnchor112)，《使用 Harbor 管理容器镜像》中，Harbor 注册表部署使用的是部署在
    Kubernetes 上的数据存储。这使得备份数据变得更加重要。
- en: 'So, to cover this important topic, we will learn how to make cluster backups
    and restore them using TMC with the following high-level steps:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为了涵盖这个重要主题，我们将学习如何通过 TMC 进行集群备份和恢复，步骤如下：
- en: Configure an S3-compatible remote backup storage location.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置一个兼容 S3 的远程备份存储位置。
- en: Configure a cluster to use the remote storage location for backup data.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置集群使用远程存储位置来备份数据。
- en: Deploy a custom application to the cluster.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将自定义应用部署到集群。
- en: Take a backup of the cluster.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对集群进行备份。
- en: Delete the cluster namespace with the custom deployment to simulate a disaster.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除集群命名空间及其中的自定义部署，以模拟灾难。
- en: Restore the cluster backup.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 恢复集群备份。
- en: Verify the presence of the deleted namespace and the custom deployment within
    it post-restoration.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在恢复后，验证已删除的命名空间和其中的自定义部署是否存在。
- en: Let’s start the work to execute these steps one by one.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始逐步执行这些步骤。
- en: Configuring the backup target location
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置备份目标位置
- en: TMC allows you to create separate backup target locations for separate cluster
    groups. Configuring a backup location for a cluster group is a one-time administrative
    activity. Once a target location is associated with a cluster group, we can make
    either on-demand or scheduled backups for any cluster in the cluster group using
    the backup location configuration. Additionally, one configured backup location
    can be used by many cluster groups. In this chapter, we will use the AWS S3 option
    as the storage location.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: TMC 允许你为不同的集群组创建独立的备份目标位置。为集群组配置备份位置是一次性的管理活动。一旦目标位置与集群组关联，我们就可以使用该备份位置配置为集群组中的任何集群进行按需或定期备份。此外，一个配置好的备份位置可以被多个集群组使用。在本章中，我们将使用
    AWS S3 选项作为存储位置。
- en: 'The following steps describe how to configure a backup target location for
    a cluster group in TMC:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤描述了如何在 TMC 中为集群组配置备份目标位置：
- en: 'Create your AWS account credentials for TMC to use to store backup data in
    an S3 bucket:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建你的 AWS 账户凭证，供 TMC 用于将备份数据存储在 S3 桶中：
- en: 'Open the **Administration** menu from the left-hand navigation bar and click
    on the **CREATE ACCOUNT CREDENTIAL** button as shown in the following screenshot:'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从左侧导航栏打开**管理**菜单，并点击**创建账户凭证**按钮，如下图所示：
- en: "![Figure \uFEFF9.47 – Creating AWS account credentials](img/B18145_09_47.jpg)"
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.47 – 创建 AWS 账户凭证](img/B18145_09_47.jpg)'
- en: Figure 9.47 – Creating AWS account credentials
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.47 – 创建 AWS 账户凭证
- en: 'Select the **AWS S3** option from the **TMC provisioned** **storage** option:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**TMC 配置的** **存储**选项中选择**AWS S3**选项：
- en: "![Figure \uFEFF9.48 – Selecting provisioned storage](img/B18145_09_48.jpg)"
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.48 – 选择已配置的存储](img/B18145_09_48.jpg)'
- en: Figure 9.48 – Selecting provisioned storage
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.48 – 选择已配置的存储
- en: 'Enter a name for the account credentials and click on the **GENERATE** **TEMPLATE**
    button:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入账户凭证名称并点击**生成** **模板**按钮：
- en: "![Figure \uFEFF9.49 – Entering a credential name and generating a template](img/B18145_09_49.jpg)"
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.49 – 输入凭证名称并生成模板](img/B18145_09_49.jpg)'
- en: Figure 9.49 – Entering a credential name and generating a template
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.49 – 输入凭证名称并生成模板
- en: 'A credential template file should be downloaded. This is an AWS CloudFormation
    template that creates the required S3 buckets and grants required permissions
    to TMC to access that bucket for backup and restoration operations. Save that
    template file on your local workstation and click on the **NEXT** button:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该下载一个凭证模板文件。这是一个 AWS CloudFormation 模板，用于创建所需的 S3 存储桶，并授予 TMC 所需的权限，以便访问该存储桶进行备份和恢复操作。将该模板文件保存在本地工作站上，并点击
    **下一步** 按钮：
- en: "![Figure \uFEFF9.50 – Downloading the generated AWS CloudFormation template](img/B18145_09_50.jpg)"
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.50 – 下载生成的 AWS CloudFormation 模板](img/B18145_09_50.jpg)'
- en: Figure 9.50 – Downloading the generated AWS CloudFormation template
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.50 – 下载生成的 AWS CloudFormation 模板
- en: 'Follow the **QUICKSTART GUIDE** link to apply the downloaded template to your
    AWS account. Once the CloudFormation template is applied, you will get the **Amazon
    Resource Name** (**ARN**) in the output of the template execution, which will
    be required in the next step. You will find these instructions in the quick start
    guide as shown in the following screenshot:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照 **快速入门指南** 链接，将下载的模板应用到你的 AWS 帐户中。应用 CloudFormation 模板后，你将在模板执行的输出中获得 **Amazon
    资源名称** (**ARN**)，该 ARN 将在下一步中使用。你可以在快速入门指南中找到这些指令，如下图所示：
- en: "![Figure \uFEFF9.51 – Following the quick start guide to create the required\
    \ AWS S3 objects](img/B18145_09_51.jpg)"
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.51 – 按照快速入门指南创建所需的 AWS S3 对象](img/B18145_09_51.jpg)'
- en: Figure 9.51 – Following the quick start guide to create the required AWS S3
    objects
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.51 – 按照快速入门指南创建所需的 AWS S3 对象
- en: 'Apply the copied ARN from AWS after applying the CloudFormation template and
    click the **CREATE** button to finally create the account credentials that we
    can use for the backup and restoration operations for any cluster:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在应用 CloudFormation 模板后，从 AWS 中复制 ARN 并点击 **创建** 按钮，最终创建我们可以用于任何集群的备份和恢复操作的帐户凭证：
- en: "![Figure \uFEFF9.52 – Applying the copied ARN and creating the credentials](img/B18145_09_52.jpg)"
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.52 – 应用复制的 ARN 并创建凭证](img/B18145_09_52.jpg)'
- en: Figure 9.52 – Applying the copied ARN and creating the credentials
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.52 – 应用复制的 ARN 并创建凭证
- en: 'Upon successful creation of the account credentials, you should see it listed
    under the **Accounts** tab of the **Administration** menu as shown in the following
    screenshot:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在帐户凭证成功创建后，你应该能在 **管理** 菜单下的 **帐户** 标签中看到它，如下图所示：
- en: "![Figure \uFEFF9.53 – Verifying account credential creation](img/B18145_09_53.jpg)"
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.53 – 验证帐户凭证的创建](img/B18145_09_53.jpg)'
- en: Figure 9.53 – Verifying account credential creation
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.53 – 验证帐户凭证的创建
- en: 'Go to the **Target locations** tab under the **Administration** menu and click
    on **CREATE TARGET LOCATION** for the AWS S3 option as per the following screenshot:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到 **管理** 菜单下的 **目标位置** 标签，并点击 **创建目标位置**，选择 AWS S3 选项，如下图所示：
- en: "![Figure \uFEFF9.54 – Creating the target location](img/B18145_09_54.jpg)"
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.54 – 创建目标位置](img/B18145_09_54.jpg)'
- en: Figure 9.54 – Creating the target location
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.54 – 创建目标位置
- en: 'Select the account credentials that we created in *step 1* of this section
    and click on the **NEXT** button:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择我们在本节 *第 1 步* 中创建的帐户凭证，并点击 **下一步** 按钮：
- en: "![Figure \uFEFF9.55 – Selecting account credentials](img/B18145_09_55.jpg)"
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.55 – 选择帐户凭证](img/B18145_09_55.jpg)'
- en: Figure 9.55 – Selecting account credentials
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.55 – 选择帐户凭证
- en: 'Select the cluster group that we created earlier in this chapter and click
    on the **NEXT** button:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择我们在本章早些时候创建的集群组，并点击 **下一步** 按钮：
- en: "![Figure \uFEFF9.56 – Adding the cluster group to the target location](img/B18145_09_56.jpg)"
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.56 – 将集群组添加到目标位置](img/B18145_09_56.jpg)'
- en: Figure 9.56 – Adding the cluster group to the target location
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.56 – 将集群组添加到目标位置
- en: 'Enter the name of the target location and click on the **CREATE** button:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入目标位置的名称并点击 **创建** 按钮：
- en: "![Figure \uFEFF9.57 – Entering the target location name](img/B18145_09_57.jpg)"
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.57 – 输入目标位置名称](img/B18145_09_57.jpg)'
- en: Figure 9.57 – Entering the target location name
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.57 – 输入目标位置名称
- en: 'You should see a new target location created under the **Target locations**
    tab of the **Administration** menu along with the account credentials and the
    relevant S3 bucket name as shown in the following screenshot:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该能在 **管理** 菜单下的 **目标位置** 标签中看到新创建的目标位置，以及帐户凭证和相关的 S3 存储桶名称，如下图所示：
- en: "![Figure \uFEFF9.58 – Verifying target location creation](img/B18145_09_58.jpg)"
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.58 – 验证目标位置的创建](img/B18145_09_58.jpg)'
- en: Figure 9.58 – Verifying target location creation
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.58 – 验证目标位置的创建
- en: Now, as we have set up a backup target location and assigned it to the cluster
    group, we can proceed to perform the backup and restoration operations for a cluster
    in that group.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然我们已经设置了备份目标位置并将其分配给集群组，我们可以继续对该组中的集群执行备份和恢复操作。
- en: Enabling data protection for a cluster
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用集群的数据保护
- en: 'After completing the steps required to perform backup and restoration for a
    cluster group, let’s enable it for one of the clusters in the group. In this example,
    we will use the externally managed GKE cluster that we had attached with TMC.
    Take the following steps to complete this task:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 完成为集群组执行备份和恢复所需的步骤后，我们来为组中的一个集群启用它。在本示例中，我们将使用之前附加到 TMC 的外部管理 GKE 集群。执行以下步骤完成此任务：
- en: 'Click on the attached cluster, **gke-cluster-1**, in the **tmc-demo-clusters**
    group. If you have used names other than the ones used in this chapter, you will
    need to select the cluster appropriately:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击附加的集群**gke-cluster-1**，位于**tmc-demo-clusters**组中。如果你使用了本章中未提到的名称，你需要相应地选择集群：
- en: "![Figure \uFEFF9.59 – Opening the attached cluster details page](img/B18145_09_59.jpg)"
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.59 – 打开附加集群的详情页面](img/B18145_09_59.jpg)'
- en: Figure 9.59 – Opening the attached cluster details page
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.59 – 打开附加集群的详情页面
- en: 'Click on the **ENABLE DATA PROTECTION** link as highlighted in the following
    screenshot:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击如下面截图中突出显示的**启用数据保护**链接：
- en: "![Figure \uFEFF9.60 – Clicking on the ENABLE DATA PROTECTION link](img/B18145_09_60.jpg)"
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.60 – 点击启用数据保护链接](img/B18145_09_60.jpg)'
- en: Figure 9.60 – Clicking on the ENABLE DATA PROTECTION link
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.60 – 点击启用数据保护链接
- en: 'Click on the **ENABLE** button to confirm the operation:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**启用**按钮以确认操作：
- en: "![Figure \uFEFF9.61 – Enabling data protection for the cluster](img/B18145_09_61.jpg)"
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.61 – 启用集群的数据保护](img/B18145_09_61.jpg)'
- en: Figure 9.61 – Enabling data protection for the cluster
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.61 – 启用集群的数据保护
- en: This operation deploys Velero, an open source Kubernetes cluster backup and
    restoration tool. With this, we have prepared the cluster to take backups with
    the required toolset deployed on it. Now, let’s deploy a test workload in the
    cluster.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 该操作部署了 Velero，这是一个开源的 Kubernetes 集群备份和恢复工具。通过这个操作，我们已经为集群准备好了备份功能，并且部署了所需的工具集。现在，让我们在集群中部署一个测试工作负载。
- en: Deploying a custom application in the cluster
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在集群中部署自定义应用
- en: 'Execute the following steps to run an nginx deployment on the cluster we enabled
    for backup and restoration in the previous task:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤，在我们之前任务中启用备份和恢复的集群上运行 nginx 部署：
- en: 'While in the targeted cluster’s `kubectl` context, run the following command,
    which creates a namespace named `nginx` and a Kubernetes deployment named `nginx-deployment`
    with three Pods running. You can check the deployment manifest file used in the
    following command to get more details:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在目标集群的`kubectl`上下文中，运行以下命令，这会创建一个名为`nginx`的命名空间，并在其中创建一个名为`nginx-deployment`的
    Kubernetes 部署，部署三个 Pod。你可以查看以下命令中使用的部署清单文件，以获取更多详细信息：
- en: '[PRE2]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Verify the deployment using the TMC portal under the **Workloads** tab of the
    cluster details page. As you can see in the following screen, the nginx deployment
    and ReplicaSet are running with a **Healthy** status. To minimize the clutter
    on the screen, you can also filter Tanzu and Kubernetes-specific workloads using
    the switches as highlighted:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TMC 门户在集群详情页的**工作负载**标签下验证部署。如以下屏幕所示，nginx 部署和副本集都在**健康**状态下运行。为了减少屏幕上的杂乱，你还可以使用突出显示的切换按钮过滤
    Tanzu 和 Kubernetes 特定的工作负载：
- en: "![Figure \uFEFF9.62 – Verifying workload deployment for a cluster](img/B18145_09_62.jpg)"
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.62 – 验证集群的工作负载部署](img/B18145_09_62.jpg)'
- en: Figure 9.62 – Verifying workload deployment for a cluster
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.62 – 验证集群的工作负载部署
- en: Now, we have a workload deployed in its own namespace on a target cluster that
    can be backed up. Let’s make a backup of the cluster using the following steps.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在目标集群的自己的命名空间中部署了一个可以备份的工作负载。让我们使用以下步骤对集群进行备份。
- en: Backing up a cluster
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备份集群
- en: 'Take the following steps to take a backup of the cluster where we deployed
    the nginx workload in the previous task:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤备份我们在上一个任务中部署 nginx 工作负载的集群：
- en: 'On the cluster’s **Overview** tab, click on the **CREATE BACKUP** link as highlighted
    in the following screenshot:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在集群的**概览**标签页中，点击如下面截图中突出显示的**创建备份**链接：
- en: "![Figure \uFEFF9.63 – Initiating the cluster backup process](img/B18145_09_63.jpg)"
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.63 – 启动集群备份过程](img/B18145_09_63.jpg)'
- en: Figure 9.63 – Initiating the cluster backup process
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.63 – 启动集群备份过程
- en: 'Select the option to make a backup of the entire cluster and click on the **NEXT**
    button. We can also make a backup only of selected namespaces or selected objects
    identified using a label value, which is a very flexible choice:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择备份整个集群的选项，并点击**NEXT**按钮。我们也可以仅备份选定的命名空间或通过标签值标识的对象，这是一个非常灵活的选择：
- en: "![Figure \uFEFF9.64 – Selecting the backup scope](img/B18145_09_64.jpg)"
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.64 – 选择备份范围](img/B18145_09_64.jpg)'
- en: Figure 9.64 – Selecting the backup scope
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.64 – 选择备份范围
- en: 'Select the backup target location that we created previously in this chapter
    pointing to an AWS S3 bucket and click on the **NEXT** button:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择我们在本章中之前创建的指向AWS S3桶的备份目标位置，并点击**NEXT**按钮：
- en: "![Figure \uFEFF9.65 – Selecting the backup target location](img/B18145_09_65.jpg)"
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.65 – 选择备份目标位置](img/B18145_09_65.jpg)'
- en: Figure 9.65 – Selecting the backup target location
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.65 – 选择备份目标位置
- en: 'Select the backup schedule. While we can create a regular backup schedule,
    here, we will select **NOW** to make an on-demand backup to learn about the concept.
    Click on the **NEXT** button to move on after that:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择备份计划。虽然我们可以创建定期的备份计划，但在这里，我们将选择**NOW**进行按需备份，以了解这一概念。然后点击**NEXT**按钮继续：
- en: "![Figure \uFEFF9.66 – Selecting the backup schedule](img/B18145_09_66.jpg)"
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.66 – 选择备份计划](img/B18145_09_66.jpg)'
- en: Figure 9.66 – Selecting the backup schedule
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.66 – 选择备份计划
- en: 'Enter the backup **Retention (days)** value and click on the **NEXT** button:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入备份**保留天数**的值并点击**NEXT**按钮：
- en: "![Figure \uFEFF9.67 – Entering the backup retention days](img/B18145_09_67.jpg)"
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.67 – 输入备份保留天数](img/B18145_09_67.jpg)'
- en: Figure 9.67 – Entering the backup retention days
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.67 – 输入备份保留天数
- en: 'Finally, enter the backup’s name and click on the **CREATE** button as shown
    in the following screenshot:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，输入备份的名称，并点击**CREATE**按钮，如下图所示：
- en: "![Figure \uFEFF9.68 – Entering the backup name](img/B18145_09_68.jpg)"
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.68 – 输入备份名称](img/B18145_09_68.jpg)'
- en: Figure 9.68 – Entering the backup name
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.68 – 输入备份名称
- en: 'This will trigger the backup process and within 2 to 5 minutes, you should
    be able to see the backup completed under the **Data protection** tab of the cluster
    as shown in the following screenshot. The backup process may take more time if
    the cluster has other workloads running:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将触发备份过程，在2到5分钟内，你应该能在集群的**数据保护**标签页下看到备份完成的状态。如果集群中有其他工作负载在运行，备份过程可能需要更长时间：
- en: "![Figure \uFEFF9.69 – Verifying backup completion](img/B18145_09_69.jpg)"
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.69 – 验证备份完成](img/B18145_09_69.jpg)'
- en: Figure 9.69 – Verifying backup completion
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.69 – 验证备份完成
- en: After successfully making a backup of the cluster, let’s now restore it – but
    before we restore it, let’s *accidentally* delete the **nginx** namespace that
    contains the nginx deployment we created earlier before making the backup.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功备份集群后，接下来我们将进行恢复操作——但在恢复之前，让我们*不小心*删除包含之前创建的nginx部署的**nginx**命名空间。
- en: Deleting a custom deployment running on the cluster
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 删除集群中运行的自定义部署
- en: 'Take the following steps to perform this task:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤执行此任务：
- en: 'Run the following command to delete the **nginx** namespace from the targeted
    cluster:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令删除目标集群中的**nginx**命名空间：
- en: '[PRE3]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Verify the absence of the nginx deployment in the **Workloads** tab of the
    cluster. As you can see, **nginx-deployment** and its corresponding ReplicaSet
    that we verified previously are now missing:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在集群的**工作负载**标签页中验证nginx部署的缺失。如图所示，我们之前验证过的**nginx-deployment**及其相应的ReplicaSet现在已经缺失：
- en: "![Figure \uFEFF9.70 – Verifying the workload deletion](img/B18145_09_70.jpg)"
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.70 – 验证工作负载删除](img/B18145_09_70.jpg)'
- en: Figure 9.70 – Verifying the workload deletion
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.70 – 验证工作负载删除
- en: Now, as we have *accidentally* deleted the entire namespace and the workloads
    running in that namespace to simulate a disaster situation, let’s use the backup
    we made for the cluster to restore the deleted namespace and get its objects back
    up and running.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于我们*不小心*删除了整个命名空间及其中运行的工作负载，以模拟灾难情景，让我们使用之前为集群创建的备份来恢复已删除的命名空间并使其对象重新运行。
- en: Restoring the cluster backup
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 恢复集群备份
- en: 'Take the following steps to restore the backup of the targeted cluster and
    bring back the deleted Kubernetes objects in that cluster:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤恢复目标集群的备份，并将已删除的Kubernetes对象恢复到该集群中：
- en: 'Go to the **Data protection** tab of the cluster, select the backup that we
    took previously from the **Backups** list, and click on the **RESTORE** link as
    highlighted in the following screenshot:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到集群的**数据保护**选项卡，从**备份**列表中选择之前所做的备份，并点击**恢复**链接，如下图所示：
- en: "![Figure \uFEFF9.71 – Restoring a backup](img/B18145_09_71.jpg)"
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![图9.71 – 恢复备份](img/B18145_09_71.jpg)'
- en: Figure 9.71 – Restoring a backup
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.71 – 恢复备份
- en: 'Since we had only one namespace deleted, we will just restore that one by selecting
    the highlighted option to restore specific namespaces. Select the **nginx** namespace
    from the list of what we need to restore. Here, TMC also allows us to restore
    the source namespace from the backup to a different target namespace if that is
    intended. This can be done using the little pencil icon beside the target namespace
    caption. After selecting the namespace, click on the **NEXT** button:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们只删除了一个命名空间，我们只需选择高亮显示的选项以恢复特定的命名空间。选择需要恢复的**nginx**命名空间。在这里，TMC还允许我们将备份中的源命名空间恢复到不同的目标命名空间（如果需要）。可以使用目标命名空间标题旁边的小铅笔图标进行此操作。选择命名空间后，点击**下一步**按钮：
- en: "![Figure \uFEFF9.72 – Selecting the Restore backup specification](img/B18145_09_72.jpg)"
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![图9.72 – 选择恢复备份规范](img/B18145_09_72.jpg)'
- en: Figure 9.72 – Selecting the Restore backup specification
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.72 – 选择恢复备份规范
- en: 'Enter a name for the restored instance and click on the **RESTORE** button:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入恢复实例的名称并点击**恢复**按钮：
- en: "![Figure \uFEFF9.73 – Entering a restore instance name](img/B18145_09_73.jpg)"
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![图9.73 – 输入恢复实例名称](img/B18145_09_73.jpg)'
- en: Figure 9.73 – Entering a restore instance name
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.73 – 输入恢复实例名称
- en: 'Upon successful restoration, you will see a restored entry in the **Data protection**
    tab of the cluster under the **Restores** section as shown in the following screenshot:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成功恢复后，您将在集群的**数据保护**选项卡下的**恢复**部分看到恢复的条目，如下图所示：
- en: "![Figure \uFEFF9.74 – Verifying that the restoration has completed](img/B18145_09_74.jpg)"
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![图9.74 – 验证恢复是否已完成](img/B18145_09_74.jpg)'
- en: Figure 9.74 – Verifying that the restoration has completed
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.74 – 验证恢复是否已完成
- en: 'Verify the presence of the **nginx** namespace and its objects under the **Workloads**
    tab of the cluster details page. As you can see, the **nginx** namespace has now
    been restored successfully:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在集群详细页面的**工作负载**选项卡下，验证**nginx**命名空间及其对象的存在。如您所见，**nginx**命名空间现已成功恢复：
- en: '![Figure 9.75 – Verifying the presence of the restored objects ](img/B18145_09_75.jpg)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![图9.75 – 验证恢复对象的存在](img/B18145_09_75.jpg)'
- en: Figure 9.75 – Verifying the presence of the restored objects
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.75 – 验证恢复对象的存在
- en: With this, we can conclude our very long but important section on backing up
    and restoring Kubernetes clusters. It is worth noting that several configurations
    made in this section were one-time activities. This includes creating AWS S3 account
    credentials, creating a target location of the backup, and associating a target
    location with a cluster group, among other things. Once this setup is done, backing
    up and restoring a cluster or a part of the cluster only takes a few clicks.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 通过此步骤，我们可以总结关于备份和恢复Kubernetes集群的漫长而重要的一节。值得注意的是，本节中进行的几项配置是一次性活动。这包括创建AWS S3账户凭证、创建备份的目标位置、将目标位置与集群组关联等。完成这些设置后，备份和恢复集群或集群的一部分仅需几次点击。
- en: Let’s now look into another very important capability of TMC – policy management.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看TMC的另一个非常重要的功能——策略管理。
- en: Applying governance policies to clusters using TMC
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TMC应用治理策略到集群
- en: 'In the previous section of the chapter, we learned how to get started with
    TMC by registering a TKG management cluster, creating a TKG workload cluster,
    attaching a GKE cluster, and finally, grouping them – but why do we bring all
    the clusters to TMC? In this section, we will check this out by performing various
    activities with these clusters using the TMC interface. We will cover the following
    activities:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前一部分中，我们学习了如何通过注册TKG管理集群、创建TKG工作负载集群、附加GKE集群以及最后进行集群分组来开始使用TMC——但我们为什么要将所有集群引入TMC？在本节中，我们将通过使用TMC界面执行这些集群的各种活动来探讨这个问题。我们将涵盖以下活动：
- en: Configuring a security policy for a cluster group
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为集群组配置安全策略
- en: Configuring an image registry governance policy for a Workspace
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为工作区配置镜像注册表治理策略
- en: Configuring a deployment governance policy for a cluster group
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为集群组配置部署治理策略
- en: Checking policy violation status for clusters
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查集群的策略违规状态
- en: Inspecting a cluster for CIS benchmark compliance
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查集群是否符合 CIS 基准
- en: This is a long list of activities to cover in this section. Let’s knock them
    off one by one.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容涵盖了一长串活动，让我们一个个完成它们。
- en: Configuring a security policy for a cluster group
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为集群组配置安全策略
- en: When it comes to running containers, several things can be misconfigured from
    a security point of view, which keeps the door open for hackers to leverage these.
    Depending on the nature of the workloads running on the cluster, a Kubernetes
    administrator may need to secure several things. In this world of microservices,
    the Kubernetes platform team often needs to allow different teams to deploy their
    apps with their own configurations required by the apps. However, ensuring that
    all the teams follow the required security practices outlined by the platform
    team can be a very difficult task. That is why Kubernetes administrators need
    to guardrail their clusters so that they do not allow workloads to be deployed
    in unsecured ways. To address this need, Kubernetes offers a construct called
    **PodSecurityPolicy**, which defines what a Pod can and cannot do in the Kubernetes
    cluster with a PodSecurityPolicy in effect. TMC allows you to configure these
    security policies for a group of clusters so that there is no chance of configuration
    drifts between clusters.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行容器时，从安全角度来看，有几件事情可能会配置错误，这会给黑客留下利用这些漏洞的机会。根据集群中运行的工作负载的性质，Kubernetes 管理员可能需要保护多个方面。在微服务的世界中，Kubernetes
    平台团队通常需要允许不同的团队部署其应用程序，并按应用程序要求配置相关设置。然而，确保所有团队遵循平台团队所概述的必要安全实践可能是一个非常困难的任务。这就是为什么
    Kubernetes 管理员需要为集群设置保护措施，以防止工作负载以不安全的方式部署。为了解决这个需求，Kubernetes 提供了一种名为 **PodSecurityPolicy**
    的构造，它定义了在 PodSecurityPolicy 生效时，Pod 在 Kubernetes 集群中可以做什么以及不能做什么。TMC 允许你为一组集群配置这些安全策略，从而避免集群之间出现配置漂移。
- en: 'Let’s create one such security policy for the cluster group that we previously
    created. This policy will prevent any Pod from gaining privileged access to the
    Kubernetes node’s operating system and resources. The following steps will help
    create and test this policy:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为之前创建的集群组创建一个这样的安全策略。此策略将防止任何 Pod 获得对 Kubernetes 节点操作系统和资源的特权访问。以下步骤将帮助创建并测试此策略：
- en: 'Ensure that a privileged access Pod can be deployed before applying the policy,
    taking the following substeps:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保在应用策略之前，可以部署具有特权访问的 Pod，按照以下子步骤进行操作：
- en: 'Deploy a privileged Pod in the TKG workload cluster under the `tmc-demo-clusters`
    group using the following command. You can check the Pod definition file used
    in the following command – notice that the security context is defined there to
    enable the Pod to get privileged access:'
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令，在 `tmc-demo-clusters` 组下的 TKG 工作负载集群中部署特权 Pod。你可以查看以下命令中使用的 Pod 定义文件——注意其中定义了安全上下文，以便允许
    Pod 获取特权访问：
- en: '[PRE4]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Verify whether the Pod has been created and is running successfully. Here,
    it is assumed that there is no security policy in place that would prevent a privileged
    Pod from being created:'
  id: totrans-377
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证 Pod 是否已创建并成功运行。这里假设没有任何安全策略阻止特权 Pod 的创建：
- en: '[PRE5]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After verifying that we can run a privileged Pod in the cluster, let’s now
    create a security policy in TMC for the cluster group using the following substeps:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在验证我们可以在集群中运行特权 Pod 后，现在让我们使用以下子步骤在 TMC 中为集群组创建安全策略：
- en: 'Open the **Policies** > **Assignments** menu from the left-hand navigation
    bar and the **Security** tab. Then, click on the cluster group, **tmc-demo-clusters**,
    and click on the **CREATE SECURITY POLICY** button as highlighted in the following
    screenshot:'
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从左侧导航栏的 **策略** > **分配** 菜单以及 **安全性** 选项卡中打开。然后，点击集群组 **tmc-demo-clusters**，并点击如下截图中突出显示的
    **创建安全策略** 按钮：
- en: "![Figure \uFEFF9.76 – Creating a security policy for a cluster group](img/B18145_09_76.jpg)"
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.76 – 为集群组创建安全策略](img/B18145_09_76.jpg)'
- en: Figure 9.76 – Creating a security policy for a cluster group
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.76 – 为集群组创建安全策略
- en: 'TMC allows you to fully customize your security requirements by defining a
    custom security policy from scratch. However, it also provides two out-of-the-box
    choices: **Baseline** and **Strict**. Let’s select **Baseline** from the **Security
    template** dropdown. You will notice that it restricts the creation of privileged
    containers as highlighted in the following screenshot. Finally, click on the **CREATE
    POLICY** button to apply the restrictions to the relevant cluster group:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TMC 允许您通过从头开始定义自定义安全策略来完全自定义您的安全要求。它还提供了两种现成的选择：**基础**和**严格**。让我们从**安全模板**下拉菜单中选择**基础**。您会注意到，它限制了特权容器的创建，如以下截图所示。最后，点击**创建策略**按钮，将限制应用于相关集群组：
- en: "![Figure \uFEFF9.77 – Entering the security policy configuration](img/B18145_09_77.jpg)"
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.77 – 进入安全策略配置](img/B18145_09_77.jpg)'
- en: Figure 9.77 – Entering the security policy configuration
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.77 – 进入安全策略配置
- en: 'You will see the policy listed for the cluster group as shown in the following
    screenshot:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将看到集群组的策略列表，如以下截图所示：
- en: "![Figure \uFEFF9.78 – Verifying security policy creation](img/B18145_09_78.jpg)"
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.78 – 验证安全策略创建](img/B18145_09_78.jpg)'
- en: Figure 9.78 – Verifying security policy creation
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.78 – 验证安全策略创建
- en: 'Let’s now verify whether the policy has been applied to the clusters in the
    group. Take the following substeps to do so:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们验证该策略是否已应用于组中的集群。请按照以下子步骤进行操作：
- en: 'First, delete the Pod we created in the first step in the TKG workload cluster
    by running the following command:'
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，通过运行以下命令删除我们在第一步中在 TKG 工作负载集群中创建的 Pod：
- en: '[PRE8]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create the same Pod again by running the following command:'
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令，再次创建相同的 Pod：
- en: '[PRE9]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You will see an error message as follows, explaining that the Pod could not
    successfully be created because of the security policy in place:'
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将看到如下错误信息，说明由于安全策略的原因，Pod 未能成功创建：
- en: '[PRE10]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That concludes our learning on how to configure a Kubernetes cluster security
    policy applicable to deploying and running workloads using TMC. We tested one
    of the clusters in the cluster group to which we applied the policy, but you can
    also do the same exercise for another cluster in the group and should see similar
    test results. Like the one explained here for running privileged containers, we
    can create several different types of security policies using TMC for a cluster
    group. Let’s now learn how to apply a Workspace-level policy that defines how
    the workloads running in the specific cluster namespaces can pull container images
    from a container registry.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们关于如何配置适用于使用 TMC 部署和运行工作负载的 Kubernetes 集群安全策略的学习。我们测试了在应用该策略的集群组中的一个集群，但您也可以对组中另一个集群进行相同的操作，并应看到类似的测试结果。就像这里解释的运行特权容器的例子一样，我们可以使用
    TMC 为集群组创建多种不同类型的安全策略。接下来，我们将学习如何应用一个工作区级别的策略，定义在特定集群命名空间中运行的工作负载如何从容器注册表拉取容器镜像。
- en: Configuring an image registry policy for a Workspace
  id: totrans-399
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为工作区配置镜像注册表策略
- en: 'When it comes to pulling container images from a container registry, several
    proven practices are recommended. The restrictions applicable to pulling images
    for regulatory environments could be more stringent. The following are the parameters
    that TMC allows you to configure for a container registry policy depending on
    different compliance requirements. A registry policy in TMC may either have all
    or some parameters applicable as required:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 在拉取容器镜像时，建议采用一些经过验证的最佳实践。对于受监管环境的镜像拉取限制，可能会更为严格。以下是 TMC 根据不同的合规要求，允许您为容器注册表策略配置的参数。TMC
    中的注册表策略可能根据需要具有所有或某些适用的参数：
- en: '**Pulling images only using a digest (SHA) and not with a tag** – This is an
    important rule to set up for a production environment, as a tag could technically
    have different content for different pull instances, whereas the content of an
    image for the same digest will always be the same. Pulling images using their
    digest will give you the confidence that they will always have the same bits inside.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅使用摘要（SHA）拉取镜像，而不使用标签** – 这是在生产环境中设置的重要规则，因为标签在技术上可能会为不同的拉取实例提供不同的内容，而同一摘要的镜像内容始终是相同的。使用摘要拉取镜像可以确保它们始终具有相同的内容。'
- en: '**Pulling images only from a certain image repository** – If there is a requirement
    that a container can only pull images from an internally hosted container registry,
    this rule can help.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅从特定镜像仓库拉取镜像** – 如果要求容器只能从内部托管的容器注册表拉取镜像，这条规则将有所帮助。'
- en: '`myapp/*`.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`myapp/*`。'
- en: '**Allowing images with specific tag values** – This is like the previous rule
    but applicable to tag names.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**允许具有特定标签值的镜像** – 这与之前的规则类似，但适用于标签名称。'
- en: 'In this section, we will learn how to configure a policy that requires image
    pulls only with digest and not with tags. The following are the steps to do so:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何配置一个策略，仅通过摘要而不是标签来要求拉取镜像。以下是实现此操作的步骤：
- en: 'Go to the **Policies** | **Assignment** menu from the left-hand navigation
    bar and select the **Image registry** tab. From there, select the Workspace we
    have previously created and click on the **CREATE IMAGE REGISTRY POLICY** button
    as highlighted in the following screenshot:'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从左侧导航栏进入**策略** | **分配**菜单，选择**镜像注册表**选项卡。然后，选择我们之前创建的工作区，点击**创建镜像注册表策略**按钮，如下图所示：
- en: "![Figure \uFEFF9.79 – Creating an image registry policy](img/B18145_09_79.jpg)"
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.79 – 创建镜像注册表策略](img/B18145_09_79.jpg)'
- en: Figure 9.79 – Creating an image registry policy
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.79 – 创建镜像注册表策略
- en: 'Select the **Require Digest** option from the **Image registry template** dropdown,
    provide a policy name, and click on the **CREATE** **POLICY** button:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**镜像注册表模板**下拉菜单中选择**要求摘要**选项，提供策略名称，并点击**创建** **策略**按钮：
- en: "![Figure \uFEFF9.80 – Entering image registry policy details](img/B18145_09_80.jpg)"
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.80 – 输入镜像注册表策略详情](img/B18145_09_80.jpg)'
- en: Figure 9.80 – Entering image registry policy details
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.80 – 输入镜像注册表策略详情
- en: 'You can see the new image registry policy created for the selected Workspace
    as shown in the following screenshot:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以看到为选定的工作区创建的新镜像注册表策略，如下图所示：
- en: "![Figure \uFEFF9.81 – Verifying the creation of the image registry policy](img/B18145_09_81.jpg)"
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.81 – 验证镜像注册表策略的创建](img/B18145_09_81.jpg)'
- en: Figure 9.81 – Verifying the creation of the image registry policy
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.81 – 验证镜像注册表策略的创建
- en: 'Create a Pod in the GKE-attached cluster in a new namespace that is not affected
    by the policy. This Pod pulls an image using a tag. The Pod definition YAML file
    is given at https://raw.githubusercontent.com/PacktPublishing/DevSecOps-in-Practice-with-VMware-Tanzu/main/chapter-10/image-tag-using-pod.yaml:'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GKE附加的集群中创建一个新的命名空间的Pod，该命名空间不受策略影响。此Pod使用标签拉取镜像。Pod定义的YAML文件可以在以下链接查看：https://raw.githubusercontent.com/PacktPublishing/DevSecOps-in-Practice-with-VMware-Tanzu/main/chapter-10/image-tag-using-pod.yaml：
- en: 'Create a new namespace in the cluster using the following command:'
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在集群中创建一个新的命名空间：
- en: '[PRE11]'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Create the Pod that uses a tagged image pull:'
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个使用标签拉取镜像的Pod：
- en: '[PRE12]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You will see that the Pod is being created without any issues:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将看到Pod成功创建，没有任何问题：
- en: '[PRE13]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create the same Pod in the **default** namespace where we have applied the
    image registry policy that does not allow you to pull an image with a tag value
    using the following command:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令，在我们应用了镜像注册表策略且不允许通过标签值拉取镜像的**default**命名空间中创建相同的Pod：
- en: '[PRE15]'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You will see that the Pod cannot be created, with the following error message
    explaining the restrictions in place:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将看到Pod无法创建，并显示以下错误信息，解释了所施加的限制：
- en: '[PRE16]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create the same Pod with a digest replacing the tag for the image using the
    following command:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建相同的Pod，替换镜像标签为摘要：
- en: '[PRE17]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You will see that the Pod is created successfully this time in the **default**
    namespace where the image policy restrictions are applicable:'
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将看到此时Pod在**default**命名空间中成功创建，且镜像策略的限制已生效：
- en: '[PRE18]'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This concludes the topic of learning how to create a policy that is applicable
    to a Workspace. The previously configured and tested restrictions will also be
    applicable in the other cluster’s **default** namespace, being part of the same
    Workspace.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容结束了如何为工作区创建适用策略的学习。之前配置和测试的限制同样适用于其他集群的**default**命名空间，作为同一工作区的一部分。
- en: In the next topic, we will learn how to apply a deployment governance policy
    for a cluster group using TMC.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个主题中，我们将学习如何使用TMC为集群组应用部署治理策略。
- en: Configuring a deployment governance policy for a cluster group
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置集群组的部署治理策略
- en: In the majority of Kubernetes platform deployments, a platform team is responsible
    for ensuring that their internal customers, the application teams, use the platform
    with discipline. This discipline involves the fair usage of the computes available
    and high-availability-prone application deployments. When multiple different Kubernetes
    platform teams are managing a smaller number of clusters, it will be challenging
    to implement a set of standards that are applicable enterprise-wide, as all the
    platform teams may have their own likes and dislikes. On the other hand, if a
    central management team manages all the clusters, it would be too much to ensure
    compliance with the governing policies. To address these challenges, TMC allows
    you to create deployment governance policies. TMC uses an open source project
    named **Open Policy Agent** (**OPA**) **Gatekeeper** (https://github.com/open-policy-agent/gatekeeper)
    to implement these policies. Because of the declarative nature of the Gatekeeper
    policy configuration, TMC also allows you to create custom deployment policies
    for any use case – the sky is the limit!
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数Kubernetes平台部署中，平台团队负责确保他们的内部客户——应用团队，能够有序使用平台。这种有序使用包括公平使用可用的计算资源以及高可用性部署的应用程序。当多个不同的Kubernetes平台团队管理较少的集群时，实施适用于全企业的标准将具有挑战性，因为各个平台团队可能有各自的偏好。另一方面，如果由中央管理团队管理所有集群，那么确保符合治理政策将变得非常困难。为了解决这些挑战，TMC允许您创建部署治理策略。TMC使用一个名为**Open
    Policy Agent**（**OPA**）**Gatekeeper**（https://github.com/open-policy-agent/gatekeeper）的开源项目来实现这些策略。由于Gatekeeper策略配置的声明式特性，TMC还允许您为任何用例创建自定义部署策略——天空是极限！
- en: 'The following are some of the out-of-the-box policies that TMC provides that
    can be applied to different cluster groups as required:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是TMC提供的部分开箱即用的策略，可以根据需要应用到不同的集群组：
- en: Blocking the creation of certain Kubernetes resources
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阻止某些Kubernetes资源的创建
- en: Mandating the assignment of labels to certain Kubernetes resources
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强制为某些Kubernetes资源分配标签
- en: Blocking specific subjects from being used for role binding in clusters
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阻止特定主题在集群中用于角色绑定
- en: Restricting the usage of certain load balancer IP addresses to be used by Kubernetes
    services
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制特定负载均衡器IP地址供Kubernetes服务使用
- en: Enforcing HTTPS for the ingress resource configuration in clusters
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强制在集群中为入口资源配置HTTPS
- en: Blocking the creation of `NodePort`-type services in clusters
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阻止在集群中创建`NodePort`类型的服务
- en: Let’s learn how to create one such governing policy for the cluster group that
    we created earlier and test its impact on the cluster operations. In the following
    example, we will create a policy to prevent the creation of a Pod without a label
    named *app*. In other words, creating a Pod in the cluster, where this policy
    is applied, should fail if that Pod does not specify the name of the application
    it belongs to.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何为之前创建的集群组创建这样的治理策略，并测试它对集群操作的影响。在以下示例中，我们将创建一个策略，以防止创建没有名为*app*的标签的Pod。换句话说，在应用了此策略的集群中，如果Pod没有指定它所属的应用程序名称，则创建该Pod将失败。
- en: 'Take the following steps to create and test this policy:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤创建并测试此策略：
- en: 'Go to the **Policies** | **Assignments** menu from the left-hand navigation
    bar and open the **Custom** tab. Then, select the cluster group we created from
    the list and click on the **CREATE CUSTOM POLICY** button as highlighted:'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从左侧导航栏的**策略** | **分配**菜单进入，并打开**自定义**标签。然后，从列表中选择我们之前创建的集群组，并点击高亮显示的**创建自定义策略**按钮：
- en: "![Figure \uFEFF9.82 – Creating a custom deployment policy](img/B18145_09_82.jpg)"
  id: totrans-446
  prefs: []
  type: TYPE_IMG
  zh: "![图 \uFEFF9.82 – 创建自定义部署策略](img/B18145_09_82.jpg)"
- en: Figure 9.82 – Creating a custom deployment policy
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.82 – 创建自定义部署策略
- en: 'Select `app` as a required key under **Labels**. Finally, scroll down and create
    the policy:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**标签**下选择`app`作为必需的键。最后，向下滚动并创建策略：
- en: "![Figure \uFEFF9.83 – Entering the custom deployment policy details](img/B18145_09_83.jpg)"
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: "![图 \uFEFF9.83 – 输入自定义部署策略详情](img/B18145_09_83.jpg)"
- en: Figure 9.83 – Entering the custom deployment policy details
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.83 – 输入自定义部署策略详情
- en: 'After creating the policy, you should be able to see these settings configured
    under the cluster group as shown in the following screenshot:'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建策略后，您应该能够看到这些设置已配置在集群组下，如下截图所示：
- en: "![Figure \uFEFF9.84 – Verifying the creation of the policy](img/B18145_09_84.jpg)"
  id: totrans-452
  prefs: []
  type: TYPE_IMG
  zh: "![图 \uFEFF9.84 – 验证策略的创建](img/B18145_09_84.jpg)"
- en: Figure 9.84 – Verifying the creation of the policy
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.84 – 验证政策的创建
- en: 'Create a Pod in one of the two clusters (the TKG workload or GKE) that we added
    to the cluster group with the policy applied using the following command. You
    can see the Pod definition file used in the following command and verify that
    the Pod does not have any labels in the specification YAML:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们已将政策应用到的集群组中添加的两个集群之一（TKG 工作负载或 GKE）中创建一个 Pod，使用以下命令。您可以看到命令中使用的 Pod 定义文件，并验证该
    Pod 在规格 YAML 中没有任何标签：
- en: '[PRE20]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You should see a similar error message as follows:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该会看到类似以下的错误信息：
- en: '[PRE21]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let’s create a Pod with the required label using the following command.
    This time, we will use a different Pod specification file where the `app` label
    is specified. Open the file used in the command to see the newly added label –
    `app`:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下命令创建一个带有所需标签的 Pod。这一次，我们将使用一个不同的 Pod 规格文件，其中指定了 `app` 标签。打开命令中使用的文件查看新增的标签——`app`：
- en: '[PRE22]'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You will see that, this time, the Pod is created without any issues, as we
    supplied the label as required by the policy we created earlier in this section:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将看到，这一次，Pod 成功创建，因为我们提供了政策要求的标签：
- en: '[PRE23]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This concludes how to create a custom policy that is applicable to all the clusters
    in a group and test its impact. TMC admins can create these policies for any logical
    requirements and create their templates. Later, these templates can be used with
    custom parameters (such as the name of the label key in the previous example)
    to apply the policy for a group of clusters – but how do we keep track of policy
    compliance failures to stay fully informed about these issues popping across hundreds
    of clusters managed by TMC? Let’s find that out in the following section.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了如何创建适用于组内所有集群的自定义政策并测试其影响。TMC 管理员可以根据任何逻辑需求创建这些政策并制定其模板。之后，可以使用这些模板和自定义参数（如前面示例中的标签键名称）来对一组集群应用政策——但是，我们如何跟踪政策合规失败，以便在
    TMC 管理的数百个集群中及时了解这些问题呢？接下来我们将在下一部分找到答案。
- en: Checking policy violation statuses across all clusters
  id: totrans-465
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查所有集群的政策违规状态
- en: 'As discussed previously in this chapter, there are several different types
    of policies we can create using TMC at the cluster group and Workspace level.
    In addition to creating guardrails for later Kubernetes platforms, TMC also provides
    a way to monitor them for policy violation insights across all the clusters under
    TMC’s purview. To do so, just open **Policies** > the **Insights** menu from the
    left-hand navigation bar and you will see a detailed report, as shown in the following
    screenshot. As you can see, it shows a couple of records for the policy testing
    we did that violated our policies:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面所讨论的，我们可以使用 TMC 在集群组和工作区级别创建几种不同类型的政策。除了为后续的 Kubernetes 平台创建安全边界外，TMC 还提供了一种方法，可以监控所有
    TMC 负责的集群中关于政策违规的见解。为此，只需打开 **政策** > 左侧导航栏中的 **见解** 菜单，您将看到如下一张详细报告截图。正如您所见，它显示了我们进行的几项政策测试，违反了我们的政策：
- en: "![Figure \uFEFF9.85 – Getting policy insights](img/B18145_09_85.jpg)"
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.85 – 获取政策见解](img/B18145_09_85.jpg)'
- en: Figure 9.85 – Getting policy insights
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.85 – 获取政策见解
- en: Having learned about policies and checking their compliance status, let’s now
    learn how to audit clusters for CIS benchmarking.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 了解了政策并检查其合规状态后，我们现在来学习如何对集群进行 CIS 基准审计。
- en: Inspecting clusters for CIS benchmark compliance
  id: totrans-470
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查集群是否符合 CIS 基准
- en: 'As discussed earlier in this chapter, a security compliance audit of the clusters
    for common loopholes is a proactive stance to prevent cyber-attacks. Scanning
    Kubernetes clusters for a set of best security practices with a long checklist
    is a challenging task unless there is sophisticated automation built to do so.
    TMC provides this capability for all its managed clusters using an open source
    tool, Sonobuoy, as discussed earlier. As of the current version, TMC can perform
    the following two types of cluster inspections:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面所讨论的，对集群进行安全合规审计以查找常见漏洞是防止网络攻击的积极措施。扫描 Kubernetes 集群以检查一套最佳安全实践并列出详细清单是一项具有挑战性的任务，除非有复杂的自动化工具来执行此操作。TMC
    为其所有管理的集群提供了这一功能，通过使用开源工具 Sonobuoy，如前所述。当前版本的 TMC 可以执行以下两种类型的集群检查：
- en: '*Conformance* – to check the cluster configuration against the official Kubernetes
    specification'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*符合性* – 检查集群配置是否符合官方 Kubernetes 规范'
- en: '*CIS benchmark* – to check whether the Kubernetes cluster deployment is following
    the security best practices outlined in the CIS benchmark for Kubernetes'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CIS 基准* – 检查 Kubernetes 集群部署是否遵循 CIS 基准中概述的安全最佳实践'
- en: 'Let’s learn how to inspect a cluster managed by TMC for CIS Benchmark. Take
    the following steps to perform the inspection and check the inspection results
    using the TMC console:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何检查由 TMC 管理的集群的 CIS 基准。按照以下步骤执行检查，并使用 TMC 控制台查看检查结果：
- en: 'Click on the **Inspections** menu in the left-hand navigation bar and click
    on the **RUN INSPECTION** button as highlighted in the screenshot:'
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧导航栏中点击**检查**菜单，然后点击截图中高亮显示的**运行检查**按钮：
- en: "![Figure \uFEFF9.86 – Running a cluster inspection](img/B18145_09_86.jpg)"
  id: totrans-476
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.86 – 运行集群检查](img/B18145_09_86.jpg)'
- en: Figure 9.86 – Running a cluster inspection
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.86 – 运行集群检查
- en: 'Select an existing cluster, set the inspection type as **CIS benchmark**, and
    click on the **RUN** button:'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个现有的集群，将检查类型设置为**CIS 基准**，然后点击**运行**按钮：
- en: "![Figure \uFEFF9.87 – Selecting the inspection to run](img/B18145_09_87.jpg)"
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.87 – 选择要运行的检查](img/B18145_09_87.jpg)'
- en: Figure 9.87 – Selecting the inspection to run
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.87 – 选择要运行的检查
- en: 'You will see the inspection results in a few minutes as displayed in the following
    screenshot. As you can see, the scan has found some failed test cases that should
    be addressed for a stronger security posture:'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将在几分钟内看到检查结果，如下图所示。正如您所看到的，扫描已发现一些失败的测试用例，需要解决这些问题以增强安全性：
- en: "![Figure \uFEFF9.88 – Checking the inspection results](img/B18145_09_88.jpg)"
  id: totrans-482
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.88 – 检查检查结果](img/B18145_09_88.jpg)'
- en: Figure 9.88 – Checking the inspection results
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.88 – 检查检查结果
- en: 'Click on the test result link on the previous screen and you will get a detailed
    report of the failed test cases, warnings, and successful cases, as shown in the
    following screenshot. The report provides details of each test case and recommendations
    for it. You can also download the report to look at it offline:'
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击前一个屏幕上的测试结果链接，您将看到有关失败的测试用例、警告和成功案例的详细报告，如下图所示。该报告提供了每个测试用例的详细信息和建议。您还可以下载报告以便离线查看：
- en: "![Figure \uFEFF9.89 – Checking the detailed inspection report](img/B18145_09_89.jpg)"
  id: totrans-485
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.89 – 查看详细的检查报告](img/B18145_09_89.jpg)'
- en: Figure 9.89 – Checking the detailed inspection report
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.89 – 查看详细的检查报告
- en: That brings us to the end of the topic of running cluster inspections. We can
    also trigger cluster inspections from the cluster detail sections as well on TMC
    in addition to the main **Inspections** menu.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了运行集群检查的主题。除了主要的**检查**菜单，我们还可以在 TMC 上的集群详情部分触发集群检查。
- en: 'With that, we have also covered all the planned operations that we wanted to
    cover around applying governance policies for Kubernetes clusters using TMC. First,
    we saw how to create a security policy to restrict the ability to run privileged
    containers. Then, we learned how to create an image registry policy, with the
    example of preventing image pulls without its digest. After that, we created a
    deployment policy to prevent Pods from being created without a specific label
    and saw all the different types of custom policies we can create using TMC. Later,
    we saw how to find policy violations. Finally, we learned how to run conformance
    inspections for clusters and get their detailed reports. In addition to all these
    capabilities of TMC, there are a few other operations that we did not cover in
    this chapter:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经涵盖了所有计划中的操作，主要是关于如何通过 TMC 为 Kubernetes 集群应用治理策略。首先，我们展示了如何创建安全策略来限制运行特权容器的能力。接着，我们学习了如何创建镜像注册表策略，示例中是防止没有摘要的镜像拉取。之后，我们创建了部署策略，防止没有特定标签的
    Pod 被创建，并查看了使用 TMC 创建的各种自定义策略类型。接下来，我们了解了如何查找策略违规。最后，我们学习了如何运行集群合规检查并获取其详细报告。除了
    TMC 的这些功能外，还有一些其他操作我们没有在本章中介绍：
- en: Managing user access policies for cluster groups, clusters, Workspaces, and
    namespaces
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理集群组、集群、工作区和命名空间的用户访问策略
- en: Installing various tools from a published catalog on a cluster managed by TMC
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在由 TMC 管理的集群上从已发布的目录中安装各种工具
- en: Inspecting various events emitted by the clusters managed by TMC
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查由 TMC 管理的集群发出的各种事件
- en: Performing several administrative activities for TMC, including configuring
    TMC access permissions, external integrations, and proxy configuration, among
    other things
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行多个与TMC相关的管理活动，包括配置TMC访问权限、外部集成和代理配置等。
- en: 'To get more details about them, refer to this documentation link: https://docs.vmware.com/en/VMware-Tanzu-Mission-Control/index.html.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多细节，请参考此文档链接：https://docs.vmware.com/en/VMware-Tanzu-Mission-Control/index.html。
- en: Summary
  id: totrans-494
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We covered a lot of ground in this chapter considering what TMC can do to minimize
    the effort of managing any distribution of upstream Kubernetes that could be running
    on different cloud environments. The high-level value proposition of this tool
    is to provide management control for multi-cloud, multi-cluster, and multi-team
    usage. First, we understood the different challenges of deployment approaches
    with a small number of large clusters or a large number of small clusters. Then,
    we saw use cases for TMC and the solutions it provides for complex problems when
    managing Kubernetes deployments. We discussed how challenging it can be to operate
    on different clusters deployed in different cloud environments and keep the operator
    and developer experience consistent across them. We saw how TMC makes this easy
    using cluster groups and Workspaces, a group of Kubernetes namespaces across different
    clusters.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了TMC如何帮助最小化管理在不同云环境中运行的任何上游Kubernetes发行版的工作量。该工具的高层次价值主张是为多云、多集群和多团队使用提供管理控制。首先，我们理解了采用少量大集群或大量小集群的部署方式所面临的不同挑战。接着，我们看到了TMC的使用案例以及它为管理Kubernetes部署时解决复杂问题提供的解决方案。我们讨论了在不同云环境中部署的不同集群的操作挑战，以及如何保持操作员和开发人员体验的一致性。我们看到TMC如何通过集群组和工作空间简化这一过程，工作空间是跨不同集群的Kubernetes命名空间的集合。
- en: Then, we learned how to get started with TMC, covering the way to integrate
    a TKG management cluster with TMC. Then, we discussed how to perform cluster lifecycle
    operations on that TKG foundation using TMC. After that, we saw how to create
    new workload clusters under the TKG control plane, which become a part of the
    TMC-managed clusters. Then, we also learned how to bring externally created or
    managed clusters such as GKE clusters into TMC and make them part of the cluster
    groups defined in TMC. We also learned how to create groups of Kubernetes namespaces,
    the Workspaces, for clusters that are deployed in two different cloud environments.
    After getting started with TMC, the first thing we learned was how to protect
    cluster data with the backup and restore capabilities of TMC. Then, we covered
    a long section on governing a fleet of clusters using various policies that we
    can apply either at the cluster group or Workspace level. Lastly, we covered how
    to inspect different clusters to take a proactive stance toward security.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们学习了如何开始使用TMC，包括如何将TKG管理集群与TMC集成。然后，我们讨论了如何使用TMC执行基于TKG基础的集群生命周期操作。之后，我们看到如何在TKG控制平面下创建新的工作负载集群，这些集群成为TMC管理的集群的一部分。接下来，我们还学习了如何将外部创建或管理的集群（如GKE集群）带入TMC，并将其纳入TMC中定义的集群组。我们还学习了如何为部署在两个不同云环境中的集群创建Kubernetes命名空间组，即工作空间。开始使用TMC后，我们首先学习了如何通过TMC的备份和恢复功能保护集群数据。然后，我们介绍了如何通过在集群组或工作空间级别应用各种策略来管理集群队列。最后，我们讲解了如何检查不同的集群，以主动应对安全问题。
- en: As TMC is a single pane of glass for *controlling* large-scale Kubernetes deployments,
    VMware Aria operations for Applications is a single pane of glass for *observing*
    the scale of a Kubernetes deployment with its full-stack observability capabilities,
    starting from the application layer and moving to the infrastructure layer. In
    the next chapter, we will cover this topic in detail.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TMC是*控制*大规模Kubernetes部署的单一视图，VMware Aria应用操作是一个用于*观察*Kubernetes部署规模的单一视图，具备全栈可观测能力，从应用层到基础设施层。我们将在下一章详细介绍这个主题。
