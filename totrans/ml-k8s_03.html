<html><head></head><body>
		<div id="_idContainer079">
			<h1 id="_idParaDest-56"><em class="italic"><a id="_idTextAnchor055"/>Chapter 4</em>: The Anatomy of a Machine Learning Platform</h1>
			<p>In this and the next few chapters, you will learn and install the components of a <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) platform on top of Kubernetes. An ML platform should be capable of providing the tooling required to run the full life cycle of an ML project as described in <a href="B18332_02_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Understanding MLOps</em>. This chapter starts with defining the different components of an ML platform in a technology-agnostic way. In the later parts, you will see the group of open source software that can satisfy the requirements of each component. We have chosen this approach to not tie you up with a specific technology stack; instead, you can replace components as you deem fit for your environment. </p>
			<p>The solution that you will build in this book will be based on open source technologies and will be hosted on the Kubernetes platform that you built in <a href="B18332_03_ePub.xhtml#_idTextAnchor040"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring Kubernetes</em>.</p>
			<p>In this chapter, you will learn about the following topics:</p>
			<ul>
				<li>Defining a self-service platform</li>
				<li>Exploring the data engineering components </li>
				<li>Exploring the ML model life cycle components</li>
				<li>Addressing security, monitoring, and automation</li>
				<li>Exploring Open Data Hub</li>
			</ul>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor056"/>Technical requirements</h1>
			<p>This chapter includes some hands-on setup. You will be needing a running Kubernetes cluster configured with the <strong class="bold">Operator Life cycle Manager</strong> (<strong class="bold">OLM</strong>). Building such a Kubernetes environment is covered in <a href="B18332_03_ePub.xhtml#_idTextAnchor040"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring Kubernetes</em>. Before attempting the technical exercises in this chapter, please make sure that you have a working Kubernetes cluster. You may choose to use a different flavor of Kubernetes than the one described in <a href="B18332_03_ePub.xhtml#_idTextAnchor040"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring Kubernetes</em>, as long as the cluster has the OLM installed.</p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor057"/>Defining a self-service platform</h1>
			<p><strong class="bold">Self-service</strong> is defined as the capability of a platform that allows platform end users to provision resources on-demand without other human intervention. Take, for example, a data scientist user <a id="_idIndexMarker256"/>who needs an instance of a Jupyter notebook server, running on a host container with eight CPUs, to perform his/her work. A self-service ML platform should allow the data scientist to provision, through an end user friendly interface, the container that will run an instance of the Jupyter notebook server on it. Another example of self-service provisioning would be a data engineer requesting a new instance of an Apache Spark cluster to be provisioned to run his/her data pipelines. The last example is a data scientist who wants to package and deploy their ML model as a REST service so that the application can use the model.</p>
			<p>One benefit of a self-service platform is that it allows cross-functional teams to work together with minimal <a id="_idIndexMarker257"/>dependencies on other teams. This independence results in better team dynamics, less friction, and increased team velocity.</p>
			<p>The self-service model, however, needs governance. Imagine every data scientist requesting GPUs or data engineers requesting tens of terabytes of storage! Self-service capability is great, but without proper governance, it could also create problems. To avoid such problems, the platform has to be managed by a platform team that can control or limit the things the end users can do. One example of this limit is resource quotas. Teams and/or individual users can be allocated with quotas and be responsible for managing their own resources within the allocated quotas. Luckily, Kubernetes has this capability, and our ML platform can utilize this capability to apply limits to the team's resources. </p>
			<p>As part of governance, the platform must have role-based access control. This is to ensure that only the users with the right role will have access to the resources they manage. For example, the platform team may be able to change the resource quotas, while data engineers can only spin up new Spark clusters and run data pipelines. </p>
			<p>Another aspect of a self-service platform is the isolation of workloads. Many teams will be sharing the same platform and, while the quotas will keep the teams within their predefined boundaries, it is <a id="_idIndexMarker258"/>critical that there is a capability to isolate workloads from each other so that multiple unrelated projects running on the same platform do not overlap.</p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor058"/>Exploring the data engineering components</h1>
			<p>In the context of this book, data engineering is the process of ingesting raw data from source systems and <a id="_idIndexMarker259"/>producing reliable data that could be used in scenarios such as analytics, business reporting, and ML. A data engineer is a person who builds software that collects and processes raw data to generate clean and meaningful datasets for data analysts and data scientists. These datasets will form the backbone for your organization's ML initiatives.</p>
			<p><em class="italic">Figure 4.1</em> shows the various stages of a typical data engineering area of an ML project:</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B18332_04_001.jpg" alt="Figure 4.1 – Data engineering stages for ML&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – Data engineering stages for ML</p>
			<p>Data engineering often overlaps with <strong class="bold">feature engineering</strong>. While a data scientist decides on which features are <a id="_idIndexMarker260"/>more useful for the ML use case, he or she may work with the data engineer to retrieve particular data points that are not available in the current feature set. This is the <a id="_idIndexMarker261"/>main collaboration point between data engineers and data scientists. The datasets created by the data engineer in the data engineering block become the feature set in the ML block.</p>
			<p>An ML platform that enables teams to perform feature engineering will have the following components and processes. </p>
			<ul>
				<li><strong class="bold">Data ingestion</strong>: Data ingestion <a id="_idIndexMarker262"/>is the process in <a id="_idIndexMarker263"/>which the team understands the data sources and builds and deploys software that collects data from one or more data sources. Data engineers understand the impact of reading data from source systems. For example, while reading data from a source, the performance of the source system may get affected. Therefore, it is important for the ML platform to have a workflow scheduling capability so that the data collection can be scheduled during a time when the source system is less active.</li>
			</ul>
			<p>An ML platform enables the team to ingest data from various sources in multiple ways. For example, some data sources would allow data to be pulled, while other <a id="_idIndexMarker264"/>data sources may be able to <a id="_idIndexMarker265"/>push data. Data may come from a relational database, data warehouses, data lakes, data pools, data streams, API calls, or even from a raw filesystem. The platform should also have the capability to understand different protocols, for example, a messaging system may have multiple protocols, such as <strong class="bold">Advanced Message Queuing Protocol</strong> (<strong class="bold">AMQP</strong>), <strong class="bold">Message Queuing Telemetry Transport</strong> (<strong class="bold">MQTT</strong>), and Kafka. In other words, the ML platform should have the capability to gather data of various shapes and sizes from different types of data sources in various ways. <em class="italic">Figure 4.2</em> shows various sources of data from where the platform should be able to ingest the data:</p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B18332_04_002.jpg" alt="Figure 4.2 – Data ingestion integrations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – Data ingestion integrations</p>
			<ul>
				<li><strong class="bold">Data transformation</strong>: Once the data is ingested from various sources, it needs to be transformed <a id="_idIndexMarker266"/>from its <a id="_idIndexMarker267"/>original form into something that is more useful for the ML model training and other use cases. According to a Forbes survey, <em class="italic">80% of data scientists' work is related to preparing data for the model training</em>; this is the stage that is generally considered as boring among the data science teams. However, if the data is not transformed into an appropriate form, it will lead to less useful and/or inefficient ML models. An ML platform enables teams to code, build, and deploy the data transformation pipelines and jobs with ease. The platform abstracts the complications of running and managing data transformation components such as Apache Spark jobs. Not only does the platform manage the execution of these processes, but it also manages the provisioning and cleaning of compute resources required to run these components, such as CPU, memory, and networking.</li>
				<li><strong class="bold">Storage</strong>: During the feature engineering process, you will read and write data at various stages. You might create a temporary representation of the dataset for further <a id="_idIndexMarker268"/>processing, or you could write the new dataset to be used for ML processes. In these scenarios, you will need storage resources that can be accessed with ease and scale as needed. An ML platform provides on-demand storage for your datasets to be stored in a reliable fashion.</li>
			</ul>
			<p>Now, let's see how the data engineer will use these components in their workflow.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor059"/>Data engineer workflow</h2>
			<p>All the capabilities mentioned in the previous section are provided by the ML platform in a self-serving <a id="_idIndexMarker269"/>manner. The workflow that a data engineer using the platform would typically perform is as follows:</p>
			<ol>
				<li value="1"><em class="italic">Log in to the platform</em>: In this step, the data engineer authenticates to the platform.</li>
				<li><em class="italic">Provisioning of the development environment</em>: In this step, the data engineer requests the resource requirements for the development environment (such as the number of CPUs, amount of memory, and specific software libraries) to the platform. The platform then provisions the requested resources automatically.</li>
				<li><em class="italic">Build a data pipeline</em>: In this step, the data engineer writes the code for data ingestion and data transformation. The data engineer then runs the code in an isolated environment to verify its validity and perform the necessary refactoring and tuning of the code.</li>
				<li><em class="italic">Run a data pipeline</em>: In this step, the data engineer schedules the code to run as needed. It can be a regular schedule with periodic intervals such as hourly or daily, or a one-off run, depending on the use case.</li>
			</ol>
			<p>You can see in the preceding steps that besides writing the code, all other steps are declarative. The data engineer's focus will be on building the code to ingest and transform data. All other aspects of the flow will be taken care of by the ML platform. This will result in improved efficiency and velocity for the team. The declarative capability of the platform will allow teams to standardize processes across your organization, which will <a id="_idIndexMarker270"/>reduce the number of bespoke toolchains and improve the security of the overall process.</p>
			<p>The main output of the data engineering flow is a usable, transformed, and partially cleaned set of data that can be used to start building and training a model.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor060"/>Exploring the model development components</h1>
			<p>Once the cleaned data is available, data scientists then go through the problem and try to determine <a id="_idIndexMarker271"/>what set of patterns would be helpful for the situation. The key here is that the data scientist's primary role is to find patterns in the data. Model development components of the ML platform explore data patterns, build and train ML models, and trial multiple configurations to find the best set of configurations and algorithms to achieve the desired performance of the model.</p>
			<p>Within the course of model development, data scientists or ML engineers build multiple models based on multiple algorithms. These models are then trained using the data gathered and prepared from the data engineering flow. The data scientist then plays around with several hyperparameters to get different results from model testing. The result of such training and testing is then compared with each of the other models. These experimentation processes are then repeated multiple times until the desired results are achieved.</p>
			<p>The experimentation phase will result in a selection of the most appropriate algorithm and configuration. The selected model will then be tagged for packaging and deployment.</p>
			<p><em class="italic">Figure 4.3</em> shows the various stages of model development for an ML project:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B18332_04_003.jpg" alt="Figure 4.3 – Data engineering stages for ML&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – Data engineering stages for ML</p>
			<p>An ML platform that <a id="_idIndexMarker272"/>enables teams to perform model development will have the following components:</p>
			<ul>
				<li><strong class="bold">Data exploration</strong>: We humans are better at finding patterns when the data is visualized as <a id="_idIndexMarker273"/>opposed to just looking at raw data sets. The ML platform enables you to visualize data. As a data scientist, you will need to collaborate with <strong class="bold">subject matter experts</strong> (<strong class="bold">SMEs</strong>) who have domain knowledge. Let's say you are analyzing a dataset of coronavirus <a id="_idIndexMarker274"/>patients. If you are not an expert in the virology or medicine domains, you will need to work with an SME who can provide insights about the dataset, the relationships of features, and the quality of the data itself. An ML platform allows you to share the visualizations you have created with the wider team for improved feedback. The platform also allows non-technical people to look at the data in a more graphical approach. This will help them gain a better understanding of the data.</li>
				<li><strong class="bold">Experimentation</strong>: As a data scientist, you will split the data into training and testing sets, and then start building the model for the given metric. You will then experiment with <a id="_idIndexMarker275"/>multiple ML algorithms such as decision trees, XGBoost, and deep learning, and apply a variety of parameter tuning to each of the algorithms, for example, the number of layers or number of neurons for a deep learning model. This is what we call experimentation, and the platform enables the team to perform the experimentation in an autonomous way. Keep in mind that for each experiment, you may have different requirements for compute resources such as a GPU. This makes the self-service provisioning capability of the platform critical.</li>
				<li><strong class="bold">Tracking</strong>: While doing multiple experiments, you need to keep track of the parameters <a id="_idIndexMarker276"/>used for each experiment and the metrics it has achieved. Some algorithms may require different sets of features, which means you also need to keep track of the version of the dataset that was used in training. There are two reasons for doing this. The first reason is that you will need a history of your experiments so that you can <a id="_idIndexMarker277"/>compare and pick the best combination. The second reason is that you may need to share the results with your fellow data scientists. The ML platform enables you to record the results of the experiments and share them seamlessly.</li>
				<li><strong class="bold">Model building and tuning</strong>: In the experimentation stage, you have found the best algorithm and the best parameters for your model. You have compared the results and <a id="_idIndexMarker278"/>associated <a id="_idIndexMarker279"/>metrics for your model and have chosen the algorithm and parameters to be used. In this stage, you will train your model with these parameters, and register it with the model registry:<ul><li><strong class="bold">Model registry</strong>: As a data scientist, when you are satisfied with your model, you work with your team to deploy it. The real world changes, however, and you <a id="_idIndexMarker280"/>will need to update your model for new datasets or different metrics or simply for improved metrics. New versions of the models come all the time and the ML platform enables you to keep track of the versions of your models. The model versioning capability will help the team to compare the efficiency of new model versions with older model versions and allow the team to roll back a new model in production to previous versions if the need arises.</li></ul></li>
				<li><strong class="bold">Storage</strong>: Storage is not only important in the data engineering phase but also in model development. During the model development process, you read and write data <a id="_idIndexMarker281"/>at various stages. You split the dataset into a testing dataset and a training dataset, and you may choose to write it once so you can experiment with different model parameters but with the same datasets. The experiment tracking module and the model registry both need storage. The ML platform provides on-demand storage for your datasets to be stored in a reliable fashion.</li>
			</ul>
			<p>Now, let's see how the data scientists use these components in their workflow.</p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor061"/>Understanding the data scientist workflow</h2>
			<p>All the capabilities <a id="_idIndexMarker282"/>mentioned in the previous section are provided by the ML platform in a self-serving way. The typical workflow for the data scientist would be as follows:</p>
			<ol>
				<li value="1"><em class="italic">Log in to the platform</em>: The data scientists authenticate to the platform.</li>
				<li><em class="italic">Provisioning of the development environment</em>: In this step, the data scientist requests, to the platform, the resource requirements for the development environment, such as the number of CPUs, amount of memory, and specific software libraries. The platform then provisions the requested resources automatically.</li>
				<li><em class="italic">Exploratory data analysis</em>: In this stage, data scientists perform several types of data transformations and visualization techniques to understand the patterns hidden in the data.</li>
				<li><em class="italic">Experimenting with different algorithms</em>: In this stage, data scientists split the full dataset into training and testing sets. Then, the data scientists apply different ML algorithms and hyperparameters to achieve the desired metrics. Data scientists then compare the parameters of each training run to select the best one for the given use case.</li>
				<li><em class="italic">Model training</em>: Data scientists train the model as per the most optimized parameters found in the previous stage, and register the model in the model registry.</li>
				<li><em class="italic">Run model deployment pipeline</em>: In this step, the data scientists package the model to be consumed as a service and build the pipeline to automate the deployment process. It can be scheduled regularly or as a one-off run, depending on the use case. </li>
			</ol>
			<p>You can see in the preceding steps that besides writing the code to facilitate model building and training, all other steps are declarative. The data scientists' focus will be on building more data science and ML engineering tasks. All other aspects of the flow will be taken care of by the ML platform. This will result in improved efficiency and velocity for the team, not to <a id="_idIndexMarker283"/>mention a happier data scientist. The declarative capability of the platform will also allow teams to standardize processes across your organization, which will reduce the use of bespoke toolchains improving consistency and improving the security of the overall process.</p>
			<p>In the next section, you will explore the common services of the ML platform. These services are critical to making the platform production-ready and easier to adopt in the enterprise environment.</p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor062"/>Security, monitoring, and automation</h1>
			<p>In this section, you will <a id="_idIndexMarker284"/>see some common components of the ML platform that apply to all the components and stages we have discussed so far. These components assist you in operationalizing the platform in your organization:</p>
			<ul>
				<li><strong class="bold">Data pipeline execution</strong>: The outcome of data engineering is a data pipeline that ingests, cleans, and processes data. You have built this pipeline with scaled-down data for <a id="_idIndexMarker285"/>development purposes. Now, you need to run this code with production data, or you want a scheduled run with new data available, say, every week. An ML platform allows you to take your code and automate its execution in different environments. This is a big step because the platform not only allows you to run your code but will also manage the packaging of all the dependencies of your code so that it can run anywhere. If the code that you have built is using Apache Spark, the platform should allow you to automate the process of provisioning a Spark cluster and all other components required to run your data pipeline.</li>
				<li><strong class="bold">Model deployment</strong>: Once the model is ready to be used, it should be available to be consumed <a id="_idIndexMarker286"/>as a service. Without the automated model packaging and deployment capability of the ML platform, the process of packaging a model and hosting it as a service requires some software engineering work. This work requires tight collaboration with software engineers and the operations team and may take days, if not weeks, to accomplish. The ML platform automates this process and it usually takes only a few seconds to <a id="_idIndexMarker287"/>a few minutes. The result of this process is an ML model deployed in an environment and is accessible as a service – typically, as a REST API.</li>
			</ul>
			<p>Deployment of the model is one aspect; over time, you may also need to re-train the model with new datasets. The platform also enables your team to automate the retraining process using the same training code you built for the first time when you trained your model. The retrained model is then redeployed automatically. This capability massively improves the efficiency of the team and this allows for more efficient use of time, such as working on newer challenges while delivering values for the business.</p>
			<ul>
				<li><strong class="bold">Monitoring</strong>: Monitoring does not just refer to having the capability to observe the dynamics of the components in production, such as monitoring the model response time, but it <a id="_idIndexMarker288"/>also enables the team to respond to events before they become problems. A good monitoring platform provides observability during the full ML project life cycle and not just monitoring in production. When you are writing code to process data, you may need to tune the joins expression between datasets from multiple systems. This is one of the examples of information you need during development. The ML platform allows you to dig into the details during the development process. The platform also provides capabilities to monitor the underlying IT infrastructure. For example, when you are running your code during the model training stage, the platform provides the metrics on hardware resource utilization.</li>
				<li><strong class="bold">Security and governance</strong>: The platform you are building allows teams to work autonomously. Teams can use the tools in the platform to perform the work anytime. However, the question of who can access what and who can use which tools proves <a id="_idIndexMarker289"/>to be a challenge for many organizations. For this, the platform must have an access control capability and provide access to only authorized users. The security component of the platform allows the users to be authenticated and authorized through standard protocols such as <strong class="bold">OAuth2</strong> or <strong class="bold">OpenID Connect</strong>. You will be using open source components to bring authentication components to the platform. The <a id="_idIndexMarker290"/>platform also uses the Kubernetes namespace feature to provide <a id="_idIndexMarker291"/>workload isolation across different teams that <a id="_idIndexMarker292"/>are sharing the same cluster. Kubernetes also provides the capability to assign limits of hardware resources to be used by individual teams. These capabilities will enable teams to share the platform across many different units within your organization while providing well-defined isolation boundaries and hardware resource quotas.</li>
				<li><strong class="bold">Source code management</strong>: When you build data pipelines or train your model, you <a id="_idIndexMarker293"/>write code. The platform provides capabilities to integrate with source code management solutions. <strong class="bold">Git</strong> is the default source code management solution integrated platform.</li>
			</ul>
			<p>Now, let's move on to cover <strong class="bold">Open Data Hub</strong> (<strong class="bold">ODH</strong>).</p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor063"/>Introducing ODH</h1>
			<p>ODH is an open source project that provides most of the components required by our ML platform. It comes with a Kubernetes operator and a curated set of open source software <a id="_idIndexMarker294"/>components that make up most of the ML platform. In this book, we will mainly use the ODH operator. There are also other components that we will be using in the platform that don't originally come with ODH. One good thing about the ODH operator is the ability to swap default components for another as you see fit for your case.</p>
			<p>To build the platform, you will use the following components. In the next few chapters, you will learn about the details of each of these components and how to use them. For now, you just need to understand their purpose at a very high-level: </p>
			<ul>
				<li><strong class="bold">ODH operator</strong>: A Kubernetes operator that manages the life cycle of different components of the <a id="_idIndexMarker295"/>ML platform. It controls and manages the installation and maintenance of the software components used in your ML platform.</li>
				<li><strong class="bold">JupyterHub</strong>: Manages instances <a id="_idIndexMarker296"/>of Jupyter Notebook servers and their related resources.</li>
				<li><strong class="bold">Jupyter notebooks</strong>: An <strong class="bold">integrated development environment</strong> (<strong class="bold">IDE</strong>) is the main <a id="_idIndexMarker297"/>data engineering and data <a id="_idIndexMarker298"/>science workspace in the platform. Data scientists <a id="_idIndexMarker299"/>and engineers will use these workspaces to write and debug code for both data engineering and ML workflows.</li>
				<li><strong class="bold">Apache Spark</strong>: A distributed, parallel data processing engine and framework for processing large <a id="_idIndexMarker300"/>datasets. It provides a wide array of data ingestion connectors to consume data from a variety of sources.</li>
				<li><strong class="bold">Apache Airflow</strong>: A workflow engine that automates the execution and scheduling of data pipelines <a id="_idIndexMarker301"/>and model deployment. Airflow orchestrates different components of your data pipelines.</li>
				<li><strong class="bold">Seldon Core</strong>: A library for packaging and deploying ML models as a REST service. It also has the <a id="_idIndexMarker302"/>capability of monitoring the deployed models. It provides support for popular ML frameworks, which gives it the capability to wrap and package ML models built with frameworks such as TensorFlow, scikit-learn, XGBoost, and PyTorch, as REST services.</li>
				<li><strong class="bold">Prometheus and Grafana</strong>: These two components provide the monitoring capabilities for <a id="_idIndexMarker303"/>our platform. Prometheus provides the metrics database to <a id="_idIndexMarker304"/>record telemetry data provided by the components <a id="_idIndexMarker305"/>of the platform, and Grafana provides the <strong class="bold">graphical user interface</strong> (<strong class="bold">GUI</strong>) to visualize the captured metrics.</li>
				<li><strong class="bold">Minio</strong>: An object storage provider that is compatible with Amazon S3 APIs. The Minio component is not part <a id="_idIndexMarker306"/>of the ODH toolchain, but we will extend and configure the ODH operator to manage the Minio component on the ML platform.</li>
				<li><strong class="bold">MLFlow</strong>: A component for tracking different model experiments and also serves as the model <a id="_idIndexMarker307"/>registry of the platform. The MLFlow component is not part of the ODH toolchain, but we will extend the ODH operator to manage the MLFlow component on the ML platform</li>
			</ul>
			<p>You will also install an open source identity provider component. The goal for this component is to provide a common single sign-on feature for all the platform components. We will use <strong class="bold">Keycloak</strong> as the identity management system, but this could be swapped with an OAuth2-based system that may already exist in your case. Keycloak is not part of the ODH, and we <a id="_idIndexMarker308"/>will show you how to install it as a separate component on your Kubernetes cluster.</p>
			<p><em class="italic">Figure 4.4</em> shows the major open source software that serves as the main components of the ML platform. The ODH extensibility model allows you to add or choose which products to use for which components as per the requirements. You can replace any of the components with other open source products of choice. However, for the exercises in this book, we <a id="_idIndexMarker309"/>will use the product listed here:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B18332_04_004.jpg" alt="Figure 4.4 – Major components of the ML platform&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – Major components of the ML platform</p>
			<p>In the next section, you will <a id="_idIndexMarker310"/>deploy the ODH operator and Keycloak server on your Kubernetes cluster. You will also install and configure the ingress controller to accept traffic from outside the cluster.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor064"/>Installing the ODH operator on Kubernetes</h2>
			<p>In this section, you <a id="_idIndexMarker311"/>will install the ODH operator <a id="_idIndexMarker312"/>onto your Kubernetes cluster. At this stage, you will not enable any components of the platform. To install the operator, you first need to register the catalog source for the operator, and then you can install it.</p>
			<p>First, let's register the catalog for the ODH operator. A catalog source contains metadata through which <a id="_idIndexMarker313"/>the OLM can discover operators <a id="_idIndexMarker314"/>and their dependencies. The ODH operator is not available in the default OLM catalog, so we need to register a new catalog that contains the ODH metadata for the OLM:  </p>
			<ol>
				<li value="1">Validate that your Kubernetes cluster is running if you are using <strong class="source-inline">minikube</strong>:<p class="source-code"><strong class="bold">minikube status</strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B18332_04_005.jpg" alt="Figure 4.5 – Validate that Kubernetes is running via minikube&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – Validate that Kubernetes is running via minikube</p>
			<p>If your Kubernetes cluster is not running, please refer to <a href="B18332_03_ePub.xhtml#_idTextAnchor040"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring Kubernetes</em>, on how to configure and start the Kubernetes cluster.</p>
			<ol>
				<li value="2">Verify that the OLM is installed and is running by executing the following:<p class="source-code"><strong class="bold">kubectl get pods -n olm</strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B18332_04_006.jpg" alt="Figure 4.6 – Command output showing OLM pods are running&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – Command output showing OLM pods are running</p>
			<p>Make sure that all the OLM pods are running. If this is not the case for you, refer to <a href="B18332_03_ePub.xhtml#_idTextAnchor040"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring Kubernetes</em>, in the <em class="italic">How to install OLM in your cluster</em> section.</p>
			<ol>
				<li value="3">Clone the <a id="_idIndexMarker315"/>Git repository and navigate to <a id="_idIndexMarker316"/>the repository's root directory. This repository contains all the source files, scripts, and manifests that you need to build the platform within the scope of this book: https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git cd Machine-Learning-on-Kubernetes.</li>
			</ol>
			<p>Register a new <strong class="source-inline">catalog source</strong> operator by using the YAML file available in the source code of this book:</p>
			<p class="source-code">kubectl create -f chapter4/catalog-source.yaml</p>
			<ol>
				<li value="4">After a couple of minutes, validate that the operator is available in your cluster:<p class="source-code"><strong class="bold">kubectl get packagemanifests -o wide -n olm | grep -I opendatahub</strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B18332_04_007.jpg" alt="Figure 4.7 – Validate that the ODH operator is available&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7 – Validate that the ODH operator is available</p>
			<p>On Windows PowerShell, you may need to replace the <strong class="source-inline">grep</strong> command with <strong class="source-inline">findstr</strong>.</p>
			<ol>
				<li value="5">Now, create the subscription for the ODH operator. Recall from the third chapter that a subscription object triggers the installation of the operator via the OLM:<p class="source-code"><strong class="bold">kubectl create -f chapter4/odh-subscription.yaml </strong></p></li>
			</ol>
			<p>You should see a response message that the subscription has been created.</p>
			<ol>
				<li value="6">After creating <a id="_idIndexMarker317"/>the subscription, the OLM will <a id="_idIndexMarker318"/>automatically install the operator and all its components. Verify that the ODH pod is running by issuing the following command. It may take a few seconds before the pods start appearing. If the pods are not listed, wait for a few seconds and rerun the same command:<p class="source-code"><strong class="bold">kubectl get pods -n operators</strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B18332_04_008.jpg" alt="Figure 4.8 – Validate that the ODH pod is up and running&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8 – Validate that the ODH pod is up and running</p>
			<p>You have just installed the ODH operator on your Kubernetes cluster. Notice that you have not used generic Kubernetes objects such as <strong class="bold">Deployments</strong> to run your operator. The OLM allows you to easily manage the installation of an operator via the <strong class="bold">Subscription</strong> object.</p>
			<p>In the next section, you install the ingress controller to allow traffic into your Kubernetes cluster.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor065"/>Enabling the ingress controller on the Kubernetes cluster</h2>
			<p>Recall from <a href="B18332_03_ePub.xhtml#_idTextAnchor040"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring Kubernetes</em>, that ingress provides a way for you to expose <a id="_idIndexMarker319"/>a particular service to make it accessible <a id="_idIndexMarker320"/>from outside the cluster. There are many ingress providers available on Kubernetes, and we leave it to you to select the right ingress provider for your cluster.</p>
			<p>If you are using <strong class="source-inline">minikube</strong>, you need to follow these steps to enable the default ingress:</p>
			<ol>
				<li value="1">Enable the <a id="_idIndexMarker321"/>NGINX-based <a id="_idIndexMarker322"/>ingress controller for your cluster by issuing the following command: <p class="source-code"><strong class="bold">minikube addons enable ingress</strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B18332_04_009.jpg" alt="Figure 4.9 – Output for enabling minikube ingress plugin&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9 – Output for enabling minikube ingress plugin</p>
			<ol>
				<li value="2">Validate that the ingress pods are running in your cluster:<p class="source-code"><strong class="bold">kubectl get pods -n ingress-nginx</strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B18332_04_010.jpg" alt="Figure 4.10 – Validate that the Nginx ingress pods are in running state&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10 – Validate that the Nginx ingress pods are in running state</p>
			<p>Now that you have enabled the external traffic onto your cluster, the next step is to install the open source authentication and authorization component for your ML platform.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor066"/>Installing Keycloak on Kubernetes</h2>
			<p>We will use Keycloak (<a href="https://www.keycloak.org">https://www.keycloak.org</a>) as our identity provider and add authentication and <a id="_idIndexMarker323"/>access management capabilities for your platform. Keycloak supports <a id="_idIndexMarker324"/>industry-standard security mechanisms such as <strong class="bold">OAuth2</strong> and <strong class="bold">OpenID Connect</strong>. In this section, you will install the <a id="_idIndexMarker325"/>Keycloak server on the Kubernetes cluster and log in to the Keycloak UI to <a id="_idIndexMarker326"/>validate the installation:</p>
			<ol>
				<li value="1">Start by creating a new namespace for the <strong class="source-inline">keycloak</strong> application:<p class="source-code"><strong class="bold">kubectl create ns keycloak</strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B18332_04_011.jpg" alt="Figure 4.11 – Output for creating a new namespace for Keycloak&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.11 – Output for creating a new namespace for Keycloak</p>
			<ol>
				<li value="2">Create the Keycloak manifest using the provided YAML file:<p class="source-code"><strong class="bold">kubectl create -f chapter4/keycloak.yaml --namespace keycloak</strong></p></li>
				<li>Validate that the <strong class="source-inline">keycloak</strong> pods are running. Note that the <strong class="source-inline">--namespace</strong> and <strong class="source-inline">-n</strong> flags are interchangeable in <strong class="source-inline">kubectl</strong>:<p class="source-code"><strong class="bold">kubectl get pods -n keycloak</strong></p></li>
			</ol>
			<p>It may take a while to start, as it will start by pulling container images from the internet. The first time you run the command, you might see that the <strong class="bold">READY </strong>status is <strong class="bold">0/1</strong>. This is normal. Once the <strong class="source-inline">Keycloak</strong> pod is running, you should see the following response:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B18332_04_012.jpg" alt="Figure 4.12 – Validate that the Keycloak pods are in running state&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.12 – Validate that the Keycloak pods are in running state</p>
			<p>In the next few steps, you will define and configure the ingress for your Keycloak pod so that it <a id="_idIndexMarker327"/>can be accessed from outside the cluster.</p>
			<ol>
				<li value="4">Get the IP address of your <strong class="source-inline">minikube</strong> machine by issuing the following command:<p class="source-code"><strong class="bold">minikube ip</strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B18332_04_013.jpg" alt="Figure 4.13 – IP address of your minikube instance&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.13 – IP address of your minikube instance</p>
			<ol>
				<li value="5">Open the <strong class="source-inline">chapter4/keycloak-ingress.yaml</strong> file and replace the <strong class="source-inline">KEYCLOAK_HOST</strong> string with the <strong class="source-inline">keycloak.&lt;THE_IP_ADDRESS_OF_YOUR_MINIKUBE&gt;.nip.io</strong> string. So, if the IP address of your <strong class="source-inline">minikube</strong> is <strong class="source-inline">192.168.61.72</strong>, then the string value would be <strong class="source-inline">keycloak.192.168.61.72.nip.io</strong> .</li>
			</ol>
			<p>There are two places in the file where you need to put this new string. The file will look like <em class="italic">Figure 4.14</em>. Do not forget to save the changes in this file.</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B18332_04_014.jpg" alt="Figure 4.14 – The IP address of your minikube instance changed in the keycloak-ingress file&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.14 – The IP address of your minikube instance changed in the keycloak-ingress file</p>
			<p>Apply the modified file to the Kubernetes cluster. This <strong class="source-inline">ingress</strong> object will create the <a id="_idIndexMarker328"/>required configuration for you to access the Keycloak server from outside the Kubernetes cluster. Run the following command to create the ingress object:</p>
			<p class="source-code"><strong class="bold">kubectl create -f chapter4/keycloak-ingress.yaml --namespace keycloak</strong></p>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B18332_04_015.jpg" alt="Figure 4.15 – Modified ingress has been applied &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.15 – Modified ingress has been applied </p>
			<ol>
				<li value="6">Validate that the <strong class="source-inline">ingress</strong> object is available by issuing the following command:<p class="source-code"><strong class="bold">kubectl get ingress --namespace keycloak</strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B18332_04_016.jpg" alt="Figure 4.16 – Ingress object has been created&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.16 – Ingress object has been created</p>
			<ol>
				<li value="7">Now that you have validated that Keycloak is running and is exposed through the <strong class="source-inline">ingress</strong> object, open a browser on your machine where <strong class="source-inline">minikube</strong> is running and <a id="_idIndexMarker329"/>access the following URL. You need to replace the correct IP address, as stated in s<em class="italic">tep 5</em>: https://keycloak.192.168.61.72.nip.io/auth/.</li>
			</ol>
			<p>You will get a warning that the <em class="italic">certificate is not valid</em>. This is because the Keycloak server uses a self-signed certificate by default. You just need to click the <strong class="bold">Advance</strong> button presented by the browser and choose to continue to the website.</p>
			<p>You should see the following page; click on the <strong class="bold">Administration Console</strong> link to proceed further:</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B18332_04_017.jpg" alt="Figure 4.17 – Keycloak landing page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.17 – Keycloak landing page</p>
			<ol>
				<li value="8">Log in using <a id="_idIndexMarker330"/>the credentials <em class="italic">admin/admin</em> in the following screen. After you enter the credentials, click <strong class="bold">Sign in</strong>:</li>
			</ol>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B18332_04_018.jpg" alt="Figure 4.18 – Keycloak login page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.18 – Keycloak login page</p>
			<ol>
				<li value="9">Validate that <a id="_idIndexMarker331"/>the main administration page of Keycloak is displayed as follows:</li>
			</ol>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B18332_04_019.jpg" alt="Figure 4.19  – Keycloak administration page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.19  – Keycloak administration page</p>
			<p>Congratulations! You have successfully installed the ODH operator and Keycloak onto your Kubernetes cluster.</p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor067"/>Summary</h1>
			<p>In this chapter, you have learned about the major components of your ML platform and how open source community projects provide software products for each of those components. Using open source software enables a great number of people to use software for free, while at the same time, contributing to improving the components while continuously evolving and adding new capabilities to the software.</p>
			<p>You have installed the operator required to set up the ML platform on your Kubernetes cluster. You have installed the ingress controller to allow traffic into your cluster and installed Keycloak to provide the identity and access management capabilities for your platform.</p>
			<p>The foundation has been set for us to go deeper into each component of the ML life cycle. In the next chapter, you will learn to set up Spark and JupyterHub on your platform, which enables data engineers to build and deploy data pipelines.</p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor068"/>Further reading</h1>
			<ul>
				<li>Data preparation is the least enjoyable task in data science:  <a href="https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/?sh=1e5986216f63">https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/?sh=1e5986216f63</a></li>
			</ul>
		</div>
	</body></html>