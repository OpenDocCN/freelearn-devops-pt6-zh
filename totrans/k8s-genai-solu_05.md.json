["```\n    module \"eks_blueprints_addons\" {\n      ....\n      eks_addons = {\n        aws-ebs-csi-driver = {\n          service_account_role_arn = module.ebs_csi_driver_irsa.iam_role_arn\n        }\n        ....\n    }\n    module \"ebs_csi_driver_irsa\" {\n    ...\n      role_name_prefix = format(\"%s-%s\", local.name, \"ebs-csi-driver-\")\n      attach_ebs_csi_policy = true\n    ...\n    }\n    ```", "```\n    resource \"kubernetes_annotations\" \"disable_gp2\" {\n      annotations = {\n        \"storageclass.kubernetes.io/is-default-class\": \"false\"\n    ...\n      metadata {\n        name = \"gp2\"\n    ...\n    resource \"kubernetes_storage_class\" \"default_gp3\" {\n      metadata {\n        name = \"gp3\"\n        annotations = {\n          \"storageclass.kubernetes.io/is-default-class\": \"true\"\n      ...\n    }\n    ```", "```\n    $ terraform init\n    $ terraform plan\n    $ terraform apply -auto-approve\n    ```", "```\n    $ aws eks describe-addon --cluster-name eks-demo --addon-name aws-ebs-csi-driver\n    ...\n            \"addonName\": \"aws-ebs-csi-driver\",\n            \"clusterName\": \"eks-demo\",\n            \"status\": \"ACTIVE\",\n    eks-data-addons (https://registry.terraform.io/modules/aws-ia/eks-data-addons/aws/latest) Terraform module on the EKS cluster. This open source module can be utilized to deploy commonly used data and AI/ML K8s add-ons. Please refer to the Terraform documentation page at https://registry.terraform.io/modules/aws-ia/eks-data-addons/aws/latest#resources to find a list of available add-ons. Make sure you download the aiml-addons.tf file from https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/aiml-addons.tf; it includes Terraform code to deploy the JupyterHub add-on on the EKS cluster. Let’s walk through what the Terraform code does:*   A random 16-character string is created to secure access to JupyterHub:\n\n        Important note\n\n        In this setup, we are using a dummy authentication method where JupyterHub uses a static username and password. It also provides other authentication methods, as listed at [https://jupyterhub.readthedocs.io/en/latest/reference/authenticators.html](https://jupyterhub.readthedocs.io/en/latest/reference/authenticators.html). Use the method that fits your needs.\n\n        ```", "```\n\n        *   A new K8s namespace called `jupyterhub` is defined to deploy the JupyterHub Helm chart:\n\n        ```", "```\n\n        *   A K8s service account and an IAM role with appropriate S3 permissions to read from the S3 buckets are defined for interacting with S3 via Jupyter notebooks. We are using the *IAM roles for service accounts* ([https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html)) feature of Amazon EKS, which provides IAM credentials to applications running in K8s Pods securely:\n\n        ```", "```\n\n        *   Now, we must deploy the JupyterHub Helm chart. We are using Helm values from a public S3 bucket available at [https://kubernetes-for-genai-models.s3.amazonaws.com/chapter5/jupyterhub-values.yaml](https://kubernetes-for-genai-models.s3.amazonaws.com/chapter5/jupyterhub-values.yaml). It contains the necessary configuration to enable dummy authentication using the password we randomly generated previously and uses a K8s service account for Jupyter notebook Pods:\n\n        ```", "```\n\n        *   Run the following commands to deploy JupyterHub on the EKS cluster:\n\n        ```", "```\n\n        *   Verify that JupyterHub has been installed by running the following commands:\n\n        ```", "```\n        module \"eks\" {\n          ....\n          eks_managed_node_groups = {\n            eks-gpu-mng = {\n              instance_types = [\"g6.2xlarge\"]\n              capacity_type = \"SPOT\"\n              taints = {\n                gpu = {\n                  key = \"nvidia.com/gpu\"\n                  value = \"true\"\n                  effect = \"NO_SCHEDULE\"\n          ....\n        }\n        ```", "```\n\n        \t\t\t\t*   Run the following commands to add the GPU node group to the EKS cluster. Please note that this may take 5-10 minutes. You can verify the GPU node’s status by running the following `kubectl` command, which outputs the node’s name and status:\n\n        ```", "```\n\n        \t\t\t\t*   Now, we can connect to the JupyterHub console to create a notebook. In this setup, we’ve limited JupyterHub console access to within the cluster by exposing it as a **ClusterIP** service. Run the following commands to connect to the console locally; alternatively, you can set the service type to **LoadBalancer** to expose it via a public NLB:\n\n        ```", "```\n\n        \t\t\t\t*   You can launch the JupyterHub console by navigating to http://localhost:8000/ in your web browser. You’ll see a login page, similar to what’s shown in *Figure 5**.2*. Here, we’ve pre-created a user named *k8s-admin1* as part of our JupyterHub installation. Run the following command to retrieve the password of that user:\n\n        ```", "```\n\n    ```", "```\n    $ kubectl get pods -n jupyterhub -l component=singleuser-server\n    NAME                   READY   STATUS    RESTARTS   AGE\n    jupyter-k8s-2dadmin1   1/1     Running   0          9m\n    ```", "```\n$ aws s3 ls s3://kubernetes-for-genai-models/chapter5/\n...\nloyalty_qa_train.jsonl\nloyalty_qa_val.jsonl\n...\n```", "```\n    $ mkdir -p llama-finetuning\n    datasets library:\n\n    ```", "```\n\n    ```", "```\n    trainer.save_model(f\"./{fine_tuned_model_name}\")\n    tokenizer.save_pretrained(f\"./{fine_tuned_model_name}\")\n    ```", "```\n    ...\n    def sync_folder_to_s3(local_folder, bucket_name, s3_folder):\n        s3 = boto3.client('s3')\n        for root, dirs, files in os.walk(local_folder):\n            for file in files:\n    ...\n                try:\n                    s3.upload_file(local_path, bucket_name, s3_path)\n                except Exception as e:\n                    print(f'Error uploading {local_path}: {e}')\n    ...\n    sync_folder_to_s3('./'+fine_tuned_model_name+'/', model_assets_bucket, fine_tuned_model_name)\n    ```", "```\n    FROM nvidia/cuda:12.8.1-runtime-ubuntu24.04\n    ...\n    RUN pip install torch transformers datasets peft accelerate bitsandbytes sentencepiece s3fs boto3\n    ...\n    COPY fine_tune.py /app/fine_tune.py\n    CMD [\"python\", \"fine_tune.py\"]\n    ```", "```\n    $ docker build -t my-llama-finetuned .\n    ```", "```\n    $ docker images\n    REPOSITORY           TAG       IMAGE ID\n    my-llama-finetuned   latest    207a07f1bf00\n    ```", "```\n     resource \"aws_ecr_repository\" \"my-llama-finetuned\" {\n      name = \"my-llama-finetuned\"\n    ...\n    ```", "```\n    resource \"random_string\" \"bucket_suffix\" {\n      length  = 8\n    ...\n    resource \"aws_s3_bucket\" \"my_llama_bucket\" {\n      bucket = \"my-llama-bucket-${random_string.bucket_suffix.result}\"\n    ...\n    output \"my_llama_bucket\" {\n      value = \"${aws_s3_bucket.my_llama_bucket.id}\"\n    ...\n    ```", "```\n    module \"llama_fine_tuning_irsa\" {\n    ...\n      role_name = \"${module.eks.cluster_name}-llama-fine-tuning\"\n      role_policy_arns = {\n        policy = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n      }\n    ...\n    resource \"kubernetes_service_account_v1\" \"llama_fine_tuning_sa\" {\n      metadata {\n        name        = \"llama-fine-tuning-sa\"\n    ...\n    ```", "```\n    $ terraform init\n    $ terraform plan\n    terraform output command to list the ECR upload commands. Copy and paste those output commands into your terminal to push the my-llama-finetuned container image to ECR:\n\n    ```", "```\n\n    ```", "```\n    apiVersion: batch/v1\n    kind: Job\n    metadata:\n      name: my-llama-job\n    spec:\n    ...\n          containers:\n          - name: my-llama-job-container\n            image: <<Replace your ECR image name here>>\n            env:\n            - name: MODEL_ASSETS_BUCKET\n              value: \"<<Replace your S3 bucket here>>\"\n            - name: HUGGING_FACE_HUB_TOKEN\n              value: \"<<Replace your Hugging face token here>>\"\n            - name: TRAIN_DATASET_FILE\n              value: \"s3://kubernetes-for-genai-models/chapter5/loyalty_qa_train.jsonl\"\n            - name: EVAL_DATASET_FILE\n              value: \"s3://kubernetes-for-genai-models/chapter5/loyalty_qa_val.jsonl\"\n    ...\n    ```", "```\n    $ kubectl apply -f llama-finetuning-job.yaml\n    job.batch/my-llama-job is created\n    ```", "```\n    $ kubectl logs -f job/my-llama-job\n    ```", "```\n    $ mkdir -p llama-finetuned-inf\n    $ cd llama-finetuned-inf\n    ```", "```\n    nvidia/cuda as the parent image, installing the necessary Python dependencies, and adding the fine-tuned model from the model-assets directory. The complete file is available at https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/Dockerfile:\n\n    ```", "```\n\n    ```", "```\n    ...\n    app = FastAPI()\n    # Load tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained('./model-assets')\n    # Define the quantization configuration for 8-bit\n    base_model_id = \"meta-llama/Meta-Llama-3-8B\"\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\n    base_model = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=torch.float16, quantization_config=bnb_config, device_map='auto')\n    # Load the Peft Model from pre-trained assets\n    model = PeftModel.from_pretrained(base_model, './model-assets')\n    ...\n    @app.route('/generate')\n    async def generate(request: Request):\n    ...\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=100,\n    ...\n        # Decode the response and return it\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return {\"response\": response}\n    ```", "```\n    $ docker build -t my-llama-finetuned:inf .\n    ```", "```\n    $ aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned\n    $ docker tag my-llama-finetuned:inf 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned:inf\n    finetuned-inf-deploy.yaml K8s manifest from https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/finetuned-inf-deploy.yaml and replace the ECR image and Hugging Face access token:\n\n    ```", "```\n\n    ```", "```\n    $ kubectl apply -f finetuned-inf-deploy.yaml\n    deployment.apps/my-llama-finetuned-deployment created\n    service/my-llama-finetuned-svc created\n    ```", "```\n    $ kubectl get all -l app.kubernetes.io/name=my-llama-finetuned\n    NAME         READY       STATUS        RESTARTS       AGE\n    pod/my-llama-finetuned-deployment-54c75f55fc-77tbc\n                 1/1         Running       0              116s\n    NAME      TYPE     CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\n    service/my-llama-finetuned-svc   ClusterIP   172.20.86.243   <none>        80/TCP    116s\n    ...\n    ```", "```\n    ...\n    resource \"helm_release\" \"qdrant\" {\n      name       = \"qdrant\"\n      repository = \"https://qdrant.github.io/qdrant-helm\"\n      chart      = \"qdrant\"\n      namespace  = \"qdrant\"\n      create_namespace = true\n    }\n    ```", "```\n     resource \"aws_ecr_repository\" \"rag-app\" {\n      name = \"rag-app\"\n    }\n    ```", "```\n    $ terraform init\n    $ terraform plan\n    $ terraform apply -auto-approve\n    $ kubectl get pods -n qdrant\n    NAME       READY   STATUS    RESTARTS   AGE\n    qdrant-0   1/1     Running   0          2m\n    ```", "```\n    /load_data that takes an input filename and creates vector embeddings by calling the OpenAI API endpoint and stores the embeddings in the Qdrant database:\n\n    ```", "```\n\n    ```", "```\n    @app.post(\"/generate\")\n    async def generate_answer(prompt_model: PromptModel):\n        try:\n            prompt = prompt_model.prompt\n            session_id = prompt_model.session_id\n    ...\n            qdrant_store = QdrantVectorStore(\n                embedding=OpenAIEmbeddings(),\n                collection_name=collection_name,\n                client=qdrant_client)\n            history_aware_retriever = create_history_aware_retriever(llm, qdrant_store.as_retriever(), contextualize_q_prompt)\n    ```", "```\n    ...\n            rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n            conversational_rag_chain = RunnableWithMessageHistory(\n                rag_chain,\n                get_session_history,\n                input_messages_key=\"input\",\n                history_messages_key=\"chat_history\",\n                output_messages_key=\"answer\")\n    ...\n    ```", "```\n            result = conversational_rag_chain.invoke(\n                {\"input\": prompt},\n                config= {\"configurable\": {\"session_id\": session_id}},\n            )[\"answer\"]\n    ...\n            return JSONResponse({\"answer\": result, \"session_id\": session_id}, status_code=200)\n    ```", "```\n    $ mkdir -p rag-app\n    $ cd rag-app\n    ```", "```\n    FROM python:slim\n    ...\n    RUN pip install --no-cache-dir -r requirements.txt\n    COPY main.py /app\n    ...\n    CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n    ```", "```\n    $ docker build -t rag-app .\n    $ aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.us-west-2.amazonaws.com/rag-app\n    $ docker tag rag-app 123456789012.dkr.ecr.us-west-2.amazonaws.com/rag-app\n    $ docker push image and OPENAI_API_KEY with your own values. Follow the OpenAI documentation at https://platform.openai.com/docs/quickstart/create-and-export-an-api-key to learn how to generate an API key. The following manifest creates a K8s deployment for the RAG application with one replica and injects the OpenAI API key as an environment variable so that the application can use it to connect to the OpenAI API:\n\n    ```", "```\n\n    ```", "```\n    $ kubectl apply -f rag-deploy.yaml\n    deployment.apps/rag-app-deployment created\n    service/rag-app-service created\n    $ kubectl get po,svc -l app.kubernetes.io/name=rag-app\n    NAME          READY       STATUS        RESTARTS         AGE\n    pod/rag-app-deployment-c4b4b49d4-wclwz\n                  1/1         Running       0                4m26s\n    NAME       TYPE      CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\n    qdrant-restore-job.yaml file from https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/qdrant-restore-job.yaml and run the following command to restore the snapshot:\n\n    ```", "```\n\n    ```", "```\n    $ kubectl apply -f chatbot-deploy.yaml\n    deployment.apps/chatbot-ui-deployment created\n    service/chatbot-ui-service created\n    ```", "```\n    $ export NLB_URL=$(kubectl get svc chatbot-ui-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n    $ echo $NLB_URL\n    ```"]