- en: Playing with Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling your containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating live containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forwarding container ports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring flexible usage of your containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Submitting Jobs on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with configuration files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When talking about container management, you need to know some of the differences
    compared to application package management, such as `rpm`/`dpkg`, because you
    can run multiple containers on the same machine. You also need to care about network
    port conflicts. This chapter covers how to update, scale, and launch a container
    application using Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling your containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling up and down the application or service based on predefined criteria
    is a common way to utilize the most compute resources in most efficient way. In
    Kubernetes, you can scale up and down manually or use a **Horizontal Pod Autoscaler**
    (**HPA**) to do autoscaling. In this section, we'll describe how to perform both
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Prepare the following YAML file, which is a simple Deployment that launches
    two `nginx` containers. Also, a NodePort service with TCP—`30080` exposed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`NodePort` will bind to all the Kubernetes nodes (port range: `30000` to `32767`);
    therefore, make sure `NodePort` is not used by other processes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use `kubectl` to create the resources used by the preceding configuration
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few seconds, we should see that the `pods` are scheduled and up and
    running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The service is up, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assume our services are expected to have a traffic spike at a certain of time.
    As a DevOps, you might want to scale it up manually, and scale it down after the
    peak time. In Kubernetes, we can use the `kubectl scale` command to do so. Alternatively,
    we could leverage a HPA to scale up and down automatically based on compute resource
    conditions or custom metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to do it manually and automatically in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Scale up and down manually with the kubectl scale command
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Assume that today we''d like to scale our `nginx` Pods from two to four:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check how many `pods` we have now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We could find two more Pods are scheduled. One is already running and another
    one is creating. Eventually, we will have four Pods up and running if we have
    enough compute resources.
  prefs: []
  type: TYPE_NORMAL
- en: Kubectl scale (also kubectl autoscale!) supports **Replication Controller**
    (**RC**) and **Replica Set** (**RS**), too. However, deployment is the recommended
    way to deploy Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could also scale down with the same `kubectl` command, just by setting the `replicas`
    parameter lower:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''ll see two Pods are scheduled to be terminated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'There is an option, `--current-replicas`, which specifies the expected current
    replicas. If it doesn''t match, Kubernetes doesn''t perform the scale function
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Horizontal Pod Autoscaler (HPA)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An HPA queries the source of metrics periodically and determines whether scaling
    is required by a controller based on the metrics it gets. There are two types
    of metrics that could be fetched; one is from Heapster ([https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)),
    another is from RESTful client access. In the following example, we'll show you
    how to use Heapster to monitor Pods and expose the metrics to an HPA.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, Heapster has to be deployed in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: If you're running minikube, use the `minikube addons enable heapster` command
    to enable heapster in your cluster. Note that `minikube logs | grep heapster command` could
    also be used to check the logs of heapster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Check if the `heapster` `pods` are up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming we continue right after the *Getting Ready* section, we will have
    two `my-nginx` Pods running in our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can use the `kubectl autoscale` command to deploy an HPA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To check if it''s running as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We find the target shows as unknown and replicas are 0\. Why is this? the runs
    as a control loop, at a default interval of 30 seconds. There might be a delay
    before it reflects the real metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default sync period of an HPA can be altered by changing the following
    parameter in control manager: `--horizontal-pod-autoscaler-sync-period`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After waiting a couple of seconds, we will find the current metrics are there
    now. The number showed in the target column presents (`current / target`). It
    means the load is currently `0%`, and scale target is `50%`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To test if HPA can scale the Pod properly, we''ll manually generate some loads
    to `my-nginx` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding command, we ran a `busybox` image which allowed us to run a
    simple command on it. We used the `–c` parameter to specify the default command,
    which is an infinite loop, to query `my-nginx` service.
  prefs: []
  type: TYPE_NORMAL
- en: 'After about one minute, you can see that the current value is changing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'With the same command, we can run more loads with different Pod names repeatedly.
    Finally, we see that the condition has been met. It''s scaling up to `3` replicas,
    and up to `4` replicas afterwards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can see that HPA just scaled our Pods from `4` to `2`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Note that cAdvisor acts as a container resource utilization monitoring service,
    which is running inside kubelet on each node. The CPU utilizations we just monitored
    are collected by cAdvisor and aggregated by Heapster. Heapster is a service running
    in the cluster that monitors and aggregates the metrics. It queries the metrics
    from each cAdvisor. When HPA is deployed, the controller will keep observing the
    metrics which are reported by Heapster, and scale up and down accordingly. An
    illustration of the process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/290e9c1e-2be5-4a53-a740-802ddb0147da.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on the specified metrics, HPA determines whether scaling is required
  prefs: []
  type: TYPE_NORMAL
- en: There is more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alternatively, you could use custom metrics, such as Pod metrics or object metrics,
    to determine if it's time to scale up or down. Kubernetes also supports multiple
    metrics. HPA will consider each metric sequentially. Check out [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale)
    for more examples.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe described how to change the number of Pods using the scaling option
    of the deployment. It is useful to scale up and scale down your application quickly.
    To know more about how to update your container, refer to the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Updating live containers* in [Chapter 3](51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml),
    *Playing with Containers*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ensuring flexible usage of your containers* in [Chapter 3](51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml)*,
    Playing with Containers*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating live containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the benefit of containers, we can easily publish new programs by executing
    the latest image, and reduce the headache of environment setup. But, what about
    publishing the program on running containers? While managing a container natively,
    we have to stop the running containers prior to booting up new ones with the latest
    images and the same configurations. There are some simple and efficient methods
    for updating your program in the Kubernetes system. One is called rolling-update,
    which means Deployment can update its Pods without downtime to clients. The other
    method is called *recreate*, which just terminates all Pods then create a new
    set. We will demonstrate how these solutions are applied in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rolling-update in Docker swarmTo achieve zero downtime application updating,
    there is a similar managing function in Docker swarm. In Docker swarm, you can
    leverage the command docker service update with the flag `--update-delay`, `--update-parallelism`
    and `--update-failure-action`. Check the official website for more details about
    Docker swarm''s rolling-update: [https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/](https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/).'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For a later demonstration, we are going to update `nginx` Pods . Please make
    sure all Kubernetes nodes and components are working healthily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Furthermore, to well understand the relationship between ReplicaSet and Deployment,
    please check *Deployment API *section in *[Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml),
    Walking through Kubernetes Concepts*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the updating of the containers in Kubernetes system, we will
    create a Deployment, change its configurations of application, and then check
    how the updating mechanism handles it. Let''s get all our resources ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This Deployment is created with `5` replicas. It is good for us to discover
    the updating procedure with multiple numbers of Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Attaching a Service on the Deployment will help to simulate the real experience
    of clients.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the beginning, take a look at the Deployment you just created and its ReplicaSet
    by executing the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Based on the preceding output, we know that the default updating strategy of
    deployment is rolling-update. Also, there is a single ReplicaSet named `<Deployment
    Name>-<hex decimal hash>` that is created along with the Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, check the content of the current Service endpoint for the sake of verifying
    our update later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We will get the welcome message in the title of the HTML response with the original
    `nginx` image.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment update strategy – rolling-update
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following will introduce the subcommands `edit` and `set`, for the purpose
    of updating the containers under Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s update the Pods in Deployment with a new command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We are not only doing the update; we record this change as well. With the flag
    `--record`, we keep the command line as a tag in revision.
  prefs: []
  type: TYPE_NORMAL
- en: 'After editing the Deployment, check the status of rolling-update with the subcommand
    `rollout` right away:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: It is possible that you get several `Waiting for …` lines, as shown in the preceding
    code. They are the standard output showing the status of the update.
  prefs: []
  type: TYPE_NORMAL
- en: 'For whole updating procedures, check the details of the Deployment to list
    its events:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you see, a new `replica set simple-nginx-694f94f77d` is created in the Deployment
    `simple-nginx`. Each time the new ReplicaSet scales one Pod up successfully, the
    old ReplicaSet will scale one Pod down. The scaling process finishes at the moment
    that the new ReplicaSet meets the original desired Pod number (as said, `5` Pods),
    and the old ReplicaSet has zero Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go ahead and check the new ReplicaSet and existing Service for this update:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Let's make another update! This time, use the subcommand `set` to modify a specific
    configuration of a Pod.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To set a new image to certain containers in a Deployment, the subcommand format
    would look like this: `kubectl set image deployment <Deployment name> <Container
    name>=<image name>`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: What else could the subcommand "set" help to configure?
  prefs: []
  type: TYPE_NORMAL
- en: 'The subcommand set helps to define the configuration of the application. Until
    version 1.9, CLI with set could assign or update the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Subcommand after set** | **Acting resource** | **Updating item** |'
  prefs: []
  type: TYPE_TB
- en: '| `env` | Pod | Environment variables |'
  prefs: []
  type: TYPE_TB
- en: '| `image` | Pod | Container image |'
  prefs: []
  type: TYPE_TB
- en: '| `resources` | Pod | Computing resource requirement or limitation |'
  prefs: []
  type: TYPE_TB
- en: '| `selector` | Any resource | Selector |'
  prefs: []
  type: TYPE_TB
- en: '| `serviceaccount` | Any resource | ServiceAccount |'
  prefs: []
  type: TYPE_TB
- en: '| `subject` | RoleBinding or ClusterRoleBinding | User, group, or ServiceAccount
    |'
  prefs: []
  type: TYPE_TB
- en: 'Now, check if the update has finished and whether the image is changed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also check out the ReplicaSets. There should be another one taking
    responsibility of the Pods for Deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Rollback the update
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes system records every update for Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can list all of the revisions with the subcommand `rollout`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: You will get three revisions, as in the preceding lines, for the Deployment
    `simple-nginx`. For Kubernetes Deployment, each revision has a matched `ReplicaSet`
    and represents a stage of running an update command. The first revision is the
    initial state of `simple-nginx`. Although there is no command tag for indication,
    Kubernetes takes its creation as its first version. However, you could still record
    the command when you create the Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Add the flag `--record` after the subcommand `create` or `run`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With the revisions, we can easily resume the change, which means rolling back
    the update. Use the following commands to rollback to previous revisions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Without specifying the revision number, the rollback process will simply jump
    back to previous version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Deployment update strategy – recreate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we are going to introduce the other update strategy, `recreate`, for
    Deployment. Although there is no subcommand or flag to create a recreate-strategy deployment,
    users could fulfill this creation by overriding the default element with the specified
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In our understanding, the `recreate` mode is good for an application under
    development. With `recreate`, Kubernetes just scales the current ReplicaSet down
    to zero Pods, and creates a new ReplicaSet with the full desired number of Pods.
    Therefore, recreate has a shorter total updating time than rolling-update because
    it scales ReplicaSets up or down simply, once for all. Since a developing Deployment
    doesn''t need to take care of any user experience, it is acceptable to have downtime
    while updating and enjoy faster updates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rolling-update works on the units of the ReplicaSet in a Deployment. The effect
    is to create a new ReplicaSet to replace the old one. Then, the new ReplicaSet
    is scaling up to meet the desired numbers, while the old ReplicaSet is scaling
    down to terminate all the Pods in it. The Pods in the new ReplicaSet are attached
    to the original labels. Therefore, if any service exposes this Deployment, it
    will take over the newly created Pods directly.
  prefs: []
  type: TYPE_NORMAL
- en: An experienced Kubernetes user may know that the resource ReplicationController
    can be rolling-update as well. So, what are the differences of rolling-update
    between ReplicationController and deployment? The scaling processing uses the
    combination of ReplicationController and a client such as `kubectl`. A new ReplicationController
    will be created to replace the previous one. Clients don't feel any interruption
    since the service is in front of ReplicationController while doing replacement.
    However, it is hard for developers to roll back to previous ReplicationControllers
    (they have been removed), because there is no built-in mechanism that records
    the history of updates.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, rolling-update might fail if the client connection is disconnected
    while rolling-update is working. Most important of all, Deployment with ReplicaSet
    is the most recommended deploying resource than ReplicationController or standalone
    ReplicaSet.
  prefs: []
  type: TYPE_NORMAL
- en: 'While paying close attention to the history of update in deployment, be aware
    that it is not always listed in sequence. The algorithm of adding revisions could
    be clarified as the following bullet points show:'
  prefs: []
  type: TYPE_NORMAL
- en: Take the number of last revision as *N*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a new rollout update comes, it would be *N+1*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roll back to a specific revision number *X*, *X* would be removed and it would
    become *N+1*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roll back to the previous version, which means *N-1,* then *N-1* would be removed
    and it would become *N+1*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this revision management, no stale and overlapped updates occupy the rollout
    history.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Taking Deployment update into consideration is a good step towards building
    a CI/CD (continuous integration and continuous delivery) pipeline. For a more
    common usage, developers don''t exploit command lines to update the Deployment.
    They may prefer to fire some API calls from CI/CD platform, or update from a previous
    configuration file. Here comes an example working with the subcommand `apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As a demonstration, modifying the container image from `nginx` to `nginx:stable`
    (you may check the code bundle `my-update-nginx-updated.yaml` for the modification).
    Then, we can use the changed file to update with the subcommand `apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can learn another way to update your Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Digging deeper into rolling-update on Deployment, there are some parameters
    we may leverage when doing updates:'
  prefs: []
  type: TYPE_NORMAL
- en: '`minReadySeconds`: After a Pod is considered to be ready, the system still
    waits for a period of time for going on to the next step. This time slot is the
    minimum ready seconds, which will be helpful when waiting for the application
    to complete post-configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxUnavailable`: The maximum number of Pods that can be unavailable during
    updating. The value could be a percentage (the default is 25%) or an integer.
    If the value of `maxSurge` is `0`, which means no tolerance of the number of Pods over
    the desired number, the value of `maxUnavailable` cannot be `0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxSurge`: The maximum number of Pods that can be created over the desired
    number of ReplicaSet during updating. The value could be a percentage (the default
    is 25%) or an integer. If the value of `maxUnavailable` is `0`, which means the
    number of serving Pods should always meet the desired number, the value of `maxSurge`
    cannot be `0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the configuration file `my-update-nginx-advanced.yaml` in the code
    bundle, try playing with these parameters by yourself and see if you can feel
    the ideas at work.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You could continue studying the following recipes to learn more ideas about
    deploying Kubernetes resources efficiently:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling your containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with configuration files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Moving monolithic to microservices*, *Integrating with Jenkins*, *Working
    with the private Docker registry*, and *Setting up the Continuous Delivery Pipeline*
    recipes in [Chapter 5](669edaf0-c274-48fa-81d8-61150fa36df5.xhtml)*, Building
    a Continuous Delivery Pipeline*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forwarding container ports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, you have learned how to work with the Kubernetes Services
    to forward the container port internally and externally. Now, it's time to take
    it a step further to see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four networking models in Kubernetes, and we''ll explore the details
    in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Container-to-container communications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod-to-pod communications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod-to-service communications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: External-to-internal communications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we go digging into Kubernetes networking, let''s study the networking
    of Docker to understand the basic concept. Each container will have a network
    namespace with its own routing table and routing policy. By default, the network
    bridge `docker0` connects the physical network interface and virtual network interfaces
    of containers, and the virtual network interface is the bidirectional cable for
    the container network namespace and the host one. As a result, there is a pair
    of virtual network interfaces for a single container: the Ethernet interface (**eth0**)
    on the container and the virtual Ethernet interface (**veth-**) on the host.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The network structure can be expressed as in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbce3365-1d24-4212-9777-b99db15a7bf6.png)'
  prefs: []
  type: TYPE_IMG
- en: Container network interfaces on host
  prefs: []
  type: TYPE_NORMAL
- en: What is a network namespace?A network namespace is the technique provided by
    Linux kernel. With this feature, the operating system can fulfill network virtualization
    by separating the network capability into independent resources. Each network
    namespace has its own iptable setup and network devices.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Pod contains one or more containers, which run on the same host. Each Pod has
    their own IP address on an overlay network; all the containers inside a Pod see
    each other as on the same host. Containers inside a Pod will be created, deployed,
    and deleted almost at the same time. We will illustrate four communication models
    between container, Pod, and Service.
  prefs: []
  type: TYPE_NORMAL
- en: Container-to-container communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this scenario, we would focus on the communications between containers within
    single Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create two containers in one Pod: a nginx web application and a CentOS,
    which checks port `80` on localhost:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We see the count in the `READY` column becomes `2/2`, since there are two containers
    inside this Pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `kubectl describe` command, we may see the details of the Pod:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the Pod is run on node `ubuntu02` and that its IP is `192.168.79.198`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we may find that the Centos container can access the `nginx` on localhost:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s log in to node `ubuntu02` to check the network setting of these two
    containers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now, we know that the two containers created are `9e35275934c1` and `e832d294f176`.
    On the other hand, there is another container, `9b3e9caf5149`, that is created
    by Kubernetes with the Docker image `gcr.io/google_containers/pause-amd64`. We
    will introduce it later. Thereafter, we may get a detailed inspection of the containers
    with the command `docker inspect`, and by adding the command `jq `([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/))
    as a pipeline, we can parse the output information to show network settings only.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking a look at both containers covered in the same Pod:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We can see that both containers have identical network settings; the network
    mode is set to mapped container mode, leaving the other configurations cleaned.
    The network bridge container is `container:9b3e9caf5149ffb0ec14c1ffc36f94b2dd55b223d0d20e4d48c4e33228103723`.
    What is this container? It is the one created by Kubernetes, container ID `9b3e9caf5149`,
    with the image `gcr.io/google_containers/pause-amd64`.
  prefs: []
  type: TYPE_NORMAL
- en: What does the container "pause" do?Just as its name suggests, this container
    does nothing but "pause". However, it preserves the network settings, and the
    Linux network namespace, for the Pod. Anytime the container shutdowns and restarts,
    the network configuration will still be the same and not need to be recreated,
    because the "pause" container holds it. You can check its code and Dockerfile
    at [https://github.com/kubernetes/kubernetes/tree/master/build/pause](https://github.com/kubernetes/kubernetes/tree/master/build/pause)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: The "pause" container is a network container, which is created when a Pod
  prefs: []
  type: TYPE_NORMAL
- en: is created and used to handle the route of the Pod network. Then, two containers
    will share the network namespace with pause; that's why they see each other as
    localhost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a network container in DockerIn Docker, you can easily make a container
    into a network container, sharing its network namespace with another container.
    Use the command line: `$ docker run --network=container:<CONTAINER_ID or CONTAINER_NAME>
    [other options].` Then, you will be able to start a container which uses the network
    namespace of the assigned container.'
  prefs: []
  type: TYPE_NORMAL
- en: Pod-to-Pod communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned, containers in a Pod share the same network namespace. And a Pod is
    the basic computing unit in Kubernetes. Kubernetes assigns an IP to a Pod in its
    world. Every Pod can see every other with the virtual IP in Kubernetes network.
    While talking about the communication between Pods , we can separate into two
    scenarios: Pods that communicate within a node, or Pods that communicate across
    nodes. For Pods in single node, since they have separate IPs, their transmissions
    can be held by bridge, same as containers in a Docker node. However, for communication
    between Pods across nodes, how would be the package routing work while Pod doesn''t
    have the host information (the host IP)?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes uses the CNI to handle cluster networking. CNI is a framework for
    managing connective containers, for assigning or deleting the network resource
    on a container. While Kubernetes takes CNI as a plugin, users can choose the implementation
    of CNI on demand. Commonly, there are the following types of CNI:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overlay**: With the technique of packet encapsulation. Every data is wrapped
    with host IP, so it is routable in the internet. An example is flannel ([https://github.com/coreos/flannel](https://github.com/coreos/flannel)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L3 gateway**: Transmission between containers pass to a gateway node first.
    The gateway will maintain the routing table to map the container subnet and host
    IP. An example is Project Calico ([https://www.projectcalico.org/](https://www.projectcalico.org/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L2 adjacency**: Happening on L2 switching. In Ethernet, two nodes have adjacency
    if the package can be transmitted directly from source to destination, without
    passing by other nodes. An example is Cisco ACI ([https://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/kb/b_Kubernetes_Integration_with_ACI.html](https://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/kb/b_Kubernetes_Integration_with_ACI.html)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are pros and cons to every type of CNI. The former type within the bullet
    points has better scalability but bad performance, while the latter one has a
    shorter latency but requires complex and customized setup. Some CNIs cover all
    three types in different modes, for example, Contiv ([https://github.com/contiv/netplugin](https://github.com/contiv/netplugin)).
    You can get more information about CNI while checking its spec at: [https://github.com/containernetworking/cni](https://github.com/containernetworking/cni).
    Additionally, look at the CNI list on official website of Kubernetes to try out
    these CNIs: [https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this](https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this).'
  prefs: []
  type: TYPE_NORMAL
- en: After introducing the basic knowledge of the packet transaction between Pods ,
    we will continue to bring you a Kubernetes API, `NetworkPolicy`, which provides
    advanced management between the communication of Pods .
  prefs: []
  type: TYPE_NORMAL
- en: Working with NetworkPolicy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a resource of Kubernetes, NetworkPolicy uses label selectors to configure
    the firewall of Pods from infrastructure level. Without a specified NetworkPolicy,
    any Pod in the same cluster can communicate with each other by default. On the
    other hand, once a NetworkPolicy with rules is attached to a Pod, either it is
    for ingress or egress, or both, and all traffic that doesn't follow the rules
    will be blocked.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before demonstrating how to build a NetworkPolicy, we should make sure the
    network plugin in Kubernetes cluster supports it. There are several CNIs that
    support NetworkPolicy: Calico, Contive, Romana ([https://github.com/romana/kube](https://github.com/romana/kube)),
    Weave Net ([https://github.com/weaveworks/weave](https://github.com/weaveworks/weave)),
    Trireme ([https://github.com/aporeto-inc/trireme-kubernetes](https://github.com/aporeto-inc/trireme-kubernetes)),
    and others.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable CNI with NetworkPolicy support as network plugin in minikubeWhile working
    on minikube, users will not need to attach a CNI specifically, since it is designed
    as a single local Kubernetes node. However, to enable the functionality of NetworkPolicy,
    it is necessary to start a NetworkPolicy-supported CNI. Be careful, as, while
    you configure the minikube with CNI, the configuration options and procedures
    could be quite different to various CNI implementations. The following steps show
    you how to start minikube with CNI, Calico:'
  prefs: []
  type: TYPE_NORMAL
- en: We take this issue [https://github.com/projectcalico/calico/issues/1013#issuecomment-325689943](https://github.com/projectcalico/calico/issues/1013#issuecomment-325689943)
    as reference for these building steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The minikube used here is the latest version, 0.24.1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reboot your minikube: `minikube start --network-plugin=cni \`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`--host-only-cidr 172.17.17.1/24 \ --extra-config=kubelet.PodCIDR=192.168.0.0/16
    \ --extra-config=proxy.ClusterCIDR=192.168.0.0/16 \` `--extra-config=controller-manager.ClusterCIDR=192.168.0.0/16`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create Calico with the configuration file "minikube-calico.yaml" from the code
    bundle `kubectl create -f minikue-calico.yaml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To illustrate the functionality of NetworkPolicy, we are going to create a
    Pod and expose it as a service, then attach a NetworkPolicy on the Pod to see
    what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can go ahead and check the Pod''s connection from a simple Deployment,
    `busybox`, using the command `wget` with `--spider` flag to verify the existence
    of endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding result, we know that the `nginx` service can be accessed
    without any constraints. Later, let''s run a `NetworkPolicy` that restricts that
    only the Pod tagging `<test: inbound>` can access `nginx` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, in the spec of NeworkPolicy, it is configured to apply to Pods with
    the label `<run: nginx-pod>`, which is the one we have on the `pod nginx-pod`.
    Also, a rule of ingress is attached in the policy, which indicates that only Pods with
    a specific label can access `nginx-pod`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, everything is looking just like what we expected. Next, check the same
    service endpoint on our previous `busybox` Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected again, now we cannot access the `nginx-pod` service after NetworkPolicy
    is attached. The `nginx-pod` can only be touched by Pod labelled with `<test:
    inbound>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Catch up with the concept of label and selector in the recipe *Working with
    labels and selectors* in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)*,*
    *Walking through Kubernetes Concepts*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, you have learned how to create a NetworkPolicy with ingress restriction
    by Pod selector. Still, there are other settings you may like to build on your
    Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Egress restriction**: Egress rules can be applied by `.spec.egress`, which
    has similar settings to ingress.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Port restriction**: Each ingress and egress rule can point out what port,
    and with what kind of port protocol, is to be accepted or blocked. Port configuration
    can be applied through `.spec.ingress.ports` or `.spec.egress.ports`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Namespace selector**: We can also make limitations on certain Namespaces.
    For example, Pods for the system daemon might only allow access to others in the
    Namespace `kube-system`. Namespace selector can be applied with `.spec.ingress.from.namespaceSelector`
    or `.spec.egress.to.namespaceSelector`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IP block**: A more customized configuration is to set rules on certain CIDR
    ranges, which come out as similar ideas to what we work with iptables. We may
    utilize this configuration through `.spec.ingress.from.ipBlock` or `.spec.egress.to.ipBlock`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is recommended to check more details in the API document: [https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#networkpolicyspec-v1-networking](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#networkpolicyspec-v1-networking).
    Furthermore, we would like to show you some more interesting setups to fulfill
    general situations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Apply to all Pod**: A NetworkPolicy can be easily pushed to every Pod by
    setting `.spec.podSelector` with an empty value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Allow all traffic**: We may allow all incoming traffic by assigning `.spec.ingress`
    with empty value, an empty array; accordingly, outgoing traffic could be set without
    any restriction by assigning `.spec.egress` with empty value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deny all traffic**: We may deny all incoming or outgoing traffic by simply
    indicating the type of NetworkPolicy without setting any rule. The type of the
    NetworkPolicy can be set at `.spec.policyTypes`. At the same time, do not set
    `.spec.ingress or .spec.egress`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go check the code bundle for the example files `networkpolicy-allow-all.yaml`
    and `networkpolicy-deny-all.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: Pod-to-Service communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the ordinary course of events, Pods can be stopped accidentally. Then, the
    IP of the Pod can be changed. When we expose the port for a Pod or a Deployment,
    we create a Kubernetes Service that acts as a proxy or a load balancer. Kubernetes
    would create a virtual IP, which receives the request from clients and proxies
    the traffic to the Pods in a service. Let''s review how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we would create a Deployment and expose it to a Service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'At this moment, check the details of the Service with the subcommand `describe`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The virtual IP of the Service is `10.101.160.245`, which exposes the port `8080`.
    The Service would then dispatch the traffic into the two endpoints `192.168.80.5:80`
    and `192.168.80.6:80`. Moreover, because the Service is created in `NodePort`
    type, clients can access this Service on every Kubernetes node at `<NODE_IP>:30615`.
    As with our understanding of the recipe *Working with Services* in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml),
    *Walking through Kubernetes Concepts*, it is the Kubernetes daemon `kube-proxy` that
    helps to maintain and update routing policy on every node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continue on, checking the `iptable` on any Kubernetes node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attention! If you are in minikube environment, you should jump into the node
    with the command `minikube ssh`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'There will be a lot of rules showing out. To focus on policies related to the
    Service `nodeport-svc`, go through the following steps for checking them all.
    The output on your screen may not be listed in the expected order:'
  prefs: []
  type: TYPE_NORMAL
- en: Find targets under chain `KUBE-NODEPORTS` with the comment mentioned `nodeport-svc`. One
    target will be named with the prefix `KUBE-SVC-`. In the preceding output, it
    is the one named `KUBE-SVC-GFPAJ7EGCNM4QF4H`. Along with the other target `KUBE-MARK-MASQ`,
    they work on passing traffics at port `30615` to the Service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find a specific target named `KUBE-SVC-XXX` under `Chain KUBE-SERVICES`. In
    this case, it is the target named `KUBE-SVC-GFPAJ7EGCNM4QF4H`, ruled as allowing
    traffics from "everywhere" to the endpoint of `nodeport-svc`, `10.160.245:8080`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find targets under the specific `Chain KUBE-SVC-XXX`. In this case, it is `Chain
    KUBE-SVC-GFPAJ7EGCNM4QF4H`. Under the Service chain, you will have number of targets
    based on the according Pods with the prefix `KUBE-SEP-`. In the preceding output,
    they are `KUBE-SEP-TC6HXYYMMLGUSFNZ` and `KUBE-SEP-DIS6NYZTQKZ5ALQS`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find targets under specific `Chain KUBE-SEP-YYY`. In this case, the two chains
    required to take a look are `Chain KUBE-SEP-TC6HXYYMMLGUSFNZ` and `Chain KUBE-SEP-DIS6NYZTQKZ5ALQS`.
    Each of them covers two targets, `KUBE-MARK-MASQ` and `DNAT`, for incoming and
    outgoing traffics between "everywhere" to the endpoint of Pod, `192.168.80.5:80`
    or `192.168.80.6:80`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One key point here is that the Service target `KUBE-SVC-GFPAJ7EGCNM4QF4H` exposing
    its cluster IP to outside world will dispatch the traffic to chain `KUBE-SEP-TC6HXYYMMLGUSFNZ`
    and `KUBE-SEP-DIS6NYZTQKZ5ALQS` with a statistic mode random probability of 0.5\.
    Both chains have DNAT targets that work on changing the destination IP of the
    packets to the private subnet one, the one of a specific Pod.
  prefs: []
  type: TYPE_NORMAL
- en: External-to-internal communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To publish applications in Kubernetes, we can leverage either Kubernetes Service,
    with type `NodePort` or `LoadBalancer`, or Kubernetes Ingress. For NodePort service,
    as introduced in previous section, the port number of the node will be a pair
    with the Service. Like the following diagram, port `30361` on both node 1 and
    node 2 points to Service A, which dispatch the traffics to Pod1 and a Pod with
    static probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'LoadBalancer Service, as you may have learned from the recipe *Working with
    Services* in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)*, Walking
    through Kubernetes Concepts*, includes the configurations of NodePort. Moreover,
    a LoadBalancer Service can work with an external load balancer, providing users
    with the functionality to integrate load balancing procedures between cloud infrastructure
    and Kubernetes resource, such as the settings `healthCheckNodePort` and `externalTrafficPolicy`.
    **Service B** in the following image is a LoadBalancer Service. Internally, **Service
    B** works the same as **Service A**, relying on **iptables** to redirect packets
    to Pod; Externally, cloud load balancer doesn''t realize Pod or container, it
    only dispatches the traffic by the number of nodes. No matter which node is chosen
    to get the request, it would still be able to pass packets to the right Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/161638c9-b9f8-44e4-84e5-48ab67514b83.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes Services with type NodePort and type LoadBalancer
  prefs: []
  type: TYPE_NORMAL
- en: Working with Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Walking through the journey of Kubernetes networking, users get the idea that
    each Pod and Service has its private IP and corresponding port to listen on request.
    In practice, developers may deliver the endpoint of service, the private IP or
    Kubernetes DNS name, for internal clients; or, developers may expose Services
    externally by type of NodePort or LoadBalancer. Although the endpoint of Service
    is more stable than Pod, the Services are offered separately, and clients should
    record the IPs without much meaning to them. In this section, we will introduce
    `Ingress`, a resource that makes your Services work as a group. More than that,
    we could easily pack our service union as an API server while we set Ingress rules
    to recognize the different URLs, and then ingress controller works for passing
    the request to specific Services based on the rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we try on Kubernetes Ingress, we should create an ingress controller
    in cluster. Different from other controllers in `kube-controller-manager`([https://kubernetes.io/docs/reference/generated/kube-controller-manager/](https://kubernetes.io/docs/reference/generated/kube-controller-manager/)),
    ingress controller is run by custom implementation instead of working as a daemon.
    In the latest Kubernetes version, 1.10, nginx ingress controller is the most stable
    one and also generally supports many platforms. Check the official documents for
    the details of deployment: [https://github.com/kubernetes/ingress-nginx/blob/master/README.md](https://github.com/kubernetes/ingress-nginx/blob/master/README.md).
    We will only demonstrate our example on minikube; please see the following information
    box for the setup of the ingress controller.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable Ingress functionality in minikubeIngress in minikube is an add-on function.
    Follow these steps to start this feature in your environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check if the add-on ingress is enabled or not: Fire the command `minikube addons
    list` on your terminal. If it is not enabled, means it shows `ingress: disabled`,
    you should keep follow below steps.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enable ingress: Enter the command `minikube addons enable ingress`, you will
    see an output like `ingress was successfully enabled`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the add-on list again to verify that the last step does work. We expect
    that the field ingress shows as `enabled`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here comes an example to demonstrate how to work with Ingress. We would run
    up two Deployments and their Services, and an additional Ingress to expose them
    as a union. In the beginning, we would add a new hostname in the host file of
    Kubernetes master. It is a simple way for our demonstration. If you work on the
    production environment, a general use case is that the hostname should be added
    as a record in the DNS server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Our first Kubernetes Deployment and Service would be `echoserver`, a dummy
    Service showing server and request information. For the other pair of Deployment
    and Service, we would reuse the NodePort Service example from the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Go ahead and create both set of resources through configuration files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Our first Ingress makes two Services that listen at the separate URLs `/nginx`
    and `/echoserver`, with the hostname `happy.k8s.io`, the dummy one we added in
    the local host file. We use annotation `rewrite-target` to guarantee that traffic
    redirection starts from root, `/`. Otherwise, the client may get page not found
    because of surfing the wrong path. More annotations we may play with are listed
    at [https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, just create the Ingress and check its information right away:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'You may find that there is no IP address in the field of description. It will
    be attached after the first DNS lookup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Although working with Ingress is not as straightforward as other resources,
    as you have to start an ingress controller implementation by yourself, it still
    makes our application exposed and flexible. There are many network features coming
    that are more stable and user friendly. Keep up with the latest updates and have
    fun!
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last part of external-to-internal communication, we learned about Kubernetes
    Ingress, the resource that makes services work as a union and dispatches requests
    to target services. Does any similar idea jump into your mind? It sounds like
    a microservice, the application structure with several loosely coupled services.
    A complicated application would be distributed to multiple lighter services. Each
    service is developed independently while all of them can cover original functions.
    Numerous working units, such as Pods in Kubernetes, run volatile and can be dynamically
    scheduled on Services by the system controller. However, such a multi-layered
    structure increases the complexity of networking and also suffers potential overhead
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'External load balancers are not aware the existence of Pods; they only balance
    the workload to hosts. A host without any served Pod running would then redirect
    the loading to other hosts. This situation comes out of a user''s expectation
    for fair load balancing. Moreover, a Pod may crash accidentally, in which case
    it is difficult to do failover and complete the request. To make up the shortcomings,
    the idea of a service mesh focus on the networking management of microservice
    was born, dedicated to delivering more reliable and performant communications
    on orchestration like Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/efbc7c62-a23c-41ed-95d0-4c5dd0540871.png)'
  prefs: []
  type: TYPE_IMG
- en: Simpe service mesh structure
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram illustrates the main components in a service mesh. They
    work together to achieve features as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Service mesh ingress**: Using applied Ingress rules to decide which Service
    should handle the incoming requests. It could also be a proxy that is able to
    check the runtime policies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service mesh proxy**: Proxies on every node not only direct the packets,
    but can also be used as an advisory agent reporting the overall status of the
    Services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service mesh service discovery pool**: Serving the central management for
    mesh and pushing controls over proxies. Its responsibility includes procedures
    of network capability, authentication, failover, and load balancing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although well-known service mesh implementations such as Linkerd ([https://linkerd.io](https://linkerd.io))
    and Istio ([https://istio.io](https://istio.io)) are not mature enough for production
    usage, the idea of service mesh is not ignorable.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes forwards ports based on the overlay network. In this chapter, we
    also run Pods and Services with nginx. Reviewing the previous sections will help
    you to understand more about how to manipulate it. Also, look at the following
    recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: The *Creating an overlay network* and*Running your first container in Kubernetes*
    recipes in [Chap](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)[ter](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)
    [1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml), *Building Your Own Kubernetes Cluster*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Working with Pods *and*Working with Services* recipes in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)**,
    Walking through Kubernetes Concepts**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Moving monolithic to microservices* recipe in [Chapter 5](669edaf0-c274-48fa-81d8-61150fa36df5.xhtml)*,
    Building Continuous Delivery Pipelines*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring flexible usage of your containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pod, in Kubernetes, means a set of containers, which is also the smallest computing
    unit. You may have know about the basic usage of Pod in the previous recipes.
    Pods are usually managed by deployments and exposed by services; they work as
    applications with this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will discuss two new features: **DaemonSets** and **StatefulSets**.
    These two features can manage Pods with more specific purpose.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are **Daemon-like Pod **and **Stateful Pod**? The regular Pods in Kubernetes
    will determine and dispatch to particular Kubernetes nodes based on current node
    resource usage and your configuration.
  prefs: []
  type: TYPE_NORMAL
- en: However, a **Daemon-like Pod **will be created in each node. For example, if
    you have three nodes, three daemon-like Pods will be created and deployed to each
    node. Whenever a new node is added, DaemonSets Pod will be deployed to the new
    node automatically. Therefore, it will be useful to use node level monitoring
    or log correction.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a **Stateful Pod **will stick to some resources such as network
    identifier (Pod name and DNS) and **p****ersistent volume** (**PV**). This also
    guarantees an order during deployment of multiple Pods and during rolling update.
    For example, if you deploy a Pod named `my-pod`, and set the scale to **4**, then
    Pod name will be assigned as `my-pod-0`, `my-pod-1`, `my-pod-2`, and `my-pod-3`.
    Not only Pod name but also DNS and persistent volume are preserved. For example,
    when `my-pod-2` is recreated due to resource shortages or application crash, those
    names and volumes are taken over by a new Pod which is also named `my-pod-2`.
    It is useful for some cluster based applications such as HDFS and ElasticSearch.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will demonstrate how to use DaemonSets and StatefulSet; however,
    to have a better understanding, it should use multiple Kubernetes Nodes environment.
    To do this, minikube is not ideal, so instead, use either kubeadm/kubespray to
    create a multiple Node environment.
  prefs: []
  type: TYPE_NORMAL
- en: Using kubeadm or kubespray to set up Kubernetes cluster was described in [Chapter
    1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)*,* *Build Your Own Kubernetes Cluster.*
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm whether that has 2 or more nodes, type `kubectl get nodes` as follows
    to check how many nodes you have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: In addition, if you want to execute the StatefulSet recipe later in this chapter,
    you need a StorageClass to set up a dynamic provisioning environment. It was described
    in *Working with volumes* section in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml), *Walking
    through Kubernetes Concepts*. It is recommended to use public cloud such as AWS
    and GCP with a CloudProvider; this will be described in [Chapter 6](b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml),
    *Building Kubernetes on AWS* and [Chapter 7](dfc46490-f109-4f07-ba76-1a381b006d76.xhtml),
    *Building Kubernetes on GCP*, as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check whether `StorageClass` is configured or not, use `kubectl get sc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no CLI for us to create DaemonSets or StatefulSets. Therefore, we will
    build these two resource types by writing all the configurations in a YAML file.
  prefs: []
  type: TYPE_NORMAL
- en: Pod as DaemonSets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If a Kubernetes DaemonSet is created, the defined Pod will be deployed in every
    single node. It is guaranteed that the running containers occupy equal resources
    in each node. In this scenario, the container usually works as the daemon process.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following template has an Ubuntu image container that keeps
    checking its memory usage half a minute at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To build it as a DaemonSet, execute the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: As the Job, the selector could be ignored, but it takes the values of the labels.
    We will always configure the restart policy of the DaemonSet as `Always`, which
    makes sure that every node has a Pod running.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abbreviation of the `daemonset` is `ds` in `kubectl` command, use this
    shorter one in the CLI for convenience:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have two Pods running in separated nodes. They can still be recognized
    in the channel of the `pod`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'It is good for you to evaluate the result using the subcommand `kubectl logs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Whenever, you add a Kubernetes node onto your existing cluster, DaemonSets will
    recognize and deploy a Pod automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check again current status of DaemonSets, there are two Pods that have
    been deployed due to having two nodes as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'So, now we are adding one more node onto the cluster through either `kubespray`
    or `kubeadm`, based on your setup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'A few moments later, without any operation, the DaemonSet''s size become `3`
    automatically, which aligns to the number of nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Running a stateful Pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see another use case. We used Deployments/ReplicaSets to replicate the
    Pods. It scales well and is easy to maintain and Kubernetes assigns a DNS to the
    Pod using the Pod's IP address, such as `<Pod IP address>.<namespace>.pod.cluster.local`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates how the Pod DNS will be assigned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this DNS entry is not guaranteed to stay in use for this Pod, because
    the Pod might crash due to an application error or node resource shortage. In
    such a case, the IP address will possibly be changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: For some applications, this will cause an issue; for example, if you manage
    a cluster application that needs to be managed by DNS or IP address. As of the
    current Kubernetes implementation, IP addresses can't be preserved for Pods .
    How about we use Kubernetes Service? Service preserves a DNS name. Unfortunately,
    it's not realistic to create the same amount of service with Pod. In the previous
    case, create three Services that bind to three Pods one to one.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has a solution for this kind of use case that uses StatefulSet**.** It
    preserves not only the DNS but also the persistent volume to keep a bind to the
    same Pod. Even if Pod is crashed, StatefulSet guarantees the binding of the same
    DNS and persistent volume to the new Pod. Note that the IP address is not preserved
    due to the current Kubernetes implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate, use **Hadoop Distributed File System** (**HDFS**) to launch
    one NameNode and three DataNodes. To perform this, use a Docker image from [https://hub.docker.com/r/uhopper/hadoop/](https://hub.docker.com/r/uhopper/hadoop/)
    that has NameNode and DataNode images. In addition, borrow the YAML configuration
    files `namenode.yaml` and `datanode.yaml` from [https://gist.github.com/polvi/34ef498a967de563dc4252a7bfb7d582](https://gist.github.com/polvi/34ef498a967de563dc4252a7bfb7d582)
    and change a little bit:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s launch a Service and StatefulSet for `namenode` and `datanode`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the Pod naming convention is `<StatefulSet-name>-<sequence number>`. For
    example, NameNode Pod's name is `hdfs-namenode-0`. Also DataNode Pod's names are
    `hdfs-datanode-0`, `hdfs-datanode-1` and `hdfs-datanode-2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, both NameNode and DataNode have a service that is configured as
    Headless mode (by `spec.clusterIP: None`). Therefore, you can access these Pods using
    DNS as `<pod-name>.<service-name>.<namespace>.svc.cluster.local`. In this case,
    this NameNode DNS entry could be `hdfs-namenode-0.hdfs-namenode-svc.default.svc.cluster.local`*.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check what NameNode Pod''s IP address is, you can get this using `kubectl
    get pods -o wide` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, log in (run `/bin/bash`) to one of the DataNodes using `kubectl exec`
    to resolve this DNS name and check whether the IP address is `10.52.2.8` or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Looks all good! For demonstration purposes, let's access the HDFS web console
    to see DataNode's status.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, use `kubectl port-forward` to access to the NameNode web port (tcp/`50070`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding result indicates that your local machine TCP port `60107` (you
    result will vary) has been forwarded to NameNode Pod TCP port `50070`. Therefore,
    use a web browser to access `http://127.0.0.1:60107/` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5bd29558-4248-4abf-8913-3959eb180f8c.png)'
  prefs: []
  type: TYPE_IMG
- en: HDFS Web console shows three DataNodes
  prefs: []
  type: TYPE_NORMAL
- en: As you may see, three DataNodes have been registered to NameNode successfully.
    The DataNodes are also using the Headless Service so that same name convention
    assigns DNS names for DataNode as well.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DaemonSets and StatefulSets; both concepts are similar but behave differently,
    especially when Pod is crashed. Let's take a look at how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Pod recovery by DaemonSets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DaemonSets keep monitoring every Kubernetes node, so when one of the Pods crashes,
    DaemonSets recreates it on the same Kubernetes node.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simulate this, go back to the DaemonSets example and use `kubectl delete
    pods` to delete an existing Pod from `node1` manually, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, a new Pod has been created automatically to recover the Pod in
    `node1`. Note that the Pod name has been changed from `ram-check-6ldng` to `ram-check-dh5hq`—it
    has beenassigned a random suffix name. In this use case, Pod name doesn't matter,
    because we don't use hostname or DNS to manage this application.
  prefs: []
  type: TYPE_NORMAL
- en: Pod recovery by StatefulSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: StatefulSet behaves differently to DaemonSet during Pod recreation. In StatefulSet
    managed Pods, the Pod name is always consisted to assign an ordered number such
    as `hdfs-datanode-0`, `hdfs-datanode-1` and`hdfs-datanode-2`*,* and if you delete
    one of them, a new Pod will take over the same Pod name.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simulate this, let''s delete one DataNode (`hdfs-datanode-1`) to see how
    StatefulSet recreates a Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'As you see, the same Pod name (`hdfs-datanode-1`) has been assigned. Approximately
    after 10 minutes (due to HDFS''s heart beat interval), HDFS web console shows
    that the old Pod has been marked as dead and the new Pod has the in service state,
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8bb7096f-30a6-4cff-9aac-c1d831eb27e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Status when one DataNode is dead
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is not a perfect ideal case for HDFS, because DataNode-1 lost
    data and expects to re-sync from other DataNodes. If the data size is bigger,
    it may take a long time to complete re-sync.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, StatefulSets has an capability that preserve a persistent volume
    while replacing a Pod. Let's see how HDFS DataNode can preserve data during Pod recreation.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: StatefulSet with persistent volume; it requires a `StorageClass` that provisions
    a volume dynamically. Because each Pod is created by StatefulSets, it will create
    a **persistent volume claim** (**PVC**) with a different identifier. If your StatefulSets
    specify a static name of PVC, there will be trouble if multiple Pods try to attach
    the same PVC.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have `StorageClass` on your cluster, update `datanode.yaml` to add `spec.volumeClaimTemplates`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'This tells Kubernetes to create a PVC and PV when a new Pod is created by StatefulSet.
    So, that Pod template (`spec.template.spec.containers.volumeMounts`) should specify
    `hdfs-data`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s recreate HDFS cluster again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'To demonstrate, use `kubectl exec` to access the NameNode, then copy some dummy
    files to HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'At this moment, `DataNode-1` is restarting, as shown in the following image.
    However, the data directory of `DataNode-1` is kept by PVC as `hdfs-data-hdfs-datanode-1`.
    The new Pod `hdfs-datanode-1` will take over this PVC again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d97460f9-8986-43df-bf2b-3311fc0f4aca.png)'
  prefs: []
  type: TYPE_IMG
- en: StatefulSet keeps PVC/PV while restarting
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, when you access HDFS after `hdfs-datanode-1` has recovered, you
    don''t see any data loss or re-sync processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'As you see, the Pod and PV pair is fully managed by StatefulSets. It is convenient
    if you want to scale more HDFS DataNode using just the `kubectl scale` command
    to make it double or hundreds—whatever you need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: You can also use PV to NameNode to persist metadata. However, `kubectl` scale
    does not work well due to HDFS architecture. In order to have high availability
    or scale out HDFS NameNode, please visit the HDFS Federation document at : [https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/Federation.html](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/Federation.html).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we went deeply into Kubernetes Pod management through DaemonSets
    and StatefulSet. It manages Pod in a particular way, such as Pod per node and
    consistent Pod names. It is useful when the Deployments/ReplicaSets stateless
    Pod management style can''t cover your application use cases. For further information,
    consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The *Working with Pods *recipe in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml),
    *Walking through Kubernetes Concepts*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with configuration files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Submitting Jobs on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your container application is designed not only for daemon processes such as
    nginx, but also for some batch Jobs which eventually exit when the task is complete.
    Kubernetes supports this scenario; you can submit a container as a Job and Kubernetes
    will dispatch to an appropriate node and execute your Job.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will discuss two new features: **Jobs** and **CronJob**.
    These two features can make another usage of Pods to utilize your resources.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since Kubernetes version 1.2, Kubernetes Jobs has been introduced as a stable
    feature (`apiVersion: batch/v1`). In addition, CronJob is a beta feature (`apiVersion:
    batch/v1beta1`) as of Kubernetes version 1.10.'
  prefs: []
  type: TYPE_NORMAL
- en: Both work well on **minikube,** which was introduced at [Chapter 1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml),
    *Building Your Own Kubernetes Cluster. T*herefore, this recipe will use minikube
    version 0.24.1.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When submitting a Job to Kubernetes, you have three types of Job that you can
    define:'
  prefs: []
  type: TYPE_NORMAL
- en: Single Job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat Job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel Job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod as a single Job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Job-like Pod is suitable for testing your containers, which can be used for
    unit test or integration test; alternatively, it can be used for batch programs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will write a Job template to check the packages
    installed in image Ubuntu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Note that restart policy for Pods created in a Job should be set to `Never`
    or `OnFailure`, since a Job goes to termination once it is completed successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you are ready to create a `job` using your template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating a `job` object, it is possible to verify the status of both
    the Pod and Job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'This result indicates that Job is already done, executed (by `SUCCESSFUL =
    1`) in `26` seconds. In this case, Pod has already disappeared:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the `kubectl` command hints to us that we can use `--show-all` or
    `-a` option to find the completed Pod, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Here you go. So why does the `Completed` Pod object remain? Because you may
    want to see the result after your program has ended. You will find that a Pod is
    booting up for handling this task. This Pod is going to be stopped very soon at
    the end of the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the subcommand `kubectl logs` to get the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Please go ahead and check the `job package-check` using the subcommand `kubectl describe`;
    the confirmation for Pod completion and other messages are shown as system information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Later, to remove the `job` you just created, delete it with the name. This
    also removes the completed Pod as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Create a repeatable Job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Users can also decide the number of tasks that should be finished in a single Job.
    It is helpful to solve some random and sampling problems. Let''s try it on the
    same template in the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the `spec.completions` item to indicate the Pod number:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating this Job, check how the Pod looks with the subcommand `kubectl describe`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, three Pods are created to complete this Job. This is useful
    if you need to run your program repeatedly at particular times. However, as you
    may have noticed from the `Age` column in preceding result, these Pods ran sequentially,
    one by one. This means that the 2nd Job was started after the 1st Job was completed,
    and the 3rd Job was started after the 2nd Job was completed.
  prefs: []
  type: TYPE_NORMAL
- en: Create a parallel Job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If your batch Job doesn''t have a state or dependency between Jobs, you may
    consider submitting Jobs in parallel. Similar to the `spec.completions` parameter,
    the Job template has a `spec.parallelism` parameter to specify how many Jobs you
    want to run in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Re-use a repeatable Job but change it to specify `spec.parallelism: 3`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is similar to `spec.completions=3`*,* which made `3` Pods to run
    your application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if you see an `Age` column through the `kubectl describe` command,
    it indicates that `3` Pods ran at the same time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: In this setting, Kubernetes can dispatch to an available node to run your application
    and that easily scale your Jobs. It is useful if you want to run something like
    a worker application to distribute a bunch of Pods to different nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Schedule to run Job using CronJob
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are familiar with **UNIX CronJob **or **Java Quartz** ([http://www.quartz-scheduler.org](http://www.quartz-scheduler.org)),
    Kubernetes CronJob is a very straightforward tool that you can define a particular
    timing to run your Kubernetes Job repeatedly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scheduling format is very simple; it specifies the following five items:'
  prefs: []
  type: TYPE_NORMAL
- en: Minutes (0 – 59)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hours (0 – 23)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Day of Month (1 – 31)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Month (1 – 12)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Day of week (0: Sunday – 6: Saturday)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if you want to run your Job only at 9:00am on November 12th, every
    year, to send a birthday greeting to me :-), the schedule format could be `0 9
    12 11 *`.
  prefs: []
  type: TYPE_NORMAL
- en: You may also use slash (`/`) to specify a step value; a `run every 5 minutes`
    interval for the previous Job example would have the following schedule format: `*/5
    * * * *`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, there is an optional parameter, `spec.concurrencyPolicy`, that
    you can specify a behavior if the previous Job is not finished but the next Job schedule
    is approaching, to determine how the next Job runs. You can set either:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Allow**: Allow execution of the next Job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Forbid**: Skip execution of the next Job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replace**: Delete the current Job, then execute the next Job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you set as `Allow`, there might be a potential risk of accumulating some
    unfinished Jobs in the Kubernetes cluster. Therefore, during the testing phase,
    you should set either `Forbid` or `Replace` to monitor Job execution and completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few moments, the Job  will be triggered by your desired timing—in this
    case, every 5 minutes. You may then see the Job entry through the `kubectl get
    jobs` and `kubectl get pods -a` commands, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: CronJob will keep remaining until you delete; this means that, every 5 minutes,
    CronJob will create a new Job entry and related Pods will also keep getting created.
    This will impact the consumption of Kubernetes resources. Therefore, by default,
    CronJob will keep up to `3` successful Jobs (by `spec.successfulJobsHistoryLimit`)
    and one failed Job (by `spec.failedJobsHistoryLimit`). You can change these parameters
    based on your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, CronJob supplement allows Jobs to automatically to run in your application
    with the desired timing. You can utilize CronJob to run some report generation
    Jobs, daily or weekly batch Jobs, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although Jobs and CronJob are the special utilities of Pods, the Kubernetes
    system has different management systems between them and Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Job, its selector cannot point to an existing pod. It is a bad idea to
    take a Pod controlled by the deployment/ReplicaSets as a Job. The deployment/ReplicaSets
    have a desired number of Pods running, which is against Job''s ideal situation:
    Pods should be deleted once they finish their tasks. The Pod in the Deployments/ReplicaSets
    won''t reach the state of end.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we executed Jobs and CronJob, demonstrating another usage of
    Kubernetes Pod that has a completion state. Even once a Pod is completed, Kubernetes
    can preserve the logs and Pod object so that you can retrieve the result easily.
    For further information, consider:'
  prefs: []
  type: TYPE_NORMAL
- en: The *Working with Pods *recipe in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml),
    *Walking through Kubernetes Concepts*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Working with configuration files *'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with configuration files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes supports two different file formats, *YAML* and *JSON*. Each format
    can describe the same function of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we study how to write a Kubernetes configuration file, learning how to
    write a correct template format is important. We can learn the standard format
    of both YAML and JSON from their official websites.
  prefs: []
  type: TYPE_NORMAL
- en: YAML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The YAML format is very simple, with few syntax rules; therefore, it is easy
    to read and write, even for users. To know more about YAML, you can refer to the
    following website link: [http://www.yaml.org/spec/1.2/spec.html](http://www.yaml.org/spec/1.2/spec.html).
    The following example uses the YAML format to set up the `nginx` Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The JSON format is also simple and easy to read for users, but more program-friendly.
    Because it has data types (number, string, Boolean, and object), it is popular
    to exchange the data between systems. Technically, YAML is a superset of JSON,
    so JSON is a valid YAML, but not the other way around. To know more about JSON,
    you can refer to the following website link: [http://json.org/](http://json.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example of the Pod is the same as the preceding YAML format,
    but using the JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes has a schema that is defined using a verify configuration format;
    schema can be generated after the first instance of running the subcommand `create`
    with a configuration file. The cached schema will be stored under the `.kube/cache/discovery/<SERVICE_IP>_<PORT>`,
    based on the version of API server you run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Each directory listed represents an API category. You will see a file named
    `serverresources.json` under the last layer of each directory, which clearly defines
    every resource covered by this API category. However, there are some alternative
    and easier ways to check the schema. From the website of Kubernetes, we can get
    any details of how to write a configuration file of specific resources. Go ahead
    and check the official API documentation of the latest version: [https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/).
    In the webpage, there are three panels: from left to right, they are the resource
    list, description, and the input and output of HTTP requests or the command kubectl.
    Taking Deployment as an example, you may click Deployment v1 app at the resource
    list, the leftmost panel, and the following screenshot will show up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc7417bc-6efa-409f-bd4f-24b297969fd7.png)'
  prefs: []
  type: TYPE_IMG
- en: Documentation of Kubernetes Deployment API
  prefs: []
  type: TYPE_NORMAL
- en: 'But, how do we know the details of setting the container part at the marked
    place on the preceding image? In the field part of object description, there are
    two values. The first one, like apiVersion, means the name, and the second one,
    like string, is the type. Type could be integer, string, array, or the other resource
    object. Therefore, for searching the containers configuration of deployment, we
    need to know the structure of layers of objects. First, according to the example
    configuration file on web page, the layer of objects to containers is `spec.template.spec.containers.`
    So, start by clicking the hyperlink spec DeploymentSpec under Deployment''s fields,
    which is the type of resource object, and go searching hierarchically. Finally,
    you can find the details listed on this page: [https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution for tracing the configuration of containers of DeploymentHere comes
    the solution for the preceding example:'
  prefs: []
  type: TYPE_NORMAL
- en: Click spec DeploymentSpec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click template PodTemplateSpec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click spec PodSpec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click containers Container array
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now you got it!
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking a careful look at the definition of container configuration. The following
    are some common descriptions you should pay attention to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Type**: The user should always set the corresponding type for an item.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optional or not**: Some items are indicated as optional, which means not
    necessary, and can be applied as a default value, or not set if you don''t specify
    it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cannot be updated**: If the item is indicated as failed to be updated, it
    is fixed when the resource is created. You need to recreate a new one instead
    of updating it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Read-only**: Some of the items are indicated as `read-only`, such as UID.
    Kubernetes generates these items. If you specify this in the configuration file,
    it will be ignored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another method for checking the schema is through swagger UI. Kubernetes uses
    swagger ([https://](https://swagger.io)[swagger.io/](https://swagger.io)) and
    OpenAPI ([https://www.openapis.org](https://www.openapis.org)) to generate the
    REST API. Nevertheless, the web console for swagger is by default disabled in
    the API server. To enable the swagger UI of your own Kubernetes API server, just
    add the flag `--enable-swagger-ui=ture` when you start the API server. Then, by
    accessing the endpoint `https://<KUBERNETES_MASTER>:<API_SERVER_PORT>/swagger-ui`,
    you can successfully browse the API document through the web console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a34175a-0a30-4e6a-a20b-70392e605d69.png)'
  prefs: []
  type: TYPE_IMG
- en: The swagger web console of Kubernetes API
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's introduce some necessary items in configuration files for creating Pod,
    Deployment, and Service.
  prefs: []
  type: TYPE_NORMAL
- en: Pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| **Item** | **Type** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| `apiVersion` | String | `v1` |'
  prefs: []
  type: TYPE_TB
- en: '| `kind` | String | `Pod` |'
  prefs: []
  type: TYPE_TB
- en: '| `metadata.name` | String | `my-nginx-pod` |'
  prefs: []
  type: TYPE_TB
- en: '| `spec` | `v1.PodSpec` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `v1.PodSpec.containers` | Array[`v1.Container`] |  |'
  prefs: []
  type: TYPE_TB
- en: '| `v1.Container.name` | String | `my-nginx` |'
  prefs: []
  type: TYPE_TB
- en: '| `v1.Container.image` | String | `nginx` |'
  prefs: []
  type: TYPE_TB
- en: Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| **Item** | **Type** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| `apiVersion` | String | `apps`/`v1beta1` |'
  prefs: []
  type: TYPE_TB
- en: '| `kind` | String | `Deployment` |'
  prefs: []
  type: TYPE_TB
- en: '| `metadata.name` | String | `my-nginx-deploy` |'
  prefs: []
  type: TYPE_TB
- en: '| `spec` | `v1.DeploymentSpec` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `v1.DeploymentSpec.template` | `v1.PodTemplateSpec` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `v1.PodTemplateSpec.metadata.labels` | Map of string | `env: test` |'
  prefs: []
  type: TYPE_TB
- en: '| `v1.PodTemplateSpec.spec` | `v1.PodSpec` | `my-nginx` |'
  prefs: []
  type: TYPE_TB
- en: '| `v1.PodSpec.containers` | Array[`v1.Container`] | As same as Pod |'
  prefs: []
  type: TYPE_TB
- en: Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| **Item** | **Type** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| `apiVersion` | String | `v1` |'
  prefs: []
  type: TYPE_TB
- en: '| `kind` | String | `Service` |'
  prefs: []
  type: TYPE_TB
- en: '| `metadata.name` | String | `my-nginx-svc` |'
  prefs: []
  type: TYPE_TB
- en: '| `spec` | `v1.ServiceSpec` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `v1.ServiceSpec.selector` | Map of string | `env: test` |'
  prefs: []
  type: TYPE_TB
- en: '| `v1.ServiceSpec.ports` | Array[`v1.ServicePort`] |  |'
  prefs: []
  type: TYPE_TB
- en: '| `v1.ServicePort.protocol` | String | `TCP` |'
  prefs: []
  type: TYPE_TB
- en: '| `v1.ServicePort.port` | Integer | `80` |'
  prefs: []
  type: TYPE_TB
- en: Please check the code bundle file `minimal-conf-resource.yaml` to find these
    three resources with minimal configuration.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe described how to find and understand a configuration syntax. Kubernetes
    has some detailed options to define containers and components. For more details,
    the following recipes will describe how to define Pods, Deployments, and Services:'
  prefs: []
  type: TYPE_NORMAL
- en: The *Working with Pods*, *Deployment API*, and *Working with Services* recipes
    in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml), *Walking through Kubernetes
    Concepts*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
