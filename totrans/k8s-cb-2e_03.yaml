- en: Playing with Containers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩转容器
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍以下内容：
- en: Scaling your containers
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展你的容器
- en: Updating live containers
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新正在运行的容器
- en: Forwarding container ports
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转发容器端口
- en: Ensuring flexible usage of your containers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保容器的灵活使用
- en: Submitting Jobs on Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes上提交任务
- en: Working with configuration files
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置文件操作
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: When talking about container management, you need to know some of the differences
    compared to application package management, such as `rpm`/`dpkg`, because you
    can run multiple containers on the same machine. You also need to care about network
    port conflicts. This chapter covers how to update, scale, and launch a container
    application using Kubernetes.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在谈论容器管理时，你需要了解与应用程序包管理（如`rpm`/`dpkg`）的一些区别，因为你可以在同一台机器上运行多个容器。你还需要注意网络端口冲突。本章将介绍如何使用Kubernetes更新、扩展和启动容器应用程序。
- en: Scaling your containers
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展你的容器
- en: Scaling up and down the application or service based on predefined criteria
    is a common way to utilize the most compute resources in most efficient way. In
    Kubernetes, you can scale up and down manually or use a **Horizontal Pod Autoscaler**
    (**HPA**) to do autoscaling. In this section, we'll describe how to perform both
    operations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 根据预定义标准对应用或服务进行扩展和缩减是利用计算资源的高效方式之一。在Kubernetes中，你可以手动扩展和缩减，也可以使用**水平Pod自动扩缩器**（**HPA**）来进行自动扩展。在本节中，我们将介绍如何执行这两种操作。
- en: Getting ready
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Prepare the following YAML file, which is a simple Deployment that launches
    two `nginx` containers. Also, a NodePort service with TCP—`30080` exposed:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 准备以下YAML文件，这是一个简单的Deployment，启动两个`nginx`容器。同时，一个NodePort服务通过TCP暴露端口`30080`：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`NodePort` will bind to all the Kubernetes nodes (port range: `30000` to `32767`);
    therefore, make sure `NodePort` is not used by other processes.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`NodePort`会绑定到所有Kubernetes节点（端口范围：`30000`至`32767`）；因此，请确保`NodePort`没有被其他进程占用。'
- en: 'Let''s use `kubectl` to create the resources used by the preceding configuration
    file:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`kubectl`来创建前述配置文件所用的资源：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After a few seconds, we should see that the `pods` are scheduled and up and
    running:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，我们应该能够看到`pods`已经被调度并且正在运行：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The service is up, too:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 服务也已启动：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Assume our services are expected to have a traffic spike at a certain of time.
    As a DevOps, you might want to scale it up manually, and scale it down after the
    peak time. In Kubernetes, we can use the `kubectl scale` command to do so. Alternatively,
    we could leverage a HPA to scale up and down automatically based on compute resource
    conditions or custom metrics.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的服务在某个时间点会出现流量高峰。作为一名DevOps工程师，你可能希望在高峰时手动扩展服务，然后在流量高峰过后缩减服务。在Kubernetes中，我们可以使用`kubectl
    scale`命令来实现。或者，我们可以利用HPA根据计算资源条件或自定义指标自动进行扩展和缩减。
- en: Let's see how to do it manually and automatically in Kubernetes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Kubernetes中手动和自动执行此操作。
- en: Scale up and down manually with the kubectl scale command
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`kubectl scale`命令手动扩展和缩减
- en: 'Assume that today we''d like to scale our `nginx` Pods from two to four:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设今天我们想将`nginx` Pod的数量从两个扩展到四个：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s check how many `pods` we have now:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下现在有多少个`pods`：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We could find two more Pods are scheduled. One is already running and another
    one is creating. Eventually, we will have four Pods up and running if we have
    enough compute resources.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以发现两个Pod被调度了。一个已经在运行，另一个正在创建。最终，如果我们有足够的计算资源，我们将有四个Pod在运行。
- en: Kubectl scale (also kubectl autoscale!) supports **Replication Controller**
    (**RC**) and **Replica Set** (**RS**), too. However, deployment is the recommended
    way to deploy Pods.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl scale`（也包括`kubectl autoscale`！）同样支持**Replication Controller**（**RC**）和**Replica
    Set**（**RS**）。不过，推荐的方式是使用Deployment来部署Pod。'
- en: 'We could also scale down with the same `kubectl` command, just by setting the `replicas`
    parameter lower:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用相同的`kubectl`命令进行缩减，只需将`replicas`参数设置为更低的值：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we''ll see two Pods are scheduled to be terminated:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们会看到两个Pod被调度以终止：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There is an option, `--current-replicas`, which specifies the expected current
    replicas. If it doesn''t match, Kubernetes doesn''t perform the scale function
    as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个选项，`--current-replicas`，它指定了期望的当前副本数。如果不匹配，Kubernetes将不会执行扩展操作，如下所示：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Horizontal Pod Autoscaler (HPA)
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 水平Pod自动扩缩器（HPA）
- en: An HPA queries the source of metrics periodically and determines whether scaling
    is required by a controller based on the metrics it gets. There are two types
    of metrics that could be fetched; one is from Heapster ([https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)),
    another is from RESTful client access. In the following example, we'll show you
    how to use Heapster to monitor Pods and expose the metrics to an HPA.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: HPA会定期查询指标来源，并根据获取到的指标由控制器决定是否需要扩展。可以获取的指标有两种：一种来自Heapster（[https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)），另一种来自RESTful客户端访问。在接下来的示例中，我们将展示如何使用Heapster监控Pods并将指标暴露给HPA。
- en: 'First, Heapster has to be deployed in the cluster:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Heapster需要在集群中部署：
- en: If you're running minikube, use the `minikube addons enable heapster` command
    to enable heapster in your cluster. Note that `minikube logs | grep heapster command` could
    also be used to check the logs of heapster.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在运行minikube，请使用`minikube addons enable heapster`命令在您的集群中启用heapster。请注意，`minikube
    logs | grep heapster`命令也可以用来检查heapster的日志。
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Check if the `heapster` `pods` are up and running:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 检查`heapster` `pods`是否已经启动并运行：
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Assuming we continue right after the *Getting Ready* section, we will have
    two `my-nginx` Pods running in our cluster:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们紧接着*准备就绪*部分继续操作，我们将有两个`my-nginx` Pods在我们的集群中运行：
- en: '[PRE11]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we can use the `kubectl autoscale` command to deploy an HPA:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`kubectl autoscale`命令来部署一个HPA：
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To check if it''s running as expected:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查它是否按预期运行：
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We find the target shows as unknown and replicas are 0\. Why is this? the runs
    as a control loop, at a default interval of 30 seconds. There might be a delay
    before it reflects the real metrics.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现目标显示为未知，副本数为0。为什么会这样？它作为一个控制循环运行，默认间隔为30秒。可能会有延迟，直到它反映出真实的指标。
- en: 'The default sync period of an HPA can be altered by changing the following
    parameter in control manager: `--horizontal-pod-autoscaler-sync-period`.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: HPA的默认同步周期可以通过更改控制管理器中的以下参数来修改：`--horizontal-pod-autoscaler-sync-period`。
- en: 'After waiting a couple of seconds, we will find the current metrics are there
    now. The number showed in the target column presents (`current / target`). It
    means the load is currently `0%`, and scale target is `50%`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 等待几秒钟后，我们将发现当前的指标已经显示出来。目标列中显示的数字表示（`current / target`）。这意味着当前负载为`0%`，扩展目标是`50%`：
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To test if HPA can scale the Pod properly, we''ll manually generate some loads
    to `my-nginx` service:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试HPA是否能正确扩展Pod，我们将手动生成一些负载到`my-nginx`服务：
- en: '[PRE15]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding command, we ran a `busybox` image which allowed us to run a
    simple command on it. We used the `–c` parameter to specify the default command,
    which is an infinite loop, to query `my-nginx` service.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的命令中，我们运行了一个`busybox`镜像，它允许我们在上面运行简单的命令。我们使用`–c`参数指定了默认命令，即无限循环，以查询`my-nginx`服务。
- en: 'After about one minute, you can see that the current value is changing:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 大约一分钟后，您可以看到当前值发生了变化：
- en: '[PRE16]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'With the same command, we can run more loads with different Pod names repeatedly.
    Finally, we see that the condition has been met. It''s scaling up to `3` replicas,
    and up to `4` replicas afterwards:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的命令，我们可以反复运行不同Pod名称的负载。最后，我们看到条件已经满足，它正在将副本数扩展到`3`，然后是`4`：
- en: '[PRE17]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can see that HPA just scaled our Pods from `4` to `2`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，HPA刚刚将我们的Pods从`4`缩放到`2`。
- en: How it works...
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Note that cAdvisor acts as a container resource utilization monitoring service,
    which is running inside kubelet on each node. The CPU utilizations we just monitored
    are collected by cAdvisor and aggregated by Heapster. Heapster is a service running
    in the cluster that monitors and aggregates the metrics. It queries the metrics
    from each cAdvisor. When HPA is deployed, the controller will keep observing the
    metrics which are reported by Heapster, and scale up and down accordingly. An
    illustration of the process is as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，cAdvisor作为容器资源利用率监控服务，运行在每个节点的kubelet内部。我们刚才监控的CPU利用率是由cAdvisor收集的，并由Heapster汇总。Heapster是一个在集群中运行的服务，用于监控和汇总指标。它从每个cAdvisor查询指标。当HPA部署后，控制器将持续观察Heapster报告的指标，并据此进行扩展和缩减。以下是该过程的示意图：
- en: '![](img/290e9c1e-2be5-4a53-a740-802ddb0147da.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/290e9c1e-2be5-4a53-a740-802ddb0147da.png)'
- en: Based on the specified metrics, HPA determines whether scaling is required
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 根据指定的指标，HPA决定是否需要进行扩展
- en: There is more…
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Alternatively, you could use custom metrics, such as Pod metrics or object metrics,
    to determine if it's time to scale up or down. Kubernetes also supports multiple
    metrics. HPA will consider each metric sequentially. Check out [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale)
    for more examples.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你还可以使用自定义指标，如Pod指标或对象指标，来判断是否该进行扩展或缩减。Kubernetes还支持多种指标，HPA会按顺序考虑每一个指标。有关更多示例，请访问[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale)。
- en: See also
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'This recipe described how to change the number of Pods using the scaling option
    of the deployment. It is useful to scale up and scale down your application quickly.
    To know more about how to update your container, refer to the following recipes:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程描述了如何使用部署的扩展选项来更改Pod数量。这对于快速扩展和缩减你的应用非常有用。要了解更多关于如何更新容器的信息，请参考以下教程：
- en: '*Updating live containers* in [Chapter 3](51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml),
    *Playing with Containers*'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更新实时容器* 在[第3章](51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml)，*与容器玩耍*'
- en: '*Ensuring flexible usage of your containers* in [Chapter 3](51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml)*,
    Playing with Containers*'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*确保容器灵活使用* 在[第3章](51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml)，*与容器玩耍*'
- en: Updating live containers
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新实时容器
- en: For the benefit of containers, we can easily publish new programs by executing
    the latest image, and reduce the headache of environment setup. But, what about
    publishing the program on running containers? While managing a container natively,
    we have to stop the running containers prior to booting up new ones with the latest
    images and the same configurations. There are some simple and efficient methods
    for updating your program in the Kubernetes system. One is called rolling-update,
    which means Deployment can update its Pods without downtime to clients. The other
    method is called *recreate*, which just terminates all Pods then create a new
    set. We will demonstrate how these solutions are applied in this recipe.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于容器的好处，我们可以通过执行最新镜像来轻松发布新程序，减少环境设置的麻烦。但是，如何在运行中的容器上发布程序呢？在本地管理容器时，我们必须先停止正在运行的容器，然后才能启动使用最新镜像和相同配置的新容器。对于在Kubernetes系统中更新程序，有一些简单且高效的方法。一个方法叫做滚动更新，这意味着Deployment可以在不造成客户端停机的情况下更新其Pods。另一个方法叫做*重建*，它会终止所有Pods，然后创建一组新的Pods。在本教程中，我们将演示如何应用这些解决方案。
- en: 'Rolling-update in Docker swarmTo achieve zero downtime application updating,
    there is a similar managing function in Docker swarm. In Docker swarm, you can
    leverage the command docker service update with the flag `--update-delay`, `--update-parallelism`
    and `--update-failure-action`. Check the official website for more details about
    Docker swarm''s rolling-update: [https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/](https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在Docker Swarm中进行滚动更新为了实现零停机时间的应用更新，Docker Swarm中有类似的管理功能。在Docker Swarm中，你可以利用命令
    `docker service update` 配合 `--update-delay`、`--update-parallelism` 和 `--update-failure-action`
    标志来进行管理。想要了解更多关于Docker Swarm滚动更新的信息，可以访问官方文档：[https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/](https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/)。
- en: Getting ready
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'For a later demonstration, we are going to update `nginx` Pods . Please make
    sure all Kubernetes nodes and components are working healthily:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的演示中，我们将更新`nginx` Pods。请确保所有Kubernetes节点和组件都健康运行：
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Furthermore, to well understand the relationship between ReplicaSet and Deployment,
    please check *Deployment API *section in *[Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml),
    Walking through Kubernetes Concepts*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了更好地理解ReplicaSet与Deployment之间的关系，请查阅*第2章*中的*Deployment API*部分，[走进Kubernetes概念](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)。
- en: 'To illustrate the updating of the containers in Kubernetes system, we will
    create a Deployment, change its configurations of application, and then check
    how the updating mechanism handles it. Let''s get all our resources ready:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示Kubernetes系统中容器的更新，我们将创建一个Deployment，修改其应用配置，然后检查更新机制如何处理。让我们准备好所有资源：
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This Deployment is created with `5` replicas. It is good for us to discover
    the updating procedure with multiple numbers of Pods:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个部署是使用`5`个副本创建的。通过这种方式，我们可以通过多个Pod来探索更新流程：
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Attaching a Service on the Deployment will help to simulate the real experience
    of clients.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署中附加一个服务有助于模拟客户的真实体验。
- en: How to do it...
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'At the beginning, take a look at the Deployment you just created and its ReplicaSet
    by executing the following code block:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 开始时，执行以下代码块查看你刚刚创建的 Deployment 及其 ReplicaSet：
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Based on the preceding output, we know that the default updating strategy of
    deployment is rolling-update. Also, there is a single ReplicaSet named `<Deployment
    Name>-<hex decimal hash>` that is created along with the Deployment.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的输出，我们知道默认的部署更新策略是滚动更新。而且，随着 Deployment 创建，还会创建一个名为 `<Deployment 名称>-<十六进制哈希>`
    的 ReplicaSet。
- en: 'Next, check the content of the current Service endpoint for the sake of verifying
    our update later:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，检查当前 Service 端点的内容，以便稍后验证我们的更新：
- en: '[PRE22]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We will get the welcome message in the title of the HTML response with the original
    `nginx` image.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 HTML 响应的标题中看到欢迎信息，并且使用原始的 `nginx` 镜像。
- en: Deployment update strategy – rolling-update
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署更新策略 – 滚动更新
- en: 'The following will introduce the subcommands `edit` and `set`, for the purpose
    of updating the containers under Deployment:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下将介绍子命令 `edit` 和 `set`，用于更新 Deployment 下的容器：
- en: 'First, let''s update the Pods in Deployment with a new command:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们使用新的命令更新 Deployment 中的 Pods：
- en: '[PRE23]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We are not only doing the update; we record this change as well. With the flag
    `--record`, we keep the command line as a tag in revision.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅仅是在做更新，我们还记录这个变化。使用 `--record` 标志，我们将命令行作为修订的标签保留。
- en: 'After editing the Deployment, check the status of rolling-update with the subcommand
    `rollout` right away:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑 Deployment 后，立即检查滚动更新的状态，使用子命令 `rollout`：
- en: '[PRE24]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: It is possible that you get several `Waiting for …` lines, as shown in the preceding
    code. They are the standard output showing the status of the update.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会看到若干 `Waiting for …` 行，正如前面的代码所示。它们是标准输出，显示更新的状态。
- en: 'For whole updating procedures, check the details of the Deployment to list
    its events:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于整个更新过程，检查 Deployment 的详细信息以列出其事件：
- en: '[PRE25]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you see, a new `replica set simple-nginx-694f94f77d` is created in the Deployment
    `simple-nginx`. Each time the new ReplicaSet scales one Pod up successfully, the
    old ReplicaSet will scale one Pod down. The scaling process finishes at the moment
    that the new ReplicaSet meets the original desired Pod number (as said, `5` Pods),
    and the old ReplicaSet has zero Pods.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`simple-nginx` 部署中创建了一个新的 `replica set simple-nginx-694f94f77d`。每当新的 ReplicaSet
    成功扩展一个 Pod 时，旧的 ReplicaSet 会将一个 Pod 缩减。当新的 ReplicaSet 达到原定的 Pod 数量（如，`5` 个 Pod），且旧的
    ReplicaSet 变为零个 Pod 时，扩展过程完成。
- en: 'Go ahead and check the new ReplicaSet and existing Service for this update:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续检查此次更新的新的 ReplicaSet 和现有的 Service：
- en: '[PRE26]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Let's make another update! This time, use the subcommand `set` to modify a specific
    configuration of a Pod.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们做另一次更新！这次，使用子命令 `set` 修改 Pod 的特定配置。
- en: 'To set a new image to certain containers in a Deployment, the subcommand format
    would look like this: `kubectl set image deployment <Deployment name> <Container
    name>=<image name>`:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要为 Deployment 中的某些容器设置新镜像，子命令的格式如下：`kubectl set image deployment <Deployment
    名称> <Container 名称>=<镜像名称>`：
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: What else could the subcommand "set" help to configure?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '"set" 子命令还能帮助配置哪些内容？'
- en: 'The subcommand set helps to define the configuration of the application. Until
    version 1.9, CLI with set could assign or update the following resources:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: set 子命令有助于定义应用程序的配置。直到版本 1.9，CLI 使用 set 可以分配或更新以下资源：
- en: '| **Subcommand after set** | **Acting resource** | **Updating item** |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| **set 后的子命令** | **作用资源** | **更新项** |'
- en: '| `env` | Pod | Environment variables |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| `env` | Pod | 环境变量 |'
- en: '| `image` | Pod | Container image |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| `image` | Pod | 容器镜像 |'
- en: '| `resources` | Pod | Computing resource requirement or limitation |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| `resources` | Pod | 计算资源需求或限制 |'
- en: '| `selector` | Any resource | Selector |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| `selector` | 任何资源 | 选择器 |'
- en: '| `serviceaccount` | Any resource | ServiceAccount |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| `serviceaccount` | 任何资源 | ServiceAccount |'
- en: '| `subject` | RoleBinding or ClusterRoleBinding | User, group, or ServiceAccount
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| `subject` | RoleBinding 或 ClusterRoleBinding | 用户、组或 ServiceAccount |'
- en: 'Now, check if the update has finished and whether the image is changed:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，检查更新是否已完成，以及镜像是否已更改：
- en: '[PRE28]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You can also check out the ReplicaSets. There should be another one taking
    responsibility of the Pods for Deployment:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以查看 ReplicaSets。应该会有另一个负责管理 Pods 的 ReplicaSet：
- en: '[PRE29]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Rollback the update
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回滚更新
- en: 'Kubernetes system records every update for Deployment:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 系统会记录每次 Deployment 的更新：
- en: 'We can list all of the revisions with the subcommand `rollout`:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用子命令 `rollout` 列出所有修订：
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: You will get three revisions, as in the preceding lines, for the Deployment
    `simple-nginx`. For Kubernetes Deployment, each revision has a matched `ReplicaSet`
    and represents a stage of running an update command. The first revision is the
    initial state of `simple-nginx`. Although there is no command tag for indication,
    Kubernetes takes its creation as its first version. However, you could still record
    the command when you create the Deployment.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`simple-nginx`部署，您将获得三个修订版本，就像前面的行一样。对于Kubernetes部署，每个修订版本都有一个匹配的`ReplicaSet`，并代表执行更新命令的一个阶段。第一个修订版本是`simple-nginx`的初始状态。虽然没有命令标签进行指示，但Kubernetes将其创建视为第一个版本。不过，在创建部署时，您仍然可以记录该命令。
- en: Add the flag `--record` after the subcommand `create` or `run`.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在子命令`create`或`run`后添加`--record`标志。
- en: 'With the revisions, we can easily resume the change, which means rolling back
    the update. Use the following commands to rollback to previous revisions:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过修订版本，我们可以轻松恢复更改，即回滚更新。使用以下命令回滚到先前的修订版本：
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Without specifying the revision number, the rollback process will simply jump
    back to previous version:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有指定修订版本号，回滚过程将直接跳回到先前版本：
- en: '[PRE32]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Deployment update strategy – recreate
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署更新策略 – recreate
- en: 'Next, we are going to introduce the other update strategy, `recreate`, for
    Deployment. Although there is no subcommand or flag to create a recreate-strategy deployment,
    users could fulfill this creation by overriding the default element with the specified
    configuration:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍另一种更新策略，`recreate`，用于部署。虽然没有子命令或标志来创建`recreate`策略的部署，但用户可以通过覆盖默认元素并指定相应的配置来实现此创建：
- en: '[PRE33]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In our understanding, the `recreate` mode is good for an application under
    development. With `recreate`, Kubernetes just scales the current ReplicaSet down
    to zero Pods, and creates a new ReplicaSet with the full desired number of Pods.
    Therefore, recreate has a shorter total updating time than rolling-update because
    it scales ReplicaSets up or down simply, once for all. Since a developing Deployment
    doesn''t need to take care of any user experience, it is acceptable to have downtime
    while updating and enjoy faster updates:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的理解，`recreate`模式适用于正在开发中的应用程序。在`recreate`模式下，Kubernetes会将当前的ReplicaSet缩放至零个Pod，然后创建一个具有所需Pod数量的新ReplicaSet。因此，`recreate`比滚动更新（rolling-update）拥有更短的整体更新时间，因为它只是简单地缩放ReplicaSet，一次性完成。由于开发中的部署不需要考虑用户体验，因此在更新过程中出现停机是可以接受的，同时可以享受更快的更新速度：
- en: '[PRE34]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: How it works...
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Rolling-update works on the units of the ReplicaSet in a Deployment. The effect
    is to create a new ReplicaSet to replace the old one. Then, the new ReplicaSet
    is scaling up to meet the desired numbers, while the old ReplicaSet is scaling
    down to terminate all the Pods in it. The Pods in the new ReplicaSet are attached
    to the original labels. Therefore, if any service exposes this Deployment, it
    will take over the newly created Pods directly.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新作用于部署中的ReplicaSet单元。其效果是创建一个新的ReplicaSet来替代旧的ReplicaSet。然后，新ReplicaSet将被扩展以满足所需的Pod数量，同时旧的ReplicaSet将缩减以终止其中的所有Pod。新ReplicaSet中的Pod会附加到原始标签。因此，如果有任何服务暴露此部署，它将直接接管新创建的Pods。
- en: An experienced Kubernetes user may know that the resource ReplicationController
    can be rolling-update as well. So, what are the differences of rolling-update
    between ReplicationController and deployment? The scaling processing uses the
    combination of ReplicationController and a client such as `kubectl`. A new ReplicationController
    will be created to replace the previous one. Clients don't feel any interruption
    since the service is in front of ReplicationController while doing replacement.
    However, it is hard for developers to roll back to previous ReplicationControllers
    (they have been removed), because there is no built-in mechanism that records
    the history of updates.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经验丰富的Kubernetes用户可能知道，资源ReplicationController也可以进行滚动更新。那么，ReplicationController和部署之间的滚动更新有何不同呢？扩缩容处理使用的是ReplicationController与`kubectl`等客户端的组合。新的ReplicationController将被创建以替代旧的ReplicationController。客户端在替换过程中不会感到任何中断，因为服务始终在ReplicationController前面。然而，开发者很难回滚到之前的ReplicationController（因为它们已被删除），因为没有内置的机制来记录更新历史。
- en: In addition, rolling-update might fail if the client connection is disconnected
    while rolling-update is working. Most important of all, Deployment with ReplicaSet
    is the most recommended deploying resource than ReplicationController or standalone
    ReplicaSet.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果在滚动更新过程中客户端连接被断开，滚动更新可能会失败。最重要的是，带有ReplicaSet的Deployment是比ReplicationController或独立ReplicaSet更推荐的部署资源。
- en: 'While paying close attention to the history of update in deployment, be aware
    that it is not always listed in sequence. The algorithm of adding revisions could
    be clarified as the following bullet points show:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在密切关注部署更新历史时，请注意它并不总是按顺序列出。添加修订版本的算法可以通过以下要点来说明：
- en: Take the number of last revision as *N*
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以最后一个修订版本的编号为*N*
- en: When a new rollout update comes, it would be *N+1*
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当新的滚动更新到来时，它将变为*N+1*
- en: Roll back to a specific revision number *X*, *X* would be removed and it would
    become *N+1*
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回滚到特定的修订版本号*X*，*X*将被删除，且它将变为*N+1*
- en: Roll back to the previous version, which means *N-1,* then *N-1* would be removed
    and it would become *N+1*
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回滚到上一个版本，即*N-1*，然后*N-1*将被删除，且它将变为*N+1*
- en: With this revision management, no stale and overlapped updates occupy the rollout
    history.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种修订管理，不会有过时或重叠的更新占用滚动历史记录。
- en: There's more...
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Taking Deployment update into consideration is a good step towards building
    a CI/CD (continuous integration and continuous delivery) pipeline. For a more
    common usage, developers don''t exploit command lines to update the Deployment.
    They may prefer to fire some API calls from CI/CD platform, or update from a previous
    configuration file. Here comes an example working with the subcommand `apply`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到部署更新，是构建CI/CD（持续集成与持续交付）流水线的一个良好步骤。对于更常见的用法，开发者通常不会使用命令行来更新部署。相反，他们可能会选择从CI/CD平台发起API调用，或者从先前的配置文件中进行更新。以下是与子命令`apply`一起工作的示例：
- en: '[PRE35]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'As a demonstration, modifying the container image from `nginx` to `nginx:stable`
    (you may check the code bundle `my-update-nginx-updated.yaml` for the modification).
    Then, we can use the changed file to update with the subcommand `apply`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 作为演示，将容器镜像从`nginx`修改为`nginx:stable`（你可以查看代码包`my-update-nginx-updated.yaml`中的修改）。然后，我们可以使用更改后的文件通过子命令`apply`进行更新：
- en: '[PRE36]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now, you can learn another way to update your Deployment.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以学习另一种更新你的部署的方式。
- en: 'Digging deeper into rolling-update on Deployment, there are some parameters
    we may leverage when doing updates:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 深入了解Deployment的滚动更新时，我们可以利用一些参数来进行更新：
- en: '`minReadySeconds`: After a Pod is considered to be ready, the system still
    waits for a period of time for going on to the next step. This time slot is the
    minimum ready seconds, which will be helpful when waiting for the application
    to complete post-configuration.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minReadySeconds`：当Pod被认为已准备好后，系统仍然会等待一段时间才能进行下一步操作。这个时间段就是最小准备时间，这在等待应用完成后配置时非常有用。'
- en: '`maxUnavailable`: The maximum number of Pods that can be unavailable during
    updating. The value could be a percentage (the default is 25%) or an integer.
    If the value of `maxSurge` is `0`, which means no tolerance of the number of Pods over
    the desired number, the value of `maxUnavailable` cannot be `0`.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxUnavailable`：更新期间可以不可用的最大Pod数。值可以是百分比（默认是25%）或整数。如果`maxSurge`的值为`0`，表示不能容忍Pod数量超过期望数，那么`maxUnavailable`的值不能为`0`。'
- en: '`maxSurge`: The maximum number of Pods that can be created over the desired
    number of ReplicaSet during updating. The value could be a percentage (the default
    is 25%) or an integer. If the value of `maxUnavailable` is `0`, which means the
    number of serving Pods should always meet the desired number, the value of `maxSurge`
    cannot be `0`.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxSurge`：更新期间可以创建的超出期望ReplicaSet数量的最大Pod数。值可以是百分比（默认是25%）或整数。如果`maxUnavailable`的值为`0`，表示服务中的Pod数应该始终符合期望值，那么`maxSurge`的值不能为`0`。'
- en: Based on the configuration file `my-update-nginx-advanced.yaml` in the code
    bundle, try playing with these parameters by yourself and see if you can feel
    the ideas at work.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 基于代码包中的配置文件`my-update-nginx-advanced.yaml`，你可以自己尝试操作这些参数，看看能否感受到它们的实际效果。
- en: See also
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'You could continue studying the following recipes to learn more ideas about
    deploying Kubernetes resources efficiently:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以继续学习以下的配方，深入了解如何高效地部署Kubernetes资源：
- en: Scaling your containers
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展你的容器
- en: Working with configuration files
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配置文件
- en: The *Moving monolithic to microservices*, *Integrating with Jenkins*, *Working
    with the private Docker registry*, and *Setting up the Continuous Delivery Pipeline*
    recipes in [Chapter 5](669edaf0-c274-48fa-81d8-61150fa36df5.xhtml)*, Building
    a Continuous Delivery Pipeline*
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第5章](669edaf0-c274-48fa-81d8-61150fa36df5.xhtml)中的*将单体应用迁移到微服务*，*与Jenkins集成*，*使用私有Docker镜像库*和*设置持续交付管道*的配方，*构建持续交付管道*'
- en: Forwarding container ports
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转发容器端口
- en: In previous chapters, you have learned how to work with the Kubernetes Services
    to forward the container port internally and externally. Now, it's time to take
    it a step further to see how it works.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，你已经学会了如何使用Kubernetes服务来内部和外部转发容器端口。现在，是时候更进一步，了解它是如何工作的。
- en: 'There are four networking models in Kubernetes, and we''ll explore the details
    in the following sections:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中有四种网络模型，我们将在以下章节中详细探讨：
- en: Container-to-container communications
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器之间的通信
- en: Pod-to-pod communications
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod之间的通信
- en: Pod-to-service communications
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod与服务的通信
- en: External-to-internal communications
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部到内部的通信
- en: Getting ready
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Before we go digging into Kubernetes networking, let''s study the networking
    of Docker to understand the basic concept. Each container will have a network
    namespace with its own routing table and routing policy. By default, the network
    bridge `docker0` connects the physical network interface and virtual network interfaces
    of containers, and the virtual network interface is the bidirectional cable for
    the container network namespace and the host one. As a result, there is a pair
    of virtual network interfaces for a single container: the Ethernet interface (**eth0**)
    on the container and the virtual Ethernet interface (**veth-**) on the host.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解Kubernetes网络之前，我们先研究Docker的网络，以便理解基本概念。每个容器将拥有一个网络命名空间，具有自己的路由表和路由策略。默认情况下，网络桥接`docker0`连接物理网络接口和容器的虚拟网络接口，虚拟网络接口则是容器网络命名空间和主机网络命名空间之间的双向连接线。因此，单个容器有一对虚拟网络接口：容器内的以太网接口（**eth0**）和主机上的虚拟以太网接口（**veth-**）。
- en: 'The network structure can be expressed as in the following image:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 网络结构可以如以下图所示：
- en: '![](img/cbce3365-1d24-4212-9777-b99db15a7bf6.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbce3365-1d24-4212-9777-b99db15a7bf6.png)'
- en: Container network interfaces on host
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 主机上的容器网络接口
- en: What is a network namespace?A network namespace is the technique provided by
    Linux kernel. With this feature, the operating system can fulfill network virtualization
    by separating the network capability into independent resources. Each network
    namespace has its own iptable setup and network devices.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是网络命名空间？网络命名空间是Linux内核提供的一种技术。通过这个特性，操作系统可以通过将网络能力分离为独立资源来实现网络虚拟化。每个网络命名空间都有自己的iptables设置和网络设备。
- en: How to do it...
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: A Pod contains one or more containers, which run on the same host. Each Pod has
    their own IP address on an overlay network; all the containers inside a Pod see
    each other as on the same host. Containers inside a Pod will be created, deployed,
    and deleted almost at the same time. We will illustrate four communication models
    between container, Pod, and Service.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Pod包含一个或多个容器，这些容器运行在同一主机上。每个Pod在叠加网络上都有自己的IP地址；Pod内的所有容器互相之间视为同一主机上的容器。Pod内的容器将几乎同时创建、部署和删除。我们将展示容器、Pod和服务之间的四种通信模型。
- en: Container-to-container communication
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器之间的通信
- en: 'In this scenario, we would focus on the communications between containers within
    single Pod:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种场景中，我们将重点讨论单一Pod内容器之间的通信：
- en: 'Let''s create two containers in one Pod: a nginx web application and a CentOS,
    which checks port `80` on localhost:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在一个Pod中创建两个容器：一个nginx Web应用和一个检查localhost上`80`端口的CentOS容器：
- en: '[PRE37]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We see the count in the `READY` column becomes `2/2`, since there are two containers
    inside this Pod.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到`READY`列中的计数变为`2/2`，因为这个Pod内有两个容器。
- en: 'Using the `kubectl describe` command, we may see the details of the Pod:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`kubectl describe`命令，我们可以查看Pod的详细信息：
- en: '[PRE38]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We can see that the Pod is run on node `ubuntu02` and that its IP is `192.168.79.198`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到Pod运行在节点`ubuntu02`上，且其IP地址是`192.168.79.198`。
- en: 'Also, we may find that the Centos container can access the `nginx` on localhost:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们可能会发现Centos容器能够访问localhost上的`nginx`：
- en: '[PRE39]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s log in to node `ubuntu02` to check the network setting of these two
    containers:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们登录到节点`ubuntu02`，检查这两个容器的网络设置：
- en: '[PRE40]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now, we know that the two containers created are `9e35275934c1` and `e832d294f176`.
    On the other hand, there is another container, `9b3e9caf5149`, that is created
    by Kubernetes with the Docker image `gcr.io/google_containers/pause-amd64`. We
    will introduce it later. Thereafter, we may get a detailed inspection of the containers
    with the command `docker inspect`, and by adding the command `jq `([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/))
    as a pipeline, we can parse the output information to show network settings only.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道创建的两个容器是`9e35275934c1`和`e832d294f176`。另一方面，还有一个容器`9b3e9caf5149`，是由Kubernetes创建的，使用Docker镜像`gcr.io/google_containers/pause-amd64`。我们稍后会介绍它。之后，我们可以通过命令`docker
    inspect`对容器进行详细检查，并通过将命令`jq`([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/))作为管道添加，可以解析输出信息，仅显示网络设置。
- en: 'Taking a look at both containers covered in the same Pod:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看一下同一Pod中的两个容器：
- en: '[PRE41]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We can see that both containers have identical network settings; the network
    mode is set to mapped container mode, leaving the other configurations cleaned.
    The network bridge container is `container:9b3e9caf5149ffb0ec14c1ffc36f94b2dd55b223d0d20e4d48c4e33228103723`.
    What is this container? It is the one created by Kubernetes, container ID `9b3e9caf5149`,
    with the image `gcr.io/google_containers/pause-amd64`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到两个容器具有相同的网络设置；网络模式设置为映射容器模式，其他配置被清理。网络桥接容器是`container:9b3e9caf5149ffb0ec14c1ffc36f94b2dd55b223d0d20e4d48c4e33228103723`。这个容器是什么？它是Kubernetes创建的，容器ID为`9b3e9caf5149`，镜像为`gcr.io/google_containers/pause-amd64`。
- en: What does the container "pause" do?Just as its name suggests, this container
    does nothing but "pause". However, it preserves the network settings, and the
    Linux network namespace, for the Pod. Anytime the container shutdowns and restarts,
    the network configuration will still be the same and not need to be recreated,
    because the "pause" container holds it. You can check its code and Dockerfile
    at [https://github.com/kubernetes/kubernetes/tree/master/build/pause](https://github.com/kubernetes/kubernetes/tree/master/build/pause)
    for more information.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: “暂停”容器做什么？正如其名称所示，这个容器什么也不做，只是“暂停”。然而，它保留了Pod的网络设置和Linux网络命名空间。每次容器关闭并重新启动时，网络配置仍然保持不变，无需重新创建，因为“暂停”容器保存了它。你可以在[https://github.com/kubernetes/kubernetes/tree/master/build/pause](https://github.com/kubernetes/kubernetes/tree/master/build/pause)查看其代码和Dockerfile，了解更多信息。
- en: The "pause" container is a network container, which is created when a Pod
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: “暂停”容器是一个网络容器，当Pod创建时会生成它。
- en: is created and used to handle the route of the Pod network. Then, two containers
    will share the network namespace with pause; that's why they see each other as
    localhost.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 它被创建并用于处理Pod网络的路由。然后，两个容器将与暂停容器共享网络命名空间；这就是它们彼此视为localhost的原因。
- en: 'Create a network container in DockerIn Docker, you can easily make a container
    into a network container, sharing its network namespace with another container.
    Use the command line: `$ docker run --network=container:<CONTAINER_ID or CONTAINER_NAME>
    [other options].` Then, you will be able to start a container which uses the network
    namespace of the assigned container.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在Docker中创建网络容器：你可以轻松地将容器转换为网络容器，与另一个容器共享网络命名空间。使用命令行：`$ docker run --network=container:<CONTAINER_ID或CONTAINER_NAME>
    [其他选项]`。然后，你就能够启动一个使用指定容器网络命名空间的容器。
- en: Pod-to-Pod communication
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod到Pod的通信
- en: 'As mentioned, containers in a Pod share the same network namespace. And a Pod is
    the basic computing unit in Kubernetes. Kubernetes assigns an IP to a Pod in its
    world. Every Pod can see every other with the virtual IP in Kubernetes network.
    While talking about the communication between Pods , we can separate into two
    scenarios: Pods that communicate within a node, or Pods that communicate across
    nodes. For Pods in single node, since they have separate IPs, their transmissions
    can be held by bridge, same as containers in a Docker node. However, for communication
    between Pods across nodes, how would be the package routing work while Pod doesn''t
    have the host information (the host IP)?'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Pod中的容器共享相同的网络命名空间。Pod是Kubernetes中的基本计算单元。Kubernetes为Pod分配一个IP，在其网络中，每个Pod都可以通过虚拟IP看到其他Pod。谈到Pod之间的通信时，我们可以分为两种情况：在同一节点内通信的Pod，或者跨节点通信的Pod。对于单节点中的Pod，由于它们有独立的IP，它们的传输可以通过桥接完成，类似于Docker节点中的容器。然而，对于跨节点的Pod之间的通信，当Pod没有主机信息（主机IP）时，如何进行包路由？
- en: 'Kubernetes uses the CNI to handle cluster networking. CNI is a framework for
    managing connective containers, for assigning or deleting the network resource
    on a container. While Kubernetes takes CNI as a plugin, users can choose the implementation
    of CNI on demand. Commonly, there are the following types of CNI:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 使用 CNI 来处理集群网络。CNI 是一个用于管理连接容器的框架，负责为容器分配或删除网络资源。Kubernetes 将 CNI
    作为插件，用户可以根据需要选择 CNI 的实现。常见的 CNI 类型包括以下几种：
- en: '**Overlay**: With the technique of packet encapsulation. Every data is wrapped
    with host IP, so it is routable in the internet. An example is flannel ([https://github.com/coreos/flannel](https://github.com/coreos/flannel)).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**覆盖网络**：采用数据包封装技术。每个数据都被包装上主机 IP，因此可以在互联网上进行路由。一个例子是 flannel ([https://github.com/coreos/flannel](https://github.com/coreos/flannel))。'
- en: '**L3 gateway**: Transmission between containers pass to a gateway node first.
    The gateway will maintain the routing table to map the container subnet and host
    IP. An example is Project Calico ([https://www.projectcalico.org/](https://www.projectcalico.org/)).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L3 网关**：容器之间的传输首先经过一个网关节点。该网关会维护路由表，将容器子网与主机 IP 映射。一个例子是 Project Calico ([https://www.projectcalico.org/](https://www.projectcalico.org/))。'
- en: '**L2 adjacency**: Happening on L2 switching. In Ethernet, two nodes have adjacency
    if the package can be transmitted directly from source to destination, without
    passing by other nodes. An example is Cisco ACI ([https://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/kb/b_Kubernetes_Integration_with_ACI.html](https://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/kb/b_Kubernetes_Integration_with_ACI.html)).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2 邻接**：发生在 L2 交换层。在以太网中，两个节点如果数据包可以直接从源节点传输到目标节点，而无需经过其他节点，则认为它们是邻接的。一个例子是
    Cisco ACI ([https://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/kb/b_Kubernetes_Integration_with_ACI.html](https://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/kb/b_Kubernetes_Integration_with_ACI.html))。'
- en: 'There are pros and cons to every type of CNI. The former type within the bullet
    points has better scalability but bad performance, while the latter one has a
    shorter latency but requires complex and customized setup. Some CNIs cover all
    three types in different modes, for example, Contiv ([https://github.com/contiv/netplugin](https://github.com/contiv/netplugin)).
    You can get more information about CNI while checking its spec at: [https://github.com/containernetworking/cni](https://github.com/containernetworking/cni).
    Additionally, look at the CNI list on official website of Kubernetes to try out
    these CNIs: [https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this](https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 每种 CNI 类型都有优缺点。前面列出的类型具有更好的可扩展性，但性能较差，而后者则具有更短的延迟，但需要复杂且定制化的设置。有些 CNI 在不同模式下涵盖了所有三种类型，例如
    Contiv ([https://github.com/contiv/netplugin](https://github.com/contiv/netplugin))。您可以通过查看其规格了解更多有关
    CNI 的信息，网址为：[https://github.com/containernetworking/cni](https://github.com/containernetworking/cni)。此外，还可以查看
    Kubernetes 官方网站上的 CNI 列表，尝试这些 CNI：[https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this](https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this)。
- en: After introducing the basic knowledge of the packet transaction between Pods ,
    we will continue to bring you a Kubernetes API, `NetworkPolicy`, which provides
    advanced management between the communication of Pods .
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了 Pod 之间数据包传输的基础知识后，我们将继续为您带来一个 Kubernetes API，`NetworkPolicy`，它提供了 Pod 之间通信的高级管理功能。
- en: Working with NetworkPolicy
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 NetworkPolicy 一起工作
- en: As a resource of Kubernetes, NetworkPolicy uses label selectors to configure
    the firewall of Pods from infrastructure level. Without a specified NetworkPolicy,
    any Pod in the same cluster can communicate with each other by default. On the
    other hand, once a NetworkPolicy with rules is attached to a Pod, either it is
    for ingress or egress, or both, and all traffic that doesn't follow the rules
    will be blocked.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 Kubernetes 的资源，NetworkPolicy 使用标签选择器来配置 Pod 的防火墙，作用于基础设施层面。如果没有指定 NetworkPolicy，集群中的任何
    Pod 默认都可以相互通信。另一方面，一旦将带有规则的 NetworkPolicy 附加到 Pod，无论是针对入口流量还是出口流量，或两者兼有，所有不符合规则的流量都将被阻止。
- en: 'Before demonstrating how to build a NetworkPolicy, we should make sure the
    network plugin in Kubernetes cluster supports it. There are several CNIs that
    support NetworkPolicy: Calico, Contive, Romana ([https://github.com/romana/kube](https://github.com/romana/kube)),
    Weave Net ([https://github.com/weaveworks/weave](https://github.com/weaveworks/weave)),
    Trireme ([https://github.com/aporeto-inc/trireme-kubernetes](https://github.com/aporeto-inc/trireme-kubernetes)),
    and others.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在演示如何构建NetworkPolicy之前，我们需要确保Kubernetes集群中的网络插件支持它。有几种CNI支持NetworkPolicy：Calico、Contive、Romana ([https://github.com/romana/kube](https://github.com/romana/kube))，Weave
    Net ([https://github.com/weaveworks/weave](https://github.com/weaveworks/weave))，Trireme ([https://github.com/aporeto-inc/trireme-kubernetes](https://github.com/aporeto-inc/trireme-kubernetes))，等等。
- en: 'Enable CNI with NetworkPolicy support as network plugin in minikubeWhile working
    on minikube, users will not need to attach a CNI specifically, since it is designed
    as a single local Kubernetes node. However, to enable the functionality of NetworkPolicy,
    it is necessary to start a NetworkPolicy-supported CNI. Be careful, as, while
    you configure the minikube with CNI, the configuration options and procedures
    could be quite different to various CNI implementations. The following steps show
    you how to start minikube with CNI, Calico:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在minikube中启用CNI并支持NetworkPolicy作为网络插件。在使用minikube时，用户不需要特别附加CNI，因为它设计为单节点本地Kubernetes。但是，为了启用NetworkPolicy的功能，必须启动一个支持NetworkPolicy的CNI。需要注意的是，在配置minikube时，不同的CNI实现可能会有不同的配置选项和步骤。以下步骤展示了如何使用CNI和Calico启动minikube：
- en: We take this issue [https://github.com/projectcalico/calico/issues/1013#issuecomment-325689943](https://github.com/projectcalico/calico/issues/1013#issuecomment-325689943)
    as reference for these building steps.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们参考这个问题[https://github.com/projectcalico/calico/issues/1013#issuecomment-325689943](https://github.com/projectcalico/calico/issues/1013#issuecomment-325689943)来进行这些构建步骤。
- en: The minikube used here is the latest version, 0.24.1.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里使用的minikube是最新版本0.24.1。
- en: 'Reboot your minikube: `minikube start --network-plugin=cni \`'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重启你的minikube：`minikube start --network-plugin=cni \`
- en: '`--host-only-cidr 172.17.17.1/24 \ --extra-config=kubelet.PodCIDR=192.168.0.0/16
    \ --extra-config=proxy.ClusterCIDR=192.168.0.0/16 \` `--extra-config=controller-manager.ClusterCIDR=192.168.0.0/16`.'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`--host-only-cidr 172.17.17.1/24 \ --extra-config=kubelet.PodCIDR=192.168.0.0/16
    \ --extra-config=proxy.ClusterCIDR=192.168.0.0/16 \` `--extra-config=controller-manager.ClusterCIDR=192.168.0.0/16`。'
- en: Create Calico with the configuration file "minikube-calico.yaml" from the code
    bundle `kubectl create -f minikue-calico.yaml`.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用代码包中的配置文件“minikube-calico.yaml”创建Calico：`kubectl create -f minikube-calico.yaml`。
- en: 'To illustrate the functionality of NetworkPolicy, we are going to create a
    Pod and expose it as a service, then attach a NetworkPolicy on the Pod to see
    what happens:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明NetworkPolicy的功能，我们将创建一个Pod并将其暴露为服务，然后在Pod上附加一个NetworkPolicy，看看会发生什么：
- en: '[PRE42]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we can go ahead and check the Pod''s connection from a simple Deployment,
    `busybox`, using the command `wget` with `--spider` flag to verify the existence
    of endpoint:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以继续检查来自简单部署`busybox`的Pod连接，使用`wget`命令并带上`--spider`标志来验证端点是否存在：
- en: '[PRE43]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'As shown in the preceding result, we know that the `nginx` service can be accessed
    without any constraints. Later, let''s run a `NetworkPolicy` that restricts that
    only the Pod tagging `<test: inbound>` can access `nginx` service:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '如前面的结果所示，我们知道`nginx`服务可以在没有任何限制的情况下访问。接下来，我们将运行一个`NetworkPolicy`，限制只有标签为`<test:
    inbound>`的Pod才能访问`nginx`服务：'
- en: '[PRE44]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'As you can see, in the spec of NeworkPolicy, it is configured to apply to Pods with
    the label `<run: nginx-pod>`, which is the one we have on the `pod nginx-pod`.
    Also, a rule of ingress is attached in the policy, which indicates that only Pods with
    a specific label can access `nginx-pod`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '如你所见，在NetworkPolicy的spec中，它被配置为应用于标签为`<run: nginx-pod>`的Pod，而我们在`pod nginx-pod`上正好有这个标签。此外，策略中还附加了一个入站规则，表示只有具有特定标签的Pod才能访问`nginx-pod`：'
- en: '[PRE45]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Great, everything is looking just like what we expected. Next, check the same
    service endpoint on our previous `busybox` Pod:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，一切看起来如我们所预期的那样。接下来，检查之前的`busybox` Pod上的相同服务端点：
- en: '[PRE46]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'As expected again, now we cannot access the `nginx-pod` service after NetworkPolicy
    is attached. The `nginx-pod` can only be touched by Pod labelled with `<test:
    inbound>`:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '如预期的那样，附加NetworkPolicy后，我们无法访问`nginx-pod`服务。`nginx-pod`只能被标签为`<test: inbound>`的Pod访问：'
- en: '[PRE47]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Catch up with the concept of label and selector in the recipe *Working with
    labels and selectors* in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)*,*
    *Walking through Kubernetes Concepts*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)*，《Kubernetes概念概览》*中，通过*《与标签和选择器的工作》*一节，了解标签和选择器的概念。
- en: 'In this case, you have learned how to create a NetworkPolicy with ingress restriction
    by Pod selector. Still, there are other settings you may like to build on your
    Pod:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您已学习如何通过Pod选择器创建一个带有入口限制的NetworkPolicy。仍然，您可能希望在Pod上进行其他设置：
- en: '**Egress restriction**: Egress rules can be applied by `.spec.egress`, which
    has similar settings to ingress.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**出口限制**：出口规则可以通过`.spec.egress`进行设置，其配置方式与入口类似。'
- en: '**Port restriction**: Each ingress and egress rule can point out what port,
    and with what kind of port protocol, is to be accepted or blocked. Port configuration
    can be applied through `.spec.ingress.ports` or `.spec.egress.ports`.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端口限制**：每个入口和出口规则可以指明要接受或阻止的端口和端口协议。端口配置可以通过`.spec.ingress.ports`或`.spec.egress.ports`进行设置。'
- en: '**Namespace selector**: We can also make limitations on certain Namespaces.
    For example, Pods for the system daemon might only allow access to others in the
    Namespace `kube-system`. Namespace selector can be applied with `.spec.ingress.from.namespaceSelector`
    or `.spec.egress.to.namespaceSelector`.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名空间选择器**：我们还可以对特定的命名空间设置限制。例如，系统守护进程的Pods可能只允许访问`kube-system`命名空间中的其他Pod。命名空间选择器可以通过`.spec.ingress.from.namespaceSelector`或`.spec.egress.to.namespaceSelector`进行应用。'
- en: '**IP block**: A more customized configuration is to set rules on certain CIDR
    ranges, which come out as similar ideas to what we work with iptables. We may
    utilize this configuration through `.spec.ingress.from.ipBlock` or `.spec.egress.to.ipBlock`.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IP封锁**：一种更定制化的配置是对特定的CIDR范围设置规则，这与我们在iptables中使用的想法类似。我们可以通过`.spec.ingress.from.ipBlock`或`.spec.egress.to.ipBlock`来利用此配置。'
- en: 'It is recommended to check more details in the API document: [https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#networkpolicyspec-v1-networking](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#networkpolicyspec-v1-networking).
    Furthermore, we would like to show you some more interesting setups to fulfill
    general situations:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 建议查阅API文档中的更多详细信息：[https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#networkpolicyspec-v1-networking](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#networkpolicyspec-v1-networking)。此外，我们还将展示一些有趣的设置，以满足常见的情况：
- en: '**Apply to all Pod**: A NetworkPolicy can be easily pushed to every Pod by
    setting `.spec.podSelector` with an empty value.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用于所有Pod**：通过将`.spec.podSelector`设置为空值，可以轻松将NetworkPolicy推送到每个Pod。'
- en: '**Allow all traffic**: We may allow all incoming traffic by assigning `.spec.ingress`
    with empty value, an empty array; accordingly, outgoing traffic could be set without
    any restriction by assigning `.spec.egress` with empty value.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**允许所有流量**：我们可以通过将`.spec.ingress`设置为空值（空数组）来允许所有传入流量；同样，通过将`.spec.egress`设置为空值，可以不加任何限制地允许所有的传出流量。'
- en: '**Deny all traffic**: We may deny all incoming or outgoing traffic by simply
    indicating the type of NetworkPolicy without setting any rule. The type of the
    NetworkPolicy can be set at `.spec.policyTypes`. At the same time, do not set
    `.spec.ingress or .spec.egress`.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拒绝所有流量**：我们可以通过简单地指明NetworkPolicy的类型而不设置任何规则，来拒绝所有的传入或传出流量。NetworkPolicy的类型可以在`.spec.policyTypes`中设置。同时，不设置`.spec.ingress`或`.spec.egress`。'
- en: Go check the code bundle for the example files `networkpolicy-allow-all.yaml`
    and `networkpolicy-deny-all.yaml`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 请检查代码包中的示例文件`networkpolicy-allow-all.yaml`和`networkpolicy-deny-all.yaml`。
- en: Pod-to-Service communication
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod与Service的通信
- en: 'In the ordinary course of events, Pods can be stopped accidentally. Then, the
    IP of the Pod can be changed. When we expose the port for a Pod or a Deployment,
    we create a Kubernetes Service that acts as a proxy or a load balancer. Kubernetes
    would create a virtual IP, which receives the request from clients and proxies
    the traffic to the Pods in a service. Let''s review how to do this:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通情况下，Pods可能会意外停止。然后，Pod的IP可能会发生变化。当我们暴露一个Pod或Deployment的端口时，我们会创建一个Kubernetes
    Service，它充当代理或负载均衡器。Kubernetes会创建一个虚拟IP，接收来自客户端的请求并将流量代理到Service中的Pods。让我们回顾一下如何操作：
- en: 'First, we would create a Deployment and expose it to a Service:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个Deployment并将其暴露为一个Service：
- en: '[PRE48]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'At this moment, check the details of the Service with the subcommand `describe`:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，请使用子命令`describe`检查Service的详细信息：
- en: '[PRE49]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The virtual IP of the Service is `10.101.160.245`, which exposes the port `8080`.
    The Service would then dispatch the traffic into the two endpoints `192.168.80.5:80`
    and `192.168.80.6:80`. Moreover, because the Service is created in `NodePort`
    type, clients can access this Service on every Kubernetes node at `<NODE_IP>:30615`.
    As with our understanding of the recipe *Working with Services* in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml),
    *Walking through Kubernetes Concepts*, it is the Kubernetes daemon `kube-proxy` that
    helps to maintain and update routing policy on every node.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 该服务的虚拟 IP 是`10.101.160.245`，暴露了端口`8080`。然后，服务将流量分发到两个端点`192.168.80.5:80`和`192.168.80.6:80`。此外，由于服务是以`NodePort`类型创建的，客户端可以通过每个
    Kubernetes 节点的`<NODE_IP>:30615`访问此服务。正如我们在[第 2 章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)《走进
    Kubernetes 概念》中对*与服务一起工作*的理解，实际上是 Kubernetes 守护进程`kube-proxy`帮助在每个节点上维护和更新路由策略。
- en: 'Continue on, checking the `iptable` on any Kubernetes node:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续检查任何 Kubernetes 节点上的`iptable`：
- en: Attention! If you are in minikube environment, you should jump into the node
    with the command `minikube ssh`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 注意！如果你在 minikube 环境中，应该使用命令`minikube ssh`进入节点。
- en: '[PRE50]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'There will be a lot of rules showing out. To focus on policies related to the
    Service `nodeport-svc`, go through the following steps for checking them all.
    The output on your screen may not be listed in the expected order:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 会有很多规则显示出来。为了专注于与服务`nodeport-svc`相关的策略，请按照以下步骤检查所有规则。屏幕上的输出可能不会按预期的顺序列出：
- en: Find targets under chain `KUBE-NODEPORTS` with the comment mentioned `nodeport-svc`. One
    target will be named with the prefix `KUBE-SVC-`. In the preceding output, it
    is the one named `KUBE-SVC-GFPAJ7EGCNM4QF4H`. Along with the other target `KUBE-MARK-MASQ`,
    they work on passing traffics at port `30615` to the Service.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在链`KUBE-NODEPORTS`下查找带有注释`nodeport-svc`的目标。一个目标将以`KUBE-SVC-`为前缀命名。在前面的输出中，它是名为`KUBE-SVC-GFPAJ7EGCNM4QF4H`的目标。与另一个目标`KUBE-MARK-MASQ`一起，它们负责将端口`30615`上的流量转发到服务。
- en: Find a specific target named `KUBE-SVC-XXX` under `Chain KUBE-SERVICES`. In
    this case, it is the target named `KUBE-SVC-GFPAJ7EGCNM4QF4H`, ruled as allowing
    traffics from "everywhere" to the endpoint of `nodeport-svc`, `10.160.245:8080`.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Chain KUBE-SERVICES`下查找名为`KUBE-SVC-XXX`的特定目标。在这个案例中，它是名为`KUBE-SVC-GFPAJ7EGCNM4QF4H`的目标，规定允许来自“任何地方”的流量到达`nodeport-svc`的端点`10.160.245:8080`。
- en: Find targets under the specific `Chain KUBE-SVC-XXX`. In this case, it is `Chain
    KUBE-SVC-GFPAJ7EGCNM4QF4H`. Under the Service chain, you will have number of targets
    based on the according Pods with the prefix `KUBE-SEP-`. In the preceding output,
    they are `KUBE-SEP-TC6HXYYMMLGUSFNZ` and `KUBE-SEP-DIS6NYZTQKZ5ALQS`.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在特定的`Chain KUBE-SVC-XXX`下查找目标。在此案例中，它是`Chain KUBE-SVC-GFPAJ7EGCNM4QF4H`。在服务链下，您将根据相应的
    Pods 有多个目标，目标的前缀为`KUBE-SEP-`。在前面的输出中，它们是`KUBE-SEP-TC6HXYYMMLGUSFNZ`和`KUBE-SEP-DIS6NYZTQKZ5ALQS`。
- en: Find targets under specific `Chain KUBE-SEP-YYY`. In this case, the two chains
    required to take a look are `Chain KUBE-SEP-TC6HXYYMMLGUSFNZ` and `Chain KUBE-SEP-DIS6NYZTQKZ5ALQS`.
    Each of them covers two targets, `KUBE-MARK-MASQ` and `DNAT`, for incoming and
    outgoing traffics between "everywhere" to the endpoint of Pod, `192.168.80.5:80`
    or `192.168.80.6:80`.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在特定的`Chain KUBE-SEP-YYY`下查找目标。在这个案例中，两个需要查看的链是`Chain KUBE-SEP-TC6HXYYMMLGUSFNZ`和`Chain
    KUBE-SEP-DIS6NYZTQKZ5ALQS`。它们各自包含两个目标，`KUBE-MARK-MASQ`和`DNAT`，用于处理从“任何地方”到 Pod
    端点 `192.168.80.5:80` 或 `192.168.80.6:80` 的进出流量。
- en: One key point here is that the Service target `KUBE-SVC-GFPAJ7EGCNM4QF4H` exposing
    its cluster IP to outside world will dispatch the traffic to chain `KUBE-SEP-TC6HXYYMMLGUSFNZ`
    and `KUBE-SEP-DIS6NYZTQKZ5ALQS` with a statistic mode random probability of 0.5\.
    Both chains have DNAT targets that work on changing the destination IP of the
    packets to the private subnet one, the one of a specific Pod.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个关键点是，服务目标`KUBE-SVC-GFPAJ7EGCNM4QF4H`将其集群 IP 暴露到外部世界，并将流量转发到链`KUBE-SEP-TC6HXYYMMLGUSFNZ`和`KUBE-SEP-DIS6NYZTQKZ5ALQS`，并以随机概率
    0.5 作为统计模式。两个链都有 DNAT 目标，负责将数据包的目标 IP 更改为特定 Pod 所在的私有子网 IP。
- en: External-to-internal communication
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部到内部的通信
- en: To publish applications in Kubernetes, we can leverage either Kubernetes Service,
    with type `NodePort` or `LoadBalancer`, or Kubernetes Ingress. For NodePort service,
    as introduced in previous section, the port number of the node will be a pair
    with the Service. Like the following diagram, port `30361` on both node 1 and
    node 2 points to Service A, which dispatch the traffics to Pod1 and a Pod with
    static probability.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 Kubernetes 中发布应用程序，我们可以利用 Kubernetes Service，类型为 `NodePort` 或 `LoadBalancer`，或者
    Kubernetes Ingress。对于 NodePort 服务，如前一节介绍的那样，节点的端口号会与 Service 配对。如下图所示，节点 1 和节点
    2 上的端口 `30361` 指向 Service A，该 Service 会将流量调度到 Pod1 和一个具有静态概率的 Pod。
- en: 'LoadBalancer Service, as you may have learned from the recipe *Working with
    Services* in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)*, Walking
    through Kubernetes Concepts*, includes the configurations of NodePort. Moreover,
    a LoadBalancer Service can work with an external load balancer, providing users
    with the functionality to integrate load balancing procedures between cloud infrastructure
    and Kubernetes resource, such as the settings `healthCheckNodePort` and `externalTrafficPolicy`.
    **Service B** in the following image is a LoadBalancer Service. Internally, **Service
    B** works the same as **Service A**, relying on **iptables** to redirect packets
    to Pod; Externally, cloud load balancer doesn''t realize Pod or container, it
    only dispatches the traffic by the number of nodes. No matter which node is chosen
    to get the request, it would still be able to pass packets to the right Pod:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: LoadBalancer Service，正如你在 [第 2 章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)*“与
    Services 一起工作”*中学到的内容，包含了 NodePort 的配置。此外，LoadBalancer Service 可以与外部负载均衡器一起工作，为用户提供在云基础设施与
    Kubernetes 资源之间集成负载均衡过程的功能，例如 `healthCheckNodePort` 和 `externalTrafficPolicy`
    的设置。下图中的 **Service B** 是一个 LoadBalancer Service。在内部，**Service B** 和 **Service
    A** 的工作方式相同，依赖 **iptables** 将数据包重定向到 Pod；在外部，云负载均衡器并不认识 Pod 或容器，它只是根据节点的数量分配流量。无论选择哪个节点来接收请求，它依然能将数据包传递到正确的
    Pod。
- en: '![](img/161638c9-b9f8-44e4-84e5-48ab67514b83.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/161638c9-b9f8-44e4-84e5-48ab67514b83.png)'
- en: Kubernetes Services with type NodePort and type LoadBalancer
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的 NodePort 类型和 LoadBalancer 类型 Services
- en: Working with Ingress
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Ingress
- en: Walking through the journey of Kubernetes networking, users get the idea that
    each Pod and Service has its private IP and corresponding port to listen on request.
    In practice, developers may deliver the endpoint of service, the private IP or
    Kubernetes DNS name, for internal clients; or, developers may expose Services
    externally by type of NodePort or LoadBalancer. Although the endpoint of Service
    is more stable than Pod, the Services are offered separately, and clients should
    record the IPs without much meaning to them. In this section, we will introduce
    `Ingress`, a resource that makes your Services work as a group. More than that,
    we could easily pack our service union as an API server while we set Ingress rules
    to recognize the different URLs, and then ingress controller works for passing
    the request to specific Services based on the rules.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在走访 Kubernetes 网络的过程中，用户会意识到每个 Pod 和 Service 都有自己的私有 IP 和相应的端口来监听请求。在实际应用中，开发人员可能会为内部客户端提供服务的端点、私有
    IP 或 Kubernetes DNS 名称；或者，开发人员可能会通过 NodePort 或 LoadBalancer 类型将 Services 暴露给外部。尽管
    Service 的端点比 Pod 更稳定，但 Services 是单独提供的，客户端应该记录这些 IP 地址，尽管这些 IP 对它们没有太大意义。在本节中，我们将介绍
    `Ingress`，一种可以使你的 Services 作为一个整体工作的资源。更重要的是，我们可以轻松地将服务集打包成一个 API 服务器，同时设置 Ingress
    规则来识别不同的 URL，然后 Ingress 控制器根据这些规则将请求转发到特定的 Services。
- en: 'Before we try on Kubernetes Ingress, we should create an ingress controller
    in cluster. Different from other controllers in `kube-controller-manager`([https://kubernetes.io/docs/reference/generated/kube-controller-manager/](https://kubernetes.io/docs/reference/generated/kube-controller-manager/)),
    ingress controller is run by custom implementation instead of working as a daemon.
    In the latest Kubernetes version, 1.10, nginx ingress controller is the most stable
    one and also generally supports many platforms. Check the official documents for
    the details of deployment: [https://github.com/kubernetes/ingress-nginx/blob/master/README.md](https://github.com/kubernetes/ingress-nginx/blob/master/README.md).
    We will only demonstrate our example on minikube; please see the following information
    box for the setup of the ingress controller.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们尝试 Kubernetes Ingress 之前，我们应在集群中创建一个 ingress controller。与`kube-controller-manager`([https://kubernetes.io/docs/reference/generated/kube-controller-manager/](https://kubernetes.io/docs/reference/generated/kube-controller-manager/))中的其他控制器不同，ingress
    controller 是通过自定义实现运行的，而不是作为守护进程工作。在最新的 Kubernetes 版本 1.10 中，nginx ingress controller
    是最稳定的，并且通常支持许多平台。有关部署的详细信息，请参见官方文档：[https://github.com/kubernetes/ingress-nginx/blob/master/README.md](https://github.com/kubernetes/ingress-nginx/blob/master/README.md)。我们只会在
    minikube 上演示我们的示例；请查看以下信息框以了解 ingress controller 的设置。
- en: 'Enable Ingress functionality in minikubeIngress in minikube is an add-on function.
    Follow these steps to start this feature in your environment:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在 minikube 中启用 Ingress 功能：Ingress 在 minikube 中是一个附加功能。按照以下步骤在你的环境中启动此功能：
- en: 'Check if the add-on ingress is enabled or not: Fire the command `minikube addons
    list` on your terminal. If it is not enabled, means it shows `ingress: disabled`,
    you should keep follow below steps.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '检查附加的 Ingress 是否启用：在终端执行命令`minikube addons list`。如果未启用，显示为`ingress: disabled`，则应按照以下步骤继续操作。'
- en: 'Enable ingress: Enter the command `minikube addons enable ingress`, you will
    see an output like `ingress was successfully enabled`.'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用 ingress：输入命令`minikube addons enable ingress`，你将看到类似`ingress was successfully
    enabled`的输出。
- en: Check the add-on list again to verify that the last step does work. We expect
    that the field ingress shows as `enabled`.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次检查附加组件列表，以验证上一步是否生效。我们期望 ingress 字段显示为`enabled`。
- en: 'Here comes an example to demonstrate how to work with Ingress. We would run
    up two Deployments and their Services, and an additional Ingress to expose them
    as a union. In the beginning, we would add a new hostname in the host file of
    Kubernetes master. It is a simple way for our demonstration. If you work on the
    production environment, a general use case is that the hostname should be added
    as a record in the DNS server:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，演示如何与 Ingress 配合使用。我们将运行两个 Deployment 及其对应的 Service，并创建一个额外的 Ingress，将它们作为一个联合体暴露出来。一开始，我们将在
    Kubernetes master 的主机文件中添加一个新主机名。这是一个简单的演示方法。如果你在生产环境中工作，一般的使用案例是将主机名作为记录添加到 DNS
    服务器中：
- en: '[PRE51]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Our first Kubernetes Deployment and Service would be `echoserver`, a dummy
    Service showing server and request information. For the other pair of Deployment
    and Service, we would reuse the NodePort Service example from the previous section:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个 Kubernetes Deployment 和 Service 将是 `echoserver`，这是一个显示服务器和请求信息的虚拟 Service。对于另一对
    Deployment 和 Service，我们将重用前一节中的 NodePort Service 示例：
- en: '[PRE52]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Go ahead and create both set of resources through configuration files:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 继续通过配置文件创建两组资源：
- en: '[PRE53]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Our first Ingress makes two Services that listen at the separate URLs `/nginx`
    and `/echoserver`, with the hostname `happy.k8s.io`, the dummy one we added in
    the local host file. We use annotation `rewrite-target` to guarantee that traffic
    redirection starts from root, `/`. Otherwise, the client may get page not found
    because of surfing the wrong path. More annotations we may play with are listed
    at [https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md):'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个 Ingress 创建了两个在不同 URL `/nginx` 和 `/echoserver` 上监听的 Service，主机名为 `happy.k8s.io`，这是我们在本地主机文件中添加的虚拟主机。我们使用注解
    `rewrite-target` 来确保流量重定向从根目录`/`开始。否则，客户端可能会因为路径错误而无法找到页面。我们可能会使用的更多注解列在[https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md)：
- en: '[PRE54]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Then, just create the Ingress and check its information right away:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，创建 Ingress 并立即检查其信息：
- en: '[PRE55]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'You may find that there is no IP address in the field of description. It will
    be attached after the first DNS lookup:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现描述字段中没有 IP 地址。它将在第一次进行 DNS 查找后附加上来：
- en: '[PRE56]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Although working with Ingress is not as straightforward as other resources,
    as you have to start an ingress controller implementation by yourself, it still
    makes our application exposed and flexible. There are many network features coming
    that are more stable and user friendly. Keep up with the latest updates and have
    fun!
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管与 Ingress 一起使用并不像其他资源那样直接，因为你需要自己启动 Ingress 控制器实现，但它仍然使我们的应用得以暴露并具有灵活性。许多更稳定、用户友好的网络功能正在到来。跟上最新的更新，享受其中的乐趣吧！
- en: There's more...
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In the last part of external-to-internal communication, we learned about Kubernetes
    Ingress, the resource that makes services work as a union and dispatches requests
    to target services. Does any similar idea jump into your mind? It sounds like
    a microservice, the application structure with several loosely coupled services.
    A complicated application would be distributed to multiple lighter services. Each
    service is developed independently while all of them can cover original functions.
    Numerous working units, such as Pods in Kubernetes, run volatile and can be dynamically
    scheduled on Services by the system controller. However, such a multi-layered
    structure increases the complexity of networking and also suffers potential overhead
    costs.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在外部到内部通信的最后部分，我们了解了 Kubernetes Ingress——这个使服务协同工作并将请求调度到目标服务的资源。有没有什么类似的想法浮现在你脑海中？它听起来像是一个微服务架构，多个松耦合的服务组成的应用结构。一个复杂的应用会分布到多个较轻量的服务中。每个服务独立开发，同时都可以覆盖原有功能。像
    Kubernetes 中的 Pods 等大量工作单元运行时是波动的，且可以由系统控制器在服务之间动态调度。然而，这种多层次的结构增加了网络的复杂性，并且可能带来额外的开销。
- en: 'External load balancers are not aware the existence of Pods; they only balance
    the workload to hosts. A host without any served Pod running would then redirect
    the loading to other hosts. This situation comes out of a user''s expectation
    for fair load balancing. Moreover, a Pod may crash accidentally, in which case
    it is difficult to do failover and complete the request. To make up the shortcomings,
    the idea of a service mesh focus on the networking management of microservice
    was born, dedicated to delivering more reliable and performant communications
    on orchestration like Kubernetes:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 外部负载均衡器并不了解 Pods 的存在；它们只会将工作负载平衡到主机。如果某个主机上没有正在运行的服务 Pod，那么它会将负载重定向到其他主机。这种情况可能不符合用户对公平负载均衡的预期。而且，Pod
    可能会意外崩溃，在这种情况下，进行故障转移并完成请求会变得非常困难。为弥补这些不足，专注于微服务网络管理的服务网格思想应运而生，旨在为像 Kubernetes
    这样的编排系统提供更可靠和高效的通信：
- en: '![](img/efbc7c62-a23c-41ed-95d0-4c5dd0540871.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/efbc7c62-a23c-41ed-95d0-4c5dd0540871.png)'
- en: Simpe service mesh structure
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的服务网格结构
- en: 'The preceding diagram illustrates the main components in a service mesh. They
    work together to achieve features as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表展示了服务网格中的主要组件。它们协同工作，达成如下功能：
- en: '**Service mesh ingress**: Using applied Ingress rules to decide which Service
    should handle the incoming requests. It could also be a proxy that is able to
    check the runtime policies.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务网格 Ingress**：使用应用的 Ingress 规则来决定哪个服务应该处理传入的请求。它也可以是一个能够检查运行时策略的代理。'
- en: '**Service mesh proxy**: Proxies on every node not only direct the packets,
    but can also be used as an advisory agent reporting the overall status of the
    Services.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务网格代理**：每个节点上的代理不仅仅是引导数据包，还可以作为一个咨询代理，报告服务的整体状态。'
- en: '**Service mesh service discovery pool**: Serving the central management for
    mesh and pushing controls over proxies. Its responsibility includes procedures
    of network capability, authentication, failover, and load balancing.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务网格服务发现池**：为网格提供中央管理并推动对代理的控制。它的职责包括网络能力、身份验证、故障转移和负载均衡等过程。'
- en: Although well-known service mesh implementations such as Linkerd ([https://linkerd.io](https://linkerd.io))
    and Istio ([https://istio.io](https://istio.io)) are not mature enough for production
    usage, the idea of service mesh is not ignorable.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像 Linkerd ([https://linkerd.io](https://linkerd.io)) 和 Istio ([https://istio.io](https://istio.io))
    这样的知名服务网格实现还不成熟，尚未适合生产环境使用，但服务网格的理念不可忽视。
- en: See also
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'Kubernetes forwards ports based on the overlay network. In this chapter, we
    also run Pods and Services with nginx. Reviewing the previous sections will help
    you to understand more about how to manipulate it. Also, look at the following
    recipes:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 基于覆盖网络进行端口转发。在本章中，我们还使用 nginx 运行 Pods 和 Services。回顾前面的章节将帮助你更好地理解如何操作它。同时，看看以下的示例：
- en: The *Creating an overlay network* and*Running your first container in Kubernetes*
    recipes in [Chap](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)[ter](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)
    [1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml), *Building Your Own Kubernetes Cluster*
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第1章](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)中的*创建覆盖网络*和*在Kubernetes中运行你的第一个容器*教程，**构建你自己的Kubernetes集群**'
- en: The *Working with Pods *and*Working with Services* recipes in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)**,
    Walking through Kubernetes Concepts**
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第2章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)中的*与Pods的工作*和*与Services的工作*教程，**深入了解Kubernetes概念**'
- en: The *Moving monolithic to microservices* recipe in [Chapter 5](669edaf0-c274-48fa-81d8-61150fa36df5.xhtml)*,
    Building Continuous Delivery Pipelines*
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第5章](669edaf0-c274-48fa-81d8-61150fa36df5.xhtml)中的*将单体应用迁移到微服务*教程，**构建持续交付管道**'
- en: Ensuring flexible usage of your containers
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保容器的灵活使用
- en: Pod, in Kubernetes, means a set of containers, which is also the smallest computing
    unit. You may have know about the basic usage of Pod in the previous recipes.
    Pods are usually managed by deployments and exposed by services; they work as
    applications with this scenario.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，Pod指一组容器，它也是最小的计算单元。你可能已经在之前的教程中了解了Pod的基本用法。Pod通常由deployments管理，并通过services暴露；在这种场景中，它们作为应用程序运行。
- en: 'In this recipe, we will discuss two new features: **DaemonSets** and **StatefulSets**.
    These two features can manage Pods with more specific purpose.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将讨论两个新特性：**DaemonSets**和**StatefulSets**。这两个特性可以管理具有更具体目的的Pods。
- en: Getting ready
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: What are **Daemon-like Pod **and **Stateful Pod**? The regular Pods in Kubernetes
    will determine and dispatch to particular Kubernetes nodes based on current node
    resource usage and your configuration.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是**Daemon-like Pod**和**Stateful Pod**？Kubernetes中的常规Pod将根据当前节点的资源使用情况和你的配置，确定并调度到特定的Kubernetes节点。
- en: However, a **Daemon-like Pod **will be created in each node. For example, if
    you have three nodes, three daemon-like Pods will be created and deployed to each
    node. Whenever a new node is added, DaemonSets Pod will be deployed to the new
    node automatically. Therefore, it will be useful to use node level monitoring
    or log correction.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，**Daemon-like Pod**将在每个节点上创建。例如，如果你有三个节点，三个Daemon-like Pods将被创建并部署到每个节点上。每当添加新节点时，DaemonSets
    Pod会自动部署到新节点。因此，它对于使用节点级别监控或日志收集非常有用。
- en: On the other hand, a **Stateful Pod **will stick to some resources such as network
    identifier (Pod name and DNS) and **p****ersistent volume** (**PV**). This also
    guarantees an order during deployment of multiple Pods and during rolling update.
    For example, if you deploy a Pod named `my-pod`, and set the scale to **4**, then
    Pod name will be assigned as `my-pod-0`, `my-pod-1`, `my-pod-2`, and `my-pod-3`.
    Not only Pod name but also DNS and persistent volume are preserved. For example,
    when `my-pod-2` is recreated due to resource shortages or application crash, those
    names and volumes are taken over by a new Pod which is also named `my-pod-2`.
    It is useful for some cluster based applications such as HDFS and ElasticSearch.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**Stateful Pod**会绑定到一些资源，如网络标识符（Pod名称和DNS）和**持久卷**（**PV**）。这也保证了在多个Pod的部署过程中以及滚动更新时的顺序。例如，如果你部署一个名为`my-pod`的Pod，并将规模设置为**4**，那么Pod的名称将被分配为`my-pod-0`、`my-pod-1`、`my-pod-2`和`my-pod-3`。不仅Pod名称，DNS和持久卷也会得到保留。例如，当`my-pod-2`由于资源短缺或应用崩溃被重新创建时，这些名称和卷将由一个新的Pod接管，并且这个Pod也会命名为`my-pod-2`。它对于一些基于集群的应用，如HDFS和ElasticSearch，尤其有用。
- en: In this recipe, we will demonstrate how to use DaemonSets and StatefulSet; however,
    to have a better understanding, it should use multiple Kubernetes Nodes environment.
    To do this, minikube is not ideal, so instead, use either kubeadm/kubespray to
    create a multiple Node environment.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将演示如何使用DaemonSets和StatefulSet；然而，为了更好地理解，建议使用多个Kubernetes节点的环境。为了实现这一点，minikube并不理想，因此，建议使用kubeadm/kubespray来创建多节点环境。
- en: Using kubeadm or kubespray to set up Kubernetes cluster was described in [Chapter
    1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)*,* *Build Your Own Kubernetes Cluster.*
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 使用kubeadm或kubespray设置Kubernetes集群的过程在[第1章](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)《构建你的Kubernetes集群》中有描述。
- en: 'To confirm whether that has 2 or more nodes, type `kubectl get nodes` as follows
    to check how many nodes you have:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 要确认是否有两个或更多节点，可以输入`kubectl get nodes`，如下所示，以检查你有多少个节点：
- en: '[PRE57]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: In addition, if you want to execute the StatefulSet recipe later in this chapter,
    you need a StorageClass to set up a dynamic provisioning environment. It was described
    in *Working with volumes* section in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml), *Walking
    through Kubernetes Concepts*. It is recommended to use public cloud such as AWS
    and GCP with a CloudProvider; this will be described in [Chapter 6](b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml),
    *Building Kubernetes on AWS* and [Chapter 7](dfc46490-f109-4f07-ba76-1a381b006d76.xhtml),
    *Building Kubernetes on GCP*, as well.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你想在本章稍后执行 StatefulSet 配方，你需要一个 StorageClass 来设置动态供应环境。它在 [第 2 章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)
    *处理存储卷* 部分中有描述，*了解 Kubernetes 概念*。建议使用公共云服务，如 AWS 和 GCP，并配置 CloudProvider；这将在
    [第 6 章](b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml) *在 AWS 上构建 Kubernetes* 和 [第
    7 章](dfc46490-f109-4f07-ba76-1a381b006d76.xhtml) *在 GCP 上构建 Kubernetes* 中进行详细描述。
- en: 'To check whether `StorageClass` is configured or not, use `kubectl get sc`:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查是否已配置 `StorageClass`，请使用 `kubectl get sc`：
- en: '[PRE58]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: How to do it...
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: There is no CLI for us to create DaemonSets or StatefulSets. Therefore, we will
    build these two resource types by writing all the configurations in a YAML file.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有 CLI 用于创建 DaemonSets 或 StatefulSets。因此，我们将通过编写 YAML 文件中的所有配置来构建这两种资源类型。
- en: Pod as DaemonSets
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod 作为 DaemonSets
- en: If a Kubernetes DaemonSet is created, the defined Pod will be deployed in every
    single node. It is guaranteed that the running containers occupy equal resources
    in each node. In this scenario, the container usually works as the daemon process.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果创建了 Kubernetes DaemonSet，定义的 Pod 将会在每个节点上部署。确保每个节点上的运行容器占用相等的资源。在这种情况下，容器通常作为守护进程运行。
- en: 'For example, the following template has an Ubuntu image container that keeps
    checking its memory usage half a minute at a time:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下模板包含一个 Ubuntu 镜像容器，该容器每半分钟检查一次其内存使用情况：
- en: 'To build it as a DaemonSet, execute the following code block:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将其构建为 DaemonSet，请执行以下代码块：
- en: '[PRE59]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: As the Job, the selector could be ignored, but it takes the values of the labels.
    We will always configure the restart policy of the DaemonSet as `Always`, which
    makes sure that every node has a Pod running.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如同作业一样，选择器可以被忽略，但它会获取标签的值。我们将始终将 DaemonSet 的重启策略配置为 `Always`，确保每个节点都有一个 Pod
    运行。
- en: 'The abbreviation of the `daemonset` is `ds` in `kubectl` command, use this
    shorter one in the CLI for convenience:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`daemonset` 在 `kubectl` 命令中的缩写是 `ds`，为了方便，在 CLI 中使用这个简短的命令：'
- en: '[PRE60]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Here, we have two Pods running in separated nodes. They can still be recognized
    in the channel of the `pod`:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们有两个 Pods 运行在分开的节点上。它们仍然可以在 `pod` 渠道中被识别：
- en: '[PRE61]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'It is good for you to evaluate the result using the subcommand `kubectl logs`:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用子命令 `kubectl logs` 评估结果是很好的：
- en: '[PRE62]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Whenever, you add a Kubernetes node onto your existing cluster, DaemonSets will
    recognize and deploy a Pod automatically.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你将一个 Kubernetes 节点添加到现有集群时，DaemonSets 会自动识别并部署一个 Pod。
- en: 'Let''s check again current status of DaemonSets, there are two Pods that have
    been deployed due to having two nodes as follows:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再次检查当前 DaemonSets 的状态，以下是由于有两个节点而部署的两个 Pods：
- en: '[PRE63]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'So, now we are adding one more node onto the cluster through either `kubespray`
    or `kubeadm`, based on your setup:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所以，现在我们通过 `kubespray` 或 `kubeadm` 添加一个新的节点到集群中，具体取决于你的设置：
- en: '[PRE64]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'A few moments later, without any operation, the DaemonSet''s size become `3`
    automatically, which aligns to the number of nodes:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 几分钟后，DaemonSet 的大小自动变为 `3`，与节点的数量一致：
- en: '[PRE65]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Running a stateful Pod
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行有状态的 Pod
- en: Let's see another use case. We used Deployments/ReplicaSets to replicate the
    Pods. It scales well and is easy to maintain and Kubernetes assigns a DNS to the
    Pod using the Pod's IP address, such as `<Pod IP address>.<namespace>.pod.cluster.local`.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看另一个使用场景。我们使用 Deployments/ReplicaSets 来复制 Pods。它扩展得很好且易于维护，Kubernetes 使用
    Pod 的 IP 地址为 Pod 分配一个 DNS，例如 `<Pod IP 地址>.<namespace>.pod.cluster.local`。
- en: 'The following example demonstrates how the Pod DNS will be assigned:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何分配 Pod 的 DNS：
- en: '[PRE66]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'However, this DNS entry is not guaranteed to stay in use for this Pod, because
    the Pod might crash due to an application error or node resource shortage. In
    such a case, the IP address will possibly be changed:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个 DNS 条目并不保证会一直用于这个 Pod，因为 Pod 可能由于应用错误或节点资源不足而崩溃。在这种情况下，IP 地址可能会改变：
- en: '[PRE67]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: For some applications, this will cause an issue; for example, if you manage
    a cluster application that needs to be managed by DNS or IP address. As of the
    current Kubernetes implementation, IP addresses can't be preserved for Pods .
    How about we use Kubernetes Service? Service preserves a DNS name. Unfortunately,
    it's not realistic to create the same amount of service with Pod. In the previous
    case, create three Services that bind to three Pods one to one.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些应用程序，这会导致问题；例如，如果您管理一个需要通过 DNS 或 IP 地址管理的集群应用程序。根据当前 Kubernetes 的实现，Pod
    的 IP 地址不能被保留。那我们如何使用 Kubernetes 服务呢？服务可以保留 DNS 名称。不幸的是，创建与 Pod 相同数量的服务并不现实。在之前的情况下，创建三个服务，每个服务与三个
    Pod 一一绑定。
- en: Kubernetes has a solution for this kind of use case that uses StatefulSet**.** It
    preserves not only the DNS but also the persistent volume to keep a bind to the
    same Pod. Even if Pod is crashed, StatefulSet guarantees the binding of the same
    DNS and persistent volume to the new Pod. Note that the IP address is not preserved
    due to the current Kubernetes implementation.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 针对这种使用场景有一个解决方案，那就是使用 **StatefulSet**。它不仅保留 DNS，还保留持久卷，以确保绑定到相同的
    Pod。即使 Pod 崩溃，StatefulSet 也能保证将相同的 DNS 和持久卷绑定到新 Pod。请注意，当前的 Kubernetes 实现并不会保留
    IP 地址。
- en: 'To demonstrate, use **Hadoop Distributed File System** (**HDFS**) to launch
    one NameNode and three DataNodes. To perform this, use a Docker image from [https://hub.docker.com/r/uhopper/hadoop/](https://hub.docker.com/r/uhopper/hadoop/)
    that has NameNode and DataNode images. In addition, borrow the YAML configuration
    files `namenode.yaml` and `datanode.yaml` from [https://gist.github.com/polvi/34ef498a967de563dc4252a7bfb7d582](https://gist.github.com/polvi/34ef498a967de563dc4252a7bfb7d582)
    and change a little bit:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，使用 **Hadoop 分布式文件系统** (**HDFS**) 启动一个 NameNode 和三个 DataNode。为此，使用来自 [https://hub.docker.com/r/uhopper/hadoop/](https://hub.docker.com/r/uhopper/hadoop/)
    的 Docker 镜像，该镜像包含 NameNode 和 DataNode 镜像。此外，从 [https://gist.github.com/polvi/34ef498a967de563dc4252a7bfb7d582](https://gist.github.com/polvi/34ef498a967de563dc4252a7bfb7d582)
    借用 YAML 配置文件 `namenode.yaml` 和 `datanode.yaml`，并稍作修改：
- en: 'Let''s launch a Service and StatefulSet for `namenode` and `datanode`:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们为 `namenode` 和 `datanode` 启动一个服务和 StatefulSet：
- en: '[PRE68]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: As you can see, the Pod naming convention is `<StatefulSet-name>-<sequence number>`. For
    example, NameNode Pod's name is `hdfs-namenode-0`. Also DataNode Pod's names are
    `hdfs-datanode-0`, `hdfs-datanode-1` and `hdfs-datanode-2`.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Pod 的命名约定是 `<StatefulSet-name>-<sequence number>`。例如，NameNode Pod 的名称是
    `hdfs-namenode-0`。DataNode Pod 的名称分别是 `hdfs-datanode-0`、`hdfs-datanode-1` 和 `hdfs-datanode-2`。
- en: 'In addition, both NameNode and DataNode have a service that is configured as
    Headless mode (by `spec.clusterIP: None`). Therefore, you can access these Pods using
    DNS as `<pod-name>.<service-name>.<namespace>.svc.cluster.local`. In this case,
    this NameNode DNS entry could be `hdfs-namenode-0.hdfs-namenode-svc.default.svc.cluster.local`*.*'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，NameNode 和 DataNode 都配置为无头模式的服务（通过 `spec.clusterIP: None`）。因此，您可以使用 DNS
    通过 `<pod-name>.<service-name>.<namespace>.svc.cluster.local` 来访问这些 Pods。在这种情况下，NameNode
    的 DNS 条目可能是 `hdfs-namenode-0.hdfs-namenode-svc.default.svc.cluster.local`*。'
- en: 'Let''s check what NameNode Pod''s IP address is, you can get this using `kubectl
    get pods -o wide` as follows:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查 NameNode Pod 的 IP 地址，您可以使用 `kubectl get pods -o wide` 来获取，如下所示：
- en: '[PRE69]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Next, log in (run `/bin/bash`) to one of the DataNodes using `kubectl exec`
    to resolve this DNS name and check whether the IP address is `10.52.2.8` or not:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用 `kubectl exec` 登录（运行 `/bin/bash`）到其中一个 DataNode，解析该 DNS 名称并检查 IP 地址是否为
    `10.52.2.8`：
- en: '[PRE70]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Looks all good! For demonstration purposes, let's access the HDFS web console
    to see DataNode's status.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 一切看起来都好！为了演示，我们访问 HDFS Web 控制台查看 DataNode 的状态。
- en: 'To do that, use `kubectl port-forward` to access to the NameNode web port (tcp/`50070`):'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为此，使用 `kubectl port-forward` 访问 NameNode 的 Web 端口（tcp/`50070`）：
- en: '[PRE71]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The preceding result indicates that your local machine TCP port `60107` (you
    result will vary) has been forwarded to NameNode Pod TCP port `50070`. Therefore,
    use a web browser to access `http://127.0.0.1:60107/` as follows:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述结果表明，您本地机器的 TCP 端口 `60107`（您的结果可能不同）已经转发到 NameNode Pod 的 TCP 端口 `50070`。因此，您可以通过网页浏览器访问
    `http://127.0.0.1:60107/`，如以下所示：
- en: '![](img/5bd29558-4248-4abf-8913-3959eb180f8c.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bd29558-4248-4abf-8913-3959eb180f8c.png)'
- en: HDFS Web console shows three DataNodes
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS Web 控制台显示三个 DataNode
- en: As you may see, three DataNodes have been registered to NameNode successfully.
    The DataNodes are also using the Headless Service so that same name convention
    assigns DNS names for DataNode as well.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，三个 DataNode 已成功注册到 NameNode。DataNode 也使用了无头服务，因此相同的命名约定为 DataNode 分配了 DNS
    名称。
- en: How it works...
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: DaemonSets and StatefulSets; both concepts are similar but behave differently,
    especially when Pod is crashed. Let's take a look at how it works.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSets和StatefulSets：这两个概念相似，但行为不同，尤其是在Pod崩溃时。我们来看看它是如何工作的。
- en: Pod recovery by DaemonSets
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过DaemonSets恢复Pod
- en: DaemonSets keep monitoring every Kubernetes node, so when one of the Pods crashes,
    DaemonSets recreates it on the same Kubernetes node.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSets会持续监控每个Kubernetes节点，因此当Pod崩溃时，DaemonSets会在同一Kubernetes节点上重新创建它。
- en: 'To simulate this, go back to the DaemonSets example and use `kubectl delete
    pods` to delete an existing Pod from `node1` manually, as follows:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟这一过程，请返回到 DaemonSets 示例并使用`kubectl delete pods`手动删除`node1`上的现有Pod，如下所示：
- en: '[PRE72]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: As you can see, a new Pod has been created automatically to recover the Pod in
    `node1`. Note that the Pod name has been changed from `ram-check-6ldng` to `ram-check-dh5hq`—it
    has beenassigned a random suffix name. In this use case, Pod name doesn't matter,
    because we don't use hostname or DNS to manage this application.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，新的Pod已自动创建，以恢复`node1`中的Pod。请注意，Pod名称已从`ram-check-6ldng`更改为`ram-check-dh5hq`——它已被分配了一个随机的后缀名称。在这个用例中，Pod名称并不重要，因为我们没有使用主机名或DNS来管理这个应用程序。
- en: Pod recovery by StatefulSet
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过StatefulSet恢复Pod
- en: StatefulSet behaves differently to DaemonSet during Pod recreation. In StatefulSet
    managed Pods, the Pod name is always consisted to assign an ordered number such
    as `hdfs-datanode-0`, `hdfs-datanode-1` and`hdfs-datanode-2`*,* and if you delete
    one of them, a new Pod will take over the same Pod name.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet在Pod重建时与DaemonSet行为不同。在StatefulSet管理的Pod中，Pod的名称始终是按顺序分配的，如`hdfs-datanode-0`、`hdfs-datanode-1`和`hdfs-datanode-2`，如果删除其中一个，新的Pod将继承相同的Pod名称。
- en: 'To simulate this, let''s delete one DataNode (`hdfs-datanode-1`) to see how
    StatefulSet recreates a Pod:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟这一过程，让我们删除一个DataNode（`hdfs-datanode-1`），看看StatefulSet如何重新创建Pod：
- en: '[PRE73]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'As you see, the same Pod name (`hdfs-datanode-1`) has been assigned. Approximately
    after 10 minutes (due to HDFS''s heart beat interval), HDFS web console shows
    that the old Pod has been marked as dead and the new Pod has the in service state,
    shown as follows:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，相同的Pod名称（`hdfs-datanode-1`）已被分配。大约10分钟后（由于HDFS的心跳间隔），HDFS web控制台显示旧Pod已标记为死亡，而新Pod处于服务中状态，如下所示：
- en: '![](img/8bb7096f-30a6-4cff-9aac-c1d831eb27e4.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bb7096f-30a6-4cff-9aac-c1d831eb27e4.png)'
- en: Status when one DataNode is dead
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 一个DataNode挂掉时的状态
- en: Note that this is not a perfect ideal case for HDFS, because DataNode-1 lost
    data and expects to re-sync from other DataNodes. If the data size is bigger,
    it may take a long time to complete re-sync.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这对HDFS来说并不是一个完美的理想情况，因为DataNode-1丢失了数据，并期望从其他DataNode重新同步。如果数据量较大，重新同步可能需要很长时间才能完成。
- en: Fortunately, StatefulSets has an capability that preserve a persistent volume
    while replacing a Pod. Let's see how HDFS DataNode can preserve data during Pod recreation.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，StatefulSets具有一个功能，可以在替换Pod时保留持久存储卷。我们来看看HDFS DataNode在Pod重建时如何保持数据。
- en: There's more...
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: StatefulSet with persistent volume; it requires a `StorageClass` that provisions
    a volume dynamically. Because each Pod is created by StatefulSets, it will create
    a **persistent volume claim** (**PVC**) with a different identifier. If your StatefulSets
    specify a static name of PVC, there will be trouble if multiple Pods try to attach
    the same PVC.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 带持久存储卷的StatefulSet；它需要一个动态分配存储卷的`StorageClass`。由于每个Pod都是由StatefulSet创建的，因此它会创建一个具有不同标识符的**持久卷声明**（**PVC**）。如果您的StatefulSets指定了PVC的静态名称，多个Pod尝试附加同一个PVC时会出现问题。
- en: 'If you have `StorageClass` on your cluster, update `datanode.yaml` to add `spec.volumeClaimTemplates`
    as follows:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的集群中有`StorageClass`，请更新`datanode.yaml`，并添加`spec.volumeClaimTemplates`，如下所示：
- en: '[PRE74]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'This tells Kubernetes to create a PVC and PV when a new Pod is created by StatefulSet.
    So, that Pod template (`spec.template.spec.containers.volumeMounts`) should specify
    `hdfs-data`, as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉Kubernetes在StatefulSet创建新Pod时创建PVC和PV。因此，Pod模板（`spec.template.spec.containers.volumeMounts`）应指定`hdfs-data`，如下所示：
- en: '[PRE75]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Let''s recreate HDFS cluster again:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次重建HDFS集群：
- en: '[PRE76]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'To demonstrate, use `kubectl exec` to access the NameNode, then copy some dummy
    files to HDFS:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，使用`kubectl exec`访问NameNode，然后将一些虚拟文件复制到HDFS：
- en: '[PRE77]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'At this moment, `DataNode-1` is restarting, as shown in the following image.
    However, the data directory of `DataNode-1` is kept by PVC as `hdfs-data-hdfs-datanode-1`.
    The new Pod `hdfs-datanode-1` will take over this PVC again:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`DataNode-1`正在重新启动，如下图所示。然而，`DataNode-1`的数据目录由PVC保持为`hdfs-data-hdfs-datanode-1`。新的Pod
    `hdfs-datanode-1`将再次继承该PVC：
- en: '![](img/d97460f9-8986-43df-bf2b-3311fc0f4aca.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d97460f9-8986-43df-bf2b-3311fc0f4aca.png)'
- en: StatefulSet keeps PVC/PV while restarting
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet 在重启时会保留 PVC/PV
- en: 'Therefore, when you access HDFS after `hdfs-datanode-1` has recovered, you
    don''t see any data loss or re-sync processes:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当 `hdfs-datanode-1` 恢复后，你访问 HDFS 时，不会看到任何数据丢失或重新同步过程：
- en: '[PRE78]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'As you see, the Pod and PV pair is fully managed by StatefulSets. It is convenient
    if you want to scale more HDFS DataNode using just the `kubectl scale` command
    to make it double or hundreds—whatever you need:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Pod 和 PV 配对由 StatefulSets 完全管理。如果你希望通过 `kubectl scale` 命令将 HDFS DataNode
    扩展为双倍或数百倍——根据需求，这种方式非常方便：
- en: '[PRE79]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: You can also use PV to NameNode to persist metadata. However, `kubectl` scale
    does not work well due to HDFS architecture. In order to have high availability
    or scale out HDFS NameNode, please visit the HDFS Federation document at : [https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/Federation.html](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/Federation.html).
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用 PV 来持久化 NameNode 的元数据。然而，由于 HDFS 架构，`kubectl` 扩展不太好用。为了实现 HDFS NameNode
    的高可用性或横向扩展，请访问 HDFS 联邦文档：[https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/Federation.html](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/Federation.html)。
- en: See also
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'In this recipe, we went deeply into Kubernetes Pod management through DaemonSets
    and StatefulSet. It manages Pod in a particular way, such as Pod per node and
    consistent Pod names. It is useful when the Deployments/ReplicaSets stateless
    Pod management style can''t cover your application use cases. For further information,
    consider the following:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们深入讨论了通过 DaemonSets 和 StatefulSet 进行 Kubernetes Pod 管理。它以特定方式管理 Pod，如每个节点一个
    Pod 和一致的 Pod 名称。当 Deployments/ReplicaSets 的无状态 Pod 管理方式无法覆盖你的应用场景时，这种方式非常有用。有关更多信息，请参阅以下内容：
- en: The *Working with Pods *recipe in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml),
    *Walking through Kubernetes Concepts*
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第二章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)中 *与 Pods 一起工作* 的教程，*深入了解
    Kubernetes 概念*
- en: Working with configuration files
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配置文件
- en: Submitting Jobs on Kubernetes
  id: totrans-390
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上提交作业
- en: Your container application is designed not only for daemon processes such as
    nginx, but also for some batch Jobs which eventually exit when the task is complete.
    Kubernetes supports this scenario; you can submit a container as a Job and Kubernetes
    will dispatch to an appropriate node and execute your Job.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 你的容器应用不仅设计用于守护进程（如 nginx），还设计用于一些任务完成后会退出的批处理作业。Kubernetes 支持这种场景；你可以将容器提交为一个作业，Kubernetes
    会将其调度到合适的节点并执行作业。
- en: 'In this recipe, we will discuss two new features: **Jobs** and **CronJob**.
    These two features can make another usage of Pods to utilize your resources.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将讨论两个新特性：**Jobs** 和 **CronJob**。这两个特性可以让 Pod 有更多的用途，充分利用你的资源。
- en: Getting ready
  id: totrans-393
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Since Kubernetes version 1.2, Kubernetes Jobs has been introduced as a stable
    feature (`apiVersion: batch/v1`). In addition, CronJob is a beta feature (`apiVersion:
    batch/v1beta1`) as of Kubernetes version 1.10.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '从 Kubernetes 版本 1.2 开始，Kubernetes 作业作为一个稳定功能引入（`apiVersion: batch/v1`）。此外，从
    Kubernetes 版本 1.10 起，CronJob 是一个 Beta 特性（`apiVersion: batch/v1beta1`）。'
- en: Both work well on **minikube,** which was introduced at [Chapter 1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml),
    *Building Your Own Kubernetes Cluster. T*herefore, this recipe will use minikube
    version 0.24.1.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 两者在 **minikube** 上都能良好运行，minikube 在[第一章](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)中有介绍，*构建你自己的
    Kubernetes 集群*。因此，本教程将使用 minikube 版本 0.24.1。
- en: How to do it...
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'When submitting a Job to Kubernetes, you have three types of Job that you can
    define:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 提交作业到 Kubernetes 时，你可以定义三种类型的作业：
- en: Single Job
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一作业
- en: Repeat Job
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复作业
- en: Parallel Job
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行作业
- en: Pod as a single Job
  id: totrans-401
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod 作为单一作业
- en: 'A Job-like Pod is suitable for testing your containers, which can be used for
    unit test or integration test; alternatively, it can be used for batch programs:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 类似作业的 Pod 适用于测试你的容器，可以用于单元测试或集成测试；或者，也可以用于批处理程序：
- en: 'In the following example, we will write a Job template to check the packages
    installed in image Ubuntu:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将编写一个作业模板，检查 Ubuntu 镜像中安装的包：
- en: '[PRE80]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Note that restart policy for Pods created in a Job should be set to `Never`
    or `OnFailure`, since a Job goes to termination once it is completed successfully.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Job 中创建的 Pod 的重启策略应该设置为 `Never` 或 `OnFailure`，因为一旦作业成功完成，Pod 就会终止。
- en: 'Now, you are ready to create a `job` using your template:'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你已经准备好使用模板创建一个 `job`：
- en: '[PRE81]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'After creating a `job` object, it is possible to verify the status of both
    the Pod and Job:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `job` 对象后，可以验证 Pod 和 Job 的状态：
- en: '[PRE82]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'This result indicates that Job is already done, executed (by `SUCCESSFUL =
    1`) in `26` seconds. In this case, Pod has already disappeared:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该结果表示 Job 已经完成，执行（通过 `SUCCESSFUL = 1`）在 `26` 秒内完成。在这种情况下，Pod 已经消失：
- en: '[PRE83]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'As you can see, the `kubectl` command hints to us that we can use `--show-all` or
    `-a` option to find the completed Pod, as follows:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如你所见，`kubectl` 命令提示我们可以使用 `--show-all` 或 `-a` 选项来查找已完成的 Pod，如下所示：
- en: '[PRE84]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Here you go. So why does the `Completed` Pod object remain? Because you may
    want to see the result after your program has ended. You will find that a Pod is
    booting up for handling this task. This Pod is going to be stopped very soon at
    the end of the process.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么为什么 `Completed` Pod 对象仍然存在？因为你可能想在程序结束后查看结果。你会发现一个 Pod 正在启动来处理此任务。这个 Pod
    在过程结束时将很快停止。
- en: 'Use the subcommand `kubectl logs` to get the result:'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用子命令 `kubectl logs` 获取结果：
- en: '[PRE85]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Please go ahead and check the `job package-check` using the subcommand `kubectl describe`;
    the confirmation for Pod completion and other messages are shown as system information:'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请继续使用子命令 `kubectl describe` 检查 `job package-check`；Pod 完成和其他消息的确认将作为系统信息显示：
- en: '[PRE86]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Later, to remove the `job` you just created, delete it with the name. This
    also removes the completed Pod as well:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，删除你刚刚创建的 `job`，用名称删除它。这也会删除已完成的 Pod：
- en: '[PRE87]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Create a repeatable Job
  id: totrans-421
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个可重复的 Job
- en: 'Users can also decide the number of tasks that should be finished in a single Job.
    It is helpful to solve some random and sampling problems. Let''s try it on the
    same template in the previous example:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 用户还可以决定每个 Job 中应完成的任务数量。这对于解决一些随机问题和采样问题非常有用。让我们尝试在前面示例中的相同模板上：
- en: 'Add the `spec.completions` item to indicate the Pod number:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加 `spec.completions` 项来指示 Pod 的数量：
- en: '[PRE88]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'After creating this Job, check how the Pod looks with the subcommand `kubectl describe`:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建该 Job 后，使用子命令 `kubectl describe` 检查 Pod 的状态：
- en: '[PRE89]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: As you can see, three Pods are created to complete this Job. This is useful
    if you need to run your program repeatedly at particular times. However, as you
    may have noticed from the `Age` column in preceding result, these Pods ran sequentially,
    one by one. This means that the 2nd Job was started after the 1st Job was completed,
    and the 3rd Job was started after the 2nd Job was completed.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，创建了三个 Pods 来完成这个 Job。如果你需要在特定时间重复运行程序，这会很有用。然而，正如你可能从前面的 `Age` 列中看到的，这些
    Pods 是顺序运行的，一个接一个。这意味着第二个 Job 在第一个 Job 完成后才开始，第三个 Job 在第二个 Job 完成后才开始。
- en: Create a parallel Job
  id: totrans-428
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个并行 Job
- en: 'If your batch Job doesn''t have a state or dependency between Jobs, you may
    consider submitting Jobs in parallel. Similar to the `spec.completions` parameter,
    the Job template has a `spec.parallelism` parameter to specify how many Jobs you
    want to run in parallel:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的批处理 Job 之间没有状态或依赖关系，你可以考虑并行提交 Job。类似于 `spec.completions` 参数，Job 模板有一个 `spec.parallelism`
    参数，用来指定你希望并行运行多少个 Job：
- en: '1\. Re-use a repeatable Job but change it to specify `spec.parallelism: 3`
    as follows:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '1\. 重新使用可重复的 Job，但将其更改为指定 `spec.parallelism: 3`，如下所示：'
- en: '[PRE90]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The result is similar to `spec.completions=3`*,* which made `3` Pods to run
    your application:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果类似于 `spec.completions=3`*，* 这使得 `3` 个 Pods 运行了你的应用程序：
- en: '[PRE91]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'However, if you see an `Age` column through the `kubectl describe` command,
    it indicates that `3` Pods ran at the same time:'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，如果通过 `kubectl describe` 命令查看到 `Age` 列，说明有 `3` 个 Pods 是同时运行的：
- en: '[PRE92]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: In this setting, Kubernetes can dispatch to an available node to run your application
    and that easily scale your Jobs. It is useful if you want to run something like
    a worker application to distribute a bunch of Pods to different nodes.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，Kubernetes 可以将应用程序调度到可用的节点上运行，并且轻松地扩展你的 Job。如果你需要运行类似工作程序的应用程序，将一批 Pods
    分配到不同的节点，这会非常有用。
- en: Schedule to run Job using CronJob
  id: totrans-437
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CronJob 调度运行 Job
- en: If you are familiar with **UNIX CronJob **or **Java Quartz** ([http://www.quartz-scheduler.org](http://www.quartz-scheduler.org)),
    Kubernetes CronJob is a very straightforward tool that you can define a particular
    timing to run your Kubernetes Job repeatedly.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉 **UNIX CronJob** 或 **Java Quartz** ([http://www.quartz-scheduler.org](http://www.quartz-scheduler.org))，Kubernetes
    CronJob 是一个非常直观的工具，你可以定义特定的时间来重复运行你的 Kubernetes Job。
- en: 'The scheduling format is very simple; it specifies the following five items:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 调度格式非常简单，它指定了以下五个项：
- en: Minutes (0 – 59)
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分钟（0 – 59）
- en: Hours (0 – 23)
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小时（0 – 23）
- en: Day of Month (1 – 31)
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每月的日期（1 – 31）
- en: Month (1 – 12)
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月份（1 – 12）
- en: 'Day of week (0: Sunday – 6: Saturday)'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一周中的天数（0：星期日 - 6：星期六）
- en: For example, if you want to run your Job only at 9:00am on November 12th, every
    year, to send a birthday greeting to me :-), the schedule format could be `0 9
    12 11 *`.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您希望每年 11 月 12 日的早上 9:00 只运行一次 Job，向我发送生日祝福 :-)，那么计划格式可能是 `0 9 12 11 *`。
- en: You may also use slash (`/`) to specify a step value; a `run every 5 minutes`
    interval for the previous Job example would have the following schedule format: `*/5
    * * * *`.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用斜杠 (`/`) 来指定步长值；例如，前一个 Job 的“每 5 分钟运行一次”间隔将采用以下计划格式：`*/5 * * * *`。
- en: 'In addition, there is an optional parameter, `spec.concurrencyPolicy`, that
    you can specify a behavior if the previous Job is not finished but the next Job schedule
    is approaching, to determine how the next Job runs. You can set either:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个可选参数 `spec.concurrencyPolicy`，您可以指定一个行为，如果前一个 Job 还没有完成，但下一个 Job 的计划时间即将到来，可以决定下一个
    Job 如何运行。您可以设置以下选项：
- en: '**Allow**: Allow execution of the next Job'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**允许**：允许执行下一个 Job'
- en: '**Forbid**: Skip execution of the next Job'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**禁止**：跳过执行下一个 Job'
- en: '**Replace**: Delete the current Job, then execute the next Job'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**替换**：删除当前的 Job，然后执行下一个 Job'
- en: 'If you set as `Allow`, there might be a potential risk of accumulating some
    unfinished Jobs in the Kubernetes cluster. Therefore, during the testing phase,
    you should set either `Forbid` or `Replace` to monitor Job execution and completion:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您设置为 `允许`，可能会存在在 Kubernetes 集群中积累一些未完成的 Jobs 的潜在风险。因此，在测试阶段，您应该设置为 `禁止` 或
    `替换` 来监控 Job 的执行和完成：
- en: '[PRE93]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'After a few moments, the Job  will be triggered by your desired timing—in this
    case, every 5 minutes. You may then see the Job entry through the `kubectl get
    jobs` and `kubectl get pods -a` commands, as follows:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，Job 将根据您预定的时间被触发——在这种情况下是每 5 分钟一次。然后，您可以通过 `kubectl get jobs` 和 `kubectl
    get pods -a` 命令查看 Job 条目，如下所示：
- en: '[PRE94]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: CronJob will keep remaining until you delete; this means that, every 5 minutes,
    CronJob will create a new Job entry and related Pods will also keep getting created.
    This will impact the consumption of Kubernetes resources. Therefore, by default,
    CronJob will keep up to `3` successful Jobs (by `spec.successfulJobsHistoryLimit`)
    and one failed Job (by `spec.failedJobsHistoryLimit`). You can change these parameters
    based on your requirements.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: CronJob 会一直保留，直到您删除它；这意味着每 5 分钟，CronJob 会创建一个新的 Job 条目，并且相关的 Pods 也会不断创建。这会影响
    Kubernetes 资源的消耗。因此，默认情况下，CronJob 会保留最多 `3` 个成功的 Jobs（由 `spec.successfulJobsHistoryLimit`
    控制）和一个失败的 Job（由 `spec.failedJobsHistoryLimit` 控制）。您可以根据需要更改这些参数。
- en: Overall, CronJob supplement allows Jobs to automatically to run in your application
    with the desired timing. You can utilize CronJob to run some report generation
    Jobs, daily or weekly batch Jobs, and so on.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，CronJob 补充了 Jobs，使得 Jobs 可以在您应用程序中按预期的时间自动运行。您可以利用 CronJob 来运行一些报告生成任务、每日或每周批处理任务等。
- en: How it works...
  id: totrans-457
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Although Jobs and CronJob are the special utilities of Pods, the Kubernetes
    system has different management systems between them and Pods.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Jobs 和 CronJob 是 Pods 的特殊工具，但 Kubernetes 系统在它们和 Pods 之间有不同的管理机制。
- en: 'For Job, its selector cannot point to an existing pod. It is a bad idea to
    take a Pod controlled by the deployment/ReplicaSets as a Job. The deployment/ReplicaSets
    have a desired number of Pods running, which is against Job''s ideal situation:
    Pods should be deleted once they finish their tasks. The Pod in the Deployments/ReplicaSets
    won''t reach the state of end.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Job，它的选择器不能指向现有的 Pod。将一个由 deployment/ReplicaSets 控制的 Pod 作为 Job 使用是一个不好的主意。deployment/ReplicaSets
    有一个期望的 Pod 数量在运行，这与 Job 的理想情况相违背：Pod 应该在完成任务后被删除。而 Deployment/ReplicaSets 中的 Pod
    不会进入结束状态。
- en: See also
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'In this recipe, we executed Jobs and CronJob, demonstrating another usage of
    Kubernetes Pod that has a completion state. Even once a Pod is completed, Kubernetes
    can preserve the logs and Pod object so that you can retrieve the result easily.
    For further information, consider:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们执行了 Jobs 和 CronJob，展示了 Kubernetes Pod 的另一种用法，该 Pod 具有完成状态。即使 Pod 完成后，Kubernetes
    也可以保留日志和 Pod 对象，以便您轻松检索结果。欲了解更多信息，请参见：
- en: The *Working with Pods *recipe in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml),
    *Walking through Kubernetes Concepts*
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [第 2 章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)中，*与 Pods 一起工作*，*走进 Kubernetes
    概念*
- en: '*Working with configuration files *'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与配置文件一起使用*'
- en: Working with configuration files
  id: totrans-464
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与配置文件一起使用
- en: Kubernetes supports two different file formats, *YAML* and *JSON*. Each format
    can describe the same function of Kubernetes.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 支持两种不同的文件格式，*YAML* 和 *JSON*。这两种格式可以描述 Kubernetes 的相同功能。
- en: Getting ready
  id: totrans-466
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Before we study how to write a Kubernetes configuration file, learning how to
    write a correct template format is important. We can learn the standard format
    of both YAML and JSON from their official websites.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习如何编写 Kubernetes 配置文件之前，学习如何编写正确的模板格式非常重要。我们可以从 YAML 和 JSON 的官方网站了解它们的标准格式。
- en: YAML
  id: totrans-468
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YAML
- en: 'The YAML format is very simple, with few syntax rules; therefore, it is easy
    to read and write, even for users. To know more about YAML, you can refer to the
    following website link: [http://www.yaml.org/spec/1.2/spec.html](http://www.yaml.org/spec/1.2/spec.html).
    The following example uses the YAML format to set up the `nginx` Pod:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: YAML 格式非常简单，语法规则少，因此即使是用户也很容易阅读和编写。要了解更多关于 YAML 的信息，可以参考以下网站链接：[http://www.yaml.org/spec/1.2/spec.html](http://www.yaml.org/spec/1.2/spec.html)。以下示例使用
    YAML 格式来设置 `nginx` Pod：
- en: '[PRE95]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: JSON
  id: totrans-471
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JSON
- en: 'The JSON format is also simple and easy to read for users, but more program-friendly.
    Because it has data types (number, string, Boolean, and object), it is popular
    to exchange the data between systems. Technically, YAML is a superset of JSON,
    so JSON is a valid YAML, but not the other way around. To know more about JSON,
    you can refer to the following website link: [http://json.org/](http://json.org/).'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 格式同样简单且易于用户阅读，但更适合程序使用。因为它有数据类型（数字、字符串、布尔值和对象），所以在系统之间交换数据时非常流行。从技术角度来说，YAML
    是 JSON 的超集，因此 JSON 是有效的 YAML，但反过来不成立。要了解更多关于 JSON 的信息，可以参考以下网站链接：[http://json.org/](http://json.org/)。
- en: 'The following example of the Pod is the same as the preceding YAML format,
    but using the JSON format:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 以下这个 Pod 示例与之前的 YAML 格式相同，只不过使用了 JSON 格式：
- en: '[PRE96]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: How to do it...
  id: totrans-475
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Kubernetes has a schema that is defined using a verify configuration format;
    schema can be generated after the first instance of running the subcommand `create`
    with a configuration file. The cached schema will be stored under the `.kube/cache/discovery/<SERVICE_IP>_<PORT>`,
    based on the version of API server you run:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 有一个通过验证配置格式定义的模式；在第一次使用配置文件运行子命令 `create` 后，可以生成该模式。缓存的模式将存储在 `.kube/cache/discovery/<SERVICE_IP>_<PORT>`
    下，具体位置取决于你运行的 API 服务器版本：
- en: '[PRE97]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Each directory listed represents an API category. You will see a file named
    `serverresources.json` under the last layer of each directory, which clearly defines
    every resource covered by this API category. However, there are some alternative
    and easier ways to check the schema. From the website of Kubernetes, we can get
    any details of how to write a configuration file of specific resources. Go ahead
    and check the official API documentation of the latest version: [https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/).
    In the webpage, there are three panels: from left to right, they are the resource
    list, description, and the input and output of HTTP requests or the command kubectl.
    Taking Deployment as an example, you may click Deployment v1 app at the resource
    list, the leftmost panel, and the following screenshot will show up:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 每个列出的目录代表一个 API 类别。在每个目录的最底层，你会看到一个名为 `serverresources.json` 的文件，它明确定义了该 API
    类别所涵盖的每个资源。然而，也有一些替代且更简便的方式来查看模式。在 Kubernetes 的官方网站上，我们可以获取有关如何编写特定资源配置文件的详细信息。请继续查看最新版本的官方
    API 文档：[https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/)。该网页包含三个面板：从左到右分别是资源列表、描述、以及
    HTTP 请求的输入和输出，或者是 kubectl 命令。以 Deployment 为例，你可以点击资源列表中的 Deployment v1 app，最左边的面板就会显示以下截图：
- en: '![](img/cc7417bc-6efa-409f-bd4f-24b297969fd7.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc7417bc-6efa-409f-bd4f-24b297969fd7.png)'
- en: Documentation of Kubernetes Deployment API
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 部署 API 文档
- en: 'But, how do we know the details of setting the container part at the marked
    place on the preceding image? In the field part of object description, there are
    two values. The first one, like apiVersion, means the name, and the second one,
    like string, is the type. Type could be integer, string, array, or the other resource
    object. Therefore, for searching the containers configuration of deployment, we
    need to know the structure of layers of objects. First, according to the example
    configuration file on web page, the layer of objects to containers is `spec.template.spec.containers.`
    So, start by clicking the hyperlink spec DeploymentSpec under Deployment''s fields,
    which is the type of resource object, and go searching hierarchically. Finally,
    you can find the details listed on this page: [https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core).'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们怎么知道前面图示中标记的容器部分的设置细节呢？在对象描述的字段部分，有两个值。第一个值，像`apiVersion`，表示名称，第二个值，像`string`，表示类型。类型可以是整数、字符串、数组或其他资源对象。因此，为了搜索deployment的容器配置，我们需要了解对象层次结构的结构。首先，根据网页上的示例配置文件，容器的对象层次是`spec.template.spec.containers.`。因此，从点击Deployment字段下的超链接`spec
    DeploymentSpec`开始，这个字段是资源对象的类型，然后逐层搜索。最终，你可以在此页面找到详细信息：[https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core)。
- en: 'Solution for tracing the configuration of containers of DeploymentHere comes
    the solution for the preceding example:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 用于追踪Deployment配置中容器的解决方案：以下是前面示例的解决方案：
- en: Click spec DeploymentSpec
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击spec DeploymentSpec
- en: Click template PodTemplateSpec
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击模板 PodTemplateSpec
- en: Click spec PodSpec
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击spec PodSpec
- en: Click containers Container array
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击容器 容器数组
- en: Now you got it!
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你明白了！
- en: 'Taking a careful look at the definition of container configuration. The following
    are some common descriptions you should pay attention to:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细查看容器配置的定义。以下是一些常见的描述，你应该注意：
- en: '**Type**: The user should always set the corresponding type for an item.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类型**：用户应始终为每个项目设置相应的类型。'
- en: '**Optional or not**: Some items are indicated as optional, which means not
    necessary, and can be applied as a default value, or not set if you don''t specify
    it.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可选与否**：某些项目标记为可选，意味着它不是必需的，可以作为默认值应用，如果没有指定，则可以不设置。'
- en: '**Cannot be updated**: If the item is indicated as failed to be updated, it
    is fixed when the resource is created. You need to recreate a new one instead
    of updating it.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不能更新**：如果某项标记为无法更新，则在资源创建时已固定。你需要重新创建一个新的，而不是更新它。'
- en: '**Read-only**: Some of the items are indicated as `read-only`, such as UID.
    Kubernetes generates these items. If you specify this in the configuration file,
    it will be ignored.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**只读**：一些项目标记为`只读`，例如UID。这些项由Kubernetes生成。如果你在配置文件中指定了这些项，它们将被忽略。'
- en: 'Another method for checking the schema is through swagger UI. Kubernetes uses
    swagger ([https://](https://swagger.io)[swagger.io/](https://swagger.io)) and
    OpenAPI ([https://www.openapis.org](https://www.openapis.org)) to generate the
    REST API. Nevertheless, the web console for swagger is by default disabled in
    the API server. To enable the swagger UI of your own Kubernetes API server, just
    add the flag `--enable-swagger-ui=ture` when you start the API server. Then, by
    accessing the endpoint `https://<KUBERNETES_MASTER>:<API_SERVER_PORT>/swagger-ui`,
    you can successfully browse the API document through the web console:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种检查架构的方法是通过Swagger UI。Kubernetes使用Swagger（[https://](https://swagger.io)[swagger.io/](https://swagger.io))和OpenAPI（[https://www.openapis.org](https://www.openapis.org)）生成REST
    API。然而，默认情况下，Swagger的Web控制台在API服务器中是禁用的。要启用你自己的Kubernetes API服务器的Swagger UI，只需在启动API服务器时添加`--enable-swagger-ui=true`标志。然后，通过访问`https://<KUBERNETES_MASTER>:<API_SERVER_PORT>/swagger-ui`，你可以成功地通过Web控制台浏览API文档：
- en: '![](img/6a34175a-0a30-4e6a-a20b-70392e605d69.png)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a34175a-0a30-4e6a-a20b-70392e605d69.png)'
- en: The swagger web console of Kubernetes API
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes API的Swagger Web控制台
- en: How it works...
  id: totrans-496
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Let's introduce some necessary items in configuration files for creating Pod,
    Deployment, and Service.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们介绍一些在配置文件中创建Pod、Deployment和Service所必需的项目。
- en: Pod
  id: totrans-498
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod
- en: '| **Item** | **Type** | **Example** |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| **项目** | **类型** | **示例** |'
- en: '| `apiVersion` | String | `v1` |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| `apiVersion` | 字符串 | `v1` |'
- en: '| `kind` | String | `Pod` |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| `kind` | 字符串 | `Pod` |'
- en: '| `metadata.name` | String | `my-nginx-pod` |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| `metadata.name` | 字符串 | `my-nginx-pod` |'
- en: '| `spec` | `v1.PodSpec` |  |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| `spec` | `v1.PodSpec` |  |'
- en: '| `v1.PodSpec.containers` | Array[`v1.Container`] |  |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| `v1.PodSpec.containers` | 数组[`v1.Container`] |  |'
- en: '| `v1.Container.name` | String | `my-nginx` |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| `v1.Container.name` | 字符串 | `my-nginx` |'
- en: '| `v1.Container.image` | String | `nginx` |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| `v1.Container.image` | 字符串 | `nginx` |'
- en: Deployment
  id: totrans-507
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: '| **Item** | **Type** | **Example** |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '**项** | **类型** | **示例** |'
- en: '| `apiVersion` | String | `apps`/`v1beta1` |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| `apiVersion` | 字符串 | `apps`/`v1beta1` |'
- en: '| `kind` | String | `Deployment` |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| `kind` | 字符串 | `Deployment` |'
- en: '| `metadata.name` | String | `my-nginx-deploy` |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| `metadata.name` | 字符串 | `my-nginx-deploy` |'
- en: '| `spec` | `v1.DeploymentSpec` |  |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| `spec` | `v1.DeploymentSpec` |  |'
- en: '| `v1.DeploymentSpec.template` | `v1.PodTemplateSpec` |  |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| `v1.DeploymentSpec.template` | `v1.PodTemplateSpec` |  |'
- en: '| `v1.PodTemplateSpec.metadata.labels` | Map of string | `env: test` |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| `v1.PodTemplateSpec.metadata.labels` | 字符串映射 | `env: test` |'
- en: '| `v1.PodTemplateSpec.spec` | `v1.PodSpec` | `my-nginx` |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| `v1.PodTemplateSpec.spec` | `v1.PodSpec` | `my-nginx` |'
- en: '| `v1.PodSpec.containers` | Array[`v1.Container`] | As same as Pod |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| `v1.PodSpec.containers` | 数组[`v1.Container`] | 与Pod相同 |'
- en: Service
  id: totrans-517
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务
- en: '| **Item** | **Type** | **Example** |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| **项** | **类型** | **示例** |'
- en: '| `apiVersion` | String | `v1` |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| `apiVersion` | 字符串 | `v1` |'
- en: '| `kind` | String | `Service` |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| `kind` | 字符串 | `Service` |'
- en: '| `metadata.name` | String | `my-nginx-svc` |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| `metadata.name` | 字符串 | `my-nginx-svc` |'
- en: '| `spec` | `v1.ServiceSpec` |  |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| `spec` | `v1.ServiceSpec` |  |'
- en: '| `v1.ServiceSpec.selector` | Map of string | `env: test` |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| `v1.ServiceSpec.selector` | 字符串映射 | `env: test` |'
- en: '| `v1.ServiceSpec.ports` | Array[`v1.ServicePort`] |  |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| `v1.ServiceSpec.ports` | 数组[`v1.ServicePort`] |  |'
- en: '| `v1.ServicePort.protocol` | String | `TCP` |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| `v1.ServicePort.protocol` | 字符串 | `TCP` |'
- en: '| `v1.ServicePort.port` | Integer | `80` |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| `v1.ServicePort.port` | 整数 | `80` |'
- en: Please check the code bundle file `minimal-conf-resource.yaml` to find these
    three resources with minimal configuration.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 请检查代码包文件`minimal-conf-resource.yaml`，以找到这些具有最小配置的三个资源。
- en: See also
  id: totrans-528
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'This recipe described how to find and understand a configuration syntax. Kubernetes
    has some detailed options to define containers and components. For more details,
    the following recipes will describe how to define Pods, Deployments, and Services:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方描述了如何查找和理解配置语法。Kubernetes有一些详细的选项来定义容器和组件。更多细节将在以下配方中描述如何定义Pod、Deployment和Service：
- en: The *Working with Pods*, *Deployment API*, and *Working with Services* recipes
    in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml), *Walking through Kubernetes
    Concepts*
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第2章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)的*与Pods一起工作*、*Deployment API*和*与Services一起工作*配方中，*走进Kubernetes概念*
