- en: '19'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing on EKS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the book, we’ve looked at how to build EKS clusters and deploy workloads.
    In this chapter, we will look at some ways you can make these activities more
    efficient if you’re a developer or DevOps engineer using automation and CI/CD.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the tools and techniques you can use to deploy
    and test clusters and workloads natively on AWS, or by using third-party tools.
    We will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Different IT personas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Cloud9 as your integrated development environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building clusters with EKS Blueprints and Terraform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using CodePipeline and CodeBuild to build clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Argo CD, Crossplane, and GitOps to deploy workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should have a familiarity with YAML, AWS IAM, and EKS architecture. Before
    getting started with this chapter, please ensure the following:'
  prefs: []
  type: TYPE_NORMAL
- en: You have network connectivity to your EKS cluster API endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AWS CLI, Docker, and `kubectl` binaries are installed on your workstation
    and have administrator access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different IT personas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we look at the technology that supports development, it’s important
    to consider who in your organization or team will deploy your EKS clusters or
    applications/workloads. The following diagram illustrates IT functional groups
    you might find in a typical enterprise; this is often referred to as the cloud
    operating model and consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Application engineers that build applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application operations that operate and support applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform engineers that build middleware, networks, databases, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform operations that operate and support infrastructure and middleware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 19.1 – Cloud operating model functional architecture](img/Figure_19.01_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.1 – Cloud operating model functional architecture
  prefs: []
  type: TYPE_NORMAL
- en: Many organizations now use a DevOps model, where they combine **Application
    Engineering** and **Application Operations** in the developer team, using the
    mantra “*you build it, you run it.*” This can also include platform engineering
    but often traditional IT operational teams for network and databases exist, and
    they must work with the application teams.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, platform engineering teams have also started appearing to being
    engineering and supporting parts of the infrastructure used by developers/DevOps
    engineers, such as EKS, databases, messaging, and APIs. This team’s mantra is
    “*you code and test and we’ll do all* *the rest.*”
  prefs: []
  type: TYPE_NORMAL
- en: Which specific model is used and which teams do what is really down to your
    organization. However, for the rest of this section, we will use the term *DevOps
    engineers* to refer to roles that are responsible for application engineering/operations,
    and *platform engineers* for roles that are responsible for the EKS cluster and
    support infrastructure, such as databases or networking.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore how we can interact with the AWS environment to build, deploy,
    and test platform and application services.
  prefs: []
  type: TYPE_NORMAL
- en: Using Cloud9 as your integrated development environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cloud9 is a simple **integrated development environment** (**IDE**) that runs
    on top of EC2 and is similar in nature to other IDEs, such as Microsoft’s Visual
    Studio Code, Eclipse, or PyCharm. It can be used by platform engineers or developers
    alike. While Cloud9 isn’t as extensible as those IDEs, it does have several advantages,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It runs on EC2 inside your account/s, which will allow you to communicate with
    private resources, such as private EKS clusters, without network access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use AWS Systems Manager Session Manager to connect to your instance,
    which only needs IAM permissions and access to the AWS console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As it’s an EC2 instance, you can assign a role to your instance with the permissions
    required, and these credentials are automatically refreshed and don’t expire (which
    is useful when you’re provisioning clusters, as this can take some time)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides an integrated AWS toolkit to simplify your interaction with resources
    such as S3 and Lambda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can run Docker containers on your instance and preview HTTP if your container
    or code uses a localhost address on port `8080`, `8081`, or `8082`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most recently, it has been integrated with Amazon CodeWhisperer, which uses
    machine learning and can generate code for languages such as Python and Java
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’ve used Cloud9 extensively throughout this book’s development, as it is simple
    and secure to use, but you can, of course, use any IDE. In the rest of this section,
    we will look at how you can set up and use Cloud9 to develop for EKS.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and configuring your Cloud9 instance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the following Terraform code to create a Cloud9 instance, allow
    the user defined within the `myuser_arn` local to use it, and connect it to the
    subnet defined in `subnet_id`. As we have defined the connection type as `CONNECT_SSM`,
    this subnet can be private as long as it has connectivity to the AWS SSM API,
    either through a private endpoint or a NAT gateway:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please note that can you modify `instance_type` to whatever you are comfortable
    paying for because, although there is no charge for Cloud9, there is a charge
    for the EC2 instance hosting it.
  prefs: []
  type: TYPE_NORMAL
- en: Once the Terraform code has completed, you can use the AWS console, browse to
    the **Cloud9** | **Environments** tab, and use the **Open** link to start up your
    EC2 instance and launch the IDE in your browser, using an SSM session. This is
    illustrated in the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.2 – Launching a Cloud9 SSM session](img/Figure_19.02_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.2 – Launching a Cloud9 SSM session
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Cloud9 will use AWS managed temporary credentials, which have limited
    permissions and can be found at the following link: [https://docs.aws.amazon.com/cloud9/latest/user-guide/security-iam.html#auth-and-access-control-temporary-managed-credentials-supported](https://docs.aws.amazon.com/cloud9/latest/user-guide/security-iam.html#auth-and-access-control-temporary-managed-credentials-supported).
    They won’t allow you to fully interact with the AWS platform. We will create a
    role with `AdministratorAccess`, turn off managed temporary credentials in our
    Cloud9 instance, and then associate this new role with the EC2 instance hosting
    the Cloud9 IDE.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The role description is shown next, and it has an explicit trust with the `ec2.amazonaws.com`
    service. You can follow the process described in the following link to configure
    your Cloud9 instance: [https://catalog.us-east-1.prod.workshops.aws/workshops/c15012ac-d05d-46b1-8a4a-205e7c9d93c9/en-US/15-aws-event/cloud9](https://catalog.us-east-1.prod.workshops.aws/workshops/c15012ac-d05d-46b1-8a4a-205e7c9d93c9/en-US/15-aws-event/cloud9).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.3 – An example IAM role](img/Figure_19.03_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.3 – An example IAM role
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We only add the `AdministratorAccess` policy for simplicity. Ideally, you would
    tailor the Cloud9 permissions to support the least amount of privilege needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify the role is attached using the following command in a Cloud9
    terminal session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need to install the necessary tools; as Cloud9 comes with the AWS CLI,
    Python, and Docker, we still need to install `kubectl` and so on. You can install
    these components manually, but AWS provides a handy script as part of its Cloud9
    workshop (in the URL shown previously), so we will use this to install the necessary
    tools, including `kubectl` and AWS **Cloud Development Kit** (**CDK**). The commands
    are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also install/upgrade `terraform`, as we will use this later on in this
    section, using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also add `eksctl`, which was used in the earlier sections of the book,
    using these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will just set the default region so that all tools that use the
    SDK will use the region we specify:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have our Cloud9 instance configured. We will now use it to deploy clusters
    using EKS Blueprints.
  prefs: []
  type: TYPE_NORMAL
- en: Building clusters with EKS Blueprints and Terraform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we mainly used `eksctl` to build our clusters and leverage add-ons
    to support simpler upgrades of standard components, such as the VPC CNI plugin
    or kube-proxy. We also deployed additional software such as Prometheus and KEDA
    ([*Chapter 18*](B18129_18.xhtml#_idTextAnchor264)). EKS blueprints provides you
    with a way to build an opinionated cluster, with this operational software already
    deployed. This simplifies the job of the platform of DevOps engineers, and they
    can use blueprints to repeatedly build clusters for different environments and/or
    teams with very little effort.
  prefs: []
  type: TYPE_NORMAL
- en: EKS Blueprint Clusters are built using AWS CDK, which is a set of libraries
    and constructs that allow you to create and deploy complex CloudFormation templates,
    using standard programming languages such as TypeScript or Python. Recently, AWS
    has released EKS Blueprints for Terraform, and this is what we will use in the
    rest of the section to create a cluster that can be used by our developers to
    deploy their applications.
  prefs: []
  type: TYPE_NORMAL
- en: You can follow a phased approach to developing your cluster configuration. The
    following diagram shows the suggested approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.4 – The EKS blueprints development life cycle](img/Figure_19.04_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.4 – The EKS blueprints development life cycle
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will step through each phase of the development life
    cycle as we download, version, and customize our blueprint code.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing and versioning EKS Blueprints for Terraform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing we’re going to do is use our Cloud9 instance to create a Git-compliant
    repository in `CodeCommit` to store our version of the Terraform code. The following
    commands can be used to create the repository, clone it, and create a new branch
    for our work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In our `cluster-tf` directory, we will create a new `.gitignore` file based
    on the template found at [https://github.com/github/gitignore/blob/main/Terraform.gitignore](https://github.com/github/gitignore/blob/main/Terraform.gitignore)
    (you can create your own).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the base variables and providers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To use the Terraform blueprint modules, we need to configure key Terraform
    resources such as the providers and data sources to use. Let’s start with the
    providers, which are the base “engines” of Terraform, and translate the Terraform
    resources into actual deployed objects in AWS or K8s. The following configuration
    is saved in the `providers.tf` file in our cloned repository directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also create a `data.tf` file that will get the current AWS credentials,
    Region, and Availability Zones for that Region:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also create a `local.tf` file that maintains the base configuration,
    including the cluster name and version. The cluster name is derived from the repository
    path, but for production usage, you will want to use the `locals int` variables
    and then populate them at build time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now run the following commands to initialize Terraform with the providers
    and push the initial code to our **CodeCommit** repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the providers and the base configuration stored, we can use
    it to create a VPC and tag it for use with EKS.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the EKS VPC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Your cluster needs an existing VPC for the EKS cluster. We will create a new
    one with the code shown next, but you can modify the code shown in the *Creating
    the EKS cluster* section to use a pre-existing one and skip this step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also create an `outputs.tf` file that stores the newly created VPC’s
    ID, which can be used when we create the EKS cluster using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now validate that the code is correct, create the VPC, and save the
    final code by using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Terraform stores its state in a state file, `terraform.tfstate`. At present,
    this will be stored locally in the repository directory and ignored by Git (because
    of the `.gitignore` file). We will discuss strategies for managing this file later
    on in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a new VPC, we will use EKS Blueprints to configure and deploy
    an EKS cluster referencing the new VPC.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the EKS cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now create an EKS cluster using the Blueprint module; we will use 4.31.0,
    which is the latest at the time of writing. An example configuration, `main.tf`,
    is shown in the following snippet. This will create the cluster in the VPC we
    created previously with just the standard K8s services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In the `eks blueprints` module shown previously, we use the `ref` keyword to
    indicate which version of the blueprint module we will call; this may change depending
    on the blueprints’ release schedule.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to configure some additional data sources for our cluster deployment
    to work, including one in another region, `eu-east-1`. A sample configuration
    created in the `eks-data.tf` file is shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to configure an additional output in the `eks-output.tf` file,
    shown next, so that we can interact manually with the cluster using our Cloud9
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have all the configurations in place, we can validate whether the code
    is correct, create the EKS cluster, and save the code by using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please note that it can take up to 15 minutes to deploy your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a working cluster, we can allow access to different users,
    roles, or teams.
  prefs: []
  type: TYPE_NORMAL
- en: Adding users/teams to your cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At present, only the role/identity associated with the credentials you’ve used
    to run the terraform will have access to your clusters. So, we will add a new
    administrator to the cluster and then add a tenant.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `main.tf` file, you can add a `map roles` section, which will add a
    single role as an administrator for the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You will need to replace the role/admin with an appropriate role in your account.
    Remember that if the IAM credentials used by Terraform are not included in the
    configuration after its application, Terraform may lose access to the cluster
    to perform K8s API actions, such as modifying the `aws-auth` config map.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the tenants/users, we will create a new `locals` file, `locals-team.tf`,
    but again, you will probably want to use a variable. An example is shown next
    for two teams, a platform team and an application team:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You will need to use valid user account ARNs or `[data.aws_caller_identity.current.arn]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need to modify our `main.tf` file for the cluster and add the following
    code snippet, `platform_teams`, to provide cluster admin access to a list of platform
    team users. This will create a new IAM role with cluster access and allow the
    list of users assigned to the platform teams to assume that role and get admin
    access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'And in the `main.tf` file, we can add also add a tenant DevOps or application
    team with limits, which will also create a namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now do the regular Terraform `plan` and `apply` commands to deploy these
    changes, grant access to the ARNs listed in the local file, and use our standard
    Git commands to commit the changes to our repository. Examples of the main commands
    are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to see the `kubectl` commands that each team needs to configure,
    you can add the following configuration to the `outputs.tf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve set up our cluster with the right access for both the platform
    engineering teams, as well as the application development teams, we can deploy
    a number of add-ons.
  prefs: []
  type: TYPE_NORMAL
- en: Adding blueprints to your cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we saw in previous chapters, deploying tools such as the AWS Load Balancer
    Controller or Karpenter can take quite a bit of work. Blueprints extend the EKs
    add-on concepts to other tools and can leverage the EKS add-on or ArgoCD to deploy
    this software. The currently supported add-ons can be found at https://aws-ia.github.io/terraform-aws-eks-blueprints/add-ons.
  prefs: []
  type: TYPE_NORMAL
- en: We will deploy ArgoCD, which is a GitOps deployment tool (discussed in more
    detail in the *Using ArgoCD, Crossplane, and GitOps to deploy workloads* section),
    and then ArgoCD will deploy (most of the) other add-ons.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we will do is create a `locals-blueprints.tf` file in our repository,
    with the contents shown next. This will tell ArgoCD where to look for the different
    helm charts to deploy the add-ons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to deploy `argodCD` and tell it which add-ons to deploy. Note
    that the blueprint add-on module is opinionated, so some of the add-ons, such
    as the AWS CSI driver, will be deployed directly as EKS add-ons (and will appear
    as add-ons), whereas others will be handled by `argoCD`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will deploy `argoCD` and the AWS EBS CSI driver directly, and then Argo
    CD (`argoCD`) will deploy the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The AWS Load Balancer Controller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fluent Bit for logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metrics server for standard metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karpenter for autoscaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crossplane (discussed later) for infrastructure as code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code snippet will be used as `blueprints.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following commands can be used to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy the Terraform updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Validate that the add-ons in EKS have all been deployed successfully:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now get the ArgoCD details and access them to review the details of
    the other add-ons, by using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now browse to the ArgoCD URL and log in with the details shown previously
    to see the status of the other add-ons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.5 – The argoCD applications/add-ons status](img/Figure_19.05_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.5 – The argoCD applications/add-ons status
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please make sure you have pushed all changes to the `CodeCommit` repository
    and have also destroyed the cluster using the `terraform destroy` command. It
    can take some time to destroy the VPC and the networking components.
  prefs: []
  type: TYPE_NORMAL
- en: Modification and upgrades follow a similar pattern; the Terraform code is modified,
    and then the Terraform `plan` and `apply` commands are used to upgrade or reconfigure
    the cluster, node groups, access permission, and blueprints.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have created (and destroyed) our resources manually using Terraform,
    let’s look at how we can use AWS CI/CD tools to automate the testing and deployment
    of your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Using CodePipeline and CodeBuild to build clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have done the deployment manually so far by running the Terraform `plan`
    and `apply` commands. CodeBuild is an AWS service that acts as a CI build server
    but also deploys our Terraform configuration. CodePipeline automates the end-to-end
    release pipeline and sequences build, test, and deploy phases, based on commits
    to a repository such as CodeCommit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is adjust our Terraform code to support the storage
    of state in an S3 bucket. This is necessary, as by default, Terraform will use
    local storage for its state, and as CodeBuild is a transient environment, that
    state will be lost between builds. Terraform relies on a state file to determine
    what needs adding, changing, or removing. We will simply add the backend configuration
    code shown next to the `providers.tf` file we created previously. We don’t need
    to specify any details, as this will be configured dynamically during the Terraform
    `init` phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve modified the code, commit the changes to your repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing we need to do is add a `buildspec.yml` file to the `root` directory
    of our repository. This file is used by CodeBuild to run the build/deploy commands.
    The `buildspec` file is a specific format and consists of a number of phases:'
  prefs: []
  type: TYPE_NORMAL
- en: In the *install* phase, install the latest version of **Terraform** and **jq**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the *pre-build* phase, run `terraform init` and configure it to use an S3
    bucket and prefix in a specific region, using environment variables, and also
    run the Terraform `validate` command as a basic test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the *build* phase, a Terraform `plan`, `apply`, or `destroy` command can
    be run based on the action specified in an environment variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example of the `buildspec.yml` file is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Once you’ve modified the code, commit the changes to your repository.
  prefs: []
  type: TYPE_NORMAL
- en: As we now have the Terraform backend configured and a `buildspec` file that
    CodeBuild will use to build/deploy the resources, we need to create and configure
    the build project.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the CodeBuild project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the AWS console, navigate to the AWS CodeBuild Service and add a new
    build project. Fill in the **Project name** and **Description** fields, as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.6 – CodeBuild project configuration](img/Figure_19.06_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.6 – CodeBuild project configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `CodeCommit` repository and branch where the Terraform code and `buildspec`
    files are located, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.7 – CodeBuild source configuration](img/Figure_19.07_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.7 – CodeBuild source configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first part of the **Environment** panel, define the build environment
    as a standard Linux environment, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.8 – CodeBuild environment configuration](img/Figure_19.08_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.8 – CodeBuild environment configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Leave the service role to create a new service role and the **Buildspec** panel
    as is (shown next). Then, click on **Create Build Project** at the bottom of the
    screen (not shown):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.9 – CodeBuild buildspec configuration](img/Figure_19.09_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.9 – CodeBuild buildspec configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to configure the following environment variables, which are used
    by the `buildspec` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TFSTATE_BUCKET** points to an existing S3 bucket name'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TF_ACTION** will perform and **apply -auto-approve**, but this could be changed
    to a **destroy** or **plan** action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster/cluster-tf/terraform.tfstate` value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TFSTATE_REGION** points to the correct region'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 19.10 – CodeBuild environment variables](img/Figure_19.10_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.10 – CodeBuild environment variables
  prefs: []
  type: TYPE_NORMAL
- en: Once you have configured the environment variables, click on **Create** **Build
    Project**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The build project service role created for the project will need to have the
    relevant IAM permissions added to it to allow the Terraform code to create the
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should also add the service role used explicitly by the code build to the
    `map_role` section in our `main.tf` code, as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Replace the `service-role` name with the one your CodeBuild project uses, and
    commit your changes to the repository.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have built a project pointing to our repository and branch with a specific
    `buildspec.yml` file, which provides the commands we need to deploy the Terraform
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.11 – The CodeBuild Start build dropdown](img/Figure_19.11_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.11 – The CodeBuild Start build dropdown
  prefs: []
  type: TYPE_NORMAL
- en: Once the build starts, you can look at the logs and see any errors, but it will
    eventually be complete, and then you can review the build history. If you look
    at the example shown next, you can see it took just over 30 minutes to deploy
    Terraform, and it completed successfully.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.12 – The CodeBuild Build history screen](img/Figure_19.12_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.12 – The CodeBuild Build history screen
  prefs: []
  type: TYPE_NORMAL
- en: If we look in the S3 bucket, we can see the prefix we defined in the `TFSTATE_KEY`
    environment variable and the `terraform.tfstate` file.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.13 – The S3 Terraform state file](img/Figure_19.13_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.13 – The S3 Terraform state file
  prefs: []
  type: TYPE_NORMAL
- en: In order to trigger the build job, we need to either click on the **Start build**
    button or use the CodeBuild API. Next, we will look at how we can use CodePipeline
    to trigger the build on a code change.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You should delete the Terraform-created resource before progressing, either
    manually or by changing the build job `TF_ACTION` to `destroy -auto-approve` and
    rerunning the build job.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up CodePipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we set up CodePipeline, we will configure two stages – a *source* stage
    that references our CodeCommit repository with the Terraform and `buildspec` files,
    and a *build* stage that references our CodeBuild project. An example is shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.14 – The CodePipeline stages](img/Figure_19.14_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.14 – The CodePipeline stages
  prefs: []
  type: TYPE_NORMAL
- en: 'We will configure our source stage with the CodeCommit repository and the branch
    details we will use for our code, and we will leave the default change detection
    and output artifacts. This means that when we make a change (commit), we will
    trigger a CloudWatch event that will be used in the next stage. An example of
    the CodeCommit configuration is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.15 – A CodePipeline source stage configuration snippet](img/Figure_19.15_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.15 – A CodePipeline source stage configuration snippet
  prefs: []
  type: TYPE_NORMAL
- en: 'We then need to configure our build stage to point to our existing CodeBuild
    project in the correct region. An example of the CodeBuild configuration is shown
    next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.16 – A CodePipeline build stage configuration snippet](img/Figure_19.16_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.16 – A CodePipeline build stage configuration snippet
  prefs: []
  type: TYPE_NORMAL
- en: 'Now when we make a commit, CodePipeline detects it and triggers CodeBuild to
    run the build project we created previously. We can see in the example next that
    the commit ID and message are shown as the trigger:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.17 – A successful pipeline run](img/Figure_19.17_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.17 – A successful pipeline run
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: As the source code is now generated by CodePipeline and in the Terraform code,
    we will use the filepath of the repository and see a cluster built with the name
    of `src` (which is the name of the directory generated by CodePipeline). We should
    change the way Terraform generates the cluster name using a variable or local.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have had a quick review of using CodePipeline and CodeBuild to build
    our cluster based on changes to our code, let’s see how we can use ArgoCD and
    Crossplane to deploy EKS workloads in a similar manner.
  prefs: []
  type: TYPE_NORMAL
- en: Using ArgoCD, Crossplane, and GitOps to deploy workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we used CodePipline to deploy changes to our AWS environment
    based on a commit and a CodePipline configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '**GitOps** is a way of implementing **continuous deployment** (**CD**) to deploy
    both a containerized application and infrastructure but with a focus on self-service
    and developer experience. This means that the developer can use the Git repository
    to not only store, version, test, and build their code but also do the same for
    their infrastructure as code, deploying both things together.'
  prefs: []
  type: TYPE_NORMAL
- en: We will use two open source projects in this chapter – **ArgoCD**, which is
    a deployment tool that will continually poll our application repository to look
    for changes, and the **K8s API** to deploy them. Crossplane allows us to use a
    custom Kubernetes resource to build infrastructure resources that support our
    application like a database. ArgoCD can use Helm to deploy and modify (patch)
    K8s resources or Kustomize. **Kustomize** allows you to easily customize K8s manifest
    files and can also be used directly by the **kubectl** tool. The architecture
    used is shown next.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.18 – GitOps architecture](img/Figure_19.18_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.18 – GitOps architecture
  prefs: []
  type: TYPE_NORMAL
- en: We will use the cluster we created using the Terraform BluePrint, which already
    has ArgoCD installed and running, so we will start with the ArgoCD repository
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up our application repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will create and clone a new CodeCommit repository called `myapp`, using the
    same commands shown in the *Customizing and versioning EKS Blueprints for Terraform*
    section, to create and clone the repository into our Cloud9 instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also should install Kustomize locally in our environment for local testing,
    using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the repository and the **Kustomize** tool installed, we can
    set up the general structure. We will use the pause container image and adjust
    the namespace, replica count, and memory request size based on the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use two manifest files, which, once created, should only be changed
    when resources are added or deleted. The `namespace.yaml` file will define the
    namespace; an example is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The `deployment.yaml` file will define the deployment for the pause container.
    An example is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now create the `base` directory and `kustomise.yaml` file that will
    reference the preceding templates, and also do a dry run of the deployment using
    the `kubectl create -k` command. These commands are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve discussed the namespace and deployment file, but the `kustomize.yaml`
    file is also used by Kustomize to understand what resources it needs to deploy
    or modify (patch). An example is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As this file is created in the base directory, it simply references the two
    manifest files with no amendments. We will now create two overlays that adjust
    the values of these files for the non-production and production environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `kustomize.yaml` file for non-production is shown next and will adjust
    the namespace and `non-production-` prefix to all the resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'It also references a `deployment.yaml` file in the local directory, which is
    shown next, increases the replica count in the base template to `1`, and also
    adds new limits and requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the `kubectl create -k` command, these changes will be merged with
    the base manifests and deployed. The following commands will deploy and verify
    our customizations for non-production:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We can now replicate the configuration into the `./overlays/production` directory,
    changing the prefix and namespace to `production`, the limits and request to `2Gi`,
    and the number of replicas to `3`. We can now commit these changes to our repository,
    and we know that if we run the `Kustomize` command from either the production
    or non-production `overlays` directories, we will get slightly different configurations
    for each environment.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to configure ArgoCD to deploy these resources.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the ArgoCD application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**ArgoCD** uses the *application* concept, which represents a Git repository.
    Depending on the configuration of the application, ArgoCD will poll that repository
    and, in our case, use Kustomize to add, change, or delete resources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ArgoCD doesn’t support AWS IAM roles, so it will use SSH credentials to poll
    the repository. So, we need to configure SSH credentials for a CI/CD user that
    has permission to access the `codecommit` repository. We will use the instructions
    shown at this link: [https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-unixes.html](https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-unixes.html)
    to create an SSH key, and add it to a user with CodeCommit privileges. Once we
    have the SSH key ID, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Install and configure the ArgoCD client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a secret for ArgoCD to use to connect to the repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add our application and check the deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following commands will install the ArgoCD client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will configure the necessary environment variables to connect to our
    environment; examples are shown next, but you should add details relevant to your
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now add our repository and SSH keys to ArgoCD using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We can set up our application to use the repository and private key to deploy
    the resources. We will point it to the non-production overlay so that it will
    use the Kustomize configuration located there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: If we look at the Argo CD UI, we can see the app is healthy and the components
    have been deployed, and ArgoCD will continue to keep them in sync as we make changes
    to the underlying CodeCommit repository.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.19 – The myapp application status in ArgoCD](img/Figure_19.19_B18129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.19 – The myapp application status in ArgoCD
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have a working application that will be continually deployed by Argo
    CD. We can see how we add infrastructure resources to the same repository and
    have them provisioned by Crossplane.
  prefs: []
  type: TYPE_NORMAL
- en: Adding AWS infrastructure with Crossplane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this book, we saw how we can add K8s resources and use K8s controllers,
    such as the AWS Load Balancer Controller, to create AWS resources such as a network
    or application load balancer. Crossplane can be seen as a generic controller for
    AWS resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the cluster we created with Blueprints but replace the Crossplane
    deployment with the latest version. So, we will install helm and then use it to
    deploy the Crossplane charts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You may need to delete the `Crossplane-system` namespace before deploying.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have installed the latest version of Crossplane, we need to configure
    the provider and its associated permissions.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up our Crossplane AWS providers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As Crossplane will create resources in AWS, it requires a role/permission to
    do this. We will start by creating an IRSA role, mapping it to our cluster, and
    assigning it the admin role. The commands for this are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have a role that trusts our cluster’s OIDC provider and has permission
    to provision AWS resources. Next, we need to configure the Crossplane deployment
    to use it. This can be done using the following manifest to configure the provider
    and the controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We can deploy the AWS provider using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the provider is *healthy*, we can finish the configuration by adding a
    provider configuration and defining the credential insertion method as IRSA. This
    is one of the differences of the `upbound` AWS provider – it uses a different
    API and the IRSA source key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We can deploy this manifest file with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'As we enabled debug logging, we can look at the logs of the provider to confirm
    that all the configurations and AWS permissions are in place with the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For a production configuration, you should disable debug logging, as it is very
    verbose and generates a lot of data.
  prefs: []
  type: TYPE_NORMAL
- en: Creating infrastructure resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have a working Crossplane AWS provider, we can actually provision
    an AWS resource. We will configure an S3 bucket with some basic configuration.
    The following manifest will create an S3 bucket called `myapp-Crossplane-bucket637678`
    and use the AWS provider we created in the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We can deploy and verify the bucket using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This manifest can be added to our application repository and the relevant `kustomize.yaml`
    files modified. This now means that, as a DevOps engineer or developer, we can
    configure not only an application but also any supporting infrastructure. If you
    want to use ArgoCD to deploy a Crossplane resource, please refer to this link:
    https://docs.Crossplane.io/v1.10/guides/argo-cd-Crossplane/.'
  prefs: []
  type: TYPE_NORMAL
- en: While this is a long chapter, I have only touched the surface of developing
    for EKS, but hopefully, you have enough information to allow you to explore further!
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at how we can develop on EKS, use a variety of AWS
    services and open source tools to automate our cluster builds, and deploy and
    test our applications. We’ll now revisit the key learning points from this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started by considering that there are multiple personas
    that need to develop on EKS, from traditional developers to DevOps or platform
    engineers. Each of these roles needs similar but different things to do their
    job, so it is really important to consider your operating model when looking at
    EKS development.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we looked at how you can use an IDE to develop your infrastructure/application
    code and how AWS Cloud9 provides a simple and secure interface to do this on EKS.
    We then built a `CodeCommit` repository, and deploying it using the Terraform
    commands. This created a complete EKS cluster, in a new VPC, with a set of applications
    and add-ons automatically configured.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at how platform/DevOps engineers can automate the deployment/testing
    of the EKS cluster using a `buildspec.yaml` file, and we automated this process
    using `CodeCommit` branch. Additionally, we looked at how DevOps engineers or
    developers can use ArgoCD/Kustomize to automate the customization and deployment
    of K8s manifest files.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at how we can incorporate AWS infrastructure resources into
    our application repository, by using Crossplane and creating an S3 bucket in AWS
    using a K8s manifest and custom resource.
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter, we will look at how to troubleshoot common EKS problems.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using Cloud9 in headerless mode:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: https://aws.amazon.com/blogs/devops/how-to-run-headless-front-end-tests-with-aws-cloud9-and-aws-codebuild/
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting started with EKS blueprints for Terraform:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws-ia.github.io/terraform-aws-eks-blueprints/getting-started/](https://aws-ia.github.io/terraform-aws-eks-blueprints/getting-started/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a secure AWS CI/CD pipeline:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: https://aws.amazon.com/blogs/devops/setting-up-a-secure-ci-cd-pipeline-in-a-private-amazon-virtual-private-cloud-with-no-public-internet-access/
  prefs: []
  type: TYPE_NORMAL
- en: 'GitOps on AWS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/containers/gitops-model-for-provisioning-and-bootstrapping-amazon-eks-clusters-using-Crossplane-and-argo-cd/](https://aws.amazon.com/blogs/containers/gitops-model-for-provisioning-and-bootstrapping-amazon-eks-clusters-using-crossplane-and-argo-cd/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 5: Overcoming Common EKS Challenges'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this fifth section is to provide more details on troubleshooting
    common EKS issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section has the following chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 20*](B18129_20.xhtml#_idTextAnchor331)*, Troubleshooting Common Issues*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
