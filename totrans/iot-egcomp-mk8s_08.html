<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer211">
<h1 class="chapter-number" id="_idParaDest-120"><a id="_idTextAnchor121"/>8</h1>
<h1 id="_idParaDest-121"><a id="_idTextAnchor122"/>Monitoring the Health of Infrastructure and Applications</h1>
<p>In the last chapter, we looked at how to expose Services outside of the cluster, and load balancers were used to expose applications to the outside network. The single <strong class="bold">Internet Protocol</strong> (<strong class="bold">IP</strong>) address of a load balancer is used to redirect incoming requests to your application. The <strong class="source-inline">LoadBalancer</strong> Kubernetes service is implemented by <strong class="source-inline">MetalLB</strong>. <strong class="source-inline">MetalLB</strong> assigns a client an IP address from a predefined range when a <strong class="source-inline">LoadBalancer</strong> service is requested and informs the network that the IP address is in the cluster. <strong class="source-inline">MetalLB</strong>, which may be deployed alongside Ingress in the same Kubernetes cluster, can also be utilized as a load balancer. Another technique to expose the Ingress controller to the outside world is through <strong class="source-inline">NodePort</strong>. Both options were explored in detail in the previous chapter, with various examples.</p>
<p>In this chapter, we will look at various options for monitoring, logging, and alerting for your infrastructure and applications. In a traditional, host-centric infrastructure, there used to be only two levels of monitoring: applications and the hosts that run them. Then, container abstraction came in, between the host and your applications, after which Kubernetes came in to orchestrate your containers.</p>
<p>To manage infrastructure thoroughly, Kubernetes must now be observed as well. As a result, four distinct components must now be monitored, each with its own set of challenges:</p>
<ul>
<li>Hosts </li>
<li>Containers </li>
<li>Applications </li>
<li>And finally, the Kubernetes cluster itself</li>
</ul>
<p>To keep track of the health of the Kubernetes infrastructure, there is a need to collect metrics and events from all containers and Pods. However, to fully comprehend what clients or users are going through, there is now a need to keep track of the applications that are operating in these Pods. Note that you normally have very little influence over where workloads run when using Kubernetes, which automatically schedules them.</p>
<p>When it comes to monitoring, Kubernetes forces you to reconsider your strategy. But if you know what to look for, where to look for it, and how to aggregate and analyze it, you can make sure your applications are running smoothly and Kubernetes is doing its job effectively.</p>
<p>For aggregating and reporting monitoring data from your cluster, the Kubernetes ecosystem currently offers two in-built add-ons, as detailed next. </p>
<p>Metrics Server<a id="_idIndexMarker553"/> collects resource consumption statistics from each <strong class="source-inline">kubelet</strong> on each node and returns aggregated metrics via the <strong class="source-inline">Metrics</strong> <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>). Metrics Server only stores near-real-time metrics in memory, so it's best used for spot checks of the <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>) or memory utilization, or for periodic querying through a full-featured monitoring service that keeps data for long periods of time. The <strong class="source-inline">kube-state-metrics</strong> add-on service makes cluster state data public. Unlike Metrics Server, which provides metrics on Pod and node resource utilization, <strong class="source-inline">kube-state-metrics</strong> polls the control plane API server for information on the overall status of Kubernetes objects (nodes, Pods, Deployments, and so on), as well as resource restrictions and allocations. The information is then utilized to generate metrics, which may be accessed through the Metrics API. In short, <strong class="source-inline">kube-state-metrics</strong> focuses on creating whole new metrics from Kubernetes' object state, whereas <strong class="source-inline">metrics-server</strong> merely saves the most recent data and is not responsible for transmitting metrics to third-party destinations.</p>
<p>In the following sections, we'll go over in detail the various options for retrieving metrics using Metrics Server and <strong class="source-inline">kube-state-metrics</strong>. The advantage of MicroK8s is that the monitoring tools can be enabled in under a minute with only a few commands. It is small enough to fit on a Raspberry Pi and it can be used to develop a monitoring stack that can be deployed anywhere, even at the edge. Furthermore, this is built using some of the most popular open source components that come preinstalled with MicroK8s. </p>
<p>We'll look at how to easily deploy monitoring tools at the edge in this chapter. Such a deployment provides privacy, low latency, and minimal bandwidth costs in <strong class="bold">internet of things</strong> (<strong class="bold">IoT</strong>)/edge applications. In this chapter, we're going to cover the following main topics:</p>
<ul>
<li>Overview of monitoring, logging, and alerting options</li>
<li>Configuring a monitoring and alerting stack using the Prometheus, Grafana, and Alertmanager tools</li>
<li>Configuring a monitoring, logging, and alerting stack using the <strong class="bold">Elasticsearch, Fluentd, and Kibana</strong> (<strong class="bold">EFK</strong>) toolset</li>
<li>Key metrics that need to be monitored</li>
</ul>
<h1 id="_idParaDest-122"><a id="_idTextAnchor123"/>Overview of monitoring, logging, and alerting options</h1>
<p>Kubernetes has a lot of advantages, but it also adds<a id="_idIndexMarker554"/> a lot of complexity. Its capacity to distribute<a id="_idIndexMarker555"/> containerized applications across several nodes<a id="_idIndexMarker556"/> and even different data centers (cloud providers, for example) necessitates a comprehensive monitoring solution that can collect and aggregate metrics from a variety of sources.</p>
<p>Many free and paid solutions provide real-time monitoring of Kubernetes clusters and the applications they host, and continuous monitoring of system and application health is critical. Here, we list some prominent open source Kubernetes monitoring tools:</p>
<ol>
<li><strong class="bold">Metrics Server</strong> (in-built) collects resource<a id="_idIndexMarker557"/> metrics from <strong class="source-inline">kubelets</strong> and exposes<a id="_idIndexMarker558"/> them in the Kubernetes API server through the following Metrics API endpoints:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer177">
<img alt="Table 8.1 – Metrics API endpoints " height="963" src="image/01.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.1 – Metrics API endpoints</p>
<ol>
<li value="2"><strong class="bold">Kubernetes Dashboard</strong> (in-built) is a web <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>) add-on for Kubernetes clusters<a id="_idIndexMarker559"/> that allows you to keep track of workload<a id="_idIndexMarker560"/> health. Using a simple web interface, Kubernetes Dashboard allows you to manage cluster resources and troubleshoot containerized applications. It provides a concise overview of cluster-wide and individual node resources. It also lists all clusters' namespaces as well as all storage classes that have been declared. </li>
<li><strong class="bold">Prometheus</strong> is an open source system for collecting<a id="_idIndexMarker561"/> metrics on Kubernetes health. It deploys<a id="_idIndexMarker562"/> node exporter Pods on each cluster node, and its server collects data from nodes, Pods, and jobs. Final time-series metrics data is saved in a database, and alerts can be generated automatically based on predefined conditions.</li>
</ol>
<p>Prometheus has its own dashboard with limited capabilities that have been extended by the usage of other visualization tools such as Grafana, which uses the Prometheus database to provide advanced inquiries, debugging, and reporting designed for development, test, and production teams.</p>
<p>It was built with the objective of monitoring applications and microservices in containers at scale, and it can connect to a wide range of third-party databases and supports the bridging of data from other tools. It is made up of three components at its core, as outlined here:</p>
<ul>
<li>All metrics data will be stored in an in-built time-series database.</li>
<li>A data retrieval worker is in charge of obtaining metrics from outside sources and entering them into the database.</li>
<li>A web server with a simple web<a id="_idIndexMarker563"/> interface for configuring<a id="_idIndexMarker564"/> and querying the stored data.</li>
</ul>
<p>Some of the key features<a id="_idIndexMarker565"/> of Prometheus are presented here:</p>
<ul>
<li>Time-series data classified by metric name and key/value pairs in a multidimensional data model.</li>
<li>Using <strong class="bold">Prometheus Query Language</strong> (<strong class="bold">PromQL</strong>), a flexible query language that allows<a id="_idIndexMarker566"/> us to make use of this dimensionality without relying on distributed storage. </li>
<li>Single-server <a id="_idIndexMarker567"/>nodes are self-contained, and time series are collected using a pull model over <strong class="bold">HyperText Transfer Protocol</strong> (<strong class="bold">HTTP</strong>). </li>
<li>Alternatively, an intermediary gateway can be used to push time series to destinations that are discovered using service discovery or static configuration.</li>
<li>Multiple graphing and dashboarding options are supported.</li>
</ul>
<ol>
<li value="4"><strong class="bold">Grafana</strong>, an open source analytics<a id="_idIndexMarker568"/> and metric visualization platform, includes<a id="_idIndexMarker569"/> four dashboards: <strong class="bold">Cluster</strong>, <strong class="bold">Node</strong>, <strong class="bold">Pod/Container</strong>, and <strong class="bold">Deployment</strong>. Grafana and the Prometheus data source are frequently used by Kubernetes administrators to create information-rich dashboards.</li>
<li><strong class="bold">Elasticsearch, Fluentd, and Kibana</strong> make up the EFK stack, which is a combination of three<a id="_idIndexMarker570"/> tools that function well together. Fluentd is a data<a id="_idIndexMarker571"/> collector for Kubernetes cluster<a id="_idIndexMarker572"/> nodes that collects logs from Pods. It sends<a id="_idIndexMarker573"/> these logs to the Elasticsearch<a id="_idIndexMarker574"/> search engine, which ingests<a id="_idIndexMarker575"/> and stores the data in a central location. The EFK stack's UI is Kibana, a data visualization plugin for Elasticsearch that allows users to visualize collected logs and metrics and construct custom dashboards. </li>
</ol>
<p>Now that we've seen a variety of choices for monitoring, logging, and alerting, we'll go over how to configure them.</p>
<h1 id="_idParaDest-123"><a id="_idTextAnchor124"/>Configuring a monitoring and alerting stack using the Prometheus, Grafana, and Alertmanager tools</h1>
<p>MicroK8s ships pre-integrated<a id="_idIndexMarker576"/> add-ons <a id="_idIndexMarker577"/>with Prometheus Operator<a id="_idIndexMarker578"/> for Kubernetes, which handles<a id="_idIndexMarker579"/> simplified monitoring<a id="_idIndexMarker580"/> definitions<a id="_idIndexMarker581"/> for Kubernetes<a id="_idIndexMarker582"/> services, as<a id="_idIndexMarker583"/> well as Prometheus<a id="_idIndexMarker584"/> instance deployment<a id="_idIndexMarker585"/> and management. </p>
<p class="callout-heading">Note</p>
<p class="callout">Operators are Kubernetes-specific applications (Pods) that automate the configuration, management, and optimization of other Kubernetes deployments. Operators typically take care of the following:</p>
<p class="callout">a. Installing your Kubernetes cluster's specifications and offering initial setup and sizing for your deployment.</p>
<p class="callout">b. Reloading Deployments and Pods in real time to accommodate any user-requested parameter changes (hot config reloading).</p>
<p class="callout">c. Scaling up or down automatically based on performance data.</p>
<p class="callout">d. Backups, integrity checks, and other maintenance tasks should all be performed.</p>
<p>Once<a id="_idIndexMarker586"/> the Prometheus<a id="_idIndexMarker587"/> add-on is<a id="_idIndexMarker588"/> enabled, Prometheus<a id="_idIndexMarker589"/> Operator<a id="_idIndexMarker590"/> takes care<a id="_idIndexMarker591"/> of the<a id="_idIndexMarker592"/> installation<a id="_idIndexMarker593"/> and configuration<a id="_idIndexMarker594"/> of the<a id="_idIndexMarker595"/> following items:</p>
<ul>
<li><strong class="bold">Kubernetes-Prometheus stack</strong>:<ol><li>Prometheus servers </li>
<li>Alertmanager</li>
<li>Grafana</li>
<li>Host-node exporter</li>
<li><strong class="source-inline">kube-state-metrics</strong></li>
</ol></li>
<li><strong class="bold">ServiceMonitor</strong> entities that define metric endpoint autoconfiguration.</li>
<li><strong class="bold">Operator Custom Resource Definitions (CRDs) and ConfigMaps</strong> that can be used to customize and scale the services, thus making our configuration entirely portable and declarative.</li>
</ul>
<p>The following CRDs are managed by Prometheus<a id="_idIndexMarker596"/> Operator:</p>
<ul>
<li><strong class="bold">PrometheusDeployment</strong>—The Operator ensures that a deployment that matches the resource definition is operating at all times.</li>
<li><strong class="bold">ServiceMonitor</strong>—Declaratively specifies how to monitor groups of services. Based on the definition, the Operator<a id="_idIndexMarker597"/> produces a Prometheus<a id="_idIndexMarker598"/> scrape setup automatically.</li>
<li><strong class="bold">PrometheusRule</strong>—Specifies<a id="_idIndexMarker599"/> a Prometheus<a id="_idIndexMarker600"/> rule file to be<a id="_idIndexMarker601"/> loaded by<a id="_idIndexMarker602"/> a Prometheus<a id="_idIndexMarker603"/> instance<a id="_idIndexMarker604"/> with Prometheus <a id="_idIndexMarker605"/>alerting rules.</li>
<li><strong class="bold">AlertManager</strong>—Specifies<a id="_idIndexMarker606"/> the Alertmanager deployment that is desired. The Operator ensures that a deployment that matches the resource definition is operating at all times.</li>
</ul>
<p>For more information<a id="_idIndexMarker607"/> on Prometheus Operator, please refer to the following link: </p>
<p><a href="https://github.com/prometheus-operator/prometheus-operator">https://github.com/prometheus-operator/prometheus-operator</a></p>
<p>The following diagram shows the components that we discussed previously:</p>
<div>
<div class="IMG---Figure" id="_idContainer178">
<img alt="Figure 8.1 – Prometheus Operator components " height="917" src="image/Figure_8.01_B18115.jpg" width="1263"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Prometheus Operator components</p>
<p>To summarize, we <a id="_idIndexMarker608"/>are going<a id="_idIndexMarker609"/> to use<a id="_idIndexMarker610"/> the following<a id="_idIndexMarker611"/> tools to <a id="_idIndexMarker612"/>collect, aggregate, and<a id="_idIndexMarker613"/> visualize<a id="_idIndexMarker614"/> metrics:</p>
<ul>
<li>Kubernetes<a id="_idIndexMarker615"/> metrics pulled from<a id="_idIndexMarker616"/> Metrics<a id="_idIndexMarker617"/> Endpoints.</li>
<li>Host metrics using Prometheus Node Exporter.</li>
<li>Alerting using Prometheus Alertmanager.</li>
<li>Prometheus gathers data from configured targets (from the Kubernetes endpoints discussed in the previous section) at predetermined intervals, analyses rule expressions, displays the results, and can also send out alerts when certain criteria are matched.</li>
<li>Visualization using Grafana pre-built dashboards.</li>
</ul>
<p>Now that<a id="_idIndexMarker618"/> we are clear<a id="_idIndexMarker619"/> on the tools, we<a id="_idIndexMarker620"/> will dive into the steps<a id="_idIndexMarker621"/> of configuring<a id="_idIndexMarker622"/> a monitoring<a id="_idIndexMarker623"/> and alerting<a id="_idIndexMarker624"/> stack. The following<a id="_idIndexMarker625"/> diagram depicts<a id="_idIndexMarker626"/> our Raspberry<a id="_idIndexMarker627"/> Pi cluster setup:</p>
<div>
<div class="IMG---Figure" id="_idContainer179">
<img alt="Figure 8.2 – Raspberry Pi cluster setup " height="616" src="image/Figure_8.02_B18115.jpg" width="895"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Raspberry Pi cluster setup</p>
<p>Now that we know what we want to do, let's look at the requirements.</p>
<h2 id="_idParaDest-124"><a id="_idTextAnchor125"/>Requirements for setting up a MicroK8s Raspberry Pi cluster</h2>
<p>Before you begin, here<a id="_idIndexMarker628"/> are the prerequisites for building a Raspberry Pi Kubernetes cluster:</p>
<ul>
<li>A microSD card (4 <strong class="bold">gigabytes</strong> (<strong class="bold">GB</strong>) minimum; 8 GB recommended)</li>
<li>A computer with a microSD card drive</li>
<li>A Raspberry Pi 2, 3, or 4 (one or more)</li>
<li>A micro-USB power cable (USB-C for the Pi 4)</li>
<li>A Wi-Fi network or an Ethernet cable with an internet connection</li>
<li>(Optional) A monitor with a <strong class="bold">High-Definition Multimedia Interface</strong> (<strong class="bold">HDMI</strong>) interface</li>
<li>(Optional) An HDMI cable for the Pi 2 and 3 and a micro-HDMI cable for the Pi 4</li>
<li>(Optional) A <strong class="bold">Universal Serial Bus</strong> (<strong class="bold">USB</strong>) keyboard</li>
</ul>
<p>Now that we've established the requirements, we'll go on to the step-by-step instructions on how to complete the process.</p>
<h2 id="_idParaDest-125"><a id="_idTextAnchor126"/>Step 1 – Creating a MicroK8s Raspberry Pi cluster </h2>
<p>Please follow the steps<a id="_idIndexMarker629"/> that we covered in <a href="B18115_05.xhtml#_idTextAnchor070"><em class="italic">Chapter 5</em></a><em class="italic">, Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters,</em> to create a MicroK8s Raspberry Pi cluster. Here's a quick refresher:</p>
<ul>
<li><em class="italic">Step 1</em>: Installing the <strong class="bold">operating system</strong> (<strong class="bold">OS</strong>) image to a <strong class="bold">Secure Digital</strong> (<strong class="bold">SD</strong>) card</li>
<li><em class="italic">Step 1a</em>: Configuring Wi-Fi access settings</li>
<li><em class="italic">Step 1b</em>: Configuring remote access settings</li>
<li><em class="italic">Step 1c</em>: Configuring control group settings</li>
<li><em class="italic">Step 1d</em>: Configuring hostname</li>
<li><em class="italic">Step 2</em>: Installing and configuring MicroK8s</li>
<li><em class="italic">Step 3</em>: Adding worker node</li>
</ul>
<p>A fully functional multi-node Kubernetes cluster<a id="_idIndexMarker630"/> should look like the one shown in the following screenshot. To summarize, we have installed MicroK8s on the Raspberry Pi boards and joined multiple Deployments to form the cluster. We have also added nodes to the cluster:</p>
<div>
<div class="IMG---Figure" id="_idContainer180">
<img alt="Figure 8.3 – Fully functional MicroK8s Kubernetes cluster " height="489" src="image/Figure_8.03_B18115.jpg" width="1128"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Fully functional MicroK8s Kubernetes cluster</p>
<p>We can now go to the next step of deploying monitoring tools, as we have a fully functional cluster.</p>
<p>By default, none of the MicroK8s add-ons are turned on. As a result, Grafana and Prometheus must be activated post-installation.</p>
<h2 id="_idParaDest-126"><a id="_idTextAnchor127"/>Step 2 – Configuring Prometheus, Grafana, and Alertmanager</h2>
<p>In this section, we'll enable the Prometheus<a id="_idIndexMarker631"/> add-on and access<a id="_idIndexMarker632"/> the Prometheus and Grafana<a id="_idIndexMarker633"/> dashboards so that we can monitor the Kubernetes cluster and can view alerts if something goes wrong. Use the following command to enable the Dashboard and the Prometheus add-on:</p>
<p class="source-code">microk8s enable dashboard prometheus</p>
<p>The following command execution output indicates the Dashboard and the Prometheus add-on have been enabled successfully: </p>
<div>
<div class="IMG---Figure" id="_idContainer181">
<img alt="Figure 8.4 – Enabling Dashboard and the Prometheus add-on " height="311" src="image/Figure_8.04_B18115.jpg" width="802"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Enabling Dashboard and the Prometheus add-on</p>
<p>It will take some time to finish<a id="_idIndexMarker634"/> activating the add-on, but the following<a id="_idIndexMarker635"/> command execution<a id="_idIndexMarker636"/> output shows that Prometheus has been successfully enabled:</p>
<div>
<div class="IMG---Figure" id="_idContainer182">
<img alt="Figure 8.5 – Add-ons activated " height="269" src="image/Figure_8.05_B18115.jpg" width="767"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Add-ons activated</p>
<p>Grafana cannot be enabled with a command. When the Kubernetes Dashboard is enabled, it starts automatically.</p>
<p>To access the Kubernetes Dashboard, we need to create a user and admin role binding. In the next steps, we will create a deployment for it:</p>
<pre class="source-code">apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system</pre>
<p>Create a <strong class="source-inline">dashboard-adminuser.yaml</strong> file<a id="_idIndexMarker637"/> with the preceding content<a id="_idIndexMarker638"/> and use the following<a id="_idIndexMarker639"/> command to create a user and admin role binding:</p>
<p class="source-code">kubectl apply -f dashboard-adminuser.yaml</p>
<p>The following command execution output confirms that there is no error in the deployment:</p>
<div>
<div class="IMG---Figure" id="_idContainer183">
<img alt="Figure 8.6 – Creating a user and admin role binding " height="74" src="image/Figure_8.06_B18115.jpg" width="643"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Creating a user and admin role binding</p>
<p>To access the dashboard, we need<a id="_idIndexMarker640"/> an access<a id="_idIndexMarker641"/> token, which can be obtained<a id="_idIndexMarker642"/> by invoking the <strong class="source-inline">kubectl</strong> command, as follows:</p>
<p class="source-code">kubectl -n kube-system describe secret $(microk8s kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')</p>
<p>Copy the token from the command's output and use it in the following step. </p>
<p>It will be necessary to build a secure channel to the cluster with the following command in order to access the Kubernetes Dashboard:</p>
<p class="source-code">kubectl proxy &amp;</p>
<p>The following command execution output confirms that a secure channel has been created and we can access the dashboard in the next step: </p>
<div>
<div class="IMG---Figure" id="_idContainer184">
<img alt="Figure 8.7 – Creating a secure channel for the dashboard " height="76" src="image/Figure_8.07_B18115.jpg" width="587"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Creating a secure channel for the dashboard</p>
<p>After that, you'll be able to access the dashboard at the following address:</p>
<p class="source-code">http://&lt;ip address&gt;&gt;:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</p>
<p>By copying and pasting the token generated<a id="_idIndexMarker643"/> in the previous step, you will have access to the cluster's web-based <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>), as illustrated in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer185">
<img alt="Figure 8.8 – Kubernetes Dashboard " height="787" src="image/Figure_8.08_B18115.jpg" width="1308"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Kubernetes Dashboard</p>
<p>As discussed earlier, Dashboard<a id="_idIndexMarker644"/> is a Kubernetes UI that can<a id="_idIndexMarker645"/> be accessed through<a id="_idIndexMarker646"/> the web. It can be used to deploy containerized applications to a Kubernetes cluster, troubleshoot them, and control the cluster's resources. The dashboard can be used for a variety of purposes, including the following:</p>
<ul>
<li>All nodes and persistent storage volumes are listed in <strong class="bold">Admin overview</strong>, along with aggregated metrics for each node.</li>
<li><strong class="bold">Workloads view</strong> displays a list of all running applications by namespace, as well as current Pod memory utilization and the number of Pods in a Deployment that are currently ready.</li>
<li><strong class="bold">Discover view</strong> displays a list of services that have been made public and have enabled cluster discovery.</li>
<li>Drilling down logs from containers belonging to a single Pod is possible using the <strong class="bold">Logs viewer</strong> functionality.</li>
<li>For each clustered application and all Kubernetes resources running in the cluster, <strong class="bold">Storage view</strong> identifies persistent volume claims.</li>
</ul>
<p>We will go to the next step<a id="_idIndexMarker647"/> of accessing<a id="_idIndexMarker648"/> Prometheus, Grafana, and Alertmanager now that<a id="_idIndexMarker649"/> we've enabled all of the required add-ons.</p>
<h2 id="_idParaDest-127"><a id="_idTextAnchor128"/>Step 3 – Accessing Prometheus, Grafana, and Alertmanager</h2>
<p>We can validate<a id="_idIndexMarker650"/> whether Grafana, Prometheus, and Alertmanager are running<a id="_idIndexMarker651"/> on the cluster before<a id="_idIndexMarker652"/> moving on to other steps. </p>
<p>Navigate to <strong class="bold">Monitoring</strong> under <strong class="bold">Namespaces</strong> on the Kubernetes Dashboard, and then click <strong class="bold">Services</strong>. A list of monitoring services running on the cluster, as well as cluster IP addresses, internal endpoints, and ports, will be displayed, as illustrated in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer186">
<img alt="Figure 8.9 – Validating Grafana, Prometheus, and Alertmanager are running on the cluster " height="1078" src="image/Figure_8.09_B18115.jpg" width="1644"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – Validating Grafana, Prometheus, and Alertmanager are running on the cluster</p>
<p>From Kubernetes<a id="_idIndexMarker653"/> Dashboard, shown in <em class="italic">Figure 8.9</em>, we can<a id="_idIndexMarker654"/> ensure the following components<a id="_idIndexMarker655"/> are operational:</p>
<ul>
<li><strong class="source-inline">prometheus-operator</strong> Pod—The core of the stack, in charge of managing other Deployments such as Prometheus servers or Alertmanager servers</li>
<li><strong class="source-inline">node-exporter</strong> pod—Per physical host (one in this example)</li>
<li><strong class="source-inline">kube-state-metrics</strong> exporter</li>
<li><strong class="bold">Prometheus server</strong> deployment—<strong class="source-inline">prometheus-k8s</strong> (replicas: 1)</li>
<li><strong class="bold">Alertmanager </strong>deployment—<strong class="source-inline">alertmanager-main</strong> (replicas: 1)</li>
<li><strong class="bold">Grafana</strong> deployment—<strong class="source-inline">grafana</strong> (replicas: 1)</li>
</ul>
<p>The Grafana and Prometheus UIs can then be accessible simply by putting the service IP and ports into the browser in the format <strong class="source-inline">&lt;IP address&gt;:&lt;port&gt;</strong>. The login and password for Grafana will be <strong class="source-inline">admin/admin</strong>.</p>
<p>By default, Grafana uses port <strong class="source-inline">3000</strong>; so, navigate to <strong class="source-inline">http://localhost:3000</strong> in your web browser, and you'll be able<a id="_idIndexMarker656"/> to visit the Grafana interface, which<a id="_idIndexMarker657"/> is already populated with some<a id="_idIndexMarker658"/> interesting dashboards, as we can see here:</p>
<div>
<div class="IMG---Figure" id="_idContainer187">
<img alt="Figure 8.10 – Grafana pre-built dashboards " height="511" src="image/Figure_8.10_B18115.jpg" width="1134"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Grafana pre-built dashboards</p>
<p>Grafana comes with Prometheus preinstalled as a data source, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer188">
<img alt="Figure 8.11 – Grafana/Prometheus data source " height="369" src="image/Figure_8.11_B18115.jpg" width="720"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – Grafana/Prometheus data source</p>
<p>In a similar way, the Prometheus UI<a id="_idIndexMarker659"/> can be accessed. There will be no need<a id="_idIndexMarker660"/> for a username or password. By default, Prometheus<a id="_idIndexMarker661"/> uses port <strong class="source-inline">9090</strong> and exposes its internal metrics and performance. The Node Exporter Prometheus process runs on port <strong class="source-inline">9100</strong>. This exposes the details about the node, including storage space, <strong class="bold">random-access memory</strong> (<strong class="bold">RAM</strong>), and <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>) utilization. Metrics should be available on targets with an <strong class="source-inline">http://&lt;IP address:9090/metrics</strong> path. You can see an overview of the Prometheus UI here:</p>
<div>
<div class="IMG---Figure" id="_idContainer189">
<img alt="Figure 8.12 – Prometheus UI " height="677" src="image/Figure_8.12_B18115.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – Prometheus UI</p>
<p>Prometheus will scrape and store data based on the predefined configuration. Go to the dashboard to see whether Prometheus has information about the time series that this endpoint exposes on the node.</p>
<p>To see a list of metrics<a id="_idIndexMarker662"/> this server is collecting, use the dropdown<a id="_idIndexMarker663"/> next to the <strong class="bold">Execute</strong> button. A number of metrics<a id="_idIndexMarker664"/> prefixed with <strong class="source-inline">node_</strong> that have been collected by Node Exporter can be found in the list. The <strong class="source-inline">cpu metric</strong> node, for example, displays the node's CPU utilization, as illustrated in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer190">
<img alt="Figure 8.13 – Prometheus metrics visualization " height="672" src="image/Figure_8.13_B18115.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – Prometheus metrics visualization</p>
<p>ServiceMonitor automatically detects and registers each target in your Prometheus configuration, as illustrated here:</p>
<div>
<div class="IMG---Figure" id="_idContainer191">
<img alt="Figure 8.14 – Prometheus scrape targets " height="829" src="image/Figure_8.14_B18115.jpg" width="844"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – Prometheus scrape targets</p>
<p>From the Prometheus server interface, the <strong class="bold">Alerts</strong> tab displays alerts that are created, as illustrated in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer192">
<img alt="Figure 8.15 – Prometheus Alertmanager " height="279" src="image/Figure_8.15_B18115.jpg" width="492"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – Prometheus Alertmanager</p>
<p>Please refer<a id="_idIndexMarker665"/> to the Prometheus community<a id="_idIndexMarker666"/> GitHub repository<a id="_idIndexMarker667"/> for predefined alert rules, at the following link:</p>
<p><a href="https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/templates/prometheus/rules-1.14">https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/templates/prometheus/rules-1.14</a></p>
<p>To summarize, we looked at how to quickly deploy a Kubernetes monitoring and alerting stack using the Prometheus add-on, which is also easy to expand, alter, or migrate to a new set of servers based on the needs.</p>
<p>The following things should be noted for production deployments:</p>
<ul>
<li><strong class="bold">Long-term storage</strong>—The Prometheus database stores metrics for the previous 15 days by default. Prometheus doesn't offer long-term storage of metrics. There is no option for backup, data redundancy, trend analysis, data mining, and so on.</li>
<li><strong class="bold">Authorization and authentication</strong>—There is no server-side authentication, authorization, or encryption provided by Prometheus or its components.</li>
<li>There is no support for vertical/horizontal scalability.</li>
</ul>
<p>We've looked at how to use the Prometheus add-on to enable Kubernetes monitoring and alerting, and now, we'll look at how to use the EFK toolset to configure a logging, monitoring, and alerting stack.</p>
<h1 id="_idParaDest-128"><a id="_idTextAnchor129"/>Configuring a logging, monitoring, and alerting stack using the EFK toolset </h1>
<p>In cases where we need<a id="_idIndexMarker668"/> to analyze massive volumes of log data collected<a id="_idIndexMarker669"/> by Pods running many services<a id="_idIndexMarker670"/> and applications on a Kubernetes<a id="_idIndexMarker671"/> cluster, a centralized, cluster-level logging<a id="_idIndexMarker672"/> stack could be useful. EFK<a id="_idIndexMarker673"/> is the most popular centralized logging solution. Elasticsearch is a real-time search engine that supports full-text and structured searches, as well as analytics, and is distributed and scalable. It's most commonly used for indexing and searching large amounts of log data. Elasticsearch is widely used in conjunction with Kibana, a powerful data visualization frontend and dashboard for Elasticsearch. Kibana is a web-based tool that allows you to quickly query and get insight into Kubernetes applications by viewing Elasticsearch log data and creating dashboards and queries. To gather, transform, and transfer log data to the Elasticsearch backend, we'll use Fluentd, a popular open source data collector, to tail container log files, filter and change data, and feed it to an Elasticsearch cluster for indexing and storage on our Kubernetes nodes. The following diagram depicts what we want to achieve using the EFK toolset:</p>
<div>
<div class="IMG---Figure" id="_idContainer193">
<img alt="Figure 8.16 – Centralized logging solution: EFK toolset " height="831" src="image/Figure_8.16_B18115.jpg" width="1272"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.16 – Centralized logging solution: EFK toolset</p>
<p>Since EFK<a id="_idIndexMarker674"/> isn't available<a id="_idIndexMarker675"/> for <strong class="source-inline">arm64</strong> architecture, I'll be using<a id="_idIndexMarker676"/> an Ubuntu <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) for this section. The instructions<a id="_idIndexMarker677"/> for setting<a id="_idIndexMarker678"/> up a MicroK8s<a id="_idIndexMarker679"/> cluster are the same<a id="_idIndexMarker680"/> as in <a href="B18115_05.xhtml#_idTextAnchor070"><em class="italic">Chapter 5</em></a><em class="italic">, Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters</em>.</p>
<p>Now that we are clear on what we want to achieve, we will dive into the steps in detail.</p>
<h2 id="_idParaDest-129"><a id="_idTextAnchor130"/>Step 1 – Enabling the Fluentd add-on </h2>
<p>We'll enable<a id="_idIndexMarker681"/> the Fluentd add-on in this section, which allows the EFK toolset to gather log data, pass it to Elasticsearch for indexing, and then view aggregated logs using the Kibana dashboard. Use the following command to enable the Fluentd add-on:</p>
<p class="source-code">microk8s enable fluent</p>
<p>When you enable this add-on, Elasticsearch, Fluentd, and Kibana (the EFK stack) will be added to MicroK8s. </p>
<p>The following command execution<a id="_idIndexMarker682"/> output confirms that the EFK add-on has been enabled:</p>
<div>
<div class="IMG---Figure" id="_idContainer194">
<img alt="Figure 8.17 – Enabling Fluentd add-on " height="347" src="image/Figure_8.17_B18115.jpg" width="675"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.17 – Enabling Fluentd add-on</p>
<p>Before moving to the next step, let's verify the add-on has been activated. </p>
<p>To do this, use the <strong class="source-inline">microk8s status</strong> command. The following command execution output indicates that the Fluentd add-on has been enabled:</p>
<div>
<div class="IMG---Figure" id="_idContainer195">
<img alt="Figure 8.18 – Validating whether the add-on is activated " height="243" src="image/Figure_8.18_B18115.jpg" width="720"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.18 – Validating whether the add-on is activated</p>
<p>All of the services for EFK are active, as shown<a id="_idIndexMarker683"/> in the output of the command shown next:</p>
<div>
<div class="IMG---Figure" id="_idContainer196">
<img alt="Figure 8.19 – Verifying EFK Pods are running " height="160" src="image/Figure_8.19_B18115.jpg" width="894"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.19 – Verifying EFK Pods are running</p>
<p>We now have all the services of EFK up and running. To access the Kibana dashboard, we will need to build a secure channel (as we did for Kubernetes Dashboard) to the cluster with the command shown next:</p>
<p class="source-code">microk8s kubectl port-forward -n kube-system service/kibana-logging 8181:5601</p>
<p>The following command execution output confirms that port forwarding is successful:</p>
<div>
<div class="IMG---Figure" id="_idContainer197">
<img alt="Figure 8.20 – Creating a secure channel for the Kibana dashboard " height="76" src="image/Figure_8.20_B18115.jpg" width="718"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.20 – Creating a secure channel for the Kibana dashboard</p>
<p>The Kibana dashboard should be now available at the following address:</p>
<p class="source-code">http://&lt;IP address&gt;:8001/api/v1/namespaces/kube-system/services/kibana-logging/proxy/app/kibana</p>
<p>To summarize, we now have a completely<a id="_idIndexMarker684"/> functional EFK stack that can be configured. The next step is to start defining an index pattern in the Kibana dashboard.</p>
<h2 id="_idParaDest-130"><a id="_idTextAnchor131"/>Step 2 – Defining an index pattern </h2>
<p>We are going to analyze<a id="_idIndexMarker685"/> whether the EFK container can start up logs itself. To do that, we'll need to establish an index pattern. A collection of documents with similar characteristics is referred to as an index. An index is given a name, which is used to refer to it while conducting indexing, searching, updating, and deleting operations on the documents it contains. </p>
<p>Launch the Kibana dashboard, and you should see Kibana welcome page, as shown in the following screenshot: </p>
<div>
<div class="IMG---Figure" id="_idContainer198">
<img alt="Figure 8.21 – Kibana welcome page " height="872" src="image/Figure_8.21_B18115.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.21 – Kibana welcome page</p>
<p>Click <strong class="bold">Set up index patterns</strong> on the welcome<a id="_idIndexMarker686"/> page or select <strong class="bold">Discover item</strong> from the left-hand drop-down menu. On the top, click the <strong class="bold">Create index pattern</strong> button. Enter the index's name (for example, <strong class="source-inline">logstash-*</strong>), as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer199">
<img alt="Figure 8.22 – Creating an index pattern " height="693" src="image/Figure_8.22_B18115.jpg" width="1218"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.22 – Creating an index pattern</p>
<p>Kibana will then request a field<a id="_idIndexMarker687"/> with a time/timestamp that it can use to visualize time-series data. This is the <strong class="source-inline">@timestamp</strong> field in our case, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer200">
<img alt="Figure 8.23 – Creating an index pattern with a timestamp field " height="669" src="image/Figure_8.23_B18115.jpg" width="1217"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.23 – Creating an index pattern with a timestamp field</p>
<p>Click <strong class="bold">Create index pattern</strong>, and it should just take a few minutes now that we've built the index pattern. You can see the output here:</p>
<div>
<div class="IMG---Figure" id="_idContainer201">
<img alt="Figure 8.24 – Index pattern created " height="282" src="image/Figure_8.24_B18115.jpg" width="602"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.24 – Index pattern created</p>
<p>Select the <strong class="bold">Discover</strong> option<a id="_idIndexMarker688"/> from the left-hand drop-down menu. You should see container log events displayed, as shown in the following screenshot: </p>
<div>
<div class="IMG---Figure" id="_idContainer202">
<img alt="Figure 8.25 – Discovering data using the index pattern " height="1032" src="image/Figure_8.25_B18115.jpg" width="1406"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.25 – Discovering data using the index pattern</p>
<p>The next step is to filter and examine the container startup log events now that we've created an index pattern and organized the data.</p>
<h2 id="_idParaDest-131"><a id="_idTextAnchor132"/>Step 3 – Filtering and viewing the data </h2>
<p>There will be a listing<a id="_idIndexMarker689"/> of all log events with fields available for filtering on the left-hand<a id="_idIndexMarker690"/> side, as illustrated in the following screenshot. You may either create<a id="_idIndexMarker691"/> a new filter or utilize the <strong class="source-inline">kubernetes.podname</strong> parameter in <strong class="bold">Kibana Query Language</strong> (<strong class="bold">KQL</strong>) to filter events:</p>
<div>
<div class="IMG---Figure" id="_idContainer203">
<img alt="Figure 8.26 – Filtering log events for a particular Pod " height="891" src="image/Figure_8.26_B18115.jpg" width="1407"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.26 – Filtering log events for a particular Pod</p>
<p>The log list is now filtered to show only log events from that particular Pod. You can explore any event or filter to see more information.</p>
<p>When the Fluent Bit log processor is enabled, it will read, parse, and filter the logs of every Pod on the Kubernetes cluster, enriching each entry with the following data:</p>
<ul>
<li>Pod name</li>
<li>Pod <strong class="bold">identifier</strong> (<strong class="bold">ID</strong>)</li>
<li>Container name</li>
<li>Container ID</li>
<li>Labels</li>
<li>Annotations</li>
</ul>
<p>Once all the events are indexed, the alerting configuration of Kibana could be used to create rules that detect failure scenarios and then act when those criteria are fulfilled.</p>
<p>More details<a id="_idIndexMarker692"/> on alerting can be found here:</p>
<p><a href="https://www.elastic.co/guide/en/kibana/current/alerting-getting-started.xhtml">https://www.elastic.co/guide/en/kibana/current/alerting-getting-started.xhtml</a></p>
<p><strong class="source-inline">Fluentd</strong> has a lighter version, Fluent Bit, which was created<a id="_idIndexMarker693"/> by the same team for situations<a id="_idIndexMarker694"/> with more limited resources. Functionality-wise, <strong class="source-inline">Fluentd</strong> is a log aggregator, while Fluent Bit is just a forwarder. <strong class="source-inline">Fluentd</strong> offers a more robust ecosystem, whereas Fluent Bit is more prevalent in IoT devices.</p>
<p>More details<a id="_idIndexMarker695"/> on Fluent Bit can be found here: </p>
<p><a href="https://fluentbit.io/">https://fluentbit.io/</a></p>
<p>Congratulations! Using the EFK stack, we have learned how to aggregate all Kubernetes container logs and analyze them centrally.</p>
<p>To recap, we looked at some of the most popular monitoring, logging, and alerting stack options. The next step is to determine which critical metrics should be monitored in order to manage your infrastructure and applications effectively.</p>
<h1 id="_idParaDest-132"><a id="_idTextAnchor133"/>Key metrics that need to be monitored </h1>
<p>The rapid adoption of containers<a id="_idIndexMarker696"/> in enterprise organizations has provided numerous benefits to developers. However, the flexibility and scalability that Kubernetes provides in deploying containerized applications have also introduced new complications. Keeping track of the health of applications abstracted by containers and then abstracted again by Kubernetes can be difficult without the right tools because there is no longer a 1-to-1 correlation between an application and the server it runs on.</p>
<p>Containerized applications can be spread across multiple environments, and Kubernetes is a complicated environment. Monitoring tools should have the capability to collect metrics from across a distributed environment and deal with the transient nature of containerized resources. Monitoring tools rely on services as their endpoint because Pods and their containers are in constant motion and dynamically scheduled. Services broadcast an IP address that can be accessed from outside Pods, allowing services to communicate in real time as Pods and containers are built and removed.</p>
<p>In Kubernetes, there are two levels of monitoring, as outlined here:</p>
<ul>
<li><strong class="bold">Cluster monitoring</strong>—Monitors the health<a id="_idIndexMarker697"/> of a Kubernetes cluster<a id="_idIndexMarker698"/> as a whole. Helps in checking whether nodes are up to date and running, how many applications are running on each node, and how the cluster as a whole is using resources.</li>
<li><strong class="bold">Pod monitoring</strong>—Keeps track of issues<a id="_idIndexMarker699"/> that affect individual Pods, including<a id="_idIndexMarker700"/> a Pod's resource use, application metrics, and metrics linked to replication or autoscaling.</li>
</ul>
<p>As we discussed in the preceding<a id="_idIndexMarker701"/> sections, Kubernetes-based architecture already provides a framework for analyzing and monitoring your applications. You can get a comprehensive view of application health and performance with a suitable monitoring solution that integrates with Kubernetes' built-in abstractions, even if the containers that execute those applications are continually shifting across hosts or scaling up and down.</p>
<p>Next, we will look at some of the key metrics that should be monitored. These are listed here. </p>
<p><strong class="bold">Cluster level</strong>—The following cluster-state metrics<a id="_idIndexMarker702"/> can give you a high-level picture<a id="_idIndexMarker703"/> of your cluster's current state. They can reveal problems with nodes or Pods, alerting you to the risk of a bottleneck or the need to expand up your cluster:</p>
<div>
<div class="IMG---Figure" id="_idContainer204">
<img alt="Table 8.2 – Cluster-state metrics " height="748" src="image/02.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.2 – Cluster-state metrics</p>
<p><strong class="bold">Node level</strong>—The following measures<a id="_idIndexMarker704"/> give you a high-level picture<a id="_idIndexMarker705"/> of a node's health and whether or not the scheduler can schedule Pods on it. When you compare resource utilization to resource requests and limits, you may get a better idea of whether your cluster has enough resources to handle its workloads and accommodate new ones. It's critical to maintain and track resource utilization across your cluster's levels, especially for your nodes and the Pods that run on them:</p>
<div>
<div class="IMG---Figure" id="_idContainer205">
<img alt="Table 8.3 – Node-level metrics " height="996" src="image/03.jpg" width="1382"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.3 – Node-level metrics</p>
<p><strong class="bold">Pod level</strong>—Although a Pod may<a id="_idIndexMarker706"/> be functioning, if it is not accessible, this means<a id="_idIndexMarker707"/> it is not ready to accept traffic. This is normal in some situations, such as when a Pod is first launched or when a change to the Pod's specifications is made and deployed. However, if you notice a surge in the number of unavailable Pods or Pods that are constantly unavailable, it could suggest a setup issue. Keep track of the following metrics to gauge the health of Pods:</p>
<div>
<div class="IMG---Figure" id="_idContainer206">
<img alt="Table 8.4 – Pod-level metrics " height="785" src="image/04.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.4 – Pod-level metrics</p>
<p><strong class="bold">Container level</strong>—Some of the container metrics<a id="_idIndexMarker708"/> that should be tracked to<a id="_idIndexMarker709"/> assess container health are listed next:</p>
<div>
<div class="IMG---Figure" id="_idContainer207">
<img alt="Table 8.5 – Container metrics " height="902" src="image/05.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.5 – Container metrics</p>
<p><strong class="bold">Storage</strong>—Volumes serve as a crucial abstraction<a id="_idIndexMarker710"/> in the Kubernetes storage<a id="_idIndexMarker711"/> architecture. Containers can request storage resources dynamically<a id="_idIndexMarker712"/> via a mechanism called volume claims, and volumes can be persistent or non-persistent, as detailed here:</p>
<div>
<div class="IMG---Figure" id="_idContainer208">
<img alt="Table 8.6 – Storage metrics " height="697" src="image/06.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.6 – Storage metrics</p>
<p><strong class="bold">Control plane</strong>—The worker nodes and Pods<a id="_idIndexMarker713"/> in a cluster are managed<a id="_idIndexMarker714"/> by the control plane. Here are the control plane components that need to be monitored:</p>
<ul>
<li><strong class="bold">etcd</strong>—Stores configuration information that each node in the cluster can use</li>
<li><strong class="bold">API server</strong>—Validates and configures data for API objects such as Pods, services, and replication controllers, among other things</li>
<li><strong class="bold">Scheduler</strong>—Manages the use of workloads and the assignment of Pods to available nodes</li>
<li><strong class="bold">Controller Manager</strong>—A daemon in charge of gathering and sending data to the API server. </li>
</ul>
<p>You can see more details on these components here:</p>
<div>
<div class="IMG---Figure" id="_idContainer209">
<img alt="Table 8.7 – Control-plane metrics " height="1559" src="image/07.jpg" width="1639"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.7 – Control-plane metrics</p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor134"/>Kubernetes events</h2>
<p>Kubernetes events are a resource type in Kubernetes<a id="_idIndexMarker715"/> that are created automatically when other resources' states change, errors occur, or other messages need to be broadcast to the system. Here are various types of events that need to be monitored:</p>
<ol>
<li value="1"><strong class="bold">Failed events</strong>—While containers are created<a id="_idIndexMarker716"/> on a regular basis, the operation can frequently fail; as a result, Kubernetes<a id="_idIndexMarker717"/> does not successfully create that container. Failed events are frequently associated with image retrieval errors. These failures could be caused by typos, insufficient permissions, or upstream build failures. Furthermore, nodes can also fail on their own. When these failures occur, applications should fall back to functional, remaining nodes, but some kind of alerting system is required to determine why the failure occurred. Because a failure is a showstopper—that is, your container will not run until it is resolved—you should pay close attention to this event type.</li>
<li><strong class="bold">Evicted events</strong>—Certain Pods can consume<a id="_idIndexMarker718"/> a disproportionate amount<a id="_idIndexMarker719"/> of computing and memory resources in comparison to their respective runtimes. Kubernetes addresses this issue by evicting Pods and reallocating disk, memory, or CPU space elsewhere.</li>
<li><strong class="bold">Storage specific events</strong>—Storage within Pods is commonly used by applications and workloads. Volumes<a id="_idIndexMarker720"/> provided by respective providers<a id="_idIndexMarker721"/> store critical content that is required by application runtimes. Upon creation, Pods mount these volumes, paving the way for successful operation. These events can alert you when storage volumes are behaving strangely. Furthermore, a node may not be in good enough health to mount a volume. These errors, on the other hand, can make it appear as if a Pod is just getting started. However, discovering<a id="_idIndexMarker722"/> these events can assist you in resolving the underlying issues caused by faulty volume mounting.</li>
<li><strong class="bold">Kubernetes Scheduling events</strong>—These are scheduling events such as a <strong class="source-inline">FailedScheduling</strong> event occurring<a id="_idIndexMarker723"/> when the Kubernetes<a id="_idIndexMarker724"/> scheduler is unable to find a suitable node.</li>
<li><strong class="bold">Node-specific events</strong>—Node events can indicate<a id="_idIndexMarker725"/> erratic or unhealthy behavior<a id="_idIndexMarker726"/> elsewhere in the system; for example, the <strong class="source-inline">NodeNotReady</strong> event denotes a node that is not ready for Pod scheduling.</li>
</ol>
<p>For Kubernetes events, metrics and alert criteria that must be monitored are listed here:</p>
<div>
<div class="IMG---Figure" id="_idContainer210">
<img alt="Table 8.8 – Kubernetes events metrics " height="432" src="image/08.jpg" width="1554"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.8 – Kubernetes events metrics</p>
<h1 id="_idParaDest-134"><a id="_idTextAnchor135"/>Summary</h1>
<p>To summarize, we've covered key Kubernetes components, as well as the metrics and events that can help you track their health and performance over time. We have also covered how to collect all of the metrics using built-in Kubernetes APIs and utilities, allowing you to gain comprehensive visibility into your container infrastructure and workloads.</p>
<p>We looked at Prometheus, Grafana, and Alertmanager as tools for setting up a monitoring and alerting stack. We've also looked at how to set up a centralized, cluster-level logging stack with the EFK toolset, which can handle massive amounts of log data. Finally, we went over the essential indicators that should be watched in order to successfully manage your infrastructure and apps.</p>
<p>We'll look at how to develop and deploy a machine learning model using the Kubeflow MLOps platform in the next chapter. Kubeflow and MicroK8s deliver reliable and efficient operations as well as infrastructure optimization.</p>
</div>
<div class="Basic-Text-Frame" id="_idContainer212">
<p class="hidden">Configuring a monitoring and alerting stack using the Prometheus, Grafana, and Alertmanager tools</p>
</div>
<div class="Basic-Text-Frame" id="_idContainer213">
<p class="hidden">Configuring a monitoring and alerting stack using the Prometheus, Grafana, and Alertmanager tools</p>
</div>
<div class="Basic-Text-Frame" id="_idContainer214">
<p class="hidden">Configuring a monitoring and alerting stack using the Prometheus, Grafana, and Alertmanager tools</p>
</div>
<div class="Basic-Text-Frame" id="_idContainer215">
<p class="hidden">Configuring a monitoring and alerting stack using the Prometheus, Grafana, and Alertmanager tools</p>
</div>
<div class="Basic-Text-Frame" id="_idContainer216">
<p class="hidden">Configuring a monitoring and alerting stack using the Prometheus, Grafana, and Alertmanager tools</p>
</div>
<div class="Basic-Text-Frame" id="_idContainer217">
<p class="hidden">Configuring a monitoring and alerting stack using the Prometheus, Grafana, and Alertmanager tools</p>
</div>
<div class="Basic-Text-Frame" id="_idContainer218">
<p class="hidden">Configuring a monitoring and alerting stack using the Prometheus, Grafana, and Alertmanager tools</p>
</div>
</div>
</body></html>