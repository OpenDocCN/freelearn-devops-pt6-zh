<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer067">
<h1 class="chapter-number" id="_idParaDest-59"><a id="_idTextAnchor060"/>4</h1>
<h1 id="_idParaDest-60"><a id="_idTextAnchor061"/>Handling the Kubernetes Platform for IoT and Edge Computing</h1>
<p>Kubernetes has achieved significant adoption in data center and cloud environments since its launch in 2014. Kubernetes has progressed from orchestrating lightweight application containers to managing and scheduling a diverse set of IT workloads, ranging from virtualized network operations to AI/ML and GPU hardware resources. </p>
<p>Kubernetes is quickly gaining popularity as the most popular control plane for scheduling and managing work in distributed systems. These activities could involve deploying virtual machines on real hosts, pushing containers to edge nodes, and even expanding the control plane to incorporate additional schedulers, such as serverless environments. Its extensibility makes it a universal scheduler and the most preferred management platform. In this chapter, we are going to explore various deployment approaches to how Kubernetes, the edge, and the cloud can collaborate to drive intelligent business decisions.</p>
<p>Reiterating the points we discussed in the last chapter, the following considerations must be noted while building the edge architecture: </p>
<ul>
<li><strong class="bold">Autonomy and resiliency</strong>: Because the solution necessitates autonomy, connection interruptions cannot be permitted.</li>
<li><strong class="bold">Resource constraints</strong>: Low compute capability and small device footprints.</li>
<li><strong class="bold">Security challenges</strong>: Data privacy, physical device security, and the network security of the connected devices.</li>
<li><strong class="bold">Manageability</strong>: Manage application software across thousands of devices from many different suppliers.</li>
<li><strong class="bold">Reliability</strong>: Consistency in the building, deployment, and maintenance of applications.</li>
<li><strong class="bold">Automation</strong>: With high levels of automation, provision for automated mechanisms to deploy and maintain multiple distributed applications over any number of physical or virtual computers.</li>
</ul>
<p>Let's look at four architectural approaches that meet these criteria. In this chapter, we're going to cover the following main topics: </p>
<ul>
<li>Deployment approaches for edge computing </li>
<li>Propositions that Kubernetes offers</li>
</ul>
<h1 id="_idParaDest-61"><a id="_idTextAnchor062"/>Deployment approaches for edge computing</h1>
<p>The following approaches demonstrate<a id="_idIndexMarker210"/> how Kubernetes can be used for edge workloads, as well as support for the architecture that meets enterprise applications' requirements – low latency, resource constraints, data privacy, bandwidth scalability, and others.</p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor063"/>Deployment of the entire Kubernetes cluster at the edge</h2>
<p>The complete Kubernetes cluster<a id="_idIndexMarker211"/> is deployed among edge<a id="_idIndexMarker212"/> nodes in this approach. This solution is better suited to situations where the edge node has limited capacity and does not want to consume additional resources for control planes and nodes. </p>
<p>The simplest<a id="_idIndexMarker213"/> production-grade upstream <strong class="bold">K8s</strong> is <strong class="bold">MicroK8s</strong>, a <strong class="bold">Cloud Native Computing Foundation</strong>-certified Kubernetes distribution<a id="_idIndexMarker214"/> that is lightweight<a id="_idIndexMarker215"/> and focused, with options to install on Linux, Windows, and macOS.</p>
<p>Another example is K3s from Rancher, which is a Cloud Native Computing Foundation-certified Kubernetes distribution and is designed for production workloads running in resource-constrained environments such as IoT and edge computing deployments. </p>
<p>The minimal K3s Kubernetes cluster running on edge nodes is depicted in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer059">
<img alt="Figure 4.1 – K3s architecture " height="741" src="image/Figure_4.1_B18115.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – K3s architecture</p>
<p>MicroK8s and K3s can be installed<a id="_idIndexMarker216"/> on public cloud virtual machines or even<a id="_idIndexMarker217"/> on a Raspberry Pi device. The architecture is highly optimized for unattended, remote installations on resource-constrained devices while preserving complete compatibility and compliance with Cloud Native Computing Foundation Kubernetes conformance tests.</p>
<p>By making it accessible and lightweight, MicroK8s and K3s are bringing Kubernetes to the edge computing layer.</p>
<p>A quick comparison<a id="_idIndexMarker218"/> of MicroK8s and K3s<a id="_idIndexMarker219"/> is shown in the following table:</p>
<div>
<div class="IMG---Figure" id="_idContainer060">
<img alt="Table 4.1 – Comparison of MicroK8s and K3s " height="856" src="image/B18115_04_Table_4.1.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.1 – Comparison of MicroK8s and K3s</p>
<p>Optionally, you can also use platforms<a id="_idIndexMarker220"/> such as Google Anthos or AKS to manage<a id="_idIndexMarker221"/> and orchestrate container workloads on multiple clusters like the one shown here:</p>
<div>
<div class="IMG---Figure" id="_idContainer061">
<img alt="Figure 4.2 – Google Anthos on the cloud and MicroK8s at the edge " height="744" src="image/Figure_4.2_B18115.jpg" width="1084"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Google Anthos on the cloud and MicroK8s at the edge</p>
<p>In the following chapters, we'll look at implementation<a id="_idIndexMarker222"/> aspects of common edge computing<a id="_idIndexMarker223"/> applications using MicroK8s.</p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor064"/>Deployment of Kubernetes nodes at the edge</h2>
<p>The core Kubernetes cluster<a id="_idIndexMarker224"/> is installed at a cloud provider or in your data<a id="_idIndexMarker225"/> center in this approach, with Kubernetes nodes deployed at the edge nodes. This is more appropriate for use cases where the infrastructure at the edge is constrained.</p>
<p><strong class="bold">KubeEdge</strong>, an open source application<a id="_idIndexMarker226"/> that extends native containerized application orchestration and device management to hosts at the edge, is an example of this approach. KubeEdge is made up of two parts: the cloud and the edge.</p>
<p>It is based on Kubernetes and enables networking, application deployment, and metadata synchronization between the cloud and edge infrastructures. Developers can also use MQTT to write custom logic and enable resource-constrained device communication at the edge. KubeEdge is reliable and supports the most common IoT and edge use cases. It can be run <a id="_idIndexMarker227"/>on a compatible Linux distribution or an ARM<a id="_idIndexMarker228"/> device such as a Raspberry Pi.</p>
<p>The architecture<a id="_idIndexMarker229"/> of KubeEdge is shown here:</p>
<div>
<div class="IMG---Figure" id="_idContainer062">
<img alt="Figure 4.3 – KubeEdge architecture " height="1024" src="image/Figure_4.3_B18115.jpg" width="1180"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – KubeEdge architecture</p>
<p>Here's a quick rundown of KubeEdge's different<a id="_idIndexMarker230"/> components:</p>
<div>
<div class="IMG---Figure" id="_idContainer063">
<img alt="Table 4.2 – KubeEdge components " height="1013" src="image/B18115_04_Table_4.2.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.2 – KubeEdge components</p>
<p>The following are some<a id="_idIndexMarker231"/> of KubeEdge's primary features:</p>
<ul>
<li>Users can<a id="_idIndexMarker232"/> orchestrate apps, manage devices, and monitor<a id="_idIndexMarker233"/> app and device status on edge nodes using KubeEdge, just like they can with a regular Kubernetes cluster in the cloud.</li>
<li>Bidirectional communication, able to talk to edge nodes located in private subnets. Supports both metadata and data.</li>
<li>Even when the edge is disconnected from the cloud, it can operate autonomously. Metadata is persistent per node; thus, no list-watching is required during node recovery. This allows you to get ready faster.</li>
<li>At the edge, resource use is optimized. The memory footprint has been reduced to around 70 MB.</li>
<li>For IoT and Industrial IoT, connectivity between applications and devices is simplified.</li>
<li>Native support for x86, ARMv7, and ARMv8 architectures.</li>
<li>Support for running<a id="_idIndexMarker234"/> third-party plugins and apps<a id="_idIndexMarker235"/> that rely on Kubernetes APIs on edge<a id="_idIndexMarker236"/> nodes with an autonomous Kube-API endpoint at the edge.</li>
</ul>
<h2 id="_idParaDest-64"><a id="_idTextAnchor065"/>Deployment of virtual Kubernetes nodes at the edge</h2>
<p>Virtual node agents<a id="_idIndexMarker237"/> reside in the cloud in this approach, but the abstract of nodes <a id="_idIndexMarker238"/>and Pods is deployed at the edge. Edge nodes carrying containers are given command control via virtual node agents.</p>
<p>Although there are others, Microsoft's Virtual Kubelet project is a nice example of a kubelet agent with a Kubernetes API extension. Virtual Kubelet is a Kubernetes agent that runs in a remote environment and registers itself as a cluster node. To build a node resource on the cluster, the agent uses the Kubernetes API. It uses the notions of taints and tolerations to schedule Pods in an external environment by calling its native API:</p>
<div>
<div class="IMG---Figure" id="_idContainer064">
<img alt="Figure 4.4 – Microsoft Virtual Kubelet " height="877" src="image/Figure_4.4_B18115.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Microsoft Virtual Kubelet</p>
<p>Virtual Kubelet works<a id="_idIndexMarker239"/> with providers such as AWS Fargate control plane, Azure Container<a id="_idIndexMarker240"/> Instances, and Azure IoT Edge. Following is a list of current providers (as of this writing).</p>
<h3>Current list of Virtual Kubelet providers</h3>
<p>In this section, we'll take a look<a id="_idIndexMarker241"/> at some of the Virtual Kubelet providers:</p>
<ul>
<li><strong class="bold">AWS Fargate</strong>: Your Kubernetes cluster is connected to an AWS Fargate cluster using the AWS Fargate<a id="_idIndexMarker242"/> virtual-kubelet provider. The Fargate cluster appears as a virtual node with the CPU and memory resources you choose </li>
</ul>
<p>Pods scheduled on the virtual node execute on Fargate in the same way as they would on a regular Kubernetes node.</p>
<ul>
<li><strong class="bold">Admiralty Multi-Cluster Scheduler</strong>: Admiralty is a Kubernetes controller system that schedules<a id="_idIndexMarker243"/> workloads intelligently among clusters. It's easy to use and integrates with other applications.</li>
<li><strong class="bold">Alibaba Cloud Elastic Container Instance</strong> (<strong class="bold">ECI</strong>): The Alibaba ECI provider is an adaptor that connects<a id="_idIndexMarker244"/> your Kubernetes cluster to the ECI service, allowing Pods from a K8s cluster to be implemented on Alibaba's cloud platform.</li>
<li><strong class="bold">Azure Batch</strong>: Azure Batch provides<a id="_idIndexMarker245"/> a distributed HPC computing environment on Azure. Azure Batch<a id="_idIndexMarker246"/> is a service that manages the scheduling of discrete processes and tasks across pools of virtual machines. It's frequently used for batch processing jobs such as rendering.</li>
<li><strong class="bold">Azure Container Instances</strong> (<strong class="bold">ACI</strong>): ACI in Azure provides a hosted environment<a id="_idIndexMarker247"/> for running containers. When you use ACI, you don't have to worry about managing the underlying compute infrastructure because Azure does it for you. When using ACI to run containers, you are charged per second for each container that is running</li>
</ul>
<p>The Virtual Kubelet's ACI provider configures an ACI instance as a node in any Kubernetes cluster. Pods can be scheduled on an ACI instance as if it were a conventional Kubernetes node when utilizing the Virtual Kubelet ACI provider</p>
<p>This setup enables you to benefit from both Kubernetes' capabilities and ACI's management value and cost savings.</p>
<ul>
<li><strong class="bold">Elotl Kip</strong>: Kip is a Virtual Kubelet provider<a id="_idIndexMarker248"/> that enables a Kubernetes cluster to launch Pods on their own cloud instances in a transparent manner. The kip Pod runs on a cluster and creates a virtual Kubernetes node within it</li>
</ul>
<p>Kip starts a right-sized cloud instance for the Pod's workload and sends the Pod to the instance when a Pod is scheduled on the Virtual Kubelet. The cloud instance is terminated once the Pod has finished operating. These cloud instances<a id="_idIndexMarker249"/> are referred to as <em class="italic">cells</em></p>
<p>When workloads run on Kip, your cluster size naturally scales with the cluster workload, Pods are strongly isolated from each other, and the user is freed from managing worker nodes and strategically packing Pods onto nodes. This results in lower cloud costs, improved security, and simpler operational overhead.</p>
<ul>
<li><strong class="bold">Kubernetes Container Runtime Interface</strong> (<strong class="bold">CRI</strong>): The CRI provider implementation should be regarded<a id="_idIndexMarker250"/> as a bare-bones minimal implementation for testing the Virtual Kubelet project's core against real Pods and containers; in other words, it is more extensive than MockProvider</li>
</ul>
<p>This provider implementation<a id="_idIndexMarker251"/> is also built in such a way that it may be used to prototype new architectural features on local Linux infrastructure. If the CRI provider can be demonstrated to run effectively within a Linux guest, it may be assumed that the abstraction will work for other providers as well.</p>
<ul>
<li><strong class="bold">Huawei Cloud Container Instance</strong> (<strong class="bold">CCI</strong>): The Huawei CCI virtual<a id="_idIndexMarker252"/> kubelet provider sets up a CCI project as a node in any Kubernetes cluster, including <strong class="bold">Huawei Cloud Container Engine</strong> (<strong class="bold">CCE</strong>). As a private cluster, CCE provides<a id="_idIndexMarker253"/> native Kubernetes applications and tools, allowing you to quickly build up a container runtime environment. The virtual kubelet provider's scheduled Pod will run in CCI, taking advantage of CCI's high performance.</li>
<li><strong class="bold">HashiCorp Nomad</strong>: By exposing the Nomad cluster<a id="_idIndexMarker254"/> as a node in Kubernetes, the HashiCorp Nomad provider for Virtual Kubelet connects your Kubernetes cluster with the Nomad cluster. Pods scheduled on the virtual Nomad node registered on Kubernetes will run as jobs on Nomad clients, just like they would on a Kubernetes node, if you use the provider.</li>
<li><strong class="bold">Liqo</strong>: Liqo is an on-prem<a id="_idIndexMarker255"/> or managed platform that enables dynamic and decentralized resource sharing among Kubernetes clusters. Liqo makes it possible to launch Pods on a distant cluster without modifying Kubernetes or the apps</li>
</ul>
<p>With Liqo, you can extend a Kubernetes cluster's control plane across cluster boundaries, making multi-clusters native and transparent: collapse a complete remote cluster to a virtual local node, allowing task offloading and resource management in accordance with standard Kubernetes practices.</p>
<ul>
<li><strong class="bold">OpenStack Zun</strong>: Your Kubernetes cluster is connected to an OpenStack cloud through the OpenStack Zun virtual kubelet<a id="_idIndexMarker256"/> provider. Because each Pod is provided with dedicated Neutron ports in your tenant subnets, your OpenStack Pods have access to OpenStack tenant networks.</li>
<li><strong class="bold">Tencent Games tensile-kube</strong>: This allows Kubernetes clusters to collaborate. Tensile-kube<a id="_idIndexMarker257"/> is based on Virtual Kubelet and offers the following features:<ul><li>Cluster resources are discovered automatically.</li>
<li>Pods are notified of changes in real time, reducing the expense of frequent lists.</li>
<li>All kubectl logs and kubectl exec operations are supported.</li>
<li>When utilizing a multi-scheduler, schedule Pods globally to avoid unscheduled Pods owing to resource fragmentation.</li>
<li>If Pods can't be scheduled<a id="_idIndexMarker258"/> in lower clusters, use a descheduler to reschedule them.</li>
<li>Supports PV/PVC and service abstractions.</li>
</ul></li>
</ul>
<h2 id="_idParaDest-65"><a id="_idTextAnchor066"/>Deployment of Kubernetes devices at the edge</h2>
<p>The Kubernetes device plugin framework<a id="_idIndexMarker259"/> is used to expose leaf devices as resources<a id="_idIndexMarker260"/> in a Kubernetes cluster in this approach.</p>
<p>This technique is demonstrated by <strong class="bold">Microsoft Akri</strong>, which exposes a variety <a id="_idIndexMarker261"/>of sensors, controllers, and MCU class leaf devices as resources in a Kubernetes cluster. The Akri project brings the Kubernetes device<a id="_idIndexMarker262"/> plugin concept to the edge, where a variety<a id="_idIndexMarker263"/> of leaf devices use different communication protocols and have sporadic availability:</p>
<div>
<div class="IMG---Figure" id="_idContainer065">
<img alt="Figure 4.5 – Akri architecture " height="647" src="image/Figure_4.5_B18115.jpg" width="1291"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Akri architecture</p>
<p>ONVIF, Udev, and OPC UA discovery handlers are now supported by Akri. More protocol support is being developed.</p>
<p>As seen previously, a variety of deployment approaches indicate how Kubernetes may be utilized for edge workloads, as well as support for the architecture that fulfills enterprise application needs such as low latency, resource constraints, data privacy, and bandwidth scalability, among others.</p>
<p>In the next section, we will look at how Kubernetes is suitable for running edge workloads.</p>
<h1 id="_idParaDest-66"><a id="_idTextAnchor067"/>Propositions that Kubernetes offers</h1>
<p>In terms of resource and workload control, edge-based infrastructure poses various challenges. There would be thousands of edge nodes and distant edge nodes to control in a short amount of time. Organizations' edge architecture is designed to provide more centralized autonomy from the cloud, security standards, and relatively low latency. Take a peek at what Kubernetes<a id="_idIndexMarker264"/> for edge has to offer:</p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<img alt="Table 4.3 – Propositions that Kubernetes offers " height="1774" src="image/B18115_04_Table_4.3.jpg" width="1634"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.3 – Propositions that Kubernetes offers</p>
<p>Kubernetes is a critical component of businesses that are evolving into digital-first enterprises. Kubernetes is currently being deployed in 59% of data centers to increase resource efficiency and offer agility to the software development cycle, according to reports. In a distributed cloud environment, Kubernetes can manage and orchestrate containerized applications as well as legacy virtual machines. Kubernetes can also be used to execute AI/ML and GPU workloads, in addition to pure applications.</p>
<p>Kubernetes is certainly the go-to platform for edge computing, at least for those edge contexts that require dynamic orchestration for apps and centralized administration of workloads, according to the Linux Foundation's <em class="italic">The State of the Edge Report</em>. Kubernetes extends the benefits of cloud-native computing software development to the edge, allowing for flexible and automated management of applications that span a disaggregated cloud environment.</p>
<p>By deploying and testing Kubernetes<a id="_idIndexMarker265"/> at the edge, enterprises and telecom operators can achieve a high level of flexibility, observability, and dynamic orchestration.</p>
<h1 id="_idParaDest-67"><a id="_idTextAnchor068"/>Summary</h1>
<p>As companies embrace digital transformation, Industry 4.0, industrial automation, smart manufacturing, and all the advanced use cases that these initiatives provide, the relevance of Kubernetes, edge, and cloud collaborating to drive intelligent business decisions is becoming clear. We've looked at a different approach that shows how Kubernetes may be used for running edge workloads. In the next chapters, we'll go over the deployment of a whole Kubernetes cluster at the edge approach in depth. Other approaches are beyond the scope of this book.</p>
<p>In the following chapters, we'll go over implementation aspects of common edge computing applications using MicroK8s in detail, such as running your applications on a multi-node Raspberry Pi cluster; configuring load balancing; installing/configuring different CNI plugins for network connectivity; configuring logging, monitoring, and alerting options; and building/deploying ML models and serverless applications. </p>
<p>Also, we will look into setting up storage replication for your stateful applications, implementing a service mesh for cross-cutting concerns, high-availability clusters to withstand a component failure and continue to serve workloads without interruption, the configuration of containers with workload isolation, and running secured containers with isolation from the host system.</p>
</div>
</div>


<div id="sbo-rt-content"><div class="Content" id="_idContainer068">
<h1 id="_idParaDest-68"><a id="_idTextAnchor069"/>Part 3: Running Applications on MicroK8s</h1>
<p>This part focuses on the implementation aspects that are common for any IoT/edge computing applications, such as running your applications on a multi-node Raspberry Pi cluster, installing/configuring different CNI plugins for network connectivity, configuring load balancing, configuring logging, monitoring, alerting options, building, and deploying machine learning models, and serverless applications. </p>
<p>This part of the book comprises the following chapters:</p>
<ul>
<li><a href="B18115_05.xhtml#_idTextAnchor070"><em class="italic">Chapter 5</em></a>, <em class="italic">Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters</em></li>
<li><a href="B18115_06.xhtml#_idTextAnchor085"><em class="italic">Chapter 6</em></a>, <em class="italic">Configuring Connectivity for Containers</em> </li>
<li><a href="B18115_07.xhtml#_idTextAnchor107"><em class="italic">Chapter 7</em></a>, <em class="italic">Setting Up MetalLB and Ingress for Load Balancing</em> </li>
<li><a href="B18115_08.xhtml#_idTextAnchor121"><em class="italic">Chapter 8</em></a>, <em class="italic">Monitoring the Health of Infrastructure and Applications</em></li>
<li><a href="B18115_09.xhtml#_idTextAnchor136"><em class="italic">Chapter 9</em></a>, <em class="italic">Using Kubeflow to Run AI/MLOps Workloads</em></li>
<li><a href="B18115_10.xhtml#_idTextAnchor157"><em class="italic">Chapter 10</em></a>, <em class="italic">Going Serverless with Knative and OpenFaaS Frameworks</em></li>
</ul>
</div>
</div>
</body></html>