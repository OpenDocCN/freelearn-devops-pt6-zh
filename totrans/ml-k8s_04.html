<html><head></head><body>
		<div id="_idContainer124">
			<h1 id="_idParaDest-70"><em class="italic"><a id="_idTextAnchor069"/>Chapter 5</em>: Data Engineering</h1>
			<p>Data engineering, in general, refers to the management and organization of data and data flows across an organization. It involves data gathering, processing, versioning, data governance, and analytics. It is a huge topic that revolves around the development and maintenance of data processing platforms, data lakes, data marts, data warehouses, and data streams. It is an important practice that contributes to the success<a id="_idIndexMarker332"/> of <strong class="bold">big data</strong> and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) projects. In this chapter, you will<a id="_idIndexMarker333"/> learn about the ML-specific topics of data engineering.</p>
			<p>A sizable number of ML tutorials/books start with a clean dataset and a CSV file to build your model against. The real world is different. Data comes in many shapes and sizes, and it is important that you have a well-defined strategy to harvest, process, and prepare data at scale. This chapter will discuss open source tools that can provide the foundations for data engineering in ML projects. You will learn how to install the open source toolsets on the Kubernetes platform and how these tools will enable you and your team to be more efficient and agile.</p>
			<p> In this chapter, you will learn about the following topics:</p>
			<ul>
				<li>Configuring Keycloak for authentication</li>
				<li>Configuring Open Data Hub components</li>
				<li>Understanding and using the JupyterHub IDE </li>
				<li>Understanding the basics of Apache Spark</li>
				<li>Understanding how Open Data Hub provisions on-demand Apache Spark clusters</li>
				<li>Writing and running a Spark application from Jupyter Notebook</li>
			</ul>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor070"/>Technical requirements</h1>
			<p>This chapter includes some hands-on setup and exercises. You will need a running Kubernetes cluster configured with <strong class="bold">Operator Lifecycle Manager</strong>. Building such a Kubernetes environment is covered in <a href="B18332_03_ePub.xhtml#_idTextAnchor040"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring Kubernetes</em>. Before attempting the technical exercises in this chapter, please make sure that you have a working Kubernetes cluster and <strong class="bold">Open Data Hub</strong> (<strong class="bold">ODH</strong>) installed on your Kubernetes cluster. Installing the ODH is covered in <a href="B18332_04_ePub.xhtml#_idTextAnchor055"><em class="italic">Chapter 4</em></a>, <em class="italic">The Anatomy of a Machine Learning Platform</em>. You can find all the code associated with this book at <a href="https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes">https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes</a>.</p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor071"/>Configuring Keycloak for authentication</h1>
			<p>Before you <a id="_idIndexMarker334"/>start using any component of your platform, you need to configure the authentication system to be associated with the platform components. As mentioned in <a href="B18332_04_ePub.xhtml#_idTextAnchor055"><em class="italic">Chapter 4</em></a>, <em class="italic">The Anatomy of a Machine Learning Platform</em>, you will use Keycloak, an open source software to provide authentication services.</p>
			<p>As a first step, import the configuration from <strong class="source-inline">chapter5/realm-export.json</strong>, which is available in the code repository associated with this book. This file contains the configuration required to associate the OAuth2 capabilities for the platform components.</p>
			<p>Though this book is not a Keycloak guide by any means, we will provide some basic definitions for you to understand the high-level taxonomy of the Keycloak server: </p>
			<ul>
				<li><strong class="bold">Realm</strong>: A Keycloak<a id="_idIndexMarker335"/> realm is an object that manages the users, roles, groups, and client applications that belong to the same domain. One Keycloak server can have multiple realms, so you have multiple sets of configurations, such as one realm for internal applications and one for external applications.</li>
				<li><strong class="bold">Clients</strong>: Clients are<a id="_idIndexMarker336"/> entities that can request user authentication. A Keycloak client object is <a id="_idIndexMarker337"/>associated with a realm. All the applications in our platform that require <strong class="bold">single sign-on</strong> (<strong class="bold">SSO</strong>) will be registered as <strong class="bold">clients</strong> in the Keycloak server.</li>
				<li><strong class="bold">Users and groups</strong>: These two <a id="_idIndexMarker338"/>terms are self-explanatory, and you will be creating a new user in the following steps and using it to log into different software<a id="_idIndexMarker339"/> of the platform.</li>
			</ul>
			<p>The next step is to configure Keycloak to provide OAuth capabilities to our ML platform component. </p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor072"/>Importing the Keycloak configuration for the ODH components</h2>
			<p>In this<a id="_idIndexMarker340"/> section, you will import the<a id="_idIndexMarker341"/> clients and group configurations onto the Keycloak server running on your Kubernetes cluster. The following steps will import everything onto the master realm of the Keycloak server:</p>
			<ol>
				<li value="1">Log in to your Keycloak server using the username <strong class="source-inline">admin</strong> and the password <strong class="source-inline">admin</strong>. Click on the <strong class="bold">Import</strong> link on the left-hand sidebar under the <strong class="bold">Manage</strong> heading:</li>
			</ol>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B18332_05_001.jpg" alt="Figure 5.1 – Keycloak Master realm&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Keycloak Master realm</p>
			<ol>
				<li value="2">Click on the <strong class="bold">Select file</strong> button on the screen, as follows:</li>
			</ol>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B18332_05_002.jpg" alt="Figure 5.2 – Keycloak import configuration page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Keycloak import configuration page</p>
			<ol>
				<li value="3">Select<a id="_idIndexMarker342"/> the <strong class="source-inline">chapter5/realm-export.json</strong> file<a id="_idIndexMarker343"/> from the pop-up window. After that, select <strong class="bold">Skip</strong> for the <strong class="bold">If a resource exists</strong> drop-down options, and click <strong class="bold">Import</strong>:</li>
			</ol>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B18332_05_003.jpg" alt="Figure 5.3 – Keycloak import configuration page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Keycloak import configuration page</p>
			<ol>
				<li value="4">Validate <a id="_idIndexMarker344"/>that<a id="_idIndexMarker345"/> the records have been imported successfully onto your Keycloak server:</li>
			</ol>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B18332_05_004.jpg" alt="Figure 5.4 – Keycloak import configuration results page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – Keycloak import configuration results page</p>
			<ol>
				<li value="5">Validate<a id="_idIndexMarker346"/> that<a id="_idIndexMarker347"/> there are four clients created by clicking on the <strong class="bold">Clients</strong> item on the left-hand side menu. The following client IDs should exist: <strong class="bold">aflow</strong>, <strong class="bold">mflow</strong>, <strong class="bold">grafana</strong>, and <strong class="bold">jhub</strong>. The <strong class="bold">aflow</strong> client is for the<a id="_idIndexMarker348"/> workflow engine of the platform, which is an instance of <strong class="bold">Apache Airflow</strong>. The <strong class="bold">mflow</strong> client is for the model registry and training tracker tool and is an<a id="_idIndexMarker349"/> instance of <strong class="bold">MLflow</strong>. The <strong class="bold">grafana</strong> client is for monitoring UI and<a id="_idIndexMarker350"/> is an instance of <strong class="bold">Grafana</strong>. And last, the <strong class="bold">jhub</strong> client<a id="_idIndexMarker351"/> is for the <strong class="bold">JupyterHub</strong> server instance.</li>
			</ol>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B18332_05_005.jpg" alt="Figure 5.5 – Keycloak clients page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – Keycloak clients page</p>
			<ol>
				<li value="6">Validate that <a id="_idIndexMarker352"/>a <a id="_idIndexMarker353"/>group called <strong class="bold">ml-group</strong> has been created by clicking on the <strong class="bold">Groups</strong> link on the left-hand panel: </li>
			</ol>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B18332_05_006.jpg" alt="Figure 5.6 – Keycloak groups page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – Keycloak groups page</p>
			<p>You will use this user group to create a user of the platform.</p>
			<p>Great work! You have just configured multiple Keycloak clients for the ML platform. The next step is to create a user in Keycloak that you will be using for the rest of this book. It is important to note that Keycloak can be hooked with your enterprise directory or any other<a id="_idIndexMarker354"/> database<a id="_idIndexMarker355"/> and to use them as a source of the users. Keep in mind that the realm configuration we are using here is very basic and is not recommended for production use.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor073"/>Creating a Keycloak user</h2>
			<p>In this section, you <a id="_idIndexMarker356"/>will create a new user and associate the newly created user with the group imported in the preceding section. Associating the user with the group gives the roles required for the different ODH software:</p>
			<ol>
				<li value="1">On the left-hand side of the Keycloak page, click on the <strong class="bold">Users</strong> link to come to this page. To add a new user, click the <strong class="bold">Add user</strong> button on the right:</li>
			</ol>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B18332_05_007.jpg" alt="Figure 5.7 – Keycloak users list&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – Keycloak users list</p>
			<ol>
				<li value="2">Add the username <strong class="source-inline">mluser</strong> and make sure the <strong class="bold">User Enabled</strong> and <strong class="bold">Email Verified</strong> toggle buttons are set to <strong class="bold">ON</strong>. In <strong class="bold">Groups</strong>, select the <strong class="bold">ml-group</strong> group and fill in the <strong class="bold">Email</strong>, <strong class="bold">First Name</strong>, and <strong class="bold">Last Name</strong> fields, as shown in <em class="italic">Figure 5.8</em>, and then<a id="_idIndexMarker357"/> hit the <strong class="bold">Save</strong> button:</li>
			</ol>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B18332_05_008.jpg" alt="Figure 5.8 – Keycloak Add user page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.8 – Keycloak Add user page</p>
			<ol>
				<li value="3">Click on the <strong class="bold">Credentials</strong> tab to set the password for your user:</li>
			</ol>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B18332_05_009.jpg" alt="Figure 5.9 – Keycloak Credentials page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9 – Keycloak Credentials page</p>
			<ol>
				<li value="4">Type in the password of your choice, then disable the <strong class="bold">Temporary</strong> flag, and hit the <strong class="bold">Set Password</strong> button.</li>
			</ol>
			<p>You have just created and configured a user in Keycloak. Your Keycloak server is now ready to be used by the ML platform components. The next step is to explore the component of<a id="_idIndexMarker358"/> the platform that provides the main coding environment for all personas in the ML project.</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor074"/>Configuring ODH components</h1>
			<p>In <a href="B18332_04_ePub.xhtml#_idTextAnchor055"><em class="italic">Chapter 4</em></a>, <em class="italic">The Anatomy of a Machine Learning Platform</em>, you have installed the ODH operator. Using<a id="_idIndexMarker359"/> the ODH operator, you will now configure an instance of ODH that will automatically install the components of the ML platform. ODH <a id="_idIndexMarker360"/>executes <strong class="bold">Kustomize</strong> scripts to install the components of the ML platform. As part of the code for this book, we have provided templates to install and configure all the components required to run the platform.</p>
			<p>You can also configure what components ODH operators install for you through a <strong class="source-inline">manifests</strong> file. You can pass the specific configuration to the manifests and choose the components you need. One such manifest is available in the code repository of the book at <strong class="source-inline">manifests/kfdef/ml-platform.yaml</strong>. This YAML file is configured for the ODH operator to do its magic and install the software we need to be part of the platform. You will need to make some modifications to this file, as you will see in the following section.</p>
			<p>This file defines the components of your platform and the location from where these components will get their settings: </p>
			<ul>
				<li><strong class="bold">Name</strong>: Defines the name of the component.</li>
				<li><strong class="bold">repoRef</strong>: This section contains the <strong class="source-inline">path</strong> property where you define the relative path location of the files required to configure this component.</li>
				<li><strong class="bold">Parameters</strong>: This section contains the parameters that will be used to configure the component. Note that, in the following example, the IP address for <strong class="source-inline">KEYCLOAK_URL</strong> and <strong class="source-inline">JUPYTERHUB_HOST</strong> will need to be changed as per your configuration.</li>
				<li><strong class="bold">Overlays</strong>: The ODH <a id="_idIndexMarker361"/>operator contains a default set of configurations for each component. Overlays provide a way to further tune the default configuration. The list of overlays is a set of folders, under the same location as the manifest file. The ODH operator will read the files from these overlay folders and merge them on the fly to produce a final configuration. You can find the overlays for JupyterHub in the <strong class="source-inline">manifests/jupytherhub/overlays</strong> folder in the code repository.</li>
				<li><strong class="bold">Repos</strong>: This configuration section is specific to each manifest file and applies to all the components in the manifest. It defines the location and version of the Git repository that contains all the files being referred to by this manifest file. If you want the manifest to reference your own files for the installation, you need to refer here to the right Git repository (the repository that contains your files).</li>
			</ul>
			<p><em class="italic">Figure 5.10</em> shows the part of the manifest file that holds the definition of the JupyterHub component:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B18332_05_010.jpg" alt="Figure 5.10 – A component in the ODH manifest file&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.10 – A component in the ODH manifest file</p>
			<p>You will use the provided manifest file to create an instance of the ML platform. You may also tweak configurations or add or remove components of the platform as you wish by modifying this file. However, for the exercises in the book, we do not recommend <a id="_idIndexMarker362"/>changing this unless you are instructed to do so.</p>
			<p>Now that you have seen the ODH manifest file, it's time to make good use of it to create your first ML platform on Kubernetes.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor075"/>Installing ODH</h2>
			<p>Before we can<a id="_idIndexMarker363"/> install the data engineering components of the platform, we first need to create an instance of ODH. An ODH instance is a curated collection of related toolsets that serve as the components of an ML platform. Although the ML platform may contain components other than what is provided by ODH, it is fair to say that an instance of ODH is an instance of the ML platform. You may also run multiple instances of ODH on the same Kubernetes cluster as long as they run on their own isolated Kubernetes namespaces. This is useful when multiple teams or departments in your organization are sharing a single Kubernetes cluster.</p>
			<p>The following are the steps you need to follow to create an instance of ODH on your Kubernetes cluster:</p>
			<ol>
				<li value="1">Create a new namespace in your Kubernetes cluster using the following command:<p class="source-code"><strong class="bold">kubectl create ns ml-workshop</strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B18332_05_011.jpg" alt="Figure 5.11 – New namespace in your Kubernetes cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.11 – New namespace in your Kubernetes cluster</p>
			<ol>
				<li value="2">Make<a id="_idIndexMarker364"/> sure that the ODH operator is running by issuing the following command:<p class="source-code"><strong class="bold">kubectl get pods -n operators</strong></p></li>
			</ol>
			<p>You should see the following response. Make sure the status says <strong class="source-inline">Running</strong>:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B18332_05_012.jpg" alt="Figure 5.12 – Status of the ODH operator &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.12 – Status of the ODH operator </p>
			<ol>
				<li value="3">Get the IP address of your <strong class="source-inline">minikube</strong> environment. This IP address will be used to create ingress for different components of the platform the same way we did for Keycloak. Note that your IP may be different for each <strong class="source-inline">minikube</strong> instance depending on your underlying infrastructure:<p class="source-code"><strong class="bold">minikube ip</strong></p></li>
			</ol>
			<p>This command should give you the IP address of your <strong class="source-inline">minikube</strong> cluster.</p>
			<ol>
				<li value="4">Open the <strong class="source-inline">manifests/kfdef/ml-platform.yaml</strong> file and change the value of the following parameters to a NIP (<strong class="source-inline">nip.io</strong>) domain name of your <strong class="source-inline">minikube</strong> instance. Only replace the IP address part of the domain name. For example, <strong class="source-inline">KEYCLOAK_URL keycloak.&lt;IP Address&gt;.nip.io</strong> should become <strong class="source-inline">keycloak.192.168.61.72.nip.io</strong>. Note that these parameters may be referenced in more than one place in this file. In a full Kubernetes environment, <strong class="source-inline">&lt;IP Address&gt;</strong> should<a id="_idIndexMarker365"/> be the domain name of your Kubernetes cluster:<ol><li><strong class="source-inline">KEYCLOAK_URL</strong></li><li><strong class="source-inline">JUPYTERHUB_HOST</strong></li><li><strong class="source-inline">AIRFLOW_HOST</strong></li><li><strong class="source-inline">MINIO_HOST</strong></li><li><strong class="source-inline">MLFLOW_HOST</strong></li><li><strong class="source-inline">GRAFANA_HOST</strong></li></ol></li>
				<li>Apply the manifest file to your Kubernetes cluster using the following command:<p class="source-code"><strong class="bold">kubectl create -f manifests/kfdef/ml-platform.yaml -n ml-workshop</strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B18332_05_013.jpg" alt="Figure 5.13 – Result from applying manifests for the ODH components&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.13 – Result from applying manifests for the ODH components</p>
			<ol>
				<li value="6">Start watching the pods being created in the <strong class="source-inline">ml-workshop</strong> namespace by using the following command. It will take a while for all the components to be installed. After several minutes, all the Pods will be in a running state. While the pods are being created, you may see some pods throw errors. This is normal because some pods are dependent on other pods. Be patient as all the components come together and the pods will come into a running state:<p class="source-code"><strong class="bold">watch kubectl get pods -n ml-workshop</strong></p></li>
			</ol>
			<p>You should see the following response when all the pods are running:</p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B18332_05_014.jpg" alt="Figure 5.14 – CLI response showing the ODH components running on the Kubernetes cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.14 – CLI response showing the ODH components running on the Kubernetes cluster</p>
			<p>So, what <a id="_idIndexMarker366"/>does this command do? The <strong class="bold">Open Data Hub</strong> (<strong class="bold">ODH</strong>) operator consumed the <strong class="source-inline">kfdef</strong> <strong class="bold">Custom Resource Definition</strong> (<strong class="bold">CRD</strong>) that you have created in <em class="italic">Step 5</em>. The operator then goes through each of the application objects in the CRD and creates the required Kubernetes objects to run these applications. The Kubernetes objects created in your cluster include several Deployments, Pods, Services, Ingresses, ConfigMaps, Secrets, and PersistentVolumeClaims. You may also run the following command to see all the objects created in the <strong class="source-inline">ml-workshop</strong> namespace:</p>
			<p class="source-code"><strong class="bold">kubectl get all -n ml-workshop</strong></p>
			<p>You should see all the objects that were created in the <strong class="source-inline">ml-workshop</strong> namespace by the ODH operator.</p>
			<p>Congratulations! You have just created a fresh instance of ODH. Now that you have seen the process of creating an instance of the ML platform from a manifest file, it is time to take a look at each of the components of the platform that the data engineers will use for their <a id="_idIndexMarker367"/>activities.</p>
			<p class="callout-heading">Minikube Using Podman Driver</p>
			<p class="callout">Note<a id="_idIndexMarker368"/> that<a id="_idIndexMarker369"/> for some <strong class="source-inline">minikube</strong> setups that use <strong class="source-inline">podman</strong> drivers in Linux, the Spark operator may fail due to the limit of the number of threads. To solve this problem, you need to use a <strong class="source-inline">kvm2</strong> driver in your <strong class="source-inline">minikube</strong> configuration. You can do this by adding the <strong class="source-inline">--driver=kvm2</strong> parameter to your <strong class="source-inline">minikube start</strong> command.</p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor076"/>Understanding and using JupyterHub</h1>
			<p>Jupyter <a id="_idIndexMarker370"/>Notebook<a id="_idIndexMarker371"/> has become an extremely popular tool for writing code for ML projects. JupyterHub is a software that facilitates the self-service provisioning of computing environments that includes spinning up pre-configured Jupyter Notebook servers and provisioning the associated compute resources on the Kubernetes platform. On-demand end users such as data engineers and data scientists can provision their own instances of Jupyter Notebook dedicated only to them. If a requesting user already has his/her own running instance of Jupyter Notebook, the hub will just direct the user to the existing instance, avoiding duplicated environments. From the end user's perspective, the whole interaction is seamless. You will see this in the next section of this chapter.</p>
			<p>When a user requests an environment in JupyterHub, they are also given the option to choose a pre-configured sizing of hardware resources such as CPU, memory, and storage. This allows for a flexible way for developers, data engineers, and data scientists to provision just the right amount of computing resources for a given task. This dynamic allocation of resources is facilitated by the underlying Kubernetes platform.</p>
			<p>Different users may require different frameworks, libraries, and flavors of coding environments. Some data scientists may want to use TensorFlow while others want to use scikit-learn or PyTorch. Some data engineers may prefer to use pandas while some may need to run their data pipelines in PySpark. In JupyterHub, they can configure multiple pre-defined environments for such scenarios. Users can then select a predefined configuration when they request a new environment. These predefined environments are actually container images. This means that the platform operator or platform administrator can prepare several predefined container images that will serve as the end user's computing environment. This feature also enables the standardization of environments. How many times do you have to deal with different versions of the libraries on different developer computers? The standardization of environments can reduce the number of problems related to library version inconsistencies and generally reduce the <em class="italic">it works on my machine</em> issues.</p>
			<p><em class="italic">Figure 5.15</em> shows<a id="_idIndexMarker372"/> the three-step process of provisioning a new JupyterHub<a id="_idIndexMarker373"/> environment: </p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B18332_05_015.jpg" alt="Figure 5.15 – Workflow for creating a new environment in JupyterHub&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.15 – Workflow for creating a new environment in JupyterHub</p>
			<p>Now that you know what JupyterHub can do, let's see it in action.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor077"/>Validating the JupyterHub installation</h2>
			<p>Every data <a id="_idIndexMarker374"/>engineer in the team follows a simple and standard workflow of provisioning an environment. No more manual installations and fiddling with their workstation configurations. This is great for autonomous teams and will definitely help improve your team's velocity.</p>
			<p>The ODH operator has already installed the JupyterHub for you in the previous sections. Now, you will spin up a new Jupyter Notebook environment, as a data engineer, and write your data pipelines:</p>
			<ol>
				<li value="1">Get the ingress objects created in your Kubernetes environment using the following command. We are running this command to find the URL of JupyterHub:<p class="source-code"><strong class="bold">kubectl get ingress -n ml-workshop</strong></p></li>
			</ol>
			<p>You should see the following example response. Take note of the JupyterHub URL as displayed in the <strong class="source-inline">HOSTS</strong> column:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B18332_05_016.jpg" alt="Figure 5.16 – All ingresses in your cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.16 – All ingresses in your cluster</p>
			<ol>
				<li value="2">Open a browser from the same machine where minikube is running and navigate to the JupyterHub URL. The URL looks like https://jupyterhub.&lt;MINIKUBE IP ADDRESS&gt;.nip.io. This URL will take you to the <a id="_idIndexMarker375"/>Keycloak login page to perform SSO authentication. Make sure that you replace the IP address with your minikube IP address in this URL:</li>
			</ol>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B18332_05_017.jpg" alt="Figure 5.17 – SSO challenge for JupyterHub&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.17 – SSO challenge for JupyterHub</p>
			<ol>
				<li value="3">Type <strong class="source-inline">mluser</strong> for the username, then type whatever password you have set up for this user, and click <strong class="bold">Sign In</strong>.</li>
			</ol>
			<p>You will<a id="_idIndexMarker376"/> see the landing page of the JupyterHub server where it allows you to select the notebook container image that you want to use and also a predefined size of computing resources you need.</p>
			<p>The notebook image section contains the standard notebooks that you have provisioned using the ODH manifests from the <strong class="source-inline">manifests/jupyterhub-images</strong> folder of the code repository.</p>
			<p>The container size drop-down allows you to select the right size of environment for your need. This configuration is also controlled via the <strong class="source-inline">manifests/jupyterhub/jupyterhub/overlays/mlops/jupyterhub-singleuser-profiles-sizes-configmap.yaml</strong> manifest file.</p>
			<p>We encourage you to look into these files to familiarize yourself with what configuration you can set for each manifest.</p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B18332_05_018.jpg" alt="Figure 5.18 – JupyterHub landing page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.18 – JupyterHub landing page</p>
			<p>Select <strong class="bold">Base Elyra Notebook Image</strong> and <a id="_idIndexMarker377"/>the <strong class="bold">Default</strong> container size and hit <strong class="bold">Start server</strong>.</p>
			<ol>
				<li value="4">Validate that a new pod has been created for your user by issuing the following command. Jupyter Notebook instance names start with <strong class="source-inline">jupyter-nb-</strong> and are suffixed with the username of the user. This allows for a unique name of notebook pods for each user:<p class="source-code"><strong class="bold">kubectl get pods -n ml-workshop | grep mluser </strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B18332_05_019.jpg" alt="Figure 5.19 – Jupyter Notebook pod created by JupyterHub&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.19 – Jupyter Notebook pod created by JupyterHub</p>
			<ol>
				<li value="5">Congratulations! You are now running your own self-provisioned Jupyter Notebook server on the Kubernetes platform.</li>
			</ol>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B18332_05_020.jpg" alt="Figure 5.20 – Jupyter Notebook landing page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.20 – Jupyter Notebook landing page</p>
			<ol>
				<li value="6">Now, let's <a id="_idIndexMarker378"/>stop the notebook server. Click on the <strong class="bold">File &gt; Hub Control Panel </strong>menu option to go to the <strong class="bold">Hub Control Panel</strong> page shown as follows:</li>
			</ol>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B18332_05_021.jpg" alt="Figure 5.21 – Menu option to see the Hub Control Panel&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.21 – Menu option to see the Hub Control Panel</p>
			<ol>
				<li value="7">Click on <a id="_idIndexMarker379"/>the <strong class="bold">Stop My Server</strong> button. This is how you stop your instance of Jupyter Notebook. You may want to start it back again later for the next steps.</li>
			</ol>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B18332_05_022.jpg" alt="Figure 5.22 – Hub Control Panel&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.22 – Hub Control Panel</p>
			<ol>
				<li value="8">Validate that a new pod has been destroyed for your user by issuing the following command:<p class="source-code"><strong class="bold">kubectl get pods -n ml-workshop | grep mluser </strong></p></li>
			</ol>
			<p>There should be no output for this command because the Jupyter Notebook pod has been destroyed by JupyterHub.</p>
			<p>We leave it up to you to explore the different bits of the configuration of the notebook in your environment. You will write code using this Jupyter notebook in the later sections of <a id="_idIndexMarker380"/>this chapter and the next few chapters of this book, so if you just want to continue reading, you will not miss anything.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor078"/>Running your first Jupyter notebook</h2>
			<p>Now that your Jupyter <a id="_idIndexMarker381"/>notebook is running, it is time to write the <strong class="source-inline">Hello World!</strong> program. In the code repository of this book, we have provided one such program, and in the following steps, you will check out the code using Git and run the program. Before we start these steps, make sure that you can access your Jupyter notebook using the browser, as mentioned in the preceding section:</p>
			<ol>
				<li value="1">Click on the Git icon on the left-hand side menu on your Jupyter notebook. The icon is the third from the top. It will display three buttons for different operations. Click on the <strong class="bold">Clone a Repository</strong> button:</li>
			</ol>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B18332_05_024.jpg" alt="Figure 5.23 – Git operations in the Jupyter notebook&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.23 – Git operations in the Jupyter notebook</p>
			<ol>
				<li value="2">In the <strong class="bold">Clone a repo</strong> pop-up <a id="_idIndexMarker382"/>box, type in the location of the code repository of this book, <a href="https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git">https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git</a>, and hit <strong class="bold">CLONE</strong>.</li>
			</ol>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B18332_05_025.jpg" alt="Figure 5.24 – Git clone a repo in the Jupyter notebook&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.24 – Git clone a repo in the Jupyter notebook</p>
			<ol>
				<li value="3">You will see<a id="_idIndexMarker383"/> that the code repository is cloned onto your Jupyter notebook's file system. As shown in <em class="italic">Figure 5.25</em>, navigate to the <strong class="source-inline">chapter5/helloworld.ipynb</strong> file and open it in your notebook. Click on the little play icon on the top bar to run the cell:</li>
			</ol>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B18332_05_026.jpg" alt="Figure 5.25 – Notebook on your Jupyter environment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.25 – Notebook on your Jupyter environment</p>
			<ol>
				<li value="4">Et voila! You have just executed a Python code in your own self-provisioned Jupyter Notebook server running on Kubernetes.</li>
				<li>Shut down your notebook by selecting the <strong class="bold">File &gt; Hub Control Panel</strong> menu option. Click on the <strong class="bold">Stop My Server</strong> button to shut down your environment. Note that ODH will save your disk and next time you start your notebook, all your saved files will be available.</li>
			</ol>
			<p>Congratulations! Now, you <a id="_idIndexMarker384"/>can run your code on the platform. Next, we'll get some basics refreshed for the Apache Spark engine.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor079"/>Understanding the basics of Apache Spark</h1>
			<p>Apache Spark is an open<a id="_idIndexMarker385"/> source data <a id="_idIndexMarker386"/>processing engine designed for distributed large-scale processing of data. This means that if you have smaller datasets, say 10s or even a few 100s of GB, a tuned traditional database may provide faster processing times. The main differentiator for Apache Spark is its capability to perform in-memory intermediate computations, which makes Apache Spark much faster than Hadoop MapReduce.</p>
			<p>Apache Spark is built for speed, flexibility, and ease of use. Apache Spark offers more than 70 high-level data processing operators that make it easy for data engineers to build data applications, so it is easy to write data processing logic using Apache Spark APIs. Being flexible means that Spark works as a unified data processing engine and works on several types of data workloads such as batch applications, streaming applications, interactive queries, and even ML algorithms.</p>
			<p><em class="italic">Figure 5.26</em> shows the Apache Spark components:</p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B18332_05_027.jpg" alt="Figure 5.26 – Apache Spark components&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.26 – Apache Spark components</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor080"/>Understanding Apache Spark job execution</h2>
			<p>Most data engineers<a id="_idIndexMarker387"/> now know that Apache Spark is a massively parallel data processing engine. It is one of the most successful projects of the Apache Software Foundation. Spark traditionally runs on a cluster of multiple <strong class="bold">virtual machines</strong> (<strong class="bold">VMs</strong>) or<a id="_idIndexMarker388"/> bare metal servers. However, with the popularity of containers and Kubernetes, Spark added support for running Spark clusters on containers on Kubernetes.</p>
			<p>There are two most common ways of running Spark on Kubernetes. The first, and the native way, is by using the Kubernetes engine itself to orchestrate the Kubernetes worker pods. In this approach, the Spark cluster instance is always running and the Spark applications are submitted to the Kubernetes API that will schedule the submitted application. We will not dig deeper into how this is implemented. The second approach is through Kubernetes operators. Operators take advantage of the Kubernetes CRDs to create Spark objects natively in Kubernetes. In this approach, the Spark cluster is created on the fly by the Spark operator. Instead of submitting a Spark application to an existing cluster, the operator spins up spark clusters on-demand.</p>
			<p>A Spark cluster follows a manager/worker architecture. The Spark cluster manager knows where the workers are located, and the resources available for the worker. The Spark cluster manages the resources for the cluster of worker nodes where your application will run. Each worker has one or more executors that run the assigned jobs through an executor.</p>
			<p>Spark applications have two parts, the driver component, and the data processing logic. A driver component is responsible for executing the flow of data processing operations. The driver run first talks to the cluster manager to find out what worker nodes will run the application logic. The driver transforms all the application operations into tasks, schedules them, and assigns tasks directly to the executor processes on the worker node. One executor can run multiple tasks that are associated with the same Spark context.</p>
			<p>If your application requires you to collect the computed result and merge them, the driver is the one who will be responsible for this activity. As a data engineer, all this activity is abstracted from you via the SparkSession object. You only need to write the data processing logic. Did we mention Apache Spark aims to be simple?</p>
			<p><em class="italic">Figure 5.27</em> shows the relationship between the Spark driver, Spark cluster manager, and Spark worker<a id="_idIndexMarker389"/> nodes:</p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B18332_05_028.jpg" alt="Figure 5.27 – Relationship between Apache Spark components&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.27 – Relationship between Apache Spark components</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor081"/>Understanding how ODH provisions Apache Spark cluster on-demand</h1>
			<p>We have<a id="_idIndexMarker390"/> talked <a id="_idIndexMarker391"/>about how the ODH allows you to create a dynamic and flexible development environment to write code such as data pipelines using Jupyter Notebook. We have noticed that data developers need to interact with IT to get time on the data processing clusters such as Apache Spark. These interactions reduce the agility of the team, and this is one of the problems the ML platform solves. To adhere to this scenario, ODH provides the following components:</p>
			<ul>
				<li>A Spark operator that spawns the Apache Spark cluster. For this book, we have forked the original Spark operator provided by ODH and radanalytics to adhere to the latest changes to the Kubernetes API.</li>
				<li>A capability in JupyterHub to issue a request for a new Spark cluster to the Spark operator when certain notebook environments are created by the user.</li>
			</ul>
			<p>As a data engineer, when you spin up a new notebook environment using certain notebook images, JupyterHub not only spawns a new notebook server, it also creates the Apache Spark cluster dedicated for you through the Spark operator.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor082"/>Creating a Spark cluster</h2>
			<p>Let's first see <a id="_idIndexMarker392"/>how the Spark operator works on the Kubernetes cluster. ODH creates the Spark controller. You can see the configuration in the <strong class="source-inline">chapter5/ml-platform.yaml</strong> file under the name <strong class="source-inline">radanalyticsio-spark-cluster</strong>, as shown in <em class="italic">Figure 5.28</em>. You can see this is another set of Kubernetes YAML files that <a id="_idIndexMarker393"/>defines the <strong class="bold">custom resource definitions</strong> (<strong class="bold">CRDs</strong>), required roles, and the Spark operator deployment. All these files are in the <strong class="source-inline">manifests/radanalyticsio</strong> folder in the code repository of this book.</p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B18332_05_029.jpg" alt="Figure 5.28 – Snippet of the section of the manifest that installs the Spark operator&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.28 – Snippet of the section of the manifest that installs the Spark operator</p>
			<p>When you need to spin up an Apache Spark cluster, you can do this by creating a Kubernetes custom resource <a id="_idIndexMarker394"/>called <strong class="bold">SparkCluster</strong>. Upon receiving the request, the Spark operator will provision a new Spark cluster as per the required configuration. The following steps will show you the steps for provisioning a Spark cluster on your platform:</p>
			<ol>
				<li value="1">Validate<a id="_idIndexMarker395"/> that the Spark operator pod is running:<p class="source-code"><strong class="bold">kubectl get pods -n ml-workshop | grep spark-operator</strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B18332_05_030.jpg" alt="Figure 5.29 – Spark operator pod&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.29 – Spark operator pod</p>
			<ol>
				<li value="2">Create a simple Spark cluster with one worker node using the file available at <strong class="source-inline">chapter5/simple-spark-cluster.yaml</strong>. You can see that this file is requesting a Spark cluster with one master and one worker node. Through this custom resource, you can set several Spark configurations, as we shall see in the next section:</li>
			</ol>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B18332_05_031.jpg" alt="Figure 5.30 – Spark custom resource&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.30 – Spark custom resource</p>
			<p>Create this Spark cluster custom resource in your Kubernetes cluster by running the following command. The Spark operator constantly scans for this resource in the Kubernetes platform and automatically creates a new instance of Apache Spark cluster<a id="_idIndexMarker396"/> for each given Spark cluster custom resource:</p>
			<p class="source-code"><strong class="bold">kubectl create -f chapter5/simple-spark-cluster.yaml -n ml-workshop</strong></p>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B18332_05_032.jpg" alt="Figure 5.31 – Response to creating a Spark cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.31 – Response to creating a Spark cluster</p>
			<ol>
				<li value="3">Validate that the Spark cluster pods are running in your cluster:<p class="source-code"><strong class="bold">kubectl get pods -n ml-workshop | grep simple-spark</strong></p></li>
			</ol>
			<p>You should see the following response. There are two pods created by the Spark operator, one for the Spark master node and another for the worker node. The number of worker pods depends on the value of the <strong class="source-inline">instances</strong> parameters in the <strong class="source-inline">SparkCluster</strong> resource. It may take some time for the pods to come to a running state the first time:</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B18332_05_033.jpg" alt="Figure 5.32 – List of running Spark cluster pods &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.32 – List of running Spark cluster pods </p>
			<p>Now, you know how the Spark operator works on the Kubernetes cluster. The next step is to see how JupyterHub is configured to request the cluster dynamically while provisioning<a id="_idIndexMarker397"/> a new notebook for you.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor083"/>Understanding how JupyterHub creates a Spark cluster</h2>
			<p>Simply put, JupyterHub <a id="_idIndexMarker398"/>does what you did in <a id="_idIndexMarker399"/>the preceding section. JupyterHub creates a <strong class="source-inline">SparkCluster</strong> resource in Kubernetes so that the Spark operator can provision the Apache Spark cluster for your use. This <strong class="source-inline">SparkCluster</strong> resource configuration is a Kubernetes <strong class="source-inline">ConfigMap</strong> file and can be found at <strong class="source-inline">manifests/jupyterhub/jupyterhub/base/jupyterhub-spark-operator-configmap.yaml</strong>. Look for <strong class="source-inline">sparkClusterTemplate</strong> in this file, as shown in <em class="italic">Figure 5.33</em>. You can see that it looks like the file you have created in the previous section:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B18332_05_034.jpg" alt="Figure 5.33 – JupyterHub template for Spark resources&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.33 – JupyterHub template for Spark resources</p>
			<p>Some of <a id="_idIndexMarker400"/>you <a id="_idIndexMarker401"/>might have noticed that this is a template, and it needs the values for specific variables mentioned in this template. Variables such as <strong class="source-inline">{{ user }}</strong> and <strong class="source-inline">{{ worker_nodes }}</strong> and so on. Recall that we have mentioned that JupyterHub creates the <strong class="source-inline">SparkCluster</strong> request while it is provisioning a container for your notebook. JupyterHub uses this file as a template and fills in the values while creating your notebook. How does JupyterHub decide to create a Spark cluster? This configuration is <a id="_idIndexMarker402"/>called <strong class="bold">profiles</strong> and is available as a <strong class="source-inline">ConfigMap</strong> file in <strong class="source-inline">manifests/jupyterhub/jupyterhub/overlays/spark3/jupyterhub-singleuser-profiles-configmap.yaml</strong>. This looks like the file shown in <em class="italic">Figure 5.33</em>.</p>
			<p>You can see that the <strong class="source-inline">image</strong> field specifies the name of the container image on which this profile will be triggered. So, as a data engineer, when you select this notebook image from the JupyterHub landing page, JupyterHub will apply this profile. The second thing in the profile is the <strong class="source-inline">env</strong> section, which specifies the environment variables that will be pushed to the notebook container instance. The <strong class="source-inline">configuration</strong> object defines the values <a id="_idIndexMarker403"/>that<a id="_idIndexMarker404"/> will be applied to the template that is mentioned in the <strong class="source-inline">resources</strong> key:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B18332_05_035.jpg" alt="Figure 5.34 – JupyterHub profile for Spark resources&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.34 – JupyterHub profile for Spark resources</p>
			<p>As you may appreciate, there is a lot of work done behind the scenes to make a streamlined experience for you and your team, and in the true sense of open source, you can configure everything and even give back to the project if you come up with any modifications <a id="_idIndexMarker405"/>or <a id="_idIndexMarker406"/>new features.</p>
			<p>In the next section, you will see how easy it is to write and run a Spark application on your platform running these components.</p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor084"/>Writing and running a Spark application from Jupyter Notebook</h1>
			<p>Before you <a id="_idIndexMarker407"/>run the following<a id="_idIndexMarker408"/> steps, make sure that you grasped the components and their interactions that we have introduced in the previous section of this chapter:</p>
			<ol>
				<li value="1">Validate that the Spark operator pod is running by running the following command:<p class="source-code"><strong class="bold">kubectl get pods -n ml-workshop | grep spark-operator</strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B18332_05_036.jpg" alt="Figure 5.35 – Spark operator pod&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.35 – Spark operator pod</p>
			<ol>
				<li value="2">Validate that the JupyterHub pod is running by running the following command:<p class="source-code"><strong class="bold">kubectl get pods -n ml-workshop | grep jupyterhub</strong></p></li>
			</ol>
			<p>You should see the following response:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B18332_05_037.jpg" alt="Figure 5.36 – JupyterHub pod&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.36 – JupyterHub pod</p>
			<ol>
				<li value="3">Before you start the notebook, let's delete the Spark cluster you have created in the previous sections by running the following command. This is to demonstrate that JupyterHub will automatically create a new instance of Spark cluster for you:<p class="source-code"><strong class="bold">kubectl delete sparkcluster simple-spark-cluster -n ml-workshop</strong></p></li>
				<li>Log in to your JupyterHub server. Refer to the <em class="italic">Validating JupyterHub configuration</em> section earlier in this chapter. You will get the landing page of your server. Select the <strong class="bold">Elyra Notebook Image with Spark</strong> image and the <strong class="bold">Small</strong> container<a id="_idIndexMarker409"/> size. This<a id="_idIndexMarker410"/> is the same image that you have configured in the <strong class="source-inline">manifests/jupyterhub/jupyterhub/overlays/spark3/jupyterhub-singleuser-profiles-configmap.yaml</strong> file. </li>
				<li>Click on <strong class="bold">Start server</strong>:</li>
			</ol>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B18332_05_038.jpg" alt="Figure 5.37 – JupyterHub landing page showing Elyra Notebook Image with Spark&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.37 – JupyterHub landing page showing Elyra Notebook Image with Spark</p>
			<p>The notebook you have just started will also trigger the creation of a dedicated Spark cluster for you. It may take some time for the notebook to start because it has to wait for the Spark cluster to be ready.</p>
			<p>Also, you may have noticed that the image you have configured in the <strong class="source-inline">jupyterhub-singleuser-profiles-configmap.yaml</strong> file is <strong class="source-inline">quay.io/ml-aml-workshop/elyra-spark:0.0.4</strong>, while the name we have selected is <strong class="bold">Elyra Notebook Image with Spark</strong>, and they are not the same. The mapping of the image with the descriptive text is configured in the <strong class="source-inline">manifests/jupyterhub-images/elyra-notebook-spark3-imagestream.yaml</strong> file. You will find the descriptive text displayed on the JupyterHub<a id="_idIndexMarker411"/> landing<a id="_idIndexMarker412"/> page coming from the <em class="italic">annotations</em> section of this file. If you want to add your own images with specific libraries, you can just add another file here and it will be available for your team. This feature of JupyterHub enables the standardization of notebook container images, which allows everyone in the teams to have the same environment configurations and the same set of libraries.</p>
			<ol>
				<li value="6">After the notebook has started, validate that the Spark cluster is provisioned for you. Note that this is the Spark cluster for the user of this notebook and is dedicated to this user only:<p class="source-code"><strong class="bold">kubectl get pods -n ml-workshop | grep mluser</strong></p></li>
			</ol>
			<p>You should see the following response. The response contains a notebook pod and two Spark pods; the one with a little <strong class="source-inline">-m</strong> character is the master, while the other is the worker. Notice how your username (<strong class="source-inline">mluser</strong>) is associated with the pod names:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B18332_05_039.jpg" alt="Figure 5.38 – Jupyter Notebook and Spark cluster pods&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.38 – Jupyter Notebook and Spark cluster pods</p>
			<p>Now, everyone in your team will get their own developer environment with dedicated Spark instances to write and test the data processing code.</p>
			<ol>
				<li value="7">Apache Spark provides a UI through which you can monitor applications and data processing jobs. The ODH-provisioned Spark cluster provides this GUI, and it<a id="_idIndexMarker413"/> is <a id="_idIndexMarker414"/>available at <strong class="source-inline">https://spark-cluster-mluser.192.168.61.72.nip.io</strong>. Make sure to change the IP address to your minikube IP address. You may also notice that the username you have used to log in to JupyterHub, <strong class="source-inline">mluser</strong>, is part of the URL. If you have used a different username, you may need to adjust the URL accordingly.</li>
			</ol>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B18332_05_040.jpg" alt="Figure 5.39 – Spark UI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.39 – Spark UI</p>
			<p>The preceding UI mentions that you have one worker in the cluster, and you can click on the worker node to find out the executors running inside the worker node. If you want to refresh your knowledge of the Spark cluster, please refer to the <em class="italic">Understanding the basics of Apache Spark</em> section earlier in this chapter.</p>
			<ol>
				<li value="8">Open the <strong class="source-inline">chapter5/hellospark.ipynb</strong> file from your notebook. This is quite a simple job that calculates the square of the given array. Remember that Spark will <a id="_idIndexMarker415"/>automatically schedule the job and distribute it among executors. The notebook<a id="_idIndexMarker416"/> here is the Spark Driver program, which talks to the Spark cluster, and all of this is abstracted via the <strong class="source-inline">SparkSession</strong> object.</li>
			</ol>
			<p>On the second code cell of this notebook, you are creating a <strong class="source-inline">SparkSession</strong> object. The <strong class="source-inline">getOrCreateSparkSession</strong> utility function will connect to the Spark cluster provisioned for you by the platform.</p>
			<p>The last cell is where your data processing logic resides. In this example, the logic was to take the data and calculate the square of each element in the array. Once the data is processed, the <strong class="source-inline">collect</strong> method will bring the response to the driver that is running in the Spark application in your notebook.</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B18332_05_041.jpg" alt="Figure 5.40 – A notebook with a simple Spark application&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.40 – A notebook with a simple Spark application</p>
			<p>Click on the <strong class="bold">Run &gt; Run All cells</strong> menu option, and the notebook will connect to the <a id="_idIndexMarker417"/>Spark <a id="_idIndexMarker418"/>cluster, and submit and execute your job.</p>
			<ol>
				<li value="9">While the job is progressing, open the Spark UI at <a href="https://spark-cluster-mluser.192.168.61.72.nip.io">https://spark-cluster-mluser.192.168.61.72.nip.io</a>. Remember to adjust the IP address as per your settings, and click on the table with the <strong class="bold">Application ID</strong> heading under the <strong class="bold">Running Applications</strong> heading on this page.</li>
			</ol>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B18332_05_042.jpg" alt="Figure 5.41 – Apache Spark UI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.41 – Apache Spark UI</p>
			<ol>
				<li value="10">Navigate to the details page of the Spark application. Note that the application title, <strong class="bold">Hello from ODH</strong>, has <a id="_idIndexMarker419"/>been<a id="_idIndexMarker420"/> set up in your notebook. Click on the <strong class="bold">Application Detail UI</strong> link:</li>
			</ol>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B18332_05_043.jpg" alt="Figure 5.42 – Spark UI showing the submitted Spark job&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.42 – Spark UI showing the submitted Spark job</p>
			<p>You should see a page showing the detailed metrics of the job that you have just executed on the Spark cluster from your Jupyter notebook:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B18332_05_044.jpg" alt="Figure 5.43 – Spark UI showing the submitted job details&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.43 – Spark UI showing the submitted job details</p>
			<ol>
				<li value="11">Once you <a id="_idIndexMarker421"/>are<a id="_idIndexMarker422"/> done with your work, go to the <strong class="bold">File &gt; Hub Control Panel</strong> menu option and click on the <strong class="bold">Stop My Server</strong> button:</li>
			</ol>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B18332_05_045.jpg" alt="Figure 5.44 – Jupyter Notebook control panel&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.44 – Jupyter Notebook control panel</p>
			<ol>
				<li value="12">Validate that the Spark cluster has been terminated by issuing the following command:<p class="source-code"><strong class="bold">kubectl get pods -n ml-workshop | grep mluser</strong></p></li>
			</ol>
			<p>You should not see a response because the pods are terminated by the Spark operator on your cluster.</p>
			<p>You have finally run a basic data processing job in an on-demand ephemeral Spark cluster that is running on Kubernetes. Note that you have done all this from a Jupyter notebook running on Kubernetes.</p>
			<p>With this capability in the platform, data engineers can perform huge data processing tasks directly <a id="_idIndexMarker423"/>from <a id="_idIndexMarker424"/>the browser. This capability also allows them to collaborate easily with each other to provide transformed, cleaned, high-quality data for your ML project.</p>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor085"/>Summary</h1>
			<p>In this chapter, you have just created your first ML platform. You have configured the ODH components via the ODH Kubernetes operator. You have seen how a data engineer persona will use JupyterHub to provision the Jupyter notebook and the Apache Spark cluster instance while the platform provides the provisioning of the environments automatically. You have also seen how the platform enables standardization of the operating environment via the container images, which bring consistency and security. You have seen how a data engineer could run Apache Spark jobs from the Jupyter notebook.</p>
			<p>All these capabilities allow the data engineer to work autonomously and in a self-serving fashion. You have seen that all these components were available autonomously and on-demand. The elastic and self-serving nature of the platform will allow teams to be more productive and agile while responding to the ever-changing requirements of the data and the ML world.</p>
			<p>In the next chapter, you will see how data scientists can benefit from the platform and be more efficient.</p>
		</div>
	</body></html>