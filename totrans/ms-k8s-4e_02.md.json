["```\nbrew install --cask rancher \n```", "```\nchoco install rancher-desktop \n```", "```\n$ docker version\nClient:\n Version:           20.10.9\n API version:       1.41\n Go version:        go1.16.8\n Git commit:        c2ea9bc\n Built:             Thu Nov 18 21:17:06 2021\n OS/Arch:           darwin/arm64\n Context:           rancher-desktop\n Experimental:      true\nServer:\n Engine:\n  Version:          20.10.14\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.17.9\n  Git commit:       87a90dc786bda134c9eb02adbae2c6a7342fb7f6\n  Built:            Fri Apr 15 00:05:05 2022\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          v1.5.11\n  GitCommit:        3df54a852345ae127d1fa3092b95168e4a88e2f8\n runc:\n  Version:          1.0.2\n  GitCommit:        52b36a2dd837e8462de8e01458bf02cf9eea47dd\n docker-init:\n  Version:          0.19.0\n  GitCommit: \n```", "```\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.4\", GitCommit:\"e6c093d87ea4cbb530a7b2ae91e54c0842d8308a\", GitTreeState:\"clean\", BuildDate:\"2022-02-16T12:38:05Z\", GoVersion:\"go1.17.7\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.6+k3s1\", GitCommit:\"418c3fa858b69b12b9cefbcff0526f666a6236b9\", GitTreeState:\"clean\", BuildDate:\"2022-04-28T22:16:58Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"linux/arm64\"} \n```", "```\n$ kubectl get pods -n kube-system\nNAME                                      READY   STATUS    RESTARTS         AGE\nsvclb-traefik-fv84n                       2/2     Running   6 (7d20h ago)    8d\nlocal-path-provisioner-84bb864455-s2xmp   1/1     Running   20 (7d20h ago)   27d\nmetrics-server-ff9dbcb6c-lsffr            0/1     Running   88 (10h ago)     27d\ncoredns-d76bd69b-mc6cn                    1/1     Running   11 (22h ago)     8d\ntraefik-df4ff85d6-2fskv                   1/1     Running   7 (3d ago)       8d \n```", "```\nPS C:\\Windows\\system32> choco install minikube -y\nChocolatey v0.12.1\nInstalling the following packages:\nminikube\nBy installing, you accept licenses for the packages.\nProgress: Downloading Minikube 1.25.2... 100%\nkubernetes-cli v1.24.0 [Approved]\nkubernetes-cli package files install completed. Performing other installation steps.\nExtracting 64-bit C:\\ProgramData\\chocolatey\\lib\\kubernetes-cli\\tools\\kubernetes-client-windows-amd64.tar.gz to C:\\ProgramData\\chocolatey\\lib\\kubernetes-cli\\tools...\nC:\\ProgramData\\chocolatey\\lib\\kubernetes-cli\\tools\nExtracting 64-bit C:\\ProgramData\\chocolatey\\lib\\kubernetes-cli\\tools\\kubernetes-client-windows-amd64.tar to C:\\ProgramData\\chocolatey\\lib\\kubernetes-cli\\tools...\nC:\\ProgramData\\chocolatey\\lib\\kubernetes-cli\\tools\n ShimGen has successfully created a shim for kubectl-convert.exe\n ShimGen has successfully created a shim for kubectl.exe\n The install of kubernetes-cli was successful.\n  Software installed to 'C:\\ProgramData\\chocolatey\\lib\\kubernetes-cli\\tools'\nMinikube v1.25.2 [Approved]\nminikube package files install completed. Performing other installation steps.\n ShimGen has successfully created a shim for minikube.exe\n The install of minikube was successful.\n  Software installed to 'C:\\ProgramData\\chocolatey\\lib\\Minikube'\nChocolatey installed 2/2 packages.\n See the log for details (C:\\ProgramData\\chocolatey\\logs\\chocolatey.log). \n```", "```\nchoco install microsoft-windows-terminal --pre \n```", "```\nfunction k { kubectl.exe $args } function mk { minikube.exe $args } \n```", "```\nalias k='kubectl.exe'\nalias mk=minikube.exe' \n```", "```\n$ mk version\nminikube version: v1.25.2\ncommit: 362d5fdc0a3dbee389b3d3f1034e8023e72bd3a7 \n```", "```\n$ mk start\n![](img/B18998_02_001.png)  minikube v1.25.2 on Microsoft Windows 10 Pro 10.0.19044 Build 19044\n![](img/B18998_02_002.png)  Automatically selected the docker driver. Other choices: hyperv, ssh\n![](img/B18998_02_003.png)  Starting control plane node minikube in cluster minikube\n![](img/B18998_02_004.png)  Pulling base image ...\n![](img/B18998_02_005.png)  Downloading Kubernetes v1.23.3 preload ...\n    > preloaded-images-k8s-v17-v1...: 505.68 MiB / 505.68 MiB  100.00% 3.58 MiB\n    > gcr.io/k8s-minikube/kicbase: 379.06 MiB / 379.06 MiB  100.00% 2.61 MiB p/\n![](img/B18998_02_006.png)  Creating docker container (CPUs=2, Memory=8100MB) ...\n![](img/B18998_02_007.png)  docker \"minikube\" container is missing, will recreate.\n![](img/B18998_02_006.png)  Creating docker container (CPUs=2, Memory=8100MB) ...\n![](img/B18998_02_009.png)  Downloading VM boot image ...\n    > minikube-v1.25.2.iso.sha256: 65 B / 65 B [-------------] 100.00% ? p/s 0s\n    > minikube-v1.25.2.iso: 237.06 MiB / 237.06 MiB [ 100.00% 12.51 MiB p/s 19s\n![](img/B18998_02_003.png)  Starting control plane node minikube in cluster minikube\n![](img/B18998_02_006.png)  Creating hyperv VM (CPUs=2, Memory=6000MB, Disk=20000MB) ...\n![](img/B18998_02_012.png)  This VM is having trouble accessing https://k8s.gcr.io\n![](img/B18998_02_013.png)  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networki\nng/proxy/\n![](img/B18998_02_014.png)  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...\n    ▪ kubelet.housekeeping-interval=5m\n    ▪ Generating certificates and keys ...\n    ▪ Booting up control plane ...\n    ▪ Configuring RBAC rules ...\n![](img/B18998_02_015.png)  Verifying Kubernetes components...\n    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5\n![](img/B18998_02_016.png)  Enabled addons: storage-provisioner, default-storageclass\n![](img/B18998_02_017.png)  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default \n```", "```\n$ mk status\nminikube\ntype: Control Plane\nhost: Running\nkubelet: Running\napiserver: Running\nkubeconfig: Configured \n```", "```\n$ mk stop\n![](img/B18998_02_018.png) Stopping node \"minikube\" ... \n![](img/B18998_02_019.png) Powering off \"minikube\" via SSH ... \n![](img/B18998_02_019.png) 1 node stopped. \n```", "```\n$ time mk start\n![](img/B18998_02_001.png)  minikube v1.25.2 on Microsoft Windows 10 Pro 10.0.19044 Build 19044\n![](img/B18998_02_002.png)  Using the hyperv driver based on existing profile\n![](img/B18998_02_003.png)  Starting control plane node minikube in cluster minikube\n![](img/B18998_02_024.png)  Restarting existing hyperv VM for \"minikube\" ...\n![](img/B18998_02_025.png)  This VM is having trouble accessing https://k8s.gcr.io\n![](img/B18998_02_026.png)  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networki\nng/proxy/\n![](img/B18998_02_014.png)  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...\n    ▪ kubelet.housekeeping-interval=5m\n![](img/B18998_02_015.png)  Verifying Kubernetes components...\n    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5\n![](img/B18998_02_016.png)  Enabled addons: storage-provisioner, default-storageclass\n![](img/B18998_02_030.png)  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\nreal    1m8.666s\nuser    0m0.004s\nsys     0m0.000s \n```", "```\n$ brew install minikube\nRunning `brew update --preinstall`...\n==> Auto-updated Homebrew!\nUpdated 2 taps (homebrew/core and homebrew/cask).\n==> Updated Formulae\nUpdated 39 formulae.\n==> New Casks\ncontour                                                        hdfview                         rancher-desktop | kube-system\n==> Updated Casks\nUpdated 17 casks.\n==> Downloading https://ghcr.io/v2/homebrew/core/kubernetes-cli/manifests/1.24.0\n######################################################################## 100.0%\n==> Downloading https://ghcr.io/v2/homebrew/core/kubernetes-cli/blobs/sha256:e57f8f7ea19d22748d1bcae5cd02b91e71816147712e6dcd\n==> Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sha256:e57f8f7ea19d22748d1bcae5cd02b91e71816147\n######################################################################## 100.0%\n==> Downloading https://ghcr.io/v2/homebrew/core/minikube/manifests/1.25.2\nAlready downloaded: /Users/gigi.sayfan/Library/Caches/Homebrew/downloads/fa0034afe1330adad087a8b3dc9ac4917982d248b08a4df4cbc52ce01d5eabff--minikube-1.25.2.bottle_manifest.json\n==> Downloading https://ghcr.io/v2/homebrew/core/minikube/blobs/sha256:6dee5f22e08636346258f4a6daa646e9102e384ceb63f33981745d\nAlready downloaded: /Users/gigi.sayfan/Library/Caches/Homebrew/downloads/ceeab562206fd08fd3b6523a85b246d48d804b2cd678d76cbae4968d97b5df1f--minikube--1.25.2.arm64_monterey.bottle.tar.gz\n==> Installing dependencies for minikube: kubernetes-cli\n==> Installing minikube dependency: kubernetes-cli\n==> Pouring kubernetes-cli--1.24.0.arm64_monterey.bottle.tar.gz\n![](img/B18998_02_031.png)  /opt/homebrew/Cellar/kubernetes-cli/1.24.0: 228 files, 55.3MB\n==> Installing minikube\n==> Pouring minikube--1.25.2.arm64_monterey.bottle.tar.gz\n==> Caveats\nzsh completions have been installed to:\n  /opt/homebrew/share/zsh/site-functions\n==> Summary\n![](img/B18998_02_031.png)  /opt/homebrew/Cellar/minikube/1.25.2: 9 files, 70.3MB\n==> Running `brew cleanup minikube`...\nDisable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\nHide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n==> Caveats\n==> minikube\nzsh completions have been installed to:\n  /opt/homebrew/share/zsh/site-functions \n```", "```\nalias k='kubectl'\nalias mk='$(brew --prefix)/bin/minikube' \n```", "```\n$ mk version\nminikube version: v1.25.2\ncommit: 362d5fdc0a3dbee389b3d3f1034e8023e72bd3a7 \n```", "```\n$ k version\nI0522 15:41:13.663004   68055 versioner.go:58] invalid configuration: no configuration has been provided\nClient Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.4\", GitCommit:\"e6c093d87ea4cbb530a7b2ae91e54c0842d8308a\", GitTreeState:\"clean\", BuildDate:\"2022-02-16T12:38:05Z\", GoVersion:\"go1.17.7\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nThe connection to the server localhost:8080 was refused - did you specify the right host or port? \n```", "```\n$ tree ~/.minikube\\ -L 2\nC:\\Users\\the_g\\.minikube\\\n|-- addons\n|-- ca.crt\n|-- ca.key\n|-- ca.pem\n|-- cache\n|   |-- iso\n|   |-- kic\n|   `-- preloaded-tarball\n|-- cert.pem\n|-- certs\n|   |-- ca-key.pem\n|   |-- ca.pem\n|   |-- cert.pem\n|   `-- key.pem\n|-- config\n|-- files\n|-- key.pem\n|-- logs\n|   |-- audit.json\n|   `-- lastStart.txt\n|-- machine_client.lock\n|-- machines\n|   |-- minikube\n|   |-- server-key.pem\n|   `-- server.pem\n|-- profiles\n|   `-- minikube\n|-- proxy-client-ca.crt\n`-- proxy-client-ca.key\n13 directories, 16 files \n```", "```\n$ mk ssh\n                         _             _\n            _         _ ( )           ( )\n  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __\n/' _ ` _ `\\| |/' _ `\\| || , <  ( ) ( )| '_`\\  /'__`\\\n| ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )(  ___/\n(_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____)\n$ uname -a\nLinux minikube 4.19.202 #1 SMP Tue Feb 8 19:13:02 UTC 2022 x86_64 GNU/Linux\n$ \n```", "```\n$ logout \n```", "```\n$ k cluster-info\nKubernetes control plane is running at https://172.26.246.89:8443\nCoreDNS is running at https://172.26.246.89:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. \n```", "```\n$ k get nodes\nNAME       STATUS   ROLES                  AGE   VERSION\nminikube   Ready    control-plane,master   62m   v1.23.3 \n```", "```\nk describe node minikube \n```", "```\n mk addons list\n|-----------------------------|----------|--------------|--------------------------------|\n|         ADDON NAME          | PROFILE  |    STATUS    |           MAINTAINER           |\n|-----------------------------|----------|--------------|--------------------------------|\n| ambassador                  | minikube | disabled     | third-party (ambassador)       |\n| auto-pause                  | minikube | disabled     | google                         |\n| csi-hostpath-driver         | minikube | disabled     | kubernetes                     |\n| dashboard                   | minikube | disabled     | kubernetes                     |\n| default-storageclass        | minikube | enabled ![](img/B18998_02_033.png)  | kubernetes                     |\n| efk                         | minikube | disabled     | third-party (elastic)          |\n| freshpod                    | minikube | disabled     | google                         |\n| gcp-auth                    | minikube | disabled     | google                         |\n| gvisor                      | minikube | disabled     | google                         |\n| helm-tiller                 | minikube | disabled     | third-party (helm)             |\n| ingress                     | minikube | disabled     | unknown (third-party)          |\n| ingress-dns                 | minikube | disabled     | google                         |\n| istio                       | minikube | disabled     | third-party (istio)            |\n| istio-provisioner           | minikube | disabled     | third-party (istio)            |\n| kong                        | minikube | disabled     | third-party (Kong HQ)          |\n| kubevirt                    | minikube | disabled     | third-party (kubevirt)         |\n| logviewer                   | minikube | disabled     | unknown (third-party)          |\n| metallb                     | minikube | disabled     | third-party (metallb)          |\n| metrics-server              | minikube | disabled     | kubernetes                     |\n| nvidia-driver-installer     | minikube | disabled     | google                         |\n| nvidia-gpu-device-plugin    | minikube | disabled     | third-party (nvidia)           |\n| olm                         | minikube | disabled     | third-party (operator          |\n|                             |          |              | framework)                     |\n| pod-security-policy         | minikube | disabled     | unknown (third-party)          |\n| portainer                   | minikube | disabled     | portainer.io                   |\n| registry                    | minikube | disabled     | google                         |\n| registry-aliases            | minikube | disabled     | unknown (third-party)          |\n| registry-creds              | minikube | disabled     | third-party (upmc enterprises) |\n| storage-provisioner         | minikube | enabled ![](img/B18998_02_033.png)  | google                         |\n| storage-provisioner-gluster | minikube | disabled     | unknown (third-party)          |\n| volumesnapshots             | minikube | disabled     | kubernetes                     |\n|-----------------------------|----------|--------------|--------------------------------| \n```", "```\n$ k create deployment echo --image=k8s.gcr.io/e2e-test-images/echoserver:2.5 \ndeployment.apps/echo created \n```", "```\n$ k get po -w\nNAME                    READY   STATUS              RESTARTS   AGE\necho-7fd7648898-6hh48   0/1     ContainerCreating   0          5s\necho-7fd7648898-6hh48   1/1     Running             0          6s \n```", "```\n$ k expose deployment echo --type=NodePort --port=8080\nservice/echo exposed \n```", "```\n$ mk ip\n172.26.246.89\n$  k get service echo -o jsonpath='{.spec.ports[0].nodePort}'\n32649 \n```", "```\nn$ curl http://172.26.246.89:32649/hi\nHostname: echo-7fd7648898-6hh48\nPod Information:\n        -no pod information available-\nServer values:\n        server_version=nginx: 1.14.2 - lua: 10015\nRequest Information:\n        client_address=172.17.0.1\n        method=GET\n        real path=/hi\n        query=\n        request_version=1.1\n        request_scheme=http\n        request_uri=http://172.26.246.89:8080/hi\nRequest Headers:\n        accept=*/*\n        host=172.26.246.89:32649\n        user-agent=curl/7.79.1\nRequest Body:\n        -no body in request- \n```", "```\n$ mk addons enable dashboard\n    ▪ Using image kubernetesui/dashboard:v2.3.1\n    ▪ Using image kubernetesui/metrics-scraper:v1.0.7\n![](img/B18998_02_026.png)  Some dashboard features require the metrics-server addon. To enable all features please run:\n        minikube addons enable metrics-server\n![](img/B18998_02_016.png)  The 'dashboard' addon is enabled \n```", "```\n$ mk dashboard\n![](img/B18998_02_037.png)  Verifying dashboard health ...\n![](img/B18998_02_038.png)  Launching proxy ...\n![](img/B18998_02_037.png)  Verifying proxy health ...\n![](img/B18998_02_040.png)  Opening http://127.0.0.1:63200/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser... \n```", "```\n$ mk delete\n![](img/B18998_02_006.png)  Deleting \"minikube\" in docker ...\n![](img/B18998_02_042.png)  Deleting container \"minikube\" ...\n![](img/B18998_02_006.png)  Removing /Users/gigi.sayfan/.minikube/machines/minikube ...\n![](img/B18998_02_044.png)  Removed all traces of the \"minikube\" cluster. \n```", "```\ngo install sigs.k8s.io/kind@v0.14.0 \n```", "```\nbrew install kind \n```", "```\nchoco install kind \n```", "```\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? \n```", "```\n$ docker context ls\nNAME              DESCRIPTION                               DOCKER ENDPOINT                                 KUBERNETES ENDPOINT                ORCHESTRATOR\ncolima            colima                                    unix:///Users/gigi.sayfan/.colima/docker.sock\ndefault *         Current DOCKER_HOST based configuration   unix:///var/run/docker.sock                     https://127.0.0.1:6443 (default)   swarm\nrancher-desktop   Rancher Desktop moby context              unix:///Users/gigi.sayfan/.rd/docker.sock       https://127.0.0.1:6443 (default) \n```", "```\n$ docker context use rancher-desktop \n```", "```\n$ kind create cluster\nCreating cluster \"kind\" ...\n ![](img/B18998_02_045.png) Ensuring node image (kindest/node:v1.23.4) ![](img/B18998_02_046.png)\n ![](img/B18998_02_045.png) Preparing nodes ![](img/B18998_02_048.png)\n ![](img/B18998_02_045.png) Writing configuration ![](img/B18998_02_050.png)\n ![](img/B18998_02_051.png) Starting control-plane ![](img/B18998_02_052.png)\n ![](img/B18998_02_051.png) Installing CNI ![](img/B18998_02_054.png)\n ![](img/B18998_02_045.png) Installing StorageClass ![](img/B18998_02_056.png)\nSet kubectl context to \"kind-kind\"\nYou can now use your cluster with:\nkubectl cluster-info --context kind-kind\nThanks for using kind! ![](img/B18998_02_057.png) \n```", "```\n$ k config current-context\nkind-kind\n$ k cluster-info\nKubernetes control plane is running at https://127.0.0.1:51561\nCoreDNS is running at https://127.0.0.1:51561/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. \n```", "```\n$ k get no\nNAME                 STATUS   ROLES                  AGE   VERSION\nkind-control-plane   Ready    control-plane,master   4m   v1.23.4 \n```", "```\n$ kind delete cluster \nDeleting cluster \"kind\" ... \n```", "```\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: multi-node-cluster\nnodes:\n- role: control-plane\n- role: worker\n- role: worker \n```", "```\n$ kind create cluster --config kind-multi-node-config.yaml --kubeconfig $TMPDIR/kind-multi-node-config\nCreating cluster \"multi-node-cluster\" ...\n ![](img/B18998_02_045.png) Ensuring node image (kindest/node:v1.23.4) ![](img/B18998_02_046.png)\n ![](img/B18998_02_060.png) Preparing nodes ![](img/B18998_02_061.png)\n ![](img/B18998_02_060.png) Writing configuration ![](img/B18998_02_063.png)\n ![](img/B18998_02_060.png) Starting control-plane ![](img/B18998_02_052.png)\n ![](img/B18998_02_045.png) Installing CNI ![](img/B18998_02_054.png)\n ![](img/B18998_02_045.png) Installing StorageClass ![](img/B18998_02_005.png)\n ![](img/B18998_02_060.png) Joining worker nodes ![](img/B18998_02_004.png)\nSet kubectl context to \"kind-multi-node-cluster\"\nYou can now use your cluster with:\nkubectl cluster-info --context kind-multi-node-cluster --kubeconfig /var/folders/qv/7l781jhs6j19gw3b89f4fcz40000gq/T//kind-multi-node-config\nHave a nice day! ![](img/B18998_02_072.png) \n```", "```\n$ k get nodes --kubeconfig $TMPDIR/kind-multi-node-config\nNAME                               STATUS   ROLES                  AGE     VERSION\nmulti-node-cluster-control-plane   Ready    control-plane,master   2m17s   v1.23.4\nmulti-node-cluster-worker          Ready    <none>                 100s    v1.23.4\nmulti-node-cluster-worker2         Ready    <none>                 100s    v1.23.4 \n```", "```\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: ha-multi-node-cluster\nnodes:\n- role: control-plane\n- role: control-plane\n- role: control-plane\n- role: worker\n- role: worker \n```", "```\n$ kind create cluster --config kind-ha-multi-node-config.yaml --kubeconfig $TMPDIR/kind-ha-multi-node-config\nCreating cluster \"ha-multi-node-cluster\" ...\n ![](img/B18998_02_060.png) Ensuring node image (kindest/node:v1.23.4) ![](img/B18998_02_046.png)\n ![](img/B18998_02_060.png) Preparing nodes ![](img/B18998_02_076.png)\n ![](img/B18998_02_060.png) Configuring the external load balancer ![](img/B18998_02_078.png)\n ![](img/B18998_02_060.png) Writing configuration ![](img/B18998_02_063.png)\n ![](img/B18998_02_060.png) Starting control-plane ![](img/B18998_02_052.png)\n ![](img/B18998_02_060.png) Installing CNI ![](img/B18998_02_054.png)\n ![](img/B18998_02_045.png) Installing StorageClass ![](img/B18998_02_056.png)\n ![](img/B18998_02_060.png) Joining more control-plane nodes ![](img/B18998_02_088.png)\n ![](img/B18998_02_045.png) Joining worker nodes ![](img/B18998_02_004.png)\nSet kubectl context to \"kind-ha-multi-node-cluster\"\nYou can now use your cluster with:\nkubectl cluster-info --context kind-ha-multi-node-cluster --kubeconfig /var/folders/qv/7l781jhs6j19gw3b89f4fcz40000gq/T//kind-ha-multi-node-config\nNot sure what to do next? ![](img/B18998_02_091.png) Check out https://kind.sigs.k8s.io/docs/user/quick-start/ \n```", "```\n$ k get nodes --kubeconfig $TMPDIR/kind-ha-multi-node-config\nNAME                                   STATUS   ROLES                  AGE     VERSION\nha-multi-node-cluster-control-plane    Ready    control-plane,master   3m31s   v1.23.4\nha-multi-node-cluster-control-plane2   Ready    control-plane,master   3m19s   v1.23.4\nha-multi-node-cluster-control-plane3   Ready    control-plane,master   2m22s   v1.23.4\nha-multi-node-cluster-worker           Ready    <none>                 2m4s    v1.23.4\nha-multi-node-cluster-worker2          Ready    <none>                 2m5s    v1.23.4 \n```", "```\n$ kind get nodes --name ha-multi-node-cluster\nha-multi-node-cluster-control-plane2\nha-multi-node-cluster-external-load-balancer\nha-multi-node-cluster-control-plane\nha-multi-node-cluster-control-plane3\nha-multi-node-cluster-worker\nha-multi-node-cluster-worker2 \n```", "```\n$ k create deployment echo --image=g1g1/echo-server:0.1 --kubeconfig $TMPDIR/kind-ha-multi-node-config\ndeployment.apps/echo created\n$ k expose deployment echo --type=NodePort --port=7070 --kubeconfig $TMPDIR/kind-ha-multi-node-config\nservice/echo exposed \n```", "```\n$ k get svc echo --kubeconfig $TMPDIR/kind-ha-multi-node-config\nNAME   TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE\necho   NodePort   10.96.52.33   <none>        7070:31953/TCP   10s \n```", "```\n$ k proxy --kubeconfig $TMPDIR/kind-ha-multi-node-config &\n[1] 32479\nStarting to serve on 127.0.0.1:8001 \n```", "```\n$ http http://localhost:8001/api/v1/namespaces/default/services/echo:7070/proxy/yeah-it-works\nHTTP/1.1 200 OK\nAudit-Id: 294cf10b-0d60-467d-8a51-4414834fc173\nCache-Control: no-cache, private\nContent-Length: 13\nContent-Type: text/plain; charset=utf-8\nDate: Mon, 23 May 2022 21:54:01 GMT\nyeah-it-works \n```", "```\nbrew install k3d \n```", "```\nchoco install -y k3d \n```", "```\nalias k3d='k3d.exe' \n```", "```\n$ k3d version\nk3d version v5.4.1\nk3s version v1.22.7-k3s1 (default) \n```", "```\n$ time k3d cluster create\nINFO[0000] Prep: Network\nINFO[0000] Created network 'k3d-k3s-default'\nINFO[0000] Created image volume k3d-k3s-default-images\nINFO[0000] Starting new tools node...\nINFO[0000] Starting Node 'k3d-k3s-default-tools'\nINFO[0001] Creating node 'k3d-k3s-default-server-0'\nINFO[0001] Creating LoadBalancer 'k3d-k3s-default-serverlb'\nINFO[0002] Using the k3d-tools node to gather environment information\nINFO[0002] HostIP: using network gateway 172.19.0.1 address\nINFO[0002] Starting cluster 'k3s-default'\nINFO[0002] Starting servers...\nINFO[0002] Starting Node 'k3d-k3s-default-server-0'\nINFO[0008] All agents already running.\nINFO[0008] Starting helpers...\nINFO[0008] Starting Node 'k3d-k3s-default-serverlb'\nINFO[0015] Injecting records for hostAliases (incl. host.k3d.internal) and for 2 network members into CoreDNS configmap...\nINFO[0017] Cluster 'k3s-default' created successfully!\nINFO[0018] You can now use it like this:\nkubectl cluster-info\nreal    0m18.154s\nuser    0m0.005s\nsys     0m0.000s \n```", "```\n$ k3d cluster delete\nINFO[0000] Deleting cluster 'k3s-default'\nINFO[0000] Deleting cluster network 'k3d-k3s-default'\nINFO[0000] Deleting 2 attached volumes...\nWARN[0000] Failed to delete volume 'k3d-k3s-default-images' of cluster 'k3s-default': failed to find volume 'k3d-k3s-default-images': Error: No such volume: k3d-k3s-default-images -> Try to delete it manually\nINFO[0000] Removing cluster details from default kubeconfig...\nINFO[0000] Removing standalone kubeconfig file (if there is one)...\nINFO[0000] Successfully deleted cluster k3s-default! \n```", "```\n$ time k3d cluster create --agents 3\nINFO[0000] Prep: Network\nINFO[0000] Created network 'k3d-k3s-default'\nINFO[0000] Created image volume k3d-k3s-default-images\nINFO[0000] Starting new tools node...\nINFO[0000] Starting Node 'k3d-k3s-default-tools'\nINFO[0001] Creating node 'k3d-k3s-default-server-0'\nINFO[0001] Creating node 'k3d-k3s-default-agent-0'\nINFO[0002] Creating node 'k3d-k3s-default-agent-1'\nINFO[0002] Creating node 'k3d-k3s-default-agent-2'\nINFO[0002] Creating LoadBalancer 'k3d-k3s-default-serverlb'\nINFO[0002] Using the k3d-tools node to gather environment information\nINFO[0002] HostIP: using network gateway 172.22.0.1 address\nINFO[0002] Starting cluster 'k3s-default'\nINFO[0002] Starting servers...\nINFO[0002] Starting Node 'k3d-k3s-default-server-0'\nINFO[0008] Starting agents...\nINFO[0008] Starting Node 'k3d-k3s-default-agent-0'\nINFO[0008] Starting Node 'k3d-k3s-default-agent-2'\nINFO[0008] Starting Node 'k3d-k3s-default-agent-1'\nINFO[0018] Starting helpers...\nINFO[0019] Starting Node 'k3d-k3s-default-serverlb'\nINFO[0029] Injecting records for hostAliases (incl. host.k3d.internal) and for 5 network members into CoreDNS configmap...\nINFO[0032] Cluster 'k3s-default' created successfully!\nINFO[0032] You can now use it like this:\nkubectl cluster-info\nreal    0m32.512s\nuser    0m0.005s\nsys     0m0.000s \n```", "```\n$ k cluster-info\nKubernetes control plane is running at https://0.0.0.0:60490\nCoreDNS is running at https://0.0.0.0:60490/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nMetrics-server is running at https://0.0.0.0:60490/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. \n```", "```\n$ k get nodes\nNAME                       STATUS   ROLES                  AGE     VERSION\nk3d-k3s-default-server-0   Ready    control-plane,master   5m33s   v1.22.7+k3s1\nk3d-k3s-default-agent-0    Ready    <none>                 5m30s   v1.22.7+k3s1\nk3d-k3s-default-agent-2    Ready    <none>                 5m30s   v1.22.7+k3s1\nk3d-k3s-default-agent-1    Ready    <none>                 5m29s   v1.22.7+k3s1 \n```", "```\n$ k3d\nUsage:\n  k3d [flags]\n  k3d [command]\nAvailable Commands:\n  cluster      Manage cluster(s)\n  completion   Generate completion scripts for [bash, zsh, fish, powershell | psh]\n  config       Work with config file(s)\n  help         Help about any command\n  image        Handle container images.\n  kubeconfig   Manage kubeconfig(s)\n  node         Manage node(s)\n  registry     Manage registry/registries\n  version      Show k3d and default k3s version\nFlags:\n  -h, --help         help for k3d\n      --timestamps   Enable Log timestamps\n      --trace        Enable super verbose output (trace logging)\n      --verbose      Enable verbose output (debug logging)\n      --version      Show k3d and default k3s version\nUse \"k3d [command] --help\" for more information about a command. \n```", "```\n$ kubectl config use-context rancher-desktop\nSwitched to context \"rancher-desktop\". \n```", "```\ntype Interface interface {\n     Initialize(clientBuilder ControllerClientBuilder, stop <-chan struct{})\n    LoadBalancer() (LoadBalancer, bool)\n    Instances() (Instances, bool)\n    InstancesV2() (InstancesV2, bool)\n    Zones() (Zones, bool)\n    Clusters() (Clusters, bool)\n    Routes() (Routes, bool)\n    ProviderName() string\n    HasClusterID() bool\n} \n```", "```\ntype Clusters interface {\n    ListClusters(ctx context.Context) ([]string, error)\n    Master(ctx context.Context, clusterName string) (string, error)\n} \n```", "```\nkops create cluster \\\n    --name=${NAME} \\\n    --cloud=aws \\\n    --zones=us-west-2a \\\n    --discovery-store=s3://prefix-example-com-oidc-store/${NAME}/discovery \n```"]