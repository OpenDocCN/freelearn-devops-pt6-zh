<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer274">
    <h1 class="chapterNumber">14</h1>
    <h1 class="chapterTitle" id="_idParaDest-470">Working with Helm Charts and Operators</h1>
    <p class="normal">In the Kubernetes ecosystem, it is very important to manage application redistribution and dependency management in cases when you want your applications to be easily downloadable and installable for customers, or when you want to share them between teams. One of the big differences between Linux package managers like APT or YUM and Kubernetes tools, such as Helm, is that they work universally and don’t depend on a particular Kubernetes distribution. Helm enables the easily distributive creation of applications by means of packaging multiple resources into charts, supporting easy reuse and customization of applications across diverse environments. This relieves the struggle of managing a large number of YAML manifests in application deployments and therefore lightens the load on a developer or an operator.</p>
    <p class="normal">Kubernetes Operators further complement Helm by adding the ability to manage an application’s life cycle in automated ways such as upgrades, failovers, and backups, and by maintaining consistent configurations. Together, Helm and the Operators help with some of the challenges of scaling and managing applications within Kubernetes. This chapter explores these tools in depth: Helm chart development, how to install popular components, and successful redistribution of Kubernetes applications.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Understanding Helm</li>
      <li class="bulletList">Releasing software to Kubernetes using Helm</li>
      <li class="bulletList">Installing Kubernetes Dashboard using Helm Charts</li>
      <li class="bulletList">Introducing Kubernetes Operators</li>
      <li class="bulletList">Enabling Kubernetes monitoring using Prometheus and Grafana</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-471">Technical requirements</h1>
    <p class="normal">For this chapter, you will need the following:</p>
    <ul>
      <li class="bulletList">A Kubernetes cluster deployed. We recommend using a <em class="italic">multi-node, or</em> cloud-based, Kubernetes cluster if possible. You need to ensure CPU, memory, and storage resources are allocated for the Kubernetes cluster to ensure multiple Pods can be scheduled (e.g., you can create a larger minikube cluster using the command <code class="inlineCode">minikube start --kubernetes-version=1.31.0 --cpus=4 --memory=8g</code>).</li>
      <li class="bulletList">The Kubernetes <strong class="keyWord">command-line interface </strong>(<strong class="keyWord">CLI</strong>) (<code class="inlineCode">kubectl</code>) installed on your local machine and configured to manage your Kubernetes cluster.</li>
    </ul>
    <p class="normal">Basic Kubernetes cluster deployment (local and cloud-based) and <code class="inlineCode">kubectl</code> installation have been covered in <em class="chapterRef">Chapter 3</em>, <em class="italic">Installing Your First Kubernetes Cluster</em>. The upcoming chapters, <em class="chapterRef">15</em>, <em class="chapterRef">16</em>, and <em class="chapterRef">17</em>, can give you an overview of how to deploy a fully functional Kubernetes cluster on different cloud platforms. You can download the latest code samples for this chapter from the official GitHub repository: <a href="https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter14&#13;"><span class="url">https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter14</span></a></p>
    <h1 class="heading-1" id="_idParaDest-472">Understanding Helm</h1>
    <p class="normal">This approach is often used as a basic showcase of how you can run a given application as a container on Kubernetes. However, sharing raw YAML manifests has quite a few disadvantages:</p>
    <ul>
      <li class="bulletList">All values in YAML <a id="_idIndexMarker1208"/>templates are <em class="italic">hardcoded</em>. This means that if you want to change the number of replicas of a Service object target or a value stored in the ConfigMap object, you need to go through the manifest files, find the values you want to configure, and then edit them. Similarly, if you want to deploy the manifests to a different namespace in the cluster than the creators intended, you need to edit all YAML files. On top of that, you do not really know which values in the YAML templates are intended to be configurable by the creator unless they document this.</li>
      <li class="bulletList">The deployment process can vary for each application. There is no standardized approach regarding which YAML manifests the creator provides and which components you are required to deploy manually.</li>
      <li class="bulletList">There is no <em class="italic">dependency management</em>. For example, if your application requires a <strong class="keyWord">MySQL</strong> server<a id="_idIndexMarker1209"/> running as a StatefulSet in the cluster, you either need to deploy it yourself or rely on the creator of the application <a id="_idIndexMarker1210"/>to provide YAML manifests for the MySQL server.</li>
    </ul>
    <p class="normal">This is a bit similar to what you see with the other applications if you do not use <strong class="keyWord">Application Store</strong> or a package manager such<a id="_idIndexMarker1211"/> as <strong class="keyWord">Chocolatey</strong>, <strong class="keyWord">APT</strong>, <strong class="keyWord">YUM</strong>, <strong class="keyWord">DNF</strong>, and so <a id="_idIndexMarker1212"/>on. Some<a id="_idIndexMarker1213"/> applications that you download will come with an installer as a <code class="inlineCode">setup.sh</code> script file, some with a <code class="inlineCode">.exe</code> file, some as a <code class="inlineCode">.msi</code>, and others will just be<code class="inlineCode">.zip</code> files that you need to extract and configure yourself.</p>
    <p class="normal">In Kubernetes, you can <a id="_idIndexMarker1214"/>use <strong class="keyWord">Helm</strong> (<a href="https://helm.sh"><span class="url">https://helm.sh</span></a>), which is one of the most popular package managers for Kubernetes applications and services. If you are familiar with popular package managers such as APT, yum, npm, or Chocolatey, you will find many concepts in Helm similar and easy to understand. The following are the three most important concepts in Helm:</p>
    <ul>
      <li class="bulletList">A <strong class="keyWord">chart</strong> is a Helm <em class="italic">package</em>. This is what you <em class="italic">install</em> when you use the Helm <strong class="keyWord">CLI</strong>. A Helm chart<a id="_idIndexMarker1215"/> contains all Kubernetes YAML manifests required to deploy the particular application on the cluster. Please note that these YAML manifests may be <em class="italic">parameterized</em>, so that you can easily inject configuration values provided by the user who installs the chart.</li>
      <li class="bulletList">A <strong class="keyWord">repository</strong> is a <a id="_idIndexMarker1216"/>storage location for Helm charts, used to collect and share charts. They can be public or private – there are multiple public repositories that are available, which you can browse on Artifact Hub (<a href="https://artifacthub.io"><span class="url">https://artifacthub.io</span></a>). Private repositories are usually used for distributing components running on Kubernetes between teams working on the same product.</li>
      <li class="bulletList">A <strong class="keyWord">release</strong> is <a id="_idIndexMarker1217"/>an <em class="italic">instance</em> of a Helm chart that was installed and is running in a Kubernetes cluster. This is what you manage with the Helm CLI, for example, by upgrading or uninstalling it. You can install one chart many times on the same cluster and have multiple releases of it that are identified uniquely by release names.</li>
    </ul>
    <div class="packt_tip">
      <p class="normal">In short, Helm charts contain parameterizable YAML manifests that you store in a Helm repository for distribution. When you install a Helm chart, a Helm release is created in your cluster that you can further manage.</p>
    </div>
    <p class="normal">Let’s quickly summarize some of the common use cases<a id="_idIndexMarker1218"/> for Helm:</p>
    <ul>
      <li class="bulletList">Deploying popular software to your Kubernetes cluster. This makes <em class="italic">development</em> on Kubernetes much easier – you can deploy third-party components to the cluster in a matter of seconds. The same approach may be used in <em class="italic">production</em> clusters. You do not need to rely on writing your own YAML manifest for such third-party components.</li>
      <li class="bulletList">Helm charts provide <em class="italic">dependency management</em> capabilities. If chart A requires chart B to be installed first with specific parameters, Helm supports syntax for this out of the box.</li>
      <li class="bulletList">Sharing your own applications as Helm charts. This can include packaging a product for consumption by the end users or using Helm as an internal package and dependency manager for microservices in your product.</li>
      <li class="bulletList">Ensuring that the applications receive proper upgrades. Helm has its own process for upgrading Helm releases.</li>
      <li class="bulletList">Configuring software deployments for your needs. Helm charts are basically generic YAML templates for Kubernetes object manifests that can be <em class="italic">parameterized</em>. Helm<a id="_idIndexMarker1219"/> uses <strong class="keyWord">Go</strong> templates (<a href="https://godoc.org/text/template"><span class="url">https://godoc.org/text/template</span></a>) for parameterization.</li>
    </ul>
    <p class="normal">Currently, Helm<a id="_idIndexMarker1220"/> is distributed as a binary client (library) that has a CLI similar to <code class="inlineCode">kubectl</code>. All operations that you perform using Helm do not require any additional components to be installed on the Kubernetes cluster.</p>
    <div class="note">
      <p class="normal">Please note that the Helm architecture was changed with the release of version 3.0.0 of Helm. Previously, the architecture of Helm was different, and it required a special, dedicated service running on Kubernetes named Tiller. This was causing various problems, such as with security around <strong class="keyWord">Role-Based Access Control</strong> (<strong class="keyWord">RBAC</strong>) and <a id="_idIndexMarker1221"/>elevated-privilege Pods running inside the cluster. You can read more about the differences between the latest major version of Helm and previous ones in the official FAQ:</p>
      <p class="normal"><span class="url">https://helm.sh/docs/faq/#changes-since-helm-2</span></p>
      <p class="normal">This is useful to know if you find any online guides that still mention Tiller – they are most likely intended for older versions of Helm.</p>
    </div>
    <p class="normal">Now that we have learned about Helm and its important concepts, we are going to install Helm and deploy a simple Helm chart from Artifact Hub to verify that it works correctly on your cluster.</p>
    <h1 class="heading-1" id="_idParaDest-473">Releasing software to Kubernetes using Helm</h1>
    <p class="normal">In this section, you will learn how to install Helm and how to test the installation by deploying an example Helm chart. Helm is provided as binary releases (<a href="https://github.com/helm/helm/releases"><span class="url">https://github.com/helm/helm/releases</span></a>) available for multiple platforms. You can use them or refer to the following guides for installation using a package manager on your desired operating system.</p>
    <h2 class="heading-2" id="_idParaDest-474">Installing Helm on Linux</h2>
    <p class="normal">To install <a id="_idIndexMarker1222"/>Helm on Fedora, you need to ensure that the default <a id="_idIndexMarker1223"/>Fedora repository is configured and working:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">sudo</span> dnf repolist | grep fedora
fedora                                         Fedora 39 – x86_64
</code></pre>
    <p class="normal">Then install Helm as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">sudo</span> dnf install helm
</code></pre>
    <p class="normal">Once installed, you can verify the version of the installed Helm package:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>helm version
version.BuildInfo{Version:"v3.11", GitCommit:"", GitTreeState:"", GoVersion:"go1.21rc3"}
</code></pre>
    <div class="note">
      <p class="normal">It is also possible to install Helm using the script (<span class="url">https://helm.sh/docs/intro/install/#from-script</span>), which will automatically detect the platform, download the latest Helm, and install it on your machine.</p>
    </div>
    <p class="normal">Once installed, you can move on to <em class="italic">Deploying an example chart – WordPress</em> in this section.</p>
    <h2 class="heading-2" id="_idParaDest-475">Installing Helm on Windows</h2>
    <p class="normal">To install <a id="_idIndexMarker1224"/>Helm on Windows, the easiest way is to use the <a id="_idIndexMarker1225"/>Chocolatey package manager. If you have not used Chocolatey before, you can find more details and the installation guide in the official documentation at <a href="https://chocolatey.org/install"><span class="url">https://chocolatey.org/install</span></a>.</p>
    <p class="normal">Execute the following command in PowerShell or Command shell to install Helm:</p>
    <pre class="programlisting con"><code class="hljs-con">PS C:\Windows\system32&gt; choco install kubernetes-helm
PS C:\Windows\system32&gt; helm version
version.BuildInfo{Version:"v3.15.0-rc.2", GitCommit:"c4e37b39dbb341cb3f716220df9f9d306d123a58", GitTreeState:"clean", GoVersion:"go1.22.3"}
</code></pre>
    <p class="normal">Once installed, you can move on to <em class="italic">Deploying an example chart – WordPress</em>, which is later in this section.</p>
    <h2 class="heading-2" id="_idParaDest-476">Installing Helm on macOS</h2>
    <p class="normal">To install<a id="_idIndexMarker1226"/> Helm on macOS, you can use the standard <strong class="keyWord">Homebrew</strong> package <a id="_idIndexMarker1227"/>manager. Use the following command to<a id="_idIndexMarker1228"/> install the Helm formula:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>brew install helm
</code></pre>
    <p class="normal">Verify that the installation was successful by trying to get the Helm version from the command line:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta"> $ </span>helm version
version.BuildInfo{Version:"v3.16.2", GitCommit:"13654a52f7c70a143b1dd51416d633e1071faffb", GitTreeState:"dirty", GoVersion:"go1.23.2"}
</code></pre>
    <p class="normal">Once installed, we can deploy an example chart to verify that Helm works properly on your Kubernetes cluster.</p>
    <h2 class="heading-2" id="_idParaDest-477">Installing from the binary releases</h2>
    <p class="normal">It is also <a id="_idIndexMarker1229"/>possible to install the latest Helm package from the binary<a id="_idIndexMarker1230"/> itself. You need to find the latest or desired version of the binary from the release page (<a href="https://github.com/helm/helm/releases"><span class="url">https://github.com/helm/helm/releases</span></a>) and download it for your operating system. In the following example, we will see how to install the latest Helm from the binary on the Fedora workstation:</p>
    <p class="normal">Download and install Helm:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>wget https://get.helm.sh/helm-v3.15.1-linux-amd64.tar.gz
<span class="hljs-con-meta">$ </span>tar -zxvf helm-v3.15.1-linux-amd64.tar.gz
linux-amd64/
linux-amd64/README.md
linux-amd64/LICENSE
linux-amd64/helm
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">sudo</span> <span class="hljs-con-built_in">mv</span> linux-amd64/helm /usr/local/bin/helm
<span class="hljs-con-meta">$ </span>helm version
version.BuildInfo{Version:"v3.15.1", GitCommit:"e211f2aa62992bd72586b395de50979e31231829", GitTreeState:"clean", GoVersion:"go1.22.3"}
</code></pre>
    <p class="normal">Now, we will test the Helm package by <em class="italic">Deploying an example chart - WordPress</em> in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-478">Deploying an example chart – WordPress</h2>
    <p class="normal">By default, Helm comes <a id="_idIndexMarker1231"/>with no repositories configured. One possibility, which is no longer recommended, is to add the <code class="inlineCode">stable</code> repository so that you can browse the most popular Helm charts:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>helm repo add stable https://charts.helm.sh/stable
"stable" has been added to your repositories
</code></pre>
    <div class="note">
      <p class="normal">Adding random Helm chart repositories for deployment can pose serious security risks. Security audits are a must to ensure that only trusted and secure payloads are deployed within your Kubernetes environment.</p>
    </div>
    <p class="normal">Please note that most charts are now in the process of deprecation as they are moved to different Helm repositories where they will be maintained by the original creators. You can see this if you try to search for available Helm charts using the <code class="inlineCode">helm search repo</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>helm search repo stable|grep -i deprecated|<span class="hljs-con-built_in">head</span>
stable/acs-engine-autoscaler            2.2.2           2.1.1                   DEPRECATED Scales worker nodes within agent pools
stable/aerospike                        0.3.5           v4.5.0.5                DEPRECATED A Helm chart for Aerospike in Kubern...
stable/airflow                          7.13.3          1.10.12                 DEPRECATED ...&lt;removed for brevity&gt;...
</code></pre>
    <p class="normal">Instead, the new recommended way is to use the <code class="inlineCode">helm search hub</code> command, which allows you to browse the Artifact Hub directly from the CLI:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>helm search hub|<span class="hljs-con-built_in">head</span>
URL                                                     CHART VERSION                                           APP VERSION                                             DESCRIPTION                                      
https://artifacthub.io/packages/helm/mya/12factor       24.1.2                                                                                                          Easily deploy any application that conforms to ...
https://artifacthub.io/packages/helm/gabibbo97/...      0.1.0                                                   fedora-32                                               389 Directory Server                             
...&lt;removed for brevity&gt;...
</code></pre>
    <p class="normal">Now, let’s try <a id="_idIndexMarker1232"/>searching for some of the most popular Helm charts that we can use to test our installation. We would like to deploy <strong class="keyWord">WordPress</strong> on our Kubernetes cluster. We chose <a id="_idIndexMarker1233"/>WordPress to demonstrate here because it is a typical three-tier application with a public access tier (The Service), a web tier (WordPress), and a database tier (MariaDB). First, let’s check what the available charts are for WordPress on Artifact Hub:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>helm search hub wordpress
URL                                                     CHART VERSION   APP VERSION            DESCRIPTION                                      
https://artifacthub.io/packages/helm/kube-wordp...      0.1.0           1.1                    this is my wordpress package                     
https://artifacthub.io/packages/helm/wordpress-...      1.0.2           1.0.0                  A Helm chart for deploying Wordpress+Mariadb st...
https://artifacthub.io/packages/helm/bitnami-ak...      15.2.13         6.1.0                  WordPress is the world's most popular blogging ...
...&lt;removed for brevity&gt;...
</code></pre>
    <p class="normal">Similarly, you can directly use the Artifact Hub web UI and search for WordPress Helm charts as follows:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_01.png"/></figure>
    <p class="packt_figref">Figure 14.1: Artifact Hub search results for WordPress Helm charts</p>
    <p class="normal">We will be using <a id="_idIndexMarker1234"/>the Helm chart provided and maintained by <strong class="keyWord">Bitnami</strong>, a <a id="_idIndexMarker1235"/>company specializing in distributing open-source software on various platforms, such as Kubernetes. If you navigate to the search result for WordPress charts by Bitnami, you will see the following:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_02.png"/></figure>
    <p class="packt_figref">Figure 14.2: Bitnami WordPress Helm chart on Artifact Hub with install instructions</p>
    <p class="normal">The page gives <a id="_idIndexMarker1236"/>you detailed information about how you can add the <code class="inlineCode">bitnami</code> repository and install the Helm chart for WordPress. Additionally, you will find a lot of details about available configurations, known limitations, and troubleshooting. You can also navigate to the home page of each of the charts in order to see the YAML templates that make up the chart (<a href="https://github.com/bitnami/charts/tree/master/bitnami/wordpress"><span class="url">https://github.com/bitnami/charts/tree/master/bitnami/wordpress</span></a>).</p>
    <p class="normal">We can now do the installation by following the instructions on the web page. First, add the <code class="inlineCode">bitnami</code> repository to your Helm installation:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>helm repo add bitnami https://charts.bitnami.com/bitnami
"bitnami" has been added to your repositories
</code></pre>
    <p class="normal">As a best practice, let us install WordPress inside a dedicated namespace called <code class="inlineCode">wordpress</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create ns wordpress 
namespace/wordpress created
</code></pre>
    <p class="normal">With the repository present, we can install the <code class="inlineCode">bitnami/wordpress</code> Helm chart, but before that, we need to prepare some details for the deployment. Check the chart page in Artifact Hub (<a href="https://artifacthub.io/packages/helm/bitnami/wordpress"><span class="url">https://artifacthub.io/packages/helm/bitnami/wordpress</span></a>). You will find a lot of parameters given here for you to configure and customize your WordPress deployment. If you do not provide any parameters, default values will be used for the Helm release deployment. For this demonstration, let us configure some of the parameters instead of using the default values.</p>
    <p class="normal">You can pass the individual parameters using the <code class="inlineCode">--set</code> argument as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>helm install wp-demo bitnami/wordpress -n wordpress --<span class="hljs-con-built_in">set</span> wordpressUsername=wpadmin
</code></pre>
    <p class="normal">When you have multiple parameters to configure, you can pass multiple <code class="inlineCode">--set</code> arguments but it is recommended to use variables in files; you can use one or more files to pass the variables.</p>
    <p class="normal">Let us create a <code class="inlineCode">wp-values.yaml</code> file to store the variables and values as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta"># </span>wp-values.yaml
wordpressUsername: wpadmin
wordpressPassword: wppassword
wordpressEmail: admin@example.com
wordpressFirstName: WP
wordpressLastName: Admin
service:
  type: NodePort
volumePermissions:
  enabled: true
</code></pre>
    <p class="normal">As you can see, we are passing some of the WordPress configurations to the Helm chart. Please note that we are changing the default WordPress type to <code class="inlineCode">NodePort</code> as we are using a minikube cluster here. If you are using a different Kubernetes cluster – for example, cloud-based Kubernetes – then you may leave it as the default, which is <code class="inlineCode">LoadBalancer</code>.</p>
    <p class="normal">Now that we have the<a id="_idIndexMarker1237"/> Helm repo configured, a dedicated namespace created, and parameters configured in a variable file, let us deploy WordPress using Helm; we will use the name <code class="inlineCode">wp-demo</code> for this release:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>helm install wp-demo bitnami/wordpress -n wordpress --values wp-values.yaml
NAME: wp-demo
LAST DEPLOYED: Tue Jun  4 21:27:49 2024
NAMESPACE: wordpress
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: wordpress
CHART VERSION: 22.4.2
APP VERSION: 6.5.3
...&lt;to be continued&gt;...
</code></pre>
    <p class="normal">Helm will show the basic release information, as shown here. After a while, you will also see the deployment details, including the service name, how to access the WordPress website, and more:</p>
    <pre class="programlisting con"><code class="hljs-con">...
** Please be patient while the chart is being deployed **
Your WordPress site can be accessed through the following DNS name from within your cluster:
    wp-demo-wordpress.wordpress.svc.cluster.local (port 80)
To access your WordPress site from outside the cluster follow the steps below:
Get the WordPress URL by running these commands:
   export NODE_PORT=$(kubectl get --namespace wordpress -o jsonpath="{.spec.ports[0].nodePort}" services wp-demo-wordpress)
   export NODE_IP=$(kubectl get nodes --namespace wordpress -o jsonpath="{.items[0].status.addresses[0].address}")
   echo "WordPress URL: http://$NODE_IP:$NODE_PORT/"
...&lt;removed for brevity&gt;...
</code></pre>
    <p class="normal">This is the beauty of <a id="_idIndexMarker1238"/>Helm – you have executed a single <code class="inlineCode">helm install</code> command and you are presented with a detailed guide on how to use the deployed component on <em class="italic">your</em> cluster. Meanwhile, the WordPress instance deploys without any intervention from you!</p>
    <div class="packt_tip">
      <p class="normal">It is a good practice to first inspect what Kubernetes objects’ YAML manifests were produced by Helm. You can do that by running the <code class="inlineCode">helm install</code> command with additional flags: <code class="inlineCode">helm install wp-demo bitnami/wordpress --dry-run --debug</code>. The output will contain the joint output of YAML manifests, and they will not be applied to the cluster.</p>
    </div>
    <p class="normal">You can also mention the specific version of the Helm chart using the <code class="inlineCode">--version</code> argument as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>helm install my-wordpress bitnami/wordpress --version 22.4.2
</code></pre>
    <p class="normal">Let’s now follow the instructions from the Helm chart installation output:</p>
    <ol>
      <li class="numberedList" value="1">Wait for a while as the database needs to initialize and deploy the Pods. Check the Pod status:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get po -n wordpress
NAME                                 READY   STATUS    RESTARTS        AGE
wp-demo-mariadb-0                    1/1     Running   9 (5m57s ago)   31m
wp-demo-wordpress-5d98c44785-9xd6h   1/1     Running   0               31m
</code></pre>
      </li>
      <li class="numberedList">Notice the database is deployed as a StatefulSet as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get statefulsets.apps -n wordpress
NAME              READY   AGE
wp-demo-mariadb   1/1     99s
</code></pre>
      </li>
      <li class="numberedList">Wait for the <code class="inlineCode">wp-demo</code> Service object (of the NodePort type) to acquire port details:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span> kubectl get svc -n wordpress
NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
wp-demo-mariadb     ClusterIP   10.100.118.79   &lt;none&gt;        3306/TCP                     2m39s
wp-demo-wordpress   NodePort    10.100.149.20   &lt;none&gt;        80:30509/TCP,443:32447/TCP   2m39s
</code></pre>
      </li>
    </ol>
    <p class="normal-one">In our case, the port will be <code class="inlineCode">80:31979/TCP</code>.</p>
    <ol>
      <li class="numberedList" value="4">Since we are <a id="_idIndexMarker1239"/>using minikube in this case, let us find the IP address and port details (if you are using the <code class="inlineCode">LoadBalancer</code> type, then you can directly access the IP address to see the WordPress site):
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube service --url wp-demo-wordpress -n wordpress
http://192.168.59.150:30509
http://192.168.59.150:32447
</code></pre>
      </li>
      <li class="numberedList">Now open your web browser and navigate to the WordPress URL, <code class="inlineCode">http://192.168.59.150:30509</code> (the other port is for the HTTPS URL):</li>
    </ol>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_03.png"/></figure>
    <p class="packt_figref">Figure 14.3: WordPress chart deployed on Kubernetes – main page</p>
    <ol>
      <li class="numberedList" value="6">Now you can<a id="_idIndexMarker1240"/> log in to the WordPress dashboard at <code class="inlineCode">http://192.168.59.150:30509/wp-admin</code>. Please note that if you missed setting the WordPress parameters, including the password, you need to find the default values Helm has used. For example, to retrieve the WordPress login password, check the secret as follows. Use the following commands to obtain the credentials that are stored in a dedicated <code class="inlineCode">wp-demo-wordpress</code> Secret object deployed as part of the chart:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get secret --namespace wordpress wp-demo-wordpress -o jsonpath=<span class="hljs-con-string">"{.data.wordp</span>
ress-password}" | base64 --decode
wppassword
</code></pre>
      </li>
      <li class="numberedList">Use the credentials to log in as the WordPress admin:</li>
    </ol>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_04.png"/></figure>
    <p class="packt_figref">Figure 14.4: WordPress chart deployed on Kubernetes – admin dashboard</p>
    <p class="normal">You can enjoy your <a id="_idIndexMarker1241"/>WordPress now, congratulations! If you are interested, you can inspect the Pods, Services, Deployments, and StatefulSets that were deployed as part of this Helm chart. This will give you a lot of insight into what the components of the chart are and how they interact.</p>
    <div class="packt_tip">
      <p class="normal">The Helm CLI offers <em class="italic">autocompletion</em> for various shells. You can run the <code class="inlineCode">helm completion</code> command to learn more.</p>
    </div>
    <p class="normal">If you want to get information about all Helm releases that are deployed in your Kubernetes cluster, use the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>helm list -n wordpress
NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART              APP VERSION
wp-demo wordpress       1               2024-06-04 22:20:32.556683096 +0800 +08 deployed        wordpress-22.4.2   6.5.3
</code></pre>
    <p class="normal">In the next section, we will learn how to delete a deployed release using Helm.</p>
    <h2 class="heading-2" id="_idParaDest-479">Deleting a Helm release</h2>
    <p class="normal">As we learned in the <a id="_idIndexMarker1242"/>previous sections, the Helm chart has deployed several resources, including Deployment, PVC, Services, Secrets, and so on. It is not practical to find and delete these items one by one. But Helm provides an easy method to remove the deployment using the <code class="inlineCode">helm uninstall</code> command. When you are ready, you can clean up the release by uninstalling the Helm release using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>helm uninstall wp-demo -n wordpress
release "wp-demo" uninstalled
</code></pre>
    <p class="normal">This will delete all Kubernetes objects that the release has created. Please note though that PersistentVolumes and PersistentVolumeClaims, created by the Helm chart, will not be cleaned up – you need to clean them up manually. We will now take a closer look at how Helm charts are structured internally.</p>
    <h2 class="heading-2" id="_idParaDest-480">Helm chart anatomy</h2>
    <p class="normal">As an example, we will take the<a id="_idIndexMarker1243"/> WordPress Helm chart by Bitnami (<a href="https://github.com/bitnami/charts/tree/master/bitnami/wordpress"><span class="url">https://github.com/bitnami/charts/tree/master/bitnami/wordpress</span></a>) that we have just used to perform a test Deployment in the cluster. Helm charts are simply directories with a specific structure (convention) that can live either in your local filesystem or in a Git repository. The directory name is at the same time the name of the chart – in this case, <code class="inlineCode">wordpress</code>. The structure of files in the chart directory is as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Chart.yaml</code>: YAML file that contains metadata about the chart such as version, keywords, and references to dependent charts that must be installed.</li>
      <li class="bulletList"><code class="inlineCode">LICENSE</code>: Optional, plain-text file with license information.</li>
      <li class="bulletList"><code class="inlineCode">README.md</code>: End user README file that will be visible on Artifact Hub.</li>
      <li class="bulletList"><code class="inlineCode">values.yaml</code>: The default configuration values for the chart that will be used as YAML template parameters. These values can be overridden by the Helm user, either one by one in the CLI or as a separate YAML file with values. You have already used this method by passing the <strong class="keyWord">-</strong><code class="inlineCode">-values wp-values.yaml</code> argument.</li>
      <li class="bulletList"><code class="inlineCode">values.schema.json</code>: Optionally, you can provide a JSON schema that <code class="inlineCode">values.yaml</code> must follow.</li>
      <li class="bulletList"><code class="inlineCode">charts/</code>: Optional directory with additional, dependent charts.</li>
      <li class="bulletList"><code class="inlineCode">crds/</code>: Optional Kubernetes custom resource definitions.</li>
      <li class="bulletList"><code class="inlineCode">templates/</code>: The most important directory that contains all YAML <em class="italic">templates</em> for generating Kubernetes YAML manifest files. The YAML templates will be combined with the provided <em class="italic">values</em>. The resulting YAML manifest files will be applied to the cluster.</li>
      <li class="bulletList"><code class="inlineCode">templates/NOTES.txt</code>: Optional file with short usage notes.</li>
    </ul>
    <p class="normal">For example, if <a id="_idIndexMarker1244"/>you inspect <code class="inlineCode">Chart.yaml</code> in the WordPress Helm chart, you can see that it depends on the <strong class="keyWord">MariaDB</strong> chart by Bitnami, if an appropriate value of <code class="inlineCode">mariadb.enabled</code> is set to <code class="inlineCode">true</code> in the provided values:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-string">...</span>
<span class="hljs-attr">appVersion:</span> <span class="hljs-number">6.5.3</span>
<span class="hljs-attr">dependencies:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">condition:</span> <span class="hljs-string">memcached.enabled</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">memcached</span>
  <span class="hljs-attr">repository:</span> <span class="hljs-string">oci://registry-1.docker.io/bitnamicharts</span>
  <span class="hljs-attr">version:</span> <span class="hljs-number">7.</span><span class="hljs-string">x.x</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">condition:</span> <span class="hljs-string">mariadb.enabled</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">mariadb</span>
  <span class="hljs-attr">repository:</span> <span class="hljs-string">oci://registry-1.docker.io/bitnamicharts</span>
  <span class="hljs-attr">version:</span> <span class="hljs-number">18.</span><span class="hljs-string">x.x</span>
<span class="hljs-string">...</span>
</code></pre>
    <p class="normal">Now, if you take a look at the <code class="inlineCode">values.yaml</code> file with the default values, which is quite verbose, you can see that by default MariaDB is enabled:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-string">...</span>
<span class="hljs-comment">## MariaDB chart configuration</span>
<span class="hljs-comment">## ref: https://github.com/bitnami/charts/blob/main/bitnami/mariadb/values.yaml</span>
<span class="hljs-comment">##</span>
<span class="hljs-attr">mariadb:</span>
  <span class="hljs-comment">## @param mariadb.enabled Deploy a MariaDB server to satisfy the applications database requirements</span>
  <span class="hljs-comment">## To use an external database set this to false and configure the `externalDatabase.*` parameters</span>
  <span class="hljs-comment">##</span>
  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>
<span class="hljs-string">...</span>
</code></pre>
    <p class="normal">And lastly, let’s check <a id="_idIndexMarker1245"/>what one of the YAML templates looks like – open the <code class="inlineCode">deployment.yaml</code> file (<a href="https://github.com/bitnami/charts/blob/master/bitnami/wordpress/templates/deployment.yaml"><span class="url">https://github.com/bitnami/charts/blob/master/bitnami/wordpress/templates/deployment.yaml</span></a>), which is a template for the Kubernetes Deployment object for Pods with WordPress containers. For example, you can see how the number of <code class="inlineCode">replicas</code> is referenced from the provided values:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-string">...</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-string">...</span>
  <span class="hljs-attr">replicas:</span> {{ <span class="hljs-string">.Values.replicaCount</span> }}
<span class="hljs-string">...</span>
</code></pre>
    <p class="normal">This will be replaced by the <code class="inlineCode">replicaCount</code> value (for which the default value of <code class="inlineCode">1</code> is found in the <code class="inlineCode">values.yaml</code> file). The details about how to use Go templates can be found at <a href="https://pkg.go.dev/text/template"><span class="url">https://pkg.go.dev/text/template</span></a>. You can also learn by example by analyzing the existing Helm charts – most of them use similar patterns for processing provided values.</p>
    <div class="packt_tip">
      <p class="normal">The detailed documentation on Helm chart structure can be found at <span class="url">https://helm.sh/docs/topics/charts/</span>.</p>
    </div>
    <p class="normal">In most cases, you will need to override some of the default values from the <code class="inlineCode">values.yaml</code> file during the installation of a chart, as we learned earlier.</p>
    <p class="normal">Now, that you know the most important details about the Helm chart structure, in the next section, we can deploy Kubernetes Dashboard using Helm charts.</p>
    <h1 class="heading-1" id="_idParaDest-481">Installing Kubernetes Dashboard using Helm Charts</h1>
    <p class="normal">Kubernetes Dashboard<a id="_idIndexMarker1246"/> is the official web UI for providing an overview of your cluster. The Helm chart for this component is officially maintained by the Kubernetes community (<a href="https://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard"><span class="url">https://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard</span></a>). We are going to install it with the default parameters, as there is no need for any customizations at this point.</p>
    <div class="note">
      <p class="normal">For minikube clusters, you can enable the dashboard and access it using a single command: <code class="inlineCode">minikube dashboard</code>. But our intention here is to learn how to deploy a dashboard for any type of Kubernetes cluster.</p>
    </div>
    <p class="normal">First, add the <code class="inlineCode">kubernetes-dashboard</code> repository to Helm:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
"kubernetes-dashboard" has been added to your repositories
</code></pre>
    <p class="normal">Now, we can<a id="_idIndexMarker1247"/> install the Helm chart as a <code class="inlineCode">kubernetes-dashboard</code> release <a id="_idIndexMarker1248"/>in the cluster as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard
</code></pre>
    <p class="normal">Wait for the installation to finish and check the output messages. Notice the following message as we will use it later to access the dashboard WEBUI:</p>
    <pre class="programlisting con"><code class="hljs-con">...
Congratulations! You have just installed Kubernetes Dashboard in your cluster.
To access the Dashboard, run the following:
  kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443
...
</code></pre>
    <p class="normal">Also, ensure Pods have a <code class="inlineCode">Running</code> status:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span> kubectl get pod -n kubernetes-dashboard
NAME                                                    READY   STATUS    RESTARTS   AGE
kubernetes-dashboard-api-86c68c7896-7jxwz               1/1     Running   0          2m53s
kubernetes-dashboard-auth-59784dd8b-vsr99               1/1     Running   0          2m53s
kubernetes-dashboard-kong-7696bb8c88-6q7zs              1/1     Running   0          2m53s
kubernetes-dashboard-metrics-scraper-5485b64c47-9d69q   1/1     Running   0          2m53s
kubernetes-dashboard-web-84f8d6fff4-nxt5f               1/1     Running   0          2m53s
</code></pre>
    <p class="normal">You may ignore other Pods deployed by the Helm chart for now. We will learn how to access the dashboard UI in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-482">Secure access to the Kubernetes Dashboard</h2>
    <p class="normal">By default, the Kubernetes <a id="_idIndexMarker1249"/>Dashboard prioritizes security by using a minimal RBAC configuration. This helps safeguard your cluster data. Currently, logging in to the dashboard requires a Bearer Token.</p>
    <div class="note">
      <p class="normal">This sample user creation guide will likely grant administrative privileges. Be sure to use it only for educational purposes and implement proper RBAC controls for production environments.</p>
    </div>
    <p class="normal">Follow these steps to create a token to access the Kubernetes Dashboard WEBUI:</p>
    <ol>
      <li class="numberedList" value="1">Create a ServiceAccount; prepare the YAML as follows:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># dashboard-sa.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">admin-user</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kubernetes-dashboard</span>
</code></pre>
      </li>
      <li class="numberedList">Create the ServiceAccount as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f dashboard-sa.yaml
serviceaccount/admin-user created
</code></pre>
      </li>
      <li class="numberedList">Create <code class="inlineCode">ClusterRoleBinding</code> to allow the access. Prepare the YAML as follows:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># dashboard-rbac.yml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">admin-user</span>
<span class="hljs-attr">roleRef:</span>
  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>
  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">cluster-admin</span>
<span class="hljs-attr">subjects:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">admin-user</span>
    <span class="hljs-attr">namespace:</span> <span class="hljs-string">kubernetes-dashboard</span>
</code></pre>
      </li>
      <li class="numberedList">Create <code class="inlineCode">ClusterRoleBinding</code> by applying the YAML definition:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f dashboard-rbac.yml
clusterrolebinding.rbac.authorization.k8s.io/admin-user created
</code></pre>
      </li>
      <li class="numberedList">Create<a id="_idIndexMarker1250"/> the token:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl -n kubernetes-dashboard create token admin-user
</code></pre>
      </li>
    </ol>
    <p class="normal">Copy the long token string generated, and we will use it in the next section to log in to the cluster dashboard.</p>
    <h2 class="heading-2" id="_idParaDest-483">Accessing Dashboard WEBUI</h2>
    <p class="normal">The Kubernetes <a id="_idIndexMarker1251"/>Dashboard offers various access methods. Here, we’ll focus on the default approach. This method assumes you haven’t altered the standard configuration during installation. If you’ve made modifications, the steps might differ.</p>
    <p class="normal">Execute the following command (which you copied from the <code class="inlineCode">helm install</code> output earlier) to get Dashboard access. The command will stay there with the status of <code class="inlineCode">port-forward</code>; do not exit from the command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443
Forwarding from 127.0.0.1:8443 -&gt; 8443
Forwarding from [::1]:8443 -&gt; 8443
...
</code></pre>
    <p class="normal">Now, access the URL <code class="inlineCode">https://localhost:8443/</code> in a web browser. You can ignore the SSL certificate warning as the dashboard is using the self-signed SSL certificates. Enter the token you generated in <em class="italic">Step</em> <em class="italic">3</em>, of <em class="italic">Secure Access to the Kubernetes Dashboard</em>, and log in to Dashboard as shown in the following figure.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_05.png"/></figure>
    <p class="packt_figref">Figure 14.5: Kubernetes Dashboard chart – login page</p>
    <p class="normal">At this point, you <a id="_idIndexMarker1252"/>have access to the dashboard, and you can browse its functionalities:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_06.png"/></figure>
    <p class="packt_figref">Figure 14.6: Kubernetes Dashboard chart – Deployments page</p>
    <p class="normal">The bearer token is<a id="_idIndexMarker1253"/> for a user with the <code class="inlineCode">cluster-admin</code> role, so be careful, as you can perform any operations, including deleting resources.</p>
    <p class="normal">Congratulations; you have successfully deployed the Kubernetes Dashboard using Helm charts, and verified the access using the token. You can explore more deployments using Helm charts as we explain in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-484">Installing other popular solutions using Helm charts</h2>
    <p class="normal">To practice more with Helm charts, you can quickly install some of the other software for your Kubernetes cluster. It can be useful in your development scenarios or as building blocks of your cloud-native applications. </p>
    <h3 class="heading-3" id="_idParaDest-485">Elasticsearch with Kibana</h3>
    <p class="normal">Elasticsearch<a id="_idIndexMarker1254"/> is a popular full-text search engine that is commonly used for log indexing and log analytics. Kibana, which <a id="_idIndexMarker1255"/>is part of the Elasticsearch ecosystem, is a visualization UI for the Elasticsearch database. To install this stack, we will<a id="_idIndexMarker1256"/> need to use two charts, both of which are maintained by<a id="_idIndexMarker1257"/> Elasticsearch creators: </p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Elasticsearch chart</strong> (<a href="https://artifacthub.io/packages/helm/elastic/elasticsearch"><span class="url">https://artifacthub.io/packages/helm/elastic/elasticsearch</span></a>)</li>
      <li class="bulletList"><strong class="keyWord">Kibana chart</strong> (<a href="https://artifacthub.io/packages/helm/elastic/kibana"><span class="url">https://artifacthub.io/packages/helm/elastic/kibana</span></a>)</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-486">Prometheus with Grafana</h3>
    <p class="normal">Prometheus<a id="_idIndexMarker1258"/> is a popular monitoring system with a time series database and Grafana is used as a visualization UI. Similar to the Elastic Stack, to install this Prometheus and<a id="_idIndexMarker1259"/> Grafana<a id="_idIndexMarker1260"/> stack, we will need to use two charts:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Prometheus</strong> (<a href="https://artifacthub.io/packages/helm/prometheus-community/prometheus"><span class="url">https://artifacthub.io/packages/helm/prometheus-community/prometheus</span></a>)</li>
      <li class="bulletList"><strong class="keyWord">Grafana</strong> (<a href="https://artifacthub.io/packages/helm/grafana/grafana"><span class="url">https://artifacthub.io/packages/helm/grafana/grafana</span></a>)</li>
    </ul>
    <p class="normal">Since we have already explained how to deploy Helm charts in the Kubernetes cluster, we will skip the step-by-step instructions for these. You may continue deploying the charts and exploring the functionalities.</p>
    <p class="normal">In this section, we will explore some of the key security considerations for Helm Charts.</p>
    <h2 class="heading-2" id="_idParaDest-487">Security considerations for Helm Charts</h2>
    <p class="normal">Helm charts ease the deployment of<a id="_idIndexMarker1261"/> applications into Kubernetes, but they can introduce several security vulnerabilities that need to be managed. Some of the most important things to bear in mind include the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Source Verification: </strong>Always check the source for Helm charts before deploying and never install charts originating from non-trusted or less reputed repositories. Malicious/insecure applications may be contained. Verify the origin of a chart, and try using official ones or well-maintained community repositories.</li>
      <li class="bulletList"><strong class="keyWord">Regular Audits</strong>: Periodically run security audits of Helm charts and their dependencies. This process helps in identifying known vulnerabilities that, in turn, ensure applications deployed are secure enough to meet standards set by your organization. Perform vulnerability scanning within Helm charts using tools like Trivy or Anchore.</li>
      <li class="bulletList"><strong class="keyWord">Chart Configuration</strong>: Beware of the defaults that come in Helm charts. Most are shipped with configuration settings that are not appropriate to your production environment. Consider reviewing and adjusting the default settings as necessary, using your organizational security policy and best practices.</li>
      <li class="bulletList"><strong class="keyWord">Role-Based Access Control (RBAC)</strong>: Implement this to restrict the deployment and management of Helm charts in your Kubernetes cluster. This will reduce unauthorized changes and also ensure that only trusted persons can deploy sensitive applications.</li>
      <li class="bulletList"><strong class="keyWord">Dependency Management: </strong>Monitor and manage the list of dependencies shown within your Helm charts. Regularly update these to avoid security gaps in applications and make sure they receive the latest security patches and improvements.</li>
      <li class="bulletList"><strong class="keyWord">Namespace Isolation: </strong>Consider using Helm charts, each in their own separate namespace, as this will increase security. If something bad happens, the blast radius will be limited, thus giving better isolation for applications <a id="_idIndexMarker1262"/>and their resources.</li>
    </ul>
    <p class="normal">By being proactive in these areas, you can go a long way toward improving the security posture of your Kubernetes deployments and ensuring that potential vulnerabilities do not bring down your applications and data.</p>
    <p class="normal">Now, we have explored and practiced the Helm charts in the first half of this chapter. We also learned that the Helm chart is a great way to deploy and manage complex applications in Kubernetes.</p>
    <p class="normal">As Kubernetes adoption increases, so does the complexity of the production environment for managing applications. While Helm and similar tools improve the process of deploying applications, they cannot independently address stateful applications’ operational needs such as scaling, configuration management, and failure recovery at runtime. We need solutions that package application knowledge and best practices for operation to automate operational tasks at every stage of the applications’ lives with health and high performance in mind. With these solutions, teams minimize human interaction as well as reducing human error, and focus on delivering value through the application and not managing the infrastructure. Kubernetes Operators were introduced to help with such requirements.</p>
    <p class="normal">In the following section, we will learn what Kubernetes Operators are and how to install complex deployments with Kubernetes Operators.</p>
    <h1 class="heading-1" id="_idParaDest-488">Introducing Kubernetes Operators</h1>
    <p class="normal">We’ve explored the differences between Deployments and StatefulSets, with StatefulSets managing stateful applications that require persistent data storage. We also learned about the manual (and automated) operations needed for StatefulSets to function, such as data synchronization between Pod replicas and initialization tasks.</p>
    <p class="normal">However, manual intervention goes against the core principles of Kubernetes, where automation and self-healing are paramount. This is where Kubernetes Operators step in.</p>
    <h2 class="heading-2" id="_idParaDest-489">From humans to software</h2>
    <p class="normal">Imagine replacing human Operators with software Operators. Kubernetes Operators<a id="_idIndexMarker1263"/> are essentially software extensions that automate complex application management tasks, especially for stateful applications. Instead of relying on manual intervention to maintain application stacks, Operators leverage their built-in software components and intelligence to execute and manage these tasks effectively.</p>
    <p class="normal">The following image shows the high-level relationship between the components in a Kubernetes cluster with Operators.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_07.png"/></figure>
    <p class="packt_figref">Figure 14.7: How Operators manage resources in Kubernetes</p>
    <p class="normal">In the following sections, we will discuss the advantages and benefits of Kubernetes Operators.</p>
    <h2 class="heading-2" id="_idParaDest-490">Helm Charts versus Kubernetes Operators</h2>
    <p class="normal">While Helm charts<a id="_idIndexMarker1264"/> are associated components related to<a id="_idIndexMarker1265"/> Operators in application management on Kubernetes, they are different. With Helm, users can install applications much faster than with Operators because Helm is an effective package manager, using pre-designed charts that make the process easier.</p>
    <p class="normal">It finds its best application in situations that demand speed and, therefore, it is ideal for a one-time installation or even for smaller applications. Yet Helm still focuses way more on the deployment phase rather than on the continued operational needs across an application’s runtime.</p>
    <p class="normal">Operators are software programs that intend to assume responsibility for the entire life cycle of complex and stateful applications. Using <strong class="keyWord">Custom Resource Definitions</strong> (<strong class="keyWord">CRDs</strong>), they encode knowledge about how an <a id="_idIndexMarker1266"/>application operates and allow scaling, configuration management, automation of upgrades, and self-healing mechanisms. Operators are constantly observing the health of an application and taking corrective action to head it back toward the desired state. That’s why they become so valuable for applications that are very management- and observation-intensive. To put it succinctly, Helm charts make the deployment easy, but Operators extend Kubernetes into production operability, letting teams automate and simplify how they manage their apps in production.</p>
    <p class="normal">In the next section, let us explore some of the major features of Kubernetes Operators compared to Helm charts.</p>
    <h2 class="heading-2" id="_idParaDest-491">How can the Operators help in the application deployment?</h2>
    <p class="normal">Operators are created with the <a id="_idIndexMarker1267"/>capabilities to manage and maintain the application with all possible operations, including the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Life Cycle Management</strong>: Operators <a id="_idIndexMarker1268"/>manage the life cycle of your application or application stack beyond mere initial deployment. They automate key operational tasks like upgrades, scaling, and recovery from failures to keep the application healthy and performing over time. Other approaches, like Helm charts, generally stop at deployment, whereas Operators constantly monitor the current application state and automatically reconcile it to the desired configuration in case changes or issues arise. This allows for automated upgrades, configuration changes, and status monitoring – all with no intervention required or even desired. For complex stateful applications with demanding life cycle management, Operators offer significant advantages over Helm and must be preferred for such applications that require continuous care and automated <a id="_idIndexMarker1269"/>management.</li>
      <li class="bulletList"><strong class="keyWord">Resource Orchestration</strong>: Operators<a id="_idIndexMarker1270"/> create <a id="_idIndexMarker1271"/>essential resources like <strong class="keyWord">ConfigMaps</strong>, <strong class="keyWord">Secrets</strong>, and <strong class="keyWord">PVCs</strong> required by your <a id="_idIndexMarker1272"/>application to <a id="_idIndexMarker1273"/>function properly.</li>
      <li class="bulletList"><strong class="keyWord">Automated Deployment</strong>: Operators can <a id="_idIndexMarker1274"/>deploy your application stack based on either user-provided configurations or default values, streamlining the deployment process.</li>
      <li class="bulletList"><strong class="keyWord">Database Management</strong>: Take <a id="_idIndexMarker1275"/>PostgreSQL clusters, for example. Operators can leverage StatefulSets to deploy them and ensure data synchronization across replicas.</li>
      <li class="bulletList"><strong class="keyWord">Self-Healing Capabilities</strong>: Operators can detect and react to application failures, triggering <a id="_idIndexMarker1276"/>recovery or <a id="_idIndexMarker1277"/>failover mechanisms to maintain service continuity.</li>
    </ul>
    <h2 class="heading-2" id="_idParaDest-492">Reusability of the automation</h2>
    <p class="normal">Operators promote reusability. The same <a id="_idIndexMarker1278"/>Operator can be utilized across different projects or in multiple Kubernetes clusters, ensuring consistent and efficient application management throughout your infrastructure.</p>
    <h2 class="heading-2" id="_idParaDest-493">How Operators ensure the application state</h2>
    <p class="normal">Kubernetes Operators function<a id="_idIndexMarker1279"/> similarly to Kubernetes itself, utilizing a control loop to manage applications. This loop continuously monitors the desired state of your application, defined by a CRD, and compares it to the application’s actual state within the cluster. Any discrepancies trigger corrective actions from the Operator. These actions can involve scaling the application, updating configurations, or restarting Pods. The control loop’s continuous operation ensures your application remains aligned with the desired state, promoting self-healing and automated<a id="_idIndexMarker1280"/> management.</p>
    <h2 class="heading-2" id="_idParaDest-494">Custom resource definitions – building blocks for Operators</h2>
    <p class="normal">Kubernetes Operators rely on<a id="_idIndexMarker1281"/> CRDs. These <a id="_idIndexMarker1282"/>are essentially extensions of the Kubernetes API that allow you to define custom resources specific to your application or its needs. Think of them as blueprints for your application’s desired configuration within the Kubernetes ecosystem.</p>
    <p class="normal">CRDs essentially extend the Kubernetes API, allowing you to define custom resources specific to your application. These resources represent the building blocks of your application within the Kubernetes cluster. They can specify details like the following:</p>
    <ul>
      <li class="bulletList">The desired number of application replicas (Pods).</li>
      <li class="bulletList">Resource requests and limits for memory and CPU.</li>
      <li class="bulletList">Storage configurations for persistent data.</li>
      <li class="bulletList">Environment variables and configuration settings.</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-495">Benefits of CRDs</h3>
    <p class="normal">There are multiple benefits<a id="_idIndexMarker1283"/> of using CRDs to deploy applications, including the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Declarative Management:</strong> Instead of manually configuring individual resources like Deployments or Services, CRDs let you define the desired state of your application in a declarative manner. The Operator then takes care of translating that desired state into actual running resources within the cluster.</li>
      <li class="bulletList"><strong class="keyWord">Application-Specific Configuration</strong>: CRDs cater to the unique needs of your application. You can define custom fields specific to your application logic or configuration requirements.</li>
      <li class="bulletList"><strong class="keyWord">Simplified Management:</strong> CRDs provide a central point for managing your application’s configuration. This streamlines the process compared to scattered configurations across different resource types.</li>
    </ul>
    <h2 class="heading-2" id="_idParaDest-496">Operator distribution mechanism</h2>
    <p class="normal">Operators are <a id="_idIndexMarker1284"/>primarily created and distributed by <a id="_idIndexMarker1285"/>application vendors who possess the expertise in deploying those specific application stacks. However, a vibrant community actively develops, distributes, and maintains a vast array of Operators.</p>
    <p class="normal">OperatorHub (<a href="https://operatorhub.io/"><span class="url">https://operatorhub.io/</span></a>) serves <a id="_idIndexMarker1286"/>as a central repository where you can discover and install Operators for a wide range of applications and functionalities.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_08.png"/></figure>
    <p class="packt_figref">Figure 14.8: OperatorHub main page</p>
    <h2 class="heading-2" id="_idParaDest-497">Building your own Operator</h2>
    <p class="normal">The <strong class="keyWord">Operator Framework</strong> provides a<a id="_idIndexMarker1287"/> powerful toolkit known as the <strong class="keyWord">Operator SDK</strong>. This <a id="_idIndexMarker1288"/>open-source gem empowers you to develop and build custom <a id="_idIndexMarker1289"/>Operators, taking control of your application management within Kubernetes. The Operator SDK streamlines the often-complex process of crafting Operators. It offers pre-built components and functionalities that handle common tasks like interacting with the Kubernetes API, managing custom resources, and implementing the control loop. This allows you to focus on the unique logic and configuration needs of your specific application.</p>
    <p class="normal">Building your own Operators unlocks several advantages. Firstly, you gain fine-grained control over application management. The Operator can be tailored to your application’s specific needs, handling configuration, deployment strategies, and scaling requirements perfectly. Secondly, Operators automate repetitive tasks associated with the application life cycle, leading to significant efficiency gains. Finally, custom Operators are reusable. Once built for a particular application, they can be applied across different <a id="_idIndexMarker1290"/>deployments of the same application, saving you time and effort in managing your Kubernetes infrastructure.</p>
    <p class="normal">You can learn more about the Operator SDK<a id="_idIndexMarker1291"/> at <a href="https://sdk.operatorframework.io/"><span class="url">https://sdk.operatorframework.io/</span></a>.</p>
    <p class="normal">Before we do some hands-on with the Operator, let us learn a few details about <strong class="keyWord">Operator Lifecycle Manager</strong> (<strong class="keyWord">OLM</strong>) in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-498">Operator Lifecycle Manager (OLM)</h2>
    <p class="normal">Operator Lifecycle Manager in <a id="_idIndexMarker1292"/>Kubernetes simplifies the process of deploying and managing applications packaged as Operators. OLM uses a declarative way of specifying the resources using YAML files. In this, there is no need for complex multi-file deployments that rely on specific ordering. Installation and automatic upgrades also keep your Operators up to date with OLM. It also exposes a package discovery capability called Catalog Sources, which enables the use of Operators from, for example, OperatorHub, or from sources of your choosing. With OLM, you will gain several advantages. First, it reduces deployment complexity by managing dependencies and order. OLM can manage thousands of Operators over large clusters. What’s more, OLM enforces the desired configuration. This will ease rollouts and updates. Not to mention that OLM promotes standardization by providing a consistent way to package and deploy applications as Operators.</p>
    <p class="normal">This is a question of your environment and personal preference. OLM is much more integrated and native to Kubernetes, while Helm, being familiar for other deployments, comes with a richer package ecosystem.</p>
    <p class="normal">When we learn about Operators and OLM, it is also important to know about the <strong class="keyWord">ClusterServiceVersion</strong> (<strong class="keyWord">CSV</strong>). The <a id="_idIndexMarker1293"/>ClusterServiceVersion will be the central point of an important part of Kubernetes’ Operator Lifecycle Manager, the core metadata processing and deploying information of the Operator. It goes on to define the name of the Operator and the version it holds, and gives a brief description. It also outlines the permissions or roles required by the Operator to operate correctly. It defines the CRDs that the Operator governs, installation strategy, and upgrade flows. Refer to the <a id="_idIndexMarker1294"/>documentation (<a href="https://olm.operatorframework.io/docs/concepts/crds/clusterserviceversion/"><span class="url">https://olm.operatorframework.io/docs/concepts/crds/clusterserviceversion/</span></a>) to learn more about the CSV.</p>
    <p class="normal">In the following section, we’ll explore how to deploy Prometheus monitoring on Kubernetes using both OLM and the Prometheus Operator, showcasing the power of both approaches in application management.</p>
    <h1 class="heading-1" id="_idParaDest-499">Enabling Kubernetes monitoring using Prometheus and Grafana</h1>
    <p class="normal">Success in keeping your Kubernetes applications healthy and performant depends on different factors; one<a id="_idIndexMarker1295"/> of those is having a robust, reliable environment. Here, monitoring tools like <strong class="keyWord">Prometheus</strong> and <strong class="keyWord">Grafana</strong> can help. Prometheus works behind the scenes; it <a id="_idIndexMarker1296"/>gathers and stores valuable metrics about your Kubernetes cluster. Grafana visualizes this treasure trove of data, presenting it in an understandable format for you to gain deep insight into the health and behavior of your applications and infrastructure.</p>
    <p class="normal">The following figure shows the high-level architecture of Prometheus components. (It’s an official reference.)</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_09.png"/></figure>
    <p class="packt_figref">Figure 14.9: Architecture of Prometheus and some of its ecosystem components (Source: https://prometheus.io/docs/introduction/overview/)</p>
    <p class="normal">Traditional<a id="_idIndexMarker1297"/> deployments, say, of monitoring stacks comprising Prometheus and Grafana, are pretty cumbersome. Writing several YAML manifests by hand, with all the dependencies and the proper ordering of deployment, is rather laborious and prone to errors. The Prometheus and Grafana Operators offer an even more efficient and maintainable solution.</p>
    <div class="note">
      <p class="normal">It is possible to utilize Helm charts to deploy this entire monitoring stack – including Operators and instances – using projects such <a id="_idIndexMarker1298"/>as <code class="inlineCode">kube-prometheus-stack</code> (<span class="url">https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack</span>). But in the next section, our intention is to set up the same monitoring stack using OLM.</p>
    </div>
    <p class="normal">In the following section, we’ll explore the process of deploying Prometheus and Grafana within your Kubernetes cluster, equipping you to effectively monitor your applications and ensure their smooth operation.</p>
    <h2 class="heading-2" id="_idParaDest-500">Installing Operator Lifecycle Manager</h2>
    <p class="normal">To utilize the <a id="_idIndexMarker1299"/>OLM-based operator installation, we need to install OLM in the cluster. You can install OLM in the cluster using the <code class="inlineCode">operator-sdk</code> utility, Helm charts, or even by applying the Kubernetes YAML manifests for OLM. For this exercise, let us use the <code class="inlineCode">install.sh</code> script-based installation as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.28.0/install.sh | bash -s v0.28.0
</code></pre>
    <p class="normal">Wait for the script to complete and configure the Kubernetes cluster with OLM. Once finished, verify the Pods are running in the <code class="inlineCode">olm</code> namespace as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods -n olm
NAME                               READY   STATUS    RESTARTS   AGE
catalog-operator-9f6dc8c87-rt569   1/1     Running   0          36s
olm-operator-6bccddc987-nrlkm      1/1     Running   0          36s
operatorhubio-catalog-6l8pw        0/1     Running   0          21s
packageserver-6df47456b9-8fdt7     1/1     Running   0          24s
packageserver-6df47456b9-lrvzp     1/1     Running   0          24s
</code></pre>
    <p class="normal">Also, check the ClusterServiceVersion details using the <code class="inlineCode">kubectl get csv</code> command as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get csv -n olm
NAME            DISPLAY          VERSION   REPLACES   PHASE
packageserver   Package Server   0.28.0               Succeeded
</code></pre>
    <p class="normal">That has a <a id="_idIndexMarker1300"/>success status – we can see the OLM is deployed and ready to manage the Operators. In the next section, we will deploy Prometheus and Grafana Operators using OLM.</p>
    <h2 class="heading-2" id="_idParaDest-501">Installing Prometheus and Grafana Operators using OLM</h2>
    <p class="normal">Once you <a id="_idIndexMarker1301"/>configure<a id="_idIndexMarker1302"/> OLM, deploying Operators is pretty <a id="_idIndexMarker1303"/>straightforward. Most of the time, you will find the operator installation command and instructions on the Operators page at <code class="inlineCode">operatorhub.io</code>.</p>
    <p class="normal">To install the Prometheus operator (<a href="https://operatorhub.io/operator/prometheus"><span class="url">https://operatorhub.io/operator/prometheus</span></a>) using OLM, follow these steps:</p>
    <ol>
      <li class="numberedList" value="1">Search for the Prometheus Operator in OperatorHub:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get packagemanifests | grep prometheus
ack-prometheusservice-controller           Community Operators   12m
prometheus                                 Community Operators   12m
prometheus-exporter-operator               Community Operators   12m
</code></pre>
      </li>
      <li class="numberedList">Install the Prometheus operator using OLM:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create -f https://operatorhub.io/install/prometheus.yaml
subscription.operators.coreos.com/my-prometheus created
</code></pre>
        <div class="note-one">
          <p class="normal"><span class="url">https://operatorhub.io/install/prometheus.yaml</span> provides a basic YAML definition to create a subscription. You can create local YAML files with all the customization required.</p>
        </div>
      </li>
    </ol>
    <ol>
      <li class="numberedList" value="3">Wait for a few minutes and ensure the Prometheus operator is deployed properly:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get csv -n operators
NAME                         DISPLAY               VERSION   REPLACES                     PHASE
prometheusoperator.v0.70.0   Prometheus Operator   0.70.0    prometheusoperator.v0.65.1   Succeeded
</code></pre>
      </li>
      <li class="numberedList">Verify the <a id="_idIndexMarker1304"/>Prometheus operator <a id="_idIndexMarker1305"/>Pods are <a id="_idIndexMarker1306"/>running:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods -n operators
NAME                                   READY   STATUS    RESTARTS   AGE
prometheus-operator-84f9b76686-2j27n   1/1     Running   0          87s
</code></pre>
      </li>
      <li class="numberedList">In the same way, find and install the Grafana operator using OLM (follow the previous steps for references):
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create -f https://operatorhub.io/install/grafana-operator.yaml
</code></pre>
      </li>
      <li class="numberedList">Now, also verify the CRDs created in the backend as these entries are created as part of the Operator installation:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get crd
AME                                                  CREATED AT
alertmanagerconfigs.monitoring.coreos.com             2024-10-18T09:14:03Z
alertmanagers.monitoring.coreos.com                   2024-10-18T09:14:04Z
catalogsources.operators.coreos.com                   2024-10-18T09:10:00Z
clusterserviceversions.operators.coreos.com           2024-10-18T09:10:00Z
grafanaalertrulegroups.grafana.integreatly.org        2024-10-18T09:25:19Z
...&lt;removed for brevity&gt;...
</code></pre>
      </li>
      <li class="numberedList">You will <a id="_idIndexMarker1307"/>find multiple <a id="_idIndexMarker1308"/>CRDs created in the <a id="_idIndexMarker1309"/>Kubernetes cluster.</li>
    </ol>
    <p class="normal">Now that we have deployed the Operators, it is time to create the Prometheus and Grafana instances and configure the stack to monitor the Kubernetes cluster. We will learn about these operations in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-502">Configuring Prometheus and Grafana instances using Operators</h2>
    <p class="normal">To configure <a id="_idIndexMarker1310"/>new instances, let us utilize standard YAML definitions with the CRD configuration. The instructions and the YAML definition files for this exercise are stored in the <code class="inlineCode">Chapter 14</code> directory of the GitHub repository.</p>
    <p class="normal">Follow the steps to configure a monitoring stack in Kubernetes with Prometheus and Grafana:</p>
    <ol>
      <li class="numberedList" value="1">As a best practice, let us create a namespace to deploy the monitoring solution (refer to <code class="inlineCode">monitoring-ns.yaml</code>):
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f monitoring-ns.yaml
namespace/monitoring created
</code></pre>
      </li>
      <li class="numberedList">Configure a ServiceAccount with the appropriate role and RBAC (refer to <code class="inlineCode">monitoring-sa.yaml</code> in the repo):
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f monitoring-sa.yaml
serviceaccount/prometheus created
role.rbac.authorization.k8s.io/prometheus-role created
rolebinding.rbac.authorization.k8s.io/prometheus-rolebinding created
</code></pre>
      </li>
      <li class="numberedList">Prepare YAML for the new Prometheus instance (refer to <code class="inlineCode">promethues-instance.yaml</code>):
        <pre class="programlisting con-one"><code class="hljs-con">apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: example-prometheus
  namespace: monitoring
spec:
  replicas: 2
  serviceAccountName: prometheus
  serviceMonitorSelector:
    matchLabels:
      app.kubernetes.io/name: node-exporter
</code></pre>
      </li>
    </ol>
    <div class="note-one">
      <p class="normal">Notice the <code class="inlineCode">kind: Prometheus</code> in the preceding YAML definition, as we are using a CRD here; the Prometheus operator will understand this CRD and take necessary actions to create the deployment.</p>
    </div>
    <ol>
      <li class="numberedList" value="4">Create a new<a id="_idIndexMarker1311"/> Prometheus instance by applying the configuration to the cluster:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f promethues-instance.yaml
prometheus.monitoring.coreos.com/example-prometheus
</code></pre>
      </li>
      <li class="numberedList">In a similar way, deploy the <a id="_idIndexMarker1312"/>Grafana instance using the operator:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># grafana-instnace.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">grafana.integreatly.org/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Grafana</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">dashboards:</span> <span class="hljs-string">grafana-a</span>
    <span class="hljs-attr">folders:</span> <span class="hljs-string">grafana-a</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">grafana-a</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">monitoring</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">config:</span>
    <span class="hljs-attr">auth:</span>
      <span class="hljs-attr">disable_login_form:</span> <span class="hljs-string">'false'</span>
    <span class="hljs-attr">log:</span>
      <span class="hljs-attr">mode:</span> <span class="hljs-string">console</span>
    <span class="hljs-attr">security:</span>
      <span class="hljs-attr">admin_password:</span> <span class="hljs-string">start</span>
      <span class="hljs-attr">admin_user:</span> <span class="hljs-string">root</span>
</code></pre>
      </li>
    </ol>
    <div class="note-one">
      <p class="normal">We are using the configuration and passwords here in plain text directly inside the YAML definition. In a production environment, you should be using Kubernetes Secrets to store such sensitive data.</p>
    </div>
    <ol>
      <li class="numberedList" value="6">Apply the YAML definition to create a Grafana instance:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f grafana-instance.yaml
grafana.grafana.integreatly.org/grafana-a created
</code></pre>
      </li>
      <li class="numberedList">Verify the objects<a id="_idIndexMarker1313"/> created by Prometheus and Grafana Operators in the <code class="inlineCode">monitoring</code> namespace:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pod,svc,sts -n monitoring
NAME                                     READY   STATUS    RESTARTS   AGE
pod/grafana-a-deployment-69f8999f8-82zbv   1/1   Running   0          17m
pod/node-exporter-n7tlb                    1/1   Running   0          93s
pod/prometheus-example-prometheus-0        2/2   Running   0          20m
pod/prometheus-example-prometheus-1        2/2   Running   0          20m
NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/grafana-a-service     ClusterIP   10.107.212.241   &lt;none&gt;        3000/TCP   17m
service/prometheus-operated   ClusterIP   None             &lt;none&gt;        9090/TCP   20m
NAME                                             READY   AGE
statefulset.apps/prometheus-example-prometheus   2/2     20m
</code></pre>
      </li>
      <li class="numberedList">You can see, in the preceding output, that the Operators have created appropriate Kubernetes resources based on the CRD. You can even see the Prometheus and Grafana Service created and will be able to access it. We will demonstrate it at a later stage of this exercise.</li>
    </ol>
    <p class="normal">For demonstration purposes, let us enable Node Exporter in the cluster and visualize it using Grafana. Node Exporter<a id="_idIndexMarker1314"/> is one of the Prometheus exporters, which exposes detailed metrics of a host machine, including hardware and OS details, such as CPU usage, memory usage, and other system-level metrics. It runs on each node in a Kubernetes cluster – or physical or virtual servers – as a separate service and exposes these metrics through an HTTP endpoint. By scraping this data, Prometheus can know the health and performance of a node, thereby enabling an administrator to understand resource utilization and point out problems in the infrastructure.</p>
    <p class="normal">Running Node Exporter as a DaemonSet would imply that each node in the Kubernetes cluster is running an instance of the exporter. In that way, Prometheus would be able to consistently scrape the system metrics across all nodes for effective observation of the overall health and performance of the cluster. Create the Node Exporter using <code class="inlineCode">node-exporter-daemonset.yaml</code> as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f node-exporter-daemonset.yaml
daemonset.apps/node-exporter created
</code></pre>
    <p class="normal">A Node Exporter Service (<code class="inlineCode">svc</code>) should be created to expose the metrics that the Node Exporter is collecting to Prometheus. The service provides a way for Prometheus to discover and scrape metrics from the Node Exporter pods running on each node, thus providing a capability of centralized monitoring of node performance across the Kubernetes cluster. Create a Service for Node Exporter as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f node-exporter-svc.yaml
service/node-exporter created
</code></pre>
    <p class="normal">A Node Exporter serviceMonitor conventionally enables Prometheus to discover and scrape the Node Exporter service for metrics. This described configuration simplifies the whole process of monitoring by defining how and where Prometheus should scrape for metrics, like specifying the target service, interval, labels, and others serving to ensure consistency in the collection without an administrator’s intervention. Create a serviceMonitor CRD as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f servicemonitor-instance.yaml
servicemonitor.monitoring.coreos.com/node-exporter created
Now it is our time to verify the monitoring stack deployment in Kubernetes cluster.
</code></pre>
    <p class="normal">Let us verify the <a id="_idIndexMarker1315"/>Prometheus portal to ensure the details are collected there. In one of your consoles, start a <code class="inlineCode">kubectl port-forward</code> command to expose the Prometheus service as follows (you can end the <code class="inlineCode">port-forward</code> using <em class="keystroke">Ctrl+C</em> later once you finish testing):</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl port-forward -n monitoring svc/prometheus-operated 9091:9090
Forwarding from 127.0.0.1:9091 -&gt; 9090
Forwarding from [::1]:9091 -&gt; 9090
</code></pre>
    <p class="normal">Open a browser and access <code class="inlineCode">http://localhost:9091/targets</code> to ensure <code class="inlineCode">node-exporter</code> is visible to Prometheus.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_10.png"/></figure>
    <p class="packt_figref">Figure 14.10: Prometheus portal with node-exporter visible for Kubernetes node</p>
    <p class="normal">You can confirm from the preceding screenshot that Prometheus is getting the node metrics successfully.</p>
    <p class="normal">Now, let us visualize the metrics and monitoring data using our visualization tool, Grafana. Open a console and use the <code class="inlineCode">kubectl port-foward</code> command to expose the Grafana service as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl port-forward -n monitoring service/grafana-a-service 3000:3000
Forwarding from 127.0.0.1:3000 -&gt; 3000
Forwarding from [::1]:3000 -&gt; 3000
</code></pre>
    <p class="normal">Open a browser and <a id="_idIndexMarker1316"/>access the URL <span class="Hyperlink">http://localhost:3000</span> to see the Grafana dashboard.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_11.png"/></figure>
    <p class="packt_figref">Figure 14.11: Grafana portal</p>
    <p class="normal">Use the login credential you have configured in the <code class="inlineCode">grafana-instance.yaml</code> (or the secret if you used one) and log in to the Grafana dashboard.</p>
    <p class="normal">You will only find a default dashboard there as we need to configure a new dashboard for our own purpose.</p>
    <p class="normal">From the left-side menu, go to <strong class="screenText">Connections | Data sources</strong> as shown in the following figure.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_12.png"/></figure>
    <p class="packt_figref">Figure 14.12: Adding a data source in Grafana</p>
    <p class="normal">In the next window, select <strong class="screenText">Prometheus</strong> as the data source and enter the Prometheus URL as shown in the following<a id="_idIndexMarker1317"/> figure. Remember to enter the FQDN (e.g., <code class="inlineCode">http://prometheus-operated.monitoring.svc.cluster.local:9090</code> – r to <em class="chapterRef">Chapter 8</em>, <em class="italic">Exposing Your Pods with Services</em>, to learn more about Services and FQDNs).</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_13.png"/></figure>
    <p class="packt_figref">Figure 14.13: Configure Grafana data source</p>
    <p class="normal">Click on the <strong class="screenText">Save &amp; test</strong> button at the bottom of the page and you will receive a success message as shown in the following figure. (If you get any error messages, then please check the Prometheus URL you have used, including the FQDN and the port number.)</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_14.png"/></figure>
    <p class="packt_figref">Figure 14.14: Grafana data source configured successfully</p>
    <p class="normal">Now that we have the<a id="_idIndexMarker1318"/> data source, we need to create a dashboard to visualize the data. You can either create a dashboard from scratch or import the dashboard with predefined configurations. For that, visit <code class="inlineCode">https://grafana.com/grafana/dashboards/</code> and find the <strong class="screenText">Node Exporter Full</strong> dashboard. Click on the <strong class="screenText">Copy ID to clipboard</strong> button as follows.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_15.png"/></figure>
    <p class="packt_figref">Figure 14.15: Copy the Grafana dashboard ID to import</p>
    <p class="normal">Go back to the <a id="_idIndexMarker1319"/>Grafana <strong class="screenText">WEBUI | Dashboards | New | Import</strong>, as shown in the following figure.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_16.png"/></figure>
    <p class="packt_figref">Figure 14.16: Importing a new dashboard to Grafana</p>
    <p class="normal">Enter the <strong class="screenText">Node </strong><strong class="screenText"><a id="_idIndexMarker1320"/></strong><strong class="screenText">Exporter Full</strong> dashboard ID that you already copied in the previous step, as shown here, and click on the <strong class="screenText">Load</strong> button.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_17.png"/></figure>
    <p class="packt_figref">Figure 14.17: Provide the dashboard ID to import in Grafana</p>
    <p class="normal">On the next screen, select <strong class="screenText">Prometheus</strong> as the data source (which you configured earlier) and click the <strong class="screenText">Import</strong> button as follows.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_18.png"/></figure>
    <p class="packt_figref">Figure 14.18: Complete dashboard import in Grafana</p>
    <p class="normal">That’s it; you will see the nice<a id="_idIndexMarker1321"/> dashboard with preconfigured widgets and graphs, as shown in the following figure. You can explore the dashboard and find the details about your cluster, such as CPU, memory, network traffic, and so on.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_14_19.png"/></figure>
    <p class="packt_figref">Figure 14.19: Node Exporter: full dashboard imported in Grafana</p>
    <p class="normal">Congratulations, you have <a id="_idIndexMarker1322"/>successfully deployed the Prometheus and Grafana stack on your Kubernetes cluster with Node Exporter enabled!</p>
    <p class="normal">That is all for this chapter. As you can see, working with Helm charts and Operators, even for complex, multi-component solutions, is easy and can provide a lot of benefits for your development and production environments.</p>
    <h1 class="heading-1" id="_idParaDest-503">Summary</h1>
    <p class="normal">This chapter covered the details of working with Helm, Helm charts, and Kubernetes Operators. First, you learned what the purpose of package management is and how Helm works as a package manager for Kubernetes. We demonstrated how you can install Helm on your local machine, and how you can deploy a WordPress chart to test the installation. Then, we went through the structure of Helm charts, and we showed how the YAML templates in charts can be configured using user-provided values. Next, we showed the installation of popular solutions on a Kubernetes cluster using Helm. We installed Kubernetes Dashboard and explored the components. After that, we learned about Kubernetes Operators and other components, including customer resource definitions. We also deployed the Prometheus stack, including Grafana using Helm and Operators.</p>
    <p class="normal">In the next part, you will get all the details required to effectively deploy Kubernetes clusters in different cloud environments. We will first take a look at working with clusters deployed on Google Kubernetes Engine.</p>
    <h1 class="heading-1" id="_idParaDest-504">Further reading</h1>
    <ul>
      <li class="bulletList"><strong class="keyWord">Helm website</strong>: <a href="https://helm.sh/&#13;"><span class="url">https://helm.sh/</span></a></li>
      <li class="bulletList"><strong class="keyWord">Kubernetes Dashboard</strong>: <a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/&#13;"><span class="url">https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/</span></a></li>
      <li class="bulletList"><strong class="keyWord">Accessing Dashboard</strong>: <a href="https://github.com/kubernetes/dashboard/blob/master/docs/user/accessing-dashboard/README.md&#13;"><span class="url">https://github.com/kubernetes/dashboard/blob/master/docs/user/accessing-dashboard/README.md</span></a></li>
      <li class="bulletList"><strong class="keyWord">Deploy and Access the Kubernetes Dashboard</strong>: <a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/&#13;"><span class="url">https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/</span></a></li>
      <li class="bulletList"><strong class="keyWord">Creating a sample user for dashboard access</strong>: <a href="https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md&#13;"><span class="url">https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md</span></a></li>
      <li class="bulletList"><strong class="keyWord">Build a Kubernetes Operator in six steps</strong>: <a href="https://developers.redhat.com/articles/2021/09/07/build-kubernetes-operator-six-steps&#13;"><span class="url">https://developers.redhat.com/articles/2021/09/07/build-kubernetes-operator-six-steps</span></a></li>
      <li class="bulletList"><strong class="keyWord">Custom resources</strong>: <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&#13;"><span class="url">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/</span></a></li>
    </ul>
    <p class="normal">For more information regarding Helm and Helm charts, please refer to the following <em class="italic">Packt Publishing</em> book:</p>
    <ul>
      <li class="bulletList"><em class="italic">Learn Helm</em>, by <em class="italic">Andrew Block</em>, <em class="italic">Austin Dewey</em> (<a href="https://www.packtpub.com/product/learn-helm/9781839214295"><span class="url">https://www.packtpub.com/product/learn-helm/9781839214295</span></a>)</li>
    </ul>
    <p class="normal">You can learn more about Elasticsearch and Prometheus in the following <em class="italic">Packt Publishing</em> books:</p>
    <ul>
      <li class="bulletList"><em class="italic">Learning Elasticsearch</em>, by <em class="italic">Abhishek Andhavarapu</em> (<a href="https://www.packtpub.com/product/learning-elasticsearch/9781787128453"><span class="url">https://www.packtpub.com/product/learning-elasticsearch/9781787128453</span></a>)</li>
      <li class="bulletList"><em class="italic">Hands-On Infrastructure Monitoring with Prometheus</em>, by <em class="italic">Joel Bastos</em>, <em class="italic">Pedro Araujo</em> (<a href="https://www.packtpub.com/product/hands-on-infrastructure-monitoring-with-prometheus/9781789612349"><span class="url">https://www.packtpub.com/product/hands-on-infrastructure-monitoring-with-prometheus/9781789612349</span></a>)</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-505">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img alt="" src="image/QR_Code119001106479081656.png"/></p>
  </div>
</body></html>