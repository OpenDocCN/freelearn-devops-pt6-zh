- en: '*Chapter 10*: Building, Deploying, and Monitoring Your Model'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 10 章*：构建、部署和监控你的模型'
- en: In the previous chapter, you built the data pipeline and created a basic flight
    dataset that can be used by your data science team. In this chapter, your data
    science team will use the flight dataset to build a **machine learning** (**ML**)
    model. The model will be used to predict the on-time performance of the flights.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，你已经构建了数据管道，并创建了一个基础的航班数据集，供你的数据科学团队使用。在本章中，你的数据科学团队将使用该航班数据集来构建 **机器学习**
    (**ML**) 模型。该模型将用于预测航班的准点表现。
- en: In this chapter, you will see how the platform assists you in visualizing and
    experimenting with the data to build the right model. You will see how to tune
    hyperparameters and compare the results of different runs of model training. You
    will see how to register and version models using the components provided by the
    platform. You will deploy the model as a REST service and start monitoring the
    deployed model using the components provided by the platform.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章，你将看到平台如何帮助你可视化和实验数据，以构建正确的模型。你将学习如何调整超参数，并比较不同训练过程的结果。你还将看到如何使用平台提供的组件注册和管理模型版本。你将把模型作为
    REST 服务进行部署，并开始使用平台提供的组件来监控已部署的模型。
- en: Remember that this book is not about data science, instead, the focus is on
    enabling teams to work autonomously and efficiently. You may see some concepts
    and steps being repeated from earlier chapters. This is intentional to show you
    how the concepts provided in the previous chapters help you build a full life
    cycle.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，本书并不是关于数据科学的，重点在于使团队能够自主高效地工作。你可能会看到一些概念和步骤在前面的章节中被重复。这是故意的，目的是向你展示前面章节中提供的概念如何帮助你构建完整的生命周期。
- en: 'Keeping the goal in mind, you will learn about the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 保持目标清晰，你将学习以下内容：
- en: Visualizing and exploring data using JupyterHub
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 JupyterHub 可视化和探索数据
- en: Building and tuning your model using JupyterHub
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 JupyterHub 构建和调整你的模型
- en: Tracking model experiments and versioning using MLflow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MLflow 跟踪模型实验和版本管理
- en: Deploying your model as a service via Seldon and Airflow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 Seldon 和 Airflow 将你的模型部署为服务
- en: Monitoring your model using Prometheus and Grafana
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Prometheus 和 Grafana 监控你的模型
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter includes some hands-on setup and exercises. You will need a running
    Kubernetes cluster configured with the **Operator Lifecycle Manager** (**OLM**).
    Building such a Kubernetes environment is covered in [*Chapter 3*](B18332_03_ePub.xhtml#_idTextAnchor040),
    *Exploring Kubernetes*. Before attempting the technical exercises in this chapter,
    please make sure that you have a working Kubernetes cluster and **Open Data Hub**
    (**ODH**) is installed on your Kubernetes cluster. Installing ODH is covered in
    [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The Anatomy of a Machine
    Learning Platform*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含一些实践设置和练习。你需要一个已配置**Operator Lifecycle Manager**（**OLM**）的运行中的 Kubernetes
    集群。如何构建这样的 Kubernetes 环境，已在[*第 3 章*](B18332_03_ePub.xhtml#_idTextAnchor040)，*探索
    Kubernetes* 中进行了介绍。在尝试本章的技术练习之前，请确保你拥有一个工作中的 Kubernetes 集群，并且**Open Data Hub**（**ODH**）已安装在你的
    Kubernetes 集群上。ODH 的安装内容已在[*第 4 章*](B18332_04_ePub.xhtml#_idTextAnchor055)，*机器学习平台的构成*
    中进行了详细讲解。
- en: Visualizing and exploring data using JupyterHub
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 JupyterHub 可视化和探索数据
- en: Recall from [*Chapter 9*](B18332_09_ePub.xhtml#_idTextAnchor132), *Building
    Your Data Pipeline*, that the data engineer has worked with the SME of the business
    and prepared the flight data that can be used to predict the flights' on-time
    performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下[*第 9 章*](B18332_09_ePub.xhtml#_idTextAnchor132)，*构建你的数据管道*，数据工程师与业务领域专家（SME）合作，准备了可用于预测航班准点表现的航班数据。
- en: In this section, you will understand the data produced by the data engineering
    team. This is the role of the data scientist who is responsible for building the
    model. You will see how the platform enables your data science and data engineering
    teams to collaborate and how the data scientist can use the platform to build
    a model for the given problem.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解数据工程团队产生的数据。这是数据科学家的职责，他们负责构建模型。你将看到平台如何使数据科学团队和数据工程团队进行协作，以及数据科学家如何利用平台为给定问题构建模型。
- en: 'Let''s do some base data exploring using the platform. Keep in mind that the
    focus of this book is to enable your team to work efficiently. The focus is not
    on data science or data engineering but on building and using the platform:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Launch JupyterHub, but this time select the image that is relative to the data
    science life cycle. SciKit is one such image available on the platform. Do not
    click on the **Start server** button just yet.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.1 – JupyterHub landing page'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_001.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 – JupyterHub landing page
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: On the JupyterHub landing page, add an `AWS_SECRET_ACCESS_KEY` variable and
    populate it with the password for your S3 environment. The value for this key
    for this exercise would be `minio123`. Notice that we have used the **Medium**
    container size to accommodate the dataset. Now, hit the **Start server** button
    to start your JupyterHub IDE.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.2 – JupyterHub landing page'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_002.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – JupyterHub landing page
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Open the `chapter10/visualize.ipynb` file notebook in your JupyterHub IDE.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first step is to read the data provided by the data engineering team. Note
    that the data is available on the same platform, which improves the velocity of
    the teams. *Cell 2* in the notebook is using the `PyArrow` library to read the
    data as a pandas data frame. You will read the data from the `flights-data` bucket,
    where data is placed by the data team. You can see the data read code as follows:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Cell 2 for the chapter10/visualize notebook'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_003.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – Cell 2 for the chapter10/visualize notebook
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing you will do is to look at the data. Trying to make sense of
    it and familiarizing yourself with what is available can be the ideal take here.
    You can see in *Cell 3* that the DataFrame''s `head` function has been used to
    see the first few rows. You will notice the field names and the data in them and
    see whether you can understand one record. Notice that some fields are `NaN` and
    some are `None`. This gives you a clue that the dataset may not yet be ready for
    building models. The following screen captures partial output, and it is expected
    that you run this code in your environment to get the full picture:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Cell 3 for the chapter10/visualize notebook'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_004.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 – Cell 3 for the chapter10/visualize notebook
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'The next stage is to do a simple verification to see how much data is available
    for you and if you are reading all the records. You can see in *Cell 4* that the
    DataFrame''s `count` function has been used for this. The following screen captures
    partial output, and it is expected that you run this code in your environment
    to get the full picture:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Cell 4 for the chapter10/visualize notebook'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_005.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – Cell 4 for the chapter10/visualize notebook
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '*Cells 5* and *6* are using the DataFrame''s shape and the columns'' functions
    are self-explanatory.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Cell 7* is using the DataFrame''s `describe` function to generate some basic
    statistics for the dataset. You may use this to verify whether there is some data
    that may not make sense. An example could be an exceedingly high value as maximum
    for the `taxi_in` time. In such cases, you will work with your SME to clarify
    and adjust the records as needed. The following screen captures partial output,
    and it is expected that you run this code in your environment to get the full
    picture:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*第7单元格*使用了DataFrame的`describe`函数来生成数据集的一些基本统计信息。你可以使用这个方法验证是否有一些数据不合逻辑。例如，`taxi_in`时间的最大值可能过高。在这种情况下，你需要与主题专家（SME）合作，澄清并根据需要调整记录。以下截图显示了部分输出，建议你在自己的环境中运行这段代码以查看完整内容：'
- en: '![Figure 10.6 – Cell 7 for the chapter10/visualize notebook'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.6 – 第10章/可视化笔记本的第7单元格'
- en: '](img/B18332_10_006.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_006.jpg)'
- en: Figure 10.6 – Cell 7 for the chapter10/visualize notebook
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – 第10章/可视化笔记本的第7单元格
- en: 'Next, you want to see whether the data has null values. You have seen in *Step
    3*, that there are some `NaN` and `None` values in the data. You have found out
    that there are many columns with missing data problems. The following screen captures
    partial output, and it is expected that you run this code in your environment
    to get the full picture:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你想查看数据是否有空值。你在*步骤3*中已经看到数据中存在一些`NaN`和`None`值，并发现有许多列存在缺失数据的问题。以下截图显示了部分输出，建议你在自己的环境中运行这段代码以查看完整内容：
- en: '![Figure 10.7 – Cell 8 for the chapter10/visualize notebook'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.7 – 第10章/可视化笔记本的第8单元格'
- en: '](img/B18332_10_007.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_007.jpg)'
- en: Figure 10.7 – Cell 8 for the chapter10/visualize notebook
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 第10章/可视化笔记本的第8单元格
- en: You will use the Dataframe's `isnull` function to find out how many records
    have this missing data. Using the output from the `df.isnull().sum().sort_values(ascending
    = False)` code, there are two different groups. The first six rows of the output
    show column names that have a very high missing data rate and for these columns,
    you may talk to data engineering and the SME to find resources from where you
    can fetch the data for them. For our example, we will just drop these columns.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将使用Dataframe的`isnull`函数来查找有多少记录缺失数据。通过运行`df.isnull().sum().sort_values(ascending
    = False)`代码输出，你会发现数据分为两组。输出的前六行显示了缺失数据率非常高的列，对于这些列，你可以与数据工程团队和主题专家（SME）沟通，找出可以从中获取数据的资源。对于本示例，我们将删除这些列。
- en: '![Figure 10.8 – Cell 9 for the chapter10/visualize notebook'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.8 – 第10章/可视化笔记本的第9单元格'
- en: '](img/B18332_10_008.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_008.jpg)'
- en: Figure 10.8 – Cell 9 for the chapter10/visualize notebook
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 – 第10章/可视化笔记本的第9单元格
- en: In the second group, starting from the `wheels_on` column, you may either choose
    to drop the rows containing no data or try to fill the data with a suitable statistics
    function. For example, the missing `taxi_in` columns could be the mean for the
    same airport and same time. The strategy must be discussed with the team. For
    this exercise, we will just drop the rows.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二组数据中，从`wheels_on`列开始，你可以选择删除没有数据的行，或者尝试使用适当的统计函数填充数据。例如，缺失的`taxi_in`列可以用同一机场和相同时间的均值来填补。此策略需与团队讨论。对于本次练习，我们将直接删除这些行。
- en: '![Figure 10.9 – Cell 9 for the chapter10/visualize notebook'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.9 – 第10章/可视化笔记本的第9单元格'
- en: '](img/B18332_10_009.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_009.jpg)'
- en: Figure 10.9 – Cell 9 for the chapter10/visualize notebook
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 – 第10章/可视化笔记本的第9单元格
- en: 'Often, it is a good idea to investigate sample rows where a particular column
    has no data. You may find a pattern in the data that could be extremely useful
    in further understanding the data. You have chosen to see the rows where the `tail_number`
    field has no value and see whether you can find any patterns. The following screen
    captures partial output, and it is expected that you run this code in your environment
    to get the full picture:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常，调查某一列缺失数据的示例行是一个不错的主意。你可能会在数据中发现某些模式，这对进一步理解数据可能非常有用。你选择查看`tail_number`字段没有值的行，并查看是否能发现任何模式。以下截图显示了部分输出，建议你在自己的环境中运行这段代码以查看完整内容：
- en: '![Figure 10.10 – Cell 10 for the chapter10/visualize notebook'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.10 – 第10章/可视化笔记本的第10单元格'
- en: '](img/B18332_10_010.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_010.jpg)'
- en: Figure 10.10 – Cell 10 for the chapter10/visualize notebook
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 – 第10章/可视化笔记本的第10单元格
- en: 'You will then run the Dataframe''s `info` function to find out the data types
    of the columns. A lot of times, the data types of columns are not the ones that
    you are expecting. You will then talk to the SME and data teams to improve the
    data quality. The following screen captures partial output, and it is expected
    that you run this code in your environment to get the full picture:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你将运行Dataframe的`info`函数，以找出列的数据类型。很多时候，列的数据类型并不是你所预期的。然后你将与SME和数据团队合作，改善数据质量。以下截图展示了部分输出，预计你会在自己的环境中运行这段代码以获取完整信息：
- en: '![Figure 10.11 – Cell 11 for the chapter10/visualize notebook'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.11 – 第10章/可视化笔记本中的单元格11](img/B18332_10_011.jpg)'
- en: '](img/B18332_10_011.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_011.jpg)'
- en: Figure 10.11 – Cell 11 for the chapter10/visualize notebook
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11 – 第10章/可视化笔记本中的单元格11
- en: Visualization is one particularly important tool to understand data. You can
    use any of the libraries that you feel comfortable with. For example, in the last
    cell of this notebook, you build a graph to find out the data distribution for
    the `DELAYED` column. Imagine that 99% of the records are with the `DELAYED` column
    as `0`. If that is the case, the data may not be enough to predict the flights'
    on-time performance and you will need to engage the SME and data teams to get
    more data. For this exercise, we will use the existing data distribution.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化是理解数据的一个特别重要的工具。你可以使用任何你熟悉的库。例如，在这个笔记本的最后一个单元格中，你构建了一个图表来找出`DELAYED`列的数据分布。假设99%的记录中，`DELAYED`列为`0`。如果是这样，数据可能不足以预测航班的准点表现，你需要与SME和数据团队合作，获取更多的数据。对于这个练习，我们将使用现有的数据分布。
- en: '![Figure 10.12 – Cell 12 for the chapter10/visualize notebook'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.12 – 第10章/可视化笔记本中的单元格12](img/B18332_10_012.jpg)'
- en: '](img/B18332_10_012.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_012.jpg)'
- en: Figure 10.12 – Cell 12 for the chapter10/visualize notebook
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12 – 第10章/可视化笔记本中的单元格12
- en: Now that we understand flight data a bit better, let's start building our model.
    In the real world, you would invest a lot more time to understand the data. The
    focus of this book is to show you how to execute the model development life cycle
    and so we kept the examples to a minimum.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对航班数据有了更好的理解，让我们开始构建模型。在现实世界中，你需要投入更多的时间来理解数据。本书的重点是展示如何执行模型开发生命周期，因此我们将示例保持在最低限度。
- en: Building and tuning your model using JupyterHub
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用JupyterHub构建和调优模型
- en: As a data scientist, you will want to try different models with different parameters
    to find the right fit. Before you start building the model, recall from [*Chapter
    8*](B18332_08_ePub.xhtml#_idTextAnchor116), *Building a Complete ML Project Using
    the Platform*, that you need to define the evaluation criteria, and that **accuracy**
    may be a misleading criterion for a lot of use cases.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名数据科学家，你将希望尝试不同的模型和参数，以找到最合适的模型。在你开始构建模型之前，请回忆一下在[*第8章*](B18332_08_ePub.xhtml#_idTextAnchor116)《使用平台构建完整的机器学习项目》中提到的，你需要定义评估标准，而且**准确率（accuracy）**可能对于很多使用案例来说是一个具有误导性的标准。
- en: For the flight use case, let's assume that your team and the SME agree on the
    **PRECISION** metric. Note that precision measures the portion of correct positive
    identification in the provided dataset.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于航班使用案例，假设你的团队和SME（主题专家）一致同意使用**精准度（PRECISION）**指标。请注意，精准度衡量的是提供的数据集中正确正向识别的比例。
- en: 'Let''s start writing our model and see how the platform enables data scientists
    to perform their work efficiently:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始编写我们的模型，看看平台如何使数据科学家高效地完成工作：
- en: Open the `chapter10/experiments.ipynb` file notebook in your JupyterHub environment.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的JupyterHub环境中打开`chapter10/experiments.ipynb`文件笔记本。
- en: In *Cell 2*, add the connection information to MLflow. Recall that MLflow is
    the component in the platform that records the model experiments and works as
    the model registry. In the code, you will configure `EXPERIMENT_NAME`, which provides
    a name for your experiment runs. The last line of this cell mentions how MLflow
    will record the experiment run. The `autolog` feature enables MLflow to register
    automatic callbacks during training to record the parameters for later use.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*单元格2*中，添加MLflow的连接信息。请回想一下，MLflow是平台中的组件，用于记录模型实验并充当模型注册中心。在代码中，你将配置`EXPERIMENT_NAME`，它为你的实验运行提供一个名称。该单元格的最后一行提到了MLflow如何记录实验运行。`autolog`功能使得MLflow在训练过程中注册自动回调，以记录参数以供以后使用。
- en: 'You also provide the configuration for the S3 bucket, which will be used by
    MLflow to store the artifacts of your experiments:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要提供S3桶的配置，该桶将由MLflow用于存储你的实验结果：
- en: '![Figure 10.13 – Cell 2 for the chapter10/experiments notebook'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.13 – 第 2 单元，位于第 10 章/实验笔记本中'
- en: '](img/B18332_10_013.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_013.jpg)'
- en: Figure 10.13 – Cell 2 for the chapter10/experiments notebook
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13 – 第 2 单元，位于第 10 章/实验笔记本中
- en: '*Cell 3* reads the data available from the data engineering team, and *Cell
    4* is again providing the information on the missing data from multiple columns.
    In this notebook, you will use this information to drop the columns that you do
    not find useful. The following screen captures partial output, and it is expected
    that you run this code in your environment to get the full picture:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*第 3 单元* 从数据工程团队读取可用数据，*第 4 单元* 再次提供了多列缺失数据的信息。在这本笔记本中，你将使用这些信息来删除你认为不实用的列。以下是部分输出的屏幕截图，建议在你的环境中运行此代码以获取完整的图像：'
- en: '![Figure 10.14 – Cell 3 for the chapter10/experiments notebook'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.14 – 第 3 单元，位于第 10 章/实验笔记本中'
- en: '](img/B18332_10_014.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_014.jpg)'
- en: Figure 10.14 – Cell 3 for the chapter10/experiments notebook
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.14 – 第 3 单元，位于第 10 章/实验笔记本中
- en: '*Cell 5* is dropping two sets of columns. The first set drops the columns for
    which you do not have data in most of the rows. You selected these columns based
    on the previous step. We kept it simple here and dropped the columns; however,
    it is highly recommended that you work with data teams to find the reason for
    this anomaly and aim to get as much data as possible. The columns you are dropping
    are `"cancellation_reason"`, `"late_aircraft_delay"`, `"weather_delay"`, `"airline_delay"`,
    `"security_delay"`, and `"air_system_delay"`, and are shown in the following screenshot:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*第 5 单元* 删除了两组列。第一组是那些在大多数行中没有数据的列。你根据前一步骤选择了这些列。在这里，我们简单地删除了这些列；然而，强烈建议你与数据团队合作，找出这种异常的原因，并尽可能获取更多数据。你正在删除的列包括
    `"cancellation_reason"`, `"late_aircraft_delay"`, `"weather_delay"`, `"airline_delay"`,
    `"security_delay"`, 和 `"air_system_delay"`, 并显示在以下截图中：'
- en: '![Figure 10.15 – Cell 5 for the chapter10/experiments notebook'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.15 – 第 5 单元，位于第 10 章/实验笔记本中'
- en: '](img/B18332_10_015.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_015.jpg)'
- en: Figure 10.15 – Cell 5 for the chapter10/experiments notebook
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.15 – 第 5 单元，位于第 10 章/实验笔记本中
- en: 'The second `drop` statement is dropping the `tail_number` column. This column
    may not play any role in flights getting delayed. In a real-world scenario, you
    will need to discuss this with the SMEs:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个 `drop` 语句删除了 `tail_number` 列。在实际场景中，你需要与专家讨论这一列是否对航班延误起到任何作用。
- en: '![Figure 10.16 – Cell 5 for the chapter10/experiments notebook'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.16 – 第 5 单元，位于第 10 章/实验笔记本中'
- en: '](img/B18332_10_016.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_016.jpg)'
- en: Figure 10.16 – Cell 5 for the chapter10/experiments notebook
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.16 – 第 5 单元，位于第 10 章/实验笔记本中
- en: '*Cell 6* is dropping rows for which the data is not available using the Dataframe''s
    `dropna` function. Recall, from *Step 3*, that the number of rows where data is
    missing from these columns is less compared to the total rows available. `air_time`,
    `arrival_delay`, and `elapsed_time` are examples of such columns from *Step 5*.
    We have adopted this approach to keep things simple; a better way would be to
    find a way to get the missing data or to create this data from existing values.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*第 6 单元* 使用 Dataframe 的 `dropna` 函数删除数据不可用的行。回想一下，从 *步骤 3* 中，与总行数相比，这些列缺失数据的行数较少。`air_time`,
    `arrival_delay` 和 `elapsed_time` 是 *步骤 5* 中的示例列。我们采用了这种方法来保持简单；更好的方法是找到获取缺失数据的途径或从现有值创建此数据。'
- en: '![Figure 10.17 – Cell 6 for the chapter10/experiments notebook'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.17 – 第 6 单元，位于第 10 章/实验笔记本中'
- en: '](img/B18332_10_017.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_017.jpg)'
- en: Figure 10.17 – Cell 6 for the chapter10/experiments notebook
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.17 – 第 6 单元，位于第 10 章/实验笔记本中
- en: In *Cell 7*, you are dropping columns for which you do not have data for future
    flights. Recall that the model aims to predict the future flight on-time performance.
    However, columns such as `departure_time` and `arrival_time` contain the actual
    departure and arrival times. For predicting future flights, you will not have
    such data available at the time of prediction, and so you need to drop these columns
    while training your model.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *第 7 单元*，你正在删除那些未来航班没有数据的列。回想一下，该模型旨在预测未来航班的准时性。然而，诸如 `departure_time` 和 `arrival_time`
    这样的列包含实际的出发和到达时间。对于预测未来航班，你在预测时将无法使用这些数据，因此在训练模型时需要删除这些列。
- en: '![Figure 10.18 – Cell 7 for the chapter10/experiments notebook'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.18 – 第 7 单元，位于第 10 章/实验笔记本中'
- en: '](img/B18332_10_018.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_018.jpg)'
- en: Figure 10.18 – Cell 7 for the chapter10/experiments notebook
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.18 – 第 7 单元，位于第 10 章/实验笔记本中
- en: In the dataset, the scheduled departure and arrival time is available in HHMM
    format, where HH is hours and MM is minutes. In *Cell 8*, as a data scientist,
    you may choose to split this data into two different columns where one column
    represents the hours and the other one represents the minutes. Doing this may
    simplify the dataset and improve the model performance if some correlation exists
    between the expected classification and split data. You may do it out of your
    intuition, or you may discuss this option with the SMEs.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You have chosen to split the `scheduled_departure` and `scheduled_arrival`
    columns:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.19 – Cell 8 for the chapter10/experiments notebook'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_019.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.19 – Cell 8 for the chapter10/experiments notebook
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Cell 9*, you drop a few more columns. The first set contains columns for
    which we have to split the time into hours and minutes, such as `scheduled_arrival`:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.20 – Cell 9 for the chapter10/experiments notebook'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_020.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.20 – Cell 9 for the chapter10/experiments notebook
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'The second set contains the columns that are represented in other columns.
    For example, the `origin_airport` column has a key for the airport, and the `ORIG_AIRPORT`
    column is a descriptive name. Both these columns represent the same information:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.21 – Cell 9 for the chapter10/experiments notebook'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_021.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.21 – Cell 9 for the chapter10/experiments notebook
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Cell 10*, you visually see the dataset again using the `head` statement.
    You have noticed that you have some data in string format, such as the `airline`
    column:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.22 – Cell 10 for the chapter10/experiments notebook'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_022.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.22 – Cell 10 for the chapter10/experiments notebook
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'You choose to encode that data to convert it into numbers. There are many techniques
    available, such as `OrdinalEncoder`. This encoder encodes categorical values as
    an integer array. In *Cell 12*, you have applied the category encoding to the
    selected fields such as `airline` and `origin_airport`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.23 – Cell 12 for the chapter10/experiments notebook'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_023.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.23 – Cell 12 for the chapter10/experiments notebook
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the input string data for these fields will be converted into
    integers. This is good for training; however, at inferencing time, the caller
    may not know about this encoding that you have just performed. One way is to save
    this encoder and use it at inferencing time to convert the value from string to
    integers. So, your inferencing pipeline would consist of two steps. The first
    step is to apply the encoding and the second step is to predict the response using
    the saved mode. In the last four lines of *Cell 12*, you have saved the encoder
    and have to register it with MLflow:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.24 – Cell 12 for the chapter10/experiments notebook'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_024.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.24 – Cell 12 for the chapter10/experiments notebook
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Cell 13*, you validate the data using the `head` statement. Notice that
    the `airline` column (one of the columns that you have applied the category encoding
    to) has changed. For example, compare the value of the `airline` column from *Cell
    10* and *Cell 13* and notice that the value of `airline` column has been changed
    to `1`. This confirms that the encoding has been applied to the dataset successfully:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.25 – Cell 13 for the chapter10/experiments notebook'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_025.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.25 – Cell 13 for the chapter10/experiments notebook
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: In *Cell 14*, you used the `dftype` statement to validate the data types of
    each column in the dataset. Many algorithms need data to be in a numerical format
    and, based on the available models, you may need to move all the fields to a numerical
    format.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In *Cell 15*, you have split your data into training and testing sets. You will
    train the model using the `X_Train` and `y_train` set and use the `X_Test` and
    `y_test` for validation of your model performance. You can perform cross-validation
    to further assess the model performance on unseen data. We assume that you, as
    a data scientist, are aware of such concepts and, therefore, will not provide
    more details on this.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.26 – Cell 15 for the chapter10/experiments notebook'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_026.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.26 – Cell 15 for the chapter10/experiments notebook
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Cell 16*, you visualize the data distribution of the dataset. The following
    screenshot captures partial output, and it is expected that you run this code
    in your environment to get the full picture:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.27 – Cell 16 for the chapter10/experiments notebook'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_027.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.27 – Cell 16 for the chapter10/experiments notebook
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: You can see from the preceding chart that the data is biased towards the on-time
    flights. This may impact the performance of the model. Luckily, the `RandomForestClassifier`
    object of the `SciKit` library provides a `class_weight` parameter. It can take
    a Python `dictionary` object where we can provide the desired weights for respective
    labels. One such example would be to allocate less weight for a value of `0` in
    the `DELAYED` column, which represents the on-time flight. A different value for
    `class_weight` could be `balanced`, which will direct the algorithm to weigh the
    labels as per the inverse proportion to their occurrence frequency. Simply, for
    our case, the `balanced` value will put more weight on the value of `1` as compared
    to the value of `0` in the `DELAYED` column.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: In *Cell 19*, you define a random forest classification model and in *Cell 20*,
    you train the model. You have noticed that we have defined very minimal hyperparameters
    and then used `GridSearchCV` to find the best estimator for the given dataset.
    We have placed a separate set of hyperparameters in the comments of this cell.
    You are encouraged to try different combinations.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.28 – Cell 19 for the chapter10/experiments notebook'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_028.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.28 – Cell 19 for the chapter10/experiments notebook
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.28 – 第19单元格，来自chapter10/experiments笔记本
- en: '*Figure 10.29* shows how the model training is performed by executing the `model.fit()`
    function:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.29*展示了通过执行`model.fit()`函数来进行模型训练：'
- en: '![Figure 10.29 – Cell 20 for the chapter10/experiments notebook'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.29 – 第20单元格，来自chapter10/experiments笔记本]'
- en: '](img/B18332_10_029.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_029.jpg)'
- en: Figure 10.29 – Cell 20 for the chapter10/experiments notebook
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.29 – 第20单元格，来自chapter10/experiments笔记本
- en: The training will take some time to complete, so for *Cell 20*, where you are
    training your model, be patient.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 训练将需要一些时间，因此对于*第20单元格*，即您正在训练模型的单元格，请耐心等待。
- en: 'In *Cell 21*, you have used the `predict` method to capture the model prediction
    for the test data. Note that the `rf_best_model` model is the output of the `GridSearchCV`
    object:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*第21单元格*中，您使用了`predict`方法来捕获模型对测试数据的预测。请注意，`rf_best_model`模型是`GridSearchCV`对象的输出：
- en: '![Figure 10.30 – Cell 21 for the chapter10/experiments notebook'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.30 – 第21单元格，来自chapter10/experiments笔记本]'
- en: '](img/B18332_10_030.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_030.jpg)'
- en: Figure 10.30 – Cell 21 for the chapter10/experiments notebook
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.30 – 第21单元格，来自chapter10/experiments笔记本
- en: 'In *Cell 22*, you have used the `confusion_matrix` function to calculate the
    matrix and validate the performance of your model:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*第22单元格*中，您使用了`confusion_matrix`函数来计算矩阵并验证模型的性能：
- en: '![Figure 10.31 – Cell 22 for the chapter10/experiments notebook'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.31 – 第22单元格，来自chapter10/experiments笔记本]'
- en: '](img/B18332_10_031.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_031.jpg)'
- en: Figure 10.31 – Cell 22 for the chapter10/experiments notebook
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.31 – 第22单元格，来自chapter10/experiments笔记本
- en: 'In *Cell 23*, you have used the `precision_score` function to calculate `recallscore`
    for your model on the test dataset. You can see that you have achieved 72% precision,
    which is good for the first experiment run. You can run more experiments and improve
    the metrics for your model using the platform:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*第23单元格*中，您使用了`precision_score`函数来计算模型在测试数据集上的`recallscore`。您可以看到，您在第一次实验运行中达到了72%的精度，这对于第一次实验运行来说是不错的。您可以通过平台运行更多实验并提升模型的指标：
- en: '![Figure 10.32 – Cell 23 for the chapter10/experiments notebook'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.32 – 第23单元格，来自chapter10/experiments笔记本]'
- en: '](img/B18332_10_032.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_10_032.jpg)'
- en: Figure 10.32 – Cell 23 for the chapter10/experiments notebook
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.32 – 第23单元格，来自chapter10/experiments笔记本
- en: You have completed one experiment run with multiple parameters and the `RandomForestClassifier`
    model. At this stage, you may want to check MLflow and see all the runs the grid
    search has performed, captured parameters, and model performance data.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经完成了一个带有多个参数的实验运行，并使用了`RandomForestClassifier`模型。在此阶段，您可能想检查MLflow，查看网格搜索执行的所有运行、捕获的参数以及模型性能数据。
- en: Typically, data scientists try multiple algorithms to find the right fit for
    the given problem. It is up to you to execute and enhance the code and use MLflow
    to compare different algorithms.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据科学家会尝试多种算法，以找到适合给定问题的最佳算法。您可以执行并优化代码，并使用MLflow比较不同的算法。
- en: Let's see what MLflow has recorded for us.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看MLflow为我们记录了什么。
- en: Tracking model experiments and versioning using MLflow
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MLflow跟踪模型实验和版本控制
- en: In this section, you will use MLflow to track your experiment and version your
    model. This small section is a review of the capabilities highlighted to you in
    [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086), *Machine Learning Engineering*,
    where we discussed MLflow in detail.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将使用MLflow来跟踪您的实验并对您的模型进行版本控制。此小节是对[*第6章*](B18332_06_ePub.xhtml#_idTextAnchor086)的回顾，*机器学习工程*，我们在其中详细讨论了MLflow。
- en: Tracking model experiments
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪模型实验
- en: In this section, you will see the data recorded by MLflow for your experiment.
    Note that you have just registered the MLflow and called the `autolog` function,
    and MLflow automatically records all your data. This is a powerful capability
    in your platform through which you can compare multiple runs and share your findings
    with your team members.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将看到MLflow为您的实验记录的数据。请注意，您刚刚注册了MLflow并调用了`autolog`函数，MLflow会自动记录您的所有数据。这是您平台的一项强大功能，您可以通过它比较多个实验并与团队成员共享您的发现。
- en: 'The following steps shows you how experiment tracking is performed in MLflow:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤展示了如何在MLflow中执行实验跟踪：
- en: Log in to the MLflow UI of the platform.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到平台的MLflow UI。
- en: 'On the left-hand side, you will see the **Experiments** section and it contains
    your experiment named **FlightsDelay-mluser**. Click on it and you will see the
    following screen. The right-hand side shows all the runs. Recall that we have
    used GridSearchCV so there will be multiple runs:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.33 – The model tracking details in MLflow'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_033.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.33 – The model tracking details in MLflow
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Click on the `autolog` feature and MLflow will capture the bulk of the metrics
    automatically. Select all four runs and hit the **Compare** button.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 10.34* shows the comparison of each run and the hyperparameters associated
    with the run:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.34 – Comparing models in MLflow'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_034.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.34 – Comparing models in MLflow
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the run next to the `FlightsDelayOrdinalEncoder.pkl`:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.35 – Files and data captured by MLflow'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_035.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.35 – Files and data captured by MLflow
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you have seen that MLflow captured all the metrics from your
    training run and assisted you in selecting the right model by providing a comparison
    function.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: The next stage is to version your model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Versioning models
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After giving some thought to the model performance and sharing the data with
    other team members, you have selected the model that can be used for this project.
    In this section, you will version your model to be used. Refer to [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086),
    *Machine Learning Engineering*, where we discussed model versioning in detail.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will guide you on how to version your model:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Go to MLflow and click on the **FlightDelay-mluser** experiment on the left-hand
    side.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, on the right-hand side of the screen, click on the **+** icon for your
    run. You will see the following screen:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.36 – Files and data captured by MLflow'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_036.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.36 – Files and data captured by MLflow
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **model** folder under artifacts and a blue button with the **Register
    Model** label will appear:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.37 – Versioning your models in MLflow'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_037.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.37 – Versioning your models in MLflow
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the `flights-ontime`:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.38 – Model registration in MLflow'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_038.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.38 – Model registration in MLflow
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: As a data scientist, you have registered your model for predicting flight delays
    onto the model registry. The next step is to deploy your model.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the model as a service
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will deploy your model as a REST service. You will see
    that using the details mentioned in [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098),
    *Model Deployment and Automation*, the team can package and deploy the model as
    a service. This service will then be consumed by users of your model. We highly
    encourage you to refresh your knowledge from [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098),
    *Model Deployment and Automation* before proceeding to this section.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098), *Model Deployment and
    Automation*, you have deployed the model with a `Predictor` class, which exposes
    the model as a REST service. You will use the same class here, however, in the
    flight project, you applied categorical encoding to the data before it was used
    for model training. This means that you will need to apply the same encoding to
    the input data at the inferencing time. Recall that, earlier in this chapter,
    you saved the file as `FlightsDelayOrdinalEncoder.pkl` and it is available in
    the MLflow repository.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to write a simple class that can apply the transformation to
    the input data. Once this class is defined, you will define your inference pipeline
    using Seldon and then package your model as a container. So, your inference pipeline
    will consist of two stages; the first stage is to apply the encoding and the second
    stage is to use the model to predict the class.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Sounds difficult? You will see that the platform abstracts most of the details,
    and you will provide a few configuration parameters to package and deploy your
    model as a service.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first see the `Transformer` class, which will load the `FlightsDelayOrdinalEncoder.pkl`
    file and apply the encoding to the input data. Open the `chapter10/model_deploy_pipeline/model_build_push/Transformer.py`
    file. You will see that the `__init__` function loads the encoder file and the
    `transform_input` function applies the transformation to the input data using
    the standard `transform` function. This is the same function you have used during
    the model training. *Figure 10.39* shows the code file:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.39 – Transformer class'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_039.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.39 – Transformer class
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: The second artifact is to define the model inference graph. Recall from [*Chapter
    7*](B18332_07_ePub.xhtml#_idTextAnchor098), *Model Deployment and Automation*,
    that you have defined a container and one stage in your inference graph using
    the `SeldonDeploy.yaml` file. In this section, you will extend the inference graph
    to cater to the transformation and the prediction part of the inference pipeline.
    Naturally, when you define a new component in your graph, you will also need to
    define the corresponding container that will be the service for the graph node.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Note that you may choose to execute the transformation logic in `Predict.py`
    to keep things simple. However, we wanted to show how Seldon can build complicated
    graphs and each graph could be a separate instance of a container. This approach
    brings versatility to running your production models in an elastic fashion.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: So, let's look into the `chapter10/model_deploy_pipeline/model_deploy/SeldonDeploy.yaml`
    file. This file has been copied from [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098),
    *Model Deployment and Automation*, and the following changes have been made to
    it.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'The first change is to build the inference graph. You need to apply the transformation
    first and then run the model prediction. *Figure 10.40* displays this graph. Note
    that the root element for the graph is of the `TRANSFORMER` type with the name
    `transformer`, and there is a `children` node in the graph. The `children` node
    will be executed after the root node. This setup allows you to have different
    graphs as per your model requirements. The child node in this example is the actual
    prediction:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.40 – Seldon deployment YAML'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_040.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.40 – Seldon deployment YAML
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: The second change to the `chapter10/model_deploy_pipeline/model_deploy/SeldonDeploy.yaml`
    file is registering the containers for both the root and the child node. The `name`
    field in the graph is the one that associates the container to the graph node.
    So, we will have two instances of a container, one for `transformer` and the second
    for `predictor`. The `transformer` instance will execute the `Transformer.py`
    file and the `predictor` instance will execute the `Predictor.py` file. What we
    have done is create a single container image with all these files, so our container
    image is the same. You can examine the `chapter10/model_deploy_pipeline/model_build_push/Dockerfile.py`
    file where you package all the files into a container image. *Figure 10.41* highlights
    the part of `SeldonDeploy.yaml` where the containers have been configured.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the first container is with the name `transformer`. The `MODEL_NAME`
    variable mentions the name of the Python file and the `SERVICE_TYPE` variable
    mentions the type of callback to call by Seldon. Recall that `Transformer.py`
    has a `transform_input` method, and `SERVICE_TYPE` guides the Seldon system to
    call the right function. The same is applied to the `predictor` container instance,
    and note how `MODEL_NAME` and `SERVICE_TYPE` are different for the `predictor`
    instance:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.41 – Seldon deployment YAML'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_041.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.41 – Seldon deployment YAML
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: That is it! For some of you, this may be a little overwhelming, but once you
    have defined the structure for your projects, these files can be standardized,
    and the data scientists will not need to change them for every project. You have
    seen how the ML platform allows you to be self-sufficient in not only building
    the models but also packaging them.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to write a simple Airflow pipeline to deploy your model. Before
    you start this section, we recommend refreshing your knowledge of deploying the
    models using Airflow as detailed in [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098),
    *Model Deployment and Automation*. There is no change required in the pipeline
    that you have built, and you will just be changing a few configuration parameters
    to provide the right model name and version to the pipeline.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: We have prebuilt this pipeline for you, so, open the `chapter10/model_deploy_pipeline/flights_model.pipeline`
    file. Open this file and validate that it has the same two stages as mentioned
    in [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098)*, Model Deployment and
    Automation*. The first stage builds and pushes the container image to a container
    registry and the second stage deploys the model using Seldon.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10.42* displays the first stage with the parameters used for building
    and pushing the container image. **Runtime Image** and **File Dependencies** have
    the same values as shown earlier. Notice the **Environment Variables** section,
    where you have the same variable names but different values:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.42 – Flights model deploy pipeline'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_042.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.42 – Flights model deploy pipeline
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see each of them:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '`MODEL_NAME` has a value of `flights-ontime`. This is the name of the model
    you were given when you registered the model with MLflow.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MODEL_VERSION` has a value of `1`. This is the version of the model you would
    like to deploy. This version is recorded in the MLflow system.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTAINER_DETAILS` has a value of `flights-ontime`. This is the name of the
    model you were given when you registered the model with MLflow.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTAINER_REGISTRY` is the container registry API endpoint. For DockerHub,
    this is at [https://index.docker.io/v1](https://index.docker.io/v1). Set the value
    of this variable to [https://index.docker.io/v1/](https://index.docker.io/v1/).
    In this example, we have used [quay.io](http://quay.io) as the registry. This
    is another free registry that you can use.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTAINER_REGISTRY_USER` is the username of the user that will push images
    to the image registry. Set this to your DockerHub username or Quay username.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTAINER_REGISTRY_PASSWORD` is the password of your container registry user.
    In production, you do not want to do this. You may use secret management tools
    to serve your password.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTAINER_DETAILS` is also the name of the repository to where the image will
    be pushed and the image name and image tag. *Figure 10.43* displays the second
    stage with the parameters used for deploying the container image using Seldon.
    `MODEL_NAME`, `MODEL_VERSION`, `CONTAINER_DETAILS`, and `CLUSTER_DOMAIN`. You
    have seen all the variables in the preceding paragraph, but `CLUSTER_DOMAIN` is
    the DNS name of your Kubernetes cluster. In this case, the IP address of minikube
    is `<Minikube IP>.nip.io`.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.43 – Flights model deploy pipeline'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_043.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.43 – Flights model deploy pipeline
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Save and deploy this DAG to your Airflow environment and the model will be available
    for consumption when the Airflow DAG has finished execution. Validate that this
    DAG has been executed correctly by logging into Airflow and checking the status
    of the DAG. *Figure 10.44* shows the Airflow UI where you have validated the DAG's
    status. Notice that we have saved the DAG under the name `flights-model-deploy`;
    if you have chosen some other name, your DAG name will reflect accordingly.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.44 – Airflow DAG for the flights pipeline'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_044.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.44 – Airflow DAG for the flights pipeline
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Recall that MLflow associates a run ID for each of the experiments. You register
    one of these experiments in the model registry so it can be deployed. Refer to
    *Figure 10.34*, which shows a screenshot of the run ID for this model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'This model run will be associated with the deployed model, so your team can
    track the models running in the environment to an individual run. This capability
    provides a trace back on what version of the model is running in different environments.
    Run the following command to see the resources created by the model:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You should get the following response. As you can see, the Kubernetes service
    and ingress have a run ID that starts with `bf32` for this example. Note that
    it will have a different value for your case, and you will need to adjust the
    run ID in the preceding command:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.45 – Kubernetes objects created by the platform'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_045.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.45 – Kubernetes objects created by the platform
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Now, the model is deployed; you now test the model by running a RESTful call
    to your model.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Calling your model
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall that the model is exposed via the Kubernetes Ingress, which is created
    by automation. In order to test whether the model is running properly as a RESTful
    API, follow these steps:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to get the `ingress` object. Note that the name of
    the `ingress` object will be different for your setup:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, make an HTTP call to the location where your model is available for inference.
    Run the following commands. The `chapter10/inference` folder contains a payload
    for the flight data and in return, the model will predict the probability of the
    flight getting delayed.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, change the directory to the `chapter10/inference` folder:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, run a `curl` command to send the payload to the model. Note to change
    the HTTP address as per your setup:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Windows users may choose to use the excellent Postman application ([https://www.postman.com/](https://www.postman.com/))
    to make an HTTP call.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `chapter10/inference/data.json` file to see the payload that we are
    sending to the model. You will notice that there are two sections of the `json`
    payload. The first part is with the `names` key, which captures the feature columns
    that you have used to train the model. Notice that there is no `DELAYED` column
    here because the model will predict the probability of the `DELAYED` column. The
    second part is with the `ndarrray` key, which has the values for the feature columns.
    Note that the values for the categorical columns are in the original form and
    the inference pipeline will convert them into the categorical values before executing
    the model. *Figure 10.46* shows the following file:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.46 – Sample payload for flights model inferencing'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_046.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.46 – Sample payload for flights model inferencing
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have successfully performed an inference call over HTTP, let's
    see how the information has been captured by the monitoring system.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring your model
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this last section, you will see how the platform automatically starts capturing
    the typical performance metrics of your model. The platform also helps you visualize
    the performance of the inference. The platform uses Seldon to package the model,
    and Seldon exposes default metrics to be captured. Seldon also allows you to write
    custom metrics for specific models; however, it is out of the scope of this book.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by understanding how the metrics capture and visualization work.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Understanding monitoring components
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The way metrics capture works is that your model is wrapped by Seldon. Seldon
    then exposes the metrics to a well-defined URL endpoint, which was detailed in
    [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098), *Model Deployment and Automation*.
    Prometheus harvests this information and stores it in its database. The platform's
    Grafana connects to Prometheus and helps you visualize the recorded metrics.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10.47* summarizes the relationship between the model and monitoring
    components:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.47 – ML platform monitoring components'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_047.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.47 – ML platform monitoring components
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand each component of this diagram:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '**Open Data Hub (ODH) Operator**: This is the base operator for our platform.
    Its role is to provision all the different components for your platform. We have
    discussed this operator in various chapters of this book and so we do not describe
    it in this section.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`manifests/prometheus/base/subscription.yaml`. The following snippet shows
    that it uses the OLM mechanism to install the Prometheus operator:'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.48 – Subscription for Prometheus operator'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_048.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.48 – Subscription for Prometheus operator
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '`manifests/prometheus/base/prometheus.yaml`. The following snippet shows the
    file:'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.49 – Prometheus server configuration'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_049.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.49 – Prometheus server configuration
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '`manifests/prometheus/base/prometheus.yaml`. The following snippet shows the
    file. Note that the configuration uses port `8000`, which is the port at which
    Seldon exposes the metrics information. The `selector` object defines the filter
    by which Prometheus will decide what pods to scrape data from:'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.50 – Prometheus server monitors for Seldon pods'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_050.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.50 – Prometheus server monitors for Seldon pods
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '`manifests/grafana/base/deployment.yaml` file.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, you have seen how the platform provides and wires different
    components to provide you with a visualization framework for your observability
    requirements.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Next is to configure Grafana.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Grafana and a dashboard
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will configure Grafana to connect to Prometheus and build
    a dashboard to visualize the model's metrics. What is a dashboard? It is a set
    of graphs, tables, and other visualizations of your model. You will create a dashboard
    for the flight model.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is a one-time configuration, and it does not need to be repeated
    for every model. This means that once you have a dashboard, you can use it for
    multiple models. Your team may create a few standard dashboards and as soon as
    a new model is deployed, the platform will automatically find it and make it available
    for monitoring.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the configuration of the Grafana instance:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to Grafana using https://grafna.192.128.36.219.nip.io. Notice that you
    will need to change the IP address as per your setup. On the login page, click
    the **Sign in With KeyCloak** button, which is at the bottom of the login window:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.51 – Grafana login page'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_051.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.51 – Grafana login page
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you will need to add a data source. A data source is a system that will
    provide the data that Grafana will help you visualize. The data provider in Prometheus
    scrapes the metrics data from your models. Select the **Configuration** | **Data
    sources** option from the left-hand menu:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.52 – Grafana Data sources menu option'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_052.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.52 – Grafana Data sources menu option
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Add data source** button, as shown in the following screenshot:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.53 – Add new Grafana data source'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_053.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.53 – Add new Grafana data source
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Select the data source type, which will be Prometheus for your case. You may
    notice that Grafana can talk to a variety of data sources, including InfluxDB
    and YYYY, to name a couple.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.54 – Add new Prometheus Grafana data source'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_054.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.54 – Add new Prometheus Grafana data source
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you need to add the details for the Prometheus server. Grafana will use
    these details to connect and fetch data from the Prometheus server. Add the following
    properties in the screen mentioned:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Prometheus`'
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**URL**: http://prometheus-operated:9090'
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then click the **Save & test** button. The URL is the location of the Prometheus
    service created by the platform. Because the Grafana pod will talk to the Prometheus
    pod using the internal Kubernetes network, this URL will be the same for your
    setup too:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.55 – Configuration for the Prometheus Grafana data source'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_055.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.55 – Configuration for the Prometheus Grafana data source
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the `prometheus` service details by issuing the following command:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After you configure Grafana to connect to Prometheus, the next step is to build
    the dashboard. As mentioned earlier, a dashboard is a set of visualizations, and
    each visualization is backed by a query. Grafana runs those queries and plots
    the data for you. Building dashboards is out of the scope of this book, but we
    have provided a dashboard that you can use. Select the **Import** option from
    the left-hand menu:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.56 – Adding a new dashboard in Grafana'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_056.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.56 – Adding a new dashboard in Grafana
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'On the `chapter10/grafana-dashboard/sample-seldon-dashboard.json` file and
    paste it into the **Import via panel json** textbox. Click on the **Load** button
    to import the dashboard:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.57 – Importing a Seldon dashboard in Grafana'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_057.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.57 – Importing a Seldon dashboard in Grafana
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the name for your imported dashboard and click on the `Flights Prediction
    Analytics`, as you can see in the following screenshot:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.58 – Importing Seldon dashboard in Grafana'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_058.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.58 – Importing Seldon dashboard in Grafana
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: After you import the dashboard, Grafana will start displaying the dashboard
    immediately. You can see a few metrics such as response times, success rate, and
    other relative metrics for your deployed model. You may need to hit your model
    a few times to start populating this board. Refer to the *Calling your model*
    section earlier in this chapter on how to make calls to your deployed models.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.59 – Dashboard for Seldon models'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_10_059.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.59 – Dashboard for Seldon models
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the board captures the metrics that have been emitted by your
    model wrapped in Seldon. As more models get deployed, they will be available in
    this dashboard, and you can filter the models through the filters provided in
    the top bar of the dashboard.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Your flights on-time prediction service is now available for consumption. You
    will now work with the product development team and the website team of your organization
    so that they can integrate this functionality and provide a better service for
    your customers. Your work does not finish here; you will need to continuously
    see how the model is performing and bring on improvements via new data and/or
    optimizing your models further. The platform will help you to perform this cycle
    with higher velocity and continuously improve the offerings to your customers.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This was another long chapter that covered the model development and deployment
    life cycle for the flights on-time performance project. You have seen how the
    platform enables you and your team to become autonomous in EDA, model experimentation
    and tracking, model registry, and model deployment.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a step back and summarize our journey of the
    overall platform and how you can use it as your own solution that fits your vertical.
    You can use the concepts and tools to build a platform for your team and enable
    your business to realize the power of AI.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
