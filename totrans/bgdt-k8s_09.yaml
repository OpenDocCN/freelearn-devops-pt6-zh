- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Data Consumption Layer
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据消费层
- en: In today’s data-driven world, organizations are dealing with an ever-increasing
    volume of data, and the ability to effectively consume and analyze this data is
    crucial for making informed business decisions. As we delve into the realm of
    big data on Kubernetes, we must address the critical component of the data consumption
    layer. This layer serves as the bridge between the vast repositories of data and
    the business analysts who need to extract valuable insights and make decisions
    that have an impact on the business.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的数据驱动世界中，组织正在处理日益增长的数据量，有效地消费和分析这些数据对于做出明智的业务决策至关重要。当我们深入探讨基于 Kubernetes
    的大数据领域时，必须解决数据消费层的关键组成部分。这一层将作为数据存储库和需要提取有价值洞察并对业务产生影响的业务分析师之间的桥梁。
- en: 'In this chapter, we will explore two powerful tools that will enable you to
    unlock the potential of your Kubernetes-based data architecture: **Trino** and
    **Elasticsearch**. Trino, a distributed SQL query engine, will empower you to
    directly query your data lake, eliminating the need for a traditional data warehouse.
    You will learn how to deploy Trino on Kubernetes, monitor its performance, and
    execute SQL queries against your data stored in Amazon S3.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨两个强大的工具，它们将使您能够释放基于 Kubernetes 的数据架构的潜力：**Trino** 和 **Elasticsearch**。Trino，一个分布式
    SQL 查询引擎，将使您能够直接查询您的数据湖，消除了传统数据仓库的需求。您将学习如何在 Kubernetes 上部署 Trino，监视其性能，并对存储在
    Amazon S3 中的数据执行 SQL 查询。
- en: Furthermore, we will introduce Elasticsearch, a highly scalable and efficient
    search engine widely used in real-time data pipelines, along with **Kibana**,
    its powerful data visualization tool. You will gain hands-on experience in deploying
    Elasticsearch on Kubernetes, indexing data for optimized storage and retrieval,
    and building simple yet insightful visualizations using Kibana. This combination
    will equip you with the ability to analyze real-time data streams and uncover
    valuable patterns and trends.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将介绍 Elasticsearch，这是一个广泛用于实时数据管道的高度可伸缩和高效的搜索引擎，以及其强大的数据可视化工具 **Kibana**。您将获得在
    Kubernetes 上部署 Elasticsearch、对数据进行索引以进行优化存储和检索，并使用 Kibana 构建简单而富有洞察力的可视化的实际经验。这将使您能够分析实时数据流并发现有价值的模式和趋势。
- en: By the end of this chapter, you will have acquired the skills necessary to successfully
    deploy and utilize Trino and Elasticsearch on Kubernetes. You will be able to
    execute SQL queries directly against your data lake, monitor query execution and
    history, and leverage the power of Elasticsearch and Kibana for real-time data
    analysis and visualization.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到达本章结束时，您将掌握成功部署和利用 Trino 和 Elasticsearch 在 Kubernetes 上所需的技能。您将能够直接对数据湖执行 SQL
    查询，监视查询执行和历史记录，并利用 Elasticsearch 和 Kibana 进行实时数据分析和可视化。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主要主题：
- en: Getting started with SQL query engines
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用 SQL 查询引擎
- en: Deploying Trino in Kubernetes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中部署 Trino
- en: Deploying Elasticsearch in Kubernetes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中部署 Elasticsearch
- en: Running queries and connecting to other tools
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行查询和连接其他工具
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, you should have an AWS EKS cluster ready for deployment and
    DBeaver Community locally installed ([https://dbeaver.io/](https://dbeaver.io/))
    We will continue working on the cluster we deployed in [*Chapter 8*](B21927_08.xhtml#_idTextAnchor134).
    All the code for this chapter is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes)
    in the `Chapter09` folder.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您应准备好一个用于部署的 AWS EKS 集群，并在本地安装了 DBeaver Community ([https://dbeaver.io/](https://dbeaver.io/))。我们将继续在我们在
    [*第8章*](B21927_08.xhtml#_idTextAnchor134) 部署的集群上工作。本章的所有代码都可以在 [https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes)
    的 `Chapter09` 文件夹中找到。
- en: Getting started with SQL query engines
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 SQL 查询引擎
- en: In the world of big data, the way we store and analyze data has undergone a
    significant transformation. Traditional data warehouses, which were once the go-to
    solution for data analysis, have given way to more modern and scalable approaches,
    such as SQL query engines. These engines, such as **Trino** (formerly known as
    Presto), **Dremio**, and **Apache Spark SQL**, offer a high-performance, cost-effective,
    and flexible alternative to traditional data warehouses.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据的世界中，我们存储和分析数据的方式发生了重大转变。曾经是数据分析首选方案的传统数据仓库，已经被更现代、更具可扩展性的方法所取代，例如SQL查询引擎。这些引擎，如**Trino**（前身为Presto）、**Dremio**和**Apache
    Spark SQL**，提供了比传统数据仓库更高性能、更具性价比和更灵活的替代方案。
- en: Next, we are going to see the main differences between data warehouses and SQL
    query engines.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到数据仓库和SQL查询引擎之间的主要区别。
- en: The limitations of traditional data warehouses
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统数据仓库的局限性
- en: 'Traditional data warehouses were designed to store and analyze structured data
    from relational databases. However, with the advent of big data and the proliferation
    of diverse data sources, such as log files, sensor data, and social media data,
    the limitations of data warehouses became apparent. These limitations include
    the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 传统数据仓库是为存储和分析关系数据库中的结构化数据而设计的。然而，随着大数据的到来以及日志文件、传感器数据和社交媒体数据等多样化数据源的普及，数据仓库的局限性逐渐显现。这些局限性包括：
- en: '**Scalability**: Data warehouses often struggle to scale horizontally, making
    it challenging to handle large volumes of data efficiently'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：数据仓库通常难以实现水平扩展，处理大量数据时效率较低。'
- en: '**Data ingestion**: The process of **extracting, transforming, and loading**
    (**ETL**) data into a data warehouse can be complex, time-consuming, and resource-intensive'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据摄取**：将数据**提取、转换和加载**（**ETL**）到数据仓库的过程可能复杂、耗时且资源密集。'
- en: '**Cost**: Data warehouses can be expensive to set up and maintain, especially
    when dealing with large volumes of data'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**：数据仓库的搭建和维护可能非常昂贵，尤其是在处理大量数据时。'
- en: '**Flexibility**: Data warehouses are typically optimized for structured data
    and may not handle semi-structured or unstructured data as efficiently'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性**：数据仓库通常优化用于结构化数据，可能无法高效处理半结构化或非结构化数据。'
- en: SQL query engines were developed to address these limitations. Let’s see how
    they work.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: SQL查询引擎的开发旨在解决这些局限性。让我们来看它们是如何工作的。
- en: The rise of SQL query engines
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SQL查询引擎的崛起
- en: SQL query engines, such as Trino, provide a distributed, scalable, and cost-effective
    solution for querying large datasets stored in various data sources, including
    object storage (e.g., Amazon S3, Google Cloud Storage, and Azure Blob Storage),
    relational databases, and NoSQL databases. We will take a deeper dive into SQL
    query engines’ architecture in the next section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: SQL查询引擎，如Trino，提供了一种分布式、可扩展且具性价比的解决方案，用于查询存储在各种数据源中的大规模数据集，包括对象存储（如Amazon S3、Google
    Cloud Storage和Azure Blob Storage）、关系数据库和NoSQL数据库。我们将在下一部分深入探讨SQL查询引擎的架构。
- en: 'Here are some key advantages of SQL query engines:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是SQL查询引擎的一些关键优势：
- en: '**High performance**: SQL query engines are designed to leverage the power
    of distributed computing, allowing them to process large datasets in parallel
    across multiple nodes. This parallelization enables high-performance queries,
    even on massive datasets.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高性能**：SQL查询引擎旨在利用分布式计算的优势，使它们能够在多个节点上并行处理大规模数据集。这种并行化使得即使在庞大的数据集上也能进行高性能查询。'
- en: '**Cost-effective**: By leveraging object storage and separating storage from
    compute, SQL query engines can significantly reduce the cost of data storage and
    processing compared to traditional data warehouses.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性价比高**：通过利用对象存储并将存储与计算分离，SQL查询引擎相比传统数据仓库可以显著降低数据存储和处理的成本。'
- en: '**Scalability**: SQL query engines can scale horizontally by adding more nodes
    to the cluster, enabling them to handle increasing volumes of data efficiently.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：SQL查询引擎可以通过向集群中添加更多节点来水平扩展，使其能够高效处理不断增长的数据量。'
- en: '**Flexibility**: SQL query engines can query a wide range of data sources,
    including structured, semi-structured, and unstructured data, making them highly
    flexible and adaptable to various data formats and storage systems.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性**：SQL查询引擎能够查询多种数据源，包括结构化、半结构化和非结构化数据，使其具有高度的灵活性和适应性，能够应对各种数据格式和存储系统。'
- en: '**Open source**: Many SQL query engines are open source projects, allowing
    organizations to leverage the power of community contributions and avoid vendor
    lock-in.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源**：许多SQL查询引擎都是开源项目，允许组织利用社区贡献的力量并避免供应商锁定。'
- en: Now, let’s understand the underlying architecture of this type of solution.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们理解一下这种解决方案的底层架构。
- en: The architecture of SQL query engines
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SQL查询引擎的架构
- en: 'SQL query engines such as Trino are designed to work in a distributed computing
    environment, where multiple nodes work together to process queries and return
    results. The architecture typically consists of the following components:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Trino等SQL查询引擎设计为在分布式计算环境中工作，其中多个节点协同处理查询并返回结果。该架构通常包括以下组件：
- en: A **coordinator node**, which is responsible for parsing SQL queries, creating
    a distributed execution plan, and coordinating the execution of the query across
    the worker nodes.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协调节点**，负责解析SQL查询，创建分布式执行计划，并协调查询在工作节点之间的执行。'
- en: A set of **worker nodes**, which are responsible for executing the tasks assigned
    by the coordinator node. They read data from the underlying data sources, perform
    computations, and exchange intermediate results with other worker nodes as needed.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组**工作节点**，负责执行协调节点分配的任务。它们从底层数据源读取数据，执行计算，并根据需要与其他工作节点交换中间结果。
- en: A **metadata store**, which contains information about the data sources, table
    definitions, and other metadata required for query execution.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**元数据存储**，其中包含有关数据源、表定义及查询执行所需的其他元数据的信息。
- en: 'When a user submits a SQL query to the SQL query engine, this is what occurs:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户向SQL查询引擎提交SQL查询时，发生的过程如下：
- en: First, the coordinator node receives the query and parses it to create a distributed
    **execution plan**.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，协调节点接收查询并解析它，以创建一个分布式**执行计划**。
- en: The execution plan is divided into smaller tasks, and these tasks are assigned
    to the available worker nodes.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行计划被划分为更小的任务，这些任务被分配给可用的工作节点。
- en: The worker nodes read data from the underlying data sources, perform computations,
    and exchange intermediate results as needed.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作节点从底层数据源读取数据，执行计算，并根据需要交换中间结果。
- en: The coordinator node collects and combines the results from the worker nodes
    to produce the final query result, which is returned to the user of the client
    application.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 协调节点收集并合并来自工作节点的结果，生成最终的查询结果，然后返回给客户端应用程序的用户。
- en: This distributed architecture allows SQL query engines to leverage the combined
    computing power of multiple nodes, enabling them to process large datasets efficiently
    and deliver high-performance query execution.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分布式架构使SQL查询引擎能够利用多个节点的联合计算能力，使其能够高效处理大数据集并提供高性能的查询执行。
- en: In the case of Trino, it can directly connect to object storage systems such
    as Amazon S3, Azure Blob Storage, or Google Cloud Storage, where data is often
    stored in formats such as Parquet, ORC, or CSV. Trino can read and process this
    data directly from the object storage, without the need for intermediate data
    loading or transformation steps. This capability eliminates the need for a separate
    data ingestion process, reducing complexity and enabling faster time to insight.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在Trino的情况下，它可以直接连接到对象存储系统，如Amazon S3、Azure Blob Storage或Google Cloud Storage，其中数据通常以Parquet、ORC或CSV等格式存储。Trino可以直接从对象存储读取并处理这些数据，无需中间的数据加载或转换步骤。这种能力消除了单独的数据摄取过程的需要，简化了复杂性并加速了洞察的获取时间。
- en: Trino’s distributed architecture allows it to split the query execution across
    multiple worker nodes, each processing a portion of the data in parallel. This
    parallelization enables Trino to leverage the combined computing power of the
    cluster, resulting in high-performance query execution, even on massive datasets.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Trino的分布式架构使其能够将查询执行分配到多个工作节点，每个节点并行处理数据的一部分。这样的并行化使Trino能够利用集群的计算能力，从而在处理大规模数据集时实现高性能查询执行。
- en: Furthermore, Trino’s cost-effectiveness stems from its ability to separate storage
    from compute. By leveraging object storage for data storage, organizations can
    take advantage of the low-cost and scalable nature of these storage systems, while
    dynamically provisioning compute resources (worker nodes) as needed for query
    execution. This separation of concerns allows organizations to optimize their
    infrastructure costs and scale resources independently based on their specific
    needs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Trino的成本效益来源于它能够将存储与计算分离。通过利用对象存储进行数据存储，组织可以利用这些存储系统的低成本和可扩展性，同时根据需要动态分配计算资源（工作节点）以执行查询。这种关注点的分离使组织能够优化其基础设施成本，并根据特定需求独立地扩展资源。
- en: Now, let’s move on to a hands-on exercise and see how we can deploy Trino to
    Kubernetes and connect it to Amazon S3 as a data source.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进行一个实践练习，看看如何将Trino部署到Kubernetes，并将其连接到Amazon S3作为数据源。
- en: Deploying Trino in Kubernetes
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes中部署Trino
- en: 'Trino deployment is very straightforward using its official Helm chart. First,
    we install the chart with the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用官方Helm图表部署Trino非常简单。首先，我们使用以下命令安装该图表：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will configure the `custom_values.yaml` file. The full version of
    the file is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter09/trino/custom_values.yaml](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter09/trino/custom_values.yaml).
    There are only a few custom configurations needed for this deployment. First,
    the `server.workers` parameter allows us to set the number of worker pods we want
    for the Trino cluster. We will set this to `2` but it is advisable to scale if
    you will run queries on big data:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将配置`custom_values.yaml`文件。该文件的完整版本可以在[https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter09/trino/custom_values.yaml](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter09/trino/custom_values.yaml)找到。此部署仅需少量自定义配置。首先，`server.workers`参数允许我们设置Trino集群的工作节点数。我们将其设置为`2`，但如果要处理大数据查询，建议进行扩展：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the block of parameters, set the `image.tag` parameter to `432` as this
    is the latest Trino version compatible with the chart version we are using (0.19.0):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在参数块中，将`image.tag`参数设置为`432`，因为这是与我们使用的图表版本（0.19.0）兼容的最新Trino版本：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the `additionalCatalogs` section, we must configure Trino to use the AWS
    Glue Data Catalog. The block should be as shown here:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在`additionalCatalogs`部分，我们必须配置Trino使用AWS Glue数据目录。该代码块应该如下所示：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we will set the `service.type` parameter to `LoadBalancer` to be able
    to access Trino from outside AWS (for testing only, not suited for production):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将`service.type`参数设置为`LoadBalancer`，以便能够从AWS外部访问Trino（仅用于测试，不适用于生产环境）：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And that’s it. We are ready to launch Trino on Kubernetes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们已经准备好在Kubernetes上启动Trino。
- en: Note
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We are not using any authentication method (password, OAuth, certificate, etc.).
    In a production environment, you should set an appropriate authentication method
    and keep the traffic to Trino private inside your VPC (private network), not exposing
    the load balancer to the internet as we are doing here. This simple configuration
    is just for training and non-critical data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有使用任何认证方法（密码、OAuth、证书等）。在生产环境中，你应该设置合适的认证方法，并将流量限制在你的VPC（私有网络）内，而不是像这里一样将负载均衡器暴露到互联网上。这个简单的配置仅用于培训和非关键数据。
- en: 'After saving the `custom_values.yaml` file, use the following command to deploy
    Trino:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 保存`custom_values.yaml`文件后，使用以下命令部署Trino：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we can check whether the deployment was successful with the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下命令检查部署是否成功：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This yields this output:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生如下输出：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output was simplified to improve visualization. We can see one coordinator
    node and two workers, which is what we set. Now, copy the URL provided in the
    `EXTERNAL-IP` column of the output and paste it into your browser, adding `:8080`
    at the end. You should see a login page.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 输出已简化以提高可视化效果。我们可以看到一个协调节点和两个工作节点，这正是我们设置的节点数。现在，复制输出中`EXTERNAL-IP`列提供的URL，并将其粘贴到浏览器中，在末尾加上`:8080`。你应该能看到登录页面。
- en: '![Figure 9.1 – Trino login page](img/B21927_09_01.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – Trino登录页面](img/B21927_09_01.jpg)'
- en: Figure 9.1 – Trino login page
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – Trino登录页面
- en: The default user is `trino`. No password is required as we did not set any in
    the deployment. After clicking **Log In**, you will see Trino’s monitoring page.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 默认用户是`trino`。由于我们在部署时没有设置任何密码，因此不需要密码。点击**登录**后，你将看到Trino的监控页面。
- en: '![Figure 9.2 – Trino’s monitoring page](img/B21927_09_02.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – Trino 的监控页面](img/B21927_09_02.jpg)'
- en: Figure 9.2 – Trino’s monitoring page
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – Trino 的监控页面
- en: Next, we will use the same `LoadBalancer` URL to interact with Trino using DBeaver,
    an open source SQL client.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用相同的`LoadBalancer` URL，通过 DBeaver（一款开源 SQL 客户端）与 Trino 进行交互。
- en: Connecting DBeaver with Trino
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接 DBeaver 和 Trino
- en: To connect with Trino, first, open DBeaver and create a new Trino connection.
    In the configuration section (`trino` as the username. Leave the password blank.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接 Trino，首先，打开 DBeaver 并创建一个新的 Trino 连接。在配置部分（`trino`作为用户名，密码留空）。
- en: '![Figure 9.3 – DBeaver connection configuration](img/B21927_09_03.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – DBeaver 连接配置](img/B21927_09_03.jpg)'
- en: Figure 9.3 – DBeaver connection configuration
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – DBeaver 连接配置
- en: Then, click on **Test Connection …**. If this is the first time you are configuring
    a Trino connection, DBeaver will automatically find the necessary drivers and
    show a new window asking you to download it. You can hit **OK** and go through
    the installation, then finish the configuration.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，点击**测试连接 …**。如果这是第一次配置 Trino 连接，DBeaver 会自动找到必要的驱动程序并弹出新窗口，提示您下载它。您可以点击**确定**并完成安装，然后完成配置。
- en: Before we try to access our data, we need to catalog some data and make it available
    in Glue Data Catalog, as well as setting up an IAM permission that will allow
    Trino to access the catalog and the underlying data. Let’s get to it.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试访问数据之前，我们需要对一些数据进行目录化，并使其在 Glue 数据目录中可用，同时设置一个 IAM 权限，允许 Trino 访问目录和底层数据。让我们开始吧。
- en: 'Download the dataset from [https://github.com/neylsoncrepalde/titanic_data_with_semicolon](https://github.com/neylsoncrepalde/titanic_data_with_semicolon)
    and store the CSV file in an S3 bucket inside a folder named `titanic`. Glue only
    understands tables from folders, not from isolated files. Now, we will create
    a **Glue crawler**. This crawler will look into the dataset, map its columns and
    column types, and register the metadata in the catalog:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从[https://github.com/neylsoncrepalde/titanic_data_with_semicolon](https://github.com/neylsoncrepalde/titanic_data_with_semicolon)下载数据集，并将
    CSV 文件存储在一个名为`titanic`的文件夹中的 S3 存储桶里。Glue 只理解来自文件夹中的表，而不是孤立的文件。现在，我们将创建一个**Glue
    爬虫**。该爬虫将扫描数据集，映射其列和列类型，并在目录中注册元数据：
- en: In your AWS account, type `Glue` to enter the AWS Glue service, expand the **Data
    Catalog** option in the side menu, and click on **Crawlers** (*Figure 9**.4*).
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的 AWS 账户中，输入`Glue`以进入 AWS Glue 服务，展开侧边菜单中的**数据目录**选项，点击**爬虫**（*图 9.4*）。
- en: '![Figure 9.4 – AWS Glue landing page](img/B21927_09_04.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – AWS Glue 着陆页](img/B21927_09_04.jpg)'
- en: Figure 9.4 – AWS Glue landing page
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – AWS Glue 着陆页
- en: Next, click on `bdok-titanic-crawler` (you can choose any name). Click **Next**.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击`bdok-titanic-crawler`（您可以选择任何名称）。点击**下一步**。
- en: On the next page, click on `s3://<YOUR_BUCKET_NAME>/titanic/`. You can leave
    the other configurations as the default. Click on **Add an S3 data source** and
    then **Next**.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一页，点击`s3://<YOUR_BUCKET_NAME>/titanic/`。您可以保持其他配置为默认。点击**添加 S3 数据源**，然后点击**下一步**。
- en: In the next step, click on `AWSGlueServiceRole-titanic`. Click **Next**.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，点击`AWSGlueServiceRole-titanic`。点击**下一步**。
- en: On the next page, click on `bdok-database`, click **Create database**, and then
    close this window and go back to the **Glue crawler** **configuration** tab.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一页，点击`bdok-database`，点击**创建数据库**，然后关闭此窗口并返回到**Glue 爬虫** **配置**标签页。
- en: Back to the crawler, hit the refresh button and select your new **bdok-database**
    database. Leave the other options as the default. Click **Next**.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回到爬虫页面，点击刷新按钮，选择您的新**bdok-database**数据库。保持其他选项为默认。点击**下一步**。
- en: Now, in the last section, carefully review all the information and click **Create
    crawler**.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在最后一节中，仔细检查所有信息并点击**创建爬虫**。
- en: When it is ready, you will be taken to the crawler page on the AWS console.
    Click **Run crawler** to start the crawler. It should run for about 1 to 2 minutes
    (*Figure 9**.5*).
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当准备好后，您将被带到 AWS 控制台的爬虫页面。点击**运行爬虫**以启动爬虫。它应该运行约 1 到 2 分钟（*图 9.5*）。
- en: '![Figure 9.5 – bdok-titanic-crawler](img/B21927_09_05.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – bdok-titanic-crawler](img/B21927_09_05.jpg)'
- en: Figure 9.5 – bdok-titanic-crawler
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – bdok-titanic-crawler
- en: After the crawler is finished, you can validate that the table was correctly
    cataloged by accessing the **Data Catalog tables** menu item. The **titanic**
    table should be listed with the **bdok-database** database (*Figure 9**.6*).
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 爬虫完成后，您可以通过访问**数据目录表**菜单项验证表是否已正确目录化。**titanic** 表应与**bdok-database**数据库一起列出（*图
    9.6*）。
- en: '![Figure 9.6 – Glue Data Catalog tables](img/B21927_09_06.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – Glue数据目录表](img/B21927_09_06.jpg)'
- en: Figure 9.6 – Glue Data Catalog tables
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – Glue数据目录表
- en: Click on the `titanic` table’s name to check whether the columns were correctly
    mapped.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`titanic`表的名称，检查列是否正确映射。
- en: Now, we need to create an IAM policy that gives Kubernetes permission to access
    the catalog and the data stored in S3\. To that, in the console, go to the `studycluster`
    and you will see two roles created for Kubernetes, one service role and one node
    instance role. We want to change permissions on the node instance role (*Figure
    9**.7*).
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一个IAM策略，授权Kubernetes访问目录和存储在S3中的数据。为此，在控制台中，进入`studycluster`，你会看到为Kubernetes创建的两个角色，一个是服务角色，一个是节点实例角色。我们需要更改节点实例角色的权限（*图9.7*）。
- en: '![Figure 9.7 – IAM Roles page](img/B21927_09_07.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – IAM角色页面](img/B21927_09_07.jpg)'
- en: Figure 9.7 – IAM Roles page
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – IAM角色页面
- en: Click on the node instance role, then click on the **Add permissions** button
    and select **Create** **inline policy**.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击节点实例角色，然后点击**添加权限**按钮，选择**创建** **内联策略**。
- en: 'On the **Specify permissions** page, click to edit as a JSON document and paste
    the JSON file available in this GitHub repository: [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter09/trino/iam/AthenaFullWithAllBucketsPolicy.json](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter09/trino/iam/AthenaFullWithAllBucketsPolicy.json)
    (*Figure 9**.8*). This policy allows Athena and Glue permissions, as well as getting
    all S3 data from any bucket. Remember that this is a very open policy and should
    not be used in production. It is a best security practice to allow access only
    in the needed buckets.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**指定权限**页面，点击以JSON文档方式编辑，并粘贴此GitHub仓库中的JSON文件：[https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter09/trino/iam/AthenaFullWithAllBucketsPolicy.json](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter09/trino/iam/AthenaFullWithAllBucketsPolicy.json)（*图9.8*）。此策略允许Athena和Glue权限，并获取任何桶中的所有S3数据。请记住，这是一个非常开放的策略，不应在生产环境中使用。最好的安全实践是仅允许访问所需的桶。
- en: '![Figure 9.8 – Specifying permissions](img/B21927_09_08.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8 – 指定权限](img/B21927_09_08.jpg)'
- en: Figure 9.8 – Specifying permissions
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 指定权限
- en: Click `AthenaFullWithAllBucketsPolicy` to facilitate this policy search later.
    Then, click on **Create policy**. And we are set to go!
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`AthenaFullWithAllBucketsPolicy`，以便稍后更方便地搜索此策略。然后，点击**创建策略**。我们就准备好了！
- en: Now, let’s get back to DBeaver and play with some queries. First, we need to
    find where the table is stored. Expand the Trino connection in DBeaver and you
    will see a database named **hive**. This is where data from the Glue Data Catalog
    is mirrored in Trino. Expand **hive** and you will see the **bdok-database** catalog
    there. If you expand **Tables**, you will see the **titanic** dataset mapped.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到DBeaver并玩一些查询。首先，我们需要找到表存储的位置。在DBeaver中展开Trino连接，你会看到一个名为**hive**的数据库。这是Glue数据目录中的数据在Trino中的镜像。展开**hive**，你会看到**bdok-database**目录。如果展开**表格**，你会看到映射的**titanic**数据集。
- en: 'To test a query, right-click the **hive** database and select **SQL Editor**
    and then **New SQL Script**. Now, run the query:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试查询，右键点击**hive**数据库，选择**SQL编辑器**，然后选择**新建SQL脚本**。现在，运行查询：
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You should see the results (*Figure 9**.9*):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能看到如下结果（*图9.9*）：
- en: '![Figure 9.9 – DBeaver results from Trino](img/B21927_09_09.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图9.9 – Trino的DBeaver结果](img/B21927_09_09.jpg)'
- en: Figure 9.9 – DBeaver results from Trino
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 – Trino的DBeaver结果
- en: And, of course, Trino can perform any calculations or aggregations we like.
    Let’s try a simple query to get the count and average age of all the passengers
    by `pclass` and `sex`. We will show the results ordered by `sex` and `pclass`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Trino可以执行我们喜欢的任何计算或聚合。让我们尝试一个简单的查询，按`pclass`和`sex`统计所有乘客的数量和平均年龄。我们将按`sex`和`pclass`的顺序显示结果。
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This query yields this result:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询返回了以下结果：
- en: '![Figure 9.10 – Simple query on Titanic dataset](img/B21927_09_10.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图9.10 – Titanic数据集上的简单查询](img/B21927_09_10.jpg)'
- en: Figure 9.10 – Simple query on Titanic dataset
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 – Titanic数据集上的简单查询
- en: Now, let’s visit Trino’s monitoring page once again to see the query we just
    ran. Check the **Finished** box under **Query details** to see all queries; the
    first one shown is the query we just ran. Click on it to see the details.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们再次访问Trino的监控页面，查看我们刚才运行的查询。在**查询详情**下勾选**已完成**框，以查看所有查询；第一个显示的就是我们刚才运行的查询。点击它查看详细信息。
- en: That’s it! You have successfully deployed Trino on Kubernetes and used it to
    query data from a data lake on Amazon S3\. In the next section, we will work with
    Elasticsearch.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！你已经成功地在 Kubernetes 上部署了 Trino，并利用它查询了来自 Amazon S3 数据湖的数据。在接下来的部分，我们将开始使用
    Elasticsearch。
- en: Deploying Elasticsearch in Kubernetes
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中部署 Elasticsearch
- en: While Trino provides a powerful SQL interface for querying structured data in
    your data lake, many modern applications also need to analyze semi-structured
    and unstructured data, such as logs, metrics, and text, in real time. For these
    types of use cases, Elasticsearch (or the ELK Stack, as it came to be known, referring
    to Elasticsearch, Logstash, and Kibana) provides a powerful solution.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Trino 提供了一个强大的 SQL 接口，用于查询数据湖中的结构化数据，但许多现代应用程序也需要实时分析半结构化和非结构化数据，如日志、指标和文本。对于这些类型的用例，Elasticsearch（或者
    ELK 堆栈，指代 Elasticsearch、Logstash 和 Kibana）提供了一个强大的解决方案。
- en: Elasticsearch is an open source, distributed, RESTful search and analytics engine
    built on top of Apache Lucene. It is designed to store, search, and analyze large
    volumes of data quickly and in near real time.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 是一个开源的分布式、RESTful 搜索与分析引擎，建立在 Apache Lucene 之上。它旨在快速并几乎实时地存储、搜索和分析大量数据。
- en: At its core, Elasticsearch is a NoSQL database that uses JSON documents to represent
    data. It indexes all data in every field and uses advanced data structures and
    indexing techniques to make searches extremely fast.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 的核心是一个 NoSQL 数据库，它使用 JSON 文档来表示数据。它会对每个字段中的所有数据进行索引，并使用高级数据结构和索引技术使得搜索速度极快。
- en: How Elasticsearch stores, indexes and manages data
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Elasticsearch 如何存储、索引和管理数据
- en: Data is stored in Elasticsearch in individual JSON documents. These documents
    are grouped into types within an index. You can think of an index like a database
    table that has a defined mapping or schema.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以单独的 JSON 文档存储在 Elasticsearch 中。这些文档会被分组为索引内的类型。你可以将索引视为一个具有定义映射或模式的数据库表。
- en: To add data to Elasticsearch, you make an HTTP request to the appropriate index
    with the JSON document in the request body. Elasticsearch automatically indexes
    all data in the document’s fields using advanced data structures such as the inverted
    index from Apache Lucene.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 向 Elasticsearch 添加数据时，你需要向相应的索引发送一个 HTTP 请求，请求体中包含 JSON 文档。Elasticsearch 会使用先进的数据结构（如
    Apache Lucene 的倒排索引）自动对文档中的所有字段数据进行索引。
- en: This indexing process optimizes data for extremely fast queries and aggregations.
    Elasticsearch distributes data across shards, which can be allocated to different
    nodes in the cluster for redundancy and scalability.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个索引过程优化了数据，使其能够进行极快的查询和聚合。Elasticsearch 将数据分布到分片中，这些分片可以被分配到集群中的不同节点，以实现冗余和可扩展性。
- en: When you want to query or retrieve data from Elasticsearch, you use the RESTful
    search API to define the query using a simple JSON request body. Results are returned
    in JSON format as well.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想要从 Elasticsearch 查询或检索数据时，你可以使用 RESTful 搜索 API，通过简单的 JSON 请求体定义查询。查询结果同样以
    JSON 格式返回。
- en: 'Elasticsearch is designed as a distributed system from the ground up. It can
    scale out to hundreds of servers and handle petabytes of data. Its core elements
    are as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 从一开始就是作为一个分布式系统设计的。它可以扩展到数百台服务器，并处理 PB 级的数据。其核心元素如下：
- en: '**Nodes**, running instances of Elasticsearch that together form a **cluster**'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**，是 Elasticsearch 的运行实例，它们共同构成一个 **集群**'
- en: '**Indexes**, which are collections of documents that have similar characteristics'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索引**，是具有相似特征的文档集合'
- en: '**Shards**, which are low-level partitions of an index that contain a slice
    of all the documents in the index'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分片**，是索引的低级分区，包含索引中所有文档的一部分'
- en: '**Replicas**, which are copies of a shard for redundancy and improved performance'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**副本**，是用于冗余和提高性能的分片副本'
- en: 'At the core of Elasticsearch’s distributed architecture is the sharding system.
    Sharding refers to horizontally splitting an Elasticsearch index into multiple
    pieces, called shards. This allows the index data to be distributed across multiple
    nodes in the cluster, providing several key benefits:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 分布式架构的核心是分片系统。分片指的是将 Elasticsearch 索引水平拆分成多个部分，称为分片。这使得索引数据可以分布在集群中的多个节点上，从而提供多个关键好处：
- en: '**Horizontal scalability**: By distributing shards across nodes, Elasticsearch
    can effectively scale out to handle more data and higher query/indexing throughput.
    As the dataset grows, you can simply add more nodes to the cluster and Elasticsearch
    will automatically migrate shards to balance the load.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**横向扩展性**：通过将分片分布到各个节点，Elasticsearch 可以有效地横向扩展，以处理更多数据和更高的查询/索引吞吐量。随着数据集的增长，您只需向集群中添加更多节点，Elasticsearch
    会自动迁移分片以平衡负载。'
- en: '**High availability**: Each shard can have one or more replica shards. A replica
    is a full copy of the primary shard. Replicas provide redundancy and high availability
    – if a node hosting a primary shard fails, Elasticsearch will automatically promote
    a replica as the new primary to take over.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高可用性**：每个分片可以有一个或多个副本分片。副本是主分片的完整副本。副本提供冗余和高可用性——如果承载主分片的节点发生故障，Elasticsearch
    会自动将副本提升为新的主分片以接管任务。'
- en: '**Parallelization of operations**: Since index operations such as searches
    and aggregations are executed in parallel on each shard, having more shards allows
    greater parallelization and thus higher performance.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作并行化**：由于索引操作（如搜索和聚合）会在每个分片上并行执行，拥有更多的分片可以实现更大的并行化，从而提升性能。'
- en: When you create an index in Elasticsearch, you need to specify the number of
    primary shards the index should have. For example, if you configure an index with
    three primary shards, Elasticsearch will horizontally partition the index data
    into three shards and distribute them across nodes in the cluster.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在 Elasticsearch 中创建索引时，需要指定该索引应该拥有的主分片数量。例如，如果您为索引配置了三个主分片，Elasticsearch 将把索引数据横向分区为三个分片，并将它们分布到集群中的各个节点上。
- en: Each primary shard can also have zero or more replica shards configured. A common
    setup is to have one replica, meaning there are two copies of each shard – the
    primary and one replica. The replica shards are also distributed across nodes
    in the cluster, with each replica on a different node than its respective primary
    for redundancy.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 每个主分片也可以配置零个或多个副本分片。一个常见的设置是拥有一个副本，这意味着每个分片都有两个副本——一个主分片和一个副本。副本分片也会分布到集群中的节点上，每个副本与其相应的主分片位于不同的节点上以确保冗余。
- en: Elasticsearch automatically manages shard allocation across nodes using a shard
    allocation strategy. The default is to spread shards across as many nodes as possible
    to balance the load. As nodes are added or removed from the cluster, Elasticsearch
    will automatically migrate shards to rebalance the cluster.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 使用分片分配策略自动管理节点之间的分片分配。默认情况下，它会将分片尽可能多地分布到各个节点上以平衡负载。当节点被添加或从集群中移除时，Elasticsearch
    会自动迁移分片以重新平衡集群。
- en: Queries are executed in parallel on each shard, with results being merged to
    produce the final result set. Writes (indexing new documents) are sent to a primary
    shard, which is responsible for validating the data, making changes persistent,
    and replicating changes to associated replica shards.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 查询会在每个分片上并行执行，结果会被合并以产生最终的结果集。写入（索引新文档）会发送到一个主分片，该主分片负责验证数据、使更改持久化，并将更改复制到相关的副本分片。
- en: The number of shards configured for an index is fixed at index creation time.
    It cannot be changed later, so proper shard planning is important. Having more
    shards allows greater parallelization, but too many shards can also increase overhead.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为索引配置的分片数量在索引创建时是固定的，之后不能更改，因此合理的分片规划非常重要。拥有更多的分片可以实现更大的并行化，但过多的分片也可能增加开销。
- en: A good rule of thumb is to start with enough shards (3 to 5 shards) that index
    data can be distributed across multiple nodes. The number can be increased if
    the index grows very large and more parallelization is needed. However, having
    hundreds or thousands of shards is generally not recommended due to increased
    cluster management overhead.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的经验法则是从足够数量的分片（3到5个分片）开始，以便索引数据可以分布到多个节点上。如果索引变得非常庞大且需要更多的并行化，可以增加分片数量。然而，一般不建议拥有成百上千个分片，因为这会增加集群管理的开销。
- en: Now, let’s see how to deploy Elasticsearch on Kubernetes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下如何在 Kubernetes 上部署 Elasticsearch。
- en: Elasticsearch deployment
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Elasticsearch 部署
- en: 'Here, we will work with **Elastic Cloud on Kubernetes** (**ECK**), an official
    Elastic operator that allows you to provision, manage, and orchestrate Elastic
    Stack applications on Kubernetes clusters. We will use the official Helm chart
    to install the operator. In your terminal, type the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用**Elastic Cloud on Kubernetes**（**ECK**），这是一个官方的Elastic操作员，允许你在Kubernetes集群上部署、管理和编排Elastic
    Stack应用程序。我们将使用官方的Helm图表来安装该操作员。在终端中，输入以下命令：
- en: '[PRE10]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This will download the Helm chart locally and deploy the default definitions
    for the Elastic Stack in a new environment named `elastic`. Here, we will use
    the `2.12.1` version of the Helm Chart.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这将会将Helm图表下载到本地，并在一个名为`elastic`的新环境中部署Elastic Stack的默认定义。在这里，我们将使用版本`2.12.1`的Helm图表。
- en: Now, we will configure the deployment for an Elasticsearch cluster. The `elastic_cluster.yaml`
    YAML file does the trick.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将为Elasticsearch集群配置部署。`elastic_cluster.yaml` YAML文件可以完成这项工作。
- en: '[PRE11]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let’s take a closer look at this code. The first block specifies the API version
    and the kind of Kubernetes resource we are defining. In this case, it’s an `Elasticsearch`
    resource from the `elasticsearch.k8s.elastic.co/v1` API group, which is provided
    by the ECK operator. The `metadata` block specifies the name of the cluster, in
    this case, `elastic`. Within the `spec` block, we set the Elasticsearch version
    to be used (`8.13.0`) and a policy that determines when the `DeleteOnScaledownAndClusterDeletion`
    policy deletes the PVCs when the Elasticsearch cluster is scaled down or deleted
    entirely.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看这段代码。第一个块指定了API版本和我们正在定义的Kubernetes资源类型。在这种情况下，它是来自`elasticsearch.k8s.elastic.co/v1`
    API组的`Elasticsearch`资源，由ECK操作员提供。`metadata`块指定了集群的名称，这里是`elastic`。在`spec`块中，我们设置了要使用的Elasticsearch版本（`8.13.0`）以及一个策略，该策略决定了`DeleteOnScaledownAndClusterDeletion`策略在Elasticsearch集群缩减或完全删除时删除PVCs。
- en: The `nodeSets` block defines the configuration for the Elasticsearch nodes.
    In this case, we have a single node set named `default` with a count of `2`, meaning
    we will have two Elasticsearch nodes in the cluster. The `podTemplate` block specifies
    the configuration for the Pods that will run the Elasticsearch containers. Here,
    we define the resource requests and limits for the Elasticsearch container, setting
    the memory request and limit to 2 GiB and the CPU request to one vCPU.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`nodeSets`块定义了Elasticsearch节点的配置。在这种情况下，我们有一个名为`default`的节点集，节点数为`2`，意味着集群中将有两个Elasticsearch节点。`podTemplate`块指定了运行Elasticsearch容器的Pod的配置。在这里，我们定义了Elasticsearch容器的资源请求和限制，将内存请求和限制设置为2
    GiB，CPU请求为一个vCPU。'
- en: The `initContainers` block is a recommendation from the official Elastic documentation
    for a production environment. It defines a container that will run before the
    main Elasticsearch container starts. In this case, we have an `initContainer`
    named `sysctl` that runs with privileged security context and sets the `vm.max_map_count`
    kernel setting to `262144`. This setting is recommended for running Elasticsearch
    on Linux to allow for a higher limit on memory-mapped areas in use.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`initContainers`块是官方Elastic文档对生产环境的推荐。它定义了一个将在主Elasticsearch容器启动之前运行的容器。在这里，我们有一个名为`sysctl`的`initContainer`，它以特权安全上下文运行，并将`vm.max_map_count`内核设置为`262144`。这个设置推荐在Linux上运行Elasticsearch时使用，以允许在内存映射区域使用中设置更高的限制。'
- en: Finally, the `volumeClaimTemplates` block defines the PVCs that will be used
    to store Elasticsearch data. In this case, we have a single PVC named `elasticsearch-data`
    with a requested storage size of 5 GiB. `accessModes` specifies that the volume
    should be `ReadWriteOnce`, meaning it can be mounted as read-write by a single
    node. `storageClassName` is set to `gp2`, which is an AWS EBS storage class for
    general-purpose SSD volumes.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`volumeClaimTemplates`块定义了用于存储Elasticsearch数据的PVC。在这种情况下，我们有一个名为`elasticsearch-data`的PVC，要求的存储大小为5
    GiB。`accessModes`指定该卷应该是`ReadWriteOnce`，意味着它可以被单个节点以读写方式挂载。`storageClassName`设置为`gp2`，这是AWS
    EBS存储类别的通用SSD卷。
- en: 'After saving this file locally, run the following command to deploy an Elasticsearch
    cluster:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地保存该文件后，运行以下命令以部署一个Elasticsearch集群：
- en: '[PRE12]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Monitor the deployment with the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令监控部署过程：
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Alternatively, you can use the following:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你也可以使用以下命令：
- en: '[PRE14]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will give a little more information. Note that this deployment will take
    a few minutes to finish. You can also get some detailed information on the cluster
    with the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提供更多的信息。请注意，这个部署过程可能需要几分钟才能完成。你还可以使用以下命令获取集群的详细信息：
- en: '[PRE15]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the output, `HEALTH` should be `green` and the `PHASE` column should display
    `Ready`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，`HEALTH`应为`green`，`PHASE`列应显示`Ready`：
- en: '[PRE16]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, let’s move on to Kibana. We will follow the same process. The first thing
    to do is to set a YAML file named `kibana.yaml` with the deployment configuration.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转向Kibana。我们将按照相同的流程进行。首先要做的是设置一个名为`kibana.yaml`的YAML文件，并配置部署。
- en: '[PRE17]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This code is very similar to the previous one, but simpler. The main differences
    are in the `spec` block. First, the `elasticsearchRef` parameter specifies the
    name of the Elasticsearch cluster that Kibana should connect to. In this case,
    it’s referencing the Elasticsearch cluster we created before named `elastic`.
    The http block configures the Kubernetes Service that will expose the Kibana deployment.
    Specifically, we are setting the type of the Service to `LoadBalancer`, which
    means that a load balancer will be provisioned by the cloud provider to distribute
    traffic across the Kibana instances. Finally, in the `podTemplate` block, we have
    an `env` configuration that sets an environment variable, `NODE_OPTIONS`, with
    the value `--max-old-space-size=2048`, which increases the maximum heap size for
    Kibana.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码与以前非常相似，但更简单。主要区别在于`spec`块。首先，`elasticsearchRef`参数指定了Kibana应连接到的Elasticsearch集群的名称。在这种情况下，它引用了我们之前创建的名为`elastic`的Elasticsearch集群。http块配置了将公开Kibana部署的Kubernetes服务。具体来说，我们将服务的类型设置为`LoadBalancer`，这意味着云提供商将提供一个负载均衡器来分发Kibana实例的流量。最后，在`podTemplate`块中，我们有一个`env`配置，设置了一个名为`NODE_OPTIONS`的环境变量，其值为`--max-old-space-size=2048`，这会增加Kibana的最大堆大小。
- en: 'Now, we are ready to deploy:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好部署：
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We use the same commands as before to monitor whether the deployment was successful.
    Now, we need to access the automatically generated password for Elastic and Kibana.
    We can do this with the following command:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与之前相同的命令来监视部署是否成功。现在，我们需要访问Elastic和Kibana的自动生成密码。我们可以通过以下命令完成：
- en: '[PRE19]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This command will print the generated password on the screen. Copy it and keep
    it safe. Now, run the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将在屏幕上打印生成的密码。复制并妥善保管。现在，运行以下命令：
- en: '[PRE20]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: To get the services list, copy the URL address for the LoadBalancer and paste
    it into a browser, adding `:5601` at the end and https:// at the beginning. Kibana
    will not accept regular HTTP protocol connections. You should see the login page
    as in *Figure 9**.11*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取服务列表，请复制负载均衡器的URL地址，并将其粘贴到浏览器中，在末尾添加`:5601`并以https://开头。Kibana将不接受常规HTTP协议连接。您应该看到登录页面，如*图
    9.11*所示。
- en: '![](img/B21927_09_11.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21927_09_11.jpg)'
- en: FIgure 9.11 – Kibana login page
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 – Kibana登录页面
- en: After inserting the username and password, you should be able to access Kibana’s
    first empty page (*Figure 9**.12*).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 插入用户名和密码后，您应该能够访问Kibana的第一个空白页面（*图 9.12*）。
- en: '![Figure 9.12 – Kibana’s first empty page](img/B21927_09_12.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.12 – Kibana的第一个空白页面](img/B21927_09_12.jpg)'
- en: Figure 9.12 – Kibana’s first empty page
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 – Kibana的第一个空白页面
- en: Click on **Explore on my own** and you will now be able to play with Elastic
    as much as you want (although it does not have any data just yet). To do that,
    we will experiment with our (well-known) Titanic dataset. On the **Home** page,
    click on the menu in the upper-left corner and then click on **Stack Management**
    (the last option). On the next page, in the left menu, click on **Data Views**
    and then click on the **Upload a file** button in the center (*Figure 9**.13*).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**自行探索**，现在您可以尽情使用Elastic（虽然目前还没有任何数据）。为此，我们将尝试使用我们熟悉的Titanic数据集。在**主页**上，点击左上角的菜单，然后点击**堆栈管理**（最后一个选项）。在下一页中，左侧菜单中，点击**数据视图**，然后在中心点击**上传文件**按钮（*图
    9.13*）。
- en: '![Figure 9.13 – Upload a file option in Kibana](img/B21927_09_13.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.13 – 在Kibana中上传文件选项](img/B21927_09_13.jpg)'
- en: Figure 9.13 – Upload a file option in Kibana
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 – 在Kibana中上传文件选项
- en: Now, select the Titanic dataset CSV file you already have and upload it to Kibana.
    You will see a page with the mapped contents from the file (*Figure 9**.14*).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，选择你已经拥有的Titanic数据集CSV文件并将其上传到Kibana。您将看到一个页面，显示来自文件的映射内容（*图 9.14*）。
- en: '![Figure 9.14 – Titanic dataset mapped contents](img/B21927_09_14.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.14 – Titanic数据集的映射内容](img/B21927_09_14.jpg)'
- en: Figure 9.14 – Titanic dataset mapped contents
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 – Titanic数据集的映射内容
- en: Now, click on `titanic` and make sure that the **Create data view** option is
    checked. Click on **Import** (*Figure 9**.15*).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，点击`titanic`，确保**创建数据视图**选项已选中。点击**导入**（*图 9.15*）。
- en: '![Figure 9.15 – Kibana – creating an index](img/B21927_09_15.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.15 – Kibana – 创建索引](img/B21927_09_15.jpg)'
- en: Figure 9.15 – Kibana – creating an index
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15 – Kibana – 创建索引
- en: In a few seconds, you should see a success screen. Now, let’s play a little
    bit with this data with visualizations. Go back to the home page and, in the left
    menu, click on **Dashboards**. Then, click on **Create a dashboard** and then
    on **Create visualization**. This will get you to the visualization building in
    Kibana (*Figure 9**.16*).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，你应该会看到一个成功的屏幕。现在，让我们使用这些数据进行一些可视化操作。返回主页，在左侧菜单中点击**仪表板**。然后点击**创建仪表板**，再点击**创建可视化**。这将带你进入Kibana中的可视化构建页面（*图
    9.16*）。
- en: '![](img/B21927_09_16.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21927_09_16.jpg)'
- en: FIgure 9.16 – Kibana visualization creation
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16 – Kibana可视化创建
- en: Now, let’s build some quick visualizations. On the right side of the page, select
    the type of visualization (let’s keep **Bar vertical stacked**). For **Horizontal
    axis**, drag and drop the **Pclass** field. For **Vertical axis**, drag and drop
    the **Fare** field. As it is a numeric field, Kibana will automatically suggest
    the median as an aggregation function. Click on it to change it to **Average**.
    For **Breakdown**, drag and drop the **Sex** field. We should end up with a nice
    bar graph, as shown in *Figure 9**.17*.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来创建一些快速的可视化。在页面的右侧，选择可视化的类型（我们选择**垂直堆叠条形图**）。对于**横轴**，拖动并放置**Pclass**字段。对于**纵轴**，拖动并放置**Fare**字段。由于这是一个数值字段，Kibana
    会自动建议使用中位数作为聚合函数。点击它将其更改为**平均值**。对于**细分**，拖动并放置**Sex**字段。最后我们将得到一个漂亮的条形图，如*图 9.17*所示。
- en: '![Figure 9.17 – Average fare price per sex and Pclass](img/B21927_09_17.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.17 – 按性别和Pclass分组的平均票价](img/B21927_09_17.jpg)'
- en: Figure 9.17 – Average fare price per sex and Pclass
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.17 – 按性别和Pclass分组的平均票价
- en: Click on **Save and return** to view your newly created graphic on a new dashboard.
    Let’s do another quick analysis. Click **Create visualization** again. This time,
    we will make a scatter plot with **Age** and **Fare** to see whether there is
    any correlation between those variables. Drop **Age** in **Horizontal axis** and
    **Fare** in **Vertical axis**. Click on **Vertical axis** to change the aggregation
    function to **Average**. Now, you have a nice scatter plot showing the interaction
    between those two variables. No significant correlation so far. Let’s add the
    **Pclass** field as the breakdown and we will get a cool visualization of the
    data (*Figure 9**.18*).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**保存并返回**，在新仪表板上查看你新创建的图形。让我们再进行一次快速分析。再次点击**创建可视化**。这次，我们将制作一个散点图，展示**年龄**和**票价**之间是否存在关联。将**年龄**放入**横轴**，将**票价**放入**纵轴**。点击**纵轴**将聚合函数更改为**平均值**。现在，你将看到一个漂亮的散点图，展示这两个变量之间的互动。到目前为止，没有显著的相关性。接下来，我们将**Pclass**字段添加为细分，你将得到一个很酷的数据可视化图像（*图
    9.18*）。
- en: '![Figure 9.18 – Scatter plot in Kibana](img/B21927_09_18.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.18 – Kibana中的散点图](img/B21927_09_18.jpg)'
- en: Figure 9.18 – Scatter plot in Kibana
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18 – Kibana中的散点图
- en: Now, click on `Survivors` (*Figure 9**.19*).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，点击`Survivors`（*图 9.19*）。
- en: '![Figure 9.19 – Survivors count visualization](img/B21927_09_19.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.19 – 生还者计数可视化](img/B21927_09_19.jpg)'
- en: Figure 9.19 – Survivors count visualization
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19 – 生还者计数可视化
- en: Then, click **Save and return** and rearrange the dashboard manually as you
    wish (a simple example is shown in *Figure 9**.20*).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，点击**保存并返回**，并根据需要手动重新排列仪表板（*图 9.20*中展示了一个简单的示例）。
- en: '![Figure 9.20 – Your first Kibana dashboard](img/B21927_09_20.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.20 – 你的第一个Kibana仪表板](img/B21927_09_20.jpg)'
- en: Figure 9.20 – Your first Kibana dashboard
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.20 – 你的第一个Kibana仪表板
- en: And that’s it! You successfully deployed Elasticsearch and Kibana on Kubernetes,
    added data manually, and built a dashboard (with lots of potential). Feel free
    to play with Kibana, trying out other datasets and visualizations.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！你成功地在Kubernetes上部署了Elasticsearch和Kibana，手动添加了数据，并创建了一个仪表板（具有巨大潜力）。随时可以尝试使用Kibana，尝试其他数据集和可视化。
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored two powerful tools, Trino and Elasticsearch, which
    enable effective data consumption and analysis in a Kubernetes-based big data
    architecture. We learned the importance of having a robust data consumption layer
    that bridges the gap between data repositories and business analysts, allowing
    them to extract valuable insights and make informed decisions.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了两个强大的工具，Trino和Elasticsearch，它们使得在基于Kubernetes的大数据架构中进行有效的数据消费和分析成为可能。我们学习了拥有一个强大的数据消费层的重要性，它可以在数据仓库和业务分析师之间架起桥梁，帮助他们提取有价值的洞察，并做出明智的决策。
- en: We learned how to deploy Trino, a distributed SQL query engine, on Kubernetes
    and leverage its ability to directly query data stored in object storage systems
    such as Amazon S3\. This eliminates the need for a traditional data warehouse
    and provides a cost-effective, scalable, and flexible solution for querying large
    datasets. We acquired hands-on experience in deploying Trino, configuring it to
    use the AWS Glue Data Catalog, and executing SQL queries against our data lake.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何在 Kubernetes 上部署 Trino，一个分布式 SQL 查询引擎，并利用它直接查询存储在对象存储系统（如 Amazon S3）中的数据。这消除了传统数据仓库的需求，并提供了一种具有成本效益、可扩展且灵活的解决方案，用于查询大规模数据集。我们通过实际操作获得了在
    Kubernetes 上部署 Trino、配置其使用 AWS Glue 数据目录，并执行针对数据湖的 SQL 查询的经验。
- en: Additionally, we dove into Elasticsearch, a highly scalable and efficient search
    engine, along with Kibana, its powerful data visualization tool. We learned how
    to deploy Elasticsearch on Kubernetes using the ECK operator, index data for optimized
    storage and retrieval, and build simple yet insightful visualizations using Kibana.
    This combination equips us with the ability to analyze real-time data streams
    and uncover valuable patterns and trends.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还深入学习了 Elasticsearch，一个高度可扩展且高效的搜索引擎，以及 Kibana，它是一个强大的数据可视化工具。我们学习了如何使用
    ECK 操作符在 Kubernetes 上部署 Elasticsearch，如何为优化存储和检索创建索引，以及如何使用 Kibana 构建简单但富有洞察力的可视化。这一组合使我们具备了分析实时数据流并发现有价值的模式和趋势的能力。
- en: The skills learned in this chapter are crucial in today’s data-driven world,
    where organizations need to effectively consume and analyze vast amounts of data
    to make informed business decisions. Trino and Elasticsearch can also be extremely
    helpful for business teams who are not acquainted with coding to explore data
    (with simple SQL queries or in a visual way) and enhance their everyday decision-making.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中学习的技能在今天这个数据驱动的世界中至关重要，组织需要有效地消耗和分析海量数据，以做出明智的业务决策。Trino 和 Elasticsearch
    对于不熟悉编码的业务团队来说也非常有帮助，可以通过简单的 SQL 查询或可视化的方式探索数据，从而提升日常决策能力。
- en: In the next chapter, we will put all the pieces we have seen so far together
    to build a complete data pipeline on Kubernetes.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把到目前为止所学的所有内容结合起来，构建一个完整的 Kubernetes 数据管道。
