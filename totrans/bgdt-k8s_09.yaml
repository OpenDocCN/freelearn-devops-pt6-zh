- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Consumption Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today’s data-driven world, organizations are dealing with an ever-increasing
    volume of data, and the ability to effectively consume and analyze this data is
    crucial for making informed business decisions. As we delve into the realm of
    big data on Kubernetes, we must address the critical component of the data consumption
    layer. This layer serves as the bridge between the vast repositories of data and
    the business analysts who need to extract valuable insights and make decisions
    that have an impact on the business.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore two powerful tools that will enable you to
    unlock the potential of your Kubernetes-based data architecture: **Trino** and
    **Elasticsearch**. Trino, a distributed SQL query engine, will empower you to
    directly query your data lake, eliminating the need for a traditional data warehouse.
    You will learn how to deploy Trino on Kubernetes, monitor its performance, and
    execute SQL queries against your data stored in Amazon S3.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we will introduce Elasticsearch, a highly scalable and efficient
    search engine widely used in real-time data pipelines, along with **Kibana**,
    its powerful data visualization tool. You will gain hands-on experience in deploying
    Elasticsearch on Kubernetes, indexing data for optimized storage and retrieval,
    and building simple yet insightful visualizations using Kibana. This combination
    will equip you with the ability to analyze real-time data streams and uncover
    valuable patterns and trends.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have acquired the skills necessary to successfully
    deploy and utilize Trino and Elasticsearch on Kubernetes. You will be able to
    execute SQL queries directly against your data lake, monitor query execution and
    history, and leverage the power of Elasticsearch and Kibana for real-time data
    analysis and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with SQL query engines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Trino in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Elasticsearch in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running queries and connecting to other tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you should have an AWS EKS cluster ready for deployment and
    DBeaver Community locally installed ([https://dbeaver.io/](https://dbeaver.io/))
    We will continue working on the cluster we deployed in [*Chapter 8*](B21927_08.xhtml#_idTextAnchor134).
    All the code for this chapter is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes)
    in the `Chapter09` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with SQL query engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the world of big data, the way we store and analyze data has undergone a
    significant transformation. Traditional data warehouses, which were once the go-to
    solution for data analysis, have given way to more modern and scalable approaches,
    such as SQL query engines. These engines, such as **Trino** (formerly known as
    Presto), **Dremio**, and **Apache Spark SQL**, offer a high-performance, cost-effective,
    and flexible alternative to traditional data warehouses.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to see the main differences between data warehouses and SQL
    query engines.
  prefs: []
  type: TYPE_NORMAL
- en: The limitations of traditional data warehouses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Traditional data warehouses were designed to store and analyze structured data
    from relational databases. However, with the advent of big data and the proliferation
    of diverse data sources, such as log files, sensor data, and social media data,
    the limitations of data warehouses became apparent. These limitations include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: Data warehouses often struggle to scale horizontally, making
    it challenging to handle large volumes of data efficiently'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data ingestion**: The process of **extracting, transforming, and loading**
    (**ETL**) data into a data warehouse can be complex, time-consuming, and resource-intensive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost**: Data warehouses can be expensive to set up and maintain, especially
    when dealing with large volumes of data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: Data warehouses are typically optimized for structured data
    and may not handle semi-structured or unstructured data as efficiently'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL query engines were developed to address these limitations. Let’s see how
    they work.
  prefs: []
  type: TYPE_NORMAL
- en: The rise of SQL query engines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SQL query engines, such as Trino, provide a distributed, scalable, and cost-effective
    solution for querying large datasets stored in various data sources, including
    object storage (e.g., Amazon S3, Google Cloud Storage, and Azure Blob Storage),
    relational databases, and NoSQL databases. We will take a deeper dive into SQL
    query engines’ architecture in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some key advantages of SQL query engines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High performance**: SQL query engines are designed to leverage the power
    of distributed computing, allowing them to process large datasets in parallel
    across multiple nodes. This parallelization enables high-performance queries,
    even on massive datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost-effective**: By leveraging object storage and separating storage from
    compute, SQL query engines can significantly reduce the cost of data storage and
    processing compared to traditional data warehouses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: SQL query engines can scale horizontally by adding more nodes
    to the cluster, enabling them to handle increasing volumes of data efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: SQL query engines can query a wide range of data sources,
    including structured, semi-structured, and unstructured data, making them highly
    flexible and adaptable to various data formats and storage systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open source**: Many SQL query engines are open source projects, allowing
    organizations to leverage the power of community contributions and avoid vendor
    lock-in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s understand the underlying architecture of this type of solution.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of SQL query engines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SQL query engines such as Trino are designed to work in a distributed computing
    environment, where multiple nodes work together to process queries and return
    results. The architecture typically consists of the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: A **coordinator node**, which is responsible for parsing SQL queries, creating
    a distributed execution plan, and coordinating the execution of the query across
    the worker nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of **worker nodes**, which are responsible for executing the tasks assigned
    by the coordinator node. They read data from the underlying data sources, perform
    computations, and exchange intermediate results with other worker nodes as needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **metadata store**, which contains information about the data sources, table
    definitions, and other metadata required for query execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When a user submits a SQL query to the SQL query engine, this is what occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the coordinator node receives the query and parses it to create a distributed
    **execution plan**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The execution plan is divided into smaller tasks, and these tasks are assigned
    to the available worker nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The worker nodes read data from the underlying data sources, perform computations,
    and exchange intermediate results as needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The coordinator node collects and combines the results from the worker nodes
    to produce the final query result, which is returned to the user of the client
    application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This distributed architecture allows SQL query engines to leverage the combined
    computing power of multiple nodes, enabling them to process large datasets efficiently
    and deliver high-performance query execution.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of Trino, it can directly connect to object storage systems such
    as Amazon S3, Azure Blob Storage, or Google Cloud Storage, where data is often
    stored in formats such as Parquet, ORC, or CSV. Trino can read and process this
    data directly from the object storage, without the need for intermediate data
    loading or transformation steps. This capability eliminates the need for a separate
    data ingestion process, reducing complexity and enabling faster time to insight.
  prefs: []
  type: TYPE_NORMAL
- en: Trino’s distributed architecture allows it to split the query execution across
    multiple worker nodes, each processing a portion of the data in parallel. This
    parallelization enables Trino to leverage the combined computing power of the
    cluster, resulting in high-performance query execution, even on massive datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, Trino’s cost-effectiveness stems from its ability to separate storage
    from compute. By leveraging object storage for data storage, organizations can
    take advantage of the low-cost and scalable nature of these storage systems, while
    dynamically provisioning compute resources (worker nodes) as needed for query
    execution. This separation of concerns allows organizations to optimize their
    infrastructure costs and scale resources independently based on their specific
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move on to a hands-on exercise and see how we can deploy Trino to
    Kubernetes and connect it to Amazon S3 as a data source.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Trino in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Trino deployment is very straightforward using its official Helm chart. First,
    we install the chart with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will configure the `custom_values.yaml` file. The full version of
    the file is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter09/trino/custom_values.yaml](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter09/trino/custom_values.yaml).
    There are only a few custom configurations needed for this deployment. First,
    the `server.workers` parameter allows us to set the number of worker pods we want
    for the Trino cluster. We will set this to `2` but it is advisable to scale if
    you will run queries on big data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the block of parameters, set the `image.tag` parameter to `432` as this
    is the latest Trino version compatible with the chart version we are using (0.19.0):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `additionalCatalogs` section, we must configure Trino to use the AWS
    Glue Data Catalog. The block should be as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will set the `service.type` parameter to `LoadBalancer` to be able
    to access Trino from outside AWS (for testing only, not suited for production):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it. We are ready to launch Trino on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We are not using any authentication method (password, OAuth, certificate, etc.).
    In a production environment, you should set an appropriate authentication method
    and keep the traffic to Trino private inside your VPC (private network), not exposing
    the load balancer to the internet as we are doing here. This simple configuration
    is just for training and non-critical data.
  prefs: []
  type: TYPE_NORMAL
- en: 'After saving the `custom_values.yaml` file, use the following command to deploy
    Trino:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can check whether the deployment was successful with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The output was simplified to improve visualization. We can see one coordinator
    node and two workers, which is what we set. Now, copy the URL provided in the
    `EXTERNAL-IP` column of the output and paste it into your browser, adding `:8080`
    at the end. You should see a login page.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Trino login page](img/B21927_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Trino login page
  prefs: []
  type: TYPE_NORMAL
- en: The default user is `trino`. No password is required as we did not set any in
    the deployment. After clicking **Log In**, you will see Trino’s monitoring page.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Trino’s monitoring page](img/B21927_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Trino’s monitoring page
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will use the same `LoadBalancer` URL to interact with Trino using DBeaver,
    an open source SQL client.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting DBeaver with Trino
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To connect with Trino, first, open DBeaver and create a new Trino connection.
    In the configuration section (`trino` as the username. Leave the password blank.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – DBeaver connection configuration](img/B21927_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – DBeaver connection configuration
  prefs: []
  type: TYPE_NORMAL
- en: Then, click on **Test Connection …**. If this is the first time you are configuring
    a Trino connection, DBeaver will automatically find the necessary drivers and
    show a new window asking you to download it. You can hit **OK** and go through
    the installation, then finish the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Before we try to access our data, we need to catalog some data and make it available
    in Glue Data Catalog, as well as setting up an IAM permission that will allow
    Trino to access the catalog and the underlying data. Let’s get to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the dataset from [https://github.com/neylsoncrepalde/titanic_data_with_semicolon](https://github.com/neylsoncrepalde/titanic_data_with_semicolon)
    and store the CSV file in an S3 bucket inside a folder named `titanic`. Glue only
    understands tables from folders, not from isolated files. Now, we will create
    a **Glue crawler**. This crawler will look into the dataset, map its columns and
    column types, and register the metadata in the catalog:'
  prefs: []
  type: TYPE_NORMAL
- en: In your AWS account, type `Glue` to enter the AWS Glue service, expand the **Data
    Catalog** option in the side menu, and click on **Crawlers** (*Figure 9**.4*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.4 – AWS Glue landing page](img/B21927_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – AWS Glue landing page
  prefs: []
  type: TYPE_NORMAL
- en: Next, click on `bdok-titanic-crawler` (you can choose any name). Click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next page, click on `s3://<YOUR_BUCKET_NAME>/titanic/`. You can leave
    the other configurations as the default. Click on **Add an S3 data source** and
    then **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next step, click on `AWSGlueServiceRole-titanic`. Click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next page, click on `bdok-database`, click **Create database**, and then
    close this window and go back to the **Glue crawler** **configuration** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Back to the crawler, hit the refresh button and select your new **bdok-database**
    database. Leave the other options as the default. Click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, in the last section, carefully review all the information and click **Create
    crawler**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When it is ready, you will be taken to the crawler page on the AWS console.
    Click **Run crawler** to start the crawler. It should run for about 1 to 2 minutes
    (*Figure 9**.5*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.5 – bdok-titanic-crawler](img/B21927_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – bdok-titanic-crawler
  prefs: []
  type: TYPE_NORMAL
- en: After the crawler is finished, you can validate that the table was correctly
    cataloged by accessing the **Data Catalog tables** menu item. The **titanic**
    table should be listed with the **bdok-database** database (*Figure 9**.6*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Glue Data Catalog tables](img/B21927_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Glue Data Catalog tables
  prefs: []
  type: TYPE_NORMAL
- en: Click on the `titanic` table’s name to check whether the columns were correctly
    mapped.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we need to create an IAM policy that gives Kubernetes permission to access
    the catalog and the data stored in S3\. To that, in the console, go to the `studycluster`
    and you will see two roles created for Kubernetes, one service role and one node
    instance role. We want to change permissions on the node instance role (*Figure
    9**.7*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.7 – IAM Roles page](img/B21927_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – IAM Roles page
  prefs: []
  type: TYPE_NORMAL
- en: Click on the node instance role, then click on the **Add permissions** button
    and select **Create** **inline policy**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the **Specify permissions** page, click to edit as a JSON document and paste
    the JSON file available in this GitHub repository: [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter09/trino/iam/AthenaFullWithAllBucketsPolicy.json](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter09/trino/iam/AthenaFullWithAllBucketsPolicy.json)
    (*Figure 9**.8*). This policy allows Athena and Glue permissions, as well as getting
    all S3 data from any bucket. Remember that this is a very open policy and should
    not be used in production. It is a best security practice to allow access only
    in the needed buckets.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Specifying permissions](img/B21927_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Specifying permissions
  prefs: []
  type: TYPE_NORMAL
- en: Click `AthenaFullWithAllBucketsPolicy` to facilitate this policy search later.
    Then, click on **Create policy**. And we are set to go!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let’s get back to DBeaver and play with some queries. First, we need to
    find where the table is stored. Expand the Trino connection in DBeaver and you
    will see a database named **hive**. This is where data from the Glue Data Catalog
    is mirrored in Trino. Expand **hive** and you will see the **bdok-database** catalog
    there. If you expand **Tables**, you will see the **titanic** dataset mapped.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test a query, right-click the **hive** database and select **SQL Editor**
    and then **New SQL Script**. Now, run the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the results (*Figure 9**.9*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – DBeaver results from Trino](img/B21927_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – DBeaver results from Trino
  prefs: []
  type: TYPE_NORMAL
- en: And, of course, Trino can perform any calculations or aggregations we like.
    Let’s try a simple query to get the count and average age of all the passengers
    by `pclass` and `sex`. We will show the results ordered by `sex` and `pclass`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This query yields this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Simple query on Titanic dataset](img/B21927_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Simple query on Titanic dataset
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s visit Trino’s monitoring page once again to see the query we just
    ran. Check the **Finished** box under **Query details** to see all queries; the
    first one shown is the query we just ran. Click on it to see the details.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it! You have successfully deployed Trino on Kubernetes and used it to
    query data from a data lake on Amazon S3\. In the next section, we will work with
    Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Elasticsearch in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Trino provides a powerful SQL interface for querying structured data in
    your data lake, many modern applications also need to analyze semi-structured
    and unstructured data, such as logs, metrics, and text, in real time. For these
    types of use cases, Elasticsearch (or the ELK Stack, as it came to be known, referring
    to Elasticsearch, Logstash, and Kibana) provides a powerful solution.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch is an open source, distributed, RESTful search and analytics engine
    built on top of Apache Lucene. It is designed to store, search, and analyze large
    volumes of data quickly and in near real time.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, Elasticsearch is a NoSQL database that uses JSON documents to represent
    data. It indexes all data in every field and uses advanced data structures and
    indexing techniques to make searches extremely fast.
  prefs: []
  type: TYPE_NORMAL
- en: How Elasticsearch stores, indexes and manages data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data is stored in Elasticsearch in individual JSON documents. These documents
    are grouped into types within an index. You can think of an index like a database
    table that has a defined mapping or schema.
  prefs: []
  type: TYPE_NORMAL
- en: To add data to Elasticsearch, you make an HTTP request to the appropriate index
    with the JSON document in the request body. Elasticsearch automatically indexes
    all data in the document’s fields using advanced data structures such as the inverted
    index from Apache Lucene.
  prefs: []
  type: TYPE_NORMAL
- en: This indexing process optimizes data for extremely fast queries and aggregations.
    Elasticsearch distributes data across shards, which can be allocated to different
    nodes in the cluster for redundancy and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: When you want to query or retrieve data from Elasticsearch, you use the RESTful
    search API to define the query using a simple JSON request body. Results are returned
    in JSON format as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Elasticsearch is designed as a distributed system from the ground up. It can
    scale out to hundreds of servers and handle petabytes of data. Its core elements
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nodes**, running instances of Elasticsearch that together form a **cluster**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Indexes**, which are collections of documents that have similar characteristics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shards**, which are low-level partitions of an index that contain a slice
    of all the documents in the index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replicas**, which are copies of a shard for redundancy and improved performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the core of Elasticsearch’s distributed architecture is the sharding system.
    Sharding refers to horizontally splitting an Elasticsearch index into multiple
    pieces, called shards. This allows the index data to be distributed across multiple
    nodes in the cluster, providing several key benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Horizontal scalability**: By distributing shards across nodes, Elasticsearch
    can effectively scale out to handle more data and higher query/indexing throughput.
    As the dataset grows, you can simply add more nodes to the cluster and Elasticsearch
    will automatically migrate shards to balance the load.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High availability**: Each shard can have one or more replica shards. A replica
    is a full copy of the primary shard. Replicas provide redundancy and high availability
    – if a node hosting a primary shard fails, Elasticsearch will automatically promote
    a replica as the new primary to take over.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelization of operations**: Since index operations such as searches
    and aggregations are executed in parallel on each shard, having more shards allows
    greater parallelization and thus higher performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you create an index in Elasticsearch, you need to specify the number of
    primary shards the index should have. For example, if you configure an index with
    three primary shards, Elasticsearch will horizontally partition the index data
    into three shards and distribute them across nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Each primary shard can also have zero or more replica shards configured. A common
    setup is to have one replica, meaning there are two copies of each shard – the
    primary and one replica. The replica shards are also distributed across nodes
    in the cluster, with each replica on a different node than its respective primary
    for redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch automatically manages shard allocation across nodes using a shard
    allocation strategy. The default is to spread shards across as many nodes as possible
    to balance the load. As nodes are added or removed from the cluster, Elasticsearch
    will automatically migrate shards to rebalance the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Queries are executed in parallel on each shard, with results being merged to
    produce the final result set. Writes (indexing new documents) are sent to a primary
    shard, which is responsible for validating the data, making changes persistent,
    and replicating changes to associated replica shards.
  prefs: []
  type: TYPE_NORMAL
- en: The number of shards configured for an index is fixed at index creation time.
    It cannot be changed later, so proper shard planning is important. Having more
    shards allows greater parallelization, but too many shards can also increase overhead.
  prefs: []
  type: TYPE_NORMAL
- en: A good rule of thumb is to start with enough shards (3 to 5 shards) that index
    data can be distributed across multiple nodes. The number can be increased if
    the index grows very large and more parallelization is needed. However, having
    hundreds or thousands of shards is generally not recommended due to increased
    cluster management overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how to deploy Elasticsearch on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will work with **Elastic Cloud on Kubernetes** (**ECK**), an official
    Elastic operator that allows you to provision, manage, and orchestrate Elastic
    Stack applications on Kubernetes clusters. We will use the official Helm chart
    to install the operator. In your terminal, type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This will download the Helm chart locally and deploy the default definitions
    for the Elastic Stack in a new environment named `elastic`. Here, we will use
    the `2.12.1` version of the Helm Chart.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will configure the deployment for an Elasticsearch cluster. The `elastic_cluster.yaml`
    YAML file does the trick.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a closer look at this code. The first block specifies the API version
    and the kind of Kubernetes resource we are defining. In this case, it’s an `Elasticsearch`
    resource from the `elasticsearch.k8s.elastic.co/v1` API group, which is provided
    by the ECK operator. The `metadata` block specifies the name of the cluster, in
    this case, `elastic`. Within the `spec` block, we set the Elasticsearch version
    to be used (`8.13.0`) and a policy that determines when the `DeleteOnScaledownAndClusterDeletion`
    policy deletes the PVCs when the Elasticsearch cluster is scaled down or deleted
    entirely.
  prefs: []
  type: TYPE_NORMAL
- en: The `nodeSets` block defines the configuration for the Elasticsearch nodes.
    In this case, we have a single node set named `default` with a count of `2`, meaning
    we will have two Elasticsearch nodes in the cluster. The `podTemplate` block specifies
    the configuration for the Pods that will run the Elasticsearch containers. Here,
    we define the resource requests and limits for the Elasticsearch container, setting
    the memory request and limit to 2 GiB and the CPU request to one vCPU.
  prefs: []
  type: TYPE_NORMAL
- en: The `initContainers` block is a recommendation from the official Elastic documentation
    for a production environment. It defines a container that will run before the
    main Elasticsearch container starts. In this case, we have an `initContainer`
    named `sysctl` that runs with privileged security context and sets the `vm.max_map_count`
    kernel setting to `262144`. This setting is recommended for running Elasticsearch
    on Linux to allow for a higher limit on memory-mapped areas in use.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `volumeClaimTemplates` block defines the PVCs that will be used
    to store Elasticsearch data. In this case, we have a single PVC named `elasticsearch-data`
    with a requested storage size of 5 GiB. `accessModes` specifies that the volume
    should be `ReadWriteOnce`, meaning it can be mounted as read-write by a single
    node. `storageClassName` is set to `gp2`, which is an AWS EBS storage class for
    general-purpose SSD volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'After saving this file locally, run the following command to deploy an Elasticsearch
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Monitor the deployment with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give a little more information. Note that this deployment will take
    a few minutes to finish. You can also get some detailed information on the cluster
    with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the output, `HEALTH` should be `green` and the `PHASE` column should display
    `Ready`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s move on to Kibana. We will follow the same process. The first thing
    to do is to set a YAML file named `kibana.yaml` with the deployment configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This code is very similar to the previous one, but simpler. The main differences
    are in the `spec` block. First, the `elasticsearchRef` parameter specifies the
    name of the Elasticsearch cluster that Kibana should connect to. In this case,
    it’s referencing the Elasticsearch cluster we created before named `elastic`.
    The http block configures the Kubernetes Service that will expose the Kibana deployment.
    Specifically, we are setting the type of the Service to `LoadBalancer`, which
    means that a load balancer will be provisioned by the cloud provider to distribute
    traffic across the Kibana instances. Finally, in the `podTemplate` block, we have
    an `env` configuration that sets an environment variable, `NODE_OPTIONS`, with
    the value `--max-old-space-size=2048`, which increases the maximum heap size for
    Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are ready to deploy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the same commands as before to monitor whether the deployment was successful.
    Now, we need to access the automatically generated password for Elastic and Kibana.
    We can do this with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will print the generated password on the screen. Copy it and keep
    it safe. Now, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: To get the services list, copy the URL address for the LoadBalancer and paste
    it into a browser, adding `:5601` at the end and https:// at the beginning. Kibana
    will not accept regular HTTP protocol connections. You should see the login page
    as in *Figure 9**.11*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21927_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIgure 9.11 – Kibana login page
  prefs: []
  type: TYPE_NORMAL
- en: After inserting the username and password, you should be able to access Kibana’s
    first empty page (*Figure 9**.12*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Kibana’s first empty page](img/B21927_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – Kibana’s first empty page
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Explore on my own** and you will now be able to play with Elastic
    as much as you want (although it does not have any data just yet). To do that,
    we will experiment with our (well-known) Titanic dataset. On the **Home** page,
    click on the menu in the upper-left corner and then click on **Stack Management**
    (the last option). On the next page, in the left menu, click on **Data Views**
    and then click on the **Upload a file** button in the center (*Figure 9**.13*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Upload a file option in Kibana](img/B21927_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – Upload a file option in Kibana
  prefs: []
  type: TYPE_NORMAL
- en: Now, select the Titanic dataset CSV file you already have and upload it to Kibana.
    You will see a page with the mapped contents from the file (*Figure 9**.14*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.14 – Titanic dataset mapped contents](img/B21927_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – Titanic dataset mapped contents
  prefs: []
  type: TYPE_NORMAL
- en: Now, click on `titanic` and make sure that the **Create data view** option is
    checked. Click on **Import** (*Figure 9**.15*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.15 – Kibana – creating an index](img/B21927_09_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 – Kibana – creating an index
  prefs: []
  type: TYPE_NORMAL
- en: In a few seconds, you should see a success screen. Now, let’s play a little
    bit with this data with visualizations. Go back to the home page and, in the left
    menu, click on **Dashboards**. Then, click on **Create a dashboard** and then
    on **Create visualization**. This will get you to the visualization building in
    Kibana (*Figure 9**.16*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21927_09_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIgure 9.16 – Kibana visualization creation
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s build some quick visualizations. On the right side of the page, select
    the type of visualization (let’s keep **Bar vertical stacked**). For **Horizontal
    axis**, drag and drop the **Pclass** field. For **Vertical axis**, drag and drop
    the **Fare** field. As it is a numeric field, Kibana will automatically suggest
    the median as an aggregation function. Click on it to change it to **Average**.
    For **Breakdown**, drag and drop the **Sex** field. We should end up with a nice
    bar graph, as shown in *Figure 9**.17*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.17 – Average fare price per sex and Pclass](img/B21927_09_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.17 – Average fare price per sex and Pclass
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Save and return** to view your newly created graphic on a new dashboard.
    Let’s do another quick analysis. Click **Create visualization** again. This time,
    we will make a scatter plot with **Age** and **Fare** to see whether there is
    any correlation between those variables. Drop **Age** in **Horizontal axis** and
    **Fare** in **Vertical axis**. Click on **Vertical axis** to change the aggregation
    function to **Average**. Now, you have a nice scatter plot showing the interaction
    between those two variables. No significant correlation so far. Let’s add the
    **Pclass** field as the breakdown and we will get a cool visualization of the
    data (*Figure 9**.18*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18 – Scatter plot in Kibana](img/B21927_09_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.18 – Scatter plot in Kibana
  prefs: []
  type: TYPE_NORMAL
- en: Now, click on `Survivors` (*Figure 9**.19*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.19 – Survivors count visualization](img/B21927_09_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.19 – Survivors count visualization
  prefs: []
  type: TYPE_NORMAL
- en: Then, click **Save and return** and rearrange the dashboard manually as you
    wish (a simple example is shown in *Figure 9**.20*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20 – Your first Kibana dashboard](img/B21927_09_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.20 – Your first Kibana dashboard
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it! You successfully deployed Elasticsearch and Kibana on Kubernetes,
    added data manually, and built a dashboard (with lots of potential). Feel free
    to play with Kibana, trying out other datasets and visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored two powerful tools, Trino and Elasticsearch, which
    enable effective data consumption and analysis in a Kubernetes-based big data
    architecture. We learned the importance of having a robust data consumption layer
    that bridges the gap between data repositories and business analysts, allowing
    them to extract valuable insights and make informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: We learned how to deploy Trino, a distributed SQL query engine, on Kubernetes
    and leverage its ability to directly query data stored in object storage systems
    such as Amazon S3\. This eliminates the need for a traditional data warehouse
    and provides a cost-effective, scalable, and flexible solution for querying large
    datasets. We acquired hands-on experience in deploying Trino, configuring it to
    use the AWS Glue Data Catalog, and executing SQL queries against our data lake.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we dove into Elasticsearch, a highly scalable and efficient search
    engine, along with Kibana, its powerful data visualization tool. We learned how
    to deploy Elasticsearch on Kubernetes using the ECK operator, index data for optimized
    storage and retrieval, and build simple yet insightful visualizations using Kibana.
    This combination equips us with the ability to analyze real-time data streams
    and uncover valuable patterns and trends.
  prefs: []
  type: TYPE_NORMAL
- en: The skills learned in this chapter are crucial in today’s data-driven world,
    where organizations need to effectively consume and analyze vast amounts of data
    to make informed business decisions. Trino and Elasticsearch can also be extremely
    helpful for business teams who are not acquainted with coding to explore data
    (with simple SQL queries or in a visual way) and enhance their everyday decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will put all the pieces we have seen so far together
    to build a complete data pipeline on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
