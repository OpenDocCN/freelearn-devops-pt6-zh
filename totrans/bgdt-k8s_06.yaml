- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Pipelines with Apache Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Airflow has become the *de facto* standard for building, monitoring,
    and maintaining data pipelines. As data volumes and complexity grow, the need
    for robust and scalable orchestration is paramount. In this chapter, we will cover
    the fundamentals of Airflow – installing it locally, exploring its architecture,
    and developing your first **Directed Acyclic** **Graphs** (**DAGs**).
  prefs: []
  type: TYPE_NORMAL
- en: We will start by spinning up Airflow using Docker and the Astro CLI. This will
    allow you to get hands-on without the overhead of a full production installation.
    Next, we’ll get to know Airflow’s architecture and its key components, such as
    the scheduler, workers, and metadata database.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, you’ll create your first DAG – the core building block of any Airflow
    workflow. Here, you’ll get exposed to operators – the tasks that comprise your
    pipelines. We’ll cover the most common operators used in data engineering, such
    as `PythonOperator`, `BashOperator`, and sensors. By chaining these operators
    together, you’ll build autonomous, robust DAGs.
  prefs: []
  type: TYPE_NORMAL
- en: Later in the chapter, we’ll level up – tackling more complex pipelines and integrating
    with external tools such as databases and cloud-based storage services. You’ll
    learn best practices for creating production-grade workflows. Finally, we’ll run
    an end-to-end pipeline orchestrating an entire data engineering process – ingestion,
    processing, and data delivery.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll understand how to build, monitor, and maintain
    data pipelines with Airflow. You’ll be able to develop effective DAGs using Python
    and apply Airflow best practices for scale and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a data pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Airflow integration with other tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the activities in this chapter, you should have Docker installed and a valid
    AWS account. If you have doubts about how to do the installations and account
    setup, see [*Chapter 1*](B21927_01.xhtml#_idTextAnchor015) and [*Chapter 3*](B21927_03.xhtml#_idTextAnchor053).
    All the code for this chapter is available online in the GitHub repository ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes))
    in the `Chapter`0`6` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this first section, we will get Apache Airflow up and running on our local
    machine using the Astro CLI. Astro makes it easy to install and manage Apache
    Airflow. We will also take a deep dive into the components that make up Airflow’s
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Airflow with Astro
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Astro is a command-line interface provided by Astronomer that allows you to
    quickly install and run Apache Airflow. With Astro, we can quickly spin up a local
    Airflow environment. It abstracts away the complexity of manually installing all
    Airflow components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing the Astro CLI is very straightforward. You can find instructions
    for its installation here: [https://docs.astronomer.io/astro/cli/install-cli](https://docs.astronomer.io/astro/cli/install-cli).
    Once installed, the first thing to do is to initiate a new Airflow project. In
    the terminal, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a folder structure for an Airflow project locally. Next, start
    up Airflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This will pull the necessary Docker images and start containers for the Airflow
    web server, scheduler, worker, and PostgreSQL database.
  prefs: []
  type: TYPE_NORMAL
- en: You can access the Airflow UI at [http://localhost:8080](http://localhost:8080).
    The default username and password are *admin*.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it! In just a few commands, we have a fully functioning Airflow environment
    up and running locally. Now let’s take a deeper look into Airflow’s architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Airflow is composed of different components that fit together to provide a scalable
    and reliable orchestration platform for data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, Airflow has the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A metadata database that stores state for DAGs, task instances, XComs, and so
    on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A web server that serves the Airflow UI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A scheduler that handles triggering DAGs and task instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executors that run task instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workers that execute tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other components, such as the CLI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This architecture is depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Airflow Architecture](img/B21927_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Airflow Architecture
  prefs: []
  type: TYPE_NORMAL
- en: Airflow relies heavily on the metadata database as the source of truth for state.
    The web server, scheduler, and worker processes talk to this database. When you
    look at the Airflow UI, underneath, it simply queries this database to get info
    to display.
  prefs: []
  type: TYPE_NORMAL
- en: The metadata database is also used to enforce certain constraints. For example,
    the scheduler uses database locks when examining task instances to determine what
    to schedule next. This prevents race conditions between multiple scheduler processes.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A race condition occurs when two or more threads or processes access a shared
    resource concurrently, and the final output depends on the sequence or timing
    of the execution. The threads “race” to access or modify the shared resource,
    and the final state depends, unpredictably, on who gets there first. Race conditions
    are a common source of bugs and unpredictable behavior in concurrent systems.
    They can result in corrupted data, crashes, or incorrect outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s examine some of the key components in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Web server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Airflow web server is responsible for hosting the Airflow UI you interact
    with, providing REST APIs for other services to communicate with Airflow, and
    serving static assets and pages. The Airflow UI allows you to monitor, trigger,
    and troubleshoot DAGs and tasks. It provides visibility into the overall health
    of your data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: The web server also exposes REST APIs that are used by the CLI, scheduler, workers,
    and custom applications to talk to Airflow. For example, the CLI uses the API
    to trigger DAGs. The scheduler uses it to update state for DAGs. Workers use it
    to update task instance state as they process them.
  prefs: []
  type: TYPE_NORMAL
- en: While the UI is very convenient for humans, services rely on the underlying
    REST APIs. Overall, the Airflow web server is critical as it provides a central
    way for users and services to interact with Airflow metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Airflow scheduler is the brains behind examining task instances and determining
    what to run next. Its key responsibilities include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Checking the status of task instances in the metadata database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining dependencies between tasks to create a DAG run execution plan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting tasks to scheduled or queued in the database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking the progress as task instances move through different states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling the backfilling of historical runs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To perform these duties, the scheduler does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Refreshes the DAG dictionary with details about all active DAGs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examines active DAG runs to see what tasks need to be scheduled
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Checks on the status of running tasks via the job tracker
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Updates the state of tasks in the database – queued, running, success, failed,
    and so on
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Critical to the scheduler’s functioning is the metadata database. This allows
    it to be highly scalable since multiple schedulers can coordinate and sync via
    the single source of truth in the database.
  prefs: []
  type: TYPE_NORMAL
- en: The scheduler is very versatile – you can run a single scheduler for small workloads
    or scale up to multiple active schedulers for large workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Executors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a task needs to run, the executor is responsible for actually running the
    task. Executors interface with a pool of workers that execute tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common executors are `LocalExecutor`, `CeleryExecutor`, and `KubernetesExecutor`:'
  prefs: []
  type: TYPE_NORMAL
- en: LocalExecutor runs task instances in parallel processes on the host system.
    It is great for testing but has very limited scalability for large workloads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CeleryExecutor uses a Celery pool to distribute tasks. It allows running workers
    across multiple machines and, thus, provides horizontal scalability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KubernetesExecutor is specially designed for Airflow deployments running in
    Kubernetes. It launches worker Pods in Kubernetes dynamically. It provides excellent
    scalability and resource isolation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we move our Airflow to production, being able to scale out workers is critical.
    KubernetesExecutor will play a main role in our case.
  prefs: []
  type: TYPE_NORMAL
- en: For testing locally, LocalExecutor is the simplest. Astro configures this by
    default.
  prefs: []
  type: TYPE_NORMAL
- en: Workers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Workers execute the actual logic for task instances. The executor manages and
    interfaces with the worker pool. Workers carry out tasks such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Running Python functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing Bash commands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making API requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing data transfers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communicating task status
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the executor, workers may run on threads, server processes, or in separate
    containers. The worker communicates the status of task instances to the metadata
    database. It updates state to queued, running, success, failed, and so on. This
    allows the scheduler to monitor progress and coordinate pipeline execution across
    workers.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, workers provide the compute resources necessary to run our pipeline
    tasks. The executor interfaces with and manages these workers.
  prefs: []
  type: TYPE_NORMAL
- en: Queueing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For certain executors, such as Celery and Kubernetes, you need an additional
    queueing service. This queue stores tasks before workers pick them up. There are
    a few common queueing technologies that can be used with Celery, such as RabbitMQ
    (a popular open source queue), Redis (an in-memory datastore), and Amazon SQS
    (a fully managed queue service by AWS).
  prefs: []
  type: TYPE_NORMAL
- en: For Kubernetes, we don’t need any of these tools as KubernetesExecutor dynamically
    launches Pods to execute tasks and kill them when the tasks are done.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As highlighted earlier, Airflow relies on its metadata database heavily. This
    database stores the state and metadata for Airflow to function. The default for
    local testing is SQLite, which is simple but has major scalability limitations.
    Even for moderate workloads, it is recommended to switch to a more production-grade
    database.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow works with PostgreSQL, MySQL, and a variety of cloud-based database
    services, such as Amazon RDS.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow’s distributed architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we can see, Airflow works with a modular distributed architecture. This
    design brings several advantages for production workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Separation of concerns**: Each component focuses on a specific job. The scheduler
    handles examining DAGs and scheduling. The worker runs task instances. This separation
    of concerns keeps components simple and maintainable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Components such as the scheduler, worker, and database can
    be easily scaled out. Run multiple schedulers or workers as your workload grows.
    Leverage a hosted database for automatic scaling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reliability**: If one scheduler or worker dies, there is no overall outage
    since components are decoupled. The single source of truth in the database also
    provides consistency across Airflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extensibility**: You can swap out certain components, such as the executor
    or queueing service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, Airflow provides scalability, reliability, and flexibility via its
    modular architecture. Each component has a focused job, leading to simplicity
    and stability in the overall system.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get back to Airflow and start building some simple DAGs.
  prefs: []
  type: TYPE_NORMAL
- en: Building a data pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s start developing a simple DAG. All your Python code should be inside
    the `dags` folder. For our first hands-on exercise, we will work with the `Titanic`
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a file in the `dags` folder and save it as `titanic_dag.py`. We will begin
    by importing the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will define some default arguments for our DAG – in this case, the
    owner (important for DAG filtering) and the start date:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we will define a function for our DAG using the `@dag` decorator. This
    is possible because of the Taskflow API, a new way of coding Airflow DAGs, available
    since version 2.0\. It makes it easier and faster to develop DAGs’ Python code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inside the `@dag` decorator, we define some important parameters. The default
    arguments are already set in a Python dictionary. The `schedule_interval` is set
    to `@once`, meaning this DAG will only run one time when triggered. The `description`
    parameter helps us understand in the UI what this DAG is doing. It is a good practice
    to always define it. The `catchup` is also important and it should always be set
    to `False`. When you have a DAG with several pending runs, when you trigger the
    execution, Airflow will automatically try to run all the past runs at once, which
    can cause an overload. Setting this parameter to `False` tells Airflow that, if
    there are any pending runs, Airflow will just run the last one and continue normally
    with the schedule. Finally, tags are not a required parameter but are great for
    filtering in the UI. Immediately after the `@dag` decorator, you should define
    a function for the DAG. In our case, we’ll define a function called `titanic_processing`.
    Inside this function, we will define our tasks. We can do that using an Airflow
    operator (such as `DummyOperator`) or using functions with the `@``task` decorator:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding example, we have two tasks defined so far. One of them is using
    `DummyOperator`, which does literally nothing. It is often used to set marks on
    your DAG. We will use this just to mark the start and the end of the DAG.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we have our first task, just printing `"And so, it begins!"` in the logs.
    This task is defined with a simple Python function and the `@task` decorator.
    Now, we will define the tasks that download and process the dataset. Remember
    that all of the following code should be indented (inside the `titanic_processing`
    function). You can check the complete code in the book’s GitHub repository ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/titanic_dag.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/titanic_dag.py)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first few tasks print messages, download the dataset, and save it to `/tmp`
    (temporary folder). Then the `analyze_survivors` task loads the CSV data, counts
    the number of survivors, and prints the result. The `survivors_sex` task groups
    the survivors by sex and prints the counts. Those prints can be visualized in
    the log of each task in the Airflow UI.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You might ask “*Why divide the download of the data and two analyses in three
    steps? Why do we not just do everything as one whole task?*” First, it is important
    to acknowledge that Airflow is not a data processing tool but an orchestration
    tool. Big data should not run inside Airflow (as we are doing here) because you
    could easily run out of resources. Instead, Airflow should trigger processing
    tasks that will run somewhere else. We’ll see an example of how to trigger processing
    in PostgreSQL from a task later in the chapter, in the next section. Second, it
    is good practice to keep tasks as simple as possible and as independent as possible.
    This allows for more parallelism and a DAG that is easier to debug.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will code two more tasks to exemplify other possibilities using
    Airflow operators. First, we will code a simple `BashOperator` task to print a
    message. It can be used to run any bash command with Airflow. Later, we have another
    `DummyOperator` task that does nothing – it only marks the end of the pipeline.
    This task is optional. Remember that those tasks should be indented, inside the
    `titanic_processing` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have defined all the tasks we need, we are going to orchestrate
    the pipeline, that is, tell Airflow how to chain the tasks. We can do this in
    two ways. The universal way is to use the `>>` operator, which indicates the order
    between tasks. The other way is usable when we have function tasks with parameters.
    We can pass an output of a function as a parameter to another and Airflow will
    automatically understand that there is a dependency between those tasks. These
    lines should also be indented, so be careful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: First, we have to run the function tasks and save them as Python objects. Then,
    we chain them in order using `>>`. The third line tells Airflow that there is
    a dependency between the `start` task, the `first` task, and the `download_data`
    task, which should be triggered in this order. Next, we run the `analyze_survivors`
    and `survivors_sex` tasks and give the `downloaded` output as a parameter. With
    this, Airflow can detect that there is a dependency between them. Finally, we
    tell Airflow that after the `analyze_survivors` and `survivors_sex` tasks, we
    have the `last` and `end` tasks. Note that `analyze_survivors` and `survivors_sex`
    are inside a list, meaning that they can run in parallel. This is an important
    feature of dependency management in Airflow. The “rule of thumb” is that any tasks
    that do not depend on each other should run in parallel to optimize the pipeline
    delivery time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the last thing is to initialize the DAG, running the function and saving
    it in a Python object. This code should not be indented, as it is outside the
    `titanic_processing` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we’re good to go. Go to the terminal. Be sure you are in the same folder
    we used to initialize the Airflow project with the Astro CLI. Then, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This will download the Airflow Docker image and start the containers. After
    Airflow is correctly started, Astro will open a browser tab to the login page
    of the Airflow UI. If it does not open automatically, you can access it at [http://localhost:8080/](http://localhost:8080/).
    Log in with the default username and password (`admin`, `admin`). You should see
    your DAG in the Airflow UI (*Figure 6**.2*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Airflow UI – DAGs view](img/B21927_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Airflow UI – DAGs view
  prefs: []
  type: TYPE_NORMAL
- en: There is a button on the left side of the DAG to turn its scheduler on. Don’t
    click it yet. First, click on the DAG and take a look at all the views Airflow
    offers in a DAG. At first, you should see a summary with the DAG information (*Figure
    6**.3*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Airflow UI – DAG grid view](img/B21927_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Airflow UI – DAG grid view
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Graph** button to see a nice visualization of the pipeline (*Figure
    6**.4*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Airflow UI – DAG Graph view](img/B21927_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Airflow UI – DAG Graph view
  prefs: []
  type: TYPE_NORMAL
- en: Note how Airflow automatically detects the dependencies and the parallelisms
    of the tasks. Let’s turn on the scheduler for this DAG and see the results of
    the execution (*Figure 6**.5*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Airflow UI – DAG Graph view after execution](img/B21927_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Airflow UI – DAG Graph view after execution
  prefs: []
  type: TYPE_NORMAL
- en: When we turn on the scheduler, as the scheduler is set to `@once`, Airflow will
    automatically start the execution. It marks the tasks as a success when they are
    done. Click on the task with the name `first_task` and click on **Logs** to check
    the output (*Figure 6**.6*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Airflow UI – first_task output](img/B21927_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Airflow UI – first_task output
  prefs: []
  type: TYPE_NORMAL
- en: Note that the output we programmed is shown in the logs – “And so, it begins!”.
    You can also check the logs of the other tasks to ensure that everything went
    as expected. Another important view in Airflow is the Gantt chart. Click on that
    at the top of the page to visualize how much time was spent on each task (*Figure
    6**.7*). This is a great tool to check for execution bottlenecks and possibilities
    for optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Airflow UI – Gantt view](img/B21927_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Airflow UI – Gantt view
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You just built your first Airflow DAG! Now, let’s see how we
    can integrate Airflow with other tools and orchestrate a more complex pipeline
    with it.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow integration with other tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will take the DAG code developed in the last section and rebuild it with
    some different tasks. Our DAG is going to download the `Titanic` data, write it
    to a PostgreSQL table, and write it as a CSV file to Amazon S3\. Also, we will
    create a view with a simple analysis on Postgres directly from Airflow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file in the `dags` folder and name it `postgres_aws_dag.py`.
    The first part of our code will define the necessary modules. Note that, this
    time, we are importing the `PostgresOperator` class to interact with this database
    and the `Variable` class. This will help us manage secrets and parameters in Airflow.
    We are also creating an SQLAlchemy engine to connect to a local Postgres database
    and creating an S3 client that will allow writing files to S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s start developing our DAG. First, let’s define four tasks – one to
    download the data and the second one to write it as a Postgres table. The third
    task will create a view in Postgres with a grouped summarization, and the last
    one will upload the CSV file to S3\. This last one is an example of a good practice,
    a task sending data processing to run outside of Airflow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: At this point, you should have a bucket created in S3 with the name `bdok-<YOUR_ACCOUNT_NUMBER>`.
    Appending your account number in a bucket’s name is a great way to guarantee its
    uniqueness.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, save your file and take a look at the Airflow UI. Note that the DAG is
    not available, and Airflow shows an error. Expand the error message and you will
    see that it’s complaining about the variables we are trying to get in our code
    (*Figure 6**.8*). They don’t exist just yet. Let’s create them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Airflow UI – variables error](img/B21927_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Airflow UI – variables error
  prefs: []
  type: TYPE_NORMAL
- en: In the upper menu, click on **Admin** and choose **Variables**. Now, we will
    create the Airflow environment variables that we need. Copy your AWS secret access
    key and access key ID and create those variables accordingly. After creating the
    variables, you can see that the secret access key is hidden in the UI (*Figure
    6**.9*). Airflow automatically detects secrets and sensitive credentials and hides
    them in the UI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Airflow UI – variables created](img/B21927_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Airflow UI – variables created
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get back to the main page of the UI. Now, the DAG is showing correctly
    but we are not done yet. For the `PostgresOperator` task to run correctly, it
    is expecting a Postgres connection (remember the `postgres_conn_id` parameter?).
    In the upper menu, click on **Admin** and then on **Connections**. Add a new connection
    as shown in *Figure 6**.10*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Airflow UI – new connection](img/B21927_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Airflow UI – new connection
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating the connection, let’s finish developing our DAG. We have already
    set the tasks. Now it’s time to chain them together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: And now, we’re good to go. You can also check the complete code, available on
    GitHub ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/postgres_aws_dag.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/postgres_aws_dag.py)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Check the DAG’s graph in the Airflow UI (*Figure 6**.11*) and note how it automatically
    parallelizes all tasks that are possible – in this case, `write_to_postgres` and
    `upload_to_s3`. Turn on the DAG’s scheduler for it to run. After the DAG runs
    successfully, check the S3 bucket to validate that the file was correctly uploaded.
    Then, choose your preferred SQL client, and let’s check if the data was correctly
    ingested to Postgres. For this example, I’m using DBeaver, but you can choose
    any one you like.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Airflow UI – final DAG](img/B21927_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Airflow UI – final DAG
  prefs: []
  type: TYPE_NORMAL
- en: In DBeaver, create a new connection to PostgreSQL. For DBeaver’s connection,
    we will use `localhost` as the host for Postgres (in Airflow, we need to use `postgres`,
    as it is running in a shared network inside Docker). Complete it with the username
    and password (in this case, `postgres` and `postgres`) and test the connection.
    If everything is OK, create the connection, and let’s run some queries on this
    database. In *Figure 6**.12*, we can see that the data was ingested correctly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.12 – DBeaver – titanic table](img/B21927_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – DBeaver – titanic table
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s check whether the view was correctly created. The results are shown
    in *Figure 6**.13*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.13 – DBeaver – created view](img/B21927_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – DBeaver – created view
  prefs: []
  type: TYPE_NORMAL
- en: 'And that’s it! You created a table and a view and uploaded data to AWS using
    Airflow! To stop Airflow’s containers, go back to your terminal and type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the fundamentals of Apache Airflow – from installation
    to developing data pipelines. You learned how to leverage Airflow to orchestrate
    complex workflows involving data acquisition, processing, and integration with
    external systems.
  prefs: []
  type: TYPE_NORMAL
- en: We installed Airflow locally using the Astro CLI and Docker. This provided a
    quick way to get hands-on without a heavy setup. You were exposed to Airflow’s
    architecture and key components, such as the scheduler, worker, and metadata database.
    Understanding these pieces is crucial for monitoring and troubleshooting Airflow
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: Then, there was a major section focused on building your first Airflow DAGs.
    You used core Airflow operators and the task and DAG decorators to define and
    chain tasks. We discussed best practices such as keeping tasks small and autonomous.
    You also learned how Airflow handles task dependencies – allowing parallel execution
    of independent tasks. These learnings will help you develop effective DAGs that
    are scalable and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we integrated Airflow with external tools – writing to PostgreSQL, creating
    views, and uploading files to S3\. This showcased Airflow’s versatility to orchestrate
    workflows involving diverse systems. We also configured Airflow connections and
    variables to securely pass credentials.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should have grasped the fundamentals of Airflow
    and have had hands-on experience building data pipelines. You are now equipped
    to develop DAGs, integrate other tools, and apply best practices for production-grade
    workflows. As data teams adopt Airflow, these skills will be invaluable for creating
    reliable and scalable data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will study one of the core technologies for real-time
    data – Apache Kafka.
  prefs: []
  type: TYPE_NORMAL
