- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Building Pipelines with Apache Airflow
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Airflow构建管道
- en: Apache Airflow has become the *de facto* standard for building, monitoring,
    and maintaining data pipelines. As data volumes and complexity grow, the need
    for robust and scalable orchestration is paramount. In this chapter, we will cover
    the fundamentals of Airflow – installing it locally, exploring its architecture,
    and developing your first **Directed Acyclic** **Graphs** (**DAGs**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Airflow已成为构建、监控和维护数据管道的*事实*标准。随着数据量和复杂性的增长，对强大且可扩展的编排的需求变得至关重要。在本章中，我们将介绍Airflow的基础知识——如何在本地安装它，探索其架构，并开发你的第一个**有向无环图**(**DAGs**)。
- en: We will start by spinning up Airflow using Docker and the Astro CLI. This will
    allow you to get hands-on without the overhead of a full production installation.
    Next, we’ll get to know Airflow’s architecture and its key components, such as
    the scheduler, workers, and metadata database.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用Docker和Astro CLI来启动Airflow。这将使你可以动手操作，而无需承担完整生产环境安装的负担。接下来，我们将了解Airflow的架构及其关键组件，如调度器、工作节点和元数据数据库。
- en: Moving on, you’ll create your first DAG – the core building block of any Airflow
    workflow. Here, you’ll get exposed to operators – the tasks that comprise your
    pipelines. We’ll cover the most common operators used in data engineering, such
    as `PythonOperator`, `BashOperator`, and sensors. By chaining these operators
    together, you’ll build autonomous, robust DAGs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将创建你的第一个DAG——任何Airflow工作流的核心构建块。在这里，你将接触到操作符——组成你管道的任务。我们将介绍数据工程中最常用的操作符，如`PythonOperator`、`BashOperator`和传感器。通过将这些操作符串联在一起，你将构建出自主且强大的DAG。
- en: Later in the chapter, we’ll level up – tackling more complex pipelines and integrating
    with external tools such as databases and cloud-based storage services. You’ll
    learn best practices for creating production-grade workflows. Finally, we’ll run
    an end-to-end pipeline orchestrating an entire data engineering process – ingestion,
    processing, and data delivery.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章后面，我们将提升难度——处理更复杂的管道并与外部工具如数据库和云存储服务进行集成。你将学习创建生产级工作流的最佳实践。最后，我们将运行一个端到端的管道，编排整个数据工程过程——数据摄取、处理和数据交付。
- en: By the end of this chapter, you’ll understand how to build, monitor, and maintain
    data pipelines with Airflow. You’ll be able to develop effective DAGs using Python
    and apply Airflow best practices for scale and reliability.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将理解如何使用Airflow构建、监控和维护数据管道。你将能够使用Python开发有效的DAG，并应用Airflow最佳实践以实现扩展性和可靠性。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Getting started with Airflow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入门Airflow
- en: Building a data pipeline
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建数据管道
- en: Airflow integration with other tools
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Airflow与其他工具的集成
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For the activities in this chapter, you should have Docker installed and a valid
    AWS account. If you have doubts about how to do the installations and account
    setup, see [*Chapter 1*](B21927_01.xhtml#_idTextAnchor015) and [*Chapter 3*](B21927_03.xhtml#_idTextAnchor053).
    All the code for this chapter is available online in the GitHub repository ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes))
    in the `Chapter`0`6` folder.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的活动要求你已安装Docker，并拥有有效的AWS账号。如果你对如何进行安装和账号设置有疑问，请参见[*第1章*](B21927_01.xhtml#_idTextAnchor015)和[*第3章*](B21927_03.xhtml#_idTextAnchor053)。本章的所有代码可以在线访问，位于GitHub仓库中([https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes))的`Chapter06`文件夹中。
- en: Getting started with Airflow
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门Airflow
- en: In this first section, we will get Apache Airflow up and running on our local
    machine using the Astro CLI. Astro makes it easy to install and manage Apache
    Airflow. We will also take a deep dive into the components that make up Airflow’s
    architecture.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Astro CLI在本地机器上启动Apache Airflow。Astro使得安装和管理Apache Airflow变得容易。我们还将深入了解构成Airflow架构的各个组件。
- en: Installing Airflow with Astro
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Astro安装Airflow
- en: Astro is a command-line interface provided by Astronomer that allows you to
    quickly install and run Apache Airflow. With Astro, we can quickly spin up a local
    Airflow environment. It abstracts away the complexity of manually installing all
    Airflow components.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Astro是Astronomer提供的命令行界面，允许你快速安装和运行Apache Airflow。使用Astro，我们可以快速启动一个本地Airflow环境。它抽象了手动安装所有Airflow组件的复杂性。
- en: 'Installing the Astro CLI is very straightforward. You can find instructions
    for its installation here: [https://docs.astronomer.io/astro/cli/install-cli](https://docs.astronomer.io/astro/cli/install-cli).
    Once installed, the first thing to do is to initiate a new Airflow project. In
    the terminal, run the following command:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Astro CLI 非常简单。你可以在这里找到安装说明：[https://docs.astronomer.io/astro/cli/install-cli](https://docs.astronomer.io/astro/cli/install-cli)。安装完成后，第一件事就是启动一个新的
    Airflow 项目。在终端中运行以下命令：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will create a folder structure for an Airflow project locally. Next, start
    up Airflow:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为本地 Airflow 项目创建一个文件夹结构。接下来，启动 Airflow：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will pull the necessary Docker images and start containers for the Airflow
    web server, scheduler, worker, and PostgreSQL database.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这将拉取必要的 Docker 镜像，并启动 Airflow Web 服务器、调度器、工作节点和 PostgreSQL 数据库的容器。
- en: You can access the Airflow UI at [http://localhost:8080](http://localhost:8080).
    The default username and password are *admin*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过访问 Airflow 用户界面：[http://localhost:8080](http://localhost:8080)。默认的用户名和密码是
    *admin*。
- en: That’s it! In just a few commands, we have a fully functioning Airflow environment
    up and running locally. Now let’s take a deeper look into Airflow’s architecture.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！只需几条命令，我们就可以在本地搭建一个完全功能的 Airflow 环境。现在，让我们更深入地了解 Airflow 的架构。
- en: Airflow architecture
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Airflow 架构
- en: Airflow is composed of different components that fit together to provide a scalable
    and reliable orchestration platform for data pipelines.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 由多个组件组成，这些组件紧密结合，为数据管道提供一个可扩展且可靠的编排平台。
- en: 'At a high level, Airflow has the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，Airflow 包含以下内容：
- en: A metadata database that stores state for DAGs, task instances, XComs, and so
    on
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储 DAG、任务实例、XCom 等状态的元数据数据库
- en: A web server that serves the Airflow UI
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供 Airflow 用户界面的 Web 服务器
- en: A scheduler that handles triggering DAGs and task instances
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理触发 DAG 和任务实例的调度器
- en: Executors that run task instances
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行任务实例的执行器
- en: Workers that execute tasks
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行任务的工作节点
- en: Other components, such as the CLI
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他组件，例如 CLI
- en: 'This architecture is depicted here:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构如图所示：
- en: '![Figure 6.1 – Airflow Architecture](img/B21927_06_01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – Airflow 架构](img/B21927_06_01.jpg)'
- en: Figure 6.1 – Airflow Architecture
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – Airflow 架构
- en: Airflow relies heavily on the metadata database as the source of truth for state.
    The web server, scheduler, and worker processes talk to this database. When you
    look at the Airflow UI, underneath, it simply queries this database to get info
    to display.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 在很大程度上依赖元数据数据库作为状态的真实来源。Web 服务器、调度器和工作节点进程都与这个数据库进行通信。当你查看 Airflow 用户界面时，实际上它只是查询该数据库以获取显示信息。
- en: The metadata database is also used to enforce certain constraints. For example,
    the scheduler uses database locks when examining task instances to determine what
    to schedule next. This prevents race conditions between multiple scheduler processes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据数据库也用于强制执行某些约束。例如，调度器在检查任务实例时会使用数据库锁来确定下一步该调度什么。这可以防止多个调度器进程之间发生竞态条件。
- en: Important note
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: A race condition occurs when two or more threads or processes access a shared
    resource concurrently, and the final output depends on the sequence or timing
    of the execution. The threads “race” to access or modify the shared resource,
    and the final state depends, unpredictably, on who gets there first. Race conditions
    are a common source of bugs and unpredictable behavior in concurrent systems.
    They can result in corrupted data, crashes, or incorrect outputs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 竞态条件发生在两个或多个线程或进程并发访问共享资源时，最终输出取决于执行的顺序或时机。线程们“竞速”访问或修改共享资源，最终的状态不可预测地取决于谁先到达。竞态条件是并发系统中常见的错误来源，可能导致数据损坏、崩溃或错误的输出。
- en: Now let’s examine some of the key components in more detail.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更详细地看看一些关键组件。
- en: Web server
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Web 服务器
- en: The Airflow web server is responsible for hosting the Airflow UI you interact
    with, providing REST APIs for other services to communicate with Airflow, and
    serving static assets and pages. The Airflow UI allows you to monitor, trigger,
    and troubleshoot DAGs and tasks. It provides visibility into the overall health
    of your data pipelines.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow Web 服务器负责托管你与之交互的 Airflow 用户界面，提供 REST API 以供其他服务与 Airflow 通信，并提供静态资源和页面。Airflow
    用户界面允许你监控、触发和排查 DAG 和任务。它提供了你数据管道整体健康状况的可视化。
- en: The web server also exposes REST APIs that are used by the CLI, scheduler, workers,
    and custom applications to talk to Airflow. For example, the CLI uses the API
    to trigger DAGs. The scheduler uses it to update state for DAGs. Workers use it
    to update task instance state as they process them.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Web服务器还暴露了REST API，CLI、调度器、工作节点和自定义应用程序使用这些API与Airflow进行通信。例如，CLI使用API触发DAG，调度器使用它来更新DAG的状态，工作节点在处理任务时使用它来更新任务实例的状态。
- en: While the UI is very convenient for humans, services rely on the underlying
    REST APIs. Overall, the Airflow web server is critical as it provides a central
    way for users and services to interact with Airflow metadata.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然UI对于人类来说非常方便，但服务依赖于底层的REST API。总体而言，Airflow Web服务器至关重要，因为它为用户和服务提供了与Airflow元数据交互的中央方式。
- en: Scheduler
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调度器
- en: 'The Airflow scheduler is the brains behind examining task instances and determining
    what to run next. Its key responsibilities include the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow调度器是大脑，负责检查任务实例并决定接下来要运行的任务。其主要职责包括：
- en: Checking the status of task instances in the metadata database
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查元数据数据库中任务实例的状态
- en: Examining dependencies between tasks to create a DAG run execution plan
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查任务之间的依赖关系，以创建DAG运行执行计划
- en: Setting tasks to scheduled or queued in the database
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将任务设置为调度或排队状态并存储在数据库中
- en: Tracking the progress as task instances move through different states
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪任务实例在不同状态之间的进度
- en: Handling the backfilling of historical runs
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理历史任务的回填
- en: 'To perform these duties, the scheduler does the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行这些职责，调度器执行以下操作：
- en: Refreshes the DAG dictionary with details about all active DAGs
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 刷新DAG字典，获取所有活动DAG的详细信息
- en: Examines active DAG runs to see what tasks need to be scheduled
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查活动的DAG运行，查看需要调度哪些任务
- en: Checks on the status of running tasks via the job tracker
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过作业跟踪器检查正在运行的任务状态
- en: Updates the state of tasks in the database – queued, running, success, failed,
    and so on
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新数据库中任务的状态——排队中、运行中、成功、失败等等
- en: Critical to the scheduler’s functioning is the metadata database. This allows
    it to be highly scalable since multiple schedulers can coordinate and sync via
    the single source of truth in the database.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对调度器的功能至关重要的是元数据数据库。这使得它具有高度可扩展性，因为多个调度器可以通过数据库中的单一真实数据源进行协调和同步。
- en: The scheduler is very versatile – you can run a single scheduler for small workloads
    or scale up to multiple active schedulers for large workloads.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器非常灵活——你可以为小型工作负载运行一个调度器，或为大型工作负载扩展到多个活动调度器。
- en: Executors
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行器
- en: When a task needs to run, the executor is responsible for actually running the
    task. Executors interface with a pool of workers that execute tasks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务需要运行时，执行器负责实际执行任务。执行器通过与工作节点池的接口来执行任务。
- en: 'The most common executors are `LocalExecutor`, `CeleryExecutor`, and `KubernetesExecutor`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的执行器有`LocalExecutor`、`CeleryExecutor`和`KubernetesExecutor`：
- en: LocalExecutor runs task instances in parallel processes on the host system.
    It is great for testing but has very limited scalability for large workloads.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LocalExecutor在主机系统上并行进程中运行任务实例。它非常适合测试，但在处理大型工作负载时扩展性非常有限。
- en: CeleryExecutor uses a Celery pool to distribute tasks. It allows running workers
    across multiple machines and, thus, provides horizontal scalability.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CeleryExecutor使用Celery池来分配任务。它允许在多台机器上运行工作节点，从而提供水平扩展性。
- en: KubernetesExecutor is specially designed for Airflow deployments running in
    Kubernetes. It launches worker Pods in Kubernetes dynamically. It provides excellent
    scalability and resource isolation.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KubernetesExecutor专为在Kubernetes中运行的Airflow部署而设计。它动态启动Kubernetes中的工作节点Pod，提供出色的扩展性和资源隔离。
- en: As we move our Airflow to production, being able to scale out workers is critical.
    KubernetesExecutor will play a main role in our case.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将Airflow推向生产环境时，能够扩展工作节点至关重要。在我们的情况下，KubernetesExecutor将发挥主导作用。
- en: For testing locally, LocalExecutor is the simplest. Astro configures this by
    default.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本地测试，LocalExecutor是最简单的。Astro默认配置此执行器。
- en: Workers
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作节点
- en: 'Workers execute the actual logic for task instances. The executor manages and
    interfaces with the worker pool. Workers carry out tasks such as the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点执行任务实例的实际逻辑。执行器管理并与工作节点池进行接口。工作节点执行以下任务：
- en: Running Python functions
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行Python函数
- en: Executing Bash commands
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行Bash命令
- en: Making API requests
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发起API请求
- en: Performing data transfers
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行数据传输
- en: Communicating task status
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通信任务状态
- en: Based on the executor, workers may run on threads, server processes, or in separate
    containers. The worker communicates the status of task instances to the metadata
    database. It updates state to queued, running, success, failed, and so on. This
    allows the scheduler to monitor progress and coordinate pipeline execution across
    workers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 根据执行器，工作进程可以在线程、服务器进程或独立容器中运行。工作进程将任务实例的状态传递给元数据数据库，并更新状态为排队、运行、成功、失败等。这使得调度器能够监控进度并协调跨工作进程的管道执行。
- en: In summary, workers provide the compute resources necessary to run our pipeline
    tasks. The executor interfaces with and manages these workers.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，工作进程提供了运行管道任务所需的计算资源。执行器与这些工作进程进行接口对接并进行管理。
- en: Queueing
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排队
- en: For certain executors, such as Celery and Kubernetes, you need an additional
    queueing service. This queue stores tasks before workers pick them up. There are
    a few common queueing technologies that can be used with Celery, such as RabbitMQ
    (a popular open source queue), Redis (an in-memory datastore), and Amazon SQS
    (a fully managed queue service by AWS).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些执行器，如Celery和Kubernetes，您需要一个额外的排队服务。这个队列在工作进程拾取任务之前存储任务。Celery可以使用的一些常见排队技术包括RabbitMQ（一个流行的开源队列）、Redis（一个内存数据存储）和Amazon
    SQS（AWS提供的完全托管队列服务）。
- en: For Kubernetes, we don’t need any of these tools as KubernetesExecutor dynamically
    launches Pods to execute tasks and kill them when the tasks are done.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Kubernetes，我们不需要这些工具，因为KubernetesExecutor会动态启动Pods来执行任务，并在任务完成时终止它们。
- en: Metadata database
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 元数据数据库
- en: As highlighted earlier, Airflow relies on its metadata database heavily. This
    database stores the state and metadata for Airflow to function. The default for
    local testing is SQLite, which is simple but has major scalability limitations.
    Even for moderate workloads, it is recommended to switch to a more production-grade
    database.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Airflow严重依赖其元数据数据库。这个数据库存储Airflow功能所需的状态和元数据。默认的本地测试数据库是SQLite，它简单但有较大的可扩展性限制。即使是中等工作负载，也建议切换到更适合生产环境的数据库。
- en: Airflow works with PostgreSQL, MySQL, and a variety of cloud-based database
    services, such as Amazon RDS.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow支持PostgreSQL、MySQL以及各种基于云的数据库服务，如Amazon RDS。
- en: Airflow’s distributed architecture
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Airflow的分布式架构
- en: 'As we can see, Airflow works with a modular distributed architecture. This
    design brings several advantages for production workloads:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，Airflow采用了模块化的分布式架构。这个设计为生产工作负载带来了多个优势：
- en: '**Separation of concerns**: Each component focuses on a specific job. The scheduler
    handles examining DAGs and scheduling. The worker runs task instances. This separation
    of concerns keeps components simple and maintainable.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注点分离**：每个组件专注于特定的任务。调度器负责检查DAG并进行调度，工作进程负责运行任务实例。这种关注点分离使得各个组件简单且易于维护。'
- en: '**Scalability**: Components such as the scheduler, worker, and database can
    be easily scaled out. Run multiple schedulers or workers as your workload grows.
    Leverage a hosted database for automatic scaling.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：调度器、工作进程和数据库等组件可以轻松地横向扩展。随着工作负载的增加，可以运行多个调度器或工作进程。利用托管数据库实现自动扩展。'
- en: '**Reliability**: If one scheduler or worker dies, there is no overall outage
    since components are decoupled. The single source of truth in the database also
    provides consistency across Airflow.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可靠性**：如果一个调度器或工作进程崩溃，由于各组件解耦，系统不会出现整体故障。数据库中的单一真实数据源还提供了Airflow的一致性。'
- en: '**Extensibility**: You can swap out certain components, such as the executor
    or queueing service.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展性**：可以更换某些组件，如执行器或排队服务。'
- en: In summary, Airflow provides scalability, reliability, and flexibility via its
    modular architecture. Each component has a focused job, leading to simplicity
    and stability in the overall system.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Airflow通过其模块化架构提供了可扩展性、可靠性和灵活性。每个组件都有明确的职能，这使得系统整体简洁且稳定。
- en: Now, let’s get back to Airflow and start building some simple DAGs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到Airflow并开始构建一些简单的DAG。
- en: Building a data pipeline
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建数据管道
- en: 'Let’s start developing a simple DAG. All your Python code should be inside
    the `dags` folder. For our first hands-on exercise, we will work with the `Titanic`
    dataset:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始开发一个简单的DAG。您的所有Python代码应放在`dags`文件夹中。为了进行第一次实操，我们将使用`Titanic`数据集：
- en: 'Open a file in the `dags` folder and save it as `titanic_dag.py`. We will begin
    by importing the necessary libraries:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`dags`文件夹中的一个文件，并将其保存为`titanic_dag.py`。我们将首先导入必要的库：
- en: '[PRE2]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we will define some default arguments for our DAG – in this case, the
    owner (important for DAG filtering) and the start date:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将为我们的 DAG 定义一些默认参数 - 在本例中，所有者（用于 DAG 过滤）和开始日期：
- en: '[PRE3]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, we will define a function for our DAG using the `@dag` decorator. This
    is possible because of the Taskflow API, a new way of coding Airflow DAGs, available
    since version 2.0\. It makes it easier and faster to develop DAGs’ Python code.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用 `@dag` 装饰器为我们的 DAG 定义一个函数。这是由于 Taskflow API 的存在，这是一种新的编写 Airflow DAGs
    的方式，自版本 2.0 起可用。它使得开发 DAGs 的 Python 代码变得更加简单和快速。
- en: 'Inside the `@dag` decorator, we define some important parameters. The default
    arguments are already set in a Python dictionary. The `schedule_interval` is set
    to `@once`, meaning this DAG will only run one time when triggered. The `description`
    parameter helps us understand in the UI what this DAG is doing. It is a good practice
    to always define it. The `catchup` is also important and it should always be set
    to `False`. When you have a DAG with several pending runs, when you trigger the
    execution, Airflow will automatically try to run all the past runs at once, which
    can cause an overload. Setting this parameter to `False` tells Airflow that, if
    there are any pending runs, Airflow will just run the last one and continue normally
    with the schedule. Finally, tags are not a required parameter but are great for
    filtering in the UI. Immediately after the `@dag` decorator, you should define
    a function for the DAG. In our case, we’ll define a function called `titanic_processing`.
    Inside this function, we will define our tasks. We can do that using an Airflow
    operator (such as `DummyOperator`) or using functions with the `@``task` decorator:'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 `@dag` 装饰器内部，我们定义了一些重要的参数。默认参数已经在 Python 字典中设置好了。`schedule_interval` 设置为 `@once`，意味着此
    DAG 仅在触发时运行一次。`description` 参数帮助我们在 UI 中理解此 DAG 的作用。始终定义它是一个良好的实践。`catchup` 也很重要，应始终设置为
    `False`。当您有多个待运行的 DAG 时，触发执行时，Airflow 会自动尝试一次性运行所有过去的运行，这可能会导致负载过重。将此参数设置为 `False`
    告诉 Airflow，如果有任何待运行的 DAG，则只运行最后一个，并按照计划正常继续。最后，标签不是必需的参数，但在 UI 中用于过滤非常有效。在 `@dag`
    装饰器之后，应该定义一个用于 DAG 的函数。在我们的情况下，我们将定义一个名为 `titanic_processing` 的函数。在这个函数内部，我们将定义我们的任务。我们可以使用
    Airflow 运算符（如 `DummyOperator`）或使用带有 `@task` 装饰器的函数来完成这些任务：
- en: '[PRE4]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding example, we have two tasks defined so far. One of them is using
    `DummyOperator`, which does literally nothing. It is often used to set marks on
    your DAG. We will use this just to mark the start and the end of the DAG.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在上面的示例中，到目前为止我们已经定义了两个任务。其中一个使用了 `DummyOperator`，它实际上什么也不做。通常用于设置 DAG 的标志。我们将使用它来标记
    DAG 的开始和结束。
- en: 'Next, we have our first task, just printing `"And so, it begins!"` in the logs.
    This task is defined with a simple Python function and the `@task` decorator.
    Now, we will define the tasks that download and process the dataset. Remember
    that all of the following code should be indented (inside the `titanic_processing`
    function). You can check the complete code in the book’s GitHub repository ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/titanic_dag.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/titanic_dag.py)):'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们有我们的第一个任务，在日志中打印 `"And so, it begins!"`。这个任务使用简单的 Python 函数和 `@task` 装饰器定义。现在，我们将定义下载和处理数据集的任务。请记住，以下所有代码都应该缩进（在
    `titanic_processing` 函数内部）。您可以在本书的 GitHub 代码库中查看完整的代码（[https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/titanic_dag.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/titanic_dag.py)）：
- en: '[PRE5]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first few tasks print messages, download the dataset, and save it to `/tmp`
    (temporary folder). Then the `analyze_survivors` task loads the CSV data, counts
    the number of survivors, and prints the result. The `survivors_sex` task groups
    the survivors by sex and prints the counts. Those prints can be visualized in
    the log of each task in the Airflow UI.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 前几个任务打印消息，下载数据集，并将其保存到 `/tmp`（临时文件夹）。然后 `analyze_survivors` 任务加载 CSV 数据，计算幸存者的数量，并打印结果。`survivors_sex`
    任务按性别分组幸存者并打印计数。这些打印可以在 Airflow UI 中每个任务的日志中看到。
- en: Important note
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You might ask “*Why divide the download of the data and two analyses in three
    steps? Why do we not just do everything as one whole task?*” First, it is important
    to acknowledge that Airflow is not a data processing tool but an orchestration
    tool. Big data should not run inside Airflow (as we are doing here) because you
    could easily run out of resources. Instead, Airflow should trigger processing
    tasks that will run somewhere else. We’ll see an example of how to trigger processing
    in PostgreSQL from a task later in the chapter, in the next section. Second, it
    is good practice to keep tasks as simple as possible and as independent as possible.
    This allows for more parallelism and a DAG that is easier to debug.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问：“*为什么要将数据下载和两次分析分为三步？我们为什么不将一切做成一个整体任务？*”首先，重要的是要认识到，Airflow 不是一个数据处理工具，而是一个编排工具。大数据不应该在
    Airflow 中运行（就像我们现在做的那样），因为你可能会轻易耗尽资源。相反，Airflow 应该触发在其他地方运行的处理任务。我们将在本章的下一部分看到如何通过任务触发
    PostgreSQL 中的处理。其次，保持任务尽可能简单和独立是一种良好的实践。这样可以实现更多的并行性，并且使得 DAG 更容易调试。
- en: 'Finally, we will code two more tasks to exemplify other possibilities using
    Airflow operators. First, we will code a simple `BashOperator` task to print a
    message. It can be used to run any bash command with Airflow. Later, we have another
    `DummyOperator` task that does nothing – it only marks the end of the pipeline.
    This task is optional. Remember that those tasks should be indented, inside the
    `titanic_processing` function:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将再编写两个任务，以示范使用 Airflow 运算符的其他可能性。首先，我们将编写一个简单的 `BashOperator` 任务来打印一条消息。它可以用于通过
    Airflow 运行任何 bash 命令。接下来，我们有另一个 `DummyOperator` 任务，它什么也不做——它只是标记管道的结束。这个任务是可选的。记住，这些任务应该缩进，在
    `titanic_processing` 函数内部：
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now that we have defined all the tasks we need, we are going to orchestrate
    the pipeline, that is, tell Airflow how to chain the tasks. We can do this in
    two ways. The universal way is to use the `>>` operator, which indicates the order
    between tasks. The other way is usable when we have function tasks with parameters.
    We can pass an output of a function as a parameter to another and Airflow will
    automatically understand that there is a dependency between those tasks. These
    lines should also be indented, so be careful:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了所有需要的任务，接下来我们将编排管道，也就是告诉 Airflow 如何将任务串联起来。我们可以通过两种方式来实现。通用的方式是使用 `>>`
    运算符，它表示任务之间的顺序。另一种方式适用于有参数的函数任务。我们可以将一个函数的输出作为参数传递给另一个任务，Airflow 会自动理解这些任务之间存在依赖关系。这些行也应该缩进，所以要小心：
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First, we have to run the function tasks and save them as Python objects. Then,
    we chain them in order using `>>`. The third line tells Airflow that there is
    a dependency between the `start` task, the `first` task, and the `download_data`
    task, which should be triggered in this order. Next, we run the `analyze_survivors`
    and `survivors_sex` tasks and give the `downloaded` output as a parameter. With
    this, Airflow can detect that there is a dependency between them. Finally, we
    tell Airflow that after the `analyze_survivors` and `survivors_sex` tasks, we
    have the `last` and `end` tasks. Note that `analyze_survivors` and `survivors_sex`
    are inside a list, meaning that they can run in parallel. This is an important
    feature of dependency management in Airflow. The “rule of thumb” is that any tasks
    that do not depend on each other should run in parallel to optimize the pipeline
    delivery time.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要运行函数任务并将它们保存为 Python 对象。然后，使用 `>>` 将它们按顺序串联起来。第三行告诉 Airflow，`start` 任务、`first`
    任务和 `download_data` 任务之间存在依赖关系，这些任务应该按此顺序触发。接下来，我们运行 `analyze_survivors` 和 `survivors_sex`
    任务，并将 `downloaded` 输出作为参数传递。这样，Airflow 可以检测到它们之间的依赖关系。最后，我们告诉 Airflow，在 `analyze_survivors`
    和 `survivors_sex` 任务之后，我们有 `last` 和 `end` 任务。请注意，`analyze_survivors` 和 `survivors_sex`
    位于一个列表中，意味着它们可以并行运行。这是 Airflow 依赖关系管理中的一个重要特性。一般经验法则是，任何没有相互依赖的任务应该并行运行，以优化管道的交付时间。
- en: 'Now, the last thing is to initialize the DAG, running the function and saving
    it in a Python object. This code should not be indented, as it is outside the
    `titanic_processing` function:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最后一步是初始化 DAG，运行函数并将其保存在 Python 对象中。此代码不应缩进，因为它位于 `titanic_processing` 函数外部：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we’re good to go. Go to the terminal. Be sure you are in the same folder
    we used to initialize the Airflow project with the Astro CLI. Then, run the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始了。打开终端。确保你处于我们用 Astro CLI 初始化 Airflow 项目时使用的相同文件夹中。然后，运行以下命令：
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This will download the Airflow Docker image and start the containers. After
    Airflow is correctly started, Astro will open a browser tab to the login page
    of the Airflow UI. If it does not open automatically, you can access it at [http://localhost:8080/](http://localhost:8080/).
    Log in with the default username and password (`admin`, `admin`). You should see
    your DAG in the Airflow UI (*Figure 6**.2*).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这将下载 Airflow Docker 镜像并启动容器。Airflow 正常启动后，Astro 将打开一个浏览器标签，跳转到 Airflow UI 的登录页面。如果没有自动打开，你可以通过
    [http://localhost:8080/](http://localhost:8080/) 访问它。使用默认的用户名和密码（`admin`，`admin`）登录。你应该能够在
    Airflow UI 中看到你的 DAG（*图 6.2*）。
- en: '![Figure 6.2 – Airflow UI – DAGs view](img/B21927_06_02.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – Airflow UI – DAG 视图](img/B21927_06_02.jpg)'
- en: Figure 6.2 – Airflow UI – DAGs view
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – Airflow UI – DAG 视图
- en: There is a button on the left side of the DAG to turn its scheduler on. Don’t
    click it yet. First, click on the DAG and take a look at all the views Airflow
    offers in a DAG. At first, you should see a summary with the DAG information (*Figure
    6**.3*).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: DAG 左侧有一个按钮可以启动其调度器。暂时不要点击它。首先，点击 DAG，查看 Airflow 在 DAG 中提供的所有视图。初始视图应该是包含 DAG
    信息的摘要（*图 6.3*）。
- en: '![Figure 6.3 – Airflow UI – DAG grid view](img/B21927_06_03.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – Airflow UI – DAG 网格视图](img/B21927_06_03.jpg)'
- en: Figure 6.3 – Airflow UI – DAG grid view
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – Airflow UI – DAG 网格视图
- en: Click on the **Graph** button to see a nice visualization of the pipeline (*Figure
    6**.4*).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 点击 **Graph** 按钮，查看管道的漂亮可视化效果（*图 6.4*）。
- en: '![Figure 6.4 – Airflow UI – DAG Graph view](img/B21927_06_04.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – Airflow UI – DAG 图形视图](img/B21927_06_04.jpg)'
- en: Figure 6.4 – Airflow UI – DAG Graph view
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – Airflow UI – DAG 图形视图
- en: Note how Airflow automatically detects the dependencies and the parallelisms
    of the tasks. Let’s turn on the scheduler for this DAG and see the results of
    the execution (*Figure 6**.5*).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意观察 Airflow 如何自动检测任务的依赖关系和并行性。现在让我们启用这个 DAG 的调度器，查看执行结果（*图 6.5*）。
- en: '![Figure 6.5 – Airflow UI – DAG Graph view after execution](img/B21927_06_05.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – Airflow UI – 执行后的 DAG 图形视图](img/B21927_06_05.jpg)'
- en: Figure 6.5 – Airflow UI – DAG Graph view after execution
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – Airflow UI – 执行后的 DAG 图形视图
- en: When we turn on the scheduler, as the scheduler is set to `@once`, Airflow will
    automatically start the execution. It marks the tasks as a success when they are
    done. Click on the task with the name `first_task` and click on **Logs** to check
    the output (*Figure 6**.6*).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们启动调度器时，由于调度器设置为 `@once`，Airflow 会自动开始执行。任务完成后，它会将任务标记为成功。点击名为 `first_task`
    的任务，然后点击 **日志** 查看输出（*图 6.6*）。
- en: '![Figure 6.6 – Airflow UI – first_task output](img/B21927_06_06.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – Airflow UI – first_task 输出](img/B21927_06_06.jpg)'
- en: Figure 6.6 – Airflow UI – first_task output
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – Airflow UI – first_task 输出
- en: Note that the output we programmed is shown in the logs – “And so, it begins!”.
    You can also check the logs of the other tasks to ensure that everything went
    as expected. Another important view in Airflow is the Gantt chart. Click on that
    at the top of the page to visualize how much time was spent on each task (*Figure
    6**.7*). This is a great tool to check for execution bottlenecks and possibilities
    for optimization.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们编程的输出显示在日志中——“于是，它开始了！”。你还可以检查其他任务的日志，确保一切按预期进行。Airflow 中的另一个重要视图是甘特图。点击页面顶部的甘特图，可以查看每个任务花费的时间（*图
    6.7*）。这是检查执行瓶颈和优化可能性的一个好工具。
- en: '![Figure 6.7 – Airflow UI – Gantt view](img/B21927_06_07.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – Airflow UI – 甘特图视图](img/B21927_06_07.jpg)'
- en: Figure 6.7 – Airflow UI – Gantt view
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – Airflow UI – 甘特图视图
- en: Congratulations! You just built your first Airflow DAG! Now, let’s see how we
    can integrate Airflow with other tools and orchestrate a more complex pipeline
    with it.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你刚刚创建了你的第一个 Airflow DAG！接下来，让我们看看如何将 Airflow 与其他工具集成，并用它来编排一个更复杂的工作流。
- en: Airflow integration with other tools
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Airflow 与其他工具的集成
- en: 'We will take the DAG code developed in the last section and rebuild it with
    some different tasks. Our DAG is going to download the `Titanic` data, write it
    to a PostgreSQL table, and write it as a CSV file to Amazon S3\. Also, we will
    create a view with a simple analysis on Postgres directly from Airflow:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用上一节开发的 DAG 代码，并用一些不同的任务进行重建。我们的 DAG 将下载 `Titanic` 数据，将其写入 PostgreSQL 表，并将其作为
    CSV 文件写入 Amazon S3。此外，我们还将直接通过 Airflow 在 Postgres 上创建一个包含简单分析的视图：
- en: 'Create a new Python file in the `dags` folder and name it `postgres_aws_dag.py`.
    The first part of our code will define the necessary modules. Note that, this
    time, we are importing the `PostgresOperator` class to interact with this database
    and the `Variable` class. This will help us manage secrets and parameters in Airflow.
    We are also creating an SQLAlchemy engine to connect to a local Postgres database
    and creating an S3 client that will allow writing files to S3:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`dags`文件夹中创建一个新的Python文件，命名为`postgres_aws_dag.py`。代码的第一部分将定义所需的模块。请注意，这次我们导入了`PostgresOperator`类来与该数据库交互，以及`Variable`类。这将帮助我们在Airflow中管理秘密和参数。我们还创建了一个SQLAlchemy引擎来连接到本地Postgres数据库，并创建了一个S3客户端，允许将文件写入S3：
- en: '[PRE10]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let’s start developing our DAG. First, let’s define four tasks – one to
    download the data and the second one to write it as a Postgres table. The third
    task will create a view in Postgres with a grouped summarization, and the last
    one will upload the CSV file to S3\. This last one is an example of a good practice,
    a task sending data processing to run outside of Airflow:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们开始开发我们的DAG。首先，定义四个任务——一个用于下载数据，第二个将其写入Postgres表。第三个任务将在Postgres中创建一个带有分组汇总的视图，最后一个任务将CSV文件上传到S3。这最后一个任务是一个良好实践的示例，任务将数据处理发送到Airflow外部运行：
- en: '[PRE11]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: At this point, you should have a bucket created in S3 with the name `bdok-<YOUR_ACCOUNT_NUMBER>`.
    Appending your account number in a bucket’s name is a great way to guarantee its
    uniqueness.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此时，你应该已经在S3中创建了一个名为`bdok-<YOUR_ACCOUNT_NUMBER>`的桶。在桶名中添加你的账户号是保证其唯一性的一种好方法。
- en: Now, save your file and take a look at the Airflow UI. Note that the DAG is
    not available, and Airflow shows an error. Expand the error message and you will
    see that it’s complaining about the variables we are trying to get in our code
    (*Figure 6**.8*). They don’t exist just yet. Let’s create them.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，保存你的文件并查看Airflow UI。注意DAG不可用，Airflow显示一个错误。展开错误消息，你会看到它抱怨我们在代码中尝试获取的变量（*图6.8*）。这些变量还不存在。让我们创建它们。
- en: '![Figure 6.8 – Airflow UI – variables error](img/B21927_06_08.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – Airflow UI – 变量错误](img/B21927_06_08.jpg)'
- en: Figure 6.8 – Airflow UI – variables error
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – Airflow UI – 变量错误
- en: In the upper menu, click on **Admin** and choose **Variables**. Now, we will
    create the Airflow environment variables that we need. Copy your AWS secret access
    key and access key ID and create those variables accordingly. After creating the
    variables, you can see that the secret access key is hidden in the UI (*Figure
    6**.9*). Airflow automatically detects secrets and sensitive credentials and hides
    them in the UI.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上方菜单中，点击**Admin**并选择**Variables**。现在，我们将创建我们需要的Airflow环境变量。复制你的AWS密钥和访问密钥ID，并相应地创建这些变量。创建变量后，你可以看到密钥在UI中被隐藏（*图6.9*）。Airflow会自动检测到秘密和敏感凭证，并在UI中隐藏它们。
- en: '![Figure 6.9 – Airflow UI – variables created](img/B21927_06_09.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9 – Airflow UI – 变量已创建](img/B21927_06_09.jpg)'
- en: Figure 6.9 – Airflow UI – variables created
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – Airflow UI – 变量已创建
- en: Now, let’s get back to the main page of the UI. Now, the DAG is showing correctly
    but we are not done yet. For the `PostgresOperator` task to run correctly, it
    is expecting a Postgres connection (remember the `postgres_conn_id` parameter?).
    In the upper menu, click on **Admin** and then on **Connections**. Add a new connection
    as shown in *Figure 6**.10*.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们回到UI的主页。DAG现在显示正确，但我们还没有完成。为了使`PostgresOperator`任务正确运行，它需要一个Postgres连接（记得`postgres_conn_id`参数吗？）。在上方菜单中，点击**Admin**，然后点击**Connections**。如*图6.10*所示，添加一个新的连接。
- en: '![Figure 6.10 – Airflow UI – new connection](img/B21927_06_10.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10 – Airflow UI – 新连接](img/B21927_06_10.jpg)'
- en: Figure 6.10 – Airflow UI – new connection
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – Airflow UI – 新连接
- en: 'After creating the connection, let’s finish developing our DAG. We have already
    set the tasks. Now it’s time to chain them together:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建连接后，让我们继续开发DAG。我们已经设置好了任务。现在是时候将它们链接在一起了：
- en: '[PRE12]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: And now, we’re good to go. You can also check the complete code, available on
    GitHub ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/postgres_aws_dag.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/postgres_aws_dag.py)).
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们准备好了。你还可以查看完整的代码，代码在GitHub上可用([https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/postgres_aws_dag.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/postgres_aws_dag.py))。
- en: Check the DAG’s graph in the Airflow UI (*Figure 6**.11*) and note how it automatically
    parallelizes all tasks that are possible – in this case, `write_to_postgres` and
    `upload_to_s3`. Turn on the DAG’s scheduler for it to run. After the DAG runs
    successfully, check the S3 bucket to validate that the file was correctly uploaded.
    Then, choose your preferred SQL client, and let’s check if the data was correctly
    ingested to Postgres. For this example, I’m using DBeaver, but you can choose
    any one you like.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Airflow UI 中查看 DAG 的图表（*图 6.11*），并注意它是如何自动并行化所有可能的任务——在这种情况下，`write_to_postgres`
    和 `upload_to_s3`。启动 DAG 的调度器让它运行。DAG 成功运行后，检查 S3 存储桶，以验证文件是否正确上传。然后，选择你喜欢的 SQL
    客户端，检查数据是否已正确导入 Postgres。此示例中，我使用的是 DBeaver，但你可以选择任何你喜欢的客户端。
- en: '![Figure 6.11 – Airflow UI – final DAG](img/B21927_06_11.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – Airflow UI – 最终 DAG](img/B21927_06_11.jpg)'
- en: Figure 6.11 – Airflow UI – final DAG
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – Airflow UI – 最终 DAG
- en: In DBeaver, create a new connection to PostgreSQL. For DBeaver’s connection,
    we will use `localhost` as the host for Postgres (in Airflow, we need to use `postgres`,
    as it is running in a shared network inside Docker). Complete it with the username
    and password (in this case, `postgres` and `postgres`) and test the connection.
    If everything is OK, create the connection, and let’s run some queries on this
    database. In *Figure 6**.12*, we can see that the data was ingested correctly.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 DBeaver 中，创建一个新的 PostgreSQL 连接。对于 DBeaver 的连接，我们将使用 `localhost` 作为 Postgres
    的主机（在 Airflow 中，我们需要使用 `postgres`，因为它运行在 Docker 中的共享网络内）。填写用户名和密码（在此例中为 `postgres`
    和 `postgres`），并测试连接。如果一切正常，创建连接，然后让我们在这个数据库上执行一些查询。在 *图 6.12* 中，我们可以看到数据已正确导入。
- en: '![Figure 6.12 – DBeaver – titanic table](img/B21927_06_12.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.12 – DBeaver – 泰坦尼克号表格](img/B21927_06_12.jpg)'
- en: Figure 6.12 – DBeaver – titanic table
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – DBeaver – 泰坦尼克号表格
- en: Now, let’s check whether the view was correctly created. The results are shown
    in *Figure 6**.13*.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们检查视图是否正确创建。结果显示在 *图 6.13* 中。
- en: '![Figure 6.13 – DBeaver – created view](img/B21927_06_13.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.13 – DBeaver – 创建的视图](img/B21927_06_13.jpg)'
- en: Figure 6.13 – DBeaver – created view
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 – DBeaver – 创建的视图
- en: 'And that’s it! You created a table and a view and uploaded data to AWS using
    Airflow! To stop Airflow’s containers, go back to your terminal and type the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你创建了一个表格和一个视图，并通过 Airflow 上传了数据到 AWS！要停止 Airflow 容器，请返回终端并输入以下命令：
- en: '[PRE13]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the fundamentals of Apache Airflow – from installation
    to developing data pipelines. You learned how to leverage Airflow to orchestrate
    complex workflows involving data acquisition, processing, and integration with
    external systems.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了 Apache Airflow 的基础知识——从安装到开发数据管道。你学习了如何利用 Airflow 来协调涉及数据获取、处理和与外部系统集成的复杂工作流。
- en: We installed Airflow locally using the Astro CLI and Docker. This provided a
    quick way to get hands-on without a heavy setup. You were exposed to Airflow’s
    architecture and key components, such as the scheduler, worker, and metadata database.
    Understanding these pieces is crucial for monitoring and troubleshooting Airflow
    in production.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 Astro CLI 和 Docker 在本地安装了 Airflow。这提供了一种无需复杂设置即可快速上手的方法。你接触到了 Airflow 的架构和关键组件，如调度器、工作节点和元数据数据库。理解这些组件对于监控和故障排除生产环境中的
    Airflow 至关重要。
- en: Then, there was a major section focused on building your first Airflow DAGs.
    You used core Airflow operators and the task and DAG decorators to define and
    chain tasks. We discussed best practices such as keeping tasks small and autonomous.
    You also learned how Airflow handles task dependencies – allowing parallel execution
    of independent tasks. These learnings will help you develop effective DAGs that
    are scalable and reliable.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有一个重点部分，介绍了如何构建你的第一个 Airflow DAG。你使用了核心 Airflow 操作符和任务、DAG 装饰器来定义和链式任务。我们讨论了最佳实践，比如保持任务小巧且独立。你还了解了
    Airflow 如何处理任务依赖关系——允许并行执行独立任务。这些学习将帮助你开发出可扩展且可靠的 DAG。
- en: Later, we integrated Airflow with external tools – writing to PostgreSQL, creating
    views, and uploading files to S3\. This showcased Airflow’s versatility to orchestrate
    workflows involving diverse systems. We also configured Airflow connections and
    variables to securely pass credentials.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们将 Airflow 与外部工具集成——写入 PostgreSQL，创建视图，并将文件上传到 S3。这展示了 Airflow 在协调涉及不同系统的工作流方面的多功能性。我们还配置了
    Airflow 连接和变量，以安全地传递凭证。
- en: By the end of this chapter, you should have grasped the fundamentals of Airflow
    and have had hands-on experience building data pipelines. You are now equipped
    to develop DAGs, integrate other tools, and apply best practices for production-grade
    workflows. As data teams adopt Airflow, these skills will be invaluable for creating
    reliable and scalable data pipelines.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你应该已经掌握了Airflow的基础知识，并有了构建数据管道的实践经验。你现在已经具备了开发DAG、整合其他工具以及应用生产级工作流最佳实践的能力。随着数据团队开始采用Airflow，这些技能将对构建可靠且可扩展的数据管道至关重要。
- en: In the next chapter, we will study one of the core technologies for real-time
    data – Apache Kafka.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习实时数据的核心技术之一——Apache Kafka。
