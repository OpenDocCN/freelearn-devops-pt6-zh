<html><head></head><body>
  <div id="_idContainer311" class="Basic-Text-Frame">
    <h1 class="chapterNumber">16</h1>
    <h1 id="_idParaDest-740" class="chapterTitle">Governing Kubernetes</h1>
    <p class="normal">In the previous chapter, we discussed at length different ways to extend Kubernetes, including validating and mutating requests during the admission control phase.</p>
    <p class="normal">In this chapter, we will learn about the growing role of Kubernetes in large enterprise organizations, what governance is, and how it is applied in Kubernetes. We will look at policy engines, review some popular ones, and then dive deep into Kyverno.</p>
    <p class="normal">This ties in nicely with the previous chapter because policy engines are built on top of the Kubernetes admission control mechanism.</p>
    <p class="normal">More and more enterprise organizations put more and more of their proverbial eggs in the Kubernetes basket. These large organizations have severe security, compliance, and governance needs. Kubernetes policy engines are here to address these concerns and make sure that enterprise organizations can fully embrace Kubernetes.</p>
    <p class="normal">Here are the topics we will cover:</p>
    <ul>
      <li class="bulletList">Kubernetes in the enterprise</li>
      <li class="bulletList">What is Kubernetes governance?</li>
      <li class="bulletList">Policy engines</li>
      <li class="bulletList">A Kyverno deep dive</li>
    </ul>
    <p class="normal">Let’s jump right in and understand the growing role and importance of Kubernetes in the enterprise.</p>
    <h1 id="_idParaDest-741" class="heading-1">Kubernetes in the enterprise</h1>
    <p class="normal">The<a id="_idIndexMarker1723"/> journey and adoption rate of the Kubernetes platform are unprecedented. It launched officially in 2016, and in a few years it has conquered the world of infrastructure. 96% of organizations that participated in the most recent<a id="_idIndexMarker1724"/> CNCF survey are using or evaluating Kubernetes. The penetration of Kubernetes is across multiple dimensions: organization size, geographical location, and production and no-production environments. What is even more impressive is that Kubernetes can go under the hood and become the foundation that other technologies and platforms are built on. </p>
    <p class="normal">You can see this in its widespread adoption by all the cloud providers that offer various flavors of managed Kubernetes, as well as with the hosted platform-as-a-service offerings from many vendors. Check out the CNCF-certified Kubernetes software conformance list: <a href="https://www.cncf.io/certification/software-conformance"><span class="url">https://www.cncf.io/certification/software-conformance</span></a>.</p>
    <p class="normal">Having a variety of certified vendors and value-added resellers, an ecosystem of multiple companies, etc. is extremely important for enterprise organizations. Enterprise organizations need much more than the latest shiny technology. The stakes are high, the failure rate of large infrastructure projects is high, and the consequences of failure are harsh. Combine all these factors, and the result is that enterprise organizations are very change-resistent and risk-averse when it comes to their technology. A lot of critical software systems in diverse fields like traffic control, insurance, healthcare, communication systems, and airlines are still running on software that was written 40–50 years ago, using languages like COBOL and Fortran.</p>
    <h2 id="_idParaDest-742" class="heading-2">Requirements of enterprise software</h2>
    <p class="normal">Let’s look <a id="_idIndexMarker1725"/>at some requirements of enterprise software:</p>
    <ul>
      <li class="bulletList">Handling large amounts of data</li>
      <li class="bulletList">Integrating with other systems and applications</li>
      <li class="bulletList">Providing robust security features</li>
      <li class="bulletList">Being scalable and available</li>
      <li class="bulletList">Being flexible and customizable</li>
      <li class="bulletList">Being compliant</li>
      <li class="bulletList">Having support from trusted vendors</li>
      <li class="bulletList">Having strong governance (much more on that later)</li>
    </ul>
    <p class="normal">How does Kubernetes fit the bill?</p>
    <h2 id="_idParaDest-743" class="heading-2">Kubernetes and enterprise software</h2>
    <p class="normal">The reason<a id="_idIndexMarker1726"/> Kubernetes usage has grown so much in the enterprise software area is that it actually ticks all the boxes and keeps improving.</p>
    <p class="normal">As the <a id="_idIndexMarker1727"/>de facto standard for container orchestration platforms, it can serve as the foundation for all container-based deployment. Its ecosystem satisfies any integration needs, as every vendor must be able to run on Kubernetes. The long-term prospects for Kubernetes are extremely high, as it is a true team effort from many companies and organizations, and it is steered by an open and successful process that keeps delivering. Kubernetes spearheads the shift toward multi-cloud and hybrid-cloud deployments following industry-wide standards</p>
    <p class="normal">The extensibility and flexibility of Kubernetes mean it can cater to any type of customization a particular enterprise will need.</p>
    <p class="normal">It is truly a remarkable project that is designed on solid conceptual architecture and is able to deliver results consistently in the real world.</p>
    <p class="normal">At this point it’s clear that Kubernetes is great for enterprise organizations, but how does it address the need for governance?</p>
    <h1 id="_idParaDest-744" class="heading-1">What is Kubernetes governance?</h1>
    <p class="normal">Governance<a id="_idIndexMarker1728"/> is one of the important requirements for enterprise organizations. In a nutshell, it means controlling the way an organization operates. Some elements of governance are:</p>
    <ul>
      <li class="bulletList">Policies</li>
      <li class="bulletList">Ethics</li>
      <li class="bulletList">Processes</li>
      <li class="bulletList">Risk management</li>
      <li class="bulletList">Administration</li>
    </ul>
    <p class="normal">Governance includes a way to specify policies and mechanisms to enforce the policies, as well as reporting and auditing. Let’s look at various areas and practices of governance in Kubernetes.</p>
    <h2 id="_idParaDest-745" class="heading-2">Image management</h2>
    <p class="normal">Containers run<a id="_idIndexMarker1729"/> software baked into images. Managing these images is a critical activity in operating a Kubernetes-based system. There are several dimensions to consider: how do you bake your images? How do you vet third-party images? Where do you store your images? Making poor choices here can impact the performance of your system (for example, if you use large bloated base images) and crucially the security of your system (for example, if you use compromised or vulnerable base images). Image management policies can force image scanning or ensure that you can only use vetted images from specific image registries.</p>
    <h2 id="_idParaDest-746" class="heading-2">Pod security</h2>
    <p class="normal">The <a id="_idIndexMarker1730"/>unit of work of Kubernetes is the pod. There are many security settings you can set for a pod and its containers. The default security settings are unfortunately very lax. Validating and enforcing pod security policies can remediate this. Kubernetes has strong support and guidance for pod security standards as well as several built-in profiles. Each pod has a security context as we discussed in <em class="chapterRef">Chapter 4</em>, <em class="italic">Securing Kubernetes</em>.</p>
    <p class="normal">See <a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/"><span class="url">https://kubernetes.io/docs/concepts/security/pod-security-standards/</span></a> for more details.</p>
    <h2 id="_idParaDest-747" class="heading-2">Network policy</h2>
    <p class="normal">Kubernetes <a id="_idIndexMarker1731"/>network policies control traffic flow between pods and other network entities at layers 3 and 4 of the OSI network model (IP addresses and ports). The network entities may be pods that have a specific set of labels or all pods in a namespace with a specific set of labels. Finally, a network policy can also block pod access to/from a specific IP block.</p>
    <p class="normal">In the context of governance, network policies can be used to enforce compliance with security and regulatory requirements by controlling network access and communication between pods and other resources. </p>
    <p class="normal">For example, network policies can be used to prevent pods from communicating with certain external networks. Network policies can also be used to enforce the separation of duties and prevent unauthorized access to sensitive resources within a cluster.</p>
    <p class="normal">See <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/"><span class="url">https://kubernetes.io/docs/concepts/services-networking/network-policies/</span></a> for more details.</p>
    <h2 id="_idParaDest-748" class="heading-2">Configuration constraints</h2>
    <p class="normal">Kubernetes<a id="_idIndexMarker1732"/> is very flexible and provides a lot of controls for many aspects of its operation. The DevOps practices often used in a Kubernetes-based system allow teams a lot of control over how their workloads are deployed, how they scale, and what resources they use. Kubernetes provides configuration constraints like quotas and limits. With more advanced admission controllers you can validate and enforce policies that control any aspect of resource creation, such as the maximum size of an auto-scaling deployment, the total amount of persistent volume claims, and requiring that memory requests always equal memory limits (not necessarily a good idea).</p>
    <h2 id="_idParaDest-749" class="heading-2">RBAC and admission control</h2>
    <p class="normal">Kubernetes <strong class="keyWord">RBAC</strong> (<strong class="keyWord">Role-Based Access Control</strong>) operates <a id="_idIndexMarker1733"/>at the resource and verb level. Every Kubernetes resource has operations (verbs) that can be performed against it. With RBAC you define roles that are sets of permissions over resources, which you can apply at the namespace level or <a id="_idIndexMarker1734"/>cluster level. It is a bit of a coarse-grained tool, but it is very convenient, especially if you segregate your resources at the namespace level and use cluster-level permissions only to manage workloads that operate across the entire cluster.</p>
    <p class="normal">If you need something more granular that depends on specific attributes of resources, then admission controllers can handle it. We will explore this option later in the chapter when discussing policy engines.</p>
    <h2 id="_idParaDest-750" class="heading-2">Policy management</h2>
    <p class="normal">Governance is <a id="_idIndexMarker1735"/>built around policies. Managing all these policies, organizing them, and ensuring they address the governance needs of an organization takes a lot of effort and is an ongoing task. Be prepared to devote resources to evolving and maintaining your policies.</p>
    <h2 id="_idParaDest-751" class="heading-2">Policy validation and enforcement</h2>
    <p class="normal">Once a set <a id="_idIndexMarker1736"/>of policies are in place, you need to validate requests to the Kubernetes API server against those policies and reject requests that violate these policies. There is another approach to enforcing policies that involves mutating incoming requests to comply with a policy. For example, if a policy requires that each pod must have a memory request of at most 2 GiB, then a <a id="_idIndexMarker1737"/>mutating policy can trim down the memory request of pods with larger memory requests to 2 GiB.</p>
    <p class="normal">Polices don’t have to be rigid. Exceptions and exclusions can be made for special cases.</p>
    <h2 id="_idParaDest-752" class="heading-2">Reporting</h2>
    <p class="normal">When you <a id="_idIndexMarker1738"/>manage a large number of policies and vet all requests it’s important to have visibility into how your policies can help you govern your system, prevent issues, and learn from usage patterns. Reports can provide insights by capturing and consolidating the results of policy decisions. As a human user you may view reports about policy violations and rejected and mutated requests, and detect trends or anomalies. At a higher level you can employ automated analysis, including an ML-based model to extract meaning from a large number of detailed reports.</p>
    <h2 id="_idParaDest-753" class="heading-2">Audit</h2>
    <p class="normal">Kubernetes audit logs <a id="_idIndexMarker1739"/>provide a timestamped play-by-play of every event in the system. When you couple audit data with governance reports you can piece together the timeline of incidents, especially security incidents, where the culprit can be identified by combining data from multiple sources, starting with a policy violation and ending with a root cause.</p>
    <p class="normal">So far, we have covered the terrain of what governance is and how it specifically relates to Kubernetes. We emphasized the importance of policies to govern your system. Let’s look at policy engines and how they implement these concepts.</p>
    <h1 id="_idParaDest-754" class="heading-1">Policy engines</h1>
    <p class="normal">Policy engines <a id="_idIndexMarker1740"/>in Kubernetes provide comprehensive coverage of governance needs and complement built-in mechanisms, like network policies and RBAC. Policy engines <a id="_idIndexMarker1741"/>can verify and ensure that your system utilizes best practices, follows security guidelines, and complies with external policies. In this section, we will look at admission control as the primary mechanism where policy engines hook into the system, the responsibilities of a policy engine, and a review of existing policy engines. After this, we will then dive deep into one of the best policy engines out there – Kyverno.</p>
    <h2 id="_idParaDest-755" class="heading-2">Admission control as the foundation of policy engines</h2>
    <p class="normal">Admission control <a id="_idIndexMarker1742"/>is part of the life cycle of requests hitting the Kubernetes API server. We discussed it in depth in <em class="chapterRef">Chapter 15</em>, <em class="italic">Extending Kubernetes</em>. As you recall, dynamic admission controllers are webhook servers that listen for admission review requests and accept, deny, or mutate them. Policy engines are first and foremost sophisticated admission controllers that register to listen for all requests that are relevant to their policies.</p>
    <p class="normal">When a request comes in, the policy engine will apply all relevant policies to decide the fate of the request. For example, if a policy determines that Kubernetes services of the <code class="inlineCode">LoadBalancer</code> type may be created only in a namespace called <code class="inlineCode">load_balancer</code>, then the policy engine will register to listen for all Kubernetes service creation and update requests. When a service creation or update request arrives, the policy engine will check the type of the service and its namespace. If the service type is <code class="inlineCode">LoadBalancer</code> and the namespace is not <code class="inlineCode">load_balancer</code>, then the policy engine will reject the request. Note that this is something that can’t be done using RBAC. This is because RBAC can’t look at the type of service to determine if the request is valid or not.</p>
    <p class="normal">Now that we understand how the policy engine utilizes the dynamic admission control process of Kubernetes, let’s look at the responsibilities of a policy engine.</p>
    <h2 id="_idParaDest-756" class="heading-2">Responsibilities of a policy engine</h2>
    <p class="normal">The<a id="_idIndexMarker1743"/> policy engine is the primary tool for applying governance to a Kubernetes-based system. The policy engine should allow the administrators to define policies that go above and beyond the built-in Kubernetes policies, like RBAC and network policies. That often means coming up with a policy declaration language. The policy declaration language needs to be rich enough to cover all the nuances of Kubernetes, including fine-grained application to different resources and access to all the relevant information to base accept or reject decisions on for each resource.</p>
    <p class="normal">The policy engine should also provide a way to organize, view, and manage policies. Ideally, the policy engine provides a good way to test policies before applying them to a live cluster.</p>
    <p class="normal">The policy engine has to provide a way to deploy policies to the cluster, and of course, it needs to apply the policies that are relevant for each request and decide if the request should be accepted as is, rejected, or modified (mutated). A policy engine may provide a way to<a id="_idIndexMarker1744"/> generate additional resources when a request comes in. For example, when a new Kubernetes deployment is created, a policy engine may automatically generate a Horizontal Pod Autoscaler for the deployment. A policy engine may also listen to events that occur in the cluster and take action. Note that this capability goes beyond dynamic admission control, but it still enforces policies on the cluster.</p>
    <p class="normal">Let’s review some Kubernetes policy engines and how they fulfill these responsibilities.</p>
    <h2 id="_idParaDest-757" class="heading-2">Quick review of open source policy engines</h2>
    <p class="normal">When <a id="_idIndexMarker1745"/>evaluating solutions, it’s very helpful to come up with evaluation criteria up front, since policy engines can deeply impact the operation of your Kubernetes cluster and its workloads’ maturity is a key element. Excellent documentation is crucial too, since the surface area of a policy engine is very large and you need to understand how to work with it. The capabilities of a policy engine determine what use cases it can handle. Writing policies is how administrators convey their governance intentions to the policy engine. It’s important to evaluate the user experience of writing and testing policies and what tooling is available to support these activities. Deploying the policies to the cluster is another must-have element. Finally, viewing reports and understanding the state of governance can be neglected.</p>
    <p class="normal">We will review five policy engines along these dimensions.</p>
    <h3 id="_idParaDest-758" class="heading-3">OPA/Gatekeeper</h3>
    <p class="normal"><strong class="keyWord">Open Policy Agent</strong> (<strong class="keyWord">OPA</strong>) is a<a id="_idIndexMarker1746"/> general-purpose policy <a id="_idIndexMarker1747"/>engine that goes beyond Kubernetes (<a href="https://www.openpolicyagent.org"><span class="url">https://www.openpolicyagent.org</span></a>). Its<a id="_idIndexMarker1748"/> scope is very broad and it operates on any JSON value. </p>
    <p class="normal">Gatekeeper (<a href="https://open-policy-agent.github.io/gatekeeper"><span class="url">https://open-policy-agent.github.io/gatekeeper</span></a>) brings<a id="_idIndexMarker1749"/> the OPA policy <a id="_idIndexMarker1750"/>engine to Kubernetes by <a id="_idIndexMarker1751"/>packaging it as an admission control webhook. </p>
    <p class="normal">OPA/Gatekeeper is definitely the most mature policy engine. It was created in 2017. It is a graduated CNCF project, and it has 2.9k stars on GitHub at the time of writing. It is even used as a foundation for Azure policy on AKS. See <a href="https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes"><span class="url">https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes</span></a>.</p>
    <p class="normal">OPA has<a id="_idIndexMarker1752"/> its <a id="_idIndexMarker1753"/>own special language called <a id="_idIndexMarker1754"/>Rego (<a href="https://www.openpolicyagent.org/docs/latest/policy-language/"><span class="url">https://www.openpolicyagent.org/docs/latest/policy-language/</span></a>) for defining policies. Rego has a strong theoretical basis inspired by Datalog, but it may not be very intuitive and easy to grasp.</p>
    <p class="normal">The following diagram shows <a id="_idIndexMarker1755"/>the architecture<a id="_idIndexMarker1756"/> of OPA/Gatekeeper:</p>
    <figure class="mediaobject"><img src="../Images/B18998_16_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.1: OPA/Gatekeeper architecture</p>
    <p class="normal">Overall, OPA/Gatekeeper <a id="_idIndexMarker1757"/>is very powerful but seems a little clunky compared to other Kubernetes policy engines, as the OPA policy engine is bolted on top of Kubernetes via Gatekeeper.</p>
    <p class="normal">OPA/Gatekeeper has mediocre documentation that is not very easy to navigate. However, it does have a policy library you can use as a starting point.</p>
    <p class="normal">However, if you appreciate the maturity, and you’re not too concerned about using Rego and some friction, it may be a good choice for you.</p>
    <h3 id="_idParaDest-759" class="heading-3">Kyverno</h3>
    <p class="normal">Kyverno (<a href="https://kyverno.io"><span class="url">https://kyverno.io</span></a>) is a<a id="_idIndexMarker1758"/> mature and robust policy <a id="_idIndexMarker1759"/>engine that was designed especially for Kubernetes from the get-go. It was<a id="_idIndexMarker1760"/> created in 2019 and has made huge strides since then. It is a CNCF incubating project and has surpassed OPA/Gatekeeper in popularity on GitHub with 3.3k stars at the time of writing. Kyverno uses <a id="_idIndexMarker1761"/>YAML JMESPath (<a href="https://jmespath.org"><span class="url">https://jmespath.org</span></a>) to <a id="_idIndexMarker1762"/>define policies, which are really just Kubernetes custom resources. It has excellent documentation and a lot of examples to get you started writing your own policy.</p>
    <p class="normal">Overall, Kyverno is both powerful and easy to use. It has huge momentum behind it, and it keeps getting better and improving its performance and operation at scale. It is the best Kubernetes policy engine at the moment in my opinion. We will dive deep into Kyverno later in this chapter.</p>
    <h3 id="_idParaDest-760" class="heading-3">jsPolicy</h3>
    <p class="normal">jsPolicy (<a href="https://www.jspolicy.com"><span class="url">https://www.jspolicy.com</span></a>) is <a id="_idIndexMarker1763"/>an interesting project from Loft that has<a id="_idIndexMarker1764"/> brought virtual clusters to the Kubernetes community. Its claim to fame is that it runs policies in a secure and performant browser-like sandbox, and<a id="_idIndexMarker1765"/> you define the policies in JavaScript or TypeScript. The approach is refreshing, and the project is very slick and streamlined with good documentation. Unfortunately, it seems like Loft is focused on other projects and jsPolicy doesn’t get a lot of attention. It has only 242 GitHub stars (<a href="https://github.com/loft-sh/jspolicy"><span class="url">https://github.com/loft-sh/jspolicy</span></a>) at the time of writing, and the last commit was 6 months ago.</p>
    <p class="normal">The idea of utilizing the JavaScript ecosystem to package and share policies, as well as use its robust tooling to test and debug policies, has a lot of merit.</p>
    <p class="normal">jsPolicy provides validating, mutating, and controller policies. Controller policies allow you to react to events occurring in the cluster outside the scope of admission control.</p>
    <p class="normal">The following diagram shows the architecture of jsPolicy:</p>
    <figure class="mediaobject"><img src="../Images/B18998_16_02.png" alt="js Policy architecture"/></figure>
    <p class="packt_figref">Figure 16.2: jsPolicy architecture</p>
    <p class="normal">At this<a id="_idIndexMarker1766"/> point, I <a id="_idIndexMarker1767"/>wouldn’t commit to jsPolicy since it may have been abandoned. However, if Loft or someone else decides to invest in it, it may be a contender in the field of Kubernetes policy engines.</p>
    <h3 id="_idParaDest-761" class="heading-3">Kubewarden</h3>
    <p class="normal">Kubewarden (<a href="https://www.kubewarden.io"><span class="url">https://www.kubewarden.io</span></a>) is <a id="_idIndexMarker1768"/>another<a id="_idIndexMarker1769"/> innovative policy engine. It is a CNCF sandbox project. Kubewarden<a id="_idIndexMarker1770"/> focuses on being language-agnostic and allows you to write your policies in a variety of languages. The policies are then packaged into WebAssembly modules that are stored in any OCI registry.</p>
    <p class="normal">In theory, you can use any language that can be compiled into WebAssembly. In practice, the following languages are supported, but there are limitations:</p>
    <ul>
      <li class="bulletList">Rust (of course, the most mature)</li>
      <li class="bulletList">Go (you need to use a special compiler, TinyGo, which doesn’t support all of Go’s features)</li>
      <li class="bulletList">Rego (using OPA directly or Gatekeeper – missing mutating policies)</li>
      <li class="bulletList">Swift (using SwiftWasm, which requires some post-build optimizations)</li>
      <li class="bulletList">TypeScript (or rather a subset called AssemblyScript)</li>
    </ul>
    <p class="normal">Kubewarden supports validating, mutating, and context-aware policies. Context-aware policies are policies that use additional information to form an opinion of whether a request should be admitted <a id="_idIndexMarker1771"/>or rejected. The additional information may include, for example, lists of namespaces, services, and ingresses that exist in the cluster.</p>
    <p class="normal">Kubewarden<a id="_idIndexMarker1772"/> has a CLI called kwctl (<a href="https://github.com/kubewarden/kwctl"><span class="url">https://github.com/kubewarden/kwctl</span></a>) for managing your policies.</p>
    <p class="normal">Here is a <a id="_idIndexMarker1773"/>diagram of Kubewarden’s architecture:</p>
    <figure class="mediaobject"><img src="../Images/B18998_16_03.png" alt="KubeWarden architecture"/></figure>
    <p class="packt_figref">Figure 16.3: Kubewarden architecture</p>
    <p class="normal">Kubewarden is still evolving and growing. It has some nice ideas and motivations, but at this stage, it may appeal to you most if you are on the Rust wagon and prefer to write your policies in Rust.</p>
    <p class="normal">Now that we have looked at the landscape of Kubernetes open source policy engines, let’s dive in and take a closer look at Kyverno.</p>
    <h1 id="_idParaDest-762" class="heading-1">Kyverno deep dive</h1>
    <p class="normal">Kyverno<a id="_idIndexMarker1774"/> is a rising star in the Kubernetes policy engine arena. Let’s get hands-on with it, and see how it works and why it is so popular. In this section, we will introduce Kyverno, install it, and learn how to write, apply, and test policies.</p>
    <h2 id="_idParaDest-763" class="heading-2">Quick intro to Kyverno</h2>
    <p class="normal">Kyverno is a policy engine that was designed especially for Kubernetes. If you have some experience working with kubectl, Kubernetes manifests, or YAML, then Kyverno will feel very familiar. You define policies and configuration using YAML manifests and the JMESPath language, which is very close to the JSONPATH format of kubectl. </p>
    <p class="normal">The following diagram shows<a id="_idIndexMarker1775"/> the Kyverno architecture:</p>
    <figure class="mediaobject"><img src="../Images/B18998_16_04.png" alt="Kyverno architecture"/></figure>
    <p class="packt_figref">Figure 16.4: Kyverno architecture</p>
    <p class="normal">Kyverno covers<a id="_idIndexMarker1776"/> a lot of ground and has many features:</p>
    <ul>
      <li class="bulletList">GitOps for policy management</li>
      <li class="bulletList">Resource validation (to reject invalid resources)</li>
      <li class="bulletList">Resource mutation (to modify invalid resources)</li>
      <li class="bulletList">Resource generation (to generate additional resources automatically)</li>
      <li class="bulletList">Verifying container images (important for software supply chain security)</li>
      <li class="bulletList">Inspecting image metadata</li>
      <li class="bulletList">Using label selectors and wildcards to match and exclude resources (Kubernetes-native)</li>
      <li class="bulletList">Using overlays to validate and mutate resources (similar to Kustomize!)</li>
      <li class="bulletList">Synchronizing configurations across namespaces</li>
      <li class="bulletList">Operating in reporting or enforcing mode</li>
      <li class="bulletList">Applying policies using a dynamic admission webhook</li>
      <li class="bulletList">Applying policies at CI/CD time using the Kyverno CLI</li>
      <li class="bulletList">Testing policies and validating resources ad hoc using the Kyverno CLI</li>
      <li class="bulletList">High-availability mode</li>
      <li class="bulletList">Fail open or closed (allowing or rejecting resources when the Kyverno admission webhook is down)</li>
      <li class="bulletList">Policy violation reports</li>
      <li class="bulletList">Web UI for easy visualization</li>
      <li class="bulletList">Observability support</li>
    </ul>
    <p class="normal">This is an <a id="_idIndexMarker1777"/>impressive list of features and capabilities. The Kyverno developers keep evolving and improving it. Kyverno has made big strides in scalability, performance, and the ability to handle a large number of policies and resources.</p>
    <p class="normal">Let’s install Kyverno and configure it.</p>
    <h2 id="_idParaDest-764" class="heading-2">Installing and configuring Kyverno</h2>
    <p class="normal">Kyverno <a id="_idIndexMarker1778"/>follows a similar upgrade policy as Kubernetes itself, where the node components version must be at most two minor versions below the control plane version. At the time of writing, Kyverno 1.8 is the latest version, which supports Kubernetes versions 1.23–1.25.</p>
    <p class="normal">We can install Kyverno using kubectl or Helm. Let’s go with the Helm option:</p>
    <pre class="programlisting gen"><code class="hljs">$ helm repo add kyverno https://kyverno.github.io/kyverno/
"kyverno" has been added to your repositories
$ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kyverno" chart repository
Update Complete. <img src="../Images/B18998_09_001.png" alt=""/>Happy Helming!<img src="../Images/B18998_09_001.png" alt=""/>
</code></pre>
    <p class="normal">Let’s install <a id="_idIndexMarker1779"/>Kyverno, using the default single replica, into its own namespace. Using one replica is NOT recommended for production, but it’s okay for experimenting with Kyverno. To install it in high-availability mode, add the <code class="inlineCode">--set replicaCount=3</code> flag:</p>
    <pre class="programlisting gen"><code class="hljs">$ helm install kyverno kyverno/kyverno -n kyverno --create-namespace
NAME: kyverno
LAST DEPLOYED: Sat Dec 31 15:34:11 2022
NAMESPACE: kyverno
STATUS: deployed
REVISION: 1
NOTES:
Chart version: 2.6.5
Kyverno version: v1.8.5
Thank you for installing kyverno! Your release is named kyverno.
<img src="../Images/B18998_16_001.png" alt=""/> WARNING: Setting replicas count below 3 means Kyverno is not running in high availability mode.
<img src="../Images/B18998_16_002.png" alt=""/> Note: There is a trade-off when deciding which approach to take regarding Namespace exclusions. Please see the documentation at https://kyverno.io/docs/installation/#security-vs-operability to understand the risks.
</code></pre>
    <p class="normal">Let’s observe what we have<a id="_idIndexMarker1780"/> just installed using the ketall kubectl plugin: (<a href="https://github.com/corneliusweig/ketall"><span class="url">https://github.com/corneliusweig/ketall</span></a>):</p>
    <pre class="programlisting gen"><code class="hljs">$ k get-all -n kyverno
NAME                                                          NAMESPACE  AGE
configmap/kube-root-ca.crt                                    kyverno    2m27s
configmap/kyverno                                             kyverno    2m26s
configmap/kyverno-metrics                                     kyverno    2m26s
endpoints/kyverno-svc                                         kyverno    2m26s
endpoints/kyverno-svc-metrics                                 kyverno    2m26s
pod/kyverno-7c444878f7-gfht8                                  kyverno    2m26s
secret/kyverno-svc.kyverno.svc.kyverno-tls-ca                 kyverno    2m22s
secret/kyverno-svc.kyverno.svc.kyverno-tls-pair               kyverno    2m21s
secret/sh.helm.release.v1.kyverno.v1                          kyverno    2m26s
serviceaccount/default                                        kyverno    2m27s
serviceaccount/kyverno                                        kyverno    2m26s
service/kyverno-svc                                           kyverno    2m26s
service/kyverno-svc-metrics                                   kyverno    2m26s
deployment.apps/kyverno                                       kyverno    2m26s
replicaset.apps/kyverno-7c444878f7                            kyverno    2m26s
lease.coordination.k8s.io/kyverno                             kyverno    2m23s
lease.coordination.k8s.io/kyverno-health                      kyverno    2m13s
lease.coordination.k8s.io/kyvernopre                          kyverno    2m25s
lease.coordination.k8s.io/kyvernopre-lock                     kyverno    2m24s
endpointslice.discovery.k8s.io/kyverno-svc-7ghzl              kyverno    2m26s
endpointslice.discovery.k8s.io/kyverno-svc-metrics-qflr5      kyverno    2m26s
rolebinding.rbac.authorization.k8s.io/kyverno:leaderelection  kyverno    2m26s
role.rbac.authorization.k8s.io/kyverno:leaderelection         kyverno    2m26s
</code></pre>
    <p class="normal">As you can see, Kyverno installed <a id="_idIndexMarker1781"/>all the expected resources: deployment, services, roles and role bindings, config maps, and secrets. We can tell that Kyverno exposes metrics and uses leader election too.</p>
    <p class="normal">In addition, Kyverno installed many CRDs (at the cluster scope):</p>
    <pre class="programlisting gen"><code class="hljs">$ k get crd
NAME                                      CREATED AT
admissionreports.kyverno.io               2022-12-31T23:34:12Z
backgroundscanreports.kyverno.io          2022-12-31T23:34:12Z
clusteradmissionreports.kyverno.io        2022-12-31T23:34:12Z
clusterbackgroundscanreports.kyverno.io   2022-12-31T23:34:12Z
clusterpolicies.kyverno.io                2022-12-31T23:34:12Z
clusterpolicyreports.wgpolicyk8s.io       2022-12-31T23:34:12Z
generaterequests.kyverno.io               2022-12-31T23:34:12Z
policies.kyverno.io                       2022-12-31T23:34:12Z
policyreports.wgpolicyk8s.io              2022-12-31T23:34:12Z
updaterequests.kyverno.io                 2022-12-31T23:34:12Z
</code></pre>
    <p class="normal">Finally, Kyverno <a id="_idIndexMarker1782"/>configures several admission control webhooks:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get validatingwebhookconfigurations
NAME                                      WEBHOOKS   AGE
kyverno-policy-validating-webhook-cfg     1          40m
kyverno-resource-validating-webhook-cfg   1          40m
$ k get mutatingwebhookconfigurations
NAME                                    WEBHOOKS   AGE
kyverno-policy-mutating-webhook-cfg     1          40m
kyverno-resource-mutating-webhook-cfg   0          40m
kyverno-verify-mutating-webhook-cfg     1          40m
</code></pre>
    <p class="normal">The following<a id="_idIndexMarker1783"/> diagram shows the result of a typical Kyverno installation: </p>
    <figure class="mediaobject"><img src="../Images/B18998_16_05.png" alt="Kyverno installation"/></figure>
    <p class="packt_figref">Figure 16.5: Typical Kyverno installation</p>
    <h3 id="_idParaDest-765" class="heading-3">Installing pod security policies</h3>
    <p class="normal">Kyverno<a id="_idIndexMarker1784"/> has an extensive library of pre-built policies. We can install the pod security standard policies (see <a href="https://kyverno.io/policies/pod-security/"><span class="url">https://kyverno.io/policies/pod-security/</span></a>) using Helm too:</p>
    <pre class="programlisting gen"><code class="hljs">$ helm install kyverno-policies kyverno/kyverno-policies -n kyverno-policies --create-namespace
NAME: kyverno-policies
LAST DEPLOYED: Sat Dec 31 15:48:26 2022
NAMESPACE: kyverno-policies
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Thank you for installing kyverno-policies 2.6.5 <img src="../Images/B18998_16_003.png" alt=""/>
We have installed the "baseline" profile of Pod Security Standards and set them in audit mode.
Visit https://kyverno.io/policies/ to find more sample policies.
</code></pre>
    <p class="normal">Note that the policies themselves are cluster policies and are not visible in the namespace <code class="inlineCode">kyverno-policies</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get clusterpolicies.kyverno.io
NAME                             BACKGROUND   VALIDATE ACTION   READY
disallow-capabilities            true         audit             true
disallow-host-namespaces         true         audit             true
disallow-host-path               true         audit             true
disallow-host-ports              true         audit             true
disallow-host-process            true         audit             true
disallow-privileged-containers   true         audit             true
disallow-proc-mount              true         audit             true
disallow-selinux                 true         audit             true
restrict-apparmor-profiles       true         audit             true
restrict-seccomp                 true         audit             true
restrict-sysctls                 true         audit             true
</code></pre>
    <p class="normal">We will review some of these policies in depth later. First, let’s see how to configure Kyverno.</p>
    <h3 id="_idParaDest-766" class="heading-3">Configuring Kyverno</h3>
    <p class="normal">You<a id="_idIndexMarker1785"/> can configure the behavior of Kyverno by editing the Kyverno config map:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get cm kyverno -o yaml -n kyverno | yq .data
resourceFilters: '[*,kyverno,*][Event,*,*][*,kube-system,*][*,kube-public,*][*,kube-node-lease,*][Node,*,*][APIService,*,*][TokenReview,*,*][SubjectAccessReview,*,*][SelfSubjectAccessReview,*,*][Binding,*,*][ReplicaSet,*,*][AdmissionReport,*,*][ClusterAdmissionReport,*,*][BackgroundScanReport,*,*][ClusterBackgroundScanReport,*,*][ClusterRole,*,kyverno:*][ClusterRoleBinding,*,kyverno:*][ServiceAccount,kyverno,kyverno][ConfigMap,kyverno,kyverno][ConfigMap,kyverno,kyverno-metrics][Deployment,kyverno,kyverno][Job,kyverno,kyverno-hook-pre-delete][NetworkPolicy,kyverno,kyverno][PodDisruptionBudget,kyverno,kyverno][Role,kyverno,kyverno:*][RoleBinding,kyverno,kyverno:*][Secret,kyverno,kyverno-svc.kyverno.svc.*][Service,kyverno,kyverno-svc][Service,kyverno,kyverno-svc-metrics][ServiceMonitor,kyverno,kyverno-svc-service-monitor][Pod,kyverno,kyverno-test]'
webhooks: '[{"namespaceSelector": {"matchExpressions": 
[{"key":"kubernetes.io/metadata.name","operator":"NotIn","values":["kyverno"]}]}}]'
</code></pre>
    <p class="normal">The <code class="inlineCode">resourceFilters</code> flag is a list in the format <code class="inlineCode">[kind,namespace,name]</code>, where each element may be a wildcard too, that tells Kyverno which resources to ignore. Resources that match any of the filters will not be subject to any Kyverno policy. This is good practice if you have a lot of policies to save the evaluation effort against all policies.</p>
    <p class="normal">The <code class="inlineCode">webHooks</code> flag allows you to filter out whole namespaces.</p>
    <p class="normal">The <code class="inlineCode">excludeGroupRole</code> flag is a string of comma-separated roles. It will exclude requests, where a user has one of the specified roles from Kyverno admission control. The default list is <code class="inlineCode">system:serviceaccounts:kube-system,system:nodes,system:kube-scheduler</code>.</p>
    <p class="normal">The <code class="inlineCode">excludeUsername</code> flag represents a string consisting of Kubernetes usernames separated by commas. When a user enables <code class="inlineCode">Synchronize</code> in <code class="inlineCode">generate policy</code>, Kyverno becomes the only entity capable of updating or deleting generated resources. However, administrators have the ability to exclude specific usernames from accessing the delete/update-generated resource functionality.</p>
    <p class="normal">The <code class="inlineCode">generateSuccessEvents</code> flag is a Boolean parameter used to determine whether success events should be generated. By default, this flag is set to <code class="inlineCode">false</code>, indicating that success events are not generated.</p>
    <p class="normal">Furthermore, the Kyverno container provides several container arguments that can be configured to customize its behavior and functionality. These arguments allow for fine-tuning and customization of Kyverno’s behavior within the container. You can edit the list of args in the Kyverno deployment:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get deploy kyverno -n kyverno -o yaml | yq '.spec.template.spec.containers[0].args'
- --autogenInternals=true
- --loggingFormat=text
</code></pre>
    <p class="normal">In addition<a id="_idIndexMarker1786"/> to the pre-configured <code class="inlineCode">--autogenInternals</code> and <code class="inlineCode">--loggingFormat</code>, the following flags are available:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">admissionReports</code></li>
      <li class="bulletList"><code class="inlineCode">allowInsecureRegistry</code></li>
      <li class="bulletList"><code class="inlineCode">autoUpdateWebhooks</code></li>
      <li class="bulletList"><code class="inlineCode">backgroundScan</code></li>
      <li class="bulletList"><code class="inlineCode">clientRateLimitBurst</code></li>
      <li class="bulletList"><code class="inlineCode">clientRateLimitQPS</code></li>
      <li class="bulletList"><code class="inlineCode">disableMetrics</code></li>
      <li class="bulletList"><code class="inlineCode">enableTracing</code></li>
      <li class="bulletList"><code class="inlineCode">genWorkers</code></li>
      <li class="bulletList"><code class="inlineCode">imagePullSecrets</code></li>
      <li class="bulletList"><code class="inlineCode">imageSignatureRepository</code></li>
      <li class="bulletList"><code class="inlineCode">kubeconfig</code></li>
      <li class="bulletList"><code class="inlineCode">maxQueuedEvents</code></li>
      <li class="bulletList"><code class="inlineCode">metricsPort</code></li>
      <li class="bulletList"><code class="inlineCode">otelCollector</code></li>
      <li class="bulletList"><code class="inlineCode">otelConfig</code></li>
      <li class="bulletList"><code class="inlineCode">profile</code></li>
      <li class="bulletList"><code class="inlineCode">profilePort</code></li>
      <li class="bulletList"><code class="inlineCode">protectManagedResources</code></li>
      <li class="bulletList"><code class="inlineCode">reportsChunkSize</code></li>
      <li class="bulletList"><code class="inlineCode">serverIP</code></li>
      <li class="bulletList"><code class="inlineCode">splitPolicyReport</code> (deprecated – will be removed in 1.9)</li>
      <li class="bulletList"><code class="inlineCode">transportCreds</code></li>
      <li class="bulletList"><code class="inlineCode">webhookRegistrationTimeout</code></li>
      <li class="bulletList"><code class="inlineCode">webhookTimeout</code></li>
    </ul>
    <p class="normal">All the flags have a default value, and you only need to specify them if you want to override the defaults.</p>
    <p class="normal">Check out <a href="https://kyverno.io/docs/installation/#container-flags"><span class="url">https://kyverno.io/docs/installation/#container-flags</span></a> for details on each flag.</p>
    <p class="normal">We installed Kyverno, observed the various resources it installed, and looked at its configuration. It’s time to check out the policies and rules of Kyverno.</p>
    <h2 id="_idParaDest-767" class="heading-2">Applying Kyverno policies</h2>
    <p class="normal">At the <a id="_idIndexMarker1787"/>user level, the unit of work in Kyverno is policies. You can apply policies as Kubernetes resources, write and edit your own policies, and test policies using the Kyverno CLI. </p>
    <p class="normal">Applying a Kyverno policy is as simple as applying any other resource. Let’s take a look at one of the policies we installed earlier:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get clusterpolicies.kyverno.io disallow-capabilities
NAME                    BACKGROUND   VALIDATE ACTION   READY
disallow-capabilities   true         audit             true
</code></pre>
    <p class="normal">The purpose of the policy is to prevent pods from requesting extra Linux capabilities beyond the allowed list (see <a href="https://linux-audit.com/linux-capabilities-101/"><span class="url">https://linux-audit.com/linux-capabilities-101/</span></a>). One of the capabilities that is not allowed is <code class="inlineCode">NET_ADMIN</code>. Let’s create a pod that requests this capability:</p>
    <pre class="programlisting gen"><code class="hljs">$ cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: some-pod
spec:
  containers:
  - name: some-container
    command: [ "sleep", "999999" ]
    image: g1g1/py-kube:0.3
    securityContext:
      capabilities:
        add: ["NET_ADMIN"]
EOF    
pod/some-pod created
</code></pre>
    <p class="normal">The pod was created, and we can verify that it has the <code class="inlineCode">NET_ADMIN</code> capability. I use a kind cluster, so the cluster node is just a Docker process we can exec into:</p>
    <pre class="programlisting gen"><code class="hljs">$ docker exec -it kind-control-plane sh
#
</code></pre>
    <p class="normal">Now that we’re in a shell inside the node, we can search for the process of our container, which just sleeps for 999,999 seconds:</p>
    <pre class="programlisting gen"><code class="hljs"># ps aux | grep 'PID\|sleep' | grep -v grep
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root        4549  0.0  0.0 148276  6408 ?        Ssl  02:54   0:00 /usr/bin/qemu-x86_64 /bin/sleep 999999
</code></pre>
    <p class="normal">Let’s check<a id="_idIndexMarker1788"/> the capabilities of our process, 4549:</p>
    <pre class="programlisting gen"><code class="hljs"># getpcaps 4549
4549: cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_admin,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap=ep
</code></pre>
    <p class="normal">As you can see the <code class="inlineCode">cap_net_admin</code> is present.</p>
    <p class="normal">Kyverno didn’t prevent the pod from being created because the policy operates in audit mode only:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get clusterpolicies.kyverno.io disallow-capabilities -o yaml | yq .spec.validationFailureAction
audit
</code></pre>
    <p class="normal">Let’s delete the pod and change the policy to “enforce” mode:</p>
    <pre class="programlisting gen"><code class="hljs">$ k delete po some-pod
pod "some-pod" deleted
$ k patch clusterpolicies.kyverno.io disallow-capabilities --type merge -p '{"spec": {"validationFailureAction": "enforce"}}'
clusterpolicy.kyverno.io/disallow-capabilities patched
</code></pre>
    <p class="normal">Now, if we try to create the pod again, the result is very different:</p>
    <pre class="programlisting gen"><code class="hljs">$ cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: some-pod
spec:
  containers:
  - name: some-container
    command: [ "sleep", "999999" ]
    image: g1g1/py-kube:0.3
    securityContext:
      capabilities:
        add: ["NET_ADMIN"]
EOF
Error from server: error when creating "STDIN": admission webhook "validate.kyverno.svc-fail" denied the request:
policy Pod/kyverno-policies/some-pod for resource violation:
disallow-capabilities:
  adding-capabilities: Any capabilities added beyond the allowed list (AUDIT_WRITE,
    CHOWN, DAC_OVERRIDE, FOWNER, FSETID, KILL, MKNOD, NET_BIND_SERVICE, SETFCAP, SETGID,
    SETPCAP, SETUID, SYS_CHROOT) are disallowed.
</code></pre>
    <p class="normal">The<a id="_idIndexMarker1789"/> Kyverno admission webhook enforced the policy and rejected the pod creation. It even tells us which policy was responsible (<code class="inlineCode">disallow-capabilities</code>) and displays a nice message that explains the reason for the rejection, including a list of the allowed capabilities.</p>
    <p class="normal">It is pretty simple to apply policies. Writing policies is much more involved and requires an understanding of resource requests, Kyverno matching rules, and the JMESPath language. Before we can write policies, we need to understand how they are structured and what their different elements are.</p>
    <h2 id="_idParaDest-768" class="heading-2">Kyverno policies in depth</h2>
    <p class="normal">In this section, we <a id="_idIndexMarker1790"/>will learn all the fine details about Kyverno policies. A Kyverno policy has a set of rules that define what the policy actually does and several general settings that define how the policy behaves in different scenarios. Let’s start with the policy settings and then move on to rules and different use cases, such as validating, mutating, and generating resources.</p>
    <h3 id="_idParaDest-769" class="heading-3">Understanding policy settings</h3>
    <p class="normal">A <a id="_idIndexMarker1791"/>Kyverno policy may have the following settings:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">applyRules</code></li>
      <li class="bulletList"><code class="inlineCode">validationFailureAction</code></li>
      <li class="bulletList"><code class="inlineCode">validationFailureActionOverrides</code></li>
      <li class="bulletList"><code class="inlineCode">background</code></li>
      <li class="bulletList"><code class="inlineCode">schemaValidation</code></li>
      <li class="bulletList"><code class="inlineCode">failurePolicy</code></li>
      <li class="bulletList"><code class="inlineCode">webhookTimeoutSeconds</code></li>
    </ul>
    <p class="normal">The <code class="inlineCode">applyRules</code> setting<a id="_idIndexMarker1792"/> determines if only one or multiple rules apply to matching resources. The valid values are “One” and “All” (the default). If <code class="inlineCode">applyRules</code> is set to “One” then the first matching rule will be evaluated and other rules will be ignored.</p>
    <p class="normal">The <code class="inlineCode">validationFailureAction</code> setting determines if a failed validation policy rule should reject the admission request or just report it. The valid values are “audit” (default – always allows and just reports violations) and “enforce” (blocks invalid requests).</p>
    <p class="normal">The <code class="inlineCode">validationFailureActionOverrides</code> setting is a <code class="inlineCode">ClusterPolicy</code> attribute that overrides the <code class="inlineCode">validationFailureAction</code> for specific namespaces.</p>
    <p class="normal">The <code class="inlineCode">background</code> setting determines if policies are applied to existing resources during a background scan. The default is “true”.</p>
    <p class="normal">The <code class="inlineCode">schemaValidation</code> setting determines if policy validation checks are applied. The default is “true”.</p>
    <p class="normal">The <code class="inlineCode">failurePolicy</code> setting determines how the API server behaves if the webhook fails to respond. The valid values are “Ignore” and “Fail” (the default). If the setting is “Fail” then even valid resource requests will be denied, while the webhook is unreachable.</p>
    <p class="normal">The <code class="inlineCode">webhookTimeoutSeconds</code> determines the maximum time in seconds that the webhook is allowed to evaluate a policy. The valid values are between 1 and 30 seconds. The default is 10 seconds. If the webhook failed to respond in time, the <code class="inlineCode">failurePolicy</code> (see above) determines the fate of the request.</p>
    <h3 id="_idParaDest-770" class="heading-3">Understanding Kyverno policy rules</h3>
    <p class="normal">Each <a id="_idIndexMarker1793"/>Kyverno policy has one or more rules. Each rule has a <code class="inlineCode">match</code> declaration, an optional <code class="inlineCode">exclude</code> declaration, an optional <code class="inlineCode">preconditions</code> declaration, and exactly one of the following declarations: </p>
    <ul>
      <li class="bulletList"><code class="inlineCode">validate</code></li>
      <li class="bulletList"><code class="inlineCode">mutate</code> </li>
      <li class="bulletList"><code class="inlineCode">generate</code> </li>
      <li class="bulletList"><code class="inlineCode">verifyImages</code></li>
    </ul>
    <p class="normal">The diagram below demonstrates the <a id="_idIndexMarker1794"/>structure of a Kyverno policy and its rules (policy settings are omitted):</p>
    <figure class="mediaobject"><img src="../Images/B18998_16_06.png" alt="Kyverno rule structure"/></figure>
    <p class="packt_figref">Figure 16.6: Kyverno rule structure</p>
    <p class="normal">Let’s go over the different declarations and explore some advanced topics too.</p>
    <h4 class="heading-4">Matching requests</h4>
    <p class="normal">When a <a id="_idIndexMarker1795"/>resource request arrives, the Kyverno webhook needs to determine for each policy if the requested resource and/or operation is relevant for the current policy. The mandatory <code class="inlineCode">match</code> declaration has several filters that determine if the policy should evaluate the current request. The filters are: </p>
    <ul>
      <li class="bulletList"><code class="inlineCode">resources</code></li>
      <li class="bulletList"><code class="inlineCode">subjects</code></li>
      <li class="bulletList"><code class="inlineCode">roles</code></li>
      <li class="bulletList"><code class="inlineCode">clusterRoles</code></li>
    </ul>
    <p class="normal">A <code class="inlineCode">match</code> declaration may have multiple filters grouped under an <code class="inlineCode">any</code> statement or an <code class="inlineCode">all</code> statement. When filters are grouped under <code class="inlineCode">any</code>, then Kyverno will apply OR semantics to match them, and if any of the filters match the request, the request is considered matched. When filters are grouped under <code class="inlineCode">all</code>, then Kyverno will apply AND semantics, and all the filters must match in order for the request to be considered a match.</p>
    <p class="normal">This can be a <a id="_idIndexMarker1796"/>little overwhelming. Let’s look at an example. The following policy spec has a single rule called <code class="inlineCode">some-rule</code>. The rule has a <code class="inlineCode">match</code> declaration with two resource filters under an <code class="inlineCode">any</code> statement. The first resource filter matches resources of kind <code class="inlineCode">Service</code> with names <code class="inlineCode">service-1</code> or <code class="inlineCode">service-2</code>. The second resource filter matches resources of kind <code class="inlineCode">Service</code> in the namespace <code class="inlineCode">ns-1</code>. This rule will match any Kubernetes service named <code class="inlineCode">service-1</code> or <code class="inlineCode">service-2</code> in any namespace, as well as any service in the namespace <code class="inlineCode">ns-1</code>.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">spec:</span>
  <span class="hljs-attr">rules:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">some-rule</span>
    <span class="hljs-attr">match:</span>
      <span class="hljs-attr">any:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">resources:</span>
          <span class="hljs-attr">kinds:</span> 
          <span class="hljs-bullet">-</span> <span class="hljs-string">Service</span>
          <span class="hljs-attr">names:</span> 
          <span class="hljs-bullet">-</span> <span class="hljs-string">"service-1"</span> 
          <span class="hljs-bullet">-</span> <span class="hljs-string">"service-2"</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">resources:</span>
          <span class="hljs-attr">kinds:</span> 
          <span class="hljs-bullet">-</span> <span class="hljs-string">Service</span>
          <span class="hljs-attr">namespaces:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-string">"ns-1"</span>
</code></pre>
    <p class="normal">Let’s look at another example. This time we add a cluster role filter. The following rule will match requests where the kind is a service named <code class="inlineCode">service-1</code> and the requesting user has a cluster role called <code class="inlineCode">some-cluster-role</code>.</p>
    <pre class="programlisting code"><code class="hljs-code">  <span class="hljs-attr">rules:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">some-rule</span>
      <span class="hljs-attr">match:</span>
        <span class="hljs-attr">all:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">resources:</span>
              <span class="hljs-attr">kinds:</span>
                <span class="hljs-bullet">-</span> <span class="hljs-string">Service</span>
              <span class="hljs-attr">names:</span>
                <span class="hljs-bullet">-</span> <span class="hljs-string">"service-1"</span>
            <span class="hljs-attr">clusterRoles:</span>
              <span class="hljs-bullet">-</span> <span class="hljs-string">some-cluster-role</span>
</code></pre>
    <p class="normal">The admission<a id="_idIndexMarker1797"/> review resource contains all the roles and cluster roles bound to the requesting user or service account.</p>
    <h4 class="heading-4">Excluding resources</h4>
    <p class="normal">Excluding resources<a id="_idIndexMarker1798"/> is very similar to matching. It is very common to set policies that disallow all requests to create or update some resources unless they are made in certain namespaces or by users with certain roles. Here is an example that matches all services but excludes the <code class="inlineCode">ns-1</code> namespace:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">rules:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">some-rule</span>
  <span class="hljs-attr">match:</span>
    <span class="hljs-attr">any:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">resources:</span>
          <span class="hljs-attr">kinds:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">Service</span>
  <span class="hljs-attr">exclude:</span>
    <span class="hljs-attr">any:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">resources:</span>
          <span class="hljs-attr">namespaces:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">"ns-1"</span>
</code></pre>
    <p class="normal">Another common exclusion is for specific roles like <code class="inlineCode">cluster-admin</code>.</p>
    <h4 class="heading-4">Using preconditions</h4>
    <p class="normal">Limiting<a id="_idIndexMarker1799"/> the scope of a policy using <code class="inlineCode">match</code> and <code class="inlineCode">exclude</code> is great, but in many cases it is not sufficient. Sometimes, you need to select resources based on fine-grained details such as memory requests. Here is an example that matches all pods that request memory of less than 1 GiB.</p>
    <p class="normal">The syntax for the key value uses<a id="_idIndexMarker1800"/> JMESPath (<a href="https://jmespath.org"><span class="url">https://jmespath.org</span></a>) on the built-in request object:</p>
    <pre class="programlisting code"><code class="hljs-code">  <span class="hljs-attr">rules:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">memory-limit</span>
    <span class="hljs-attr">match:</span>
      <span class="hljs-attr">any:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">resources:</span>
          <span class="hljs-attr">kinds:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-string">Pod</span>
    <span class="hljs-attr">preconditions:</span>
      <span class="hljs-attr">any:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">"</span><span class="hljs-template-variable">{{request.object.spec.containers[*].resources.requests.memory}}</span><span class="hljs-string">"</span>
        <span class="hljs-attr">operator:</span> <span class="hljs-string">LessThan</span>
        <span class="hljs-attr">value:</span> <span class="hljs-string">1Gi</span>
</code></pre>
    <h3 id="_idParaDest-771" class="heading-3">Validating requests</h3>
    <p class="normal">The <a id="_idIndexMarker1801"/>primary use case of Kyverno is validating<a id="_idIndexMarker1802"/> requests. Validating rules have a <code class="inlineCode">validate</code> statement. The <code class="inlineCode">validate</code> statement has a <code class="inlineCode">message</code> field that will be displayed if the request fails to validate. A validating rule has two forms, pattern-based validation and deny-based validation. Let’s examine each of them. As you may recall, the result of a resource failing to validate depends on the <code class="inlineCode">validationFailureAction</code> field, which can be <code class="inlineCode">audit</code> or <code class="inlineCode">enforce</code>.</p>
    <h4 class="heading-4">Pattern-based validation</h4>
    <p class="normal">A rule with<a id="_idIndexMarker1803"/> pattern-based validation has a <code class="inlineCode">pattern</code> field under the <code class="inlineCode">validate</code> statement. If the resource doesn’t match the pattern, the rule failed. Here is an example of pattern-based validation, where the resource must have a label called <code class="inlineCode">app</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">   <span class="hljs-attr">validate:</span>
      <span class="hljs-attr">message:</span> <span class="hljs-string">"The resource must have a label named `app`."</span>
      <span class="hljs-attr">pattern:</span>
        <span class="hljs-attr">metadata:</span>
          <span class="hljs-attr">labels:</span>
            <span class="hljs-attr">some-label:</span> <span class="hljs-string">"app"</span>
</code></pre>
    <p class="normal">The validation part will only be applied to requests that comply with the <code class="inlineCode">match</code> and <code class="inlineCode">preconditions</code> statements and are not excluded by the <code class="inlineCode">exclude</code> statement, if there are any.</p>
    <p class="normal">You can also apply operators to values in the pattern – for example, here is a validation rule that requires that the number of replicas of a deployment will be at least 3:</p>
    <pre class="programlisting code"><code class="hljs-code">  <span class="hljs-attr">rules:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">validate-replica-count</span>
      <span class="hljs-attr">match:</span>
        <span class="hljs-attr">any:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">resources:</span>
            <span class="hljs-attr">kinds:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">Deployment</span>
      <span class="hljs-attr">validate:</span>
        <span class="hljs-attr">message:</span> <span class="hljs-string">"Replica count for a Deployment must be at least 3."</span>
        <span class="hljs-attr">pattern:</span>
          <span class="hljs-attr">spec:</span>
            <span class="hljs-attr">replicas:</span> <span class="hljs-string">"&gt;=3"</span>
</code></pre>
    <h4 class="heading-4">Deny-based validation</h4>
    <p class="normal">A rule with <a id="_idIndexMarker1804"/>deny-based validation has a <code class="inlineCode">deny</code> field under the <code class="inlineCode">validate</code> statement. Deny rules are similar to the preconditions that we saw earlier for selecting resources. Each deny condition has a key, an operator, and a value. A common use for the deny condition is disallowing a specific operation such as <code class="inlineCode">DELETE</code>. The following examples use deny-based validation to prevent the deletion of Deployments and StatefulSets. Note the use of request variables both for the message and the key. For <code class="inlineCode">DELETE</code> operations the deleted object is defined as <code class="inlineCode">request.oldObject</code> and not <code class="inlineCode">request.object</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">rules:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">block-deletes-of-deployments-and-statefulsets</span>
    <span class="hljs-attr">match:</span>
      <span class="hljs-attr">any:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">resources:</span>
          <span class="hljs-attr">kinds:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">Deployment</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">Statefulset</span>
    <span class="hljs-attr">validate:</span>
      <span class="hljs-attr">message:</span> <span class="hljs-string">"Deleting </span><span class="hljs-template-variable">{{request.oldObject.kind}}</span><span class="hljs-string">/</span><span class="hljs-template-variable">{{request.oldObject.metadata.name}}</span><span class="hljs-string"> is not allowed"</span>
      <span class="hljs-attr">deny:</span>
        <span class="hljs-attr">conditions:</span>
          <span class="hljs-attr">any:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">"</span><span class="hljs-template-variable">{{request.operation}}</span><span class="hljs-string">"</span>
            <span class="hljs-attr">operator:</span> <span class="hljs-string">Equals</span>
            <span class="hljs-attr">value:</span> <span class="hljs-string">DELETE</span>
</code></pre>
    <p class="normal">There is more to validation, which you can explore here: <a href="https://kyverno.io/docs/writing-policies/validate/"><span class="url">https://kyverno.io/docs/writing-policies/validate/</span></a></p>
    <p class="normal">Let’s turn our attention to mutations.</p>
    <h3 id="_idParaDest-772" class="heading-3">Mutating resources</h3>
    <p class="normal">Mutation <a id="_idIndexMarker1805"/>may sound scary, but all it is is modifying the resource in a request in some way. Note that the mutated request will still go through validation even if it matches any policy. It is not possible to change the kind of the requested object, but you can change its properties. The benefit of mutation is that you can automatically fix invalid requests, which is typically a better user experience instead of blocking invalid requests. The downside (especially if the invalid resources were created as part of a CI/CD pipeline) is that it creates a dissonance between the source code and the actual resources in the cluster. However, it is great for use cases where you want to control some aspects that users don’t need to be aware of, as well as during migration.</p>
    <p class="normal">Enough theory – let’s see what mutation looks like in Kyverno. You still need to select the resources to mutate, which means that the <code class="inlineCode">match</code>, <code class="inlineCode">exclude</code>, and <code class="inlineCode">precondition</code> statements are still needed for mutation policies. </p>
    <p class="normal">However, instead of a <code class="inlineCode">validate</code> statement you will have a <code class="inlineCode">mutate</code> statement. Here is an example that uses the <code class="inlineCode">patchStrategicMerge</code> flavor to set the <code class="inlineCode">imagePullPolicy</code> of containers that use an image with the <code class="inlineCode">latest</code> tag. The syntax is similar to Kustomize overlays and merges with the existing resource. The reason the <code class="inlineCode">image</code> field is in parentheses is because of a JMESPath feature called anchors (<a href="https://kyverno.io/docs/writing-policies/validate/#anchors"><span class="url">https://kyverno.io/docs/writing-policies/validate/#anchors</span></a>), where the rest of the subtree is applied only if the given field matches it. In this case it means the <code class="inlineCode">imagePullPolicy</code> will only be set for images that satisfy the condition:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">mutate:</span>
  <span class="hljs-attr">patchStrategicMerge:</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
        <span class="hljs-comment"># match images which end with :latest</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">(image):</span> <span class="hljs-string">"*:latest"</span>
          <span class="hljs-comment"># set the imagePullPolicy to "IfNotPresent"</span>
          <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">"IfNotPresent"```</span>
</code></pre>
    <p class="normal">The other flavor of mutation is JSON Patch (<a href="http://jsonpatch.com"><span class="url">http://jsonpatch.com</span></a>), which is specified in RFC 6902 (<a href="https://datatracker.ietf.org/doc/html/rfc6902"><span class="url">https://datatracker.ietf.org/doc/html/rfc6902</span></a>). JSON Patch has similar semantics to preconditions and deny rules. The patch has an operation, path, and value. It applies the operation to the patch with the value. The operation can be one of:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">add</code></li>
      <li class="bulletList"><code class="inlineCode">remove</code></li>
      <li class="bulletList"><code class="inlineCode">replace</code></li>
      <li class="bulletList"><code class="inlineCode">copy</code></li>
      <li class="bulletList"><code class="inlineCode">move</code></li>
      <li class="bulletList"><code class="inlineCode">test</code></li>
    </ul>
    <p class="normal">Here is an <a id="_idIndexMarker1806"/>example of adding some data to a config map using JSON Patch. It adds multiple fields to the <code class="inlineCode">/data/properties</code> path and a single value to the <code class="inlineCode">/data/key</code> path:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">spec:</span>
  <span class="hljs-attr">rules:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">patch-config-map</span>
      <span class="hljs-attr">match:</span>
        <span class="hljs-attr">any:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">resources:</span>
            <span class="hljs-attr">names:</span>
              <span class="hljs-bullet">-</span> <span class="hljs-string">the-config-map</span>
            <span class="hljs-attr">kinds:</span>
              <span class="hljs-bullet">-</span> <span class="hljs-string">ConfigMap</span>
      <span class="hljs-attr">mutate:</span>
        <span class="hljs-attr">patchesJson6902:</span> <span class="hljs-string">|-</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">path:</span> <span class="hljs-string">"/data/properties"</span>
            <span class="hljs-attr">op:</span> <span class="hljs-string">add</span>
            <span class="hljs-attr">value:</span> <span class="hljs-string">|</span>
              <span class="hljs-string">prop-1=value-1</span>
              <span class="hljs-string">prop-2=value-2</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">path:</span> <span class="hljs-string">"/data/key"</span>
            <span class="hljs-attr">op:</span> <span class="hljs-string">add</span>
            <span class="hljs-attr">value:</span> <span class="hljs-string">some-string</span>
</code></pre>
    <h3 id="_idParaDest-773" class="heading-3">Generating resources</h3>
    <p class="normal">Generating <a id="_idIndexMarker1807"/>resources is an interesting use case. Whenever a request comes in, Kyverno may create new resources instead of mutating or validating the request (other policies may validate or mutate the original request).</p>
    <p class="normal">A policy with a <code class="inlineCode">generate</code> rule has the same <code class="inlineCode">match</code> and/or <code class="inlineCode">exclude</code> statements as other policies. This means it can be triggered by any resource request as well as existing resources. However, instead of validating or mutating, it generates a new resource when the origin resource is created. A <code class="inlineCode">generate</code> rule has an important property called <code class="inlineCode">synchronize</code>. When <code class="inlineCode">synchronize</code> is true, the generated resource is always in sync with the origin resource (when the origin resource is deleted, the generated resource is deleted as well). Users can’t modify or delete a generated resource. When <code class="inlineCode">synchronize</code> is false, Kyverno doesn’t keep track of the generated resource, and users can modify or delete it at will.</p>
    <p class="normal">Here<a id="_idIndexMarker1808"/> is a <code class="inlineCode">generate</code> rule that creates a <code class="inlineCode">NetworkPolicy</code> that prevents any traffic when a new <code class="inlineCode">Namespace</code> is created. Note the <code class="inlineCode">data</code> field, which defines the generated resource:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">spec:</span>
  <span class="hljs-attr">rules:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">deny-all-traffic</span>
    <span class="hljs-attr">match:</span>
      <span class="hljs-attr">any:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">resources:</span>
          <span class="hljs-attr">kinds:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-string">Namespace</span>
    <span class="hljs-attr">generate:</span>
      <span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span>
      <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">deny-all-traffic</span>
      <span class="hljs-attr">namespace:</span> <span class="hljs-string">"</span><span class="hljs-template-variable">{{request.object.metadata.name}}</span><span class="hljs-string">"</span>
      <span class="hljs-attr">data:</span>  
        <span class="hljs-attr">spec:</span>
          <span class="hljs-comment"># select all pods in the namespace</span>
          <span class="hljs-attr">podSelector:</span> {}
          <span class="hljs-attr">policyTypes:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-string">Ingress</span>
          <span class="hljs-bullet">-</span> <span class="hljs-string">Egress</span>
</code></pre>
    <p class="normal">When generating resources for an existing origin resource instead of a <code class="inlineCode">data</code> field, a <code class="inlineCode">clone</code> field is used. For example, if we have a config map called <code class="inlineCode">config-template</code> in the <code class="inlineCode">default</code> namespace, the following <code class="inlineCode">generate</code> rule will clone that config map into every new namespace:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">spec:</span>
  <span class="hljs-attr">rules:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">clone-config-map</span>
    <span class="hljs-attr">match:</span>
      <span class="hljs-attr">any:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">resources:</span>
          <span class="hljs-attr">kinds:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-string">Namespace</span>
    <span class="hljs-attr">generate:</span>
      <span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span>
      <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
      <span class="hljs-comment"># Name of the generated resource</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">default-config</span>
      <span class="hljs-attr">namespace:</span> <span class="hljs-string">"</span><span class="hljs-template-variable">{{request.object.metadata.name}}</span><span class="hljs-string">"</span>
      <span class="hljs-attr">synchronize:</span> <span class="hljs-literal">true</span>
      <span class="hljs-attr">clone:</span>
        <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>
        <span class="hljs-attr">name:</span> <span class="hljs-string">config-template</span>
</code></pre>
    <p class="normal">It’s also possible<a id="_idIndexMarker1809"/> to clone multiple resources by using a <code class="inlineCode">cloneList</code> field instead of a <code class="inlineCode">clone</code> field.</p>
    <h3 id="_idParaDest-774" class="heading-3">Advanced policy rules</h3>
    <p class="normal">Kyverno <a id="_idIndexMarker1810"/>has some additional advanced capabilities, such as external data sources and autogen rules for pod controllers.</p>
    <h4 class="heading-4">External data sources</h4>
    <p class="normal">So far we’ve<a id="_idIndexMarker1811"/> seen how Kyverno uses information from an admission review object to perform validation, mutation, and generation. However, sometimes additional data is needed. This is done by defining a context field with variables that can be populated from an external config map, the Kubernetes API server, or an image registry.</p>
    <p class="normal">Here is an example of defining a variable called <code class="inlineCode">dictionary</code> and using it to mutate a pod and add a label called <code class="inlineCode">environment</code>, where the value comes from the config map variable:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-attr">rules:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">configmap-lookup</span>
      <span class="hljs-attr">context:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">dictionary</span>
        <span class="hljs-attr">configMap:</span>
          <span class="hljs-attr">name:</span> <span class="hljs-string">some-config-map</span>
          <span class="hljs-attr">namespace:</span> <span class="hljs-string">some-namespace</span>
      <span class="hljs-attr">match:</span>
        <span class="hljs-attr">any:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">resources:</span>
            <span class="hljs-attr">kinds:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">Pod</span>
      <span class="hljs-attr">mutate:</span>
        <span class="hljs-attr">patchStrategicMerge:</span>
          <span class="hljs-attr">metadata:</span>
            <span class="hljs-attr">labels:</span>
              <span class="hljs-attr">environment:</span> <span class="hljs-string">"</span><span class="hljs-template-variable">{{dictionary.data.env}}</span><span class="hljs-string">"</span>
</code></pre>
    <p class="normal">The way it <a id="_idIndexMarker1812"/>works is that the context named “dictionary” points to a config map. Inside the config map there is a section called “data” with a key called “env”.</p>
    <h4 class="heading-4">Autogen rules for pod controllers</h4>
    <p class="normal">Pods<a id="_idIndexMarker1813"/> are one of the most common resources to apply policies to. However, pods can be created indirectly by many types of resources: Pods (directly), Deployments, StatefulSets, DaemonSets, and Jobs. If we want to verify that every pod has a label called “app” then we will be forced to write complex match rules with an <code class="inlineCode">any</code> statement that covers all the various resources that create pods. Kyverno provides a very elegant solution in the form of autogen rules for pod controllers.</p>
    <p class="normal">The auto-generated rules can be observed in the status of the policy object. We will see an example in the next section.</p>
    <p class="normal">We covered in detail a lot of the powerful capabilities Kyverno brings to the table. Let’s write some policies and see them in action.</p>
    <h2 id="_idParaDest-775" class="heading-2">Writing and testing Kyverno policies</h2>
    <p class="normal">In this<a id="_idIndexMarker1814"/> section, we will actually write some Kyverno policies and see them in action. We will use some of the rules we explored in the previous section and <a id="_idIndexMarker1815"/>embed them in full-fledged policies, apply the policies, create resources that comply with the policies as well as resources that violate the policies (in the case of validating policies), and see the outcome.</p>
    <h3 id="_idParaDest-776" class="heading-3">Writing validating policies</h3>
    <p class="normal">Let’s <a id="_idIndexMarker1816"/>start with a validating policy that disallows services in the namespace <code class="inlineCode">ns-1</code> as well as services named <code class="inlineCode">service-1</code> or <code class="inlineCode">service-2</code> in any namespace:</p>
    <pre class="programlisting gen"><code class="hljs">$ cat &lt;&lt;EOF | k apply -f - 
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: disallow-some-services
spec:
  validationFailureAction: Enforce
  rules:
    - name: some-rule
      match:
        any:
          - resources:
              kinds:
                - Service
              names:
                - "service-1"
                - "service-2"
          - resources:
              kinds:
                - Service
              namespaces:
                - "ns-1"
      validate:
        message: &gt;-
         services named service-1 and service-2 and
         any service in namespace ns-1 are not allowed
        deny: {}
EOF
clusterpolicy.kyverno.io/disallow-some-services created
</code></pre>
    <p class="normal">Now that the policy is in place, let’s try to create a service named “service-1” in the default namespace that violates the policy. Note that there is no need to actually create resources to check the outcome of admission control. It is sufficient to run in dry-run mode as long as the dry-run happens on the server side:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create service clusterip service-1 -n default --tcp=80 --dry-run=server
error: failed to create ClusterIP service: admission webhook "validate.kyverno.svc-fail" denied the request:
policy Service/default/service-1 for resource violations:
disallow-some-services:
  some-rule: services named service-1 and service-2 and  any service in namespace
    ns-1 are not allowed
exclude-services-namespace:
  some-rule: services are not allowed, except in the ns-1 namespace
</code></pre>
    <p class="normal">As you can see, the request was rejected, with a nice message from the policy that explains why.</p>
    <p class="normal">If we try to<a id="_idIndexMarker1817"/> do the dry-run on the client side, it succeeds (but doesn’t actually create any service), as the admission control check happens only on the server:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create service clusterip service-1 -n default --tcp=80 --dry-run=client
service/service-1 created (dry run)
</code></pre>
    <p class="normal">Now that we have proved the point, we will use only a server-side dry-run.</p>
    <p class="normal">Let’s try to create a service called <code class="inlineCode">service-3</code> in the default namespace, which should be allowed:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create service clusterip service-3 -n default --tcp=80 --dry-run=server
service/service-3 created (server dry run)
</code></pre>
    <p class="normal">Let’s try to create <code class="inlineCode">service-3</code> in the forbidden <code class="inlineCode">ns-1</code> namespace:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create ns ns-1
$ k create service clusterip service-3 -n ns-1 --tcp=80 --dry-run=server
error: failed to create ClusterIP service: admission webhook "validate.kyverno.svc-fail" denied the request:
policy Service/ns-1/service-3 for resource violation:
disallow-some-services:
  some-rule: services named service-1 and service-2 and  any service in namespace
    ns-1 are not allowed
</code></pre>
    <p class="normal">Yep. That failed as expected. Let’s see what happens if we change the <code class="inlineCode">validationFailureAction</code> from <code class="inlineCode">Enforce</code> to <code class="inlineCode">Audit</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ k patch clusterpolicies.kyverno.io disallow-some-services --type merge -p '{"spec": {"validationFailureAction": "Audit"}}'
clusterpolicy.kyverno.io/disallow-some-services patched
$ k create service clusterip service-3 -n ns-1 --tcp=80 --dry-run=server
service/service-3 created (server dry run)
</code></pre>
    <p class="normal">However, it generated a report of validation failure:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get policyreports.wgpolicyk8s.io -n ns-1
NAME                          PASS   FAIL   WARN   ERROR   SKIP   AGE
cpol-disallow-some-services   0      1      0      0       0      2m4s
</code></pre>
    <p class="normal">Now, the service <a id="_idIndexMarker1818"/>passes the admission control, but a record of the violation was captured in the policy report. We will look at reports in more detail later in the chapter.</p>
    <p class="normal">For now, let’s look at mutating policies.</p>
    <h3 id="_idParaDest-777" class="heading-3">Writing mutating policies</h3>
    <p class="normal">Mutating policies <a id="_idIndexMarker1819"/>are a lot of fun. They quietly modify incoming requests to comply with the policy. They don’t cause failures like validating policies in “enforce” mode, and they don’t generate reports you need to scour through like validating policies in “audit” mode. If an invalid or incomplete request comes in, you just change it until it’s valid.</p>
    <p class="normal">Here is a policy that sets the <code class="inlineCode">imagePullPolicy</code> to <code class="inlineCode">IfNotPresent</code> when the tag is <code class="inlineCode">latest</code> (by default it is <code class="inlineCode">Always</code>).</p>
    <pre class="programlisting gen"><code class="hljs">$ cat &lt;&lt;EOF | k apply -f -
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: set-image-pull-policy
spec:
  rules:
    - name: set-image-pull-policy
      match:
        any:
          - resources:
              kinds:
                - Pod
      mutate:
        patchStrategicMerge:
          spec:
            containers:
              # match images which end with :latest
              - (image): "*:latest"
                # set the imagePullPolicy to "IfNotPresent"
                imagePullPolicy: "IfNotPresent"
EOF
clusterpolicy.kyverno.io/set-image-pull-policy created
</code></pre>
    <p class="normal">Let’s see it in <a id="_idIndexMarker1820"/>action. Note that for a mutating policy, we can’t use dry-run because the whole point is to actually mutate a resource.</p>
    <p class="normal">The following pod matches our policy and doesn’t have <code class="inlineCode">imagePullPolicy</code> set:</p>
    <pre class="programlisting gen"><code class="hljs">$ cat &lt;&lt;EOF | k apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: some-pod
spec:
  containers:
    - name: some-container
      image: g1g1/py-kube:latest
      command:
        - sleep
        - "9999"
EOF
pod/some-pod created
</code></pre>
    <p class="normal">Let’s verify that the mutation worked and check the container’s <code class="inlineCode">imagePullPolicy</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po some-pod -o yaml | yq '.spec.containers[0].imagePullPolicy'
IfNotPresent
</code></pre>
    <p class="normal">Yes. It was set correctly. Let’s confirm that Kyverno was responsible for setting the <code class="inlineCode">imagePullPolicy</code> by deleting the policy and then creating another pod:</p>
    <pre class="programlisting gen"><code class="hljs">$ k delete clusterpolicy set-image-pull-policy
clusterpolicy.kyverno.io "set-image-pull-policy" deleted
$ cat &lt;&lt;EOF | k apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: another-pod
spec:
  containers:
    - name: some-container
      image: g1g1/py-kube:latest
      command:
        - sleep
        - "9999"
EOF
pod/another-pod created
</code></pre>
    <p class="normal">The<a id="_idIndexMarker1821"/> Kyverno policy was deleted, and another pod called <code class="inlineCode">another-pod</code> with the same image <code class="inlineCode">g1g1/py-kube:latest</code> was created. Let’s see if its <code class="inlineCode">imagePullPolicy</code> is the expected <code class="inlineCode">Always</code> (the default for images with the <code class="inlineCode">latest</code> image tag):</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po another-pod -o yaml | yq '.spec.containers[0].imagePullPolicy'
Always
</code></pre>
    <p class="normal">Yes, it works how it should! Let’s move on to another type of exciting Kyverno policy – a generating policy, which can create new resources out of thin air.</p>
    <h3 id="_idParaDest-778" class="heading-3">Writing generating policies</h3>
    <p class="normal">Generating policies <a id="_idIndexMarker1822"/>create new resources in addition to the requested resource when a new resource is created. Let’s take our previous example of creating an automatic network policy for new namespaces that prevents any network traffic from coming in and out. This is a cluster policy that applies to any new namespace except the excluded namespaces:</p>
    <pre class="programlisting gen"><code class="hljs">cat &lt;&lt;EOF | k apply -f -
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: deny-all-traffic
spec:
  rules:
  - name: deny-all-traffic
    match:
      any:
      - resources:
          kinds:
          - Namespace
    exclude:
      any:
      - resources:
          namespaces:
          - kube-system
          - default
          - kube-public
          - kyverno
    generate:
      kind: NetworkPolicy
      apiVersion: networking.k8s.io/v1
      name: deny-all-traffic
      namespace: "{{request.object.metadata.name}}"
      data:  
        spec:
          # select all pods in the namespace
          podSelector: {}
          policyTypes:
          - Ingress
          - Egress
EOF
clusterpolicy.kyverno.io/deny-all-traffic created
</code></pre>
    <p class="normal">The <code class="inlineCode"><a id="_idIndexMarker1823"/></code><code class="inlineCode">deny-all-traffic</code> Kyverno policy was created successfully. Let’s create a new namespace, <code class="inlineCode">ns-2</code>, and see if the expected NetworkPolicy is generated:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create ns ns-2
namespace/ns-2 created
$ k get networkpolicy -n ns-2
NAME               POD-SELECTOR   AGE
deny-all-traffic   &lt;none&gt;         15s
</code></pre>
    <p class="normal">Yes, it worked! Kyverno lets you easily generate additional resources.</p>
    <p class="normal">Now that we have some hands-on experience in creating Kyverno policies, let’s learn about how to test them and why.</p>
    <h2 id="_idParaDest-779" class="heading-2">Testing policies</h2>
    <p class="normal">Testing <a id="_idIndexMarker1824"/>Kyverno policies before deploying them to production is very important because Kyverno policies are very powerful, and they could easily cause outages and incidents if misconfigured by blocking valid requests, allowing invalid requests, improperly mutating resources, and generating resources in the wrong namespaces.</p>
    <p class="normal">Kyverno offers tooling as well as guidance about testing its policies.</p>
    <h3 id="_idParaDest-780" class="heading-3">The Kyverno CLI</h3>
    <p class="normal">The Kyverno CLI <a id="_idIndexMarker1825"/>is a versatile command-line program that lets you apply policies on the client side and see the results, run tests, and evaluate JMESPath expressions.</p>
    <p class="normal">Follow these<a id="_idIndexMarker1826"/> instructions to install the Kyverno CLI: <a href="https://kyverno.io/docs/kyverno-cli/#building-and-installing-the-cli"><span class="url">https://kyverno.io/docs/kyverno-cli/#building-and-installing-the-cli</span></a>.</p>
    <p class="normal">Verify that it was installed correctly by checking the version:</p>
    <pre class="programlisting gen"><code class="hljs">$ kyverno version
Version: 1.8.5
Time: 2022-12-20T08:41:43Z
Git commit ID: c19061758dc4203106ab6d87a245045c20192721
</code></pre>
    <p class="normal">Here is the help screen if you just type kyverno with no additional command:</p>
    <pre class="programlisting gen"><code class="hljs">$ kyverno
Kubernetes Native Policy Management
Usage:
  kyverno [command]
Available Commands:
  apply       applies policies on resources
  completion  Generate the autocompletion script for the specified shell
  help        Help about any command
  jp          Provides a command-line interface to JMESPath, enhanced with Kyverno specific custom functions
  test        run tests from directory
  version     Shows current version of kyverno
Flags:
      --add_dir_header           If true, adds the file directory to the header of the log messages
  -h, --help                     help for kyverno
      --log_file string          If non-empty, use this log file (no effect when -logtostderr=true)
      --log_file_max_size uint   Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)
      --one_output               If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)
      --skip_headers             If true, avoid header prefixes in the log messages
      --skip_log_headers         If true, avoid headers when opening log files (no effect when -logtostderr=true)
  -v, --v Level                  number for the log level verbosity
Use "kyverno [command] --help" for more information about a command.
</code></pre>
    <p class="normal">Earlier in the <a id="_idIndexMarker1827"/>chapter we saw how to evaluate the results of a validating Kyverno policy without actually creating resources, using a dry-run. This is not possible for mutating or generating policies. With <code class="inlineCode">kyverno apply</code> we can achieve the same effect for all policy types.</p>
    <p class="normal">Let’s see how to apply a mutating policy to a resource and examine the results. We will apply the <code class="inlineCode">set-image-pull-policy</code> to a pod stored in the file <code class="inlineCode">some-pod.yaml</code>. The policy was defined earlier, and is available in the attached code as the file <code class="inlineCode">mutate-image-pull-policy.yaml</code>. </p>
    <p class="normal">First, let’s see what the result would be if we just created the pod without applying the Kyverno policy:</p>
    <pre class="programlisting gen"><code class="hljs">$ k apply -f some-pod.yaml -o yaml --dry-run=server | yq '.spec.containers[0].imagePullPolicy'
Always
</code></pre>
    <p class="normal">It is<code class="inlineCode"> Always</code>. Now, we will apply the Kyverno policy to this pod resource and check the outcome:</p>
    <pre class="programlisting gen"><code class="hljs">$ kyverno apply mutate-image-pull-policy.yaml --resource some-pod.yaml
Applying 1 policy rule to 1 resource...
mutate policy set-image-pull-policy applied to default/Pod/some-pod:
apiVersion: v1
kind: Pod
metadata:
  name: some-pod
  namespace: default
spec:
  containers:
  - command:
    - sleep
    - "9999"
    image: g1g1/py-kube:latest
    imagePullPolicy: IfNotPresent
    name: some-container
---
pass: 1, fail: 0, warn: 0, error: 0, skip: 2
</code></pre>
    <p class="normal">As you can see, after the mutating policy is applied to <code class="inlineCode">some-pod</code>, the <code class="inlineCode">imagePullPolicy</code> is <code class="inlineCode">IfNotPresent</code> as expected.</p>
    <p class="normal">Let’s play with the <code class="inlineCode">kyverno jp</code> sub-command. It accepts standard input or can take a file.</p>
    <p class="normal">Here is an <a id="_idIndexMarker1828"/>example that checks how many arguments the command of the first container in a pod has. We will use this pod manifest as input:</p>
    <pre class="programlisting gen"><code class="hljs">$ cat some-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: some-pod
spec:
  containers:
    - name: some-container
      image: g1g1/py-kube:latest
      command:
        - sleep
        - "9999"
</code></pre>
    <p class="normal">Note that it has a command called <code class="inlineCode">sleep</code> with a single argument, “9999”. We expect the answer to be 1. The following command does the trick:</p>
    <pre class="programlisting gen"><code class="hljs">$ cat some-pod.yaml | kyverno jp 'length(spec.containers[0].command) | subtract(@, `1`)'
1
</code></pre>
    <p class="normal">How does it work? First it pipes the content of <code class="inlineCode">some-pod.yaml</code> to the <code class="inlineCode">kyverno jp</code> command with the JMESPath expression that takes the length of the command of the first container (an array with two elements, “sleep” and “9000”), and then it pipes it to the <code class="inlineCode">subtract()</code> function, which subtracts 1 and, hence, ends up with the expected result of 1.</p>
    <p class="normal">The Kyverno CLI commands <code class="inlineCode">apply</code> and <code class="inlineCode">jp</code> are great for ad hoc exploration and the quick prototyping of complex JMESPath expressions. However, if you use Kyverno policies at scale (and you should), then I recommend a more rigorous testing practice. Luckily Kyverno has good support for testing via the <code class="inlineCode">kyverno test</code> command. Let’s see how to write and run Kyverno tests.</p>
    <h3 id="_idParaDest-781" class="heading-3">Understanding Kyverno tests</h3>
    <p class="normal">The <code class="inlineCode">kyverno test</code> command <a id="_idIndexMarker1829"/>operates on a set of resources and policies governed by a file called <code class="inlineCode">kyverno-test.yaml</code>, which defines what policy rules should be applied to which resources and what the expected outcome is. It then returns the results.</p>
    <p class="normal">The result of applying a policy rule to a resource can be one of the following four:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">pass</code> – the resource matches the policy and doesn’t trigger the <code class="inlineCode">deny</code> statement (only for validating policies)</li>
      <li class="bulletList"><code class="inlineCode">fail</code> – the resource matches the policy and triggers the deny statement (only for validating policies)</li>
      <li class="bulletList"><code class="inlineCode">skip</code> – the resource doesn’t match the policy definition and the policy wasn’t applied</li>
      <li class="bulletList"><code class="inlineCode">warn</code> – the resource doesn’t comply with the policy but has an annotation: <code class="inlineCode">policies.kyverno.io/scored: "false"</code></li>
    </ul>
    <p class="normal">If the expected outcome of the test doesn’t match the result of applying the policy to the resource, then the test will be considered a failure.</p>
    <p class="normal">For mutating and generating policies, the test will include <code class="inlineCode">patchedResource</code> and <code class="inlineCode">generatedResource</code> respectively.</p>
    <p class="normal">Let’s see what the <code class="inlineCode">kyverno-test.yaml</code> file looks like:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">name:</span> <span class="hljs-string">&lt;some</span> <span class="hljs-string">name&gt;</span>
<span class="hljs-attr">policies:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">&lt;path/to/policy.yaml&gt;</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">&lt;path/to/policy.yaml&gt;</span>
<span class="hljs-attr">resources:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">&lt;path/to/resource.yaml&gt;</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">&lt;path/to/resource.yaml&gt;</span>
<span class="hljs-attr">variables:</span> <span class="hljs-string">variables.yaml</span> <span class="hljs-comment"># optional file for declaring variables</span>
<span class="hljs-attr">userinfo:</span> <span class="hljs-string">user_info.yaml</span> <span class="hljs-comment"># optional file for declaring admission request information (roles, cluster roles and subjects)</span>
<span class="hljs-attr">results:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">policy:</span> <span class="hljs-string">&lt;name&gt;</span>
  <span class="hljs-attr">rule:</span> <span class="hljs-string">&lt;name&gt;</span>
  <span class="hljs-attr">resource:</span> <span class="hljs-string">&lt;name&gt;</span>
  <span class="hljs-attr">resources:</span> <span class="hljs-comment"># optional, primarily for `validate` rules. One of either `resource` or `resources[]` must be specified. Use `resources[]` when a number of different resources should all share the same test result.</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">&lt;name_1&gt;</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">&lt;name_2&gt;</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">&lt;name&gt;</span> <span class="hljs-comment"># when testing for a resource in a specific Namespace</span>
  <span class="hljs-attr">patchedResource:</span> <span class="hljs-string">&lt;file_name.yaml&gt;</span> <span class="hljs-comment"># when testing a mutate rule this field is required.</span>
  <span class="hljs-attr">generatedResource:</span> <span class="hljs-string">&lt;file_name.yaml&gt;</span> <span class="hljs-comment"># when testing a generate rule this field is required.</span>
  <span class="hljs-attr">kind:</span> <span class="hljs-string">&lt;kind&gt;</span>
  <span class="hljs-attr">result:</span> <span class="hljs-string">pass</span>
</code></pre>
    <p class="normal">Many different <a id="_idIndexMarker1830"/>test cases can be defined in a single <code class="inlineCode">kyverno-test.yaml</code> file. The file has five sections:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">policies</code></li>
      <li class="bulletList"><code class="inlineCode">resources</code></li>
      <li class="bulletList"><code class="inlineCode">variables</code></li>
      <li class="bulletList"><code class="inlineCode">userInfo</code></li>
      <li class="bulletList"><code class="inlineCode">results</code></li>
    </ul>
    <p class="normal">The <code class="inlineCode">policies</code> and <code class="inlineCode">resources</code> sections specify paths to all the policies and resources that participate in the tests. The <code class="inlineCode">variables</code> and <code class="inlineCode">userInfo</code> optional sections can define additional information that will be used by the test cases.</p>
    <p class="normal">The <code class="inlineCode">results</code> section is where the various test cases are specified. Each test case tests the application of a single policy rule to a single resource. If it’s a validating rule, then the <code class="inlineCode">result</code> field should contain the expected outcome. </p>
    <p class="normal">If it’s a mutating or generating rule, then the corresponding <code class="inlineCode">patchedResource</code> or <code class="inlineCode">generatedResource</code> should contain the expected outcome.</p>
    <p class="normal">Let’s write some Kyverno tests for our policies.</p>
    <h3 id="_idParaDest-782" class="heading-3">Writing Kyverno tests</h3>
    <p class="normal">All the<a id="_idIndexMarker1831"/> files mentioned here are available in the <code class="inlineCode">tests</code> sub-directory of the code attached to the chapter.</p>
    <p class="normal">Let’s start by writing our <code class="inlineCode">kyverno-test.yaml</code> file:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">name:</span> <span class="hljs-string">test-some-rule</span>
<span class="hljs-attr">policies:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">../disallow-some-services-policy.yaml</span>
<span class="hljs-attr">resources:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">test-service-ok.yaml</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">test-service-bad-name.yaml</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">test-service-bad-namespace.yaml</span>
<span class="hljs-attr">results:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">policy:</span> <span class="hljs-string">disallow-some-services</span>
    <span class="hljs-attr">rule:</span> <span class="hljs-string">some-rule</span>
    <span class="hljs-attr">resources:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">service-ok</span>
    <span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
    <span class="hljs-attr">result:</span> <span class="hljs-string">skip</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">policy:</span> <span class="hljs-string">disallow-some-services</span>
    <span class="hljs-attr">rule:</span> <span class="hljs-string">some-rule</span>
    <span class="hljs-attr">resources:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">service-1</span>
    <span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
    <span class="hljs-attr">result:</span> <span class="hljs-string">fail</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">policy:</span> <span class="hljs-string">disallow-some-services</span>
    <span class="hljs-attr">rule:</span> <span class="hljs-string">some-rule</span>
    <span class="hljs-attr">resources:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">service-in-ns-1</span>
    <span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
    <span class="hljs-attr">namespace:</span> <span class="hljs-string">ns-1</span>
    <span class="hljs-attr">result:</span> <span class="hljs-string">fail</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">policies</code> section <a id="_idIndexMarker1832"/>contains the <code class="inlineCode">disallow-some-services-policy.yaml</code> file. This policy rejects services named <code class="inlineCode">service-1</code> or <code class="inlineCode">service-2</code> and any service in the <code class="inlineCode">ns-1</code> namespace.</p>
    <p class="normal">The <code class="inlineCode">resources</code> section contains three different files that all contain a Service resource:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">test-service-ok.yaml</code></li>
      <li class="bulletList"><code class="inlineCode">test-service-bad-name.yaml</code></li>
      <li class="bulletList"><code class="inlineCode">test-service-bad-namespace.yaml</code></li>
    </ul>
    <p class="normal">The <code class="inlineCode">test-service-ok.yaml</code> file contains a service that doesn’t match any of the rules of the policy. The <code class="inlineCode">test-service-bad-name.yaml</code> file contains a service named <code class="inlineCode">service-1</code>, which is not allowed. Finally, the <code class="inlineCode">test-service-bad-namespace.yaml</code> file contains a resource named <code class="inlineCode">service-in-ns-1</code>, which is allowed. However, it has the <code class="inlineCode">ns-1</code> namespace, which is not allowed.</p>
    <p class="normal">Let’s look at the <code class="inlineCode">results</code> section. There are three different test cases here. They all test the same rule in our policy, but each test case uses a different resource name. This comprehensively covers the behavior of the policy.</p>
    <p class="normal">The first test case verifies that a service that doesn’t match the rule is skipped. It specifies the policy, the rule name, the resources the test case should be applied to, and most <a id="_idIndexMarker1833"/>importantly, the expected result, which is <code class="inlineCode">skip</code> in this case:</p>
    <pre class="programlisting code"><code class="hljs-code">  <span class="hljs-bullet">-</span> <span class="hljs-attr">policy:</span> <span class="hljs-string">disallow-some-services</span>
    <span class="hljs-attr">rule:</span> <span class="hljs-string">some-rule</span>
    <span class="hljs-attr">resources:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">service-ok</span>
    <span class="hljs-attr">result:</span> <span class="hljs-string">skip</span>
</code></pre>
    <p class="normal">The second test case is similar except that the resource name is different and the expected result is <code class="inlineCode">fail</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">  <span class="hljs-bullet">-</span> <span class="hljs-attr">policy:</span> <span class="hljs-string">disallow-some-services</span>
    <span class="hljs-attr">rule:</span> <span class="hljs-string">some-rule</span>
    <span class="hljs-attr">resources:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">service-1</span>
    <span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
    <span class="hljs-attr">result:</span> <span class="hljs-string">fail</span>
</code></pre>
    <p class="normal">There is one more slight difference. In this test case, the kind of the target resource is explicitly specified (<code class="inlineCode">kind: Service</code>). This may seem redundant at first glance because the <code class="inlineCode">service-1</code> resource defined in <code class="inlineCode">test-service-bad-name.yaml</code> already has the <code class="inlineCode">kind</code> listed:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">service-1</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">service-1</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">ns-2</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">ports:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">https</span>
      <span class="hljs-attr">port:</span> <span class="hljs-number">443</span>
      <span class="hljs-attr">targetPort:</span> <span class="hljs-string">https</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">some-app</span>
</code></pre>
    <p class="normal">The reason the <code class="inlineCode">kind</code> field is needed is to disambiguate which resource is targeted, in case the resource file contains multiple resources with the same name.</p>
    <p class="normal">The third test case is the same as the second test case, except it targets a different resource and, as a consequence, a <a id="_idIndexMarker1834"/>different part of the rule (disallowing services in the <code class="inlineCode">ns-1</code> namespace):</p>
    <pre class="programlisting code"><code class="hljs-code">  <span class="hljs-bullet">-</span> <span class="hljs-attr">policy:</span> <span class="hljs-string">disallow-some-services</span>
    <span class="hljs-attr">rule:</span> <span class="hljs-string">some-rule</span>
    <span class="hljs-attr">resources:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">service-in-ns-1</span>
    <span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
    <span class="hljs-attr">namespace:</span> <span class="hljs-string">ns-1</span>
    <span class="hljs-attr">result:</span> <span class="hljs-string">fail</span>
</code></pre>
    <p class="normal">OK. We have our test cases. Let’s see how to run these tests.</p>
    <h3 id="_idParaDest-783" class="heading-3">Running Kyverno tests</h3>
    <p class="normal">Running <a id="_idIndexMarker1835"/>Kyverno tests is very simple. You just type <code class="inlineCode">kyverno test</code> and the path to the folder containing a <code class="inlineCode">kyverno-test.yaml</code> file or a Git repository and a branch.</p>
    <p class="normal">Let’s run our tests:</p>
    <pre class="programlisting gen"><code class="hljs">$ kyverno test .
Executing test-some-rule...
applying 1 policy to 3 resources...
│───│────────────────────────│───────────│──────────────────────────────│────────│
│ # │ POLICY                 │ RULE      │ RESOURCE                     │ RESULT │
│───│────────────────────────│───────────│──────────────────────────────│────────│
│ 1 │ disallow-some-services │ some-rule │ ns-2//service-ok             │ Pass   │
│ 2 │ disallow-some-services │ some-rule │ ns-2/Service/service-1       │ Pass   │
│ 3 │ disallow-some-services │ some-rule │ ns-1/Service/service-in-ns-1 │ Pass   │
│───│────────────────────────│───────────│──────────────────────────────│────────│
Test Summary: 3 tests passed and 0 tests failed
</code></pre>
    <p class="normal">We get a nice output that lists each test case and then a one-line summary. All three tests passed, so that’s great.</p>
    <p class="normal">When you <a id="_idIndexMarker1836"/>have test files that contain a lot of test cases and you try to tweak one specific rule, you may want to run a specific test case only. Here is the syntax:</p>
    <pre class="programlisting gen"><code class="hljs">kyverno test . --test-case-selector "policy=disallow-some-services, rule=some-rule, resource=service-ok"
</code></pre>
    <p class="normal">The <code class="inlineCode">kyverno test</code> command has very good documentation with a lot of examples. Just type <code class="inlineCode">kyverno test -h</code>.</p>
    <p class="normal">So far, we have written policies, rules, and policy tests and executed them. The last piece of the puzzle is viewing reports when Kyverno is running.</p>
    <h2 id="_idParaDest-784" class="heading-2">Viewing Kyverno reports</h2>
    <p class="normal">Kyverno <a id="_idIndexMarker1837"/>generates reports for policies with <code class="inlineCode">validate</code> or <code class="inlineCode">verifyImages</code> rules. Only policies in <code class="inlineCode">audit</code> mode or that have <code class="inlineCode">spec.background: true</code> will generate reports.</p>
    <p class="normal">As you recall Kyverno can generate two types of reports in the form of custom resources. <code class="inlineCode">PolicyReports</code> are generated for namespace-scoped resources (like services) in the namespace the resource was applied. <code class="inlineCode">ClusterPolicyReports</code> are generated for cluster-scoped resources (like namespaces).</p>
    <p class="normal">Our <code class="inlineCode">disallow-some-services</code> policy has a <code class="inlineCode">validate</code> rule and operates in <code class="inlineCode">audit</code> mode, which means that if we create a service that violates the rule, the service will be created, but a report will be generated. Here we go:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create service clusterip service-3 -n ns-1 --tcp=80
service/service-3 created
</code></pre>
    <p class="normal">We created a service in the forbidden <code class="inlineCode">ns-1</code> namespace. Kyverno didn’t block the creation of the service because of audit mode. Let’s review the report (that <code class="inlineCode">polr</code> is shorthand for <code class="inlineCode">policyreports</code>):</p>
    <pre class="programlisting gen"><code class="hljs">$ k get polr -n ns-1
NAME                          PASS   FAIL   WARN   ERROR   SKIP   AGE
cpol-disallow-some-services   0      1      0      0       0      1m
</code></pre>
    <p class="normal">A report named <code class="inlineCode">cpol-disallow-some-services</code> was created. We can see that it counted one failure. What happens if we create another service?</p>
    <pre class="programlisting gen"><code class="hljs">$ k create service clusterip service-4 -n ns-1 --tcp=80
service/service-4 created
$ k get polr -n ns-1
NAME                          PASS   FAIL   WARN   ERROR   SKIP   AGE
cpol-disallow-some-services   0      2      0      0       0      2m
</code></pre>
    <p class="normal">Yep. Another<a id="_idIndexMarker1838"/> failure is reported. The meaning of these failures is that the resource failed to pass the <code class="inlineCode">validate</code> rule. Let’s peek inside. The report has a <code class="inlineCode">metadata</code> field, which includes an annotation for the policy it represents. Then there is a <code class="inlineCode">results</code> section where each failed resource is listed. The info for each result includes the resource that caused the failure and the rule it violated. Finally, the <code class="inlineCode">summary</code> contains aggregate information about the results:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get polr cpol-disallow-some-services -n ns-1 -o yaml
apiVersion: wgpolicyk8s.io/v1alpha2
kind: PolicyReport
metadata:
  creationTimestamp: "2023-01-22T04:01:12Z"
  generation: 3
  labels:
    app.kubernetes.io/managed-by: kyverno
    cpol.kyverno.io/disallow-some-services: "2472317"
  name: cpol-disallow-some-services
  namespace: ns-1
  resourceVersion: "2475547"
  uid: dadcd6ae-a867-4ec8-bf09-3e6ca76da7ba
results:
- message: services named service-1 and service-2 and  any service in namespace ns-1
    are not allowed
  policy: disallow-some-services
  resources:
  - apiVersion: v1
    kind: Service
    name: service-4
    namespace: ns-1
    uid: 4d473ac1-c1b1-4929-a70d-fad98a411428
  result: fail
  rule: some-rule
  scored: true
  source: kyverno
  timestamp:
    nanos: 0
    seconds: 1674361576
- message: services named service-1 and service-2 and  any service in namespace ns-1
    are not allowed
  policy: disallow-some-services
  resources:
  - apiVersion: v1
    kind: Service
    name: service-3
    namespace: ns-1
    uid: 62458ac4-fe39-4854-9f5a-18b26109511a
  result: fail
  rule: some-rule
  scored: true
  source: kyverno
  timestamp:
    nanos: 0
    seconds: 1674361426
summary:
  error: 0
  fail: 2
  pass: 0
  skip: 0
  warn: 0
</code></pre>
    <p class="normal">This is pretty nice, but<a id="_idIndexMarker1839"/> it might not be the best option to keep track of your cluster if you have a lot of namespaces. It’s considered best practice to collect all the reports and periodically export them to a central location. Check out the Policy reporter project: <a href="https://github.com/kyverno/policy-reporter"><span class="url">https://github.com/kyverno/policy-reporter</span></a>. It also comes with a web-based policy reporter UI.</p>
    <p class="normal">Let’s install it:</p>
    <pre class="programlisting gen"><code class="hljs">$ helm repo add policy-reporter https://kyverno.github.io/policy-reporter
"policy-reporter" has been added to your repositories
$ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "policy-reporter" chart repository
...Successfully got an update from the "kyverno" chart repository
Update Complete. <img src="../Images/B18998_09_001.png" alt=""/>Happy Helming!<img src="../Images/B18998_09_001.png" alt=""/>
$ helm upgrade --install policy-reporter policy-reporter/policy-reporter --create-namespace -n policy-reporter --set ui.enabled=true
Release "policy-reporter" does not exist. Installing it now.
NAME: policy-reporter
LAST DEPLOYED: Sat Jan 21 20:39:42 2023
NAMESPACE: policy-reporter
STATUS: deployed
REVISION: 1
TEST SUITE: None
</code></pre>
    <p class="normal">The policy reporter has <a id="_idIndexMarker1840"/>been installed successfully in the <code class="inlineCode">policy-reporter</code> namespace, and we enabled the UI.</p>
    <p class="normal">The next step is to do port-forwarding to access the UI:</p>
    <pre class="programlisting gen"><code class="hljs">$ k port-forward service/policy-reporter-ui 8080:8080 -n policy-reporter
Forwarding from 127.0.0.1:8080 -&gt; 8080
Forwarding from [::1]:8080 -&gt; 8080
</code></pre>
    <p class="normal">Now, we can browse <code class="inlineCode">http://localhost:8080</code> and view policy reports visually.</p>
    <p class="normal">The dashboard shows the failing policy reports. We can see our 20 failures in the <code class="inlineCode">kube-system</code> namespace and our 2 failures in the <code class="inlineCode">ns-1</code> namespace.</p>
    <figure class="mediaobject"><img src="../Images/B18998_16_07.png" alt="Policy Reporter UI - Dashboard"/></figure>
    <p class="packt_figref">Figure 16.7: Policy Reporter UI – Dashboard</p>
    <p class="normal">The failures in kube-system are due to the best practice security policies we installed with Kyverno.</p>
    <p class="normal">We can scroll <a id="_idIndexMarker1841"/>down and see more details about the failures:</p>
    <figure class="mediaobject"><img src="../Images/B18998_16_08.png" alt="Title: Inserting image..."/></figure>
    <p class="packt_figref">Figure 16.8: Policy Reporter UI – Results</p>
    <p class="normal">We can also select from the sidebar the “Policy Reports” option and then see passing results. We can also filter policy reports using different criteria like policies, kinds, categories, severities, and namespaces:</p>
    <figure class="mediaobject"><img src="../Images/B18998_16_09.png" alt="Policy Reporter UI - Policy Reports"/></figure>
    <p class="packt_figref">Figure 16.9: Policy Reporter UI – Policy Reports</p>
    <p class="normal">Overall, the <a id="_idIndexMarker1842"/>policy reporter UI has a slick look and provides a great option for exploring, filtering, and searching policy reports.</p>
    <h1 id="_idParaDest-785" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we covered the increasing adoption of Kubernetes in large enterprise organizations and the importance of governance in managing these deployments. We looked at the concept of policy engines and how they are built on top of the Kubernetes admission control mechanism. We discussed how policy engines are used to address security, compliance, and governance concerns. We also provided a review of popular policy engines. Finally, we did a deep dive into Kyverno, in which we explained in detail how it works. Then, we jumped in, wrote some policies, tested them, and reviewed policy reports. If you run a non-trivial production system on Kubernetes, you should very seriously consider having Kyverno (or another policy engine) as a core component. This is a perfect segue to the next chapter where we will discuss Kubernetes in production.</p>
  </div>
</body></html>