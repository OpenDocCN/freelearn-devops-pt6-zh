<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer106" epub:type="chapter">&#13;
			<h1 id="_idParaDest-160" class="chapter-number"><a id="_idTextAnchor160"/>12</h1>&#13;
			<h1 id="_idParaDest-161"><a id="_idTextAnchor161"/>Observability – Getting Visibility into GenAI on K8s</h1>&#13;
			<p>In this chapter, we will cover key observability concepts for monitoring GenAI applications in <strong class="bold">Kubernetes</strong> (<strong class="bold">K8s</strong>). We’ll dive into why monitoring is critical for optimizing GenAI workloads, examining both system-level metrics and application-specific signals. By integrating tools such as <strong class="bold">Prometheus</strong> for metrics collection and <strong class="bold">Grafana</strong> for visualization, and leveraging the debugging capabilities of <strong class="bold">LangChain</strong>, you’ll learn how to construct a comprehensive monitoring framework that provides real-time and <span class="No-Break">actionable insights.</span></p>&#13;
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>&#13;
			<ul>&#13;
				<li>Observability <span class="No-Break">key concepts</span></li>&#13;
				<li>Monitoring tools <span class="No-Break">in K8s</span></li>&#13;
				<li>Visualization <span class="No-Break">and debugging</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-162"><a id="_idTextAnchor162"/>Observability key concepts</h1>&#13;
			<p><strong class="bold">Observability</strong> is the foundational framework for identifying, investigating, and remediating issues in a system, as<a id="_idIndexMarker1089"/> shown in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.1</em>. It provides a holistic view of system behavior and performance. Observability is built on three core pillars: <strong class="bold">logs</strong>, <strong class="bold">metrics</strong>, and <strong class="bold">traces</strong>. Logs capture detailed event information, metrics quantify system performance, and traces provide an end-to-end view of request flows. Together, these components enable efficient monitoring and troubleshooting of complex distributed systems. This integration ensures actionable insights for maintaining system reliability <span class="No-Break">and performance.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer090" class="IMG---Figure">&#13;
					<img src="image/B31108_12_01.jpg" alt="Figure 12.1 – Observability framework" width="1592" height="733"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Observability framework</p>&#13;
			<h2 id="_idParaDest-163"><a id="_idTextAnchor163"/>Logs</h2>&#13;
			<p><strong class="bold">System logs</strong> cover events such <a id="_idIndexMarker1090"/>as transactions, system errors, and user actions. K8s <a id="_idIndexMarker1091"/>generates logs at different layers, such as container logs, node logs, and cluster-level logs. You can use the following command to see logs for <span class="No-Break">a Pod:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl logs &lt;pod-name&gt;</pre>			<p>If the Pod has multiple containers, a specific container log can be observed with <span class="No-Break">the following:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl logs &lt;pod-name&gt; -c &lt;container-name&gt;</pre>			<p>By default, logs in K8s are ephemeral and are lost when Pods restart. To persist logs, one can use sidecar <a id="_idIndexMarker1092"/>containers or logging agents that forward logs to a <a id="_idIndexMarker1093"/>centralized storage <a id="_idIndexMarker1094"/>backend such as <strong class="bold">Amazon CloudWatch Logs</strong>, <strong class="bold">Elasticsearch</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="bold">Loki</strong></span><span class="No-Break">.</span></p>&#13;
			<p>K8s does not provide cluster-wide logging by default. To implement logging at the cluster level, one <a id="_idIndexMarker1095"/>can use solutions <a id="_idIndexMarker1096"/>such as Loki or managed <a id="_idIndexMarker1097"/>services<a id="_idIndexMarker1098"/> such as <strong class="bold">Datadog</strong>, <strong class="bold">Splunk</strong>, <strong class="bold">Amazon CloudWatch</strong>, and <span class="No-Break"><strong class="bold">New Relic</strong></span><span class="No-Break">.</span></p>&#13;
			<p>Standard logs provide great insights into pod-level events and errors; however, they don’t cover a complete trace of an API response. That’s where K8s audit logs come in, offering a more granular view of API interactions for security and compliance purposes. We will cover K8s audit logs in<a id="_idIndexMarker1099"/> the <span class="No-Break">next section.</span></p>&#13;
			<h3>K8s audit logs</h3>&#13;
			<p>K8s provides <em class="italic">audit log capabilities</em> to track all API requests made to <a id="_idIndexMarker1100"/>the K8s API server. This provides a detailed <a id="_idIndexMarker1101"/>record of actions performed by users, service accounts, and <a id="_idIndexMarker1102"/>controllers and can help with security, compliance, <span class="No-Break">and troubleshooting.</span></p>&#13;
			<p>To enable audit logs, modify the K8s API server configuration by adding <span class="No-Break">the following:</span></p>&#13;
			<pre class="source-code">&#13;
--audit-log-path=/var/log/kubernetes/kube-apiserver-audit.log&#13;
--audit-policy-file=/etc/kubernetes/audit-policy.yaml</pre>			<p>The following YAML shows a sample audit policy that will log the request metadata without the <span class="No-Break">request body:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: audit.k8s.io/v1&#13;
kind: Policy&#13;
rules:&#13;
  - level: Metadata</pre>			<p>In managed K8s offerings such as Amazon EKS, control plane audit logs can be configured to stream to Amazon CloudWatch Logs. Refer to the EKS documentation at <a href="https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html">https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html</a> for instructions on how to <a id="_idIndexMarker1103"/>set up the <span class="No-Break">audit logs.</span></p>&#13;
			<p>While logs provide a detailed view of discrete events, such as errors, system activity, and user actions, metrics offer a complementary perspective by capturing continuous performance data over time. In the next section, we shift our focus from event-based insights to performance monitoring <span class="No-Break">using metrics.</span></p>&#13;
			<h2 id="_idParaDest-164"><a id="_idTextAnchor164"/>Metrics</h2>&#13;
			<p>Metrics involve <em class="italic">time-series data</em> that tracks<a id="_idIndexMarker1104"/> system performance indicators such as <a id="_idIndexMarker1105"/>memory usage and latency. In K8s, metrics provide real-time performance data about the cluster, nodes, Pods, and containers. These metrics help with monitoring, scaling, and troubleshooting workloads running inside a <span class="No-Break">K8s cluster.</span></p>&#13;
			<p>Metrics in K8s are collected at<a id="_idIndexMarker1106"/> <span class="No-Break">different layers:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Node metrics</strong>, such as CPU, memory, disk, and <a id="_idIndexMarker1107"/>network usage at the <span class="No-Break">node level</span></li>&#13;
				<li><strong class="bold">Pod and container metrics</strong>, such as <a id="_idIndexMarker1108"/>resource consumption of each Pod <span class="No-Break">and container</span></li>&#13;
				<li><strong class="bold">Cluster-level metrics</strong>, such as the <a id="_idIndexMarker1109"/>overall health and performance of <span class="No-Break">the cluster</span></li>&#13;
				<li><strong class="bold">Application-level metrics</strong>, which <a id="_idIndexMarker1110"/>are custom application metrics such as request latency and <span class="No-Break">error rates</span></li>&#13;
			</ul>&#13;
			<p>In <a href="B31108_06.xhtml#_idTextAnchor075"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, we covered the key metrics that are critical for application scaling and end user experiences. We covered both conventional metrics, such as CPU and memory usage, and custom metrics, such as queue length and HTTP request rates. Prometheus is a common way of collecting metrics in the K8s environment; we will discuss this in a <span class="No-Break">later section.</span></p>&#13;
			<h2 id="_idParaDest-165"><a id="_idTextAnchor165"/>Traces</h2>&#13;
			<p>Traces provide a visual representation <a id="_idIndexMarker1111"/>of how a single request flows through a distributed system, tracking its interactions with various services, APIs, databases, and <a id="_idIndexMarker1112"/>components. Tracing in K8s helps track requests as they propagate through different services, containers, and nodes within a distributed system. Traces provide end-to-end visibility into the lifecycle of a request, allowing developers and operators to understand latency issues, failures, and dependencies in <span class="No-Break">microservices architectures.</span></p>&#13;
			<p>In K8s, tracing is crucial because applications often consist of multiple microservices communicating over the network. Unlike logs and metrics, which provide snapshots of system behavior, tracing provides <a id="_idIndexMarker1113"/>contextual insights into <span class="No-Break">request flows.</span></p>&#13;
			<p><strong class="bold">OpenTelemetry</strong> (<strong class="bold">OTel</strong>) (<a href="https://opentelemetry.io/">https://opentelemetry.io/</a>) is a common framework used for collecting traces. Once collected, OTel can export <a id="_idIndexMarker1114"/>the traces to <strong class="bold">Zipkin</strong> (<a href="https://zipkin.io/">https://zipkin.io/</a>), <strong class="bold">Jaeger</strong> (<a href="https://www.jaegertracing.io/">https://www.jaegertracing.io/</a>), and <strong class="bold">AWS X-Ray</strong> (<a href="https://aws.amazon.com/xray/">https://aws.amazon.com/xray/</a>). We will <a id="_idIndexMarker1115"/>cover OTel in detail in <span class="No-Break">later </span><span class="No-Break"><a id="_idIndexMarker1116"/></span><span class="No-Break">sections.</span></p>&#13;
			<p>In this section, we explored<a id="_idIndexMarker1117"/> the fundamentals of observability in K8s and the <a id="_idIndexMarker1118"/>three core pillars – logs, metrics, and traces. In the next section, we will explore various tools in the K8s landscape to monitor <span class="No-Break">GenAI workloads.</span></p>&#13;
			<h1 id="_idParaDest-166"><a id="_idTextAnchor166"/>Monitoring tools in K8s</h1>&#13;
			<p>Achieving true observability in K8s requires a holistic approach that integrates logs, metrics, and traces. Each pillar offers unique strengths and is suited for specific use cases. By adopting the best<a id="_idIndexMarker1119"/> practices for combining logs, metrics, and traces, one can optimize monitoring strategies and achieve better system reliability and resilience, ultimately enhancing the user experience. </p>&#13;
			<p>The following is a sample stack for observability, as depicted in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Fluentd and Fluent Bit</strong> collect logs<a id="_idIndexMarker1120"/> from Kubernetes nodes, Pods, and applications and forwards<a id="_idIndexMarker1121"/> them to Amazon <a id="_idIndexMarker1122"/>CloudWatch/Loki/OpenSearch<a id="_idIndexMarker1123"/> <span class="No-Break">for storage.</span></li>&#13;
				<li><strong class="bold">OTel</strong> collects traces and <a id="_idIndexMarker1124"/>application-specific metrics, exporting<a id="_idIndexMarker1125"/> them to Prometheus (for metrics) and Jaeger/AWS X-Ray/Zipkin (<span class="No-Break">for traces).</span></li>&#13;
				<li><strong class="bold">Grafana</strong> provides a single<a id="_idIndexMarker1126"/> interface to visualize logs (from Loki), metrics (from Prometheus), and traces (from AWS X-Ray). Developers and operators <a id="_idIndexMarker1127"/>use Grafana dashboards to analyze performance, debug issues, and set up alerts based on logs, metrics, <span class="No-Break">and traces.</span></li>&#13;
			</ul>&#13;
			<div>&#13;
				<div id="_idContainer091" class="IMG---Figure">&#13;
					<img src="image/B31108_12_02.jpg" alt="Figure 12.2 – Observability stack in K8s" width="1176" height="933"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Observability stack in K8s</p>&#13;
			<p>We will discuss each <a id="_idIndexMarker1128"/>of these tools in the <span class="No-Break">following sections.</span></p>&#13;
			<h2 id="_idParaDest-167"><a id="_idTextAnchor167"/>Fluentd and Fluent Bit</h2>&#13;
			<p><strong class="bold">Fluentd</strong> (<a href="https://www.fluentd.org/">https://www.fluentd.org/</a>) is a lightweight <a id="_idIndexMarker1129"/>and scalable log aggregator that is widely used in K8s for collecting, processing, and <a id="_idIndexMarker1130"/>forwarding logs to various backends. In a K8s cluster, Fluentd is typically deployed as a DaemonSet, ensuring that each node has an agent responsible for collecting logs <a id="_idIndexMarker1131"/>from all running Pods and containers. Fluentd can gather logs from multiple sources, including container logs from <strong class="source-inline">/var/log/containers</strong>, K8s API server events, and system logs. It then parses, filters, and routes these logs to destinations such as Elasticsearch, Loki, Splunk, Amazon S3, and cloud-based logging solutions such as <span class="No-Break">Amazon CloudWatch.</span></p>&#13;
			<p>Fluentd is highly configurable through a plugin-based architecture. There are 100+ plugins available to support different log formats and backends (<a href="https://www.fluentd.org/plugins">https://www.fluentd.org/plugins</a>). It uses a <a id="_idIndexMarker1132"/>structured logging approach, allowing logs to be processed in JSON format for better searchability and indexing. Fluentd supports log enrichment by attaching K8s metadata, such as the namespace or Pod/container name, before sending logs to a storage system. It also supports log filtering, buffering, and compression, which helps in optimizing resource usage in large-scale K8s environments. In K8s logging pipelines, Fluentd is often used alongside Loki (for efficient log storage) or Elasticsearch (for full-text <span class="No-Break">log searching).</span></p>&#13;
			<p><strong class="bold">Fluent Bit</strong> (<a href="https://fluentbit.io/">https://fluentbit.io/</a>) is a lightweight <a id="_idIndexMarker1133"/>version of Fluentd, optimized for low-resource environments and edge computing with much smaller memory footprint requirements. The<a id="_idIndexMarker1134"/> following table <a id="_idIndexMarker1135"/>provides a high-level comparison of Fluentd and Fluent Bit, highlighting their key differences and typical <span class="No-Break">use cases:</span></p>&#13;
			<table id="table001-6" class="No-Table-Style _idGenTablePara-1">&#13;
				<colgroup>&#13;
					<col/>&#13;
					<col/>&#13;
					<col/>&#13;
				</colgroup>&#13;
				<tbody>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Features</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Fluentd</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Fluent Bit</strong></span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Resource usage</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Significant CPU and <span class="No-Break">memory usage</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Lightweight; minimal CPU and <span class="No-Break">memory usage</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Architecture</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Written in Ruby <span class="No-Break">and C</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Written <span class="No-Break">in C</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Plugin ecosystem</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Large plugin library with more than 1,000 <span class="No-Break">external plugins</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Over 100 <span class="No-Break">built-in plugins</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Deployment model</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Deployed as a K8s DaemonSet or sidecar with <span class="No-Break">multiple plugins</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Deployed as a K8s DaemonSet or sidecar with <span class="No-Break">multiple plugins</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Scalability</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Requires more resources <span class="No-Break">at scale</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Very scalable in environments with <span class="No-Break">constrained resources</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Use cases</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break">Containers/servers</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break">Embedded Linux/containers/servers</span></p>&#13;
						</td>&#13;
					</tr>&#13;
				</tbody>&#13;
			</table>&#13;
			<h2 id="_idParaDest-168"><a id="_idTextAnchor168"/>Loki</h2>&#13;
			<p><strong class="bold">Loki</strong> (<a href="https://github.com/grafana/loki">https://github.com/grafana/loki</a>) is a lightweight, scalable log aggregation system designed for the K8s environment. Unlike<a id="_idIndexMarker1136"/> traditional log management systems <a id="_idIndexMarker1137"/>such as Elasticsearch, Loki indexes only metadata (labels) instead of the full log content, making it efficient and cost-effective for large-scale deployments. Loki integrates with Prometheus and Grafana, allowing users to correlate logs with metrics for better troubleshooting <span class="No-Break">and observability.</span></p>&#13;
			<p>Loki collects logs from K8s Pods using agents such as Fluentd or Fluent Bit, which run as DaemonSets to forward logs to Loki for storage and querying, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.3</em>. This enables<a id="_idIndexMarker1138"/> developers and operators<a id="_idIndexMarker1139"/> to search logs efficiently using <strong class="bold">log query language</strong> (<strong class="bold">LogQL</strong>) (<a href="https://grafana.com/docs/loki/latest/query/">https://grafana.com/docs/loki/latest/query/</a>), filter logs <a id="_idIndexMarker1140"/>by namespace, Pod, or container, and<a id="_idIndexMarker1141"/> visualize them <span class="No-Break">in Grafana.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer092" class="IMG---Figure">&#13;
					<img src="image/B31108_12_03.jpg" alt="Figure 12.3 – Fluent Bit/Loki deployment in K8s" width="1360" height="1012"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – Fluent Bit/Loki deployment in K8s</p>&#13;
			<h2 id="_idParaDest-169"><a id="_idTextAnchor169"/>OpenTelemetry</h2>&#13;
			<p><strong class="bold">OTel</strong> is a collection of <a id="_idIndexMarker1142"/>APIs, SDKs, and tools that you can use to instrument, generate, collect, and export telemetry<a id="_idIndexMarker1143"/> data (metrics, logs, and traces). It is commonly used for collecting and exporting telemetry data to various backend services, such as Prometheus, Jaeger, Datadog, and <span class="No-Break">AWS X-Ray.</span></p>&#13;
			<p>In K8s, OTel enables unified observability by collecting metrics, traces, and logs, which can be collected from Pods, containers, or nodes. It supports multiple backends for data storage and works with auto-instrumentation for programming languages such as Go, Java, Python, and Node.js. Refer to the <a id="_idIndexMarker1144"/>OTel documentation at <a href="https://opentelemetry.io/docs/platforms/kubernetes/operator/automatic/">https://opentelemetry.io/docs/platforms/kubernetes/operator/automatic/</a> for a detailed walkthrough <span class="No-Break">of auto-instrumentation.</span></p>&#13;
			<p><strong class="bold">OTel collectors</strong> (<a href="https://opentelemetry.io/docs/collector/">https://opentelemetry.io/docs/collector/</a>) act as central agents that receive, process, and export telemetry <a id="_idIndexMarker1145"/>data. These collectors support all three<a id="_idIndexMarker1146"/> telemetry signals (metrics, traces, and logs), making it a powerful unified observability solution. <strong class="bold">OTel exporters</strong> (<a href="https://opentelemetry.io/docs/languages/go/exporters/">https://opentelemetry.io/docs/languages/go/exporters/</a>) are used to send collected data to backend<a id="_idIndexMarker1147"/> systems such as Prometheus, Jaeger, <span class="No-Break">and Datadog.</span></p>&#13;
			<p>To deploy OTel in K8s, one <a id="_idIndexMarker1148"/>can start by installing OTel collectors using a Helm chart and configure the collectors to define the receivers, processors, and exporters for telemetry data. Please visit the documentation at <a href="https://opentelemetry.io/docs/demo/kubernetes-deployment/">https://opentelemetry.io/docs/demo/kubernetes-deployment/</a> for OTel deployment in K8s. Alternatively, you can use the AWS distribution of the OTel project – <strong class="bold">AWS Distro for OpenTelemetry</strong> (<strong class="bold">ADOT</strong>) (<a href="https://aws-otel.github.io/">https://aws-otel.github.io/</a>) – for a secure, production-ready, open <a id="_idIndexMarker1149"/>source distribution with predictable performance. To deploy ADOT on Amazon EKS, install the ADOT-managed add-on and configure collectors to forward observability data to your preferred destinations. Refer to the ADOT documentation at <a href="https://aws-otel.github.io/docs/getting-started/adot-eks-add-on">https://aws-otel.github.io/docs/getting-started/adot-eks-add-on</a> for detailed <span class="No-Break">installation instructions.</span></p>&#13;
			<h2 id="_idParaDest-170"><a id="_idTextAnchor170"/>Prometheus</h2>&#13;
			<p><strong class="bold">Prometheus</strong> (<a href="https://prometheus.io/">https://prometheus.io/</a>) is an open<a id="_idIndexMarker1150"/> source monitoring<a id="_idIndexMarker1151"/> and alerting tool designed for collecting and querying time-series metrics. It was <a id="_idIndexMarker1152"/>originally built by <strong class="bold">SoundCloud</strong> (<a href="https://soundcloud.com/">https://soundcloud.com/</a>) in 2012, to address the challenges of dynamic and distributed systems, and<a id="_idIndexMarker1153"/> later joined the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>) in 2016 as<a id="_idIndexMarker1154"/> the second hosted project after K8s. Prometheus collects and stores metrics as time-series data, where each data point is recorded with a timestamp and key-value<a id="_idIndexMarker1155"/> pairs known as labels. It is widely used in K8s environments to provide real-time insights into system performance and resource utilization. In <a href="B31108_10.xhtml#_idTextAnchor128"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, we briefly discussed the Prometheus agent and adapters; let’s take a deeper <span class="No-Break">look now.</span></p>&#13;
			<p>The following diagram illustrates the high-level architecture of Prometheus and some of the key components in <span class="No-Break">its ecosystem:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer093" class="IMG---Figure">&#13;
					<img src="image/B31108_12_04.jpg" alt="Figure 12.4 – Prometheus architecture" width="1193" height="727"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Prometheus architecture</p>&#13;
			<p>The following are the key components <a id="_idIndexMarker1156"/>of the <span class="No-Break">Prometheus </span><span class="No-Break"><a id="_idIndexMarker1157"/></span><span class="No-Break">architecture:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Prometheus server</strong> (<a href="https://github.com/prometheus/prometheus">https://github.com/prometheus/prometheus</a>): The Prometheus server is a <a id="_idIndexMarker1158"/>central component that scrapes metrics from configured endpoints, such as nodes, Pods, or services. It stores these metrics in a time-series database, which can be queried using <strong class="bold">Prometheus Query Language</strong> (<strong class="bold">PromQL</strong>). Prometheus performs automatic target<a id="_idIndexMarker1159"/> discovery in K8s to simplify monitoring in dynamic, containerized environments. Instead of manually specifying the endpoints for monitoring, Prometheus uses K8s APIs to dynamically discover targets such as Pods, Endpoints, and Services. This ensures that as workloads scale or shift within the cluster, Prometheus automatically adjusts to continue monitoring them without requiring <span class="No-Break">configuration changes.</span></li>&#13;
				<li><strong class="bold">Prometheus exporters</strong> (<a href="https://prometheus.io/docs/instrumenting/exporters/">https://prometheus.io/docs/instrumenting/exporters/</a>): Prometheus exporters are libraries that <a id="_idIndexMarker1160"/>expose metrics in a format compatible with Prometheus. Exporters are essential for integrating Prometheus with systems or applications that do not natively expose metrics in the Prometheus format. They act as intermediaries, collecting data from the target system and exposing it on an HTTP endpoint for Prometheus to scrape. In K8s, exporters are widely used to monitor various components of the cluster, including nodes, applications, and external systems. For GenAI workloads, exporters are<a id="_idIndexMarker1161"/> especially critical for monitoring GPUs, inference latency, and resource utilization. <strong class="bold">NVIDIA DCGM exporter</strong> exposes GPU <a id="_idIndexMarker1162"/>metrics such as utilization, memory usage, temperature, and power consumption, critical for monitoring GPU workloads. We deployed this add-on on our EKS cluster setup in <a href="B31108_10.xhtml#_idTextAnchor128"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break">.</span></li>&#13;
				<li><strong class="bold">Prometheus Alertmanager</strong> (<a href="https://prometheus.io/docs/alerting/latest/overview/">https://prometheus.io/docs/alerting/latest/overview/</a>): Prometheus Alertmanager <a id="_idIndexMarker1163"/>handles alerts generated by <a id="_idIndexMarker1164"/>Prometheus, sending notifications to different channels such as <strong class="bold">Slack</strong> (<a href="https://slack.com/">https://slack.com/</a>), email, or <strong class="bold">PagerDuty</strong> (<a href="https://www.pagerduty.com/">https://www.pagerduty.com/</a>). It can <a id="_idIndexMarker1165"/>be configured for GenAI use cases such as resource saturation alerts or higher inference latency. Prometheus Alertmanager supports deduplication, which consolidates duplicate alerts generated by multiple Prometheus instances to prevent notification flooding, and alert grouping, which groups similar alerts into a single notification for <span class="No-Break">better readability.</span></li>&#13;
				<li><strong class="bold">Prometheus Pushgateway</strong> (<a href="https://prometheus.io/docs/instrumenting/pushing/">https://prometheus.io/docs/instrumenting/pushing/</a>): This is a component of the Prometheus ecosystem<a id="_idIndexMarker1166"/> that lets short-lived jobs push their metrics to Prometheus. This is especially useful for ephemeral workloads, such as short GenAI tasks, that cannot be directly scraped by Prometheus. In K8s environments, the Pushgateway acts as an intermediary, allowing batch jobs, cron jobs, and other transient workloads to publish metrics to a persistent endpoint. Prometheus then scrapes this endpoint at <span class="No-Break">regular intervals.</span></li>&#13;
				<li><strong class="bold">PromQL</strong> (<a href="https://prometheus.io/docs/prometheus/latest/querying/basics/">https://prometheus.io/docs/prometheus/latest/querying/basics/</a>): PromQL is a powerful <a id="_idIndexMarker1167"/>and flexible query language used to extract and analyze time-series data stored in the Prometheus ecosystem, such as the Prometheus server itself or tools that rely on Prometheus as a backend, such as Grafana. It allows users to perform computations on metrics, filter and aggregate them based on labels, and derive insights <span class="No-Break">through queries.</span></li>&#13;
			</ul>&#13;
			<p>Now that we have<a id="_idIndexMarker1168"/> covered the key components of the Prometheus stack, let’s discuss how to deploy it within the <span class="No-Break">K8s environment.</span></p>&#13;
			<h3>Deploying the Prometheus stack</h3>&#13;
			<p>In K8s, it is recommended to deploy the<a id="_idIndexMarker1169"/> Prometheus server as a <strong class="bold">StatefulSet</strong>. A StatefulSet is a K8s controller used to <a id="_idIndexMarker1170"/>manage stateful applications that require stable identities and persistent storage. Unlike Deployments, which treat Pods as interchangeable, StatefulSets assign each Pod a unique, stable hostname and ensure that storage volumes persist across Pod restarts. This ensures consistency and reliability for workloads that rely on maintaining state across restarts or rescheduling. Deploying the Prometheus server as a StatefulSet ensures that it has persistent<a id="_idIndexMarker1171"/> storage access for metrics, using a <span class="No-Break"><strong class="bold">PersistentVolumeClaim</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">PVC</strong></span><span class="No-Break">).</span></p>&#13;
			<p>To streamline the K8s setup, the Prometheus community has developed Helm charts to deploy all essential components, which are available at <a href="https://github.com/prometheus-community/helm-charts">https://github.com/prometheus-community/helm-charts</a>. In our setup, we<a id="_idIndexMarker1172"/> will deploy <strong class="source-inline">kube-prometheus-stack</strong> using the <strong class="bold">Terraform Helm provider</strong>. This Helm chart deploys Prometheus and Grafana instances in our EKS cluster, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.5</em>. After deployment, we will configure Prometheus to scrape metrics from various components within the cluster, such as the NVIDIA DCGM exporter, Qdrant vector database, Ray Serve deployments, and so on. Prometheus automatically discovers the relevant target endpoints and collects metrics at regular intervals. Using the Grafana web console, we can then visualize and query these metrics, build and import interactive dashboards, and define <span class="No-Break">alerting rules.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer094" class="IMG---Figure">&#13;
					<img src="image/B31108_12_05.jpg" alt="Figure 12.5 – Prometheus and Grafana setup in EKS cluster" width="1567" height="1241"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – Prometheus and Grafana setup in EKS cluster</p>&#13;
			<p>To get started, download <a id="_idIndexMarker1173"/>the <strong class="source-inline">addons.tf</strong> and <strong class="source-inline">kube-prometheus.yaml</strong> files from the GitHub repository at <a href="https://github.com/PacktPublishing/Kubernetes-for-Gen-AI-Models/tree/main/ch12/">https://github.com/PacktPublishing/Kubernetes-for-Gen-AI-Models/tree/main/ch12/</a>. The <strong class="source-inline">kube-prometheus-stack</strong> Helm chart, along with <strong class="source-inline">prometheus-adapter</strong>, will be deployed in the <em class="italic">monitoring</em> namespace, as shown in the following <span class="No-Break">code snippet:</span></p>&#13;
			<pre class="source-code">&#13;
module "eks_blueprints_addons" {&#13;
  source = "aws-ia/eks-blueprints-addons/aws"&#13;
  ...&#13;
  <strong class="bold">enable_kube_prometheus_stack = true</strong>&#13;
  <strong class="bold">kube_prometheus_stack</strong> = {&#13;
    namespace = "monitoring"&#13;
    values = [&#13;
      templatefile("${path.module}/kube-prometheus.yaml", {&#13;
        storage_class_type = kubernetes_storage_class.default_gp3.id&#13;
      })]&#13;
  ...&#13;
  helm_releases = {&#13;
    "<strong class="bold">prometheus-adapter</strong>" = {&#13;
      repository = "https://prometheus-community.github.io/helm-charts"&#13;
      chart      = "prometheus-adapter"&#13;
      namespace = module.eks_blueprints_addons.kube_prometheus_stack.namespace&#13;
...</pre>			<p>The Prometheus setup is customized using the <strong class="source-inline">kube-prometheus.yaml</strong> Helm values file. It does <a id="_idIndexMarker1174"/>the <span class="No-Break">following things:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break"><strong class="bold">Prometheus configuration</strong></span><span class="No-Break">:</span><ul><li>Configures the scrape interval and evaluation interval to <span class="No-Break">30 seconds</span></li><li>Specifies a scrape timeout of 10 seconds <span class="No-Break">for targets</span></li><li>Sets a data retention of 5 hours and provisions 50Gi of PVC of the <strong class="source-inline">gp3</strong> <span class="No-Break">storage class</span></li></ul></li>&#13;
				<li><span class="No-Break"><strong class="bold">Alertmanager configuration</strong></span><span class="No-Break">:</span><ul><li>Disables the Alertmanager component in this deployment; you can enable it by setting the <strong class="source-inline">alertmanager.enabled</strong> value <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">true</strong></span></li></ul></li>&#13;
				<li><span class="No-Break"><strong class="bold">Grafana configuration</strong></span><span class="No-Break">:</span><ul><li>Deploys Grafana and enables the default <span class="No-Break">monitoring dashboards</span></li></ul></li>&#13;
			</ul>&#13;
			<p>Execute the following <strong class="source-inline">terraform</strong> commands to deploy the <strong class="source-inline">kube-prometheus-stack</strong> and <strong class="source-inline">prometheus-adapter</strong> Helm charts in the <span class="No-Break">EKS cluster:</span></p>&#13;
			<pre class="console">&#13;
$ terraform init&#13;
$ terraform plan&#13;
$ terraform apply -auto-approve</pre>			<p>Verify the installation <a id="_idIndexMarker1175"/>using the following <strong class="source-inline">kubectl</strong> command. The output should confirm that the Prometheus StatefulSet, node exporter DaemonSet, Grafana, <strong class="source-inline">kube-state-metrics</strong>, <strong class="source-inline">prometheus-adapter</strong>, and <strong class="source-inline">prometheus-operator</strong> deployments are in a <span class="No-Break"><strong class="bold">READY</strong></span><span class="No-Break"> status:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl get ds -n monitoring&#13;
NAME                                             READY&#13;
kube-prometheus-stack-prometheus-node-exporter   6&#13;
$ kubectl get deploy -n monitoring&#13;
NAME                                       READY&#13;
kube-prometheus-stack-grafana              1/1&#13;
kube-prometheus-stack-kube-state-metrics   1/1&#13;
kube-prometheus-stack-operator             1/1&#13;
prometheus-adapter                         1/1&#13;
$ kubectl get sts -n monitoring&#13;
NAME                                          READY&#13;
prometheus-kube-prometheus-stack-prometheus   1/1</pre>			<p>In larger environments, it is recommended to assign sufficient CPU and memory resources and use appropriate scrape intervals for different metrics to optimize resource usage. For example, for critical metrics such as CPU or memory utilization, it is recommended to use intervals of 10 to 15 seconds, whereas for less critical metrics, this interval could be <span class="No-Break">in minutes.</span></p>&#13;
			<p>In this section, we covered how to deploy the Prometheus stack in an EKS cluster. However, to gain deeper insights into GPU performance (critical for AI/ML workloads), we need to enable metric <a id="_idIndexMarker1176"/>scraping for GPUs. In the next section, we’ll cover integrating NVIDIA’s DCGM exporter with Prometheus by configuring <span class="No-Break">Service monitors.</span></p>&#13;
			<h3>Enabling GPU monitoring</h3>&#13;
			<p>In <a href="B31108_10.xhtml#_idTextAnchor128"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, we deployed the <strong class="bold">NVIDIA DCGM Exporter add-on</strong> to gain visibility into GPU utilization metrics. During the <a id="_idIndexMarker1177"/>setup, we disabled the <a id="_idIndexMarker1178"/>service monitors that enable Prometheus to scrape the metrics at regular intervals. <strong class="bold">Prometheus Service Monitors</strong> (<a href="https://prometheus-operator.dev/docs/developer/getting-started/#using-servicemonitors">https://prometheus-operator.dev/docs/developer/getting-started/#using-servicemonitors</a>) and <strong class="bold">Pod Monitors</strong> (<a href="https://prometheus-operator.dev/docs/developer/getting-started/#using-podmonitors">https://prometheus-operator.dev/docs/developer/getting-started/#using-podmonitors</a>) are <strong class="bold">CustomResourceDefinitions</strong> (<strong class="bold">CRDs</strong>) that allow the Prometheus Operator<a id="_idIndexMarker1179"/> to automatically discover and configure monitoring<a id="_idIndexMarker1180"/> targets within a K8s cluster. By leveraging label selectors on K8s services and Pods, these monitors streamline the process of <span class="No-Break">collecting metrics.</span></p>&#13;
			<p>Let’s update the Terraform code to enable the scraping of <strong class="source-inline">dcgm-exporter</strong> metrics using Service Monitors. Begin by downloading <strong class="source-inline">aiml-addons.tf</strong> from the GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch12/aiml-addons.tf"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch12/aiml-addons.tf</span></a><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
resource "helm_release" "dcgm_exporter" {&#13;
  name       = "dcgm-exporter"&#13;
...&#13;
  values = [&#13;
      &lt;&lt;-EOT&#13;
        <strong class="bold">serviceMonitor:</strong>&#13;
          <strong class="bold">enabled: true</strong></pre>			<p>Execute the following <strong class="source-inline">terraform</strong> commands to update the NVIDIA DCGM exporter Helm chart in the <span class="No-Break">EKS cluster:</span></p>&#13;
			<pre class="console">&#13;
$ terraform init&#13;
$ terraform plan&#13;
$ terraform apply -auto-approve</pre>			<p>We can verify the scraping status by launching the Prometheus dashboard and checking the scraping target’s health. By default, the Prometheus service is internal and not exposed outside of the cluster, so let’s use the <strong class="source-inline">kubectl port-forward</strong> mechanism to connect to it from the local machine. Run the following command to initiate a port-forward connection<a id="_idIndexMarker1181"/> from local port <strong class="source-inline">9090</strong> to Prometheus service port <strong class="source-inline">9090</strong> in the <span class="No-Break"><strong class="source-inline">monitoring</strong></span><span class="No-Break"> namespace:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl port-forward svc/kube-prometheus-stack-prometheus -n monitoring 9090:9090&#13;
Forwarding from 127.0.0.1:9090 -&gt; 9090&#13;
Forwarding from [::1]:9090 -&gt; 9090</pre>			<p>Now, launch the <a href="http://localhost:9090/targets?search=dcgm-exporter">http://localhost:9090/targets?search=dcgm-exporter</a> URL in your browser to check the health of the DCGM exporter targets. <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.6</em> shows that all three <strong class="source-inline">dcgm-exporter</strong> endpoints are in the <strong class="bold">UP</strong> state, with each corresponding to a GPU worker node in <span class="No-Break">the cluster:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer095" class="IMG---Figure">&#13;
					<img src="image/B31108_12_06.jpg" alt="Figure 12.6 – Prometheus target health status" width="1211" height="553"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – Prometheus target health status</p>&#13;
			<p>Next, we can query the <strong class="source-inline">dcgm-exporter</strong> metrics using PromQL. For example, <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.7</em> shows the average GPU utilization across multiple GPUs over the past minute grouped by the K8s Pod, where <strong class="source-inline">DCGM_FI_DEV_GPU_UTIL</strong> is the metric exposed <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">dcgm-exporter</strong></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer096" class="IMG---Figure">&#13;
					<img src="image/B31108_12_07.jpg" alt="Figure 12.7 – Querying GPU metrics using PromQL" width="1148" height="504"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – Querying GPU metrics using PromQL</p>&#13;
			<p>Similarly, we will create <a id="_idIndexMarker1182"/>additional Service Monitors to collect metrics from both the Ray Serve cluster deployed in <a href="B31108_11.xhtml#_idTextAnchor145"><span class="No-Break"><em class="italic">Chapter 11</em></span></a> and various e-commerce chatbot application components. You can download the corresponding K8s manifest files from the GitHub repository at <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch12/monitoring">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch12/monitoring</a> and apply each file using the <strong class="source-inline">kubectl apply -f</strong> command to deploy them to the <span class="No-Break">EKS cluster:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl apply -f &lt;replace with service monitor file name&gt;</pre>			<p>After applying the files, navigate to the Prometheus dashboard and verify the status of the newly discovered scraping targets. You will notice both <strong class="source-inline">ray-workers</strong> and <strong class="source-inline">ray-head</strong> Pod targets in the dashboard, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer097" class="IMG---Figure">&#13;
					<img src="image/B31108_12_08.jpg" alt="Figure 12.8 – Prometheus target health status" width="1211" height="335"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – Prometheus target health status</p>&#13;
			<p>In this section, we explored various tools to monitor GenAI applications in K8s, such as Fluentd, Fluent Bit, Loki, OTel, and Prometheus. We also deployed <strong class="source-inline">kube-prometheus-stack</strong> in the EKS<a id="_idIndexMarker1183"/> cluster and set up the Service/Pod monitors to collect metrics from <strong class="source-inline">dcgm-exporter</strong> and other application components in our EKS cluster. In the next section, we will dive into visualization tools such as Grafana to view <span class="No-Break">these metrics.</span></p>&#13;
			<h1 id="_idParaDest-171"><a id="_idTextAnchor171"/>Visualization and debugging</h1>&#13;
			<p>In this section, we will explore key practices for enhancing the observability of GenAI applications deployed on K8s. We will begin by demonstrating how Grafana can be used to visualize essential metrics such as GPU utilization, system performance, and the status of GenAI applications, to provide real-time operational insights. Additionally, we will dive into debugging strategies for Gen AI workloads by examining tools such as <strong class="bold">Langfuse</strong> (<a href="https://github.com/langfuse/langfuse">https://github.com/langfuse/langfuse</a>), an open <a id="_idIndexMarker1184"/>source LLM engineering platform designed to aid in debugging and the analysis of <span class="No-Break">LLM applications.</span></p>&#13;
			<h2 id="_idParaDest-172"><a id="_idTextAnchor172"/>Grafana</h2>&#13;
			<p><strong class="bold">Grafana</strong> (<a href="https://grafana.com/">https://grafana.com/</a>) is an open <a id="_idIndexMarker1185"/>source visualization and analytics platform widely used for monitoring K8s environments. It provides a centralized interface for querying, visualizing, and alerting on metrics collected from various data sources, such as Prometheus, Amazon CloudWatch, and <span class="No-Break">Azure Monitor.</span></p>&#13;
			<p>Grafana provides prebuilt and customizable dashboards to monitor K8s components such as API servers, etcd databases, nodes, Pods, and namespaces. These dashboards help visualize metrics such as CPU and memory utilization, network activity, and application-specific metrics. Grafana allows users to configure alerts with notifications sent to channels such as Slack, email, PagerDuty, or <strong class="bold">Microsoft Teams</strong>. Alerts are typically based on threshold<a id="_idIndexMarker1186"/> conditions, though you can also implement anomaly detection through custom queries or <span class="No-Break">external integrations.</span></p>&#13;
			<p>Grafana provides the ability to define roles <a id="_idIndexMarker1187"/>and permissions using <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>), allowing fine-grained control over who can <a id="_idIndexMarker1188"/>view or edit dashboards and alerts. Grafana supports a wide range of community-contributed plugins (<a href="https://grafana.com/grafana/plugins/">https://grafana.com/grafana/plugins/</a>), custom <a id="_idIndexMarker1189"/>visualization panels (<a href="https://grafana.com/grafana/plugins/panel-plugins/">https://grafana.com/grafana/plugins/panel-plugins/</a>), and dashboards (<a href="https://grafana.com/grafana/dashboards/">https://grafana.com/grafana/dashboards/</a>), enabling users<a id="_idIndexMarker1190"/> to extend its<a id="_idIndexMarker1191"/> functionality and adapt it to specific <span class="No-Break">use cases.</span></p>&#13;
			<h3>Grafana best practices</h3>&#13;
			<p>The following lists some Grafana <a id="_idIndexMarker1192"/><span class="No-Break">best practices:</span></p>&#13;
			<ul>&#13;
				<li>Ensure Grafana dashboards and settings persist across Pod restarts by using a <strong class="bold">Persistent Volume</strong> (<strong class="bold">PV</strong>) and a PVC. This <a id="_idIndexMarker1193"/>helps maintain the state even when Grafana Pods <span class="No-Break">are rescheduled.</span></li>&#13;
				<li>Automate dashboard provisioning <a id="_idIndexMarker1194"/>using ConfigMaps or <strong class="bold">infrastructure as code</strong> (<strong class="bold">IaC</strong>) tools, such as Terraform, to maintain consistent observability setups <span class="No-Break">across environments.</span></li>&#13;
				<li>Enable robust authentication mechanisms (<a href="https://grafana.com/docs/grafana/latest/setup-grafana/configure-security/configure-authentication/">https://grafana.com/docs/grafana/latest/setup-grafana/configure-security/configure-authentication/</a>) such as <a id="_idIndexMarker1195"/>OAuth, SAML, and LDAP, to control user access. Use RBAC to manage user <span class="No-Break">permissions effectively.</span></li>&#13;
				<li>When exposing the Grafana dashboard outside the cluster, use an Ingress controller with TLS termination to secure <span class="No-Break">network communications.</span></li>&#13;
				<li>Leverage Grafana’s rich plugin ecosystem to integrate with external data sources and <span class="No-Break">specialized visualizations.</span></li>&#13;
				<li>Enhance observability by combining metrics, logs, and traces. Integrate Grafana with Loki for centralized K8s logging alongside <span class="No-Break">metrics visualization.</span></li>&#13;
				<li>Monitor Grafana’s resource usage and performance within the K8s cluster. This includes setting up alerts for abnormal behavior, which helps maintain optimal performance <span class="No-Break">and availability.</span></li>&#13;
				<li>Use managed Grafana<a id="_idIndexMarker1196"/> offerings such as <strong class="bold">Amazon Managed Grafana</strong> (<a href="https://aws.amazon.com/grafana/">https://aws.amazon.com/grafana/</a>), <strong class="bold">Grafana Cloud</strong> (<a href="https://grafana.com/products/cloud/">https://grafana.com/products/cloud/</a>), and <strong class="bold">Azure Managed Grafana</strong> (<a href="https://azure.microsoft.com/en-us/products/managed-grafana">https://azure.microsoft.com/en-us/products/managed-grafana</a>) to offload <a id="_idIndexMarker1197"/>operational<a id="_idIndexMarker1198"/> tasks such as scaling, patching, and security management. This enables us to focus on creating dashboards and analyzing data. These services also provide seamless<a id="_idIndexMarker1199"/> cloud-native integrations, auto-scaling capabilities, and <span class="No-Break">cost efficiencies.</span></li>&#13;
			</ul>&#13;
			<h3>Setting up Grafana dashboards</h3>&#13;
			<p>In our setup, we deployed Grafana as part of the <strong class="source-inline">kube-prometheus-stack</strong> installation earlier in<a id="_idIndexMarker1200"/> this chapter. Alternatively, Grafana Helm charts can be leveraged to deploy as a standalone option in K8s; refer to <a href="https://grafana.com/docs/grafana/latest/setup-grafana/installation/helm/">https://grafana.com/docs/grafana/latest/setup-grafana/installation/helm/</a> for step-by-step instructions. By default, the Grafana service is accessible within the cluster unless it is exposed outside via the K8s <strong class="source-inline">LoadBalancer</strong> service type or an Ingress resource. So, let’s use the <strong class="source-inline">kubectl port-forward</strong> mechanism to connect to the <span class="No-Break">Grafana console:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl port-forward svc/kube-prometheus-stack-grafana -n monitoring 8080:80&#13;
Forwarding from 127.0.0.1:8080 -&gt; 80&#13;
Forwarding from [::1]:8080 -&gt; 80</pre>			<p>Now, open <a href="http://localhost:8080/">http://localhost:8080/</a> in your browser to access the Grafana console. When prompted for credentials, note that a default admin user is automatically created during the Helm chart installation. You can retrieve the credentials from the K8s secret named <strong class="source-inline">kube-prometheus-stack-grafana</strong> in the <strong class="source-inline">monitoring</strong> namespace by running the <span class="No-Break">following command:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl get secret kube-prometheus-stack-grafana -n monitoring -o go-template='{{printf "Username: %s\nPassword: %s\n" (index .data "admin-user" | base64decode) (index .data "admin-password" | base64decode)}}'</pre>			<p>Once logged in to the Grafana console, navigate to <strong class="bold">Connections</strong> | <strong class="bold">Data sources</strong> in the left side menu bar to view and manage the connected data sources. You will notice the local Prometheus server is already added to the data sources, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer098" class="IMG---Figure">&#13;
					<img src="image/B31108_12_09.jpg" alt="Figure 12.9 – Grafana connected data sources" width="1212" height="390"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.9 – Grafana connected data sources</p>&#13;
			<p>As Grafana has access <a id="_idIndexMarker1201"/>to Prometheus metrics, let’s start exploring the Grafana dashboards. Navigate to <strong class="bold">Dashboards</strong> in the left side menu bar to view and manage the Grafana dashboards. You will notice a list of pre-existing dashboards that were already created by our <strong class="source-inline">kube-prometheus-stack</strong> installation, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer099" class="IMG---Figure">&#13;
					<img src="image/B31108_12_10.jpg" alt="Figure 12.10 – Grafana dashboards list" width="963" height="592"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.10 – Grafana dashboards list</p>&#13;
			<p>These Grafana dashboards provide preconfigured monitoring views tailored for various K8s components, such as <strong class="bold">CoreDNS</strong>, <strong class="bold">API server</strong>, <strong class="bold">Namespace</strong>, <strong class="bold">Pod</strong>, <strong class="bold">Workload</strong>, <strong class="bold">Node</strong>, <strong class="bold">Scheduler</strong>, <strong class="bold">Controller Manager</strong>, and <strong class="bold">kubelet</strong>. You can select and explore these dashboards to gain insights into the performance, resource usage, and health of the K8s environments. For example, select the <strong class="bold">Kubernetes</strong> | <strong class="bold">Compute Resources</strong> | <strong class="bold">Cluster </strong>dashboard to <a id="_idIndexMarker1202"/>view an overview of resource utilization, including CPU, memory, and storage metrics across the entire K8s cluster, helping you monitor and optimize cluster performance, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.11</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer100" class="IMG---Figure">&#13;
					<img src="image/B31108_12_11.jpg" alt="Figure 12.11 – Kubernetes Compute Resources dashboard" width="1209" height="1073"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.11 – Kubernetes Compute Resources dashboard</p>&#13;
			<p>In this dashboard, we can see the CPU, memory quota, and usage metrics aggregated by each K8s namespace at the <span class="No-Break">cluster level.</span></p>&#13;
			<p>While the default dashboards offer comprehensive insights into core K8s components, you may also need visibility into <a id="_idIndexMarker1203"/>specialized resources, depending on your workload. Now, let’s take a look at how to visualize GPU metrics using the NVIDIA DCGM <span class="No-Break">exporter dashboard.</span></p>&#13;
			<h4>NVIDIA DCGM dashboard</h4>&#13;
			<p>Earlier in this chapter, we<a id="_idIndexMarker1204"/> enabled metrics collection from the NVIDIA DCGM exporter add-on using Prometheus Service Monitor resources. Now, we will visualize these metrics using Grafana dashboards. NVIDIA published a Grafana dashboard at <a href="https://grafana.com/grafana/dashboards/12239-nvidia-dcgm-exporter-dashboard/">https://grafana.com/grafana/dashboards/12239-nvidia-dcgm-exporter-dashboard/</a> to monitor GPU <span class="No-Break">utilization metrics.</span></p>&#13;
			<p>You can import this dashboard to your Grafana instance using the instructions at <a href="https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/import-dashboards/">https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/import-dashboards/</a>. Once the import is successful, you will be able to visualize the GPU metrics, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer101" class="IMG---Figure">&#13;
					<img src="image/B31108_12_12.jpg" alt="Figure 12.12 – DCGM exporter Grafana dashboard" width="1209" height="983"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.12 – DCGM exporter Grafana dashboard</p>&#13;
			<p>This dashboard provides real-time visibility into key GPU performance metrics such as temperature, power usage, clock speeds, and utilization. With this information, you can quickly identify performance bottlenecks, detect potential issues, and optimize <span class="No-Break">resource usage.</span></p>&#13;
			<p>In addition to dashboards, we <a id="_idIndexMarker1205"/>can also define Prometheus alerting rules to monitor GPU health and performance. For example, we can create a <strong class="bold">Prometheus rule</strong> (<a href="https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/">https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/</a>) to trigger alerts based on various<a id="_idIndexMarker1206"/> conditions such as high and low GPU utilization, elevated GPU temperature, or critical GPU errors. To create these rules, download <strong class="source-inline">gpu-rules.yaml</strong> from our GitHub repository at <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch12/monitoring/gpu-rules.yaml">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch12/monitoring/gpu-rules.yaml</a> and run the following command to configure them in <span class="No-Break">our setup:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl apply -f gpu-rules.yaml</pre>			<p>Once deployed, we<a id="_idIndexMarker1207"/> can visualize the rules in the Prometheus or Grafana console. In the Grafana console, navigate to <strong class="bold">Alerting</strong> | <strong class="bold">Alert rules</strong> in the left side menu bar to view the status of the alert rules, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer102" class="IMG---Figure">&#13;
					<img src="image/B31108_12_13.jpg" alt="Figure 12.13 – NVIDIA GPU alert rules in the Grafana console" width="1211" height="417"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.13 – NVIDIA GPU alert rules in the Grafana console</p>&#13;
			<p>As shown in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.13</em>, one of the GPU alerting rules is in a <strong class="bold">Firing</strong> state due to low GPU utilization on one of our worker nodes. To investigate further, we can expand the alert rule to view detailed information such as the worker node, GPU identifier, and the associated K8s Pod, as illustrated in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer103" class="IMG---Figure">&#13;
					<img src="image/B31108_12_14.jpg" alt="Figure 12.14 – GPU alert rule details in the Grafana console" width="1209" height="463"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.14 – GPU alert rule details in the Grafana console</p>&#13;
			<p>While the DCGM dashboard provides deep visibility into GPU performance, it’s also important to monitor the higher-level services that rely on these resources, especially in GenAI workloads. One such example is Ray Serve, which plays a key role in serving models such as Llama 3 in<a id="_idIndexMarker1208"/> our deployment. Let’s now set up a dedicated Grafana dashboard to monitor the performance and resource usage of Ray <span class="No-Break">Serve components.</span></p>&#13;
			<h4>Ray Serve dashboard</h4>&#13;
			<p>In <a href="B31108_11.xhtml#_idTextAnchor145"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, we deployed a Ray cluster in our EKS cluster and used the Ray Serve framework to expose the<a id="_idIndexMarker1209"/> Llama 3 model. Earlier in this chapter, we also created Prometheus Service and Pod Monitor resources to gather metrics from both the Ray cluster and Ray Serve deployments. Now, we will create a <strong class="bold">Ray monitoring dashboard</strong> to visualize key metrics such as request throughput, latency, ongoing HTTP connections, and resource utilization. To do this, download the Grafana dashboard JSON file (<strong class="source-inline">ray-serve-dashboard.json</strong>) from our GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch12/dashboards"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch12/dashboards</span></a><span class="No-Break">.</span></p>&#13;
			<p>Open the Grafana console, navigate to the <strong class="bold">Dashboards</strong> page, and choose the <strong class="bold">New</strong> | <strong class="bold">Import</strong> option. Upload the <strong class="source-inline">ray-serve-dashboard.json</strong> file from your local filesystem and click the <strong class="bold">Import</strong> button. Once imported, select <strong class="bold">Ray Serve Dashboard</strong> from the dashboards list to view real-time information about your Ray Serve deployments, helping you identify bottlenecks and optimize performance, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.15</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer104" class="IMG---Figure">&#13;
					<img src="image/B31108_12_15.jpg" alt="Figure 12.15 – Ray Serve Grafana dashboard" width="1211" height="1054"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.15 – Ray Serve Grafana dashboard</p>&#13;
			<p>Just like GPU alerting rules in the previous section, we can also define Prometheus alerting rules to monitor the health and performance of Ray Serve deployments. For example, we can<a id="_idIndexMarker1210"/> configure a Prometheus rule to trigger alerts based on conditions such as high error rates, increased latency, Ray worker node failures, response latency spikes, or low throughput. To create these rules, download the <strong class="source-inline">ray-serve-rules.yaml</strong> file from our GitHub repository at <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch12/monitoring/ray-serve-rules.yaml">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch12/monitoring/ray-serve-rules.yaml</a> and run the following command to configure them in <span class="No-Break">our setup:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl apply -f ray-serve-rules.yaml</pre>			<p>Once deployed, we can visualize the rules in the Prometheus or Grafana console. In the Grafana console, navigate to <strong class="bold">Alerting</strong> | <strong class="bold">Alert rules</strong> in the left side menu bar to view the status of the alert rules, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.16</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer105" class="IMG---Figure">&#13;
					<img src="image/B31108_12_16.jpg" alt="Figure 12.16 – Ray Serve alert rules in Grafana" width="1211" height="403"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.16 – Ray Serve alert rules in Grafana</p>&#13;
			<p>As we have covered<a id="_idIndexMarker1211"/> the different observability tools for monitoring GenAI workloads in K8s, let’s now explore how to extend these concepts for GenAI frameworks, such <span class="No-Break">as LangChain.</span></p>&#13;
			<h2 id="_idParaDest-173"><a id="_idTextAnchor173"/>LangChain observability</h2>&#13;
			<p><strong class="bold">LangChain</strong> (<a href="https://github.com/langchain-ai/langchain">https://github.com/langchain-ai/langchain</a>) is a framework for building applications with LLMs, which we covered in <a href="B31108_04.xhtml#_idTextAnchor049"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>. It integrates with various tools to enable observability<a id="_idIndexMarker1212"/> <span class="No-Break">and debugging.</span></p>&#13;
			<p>LangChain provides built-in capabilities to log and trace the execution of chains, agents, and tools. These features allow developers and operators to understand how prompts, responses, and workflows behave during<a id="_idIndexMarker1213"/> execution. The <strong class="bold">verbose mode</strong> in LangChain enables detailed logs of intermediate steps, such as input prompts, output responses, and tool invocations, by setting <strong class="source-inline">verbose=True</strong> in chains, agents, <span class="No-Break">or tools.</span></p>&#13;
			<p>LangChain can integrate<a id="_idIndexMarker1214"/> with <strong class="bold">LangChainTracer</strong> to collect execution data, including steps, timing information, errors, and retries. The tracer can be <a id="_idIndexMarker1215"/>used via <strong class="bold">LangSmith</strong> (<a href="https://www.langchain.com/langsmith">https://www.langchain.com/langsmith</a>) or deployed as a self-hosted server <span class="No-Break">in K8s.</span></p>&#13;
			<p>The following code snippet defines a custom debugging and observability callback handler for LangChain, which helps track the execution flow of a chain. It logs when a chain starts, when it completes, and <a id="_idIndexMarker1216"/>the time taken for execution, and handles <span class="No-Break">errors gracefully:</span></p>&#13;
			<pre class="source-code">&#13;
import time&#13;
from langchain.callbacks.base import BaseCallbackHandler&#13;
from langchain.chains import LLMChain&#13;
from langchain_openai import OpenAI&#13;
from langchain.prompts import PromptTemplate&#13;
class DebugCallbackHandler(BaseCallbackHandler):&#13;
    def __init__(self):&#13;
        self.start_time = None&#13;
    def on_chain_start(self, inputs, **kwargs):&#13;
        """Triggered when the chain starts execution."""&#13;
        self.start_time = time.time()&#13;
        print("\n[DEBUG] Chain started...")&#13;
        print(f" Inputs: {inputs}")&#13;
    def on_chain_end(self, outputs, **kwargs):&#13;
        """Triggered when the chain successfully completes execution."""&#13;
        elapsed_time = time.time() - self.start_time&#13;
        print(f"\n[DEBUG] Chain finished in {elapsed_time:.2f} seconds")&#13;
        print(f"Outputs: {outputs}")&#13;
    def on_chain_error(self, error, **kwargs):&#13;
        print("\n [ERROR] Chain encountered an error!")&#13;
        print(f"  Error Message: {error}")&#13;
debug_handler = DebugCallbackHandler()</pre>			<p>The <strong class="source-inline">DebugCallbackHandler</strong> class extends <strong class="source-inline">BaseCallbackHandler</strong>, making it compatible with LangChain’s callback system. The constructor initializes a <strong class="source-inline">self.start_time</strong> variable, which is used to track the <span class="No-Break">execution duration.</span></p>&#13;
			<p>The <strong class="source-inline">on_chain_error</strong> method is called when an error occurs during chain execution. It prints an error message along with details about the <span class="No-Break">encountered exception.</span></p>&#13;
			<p>This callback handler is useful for debugging and performance monitoring in LangChain applications by providing real-time logs for execution tracking, timing analysis, and <span class="No-Break">error handling.</span></p>&#13;
			<p>While LangChain’s built-in<a id="_idIndexMarker1217"/> logging and tracing capabilities provide a solid foundation for understanding the internal workings of LLM chains and agents, there are scenarios where more advanced observability tools are needed, especially for production-grade applications. This is where platforms such as LangFuse come into play, offering richer insights, distributed tracing, and powerful dashboards tailored for LLM workflows. Let’s take a closer look at how LangFuse enhances observability for GenAI applications in the <span class="No-Break">next section.</span></p>&#13;
			<h2 id="_idParaDest-174"><a id="_idTextAnchor174"/>LangFuse</h2>&#13;
			<p><strong class="bold">LangFuse</strong> (<a href="https://langfuse.com/">https://langfuse.com/</a>) is an open source observability and monitoring platform tailored for<a id="_idIndexMarker1218"/> LLM applications. It provides deep insights into the execution of AI workflows by tracking user interactions, prompts, responses, and application performance. LangFuse supports key observability features such as logging, tracing, metrics collection, and visualization, making it invaluable for debugging and optimizing <span class="No-Break">LLM-based applications.</span></p>&#13;
			<p>LangFuse benefits from K8s-native capabilities such as scalability, auto-healing, and seamless integration with managed services. LangFuse collects and visualizes critical metrics and traces related to prompts, model latency, response accuracy, and system health. It supports distributed tracing, allowing developers to trace user interactions across multiple components, such as API gateways, vector databases, and LLM endpoints, to diagnose performance bottlenecks <span class="No-Break">or errors.</span></p>&#13;
			<p>Key features of LangFuse include <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li>It logs detailed information about requests, including the input prompt, LLM-generated responses, and associated metadata such as token usage and <span class="No-Break">model-specific parameters</span></li>&#13;
				<li>It captures the<a id="_idIndexMarker1219"/> end-to-end lifecycle of interactions, enabling you to monitor every step in workflows, from user input to database queries and <span class="No-Break">LLM outputs</span></li>&#13;
				<li>It provides<a id="_idIndexMarker1220"/> interactive dashboards to visualize system performance, latency trends, error rates, and other <strong class="bold">key performance </strong><span class="No-Break"><strong class="bold">indicators</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">KPIs</strong></span><span class="No-Break">)</span></li>&#13;
				<li>It links errors or delays in AI pipelines to specific users, prompts, or workflows for faster debugging <span class="No-Break">and resolution</span></li>&#13;
				<li>It easily integrates with Prometheus, Grafana, OTel, and other K8s monitoring tools to enhance existing <span class="No-Break">observability stacks</span></li>&#13;
			</ul>&#13;
			<p>For K8s deployments, LangFuse offers flexibility in deployment configurations, enabling you to run the observability stack alongside your AI workloads. It is compatible with Helm charts, ensuring smooth deployment and configuration in cloud-native environments. Detailed deployment instructions and configurations for K8s and EKS are available in the LangFuse documentation <span class="No-Break">at </span><a href="https://langfuse.com/self-hosting/kubernetes-helm"><span class="No-Break">https://langfuse.com/self-hosting/kubernetes-helm</span></a><span class="No-Break">.</span></p>&#13;
			<p>In this section, we explored various visualization and debugging tools for monitoring GenAI applications in K8s, including Grafana, LangChain, and LangFuse. We deployed Grafana in our EKS cluster and imported prebuilt dashboards to view the key performance metrics of various components, such as the API server, Ray Serve deployments, and so on. Additionally, LangChain and LangFuse provide advanced debugging and observability features for GenAI workloads, enabling you to trace LLM calls, monitor model outputs, and optimize <span class="No-Break">prompt configurations.</span></p>&#13;
			<h1 id="_idParaDest-175"><a id="_idTextAnchor175"/>Summary</h1>&#13;
			<p>In this chapter, we covered key observability concepts for monitoring GenAI applications in K8s. We understood why monitoring is critical for optimizing GenAI workloads, examining both system-level metrics and application-specific signals. We explored a comprehensive monitoring framework using tools such as Prometheus for metrics collection, Grafana for visualization, and LangFuse and LangChain <span class="No-Break">for debugging.</span></p>&#13;
			<p>In K8s, various tools cater to different facets of the observability framework. Prometheus excels at collecting and querying time-series metrics, offering built-in alerting capabilities and seamless integration with K8s. Fluentd and Fluent Bit serve as a unified logging layer, collecting data from diverse sources and routing it to multiple destinations. OpenTelemetry provides a vendor-neutral set of APIs and libraries for generating and processing telemetry data, spanning metrics, logs, <span class="No-Break">and traces.</span></p>&#13;
			<p>Grafana provides an intuitive interface to view and analyze metrics, logs, and traces, making it easy to detect anomalies and investigate performance bottlenecks. LangFuse specializes in detailed logging and observability of LLM-based requests, capturing prompts, responses, and metadata to facilitate faster debugging. LangChain offers a framework for orchestrating and experimenting with LLM workflows, helping us better understand and refine prompt engineering, chaining logic, and <span class="No-Break">model responses.</span></p>&#13;
			<p>In the next chapter, we will explore how to set up high availability and disaster recovery for GenAI applications <span class="No-Break">on K8s.</span></p>&#13;
		</div>&#13;
	</div></div></body></html>