<html><head></head><body>
  <div id="_idContainer263" class="Basic-Text-Frame">
    <h1 class="chapterNumber">13</h1>
    <h1 id="_idParaDest-609" class="chapterTitle">Monitoring Kubernetes Clusters</h1>
    <p class="normal">In the previous chapter, we looked at serverless computing and its manifestations on Kubernetes. A lot of innovation happens in this space, and it is both super useful and fascinating to follow the evolution.</p>
    <p class="normal">In this chapter, we’re going to talk about how to make sure your systems are up and running and performing correctly, and how to respond when they’re not. In <em class="chapterRef">Chapter 3</em>, <em class="italic">High Availability and Reliability</em>, we discussed related topics. The focus here is on knowing what’s going on in your system and what practices and tools you can use.</p>
    <p class="normal">There are many aspects to monitoring, such as logging, metrics, distributed tracing, error reporting, and alerting. Practices like auto-scaling and self-healing depend on monitoring to detect that there is a need to scale or to heal.</p>
    <p class="normal">The topics we will cover in this chapter include:</p>
    <ul>
      <li class="bulletList">Understanding observability</li>
      <li class="bulletList">Logging with Kubernetes</li>
      <li class="bulletList">Recording metrics with Kubernetes</li>
      <li class="bulletList">Distributed tracing with Jaeger</li>
      <li class="bulletList">Troubleshooting problems</li>
    </ul>
    <p class="normal">The Kubernetes community recognizes the importance of monitoring and has put a lot of effort to make sure Kubernetes has a solid monitoring story. The <strong class="keyWord">Cloud Native Computing Foundation</strong> (<strong class="keyWord">CNCF</strong>) is <a id="_idIndexMarker1328"/>the de-facto curator of cloud-native infrastructure projects. It has graduated twenty projects so far. Kubernetes was the first project to graduate, and out of the early graduated projects, three other projects that graduated more than two years ago are focused on monitoring: Prometheus, Fluentd, and Jaeger. This means that monitoring and observability are the foundation for a large-scale Kubernetes-based system. Before we dive into the ins and out of Kubernetes monitoring and specific projects and tools, we should get a better understanding of what monitoring is all about. A good framework for thinking about monitoring is how observable your system is.</p>
    <h1 id="_idParaDest-610" class="heading-1">Understanding observability</h1>
    <p class="normal">Observability <a id="_idIndexMarker1329"/>is a big word. What does it mean in practice? There are different definitions out there and big debates about how monitoring and observability are similar and different. I take the stance that observability is the property of the system that defines what we can tell about the state and behavior of the system right now and historically. In particular, we are interested in the health of the system and its components. Monitoring<a id="_idIndexMarker1330"/> is the collection of tools, processes, and techniques that we use to increase the observability of the system.</p>
    <p class="normal">There are different facets of information that we need to collect, record, and aggregate in order to get a good sense of what our system is doing. Those facets include logs, metrics, distributed traces, and errors. The monitoring or observability data is multidimensional and crosses many levels. Just collecting it doesn’t help much. We need to be able to query it, visualize it, and alert other systems when things go wrong. Let’s review the various components of observability.</p>
    <h2 id="_idParaDest-611" class="heading-2">Logging</h2>
    <p class="normal">Logging<a id="_idIndexMarker1331"/> is a key monitoring tool. Every self-respecting long-running software must have<a id="_idIndexMarker1332"/> logs. Logs<a id="_idIndexMarker1333"/> capture timestamped events. They are critical for many applications like business intelligence, security, compliance, audits, debugging, and troubleshooting. It’s important to understand that a complicated distributed system will have different logs for different components, and extracting insights from logs is not a trivial undertaking.</p>
    <p class="normal">There are several key attributes to logs: format, storage, and aggregation.</p>
    <h3 id="_idParaDest-612" class="heading-3">Log format</h3>
    <p class="normal">Logs may come in various formats. Plain text is <a id="_idIndexMarker1334"/>very common and human-readable but requires a lot of work to parse and merge with other logs. Structured logs are better suitable for large systems because they can be processed at scale. Binary logs make sense for systems that generate a lot of logs as they are more space efficient, but require custom tools and processing to extract their information.</p>
    <h3 id="_idParaDest-613" class="heading-3">Log storage</h3>
    <p class="normal">Logs can be stored in<a id="_idIndexMarker1335"/> memory, on the file system, in a database, in cloud storage, sent to a remote logging service, or any combination of these. In the cloud-native world, where software runs in containers, it’s important to pay special attention to where logs are stored and how to fetch them when necessary.</p>
    <p class="normal">Questions like durability come to mind when containers can come and go. In Kubernetes, the standard output and standard error streams of containers are automatically logged and available, even when the pod terminates. But, issues like having enough space for logs and log rotation are always relevant.</p>
    <h3 id="_idParaDest-614" class="heading-3">Log aggregation</h3>
    <p class="normal">In the end, the best practice<a id="_idIndexMarker1336"/> is to send local logs to a centralized logging service that is designed to handle various log formats, persist them as necessary, and aggregate many types of logs in a way that can be queried and reasoned about.</p>
    <h2 id="_idParaDest-615" class="heading-2">Metrics</h2>
    <p class="normal">Metrics<a id="_idIndexMarker1337"/> measure some aspects of the system over time. Metrics are time series of numerical values (typically floating point numbers). Each metric has a name and often a<a id="_idIndexMarker1338"/> set of labels that help later in slicing and dicing. For example, the CPU utilization of a node or the error rate of a service are metrics.</p>
    <p class="normal">Metrics are much more economical than logs. They require a fixed amount of space per time period that doesn’t ebb and flow with incoming traffic like logs.</p>
    <p class="normal">Also, since metrics are numerical in nature, they don’t need parsing or transformations. Metrics can be easily combined and analyzed using statistical methods and serve as triggers for events and alerts.</p>
    <p class="normal">A lot of metrics at different levels (node, container, process, network, and disk) are often collected for you automatically by the OS, cloud provider, or Kubernetes.</p>
    <p class="normal">But you can also create custom metrics that map to high-level concerns of your system and can be configured with application-level policies.</p>
    <h2 id="_idParaDest-616" class="heading-2">Distributed tracing</h2>
    <p class="normal">Modern <a id="_idIndexMarker1339"/>distributed systems often use microservice-based architecture, where <a id="_idIndexMarker1340"/>an incoming request is bounced between multiple microservices, waits in queues, and triggers serverless functions. When you try to analyze errors, failures, data integrity issues, or performance issues, it is critical to be able to follow the path of a request. This is where distributed tracing comes in.</p>
    <p class="normal">A distributed trace is a collection of spans and references. You can think of a trace as a <strong class="keyWord">directed acyclic graph</strong> (<strong class="keyWord">DAG</strong>) that <a id="_idIndexMarker1341"/>represents a request’s traversal through the components of a distributed system. Each span records the time the request spent in a given component and references are the edges of the graph that connect one span to the following spans.</p>
    <p class="normal">Here is an example:</p>
    <figure class="mediaobject"><img src="../Images/B18998_13_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.1: The path of a sample distributed trace</p>
    <p class="normal">Distributed tracing is indispensable for understanding complex distributed systems.</p>
    <h2 id="_idParaDest-617" class="heading-2">Application error reporting</h2>
    <p class="normal">Error and exception reporting is<a id="_idIndexMarker1342"/> sometimes done as part of logging. You definitely want to log errors, and looking at logs when things go wrong is a <a id="_idIndexMarker1343"/>time-honored tradition. However, there are levels for capturing error information that go beyond logging. When an error occurs in one of your applications, it is useful to capture an error message, the location of the error in the code, and the stack trace. This is pretty standard and most programming languages can provide all this information, although stack traces are multi-line and don’t fit well with line-based logs. A useful piece of additional information to capture is the local state in each level of the stack trace. This helps when a problem occurs in a central place but local states, like the number and size of entries in some lists can help identify the root cause.</p>
    <p class="normal">A central error reporting service like Sentry<a id="_idIndexMarker1344"/> or Rollbar<a id="_idIndexMarker1345"/> provides a lot of value specific to errors beyond logging, such as rich error information, context, and user information.</p>
    <h2 id="_idParaDest-618" class="heading-2">Dashboards and visualization</h2>
    <p class="normal">OK. You’ve done a great job<a id="_idIndexMarker1346"/> of collecting logs, defining metrics, tracing your requests, and reporting rich errors. Now, you want to figure out what your system or parts of it, are doing. What is the baseline? How does traffic fluctuate throughout the <a id="_idIndexMarker1347"/>day, week, and on holidays? When the system is<a id="_idIndexMarker1348"/> under stress, what parts are the most vulnerable?</p>
    <p class="normal">In a complicated system that involves hundreds and thousands of services and data stores and integrates with external systems, you can’t just look at the raw log files, metrics, and traces.</p>
    <p class="normal">You need to be able to combine a lot of information and build system health dashboards, visualize your infrastructure, and create business-level reports and diagrams.</p>
    <p class="normal">You may get some of it (especially for infrastructure) automatically if you’re using cloud platforms. But you should expect to do some serious work around visualization and dashboards.</p>
    <h2 id="_idParaDest-619" class="heading-2">Alerting</h2>
    <p class="normal">Dashboards <a id="_idIndexMarker1349"/>are <a id="_idIndexMarker1350"/>great for humans that want to get a broad view of the system and be able to drill down and understand how it behaves. Alerting is all about detecting abnormal situations and triggering some action. Ideally, your system is self-healing and can recover on its own from most situations. But you should at least report it, so humans can review what happened at their leisure and decide if further action is needed.</p>
    <p class="normal">Alerting can be<a id="_idIndexMarker1351"/> integrated with emails, chat rooms, and on-call systems. It is often linked to metrics, and when certain conditions apply, an alert is raised.</p>
    <p class="normal">Now that we have covered, in general, the different elements involved in monitoring complex systems, let’s see how to do it with Kubernetes.</p>
    <h1 id="_idParaDest-620" class="heading-1">Logging with Kubernetes</h1>
    <p class="normal">We need to consider carefully<a id="_idIndexMarker1352"/> our logging strategy with Kubernetes. There are several <a id="_idIndexMarker1353"/>types of logs that are relevant for monitoring purposes. Our workloads run in containers, of course, and we care about these logs, but we also care about the logs of Kubernetes components like the API server, kubelet, and container runtime. </p>
    <p class="normal">In addition, chasing logs across multiple nodes and containers is a non-starter. The best practice is to use central logging (a.k.a. log aggregation). There are several options here, which we will explore soon.</p>
    <h2 id="_idParaDest-621" class="heading-2">Container logs</h2>
    <p class="normal">Kubernetes stores the <a id="_idIndexMarker1354"/>standard output <a id="_idIndexMarker1355"/>and standard error of every container. They are available through the <code class="inlineCode">kubectl logs</code> command.</p>
    <p class="normal">Here is a pod manifest that prints the current date and time every 10 seconds:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">now</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">now</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/py-kube:0.3</span>
      <span class="hljs-attr">command:</span> [<span class="hljs-string">"/bin/bash"</span>, <span class="hljs-string">"</span><span class="hljs-string">-c"</span>, <span class="hljs-string">"while true; do sleep 10; date; done"</span>]
</code></pre>
    <p class="normal">We can save it to a file called <code class="inlineCode">now-pod.yaml</code> and create it:</p>
    <pre class="programlisting gen"><code class="hljs">$ k apply -f now-pod.yaml
pod/now created
</code></pre>
    <p class="normal">To check out the logs, we use the <code class="inlineCode">kubectl logs</code> command:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl logs now
Sat Jan  4 00:32:38 UTC 2020
Sat Jan  4 00:32:48 UTC 2020
Sat Jan  4 00:32:58 UTC 2020
Sat Jan  4 00:33:08 UTC 2020
Sat Jan  4 00:33:18 UTC 2020
</code></pre>
    <p class="normal">A few points about container logs. The <code class="inlineCode">kubectl logs</code> command expects a pod name. If the pod has multiple containers, you need to specify the container name too:</p>
    <pre class="programlisting gen"><code class="hljs">$ k logs &lt;pod name&gt; -c &lt;container name&gt;
</code></pre>
    <p class="normal">If a deployment or replica set creates multiple copies of the same pod, you can query the logs of all pods in a single call by using a shared label:</p>
    <pre class="programlisting gen"><code class="hljs">k logs -l &lt;label&gt;
</code></pre>
    <p class="normal">If a container crashes<a id="_idIndexMarker1356"/> for some<a id="_idIndexMarker1357"/> reason, you can use the <code class="inlineCode">kubectl logs -p</code> command to look at logs from the crashed container.</p>
    <h2 id="_idParaDest-622" class="heading-2">Kubernetes component logs</h2>
    <p class="normal">If you run Kubernetes in a <a id="_idIndexMarker1358"/>managed environment like GKE, EKS, or AKS, you won’t be able to access Kubernetes component logs directly, but this is expected. You’re not<a id="_idIndexMarker1359"/> responsible for the Kubernetes control plane. However, the logs of control plane components (like the API server and cluster autoscaler), as well as node components (like the kubelet and container runtime), may be important for troubleshooting issues. Cloud providers often offer proprietary ways to access these logs.</p>
    <p class="normal">Here are the standard control plane components and their log location if you run your own Kubernetes control plane:</p>
    <ul>
      <li class="bulletList">API server: <code class="inlineCode">/var/log/kube-apiserver.log</code></li>
      <li class="bulletList">Scheduler: <code class="inlineCode">/var/log/kube-scheduler.log</code></li>
      <li class="bulletList">Controller manager: <code class="inlineCode">/var/log/kube-controller-manager.log</code></li>
    </ul>
    <p class="normal">The worker node <a id="_idIndexMarker1360"/>components and <a id="_idIndexMarker1361"/>their log locations are:</p>
    <ul>
      <li class="bulletList">Kubelet: <code class="inlineCode">/var/log/kubelet.log</code></li>
      <li class="bulletList">Kube proxy: <code class="inlineCode">/var/log/kube-proxy.log</code></li>
    </ul>
    <p class="normal">Note that on a systemd-based system, you’ll need to use <code class="inlineCode">journalctl</code> to view the worker node logs.</p>
    <h2 id="_idParaDest-623" class="heading-2">Centralized logging</h2>
    <p class="normal">Reading <a id="_idIndexMarker1362"/>container logs is fine for <a id="_idIndexMarker1363"/>quick and dirty troubleshooting problems in a single pod. To diagnose and debug system-wide issues, we need centralized logging (a.k.a. log aggregation). All the logs from our containers should be sent to a central repository and made accessible for slicing and dicing using filters and queries.</p>
    <p class="normal">When deciding on your central logging approach, there are several important decisions: </p>
    <ul>
      <li class="bulletList">How to collect the logs</li>
      <li class="bulletList">Where to store the logs</li>
      <li class="bulletList">How to handle sensitive log information</li>
    </ul>
    <p class="normal">We will answer these questions in the following sections.</p>
    <h3 id="_idParaDest-624" class="heading-3">Choosing a log collection strategy</h3>
    <p class="normal">Logs are collected <a id="_idIndexMarker1364"/>typically by an agent that is running close to the <a id="_idIndexMarker1365"/>process generating the logs and making sure to deliver them to the central logging service.</p>
    <p class="normal">Let’s look at the common approaches.</p>
    <h4 class="heading-4">Direct logging to a remote logging service</h4>
    <p class="normal">In this approach, there is no log<a id="_idIndexMarker1366"/> agent. It is the <a id="_idIndexMarker1367"/>responsibility of each application container to send logs to the remote logging service. This is typically done through a client library. It is a high-touch approach and applications need to be aware of the logging target as well as be configured with proper credentials.</p>
    <figure class="mediaobject"><img src="../Images/B18998_13_02.png" alt="Direct logging"/></figure>
    <p class="packt_figref">Figure 13.2: Direct logging</p>
    <p class="normal">If you ever want to<a id="_idIndexMarker1368"/> change your log<a id="_idIndexMarker1369"/> collection strategy, it will require changes to each and every application (at least bumping to a new version of the library).</p>
    <h4 class="heading-4">Node agent</h4>
    <p class="normal">The node agent<a id="_idIndexMarker1370"/> approach is best when you control the<a id="_idIndexMarker1371"/> worker nodes, and you want to abstract away the act of log aggregation from your applications. Each application container can simply write to standard output and standard error and the agent running on each node will intercept the logs and deliver them to the remote logging service.</p>
    <p class="normal">Typically, you deploy the node agent as a DaemonSet so, as nodes are added or removed from the cluster, the log agent will always be present without additional work.</p>
    <figure class="mediaobject"><img src="../Images/B18998_13_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.3: Using a node agent for logging</p>
    <h4 class="heading-4">Sidecar container</h4>
    <p class="normal">The sidecar container is <a id="_idIndexMarker1372"/>best when you don’t have<a id="_idIndexMarker1373"/> control over your cluster nodes, or if you use some serverless computing infrastructure to deploy containers but don’t want to use the direct logging approach. The node agent approach is out of the question if you don’t control the node and can’t install the agent, but you can attach a sidecar container that will collect the logs and deliver them to the central logging service. It is not as efficient as the node agent approach because each container will need its own logging sidecar container, but it can be done at the deployment stage without requiring code changes and application knowledge.</p>
    <figure class="mediaobject"><img src="../Images/B18998_13_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.4: Using a sidecar container for logging</p>
    <p class="normal">Now that we covered the topic of log collection, let’s consider how to store and manage those logs centrally.</p>
    <h3 id="_idParaDest-625" class="heading-3">Cluster-level central logging</h3>
    <p class="normal">If your entire <a id="_idIndexMarker1374"/>system is running in a single Kubernetes cluster, then <a id="_idIndexMarker1375"/>cluster-level logging may be a great choice. You can install a central logging service like Grafana Loki, ElasticSearch, or Graylog in your cluster and enjoy a cohesive log aggregation experience without sending your log data elsewhere.</p>
    <h3 id="_idParaDest-626" class="heading-3">Remote central logging</h3>
    <p class="normal">There are situations where in-cluster <a id="_idIndexMarker1376"/>central logging doesn’t cut it for various<a id="_idIndexMarker1377"/> reasons:</p>
    <ul>
      <li class="bulletList">Logs are used for audit purposes; it may be necessary to log to a separate and controlled location (e.g., on AWS, it is common to log to a separate account).</li>
      <li class="bulletList">Your system runs on multiple clusters and logging in each cluster is not really central.</li>
      <li class="bulletList">You run on a cloud provider and prefer to log into the cloud platform logging service (e.g., StackDriver on GCP or CloudWatch on AWS).</li>
      <li class="bulletList">You already work with a remote central logging service like SumoLogic or Splunk and you prefer to continue using them.</li>
      <li class="bulletList">You just don’t want the hassle of collecting and storing log data.</li>
      <li class="bulletList">Cluster-wide issues can impact your log collection, storage, or access and impede your ability to troubleshoot them.</li>
    </ul>
    <p class="normal">Logging to a <a id="_idIndexMarker1378"/>remote central location can be done by all methods: direct <a id="_idIndexMarker1379"/>logging, node agent logging, or sidecar logging. In all cases, the endpoint and credentials to the remote logging service must be provided, and the logging is done against that endpoint. In most cases, this will be done via a client library that hides the details from the applications. As for system-level logging, the common method is to collect all the necessary logs by a dedicated logging agent and forward them to the remote logging service.</p>
    <h3 id="_idParaDest-627" class="heading-3">Dealing with sensitive log information</h3>
    <p class="normal">OK. We can collect the logs and<a id="_idIndexMarker1380"/> send them to a central logging service. If the <a id="_idIndexMarker1381"/>central logging service is remote, you might need to be selective about which information you log.</p>
    <p class="normal">For example, <strong class="keyWord">personally identifiable information</strong> (<strong class="keyWord">PII</strong>) and <strong class="keyWord">protected health information</strong> (<strong class="keyWord">PHI</strong>) are two <a id="_idIndexMarker1382"/>categories of information that you<a id="_idIndexMarker1383"/> probably shouldn’t log without making sure access to the logs is properly controlled.</p>
    <p class="normal">It is common to redact or remove PII like user names and emails from log statements.</p>
    <h2 id="_idParaDest-628" class="heading-2">Using Fluentd for log collection</h2>
    <p class="normal">Fluentd (<a href="https://www.fluentd.org"><span class="url">https://www.fluentd.org</span></a>) is <a id="_idIndexMarker1384"/>an open source CNCF-graduated project. It is<a id="_idIndexMarker1385"/> considered best<a id="_idIndexMarker1386"/> in class in Kubernetes, and it can integrate with pretty much every logging backend you want. If you set up your own centralized logging solution, I recommend using Fluentd. Fluentd <a id="_idIndexMarker1387"/>operates as a node agent. The following diagram<a id="_idIndexMarker1388"/> shows how Fluentd can be deployed as a DaemonSet in a Kubernetes cluster:</p>
    <figure class="mediaobject"><img src="../Images/B18998_13_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.5: Deploying Fluentd as a DaemonSet in a Kubernetes cluster</p>
    <p class="normal">One of the most popular DIY centralized logging solutions is ELK, where E stands for ElasticSearch, L stands for LogStash, and K stands for Kibana. On Kubernetes, EFK (where Fluentd replaces LogStash) is very common.</p>
    <p class="normal">Fluentd has<a id="_idIndexMarker1389"/> plugin-based architecture so don’t feel limited to EFK. Fluentd doesn’t require a lot of resources, but if you really need a high-performance <a id="_idIndexMarker1390"/>solution, Fluentbit (<a href="http://fluentbit.io/"><span class="url">http://fluentbit.io/</span></a>) is a pure<a id="_idIndexMarker1391"/> forwarder that uses barely 450 KB of<a id="_idIndexMarker1392"/> memory.</p>
    <p class="normal">We have covered a lot of ground about logging. Let’s look at the next piece of the observability story, which is metrics.</p>
    <h1 id="_idParaDest-629" class="heading-1">Collecting metrics with Kubernetes</h1>
    <p class="normal">Kubernetes has a <a id="_idIndexMarker1393"/>Metrics API. It supports node and pod metrics out of the box. You can also define your own custom metrics.</p>
    <p class="normal">A metric<a id="_idIndexMarker1394"/> contains a timestamp, a usage field, and the time range in which the metric was collected (many metrics are accumulated over a time period). Here is the API definition for node metrics:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">type</span> <span class="hljs-title">NodeMetrics</span> struct {
    metav1.<span class="hljs-property">TypeMeta</span>
    metav1.<span class="hljs-property">ObjectMeta</span>
    <span class="hljs-title">Timestamp</span> metav1.<span class="hljs-property">Time</span>
    <span class="hljs-title">Window</span>    metav1.<span class="hljs-property">Duration</span>
    <span class="hljs-title">Usage</span> corev1.<span class="hljs-property">ResourceList</span>
}
<span class="hljs-comment">// NodeMetricsList is a list of NodeMetrics.</span>
<span class="hljs-keyword">type</span> <span class="hljs-title">NodeMetricsList</span> struct {
    metav1.<span class="hljs-property">TypeMeta</span>
    <span class="hljs-comment">// Standard list metadata.</span>
    <span class="hljs-comment">// More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</span>
    metav1.<span class="hljs-property">ListMeta</span>
    <span class="hljs-comment">// List of node metrics.</span>
    <span class="hljs-title">Items</span> []<span class="hljs-title">NodeMetrics</span>
}
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1395"/>usage field type is <code class="inlineCode">ResourceList</code>, but it’s actually a map of a resource name to a quantity:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">// ResourceList is a set of (resource name, quantity) pairs.</span>
<span class="hljs-keyword">type</span> <span class="hljs-title">ResourceList</span> map[<span class="hljs-title">ResourceName</span>]resource.<span class="hljs-property">Quantity</span>
</code></pre>
    <p class="normal">A <code class="inlineCode">Quantity</code> represents a fixed-point number. It allows easy marshaling/unmarshaling in JSON and YAML, and accessors such as <code class="inlineCode">String()</code> and <code class="inlineCode">Int64()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">type</span> <span class="hljs-title">Quantity</span> struct {
    <span class="hljs-comment">// i is the quantity in int64 scaled form, if d.Dec == nil</span>
    i int64Amount
    
    <span class="hljs-comment">// d is the quantity in inf.Dec form if d.Dec != nil</span>
    d infDecAmount
    <span class="hljs-comment">// s is the generated value of this quantity to avoid recalculation</span>
    s <span class="hljs-built_in">string</span>
    <span class="hljs-comment">// Change Format at will. See the comment for Canonicalize for more details.</span>
    <span class="hljs-title">Format</span>
}
</code></pre>
    <h2 id="_idParaDest-630" class="heading-2">Monitoring with the Metrics Server</h2>
    <p class="normal">The Kubernetes Metrics Server<a id="_idIndexMarker1396"/> implements the Kubernetes Metrics API.</p>
    <p class="normal">You can deploy it with Helm:</p>
    <pre class="programlisting gen"><code class="hljs">$ helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
$ helm upgrade --install metrics-server metrics-server/metrics-server
Release "metrics-server" does not exist. Installing it now.
NAME: metrics-server
LAST DEPLOYED: Sun Oct  9 14:11:54 2022
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
***********************************************************************
* Metrics Server                                                      *
***********************************************************************
  Chart version: 3.8.2
  App version:   0.6.1
  Image tag:     k8s.gcr.io/metrics-server/metrics-server:v0.6.1
***********************************************************************
</code></pre>
    <p class="normal">On minikube, you can just enable it as an add-on:</p>
    <pre class="programlisting gen"><code class="hljs">$ minikube addons enable metrics-server
    ▪ Using image k8s.gcr.io/metrics-server/metrics-server:v0.4.2
<img src="../Images/B18998_13_001.png" alt=""/>  The 'metrics-server' addon is enabled
</code></pre>
    <p class="normal">Note that at the time of writing, there was an issue with the Metrics Server on minikube, which was fixed in Kubernetes 1.27 (see <a href="https://github.com/kubernetes/minikube/issues/13969"><span class="url">https://github.com/kubernetes/minikube/issues/13969</span></a>).</p>
    <p class="normal">We will use a kind cluster to deploy the <code class="inlineCode">metrics-server</code>.</p>
    <p class="normal">After a few minutes<a id="_idIndexMarker1397"/> to let the metrics server collect some data, you can query it using these commands for node metrics:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get --raw "/apis/metrics.k8s.io/v1beta1/nodes" | jq .
{
  "kind": "NodeMetricsList",
  "apiVersion": "metrics.k8s.io/v1beta1",
  "metadata": {},
  "items": [
    {
      "metadata": {
        "name": "kind-control-plane",
        "creationTimestamp": "2022-10-09T21:24:12Z",
        "labels": {
          "beta.kubernetes.io/arch": "arm64",
          "beta.kubernetes.io/os": "linux",
          "kubernetes.io/arch": "arm64",
          "kubernetes.io/hostname": "kind-control-plane",
          "kubernetes.io/os": "linux",
          "node-role.kubernetes.io/control-plane": "",
          "node.kubernetes.io/exclude-from-external-load-balancers": ""
        }
      },
      "timestamp": "2022-10-09T21:24:05Z",
      "window": "20.022s",
      "usage": {
        "cpu": "115537281n",
        "memory": "47344Ki"
      }
    }
  ]
}
</code></pre>
    <p class="normal">In addition, the <code class="inlineCode">kubectl top</code> command gets its information from the metrics server:</p>
    <pre class="programlisting gen"><code class="hljs">$ k top nodes
NAME                 CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
kind-control-plane   125m         3%     46Mi            1%
</code></pre>
    <p class="normal">We can also get metrics for pods:</p>
    <pre class="programlisting gen"><code class="hljs">$ k top pods -A
NAMESPACE            NAME                                         CPU(cores)   MEMORY(bytes)
default              metrics-server-554f79c654-hw2c7              4m           18Mi
kube-system          coredns-565d847f94-t8knf                     2m           12Mi
kube-system          coredns-565d847f94-wdqzx                     2m           14Mi
kube-system          etcd-kind-control-plane                      24m          28Mi
kube-system          kindnet-fvfs7                                1m           7Mi
kube-system          kube-apiserver-kind-control-plane            43m          339Mi
kube-system          kube-controller-manager-kind-control-plane   18m          48Mi
kube-system          kube-proxy-svdc6                             1m           11Mi
kube-system          kube-scheduler-kind-control-plane            4m           21Mi
local-path-storage   local-path-provisioner-684f458cdd-24w88      2m           6Mi
</code></pre>
    <p class="normal">The metrics server is also the source of performance information in the Kubernetes dashboard.</p>
    <h2 id="_idParaDest-631" class="heading-2">The rise of Prometheus</h2>
    <p class="normal">Prometheus (<a href="https://prometheus.io/"><span class="url">https://prometheus.io/</span></a>) is yet another graduated CNCF open source project. It focuses on metrics<a id="_idIndexMarker1398"/> collection and alert management. It has a simple yet powerful <a id="_idIndexMarker1399"/>data model for managing time-series data and a sophisticated query language. It is considered best in class in the Kubernetes world. Prometheus lets you define recording rules that are fired at regular intervals and collect data from targets. In addition, you can define alerting rules that evaluate a condition and trigger alerts if the condition is satisfied.</p>
    <p class="normal">It has several unique features<a id="_idIndexMarker1400"/> compared to other monitoring solutions:</p>
    <ul>
      <li class="bulletList">The collection system is pulled over HTTP. Nobody has to push metrics to Prometheus (but push is supported via a gateway).</li>
      <li class="bulletList">A multidimensional data model (each metric is a named time series with a set of key/value pairs attached to each data point).</li>
      <li class="bulletList">PromQL: A powerful and flexible query language to slice and dice your metrics.</li>
      <li class="bulletList">Prometheus server nodes are independent and don’t rely on shared storage.</li>
      <li class="bulletList">Target discovery can be dynamic or via static configuration.</li>
      <li class="bulletList">Built-in time series storage, but supports other backends if necessary.</li>
      <li class="bulletList">Built-in<a id="_idIndexMarker1401"/> alert manager and the ability to define alerting rules.</li>
    </ul>
    <p class="normal">The following diagram<a id="_idIndexMarker1402"/> illustrates the entire system:</p>
    <figure class="mediaobject"><img src="../Images/B18998_13_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.6: Prometheus architecture</p>
    <h3 id="_idParaDest-632" class="heading-3">Installing Prometheus</h3>
    <p class="normal">Prometheus is a <a id="_idIndexMarker1403"/>complex beast, as you can see. The best way to install it is using the <a id="_idIndexMarker1404"/>Prometheus<a id="_idIndexMarker1405"/> operator (<a href="https://github.com/prometheus-operator/"><span class="url">https://github.com/prometheus-operator/</span></a>). The kube-prometheus (<a href="https://github.com/prometheus-operator/kube-prometheus"><span class="url">https://github.com/prometheus-operator/kube-prometheus</span></a>) sub-project installs <a id="_idIndexMarker1406"/>the operator itself, and a lot of additional components, and configures them in a robust manner.</p>
    <p class="normal">The first step is cloning the git repo:</p>
    <pre class="programlisting gen"><code class="hljs">$ git clone https://github.com/prometheus-operator/kube-prometheus.git
Cloning into 'kube-prometheus'...
remote: Enumerating objects: 17062, done.
remote: Counting objects: 100% (185/185), done.
remote: Compressing objects: 100% (63/63), done.
remote: Total 17062 (delta 135), reused 155 (delta 116), pack-reused 16877
Receiving objects: 100% (17062/17062), 8.76 MiB | 11.63 MiB/s, done.
Resolving deltas: 100% (11135/11135), done.
</code></pre>
    <p class="normal">Next, the setup manifests install several CRDs and creates a namespace called <code class="inlineCode">monitoring</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl create -f manifests/setup
customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created
namespace/monitoring created
</code></pre>
    <p class="normal">Now, we can<a id="_idIndexMarker1407"/> install the manifests:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl create -f manifests
...
</code></pre>
    <p class="normal">The output is too long to display, but let’s examine what was actually installed. It turns out that it installed multiple deployments, StatefulSets, a DaemonSet, and many services:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get deployments -n monitoring
NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
blackbox-exporter     1/1     1            1           3m38s
grafana               1/1     1            1           3m37s
kube-state-metrics    1/1     1            1           3m37s
prometheus-adapter    2/2     2            2           3m37s
prometheus-operator   1/1     1            1           3m37s
$ k get statefulsets -n monitoring
NAME                READY   AGE
alertmanager-main   3/3     2m57s
prometheus-k8s      2/2     2m57s
$ k get daemonsets -n monitoring
NAME            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
node-exporter   1         1         1       1            1           kubernetes.io/os=linux   4m4s
$ k get services -n monitoring
NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
alertmanager-main       ClusterIP   10.96.231.0     &lt;none&gt;        9093/TCP,8080/TCP            4m25s
alertmanager-operated   ClusterIP   None            &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   3m35s
blackbox-exporter       ClusterIP   10.96.239.94    &lt;none&gt;        9115/TCP,19115/TCP           4m25s
grafana                 ClusterIP   10.96.80.116    &lt;none&gt;        3000/TCP                     4m24s
kube-state-metrics      ClusterIP   None            &lt;none&gt;        8443/TCP,9443/TCP            4m24s
node-exporter           ClusterIP   None            &lt;none&gt;        9100/TCP                     4m24s
prometheus-adapter      ClusterIP   10.96.139.149   &lt;none&gt;        443/TCP                      4m24s
prometheus-k8s          ClusterIP   10.96.51.85     &lt;none&gt;        9090/TCP,8080/TCP            4m24s
prometheus-operated     ClusterIP   None            &lt;none&gt;        9090/TCP                     3m35s
prometheus-operator     ClusterIP   None            &lt;none&gt;        8443/TCP                     4m24s
</code></pre>
    <p class="normal">This is a<a id="_idIndexMarker1408"/> high-availability setup. As you can see, Prometheus itself is deployed as a StatefulSet with two replicas, and the alert manager is deployed as a StatefulSet with three replicas.</p>
    <p class="normal">The deployments include the <code class="inlineCode">blackbox-exporter</code>, Grafana for visualizing the metrics, <code class="inlineCode">kube-state-metrics</code> to collect Kubernetes-specific metrics, the Prometheus adapter (a compatible replacement to the standard Kubernetes Metrics Server), and finally, the Prometheus operator.</p>
    <h3 id="_idParaDest-633" class="heading-3">Interacting with Prometheus</h3>
    <p class="normal">Prometheus <a id="_idIndexMarker1409"/>has a basic web UI that you can use to explore its metrics. Let’s do port forwarding to <code class="inlineCode">localhost</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ k port-forward -n monitoring statefulset/prometheus-k8s 9090
Forwarding from 127.0.0.1:9090 -&gt; 9090
Forwarding from [::1]:9090 -&gt; 9090
</code></pre>
    <p class="normal">Then, you can browse to <code class="inlineCode">http://localhost:9090</code>, where you can select different metrics and view raw data or graphs:</p>
    <figure class="mediaobject"><img src="../Images/B18998_13_07.png" alt="Prometheus UI"/></figure>
    <p class="packt_figref">Figure 13.7: Prometheus UI</p>
    <p class="normal">Prometheus<a id="_idIndexMarker1410"/> records an outstanding number of metrics (<code class="inlineCode">9090</code> in my current setup). The most relevant metrics on Kubernetes are the metrics exposed by the <code class="inlineCode">kube-state-metrics</code> and node exporters.</p>
    <h3 id="_idParaDest-634" class="heading-3">Incorporating kube-state-metrics</h3>
    <p class="normal">The Prometheus operator <a id="_idIndexMarker1411"/>already installs <code class="inlineCode">kube-state-metrics</code>. It is a <a id="_idIndexMarker1412"/>service that listens to Kubernetes events and exposes them through a <code class="inlineCode">/metrics</code> HTTP endpoint in the format Prometheus expects. So, it is a Prometheus exporter.</p>
    <p class="normal">This is very different from the Kubernetes metrics server, which is the standard way Kubernetes exposes metrics for nodes and pods and allows you to expose your own custom metrics too. The Kubernetes metrics server is a service that periodically queries Kubernetes for data and stores it in memory. It exposes its data through the Kubernetes Metrics API. The Prometheus adapter adapts the Kubernetes metrics server information and exposes it in Prometheus format.</p>
    <p class="normal">The metrics exposed by <code class="inlineCode">kube-state-metrics</code> are vast. Here is the list of the groups of metrics, which is pretty massive on its own. Each group corresponds to a Kubernetes API object and contains multiple metrics:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">CertificateSigningRequest</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">ConfigMap</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">CronJob</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">DaemonSet</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">Deployment</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">Endpoint</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">HorizontalPodAutoscaler</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">Ingress</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">Job</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">LimitRange</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">MutatingWebhookConfiguration</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">Namespace</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">NetworkPolicy</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">Node</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">PersistentVolume</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">PersistentVolumeClaim</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">PodDisruptionBudget</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">Pod</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">ReplicaSet</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">ReplicationController</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">ResourceQuota</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">Secret</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">Service</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">StatefulSet</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">StorageClass</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">ValidatingWebhookConfiguration</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">VerticalPodAutoscaler</code> Metrics</li>
      <li class="bulletList"><code class="inlineCode">VolumeAttachment</code> Metrics</li>
    </ul>
    <p class="normal">For example, here <a id="_idIndexMarker1413"/>are the <a id="_idIndexMarker1414"/>metrics collected for Kubernetes services:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">kube_service_info</code></li>
      <li class="bulletList"><code class="inlineCode">kube_service_labels</code></li>
      <li class="bulletList"><code class="inlineCode">kube_service_created</code></li>
      <li class="bulletList"><code class="inlineCode">kube_service_spec_type</code></li>
    </ul>
    <h3 id="_idParaDest-635" class="heading-3">Utilizing the node exporter</h3>
    <p class="normal"><code class="inlineCode">kube-state-metrics</code> collects node <a id="_idIndexMarker1415"/>information from the Kubernetes API server, but this information is pretty limited. Prometheus comes with its own node exporter, which collects tons of low-level information about the nodes. Remember that Prometheus may be the de-facto standard metrics platform on Kubernetes, but it is not Kubernetes-specific. For other systems that use Prometheus, the node exporter is super important. On Kubernetes, if you manage your own nodes, this information can be invaluable too.</p>
    <p class="normal">Here is a small subset of the metrics exposed by the node exporter:</p>
    <figure class="mediaobject"><img src="../Images/B18998_13_08.png" alt="Node exporter metrics"/></figure>
    <p class="packt_figref">Figure 13.8: Node exporter metrics</p>
    <h3 id="_idParaDest-636" class="heading-3">Incorporating custom metrics</h3>
    <p class="normal">The built-in metrics, node metrics, and <a id="_idIndexMarker1416"/>Kubernetes metrics are great, but very often, the most interesting metrics are domain-specific and need to be captured as custom metrics. There are two ways to do it:</p>
    <ul>
      <li class="bulletList">Write your own exporter and tell Prometheus to scrape it</li>
      <li class="bulletList">Use the Push gateway, which allows you to push metrics into Prometheus</li>
    </ul>
    <p class="normal">In my book <em class="italic">Hands-On Microservices with Kubernetes</em> (<a href="https://www.packtpub.com/product/hands-on-microservices-with-kubernetes/9781789805468"><span class="url">https://www.packtpub.com/product/hands-on-microservices-with-kubernetes/9781789805468</span></a>), I provide a full-fledged example of how to implement your own exporter from a Go service.</p>
    <p class="normal">The Push gateway is more appropriate if you already have a push-based metrics collector in place, and you just want to have Prometheus record those metrics. It provides a convenient migration path from other metrics collection systems to Prometheus.</p>
    <h3 id="_idParaDest-637" class="heading-3">Alerting with Alertmanager</h3>
    <p class="normal">Collecting metrics is<a id="_idIndexMarker1417"/> great, but when things go south (or ideally, BEFORE things go south), you want to get notified. In Prometheus, this is the job of the <code class="inlineCode">Alertmanager</code>. You can define rules as expressions-based metrics, and when those expressions become true, they trigger an alert.</p>
    <p class="normal">Alerts can serve multiple purposes. They can be handled automatically by a controller that is responsible for mitigating specific problems, they can wake up a poor on-call engineer at 3 am, they can result in an email or group chat message, or any combination of these.</p>
    <p class="normal">The <code class="inlineCode">Alertmanager</code> lets you <a id="_idIndexMarker1418"/>group similar alerts into a single notification, inhibiting notifications if other alerts are already firing and silencing alerts. All those features are useful when a large-scale system is in trouble. The stakeholders are aware of the situation and don’t need repeated alerts or multiple variations of the same alert to fire constantly while troubleshooting and trying to find the root cause.</p>
    <p class="normal">One of the cool things about the Prometheus operator is that it manages everything in CRDs. That includes all the rules, including the alert rules:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get prometheusrules -n monitoring
NAME                              AGE
alertmanager-main-rules           11h
grafana-rules                     11h
kube-prometheus-rules             11h
kube-state-metrics-rules          11h
kubernetes-monitoring-rules       11h
node-exporter-rules               11h
prometheus-k8s-prometheus-rules   11h
prometheus-operator-rules         11h
</code></pre>
    <p class="normal">Here is the <code class="inlineCode">NodeFilesystemAlmostOutOfSpace</code> alert that checks if available disk space for the file system on the node is less than a threshold for 30 minutes. If you notice, there are two almost identical alerts. When the available space drops below 5%, a warning alert is triggered. However, if the space drops below 3%, then a critical alert is triggered. Note the <code class="inlineCode">runbook_url</code> field, which points to a page that explains more about the alert and how to mitigate the problem:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get prometheusrules node-exporter-rules -n monitoring -o yaml | grep NodeFilesystemAlmostOutOfSpace -A 14
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available space left.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 &lt; 5
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 30m
      labels:
        severity: warning
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available space left.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
        summary: Filesystem has less than 3% space left.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 &lt; 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 30m
      labels:
        severity: critical
</code></pre>
    <p class="normal">Alerts are very important, but there are cases where you want to visualize the overall state of your system or drill down into specific aspects. This is where visualization comes into play.</p>
    <h2 id="_idParaDest-638" class="heading-2">Visualizing your metrics with Grafana</h2>
    <p class="normal">You’ve already seen the <a id="_idIndexMarker1419"/>Prometheus Expression browser, which can display your<a id="_idIndexMarker1420"/> metrics as a graph or in table form. But we can do much<a id="_idIndexMarker1421"/> better. Grafana (<a href="https://grafana.com"><span class="url">https://grafana.com</span></a>) is <a id="_idIndexMarker1422"/>an open source monitoring system that specializes in stunningly beautiful visualizations of metrics. It doesn’t store the metrics itself but works with many data sources, and Prometheus is one of them. Grafana has alerting capabilities, too. When working with Prometheus, you may prefer to rely on its <code class="inlineCode">Alertmanager</code>.</p>
    <p class="normal">The Prometheus operator installs Grafana and configures a large number of useful Kubernetes dashboards. Check <a id="_idIndexMarker1423"/>out this beautiful dashboard of Kubernetes<a id="_idIndexMarker1424"/> capacity:</p>
    <figure class="mediaobject"><img src="../Images/B18998_13_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.9: Grafana dashboard</p>
    <p class="normal">To access Grafana, type the following commands:</p>
    <pre class="programlisting gen"><code class="hljs">$ k port-forward -n monitoring deploy/grafana 3000
Forwarding from 127.0.0.1:3000 -&gt; 3000
Forwarding from [::1]:3000 -&gt; 3000
</code></pre>
    <p class="normal">Then you can browse to <code class="inlineCode">http://localhost:3000</code> and have some fun with Grafana. Grafana requires a username and password. The default credentials are <em class="italic">admin</em> for the user and <em class="italic">admin</em> for the password.</p>
    <p class="normal">Here are some of the default dashboards that are configured when deploying Grafana via kube-prometheus:</p>
    <figure class="mediaobject"><img src="../Images/B18998_13_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.10: Default Grafana Dashboards</p>
    <p class="normal">As you can see, the list is pretty extensive, but you can define your own dashboards if you want. There are a lot of fancy visualizations you can create with Grafana. I encourage you to explore it further. Grafana dashboards are stored as config maps. If you want to add a custom dashboard, just add a config map that contains your dashboard spec. There is <a id="_idIndexMarker1425"/>a dedicated sidecar container watching new config<a id="_idIndexMarker1426"/> maps being added and it will make sure to add your custom dashboard.</p>
    <p class="normal">You can also add dashboards via the Grafana UI.</p>
    <h3 id="_idParaDest-639" class="heading-3">Considering Loki</h3>
    <p class="normal">If you like <a id="_idIndexMarker1427"/>Prometheus and Grafana and you didn’t settle on a centralized logging solution yet (or if you’re unhappy with your current logging solution), then you should consider Grafana<a id="_idIndexMarker1428"/> Loki (<a href="https://grafana.com/oss/loki/"><span class="url">https://grafana.com/oss/loki/</span></a>). Loki is an open source project for log aggregation inspired by Prometheus. Unlike most log aggregation systems, it doesn’t index the log contents but rather a set of labels applied to the log. That makes it very efficient. It is still relatively new (started in 2018), so you should evaluate if it fits your needs before making the decision to adopt it. One thing is sure: Loki has excellent Grafana support.</p>
    <p class="normal">There are several advantages for<a id="_idIndexMarker1429"/> Loki compared to a solution like EFK when Prometheus is used as the metrics platform. In particular, the set of labels you use to tag your metrics will serve just as well to tag your logs. Also, the fact that Grafana is used as a uniform visualization platform for both logs and metrics is useful.</p>
    <p class="normal">We have dedicated a lot of time to discussing metrics on Kubernetes. Let’s talk about distributed tracing and the Jaeger project.</p>
    <h1 id="_idParaDest-640" class="heading-1">Distributed tracing with Kubernetes</h1>
    <p class="normal">In a <a id="_idIndexMarker1430"/>microservice-based system, every request <a id="_idIndexMarker1431"/>may travel between multiple microservices calling each other, wait in queues, and trigger serverless functions. To debug and troubleshoot such systems, you need to be able to keep track of requests and follow them along their path.</p>
    <p class="normal">Distributed tracing provides several capabilities that allow the developers and operators to understand their distributed systems:</p>
    <ul>
      <li class="bulletList">Distributed transaction monitoring</li>
      <li class="bulletList">Performance and latency tracking</li>
      <li class="bulletList">Root cause analysis</li>
      <li class="bulletList">Service dependency analysis</li>
      <li class="bulletList">Distributed context propagation</li>
    </ul>
    <p class="normal">Distributed tracing often requires the participation of the applications and services instrumenting endpoints. Since the microservices world is polyglot, multiple programming languages may be used. It makes sense to use a shared distributed tracing specification and framework that supports many programming languages. Enter OpenTelemetry.</p>
    <h2 id="_idParaDest-641" class="heading-2">What is OpenTelemetry?</h2>
    <p class="normal">OpenTelemetry (<a href="https://opentelemetry.io"><span class="url">https://opentelemetry.io</span></a>) is an API specification and a set of frameworks and<a id="_idIndexMarker1432"/> libraries in different languages to instrument, collect, and export <a id="_idIndexMarker1433"/>logs, metrics, and traces. It was born by merging the OpenCensus and OpenTracing projects in May 2019. It is also an incubating CNCF project. OpenTelemetry is supported by multiple products and became a de-facto standard. It can collect data from a variety of open source and commercial sources. Check out the full list here: <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver"><span class="url">https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver</span></a>.</p>
    <p class="normal">By using a product that complies with OpenTelemetry, you are not locked in, and you will work with an API that may be familiar to your developers.</p>
    <p class="normal">There are instrumentation libraries for pretty much all the mainstream programming languages:</p>
    <ul>
      <li class="bulletList">C++</li>
      <li class="bulletList">.NET</li>
      <li class="bulletList">Erlang/Elixir</li>
      <li class="bulletList">Go</li>
      <li class="bulletList">Java</li>
      <li class="bulletList">JavaScript</li>
      <li class="bulletList">PHP</li>
      <li class="bulletList">Python</li>
      <li class="bulletList">Ruby</li>
      <li class="bulletList">Rust</li>
      <li class="bulletList">Swift</li>
    </ul>
    <h3 id="_idParaDest-642" class="heading-3">OpenTelemetry tracing concepts</h3>
    <p class="normal">We will focus here on the <a id="_idIndexMarker1434"/>tracing concept of OpenTelemetry and skip the logging and metrics concepts we covered earlier.</p>
    <p class="normal">The two main concepts are <strong class="keyWord">Span</strong> and <strong class="keyWord">Trace</strong>.</p>
    <p class="normal">A <strong class="keyWord">Span</strong> is the basic <a id="_idIndexMarker1435"/>unit of work or operation. It has a<a id="_idIndexMarker1436"/> name, start time, and duration. Spans can be nested if one operation starts another operation. Spans propagate with a unique ID and context. The <strong class="keyWord">Trace</strong> is an acyclic graph of Spans that originated from the same request and shares the same context. A <strong class="keyWord">Trace</strong> represents<a id="_idIndexMarker1437"/> the execution path of a request <a id="_idIndexMarker1438"/>throughout the system. The following diagram illustrates the relationship between a Trace and Spans:</p>
    <figure class="mediaobject"><img src="../Images/B18998_13_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.11: The relationship between Traces and Spans in OpenTelemetry</p>
    <p class="normal">Now that we understand what OpenTelemetry is about, let’s take a look at the Jaeger project.</p>
    <h2 id="_idParaDest-643" class="heading-2">Introducing Jaeger</h2>
    <p class="normal">Jaeger (<a href="https://www.jaegertracing.io/"><span class="url">https://www.jaegertracing.io/</span></a>) is yet<a id="_idIndexMarker1439"/> another CNCF-graduated project, just like Fluentd and <a id="_idIndexMarker1440"/>Prometheus. It completes the trinity of CNCF-graduated observability projects for Kubernetes. Jaeger was developed originally by Uber and quickly became the forerunner distributed tracing solution for Kubernetes.</p>
    <p class="normal">There are other <a id="_idIndexMarker1441"/>open-source <a id="_idIndexMarker1442"/>distributed tracing systems like Zipkin (<a href="https://zipkin.io"><span class="url">https://zipkin.io</span></a>) and SigNoz (<a href="https://signoz.io"><span class="url">https://signoz.io</span></a>). The inspiration for most of these systems (as well as Jaeger) is Google’s Dapper (<a href="https://research.google.com/pubs/pub36356.html"><span class="url">https://research.google.com/pubs/pub36356.html</span></a>). Cloud platforms provide their own tracers, like AWS X-Ray. There <a id="_idIndexMarker1443"/>are also multiple commercial products in<a id="_idIndexMarker1444"/> the<a id="_idIndexMarker1445"/> space:</p>
    <ul>
      <li class="bulletList">Aspecto (<a href="https://www.aspecto.io"><span class="url">https://www.aspecto.io</span></a>)</li>
      <li class="bulletList">Honeycomb (<a href="https://www.honeycomb.io"><span class="url">https://www.honeycomb.io</span></a>)</li>
      <li class="bulletList">Lightstep (<a href="http://lightstep.com"><span class="url">http://lightstep.com</span></a>)</li>
    </ul>
    <p class="normal">Jaeger’s strong points are:</p>
    <ul>
      <li class="bulletList">Scalable <a id="_idIndexMarker1446"/>design</li>
      <li class="bulletList">Supports multiple protocols – OpenTelemetry, OpenTracing, and Zipkin</li>
      <li class="bulletList">Light memory footprint</li>
      <li class="bulletList">Agents collect metrics over UDP</li>
      <li class="bulletList">Advanced sampling control</li>
    </ul>
    <h3 id="_idParaDest-644" class="heading-3">Jaeger architecture</h3>
    <p class="normal">Jaeger is a scalable system. It<a id="_idIndexMarker1447"/> can be deployed as a single binary with all its components and store the data in memory, but also as a distributed system where spans and traces are stored in persistent storage.</p>
    <p class="normal">Jaeger has several components that collaborate to provide a world-class distributed tracing experience. The following diagram illustrates the architecture:</p>
    <figure class="mediaobject"><img src="../Images/B18998_13_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.12: Jaeger architecture</p>
    <p class="normal">Let’s understand the purpose of each component.</p>
    <h4 class="heading-4">Client libraries</h4>
    <p class="normal">Originally, Jaeger had its own client <a id="_idIndexMarker1448"/>libraries that implemented the OpenTracing API in order to instrument a service or application for distributed tracing. Now, Jaeger recommends using the OpenTelemetry client libraries. The Jaeger client libraries have been retired.</p>
    <h4 class="heading-4">Jaeger agent</h4>
    <p class="normal">The agent is deployed locally<a id="_idIndexMarker1449"/> to each node. It listens to spans over UDP – which makes it pretty performant – batches them and sends them in bulk to the collector. This way, services don’t need to discover the collector or worry about connecting to it. Instrumented services simply send their spans to the local agent. The agent can also inform the client about sampling strategies.</p>
    <h4 class="heading-4">Jaeger collector</h4>
    <p class="normal">The collector receives traces from<a id="_idIndexMarker1450"/> all the agents. It is responsible for validating and transforming the traces. It then sends the traces to a data store to a Kafka instance, which enables async processing of traces.</p>
    <h4 class="heading-4">Jaeger ingester</h4>
    <p class="normal">The ingester indexes the traces for <a id="_idIndexMarker1451"/>easy and performant queries later and stores them in the data store, which can be a Cassandra or Elasticsearch cluster.</p>
    <h4 class="heading-4">Jaeger query</h4>
    <p class="normal">The Jaeger query service is<a id="_idIndexMarker1452"/> responsible for presenting a UI to query the traces and the spans that the collector put in storage.</p>
    <p class="normal">That covers Jaeger’s architecture and its components. Let’s see how to install and work with it.</p>
    <h2 id="_idParaDest-645" class="heading-2">Installing Jaeger</h2>
    <p class="normal">There are Helm charts to <a id="_idIndexMarker1453"/>install Jaeger and the Jaeger operator:</p>
    <pre class="programlisting gen"><code class="hljs">$ helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
"jaegertracing" has been added to your repositories
$ helm search repo jaegertracing
NAME                            CHART VERSION   APP VERSION DESCRIPTION
jaegertracing/jaeger            0.62.1          1.37.0      A Jaeger Helm chart for Kubernetes
jaegertracing/jaeger-operator   2.36.0          1.38.0      jaeger-operator Helm chart for Kubernetes
</code></pre>
    <p class="normal">The Jaeger operator requires <code class="inlineCode">cert-manager</code>, but doesn’t install it automatically. Let’s install it first:</p>
    <pre class="programlisting gen"><code class="hljs">$ helm repo add jetstack https://charts.jetstack.io
"jetstack" has been added to your repositories
$ helm install \
  cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.9.1 \
  --set installCRDs=true
NAME: cert-manager
LAST DEPLOYED: Mon Oct 17 10:28:43 2022
NAMESPACE: cert-manager
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
cert-manager v1.9.1 has been deployed successfully!
In order to begin issuing certificates, you will need to set up a ClusterIssuer
or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).
More information on the different types of issuers and how to configure them
can be found in our documentation:
https://cert-manager.io/docs/configuration/
For information on how to configure cert-manager to automatically provision
Certificates for Ingress resources, take a look at the `ingress-shim`
documentation:
https://cert-manager.io/docs/usage/ingress/
</code></pre>
    <p class="normal">Now, we can<a id="_idIndexMarker1454"/> install the Jaeger operator into the observability namespace:</p>
    <pre class="programlisting gen"><code class="hljs">$ helm install jaeger jaegertracing/jaeger-operator \
       -n observability --create-namespace
NAME: jaeger
LAST DEPLOYED: Mon Oct 17 10:30:58 2022
NAMESPACE: observability
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
jaeger-operator is installed.
Check the jaeger-operator logs
  export POD=$(kubectl get pods -l app.kubernetes.io/instance=jaeger -l app.kubernetes.io/name=jaeger-operator --namespace observability --output name)
  kubectl logs $POD --namespace=observability
</code></pre>
    <p class="normal">The deployment is called <code class="inlineCode">jaeger-jaeger-operator</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get deploy -n observability
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
jaeger-jaeger-operator   1/1     1            1           3m21s
</code></pre>
    <p class="normal">Now, we can <a id="_idIndexMarker1455"/>create a Jaeger instance using the Jaeger CRD. The operator watches for this custom resource and creates all the necessary resources. Here is the simplest possible Jaeger configuration. It uses the default <code class="inlineCode">AllInOne</code> strategy, which deploys a single pod that contains all the components (agent, collector, query, ingester, and Jaeger UI) and uses in-memory storage. This is suitable for local development and testing purposes:</p>
    <pre class="programlisting gen"><code class="hljs">$ cat &lt;&lt;EOF | k apply -f -
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: simplest
  namespace: observability
EOF
jaeger.jaegertracing.io/simplest created
$ k get jaegers -n observability
NAME       STATUS   VERSION   STRATEGY   STORAGE   AGE
simplest                                           5m54s
</code></pre>
    <p class="normal">Let’s bring up the Jaeger UI:</p>
    <pre class="programlisting gen"><code class="hljs">$ k port-forward deploy/simplest 8080:16686 -n observability
Forwarding from 127.0.0.1:8080 -&gt; 16686
Forwarding from [::1]:8080 -&gt; 16686
</code></pre>
    <p class="normal">Now, we can browse to <code class="inlineCode">http://localhost:8080</code> and see the Jaeger UI:</p>
    <figure class="mediaobject"><img src="../Images/B18998_13_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.13: Jaeger UI</p>
    <p class="normal">In the next chapter, <em class="chapterRef">Chapter 14</em>, <em class="italic">Utilizing Service Meshes</em>, we will see more of Jaeger and how to use it specifically to trace requests going through the mesh. Now, let’s turn our attention to troubleshooting using all the monitoring and observability mechanisms we discussed.</p>
    <h1 id="_idParaDest-646" class="heading-1">Troubleshooting problems</h1>
    <p class="normal">Troubleshooting a complex distributed system<a id="_idIndexMarker1456"/> is no picnic. Abstractions, separation of concerns, information hiding, and encapsulation are great during development, testing, and when making changes to the system. But when things go wrong, you need to cross all those boundaries and layers of abstraction from the user action in their app, through the entire stack, all the way to the infrastructure, crossing all the business logic, asynchronous processes, legacy systems, and third-party integrations. This is a challenge even with large monolithic systems, but even more so with microservice-based distributed systems. Monitoring will assist you, but let’s talk first about preparation, processes, and best practices.</p>
    <h2 id="_idParaDest-647" class="heading-2">Taking advantage of staging environments</h2>
    <p class="normal">When building a<a id="_idIndexMarker1457"/> large system, developers work on their local machines (ignoring the cloud development environment here) and, eventually, the code is deployed to the production environment. But there are a few steps between those two extremes. Complex systems operate in an environment that is not easy to duplicate locally. </p>
    <p class="normal">You should test changes to code or configuration in an environment that is similar to your production environment. This is your staging environment, where you should catch most problems that can’t be caught by the developer running tests locally in their development environment.</p>
    <p class="normal">The software delivery process should accommodate the detection of bad code and configuration as early as possible. But, sometimes, bad changes will be detected only in production and cause an incident. You should have an incident management process in place as well, which typically involves reverting to the previous version of whatever component caused the issue and then trying to find the root cause by looking at logs, metrics, and traces – sometimes, by debugging in the staging environment too.</p>
    <p class="normal">But, sometimes, the problem is not with your code or configuration. In the end, your Kubernetes cluster runs on nodes (yes, even if it’s managed), and those nodes can suffer many issues.</p>
    <h2 id="_idParaDest-648" class="heading-2">Detecting problems at the node level</h2>
    <p class="normal">In Kubernetes’ conceptual<a id="_idIndexMarker1458"/> model, the unit of work is the pod. However, pods are scheduled on nodes. When it comes to monitoring and the reliability of the infrastructure, the nodes are what require the most attention, as Kubernetes itself (the scheduler, replica sets, and horizontal pod autoscalers) takes care of the pods. The kubelet is aware of many issues on the nodes and it will update the API server. You can see the node status and if it’s ready using this command:</p>
    <pre class="programlisting gen"><code class="hljs">$ k describe no kind-control-plane | grep Conditions -A 6
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 21 Oct 2022 01:09:33 -0700   Mon, 17 Oct 2022 10:27:24 -0700   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 21 Oct 2022 01:09:33 -0700   Mon, 17 Oct 2022 10:27:24 -0700   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 21 Oct 2022 01:09:33 -0700   Mon, 17 Oct 2022 10:27:24 -0700   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 21 Oct 2022 01:09:33 -0700   Mon, 17 Oct 2022 10:27:52 -0700   KubeletReady                 kubelet is posting ready status
</code></pre>
    <p class="normal">Note the last condition, <code class="inlineCode">Ready</code>. This means that Kubernetes can schedule pending pods to this node.</p>
    <p class="normal">But, there might be problems that the kubelet can’t detect. Some of the problems are:</p>
    <ul>
      <li class="bulletList">Bad CPU</li>
      <li class="bulletList">Bad memory</li>
      <li class="bulletList">Bad disk</li>
      <li class="bulletList">Kernel deadlock</li>
      <li class="bulletList">Corrupt filesystem</li>
      <li class="bulletList">Problems with the container runtime (e.g., Docker daemon)</li>
    </ul>
    <p class="normal">We need another solution. Enter the node problem detector.</p>
    <p class="normal">The node <a id="_idIndexMarker1459"/>problem detector is a pod that runs on every node. It needs to solve a difficult problem. It must be able to detect various low-level problems across different environments, different hardware, and different operating systems. It must be reliable enough not to be affected itself (otherwise, it can’t report on problems), and it needs to have a relatively low overhead to avoid spamming the control plane. The source code is at <a href="https://github.com/kubernetes/node-problem-detector"><span class="url">https://github.com/kubernetes/node-problem-detector</span></a>.</p>
    <p class="normal">The most natural way is to deploy the node problem detector as a DaemonSet, so every node always has a node problem detector running on it. On Google’s GKE clusters, it runs as an add-on.</p>
    <h3 id="_idParaDest-649" class="heading-3">Problem daemons</h3>
    <p class="normal">The problem with the node <a id="_idIndexMarker1460"/>problem detector (pun intended) is that there are too many problems that it needs to handle. Trying to cram all of them into a single codebase can lead to a complex, bloated, and never-stabilizing codebase. The design of the node problem detector calls for the separation of the core functionality of reporting node problems to the master from the specific problem detection. The reporting API is based on generic conditions and events. The problem detection should be done by separate problem daemons (each in its own container). This way, it is possible to add and evolve new problem detectors without impacting the core node problem detector. In addition, the control plane may have a remedy controller that can resolve some node problems automatically, therefore implementing self-healing.</p>
    <p class="normal">At this time, problem daemons are baked into the node problem detector binary, and they execute as Goroutines, so you don’t get the benefits of the loosely-coupled design just yet. In the future, each problem daemon will run in its own container.</p>
    <p class="normal">In addition to problems with nodes, the other area where things can break down is networking. The various monitoring tools we discussed earlier can help us identify problems across the infrastructure, in our code, or with third-party dependencies.</p>
    <p class="normal">Let’s talk about the various options in our toolbox, how they compare, and how to utilize them for maximal effect.</p>
    <h2 id="_idParaDest-650" class="heading-2">Dashboards vs. alerts</h2>
    <p class="normal">Dashboards <a id="_idIndexMarker1461"/>are purely for humans. The idea of a good dashboard is to provide, at one glance, a lot of useful information about the state of the system or a particular component. There are many user experience elements to designing good dashboards, just like designing any user interface. Monitoring dashboards can cover a lot of data across many components, over long time periods, and may support drilling down into finer and finer levels of detail.</p>
    <p class="normal">Alerts, on the other hand, are periodically checking certain conditions (often based on metrics) and, when triggered, can either result in automatic resolution of the cause of the alert or eventually notify a human, who will probably start the investigation by looking at some dashboards.</p>
    <p class="normal">Self-healing systems can handle certain alerts automatically (or ideally, resolve the issue before an alert is even raised). Humans will typically be involved in troubleshooting. Even in cases where the system automatically recovered from a problem at some point, a human will review the actions the system took and verify that the current behavior, including the automatic recovery from problems, is adequate.</p>
    <p class="normal">In many<a id="_idIndexMarker1462"/> cases, severe problems (a.k.a. incidents) discovered by humans looking at dashboards (not scalable) or notified by alerts will require some investigation, remediation, and later, post-mortem. In all those stages, the next layer of monitoring comes into play.</p>
    <h2 id="_idParaDest-651" class="heading-2">Logs vs metrics vs. error reports</h2>
    <p class="normal">Let’s understand where <a id="_idIndexMarker1463"/>each of these tools excels and how to best combine their strengths to debug difficult problems. Let’s assume we have good test coverage and our business/domain logic code is by and large correct. We run into problems in the production environment. There could be several types of problems that happen only in production:</p>
    <ul>
      <li class="bulletList">Misconfiguration (production configuration is incorrect or out of date)</li>
      <li class="bulletList">Infrastructure provisioning</li>
      <li class="bulletList">Insufficient permissions and access to data, services, or third-party integrations</li>
      <li class="bulletList">Environment-specific code</li>
      <li class="bulletList">Software bugs that are exposed by production inputs</li>
      <li class="bulletList">Scalability and performance issues</li>
    </ul>
    <p class="normal">That’s quite a list and it’s probably not even complete. Typically, when something goes wrong, it is in response to some change. What kind of changes are we talking about? Here are a few:</p>
    <ul>
      <li class="bulletList">Deployment of a new version of the code</li>
      <li class="bulletList">Dynamic re-configuration of a deployed application</li>
      <li class="bulletList">New users or existing users changing the way they interact with the system</li>
      <li class="bulletList">Changes to the underlying infrastructure (e.g., by the cloud provider)</li>
      <li class="bulletList">A new path in the code is utilized for the first time (e.g., fallback to another region)</li>
    </ul>
    <p class="normal">Since there is such a broad spectrum of problems and causes, it is difficult to suggest a linear path to resolution. For example, if the failure caused an error, then looking at an error report might be the best starting point. But, if the issue is that some action that was supposed to happen didn’t happen, then there is no error to look at. In this case, it might make sense to look at the logs and compare them to the logs from a previous successful request. In case of infrastructure or scalability problems, metrics may give us the best initial insight.</p>
    <p class="normal">The bottom line is that debugging distributed systems requires using multiple tools together in the pursuit of the ever-elusive root cause.</p>
    <p class="normal">Of course, in <a id="_idIndexMarker1464"/>distributed systems with lots of components and microservices, it is not even clear where to look. This is where distributed tracing shines and can help us narrow down and identify the culprit.</p>
    <h2 id="_idParaDest-652" class="heading-2">Detecting performance and root cause with distributed tracing</h2>
    <p class="normal">With <a id="_idIndexMarker1465"/>distributed tracing in place, every request will generate a trace with a graph of spans. Jaeger uses sampling of 1/1000 by default so, once in a blue moon, issues might escape it, but for persistent problems, we will be able to follow the path of a request, see how long each span takes, and if the processing of a request bails out for some reason, it will be very easy to notice. At this point, you’re back to the logs, metrics, and errors to hunt the root cause.</p>
    <p class="normal">As you can see, troubleshooting problems in a complex system like Kubernetes is far from trivial. You need comprehensive observability in place including logging, metrics, and distributed tracing. You also need deep knowledge and understanding of your system to be able to configure, monitor, and mitigate issues quickly and reliably.</p>
    <h1 id="_idParaDest-653" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we covered the topics of monitoring, observability, and troubleshooting. We started with a review of the various aspects of monitoring: logs, metrics, error reporting, and distributed tracing. Then, we discussed how to incorporate monitoring capabilities into your Kubernetes cluster. We looked at several CNCF projects, like Fluentd for log aggregation, Prometheus for metrics collection and alert management, Grafana for visualization, and Jaeger for distributed tracing. Then, we explored troubleshooting large distributed systems. We realized how difficult it can be and why we need so many different tools to conquer the issues.</p>
    <p class="normal">In the next chapter, we will take it to the next level and dive into service meshes. I’m super excited about service meshes because they take much of the complexity related to cloud-native microservice-based applications and externalize them outside of the microservices. That has a lot of real-world value.</p>
    <h1 id="_idParaDest-654" class="heading-1">Join us on Discord!</h1>
    <p class="normal">Read this book alongside other users, cloud experts, authors, and like-minded professionals.</p>
    <p class="normal">Ask questions, provide solutions to other readers, chat with the authors via. Ask Me Anything sessions and much more.</p>
    <p class="normal">Scan the QR code or visit the link to join the community now.</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img src="../Images/QR_Code844810820358034203.png" alt=""/></p>
  </div>
</body></html>