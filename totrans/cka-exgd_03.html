<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer060">
<h1 class="chapter-number" id="_idParaDest-54"><a id="_idTextAnchor059"/>3</h1>
<h1 id="_idParaDest-55"><a id="_idTextAnchor060"/>Maintaining Kubernetes Clusters</h1>
<p>Kubernetes has been the most vibrant platform in the community over the past few years and it has maintained a good release cadence, which makes Kubernetes maintenance important in order to enable organizations that work with Kubernetes to take advantage of its latest features. This chapter introduces different approaches for maintaining a Kubernetes cluster while providing practical lessons on performing upgrades for Kubernetes clusters, etcd backup, and etcd restore. It covers 25% of the CKA exam content.</p>
<p>In this chapter, we’re going to cover the following main topics: </p>
<ul>
<li>Demystifying Kubernetes cluster maintenance </li>
<li>Performing a version upgrade on a Kubernetes cluster using kubeadm</li>
<li>Working with etcd</li>
<li>Backing up and restoring etcd</li>
</ul>
<h1 id="_idParaDest-56"><a id="_idTextAnchor061"/>Demystifying Kubernetes cluster maintenance</h1>
<p>Before <a id="_idIndexMarker190"/>April 2021, Kubernetes had maintained quite a steady and sound cadence of quarterly releases throughout the year. Despite the strong growth and incredible popularity in the community, this was reduced to three releases per year. Fewer releases still mean that a regular maintenance window should be scheduled within the organization for the upgrade of security patches, and to take full advantage of enhancements and new features. </p>
<p>A general maintenance window contains the upgraded Kubernetes cluster version. We can easily break the task to be performed into two parts: </p>
<ul>
<li>Upgrading the master node, which contains the control plane </li>
<li>Upgrading the worker node </li>
</ul>
<p>Upgrading the master node is simple if you have only one master node. However, most enterprise-grade customers may have a couple of master nodes for better resilience. We should be aware that working with an organization as a Kubernetes administrator is sometimes more challenging than working with other ones. A general reference of master node management would be the two typologies of high availability mentioned in <a href="B18201_02.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Installing and Configuring Kubernetes Clusters</em>. The conventional way to upgrade the master node is to upgrade one at a time regardless of the number of master nodes in your current cluster. You will need to upgrade the kubeadm and kubectl versions. </p>
<p>When it comes to worker node upgrade, as we mentioned in the previous chapter, the worker node is where your workloads are actually running; therefore, you need to upgrade both the kubeadm and kubelet versions. Keep in mind that you need to upgrade one at a time when it comes to multiple worker nodes available in the current Kubernetes cluster. </p>
<p>If you have a separate etcd cluster set up, you will need to upgrade the etcd store version, which is not covered in the CKA exam. In general, you need to check out the official documentation to know more about Kubernetes components and version compatibility here: <a href="https://kubernetes.io/releases/version-skew-policy/">https://kubernetes.io/releases/version-skew-policy/</a>.</p>
<p>Another general task for Kubernetes cluster maintenance is backup and restore with the etcd store. The etcd stores cluster data that includes cluster state information such as pod state data, node state data, and the configurations critical for Kubernetes. In most cases, as a Kubernetes administrator, you will need to perform the following two key tasks: </p>
<ul>
<li>Back up the etcd store regularly </li>
<li>Restore the etcd from cluster failure</li>
</ul>
<p>The <a id="_idIndexMarker191"/>following section will firstly walk you through the general process of upgrading the Kubernetes cluster with kubeadm. This is one of the most time-consuming questions in the actual CKA exam. Make sure you practice it a few times until you master the general upgrade process as well as how to perform upgrade tasks by following the official Kubernetes documentation. Note that update policies vary for managed Kubernetes distributions by cloud vendors. Please check their respective official documentation. </p>
<p>Furthermore, we’ll take a look at how to back up and restore an etcd cluster. </p>
<h1 id="_idParaDest-57"><a id="_idTextAnchor062"/>Upgrading a Kubernetes cluster using kubeadm</h1>
<p>Kubernetes versions follow <a id="_idIndexMarker192"/>semantic versioning, and are expressed in three parts:</p>
<ol>
<li>Major version</li>
<li>Minor version</li>
<li>Patch version </li>
</ol>
<p>For example, Kubernetes version 1.23.3 means that it is Kubernetes 1.23 minor version with patch number 3. Similarly, 1.22 and 1.21 are both minor versions like 1.23.</p>
<p>At the time of writing this book, Kubernetes 1.19+ has one year of path support. That means for Kubernetes 1.23, released in January 2022, the end of support will be February 2023. For Kubernetes 1.8 and older, the support patch was shortened to roughly 9 months instead. <strong class="bold">Special interest group</strong> (<strong class="bold">SIG</strong>) releases<a id="_idIndexMarker193"/> manage the Kubernetes release cycle, and the best way to keep track of the release schedule is to follow them at <a href="https://github.com/kubernetes/sig-release/tree/master/releases">https://github.com/kubernetes/sig-release/tree/master/releases</a> and read the change log at <a href="https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG">https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG</a> to keep up to date. </p>
<p>If you would like to upgrade a cluster to a targeted version, check out supported versions at <a href="https://kubernetes.io/releases/version-skew-policy/#supported-versions">https://kubernetes.io/releases/version-skew-policy/#supported-versions</a>.</p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor063"/>Upgrading the master node </h2>
<p>Before you upgrade the<a id="_idIndexMarker194"/> master node, make sure you know the purpose of your upgrade and have backed up any important components. It is recommended that you start with checking out the current version and then determining which version to upgrade to. Once decided, we’ll perform the following actions to upgrade the master node as depicted in <em class="italic">Figure 2.1</em>, including upgrading with kubeadm and interacting with Kubernetes nodes: </p>
<div>
<div class="IMG---Figure" id="_idContainer041">
<img alt="Figure 3.1 – Master node upgrade process  " height="1555" src="image/Figure_3.01_B18201.jpg" width="994"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.1 – Master node upgrade process </p>
<p>Let’s start by<a id="_idIndexMarker195"/> checking out the current version with the following commands once we’re in the master node: </p>
<p class="source-code">   kubeadm version</p>
<p class="source-code">   kubectl version </p>
<p>From the output, we know that we are currently on Kubernetes 2.23.2: </p>
<p class="source-code">Client Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.2", GitCommit:"e6c093d87ea4cbb530a7b2ae91e54c0842d8308a", GitTreeState:"clean", BuildDate:"2022-02-16T12:38:05Z", GoVersion:"go1.17.7", Compiler:"gc", Platform:"linux/amd64"}</p>
<p>Let’s check out the latest versions available with the following commands: </p>
<p class="source-code">  apt update</p>
<p class="source-code">  apt-cache madison kubeadm</p>
<p>Now we know the latest versions available: </p>
<div>
<div class="IMG---Figure" id="_idContainer042">
<img alt="Figure 3.2 – Available versions   " height="170" src="image/Figure_3.02_B18201.jpg" width="658"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.2 – Available versions  </p>
<p>Once we have <a id="_idIndexMarker196"/>made up our mind about which version we want to upgrade to, let’s start prepping for the upgrade process: </p>
<ol>
<li>We will start by upgrading kubeadm where we will need to use the following command to replace <em class="italic">x</em> in 1.23.x-00 with the latest patch version, which is 1.23.3 in our case: </li>
</ol>
<p class="source-code"><strong class="bold">apt-mark unhold kubeadm &amp;&amp; \</strong></p>
<p class="source-code"><strong class="bold">apt-get update &amp;&amp; apt-get install -y kubeadm=1.23.3-00 &amp;&amp; \</strong></p>
<p class="source-code"><strong class="bold">apt-mark hold kubeadm</strong></p>
<p>The output of the <strong class="source-inline">apt-mark</strong> command is the following: </p>
<p class="source-code"><strong class="bold">kubeadm set on hold.</strong></p>
<ol>
<li>Now we can check the version of kubeadm with the <strong class="source-inline">kubeadm version</strong> command and see whether it’s 1.23.3:</li>
</ol>
<p class="source-code"><strong class="bold">Client Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.2", GitCommit:"e6c093d87ea4cbb530a7b2ae91e54c0842d8308a", GitTreeState:"clean", BuildDate:"2022-02-16T12:38:05Z", GoVersion:"go1.17.7", Compiler:"gc", Platform:"linux/amd64"}</strong></p>
<ol>
<li>We use the <strong class="source-inline">kubeadm upgrade plan</strong> command to check whether the current cluster can be upgraded and the available versions that it can be upgraded to: </li>
</ol>
<p class="source-code"><strong class="bold">    kubeadm upgrade plan</strong></p>
<p>As shown in <em class="italic">Figure 3.3</em>, I can upgrade the kubelet and control plane components such as the API server, scheduler, and controller manager from 1.23.2 to 1.23.3:</p>
<div>
<div class="IMG---Figure" id="_idContainer043">
<img alt="Figure 3.3 – kubeadm upgrade plan  " height="197" src="image/Figure_3.03_B18201.jpg" width="806"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.3 – kubeadm upgrade plan </p>
<ol>
<li>If we <a id="_idIndexMarker197"/>decide to take action to upgrade the current cluster from 1.23.2 to 1.23.3, we can use the following command. Note that after <strong class="source-inline">apply</strong>, you just replace 1.23.3 for any future available versions that you wish to upgrade to: </li>
</ol>
<p class="source-code"><strong class="bold">    kubeadm upgrade apply v1.23.3</strong></p>
<p class="callout-heading">Important Note</p>
<p class="callout">To perform the upgrade operation smoothly, we recommend you get root permission in the exam by running the <strong class="source-inline">sudo su</strong> command.</p>
<p class="callout">In your daily upgrade task, you can use <strong class="source-inline">sudo</strong> and input your password to perform this operation. </p>
<p>Once you have given the command, you will get a message stating that the upgrade was a success:</p>
<div>
<div class="IMG---Figure" id="_idContainer044">
<img alt="Figure 3.4 – Control plane successfully upgraded " height="55" src="image/Figure_3.04_B18201.jpg" width="982"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.4 – Control plane successfully upgraded</p>
<ol>
<li>We then need to cordon the node, so we drain the workloads to prepare the node for maintenance. We cordon a node called <strong class="source-inline">cloudmelonplaysrv</strong> with the following command: </li>
</ol>
<p class="source-code"><strong class="bold">kubectl drain cloudmelonplaysrv --ignore-daemonsets</strong></p>
<p>It will display a<a id="_idIndexMarker198"/> bunch of pods being evicted, which means those pods are being eliminated from the cordoned worker nodes: </p>
<div>
<div class="IMG---Figure" id="_idContainer045">
<img alt="Figure 3.5 – Draining workloads on the node " height="240" src="image/Figure_3.05_B18201.jpg" width="757"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.5 – Draining workloads on the node</p>
<p>If you’re using the <strong class="source-inline">kubectl get no</strong> command, the node will be marked as <strong class="source-inline">schedulingDisabled</strong>. </p>
<ol>
<li>We use the following command to upgrade the kubelet and kubectl:</li>
</ol>
<p class="source-code"><strong class="bold">apt-mark unhold kubelet kubectl &amp;&amp; \</strong></p>
<p class="source-code"><strong class="bold">apt-get update &amp;&amp; apt-get install -y kubelet=1.23.3-00 kubectl=1.23.3-00 &amp;&amp; \</strong></p>
<p class="source-code"><strong class="bold">apt-mark hold kubelet kubectl</strong></p>
<ol>
<li>Restart the kubelet:</li>
</ol>
<p class="source-code"><strong class="bold">sudo systemctl daemon-reload</strong></p>
<p class="source-code"><strong class="bold">sudo systemctl restart kubelet</strong></p>
<ol>
<li>Now we can uncordon the node and it will make the workloads schedulable again on the node that’s being upgraded, called <strong class="source-inline">cloudmelonplaysrv</strong>: </li>
</ol>
<p class="source-code"><strong class="bold">kubectl uncordon cloudmelonplaysrv</strong></p>
<p>This <a id="_idIndexMarker199"/>command will return the node that is now shown as <strong class="source-inline">uncordoned</strong>. </p>
<h2 id="_idParaDest-59"><a id="_idTextAnchor064"/>Upgrading the worker node </h2>
<p>Since the worker <a id="_idIndexMarker200"/>node is where your workloads are actually up and running, we need to perform an upgrade one at a time and then replicate the same operation to all the other worker nodes available in the current Kubernetes cluster. <em class="italic">Figure 3.6</em> depicts the general upgrade workflow: </p>
<div>
<div class="IMG---Figure" id="_idContainer046">
<img alt="Figure 3.6 – Draining workloads on the node " height="1555" src="image/Figure_3.06_B18201.jpg" width="994"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.6 – Draining workloads on the node</p>
<ol>
<li>Let’s <a id="_idIndexMarker201"/>start with upgrading the kubeadm from 1.23.2 to 1.23.3 with the following command: </li>
</ol>
<p class="source-code"><strong class="bold">  apt-mark unhold kubeadm &amp;&amp; \</strong></p>
<p class="source-code"><strong class="bold">  apt-get update &amp;&amp; apt-get install -y kubeadm=1.23.3-00 &amp;&amp; \</strong></p>
<p class="source-code"><strong class="bold">  apt-mark hold kubeadm</strong></p>
<ol>
<li>For a worker node, we upgrade the kubelet, which also upgrades the local kubelet configuration, with the following command: </li>
</ol>
<p class="source-code"><strong class="bold">  sudo kubeadm upgrade node</strong></p>
<ol>
<li>Similarly, we need to cordon the node so we drain the workloads to prepare the node for maintenance. Here, we are cordoning a node called <strong class="source-inline">cloudmelonplayclient</strong> using the following command: </li>
</ol>
<p class="source-code"><strong class="bold">kubectl drain cloudmelonplayclient --ignore-daemonsets</strong></p>
<p>We can then use the <strong class="source-inline">kubectl get no</strong> command to check the node status. It will be marked as <strong class="source-inline">schedulingDisabled</strong>. </p>
<ol>
<li>We use<a id="_idIndexMarker202"/> the following command to upgrade the kubelet and kubectl just as we did for the master node:</li>
</ol>
<p class="source-code"><strong class="bold">apt-mark unhold kubelet kubectl &amp;&amp; \</strong></p>
<p class="source-code"><strong class="bold">apt-get update &amp;&amp; apt-get install -y kubelet=1.23.3-00 kubectl=1.23.3-00 &amp;&amp; \</strong></p>
<p class="source-code"><strong class="bold">apt-mark hold kubelet kubectl</strong></p>
<ol>
<li>Restart the kubelet for the changes to take effect:</li>
</ol>
<p class="source-code"><strong class="bold">sudo systemctl daemon-reload</strong></p>
<p class="source-code"><strong class="bold">sudo systemctl restart kubelet</strong></p>
<ol>
<li>Finally, we can <strong class="source-inline">uncordon</strong> the node and it will make the workloads schedulable again on the node called <strong class="source-inline">cloudmelonplayclient</strong>. It will return the node that is now<a id="_idIndexMarker203"/> shown as <strong class="source-inline">uncordoned</strong>:</li>
</ol>
<p class="source-code"><strong class="bold">kubectl uncordon cloudmelonplayclient</strong></p>
<p>We have now concluded the upgrade process for worker nodes. After the upgrade process, please make sure you use the <strong class="source-inline">kubectl get nodes</strong> command to make sure all the nodes have the <strong class="source-inline">ready</strong> status. </p>
<h1 id="_idParaDest-60"><a id="_idTextAnchor065"/>Working with etcd</h1>
<p>Cluster data is stored <a id="_idIndexMarker204"/>in a key-value store in a Kubernetes cluster called etcd. The cluster data includes cluster state information such as pod state data, node state data, and configurations. As this data is critical for Kubernetes to orchestrate the workloads to the desired state, it stands to reason that it should be backed up periodically. </p>
<p>To access the etcd cluster inside the Kubernetes cluster, we can run the following command: </p>
<p class="source-code">  kubectl get po -n kube-system</p>
<p>This will list all the <a id="_idIndexMarker205"/>pods currently running in the <strong class="source-inline">kube-system</strong> namespace: </p>
<div>
<div class="IMG---Figure" id="_idContainer047">
<img alt="Figure 3.7 – Check the current etcd pod status " height="262" src="image/Figure_3.07_B18201.jpg" width="1224"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.7 – Check the current etcd pod status</p>
<p>In the following sections, we’ll take a closer look at the etcd cluster pod and learn all the related information that will be useful in the actual CKA exam. </p>
<h2 id="_idParaDest-61"><a id="_idTextAnchor066"/>Exploring the ETCD cluster pod </h2>
<p>To get a closer look at the <a id="_idIndexMarker206"/>etcd pod that we have, use the following command: </p>
<p class="source-code">  kubectl describe po &lt;etcd-podname&gt; -n kube-system</p>
<p>For example, to get detailed information for an etcd pod called <strong class="source-inline">etcd-cloudmelonplaysrv</strong>, the command would be as follows:</p>
<p class="source-code">  kubectl describe po etcd-cloudmelonplaysrv -n kube-system</p>
<p>It returns the following output: </p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<img alt="Figure 3.8 – Check the current etcd pod  " height="1097" src="image/Figure_3.08_B18201.jpg" width="954"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.8 – Check the current etcd pod </p>
<p>In the figure, you can<a id="_idIndexMarker207"/> see the following important information about etcd:</p>
<pre class="source-code">
      etcd
      --advertise-client-urls=https://172.16.16.129:2379
      --cert-file=/etc/ubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --initial-advertise-peer-urls=https://172.16.16.129:2380
      --initial-cluster=cloudmelonplaysrv=https://172.16.16.129:2380
      --key-file=/etc/ubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.16.16.129:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.16.16.129:2380
      --name=cloudmelonplaysrv
      --peer-cert-file=/etc/ubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/ubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/ubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/ubernetes/pki/etcd/ca.crt
    State:          Running</pre>
<p>Among all the configurable parameters, the following will come in handy in your daily work with etcd: </p>
<ul>
<li><strong class="source-inline">--advertise-client-urls</strong> tells etcd to accept incoming requests from the clients. It accepts a list of URLs.</li>
<li><strong class="source-inline">--cert-file</strong> is where we specify the client server <strong class="source-inline">TLS cert</strong> file path.</li>
<li><strong class="source-inline">--key-file</strong> is where we specify the client server <strong class="source-inline">TLS key</strong> file path.</li>
<li><strong class="source-inline">- -trusted-ca-file</strong> is where we specify the client server <strong class="source-inline">TLS trusted CA cert</strong> file path.</li>
</ul>
<p>These are key flags<a id="_idIndexMarker208"/> that will authenticate your request from the client with secure communication. You will need them to check the etcd status, backup, and restore etcd cluster. </p>
<p class="callout-heading">Important Note</p>
<p class="callout">Access to etcd is equivalent to getting root permission in the cluster. We make sure the authentication request is only going through the API server.</p>
<p>To know more about other configurable parameters, please check out <a href="https://etcd.io/docs/v3.5/op-guide/configuration/">https://etcd.io/docs/v3.5/op-guide/configuration/</a>.</p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor067"/>Listing etcd cluster members</h2>
<p>With the <a id="_idIndexMarker209"/>information that we acquired from the <strong class="source-inline">kubectl describe pod</strong> command, we can list the members of the etcd cluster: </p>
<p class="source-code">   kubectl exec etcd-cloudmelonplaysrv -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl member list --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key"</p>
<p>It returns the information about members. In our case, we have only one result because we are working with a single master node. Our command will look like the following: </p>
<p class="source-code">8d1f17827821818f, started, cloudmelonplaysrv, https://172.16.16.129:2380, https://172.16.16.129:2379, false</p>
<p>The output describes columns such as <strong class="source-inline">ID</strong> and <strong class="source-inline">Status</strong> of the etcd cluster, the etcd cluster name, and the peer and client address.</p>
<p>You can form the output automatically in tabular form with <strong class="source-inline">--write-out=table</strong>. It will look like this: </p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<img alt="Figure 3.9 – The current etcd member list  " height="65" src="image/Figure_3.09_B18201.jpg" width="854"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.9 – The current etcd member list </p>
<p>Notice that the <a id="_idIndexMarker210"/>client address is the same as the value of the <strong class="source-inline">--advertise-client-urls</strong> URL in the output of <strong class="source-inline">kubectl describe pod</strong>. </p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor068"/>Checking the etcd cluster status</h2>
<p>You can use the<a id="_idIndexMarker211"/> following command to check the etcd cluster status and write the output in tabular form. Note that using the correct etcdctl API version, we’re on API version 3 in the following example: </p>
<p class="source-code">ETCDCTL_API=3 etcdctl endpoint status</p>
<p>The following command is used to access an etcd pod from the Kubernetes cluster and check out the status of the etcd pod in the multi-node etcd cluster: </p>
<p class="source-code">  kubectl -n kube-system exec &lt;etcd-podname&gt; -- sh -c "ETCDCTL_API=3 etcdctl endpoint status --write-out=table --endpoints=https://&lt;IP1&gt;:2379,https://&lt;IP2&gt;:2379,https://&lt;IP3&gt;:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key"</p>
<p>You can use the information that you acquired from <strong class="source-inline">etcdctl member list</strong> in this command:</p>
<ul>
<li><strong class="source-inline">ETCDCTL_API</strong> is the etcdctl version.</li>
<li><strong class="source-inline">- - endpoints</strong> are the client addresses of your etcd members if you have multiple master nodes. </li>
</ul>
<p>In this chapter, however, we are showing off a single master node and it contains only one etcd member. Therefore, this command to access an etcd pod called <strong class="source-inline">etcd-cloudmelonplaysrv</strong> from the Kubernetes cluster and check out the status of the etcd pod will look like this: </p>
<p class="source-code">  kubectl -n kube-system exec etcd-cloudmelonplaysrv -- sh -c "ETCDCTL_API=3 etcdctl endpoint status --endpoints=https://172.16.16.129:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --write-out=table"</p>
<p>It will look like the following output:</p>
<div>
<div class="IMG---Figure" id="_idContainer050">
<img alt="Figure 3.10 – The current etcd member list  " height="69" src="image/Figure_3.10_B18201.jpg" width="1053"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.10 – The current etcd member list </p>
<p>From the output of <strong class="source-inline">kubectl describe pod &lt; etcd-podname&gt;</strong>, we also learn that we have two <strong class="source-inline">listen client</strong> IPs:</p>
<p class="source-code">  --listen-client-urls=https://127.0.0.1:2379,https://172.16.16.129:2379</p>
<p>As we’re <a id="_idIndexMarker212"/>checking the etcd cluster status inside the Kubernetes cluster, we can also use the internal endpoint address <strong class="source-inline">https://127.0.0.1:2379</strong> to check the etcd cluster status. The following command can be used to access an etcd pod named <strong class="source-inline">etcd-cloudmelonplaysrv</strong> from the Kubernetes cluster and check out the status of the etcd pod with the internal endpoint: </p>
<p class="source-code">  kubectl -n kube-system exec etcd-cloudmelonplaysrv -- sh -c "ETCDCTL_API=3 etcdctl endpoint status --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --write-out=table"</p>
<p>And it returns the information regarding the etcd cluster: </p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<img alt="Figure 3.11 – The current etcd member list  " height="70" src="image/Figure_3.11_B18201.jpg" width="1025"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.11 – The current etcd member list </p>
<p>In the following section, we’ll explore interacting with the etcd cluster from the client outside of the Kubernetes cluster.</p>
<h2 id="_idParaDest-64"><a id="_idTextAnchor069"/>Installing etcd </h2>
<p>To access etcd outside<a id="_idIndexMarker213"/> of the Kubernetes cluster, you will need to install etcdctl. You can do so by following the instructions in this section. Please note, however, that this is not part of the CKA exam.</p>
<p>To get started, we’ll need to get the etcd binary: </p>
<p class="source-code"> wget https://github.com/etcd-io/etcd/releases/download/v3.4.18/etcd-v3.4.18-linux-amd64.tar.gz</p>
<p class="source-code">tar xvf etcd-v3.4.18-linux-amd64.tar.gz</p>
<p class="source-code">sudo mv etcd-v3.4.18-linux-amd64/etcd* /usr/local/bin</p>
<p>Once you finish the installation, you can use the following command to verify the current version: </p>
<p class="source-code">    etcdctl version</p>
<p>The command returns the current etcdctl client version and the API version in the following manner: </p>
<div>
<div class="IMG---Figure" id="_idContainer052">
<img alt="Figure 3.12 – Check the current etcdctl version outside of the Kubernetes cluster " height="48" src="image/Figure_3.12_B18201.jpg" width="348"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.12 – Check the current etcdctl version outside of the Kubernetes cluster</p>
<p>Similarly, you can <a id="_idIndexMarker214"/>use the following command to check the kubectl version in the Kubernetes cluster. When you’re using the <strong class="source-inline">kubectl exec</strong> command, it executes directly on the pod named <strong class="source-inline">etcd-cloudmelonplaysrv</strong> located in the <strong class="source-inline">kube-system</strong> namespace. We can use the following command to execute the <strong class="source-inline">etcdctl version</strong> Bash command to get the version of the etcd store: </p>
<p class="source-code">    kubectl exec etcd-cloudmelonplaysrv -n kube-system -- sh -c "etcdctl version"</p>
<p>The returned result is similar to the etcdctl client version and the API version: </p>
<div>
<div class="IMG---Figure" id="_idContainer053">
<img alt="Figure 3.13 – Check the current etcdctl version in the Kubernetes cluster " height="47" src="image/Figure_3.13_B18201.jpg" width="773"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.13 – Check the current etcdctl version in the Kubernetes cluster</p>
<p>Similarly, once you <a id="_idIndexMarker215"/>have etcdctl installed, you can check the etcd store status by running the following command and you’ll get the endpoint status: </p>
<p class="source-code"><strong class="source-inline"> </strong>    kubectl exec etcd-cloudmelonplaysrv -n kube-system -- sh -c " etcdctl --write-out=table --endpoints=$ENDPOINTS endpoint status "</p>
<p class="source-code">If you want to make sure the etcd store is healthy, using the following command with the endpoint from the Previous command : </p>
<p class="source-code">   kubectl exec etcd-cloudmelonplaysrv -n kube-system -- sh -c " etcdctl --endpoints=$ENDPOINTS endpoint health "</p>
<h2 id="_idParaDest-65"><a id="_idTextAnchor070"/>Backing up etcd </h2>
<p>With all the<a id="_idIndexMarker216"/> groundwork we have laid out in the previous sections, we can generalize the backup etcd process as follows:</p>
<ol>
<li>SSH to the etcd cluster node. It could be a separate node or the same as the master node. In the CKA exam, it’s likely you’ll be starting in the master node, where etcdctl is installed; thus, this step is optional. </li>
<li>Check out the etcd status. You could acquire the necessary information from the <strong class="source-inline">kubectl describe &lt;etcd-podname&gt;</strong> command. </li>
<li>Perform the etcd backup.</li>
<li>Exit <a id="_idIndexMarker217"/>the master node. This may not be necessary in the actual CKA exam.</li>
</ol>
<p>The general process is captured in the following diagram: </p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<img alt="Figure 3.14 – Backup etcd process " height="784" src="image/Figure_3.14_B18201.jpg" width="994"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.14 – Backup etcd process</p>
<p>Now let’s look at the <a id="_idIndexMarker218"/>detailed process of how to back up etcd: </p>
<ol>
<li>If you need to connect to the master node or the etcd cluster node, you can use the <strong class="source-inline">ssh master-0</strong> command or the <strong class="source-inline">ssh username@&lt;nodeIP&gt;</strong> command. Please note that this step is optional. Following is an example of a user named <strong class="source-inline">packtuser</strong> using <strong class="source-inline">ssh</strong> to connect to a node with the IP address <strong class="source-inline">10.10.11.20</strong>: </li>
</ol>
<p class="source-code"><strong class="bold"> ssh packtuser@10.10.11.20</strong></p>
<ol>
<li>Check the etcd status using the following command from outside the cluster: </li>
</ol>
<p class="source-code"><strong class="bold">sudo ETCDCTL_API=3 etcdctl endpoint status --endpoints=https://172.16.16.129:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --write-out=table</strong></p>
<p>The output returned will be as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer055">
<img alt="Figure 3.15 – Check the etcd status from outside of the cluster " height="69" src="image/Figure_3.15_B18201.jpg" width="1038"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.15 – Check the etcd status from outside of the cluster</p>
<ol>
<li>Back up the etcd cluster using the <strong class="source-inline">etcdctl snapshot save</strong> command. It will look like this:</li>
</ol>
<p class="source-code"><strong class="bold">sudo ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshotdb</strong></p>
<p>You will need to authenticate from the API server with secure communication as you’re backing up from outside the Kubernetes cluster. For this, you can use the following command:</p>
<p class="source-code"><strong class="bold">sudo ETCDCTL_API=3 etcdctl snapshot save snapshotdb</strong></p>
<p class="source-code"><strong class="bold">--endpoints=https://172.16.16.129:2379</strong></p>
<p class="source-code"><strong class="bold">--cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key</strong></p>
<p>The returned<a id="_idIndexMarker219"/> output shows that you have backed up the etcd store successfully:</p>
<div>
<div class="IMG---Figure" id="_idContainer056">
<img alt="Figure 3.16 – Backup etcd store " height="101" src="image/Figure_3.16_B18201.jpg" width="1191"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.16 – Backup etcd store</p>
<ol>
<li>Verify the snapshot via the following command:</li>
</ol>
<p class="source-code"><strong class="bold">sudo ETCDCTL_API=3 etcdctl snapshot status snapshotdb --endpoints=https://172.16.16.129:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --write-out=table</strong></p>
<p>The following figure shows the status of the etcd cluster: </p>
<div>
<div class="IMG---Figure" id="_idContainer057">
<img alt="Figure 3.17 – Check the etcd store with the snapshot backup " height="66" src="image/Figure_3.17_B18201.jpg" width="1047"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.17 – Check the etcd store with the snapshot backup</p>
<h2 id="_idParaDest-66"><a id="_idTextAnchor071"/>Restoring etcd</h2>
<p>To restore etcd <a id="_idIndexMarker220"/>clusters, you can follow the process depicted in <em class="italic">Figure 3.18</em>. Note that if you have any API server instances running, you need to stop them before performing the restore operation. You may restart the API server instances after the etcd is restored: </p>
<ol>
<li>SSH to the etcd cluster node. </li>
<li>Check the etcd status. </li>
<li>Restore the etcd backup. </li>
<li>Exit the master node: </li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer058">
<img alt="Figure 3.18 – Restore etcd process " height="903" src="image/Figure_3.18_B18201.jpg" width="994"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.18 – Restore etcd process</p>
<p>Once you<a id="_idIndexMarker221"/> have a snapshot present, you can restore etcd from the previous backup operation using the following command: </p>
<p class="source-code">sudo ETCDCTL_API=3 etcdctl --endpoints 172.16.16.129:2379 snapshot restore snapshotdb</p>
<p>The returned output shows that the etcd store has been restored successfully:</p>
<div>
<div class="IMG---Figure" id="_idContainer059">
<img alt="Figure 3.19 – Restored etcd store with an existing snapshot " height="142" src="image/Figure_3.19_B18201.jpg" width="1365"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 3.19 – Restored etcd store with an existing snapshot</p>
<p>You have now completed the etcd restore process. </p>
<p>Note that this approach can be used if you want to restore the etcd cluster from a different patch version. It is important to back up etcd regularly, then perform the restore operation to recover the cluster data from a failed cluster. To learn more about etcd cluster backup and restore<a id="_idIndexMarker222"/> for Kubernetes, please check out <a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/">https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/</a>.</p>
<h1 id="_idParaDest-67"><a id="_idTextAnchor072"/>Summary</h1>
<p>This chapter covers one of the most common jobs of a Kubernetes administrator – that is, maintaining and upgrading Kubernetes clusters. Similar to cluster installation, this is also one of the most time-consuming tasks in the CKA exam. Again, practice makes perfect. The HA topology for a Kubernetes cluster in <a href="B18201_02.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a><em class="italic">, Installing and Configuring Kubernetes Cluster, </em>helps you understand what you are going to upgrade and how to do it. If needed, go back to <a href="B18201_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a><em class="italic">, Kubernetes Overview</em>, and make sure that you have a good understanding of the Kubernetes components. This way, you will know how and what’s needed to upgrade the control plane and worker nodes. </p>
<p>Compared to cluster upgrades, backup and restore etcd is one of the best-in-value questions in the CKA exam as it is simple to answer with a high-value score. Thoroughly practicing what we’ve learned in this chapter will help you overcome any challenges in the exam. </p>
<p>In the next chapter, we’ll talk about application scheduling and life cycle management, where we will revisit some important Kubernetes objects and concepts, and touch upon how they play out both in the CKA exam and in real life. Stay tuned!</p>
<h1 id="_idParaDest-68"><a id="_idTextAnchor073"/>Mock CKA scenario-based practice test </h1>
<p>You have two virtual machines, <strong class="source-inline">master-0</strong> and <strong class="source-inline">worker-0</strong>. Please complete the following mock scenarios: </p>
<h2 id="_idParaDest-69"><strong class="bold"><a id="_idTextAnchor074"/>Scenario 1</strong></h2>
<p>SSH to the <strong class="source-inline">master-0 </strong>node, check the current kubeadm version, and upgrade to the latest kubeadm version. Check out the current kubectl version, and upgrade to the latest kubectl version.</p>
<h2 id="_idParaDest-70"><strong class="bold"><a id="_idTextAnchor075"/>Scenario 2</strong></h2>
<p>SSH to <strong class="source-inline">worker-0</strong> node, check out the current kubeadm version, and upgrade to the latest kubeadm version. Check out the current kubelet version, and upgrade to the latest kubelet version.</p>
<h2 id="_idParaDest-71"><strong class="bold"><a id="_idTextAnchor076"/>Scenario 3</strong></h2>
<p>SSH to the <strong class="source-inline">master-0</strong> node and back up the etcd store.</p>
<h2 id="_idParaDest-72"><strong class="bold"><a id="_idTextAnchor077"/>Scenario 4</strong></h2>
<p>SSH to the <strong class="source-inline">master-0</strong> node and restore the etcd store to the previous backup. </p>
<p>You can find all the scenario resolutions in <a href="B18201_Appendix_A.xhtml#_idTextAnchor386"><em class="italic">Appendix</em></a><em class="italic"> - Mock CKA scenario-based practice test resolutions</em> of this book.</p>
<h1 id="_idParaDest-73"><a id="_idTextAnchor078"/>FAQs</h1>
<ol>
<li><em class="italic">Where can I find out about the compatible version of Kubernetes components with each release?</em> </li>
</ol>
<p>Go to the Kubernetes official documentation to learn about the version skew policy: <a href="https://kubernetes.io/releases/version-skew-policy/">https://kubernetes.io/releases/version-skew-policy/</a>.</p>
<ol>
<li><em class="italic">Where can I learn about the latest developments of the etcd store?</em> </li>
</ol>
<p>Go to <a href="https://etcd.io/">https://etcd.io/</a>, where you will find the latest developments of the etcd store. For daemons and guidance on how to get started with etcd, please go to the official documentation: <a href="https://etcd.io/docs/">https://etcd.io/docs/</a>.</p>
<ol>
<li><em class="italic">What is a recommended official Kubernetes article for upgrading a Kubernetes cluster?</em> </li>
</ol>
<p>I recommend bookmarking the article <em class="italic">Upgrading the kubeadm</em>, where you will find most key commands and processes: <a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/</a>.</p>
<ol>
<li><em class="italic">What is a recommended official Kubernetes article for backup and restore etcd?</em></li>
</ol>
<p>I recommend bookmarking the article <em class="italic">Operating etcd clusters for Kubernetes</em>, where you will find all the key commands for etcd backup and restore: <a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/">https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/</a>.</p>
</div>
</div>

<div id="sbo-rt-content"><div>
<div class="Basic-Graphics-Frame" id="_idContainer061">
</div>
</div>
<div class="Content" id="_idContainer062">
<h1 id="_idParaDest-74"><a id="_idTextAnchor079"/>Part 2: Managing Kubernetes</h1>
<p>This part describes how to manage workloads deployed on top of Kubernetes, and how to manage the security and networking of Kubernetes clusters to fulfil enterprise requirements.</p>
<p>This part of the book comprises the following chapters:</p>
<ul>
<li><a href="B18201_04.xhtml#_idTextAnchor080"><em class="italic">Chapter 4</em></a><em class="italic">, Application Scheduling and Lifecycle Management</em></li>
<li><a href="B18201_05.xhtml#_idTextAnchor149"><em class="italic">Chapter 5</em></a><em class="italic">, Demystifying Kubernetes Storage </em></li>
<li><a href="B18201_06.xhtml#_idTextAnchor192"><em class="italic">Chapter 6</em></a><em class="italic">, Securing Kubernetes</em></li>
<li><a href="B18201_07.xhtml#_idTextAnchor235"><em class="italic">Chapter 7</em></a><em class="italic">, Demystifying Kubernetes Networking</em></li>
</ul>
</div>
<div>
<div id="_idContainer063">
</div>
</div>
</div></body></html>