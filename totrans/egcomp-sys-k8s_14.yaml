- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computer Vision with Python and K3s Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial intelligence** (**AI**) is commonly used to substitute activities
    that humans do every day. It can give systems the intelligence to operate autonomously
    without human intervention in most cases. **Computer vision** (**CV**) is a subcategory
    of AI that focuses on detecting objects in videos and images. CV is often used
    to detect traffic in a city. This chapter focuses on building a basic smart traffic
    system that consists of detecting objects such as cars, trucks, and pedestrians
    when a vehicle is moving. For this, the system uses the OpenCV, TensorFlow, and
    scikit-learn Python libraries and a camera to perform computer vision at the edge
    on a Raspberry Pi. This system also shows locally to drivers a map within the
    detected objects, and it also implements a public map for global detected object
    visualization. This public map can be used as a real-time traffic state map that
    municipalities can use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision and smart traffic systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Redis to store temporary object **Global Positioning System** (**GPS**)
    positions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a computer vision service to detect car obstacles using OpenCV, TensorFlow
    Lite, and scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the edge application to visualize warnings based on computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a global visualizer for the smart traffic system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To deploy our computer vision system in this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes cluster hosted in your public cloud provider (**Amazon Web Services**
    (**AWS**), Azure, **Google Cloud Platform** (**GCP**)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Raspberry Pi 4B with an 8-GB micro **Secure Digital** (**SD**) card with a
    small-monitor **liquid-crystal display** (**LCD**) screen to use in a car.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Logitech C922 PRO webcam, recommended because of its quality and support on
    Linux.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple VK-162 G-Mouse USB GPS Dongle Navigation modules, for your edge Raspberry
    devices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic knowledge of AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl` configured to be used in your local machine for your Kubernetes cloud
    cluster to avoid using the `--kubeconfig` parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clone the [https://github.com/PacktPublishing/Edge-Computing-Systems-with-Kubernetes/tree/main/ch14](https://github.com/PacktPublishing/Edge-Computing-Systems-with-Kubernetes/tree/main/ch14)
    repository if you want to run the `kubectl apply` instead of copying the code
    from the book. Take a look at the `python` directory inside the `code` directory
    and the `yaml` directory for YAML configurations that are inside the `ch14` directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this, you can deploy Prometheus and Grafana to start experiment monitoring
    in edge environments.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision and smart traffic systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AI is an area of computer science that consists of simulating human intelligence
    using mathematics, statistics, linguistics, computer science, and other sciences.
    AI can also be defined as the study of rational agents, as depicted in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Agents'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16945_14_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.1 – Agents
  prefs: []
  type: TYPE_NORMAL
- en: Taking *Figure 14.1* as a reference, an agent receives perceptions coming from
    the environment. These perceptions are captured by sensors, and this information
    is processed to perform an action using effectors. Actions are decided by internal
    rules installed inside the agent. These actions involve the use of effectors such
    as arms, legs, or wheels, for example.
  prefs: []
  type: TYPE_NORMAL
- en: These internal rules can be implemented using different **machine learning**
    (**ML**) paradigms such as **supervised learning** (**SL**), **unsupervised learning**
    (**UL**), and **reinforcement learning** (**RL**).
  prefs: []
  type: TYPE_NORMAL
- en: 'ML is a type of AI that uses historical data as input to do predictions. Computer
    vision is a subset of ML applied to image and video analysis using predictions.
    In our chapter, we are going to do predictions about what our agent is capturing
    using a camera and take decisions according to that information, but we are going
    to apply computer vision to create a smart traffic system. Let’s have a look at
    the following diagram, which shows how our system will be implemented to create
    a smart traffic system using computer vision at the edge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – Smart traffic system using computer vision'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16945_14_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.2 – Smart traffic system using computer vision
  prefs: []
  type: TYPE_NORMAL
- en: 'Smart traffic systems are often used by municipalities to improve safety, and
    traffic flow on streets in a cost-effective way. Our system can be used in two
    modes. The static mode uses a camera in a static location point in the city, and
    the dynamic mode uses a car to scan traffic where the car is moving. We are going
    to use the dynamic mode. Now, let’s explain our system using the layers of the
    edge computing systems, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloud layer**: Here, we are going to use an **application programming interface**
    (**API**) called **Traffic Manager** that stores all detected objects at the edge
    in a Redis instance. The data stored will contain the type of object—car, truck,
    and person—which represents a level 1 warning on our system and the GPS coordinates.
    This means that a vehicle driver will be warned of previously detected objects
    by other drivers. Our API will store the GPS position of these objects, which
    potentially could be obstacles for a vehicle. This layer will also include a frontend
    application called **Traffic Map Public** that shows the objects detected on a
    map. This application could be used by the municipality to monitor all traffic
    across the city.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Near edge**: This layer has the **fourth-generation** (**4G**)/**fifth-generation**
    (**5G**) **Long-Term Evolution** (**LTE**) mobile network used to send information
    to the internet. This layer will transport information collected at the edge to
    send it to the cloud layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Far edge**: Our far edge has a Raspberry Pi that will process the information
    captured by a camera. This device has installed K3s as a single node cluster to
    manage all services that the system uses. K3s can brings automation to the system.
    K3s can easily update and maintain the system and can extend the system to use
    more nodes. These additional nodes can be used to add multiple cameras for object
    detection at multiple angles. The computer vision application that runs in the
    cluster consists of two displays and two APIs. One display runs outside K3s but
    in the same device as a Python script, and it’s the service that captures the
    video. This service consists of a Python program that captures video and detects
    objects using OpenCV and a precompiled model for TensorFlow Lite for object detection.
    Here is where computer vision occurs. The system uses a small LCD touchscreen
    connected to the device. The other display is a frontend application that runs
    on a browser; it shows detected objects across a map, not only showing these locally
    but also showing all detected objects by all vehicles in a radius of 500 meters.
    Detected objects will be classified by the Inference API, which classifies objects
    according to their level of warning for a driver. These warnings are represented
    at three levels: levels 1 and 2 represent a warning, and level 3 could be ignored
    as an obstacle for a driver. The Inference API contains a precompiled decision
    tree to do classification. The **GPS Queue** API manages all GPS coordinates and
    periodically sends information about detected objects that represent a warning
    to the cloud to be shown to other drivers. The whole application uses the Display,
    Traffic Map, Inference, and GPS Queue components to process and visualize detected
    objects. The GPS Queue service is based on the GPS service created in [*Chapter
    5*](B16945_05_Final_PG.xhtml#_idTextAnchor097), *K3s Homelab for Edge Computing
    Experiments*, with some modifications. Something important to consider is that
    you can accelerate your object detection by using an external device that accelerates
    **neural network** (**NN**) processing. Some devices that you can consider are
    the Coral USB Accelerator from Google, the Rock Pi neural compute stick **Universal
    Serial Bus** (**USB**), and the NVIDIA Jetson Nano. These devices accelerate the
    NN processing of OpenCV by delegating processing to a dedicated processing unit
    sometimes called a **graphics processing unit** (**GPU**) or a **Tensor Processing
    Unit** (**TPU**). The OpenCV library uses TensorFlow Lite models, so the use of
    these devices can increase the number of **frames per second** (**FPS**) analyzed
    that have some GPU that can be used by TensorFlow Lite, which is designed to run
    on edge devices to accelerate your video analysis. For more information, check
    the *Further reading* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tiny edge**: Here, we can find an LCD screen to display all detected objects
    in real time and warnings for the driver. You can also find the VK-162 G-Mouse
    GPS module here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To summarize this workflow, our vehicle first captures images with its camera;
    then, the video frames or images are captured using OpenCV and classified using
    TensorFlow Lite, then are classified according to their level of warning representation
    for the drivers by the Inference API. This information is shown locally in the
    LCD and browser. The GPS coordinate data sent to the cloud is shown in a public
    web frontend application in the cloud. So now, let’s get started in building a
    basic smart traffic system to alert drivers.
  prefs: []
  type: TYPE_NORMAL
- en: Using Redis to store temporary object GPS positions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to use Redis to store our GPS coordinates for all detected objects
    using computer vision. This is a basic configuration to deploy Redis for this
    purpose. This Redis instance must be deployed in the cloud. As we explained in
    [*Chapter 13*](B16945_13_Final_PG.xhtml#_idTextAnchor246), *Geolocalization Applications
    Using GPS, NoSQL, and K3s Clusters*, we are going to use a geospatial index to
    represent our data. The difference will be that we are going to implement temporary
    storage of data using a `traffic`, which stores all traffic objects detected by
    other drivers. In this way, we implemented a kind of garbage functionality to
    remove old detected objects during traffic hours. The reason is that the detected
    objects are relevant just for a certain amount of time, then have to be deleted.
    So, let’s install our Redis deployment by following the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a **PersistentVolumeClaim** for Redis to persist our data, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, create a **ConfigMap** to configure Redis to use an authentication password,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a deployment for Redis using the previous `redis-configmap` `db-pv-claim-1`
    **PersistentVolumeClaim** with some resource limits, using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, create a service for Redis opening port `6379`, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We now have Redis installed. Let’s move on to deploying our computer vision
    service at the far edge, in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a computer vision service to detect car obstacles using OpenCV, TensorFlow
    Lite, and scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to explore how to configure the object detection
    system that runs at the edge with all its components. This section also shows
    how to configure the public web application running in the cloud that stores and
    shows information about all detected objects at the edge. Let’s start by first
    configuring our Raspberry Pi device in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing your Raspberry Pi to run the computer vision application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before installing our software, we have to prepare our device to run it. For
    this, let’s start to configure our Raspberry Pi 4B following the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Install Raspbian Pi OS (32 bit) using Debian Bullseye, released at least from
    2022-04-04\. The code to run the TensorFlow Lite model in this chapter has to
    run on an ARMv7 device to support the Coral USB Accelerator device and the LCD
    screen. ARM64 is not supported yet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Depending on your webcam, you have to install drivers. In this case, we are
    using the Logitech C922 PRO webcam, which is automatically detected by Raspbian.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect and configure your GPS module. In this case, our VK-162 G-Mouse module
    is autodetected by Raspbian too.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the network to use a wireless connection, to install all the necessary
    packages to run the application. Later, you can reconfigure your wireless connection
    to connect to your access point in your smartphone, but you have to delete the
    previous connection in the `/etc/wpa_supplicant/wpa_supplicant.conf` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the drivers of your LCD screen. In this case, we are using the Miuzei
    **High-Definition Multimedia Interface** (**HDMI**). This will flip the screen
    horizontally and activate the touch feature (this will be the last step once all
    the things are configured). You can check the repository at [https://github.com/goodtft/LCD-show.git](https://github.com/goodtft/LCD-show.git),
    and you can use any LCD screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before installing K3s, remember to activate the CGROUPS in the `/boot/cmdline.txt`
    file, then add the next flags at the end of the line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about CGROUPS visit this link: [https://man7.org/linux/man-pages/man7/cgroups.7.html](https://man7.org/linux/man-pages/man7/cgroups.7.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get your current `ifconfig`, then take a look at the `wlan0` interface, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install K3s by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you can test if everything is working by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will return your unique node running.
  prefs: []
  type: TYPE_NORMAL
- en: Now, our edge device is ready to be used to run our service that performs computer
    vision at the edge. For this, let’s move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the inference service to detect objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `inference` service is used in this scenario to do predictions and to classify
    if an object represents an obstacle for a driver. We use the next table for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16945_14_Table_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For example, a car identified by the id `1` in the `n` field represents a level
    1 of warning, so all the objects with `warning_level` equal to 1 or 2 will be
    recorded as potential objects that can obstruct traffic or represent danger for
    the driver. If an object is classified with the value 1000, the object doesn’t
    represent any danger, so it is not recorded.
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code of this service consists of two files: `index.py` and `create_model.py`.
    The `index.py` file contains a basic API to return predictions by calling the
    model to predict using the `/predict` path. It has basic code to load the precompiled
    ML model. The `create_model.py` file contains code to train and generate a model
    that will be used for this API using `index.py`. The code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we read our `safety_rules.csv` `drop`. In AI, you have to represent texts
    as values. Our object column has a numeric representation in the n column, so
    the column object can be ignored. The data loaded from the CSV file is represented
    as a Pandas DataFrame that is used in scikit-learn as the source of data to generate
    a decision tree. A decision tree is an ML algorithm that can use classified data
    to do predictions using the data structure of trees for predictions. So, it is
    one of the simplest methods to do predictions using ML. After the DataFrame is
    loaded, scikit-learn does its training processes to generate a `safety_rules.model`
    model that could be used later in the API for predictions. Every time you build
    the container, the model is updated by calling the `create_model.py` file inside
    the `Dockerfile` of this API. Now, the serving code for the API will look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: By calling the `/predict` `safety_rules.csv` file. You can add more values to
    classify your images by adding new values in the file and regenerating the container
    with the new model.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the code and update the model, check the next link: [https://github.com/sergioarmgpl/containers/tree/main/inference/src](https://github.com/sergioarmgpl/containers/tree/main/inference/src).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s deploy our `inference` service in our **Advanced RISC Machine**
    (**ARM**) device by following the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a deployment for the `inference` API, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s port forward the service running, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s call the `inference` API to get some predictions. Let’s use an object
    detected and classified as `other` with the number 6; it will return a warning
    level of 3 based on the prediction table. The code is illustrated in the following
    snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will return the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Our inference service is now running, ready to be called inside our device to
    classify the detected images. Let’s continue deploying the `gps-queue` service
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the gps-queue service to store GPS coordinates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `gps-queue` service is composed of several containers dedicated to a specific
    task. First, initialize an `init` container called `init-gps-queue` that adds
    an initial value of `-1` inside the `/tmp/gps` file. This file stores the last
    GPS coordinate generated. Then, the `gps-queue` container is in charge of reading
    the GPS coordinates from our GPS module, so it needs permission to access the
    `/dev` folder from the host. Once the GPS coordinate is read, it is stored in
    `/tmp/gps`. After this, the `sync-traffic-events` container calls the `gps-api`
    container every 30 seconds by default using the `http://localhost:3000/traffic`
    endpoint, which sends the detected objects with their warning classification and
    GPS coordinate to the `http://<TRAFFIC_MANAGER_IP>:5000` public endpoint, which
    stores this information for some time to be shown in the `traffic-map-public`
    service that has public access to show the objects detected by other vehicles.
    Before deploying our service, let’s explore a little bit the code of the `gps-queue`
    container, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This code configures the GPS module and stores the coordinate in the `/tmp/gps`
    file, which is shared by the `gps-queue` and `gps-api` containers. It uses a `cid`
    variable to associate each GPS coordinate with a unique client **identifier**
    (**ID**) that could be used for customizations to create your own system. The
    information will be stored in the next format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s explore the code inside the `gps-api` container, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As an explanation, the `/gps` path of this API returns the value of the last
    GPS coordinate stored in `/tmp/gps`, and the `/traffic/event` path receives the
    object detected from the edge device running the `detect.py` program. This happens
    every second. Then, the information is stored temporarily in the `traffic_events`
    array. Inside the Pod, the `sync-traffic-events` container calls the `/traffic`
    endpoint of the API running inside the `gps-api` container, which filters the
    `traffic_events` array to have just unique objects detected because the edge program
    gets a maximum of eight detected objects per video-frame analysis. Once the array
    is filtered, it is sent to the `http://<TRAFFIC_MANAGER:5000>/traffic/1`. This
    information is requested later by the `http://<TRAFFIC_MANAGER:5000>/traffic`
    URL, which shows the globally stored objects detected from all the devices in
    a map using the Leaflet library.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy this service, execute the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a deployment for the GPS queue, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the code and create your own containers, you can check the next links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/sergioarmgpl/containers/tree/main/gps-api/src](https://github.com/sergioarmgpl/containers/tree/main/gps-api/src)
    and [https://github.com/sergioarmgpl/containers/tree/main/gps-queue/src](https://github.com/sergioarmgpl/containers/tree/main/gps-queue/src)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s pay attention to the variables that this deployment uses in its containers.
    These are explained in more detail here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gps-queue`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DEVICE`: Configures the device where your GPS module is detected. For the
    VK-162 G-Mouse module, the default value used is `/dev/ttyACM0`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gps-api`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ENDPOINT`: Configures the public endpoint where all detected objects with
    GPS coordinates and warnings are stored. This is the public service that stores
    the coordinates. By default, this is `http://<TRAFFIC_MANAGER_IP>:5000`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sync-traffic-events`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`URL`: Contains the local URL called periodically to send information about
    all detected objects. This will call the API configured in the `gps-api` container.
    By default, this is `http://localhost:3000/traffic`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DELAY`: Configures the amount of time to wait to send the last objects detected
    with their information. By default, this is 30, which represents the time in seconds.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These values could be used to customize the behavior of the service that processes
    the objects detected and its GPS coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to test the endpoints of this service, you can run inside your
    edge device `port-forward` to access the API using the `curl` command, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For example, you can execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'It will return something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We have now deployed the `gps-queue` service and it’s ready to be used. It’s
    time to deploy our local web application that will show detected objects at the
    edge using our edge device equipped with a camera. For this, we have to solve
    the `traffic-manager` public API from the local `traffic-map` application. CORS
    is a mechanism that allows or restricts resources on a web page to be requested
    from a domain outside the current one. In this scenario, it’s called a public
    API from a local web application. So, let’s move on to the next section to create
    a simple proxy to resolve this issue.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying traffic-manager to store GPS coordinates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `traffic-manager` service receives detected objects with their GPS coordinates
    and warning-level classification. This API runs in the cloud, and it’s called
    periodically by the edge device while it’s moving and detecting objects. This
    service consists of two containers: one that gives an API to recollect objects
    detected, and another that is in charge of auto-expiring detected objects and
    global traffic information. This is because traffic is constantly changing during
    the day. You can configure these values to fit your own scenario. Let’s explore
    first the code of the API in the `traffic-manager` container, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This container has two endpoints with the `/traffic/1` path. This service stores
    detected objects at the edge by creating a hash key with the form `object:<object-id>:data`
    that stores the type and the warning level, and in the `traffic` geospatial set
    stores the GPS coordinate. An expiration time to the `traffic` key is set or renewed,
    and for the new `object:<object-id>:data` hash key, the expiration time is set
    too. After calling the `/traffic/unit/<unit>/r/<radius>/lat/<lat>/lng/<lng>` path,
    the call returns near detected objects in the radius defined in the request. This
    is a public service that all the edge devices will access periodically to send
    updates of objects detected while they are moving. Now, let’s explore the code
    of the `autoexpire` container, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This container basically checks if each member of the traffic geospatial set
    has metadata available in the `object:<object-id>:data` hash key. If none exists,
    this means that the object passed the maximum amount of time to be relevant in
    the traffic, which means that it has expired too, and then this code removes the
    member from the sorted set. This process is called periodically after waiting
    for a certain number of seconds that are configured by the `DELAY` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy the `traffic-manager` service, proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a deployment for the GPS server, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This deployment uses the following variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`REDIS_HOST`: This is the name of the Redis service. This variable can be customized
    to fit your needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`REDIS_AUTH`: This is the password to connect to the Redis service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TTL_TRAFFIC`: This is the URL of the `tracking-server` service. In this case,
    the URL matches the internal `tracking-server` service on port `3000`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TTL_OBJECT`: This is the URL of the `tracking-server` service. in this case,
    the URL matches the internal `tracking-server` service on port `3000`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DELAY`: This is the time to wait to check if a member inside the traffic geospatial
    sorted set expired.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By configuring these variables, you can customize the behavior of this deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the code and create your own containers, you can check the next links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/sergioarmgpl/containers/tree/main/traffic-manager/src](https://github.com/sergioarmgpl/containers/tree/main/traffic-manager/src)
    and [https://github.com/sergioarmgpl/containers/tree/main/autoexpire/src](https://github.com/sergioarmgpl/containers/tree/main/autoexpire/src)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create a service for this deployment as a **LoadBalancer**. This
    IP address will be used in our edge device to propagate this information in the
    cloud to be accessible to all drivers that use this smart traffic system. The
    code is illustrated in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the load balancer IP address for your `traffic-manager` deployment with
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see the value of the `TRAFFIC_MANAGER_IP` environment variable by running
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that it takes some time after the IP address of the load balancer is provisioned.
    You can check the state of the services by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Wait until the `EXTERNAL_IP` environment variable is provisioned.
  prefs: []
  type: TYPE_NORMAL
- en: Also, take note that the `$TRAFFIC_MANAGER_IP` value will be used to configure
    the `proxy` service in the edge device.
  prefs: []
  type: TYPE_NORMAL
- en: '(*Optional*) If you want to test this API to insert an object manually, run
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will return the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '(*Optional*) To get all detected objects in a radius of 0.1 kilometers, run
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will return the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now, our `traffic-manager` API is running in the cloud. Let’s move on to use
    this API in our edge device using a proxy to prevent CORS restrictions when calling
    the API, in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a simple proxy to bypass CORS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `proxy` service is used to bypass the CORS restriction that occurs when
    a local website running on a private network tries to call a public API using
    a public API address. Using a proxy to forward requests to this public site could
    be one possible and simple solution to solve this. Another one is to modify the
    request headers on the API call and add the necessary headers to bypass the CORS
    restriction. In this case, we are going to use a proxy build with Flask to forward
    all local `GET` requests to the `traffic-manager` API, which is a public API deployed
    in the cloud and is accessible over the internet. Let’s explore the code a little
    bit before deploying the `proxy` service, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This code basically receives all `GET` requests on any path and forwards the
    requests with all the important headers to the URL defined in the environment
    variable. This API is accessible using port `5000`. Now, let’s move on to deploy
    this simple proxy to forward all calls from our local `proxy` service, execute
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a deployment for the GPS server, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This deployment uses the following variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`URL`: This variable has the URL where the proxy is going to redirect all `GET`
    requests received by the proxy in port `5000`. This URL will be the `traffic-manager`
    public IP address using the format `http://<TRAFFIC_MANAGER_IP>:5000`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the code and create your own container, you can check the next link:
    [https://github.com/sergioarmgpl/containers/tree/main/proxy/src](https://github.com/sergioarmgpl/containers/tree/main/proxy/src).
    This small proxy is a custom implementation that you can implement using languages
    other than Python to have all the control in your implementation. You can also
    use solutions such as using NGINX with a `proxy_pass` configuration, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can test the proxy by running something like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the remote path could be `/traffic`, which is a URL where the **Traffic
    Manager** service returns all objects globally detected by drivers.
  prefs: []
  type: TYPE_NORMAL
- en: Now our proxy is running, let’s deploy our **Traffic Map** web application to
    show the detected objects that represent warnings for drivers in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the edge application to visualize warnings based on computer vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our visual application consists of two parts: the first one is a web application
    that shows all data from all drivers using the smart traffic system, and the other
    one is a desktop application that shows the detected objects in real time. So,
    let’s start installing our web application to visualize objects detected by different
    drivers in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Traffic Map application to visualize objects detected by drivers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have now set up the necessary APIs to visualize what our device detected.
    We have to continue deploying our web application to visualize this object on
    a map. This is where our **Traffic Map** application comes in handy. But let’s
    explore the code first before deploying it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This is similar to the previous web application map used in [*Chapter 13*](B16945_13_Final_PG.xhtml#_idTextAnchor246),
    *Geolocalization Applications using GPS, NoSQL, and K3s Clusters*, but this one
    calls the GPS Queue service to get the current GPS coordinate that is running
    in the edge device and get data from the public endpoint of the `proxy` service
    to prevent CORS access restrictions. It also has the option to center the map
    at the beginning every time the page is loaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'The web part uses the `map.html` file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This code basically centers the map with initial latitude and longitude coordinates,
    shows the current position of the device in a blue globe, and shows the detected
    objects with icons, showing the object name, the GPS coordinates, the type of
    object, and the warning level. It should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – Driver current position'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16945_14_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.3 – Driver current position
  prefs: []
  type: TYPE_NORMAL
- en: 'This shows the driver’s current position in real time, while the vehicle is
    moving. The other possible visualization shows how detected objects appear across
    the map. This information is requested using the `proxy` service to visualize
    all detected objects by other drivers. This could represent a kind of **augmented
    reality** (**AR**), something similar to what Waze does with its application.
    The visualization looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – Detected object’s current position and warning message'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16945_14_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.4 – Detected object’s current position and warning message
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click inside the detected object, it will show the current GPS coordinate,
    the type of object, and a warning message. There are several objects included
    in this default implementation. The implementation includes car, truck, and person
    detection as possible obstacles and potential warnings for a driver. You can see
    the following icons on the map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.5 – Car, truck, and person icons shown in Traffic Map](img/B16945_14_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.5 – Car, truck, and person icons shown in Traffic Map
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, our web application updates the objects every 5 seconds within
    a radius of 0.5 kilometers. Those values can be customized to satisfy your own
    solution. Now, let’s deploy our Traffic Map web application by executing the next
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `traffic-map` deployment by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This deployment has the following environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LATITUDE`: Initial GPS latitude coordinate to center your map.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LONGITUDE`: Initial GPS longitude coordinate to center your map.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GPS_QUEUE`: IP address endpoint of the `gps-queue` service. In this case,
    because this runs locally, it is set by default as `localhost`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TRAFFIC_MANAGER`: IP address endpoint of your `proxy` service, we can call
    it using `localhost`, which prevents the CORS restriction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the code and create your own container of `traffic_map`, you can check
    the next link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/sergioarmgpl/containers/tree/main/traffic-map/src](https://github.com/sergioarmgpl/containers/tree/main/traffic-map/src)'
  prefs: []
  type: TYPE_NORMAL
- en: We have now deployed the **Traffic Map** web application on our edge device.
    Let’s move on to run our object detection system at the edge to perform our computer
    vision, in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting objects with computer vision using OpenCV, TensorFlow Lite, and scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The service that performs computer vision is contained in the `detect.py` file.
    This will run on our edge device. Let’s explore the code inside this file before
    preparing our device to run this program, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This code starts the video capture and then sends this image in a format that
    TensorFlow Lite can analyze. TensorFlow Lite detects coordinates where objects
    are detected and classifies the objects with a label that is their name. This
    program will use the `efficientdet_lite0_edgetpu_metadata.tflite` model. In this
    case, we are focusing on the car, person, dog, semaphore, and truck objects. These
    objects represent obstacles for drivers and represent a level of warning. If the
    detected object is different than these objects, it’s classified as `other` and
    it’s omitted as a warning. If you want to add more objects to the list, you just
    have to modify the `obj_values` array with new values, as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In each loop of this program, the detected objects are counted by groups and
    stored in the `items` array. Then, if one of these groups detects more than one
    object and the group is one of the identified objects in the `obj_values` array,
    the detected objects in the group are counted as potential object obstacles that
    represent warnings for drivers. To calculate the warning level, the script calls
    the `inference` API, and then, if a warning is detected, it calls the `traffic-map`
    service using the `proxy` service previously installed using the `http://localhost:5000/traffic/event`
    URL. Every time the proxy is called, the requests will be sent to the public endpoint
    of the `traffic-manager` service deployed in the cloud. Then, after the object
    analysis, the `items` array is cleared and the output summarizing the detected
    objects is shown in a blue box using OpenCV. It will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.6 – Object detection screen'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16945_14_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.6 – Object detection screen
  prefs: []
  type: TYPE_NORMAL
- en: 'This output also shows the detected objects marked with a red rectangle with
    the name of the detected object. In the upper-left corner, you will see the number
    of FPS analyzed. Our warning box will show two types of messages: either the group
    of objects found (for example, **person, car found**) or that there are no detected
    objects—this will show the message **No warnings**. The service closes if you
    press the *Esc* key. To install the object detection service in your edge device,
    execute the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect your edge device to a network that you can access.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Log in to your edge device, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can get the IP address of your device by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: You can run it by connecting your device to an HDMI screen and connecting a
    keyboard and mouse to your device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the repository by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install missing dependencies to run OpenCV and the camera, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Configure the device to run the object detection program, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the script to install desktop shortcuts, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at the files with a `.desktop` extension that call the `run.sh`
    script and the files with a `.desktop` extension that start the detection application
    and the local web Traffic Map application. These files are located in the `ch14/code/python/object_detection`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: Test the installation by clicking on the new **Detector** desktop shortcut.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the local Traffic Map application by clicking on the `http://localhost:5000`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reconfigure your wireless network to use the access point connection of your
    smartphone and reset your `/etc/wpa_supplicant/wpa_supplicant.conf` configuration
    file by removing the `network {}` entries to use your smartphone internet connection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, you can check the next link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://wiki.archlinux.org/title/wpa_supplicant](https://wiki.archlinux.org/title/wpa_supplicant)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you can configure your touchscreen. In this case, we are using the Miuzei
    LCD 4.0-inch HDMI display, which flips the screen. For this, execute the following
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, restart your device by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, access the **Detect** shortcut to start the service to detect objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: You can accelerate the video-frame analysis by uncommenting the `--enableEdgeTPU`
    flag in the `ch14/code/python/object_detection/run.sh` file. Our detection code
    is based on the official Tensor Flow example that uses the Coral USB Accelerator
    device. This device is a TPU, which is a dedicated unit to process information
    using NNs. The configuration of the Coral device is out of the scope of this book.
    For more information, check the Coral USB Accelerator link in the *Further reading*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Start the Traffic Map application by clicking on the **Traffic** shortcut. If
    there are objects detected, they will appear 30 seconds later in the web application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last step is to deploy a public **Traffic Map** application to visualize
    all traffic in a radius area. For this, let’s deploy the last service—**Traffic
    Map Public**—in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a global visualizer for the smart traffic system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Traffic Map Public** service is the static version of **Traffic Map**
    that only shows detected objects within a radius of 5 kilometers. This service
    is deployed in the cloud, so you should expect the same visualization as with
    the **Traffic Map** service, but the only missing part is that it doesn’t show
    your real-time GPS position because it is static. The GPS position to take into
    consideration could be a GPS coordinate that is the center of the city that you
    want to monitor. In general, this web visualization could fit a static report
    for a municipality. The code is the same as for the **Traffic Map** web application,
    but the continuous update of the GPS position is omitted. To deploy this service,
    run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `traffic-map` deployment by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This deployment has the following environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LATITUDE`: Initial GPS latitude coordinate to center your map.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LONGITUDE`: Initial GPS longitude coordinate to center your map.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GPS_QUEUE`: IP address endpoint of the `gps-queue` service. In this case,
    because this runs locally, it is set by default as `localhost`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TRAFFIC_MANAGER`: IP address endpoint of your `localhost`, which prevents
    the CORS restriction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the code and create your own container of `traffic-map-public`, you
    can check the next link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/sergioarmgpl/containers/tree/main/traffic-map-public/src](https://github.com/sergioarmgpl/containers/tree/main/traffic-map-public/src)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create a service for this deployment as a LoadBalancer. This IP
    address will be the endpoint to access the Traffic Map public web application.
    The code is illustrated in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: To troubleshoot your deployments, you can use the `$ kubectl logs pod/<POD>
    -f <CONTAINER_NAME>` command. This will show you some useful outputs to troubleshoot
    services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the load balancer IP for your `traffic-map-public` deployment with the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see the value of the `TRAFFIC_MAP_PUBLIC` environment variable by running
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that it takes some time after the IP address of the load balancer is provisioned.
    You can check the state of the services by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Wait until the `EXTERNAL_IP` environment variable is provisioned.
  prefs: []
  type: TYPE_NORMAL
- en: Access the Traffic Map public application at `http://<TRAFFIC_MAP_PUBLIC>:3000`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now everything is running, try to fill the system with data and drive your car
    with your edge device to capture objects. You will then see the objects in the
    system in a few seconds. Take a look at the *Further reading* section, where there
    are a lot of materials that you can explore to create your system. But now, it’s
    time to summarize what we learned. Let’s move on to the *Summary* section.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how you can use AI to analyze video captured by
    cameras, to detect objects that potentially represent obstacles for drivers. This
    was implemented to run at the edge on a Raspberry Pi, using the power of Kubernetes
    with K3s. With this approach, we created a decoupled system that could be easier
    to upgrade using containers. We also learned how this kind of system can be used
    in real-world scenarios to monitor traffic behavior to improve driver safety.
    Across this implementation, we also learned how this kind of system is distributed
    across the edge and the cloud to process and show information locally to drivers
    to improve their driving experience. In the last chapter, we are going to give
    an easy method to organize and design fast your own edge computing system using
    a diagram called the edge computing design system canvas.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are a few questions to validate your new knowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: How are AI, ML, and computer vision related to each other to design smart traffic
    systems?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do TensorFlow Lite and scikit-learn work to detect objects and perform predictions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does computer vision work running at the edge?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can you distribute data across the edge and the cloud?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can you use Python to build a computer vision system?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can you use K3s to design distributed systems that detect objects in real
    time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following references for more information on the topics
    covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What is artificial intelligence (AI)?*: [https://www.techtarget.com/searchenterpriseai/definition/AI-Artificial-Intelligence](https://www.techtarget.com/searchenterpriseai/definition/AI-Artificial-Intelligence)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Agents in Artificial Intelligence*: [https://www.geeksforgeeks.org/agents-artificial-intelligence](https://www.geeksforgeeks.org/agents-artificial-intelligence)
    and [https://www.educba.com/agents-in-artificial-intelligence](https://www.educba.com/agents-in-artificial-intelligence)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Smart Traffic Management: Optimizing Your City’s Infrastructure Spend*: [https://www.digi.com/blog/post/smart-traffic-management-optimizing-spend](https://www.digi.com/blog/post/smart-traffic-management-optimizing-spend)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Markers with Custom Icons*: [https://leafletjs.com/examples/custom-icons](https://leafletjs.com/examples/custom-icons)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MLOps Using Argo and K3s*: [https://github.com/sergioarmgpl/mlops-argo-k3s](https://github.com/sergioarmgpl/mlops-argo-k3s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*YOLO and Tiny-YOLO object detection on the Raspberry Pi and Movidius NCS*:
    [https://pyimagesearch.com/2020/01/27/yolo-and-tiny-yolo-object-detection-on-the-raspberry-pi-and-movidius-ncs](https://pyimagesearch.com/2020/01/27/yolo-and-tiny-yolo-object-detection-on-the-raspberry-pi-and-movidius-ncs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TensorFlow Lite example apps*: [https://www.tensorflow.org/lite/examples](https://www.tensorflow.org/lite/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TensorFlow Hub*: [https://tfhub.dev](https://tfhub.dev)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Get models for TensorFlow Lite: [https://www.tensorflow.org/lite/models](https://www.tensorflow.org/lite/models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Edge Analytics in Transportation and Logistics Space: A Case Study*: [https://www.skillsire.com/read-blog/174_edge-analytics-in-transportation-and-logistics-space-a-case-study.html](https://www.skillsire.com/read-blog/174_edge-analytics-in-transportation-and-logistics-space-a-case-study.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tutorial to set up TensorFlow Object Detection API on the Raspberry Pi*: [https://github.com/EdjeElectronics/TensorFlow-Object-Detection-on-the-Raspberry-Pi](https://github.com/EdjeElectronics/TensorFlow-Object-Detection-on-the-Raspberry-Pi)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi*: [https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TensorFlow Lite Python object detection example with Raspberry Pi*: [https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Python Project – Real-time Human Detection & Counting*: [https://data-flair.training/blogs/python-project-real-time-human-detection-counting](https://data-flair.training/blogs/python-project-real-time-human-detection-counting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coral USB Accelerator: [https://coral.ai/products/accelerator](https://coral.ai/products/accelerator)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Edge TPU simple camera examples: [https://github.com/google-coral/examples-camera](https://github.com/google-coral/examples-camera)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use NGINX as a Reverse Proxy*: [https://www.linode.com/docs/guides/use-nginx-reverse-proxy](https://www.linode.com/docs/guides/use-nginx-reverse-proxy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Movidius on Mac OS*: [https://github.com/acharroux/Movidius-On-MacOS](https://github.com/acharroux/Movidius-On-MacOS)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NCS-Pi-Stream*: [https://github.com/HanYangZhao/NCS-Pi-Stream](https://github.com/HanYangZhao/NCS-Pi-Stream)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Intel® Neural Compute Stick 2 (Intel® NCS2)*: [https://www.intel.com/content/www/us/en/developer/tools/neural-compute-stick/overview.html](https://www.intel.com/content/www/us/en/developer/tools/neural-compute-stick/overview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Surveillance with Deep Learning – Intelligent Video Surveillance Project*:
    [https://data-flair.training/blogs/deep-surveillance-with-deep-learning-intelligent-video-surveillance-project](https://data-flair.training/blogs/deep-surveillance-with-deep-learning-intelligent-video-surveillance-project)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Road Lane line detection – Computer Vision Project in Python*: [https://data-flair.training/blogs/road-lane-line-detection](https://data-flair.training/blogs/road-lane-line-detection)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Raspberry Pi and Movidius NCS Face Recognition*: [https://pyimagesearch.com/2020/01/06/raspberry-pi-and-movidius-ncs-face-recognition](https://pyimagesearch.com/2020/01/06/raspberry-pi-and-movidius-ncs-face-recognition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenVINO, OpenCV, and Movidius NCS on the Raspberry Pi*: [https://pyimagesearch.com/2019/04/08/openvino-opencv-and-movidius-ncs-on-the-raspberry-pi](https://pyimagesearch.com/2019/04/08/openvino-opencv-and-movidius-ncs-on-the-raspberry-pi)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Speed up predictions on low-power devices using Neural Compute Stick and OpenVINO*:
    [https://towardsdatascience.com/speed-up-predictions-on-low-power-devices-using-neural-compute-stick-and-openvino-98f3ae9dcf41](https://towardsdatascience.com/speed-up-predictions-on-low-power-devices-using-neural-compute-stick-and-openvino-98f3ae9dcf41)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Learning with Movidius NCS (pt.4) Installing NCSDK on a Rock64*: [https://www.youtube.com/watch?v=AXzIYk7-lr8](https://www.youtube.com/watch?v=AXzIYk7-lr8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Glyph-based video visualization on Google Map for surveillance in smart cities*:
    [https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-017-0175-4](https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-017-0175-4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Looking-In and Looking-Out of a Vehicle: Computer-Vision-Based Enhanced Vehicle
    Safety*: [https://escholarship.org/content/qt2g6313r2/qt2g6313r2_noSplash_81ae2290f201a6b25e8eecc8a1142845.pdf?t=lnpgaj](https://escholarship.org/content/qt2g6313r2/qt2g6313r2_noSplash_81ae2290f201a6b25e8eecc8a1142845.pdf?t=lnpgaj)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Install Touch Screen and Touch Calibration Program for Raspberry Pi*: [https://www.gechic.com/en/raspberry-pi-install-touch-monitor-and-touch-calibrator-driver](https://www.gechic.com/en/raspberry-pi-install-touch-monitor-and-touch-calibrator-driver)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rotating a Raspberry Pi 4 Touch Monitor*: [https://www.interelectronix.com/rotating-raspberry-pi-4-touch-monitor.html](https://www.interelectronix.com/rotating-raspberry-pi-4-touch-monitor.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Calibrating Touchscreen*: [https://wiki.archlinux.org/title/Calibrating_Touchscreen](https://wiki.archlinux.org/title/Calibrating_Touchscreen)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
