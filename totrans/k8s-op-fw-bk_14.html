<html><head></head><body>
		<div id="_idContainer044">
			<h1 id="_idParaDest-253"><em class="italic"><a id="_idTextAnchor252"/>Chapter 11</em>: Case Study for Core Operator – Etcd Operator</h1>
			<p>In the previous chapter (as for most of this book), we discussed Operators as a tool for managing applications that are deployed on Kubernetes. For most use cases, this is the main purpose of an Operator. In other words, the Operator serves to automate the applications that are developed by an organization. These applications are the products offered to users, and automating them helps to ship them without any issues and keep users happy. Beyond that, Kubernetes itself is simply the underlying architecture. As a part of this, it's usually assumed that Kubernetes doesn't need any additional automation as would be provided by Operators. After all, it was a key point of the early chapters in this book that Operators are not functionally much different than the native suite of controllers that make up the Kubernetes control plane in the first place.</p>
			<p>However, there are situations where an Operator can be used to manage aspects of core Kubernetes. While less common than application Operators, discussing some instances of core Operators helps to show the wide breadth of capabilities that the Operator Framework offers. Starting with a few of these examples, we will explore one in particular (though in less depth than the previous case study), the etcd Operator. Finally, we will explain some of the concepts around cluster stability and upgrades that are important to consider when developing Operators for Kubernetes. This will be done through the following sections:</p>
			<ul>
				<li>Core Operators – extending the Kubernetes platform</li>
				<li>etcd Operator design</li>
				<li>Stability and safety</li>
				<li>Upgrading Kubernetes </li>
			</ul>
			<h1 id="_idParaDest-254"><a id="_idTextAnchor253"/>Core Operators – extending the Kubernetes platform</h1>
			<p>There is no<a id="_idIndexMarker767"/> differentiation in the Operator Framework between <a id="_idIndexMarker768"/>Operators that manage user-facing applications and infrastructure and Operators that manage core Kubernetes components. The only difference is simply in how the concepts of Operator design and development are applied to a slightly different class of problems. Still, the various Pods and control loops that comprise an installation of Kubernetes can be viewed as no different than the workload Pods that they deploy and manage.</p>
			<p>Without getting too existential, this reduction bridges the conceptual gap between development <em class="italic">for</em> Kubernetes and the development <em class="italic">of</em> Kubernetes, making the latter seem much more approachable. This idea opens the gates to give system administrators and DevOps specialists greater control and flexibility over the cloud architectures they orchestrate.</p>
			<p>Next, we will look at a few high-level examples of Operators that extend Kubernetes. We won't go into too much technical detail (such as their API or reconciliation logic), but we will briefly look at each of these examples to understand their use case and demonstrate some of the different ways that Operators can be used to directly manage Kubernetes system processes. The Operators we will look at are as follows:</p>
			<ul>
				<li>RBAC <a id="_idIndexMarker769"/>Manager (<a href="https://github.com/FairwindsOps/rbac-manager">https://github.com/FairwindsOps/rbac-manager</a>)</li>
				<li>Kube<a id="_idIndexMarker770"/> Scheduler Operator (<a href="https://github.com/openshift/cluster-kube-scheduler-operator">https://github.com/openshift/cluster-kube-scheduler-operator</a>)</li>
				<li>etcd <a id="_idIndexMarker771"/>Operator (<a href="https://github.com/coreos/etcd-operator">https://github.com/coreos/etcd-operator</a>)</li>
			</ul>
			<p>Following this general overview, we will go into more technical detail about the etcd Operator in order to provide a similar understanding of the design concepts in this book, as we did in <a href="B18147_10_ePub.xhtml#_idTextAnchor240"><em class="italic">Chapter 10</em></a>, <em class="italic">Case Study for Optional Operators – the Prometheus Operator</em>.</p>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor254"/>RBAC Manager</h2>
			<p><strong class="bold">Role-Based Access Control</strong> (<strong class="bold">RBAC</strong>) policies are the cornerstone of Kubernetes authentication and <a id="_idIndexMarker772"/>authorization. RBAC settings in Kubernetes consist of three types of objects:</p>
			<ul>
				<li><strong class="bold">Roles</strong> (or <strong class="bold">ClusterRoles</strong>, depending <a id="_idIndexMarker773"/>on the scope), which define the <a id="_idIndexMarker774"/>level of access that is <a id="_idIndexMarker775"/>allowed for a user or service</li>
				<li><strong class="bold">ServiceAccounts</strong>, which<a id="_idIndexMarker776"/> are the identifying authorization <a id="_idIndexMarker777"/>object for a Pod</li>
				<li><strong class="bold">RoleBindings</strong> (or <strong class="bold">ClusterRoleBindings</strong>), which <a id="_idIndexMarker778"/>map <a id="_idIndexMarker779"/>ServiceAccounts to <a id="_idIndexMarker780"/>Roles (or ClusterRoles)</li>
			</ul>
			<p>These three types of Kubernetes API objects were explained in <a href="B18147_02_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, <em class="italic">Understanding How Operators Interact with Kubernetes</em>. The relationship between them can be generally summarized by the following diagram (which was also used in that chapter):</p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/Figure_11.1_B18147.jpg" alt="Figure 11.1 – A diagram of the RBAC object relationships&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – A diagram of the RBAC object relationships</p>
			<p>These objects allow for flexibility and control over the design of RBAC policies in a cluster. However, they can become confusing and cumbersome to manage, especially in large clusters with many different levels of access to different services. For example, if a user requires different authorization permissions for different namespaces, an administrator will need to create separate RoleBindings for each namespace with the appropriate grants. Then, if that user leaves the company or changes positions, the administrator will need to track each of those RoleBindings to ensure they can be appropriately updated. This approach is flexible, but it does not scale well for large organizations.</p>
			<p>The <strong class="bold">RBAC Manager</strong> addresses<a id="_idIndexMarker781"/> these problems by providing a layer of abstraction on <a id="_idIndexMarker782"/>top of the native Kubernetes RBAC policy objects. This abstraction is represented by a single <strong class="bold">CustomResourceDefinition (CRD)</strong> that<a id="_idIndexMarker783"/> allows an administrator to effectively create and manage multiple RoleBindings for a user in one spot (with a slightly simplified syntax).</p>
			<p>The effect of the RBAC Manager's simplified approach to authorization is that the management of RoleBindings is removed from a cluster administrator's manual responsibilities. This may be just one object in the chain of relational RBAC objects described previously, but it is the most repetitive and meticulous to track in large clusters. This is because the other objects, Roles/ClusterRoles and ServiceAccounts, will essentially map one-to-one against users, services, and access levels. But the intersection of users and access levels means that there is potentially a many-to-many relationship, held in place by RoleBindings.</p>
			<p>The potential <a id="_idIndexMarker784"/>complexity of even a simple setup is shown by the following<a id="_idIndexMarker785"/> diagram, with four users each having varying levels of access (among hypothetical <em class="italic">read</em>, <em class="italic">write</em>, <em class="italic">view</em>, and <em class="italic">edit</em> roles). In this diagram, each arrow represents a RoleBinding that must be manually maintained:</p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/Figure_11.2_B18147.jpg" alt="Figure 11.2 – A basic RoleBinding mapping of different users and RBAC levels"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – A basic RoleBinding mapping of different users and RBAC levels</p>
			<p>The RBAC Manager would simplify that same setup by inserting its CRD between the user and role definitions, creating a single access point to manage any user's permissions. Additionally, <strong class="bold">UserB</strong> and <strong class="bold">UserC</strong> can share an RBAC Manager CRD since they have the same role. This is shown in the following diagram:</p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/Figure_11.3_B18147.jpg" alt="Figure 11.3 – A diagram of the RBAC Manager CRDs managing RoleBindings"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – A diagram of the RBAC Manager CRDs managing RoleBindings</p>
			<p>In this setup, the individual arrows between CRDs and Roles (each still representing a single RoleBinding) are managed by the RBAC Manager Operator. This has the advantage of reducing the number of individual object relationships that administrators need to orchestrate. It also provides the state-reconciliation benefits of an Operator, wherein any updates or removals of the underlying roles are reconciled by the Operator to match the desired state of the cluster, as declared in the Operator's CRD objects. That behavior is a good example of where an Operator not only helps with the creation and management of complex systems but also ensures their ongoing stability.</p>
			<p>The RBAC <a id="_idIndexMarker786"/>Manager is an Operator whose sole function is to manage <a id="_idIndexMarker787"/>native Kubernetes objects in the cluster. Next, we will discuss the Kube Scheduler Operator, which goes a step further to directly manage a critical component in the cluster, the Scheduler.</p>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor255"/>The Kube Scheduler Operator</h2>
			<p>The <strong class="bold">Kube Scheduler</strong> is <a id="_idIndexMarker788"/>one of the main control plane components in a <a id="_idIndexMarker789"/>Kubernetes cluster. It is responsible for assigning newly created Pods to Nodes, and it tries to do this in the most optimal way possible. This task is vital to the very function of Kubernetes as a cloud platform because if there is no way to schedule Pods onto Nodes, then the Pods cannot run their application code anywhere. And while manually deploying Pods onto specific nodes is possible, the automated evaluation and assignment done by the Scheduler obviously scales much better.</p>
			<p>In addition, the definition of <em class="italic">optimal</em> Pod placement can be wildly different for different organizations (or sometimes just between different clusters from the same organization). For example, some administrators may want to spread the distribution of their workload Pods evenly among nodes to keep average resource consumption relatively low and prevent certain nodes from becoming overloaded. But other system admins may want the exact opposite, compacting as many Pods onto as few nodes as possible in order to minimize infrastructure costs and maximize efficiency. To accommodate these varying needs, the Scheduler provides a configuration API that allows you to customize its behavior.</p>
			<p>The functionality and flexibility offered by the Scheduler are useful, but working with such an important part of a cluster can be risky. This is because if the Scheduler fails, then no other Pods can be scheduled (which includes some system Pods). Also, the complex configuration syntax for the Scheduler elevates the potential for this risk. For these reasons, many Kubernetes users shy away from Scheduler customization.</p>
			<p>To address some of<a id="_idIndexMarker790"/> these issues, OpenShift (Red Hat's distribution of Kubernetes) ships with the <strong class="bold">Kube Scheduler Operator</strong> (in fact, OpenShift relies heavily on core <a id="_idIndexMarker791"/>Operators, which<a id="_idIndexMarker792"/> is discussed more thoroughly at <a href="https://www.redhat.com/en/blog/why-operators-are-essential-kubernetes">https://www.redhat.com/en/blog/why-operators-are-essential-kubernetes</a>). This Operator is built using an Operator library developed specifically for OpenShift Operators rather than the Operator SDK. This allows the Kube Scheduler Operator to manage the health and stability of the critical Scheduler Pods in a way that is consistent with the other features built into OpenShift. While most Operator developers will not need to write their own development libraries, this example shows that in certain use cases, it's fine to do so if you have unique needs that the Operator SDK does not support.</p>
			<p>The Kube Scheduler Operator does follow other design aspects of the Operator Framework, such as the use of CRDs as the primary interface between users and Operator logic. This Operator makes use of two CRDs. One is used to configure Operator-specific settings and report the health status of the Operator through Conditions, while the other holds the Scheduler configuration options that control how the Scheduler assigns Pods to nodes. The Operator goes a step further with the second CRD by predefining sets of Scheduler configurations for common use cases, completely abstracting the underlying Operand settings into easily understood pick-and-choose options.</p>
			<p>The role of the <a id="_idIndexMarker793"/>Kube Scheduler Operator as a system Operator, managing a core component of Kubernetes clusters, is an important task. Its function serves the critical purpose of placing Pods onto appropriate nodes, and its ability to recover from<a id="_idIndexMarker794"/> failures helps maintain cluster health. In the next section, we will look at one more Operator that performs similar management of another critical component, etcd.</p>
			<h2 id="_idParaDest-257"><a id="_idTextAnchor256"/>The etcd Operator</h2>
			<p>etcd (<a href="https://etcd.io/">https://etcd.io/</a>) is the<a id="_idIndexMarker795"/> primary key-value store that backs Kubernetes clusters. It<a id="_idIndexMarker796"/> is the default option for persistent storage of the<a id="_idIndexMarker797"/> API objects that exist in a cluster, preferred for its scalable and distributed design that makes it optimal for high-performance cloud computing.</p>
			<p>The <strong class="bold">etcd Operator</strong> is designed to manage the etcd component in a cluster. Even though it is no longer actively maintained, its GitHub repository is still available in an archived state to provide a historical reference for future developers. For the purpose of this chapter, the preserved status of the etcd Operator offers a permanent, unchanging reference for the design of a core Operator.</p>
			<p>etcd clusters within a Kubernetes cluster can be managed by the etcd Operator in a full variety of functions. These include creating etcd instances, resizing federated installations of etcd, recovering from failures, upgrading etcd without suffering uptime, and performing backups of etcd instances (as well as restoring from those backups). This suite of functionality qualifies the etcd Operator as a Level III Operator in the Capability Model. If you recall from <a href="B18147_01_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introducing the Operator Framework</em>, Level III Operators are referred to <a id="_idIndexMarker798"/>as <strong class="bold">Full Lifecycle</strong> Operators, indicating their ability to manage Operands beyond simple installation and support advanced management operations, such as upgrades and backups.</p>
			<p>Installing and managing etcd manually in a Kubernetes cluster is a fairly advanced task for most users. The majority of Kubernetes developers take the availability of a persistent data store for granted, assuming that all of their objects and cluster state information will always be available. But if the etcd processes fail, there is the potential for it to have catastrophic effects on the entire Kubernetes cluster. </p>
			<p>Similar to any other database, the etcd component in a cluster is responsible for storing all objects that exist in the cluster. Failure to do so can bring even basic cluster functionality to a halt. Such a failure could be caused by a bug, an incompatible API, or even by malformed input when trying to modify the etcd installation (for example, scaling it to provide higher availability). Therefore, a smooth-running cluster is dependent on efficient and accurate access to data in etcd.</p>
			<p>The etcd<a id="_idIndexMarker799"/> Operator aims to simplify the management of etcd by automating the <a id="_idIndexMarker800"/>operational commands required to create, resize, upgrade, back up, and recover etcd clusters through the Operator's various CRDs. In the next section, we will go into more detail about the CRDs that the Operator uses to do this and how those CRDs are reconciled to ensure that the current state of etcd in the cluster matches the administrator's desired state.</p>
			<h1 id="_idParaDest-258"><a id="_idTextAnchor257"/>etcd Operator design</h1>
			<p>Like most other Operators, the<a id="_idIndexMarker801"/> etcd Operator is built with CRDs as its focal interface for user interaction. Understanding the CRDs an Operator provides is a good way to get a basic understanding of how the Operator works, so that is where we will begin our examination of the etcd Operator.</p>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor258"/>CRDs</h2>
			<p>The three CRDs<a id="_idIndexMarker802"/> used by the etcd Operator are <strong class="source-inline">EtcdCluster</strong>, <strong class="source-inline">EtcdBackup</strong>, and <strong class="source-inline">EtcdRestore</strong>. The first CRD, <strong class="source-inline">EtcdCluster</strong>, controls the basic settings for the etcd installation, such as the number of Operand replicas to deploy and the version of etcd that should be installed. A sample object based on this CRD looks like the following:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">simple-etcd-cr.yaml:</p>
			<pre class="source-code">apiVersion: etcd.database.coreos.com/v1beta2</pre>
			<pre class="source-code">kind: EtcdCluster</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: example</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  size: 3</pre>
			<pre class="source-code">  version: 3.5.3</pre>
			<p>In this example, were this object to be created in a cluster (<strong class="source-inline">kubectl create -f simple-etcd-cr.yaml</strong>), it would instruct the etcd Operator to create three replicas of etcd version 3.5.3. Besides these options, the <strong class="source-inline">EtcdCluster</strong> CRD also provides configuration settings for specifying a specific repository to pull the etcd container image from, Operand Pod settings (such as <strong class="source-inline">affinity </strong>and <strong class="source-inline">nodeSelector </strong>settings), and TLS config.</p>
			<p>The other two<a id="_idIndexMarker803"/> aforementioned CRDs, <strong class="source-inline">EtcdBackup</strong> and <strong class="source-inline">EtcdRestore</strong>, work in tandem to allow users to declaratively trigger the backup and subsequent restoration<a id="_idIndexMarker804"/> of etcd data in a cluster. For example, etcd can be backed up to a <strong class="bold">Google Cloud Storage</strong> (<strong class="bold">GCS</strong>) bucket by creating the following custom resource object:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">etcd-gcs-backup.yaml:</p>
			<pre class="source-code">apiVersion: etcd.database.coreos.com/v1beta2</pre>
			<pre class="source-code">kind: EtcdBackup</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: backup-etcd-to-gcs</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  etcdEndpoints:</pre>
			<pre class="source-code">    - https://etcd-cluster-client:2379</pre>
			<pre class="source-code">  storageType: GCS</pre>
			<pre class="source-code">  gcs:</pre>
			<pre class="source-code">    path: gcsbucket/etcd</pre>
			<pre class="source-code">    gcpSecret: &lt;gcp-secret&gt;</pre>
			<p>This instructs the Operator to back up the etcd data available at the cluster endpoint <a href="https://etcd-cluster-client:2379">https://etcd-cluster-client:2379</a> and<a id="_idIndexMarker805"/> send it to the GCS bucket called <strong class="source-inline">gcsbucket/etcd</strong>, authenticated by the <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>) account secret pasted in <strong class="source-inline">&lt;gcp-secret&gt;</strong>. That<a id="_idIndexMarker806"/> data can be restored by later creating the following <strong class="source-inline">EtcdRestore</strong> object:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">etcd-gcs-restore.yaml:</p>
			<pre class="source-code">apiVersion: etcd.database.coreos.com/v1beta2</pre>
			<pre class="source-code">kind: EtcdRestore</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: restore-etcd-from-gcs</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  etcdCluster:</pre>
			<pre class="source-code">    name: sample-etcd-cluster</pre>
			<pre class="source-code">  backupStorageType: GCS</pre>
			<pre class="source-code">  gcs:</pre>
			<pre class="source-code">    path: gcsbucket/etcd</pre>
			<pre class="source-code">    gcpSecret: &lt;gcp-secret&gt; </pre>
			<p>These CRDs make it much easier to perform backups of etcd data and restore those backups by abstracting and automating the heavy lifting into Operator controller logic, but they also ensure that these operations are performed successfully. By eliminating the need for human interaction for the bulk of the backup, they also eliminate the possibility of human error in collecting and transferring the etcd data to a backup location. If the Operator does encounter an error when performing the backup, this information is reported through the <strong class="source-inline">Status</strong> section of the custom resource object for that backup.</p>
			<h2 id="_idParaDest-260"><a id="_idTextAnchor259"/>Reconciliation logic</h2>
			<p>In the etcd Operator's <a id="_idIndexMarker807"/>design, each CRD type has its own reconciliation controller. This is good Operator design, as recommended by the best practices in the Operator Framework's documentation. Similar to the Prometheus Operator from <a href="B18147_10_ePub.xhtml#_idTextAnchor240"><em class="italic">Chapter 10</em></a>, <em class="italic">Case Study for Optional Operators – the Prometheus Operator</em>, each controller monitors for cluster events, involving the CRD it reconciles. These events then trigger a level-based reconciliation loop that either creates (or modifies) an etcd cluster, performs a backup of a running etcd cluster, or restores the already backed-up data from an earlier etcd cluster.</p>
			<p>Part of this reconciliation logic involves reporting the status of the Operand etcd Pods. After receiving an event and during the ensuing reconciliation cycle, if the Operator detects a change in the etcd cluster's status, it reports on this through the matching custom resource object's <strong class="source-inline">Status</strong> field. This type has the following sub-fields:</p>
			<pre class="source-code">type ClusterStatus struct {</pre>
			<pre class="source-code">     // Phase is the cluster running phase</pre>
			<pre class="source-code">     Phase  ClusterPhase `json:"phase"`</pre>
			<pre class="source-code">     Reason string       `json:"reason,omitempty"`</pre>
			<pre class="source-code">     // ControlPaused indicates the operator pauses the control of the cluster.</pre>
			<pre class="source-code">     ControlPaused bool `json:"controlPaused,omitempty"`</pre>
			<pre class="source-code">     <strong class="bold">// Condition keeps track of all cluster conditions, if they exist.</strong></pre>
			<pre class="source-code">     <strong class="bold">Conditions []ClusterCondition `json:"conditions,omitempty"`</strong></pre>
			<pre class="source-code">     // Size is the current size of the cluster</pre>
			<pre class="source-code">     Size int `json:"size"`</pre>
			<pre class="source-code">     // ServiceName is the LB service for accessing etcd nodes.</pre>
			<pre class="source-code">     ServiceName string `json:"serviceName,omitempty"`</pre>
			<pre class="source-code">     // ClientPort is the port for etcd client to access.</pre>
			<pre class="source-code">     // It's the same on client LB service and etcd nodes.</pre>
			<pre class="source-code">     ClientPort int `json:"clientPort,omitempty"`</pre>
			<pre class="source-code">     // Members are the etcd members in the cluster</pre>
			<pre class="source-code">     Members MembersStatus `json:"members"`</pre>
			<pre class="source-code">     // CurrentVersion is the current cluster version</pre>
			<pre class="source-code">     CurrentVersion string `json:"currentVersion"`</pre>
			<pre class="source-code">     // TargetVersion is the version the cluster upgrading to.</pre>
			<pre class="source-code">     // If the cluster is not upgrading, TargetVersion is empty.</pre>
			<pre class="source-code">     TargetVersion string `json:"targetVersion"`</pre>
			<pre class="source-code">}</pre>
			<p>Note that the etcd<a id="_idIndexMarker808"/> Operator implements its own <strong class="source-inline">ClusterCondition</strong> type (rather than the <strong class="source-inline">Condition</strong> type used in <a href="B18147_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>, <em class="italic">Developing an Operator – Advanced Functionality</em>). This is because active maintenance of the etcd Operator was archived shortly before the native <strong class="source-inline">Condition</strong> type was merged into upstream Kubernetes. However, we mention this here as another tangible example where awareness of upstream <strong class="bold">Kubernetes Enhancement Proposals</strong> (<strong class="bold">KEPs</strong>) and their<a id="_idIndexMarker809"/> status throughout the timeline of a release cycle can have an impact on third-party Operator development (see <a href="B18147_08_ePub.xhtml#_idTextAnchor126"><em class="italic">Chapter 8</em></a>, <em class="italic">Preparing for Ongoing Maintenance of Your Operator</em>).</p>
			<p>Beyond just <a id="_idIndexMarker810"/>reconciling the desired state of the etcd Operands and reporting their status, the etcd Operator also has reconciliation logic to recover from failure states.</p>
			<h2 id="_idParaDest-261"><a id="_idTextAnchor260"/>Failure recovery</h2>
			<p>The etcd Operator collects status<a id="_idIndexMarker811"/> information on the etcd Operands and, based on the availability of a majority of the Operand Pods (known as a quorum), attempts <a id="_idIndexMarker812"/>to recover the etcd cluster if necessary. In the edge case where an etcd cluster has no running Operand Pods, the Operator must make an opinionated decision whether to interpret this as an etcd cluster that has completely failed or simply one that has yet to be initialized. In this case, the Operator always chooses to interpret it as a failed cluster and attempts to restore the cluster from a backup, if one is available. This is not only the simplest option for the Operator but also the safest (see the following <em class="italic">Stability and safety</em> section), as the alternative (assuming no availability is simply uninitialized) could result in ignored failure states that would otherwise be recovered from.</p>
			<p>Besides recovering from failures in its Operands, the etcd Operator also has reconciliation logic to recover from failures in itself. Part of this recovery path hinges on the etcd Operator registering its own CRD in the cluster. This is interesting because it contradicts the best practices recommended by the Operator SDK documentation. But by coupling the creation of the CRD with the Operator (rather than having an administrator create it – for example, with <strong class="source-inline">kubectl create -f</strong>), the Operator can use the presence of that CRD to determine whether it exists as a new installation or has previously been running in a cluster.</p>
			<p>This is applicable because when any Pod restarts, including Operator Pods, they will not have any inherent knowledge about their predecessors. From that Pod's perspective, it has begun life with a fresh start. So, if the etcd Operator starts and finds its CRD already registered in a Kubernetes cluster, that CRD serves as a sort of canary indicator to inform the Operator that it should begin reconstructing the state of any existing Operand Pods.</p>
			<p>Failure recovery is one of the most helpful benefits that Operators provide because the automated error handling allows small hiccups in the day-to-day operation of a cluster to be quickly and gracefully absorbed. In the case of the etcd Operator, it makes opinionated decisions in handling failures and managing its own CRD to create a support contract that clearly defines its recovery procedures. Doing so contributes greatly to the cluster's overall stability, which is the focus of the next section.</p>
			<h1 id="_idParaDest-262"><a id="_idTextAnchor261"/>Stability and safety</h1>
			<p>Applications and <a id="_idIndexMarker813"/>components in a Kubernetes cluster can occasionally be prone to <a id="_idIndexMarker814"/>unexpected failures, such as network timeouts or code panics due to unforeseen bugs. It is part of an Operator's job to monitor for these spontaneous failures and attempt to recover from them. But of course, human error in the pursuit of adjusting the system can be another source of failure. As a result, any interaction with or modification of the core system components in Kubernetes brings inherent risk. This is elevated because manual adjustments to one component can contain errors (even minor ones) that cause a domino effect, as other components that depend on it begin reacting to the original error.</p>
			<p>Perhaps the prime objective of an Operator is to provide stability and safety in production environments. Here, stability refers to the ongoing performant operation of the Operand programs, and safety is the ability of an Operator to sanitize and validate any inputs or modifications to that program. Think of an Operator like a car, whose purpose is to run its motor smoothly along the road while allowing the driver to control it within reasonable parameters. In Kubernetes, Operators for system components offer some of the leading examples of designs where exceptional care has been taken to offer a safe design that lends itself to stable functioning.</p>
			<p>The Kube Scheduler Operator from our earlier example ensures cluster stability by taking an opinionated approach toward the available configuration options it provides. In other words, the total number of possible scheduling settings is restricted (via predefined collections of options) to only those arrangements that have been tested and known to have minimal risk to the cluster. Then, changes to these settings are rolled out in a predetermined manner by the automation code in the Operator. The combination of not only automating changes but restricting the available changes significantly reduces any opportunity for users to make an error when updating their cluster. This abstraction is shown in the following diagram, where the user only needs to interact with the CRD, while all fields in the predefined settings are hidden away as black-box options:</p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/Figure_11.4_B18147.jpg" alt="Figure 11.4 – A diagram of the Kube Scheduler Operator config abstraction"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 – A diagram of the Kube Scheduler Operator config abstraction</p>
			<p>The etcd Operator has similar safeguards in place. For example, uninstalling the Operator does not automatically remove the custom resource objects associated with it. While some Operators may implement this sort of garbage collection as a feature to make uninstallation easier, the developers of the etcd Operator intentionally chose not to do so in order to prevent accidental deletion of running etcd clusters.</p>
			<p>Another approach the etcd Operator takes toward achieving operational stability is in the spreading of its Operand Pods. As mentioned earlier, the etcd Operator allows users to configure individual Pod settings for the etcd Operand instances it deploys, such as <strong class="source-inline">nodeSelectors</strong>. One interesting field to note, however, is that it provides a simple Boolean option called <strong class="source-inline">antiAffinity</strong>. This value can be set within the <strong class="source-inline">EtcdCluster </strong>CRD, as follows:</p>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  size: 3</pre>
			<pre class="source-code">  pod:</pre>
			<pre class="source-code">    antiAffinity: true</pre>
			<p>Enabling this value <a id="_idIndexMarker815"/>serves as shorthand to label the Operand etcd Pods with <a id="_idIndexMarker816"/>an <strong class="source-inline">antiAffinity</strong> field to themselves. The effect of this is that the individual Pods will not be scheduled onto the same nodes. This has the benefit of ensuring that the Pods are distributed in a highly available fashion so that if one node goes down, it does not risk bringing down a significant number of the etcd Pods.</p>
			<p>The effective change of this one line on the Operand Pods looks something like the following:</p>
			<pre class="source-code">apiVersion: v1</pre>
			<pre class="source-code">kind: Pod</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: etcd-cluster-pod</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  affinity:</pre>
			<pre class="source-code">    podAntiAffinity:</pre>
			<pre class="source-code">      requiredDuringSchedulingIgnoredDuringExecution:</pre>
			<pre class="source-code">      - labelSelector:</pre>
			<pre class="source-code">          matchLabels:</pre>
			<pre class="source-code">            etcd_cluster: &lt;etcd-resource-name&gt;</pre>
			<pre class="source-code">        topologyKey: kubernetes.io/hostname</pre>
			<p>Packaging this into a<a id="_idIndexMarker817"/> single line not only saves time but also saves users from potential <a id="_idIndexMarker818"/>mistakes. For example, the following snippet looks very similar to the preceding one. However, it has the exact opposite effect, requiring that all similar etcd Pods are scheduled onto the same node:</p>
			<pre class="source-code">apiVersion: v1</pre>
			<pre class="source-code">kind: Pod</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: etcd-cluster-pod</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  affinity:</pre>
			<pre class="source-code">    podAffinity:</pre>
			<pre class="source-code">      requiredDuringSchedulingIgnoredDuringExecution:</pre>
			<pre class="source-code">      - labelSelector:</pre>
			<pre class="source-code">          matchLabels:</pre>
			<pre class="source-code">            etcd_cluster: &lt;etcd-resource-name&gt;</pre>
			<pre class="source-code">        topologyKey: kubernetes.io/hostname</pre>
			<p>If this mistake went unnoticed and the node that hosted all of the etcd Pods went down, it would result in potentially the entire cluster being unable to function.</p>
			<p>While the <strong class="source-inline">antiAffinity</strong> setting was eventually deprecated in favor of allowing users to provide full Pod Affinity blocks (as shown previously), its presence offers an example of the safe configuration that Operators can provide. Swapping that with the option of full Affinity blocks has the trade-off of safety for flexibility, which is a delicate scale that Operator developers must balance in their own implementations.</p>
			<p>These are just a few <a id="_idIndexMarker819"/>examples of the ways in which Operators can package complex<a id="_idIndexMarker820"/> operations into abstracted user-facing settings to provide a safe interface and stable cluster management. Next, we'll look at how Operators can maintain that stability during a potentially tumultuous period of upgrades.</p>
			<h1 id="_idParaDest-263"><a id="_idTextAnchor262"/>Upgrading Kubernetes</h1>
			<p>In software, upgrades are usually <a id="_idIndexMarker821"/>a fact of life. Very rarely is a single piece of software that lives for more than a brief time able to run continuously without upgrading its code to a new version. In <a href="B18147_08_ePub.xhtml#_idTextAnchor126"><em class="italic">Chapter 8</em></a>, <em class="italic">Preparing for Ongoing Maintenance of Your Operator</em>, we discussed ways to prepare and publish new version releases of an Operator. In that chapter, we also explained the different phases of the Kubernetes release cycle and how they impact Operator development. This is especially true for Operators that are deeply entrenched in the Kubernetes platform components.</p>
			<p>For system Operators, fluctuating changes in the upstream Kubernetes code base are often more than just simple features. For example, when <strong class="source-inline">kube-scheduler</strong> was refactored to accept an entirely different format of configuration (referred to as the Scheduler Framework, which is no<a id="_idIndexMarker822"/> relation to the Operator Framework – see <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/">https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/</a> for more details – though they are not technically relevant here), the predefined settings profiles handled by the Kube Scheduler Operator's CRD needed to be completely rewritten to accommodate that change. System Operators absolutely must be aware of changes such as this between Kubernetes versions because in many cases, they themselves are performing the upgrade to a new version.</p>
			<p>The etcd Operator has logic to handle upgrades of its Operand, etcd. This adds a point of complexity, as etcd is technically not part of the Kubernetes payload that runs a cluster. So, while the upstream Kubernetes developers must coordinate their releases to support certain versions of etcd (as etcd developers work on their own releases), the etcd Operator developers need to react to changes in both dependencies.</p>
			<p>In this way, the <a id="_idIndexMarker823"/>Operand is also a dependency for an Operator, which comes with its own constraints that must be respected. For example, when performing an upgrade, the etcd Operator first verifies that the desired new version is allowed by the upgrade strategies supported by etcd. This specifies that etcd can only be upgraded by at most one minor version at a time (for example, from 3.4 to 3.5). If more than this is specified, the etcd Operator can determine that the upgrade should not proceed and aborts the process.</p>
			<p>If an upgrade is allowed to proceed, the etcd Operator updates each of its Operand Pods with a <strong class="bold">rolling upgrade</strong> strategy. This is a common way to upgrade versions of applications, and even <a id="_idIndexMarker824"/>Kubernetes itself, where one Pod (or Node, in the case of Kubernetes) in the available set is upgraded at a time. This allows for minimal downtime, but it also allows for the ability to roll back (revert to the previous working version) if an error is encountered when upgrading any single instance. This is a critical stability benefit for any Operator to support as it provides the opportunity to diagnose the error in a stable environment. The etcd Operator uses its own backup and restoration workflows to handle these kinds of rollbacks during an upgrade.</p>
			<p>Upgrading Kubernetes (as with upgrading any software) can be a tedious process, which is why it's not uncommon for cluster administrators to delay upgrading until the last possible minute. Unfortunately, this only exacerbates the problem, as more incompatible changes are introduced with each skipped version. But introducing Operators to help with handling these changes can make upgrades much smoother or, at the very least, assist with collecting information to diagnose failed upgrades. While most Operator developers will not be writing Operators for system-level Kubernetes components, the lessons from those who have will serve as examples of how to handle upgrades in even the most mission-critical scenarios.</p>
			<h1 id="_idParaDest-264"><a id="_idTextAnchor263"/>Summary</h1>
			<p>In this chapter, we took one final look at some examples of Operators responsible for some of the most critical tasks in a Kubernetes cluster. These core, or system, Operators automatically manage the most complex and delicate workflows for Kubernetes administrators while balancing functionality and care for the importance of their Operands. From these types of Operators, we can draw lessons about the fullest spectrum of capabilities in the Operator Framework.</p>
			<p>The intent of this chapter wasn't to offer these examples as tutorials or imply that many developers will write their own Operators to manage core Kubernetes components. Rather, they serve as extreme cases where concepts from the Operator Framework were applied to outlier problem sets. But understanding the edge cases in any problem is the best way to form a strong understanding of the entire problem.</p>
			<p>We did this by beginning the chapter with a brief overview of three different system Operators. Then, we took a deeper dive into the technical details behind the etcd Operator, understanding how it uses CRDs and reconciliation logic to manage the data backup storage for Kubernetes. Finally, we concluded by exploring how Operators such as the etcd Operator provide a stable and safe interface for their tasks, even when upgrading versions of Kubernetes.</p>
			<p>With that, we end this introduction to the Operator Framework. The topic of Kubernetes Operators is a broad one, with many potential technical details to explore and volumes of excellent literature already written by well-qualified authors. It would be difficult to succinctly compile all of the available information on Operators from every available resource, so in this book, we simply tried to cover the most important topics in novel and interesting ways. Hopefully, you found this book insightful and helpful in building your own understanding of the Operator Framework and will find ways to apply these lessons when building your own Operators. Happy hacking!</p>
		</div>
	</body></html>