<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer080">
    <h1 class="chapterNumber">3</h1>
    <h1 class="chapterTitle" id="_idParaDest-107">Installing Your First Kubernetes Cluster</h1>
    <p class="normal">In the previous chapter, we had the opportunity to explain what Kubernetes is, its distributed architecture, the anatomy of a working cluster, and how it can manage your Docker containers on multiple Linux machines. Now, we are going to get our hands dirty because it’s time to install Kubernetes. The main objective of this chapter is to get you a working Kubernetes installation for the coming chapters. This is so that you have your own cluster to work on, practice with, and learn about while reading this book.</p>
    <p class="normal">Installing Kubernetes means that you have to get the different components to work together. Of course, we won’t do that the hard way of setting up individual cluster components; instead, we will use automated tools. These tools have the benefit of launching and configuring all of the components for us locally. This automated Kubernetes cluster setup is particularly beneficial for DevOps teams rapidly testing changes to YAML, developers wanting a local environment to test applications, and security teams rapidly testing changes to Kubernetes object YAML definitions.</p>
    <p class="normal">If you don’t want to have a Kubernetes cluster on your local machine, we’re also going to set up minimalist yet full-featured production-ready Kubernetes clusters on <strong class="keyWord">Google Kubernetes Engine</strong> (<strong class="keyWord">GKE</strong>), <strong class="keyWord">Amazon Elastic Kubernetes Service</strong> (<strong class="keyWord">EKS</strong>), and <strong class="keyWord">Azure Kubernetes Service</strong> (<strong class="keyWord">AKS</strong>) in later chapters of this book. These are cloud-based and production-ready solutions. In this way, you can practice and learn on a real-world Kubernetes cluster hosted on the cloud.</p>
    <p class="normal">Whether you want to go local or on the cloud, it is your choice. You’ll have to choose the one that suits you best by considering each solution’s benefits and drawbacks. In both cases, however, you’ll require a working <code class="inlineCode">kubectl</code> installed on your local workstation to communicate with the resulting Kubernetes cluster. Installation instructions for <code class="inlineCode">kubectl</code> are available in the previous chapter, <em class="chapterRef">Chapter 2</em>, <em class="italic">Kubernetes Architecture – from Container Images to Running Pods</em>.</p>
    <p class="normal">In this chapter, we’re going to cover the following main topics:</p>
    <ul>
      <li class="bulletList">Installing a Kubernetes cluster with <code class="inlineCode">minikube</code></li>
      <li class="bulletList">Multi-node Kubernetes cluster with <code class="inlineCode">kind</code></li>
      <li class="bulletList">Alternative Kubernetes learning environments</li>
      <li class="bulletList">Production-grade Kubernetes clusters</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-108">Technical requirements</h1>
    <p class="normal">To follow along with the examples in this chapter, you will require the following:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">kubectl</code> installed on your local machine</li>
      <li class="bulletList">A workstation with 2 CPUs or more, 2 GB of free memory, and 20 GB of free disk space. (You will need more resources if you want to explore the multi-node cluster environments.)</li>
      <li class="bulletList">A container or virtual machine manager installed on the workstation, such as Docker, QEMU, Hyperkit, Hyper-V, KVM, Parallels, Podman, VirtualBox, or VMware Fusion/Workstation</li>
      <li class="bulletList">Reliable internet access</li>
    </ul>
    <p class="normal">You can download the latest code samples for this chapter from the official GitHub repository at <a href="https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter03"><span class="url">https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter03</span></a></p>
    <h1 class="heading-1" id="_idParaDest-109">Installing a Kubernetes cluster with minikube</h1>
    <p class="normal">In this section, we are going to learn <a id="_idIndexMarker249"/>how to install a local Kubernetes cluster using <code class="inlineCode">minikube</code>. It’s probably the easiest way to get a working Kubernetes installation locally. By the end of this section, you’re going to have a working single-node<a id="_idIndexMarker250"/> Kubernetes cluster installed on your local machine.</p>
    <p class="normal"><code class="inlineCode">minikube</code> is easy to use and is completely free. It’s going to install all of the Kubernetes components on your local machine and configure all of them. Uninstalling all of the components through <code class="inlineCode">minikube</code> is easy too, so you won’t be stuck with it if, one day, you want to destroy your local cluster.</p>
    <p class="normal"><code class="inlineCode">minikube</code> has one big advantage compared to full-fledged production cluster deployment methods: it’s a super useful tool for testing the Kubernetes scenarios quickly. If you do not wish to use <code class="inlineCode">minikube</code>, you can completely skip this section and choose other methods described in this chapter.</p>
    <p class="normal">While <code class="inlineCode">minikube</code> is a popular choice for local<a id="_idIndexMarker251"/> Kubernetes development, it comes with some trade-offs in <a id="_idIndexMarker252"/>resource usage and feature fidelity compared to a full-blown production cluster:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Resource strain</strong>: Running <code class="inlineCode">minikube</code> alongside other processes on your local machine can be resource-intensive. It requires a good amount of CPU and RAM when you want to create larger Kubernetes clusters, potentially impacting the performance of other applications.</li>
      <li class="bulletList"><strong class="keyWord">Networking discrepancies</strong>: Unlike a production Kubernetes cluster, <code class="inlineCode">minikube</code>'s default network setup may not fully mimic real-world networking environments. This can introduce challenges when replicating or troubleshooting network-related issues that might occur in production.</li>
      <li class="bulletList"><strong class="keyWord">Compatibility considerations</strong>: Certain Kubernetes features or third-party tools might require a more complete Kubernetes setup than what <code class="inlineCode">minikube</code> offers, leading to compatibility issues during development.</li>
      <li class="bulletList"><strong class="keyWord">Persistent storage challenges</strong>: Managing persistent storage for applications within <code class="inlineCode">minikube</code> can be cumbersome due to limitations in its persistent volume support compared to a full Kubernetes cluster.</li>
    </ul>
    <p class="normal">We will learn how to install <code class="inlineCode">minikube</code> and deploy and develop a Kubernetes cluster in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-110">Installing minikube</h2>
    <p class="normal">Here, we will see how the <code class="inlineCode">minikube</code> tool <a id="_idIndexMarker253"/>can be installed on Linux, macOS, and Windows. Installing <code class="inlineCode">minikube</code> using the binary or package manager method is a straightforward task, as explained in the following sections.</p>
    <div class="note">
      <p class="normal">You can<a id="_idIndexMarker254"/> install <code class="inlineCode">minikube</code> using the native package manager such as <code class="inlineCode">apt-get, yum</code>, Zypper, Homebrew (macOS), or Chocolatey (Windows). Refer to the documentation (<a href="https://minikube.sigs.k8s.io/docs/start"><span class="url">https://minikube.sigs.k8s.io/docs/start</span></a>) to learn more.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-111">Installing minikube on Linux</h3>
    <p class="normal">On Linux, <code class="inlineCode">minikube</code> can<a id="_idIndexMarker255"/> be installed using the Debian package, the RPM package, or the<a id="_idIndexMarker256"/> binary, as explained below:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">sudo</span> install minikube-linux-amd64 /usr/local/bin/minikube
<span class="hljs-con-meta"># </span>Verify minikube <span class="hljs-con-built_in">command</span> and path
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">which</span> minikube
/usr/local/bin/minikube
</code></pre>
    <p class="normal">Please note, the path can be different in your workstation depending on the operating system. You need to ensure the path is included in the <strong class="keyWord">PATH</strong> environment variables so that <code class="inlineCode">minikube</code> command will work without any issues.</p>
    <h3 class="heading-3" id="_idParaDest-112">Installing minikube on macOS</h3>
    <p class="normal">On macOS, <code class="inlineCode">minikube</code> can be installed <a id="_idIndexMarker257"/>with the binary, as explained below:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">sudo</span> install minikube-darwin-amd64 /usr/local/bin/minikube
<span class="hljs-con-meta"># </span>Verify minikube <span class="hljs-con-built_in">command</span> and path
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">which</span> minikube
</code></pre>
    <p class="normal">It is also possible to install <code class="inlineCode">minikube</code> on<a id="_idIndexMarker258"/> macOS using the package manager, Homebrew.</p>
    <h3 class="heading-3" id="_idParaDest-113">Installing minikube on Windows</h3>
    <p class="normal">Like macOS and Linux, it is <a id="_idIndexMarker259"/>possible to install <code class="inlineCode">minikube</code> on Windows using <a id="_idIndexMarker260"/>multiple methods, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta"># </span>Using Windows Package Manager (<span class="hljs-con-keyword">if</span> installed)
<span class="hljs-con-meta">$ </span>winget install minikube
<span class="hljs-con-meta"># </span>Using Chocolatey
<span class="hljs-con-meta">$ </span>choco install minikube
<span class="hljs-con-meta"># </span>Via .exe download and setting the PATH
<span class="hljs-con-meta"># </span>1. Download minikube: https://storage.googleapis.com/minikube/releases/latest/minikube-installer.exe
<span class="hljs-con-meta"># </span>2. Set PATH
</code></pre>
    <p class="normal">Once you have configured <code class="inlineCode">minikube</code>, then you can<a id="_idIndexMarker261"/> create different types of Kubernetes clusters using <code class="inlineCode">minikube</code>, as explained <a id="_idIndexMarker262"/>in the next sections.</p>
    <h2 class="heading-2" id="_idParaDest-114">minikube configurations</h2>
    <p class="normal">The <code class="inlineCode">minikube</code> utility comes <a id="_idIndexMarker263"/>with minimal but effective customizations required for a development environment.</p>
    <p class="normal">For example, the default specification of the Kubernetes cluster created by <code class="inlineCode">minikube</code> will be 2 CPUs and 2 GB memory. It is possible to adjust this value using the following command if you need a bigger Kubernetes cluster node:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube config <span class="hljs-con-built_in">set</span> cpus 4
❗  These changes will take effect upon a minikube delete and then a minikube start
<span class="hljs-con-meta">$ </span>minikube config <span class="hljs-con-built_in">set</span> memory 16000
❗  These changes will take effect upon a minikube delete and then a minikube start
<span class="hljs-con-meta">$ </span>minikube config <span class="hljs-con-built_in">set</span> container-runtime containerd
❗  These changes will take effect upon a minikube delete and then a minikube start
</code></pre>
    <p class="normal">As you see on the screen, you need to delete and recreate the <code class="inlineCode">minikube</code> cluster to apply the settings.</p>
    <h2 class="heading-2" id="_idParaDest-115">Drivers for minikube</h2>
    <p class="normal"><code class="inlineCode">minikube</code> acts as a simple and lightweight way to run a local Kubernetes cluster on your development machine. To <a id="_idIndexMarker264"/>achieve this, it leverages <strong class="keyWord">drivers</strong> – the workhorses behind managing<a id="_idIndexMarker265"/> the cluster’s lifecycle. These drivers interact with different virtualization and containerization technologies, allowing <code class="inlineCode">minikube</code> to create, configure, and control the underlying infrastructure for your local Kubernetes environment. <code class="inlineCode">minikube</code>'s driver flexibility empowers you to deploy your cluster as a virtual machine, a container, or even directly on the bare metal of your development machine, tailoring the setup to your specific needs and preferences:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Container drivers</strong>: For a containerized <a id="_idIndexMarker266"/>approach, <code class="inlineCode">minikube</code> can leverage a local Podman or Docker installation. This allows you to run <code class="inlineCode">minikube</code> directly within a container on your development machine, potentially offering a more lightweight and resource-efficient setup.</li>
      <li class="bulletList"><strong class="keyWord">Virtual machine (VM) drivers</strong>: If you prefer a VM approach, <code class="inlineCode">minikube</code> can launch VMs on your machine. These<a id="_idIndexMarker267"/> VMs will then house and wrap the necessary Kubernetes components, providing a more isolated environment for your local cluster.</li>
    </ul>
    <p class="normal">The choice between container <a id="_idIndexMarker268"/>and VM drivers depends on your specific needs and preferences, as well as your development environment’s capabilities.</p>
    <p class="normal">Refer to the <code class="inlineCode">minikube</code> driver documentation (<a href="https://minikube.sigs.k8s.io/docs/drivers/"><span class="url">https://minikube.sigs.k8s.io/docs/drivers/</span></a>) to learn about available and supported <code class="inlineCode">minikube</code> drivers and<a id="_idIndexMarker269"/> supported operating systems.</p>
    <p class="normal">It is also possible to set the default driver for <code class="inlineCode">minikube</code> using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span> minikube config <span class="hljs-con-built_in">set</span> driver docker
❗  These changes will take effect upon a minikube delete and then a minikube start
<span class="hljs-con-meta"># </span>or <span class="hljs-con-built_in">set</span> the VirtualBox as driver
<span class="hljs-con-meta">$ </span> minikube config <span class="hljs-con-built_in">set</span> driver virtualbox
❗  These changes will take effect upon a minikube delete and then a minikube start$  minikube config view driver
- driver: docker
</code></pre>
    <p class="normal">Also, the driver can be set while creating the <code class="inlineCode">minikube</code> cluster as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube start --driver=docker
</code></pre>
    <p class="normal">Prerequisites depend on the individual <code class="inlineCode">minikube</code> drivers and must be installed and prepared. These may include an installation of Docker, Podman, or VirtualBox with permissions granted on a specific operating system. Installation and configuration instructions can be found in the <code class="inlineCode">minikube</code> driver-specific documentation (<a href="https://minikube.sigs.k8s.io/docs/drivers"><span class="url">https://minikube.sigs.k8s.io/docs/drivers</span></a>).</p>
    <p class="normal">Let us learn how to launch our first Kubernetes cluster using <code class="inlineCode">minikube</code> in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-116">Launching a single-node Kubernetes cluster using minikube</h2>
    <p class="normal">The main purpose of <code class="inlineCode">minikube</code> is to launch<a id="_idIndexMarker270"/> the Kubernetes components on your local system and have them communicate with each other. In<a id="_idIndexMarker271"/> the following sections, we will learn how to deploy <code class="inlineCode">minikube</code> clusters using the VirtualBox driver and Docker.</p>
    <h3 class="heading-3" id="_idParaDest-117">Setting up minikube using VMs</h3>
    <p class="normal">The VM method requires you to install a <a id="_idIndexMarker272"/>hypervisor on top of your<a id="_idIndexMarker273"/> workstation as follows:</p>
    <ul>
      <li class="bulletList">Linux: KVM2 (preferred), VirtualBox, QEMU</li>
      <li class="bulletList">Windows: Hyper-V (preferred), VirtualBox, VMware Workstation, QEMU</li>
      <li class="bulletList">macOS: Hyperkit, VirtualBox, Parallels, VMware Fusion, QEMU</li>
    </ul>
    <p class="normal">Then, <code class="inlineCode">minikube</code> will wrap all of the Kubernetes components into a VM that will be launched.</p>
    <p class="normal">In the following example, we are using Fedora 39 as our workstation and VirtualBox as our hypervisor software as it is available for Linux, macOS, and Windows.</p>
    <p class="normal">Refer to <a href="https://www.virtualbox.org/wiki/Downloads"><span class="url">https://www.virtualbox.org/wiki/Downloads</span></a> to download and install VirtualBox for your workstation. You are free to use your own choice of virtualization software and always follow <a id="_idIndexMarker274"/>the documentation (<a href="https://minikube.sigs.k8s.io/docs/drivers/"><span class="url">https://minikube.sigs.k8s.io/docs/drivers/</span></a>) to see the supported virtualization software.</p>
    <div class="note">
      <p class="normal">Do not confuse the <code class="inlineCode">minikube</code> version and the deployed Kubernetes version. For example, <code class="inlineCode">minikube 1.32</code> uses Kubernetes 1.28 for stability and compatibility reasons. This allows for thorough testing, broader tool support, controlled rollouts, and longer-term support for older versions. Users still have the flexibility to run different versions of Kubernetes independently. This balance between stability and flexibility makes <code class="inlineCode">minikube</code> a reliable and versatile platform for developers.</p>
    </div>
    <p class="normal">On your workstation where you have installed <code class="inlineCode">minikube</code> and VirtualBox, execute the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span> minikube start --driver=virtualbox --memory=8000m --cpus=2
</code></pre>
    <p class="normal">If you are using a particular version of <code class="inlineCode">minikube</code> but want to install a different version of Kubernetes, then you can mention the specific version, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube start --driver=virtualbox --memory=8000m --cpus=2 --kubernetes-version=1.29.0
</code></pre>
    <p class="normal">You will see that <code class="inlineCode">minikube</code> is <a id="_idIndexMarker275"/>starting the Kubernetes <a id="_idIndexMarker276"/>deployment process including the VM image downloading, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">😄  minikube v1.32.0 on Fedora 39
❗  Specified Kubernetes version 1.29.0 is newer than the newest supported version: v1.28.3. Use `minikube config defaults kubernetes-version` for details.
❗  Specified Kubernetes version 1.29.0 not found in Kubernetes version list
🤔  Searching the internet for Kubernetes version...
✅  Kubernetes version 1.29.0 found in GitHub version list
✨  Using the virtualbox driver based on user configuration
👍  Starting control plane node minikube in cluster minikube
🔥  Creating virtualbox VM (CPUs=2, Memory=8000MB, Disk=20000MB) ...
🐳  Preparing Kubernetes v1.29.0 on Docker 24.0.7 ...
    ▪ Generating certificates and keys ...
    ▪ Booting up control plane ...
    ▪ Configuring RBAC rules ...
🔗  Configuring bridge CNI (Container Networking Interface) ...
    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
</code></pre>
    <p class="normal">You may also see the below information based on your workstation’s operating system and the virtualization software as a recommendation:</p>
    <pre class="programlisting con"><code class="hljs-con">│    You have selected "virtualbox" driver, but there are better options!                          │    For better performance and support consider using a different driver:
│            - kvm2                                                                                 │            - qemu2                                                                                │
│    To turn off this warning run:
│            $ minikube config set WantVirtualBoxDriverWarning false                                │    To learn more about on minikube drivers checkout https://minikube.sigs.k8s.io/docs/drivers/    │
│    To see benchmarks checkout https://minikube.sigs.k8s.io/docs/benchmarks/cpuusage/              │
</code></pre>
    <p class="normal">Finally, you will see the following success message from <code class="inlineCode">minikube</code>:</p>
    <pre class="programlisting con"><code class="hljs-con">🔎  Verifying Kubernetes components...
🌟  Enabled addons: storage-provisioner, default-storageclass
🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
</code></pre>
    <p class="normal">Yes, you have deployed a<a id="_idIndexMarker277"/> fully working Kubernetes cluster within a minute and are ready to deploy your application.</p>
    <p class="normal">Now verify the Kubernetes<a id="_idIndexMarker278"/> cluster status using the <code class="inlineCode">minikube</code> command, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span> minikube status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
</code></pre>
    <p class="normal">You can also see the new <code class="inlineCode">minikube</code> VM in your VirtualBox UI, as follows:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_03_01.png"/></figure>
    <p class="packt_figref">Figure 3.1: The minikube VM on the VirtualBox UI</p>
    <p class="normal">In the next section, we<a id="_idIndexMarker279"/> will learn how to deploy Kubernetes clusters<a id="_idIndexMarker280"/> using <code class="inlineCode">minikube</code> and containers.</p>
    <h3 class="heading-3" id="_idParaDest-118">Setting up minikube using a container</h3>
    <p class="normal">The container method is simpler. Instead of using a VM, <code class="inlineCode">minikube</code> uses a local Docker Engine instance or Podman to launch the Kubernetes <a id="_idIndexMarker281"/>components inside a big container. To use the container-based <code class="inlineCode">minikube</code>, make sure that you install Docker or Podman by following the instructions for your workstation operating system on which <a id="_idIndexMarker282"/>you are installing <code class="inlineCode">minikube</code>; <code class="inlineCode">minikube</code> will not install Podman or Docker for you. If the provided driver is missing or if the <code class="inlineCode">minikube</code> cannot find the driver on the system, you may get an error, as shown below:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube start --driver=podman
😄 minikube v1.32.0 on Fedora 39 (hyperv/amd64)
✨ Using the podman driver based on user configuration
🤷 Exiting due to PROVIDER_PODMAN_NOT_FOUND: The 'podman' provider was not found: exec: "podman": executable file not found in $PATH
</code></pre>
    <p class="normal">The Docker installation process is easy, but the steps can vary depending on your operating system, and you can take a look at the documentation (<a href="https://docs.docker.com/engine/install/"><span class="url">https://docs.docker.com/engine/install/</span></a>) for more information. Similarly, the Podman installation steps are available at <a href="https://podman.io/docs/installation"><span class="url">https://podman.io/docs/installation</span></a> for different operating system flavors.</p>
    <div class="note">
      <p class="normal">If you are using a Windows workstation and Hyper-V-based VM for your hands-on lab, remember to disable Dynamic Memory for the VM in which you are installing <code class="inlineCode">minikube</code> and the container engine.</p>
      <p class="normal">When running with the Podman driver, <code class="inlineCode">minikube</code> performs a check of the available memory when it starts, and will report the “in-use” memory (set dynamically). So, you need to ensure enough memory is available or configure memory requirements for the Kubernetes node.</p>
    </div>
    <p class="normal">In the following example, we<a id="_idIndexMarker283"/> are using Fedora 39 as our<a id="_idIndexMarker284"/> workstation and Docker as the container engine:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube start --driver=docker --kubernetes-version=1.29.0
😄  minikube v1.32.0 on Fedora 39
❗  Specified Kubernetes version 1.29.0 is newer than the newest supported version: v1.28.3. Use `minikube config defaults kubernetes-version` for details.
❗  Specified Kubernetes version 1.29.0 not found in Kubernetes version list
🤔  Searching the internet for Kubernetes version...
✅  Kubernetes version 1.29.0 found in GitHub version list
✨  Using the docker driver based on user configuration
📌  Using Docker driver with root privileges
👍  Starting control plane node minikube in cluster minikube
🚜  Pulling base image ...
🔥  Creating docker container (CPUs=2, Memory=8000MB) ...
🐳  Preparing Kubernetes v1.29.0 on Docker 24.0.7 ...
    ▪ Generating certificates and keys ...
    ▪ Booting up control plane ...
    ▪ Configuring RBAC rules ...
🔗  Configuring bridge CNI (Container Networking Interface) ...
    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
🔎  Verifying Kubernetes components...
🌟  Enabled addons: storage-provisioner, default-storageclass
🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
</code></pre>
    <p class="normal">We can also use Podman as the container engine and create a Kubernetes cluster using <code class="inlineCode">minikube</code> with the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube start --driver=podman
</code></pre>
    <p class="normal">Now we have the Kubernetes cluster created using <code class="inlineCode">minikube</code>, in the next section let us learn how to access and manage the cluster using <code class="inlineCode">kubectl</code>.</p>
    <h2 class="heading-2" id="_idParaDest-119">Accessing the Kubernetes cluster created by minikube</h2>
    <p class="normal">Now, we need to create a <code class="inlineCode">kubeconfig</code> file for our local <code class="inlineCode">kubectl</code> CLI to be able to communicate with this new Kubernetes installation. The good news is that <code class="inlineCode">minikube</code> also generated one on the fly for us when <a id="_idIndexMarker285"/>we launched the <code class="inlineCode">minikube start</code> command. The <code class="inlineCode">kubeconfig</code> file generated by <code class="inlineCode">minikube</code> is pointing to the local <code class="inlineCode">kube-apiserver</code> endpoint, and your local <code class="inlineCode">kubectl</code> was configured to call this cluster by default. So, essentially, there is nothing to do: the <code class="inlineCode">kubeconfig</code> file is already formatted and in the proper location.</p>
    <p class="normal">By default, this configuration is in <code class="inlineCode">~/.kube/config</code>, and you should be able to see that a <code class="inlineCode">minikube</code> context is now present:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cat</span> ~/.kube/config
...&lt;removed for brevity&gt;..
- context:
    cluster: minikube
    extensions:
    - extension:
        last-update: Mon, 03 Jun 2024 13:06:44 +08
        provider: minikube.sigs.k8s.io
        version: v1.33.1
      name: context_info
    namespace: default
    user: minikube
  name: minikube
...&lt;removed for brevity&gt;..
</code></pre>
    <p class="normal">Use the following command to display the current <code class="inlineCode">kubeconfig</code> file. You should observe a cluster, named <code class="inlineCode">minikube</code>, that points to a local IP address:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config view
</code></pre>
    <p class="normal">Following this, run the following command, which will show the Kubernetes cluster that your <code class="inlineCode">kubectl</code> is pointing to right now:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config current-context
minikube
</code></pre>
    <p class="normal">Now, let’s try to issue a real <code class="inlineCode">kubectl</code> command to list the nodes that are part of our <code class="inlineCode">minikube</code> cluster. If everything is okay, this command should reach the <code class="inlineCode">kube-apiserver</code> component launched by <code class="inlineCode">minikube</code>, which will return only one node since <code class="inlineCode">minikube</code> is a single-node solution. Let’s list the nodes with the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get nodes
NAME       STATUS   ROLES           AGE     VERSION
minikube   Ready    control-plane   3m52s   v1.29.0
</code></pre>
    <p class="normal">If you don’t view any errors when running this command, it means that your <code class="inlineCode">minikube</code> cluster is ready to be used and is fully working!</p>
    <p class="normal">This is the very first real <code class="inlineCode">kubectl</code> command you ran as part of this book. Here, a real <code class="inlineCode">kube-apiserver</code> component<a id="_idIndexMarker286"/> received your API call and answered back with an HTTP response containing data coming from a real <code class="inlineCode">etcd</code> data store. In our scenario, this is the list of the nodes in our cluster.</p>
    <div class="note">
      <p class="normal">Since <code class="inlineCode">minikube</code> creates a single-node Kubernetes cluster by default, this command only outputs one node. This node is both a control plane node and a compute node at the same time. It’s good for local testing, but do not deploy such a setup in production.</p>
    </div>
    <p class="normal">What we can do now is list the status of the control plane components so that you can start familiarizing yourself with <code class="inlineCode">kubectl</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get componentstatuses
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE   ERROR
controller-manager   Healthy   ok       
scheduler            Healthy   ok       
etcd-0               Healthy   ok
</code></pre>
    <p class="normal">This command should output the status of the control plane components. You should see the following:</p>
    <ul>
      <li class="bulletList">A running <code class="inlineCode">etcd</code> datastore</li>
      <li class="bulletList">A running <code class="inlineCode">kube-scheduler</code> component</li>
      <li class="bulletList">A running <code class="inlineCode">kube-controller-manager</code> component</li>
    </ul>
    <p class="normal">In the next section, we will learn how to housekeep your Kubernetes learning environment by stopping and deleting the <code class="inlineCode">minikube</code> Kubernetes cluster.</p>
    <h2 class="heading-2" id="_idParaDest-120">Stopping and deleting the local minikube cluster</h2>
    <p class="normal">You might want to stop or delete your local <code class="inlineCode">minikube</code> installation. To proceed, do not kill the VM or container directly, but <a id="_idIndexMarker287"/>rather, use the <code class="inlineCode">minikube</code> command-line utility. Here<a id="_idIndexMarker288"/> are the two commands to do so:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube stop
✋  Stopping node "minikube"  ...
🛑  1 node stopped.
</code></pre>
    <p class="normal">The preceding command will stop the cluster. However, it will continue to exist; its state will be kept, and you will be able to resume it later using the following <code class="inlineCode">minikube start</code> command again. You can check it by calling the <code class="inlineCode">minikube status</code> command again:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube status
minikube
type: Control Plane
host: Stopped
kubelet: Stopped
apiserver: Stopped
kubeconfig: Stopped
</code></pre>
    <p class="normal">It is also possible to pause the cluster instead of stopping so that you can quickly re-start the Kubernetes cluster:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span> minikube pause
⏸️  Pausing node minikube ...
⏯️  Paused 14 containers in: kube-system, kubernetes-dashboard, storage-gluster, istio-operator
<span class="hljs-con-meta">$ </span> minikube status
minikube
type: Control Plane
host: Running
kubelet: Stopped
apiserver: Paused
kubeconfig: Configured
</code></pre>
    <p class="normal">And later, you can resume the cluster as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube unpause
</code></pre>
    <p class="normal">If you want to destroy the cluster, use the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube delete
🔥  Deleting "minikube" in docker ...
🔥  Deleting container "minikube" ...
🔥  Removing /home/gmadappa/.minikube/machines/minikube ...
💀  Removed all traces of the "minikube" cluster.
</code></pre>
    <p class="normal">If you use this command, the cluster will be <a id="_idIndexMarker289"/>completely destroyed. Its state will be lost and impossible to recover.</p>
    <p class="normal">Now that your <code class="inlineCode">minikube</code> cluster is <a id="_idIndexMarker290"/>operational, it’s up to you to decide whether you want to use it to follow the next chapters or pick another solution.</p>
    <h2 class="heading-2" id="_idParaDest-121">Multi-node Kubernetes cluster using minikube</h2>
    <p class="normal">It is also possible to create multi-node kubernetes clusters using <code class="inlineCode">minikube</code>. In the following demonstration, we are <a id="_idIndexMarker291"/>creating a three-node Kubernetes <a id="_idIndexMarker292"/>cluster using <code class="inlineCode">minikube</code>:</p>
    <div class="note">
      <p class="normal">You need to ensure that, your workstation has enough resources to create multiple Kubernetes nodes (either VMs or containers) when you create multi-node clusters. Also, note that <code class="inlineCode">minikube</code> will spin up nodes with the same vCPU and memory you mentioned in settings or arguments.</p>
    </div>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube start --driver=podman --nodes=3
</code></pre>
    <p class="normal">Once the cluster is provisioned, check the node details and find all the nodes as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
minikube       Ready    control-plane   93s   v1.29.0
minikube-m02   Ready    &lt;none&gt;          74s   v1.29.0
minikube-m03   Ready    &lt;none&gt;          54s   v1.29.0
</code></pre>
    <p class="normal"><code class="inlineCode">minikube</code> created a three-node cluster (<code class="inlineCode">--nodes=3</code>) with the first node as the control plane node (or master node) and the remaining two nodes as compute nodes (you will need to assign appropriate labels later; we will learn about this in later chapters).</p>
    <h2 class="heading-2" id="_idParaDest-122">Multi-master Kubernetes cluster using minikube</h2>
    <p class="normal">There might be situations where you want to deploy and test Kubernetes clusters with a high availability control plane with <a id="_idIndexMarker293"/>multiple control plane nodes. You can implement the same using <code class="inlineCode">minikube</code> using the following<a id="_idIndexMarker294"/> command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube start \
  --driver=virtualbox \
  --nodes 5 \
  --ha <span class="hljs-con-literal">true</span> \
  --cni calico \
  --cpus=2 \
  --memory=2g \
  --kubernetes-version=v1.30.0 \
  --container-runtime=containerd
<span class="hljs-con-meta">$ </span>kubectl get nodes
NAME           STATUS   ROLES           AGE     VERSION
minikube       Ready    control-plane   6m28s   v1.30.0
minikube-m02   Ready    control-plane   4m36s   v1.30.0
minikube-m03   Ready    control-plane   2m45s   v1.30.0
minikube-m04   Ready    &lt;none&gt;          112s    v1.30.0
minikube-m05   Ready    &lt;none&gt;          62s     v1.30.0
</code></pre>
    <p class="normal"><code class="inlineCode">minikube</code> will create a five-node cluster (<code class="inlineCode">--nodes 5</code>) and configure the first three nodes as control plane nodes (<code class="inlineCode">--ha true</code>).</p>
    <p class="normal">Again, remember to ensure you have enough resources on your workstation to create such multi-node clusters.</p>
    <h2 class="heading-2" id="_idParaDest-123">Multiple Kubernetes clusters using minikube</h2>
    <p class="normal">As we learned, <code class="inlineCode">minikube</code> is meant for the development and testing of Kubernetes environments. There might be situations <a id="_idIndexMarker295"/>where you want to simulate the environment with multiple Kubernetes clusters. In that case, you can use <code class="inlineCode">minikube</code> again as it is possible to create multiple Kubernetes clusters using <code class="inlineCode">minikube</code>. But remember to give different names (<code class="inlineCode">--profile</code>) for your different Kubernetes clusters, as explained below:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta"># </span>Start a minikube cluster using VirtualBox as driver.
<span class="hljs-con-meta">$ </span> minikube start --driver=virtualbox --kubernetes-version=1.30.0 --profile cluster-vbox
<span class="hljs-con-meta"># </span>Start another minikube cluster using Docker as driver
<span class="hljs-con-meta">$ </span>minikube start --driver=docker --kubernetes-version=1.30.0 --profile cluster-docker
</code></pre>
    <p class="normal">You can list the <code class="inlineCode">minikube</code> clusters and find the details, as shown in the below image:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_03_02.png"/></figure>
    <p class="packt_figref">Figure 3.2: minikube profile list showing multiple Kubernetes clusters</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta"># </span>Stop cluster with profile name
<span class="hljs-con-meta">$ </span>minikube stop --profile cluster-docker
<span class="hljs-con-meta"># </span>Remove the cluster with profile name
<span class="hljs-con-meta">$ </span>minikube delete --profile cluster-docker
</code></pre>
    <p class="normal">We have learned<a id="_idIndexMarker296"/> how to create different types and sizes of Kubernetes clusters; now let’s examine another tool for setting up a local Kubernetes cluster, called <code class="inlineCode">kind</code>.</p>
    <h1 class="heading-1" id="_idParaDest-124">Multi-node Kubernetes cluster with kind</h1>
    <p class="normal">In this section, we are going to <a id="_idIndexMarker297"/>discuss a tool <a id="_idIndexMarker298"/>called <code class="inlineCode">kind</code>, which is also designed to run a Kubernetes cluster locally, just like <code class="inlineCode">minikube</code>.</p>
    <p class="normal">The whole idea behind <code class="inlineCode">kind</code> is to use<a id="_idIndexMarker299"/> Docker or Podman containers as<a id="_idIndexMarker300"/> Kubernetes nodes thanks to the <strong class="keyWord">Docker-in-Docker</strong> (<strong class="keyWord">DinD</strong>) or <strong class="keyWord">Containers-in-Container</strong> model. By launching containers, which themselves contain <a id="_idIndexMarker301"/>the container engines and the kubelet, it is possible to make them behave as Kubernetes worker <a id="_idIndexMarker302"/>nodes.</p>
    <p class="normal">The following diagram shows the high-level architecture of <code class="inlineCode">kind</code> cluster components:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_03_03.png"/></figure>
    <p class="packt_figref">Figure 3.3: kind cluster components (image source: https://kind.sigs.k8s.io/docs/design/initial)</p>
    <p class="normal">This is exactly the same<a id="_idIndexMarker303"/> as when you use the Docker driver for <code class="inlineCode">minikube</code>, except that there, it will not be done in a single container but in<a id="_idIndexMarker304"/> several. The result is a local multi-node cluster. Similar to <code class="inlineCode">minikube</code>, <code class="inlineCode">kind</code> is a free open-source tool.</p>
    <div class="packt_tip">
      <p class="normal">Similar to <code class="inlineCode">minikube</code>, <code class="inlineCode">kind</code> is a tool that is used for local development and testing. Please never use it in production because it is not designed for it.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-125">Installing kind onto your local system</h2>
    <p class="normal">Since <code class="inlineCode">kind</code> is a tool entirely <a id="_idIndexMarker305"/>built around Docker and Podman, you need to have either of these container engines installed <a id="_idIndexMarker306"/>and working on your local system.</p>
    <p class="normal">Since the Docker and Podman installation instructions are available as documentation, we will skip those steps here (refer to the earlier section, <em class="italic">Setting up minikube using a container</em>, for the details).</p>
    <div class="note">
      <p class="normal">Refer to the <code class="inlineCode">kind</code> release page for the <code class="inlineCode">kind</code> version information and availability (<a href="https://github.com/kubernetes-sigs/kind/releases"><span class="url">https://github.com/kubernetes-sigs/kind/releases</span></a>).</p>
    </div>
    <p class="normal">Again, the process of installing <code class="inlineCode">kind</code> will depend on your operating system:</p>
    <ul>
      <li class="bulletList">Use the following commands for Linux:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span> curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.23.0/kind-linux-amd64
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">chmod</span> +x ./kind
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">mv</span> ./kind /usr/local/bin/kind
<span class="hljs-con-meta"># </span>Check version
<span class="hljs-con-meta">$ </span>kind version
kind v0.23.0 go1.21.10 linux/amd64
</code></pre>
      </li>
      <li class="bulletList">Use the following commands for macOS:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span> curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-darwin-amd64
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">chmod</span> +x ./kind
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">mv</span> ./kind /usr/local/bin/kind
</code></pre>
      </li>
      <li class="bulletList">You can also install it with Homebrew:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>brew install kind
</code></pre>
      </li>
      <li class="bulletList">Use the following commands for Windows PowerShell:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.23.0/kind-windows-amd64
<span class="hljs-con-meta">$ </span>Move-Item .\kind-windows-amd64.exe c:\some-dir-in-your-PATH\kind.exe
</code></pre>
      </li>
      <li class="bulletList">You can also install it with Chocolatey:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>choco install kind
</code></pre>
      </li>
      <li class="bulletList">If you have Go language installed, then you can use the following command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>go install sigs.k8s.io/kind@v0.22.0 &amp;&amp; kind create cluster
</code></pre>
      </li>
    </ul>
    <p class="normal">Refer to the documentation (<span class="url">https://kind.sigs.k8s.io/docs/user/quick-start#installation</span>) to learn<a id="_idIndexMarker307"/> other installation methods for your system.</p>
    <p class="normal">Let us learn how to create a <a id="_idIndexMarker308"/>Kubernetes cluster using <code class="inlineCode">kind</code> in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-126">Creating a Kubernetes cluster with kind</h2>
    <p class="normal">Once <code class="inlineCode">kind</code> has been installed on<a id="_idIndexMarker309"/> your system, you can immediately proceed to launch a new Kubernetes cluster using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kind create cluster --name test-kind
Creating cluster "kind" ...
</code></pre>
    <p class="normal">When you run this command, <code class="inlineCode">kind</code> will<a id="_idIndexMarker310"/> start to build a Kubernetes cluster locally by pulling a container image containing all the control plane components. The result will be a single-node Kubernetes cluster with a Docker container acting as a <em class="italic">control plane node</em>.</p>
    <p class="normal">Podman can be used as the provider for the <code class="inlineCode">kind</code> cluster if you prefer, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>KIND_EXPERIMENTAL_PROVIDER=podman kind create cluster
</code></pre>
    <p class="normal">We do not want this setup since we can already achieve it with <code class="inlineCode">minikube</code>. What we want is a multi-node cluster with <code class="inlineCode">kind</code> where we can customize the cluster and nodes. To do this, we must write a very small configuration file and tell <code class="inlineCode">kind</code> to use it as a template to build the local Kubernetes cluster. So, let’s get rid of the single-node <code class="inlineCode">kind</code> cluster that we just built, and let’s rebuild it as a multi-node cluster:</p>
    <ol>
      <li class="numberedList" value="1">Run this command to delete the cluster:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kind delete cluster
Deleting cluster "kind" ...
</code></pre>
      </li>
      <li class="numberedList">Then, we need to create a <code class="inlineCode">config</code> file that will serve as a template for <code class="inlineCode">kind</code> to build our cluster. Simply copy the following content to a local file in this directory, for example, <code class="inlineCode">~/.kube/kind_cluster</code>:
        <pre class="programlisting con-one"><code class="hljs-con"> kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
- role: worker
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Please note that this file is in YAML format. Pay attention to the <code class="inlineCode">nodes</code> array, which is the most important part of the file. This is where you tell <code class="inlineCode">kind</code> how many nodes you want in<a id="_idIndexMarker311"/> your cluster. The role key can take two values: control plane and worker.</p>
    <p class="normal-one">Depending on which role you <a id="_idIndexMarker312"/>choose, a different node will be created.</p>
    <ol>
      <li class="numberedList" value="3">Let’s relaunch the <code class="inlineCode">kind create</code> command with this <code class="inlineCode">config</code> file to build our multi-node cluster. For the given file, the result will be a one-master, three-worker Kubernetes cluster:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kind create cluster --config ~/.kube/kind_cluster
</code></pre>
      </li>
    </ol>
    <p class="normal-one">It is also possible to build a specific version of Kubernetes by using the appropriate image details while creating the <code class="inlineCode">kind</code> cluster, as follows:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kind create cluster \
  --name my-kind-cluster \
  --config ~/.kube/kind_cluster \
  --image kindest/node:v1.29.0@sha256:eaa1450915475849a73a9227b8f201df25e55e268e5d619312131292e324d570
</code></pre>
    <p class="normal">A new Kubernetes cluster will be deployed and configured by <code class="inlineCode">kind</code> and you will receive the messages related to cluster access at the end, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">Creating cluster "my-kind-cluster" ...
✓ Ensuring node image (kindest/node:v1.29.0) 🖼
✓ Preparing nodes 📦 📦 📦 
✓ Writing configuration 📜
✓ Starting control-plane 🕹️
✓ Installing CNI 🔌
✓ Installing StorageClass 💾
✓ Joining worker nodes 🚜
Set kubectl context to "kind-my-kind-cluster"
You can now use your cluster with:
kubectl cluster-info --context kind-my-kind-cluster
Thanks for using kind! 😊
</code></pre>
    <p class="normal">Following this, you should have four new Docker containers: one running as a master node and the other three as worker nodes of the same Kubernetes cluster.</p>
    <p class="normal">Now, as always with Kubernetes, we<a id="_idIndexMarker313"/> need to write a <code class="inlineCode">kubeconfig</code> file for our <code class="inlineCode">Kubectl</code> utility to be able to interact with the new cluster. And guess what, <code class="inlineCode">kind</code> has already generated the proper configuration and appended it to our <code class="inlineCode">~/.kube/config</code> file, too. Additionally, <code class="inlineCode">kind</code> set the<a id="_idIndexMarker314"/> current context to our new cluster, so there is essentially nothing left to do. We can immediately start querying our new cluster. Let’s list the nodes using the <code class="inlineCode">kubectl get nodes</code> command. If everything is okay, we should view four nodes:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get nodes
NAME                            STATUS   ROLES           AGE     VERSION
my-kind-cluster-control-plane   Ready    control-plane   4m      v1.29.0
my-kind-cluster-worker          Ready    &lt;none&gt;          3m43s   v1.29.0
my-kind-cluster-worker2         Ready    &lt;none&gt;          3m42s   v1.29.0
</code></pre>
    <p class="normal">Everything seems to be perfect. Your <code class="inlineCode">kind</code> cluster is working!</p>
    <p class="normal">Just as we did with <code class="inlineCode">minikube</code>, you can also check for the component’s statuses using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl cluster-info
Kubernetes control plane is running at https://127.0.0.1:42547
CoreDNS is running at https://127.0.0.1:42547/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
</code></pre>
    <p class="normal">To further debug and diagnose cluster problems, use <code class="inlineCode">'kubectl cluster-info dump</code>':</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span> kubectl get --raw=<span class="hljs-con-string">'/readyz?verbose'</span>
[+]ping ok
[+]log ok
[+]etcd ok
...&lt;removed for brevity&gt;...
[+]poststarthook/apiservice-openapiv3-controller ok
[+]shutdown ok
readyz check passed
</code></pre>
    <p class="normal">As part of the development<a id="_idIndexMarker315"/> and learning environment housekeeping, we <a id="_idIndexMarker316"/>need to learn how to stop and delete Kubernetes clusters created using <code class="inlineCode">kind</code>. Let us learn how to do this in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-127">Stopping and deleting the local kind cluster</h2>
    <p class="normal">You might want to stop or remove<a id="_idIndexMarker317"/> everything <code class="inlineCode">kind</code> created on your local system to clean the place after your practice. To do so, you can use the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kind stop
</code></pre>
    <p class="normal">This command will stop the Docker containers that <code class="inlineCode">kind</code> is managing. You will achieve the same result if you run the Docker <code class="inlineCode">stop</code> command on your containers manually. Doing this will stop the containers <a id="_idIndexMarker318"/>but will keep the state of the cluster. That means your cluster won’t be destroyed, and simply relaunching it using the following command will get the cluster back to its state before you stop it.</p>
    <p class="normal">If you want to completely remove the cluster from your system, use the following command. Running this command will result in removing the cluster and its state from your system. You won’t be able to recover the cluster:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kind delete cluster
Deleting cluster "kind" ...
</code></pre>
    <p class="normal">Now that your <code class="inlineCode">kind</code> cluster is operational, it’s up to you to decide whether you want to use it to practice while reading the coming chapters. You can also decide whether to pick another solution described in the following sections of this chapter. <code class="inlineCode">kind</code> is particularly nice because it’s free to use and allows you to install a multi-node cluster. However, it’s not designed for production<a id="_idIndexMarker319"/> and remains a development and testing solution for a non-production<a id="_idIndexMarker320"/> environment. <code class="inlineCode">kind</code> makes use of Docker containers to create <em class="italic">Kubernetes nodes</em>, which, in the real world, are supposed to be Linux machines.</p>
    <p class="normal">Let us learn about some of the alternative Kubernetes learning and testing environments in the next section.</p>
    <h1 class="heading-1" id="_idParaDest-128">Alternative Kubernetes learning environments</h1>
    <p class="normal">You can also utilize some of the <a id="_idIndexMarker321"/>available zero-configuration learning environments, designed to make your Kubernetes journey smooth and enjoyable.</p>
    <h2 class="heading-2" id="_idParaDest-129">Play with Kubernetes</h2>
    <p class="normal">This interactive playground (<code class="inlineCode">labs.play-with-k8s.com</code>), brought to<a id="_idIndexMarker322"/> you by <strong class="keyWord">Docker</strong> and <strong class="keyWord">Tutorius</strong>, provides a simple and fun way to<a id="_idIndexMarker323"/> experiment with Kubernetes. Within seconds, you’ll be running your own Kubernetes cluster directly in your web browser.</p>
    <p class="normal">The environment comes with <a id="_idIndexMarker324"/>the following features:</p>
    <ul>
      <li class="bulletList">Free Alpine Linux VM: Experience a realistic VM environment without leaving your browser.</li>
      <li class="bulletList">DinD: This technology creates the illusion of multiple VMs, allowing you to explore distributed systems concepts.</li>
    </ul>
    <h2 class="heading-2" id="_idParaDest-130">Killercoda Kubernetes playground</h2>
    <p class="normal"><strong class="keyWord">Killercoda</strong> (<a href="https://killercoda.com/playgrounds/scenario/kubernetes"><span class="url">https://killercoda.com/playgrounds/scenario/kubernetes</span></a>) is a zero-configuration playground that <a id="_idIndexMarker325"/>offers a temporary Kubernetes environment accessible through your web browser. Stay on top <a id="_idIndexMarker326"/>of the latest trends with their commitment to providing the newest kubeadm Kubernetes version just a few weeks after release.</p>
    <p class="normal">The environment comes<a id="_idIndexMarker327"/> with the following features:</p>
    <ul>
      <li class="bulletList">Ephemeral environment: Get started quickly with a preconfigured cluster that vanishes once you’re done. This makes it perfect for quick experimentation without commitment.</li>
      <li class="bulletList">Empty kubeadm cluster with two nodes: Dive into the core functionalities of Kubernetes with a readily available two-node cluster.</li>
      <li class="bulletList">Control plane node with scheduling ability: Unlike some playgrounds, this one lets you schedule <a id="_idIndexMarker328"/>workloads on the control plane node, providing more flexibility for testing purposes.</li>
    </ul>
    <p class="normal">We will explore some of the production-grade Kubernetes options in the next section.</p>
    <h1 class="heading-1" id="_idParaDest-131">Production-grade Kubernetes clusters</h1>
    <p class="normal">We have been talking about the Kubernetes environments for development and learning purposes so far. How do you build <a id="_idIndexMarker329"/>a production-grade Kubernetes environment that meets your specific needs? Next, we’ll see some of the well-known options adopted by Kubernetes users.</p>
    <p class="normal">In the following section, let us understand the managed Kubernetes services offered by the major <strong class="keyWord">Cloud Service Providers</strong> (<strong class="keyWord">CSPs</strong>).</p>
    <h2 class="heading-2" id="_idParaDest-132">Managed Kubernetes clusters using cloud services</h2>
    <p class="normal">If you prefer to have your Kubernetes <a id="_idIndexMarker330"/>environment using managed services, then there are several options available, such as GKE, AKS, EKS, and so on:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Google Kubernetes Engine</strong> (<strong class="keyWord">GKE</strong>): Offered <a id="_idIndexMarker331"/>by <strong class="keyWord">Google Cloud Platform</strong> (<strong class="keyWord">GCP</strong>), GKE is a fully managed Kubernetes service. It takes care of the entire cluster lifecycle, from provisioning and configuration to scaling and maintenance. GKE integrates seamlessly with other GCP services, making it a great choice for existing GCP users.</li>
      <li class="bulletList"><strong class="keyWord">Azure Kubernetes Service</strong> (<strong class="keyWord">AKS</strong>): Part of Microsoft Azure, AKS is another managed Kubernetes offering. Similar<a id="_idIndexMarker332"/> to GKE, AKS handles all aspects of cluster management, allowing you to focus on deploying and managing your containerized applications. AKS integrates well with other Azure services, making it a natural fit for Azure users.</li>
      <li class="bulletList"><strong class="keyWord">Amazon Elastic Kubernetes Service</strong> (<strong class="keyWord">EKS</strong>): Offered by <strong class="keyWord">Amazon Web Services</strong> (<strong class="keyWord">AWS</strong>), EKS provides a managed<a id="_idIndexMarker333"/> Kubernetes service within the AWS ecosystem. Like GKE and AKS, EKS takes care of cluster management, freeing you to focus on your applications. EKS integrates with other AWS services, making it a strong option for AWS users.</li>
    </ul>
    <p class="normal">These managed Kubernetes <a id="_idIndexMarker334"/>services provide a convenient and scalable way to deploy and manage your containerized applications without the complexities of self-managed Kubernetes clusters.</p>
    <p class="normal">We have detailed chapters to learn how to deploy and manage such clusters as follows:</p>
    <ul>
      <li class="bulletList"><em class="chapterRef">Chapter 15</em>, <em class="italic">Kubernetes Clusters on Google Kubernetes Engine</em></li>
      <li class="bulletList"><em class="chapterRef">Chapter 16</em>, <em class="italic">Launching a Kubernetes Cluster on Amazon Web Services with Amazon Elastic Kubernetes Service</em></li>
      <li class="bulletList"><em class="chapterRef">Chapter 17</em>, <em class="italic">Kubernetes Clusters on Microsoft Azure with Azure Kubernetes Service</em></li>
    </ul>
    <p class="normal">If you do not have a local Kubernetes setup, as we explained in the previous sections of this chapter, you can create one using the managed Kubernetes service on your choice of cloud platform by referring to the respective chapter. But it is a requirement to have a working Kubernetes cluster before you start reading the next part of this book, <em class="chapterRef">Part 2</em>: <em class="italic">Diving into Kubernetes Core Concepts</em>.</p>
    <p class="normal">We will learn about the Kubernetes distributions and platforms in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-133">Kubernetes distributions</h2>
    <p class="normal">Kubernetes distributions are essentially<a id="_idIndexMarker335"/> pre-packaged versions of Kubernetes that include additional features and functionalities beyond the core Kubernetes offering. They act like value-added packages, catering to specific needs and simplifying deployments for users. For a more feature-rich experience, consider these Kubernetes distributions:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Red Hat OpenShift</strong>: This enterprise-grade distribution extends Kubernetes with developer tools (image builds<a id="_idIndexMarker336"/> and CI/CD pipelines), multi-cluster management, security features (RBAC and SCC), and built-in scaling for complex deployments (<a href="https://www.redhat.com/en/technologies/cloud-computing/openshift"><span class="url">https://www.redhat.com/en/technologies/cloud-computing/openshift</span></a>).</li>
      <li class="bulletList"><strong class="keyWord">Rancher</strong>: A complete container management platform, Rancher goes beyond Kubernetes. It offers<a id="_idIndexMarker337"/> multi-cluster management across diverse <a id="_idIndexMarker338"/>environments, workload management for various orchestration platforms, and a marketplace for preconfigured applications (<a href="https://www.rancher.com/"><span class="url">https://www.rancher.com/</span></a>).</li>
      <li class="bulletList"><strong class="keyWord">VMware Tanzu</strong>: Designed for the VMware ecosystem, Tanzu integrates seamlessly for infrastructure <a id="_idIndexMarker339"/>provisioning, security, and hybrid cloud deployments. It provides lifecycle management tools for containerized applications within the VMware environment (<a href="https://tanzu.vmware.com/platform"><span class="url">https://tanzu.vmware.com/platform</span></a>).</li>
    </ul>
    <p class="normal">Please note, some of the above-listed Kubernetes distributions are subscription-based or license-based products and are not freely available.</p>
    <p class="normal">Let us learn about some of the Kubernetes deployment tools in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-134">Kubernetes installation tools</h2>
    <p class="normal">The following tools provide flexibility and control over the Kubernetes cluster setup. Of course, you need to add more automation using other third-party tools and platforms to manage your Kubernetes environment:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">kubeadm</strong>: This official Kubernetes tool provides a user-friendly way to set up Kubernetes clusters, making it <a id="_idIndexMarker340"/>suitable for both testing and production environments. Its simplicity allows for quick cluster deployment but may require additional configuration for production-grade<a id="_idIndexMarker341"/> features like high availability (<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/"><span class="url">https://kubernetes.io/docs/reference/setup-tools/kubeadm/</span></a>).</li>
      <li class="bulletList"><strong class="keyWord">kops</strong>: For managing robust<a id="_idIndexMarker342"/> Kubernetes clusters in production, kops is an official Kubernetes project offering command-line control. It streamlines the creation, upgrading, and maintenance of highly <a id="_idIndexMarker343"/>available clusters, ensuring the reliable operation of your containerized <a id="_idIndexMarker344"/>applications (<a href="https://kops.sigs.k8s.io/"><span class="url">https://kops.sigs.k8s.io/</span></a>).</li>
      <li class="bulletList"><strong class="keyWord">Kubespray</strong>: Looking to deploy Kubernetes on bare metal or VMs? Kubespray leverages the power of Ansible <a id="_idIndexMarker345"/>automation. It combines Ansible playbooks with Kubernetes resources, allowing for <a id="_idIndexMarker346"/>automated cluster deployment on your preferred infrastructure (<a href="https://github.com/kubespray"><span class="url">https://github.com/kubespray</span></a>).</li>
      <li class="bulletList"><strong class="keyWord">Terraform</strong>: This tool allows you to define and manage your Kubernetes cluster infrastructure across various cloud providers. The <a id="_idIndexMarker347"/>code-driven approach ensures consistency and repeatability when deploying clusters in different environments (<a href="https://www.terraform.io/"><span class="url">https://www.terraform.io/</span></a>).</li>
      <li class="bulletList"><strong class="keyWord">Pulumi</strong>: Similar to Terraform, Pulumi provides infrastructure-as-code capabilities. It allows you to define and manage your Kubernetes cluster infrastructure using programming<a id="_idIndexMarker348"/> languages like Python or Go. This <a id="_idIndexMarker349"/>approach offers greater flexibility and customization compared to purely declarative configuration languages (<a href="https://www.pulumi.com/"><span class="url">https://www.pulumi.com/</span></a>).</li>
    </ul>
    <p class="normal">If the Kubernetes landscape is very large with several Kubernetes clusters, then you need to consider hybrid-multi-cluster management solutions; let us learn about those in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-135">Hybrid and multi-cloud solutions</h2>
    <p class="normal">Managing Kubernetes clusters across diverse environments requires powerful tools, and there are a few offering such multi-cluster management features:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Anthos</strong> (Google): This hybrid and multi-cloud platform facilitates managing Kubernetes clusters across diverse<a id="_idIndexMarker350"/> environments. Anthos allows organizations to leverage a consistent approach for deploying and managing containerized applications on-premises, in the cloud, or <a id="_idIndexMarker351"/>at the edge (<a href="https://cloud.google.com/anthos"><span class="url">https://cloud.google.com/anthos</span></a>).</li>
      <li class="bulletList"><strong class="keyWord">Red Hat Advanced Cluster Management (RHACM) for Kubernetes</strong>: Red Hat also offers a solution for managing Kubernetes clusters across hybrid and multi-cloud environments. Their <a id="_idIndexMarker352"/>Advanced Cluster Management platform provides a centralized control plane for consistent deployment, management, and governance of your containerized workloads (<a href="https://www.redhat.com/en/technologies/management/advanced-cluster-management"><span class="url">https://www.redhat.com/en/technologies/management/advanced-cluster-management</span></a>).</li>
      <li class="bulletList"><strong class="keyWord">VMware Tanzu Mission Control</strong>: This centralized management tool simplifies the process of overseeing <a id="_idIndexMarker353"/>Kubernetes clusters across various environments. From a single console, you can provision, monitor, and manage clusters regardless of their location, be that on-premises, cloud, or hybrid (<a href="https://docs.vmware.com/en/VMware-Tanzu-Mission-Control/index.html"><span class="url">https://docs.vmware.com/en/VMware-Tanzu-Mission-Control/index.html</span></a>).</li>
    </ul>
    <p class="normal">How do you choose your Kubernetes platform and management solutions? Let’s explore some of the key points in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-136">Choosing the right environment</h2>
    <p class="normal">The best production-grade Kubernetes environment depends on several factors:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Level of control</strong>: Do you need <a id="_idIndexMarker354"/>complete control over the cluster configuration, or are you comfortable with preconfigured managed services?</li>
      <li class="bulletList"><strong class="keyWord">Existing infrastructure</strong>: Consider your existing infrastructure (cloud provider, bare metal) when choosing a deployment method.</li>
      <li class="bulletList"><strong class="keyWord">Scalability needs</strong>: How easily do you need to scale your cluster up or down to meet changing demands?</li>
      <li class="bulletList"><strong class="keyWord">Team expertise</strong>: Evaluate your team’s experience with Kubernetes and cloud infrastructure to determine which solution best suits their skills.</li>
    </ul>
    <p class="normal">By carefully considering these factors and exploring the various options available, you can build a production-grade Kubernetes environment that delivers optimal performance and scalability for your containerized applications. </p>
    <p class="normal">In the next section we will discuss some of the cluster maintenance tasks.</p>
    <h1 class="heading-1" id="_idParaDest-137">Running Kubernetes On-Premises: Challenges and Considerations</h1>
    <p class="normal">Running Kubernetes in an on-premises environment provides more control over infrastructure but also demands careful management. Compared to cloud-managed solutions, maintaining an on-premises Kubernetes cluster requires handling all aspects, from provisioning to upgrades, manually. Below, we explore key considerations and challenges that arise when managing Kubernetes on-premises.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Infrastructure Provisioning</strong>: Setting up infrastructure for Kubernetes on-premises means automating the provisioning of nodes. Tools like Rancher’s cloud controllers or Terraform help streamline this process by ensuring consistency. Packer can also be used to create VM images, enabling smoother upgrades by deploying updated images across nodes.</li>
      <li class="bulletList"><strong class="keyWord">Cluster Setup and Maintenance</strong>: Deploying a Kubernetes cluster on-premises involves using tools such as <code class="inlineCode">kubeadm</code>. This process is often more involved than in cloud-managed environments. Cluster maintenance tasks include renewing certificates, managing nodes, and handling high availability setups, which add further complexity. </li>
      <li class="bulletList"><strong class="keyWord">Load Balancing and Access</strong>: Providing external access to applications in on-premises environments can be challenging. Standard Kubernetes options like <strong class="keyWord">NodePort</strong> and <strong class="keyWord">LoadBalancer</strong> services may not be enough. <strong class="keyWord">MetalLB</strong> can offer a load balancing solution for bare-metal setups but comes with limitations, such as not being able to load balance the API server in high availability environments. </li>
      <li class="bulletList"><strong class="keyWord">Persistent Storage</strong>: Persistent storage is critical for running production workloads. Kubernetes relies on <strong class="keyWord">PersistentVolumeClaims</strong> (<strong class="keyWord">PVCs</strong>) and <strong class="keyWord">PersistentVolumes</strong> (<strong class="keyWord">PVs</strong>), which require integration with physical storage systems. Tools like Longhorn allow dynamic provisioning of volumes and replication across nodes, providing flexibility in on-prem setups.</li>
      <li class="bulletList"><strong class="keyWord">Upgrades and Scalability</strong>: Kubernetes releases frequent updates, which means managing upgrades on-premises can be tricky. It’s essential to test new versions before rolling them out to production. Tools like Packer and Terraform can assist in scaling by simplifying node additions and upgrades.</li>
      <li class="bulletList"><strong class="keyWord">Networking</strong>: On-premises Kubernetes networking depends on your data center configuration. Manual management of DNS, load balancers, and network settings is necessary. Monitoring tools such as Prometheus, alongside solutions like MetalLB for load balancing, can help, though they require integration and constant monitoring. </li>
      <li class="bulletList"><strong class="keyWord">Monitoring and Management</strong>: Monitoring on-premises clusters is essential for ensuring the system’s health. Tools like Prometheus and Grafana can be used to monitor resource usage. Additionally, logging and alerting systems should be set up to detect and resolve issues swiftly, helping to minimize downtime.</li>
      <li class="bulletList"><strong class="keyWord">Tooling and Automation</strong>: Automating tasks such as node management and upgrades are vital in on-premises clusters. Enterprise Kubernetes platforms like Rancher or OpenShift help reduce manual intervention, providing a more streamlined and manageable Kubernetes environment.</li>
      <li class="bulletList"><strong class="keyWord">Security and Compliance</strong>: Security is crucial in enterprise Kubernetes setups. Including <strong class="keyWord">FIPS</strong> (<strong class="keyWord">Federal Information Processing Standards</strong>) support from the beginning can help meet compliance needs and maintain a secure environment as the system evolves.</li>
      <li class="bulletList">In summary, managing Kubernetes on-premises provides more flexibility but demands careful attention to infrastructure, networking, and storage setups. With the right tools and strategies, organizations can effectively scale and maintain a robust Kubernetes environment on their own infrastructure.</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-138">Summary</h1>
    <p class="normal">This chapter was quite intense! You require a Kubernetes cluster to follow this book, and so we examined five ways in which to set up Kubernetes clusters on different platforms. You learned about <code class="inlineCode">minikube</code>, which is the most common way to set up a cluster on a local machine. You also discovered <code class="inlineCode">kind</code>, which is a tool that can set up multi-node local clusters, which is a limitation of <code class="inlineCode">minikube</code>. </p>
    <p class="normal">We learned about some of the Kubernetes learning environments and also explored the production-grade Kubernetes environments including three major Kubernetes cloud services, GKE, Amazon EKS, and AKS. These three services allow you to create a Kubernetes cluster on the cloud for you to practice and train with. This was just a quick introduction to these services, and we will have the opportunity to dive deeper into these services later. For the moment, simply pick the solution that is the best for you.</p>
    <p class="normal">In the next chapter, we are going to dive into Kubernetes by exploring the concept of Pods. The Pod resource is the most important resource that Kubernetes manages. We will learn how to create, update, and delete Pods. Additionally, we will look at how to provision them, how to get information from them, and how to update the containers they are running. </p>
    <p class="normal">We will deploy an NGINX Pod on a Kubernetes cluster and examine how we can access it from the outside. By the end of the next chapter, you will be capable of launching your first containers on your Kubernetes cluster through the usage of Pods. The cluster that you installed here will be very useful when you follow the real-world examples that are coming in the next chapter.</p>
    <h1 class="heading-1" id="_idParaDest-139">Further reading</h1>
    <ul>
      <li class="bulletList">Installing <code class="inlineCode">minikube</code>: <a href="https://minikube.sigs.k8s.io/docs/start/"><span class="url">https://minikube.sigs.k8s.io/docs/start/</span></a></li>
      <li class="bulletList"><code class="inlineCode">minikube</code> drivers: <a href="https://minikube.sigs.k8s.io/docs/drivers/ "><span class="url">https://minikube.sigs.k8s.io/docs/drivers/</span></a></li>
      <li class="bulletList">Installing Docker: <a href="https://docs.docker.com/engine/install/ "><span class="url">https://docs.docker.com/engine/install/</span></a></li>
      <li class="bulletList">Installing Podman: <a href="https://podman.io/docs/installation"><span class="url">https://podman.io/docs/installation</span></a></li>
      <li class="bulletList">Multi-node Kubernetes using <code class="inlineCode">minikube</code>: <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/ "><span class="url">https://minikube.sigs.k8s.io/docs/tutorials/multi_node/</span></a></li>
      <li class="bulletList">Installing <code class="inlineCode">kind</code>: <a href="https://kind.sigs.k8s.io/docs/user/quick-start#installation "><span class="url">https://kind.sigs.k8s.io/docs/user/quick-start#installation</span></a></li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-140">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img alt="" src="image/QR_Code119001106479081656.png"/></p>
  </div>
</body></html>