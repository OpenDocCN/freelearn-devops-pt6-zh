<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer096">
<h1 class="chapter-number" id="_idParaDest-75"><a id="_idTextAnchor080"/>4</h1>
<h1 id="_idParaDest-76"><a id="_idTextAnchor081"/>Application Scheduling and Lifecycle Management</h1>
<p>This chapter describes how to use Kubernetes deployments to deploy pods, scale pods, perform rolling updates and rollbacks, carry out resource management, and use ConfigMaps to configure pods using <strong class="source-inline">kubectl</strong> commands and YAML definitions. This chapter covers 15% of the CKA exam content.</p>
<p>In this chapter, we’re going to cover the following main topics: </p>
<ul>
<li>The basics of Kubernetes workloads</li>
<li>Deploying and managing applications </li>
<li>Scaling applications</li>
<li>Performing rolling updates and rollbacks</li>
<li>Resource management </li>
<li>Workload scheduling </li>
<li>Configuring applications </li>
</ul>
<h1 id="_idParaDest-77"><a id="_idTextAnchor082"/>Technical requirements </h1>
<p>To get started, we need to make sure your local machine meets the following technical requirements: </p>
<ul>
<li>A compatible Linux host – we recommend a Debian-based Linux distribution such as Ubuntu 18.04 or later</li>
<li>Make sure your host machine has at least 2 GB RAM, 2 CPU cores, and about 20 GB of free disk space</li>
</ul>
<h1 id="_idParaDest-78"><a id="_idTextAnchor083"/>The basics of Kubernetes workloads</h1>
<p>Kubernetes orchestrates your <a id="_idIndexMarker223"/>workloads to achieve the desired status – a containerized workload with applications running on Kubernetes, including stateless, stateful, and data-processing applications. In terms of cloud-native applications, there’s an interesting white paper that introduced the notion of cloud-native applications and design patterns thoroughly, which you can check out here if you’re interested: <a href="https://www.redhat.com/en/resources/cloud-native-container-design-whitepaper">https://www.redhat.com/en/resources/cloud-native-container-design-whitepaper</a>.</p>
<p>The fundamental building blocks of any containerized workload up and running in the Kubernetes cluster are called Kubernetes API primitives or Kubernetes objects. They are the API resource types defined in Kubernetes, including pods, ReplicaSets, DaemonSets, StatefulSets, Job and CronJob objects, and Deployments, among others mentioned in <a href="B18201_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Kubernetes Overview</em>. </p>
<p>The CKA exam covers some of the main Kubernetes objects such as Pods, Deployments, ReplicaSets, and DaemonSets while working with Kubernetes clusters and we’ll dive into further detail in the following section of this chapter. </p>
<p>Please make sure your local machine meets the required technical requirements before diving into the practice. <a id="_idTextAnchor084"/></p>
<h2 id="_idParaDest-79"><a id="_idTextAnchor085"/>Imperative management versus declarative management</h2>
<p>There are a few ways to<a id="_idIndexMarker224"/> communicate with API<a id="_idIndexMarker225"/> servers in Kubernetes – mainly, they can be categorized as either imperative management or declarative management. You will need to use both <strong class="source-inline">kubectl</strong> and YAML definitions to manage Kubernetes objects. The <strong class="source-inline">kubectl</strong> utilities can support all the management techniques for managing Kubernetes objects, as Kubernetes is intended to be a desired state manager. After executing a <strong class="source-inline">kubectl</strong> command, as a result, it moves the current workload running in Kubernetes from its actual state to the desired state, which is defined in the command-line parameters or YAML-defined specifications. </p>
<p>Time management is the key to success in the CKA exam. Getting familiar with <strong class="source-inline">kubectl</strong> commands will help you save a lot of time when it comes to a new deployment. A good understanding of YAML definition will help you update the configurations quickly<a id="_idTextAnchor086"/>. </p>
<h2 id="_idParaDest-80"><a id="_idTextAnchor087"/>Understanding pods</h2>
<p>The smallest <a id="_idIndexMarker226"/>deployable unit in Kubernetes is a pod. The pod contains the actual application workload – it could be one or multiple containers. A pod in Kubernetes has a defined lifecycle. We’ll cover the following topic about pods: </p>
<ul>
<li>Understanding pods </li>
<li>Understanding health probing for pods </li>
<li>Understanding a multi-container pod</li>
<li>Understanding an init container</li>
<li>Understanding a static pod </li>
</ul>
<p>Let’s take a look at the pod first. You can create a pod using an imperative command as follows: </p>
<p class="source-code"><strong class="bold">kubectl run &lt;pod-name&gt; --image=&lt;image-name:image-tag&gt;</strong></p>
<p>This is an example of running a pod named <strong class="source-inline">ngin-pod</strong> with the image as <strong class="source-inline">nginx</strong> and the image tag as <strong class="source-inline">alpine</strong>: </p>
<p class="source-code"><strong class="bold">kubectl run nginx-pod --image=nginx:alpine</strong></p>
<p>You will see the output is returned as <strong class="source-inline">created</strong>, as follows, to indicate that your pod has been created successfully:</p>
<p class="source-code"><strong class="bold">pod/nginx-pod created</strong></p>
<p>In the process, you will see pod has the <strong class="source-inline">ContainerCreating</strong> status, indicating that the container is being created, and you can use <strong class="source-inline">kubectl</strong> to describe a pod command to see what’s going on. The following command is what we can use to check the pod’s current status: </p>
<p class="source-code">kubectl describe pod nginx-pod</p>
<p>At the bottom of the <strong class="source-inline">describe</strong> command, you will see the events – this is helpful information for you to use to check whether anything is going wrong during your deployment. We will explore<a id="_idIndexMarker227"/> troubleshooting pods further in <a href="B18201_08.xhtml#_idTextAnchor293"><em class="italic">Chapter 8</em></a>, <em class="italic">Monitoring and Logging Kubernetes Clusters and Applications</em>: </p>
<div>
<div class="IMG---Figure" id="_idContainer064">
<img alt="Figure 4.1 – The pod events " height="234" src="image/Figure_4.01_B18201.jpg" width="1546"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.1 – The pod events</p>
<p>The same pod can be YAML-defined, as follows, which will give you the same result: </p>
<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    ports:
    - containerPort: 80</pre>
<p>You can use the following command to deploy a YAML definition: </p>
<p class="source-code">kubectl apply -f &lt;your-spec&gt;.yaml</p>
<p>Similarly, we can run a BusyBox image with a single command, such as the following:</p>
<p class="source-code">kubectl run busybox --rm -it --image=busybox /bin/sh</p>
<p>You can also deploy a <strong class="source-inline">nginx</strong> image and then export the YAML definition by using the <strong class="source-inline">-o yaml</strong> flag :</p>
<p class="source-code">kubectl run nginx --image=nginx --dry-run -o yaml &gt; pod-sample.yaml</p>
<p>After running this command, a sample <strong class="source-inline">yaml</strong> file will be exported to your local PC – you can edit this <strong class="source-inline">yaml</strong> file to make changes locally if neede<a id="_idTextAnchor088"/>d. </p>
<h3>Understanding liveness, readiness, and startup probes</h3>
<p>To explore the health status of the pods further, let’s talk about health probes. Probes allow you to know how Kubernetes determines the states of <a id="_idIndexMarker228"/>your containers. Let’s have look at each of them one by one: </p>
<ul>
<li><strong class="bold">Liveness probes</strong> indicate <a id="_idIndexMarker229"/>whether the container is running <a id="_idIndexMarker230"/>properly, as they govern when the cluster will decide to restart the container automatically. </li>
<li><strong class="bold">Readiness probes</strong> indicate <a id="_idIndexMarker231"/>whether the container is ready to <a id="_idIndexMarker232"/>accept requests.</li>
<li><strong class="bold">Startup probes</strong> check<a id="_idIndexMarker233"/> when a container starts and are very handy for<a id="_idIndexMarker234"/> containers that require an additional startup time on their first initialization, preventing them from being killed by <strong class="source-inline">kubelet</strong> before they get on their feet. Once configured, they disable liveness and readiness checkers until they’re complete. </li>
</ul>
<p>We’ll have a look at these in more detail in <a href="B18201_08.xhtml#_idTextAnchor293"><em class="italic">Chapter 8</em></a><em class="italic">,</em> <em class="italic">Monitoring and Logging Kubernetes Clusters and Applications</em>. You can find further details about health probes<a id="_idIndexMarker235"/> at the following link: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-start<span id="_idTextAnchor089"/>up-probes/</a>.</p>
<h3>Understanding a multi-container pod</h3>
<p>Multi-container pods <a id="_idIndexMarker236"/>are simply pods with more than one container working together as a single unit. When it comes to multiple containers residing in a pod, a container interacts with another in the following two ways:</p>
<ul>
<li><strong class="bold">Shared networking</strong>: When <a id="_idIndexMarker237"/>two containers are running on the same host when they are in the same pod, they can access each other by simply using <em class="italic">localhost</em>. All the listening ports are accessible to other containers in the pod, even if they’re not exposed outside the pod. </li>
</ul>
<p><em class="italic">Figure 4.2</em> shows how <a id="_idIndexMarker238"/>multiple containers in the same pod share a local network with each other: </p>
<div>
<div class="IMG---Figure" id="_idContainer065">
<img alt="Figure 4.2 – A multi-container pod’s shared network  " height="459" src="image/Figure_4.02_B18201.jpg" width="927"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.2 – A multi-container pod’s shared network </p>
<ul>
<li><strong class="bold">Shared storage volumes</strong>: We <a id="_idIndexMarker239"/>can mount the same volume to two different containers so that they can both interact with the same data – it is possible to have one container write data to the volume and the other container read that data from the same volume. Some volumes even allow concurrent reading and writing. We’ll dive deeper into how storage works for multi-container pods in <a href="B18201_05.xhtml#_idTextAnchor149"><em class="italic">Chapter 5</em></a>, <em class="italic">Demystifying Kubernetes Storage</em>. </li>
</ul>
<p><em class="italic">Figure 4.3</em> shows how multiple containers in the same pod share local storage with each other: </p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<img alt="Figure 4.3 – A multi-container pod’s shared storage volume " height="518" src="image/Figure_4.03_B18201.jpg" width="927"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.3 – A multi-container pod’s shared storage volume</p>
<p>The following is an example of <a id="_idIndexMarker240"/>how to create multiple containers in a pod: </p>
<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
  name: multi-app-pod
  labels:
      app: multi-app
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  - name: busybox-sidecar
    image: busybox
    command: ['sh', '-c', 'while true; do sleep 3600; done;']</pre>
<p>In general, it is good to have a one-to-one relationship between a container and a pod, which follows the principles of building microservices by keeping each module independent. The real world is sometimes more complicated than it may seem, let’s take a look at multi-co<a id="_idTextAnchor090"/>ntainer pods. </p>
<h3>Understanding an init container</h3>
<p>An init container<a id="_idIndexMarker241"/> is configured in a pod to execute before the container host starts. It is specified inside an <strong class="source-inline">initContainers</strong> section, as in the following example. You can configure multiple init containers too, which will allow each init container to complete one at a time in sequential order: </p>
<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
  name: melon-pod
  labels:
    app: melonapp
spec:
  containers:
  - name: melonapp-container
    image: busybox:latest
    command: ['sh', '-c', 'echo The melonapp is running! &amp;&amp; sleep 3600']
<strong class="bold">  initContainers:</strong>
<strong class="bold">  - name: init-melonservice</strong>
<strong class="bold">    image: busybox:latest</strong>
<strong class="bold">    command: ['sh', '-c', 'until nslookup melonservice; do echo waiting for melonservice; sleep 2; done;']</strong>
  - name: init-melondb
    image: busybox:latest
    command: ['sh', '-c', 'until nslookup melondb; do echo waiting for melondb; <a id="_idTextAnchor091"/>sleep 2; done;']</pre>
<p>In the case that any of the<a id="_idIndexMarker242"/> init containers fail to complete, Kubernetes will restart the pod repeatedly until the init container succeeds. To learn more about init containers, visit the following link: <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">https://kubernetes.io/docs/concepts/workloads/pods/init-containers/</a>.</p>
<h3>Understanding a static Pod</h3>
<p>As the captain of a <a id="_idIndexMarker243"/>worker node, the <strong class="source-inline">kubelet</strong> agent can manage a node independently, and it can create pods. The pods that are managed directly by the <strong class="source-inline">kubelet</strong> daemon and bound to a specific node are called static pods. As opposed to pods that are managed by the Kubernetes master, static pods are watched by the <strong class="source-inline">kubelet</strong> agent, and it restarts in the case of failure. </p>
<p>The way to configure <strong class="source-inline">kubelet</strong> so that it reads the pod definition files is to add a YAML specification under the following directory where the static pod information is stored:</p>
<pre class="source-code">
 /etc/kubernetes/manifests</pre>
<p><strong class="source-inline">kubelet</strong> checks this directory periodically. This path can be configured in<a id="_idTextAnchor092"/> <strong class="source-inline">kubelet.service</strong>.</p>
<h3>Understanding Job and CronJob objects</h3>
<p><strong class="bold">Jobs</strong> can be<a id="_idIndexMarker244"/> used to reliably execute a workload and define when it completes – typically, a Job will create one or more pods. After the Job is finished, the containers will exit and the pods will enter the <strong class="source-inline">Completed</strong> status. </p>
<p>Jobs can be used to reliably execute a workload until it completes. The Job will create one or more pods. When the Job is finished, the containers will exit and the pods will enter the <strong class="source-inline">Completed</strong> status. An example use of Jobs is when we want to run a particular workload and make<a id="_idIndexMarker245"/> sure that it runs once and succeeds.</p>
<ol>
<li>You can create a Job with a YAML description:<p class="source-code">apiVersion: batch/v1</p><p class="source-code">kind: Job</p><p class="source-code">metadata:</p><p class="source-code">  name: pi</p><p class="source-code">spec:</p><p class="source-code">  template:</p><p class="source-code">    spec:</p><p class="source-code">      containers:</p><p class="source-code">      - name: pi</p><p class="source-code">        image: perl</p><p class="source-code">        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]</p><p class="source-code">      restartPolicy: Never</p><p class="source-code">  backoffLimit: 4</p></li>
</ol>
<p>The <strong class="source-inline">backoffLimit</strong> parameter means that, if it fails <strong class="source-inline">4</strong> times, this is the limit. All the Job does the same as it is while creating a pod under the hood. Although a normal pod is constantly running, when a Job is complete, it goes into the <strong class="source-inline">Completed</strong> status. This means that the container is no longer running, so the pod still exists, but the container is complete.</p>
<ol>
<li>You can use the following command to deploy a YAML definition: <p class="source-code"><strong class="bold">kubectl apply -f melon-job.yaml</strong></p></li>
<li> You can run the following command to check the Job’s status:<p class="source-code"><strong class="bold">kubectl get job</strong></p></li>
<li>When the <a id="_idIndexMarker246"/>Job is still running, you can see the <strong class="source-inline">Running</strong> status. When the Job is finished, you can see that it is complete from the following: </li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer067">
<img alt="Figure 4.4 – The Job is complete " height="96" src="image/Figure_4.04_B18201.jpg" width="822"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.4 – The Job is complete</p>
<p><strong class="bold">CronJobs</strong>, based<a id="_idIndexMarker247"/> on the capability of a Job, add value by allowing users to execute Jobs on a schedule. Users can use cron expressions to define a particular schedule as per their requirements. The following is an example of a CronJob YAML definition: </p>
<pre class="source-code">
apiVersion: batch/v1
kind: CronJob
metadata:
 name: hello
spec:
 schedule: "*/1 * * * *"
 jobTemplate:
   spec:
     template:
       spec:
         containers:
         - name: hello
           image: busybox
           args:
           - /bin/sh
           - -c
           - date; echo Hello from the Kubernetes cluster
         restartPolicy: OnFailure</pre>
<ol>
<li>You can use the following command to deploy a YAML definition: <p class="source-code"><strong class="bold">kubectl apply -f melon-cronjob.yaml</strong></p></li>
</ol>
<p>You can use the following command to check the cron job’s status:</p>
<p class="source-code">kubectl get cronjob</p>
<p>You’ll get an output as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer068">
<img alt="Figure 4.5 – The cron job shown as complete " height="96" src="image/Figure_4.05_B18201.jpg" width="934"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.5 – The cron job shown as complete</p>
<p>This cron job creates a<a id="_idIndexMarker248"/> few pods name <strong class="source-inline">hello</strong>, so we will use the following command to check the log of the Job:</p>
<p class="source-code">kubectl get pods | grep hello</p>
<p>You’ll get an output as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<img alt="Figure 4.6 – The completed cron job pods  " height="96" src="image/Figure_4.06_B18201.jpg" width="934"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.6 – The completed cron job pods </p>
<p>We can check the logs of these pods with the following command: </p>
<pre class="source-code">
kubectl logs hello-xxxx</pre>
<p>We can see that the cron job has been executed:</p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<img alt="Figure 4.7 – The logs showing how the cron job was completed " height="94" src="image/Figure_4.07_B18201.jpg" width="934"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.7 – The logs showing how the cron job was completed</p>
<p>If you want to delete cron jobs, you can use the following command: </p>
<p class="source-code">kubectl delete cronjobs hello</p>
<p>Then, you will see the following output indicating that your cron job has been deleted:</p>
<pre class="source-code">
cronjob.batch "hello" deleted</pre>
<p>CronJobs were <a id="_idIndexMarker249"/>promoted to general availability in Kubernetes v1.21. You can find a great article about running automated tasks using a CronJob here: <a href="https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs">https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs</a>.</p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor093"/>Deploying and managing applications</h1>
<p>The following <a id="_idIndexMarker250"/>sections of this chapter will take you through practical <a id="_idIndexMarker251"/>exercises with concrete examples that you would encounter in your real CKA exam, including how to deploy and scale applications, perform rolling updates and rollbacks for those applications, manage and govern the resource consumption for these applications,<a id="_idTextAnchor094"/> and configure them. </p>
<h2 id="_idParaDest-82"><a id="_idTextAnchor095"/>Deploying applications</h2>
<p>Deploying applications can be<a id="_idIndexMarker252"/> achieved in various ways, such as deploying a pod with <strong class="source-inline">kubectl</strong> or a YAML definition, as we did in the <em class="italic">The basics of Kubernetes workloads </em>section of this chapter. Now, we’ll take a look at a more effective way of using Deployments. In this section, let’s get into how to deploy an<a id="_idTextAnchor096"/>d scale applications. </p>
<h3>Deployments</h3>
<p>A Deployment<a id="_idIndexMarker253"/> is a convenient way to define the desired state deployment – it provides us with a better way of upgrading the underlying instances seamlessly using rolling updates, undoing changes, and pausing and resuming changes as required. For example, things such as deploying a ReplicaSet with a certain number of replicas are easy to roll out and roll back, and more effective. The following figure depicts how a Deployment looks conceptually: </p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<img alt="Figure 4.8 – A Deployment " height="614" src="image/Figure_4.08_B18201.jpg" width="1074"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.8 – A Deployment</p>
<p>Deployments <a id="_idIndexMarker254"/>provide a way to define a desired state for the replica pod. You can use a YAML definition as follows to define a Deployment:</p>
<pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80</pre>
<p>The following attributes are important to help you understand the preceding YAML definition:</p>
<ul>
<li><strong class="source-inline">spec.replicas</strong> gives us the number of replica pods</li>
<li><strong class="source-inline">spec.template</strong> is the template pod descriptor that defines the pods that will be created</li>
<li><strong class="source-inline">spec.selector</strong> is the deployment that will manage all pods whose labels match this selector</li>
</ul>
<p>We can create a <a id="_idIndexMarker255"/>Deployment using the following <strong class="source-inline">kubectl</strong> command:</p>
<p class="source-code">kubectl create deployment kubeserve --image=nginx:latest</p>
<p>After running the preceding command, you will then get the following output:</p>
<pre class="source-code">
deployment.apps/kubeserve created</pre>
<p>You can use <strong class="source-inline">kubectl get deploy</strong> to query all the Deployments in the current namespace as follows: </p>
<p class="source-code">kubectl get deployments</p>
<p>You will see the following Deployment status in the output:</p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<img alt="Figure 4.9 – kubectl getting the Deployments " height="86" src="image/Figure_4.09_B18201.jpg" width="808"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.9 – kubectl getting the Deployments</p>
<p>If you know the name of a<a id="_idIndexMarker256"/> Deployment, you can use the following command to get that Deployment: </p>
<p class="source-code">kubectl get deployment kubeserve</p>
<p>You will see the following output:</p>
<div>
<div class="IMG---Figure" id="_idContainer073">
<img alt="Figure 4.10 – kubectl getting a Deployment by name " height="86" src="image/Figure_4.10_B18201.jpg" width="830"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.10 – kubectl getting a Deployment by name</p>
<p>The following command allows you to get the details of the Deployment: </p>
<p class="source-code">kubectl describe deployment kubeserve</p>
<p>This command will help you understand the configurations in the Deployment, where you will see the following output: </p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<img alt="Figure 4.11 – kubectl describing a Deployment " height="906" src="image/Figure_4.11_B18201.jpg" width="1486"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.11 – kubectl describing a Deployment</p>
<p>The following command allows you to live-edit the Deployments:</p>
<p class="source-code">kubectl edit deployment kubeserve</p>
<p>The preceding <a id="_idIndexMarker257"/>command is a magical one that will allow you to <em class="italic">live-edit</em> a Deployment. The following is the sample output and you can edit it live – it works similarly to when you create a pod using the <strong class="source-inline">vim</strong> editor. You can live-edit the Deployment here, and then save and quit using <strong class="source-inline">wq!</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer075">
<img alt="Figure 4.12 – kubectl describing a Deployment for live-editing " height="1946" src="image/Figure_4.12_B18201.jpg" width="1286"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.12 – kubectl describing a Deployment for live-editing</p>
<p>Then, you can also delete<a id="_idIndexMarker258"/> Deployments if you don’t need them anymore with the <strong class="source-inline">kubectl delete</strong> command: </p>
<p class="source-code">kubectl delete deployment melon-serve</p>
<p>The following output shows that the Deployment has been deleted successfully:</p>
<p class="source-code">deployment.apps "kubeserve" deleted</p>
<p>With the deletion of the Deployment, the objects defined in that Deployment are also deleted, as they share the same lifecycle. In our third example, the deployed <strong class="source-inline">nginx</strong> pods are deleted, as we delete the <strong class="source-inline">kubeserve</strong> Deployment. </p>
<p>Learning about Deployments allows you to manage your application in a more effective way, update it as an entity easier, and roll it back to its previous versions. In the next section, we’ll have a look at rolling<a id="_idTextAnchor097"/> updates and rollbacks. </p>
<h1 id="_idParaDest-83"><a id="_idTextAnchor098"/>Performing rolling updates and rollbacks</h1>
<p>Rolling updates<a id="_idIndexMarker259"/> provide a way to update a Deployment to a newer version more effectively and efficiently. This way, you can update Kubernetes objects such as replicas and pods gradually with nearly zero downtime. In a nutshell, you may consider either using the <strong class="source-inline">kubectl set image</strong> command or going straight to updating a YAML manifest file. In this section, we will introduce <strong class="source-inline">kubectl set image</strong>, as it is very effective and handy to use <a id="_idTextAnchor099"/>in your actual CKA exam.</p>
<h2 id="_idParaDest-84"><a id="_idTextAnchor100"/>Rolling updates with kubectl </h2>
<p>From here, we’ll go through<a id="_idIndexMarker260"/> the steps of rolling updates <a id="_idIndexMarker261"/>with <strong class="source-inline">kubectl</strong>: </p>
<ol>
<li>You can spin up a new Deployment, <strong class="source-inline">kubeserve</strong>, using the following command: <p class="source-code"><strong class="bold">kubectl create deployment kubeserve --image=nginx:latest</strong></p></li>
<li>You can use <strong class="source-inline">kubectl</strong> to update the container image as follows:<p class="source-code"><strong class="bold">kubectl set image deployment/kubeserve nginx=nginx:1.18.0 --record</strong></p></li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout"><strong class="source-inline">--record flag</strong> records information about the updates so that it can be rolled back later. You can either use <strong class="source-inline">--record flag</strong> or <strong class="source-inline">--record=true flag</strong>. </p>
<p>With the preceding command, you will see the following output:</p>
<p class="source-code">deployment.apps/kubeserve image updated</p>
<ol>
<li>You can use the <strong class="source-inline">kubectl describe</strong> command to double-check whether your container image has updated successfully by typing the following command: <p class="source-code"><strong class="bold">kubectl describe deploy kubeserve</strong></p></li>
</ol>
<p>Your output should <a id="_idIndexMarker262"/>be similar to the following, in <em class="italic">Figure 4.14</em>, where you <a id="_idIndexMarker263"/>can see that the image is set to <strong class="source-inline">nginx:1.18.0</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer076">
<img alt="Figure 4.13 – kubectl describing kubeserve after updating the image " height="928" src="image/Figure_4.13_B18201.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.13 – kubectl describing kubeserve after updating the image</p>
<p>The <strong class="source-inline">kubectl describe deploy</strong> command comes in very handy when we are trying to <a id="_idIndexMarker264"/>check <a id="_idIndexMarker265"/>key information such as the container image, ports, and deployment-related events. This is also the case in the actual CKA exam – make sure you master the shortcut of this command, <strong class="source-inline">k describe deploy</strong>, which will help you work m<a id="_idTextAnchor101"/>ore effectively in the exam. </p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor102"/>Rollback</h2>
<p>Rollback <a id="_idIndexMarker266"/>allows us to revert to a previous state and a Deployment makes this super easy to achieve: </p>
<ol>
<li>You can use the following <strong class="source-inline">kubectl rollout</strong> command to quickly recover if you need to perform a rollback:<p class="source-code"><strong class="bold">kubectl rollout undo deployments kubeserve</strong></p></li>
</ol>
<p>Your output should look as follows:</p>
<p class="source-code"><strong class="bold">deployment.apps/kubeserve rolled back</strong></p>
<ol>
<li>Now, if you use the <strong class="source-inline">kubectl describe deploy kubeserve</strong> command, you will see the following output indicating that the image has been rolled back:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer077">
<img alt="Figure 4.14 – kubectl describing kubeserve after a rollback " height="980" src="image/Figure_4.14_B18201.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.14 – kubectl describing kubeserve after a rollback</p>
<ol>
<li>Now, you may be very <a id="_idIndexMarker267"/>curious as to whether we can keep a track of the history of our Deployments. You can use the following command:<p class="source-code"><strong class="bold">kubectl rollout history deployment kubeserve</strong></p></li>
</ol>
<p>The output would look as follows: </p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<img alt="Figure 4.15 – kubectl describing kubeserve " height="184" src="image/Figure_4.15_B18201.jpg" width="1256"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.15 – kubectl describing kubeserve</p>
<ol>
<li>In the case that you want to go back to a specific revision, you can use the <strong class="source-inline">--to-revision</strong> flag. You can see in <em class="italic">Figure 4.16</em> that we have revision <strong class="source-inline">2</strong> available thanks to using the <strong class="source-inline">--record</strong> flag when setting the image version. The following command is an example of undoing a Deployment and reverting to revision <strong class="source-inline">2</strong>: <p class="source-code"><strong class="bold">kubectl rollout undo deployment kubeserve --to-revision=2</strong></p></li>
</ol>
<p>Your output should look as follows: </p>
<p class="source-code">deployment.apps/kubeserve rolled back</p>
<ol>
<li>Now, if you use the <strong class="source-inline">kubectl describe deploy kubeserve</strong> command, you will see the <a id="_idIndexMarker268"/>following output indicating that the image has been rolled back to revision <strong class="source-inline">2</strong>: </li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer079">
<img alt="Figure 4.16 – kubectl describing kubeserve " height="1013" src="image/Figure_4.16_B18201.jpg" width="1641"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.16 – kubectl describing kubeserve</p>
<p>Deployments not only make the rolling update and rollback process much easier but also help us scale up and down with ease – we’ll take a look at how to scale applications, as well as all the viable options when d<a id="_idTextAnchor103"/>oing so, in the next section. </p>
<h1 id="_idParaDest-86"><a id="_idTextAnchor104"/>Scaling applications</h1>
<p>When our application<a id="_idIndexMarker269"/> becomes popular, in order to handle increasingly on-demand requests, we need to spin up multiple instances of applications to satisfy the workload requirements. </p>
<p>When you have a Deployment, scaling is achieved by changing the number of replicas. Here, you can scale a Deployment using the <strong class="source-inline">kubectl scale</strong> command to make this happen: </p>
<p class="source-code"><strong class="bold">kubectl scale deployment kubeserve --replicas=6</strong></p>
<p>Your output should look as follows: </p>
<pre class="source-code">
deployment.apps/kubeserve scaled</pre>
<p>If you use the <strong class="source-inline">kubectl get pods</strong> command now, you will see that some more copies of the pods are spinning up, as shown in the following output: </p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<img alt="Figure 4.17 – kubectl getting the pods and showing more copies of them " height="312" src="image/Figure_4.17_B18201.jpg" width="902"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.17 – kubectl getting the pods and showing more copies of them</p>
<p>Aside from manually scaling the Deployments with the <strong class="source-inline">kubectl scale</strong> command, we also have another way of scaling a<a id="_idIndexMarker270"/> Deployment and its ReplicaSets, which is <strong class="bold">HorizontalPodAutoscaler</strong> (<strong class="bold">HPA</strong>). Let’s take a <a id="_idTextAnchor105"/>look at the ReplicaSets first.</p>
<h2 id="_idParaDest-87"><a id="_idTextAnchor106"/>ReplicaSets</h2>
<p>ReplicaSets help<a id="_idIndexMarker271"/> pods achieve higher availability since users can define a certain number of replicas using a ReplicaSet. The main capability of a ReplicaSet is to make sure the cluster keeps the exact number of replicas running in the Kubernetes cluster. If any of them were to fail, new ones would be deployed. </p>
<p>The following is an example of the YAML definition of a ReplicaSet: </p>
<pre class="source-code">
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: melonapp-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: melonapp-rs
  template:
    metadata:
      labels:
        app: melonapp-rs
    spec:
      containers:
      - name: nginx
        image: nginx</pre>
<p>The <strong class="source-inline">matchLabels</strong> selector simply matches the labels specified under it to the labels on the pods. To check your ReplicaSet, use the following command:</p>
<p class="source-code">kubectl get replicaset</p>
<p>Alternatively, you can also use the following command:</p>
<p class="source-code">kubectl get rs</p>
<p>Then, you will see the <a id="_idIndexMarker272"/>output indicating the number of <strong class="source-inline">DESIRED</strong> <em class="italic">replica</em> counts and how many of them are in a <strong class="source-inline">READY</strong> state:</p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<img alt="Figure 4.18 – The kubectl get rs command showing the state of the ReplicaSet " height="202" src="image/Figure_4.18_B18201.jpg" width="902"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.18 – The kubectl get rs command showing the state of the ReplicaSet</p>
<p>Once the ReplicaSet is deployed, update the number of ReplicaSets by using the following command:</p>
<p class="source-code">kubectl scale replicaset frontend --replicas=6</p>
<p>Your output should look as follows:</p>
<pre class="source-code">
replicaset.apps/frontend scaled</pre>
<p>Alternatively, you can specify it in a YAML definition with the following command: </p>
<p class="source-code">kubectl scale --replicas=6 -f replicas.yaml</p>
<p>Your output should look as follows:</p>
<pre class="source-code">
replicaset.apps/frontend scaled</pre>
<p>Now, if you want to check whether the number of ReplicaSets has increased, you can use the <strong class="source-inline">kubectl get rs</strong> command again and you will be able to see the following output: </p>
<div>
<div class="IMG---Figure" id="_idContainer082">
<img alt="Figure 4.19 – kubectl getting the ReplicaSets  " height="96" src="image/Figure_4.19_B18201.jpg" width="902"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.19 – kubectl getting the ReplicaSets </p>
<p>In the case that you want to delete a ReplicaSet, you can use the <strong class="source-inline">kubectl delete</strong> command – in this case, we can use it to delete a ReplicaSet named <strong class="source-inline">frontend</strong>:</p>
<p class="source-code">kubectl delete replicaset frontend</p>
<p>Your output should look as follows:</p>
<pre class="source-code">
replicaset.apps "frontend" deleted</pre>
<p>Using ReplicaSets <a id="_idIndexMarker273"/>directly is not the only way to scale the applications. Let’s take a lo<a id="_idTextAnchor107"/><a id="_idTextAnchor108"/>ok at the alternative next, HPA.</p>
<h3>HPA</h3>
<p>To update <a id="_idIndexMarker274"/>a workload resource such as a Deployment or a StatefulSet, we can also use HPA – this is a Kubernetes API primitive that scales the workloads automatically based on your demands. <em class="italic">Figure 4.18</em> explains how HPA works in the context of application scaling: </p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<img alt="Figure 4.20 – HPA " height="613" src="image/Figure_4.20_B18201.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.20 – HPA</p>
<p>From the previous diagram, we can see that HPA is configured to fetch metrics provided by a metrics server based on the CPU and memory usage. These metrics are fetched from <strong class="source-inline">kubelet</strong> by the metrics server, which then exposes them to the API server using a metrics API. HPA scales the Deployment by increasing or decreasing the count of replicas, which is managed underneath by a ReplicaSet. </p>
<p>As on-demand resource <a id="_idIndexMarker275"/>requests increase, HPA scales out the Deployment and the number of replicas increases. Conversely, when the resource requests decrease, the number of replicas decreases. </p>
<p>To create an HPA, you can use the <strong class="source-inline">kubectl autoscale deployment</strong> command with the following flags for the requirements:</p>
<ul>
<li><strong class="source-inline">cpu-percent</strong> indicates the average CPU utilization usage across all pods</li>
<li><strong class="source-inline">min</strong> provides the minimum number of replicas</li>
<li><strong class="source-inline">max</strong> provides the maximum number of replicas</li>
</ul>
<p>You can use the following command to create an HPA with a CPU utilization usage of 50% and ensure a minimum of <strong class="source-inline">3</strong> copies and a maximum of up to <strong class="source-inline">10</strong> copies: </p>
<p class="source-code">kubectl autoscale deployment kubeserve --cpu-percent=50 --min=3 --max=10</p>
<p>Your output should look as follows:</p>
<pre class="source-code">
horizontalpodautoscaler.autoscaling/kubeserve autoscaled</pre>
<p>To check how many HPAs we currently have in the default namespace, use the following command: </p>
<p class="source-code">kubectl get hpa</p>
<p>The output would look as follows</p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<img alt="Figure 4.21 – Getting the HPAs in the default namespace " height="96" src="image/Figure_4.21_B18201.jpg" width="1258"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.21 – Getting the HPAs in the default namespace</p>
<p>You can also use the following<a id="_idIndexMarker276"/> YAML definition to deploy an HPA, which will help you achieve the same goal: </p>
<pre class="source-code">
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: kubeserve
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kubeserve
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50</pre>
<p>In the case that you want to delete<a id="_idIndexMarker277"/> an HPA, use a <strong class="source-inline">kubectl delete</strong> command. Here, we can delete an HPA named <strong class="source-inline">kubeserve</strong> as follows:</p>
<p class="source-code">kubectl delete hpa kubeserve</p>
<p>Your output will look as follows:</p>
<pre class="source-code">
horizontalpodautoscaler.autoscaling "kubeserve" deleted</pre>
<p>Another concept that we will cover is DaemonSets, which come in handier in real life, particularly in scenarios where at least one replica of the pod needs to be evenly distributed across the worke<a id="_idTextAnchor109"/><a id="_idTextAnchor110"/>r nodes. Let’s get right into it. </p>
<h3>DaemonSets</h3>
<p>We have learned about how ReplicaSets and <a id="_idIndexMarker278"/>Deployments help us ensure that multiple copies of our applications are up and running across various worker nodes. DaemonSets create a couple of copies of a pod, meanwhile making sure that at least one copy of the pod is evenly on each node in the Kubernetes cluster, as shown in <em class="italic">Figure 4.23</em>. </p>
<p>If a new node is added to the cluster, a replica of that pod is automatically assigned to that node. Similarly, when a node is removed, the pod is automatically removed.</p>
<div>
<div class="IMG---Figure" id="_idContainer085">
<img alt="Figure 4.22 – DaemonSets " height="579" src="image/Figure_4.22_B18201.jpg" width="1076"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.22 – DaemonSets</p>
<p>You can define a <a id="_idIndexMarker279"/>DaemonSet using the following YAML definition: </p>
<pre class="source-code">
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-system
  labels:
    k8s-app: fluentd
spec:
  selector:
    matchLabels:
      name: fluentd
  template:
    metadata:
      labels:
        name: fluentd
    spec:
      containers:
      - name: fluentd
        image: fluentd:latest</pre>
<p>Your output would look as follows:</p>
<pre class="source-code">
daemonset.apps/fluentd created</pre>
<p>Notice that we have created this <a id="_idIndexMarker280"/>DaemonSet in a namespace called <strong class="source-inline">kube-system</strong> this time – this is a namespace usually reserved for Kubernetes objects created by the Kubernetes system. We’ll get to talking about the namespace in a heartbeat. For now, you can check that the DaemonSet has been created using the following command:</p>
<p class="source-code">kubectl get daemonsets -n kube-system</p>
<p>Alternatively, we can simplify the command:</p>
<p class="source-code">kubectl get ds -n kube-system</p>
<p>Your output will look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<img alt="Figure 4.23 – Checking out the DaemonSets in the kube-system namespace " height="126" src="image/Figure_4.23_B18201.jpg" width="1348"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.23 – Checking out the DaemonSets in the kube-system namespace</p>
<p>Don’t forget to check the details of the<a id="_idIndexMarker281"/> DaemonSets by using the following:</p>
<p class="source-code">kubectl describe daemonsets fluentd -n kube-system</p>
<p>Your output would look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer087">
<img alt="Figure 4.24 – kubectl describing the DaemonSets " height="728" src="image/Figure_4.24_B18201.jpg" width="1176"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.24 – kubectl describing the DaemonSets</p>
<p>In case you want to delete a DaemonSet, use the <strong class="source-inline">kubectl delete</strong> command. Here, we can delete a DaemonSet named <strong class="source-inline">fluentd</strong> in the <strong class="source-inline">kube-system</strong> namespace as follows:</p>
<p class="source-code">kubectl delete ds fluentd -n kube-system </p>
<p>Your output should look as follows: </p>
<pre class="source-code">
daemonset.apps "fluentd" deleted</pre>
<p>The main use case of<a id="_idIndexMarker282"/> DaemonSets is to use them as a monitoring agent or a logs collector on every node, or in other cases, to run a cluster storage daemon across all the worker nodes. </p>
<p>With DaemonSets, you don’t have to worry about removing or adding new nodes that will impact the monitoring agents on these nodes. A real-life use case, such as <strong class="source-inline">fluentd</strong>, requires an agent to be dep<a id="_idTextAnchor111"/>loyed on each node in the cluster. </p>
<h1 id="_idParaDest-88"><a id="_idTextAnchor112"/>Workload scheduling </h1>
<p>Understanding the workload scheduling<a id="_idIndexMarker283"/> and how it works with the Kubernetes scheduler will be useful in your daily life as a Kubernetes Administrator. Kubernetes allows you to define node affinity rules, taints, and tolerations with the good use of labels, selectors, and annotations leading your way. Let’s first s<a id="_idTextAnchor113"/>tart with the notion of namespaces. </p>
<h2 id="_idParaDest-89"><a id="_idTextAnchor114"/>Understanding namespaces</h2>
<p>Thinking about the separation of the workloads, namespaces<a id="_idIndexMarker284"/> come in handy. A namespace is a logical separation of all the namespaced objects deployed in a single Kubernetes cluster. Deployments, Services, and Secrets are all namespaced. Otherwise, some Kubernetes objects are cluster-wide, such as Nodes, StorageClass, and PersistentVolume. The name of a resource has to be unique within a namespace. </p>
<p>You can get all namespaces using the following command: </p>
<p class="source-code">kubectl get namespaces</p>
<p>Alternatively, you can use this command: </p>
<p class="source-code">kubectl get ns</p>
<p>You will see that the output gets all the namespace currently in our Kubernetes cluster: </p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<img alt="Figure 4.25 – kubectl getting the namespaces " height="174" src="image/Figure_4.25_B18201.jpg" width="550"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.25 – kubectl getting the namespaces</p>
<p>When you define a pod or any <a id="_idIndexMarker285"/>namespaced Kubernetes object, you can specify the namespace in the YAML definition as follows: </p>
<pre class="source-code">
   apiVersion: v1
   kind: Pod
   metadata:
    name: k8s-ns-pod
<strong class="bold">    namespace: k8s-ns</strong>
    labels:
      app: k8sapp
   spec:
    containers:
    - name: k8sapp-container
      image: busybox
      command: ['sh', '-c', 'echo Salut K8S! &amp;&amp; sleep 3600']</pre>
<p>If you create that pod and specify the namespace that the pod belongs to, you can add the <strong class="source-inline">-n</strong> flag when querying this pod using the <strong class="source-inline">kubectl get pods</strong> command. The following is an example: </p>
<p class="source-code">kubectl get pods -n k8s-ns</p>
<p>Similarly, if the pod has been created in that namespace, you can use the following command to check it out:</p>
<p class="source-code">kubectl describe pod k8s-ms-pod -n k8s-ns</p>
<p>In the case that the pods are not in the default namespace, you don’t have to specify the namespace option anymore. In the following example, you want to set a namespace named <strong class="source-inline">dev</strong>, and then use the <strong class="source-inline">kubectl get</strong> command without the <strong class="source-inline">-n</strong> flag: </p>
<p class="source-code">kubectl config set-context &amp;(kubectl config current-context) --namespace=dev</p>
<p>You can then simply run the following <a id="_idIndexMarker286"/>command without the namespace option to list the pods: </p>
<p class="source-code">kubectl get pods</p>
<p>Understanding namespaces will further help you when you need to define the namespace-scoped permissions where Kubernetes objects are grouped. We’ll elaborate on this furt<a id="_idTextAnchor115"/>her in <a href="B18201_06.xhtml#_idTextAnchor192"><em class="italic">Chapter 6</em></a>, <em class="italic">Securing Kubernetes</em>. </p>
<h2 id="_idParaDest-90"><a id="_idTextAnchor116"/>Labels, node selectors, and annotations</h2>
<p>Labels, selectors, and annotations<a id="_idIndexMarker287"/> are useful notions when it comes to workload scheduling. Labels are key-value pairs attached to Kubernetes objects that can be listed in the <strong class="source-inline">metadata.labels</strong> section <a id="_idIndexMarker288"/>of an object descriptor. Selectors<a id="_idIndexMarker289"/> are used for identifying and selecting a group of objects using their labels. See the following examples of some quality-based selectors:</p>
<p class="source-code">kubectl get pods -l app=my-app</p>
<p class="source-code">kubectl get pods -l environment=production</p>
<p>When it comes to inequality, you can use the following: </p>
<p class="source-code">kubectl get pods -l environment!=production</p>
<p>The following example involves chaining multiple selectors together using a comma-delimited list:</p>
<p class="source-code">kubectl get pods -l app=myapp.environment=production</p>
<p>To assign a pod to nodes, we can use node selectors. You can specify a map of key-value pairs in the <strong class="source-inline">PodSpec</strong> field:</p>
<p>You can start by labeling the worker nodes using the following command:</p>
<p class="source-code">kubectl label node cloudmelonplayground env=dev</p>
<p>The output should be as follows: </p>
<p class="source-code">node/cloudmelonplayground labeled</p>
<p>You can use the following command to show the label of worker nodes: </p>
<p class="source-code">kubectl get nodes --show-labels</p>
<p>Then, we should get the following output:</p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<img alt="Figure 4.26 – Getting the node labels " height="127" src="image/Figure_4.26_B18201.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.26 – Getting the node labels</p>
<p>Then, you can add the <a id="_idIndexMarker290"/>node selector in the YAML definition as follows:</p>
<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector:
    env: dev</pre>
<p>We can attach annotations<a id="_idIndexMarker291"/> to objects using the <strong class="source-inline">metadata.annotations</strong> section, as with the following configuration file that has the annotation <strong class="source-inline">imageregistry: "http://hub.docker.com/"</strong>:</p>
<pre class="source-code">
    
apiVersion: v1
kind: Pod
metadata:
  name: melon-annotation
<strong class="bold">  annotations:</strong>
    imageregistry: "https://hub.docker.com/"
spec:
  containers:
  - name: nginx
    image: ngin<a id="_idTextAnchor117"/>x:latest
    ports:
    - containerPort: 80</pre>
<p>Annotations are similar to labels and they can be used to store custom metadata about objects. </p>
<h2 id="_idParaDest-91"><a id="_idTextAnchor118"/>Node affinity and anti-affinity</h2>
<p>Node affinity<a id="_idIndexMarker292"/> and anti-affinity<a id="_idIndexMarker293"/> are simply ways to help pods be assigned to the right node. Compare this to <strong class="source-inline">nodeSelector</strong>, which is designed for assigning a pod directly to the worker nodes. The following is an example of node affinity and anti-affinity in the YAML specification:</p>
<pre class="source-code">
 spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: topology.kubernetes.io/zone
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: topology.kubernetes.io/zone</pre>
<p>With particular labels, node<a id="_idIndexMarker294"/> affinity and anti-affinity allow us to cre<a id="_idTextAnchor119"/>ate matching rules with logic and operations. </p>
<h2 id="_idParaDest-92"><a id="_idTextAnchor120"/>Taints and tolerations</h2>
<p>Aside from node affinity<a id="_idIndexMarker295"/> and anti-affinity, we can also assign taints on the node<a id="_idIndexMarker296"/> and tolerations on the pods by tainting the nodes and ensuring that no pods will be scheduled to that node.</p>
<p>You can use the following command to taint a node: </p>
<p class="source-code">kubectl taint nodes melonnode app=melonapp:NoSchedule</p>
<p>The preceding definition can be<a id="_idIndexMarker297"/> translated into a pod YAML definition file to achieve the <a id="_idIndexMarker298"/>same outcome as follows:</p>
<pre class="source-code">
  apiVersion: v1
  kind: Pod
  metadata:
   name: melon-ns-pod
   namespace: melon-ns
   labels:
     app: melonapp
  spec:
   containers:
   - name: melonapp-container
     image: busybox
     command: ['sh', '-c', 'echo Salut K8S! &amp;&amp; sleep 3600']
   tolerations:
   - key: "app"
     operator: "Equal"
     value: "melonapp"
     effect: "NoSchedule"</pre>
<p>If you want to un-taint a node, you can use the following command: </p>
<p class="source-code">kubectl taint nodes yourworkernode   node-role.kubernetes.io/yourworkernode:NoSchedule-</p>
<p>We have learned how to taint certain nodes when you want to evict workloads from a node in this section. Now, let’s look at resource management.</p>
<h1 id="_idParaDest-93"><a id="_idTextAnchor121"/>Resource management</h1>
<p>Kubernetes allows us to<a id="_idIndexMarker299"/> specify the resource requirements of a container in the pod specification, which basically refers to how many resources a container needs.</p>
<p><strong class="source-inline">kube-scheduler</strong> uses the resource request information that you specify for a container in a pod to decide on which worker node to schedule the pod. It’s up to <strong class="source-inline">kubelet</strong> to enforce these resource limits when you specify them for the containers in the pod so that the running container goes beyond a set limit, as well as reserves at least the requested amount of a system resource for a container to use.</p>
<p>It usually gives us the following values: </p>
<ul>
<li><strong class="source-inline">resources.limits.cpu</strong> is the resource limit set on CPU usage.</li>
<li><strong class="source-inline">resources.limits.memory</strong> is the resource limit set on memory usage.</li>
<li><strong class="source-inline">resources.requests.cpu</strong> is the minimum CPU usage requested to allow your application to be up and running.</li>
<li><strong class="source-inline">resources.requests.memory</strong> is the minimum memory usage requested to allow your application to be up and running. In the case that a container exceeds its memory request, the worker node that it runs on becomes short on overall memory at the same time, and the pod that the container belongs to is likely to be evicted too.</li>
<li><strong class="source-inline">resources.limits.ephemeral-storage</strong> is the limit on ephemeral storage resources.</li>
<li><strong class="source-inline">resources.limits.hugepages-&lt;size&gt;</strong> is the limit on the allocation and consumption of pre-allocated huge pages by any applications in a pod.</li>
</ul>
<p>A resource request refers to the amount of resources that are necessary to run a container, and what they do is govern on which worker node the containers will actually be scheduled. So, when Kubernetes is getting ready to run a particular pod, it’s going to choose a worker node based on the resource requests of that pod’s containers. Kubernetes will use these values to ensure that it chooses a node that actually has enough resources available to run that pod. A pod will only run on a node that has enough available resources to run the pod’s containers. The following is a YAML example of defining <strong class="source-inline">resource request</strong> and <strong class="source-inline">limits</strong>:</p>
<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
 name: melonapp-pod
spec:
 containers:
 - name: melonapp-container
   image: busybox
   command: ['sh', '-c', 'echo stay tuned! &amp;&amp; sleep 3600']
   resources:
     <strong class="bold">requests:</strong>
<strong class="bold">       memory: "64Mi"   # 64 Megabytes</strong>
<strong class="bold">       cpu: "250m" </strong>
<strong class="bold">     limits:</strong>
<strong class="bold">       memory: "128Mi"</strong>
<strong class="bold">       cpu: "500m"</strong></pre>
<p>You can use<a id="_idIndexMarker300"/> the <strong class="source-inline">kubectl describe node</strong> command to check the allocation resources of that node to see whether your requests or limits definitions correspond to what is needed in the current circumstances: </p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<img alt="Figure 4.27 – kubectl describing the node resources " height="342" src="image/Figure_4.27_B18201.jpg" width="928"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.27 – kubectl describing the node resources</p>
<p>You can use the <strong class="source-inline">kubectl top</strong> command in the case that you have a metrics server installed in your cluster to ch<a id="_idTextAnchor122"/>eck the actual resource usage of the node or pod. </p>
<h1 id="_idParaDest-94"><a id="_idTextAnchor123"/>Configuring applications</h1>
<p>Configuring an <a id="_idIndexMarker301"/>application is a simple and straightforward experience thanks to ConfigM<a id="_idTextAnchor124"/>aps and Secrets. Let’s take a look at each of them.</p>
<h3>Understanding ConfigMaps</h3>
<p>A ConfigMap<a id="_idIndexMarker302"/> is simply a Kubernetes object that stores configuration data in key-value pairs. This configuration data can then be used to configure the software running in a container by configuring a pod to consume ConfigMaps using environment variables, command-line arguments, or mounting a volume with configuration files. </p>
<p>You can also use a YAML definition to define <strong class="source-inline">configmap</strong> as follows:</p>
<pre class="source-code">
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: melon-configmap
  data:
    myKey: myValue
    myFav: myHome</pre>
<p>Your output should look as follows: </p>
<pre class="source-code">
configmap/melon-configmap created</pre>
<p>You can check <strong class="source-inline">configmap</strong> using the following command:</p>
<p class="source-code">kubectl get configmap</p>
<p>Alternatively, you<a id="_idIndexMarker303"/> can use this command:</p>
<p class="source-code">kubectl get cm</p>
<p>Your output will be as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<img alt="Figure 4.28 – kubectl getting configmap " height="116" src="image/Figure_4.28_B18201.jpg" width="672"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.28 – kubectl getting configmap</p>
<p>You can check the binary data of <strong class="source-inline">configmap</strong> using the following command: </p>
<p class="source-code">k describe configmap melon-configmap</p>
<p>The following screenshot is the output of the preceding command: </p>
<div>
<div class="IMG---Figure" id="_idContainer092">
<img alt="Figure 4.29 – The configmap binary data " height="510" src="image/Figure_4.29_B18201.jpg" width="476"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.29 – The configmap binary data</p>
<p>Once you have <strong class="source-inline">configmap</strong> ready, here’s<a id="_idIndexMarker304"/> how to configure the pod to consume it: </p>
<ol>
<li>Create a pod that can use the <strong class="source-inline">configmap</strong> data by using environment variables:<p class="source-code">apiVersion: v1</p><p class="source-code">kind: Pod</p><p class="source-code">metadata:</p><p class="source-code">  name: melon-configmap</p><p class="source-code">spec:</p><p class="source-code">  containers:</p><p class="source-code">  - name: melonapp-container</p><p class="source-code">image: busybox</p><p class="source-code">    command: ['sh', '-c', "echo $(MY_VAR) &amp;&amp; sleep 3600"]</p><p class="source-code">    env:</p><p class="source-code">    - name: MY_VAR</p><p class="source-code">      valueFrom:</p><p class="source-code">        configMapKeyRef:</p><p class="source-code">          name: melon-configmap</p><p class="source-code">          key: myKey</p></li>
</ol>
<p>You can use the following command to check the <strong class="source-inline">configmap</strong> value:</p>
<p class="source-code"><strong class="bold"> kubectl logs melon-configmap</strong></p>
<p>The output will be similar to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<img alt="Figure 4.30 – The configmap mounted value " height="66" src="image/Figure_4.30_B18201.jpg" width="822"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.30 – The configmap mounted value</p>
<ol>
<li>You can create<a id="_idIndexMarker305"/> a pod to use <strong class="source-inline">configmap</strong> data via a volume. The following is an example of a YAML definition: <p class="source-code">apiVersion: v1</p><p class="source-code">kind: Pod</p><p class="source-code">metadata:</p><p class="source-code">  name: melon-volume-pod</p><p class="source-code">spec:</p><p class="source-code">  containers:</p><p class="source-code">   - name: myapp-container</p><p class="source-code">     image: busybox</p><p class="source-code">     command: ['sh', '-c', "echo $(cat /etc/config/myKey) &amp;&amp; sleep 3600"]</p><p class="source-code">     volumeMounts:</p><p class="source-code">       - name: config-volume</p><p class="source-code">         mountPath: /etc/config</p><p class="source-code">  volumes:</p><p class="source-code">    - name: config-volume</p><p class="source-code">      configMap:</p><p class="source-code">        name: melon-configmap</p></li>
</ol>
<p>You can use the <strong class="source-inline">kubectl logs</strong> command to check the pod for the mounted data value, or use the following command to check the <strong class="source-inline">configmap</strong>:</p>
<p class="source-code"><strong class="bold">kubectl exec melon-volume-pod -- ls /etc/config</strong></p>
<p>The output will look as <a id="_idTextAnchor125"/>follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer094">
<img alt="Figure 4.31 – The configmap mounted value " height="86" src="image/Figure_4.31_B18201.jpg" width="1124"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.31 – The configmap mounted value</p>
<p>In the case that <a id="_idIndexMarker306"/>you want to delete a <strong class="source-inline">configmap</strong>, use the <strong class="source-inline">kubectl delete</strong> command: </p>
<p class="source-code">kubectl delete cm melon-configmap</p>
<p>Your output would<a id="_idTextAnchor126"/> look as follows:</p>
<pre class="source-code">
configmap "melon-configmap" deleted</pre>
<p>Here, we have shown how we can work with ConfigMaps in Kubernetes. Once you feel comfortable with ConfigMaps, you’ll find a lot of similarities when it comes to working with Secrets. Next, we will have a look at how to work with Kubernetes Secret<a id="_idTextAnchor127"/>s so that they can be consumed by your application. </p>
<h3>Understanding Secrets</h3>
<p>A Kubernetes <a id="_idIndexMarker307"/>Secret is an object containing sensitive data such as a password, an API token, or a key, which is passed to a pod rather than stored in a <strong class="source-inline">PodSpec</strong> field or in the container itself:</p>
<p class="source-code">kubectl create melon-secret --from-literal=username=packtuser </p>
<p class="source-code">  --from-literal=password='S!B\*d$zDsb='</p>
<p>You can also use a YAML definition to define <strong class="source-inline">configmap</strong> as the following with base64:</p>
<pre class="source-code">
apiVersion: v1
kind: Secret
metadata:
  name: melon-secret
type: Opaque
data:
  USER_NAME: bXl1c2VybmFtZQo=
  PASSWORD: bXlwYXNzd29yZAo=</pre>
<p>You can check the Secrets by using the following command:</p>
<p class="source-code">kubectl get secrets</p>
<p>Your output would look as follows: </p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<img alt="Figure 4.32 – kubectl getting Secrets " height="108" src="image/Figure_4.32_B18201.jpg" width="1018"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 4.32 – kubectl getting Secrets</p>
<p>Once you have created the Secret, you may want to attach the Secret to an application. That’s where you need to create a pod to consume the Secret by following these steps:</p>
<ol>
<li>You can create a pod to consume the Secret using environment variables:<p class="source-code">apiVersion: v1</p><p class="source-code">kind: Pod</p><p class="source-code">metadata:</p><p class="source-code">  name: melon-secret-pod</p><p class="source-code">spec:</p><p class="source-code">  containers:</p><p class="source-code">    - name: test-container</p><p class="source-code">      image: busybox:latest</p><p class="source-code">      command: [ "/bin/sh", "-c", "env" ]</p><p class="source-code">      envFrom:</p><p class="source-code">      - secretRef:</p><p class="source-code">          name: melon-secret</p><p class="source-code">  restartPolicy: Never</p></li>
<li>You can also consume a<a id="_idIndexMarker308"/> Secret as a volume, as shown here – you will define a <strong class="source-inline">secret-volume</strong> and then mount the <strong class="source-inline">secret-volume</strong> to the <strong class="source-inline">/etc/secret-volume</strong> path:<p class="source-code"><strong class="bold">  volumes:</strong></p><p class="source-code"><strong class="bold">  - name: secret-volume</strong></p><p class="source-code"><strong class="bold">    secret:</strong></p><p class="source-code"><strong class="bold">      secretName: melon-secret</strong></p><p class="source-code">  containers:</p><p class="source-code">  - name: mybusybox</p><p class="source-code">    image: busybox:latest</p><p class="source-code">    command: [ "/bin/sh", "-c", "env" ]</p><p class="source-code"><strong class="bold">    volumeMounts:</strong></p><p class="source-code"><strong class="bold">    - name: secret-volume</strong></p><p class="source-code">      readOnly: true</p><p class="source-code"><strong class="bold">      mountPath: "/etc/secret-volume"</strong></p></li>
</ol>
<p>If you want to delete a Secret, use the <strong class="source-inline">kubectl delete</strong> command as follows:</p>
<p class="source-code">kubectl delete secret melon-secret</p>
<p>Your output will look as follows:</p>
<pre class="source-code">
secret "melon-secret" deleted</pre>
<p>Note that if you delete <a id="_idIndexMarker309"/>a Secret, make sure to update the <strong class="source-inline">PodSpec</strong> field for your application to avoid exceptions. You can do this by creating a new Secret, then attaching it to your pod, or updating yo<a id="_idTextAnchor128"/>ur application so it doesn’t need the Secret anymore. </p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor129"/>Manifest management with kustomize</h2>
<p>Starting from Kubernetes 1.14, customization files became available to facilitate smoother Kubernetes <a id="_idIndexMarker310"/>management. It supports the following <a id="_idIndexMarker311"/>use cases: </p>
<ul>
<li>Generation of YAML definitions from other resources, such as generating a Kubernetes Secret and its YAML definition</li>
<li>Common configuration across multiple YAML definitions, such as adding namespace for a group of resources</li>
<li>Composing and customizing a collection of YAML definitions, such as setting resource requests and limits for multiple Kubernetes objects</li>
</ul>
<p>This can be achieved using a central file called <strong class="source-inline">Kustomization.yaml</strong>. You can use the following command to view the resources found in the directory that are contained in a customization file: </p>
<p class="source-code">kubectl kustomize &lt;targeting_kustomization_directory&gt;</p>
<p>You can then apply those resources by running the following command: </p>
<p class="source-code">kubectl apply -k &lt;targeting_kustomization_directory&gt;</p>
<p>Let’s take Secret generation as an example and generate a Secret manifest file: </p>
<pre class="source-code">
# Create a password.txt file
cat &lt;&lt;EOF &gt;./password.txt
username=admin
password=secret
EOF
cat &lt;&lt;EOF &gt;deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  labels:
    app: my-app
spec:
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: app
        image: my-app
        volumeMounts:
        - name: password
          mountPath: /secrets
      volumes:
      - name: password
        secret:
          secretName: example-secret-1
EOF
cat &lt;&lt;EOF &gt;./kustomization.yaml
resources:
- deployment.yaml
secretGenerator:
- name: example-secret-1
  files:
  - password.txt
EOF</pre>
<p>Then, you will be<a id="_idIndexMarker312"/> able to see that you have two files created after <a id="_idIndexMarker313"/>executing the previous steps: </p>
<pre class="source-code">
kustomization.yaml  password.txt</pre>
<p>If you want to check out the content of the <strong class="source-inline">customization.yaml</strong> file, you can use <strong class="source-inline">cat customization.yaml</strong> and you will see the following output: </p>
<pre class="source-code">
secretGenerator:
- name: example-secret-1
  files:
  - password.txt</pre>
<p>Then, you can <a id="_idIndexMarker314"/>use the <strong class="source-inline">kubectl apply</strong> command to deploy the pod with the Secret mounted: </p>
<p class="source-code">kubectl apply -f ./test</p>
<p>Kustomize is a good <a id="_idIndexMarker315"/>way to customize your application configuration and now that it is built into <strong class="source-inline">kubectl apply -k</strong>, you can gain a greater understanding of the use cases of Kustomize by visiting the official doc<a id="_idTextAnchor130"/>umentation site: <a href="https://kubectl.docs.kubernetes.io/guides/">https://kubectl.docs.kubernetes.io/guides/</a>.</p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor131"/>Common package management and templating with Helm</h2>
<p>Helm is a <a id="_idIndexMarker316"/>management tool for managing packages of pre-configured Kubernetes objects in the form of charts – we call these Helm charts. Helm charts allow users to install and manage Kubernetes applications more reproducibly and effectively. Furthermore, you can find popular Helm charts from the community or share your own applications with the Helm community at this link: <a href="https://artifacthub.io/packages/search">https://artifacthub.io/packages/search</a>.</p>
<p>The standard file structure of a chart is as follows:</p>
<ul>
<li><strong class="source-inline">Charts</strong> – (the folder)</li>
<li><strong class="source-inline">Chart.yaml #</strong> – A .<strong class="source-inline">yaml</strong> file that contains the information about the chart</li>
<li><strong class="source-inline">README.md</strong></li>
<li><strong class="source-inline">requirements.lock</strong></li>
<li><strong class="source-inline">requirements.yaml</strong> – an optional file that lists the dependencies for a chart (the dependencies are actually packaged in the <strong class="source-inline">Charts</strong> folder)</li>
<li><strong class="source-inline">templates</strong> – a directory of templates that combine with values to generate Kubernetes manifest files</li>
<li><strong class="source-inline">values.yaml</strong> – contains the default configuration values for the chart (this is where Helm grabs the values for the manifest template that contains the reference values)</li>
</ul>
<p>To query the <a id="_idIndexMarker317"/>Helm charts <a id="_idIndexMarker318"/>that have been<a id="_idIndexMarker319"/> deployed, use the<a id="_idIndexMarker320"/> following command:</p>
<p class="source-code">helm install stable/melonchart</p>
<p>If you need to search for a chart, you can use the following command: </p>
<p class="source-code">helm search chartname</p>
<p>Delete a Helm chart that has been deployed using the following command:</p>
<p class="source-code">helm delete melonchart</p>
<p>Whenever you install a <a id="_idIndexMarker321"/>chart, a new release is created. So, one chart can <a id="_idIndexMarker322"/>be installed multiple times into the same cluster. Each can be<a id="_idIndexMarker323"/> independently managed and <a id="_idIndexMarker324"/>upgraded. To upgrade a release to a specified version of a chart or update the chart values, run the following:</p>
<p class="source-code">helm upgrade [RELEASE] [CHART_path] [flags]</p>
<p>To roll back to a specific version, you can use the following command:</p>
<p class="source-code">helm rollback melon-release 2</p>
<p>Helm charts<a id="_idIndexMarker325"/> help you manage, install, and upgrade Kubernetes-native applications. You can learn more about Helm <a id="_idTextAnchor132"/>by visiting their official documentation website: <a href="https://helm.sh/docs/">https://helm.sh/docs/</a>.</p>
<h1 id="_idParaDest-97"><a id="_idTextAnchor133"/>Summary</h1>
<p>In this chapter, we covered one of the most common tasks for both Kubernetes Administrators and Developers – application scheduling and managing the application lifecycle. Even though this chapter covers about 15% of the content of the CKA exam, working with Kubernetes objects is one of the most important daily tasks as a Kubernetes Administrator. Ensure that you practice enough and master the shortcuts of the <strong class="source-inline">kubectl</strong> commands before moving on. </p>
<p>In the next chapter, we’ll talk about Kubernetes storage. The content and the questions covered in <a href="B18201_04.xhtml#_idTextAnchor080"><em class="italic">Chapter 4</em></a>, <em class="italic">Application Scheduling and Lifecycle Management</em> and <a href="B18201_05.xhtml#_idTextAnchor149"><em class="italic">Chapter 5</em></a>, <em class="italic">Demystifying Kubernetes Storage</em> are considered very high-value and less time-consuming <a id="_idTextAnchor134"/>within the scheme of the actual CKA exam. Stay tuned and keep learning!</p>
<h1 id="_idParaDest-98"><a id="_idTextAnchor135"/>Mock CKA scenario-based practice test </h1>
<p>You <a id="_idIndexMarker326"/>have two virtual machines,<a id="_idTextAnchor136"/> <strong class="source-inline">master-0</strong> and <strong class="source-inline">worker-0</strong>. Please complete the following mock scenarios. </p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor137"/>Scenario 1</h2>
<p>SSH into<a id="_idIndexMarker327"/> the <strong class="source-inline">worker-0 </strong><a id="_idTextAnchor138"/>node and provision a new pod called <strong class="source-inline">ngnix</strong> with a single contai<a id="_idTextAnchor139"/>ner, <strong class="source-inline">nginx</strong>.</p>
<h2 id="_idParaDest-100"><a id="_idTextAnchor140"/>Scenario 2</h2>
<p>SSH to <strong class="source-inline">worker-0 </strong>and then scale <strong class="source-inline">nginx</strong> to 5 copies. </p>
<h2 id="_idParaDest-101"><a id="_idTextAnchor141"/>Scenario 3</h2>
<p>SSH to <strong class="source-inline">worker-0</strong>, set a Confi<a id="_idTextAnchor142"/>gMap with a username and password, and then attach a new pod to BusyBox. </p>
<h2 id="_idParaDest-102"><a id="_idTextAnchor143"/>Scenario 4</h2>
<p>SSH <a id="_idTextAnchor144"/><a id="_idTextAnchor145"/>to <strong class="source-inline">worker-0</strong> and create a <strong class="source-inline">nginx</strong> pod with an init container called <strong class="source-inline">busybox</strong>.</p>
<h2 id="_idParaDest-103"><a id="_idTextAnchor146"/>Scenario 5</h2>
<p>SSH to <strong class="source-inline">worker-0</strong>, create a <strong class="source-inline">nginx</strong> pod, and then a <strong class="source-inline">busybox</strong> container in the same p<a id="_idTextAnchor147"/>od.</p>
<p>You can find all the <a id="_idIndexMarker328"/>scenario resolutions in <a href="B18201_Appendix_A.xhtml#_idTextAnchor386"><em class="italic">Appendix</em></a><em class="italic"> - Mock CKA scenario-based practice test resolutions</em> of this book.</p>
<h1 id="_idParaDest-104"><a id="_idTextAnchor148"/>FAQs</h1>
<ul>
<li><em class="italic">Where can I find out about Helm charts? </em></li>
</ul>
<p>Go to Helm’s official documentation to learn more about Helm: <a href="https://helm.sh/docs/howto/charts_tips_and_tricks/">https://helm.sh/docs/howto/charts_tips_and_tricks/</a>.</p>
<ul>
<li><em class="italic">Where can I find out about Kustomize? </em></li>
</ul>
<p>Go to Helm’s official documentation to learn more about Kustomize: <a href="https://kubectl.docs.kubernetes.io/references/kustomize/">https://kubectl.docs.kubernetes.io/references/kustomize/</a>.</p>
<ul>
<li><em class="italic">What is the recommended official Kubernetes article about init containers? </em></li>
</ul>
<p>I recommend bookmarking this article, <em class="italic">Init Containers</em>: <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">https://kubernetes.io/docs/concepts/workloads/pods/init-containers/</a>.</p>
<ul>
<li><em class="italic">What is your recommended Kubernetes official article for ConfigMaps? </em></li>
</ul>
<p>I recommend bookmarking an article, <em class="italic">ConfigMaps</em>: <a href="https://kubernetes.io/docs/concepts/configuration/configmap/">https://kubernetes.io/docs/concepts/configuration/configmap/</a>.</p>
<ul>
<li><em class="italic">What is your recommended official Kubernetes article for resource management?</em></li>
</ul>
<p>I recommend bookmarking this article, <em class="italic">Resource Management for Pods and Containers</em>: <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>.</p>
</div>
</div></body></html>