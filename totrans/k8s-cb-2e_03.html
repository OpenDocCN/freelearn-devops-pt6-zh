<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Playing with Containers</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following topics: </p>
<ul>
<li>Scaling your containers</li>
<li>Updating live containers</li>
<li>Forwarding container ports</li>
<li>Ensuring flexible usage of your containers</li>
<li>Submitting Jobs on Kubernetes</li>
<li>Working with configuration files</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>When talking about container management, you need to know some of the differences compared to application package management, such as <kbd>rpm</kbd>/<kbd>dpkg</kbd>, because you can run multiple containers on the same machine. You also need to care about network port conflicts. This chapter covers how to update, scale, and launch a container application using Kubernetes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling your containers</h1>
                </header>
            
            <article>
                
<p>Scaling up and down the application or service based on predefined criteria is a common way to utilize the most compute resources in most efficient way. In Kubernetes, you can scale up and down manually or use a <strong>Horizontal Pod Autoscaler</strong> (<strong>HPA</strong>) to do autoscaling. In this section, we'll describe how to perform both operations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">Prepare the following YAML file, which is a simple Deployment that launches two <kbd>nginx</kbd> containers. Also, a NodePort service with TCP—<kbd>30080</kbd> exposed:</p>
<pre># cat 3-1-1_deployment.yaml<br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: my-nginx<br/>spec:<br/>  replicas: 2<br/>  selector:<br/>    matchLabels:<br/>      service : nginx<br/>  template:<br/>    metadata:<br/>      labels:<br/>        service : nginx<br/>    spec:<br/>      containers:<br/>        - name: my-container<br/>          image: nginx<br/>---<br/>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: my-nginx<br/>spec:<br/>  ports:<br/>    - protocol: TCP<br/>      port: 80<br/>      nodePort: 30080<br/>  type: NodePort<br/>  selector:<br/>    service: nginx</pre>
<div class="packt_infobox"><kbd>NodePort</kbd> will bind to all the Kubernetes nodes (port range: <kbd>30000</kbd> to <kbd>32767</kbd>); therefore, make sure <kbd>NodePort</kbd> is not used by other processes.</div>
<p class="mce-root">Let's use <kbd>kubectl</kbd> to create the resources used by the preceding configuration file:</p>
<pre>// create deployment and service<br/># kubectl create -f 3-1-1_deployment.yaml<br/>deployment "my-nginx" created<br/>service "my-nginx" created</pre>
<p class="NormalPACKT">After a few seconds, we should see that the <kbd>pods</kbd> are scheduled and up and running:</p>
<pre># kubectl get pods<br/>NAME READY STATUS RESTARTS AGE<br/>my-nginx-6484b5fc4c-9v7dc 1/1 Running 0 7s<br/>my-nginx-6484b5fc4c-krd7p 1/1 Running 0 7s</pre>
<p class="NormalPACKT">The service is up, too:</p>
<pre># kubectl get services<br/>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE<br/>kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 20d<br/>my-nginx NodePort 10.105.9.153 &lt;none&gt; 80:30080/TCP 59s</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Assume our services are expected to have a traffic spike at a certain of time. As a DevOps, you might want to scale it up manually, and scale it down after the peak time. In Kubernetes, we can use the <kbd>kubectl scale</kbd> command to do so. Alternatively, we could leverage a <span>HPA</span> to scale up and down automatically based on compute resource conditions or custom metrics.</p>
<p>Let's see how to do it manually and automatically in Kubernetes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scale up and down manually with the kubectl scale command</h1>
                </header>
            
            <article>
                
<p>Assume that today we'd like to scale our <kbd>nginx</kbd> Pods from two to four:</p>
<pre>// kubectl scale --replicas=&lt;expected_replica_num&gt; deployment &lt;deployment_name&gt;<br/># kubectl scale --replicas=4 deployment my-nginx<br/>deployment "my-nginx" scaled</pre>
<p class="NormalPACKT">Let's check how many <kbd>pods</kbd> we have now:</p>
<pre># kubectl get pods<br/>NAME READY STATUS RESTARTS AGE<br/>my-nginx-6484b5fc4c-9v7dc 1/1 Running 0 1m<br/>my-nginx-6484b5fc4c-krd7p 1/1 Running 0 1m<br/>my-nginx-6484b5fc4c-nsvzt 0/1 ContainerCreating 0 2s<br/>my-nginx-6484b5fc4c-v68dr 1/1 Running 0 2s</pre>
<p class="NormalPACKT">We could find two more Pods are scheduled. One is already running and another one is creating. Eventually, we will have four Pods up and running if we have enough compute resources.</p>
<div class="packt_tip">
<div>Kubectl scale (also kubectl autoscale!) supports <strong>Replication Controller</strong> (<strong>RC</strong>) and <strong>Replica Set</strong> (<strong>RS</strong>), too. However, deployment is the recommended way to deploy Pods.</div>
</div>
<p class="NormalPACKT">We could also scale down with the same <kbd>kubectl</kbd> command, just by setting the <kbd>replicas</kbd> parameter lower:</p>
<pre>// kubectl scale –replicas=&lt;expected_replica_num&gt; deployment &lt;deployment_name&gt;<br/># kubectl scale --replicas=2 deployment my-nginx<br/>deployment "my-nginx" scaled</pre>
<p class="NormalPACKT">Now, we'll see two Pods are scheduled to be terminated:</p>
<pre># kubectl get pods<br/>NAME READY STATUS RESTARTS AGE<br/>my-nginx-6484b5fc4c-9v7dc 1/1 Running 0 1m<br/>my-nginx-6484b5fc4c-krd7p 1/1 Running 0 1m<br/>my-nginx-6484b5fc4c-nsvzt 0/1 Terminating 0 23s<br/>my-nginx-6484b5fc4c-v68dr 0/1 Terminating 0 23s</pre>
<p class="NormalPACKT">There is an option, <kbd>--current-replicas</kbd>, which specifies the expected current replicas. If it doesn't match, Kubernetes doesn't perform the scale function as follows:</p>
<pre>// adding –-current-replicas to precheck the condistion for scaling.<br/># kubectl scale --current-replicas=3 --replicas=4 deployment my-nginx<br/>error: Expected replicas to be 3, was 2</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Horizontal Pod Autoscaler (HPA)</h1>
                </header>
            
            <article>
                
<p><span>An HPA</span> queries the source of metrics periodically and determines whether scaling is required by a controller based on the metrics it gets. There are two types of metrics that could be fetched; one is from Heapster (<a href="https://github.com/kubernetes/heapster">https://github.com/kubernetes/heapster</a>), another is from RESTful client access. In the following example, we'll show you how to use Heapster to monitor <span>Pods </span>and expose the metrics to an <span>HPA</span>.</p>
<p>First, Heapster has to be deployed in the cluster:</p>
<div class="packt_tip">
<div>If you're running minikube, use the <kbd>minikube addons enable heapster</kbd> command to enable heapster in your cluster. Note that <kbd>minikube logs | grep heapster command</kbd> could also be used to check the logs of heapster.</div>
</div>
<pre>// at the time we're writing this book, the latest configuration file of heapster in kops is 1.7.0. Check out https://github.com/kubernetes/kops/tree/master/addons/monitoring-standalone for the latest version when you use it. <br/># kubectl create -f https://raw.githubusercontent.com/kubernetes/kops/master/addons/monitoring-standalone/v1.7.0.yaml<br/>deployment "heapster" created<br/>service "heapster" created<br/>serviceaccount "heapster" created<br/>clusterrolebinding "heapster" created<br/>rolebinding "heapster-binding" created</pre>
<p class="NormalPACKT">Check if the <kbd>heapster</kbd> <kbd>pods</kbd> are up and running:</p>
<pre># kubectl get pods --all-namespaces | grep heapster<br/>kube-system heapster-56d577b559-dnjvn 2/2 Running 0 26m<br/>kube-system heapster-v1.4.3-6947497b4-jrczl 3/3 Running 0 5d</pre>
<p class="NormalPACKT">Assuming we continue right after the <em>Getting Ready</em> section, we will have two <kbd>my-nginx</kbd> Pods running in our cluster:</p>
<pre># kubectl get pods<br/>NAME READY STATUS RESTARTS AGE<br/>my-nginx-6484b5fc4c-9v7dc 1/1 Running 0 40m<br/>my-nginx-6484b5fc4c-krd7p 1/1 Running 0 40m</pre>
<p>Then, we can use the <kbd>kubectl autoscale</kbd> command to deploy an <span>HPA</span>:</p>
<pre># kubectl autoscale deployment my-nginx --cpu-percent=50 --min=2 --max=5 <br/>deployment "my-nginx" autoscaled <br/># cat 3-1-2_hpa.yaml<br/>apiVersion: autoscaling/v1<br/>kind: HorizontalPodAutoscaler<br/>metadata:<br/>  name: my-nginx<br/>spec:<br/>  scaleTargetRef:<br/>    kind: Deployment<br/>    name: my-nginx<br/>  minReplicas: 2<br/>  maxReplicas: 5<br/>  targetCPUUtilizationPercentage: 50</pre>
<p class="NormalPACKT">To check if it's running as expected:</p>
<pre>// check horizontal pod autoscaler (HPA)<br/># kubectl get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>my-nginx Deployment/my-nginx &lt;unknown&gt; / 50% 2 5 0 3s</pre>
<p class="NormalPACKT">We find the target shows as <span class="ItalicsPACKT">unknown</span> and replicas are <span class="ItalicsPACKT">0</span>. Why is this? the runs as a control loop, at a default interval of 30 seconds. There might be a delay before it reflects the real metrics.</p>
<div class="packt_infobox">
<div>The default sync period of an HPA can be altered by changing the following parameter in control manager: <kbd>--horizontal-pod-autoscaler-sync-period</kbd>.</div>
</div>
<p class="mce-root">After waiting a couple of seconds, we will find the current metrics are there now. The number showed in the target column presents (<kbd>current / target</kbd>). It means the load is currently <kbd>0%</kbd>, and scale target is <kbd>50%</kbd>:</p>
<pre># kubectl get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>my-nginx Deployment/my-nginx 0% / 50% 2 5 2 48m<br/><br/>// check details of a hpa<br/># kubectl describe hpa my-nginx<br/>Name: my-nginx<br/>Namespace: default<br/>Labels: &lt;none&gt;<br/>Annotations: &lt;none&gt;<br/>CreationTimestamp: Mon, 15 Jan 2018 22:48:28 -0500<br/>Reference: Deployment/my-nginx<br/>Metrics: ( current / target )<br/>  resource cpu on pods (as a percentage of request): 0% (0) / 50%<br/>Min replicas: 2<br/>Max replicas: 5</pre>
<p class="NormalPACKT">To test if HPA can scale the <span>Pod </span>properly, we'll manually generate some loads to <kbd>my-nginx</kbd> service:</p>
<pre>// generate the load<br/># kubectl run -it --rm --restart=Never &lt;pod_name&gt; --image=busybox -- sh -c "while true; do wget -O - -q http://my-nginx; done"</pre>
<p>In the preceding command, we ran a <kbd>busybox</kbd> image which allowed us to run a simple command on it. We used the <kbd>–c</kbd> parameter to specify the default command, which is an infinite loop, to query <kbd>my-nginx</kbd> service.</p>
<p>After about one minute, you can see that the current value is changing:</p>
<pre>// check current value – it's 43% now. not exceeding scaling condition yet.<br/># kubectl get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>my-nginx Deployment/my-nginx 43% / 50% 2 5 2 56m</pre>
<p class="NormalPACKT">With the same command, we can run more loads with different Pod names repeatedly. Finally, we see that the condition has been met. It's scaling up to <kbd>3</kbd> replicas, and up to <kbd>4</kbd> replicas afterwards:</p>
<pre># kubectl get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>my-nginx Deployment/my-nginx 73% / 50% 2 5 3 1h<br/><br/># kubectl get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>my-nginx Deployment/my-nginx 87% / 50% 2 5 4 15m<br/>Keeping observing it and deleting some busybox we deployed. It will eventually cool down and scale down without manual operation involved.<br/># kubectl get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>my-nginx Deployment/my-nginx 40% / 50% 2 5 2 27m</pre>
<p class="NormalPACKT">We can see that HPA just scaled our Pods from <kbd>4</kbd> to <kbd>2</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Note that cAdvisor acts as a container resource utilization monitoring service, which is running inside kubelet on each node. The CPU utilizations we just monitored are collected by cAdvisor and aggregated by Heapster. Heapster is a service running in the cluster that monitors and aggregates the metrics. It queries the metrics from each cAdvisor. When HPA is deployed, the controller will keep observing the metrics which are reported by Heapster, and scale up and down accordingly. An illustration of the process is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/290e9c1e-2be5-4a53-a740-802ddb0147da.png" style="width:43.08em;height:28.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Based on the specified metrics, <span>HPA</span> determines whether scaling is required</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There is more…</h1>
                </header>
            
            <article>
                
<p>Alternatively, you could use custom metrics, such as <span>Pod </span>metrics or object metrics, <span>to determine if it's time to scale up or down</span>. Kubernetes also supports multiple metrics. <span>HPA</span> will consider each metric sequentially. Check out <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale</a> for more examples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>This recipe described how to change the number of <span>Pods </span>using the scaling option of the deployment. It is useful to scale up and scale down your application quickly. To know more about how to update your container, refer to the following recipes:</p>
<ul>
<li><em>Updating live containers</em> in <a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml" target="_blank">Chapter 3</a>, <em>Playing with Containers</em></li>
<li><em>Ensuring flexible usage of your containers</em> in <a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml">Chapter 3</a><em>, Playing with Containers</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating live containers</h1>
                </header>
            
            <article>
                
<p>For the benefit of containers, we can easily publish new programs by executing the latest image, and reduce the headache of environment setup. But, what about publishing the program on running containers? While managing a container natively, we have to stop the running containers prior to booting up new ones with the latest images and the same configurations. There are some simple and efficient methods for updating your program in the Kubernetes system. One is called rolling-update, which means Deployment can update its Pods without downtime to clients. The other method is called <em>recreate</em>, which just terminates all Pods then create a new set. We will demonstrate how these solutions are applied in this recipe.</p>
<div class="packt_infobox"><span class="packt_screen">Rolling-update in Docker swarm</span><strong><br/></strong>To achieve zero downtime application updating, there is a similar managing function in Docker swarm. In Docker swarm, you can leverage the command docker service update with the flag <kbd>--update-delay</kbd>, <kbd>--update-parallelism</kbd> and <kbd>--update-failure-action</kbd>. Check the official website for more details about Docker swarm's rolling-update: <a href="https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/">https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>For a later demonstration, we are going to update <kbd>nginx</kbd> <span>Pods </span>. Please make sure all Kubernetes nodes and components are working healthily:</p>
<pre>// check components<br/>$ kubectl get cs<br/>// check nodes<br/>$ kubectl get node</pre>
<p>Furthermore, to well understand the relationship between ReplicaSet and Deployment, please check <em>Deployment API </em>section in <em><a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml">Chapter 2</a>, Walking through Kubernetes Concepts</em>.</p>
<p>To illustrate the updating of the containers in Kubernetes system, we will create a Deployment, change its configurations of application, and then check how the updating mechanism handles it. Let's get all our resources ready:</p>
<pre>// create a simple nginx Deployment with specified labels<br/>$ kubectl run simple-nginx --image=nginx --port=80 --replicas=5 --labels="project=My-Happy-Web,role=frontend,env=test"<br/>deployment.apps "simple-nginx" created</pre>
<p>This Deployment is created with <kbd>5</kbd> replicas. It is good for us to discover the updating procedure with multiple numbers of <span>Pods</span>:</p>
<pre>// expose the Deployment, and named the service "nginx-service"<br/>$ kubectl expose deployment simple-nginx --port=8080 --target-port=80 --name="nginx-service"<br/>service "nginx-service" exposed<br/>// For minikube environment only, since Kubernetes is installed in a VM, add Service type as NodePort for accessing outside the VM.<br/>$ kubectl expose deployment simple-nginx --port=8080 --target-port=80 --name="nginx-service" <strong>--type=NodePort<br/></strong>service "nginx-service" exposed</pre>
<p>Attaching a Service on the Deployment will help to simulate the real experience of clients.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>At the beginning, take a look at the Deployment you just created and its ReplicaSet by executing the following code block:</p>
<pre>$ kubectl describe deployment simple-nginx<br/>Name:                   simple-nginx<br/>Namespace:              default<br/>CreationTimestamp:      Fri, 04 May 2018 12:14:21 -0400<br/>Labels:                 env=test<br/>                        project=My-Happy-Web<br/>                        role=frontend<br/>Annotations:            deployment.kubernetes.io/revision=1<br/>Selector:               env=test,project=My-Happy-Web,role=frontend<br/>Replicas:               5 desired | 5 updated | 5 total | 5 available | 0 unavailable<br/>StrategyType:           <strong>RollingUpdate<br/></strong>MinReadySeconds:        0<br/>RollingUpdateStrategy:  1 max unavailable, 1 max surge<br/>Pod Template:<br/>  Labels:  env=test<br/>           project=My-Happy-Web<br/>           role=frontend<br/>  Containers:<br/>   simple-nginx:<br/>    Image:        nginx<br/>    Port:         80/TCP<br/>    Environment:  &lt;none&gt;<br/>    Mounts:       &lt;none&gt;<br/>  Volumes:        &lt;none&gt;<br/>Conditions:<br/>  Type           Status  Reason<br/>  ----           ------  ------<br/>  Available      True    MinimumReplicasAvailable<br/>  Progressing    True    NewReplicaSetAvailable<br/>OldReplicaSets:  &lt;none&gt;<br/>NewReplicaSet:   <strong>simple-nginx-585f6cddcd</strong> (5/5 replicas created)<br/>Events:<br/>  Type    Reason             Age   From                   Message<br/>  ----    ------             ----  ----                   -------<br/>  Normal  ScalingReplicaSet  1h    deployment-controller  Scaled up replica set simple-nginx-585f6cddcd to 5<br/>// rs is the abbreviated resource key of replicaset<br/>$ kubectl get rs<br/>NAME                      DESIRED   CURRENT   READY     AGE<br/>simple-nginx-585f6cddcd   5         5         5         1h</pre>
<p>Based on the preceding output, we know that the default updating strategy of deployment is rolling-update. Also, there is a single ReplicaSet named <kbd>&lt;Deployment Name&gt;-&lt;hex decimal hash&gt;</kbd> that is created along with the Deployment.</p>
<p>Next, check the content of the current Service endpoint for the sake of verifying our update later:</p>
<pre>// record the cluster IP of Service "nginx-service"<br/>$ export SERVICE_URL=$(kubectl get svc | grep nginx-service | awk '{print $3}'):8080<br/><br/>// For minikube environment only, record the VM host IP and port for the service<br/>$ export SERVICE_URL=$(minikube service nginx-service --url)<br/>$ curl $SERVICE_URL | grep "title"<br/>&lt;title&gt;Welcome to nginx!&lt;/title&gt;</pre>
<p>We will get the welcome message in the title of the HTML response with the original <kbd>nginx</kbd> image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment update strategy – rolling-update</h1>
                </header>
            
            <article>
                
<p>The following will introduce the subcommands <kbd>edit</kbd> and <kbd>set</kbd>, for the purpose of updating the containers under Deployment:</p>
<ol>
<li>First, let's update the <span>Pods </span>in Deployment with a new command:</li>
</ol>
<pre style="padding-left: 90px">// get into editor mode with the command below<br/>// the flag "--record" is for recording the update<br/>// add the command argument as below and save the change<br/>$ kubectl edit deployment simple-nginx --record<br/>spec:<br/>  replicas: 5<br/>  ...<br/>  template:<br/>    ...<br/>    spec:<br/>      containers:<br/>      - image: nginx<br/><strong>        command: <br/></strong><strong> </strong><strong>         - sh<br/></strong><strong>          - -c<br/></strong><strong>          - </strong><strong>echo "Happy Programming with Kubernetes!" &gt; /usr/share/nginx/html/index.html &amp;&amp; service nginx stop &amp;&amp; nginx -g "daemon off;"<br/></strong>        imagePullPolicy: Always<br/>        ...<br/>deployment.extensions "simple-nginx" edited</pre>
<p style="padding-left: 60px">We are not only doing the update; we record this change as well. With the flag <kbd>--record</kbd>, we keep the command line as a tag in revision.</p>
<ol start="2">
<li>After editing the Deployment, check the status of rolling-update with the subcommand <kbd>rollout</kbd> right away:</li>
</ol>
<pre style="padding-left: 90px">// you may see different output on your screen, but definitely has the last line showing update successfully<br/>$ kubectl rollout status deployment simple-nginx<br/>Waiting for rollout to finish: 4 out of 5 new replicas have been updated...<br/>Waiting for rollout to finish: 4 out of 5 new replicas have been updated...<br/>Waiting for rollout to finish: 4 out of 5 new replicas have been updated...<br/>Waiting for rollout to finish: 4 out of 5 new replicas have been updated...<br/>Waiting for rollout to finish: 1 old replicas are pending termination...<br/>Waiting for rollout to finish: 1 old replicas are pending termination...<br/>Waiting for rollout to finish: 1 old replicas are pending termination...<br/>Waiting for rollout to finish: 4 of 5 updated replicas are available...<br/>deployment "simple-nginx" successfully rolled out</pre>
<p style="padding-left: 60px">It is possible that you get several <kbd>Waiting for …</kbd> lines, as shown in the preceding code. They are the standard output showing the status of the update.</p>
<ol start="3">
<li>For whole updating procedures, check the details of the Deployment to list its events:</li>
</ol>
<pre style="padding-left: 90px">// describe the Deployment again<br/>$ kubectl describe deployment simple-nginx<br/>Name:                   simple-nginx<br/>...<br/>Events:<br/>  Type    Reason             Age   From                   Message<br/>  ----    ------             ----  ----                   -------<br/>  Normal  ScalingReplicaSet  1h    deployment-controller  Scaled up replica set simple-nginx-585f6cddcd to 5<br/>  Normal  ScalingReplicaSet  1h    deployment-controller  Scaled up replica set simple-nginx-694f94f77d to 1<br/>  Normal  ScalingReplicaSet  1h    deployment-controller  Scaled down replica set simple-nginx-585f6cddcd to 4<br/>  Normal  ScalingReplicaSet  1h    deployment-controller  Scaled up replica set simple-nginx-694f94f77d to 2<br/>  Normal  ScalingReplicaSet  1h    deployment-controller  Scaled down replica set simple-nginx-585f6cddcd to 3<br/>  Normal  ScalingReplicaSet  1h    deployment-controller  Scaled up replica set simple-nginx-694f94f77d to 3<br/>  Normal  ScalingReplicaSet  1h    deployment-controller  Scaled down replica set simple-nginx-585f6cddcd to 2<br/>  Normal  ScalingReplicaSet  1h    deployment-controller  Scaled up replica set simple-nginx-694f94f77d to 4<br/>  Normal  ScalingReplicaSet  1h    deployment-controller  Scaled down replica set simple-nginx-585f6cddcd to <br/>  Normal  ScalingReplicaSet  1h    deployment-controller  Scaled up replica set simple-nginx-694f94f77d to 5<br/>  Normal  ScalingReplicaSet  1h    deployment-controller  (combined from similar events): Scaled down replica set simple-nginx-585f6cddcd to 0</pre>
<p style="padding-left: 60px">As you see, a new <kbd>replica set simple-nginx-694f94f77d</kbd> is created in the Deployment <kbd>simple-nginx</kbd>. Each time the new ReplicaSet scales one Pod up successfully, the old ReplicaSet will scale one Pod down. The scaling process finishes at the moment that the new ReplicaSet meets the original desired <span>Pod </span>number (as said, <kbd>5</kbd> Pods), and the old ReplicaSet has zero <span>Pods</span>.</p>
<ol start="4">
<li>Go ahead and check the new ReplicaSet and existing Service for this update:</li>
</ol>
<pre style="padding-left: 90px">// look at the new ReplicaSet in detail, you will find it copied the labels of the old one<br/>$ kubectl describe rs simple-nginx-694f94f77d<br/>Name:           simple-nginx-694f94f77d<br/>Namespace:      default<br/><strong>Selector:       env=test,pod-template-hash=2509509338,project=My-Happy-Web,role=frontend<br/></strong>Labels:         env=test<br/>                pod-template-hash=2509509338<br/>                project=My-Happy-Web<br/>                role=frontend<br/>...<br/>// send request to the same endpoint of Service.<br/>$ curl $SERVICE_URL<br/>Happy Programming with Kubernetes!</pre>
<ol start="5">
<li>Let's make another update! This time, use the subcommand <kbd>set</kbd> to modify a specific configuration of a Pod.</li>
<li>To set a new image to certain containers in a Deployment, the subcommand format would look like this: <kbd>kubectl set image deployment &lt;Deployment name&gt; &lt;Container name&gt;=&lt;image name&gt;</kbd>:</li>
</ol>
<pre style="padding-left: 90px">// change the image version with the subcommand "set"<br/>// when describing the deployment, we can know that the container name is the same as the name of the Deployment<br/>// record this change as well<br/>$ kubectl set image deployment simple-nginx simple-nginx=nginx:stable --record<br/>deployment.apps "simple-nginx" image updated</pre>
<div class="packt_tip"><span class="packt_screen">What else could the subcommand "set" help to configure? </span><span><br/>
The subcommand set helps to define the configuration of the application. Until version 1.9, CLI with set could assign or update the following resources:</span>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>Subcommand after set</strong></td>
<td><strong>Acting resource</strong></td>
<td><strong>Updating item</strong></td>
</tr>
<tr>
<td><kbd>env</kbd></td>
<td>Pod</td>
<td>Environment variables</td>
</tr>
<tr>
<td><kbd>image</kbd></td>
<td>Pod</td>
<td>Container image</td>
</tr>
<tr>
<td><kbd>resources</kbd></td>
<td>Pod</td>
<td>Computing resource requirement or limitation</td>
</tr>
<tr>
<td><kbd>selector</kbd></td>
<td>Any resource</td>
<td>Selector</td>
</tr>
<tr>
<td><kbd>serviceaccount</kbd></td>
<td>Any resource</td>
<td>ServiceAccount</td>
</tr>
<tr>
<td><kbd>subject</kbd></td>
<td>RoleBinding or ClusterRoleBinding</td>
<td>User, group, or ServiceAccount</td>
</tr>
</tbody>
</table>
</div>
<ol start="7">
<li>Now, check if the update has finished and whether the image is changed:</li>
</ol>
<pre style="padding-left: 90px">// check update status by rollout<br/>$ kubectl rollout status deployment simple-nginx<br/>...                                            <br/>deployment "simple-nginx" successfully rolled out<br/>// check the image of Pod in simple-nginx<br/>$ kubectl describe deployment simple-nginx<br/>Name:                   simple-nginx<br/>...<br/>Pod Template:<br/>  Labels:  env=test<br/>           project=My-Happy-Web<br/>           role=frontend<br/>  Containers:<br/>   simple-nginx:<br/>    Image:  <strong>nginx:stable<br/></strong>    Port:   80/TCP<br/>    Host Port:  0/TCP<br/>...</pre>
<ol start="8">
<li>You can also check out the ReplicaSets. There should be another one taking responsibility of the <span>Pods </span>for Deployment:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl get rs<br/>NAME                      DESIRED   CURRENT   READY     AGE<br/>simple-nginx-585f6cddcd   0         0         0         1h<br/>simple-nginx-694f94f77d   0         0         0         1h<br/>simple-nginx-b549cc75c    5         5         5         1h</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rollback the update</h1>
                </header>
            
            <article>
                
<p>Kubernetes system records every update for Deployment:</p>
<ol>
<li>We can list all of the revisions with the subcommand <kbd>rollout</kbd>:</li>
</ol>
<pre style="padding-left: 90px">// check the rollout history<br/>$ kubectl rollout history deployment simple-nginx<br/>deployments "simple-nginx"<br/>REVISION  CHANGE-CAUSE<br/>1         &lt;none&gt;<br/>2         kubectl edit deployment simple-nginx --record=true<br/>3         kubectl set image deployment simple-nginx simple-nginx=nginx:stable --record=true</pre>
<p style="padding-left: 60px">You will get three revisions, as in the preceding lines, for the Deployment <kbd>simple-nginx</kbd>. For Kubernetes Deployment, each revision has a matched <kbd>ReplicaSet</kbd> and represents a stage of running an update command. The first revision is the initial state of <kbd>simple-nginx</kbd>. Although there is no command tag for indication, Kubernetes takes its creation as its first version. However, you could still record the command when you create the Deployment.</p>
<ol start="2">
<li>Add the flag <kbd>--record</kbd> after the subcommand <kbd>create</kbd> or <kbd>run</kbd>.</li>
<li>With the revisions, we can easily resume the change, which means rolling back the update. Use the following commands to rollback to previous revisions:</li>
</ol>
<pre style="padding-left: 90px">// let's jump back to initial Deployment!<br/>// with flag --to-revision, we can specify which revision for rollback processing<br/>$ kubectl rollout undo deployment simple-nginx --to-revision=1<br/>deployment.apps "simple-nginx"<br/>// check if the rollback update is finished<br/>$ kubectl rollout status deployment simple-nginx<br/>...                                             <br/>deployment "simple-nginx" successfully rolled out<br/>// take a look at ReplicaSets, you will find that the old ReplicaSet takes charge of the business now<br/>$ kubectl get rs<br/>NAME                      DESIRED   CURRENT   READY     AGE<br/>simple-nginx-585f6cddcd   5         5         5         4h<br/>simple-nginx-694f94f77d   0         0         0         4h<br/>simple-nginx-b549cc75c    0         0         0         3h<br/>// go ahead and check the nginx webpage or the details of Deployment<br/>$ curl $SERVICE_URL<br/>$ kubectl describe deployment simple-nginx</pre>
<ol start="4">
<li>Without specifying the revision number, the rollback process will simply jump back to previous version:</li>
</ol>
<pre style="padding-left: 90px">// just go back to previous status<br/>$ kubectl rollout undo deployment simple-nginx<br/>deployment.apps "simple-nginx"<br/>// look at the ReplicaSets agin, now the latest one takes the job again<br/>$ kubectl get rs<br/>NAME                      DESIRED   CURRENT   READY     AGE<br/>simple-nginx-585f6cddcd   0         0         0         4h<br/>simple-nginx-694f94f77d   0         0         0         4h<br/>simple-nginx-b549cc75c    5         5         5         4h</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment update strategy – recreate</h1>
                </header>
            
            <article>
                
<p>Next, we are going to introduce the other update strategy, <kbd>recreate</kbd>, for Deployment. <span>Although there is no subcommand or flag to create a</span> recreate-strategy<span> deployment, users could fulfill this creation by overriding the default element with the specified configuration:</span></p>
<pre>// create a new Deployment, and override the update strategy.<br/>$ kubectl run recreate-nginx --image=nginx --port=80 --replicas=5 --overrides='{"apiVersion": "apps/v1", "spec": {"strategy": {"type": "Recreate"}}}'<br/>deployment.apps "recreate-nginx" created<br/>// verify our new Deployment<br/>$ kubectl describe deployment recreate-nginx<br/>Name:               recreate-nginx<br/>Namespace:          default<br/>CreationTimestamp:  Sat, 05 May 2018 18:17:07 -0400<br/>Labels:             run=recreate-nginx<br/>Annotations:        deployment.kubernetes.io/revision=1<br/>Selector:           run=recreate-nginx<br/>Replicas:           5 desired | 5 updated | 5 total | 0 available | 5 unavailable<br/>StrategyType:       <strong>Recreate<br/></strong>...</pre>
<p>In our understanding, the <kbd>recreate</kbd> mode is good for an application under development. With <kbd>recreate</kbd>, Kubernetes just scales the current ReplicaSet down to zero <span>Pods</span>, and creates a new ReplicaSet with the full desired number of <span>Pods</span>. Therefore, recreate has a shorter total updating time than rolling-update because it scales ReplicaSets up or down simply, once for all. Since a developing Deployment doesn't need to take care of any user experience, it is acceptable to have downtime while updating and enjoy faster updates:</p>
<pre>// try to update recreate-strategy Deployment<br/>$ kubectl set image deployment recreate-nginx recreate-nginx=nginx:stable<br/>deployment.apps "recreate-nginx" image updated<br/>// check both the rollout status and the events of Deployment<br/>$ kubectl rollout status deployment recreate-nginx<br/>$ kubectl describe deployment recreate-nginx<br/>...<br/>Events:<br/>  Type    Reason             Age   From                   Message<br/>  ----    ------             ----  ----                   -------<br/>  Normal  ScalingReplicaSet  3h    deployment-controller  Scaled up replica set recreate-nginx-9d5b69986 to 5<br/>  Normal  ScalingReplicaSet  2h    deployment-controller  Scaled down replica set recreate-nginx-9d5b69986 to 0<br/>  Normal  ScalingReplicaSet  2h    deployment-controller  Scaled up replica set recreate-nginx-674d7f9c7f to 5</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Rolling-update works on the units of the ReplicaSet in a Deployment. The effect is to create a new ReplicaSet to replace the old one. Then, the new ReplicaSet is scaling up to meet the desired numbers, while the old ReplicaSet is scaling down to terminate all the <span>Pods i</span>n it. The <span>Pods </span>in the new ReplicaSet are attached to the original labels. Therefore, if any service exposes this Deployment, it will take over the newly created <span>Pods </span>directly.</p>
<p>An experienced Kubernetes user may know that the resource ReplicationController can be rolling-update as well. So, what are the differences of rolling-update between ReplicationController and deployment? The scaling processing uses the combination of ReplicationController and a client such as <kbd>kubectl</kbd>. A new ReplicationController will be created to replace the previous one. Clients don't feel any interruption since the service is in front of ReplicationController while doing replacement. However, it is hard for developers to roll back to previous ReplicationControllers (they have been removed), because there is no built-in mechanism that records the history of updates.</p>
<p>In addition, rolling-update might fail if the client connection is disconnected while rolling-update is working. Most important of all, Deployment with ReplicaSet is the most recommended deploying resource than ReplicationController or standalone ReplicaSet.</p>
<p>While paying close attention to the history of update in deployment, be aware that it is not always listed in sequence. The algorithm of adding revisions could be clarified as the following bullet points show:</p>
<ul>
<li>Take the number of last revision as <em>N</em></li>
<li>When a new rollout update comes, it would be <em>N+1</em></li>
<li>Roll back to a specific revision number <em>X</em>, <em>X</em> would be removed and it would become <em>N+1</em></li>
<li>Roll back to the previous version, which means <em>N-1,</em> then <em>N-1</em> would be removed and it would become <em>N+1</em></li>
</ul>
<p>With this revision management, no stale and overlapped updates occupy the rollout history.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Taking Deployment update into consideration is a good step towards building a CI/CD (continuous integration and continuous delivery) pipeline. For a more common usage, developers don't exploit command lines to update the Deployment. They may prefer to fire some API calls from CI/CD platform, or update from a previous configuration file. Here comes an example working with the subcommand <kbd>apply</kbd>:</p>
<pre>// A simple nginx Kubernetes configuration file<br/>$ cat my-update-nginx.yaml<br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: my-update-nginx<br/>spec:<br/>  replicas: 5<br/>  selector:<br/>    matchLabels:<br/>      run: simple-nginx<br/>  template:<br/>    metadata:<br/>      labels:<br/>        run: simple-nginx<br/>    spec:<br/>      containers:<br/>      - name: simple-nginx<br/>        image: nginx<br/>        ports:<br/>        - containerPort: 80<br/><br/>// create the Deployment by file and recording the command in the revision tag<br/>$ kubectl create -f my-update-nginx.yaml --record<br/>deployment.apps "my-update-nginx" created</pre>
<p>As a demonstration, modifying the container image from <kbd>nginx</kbd> to <kbd>nginx:stable</kbd> (you may check the code bundle <kbd>my-update-nginx-updated.yaml</kbd> for the modification). Then, we can use the changed file to update with the subcommand <kbd>apply</kbd>:</p>
<pre>$ kubectl apply -f my-update-nginx-updated.yaml --record<br/>Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply<br/>deployment.apps "my-update-nginx" configured<br/>// check the update revisions and status<br/>$ kubectl rollout history deployment my-update-nginx<br/>deployments "my-update-nginx"<br/>REVISION  CHANGE-CAUSE<br/>1         kubectl create --filename=my-update-nginx.yaml --record=true<br/>2         kubectl apply --filename=my-update-nginx-updated.yaml --record=true<br/>$ kubectl rollout status deployment my-update-nginx<br/>deployment "my-update-nginx" successfully rolled out</pre>
<p>Now, you can learn another way to update your Deployment.</p>
<p>Digging deeper into rolling-update on Deployment, there are some parameters we may leverage when doing updates:</p>
<ul>
<li><kbd>minReadySeconds</kbd>: After a <span>Pod </span>is considered to be ready, the system still waits for a period of time for going on to the next step. This time slot is the minimum ready seconds, which will be helpful when waiting for the application to complete post-configuration.</li>
<li><kbd>maxUnavailable</kbd>: The maximum number of <span>Pods </span>that can be unavailable during updating. The value could be a percentage (the default is 25%) or an integer. If the value of <kbd>maxSurge</kbd> is <kbd>0</kbd>, which means no tolerance of the number of <span>Pods </span>over the desired number, the value of <kbd>maxUnavailable</kbd> cannot be <kbd>0</kbd>.</li>
<li><kbd>maxSurge</kbd>: The maximum number of <span>Pods </span>that can be created over the desired number of ReplicaSet during updating. The value could be a percentage (the default is 25%) or an integer. If the value of <kbd>maxUnavailable</kbd> is <kbd>0</kbd>, which means the number of serving <span>Pods </span>should always meet the desired number, the value of <kbd>maxSurge</kbd> cannot be <kbd>0</kbd>.</li>
</ul>
<p>Based on the configuration file <kbd>my-update-nginx-advanced.yaml</kbd> in the code bundle, try playing with these parameters by yourself and see if you can feel the ideas at work.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>You could continue studying the following recipes to learn more ideas about deploying Kubernetes resources efficiently:</p>
<ul>
<li>Scaling your containers</li>
<li>Working with configuration files</li>
<li>The <em>Moving monolithic to microservices</em>, <em>Integrating with Jenkins</em>, <em>Working with the private Docker registry</em>, and <em>Setting up the Continuous Delivery Pipeline</em> recipes in <a href="669edaf0-c274-48fa-81d8-61150fa36df5.xhtml">Chapter 5</a><em>, Building a Continuous Delivery Pipeline</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Forwarding container ports</h1>
                </header>
            
            <article>
                
<p>In previous chapters, you have learned how to work with the Kubernetes Services to forward the container port internally and externally. Now, it's time to take it a step further to see how it works.</p>
<p>There are four networking models in Kubernetes, and we'll explore the details in the following sections:</p>
<ul>
<li>Container-to-container communications</li>
<li>Pod-to-pod communications</li>
<li>Pod-to-service communications</li>
<li>External-to-internal communications</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Before we go digging into Kubernetes networking, let's study the networking of Docker to understand the basic concept. Each container will have a network namespace with its own routing table and routing policy. By default, the network bridge <kbd>docker0</kbd> connects the physical network interface and virtual network interfaces of containers, and the virtual network interface is the bidirectional cable for the container network namespace and the host one. As a result, there is a pair of virtual network interfaces for a single container: the Ethernet interface (<strong>eth0</strong>) on the container and the virtual Ethernet interface (<strong>veth-</strong>) on the host.</p>
<p>The network structure can be expressed as in the following image:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cbce3365-1d24-4212-9777-b99db15a7bf6.png" style="width:23.58em;height:25.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Container network interfaces on host</div>
<div class="packt_infobox"><span class="packt_screen">What is a network namespace?</span><strong><br/></strong>A network namespace is the technique provided by Linux kernel. With this feature, the operating system can fulfill network virtualization by separating the network capability into independent resources. Each network namespace has its own iptable setup and network devices.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>A Pod contains one or more containers, which run on the same host. Each <span>Pod </span>has their own IP address on an overlay network; all the containers inside a <span>Pod </span>see each other as on the same host. Containers inside a <span>Pod </span>will be created, deployed, and deleted almost at the same time. We will illustrate four communication models between container, Pod, and Service.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Container-to-container communication</h1>
                </header>
            
            <article>
                
<p>In this scenario, we would focus on the communications between containers within single Pod:</p>
<ol>
<li>Let's create two containers in one <span>Pod:</span> a nginx web application and a CentOS, which checks port <kbd>80</kbd> on localhost:</li>
</ol>
<pre style="padding-left: 90px">// configuration file of creating two containers within a pod<br/>$ cat two-container-pod.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: two-container<br/>spec:<br/>  containers:<br/>    - name: web<br/>      image: nginx<br/>      ports:<br/>        - containerPort: 80<br/>          hostPort: 80<br/>    - name: centos<br/>      image: centos<br/>      command: ["/bin/sh", "-c", "while : ;do curl http://localhost:80/; sleep 30; done"]<br/><br/>// create the pod<br/>$ kubectl create -f two-container-pod.yaml<br/>pod "two-container" created<br/>// check the status of the newly-created Pod<br/>$ kubectl get pod two-container<br/>NAME            READY     STATUS    RESTARTS   AGE<br/>two-container   2/2       Running   0          5s</pre>
<p style="padding-left: 60px">We see the count in the <kbd>READY</kbd> column becomes <kbd>2/2</kbd>, since there are two containers inside this Pod.</p>
<ol start="2">
<li>Using the <kbd>kubectl describe</kbd> command, we may see the details of the Pod:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl describe pod two-container<br/>Name:         two-container<br/>Namespace:    default<br/>Node:         <strong>ubuntu02</strong>/192.168.122.102<br/>Start Time:   Sat, 05 May 2018 18:28:22 -0400<br/>Labels:       &lt;none&gt;<br/>Annotations:  &lt;none&gt;<br/>Status:       Running<br/>IP:           <strong>192.168.79.19</strong><strong>8<br/></strong>Containers:<br/>  web:<br/>    Container ID:   docker://e832d294f176f643d604445096439d485d94780faf60eab7ae5d3849cbf15d75<br/>...<br/>  centos:<br/>    Container ID:  docker://9e35275934c1acdcfac4017963dc046f9517a8c1fc972df56ca37e69d7389a72<br/>...</pre>
<p style="padding-left: 60px">We can see that the <span>Pod </span>is run on node <kbd>ubuntu02</kbd> and that its IP is <kbd>192.168.79.198</kbd>.</p>
<ol start="3">
<li>Also, we may find that the Centos container can access the <kbd>nginx</kbd> on localhost:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl logs two-container centos | grep "title"<br/>&lt;title&gt;Welcome to nginx!&lt;/title&gt;<br/>...</pre>
<ol start="4">
<li>Let's log in to node <kbd>ubuntu02</kbd> to check the network setting of these two containers:</li>
</ol>
<pre style="padding-left: 90px">// list containers of the Pod<br/>$ docker ps | grep "two-container"<br/>9e35275934c1        centos                                      "/bin/sh -c 'while..."   11 hours ago        Up 11 hours                             k8s_centos_two-container_default_113e727f-f440-11e7-ac3f-525400a9d353_0<br/>e832d294f176        nginx                                       "nginx -g 'daemon ..."   11 hours ago        Up 11 hours                             k8s_web_two-container_default_113e727f-f440-11e7-ac3f-525400a9d353_0<br/>9b3e9caf5149        gcr.io/google_containers/pause-amd64:3.1    "/pause"                 11 hours ago        Up 11 hours                             k8s_POD_two-container_default_113e727f-f440-11e7-ac3f-525400a9d353_0</pre>
<p style="padding-left: 60px">Now, we know that the two containers created are <kbd>9e35275934c1</kbd> and <kbd>e832d294f176</kbd>. On the other hand, there is another container, <kbd>9b3e9caf5149</kbd>, that is created by Kubernetes with the Docker image <kbd>gcr.io/google_containers/pause-amd64</kbd>. We will introduce it later. Thereafter, we may get a detailed inspection of the containers with the command <kbd>docker inspect</kbd>, and by adding the command <kbd>jq </kbd>(<a href="https://stedolan.github.io/jq/">https://stedolan.github.io/jq/</a>) as a pipeline, we can parse the output information to show network settings only.</p>
<ol start="5">
<li>Taking a look at both containers covered in the same Pod:</li>
</ol>
<pre style="padding-left: 90px">// inspect the nginx container, and use jq to parse it<br/>$ docker inspect e832d294f176 | jq '.[]| {NetworkMode: .HostConfig.NetworkMode, NetworkSettings: .NetworkSettings}'<br/>{<br/>  "NetworkMode": "container:9b3e9caf5149ffb0ec14c1ffc36f94b2dd55b223d0d20e4d48c4e33228103723",<br/>  "NetworkSettings": {<br/>    "Bridge": "",<br/>    "SandboxID": "",<br/>    "HairpinMode": false,<br/>    "LinkLocalIPv6Address": "",<br/>    "LinkLocalIPv6PrefixLen": 0,<br/>    "Ports": {},<br/>    "SandboxKey": "",<br/>    "SecondaryIPAddresses": null,<br/>    "SecondaryIPv6Addresses": null,<br/>    "EndpointID": "",<br/>    "Gateway": "",<br/>    "GlobalIPv6Address": "",<br/>    "GlobalIPv6PrefixLen": 0,<br/>    "IPAddress": "",<br/>    "IPPrefixLen": 0,<br/>    "IPv6Gateway": "",<br/>    "MacAddress": "",<br/>    "Networks": {}<br/>  }<br/>}<br/>// then inspect the centos one<br/>$ docker inspect 9e35275934c1 | jq '.[]| {NetworkMode: .HostConfig.NetworkMode, NetworkSettings: .NetworkSettings}'<br/>{<br/>  "NetworkMode": "container:9b3e9caf5149ffb0ec14c1ffc36f94b2dd55b223d0d20e4d48c4e33228103723",<br/>...</pre>
<p>We can see that both containers have identical network settings; the network mode is set to mapped container mode, leaving the other configurations cleaned. The network bridge container is <kbd>container:9b3e9caf5149ffb0ec14c1ffc36f94b2dd55b223d0d20e4d48c4e33228103723</kbd>. What is this container? It is the one created by Kubernetes, container ID <kbd>9b3e9caf5149</kbd>, with the image <kbd>gcr.io/google_containers/pause-amd64</kbd>.</p>
<div class="packt_infobox"><span class="packt_screen">What does the container "pause" do?</span><strong><br/></strong>Just as its name suggests, this container does nothing but "pause". However, it preserves the network settings, and the Linux network namespace, for the Pod. Anytime the container shutdowns and restarts, the network configuration will still be the same and not need to be recreated, because the "pause" container holds it. You can check its code and Dockerfile at <a href="https://github.com/kubernetes/kubernetes/tree/master/build/pause">https://github.com/kubernetes/kubernetes/tree/master/build/pause</a> for more information.</div>
<p>The "pause" container is a network container, which is created when a <span>Pod </span></p>
<p>is created and used to handle the route of the <span>Pod </span>network. Then, two containers will share the network namespace with pause; that's why they see each other as localhost.</p>
<div class="packt_infobox"><span class="packt_screen">Create a network container in Docker</span><strong><br/></strong>In Docker, you can easily make a container into a network container, sharing its network namespace with another container. Use the command line: <kbd>$ docker run --network=container:&lt;CONTAINER_ID or CONTAINER_NAME&gt; [other options].</kbd> Then, you will be able to start a container which uses the network namespace of the assigned container.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pod-to-Pod communication</h1>
                </header>
            
            <article>
                
<p>As mentioned, containers in a <span>Pod </span>share the same network namespace. And a <span>Pod </span>is the basic computing unit in Kubernetes. Kubernetes assigns an IP to a <span>Pod </span>in its world. Every <span>Pod </span>can see every other with the virtual IP in Kubernetes network. While talking about the communication between <span>Pods </span>, we can separate into two scenarios: <span>Pods </span>that communicate within a node, or <span>Pods </span>that communicate across nodes. For <span>Pods </span>in single node, since they have separate IPs, their transmissions can be held by bridge, same as containers in a Docker node. However, for communication between <span>Pods </span>across nodes, how would be the package routing work while <span>Pod </span>doesn't have the host information (the host IP)?</p>
<p>Kubernetes uses the CNI to handle cluster networking. CNI is a framework for managing connective containers, for assigning or deleting the network resource on a container. While Kubernetes takes CNI as a plugin, users can choose the implementation of CNI on demand. Commonly, there are the following types of CNI:</p>
<ul>
<li><strong>Overlay</strong>: With the technique of packet encapsulation. Every data is wrapped with host IP, so it is routable in the internet. An example is flannel (<a href="https://github.com/coreos/flannel">https://github.com/coreos/flannel</a>).</li>
<li><strong>L3 gateway</strong>: Transmission between containers pass to a gateway node first. The gateway will maintain the routing table to map the container subnet and host IP. An example is Project Calico (<a href="https://www.projectcalico.org/">https://www.projectcalico.org/</a>).</li>
<li><strong>L2 adjacency</strong>: Happening on L2 switching. In Ethernet, two nodes have adjacency if the package can be transmitted directly from source to destination, without passing by other nodes. An example is Cisco ACI (<a href="https://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/kb/b_Kubernetes_Integration_with_ACI.html">https://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/kb/b_Kubernetes_Integration_with_ACI.html</a>).</li>
</ul>
<p>There are pros and cons to every type of CNI. The former type within the bullet points has better scalability but bad performance, while the latter one has a shorter latency but requires complex and customized setup. Some CNIs cover all three types in different modes, for example, Contiv (<a href="https://github.com/contiv/netplugin">https://github.com/contiv/netplugin</a>). You can get more information about CNI while checking its spec at: <a href="https://github.com/containernetworking/cni">https://github.com/containernetworking/cni</a>. Additionally, look at the CNI list on official website of Kubernetes to try out these CNIs: <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this">https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this</a>.</p>
<p>After introducing the basic knowledge of the packet transaction between <span>Pods </span>, we will continue to bring you a Kubernetes API, <kbd>NetworkPolicy</kbd>, which provides advanced management between the communication of <span>Pods </span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with NetworkPolicy</h1>
                </header>
            
            <article>
                
<p>As a resource of Kubernetes, NetworkPolicy uses label selectors to configure the firewall of <span>Pods </span>from infrastructure level. Without a specified NetworkPolicy, any <span>Pod </span>in the same cluster can communicate with each other by default. On the other hand, once a NetworkPolicy with rules is attached to a Pod, either it is for ingress or egress, or both, and all traffic that doesn't follow the rules will be blocked.</p>
<p>Before demonstrating how to build a NetworkPolicy, we should make sure the network plugin in Kubernetes cluster supports it. There are several CNIs that support NetworkPolicy: Calico, Contive, Romana (<a href="https://github.com/romana/kube">https://github.com/romana/kube</a>), Weave Net (<a href="https://github.com/weaveworks/weave">https://github.com/weaveworks/weave</a>), Trireme (<a href="https://github.com/aporeto-inc/trireme-kubernetes">https://github.com/aporeto-inc/trireme-kubernetes</a>), and others.</p>
<div class="packt_tip"><span class="packt_screen">Enable CNI with NetworkPolicy support as network plugin in minikube</span><strong><br/></strong><span>While working on minikube, users will not need to attach a CNI specifically, since it is designed as a single local Kubernetes node. However, to enable the functionality of</span> NetworkPolicy<span>, it is necessary to start a</span> NetworkPolicy-supported <span>CNI. Be careful, as, while you configure the minikube with CNI, the configuration options and procedures could be quite different to various CNI implementations. The following steps show you how to start minikube with CNI, Calico:</span>
<ol>
<li><span>We take this issue <a href="https://github.com/projectcalico/calico/issues/1013#issuecomment-325689943">https://github.com/projectcalico/calico/issues/1013#issuecomment-325689943</a> as reference for these building steps.</span></li>
<li><span>The minikube used here is the latest version, 0.24.1.</span></li>
<li><span>Reboot your minikube: <kbd>minikube start --network-plugin=cni \</kbd><br/></span><kbd><span>--host-only-cidr 172.17.17.1/24 \<br/></span><span>--extra-config=kubelet.PodCIDR=192.168.0.0/16 \<br/></span><span>--extra-config=proxy.ClusterCIDR=192.168.0.0/16 \<br/></span></kbd><span><kbd>--extra-config=controller-manager.ClusterCIDR=192.168.0.0/16</kbd>.</span></li>
<li><span>Create Calico with the configuration file "minikube-calico.yaml" from the code bundle <kbd>kubectl create -f minikue-calico.yaml</kbd>.</span></li>
</ol>
</div>
<p>To illustrate the functionality of NetworkPolicy, we are going to create a Pod and expose it as a service, then attach a NetworkPolicy on the Pod to see what happens:</p>
<pre>// start a pod of our favourite example, nginx<br/>$ kubectl run nginx-pod --image=nginx --port=80 --restart=Never<br/>pod "nginx-pod" created<br/>//expose the pod as a service listening on port 8080<br/>$ kubectl expose pod nginx-pod --port=8080 --target-port=80<br/>service "nginx-pod" exposed<br/>// check the service IP<br/>$ kubectl get svc<br/>NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE<br/>kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    1h<br/>nginx-pod    ClusterIP   <strong>10.102.153.182</strong>   &lt;none&gt;        8080/TCP   1m</pre>
<p>Now, we can go ahead and check the Pod's connection from a simple Deployment, <kbd>busybox</kbd>, using the command <kbd>wget</kbd> with <kbd>--spider</kbd> flag to verify the existence of endpoint:</p>
<pre>// check the accessibility of the service<br/>// create busybox and open standard input and independent terminal by flag "i" and "t", similar to docker command<br/>$ kubectl run busybox -it --image=busybox /bin/sh<br/>If you don't see a command prompt, try pressing enter.<br/>/ # wget --spider 10.102.153.182:8080<br/>Connecting to 10.102.153.182:8080 (10.102.153.182:8080)</pre>
<p>As shown in the preceding result, we know that the <kbd>nginx</kbd> service can be accessed without any constraints. Later, let's run a <kbd>NetworkPolicy</kbd> that restricts that only the <span>Pod </span>tagging <kbd>&lt;test: inbound&gt;</kbd> can access <kbd>nginx</kbd> service:</p>
<pre>// a configuration file defining NetworkPolicy of pod nginx-pod<br/>$ cat networkpolicy.yaml<br/>kind: NetworkPolicy<br/>apiVersion: networking.k8s.io/v1<br/>metadata:<br/>  name: nginx-networkpolicy<br/>spec:<br/>  podSelector:<br/>    matchLabels:<br/>      <strong>run: nginx-pod<br/></strong>  ingress:<br/>    - from:<br/>      - podSelector:<br/>          matchLabels:<br/>            test: inbound</pre>
<p>As you can see, in the spec of NeworkPolicy, it is configured to apply to <span>Pods </span>with the label <kbd>&lt;run: nginx-pod&gt;</kbd>, which is the one we have on the <kbd>pod nginx-pod</kbd>. Also, a rule of ingress is attached in the policy, which indicates that only <span>Pods </span>with a specific label can access <kbd>nginx-pod</kbd>:</p>
<pre>// create the NetworkPolicy<br/>$ kubectl create -f networkpolicy.yaml<br/>networkpolicy.networking.k8s.io "nginx-networkpolicy" created<br/>// check the details of NetworkPolicy<br/>$ kubectl describe networkpolicy nginx-networkpolicy<br/>Name:         nginx-networkpolicy<br/>Namespace:    default<br/>Created on:   2018-05-05 18:36:56 -0400 EDT<br/>Labels:       &lt;none&gt;<br/>Annotations:  &lt;none&gt;<br/>Spec:<br/>  PodSelector:     run=nginx-pod<br/>  Allowing ingress traffic:<br/>    To Port: &lt;any&gt; (traffic allowed to all ports)<br/>    From PodSelector: test=inbound<br/>  Allowing egress traffic:<br/>    &lt;none&gt; (Selected pods are isolated for egress connectivity)<br/>  Policy Types: Ingress</pre>
<p>Great, everything is looking just like what we expected. Next, check the same service endpoint on our previous <kbd>busybox</kbd> Pod:</p>
<pre>// if you turned off the terminal, resume it with the subcommand attach<br/>$ kubectl attach busybox-598b87455b-s2mfq -c busybox -i -t<br/>// we add flag to specify timeout interval, otherwise it will just keep hanging on wget<br/>/ # wget --spider 10.102.153.182:8080 --timeout=3<br/>wget: download timed out</pre>
<p>As expected again, now we cannot access the <kbd>nginx-pod</kbd> service after NetworkPolicy is attached. The <kbd>nginx-pod</kbd> can only be touched by Pod labelled with <kbd>&lt;test: inbound&gt;</kbd>:</p>
<pre>// verify the connection by yourself with new busybox<br/>$ kubectl run busybox-labelled --labels="test=inbound" -it --image=busybox /bin/sh</pre>
<div class="packt_infobox">Catch up with the concept of label and selector in the recipe <em>Working with labels and selectors</em> in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml">Chapter 2</a><em>,</em> <em>Walking through Kubernetes Concepts</em>.</div>
<p class="mce-root">In this case, you have learned how to create a NetworkPolicy with ingress restriction by <span>Pod </span>selector. Still, there are other settings you may like to build on your Pod:</p>
<ul>
<li><strong>Egress restriction</strong>: Egress rules can be applied by <kbd>.spec.egress</kbd>, which has similar settings to ingress.</li>
<li><strong>Port restriction</strong>: Each ingress and egress rule can point out what port, and with what kind of port protocol, is to be accepted or blocked. Port configuration can be applied through <kbd>.spec.ingress.ports</kbd> or <kbd>.spec.egress.ports</kbd>.</li>
<li><strong>Namespace selector</strong>: We can also make limitations on certain Namespaces. For example, <span>Pods </span>for the system daemon might only allow access to others in the Namespace <kbd>kube-system</kbd>. Namespace selector can be applied with <kbd>.spec.ingress.from.namespaceSelector</kbd> or <kbd>.spec.egress.to.namespaceSelector</kbd>.</li>
<li><strong>IP block</strong>: A more customized configuration is to set rules on certain CIDR ranges, which come out as similar ideas to what we work with iptables. We may utilize this configuration through <kbd>.spec.ingress.from.ipBlock</kbd> or <kbd>.spec.egress.to.ipBlock</kbd>.</li>
</ul>
<p>It is recommended to check more details in the API document: <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#networkpolicyspec-v1-networking">https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#networkpolicyspec-v1-networking</a>. Furthermore, we would like to show you some more interesting setups to fulfill general situations:</p>
<ul>
<li><strong>Apply to all Pod</strong>: A NetworkPolicy can be easily pushed to every <span>Pod </span>by setting <kbd>.spec.podSelector</kbd> with an empty value.</li>
<li><strong>Allow all traffic</strong>: We may allow all incoming traffic by assigning <kbd>.spec.ingress</kbd> with empty value, an empty array; accordingly, outgoing traffic could be set without any restriction by assigning <kbd>.spec.egress</kbd> with empty value.</li>
<li><strong>Deny all traffic</strong>: We may deny all incoming or outgoing traffic by simply indicating the type of NetworkPolicy without setting any rule. The type of the NetworkPolicy can be set at <kbd>.spec.policyTypes</kbd>. At the same time, do not set <kbd>.spec.ingress or .spec.egress</kbd>.</li>
</ul>
<p>Go check the code bundle for the example files <kbd>networkpolicy-allow-all.yaml</kbd> and <kbd>networkpolicy-deny-all.yaml</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pod-to-Service communication</h1>
                </header>
            
            <article>
                
<p>In the ordinary course of events, <span>Pods </span>can be stopped accidentally. Then, the IP of the <span>Pod </span>can be changed. When we expose the port for a <span>Pod </span>or a Deployment, we create a Kubernetes Service that acts as a proxy or a load balancer. Kubernetes would create a virtual IP, which receives the request from clients and proxies the traffic to the <span>Pods </span>in a service. Let's review how to do this:</p>
<ol>
<li>First, we would create a Deployment and expose it to a Service:</li>
</ol>
<pre style="padding-left: 90px">$ cat nodeport-deployment.yaml<br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: nodeport-deploy<br/>spec:<br/>  replicas: 2<br/>  selector:<br/>    matchLabels:<br/>      app: nginx<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: nginx<br/>    spec:<br/>      containers:<br/>      - name: my-nginx<br/>        image: nginx<br/>---<br/>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: nodeport-svc<br/>spec:<br/>  type: NodePort<br/>  selector:<br/>    app: nginx<br/>  ports:<br/>    - protocol: TCP<br/>      port: 8080<br/>      targetPort: 80<br/>$ kubectl create -f nodeport-deployment.yaml<br/>deployment.apps "nodeport-deploy" created<br/>service "nodeport-svc" created</pre>
<ol start="2">
<li>At this moment, check the details of the Service with the subcommand <kbd>describe</kbd>:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl describe service nodeport-svc<br/>Name:                     nodeport-svc<br/>Namespace:                default<br/>Labels:                   &lt;none&gt;<br/>Annotations:              &lt;none&gt;<br/>Selector:                 app=nginx<br/>Type:                     NodePort<br/>IP:                       10.101.160.245<br/>Port:                     &lt;unset&gt;  8080/TCP<br/>TargetPort:               80/TCP<br/>NodePort:                 &lt;unset&gt;  30615/TCP<br/>Endpoints:                192.168.80.5:80,192.168.80.6:80<br/>Session Affinity:         None<br/>External Traffic Policy:  Cluster<br/>Events:                   &lt;none&gt;</pre>
<p class="mce-root" style="padding-left: 60px">The virtual IP of the Service is <kbd>10.101.160.245</kbd>, which exposes the port <kbd>8080</kbd>. The Service would then dispatch the traffic into the two endpoints <kbd>192.168.80.5:80</kbd> and <kbd>192.168.80.6:80</kbd>. Moreover, because the Service is created in <kbd>NodePort</kbd> type, clients can access this Service on every Kubernetes node at <kbd>&lt;NODE_IP&gt;:30615</kbd>. As with our understanding of the recipe <em>Working with Services</em> in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml">Chapter 2</a>, <span><em>Walking through Kubernetes Concepts</em>, </span>it is the Kubernetes daemon <kbd>kube-proxy</kbd> that helps to maintain and update routing policy on every node.</p>
<ol start="3">
<li class="mce-root">Continue on, checking the <kbd>iptable</kbd> on any Kubernetes node:</li>
</ol>
<div class="packt_infobox">Attention! If you are in minikube environment, you should jump into the node with the command <kbd>minikube ssh</kbd>.</div>
<pre style="padding-left: 90px">// Take a look at following marked "Chain"<br/>$ sudo iptables -t nat -nL<br/>...<br/>Chain <strong>KUBE-NODEPORTS</strong> (1 references)<br/>target     prot opt source               destination<br/>KUBE-MARK-MASQ  tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/nodeport-svc: */ tcp dpt:30615<br/>KUBE-SVC-GFPAJ7EGCNM4QF4H  tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/nodeport-svc: */ tcp dpt:30615<br/>...<br/>Chain <strong>KUBE-SEP-DIS6NYZTQKZ5ALQS</strong> (1 references)<br/>target     prot opt source               destination<br/>KUBE-MARK-MASQ  all  --  192.168.80.6         0.0.0.0/0            /* default/nodeport-svc: */<br/>DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/nodeport-svc: */ tcp to:192.168.80.6:80<br/>...<br/>Chain <strong>KUBE-SEP-TC6HXYYMMLGUSFNZ</strong> (1 references)<br/>target     prot opt source               destination<br/>KUBE-MARK-MASQ  all  --  192.168.80.5         0.0.0.0/0            /* default/nodeport-svc: */<br/>DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/nodeport-svc: */ tcp to:192.168.80.5:80<br/>Chain <strong>KUBE-SERVICES</strong> (2 references)<br/>target     prot opt source               destination<br/>...<br/><strong>KUBE-SVC-GFPAJ7EGCNM4QF4H</strong>  tcp  --  0.0.0.0/0            10.101.160.245       /* default/nodeport-svc: cluster IP */ tcp dpt:8080<br/>...<br/>KUBE-NODEPORTS  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL<br/>...<br/>Chain <strong>KUBE-SVC-GFPAJ7EGCNM4QF4H</strong> (2 references)<br/>target     prot opt source               destination<br/>KUBE-SEP-TC6HXYYMMLGUSFNZ  all  --  0.0.0.0/0            0.0.0.0/0            /* default/nodeport-svc: */ statistic mode random probability 0.50000000000<br/>KUBE-SEP-DIS6NYZTQKZ5ALQS  all  --  0.0.0.0/0            0.0.0.0/0            /* default/nodeport-svc: */<br/>...</pre>
<p>There will be a lot of rules showing out. To focus on policies related to the Service <kbd>nodeport-svc</kbd>, go through the following steps for checking them all. The output on your screen may not be listed in the expected order:</p>
<ol>
<li>Find targets under chain <kbd>KUBE-NODEPORTS</kbd> with the comment mentioned <kbd>nodeport-svc</kbd>. One target will be named with the prefix <kbd>KUBE-SVC-</kbd>. In the preceding output, it is the one named <kbd>KUBE-SVC-GFPAJ7EGCNM4QF4H</kbd>. Along with the other target <kbd>KUBE-MARK-MASQ</kbd>, they work on passing traffics at port <kbd>30615</kbd> to the Service.</li>
<li>Find a specific target named <kbd>KUBE-SVC-XXX</kbd> under <kbd>Chain KUBE-SERVICES</kbd>. In this case, it is the target named <kbd>KUBE-SVC-GFPAJ7EGCNM4QF4H</kbd>, ruled as allowing traffics from "everywhere" to the endpoint of <kbd>nodeport-svc</kbd>, <kbd>10.160.245:8080</kbd>.</li>
<li>Find targets under the specific <kbd>Chain KUBE-SVC-XXX</kbd>. In this case, it is <kbd>Chain KUBE-SVC-GFPAJ7EGCNM4QF4H</kbd>. Under the Service chain, you will have number of targets based on the according <span>Pods </span>with the prefix <kbd>KUBE-SEP-</kbd>. In the preceding output, they are <kbd>KUBE-SEP-TC6HXYYMMLGUSFNZ</kbd> and <kbd>KUBE-SEP-DIS6NYZTQKZ5ALQS</kbd>.</li>
<li>Find targets under specific <kbd>Chain KUBE-SEP-YYY</kbd>. In this case, the two chains required to take a look are <kbd>Chain KUBE-SEP-TC6HXYYMMLGUSFNZ</kbd> and <kbd>Chain KUBE-SEP-DIS6NYZTQKZ5ALQS</kbd>. Each of them covers two targets, <kbd>KUBE-MARK-MASQ</kbd> and <kbd>DNAT</kbd>, for incoming and outgoing traffics between "everywhere" to the endpoint of Pod, <kbd>192.168.80.5:80</kbd> or <kbd>192.168.80.6:80</kbd>.</li>
</ol>
<p>One key point here is that the Service target <kbd>KUBE-SVC-GFPAJ7EGCNM4QF4H</kbd> exposing its cluster IP to outside world will dispatch the traffic to chain <kbd>KUBE-SEP-TC6HXYYMMLGUSFNZ</kbd> and <kbd>KUBE-SEP-DIS6NYZTQKZ5ALQS</kbd> with a statistic mode random probability of 0.5. Both chains have DNAT targets that work on changing the destination IP of the packets to the private subnet one, the one of a specific Pod.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">External-to-internal communication</h1>
                </header>
            
            <article>
                
<p>To publish applications in Kubernetes, we can leverage either Kubernetes Service, with type <kbd>NodePort</kbd> or <kbd>LoadBalancer</kbd>, or Kubernetes Ingress. For NodePort service, as introduced in previous section, the port number of the node will be a pair with the Service. Like the following diagram, port <kbd>30361</kbd> on both node 1 and node 2 points to Service A, which dispatch the traffics to <span>Pod</span>1 and a <span>Pod </span>with static probability.</p>
<p>LoadBalancer Service, as you may have learned from the recipe <em>Working with Services</em> in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml">Chapter 2</a><em>, Walking through Kubernetes Concepts</em>, includes the configurations of NodePort. Moreover, a LoadBalancer Service can work with an external load balancer, providing users with the functionality to integrate load balancing procedures between cloud infrastructure and Kubernetes resource, such as the settings <kbd>healthCheckNodePort</kbd> and <kbd>externalTrafficPolicy</kbd>. <strong>Service B</strong> in the following image is a LoadBalancer Service. Internally, <strong>Service B</strong> works the same as <strong>Service A</strong>, relying on <strong>iptables</strong> to redirect packets to Pod; Externally, cloud load balancer doesn't realize <span>Pod </span>or container, it only dispatches the traffic by the number of nodes. No matter which node is chosen to get the request, it would still be able to pass packets to the right Pod:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/161638c9-b9f8-44e4-84e5-48ab67514b83.png" style="width:31.83em;height:27.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Kubernetes Services with type NodePort and type LoadBalancer</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with Ingress</h1>
                </header>
            
            <article>
                
<p>Walking through the journey of Kubernetes networking, users get the idea that each <span>Pod </span>and Service has its private IP and corresponding port to listen on request. In practice, developers may deliver the endpoint of service, the private IP or Kubernetes DNS name, for internal clients; or, developers may expose Services externally by type of NodePort or LoadBalancer. Although the endpoint of Service is more stable than Pod, the Services are offered separately, and clients should record the IPs without much meaning to them. In this section, we will introduce <kbd>Ingress</kbd>, a resource that makes your Services work as a group. More than that, we could easily pack our service union as an API server while we set Ingress rules to recognize the different URLs, and then ingress controller works for passing the request to specific Services based on the rules.</p>
<p>Before we try on Kubernetes Ingress, we should create an ingress controller in cluster. Different from other controllers in <kbd>kube-controller-manager</kbd><em> </em>(<a href="https://kubernetes.io/docs/reference/generated/kube-controller-manager/">https://kubernetes.io/docs/reference/generated/kube-controller-manager/</a>), ingress controller is run by custom implementation instead of working as a daemon. In the latest Kubernetes version, 1.10, nginx ingress controller is the most stable one and also generally supports many platforms. Check the official documents for the details of deployment: <a href="https://github.com/kubernetes/ingress-nginx/blob/master/README.md">https://github.com/kubernetes/ingress-nginx/blob/master/README.md</a>. We will only demonstrate our example on minikube; please see the following information box for the setup of the ingress controller.</p>
<div class="packt_infobox"><span class="packt_screen">Enable Ingress functionality in minikube</span><strong><br/></strong>Ingress in minikube is an add-on function. Follow these steps to start this feature in your environment:
<ol>
<li>Check if the add-on ingress is enabled or not: Fire the command <kbd>minikube addons list</kbd> on your terminal. If it is not enabled, means it shows <kbd>ingress: disabled</kbd>, you should keep follow below steps.</li>
<li>Enable ingress: Enter the command <kbd>minikube addons enable ingress</kbd>, you will see an output like <kbd>ingress was successfully enabled</kbd>.</li>
<li>Check the add-on list again to verify that the last step does work. We expect that the field ingress shows as <kbd>enabled</kbd>.</li>
</ol>
</div>
<p>Here comes an example to demonstrate how to work with Ingress. We would run up two Deployments and their Services, and an additional Ingress to expose them as a union. In the beginning, we would add a new hostname in the host file of Kubernetes master. It is a simple way for our demonstration. If you work on the production environment, a general use case is that the hostname should be added as a record in the DNS server:</p>
<pre>// add a dummy hostname in local host file<br/>$ sudo sh -c "echo `minikube ip` happy.k8s.io &gt;&gt; /etc/hosts"</pre>
<p>Our first Kubernetes Deployment and Service would be <kbd>echoserver</kbd>, a dummy Service showing server and request information. For the other pair of Deployment and Service, we would reuse the NodePort Service example from the previous section:</p>
<pre>$ cat echoserver.yaml<br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: echoserver-deploy<br/>spec:<br/>  replicas: 2<br/>  selector:<br/>    matchLabels:<br/>      app: echo<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: echo<br/>    spec:<br/>      containers:<br/>        - name: my-echo<br/>          image: gcr.io/google_containers/echoserver:1.8<br/>---<br/>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: echoserver-svc<br/>spec:<br/>  selector:<br/>    app: echo<br/>  ports:<br/>    - protocol: TCP<br/>      port: 8080<br/>      targetPort: 8080</pre>
<p>Go ahead and create both set of resources through configuration files:</p>
<pre>$ kubectl create -f echoserver.yaml<br/>deployment.apps "echoserver-deploy" created<br/>service "echoserver-svc" created<br/>$ kubectl create -f nodeport-deployment.yaml<br/>deployment.apps "nodeport-deploy" created<br/>service "nodeport-svc" created</pre>
<p>Our first Ingress makes two Services that listen at the separate URLs <kbd>/nginx</kbd> and <kbd>/echoserver</kbd>, with the hostname <kbd>happy.k8s.io</kbd>, the dummy one we added in the local host file. We use annotation <kbd>rewrite-target</kbd> to guarantee that traffic redirection starts from root, <kbd>/</kbd>. Otherwise, the client may get page not found because of surfing the wrong path. More annotations we may play with are listed at <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md">https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md</a>:</p>
<pre>$ cat ingress.yaml<br/>apiVersion: extensions/v1beta1<br/>kind: Ingress<br/>metadata:<br/>  name: happy-ingress<br/>  <strong>annotations:<br/></strong><strong>    nginx.ingress.kubernetes.io/rewrite-target:<br/></strong>spec:<br/>  rules:<br/>    - host: happy.k8s.io<br/>      http:<br/>        paths:<br/>          - path: /nginx<br/>            backend:<br/>              serviceName: nodeport-svc<br/>              servicePort: 8080<br/>          - path: /echoserver<br/>            backend:<br/>              serviceName: echoserver-svc<br/>              servicePort: 8080</pre>
<p>Then, just create the Ingress and check its information right away:</p>
<pre>$ kubectl create -f ingress.yaml<br/>ingress.extensions "happy-ingress" created<br/>// "ing" is the abbreviation of "ingress"<br/>$ kubectl describe ing happy-ingress<br/>Name:             happy-ingress<br/>Namespace:        default<br/>Address:<br/>Default backend:  default-http-backend:80 (172.17.0.3:8080)<br/>Rules:<br/>  Host          Path  Backends<br/>  ----          ----  --------<br/>  happy.k8s.io<br/>                /nginx        nodeport-svc:8080 (&lt;none&gt;)<br/>                /echoserver   echoserver-svc:8080 (&lt;none&gt;)<br/>Annotations:<br/>  nginx.ingress.kubernetes.io/rewrite-target<br/>Events:<br/>  Type    Reason  Age   From                Message<br/>  ----    ------  ----  ----                -------<br/>  Normal  CREATE  14s   ingress-controller  Ingress default/happy-ingress</pre>
<p>You may find that there is no IP address in the field of description. It will be attached after the first DNS lookup:</p>
<pre>// verify the URL set in ingress rules<br/>$ curl http://happy.k8s.io/nginx<br/>...<br/>&lt;title&gt;Welcome to nginx!&lt;/title&gt;<br/>...<br/>$ curl http://happy.k8s.io/echoserver<br/>Hostname: echoserver-deploy-5598f5796f-d8cr4<br/>Pod Information:<br/>     -no pod information available-<br/>Server values:<br/>     server_version=nginx: 1.13.3 - lua: 10008<br/>...<br/>// the IP address would be added after connection<br/>$ kubectl get ing<br/>NAME            HOSTS          ADDRESS        PORTS     AGE<br/>happy-ingress   happy.k8s.io   192.168.64.4   80        1m</pre>
<p>Although working with Ingress is not as straightforward as other resources, as you have to start an ingress controller implementation by yourself, it still makes our application exposed and flexible. There are many network features coming that are more stable and user friendly. Keep up with the latest updates and have fun!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>In the last part of external-to-internal communication, we learned about Kubernetes Ingress, the resource that makes services work as a union and dispatches requests to target services. Does any similar idea jump into your mind? It sounds like a microservice, the application structure with several loosely coupled services. A complicated application would be distributed to multiple lighter services. Each service is developed independently while all of them can cover original functions. Numerous working units, such as <span>Pods </span>in Kubernetes, run volatile and can be dynamically scheduled on Services by the system controller. However, such a multi-layered structure increases the complexity of networking and also suffers potential overhead costs.</p>
<p>External load balancers are not aware the existence of <span>Pods</span>; they only balance the workload to hosts. A host without any served <span>Pod </span>running would then redirect the loading to other hosts. This situation comes out of a user's expectation for fair load balancing. Moreover, a <span>Pod </span>may crash accidentally, in which case it is difficult to do failover and complete the request. To make up the shortcomings, the idea of a service mesh focus on the networking management of microservice was born, dedicated to delivering more reliable and performant communications on orchestration like Kubernetes:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/efbc7c62-a23c-41ed-95d0-4c5dd0540871.png" style="width:37.50em;height:36.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Simpe service mesh structure</div>
<p>The preceding diagram illustrates the main components in a service mesh. They work together to achieve features as follows:</p>
<ul>
<li><strong>Service mesh ingress</strong>: Using applied Ingress rules to decide which Service should handle the incoming requests. It could also be a proxy that is able to check the runtime policies.</li>
<li><strong>Service mesh proxy</strong>: Proxies on every node not only direct the packets, but can also be used as an advisory agent reporting the overall status of the Services.</li>
<li><strong>Service mesh service discovery pool</strong>: Serving the central management for mesh and pushing controls over proxies. Its responsibility includes procedures of network capability, authentication, failover, and load balancing.</li>
</ul>
<p>Although well-known service mesh implementations such as Linkerd (<a href="https://linkerd.io">https://linkerd.io</a>) and Istio (<a href="https://istio.io">https://istio.io</a>) are not mature enough for production usage, the idea of service mesh is not ignorable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>Kubernetes forwards ports based on the overlay network. In this chapter, we also run <span>Pods </span>and Services with nginx. Reviewing the previous sections will help you to understand more about how to manipulate it. Also, look at the following recipes:</p>
<ul>
<li>The <em>Creating an overlay network</em> and<em> </em><em>Running your first container in Kubernetes</em> recipes in <a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml">Chap</a><a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml">ter</a> <a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml">1</a>, <em>Building Your Own Kubernetes <span>Cluster<br/></span></em></li>
<li>The <em>Working with <span>Pods </span></em>and<em> </em><em>Working with Services</em> recipes in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml">Chapter 2</a><em><em>, Walking through Kubernetes Concepts</em></em></li>
<li>The <em>Moving monolithic to microservices</em> recipe in <a href="669edaf0-c274-48fa-81d8-61150fa36df5.xhtml">Chapter 5</a><em>, Building Continuous Delivery Pipelines</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensuring flexible usage of your containers</h1>
                </header>
            
            <article>
                
<p>Pod, in Kubernetes, means a set of containers, which is also the smallest computing unit. You may have know about the basic usage of <span>Pod </span>in the previous recipes. Pods are usually managed by deployments and exposed by services; they work as applications with this scenario.</p>
<p>In this recipe, we will discuss two new features: <strong>DaemonSets</strong> and <strong>StatefulSets</strong>. These two features can manage <span>Pods </span>with more specific purpose.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>What are <strong>Daemon-like <span>Pod </span></strong>and <strong>Stateful Pod</strong>? The regular Pods in Kubernetes will determine and dispatch to particular Kubernetes nodes based on current node resource usage and your configuration.</p>
<p>However, a <strong>Daemon-like <span>Pod </span></strong>will be created in each node. For example, if you have three nodes, three daemon-like <span>Pods </span>will be created and deployed to each node. Whenever a new node is added, DaemonSets Pod will be deployed to the new node automatically. Therefore, it will be useful to use node level monitoring or log correction.</p>
<p>On the other hand, a <strong>Stateful <span>Pod </span></strong>will stick to some resources such as network identifier (<span>Pod </span>name and DNS) and <strong>p</strong><span><strong>ersistent volume</strong> (<strong>PV</strong>)</span>. This also guarantees an order during deployment of multiple <span>Pods </span>and during rolling update. For example, if you deploy a <span>Pod </span>named <kbd>my-pod</kbd>, and set the scale to <strong>4</strong>, then <span>Pod </span>name will be assigned as <kbd>my-pod-0</kbd>, <kbd>my-pod-1</kbd>, <kbd>my-pod-2</kbd>, and <kbd>my-pod-3</kbd>. Not only <span>Pod </span>name but also DNS and persistent volume are preserved. For example, when <kbd>my-pod-2</kbd> is recreated due to resource shortages or application crash, those names and volumes are taken over by a new <span>Pod </span>which is also named <kbd>my-pod-2</kbd>. It is useful for some cluster based applications such as HDFS and ElasticSearch.</p>
<p>In this recipe, we will demonstrate how to use DaemonSets and StatefulSet; however, to have a better understanding, it should use multiple Kubernetes Nodes environment. To do this, minikube is not ideal, so instead, use either kubeadm/kubespray to create a multiple Node environment.</p>
<p>Using kubeadm or kubespray to set up Kubernetes cluster was described in <a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml">Chapter 1</a><em>,</em> <em>Build Your Own Kubernetes <span>Cluster</span>.</em></p>
<p>To confirm whether <span>that has 2 or more nodes,</span> type <kbd>kubectl get nodes</kbd> as follows to check how many nodes you have:</p>
<pre>//this result indicates you have 2 nodes<br/><span>$ kubectl get nodes<br/></span><span>NAME<span class="Apple-converted-space">          </span>STATUS<span class="Apple-converted-space">    </span>ROLES <span class="Apple-converted-space">        </span>AGE <span class="Apple-converted-space">      </span>VERSION<br/></span><span>node1       <span class="Apple-converted-space">  </span>Ready <span class="Apple-converted-space">    </span>master,<strong>node </strong><span class="Apple-converted-space">  </span>6h<span class="Apple-converted-space">        </span>v1.10.2<br/></span><span>node2 <span class="Apple-converted-space">        </span>Ready <span class="Apple-converted-space">    </span><strong>node</strong><span class="Apple-converted-space">          </span>6h<span class="Apple-converted-space">        </span>v1.10.2</span></pre>
<p>In addition, if you want to execute the StatefulSet recipe later in this chapter, you need a StorageClass to set up a dynamic provisioning environment. It was described in <em>Working with volumes</em> section in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml">Chapter 2</a>, <em>Walking through Kubernetes Concepts</em><span>. It is recommended to use public cloud such as AWS and GCP with a CloudProvider; this will be described in <a href="b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml">Chapter 6</a>, <em>Building Kubernetes on AWS</em> and <a href="dfc46490-f109-4f07-ba76-1a381b006d76.xhtml">Chapter 7</a>, <em>Building Kubernetes on GCP</em></span>, <span>as well.</span></p>
<p>To check whether <kbd>StorageClass</kbd> is configured or not, use <kbd>kubectl get sc</kbd>:</p>
<pre><span>//in Google Kubernetes Engine Environment<br/>$ kubectl get sc<br/></span><span>NAME                 PROVISIONER<br/></span><span>standard (default)   kubernetes.io/gce-pd</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>There is no CLI for us to create DaemonSets or StatefulSets. Therefore, we will build these two resource types by writing all the configurations in a YAML file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pod as DaemonSets</h1>
                </header>
            
            <article>
                
<p>If a Kubernetes DaemonSet is created, the defined <span>Pod </span>will be deployed in every single node. It is guaranteed that the running containers occupy equal resources in each node. In this scenario, the container usually works as the daemon process.</p>
<p>For example, the following template has an Ubuntu image container that keeps checking its memory usage half a minute at a time:</p>
<ol>
<li>To build it as a <span>DaemonSet</span>, execute the following code block:</li>
</ol>
<pre style="padding-left: 90px">$ cat daemonset-free.yaml<br/><span>apiVersion: apps/v1<br/></span><span>kind: DaemonSet<br/></span><span>metadata:<br/></span><span><span class="Apple-converted-space">  </span>name: ram-check<br/></span><span>spec:<br/></span><span><span class="Apple-converted-space">  </span>selector:<br/></span><span><span class="Apple-converted-space">    </span>matchLabels:<br/></span><span><span class="Apple-converted-space">      </span>name: checkRam<br/></span><span><span class="Apple-converted-space">  </span>template:<br/></span><span><span class="Apple-converted-space">    </span>metadata:<br/></span><span><span class="Apple-converted-space">      </span>labels:<br/></span><span><span class="Apple-converted-space">        </span>name: checkRam<br/></span><span><span class="Apple-converted-space">    </span>spec:<br/></span><span><span class="Apple-converted-space">      </span>containers:<br/></span><span><span class="Apple-converted-space">      </span>- name: ubuntu-free<br/></span><span><span class="Apple-converted-space">        </span>image: ubuntu<br/></span><span><span class="Apple-converted-space">        </span>command: ["/bin/bash","-c","while true; do free; sleep 30; done"]<br/></span><span><span class="Apple-converted-space">      </span>restartPolicy: Always</span></pre>
<p style="padding-left: 60px">As the Job, the selector could be ignored, but it takes the values of the labels. We will always configure the restart policy of the <span>DaemonSet</span> as <kbd>Always</kbd>, which makes sure that every node has a <span>Pod </span>running.</p>
<ol start="2">
<li>The abbreviation of the <kbd>daemonset</kbd> is <kbd>ds</kbd> in <kbd>kubectl</kbd> command, use this shorter one in the CLI for convenience:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl create -f daemonset-free.yaml<br/>daemonset.apps "ram-check" created<br/><br/>$ kubectl get ds<br/>NAME        DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE<br/>ram-check   2         2         2         2            2           &lt;none&gt;          5m</pre>
<ol start="3">
<li>Here, we have two <span>Pods </span>running in separated nodes. They can still be recognized in the channel of the <kbd>pod</kbd>:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl get pods -o wide<br/>NAME              READY     STATUS    RESTARTS   AGE       IP               NODE<br/>ram-check-6ldng   1/1       Running   0          9m        10.233.102.130   <strong>node1</strong><br/>ram-check-ddpdb   1/1       Running   0          9m        10.233.75.4      <strong>node2</strong></pre>
<ol start="4">
<li>It is good for you to evaluate the result using the subcommand <kbd>kubectl logs</kbd>:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl logs ram-check-6ldng<br/>              total        used        free      shared  buff/cache   available<br/>Mem:        3623848      790144      329076        9128     2504628     2416976<br/>Swap:             0           0           0<br/>              total        used        free      shared  buff/cache   available<br/>Mem:        3623848      786304      328028        9160     2509516     2420524<br/>Swap:             0           0           0<br/>              total        used        free      shared  buff/cache   available<br/>Mem:        3623848      786344      323332        9160     2514172     2415944<br/>Swap:             0           0           0<br/>.<br/>.</pre>
<p style="padding-left: 60px">Whenever, you add a Kubernetes node onto your existing cluster, DaemonSets will recognize and deploy a <span>Pod </span>automatically.</p>
<ol start="5">
<li>Let's check again current status of DaemonSets, there are two <span>Pods </span>that have been deployed due to having two nodes as follows:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl get ds<br/>NAME        DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE<br/>ram-check   <strong>2         2         2         2            2</strong>           &lt;none&gt;          14m<br/><br/>$ kubectl get nodes<br/><span>NAME<span class="Apple-converted-space">          </span>STATUS<span class="Apple-converted-space">    </span>ROLES <span class="Apple-converted-space">        </span>AGE <span class="Apple-converted-space">      </span>VERSION<br/></span><span>node1 <span class="Apple-converted-space">        </span>Ready <span class="Apple-converted-space">    </span>master,node <span class="Apple-converted-space">  </span>6h<span class="Apple-converted-space">        </span>v1.10.2<br/></span><span>node2 <span class="Apple-converted-space">        </span>Ready <span class="Apple-converted-space">    </span>node<span class="Apple-converted-space">          </span>6h<span class="Apple-converted-space">        </span>v1.10.2</span></pre>
<ol start="6">
<li>So, now we are adding one more node onto the cluster through either <kbd>kubespray</kbd> or <kbd>kubeadm</kbd>, based on your setup:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl get nodes<br/><span>NAME<span class="Apple-converted-space">          </span>STATUS<span class="Apple-converted-space">    </span>ROLES <span class="Apple-converted-space">        </span>AGE <span class="Apple-converted-space">      </span>VERSION<br/></span><span>node1 <span class="Apple-converted-space">        </span>Ready <span class="Apple-converted-space">    </span>master,node <span class="Apple-converted-space">  </span>6h<span class="Apple-converted-space">        </span>v1.10.2<br/></span><span>node2 <span class="Apple-converted-space">        </span>Ready <span class="Apple-converted-space">    </span>node<span class="Apple-converted-space">          </span>6h<span class="Apple-converted-space">        </span>v1.10.2<br/></span><strong>node3         Ready     node          3m        v1.10.2</strong></pre>
<ol start="7">
<li>A few moments later, without any operation, the DaemonSet's size become <kbd>3</kbd> automatically, which aligns to the number of nodes:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl get ds<br/>NAME        DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE<br/>ram-check   <strong>3         3         3         3            3</strong>           &lt;none&gt;          18m<br/><br/><br/>$ kubectl get pods -o wide<br/>NAME              READY     STATUS    RESTARTS   AGE       IP               NODE<br/>ram-check-6ldng   1/1       Running   0          18m       10.233.102.130   node1<br/>ram-check-ddpdb   1/1       Running   0          18m       10.233.75.4      node2<br/><strong>ram-check-dpdmt   1/1       Running   0          3m        10.233.71.0      node3</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running a stateful Pod</h1>
                </header>
            
            <article>
                
<p>Let's see another use case. We used Deployments/ReplicaSets to replicate the <span>Pods</span>. It scales well and is easy to maintain and Kubernetes assigns a DNS to the <span>Pod </span>using the Pod's IP address, such as <kbd>&lt;Pod IP address&gt;.&lt;namespace&gt;.pod.cluster.local</kbd>.</p>
<p>The following example demonstrates how the <span>Pod </span>DNS will be assigned:</p>
<pre><span>$ kubectl run apache2 --image=httpd --replicas=3<br/></span><span>deployment "apache2" created<br/><br/><br/>//one of Pod has an IP address as 10.52.1.8<br/></span><span>$ kubectl get pods -o wide<br/></span><span>NAME                       READY     STATUS    RESTARTS   AGE       IP          NODE<br/></span><span>apache2-55c684c66b-7m5zq   1/1       Running   0          5s        <strong>10.52.1.8</strong>   gke-chap7-default-pool-64212da9-z96q<br/></span><span>apache2-55c684c66b-cjkcz   1/1       Running   0          1m        10.52.0.7   gke-chap7-default-pool-64212da9-8gzm<br/></span><span>apache2-55c684c66b-v78tq   1/1       Running   0          1m        10.52.2.5   gke-chap7-default-pool-64212da9-bbs6<br/><br/><br/>//another Pod can reach to </span><span><strong>10-52-1-8.default.pod.cluster.local<br/></strong></span><span>$ kubectl exec apache2-55c684c66b-cjkcz -- ping -c 2 <strong>10-52-1-8.default.pod.cluster.local</strong><br/></span><span>PING 10-52-1-8.default.pod.cluster.local (10.52.1.8): 56 data bytes<br/></span><span>64 bytes from 10.52.1.8: icmp_seq=0 ttl=62 time=1.642 ms<br/></span><span>64 bytes from 10.52.1.8: icmp_seq=1 ttl=62 time=0.322 ms<br/></span><span>--- 10-52-1-8.default.pod.cluster.local ping statistics ---<br/></span><span>2 packets transmitted, 2 packets received, 0% packet loss<br/></span><span>round-trip min/avg/max/stddev = 0.322/0.982/1.642/0.660 ms</span><span> </span></pre>
<p>However, this DNS entry is not guaranteed to stay in use for this Pod, because the <span>Pod </span>might crash due to an application error or node resource shortage. In such a case, the IP address will possibly be changed:</p>
<pre><span>$ kubectl delete pod apache2-55c684c66b-7m5zq<br/></span><span>pod "apache2-55c684c66b-7m5zq" deleted<br/><br/><br/>//Pod IP address has been changed to 10.52.0.<strong>7</strong><br/></span><span>$ kubectl get pods -o wide<br/></span><span>NAME                       READY     STATUS        RESTARTS   AGE       IP          NODE<br/></span><span>apache2-55c684c66b-7m5zq   0/1       Terminating   0          1m        &lt;none&gt;      gke-chap7-default-pool-64212da9-z96q<br/></span><span>apache2-55c684c66b-cjkcz   1/1       Running       0          2m        <strong>10.52.0.7</strong>   gke-chap7-default-pool-64212da9-8gzm<br/></span><span>apache2-55c684c66b-l9vqt   1/1       Running       0          7s        10.52.1.9   gke-chap7-default-pool-64212da9-z96q<br/></span><span>apache2-55c684c66b-v78tq   1/1       Running       0          2m        10.52.2.5   gke-chap7-default-pool-64212da9-bbs6<br/><br/><br/>//DNS entry also changed<br/></span><span>$ kubectl exec apache2-55c684c66b-cjkcz -- ping -c 2 <strong>10-52-1-8</strong>.default.pod.cluster.local<br/></span><span>PING 10-52-1-8.default.pod.cluster.local (10.52.1.8): 56 data bytes<br/></span><span>92 bytes from gke-chap7-default-pool-64212da9-z96q.c.kubernetes-cookbook.internal (192.168.2.4): <strong>Destination Host Unreachable</strong><br/></span><span>92 bytes from gke-chap7-default-pool-64212da9-z96q.c.kubernetes-cookbook.internal (192.168.2.4): <strong>Destination Host Unreachable</strong><br/></span><span>--- 10-52-1-8.default.pod.cluster.local ping statistics ---<br/></span><span>2 packets transmitted, 0 packets received, 100% packet loss</span></pre>
<p><span>For some applications, this will cause an issue; for example, if you manage a cluster application that needs to be managed by DNS or IP address. As of the current Kubernetes implementation, IP addresses can't be preserved for Pods . How about we use Kubernetes Service? Service preserves a DNS name. Unfortunately, it's not realistic to create the same amount of service with Pod. In the previous case, create three Services that bind to three Pods one to one.</span></p>
<p>Kubernetes has a solution for this kind of use case that uses StatefulSet<strong>.</strong> It preserves not only the DNS but also the persistent volume to keep a bind to the same Pod. Even if <span>Pod </span>is crashed, StatefulSet guarantees the binding of the same DNS and persistent volume to the new Pod. Note that the IP address is not preserved due to the current Kubernetes implementation.</p>
<p><span>To demonstrate, use <strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>) to launch one NameNode and three DataNodes. To perform this, use a Docker image </span>from <a href="https://hub.docker.com/r/uhopper/hadoop/">https://hub.docker.com/r/uhopper/hadoop/</a> that has NameNode and DataNode images. In addition, borrow the YAML configuration files <kbd>namenode.yaml</kbd> and <kbd>datanode.yaml</kbd> from <a href="https://gist.github.com/polvi/34ef498a967de563dc4252a7bfb7d582">https://gist.github.com/polvi/34ef498a967de563dc4252a7bfb7d582</a> and change a little bit:</p>
<ol>
<li>Let's launch a Service and StatefulSet for <kbd>namenode</kbd> and <kbd>datanode</kbd>:</li>
</ol>
<pre class="NormalPACKT" style="padding-left: 90px">//create NameNode<br/>$ kubectl create -f https://raw.githubusercontent.com/kubernetes-cookbook/second-edition/master/chapter3/3-4/namenode.yaml <br/>service "hdfs-namenode-svc" created<br/>statefulset "hdfs-namenode" created<br/> <br/>$ kubectl get statefulset<br/>NAME            DESIRED   CURRENT   AGE<br/>hdfs-namenode   1         1         19s<br/> <br/>$ kubectl get pods<br/>NAME              READY     STATUS    RESTARTS   AGE<br/>hdfs-namenode-0   1/1       Running   0          26s<br/><br/><br/>//create DataNodes<br/>$ kubectl create -f https://raw.githubusercontent.com/kubernetes-cookbook/second-edition/master/chapter3/3-4/datanode.yaml <br/>statefulset "hdfs-datanode" created<br/> <br/>$ kubectl get statefulset<br/>NAME            DESIRED   CURRENT   AGE<br/>hdfs-datanode   3         3         50s<br/>hdfs-namenode   1         1         5m<br/> <br/>$ kubectl get pods<br/>NAME              READY     STATUS    RESTARTS   AGE<br/>hdfs-datanode-0   1/1       Running   0          9m<br/>hdfs-datanode-1   1/1       Running   0          9m<br/>hdfs-datanode-2   1/1       Running   0          9m<br/>hdfs-namenode-0   1/1       Running   0          9m</pre>
<p class="NormalPACKT" style="padding-left: 60px">As you can see, the <span>Pod </span>naming convention is <kbd>&lt;StatefulSet-name&gt;-&lt;sequence number&gt;</kbd><span class="packt_screen">.</span> For example, NameNode Pod's name is <kbd>hdfs-namenode-0</kbd>. Also DataNode Pod's names are <kbd>hdfs-datanode-0</kbd>, <kbd>hdfs-datanode-1</kbd> and <kbd>hdfs-datanode-2</kbd>. </p>
<p class="NormalPACKT" style="padding-left: 60px">In addition, both NameNode and DataNode have a service that is configured as Headless mode (by <kbd>spec.clusterIP: None</kbd>). Therefore, you can access these <span>Pods </span>using DNS as <kbd>&lt;pod-name&gt;.&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local</kbd>. In this case, this NameNode DNS entry could be <kbd>hdfs-namenode-0.hdfs-namenode-svc.default.svc.cluster.local</kbd><em>.</em></p>
<ol start="2">
<li>Let's check what NameNode Pod's IP address is, you can get this using <kbd>kubectl get pods -o wide</kbd> as follows:</li>
</ol>
<pre class="NormalPACKT" style="padding-left: 90px">//Pod hdfs-namenode-0 has an IP address as 10.52.2.8<br/>$ kubectl get pods hdfs-namenode-0 -o wide<br/><span>NAME              READY     STATUS    RESTARTS   AGE       IP          NODE<br/></span><span>hdfs-namenode-0   1/1       Running   0          9m        <strong>10.52.2.8</strong>   gke-chapter3-default-pool-97d2e17c-0dr5<br/></span></pre>
<ol start="3">
<li>Next, log in (run <kbd>/bin/bash</kbd>) to one of the DataNodes using <kbd>kubectl exec</kbd> to resolve this DNS name and check whether the IP address is <kbd>10.52.2.8</kbd> or not:</li>
</ol>
<pre class="NormalPACKT" style="padding-left: 90px">$ kubectl exec hdfs-datanode-1 -it -- /bin/bash<br/>root@hdfs-datanode-1:/#<br/>root@hdfs-datanode-1:/# ping -c 1 hdfs-namenode-0.hdfs-namenode-svc.default.svc.cluster.local<br/>PING hdfs-namenode-0.hdfs-namenode-svc.default.svc.cluster.local (<strong>10.52.2.8</strong>): 56 data bytes<br/>...<br/>...</pre>
<p style="padding-left: 60px">Looks all good! For demonstration purposes, let's access the HDFS web console to see DataNode's status.</p>
<ol start="4">
<li>To do that, use <kbd>kubectl port-forward</kbd> to access to the NameNode web port (tcp/<kbd>50070</kbd>):</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px">//check the status by HDFS web console<br/>$ kubectl port-forward hdfs-namenode-0 :50070<br/>Forwarding from 127.0.0.1:60107 -&gt; 50070</pre>
<ol start="5">
<li>The preceding result indicates that your local machine TCP port <kbd>60107</kbd> (you result will vary) has been forwarded to NameNode Pod TCP port <kbd>50070</kbd>. Therefore, use a web browser to access <kbd>http://127.0.0.1:60107/</kbd> as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-976 image-border" src="assets/5bd29558-4248-4abf-8913-3959eb180f8c.png" style="width:166.58em;height:93.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>HDFS Web console shows three DataNodes</span></div>
<p>As you may see, three DataNodes have been registered to NameNode successfully. The DataNodes are also using the Headless Service so that same name convention assigns DNS names for DataNode as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>DaemonSets and StatefulSets; both concepts are similar but behave differently, especially when <span>Pod </span>is crashed. Let's take a look at how it works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pod recovery by DaemonSets</h1>
                </header>
            
            <article>
                
<p>DaemonSets keep monitoring every Kubernetes node, so when one of the Pods crashes, DaemonSets recreates it on the same Kubernetes node.</p>
<p>To simulate this, go back to the DaemonSets example and use <kbd>kubectl delete pods</kbd> to delete an existing <span>Pod</span> from <kbd>node1</kbd> manually, as follows:</p>
<pre>$ kubectl delete pod ram-check-6ldng<br/>pod "ram-check-6ldng" deleted<br/><br/>$ kubectl get pods -o wide<br/>NAME              READY     STATUS        RESTARTS   AGE       IP               NODE<br/><strong>ram-check-6ldng</strong>   1/1       <strong>Terminating   </strong>0          29m       10.233.102.132   <strong>node1</strong><br/>ram-check-ddpdb   1/1       Running       0          29m       10.233.75.5      node2<br/>ram-check-dpdmt   1/1       Running       0          13m       10.233.71.0      node3<br/><br/>$ kubectl get pods -o wide<br/>NAME              READY     STATUS    RESTARTS   AGE       IP               NODE<br/>ram-check-ddpdb   1/1       Running   0          30m       10.233.75.5      node2<br/><strong>ram-check-dh5hq   1/1       Running   0          24s       10.233.102.135   node1<br/></strong>ram-check-dpdmt   1/1       Running   0          14m       10.233.71.0      node3</pre>
<p>As you can see, a new <span>Pod </span>has been created automatically to recover the <span>Pod</span> in <kbd>node1</kbd>. Note that the <span>Pod </span>name has been changed from <kbd>ram-check-6ldng</kbd> to <kbd>ram-check-dh5hq</kbd>—it has been<em> </em>assigned a random suffix name. In this use case, <span>Pod</span> name doesn't matter, because we don't use hostname or DNS to manage this application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pod recovery by StatefulSet</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">StatefulSet behaves differently to DaemonSet during <span>Pod </span>recreation. In StatefulSet managed <span>Pods</span>, the <span>Pod </span>name is always consisted to assign an ordered number such as <kbd>hdfs-datanode-0</kbd>, <kbd>hdfs-datanode-1</kbd> and<kbd>hdfs-datanode-2</kbd><em>,</em> and if you delete one of them, a new <span>Pod </span>will take over the same <span>Pod </span>name.</p>
<p class="NormalPACKT">To simulate this, let's delete one DataNode (<kbd>hdfs-datanode-1</kbd>) to see how StatefulSet recreates a Pod:</p>
<pre class="NormalPACKT">$ kubectl get pods<br/>NAME              READY     STATUS    RESTARTS   AGE<br/>hdfs-datanode-0   1/1       Running   0          3m<br/>hdfs-datanode-1   1/1       Running   0          2m<br/>hdfs-datanode-2   1/1       Running   0          2m<br/>hdfs-namenode-0   1/1       Running   0          23m<br/> <br/>//delete DataNode-1<br/>$ kubectl delete pod hdfs-datanode-1<br/>pod "<strong>hdfs-datanode-1</strong>" deleted<br/> <br/>//DataNode-1 is Terminating<br/>$ kubectl get pods<br/>NAME              READY     STATUS        RESTARTS   AGE<br/>hdfs-datanode-0   1/1       Running       0          3m<br/><strong>hdfs-datanode-1   1/1       Terminating   0          3m</strong><br/>hdfs-datanode-2   1/1       Running       0          2m<br/>hdfs-namenode-0   1/1       Running       0          23m<br/><br/>//DataNode-1 is recreating automatically by statefulset <br/>$ kubectl get pods<br/>NAME              READY     STATUS              RESTARTS   AGE<br/>hdfs-datanode-0   1/1       Running             0          4m<br/><strong>hdfs-datanode-1   0/1       ContainerCreating   0          16s</strong><br/>hdfs-datanode-2   1/1       Running             0          3m<br/>hdfs-namenode-0   1/1       Running             0          24m<br/> <br/>//DataNode-1 is recovered<br/>$ kubectl get pods<br/>NAME              READY     STATUS    RESTARTS   AGE<br/>hdfs-datanode-0   1/1       Running   0          4m<br/><strong>hdfs-datanode-1   1/1       Running   0          22s</strong><br/>hdfs-datanode-2   1/1       Running   0          3m<br/>hdfs-namenode-0   1/1       Running   0          24m</pre>
<p class="NormalPACKT">As you see, the same <span>Pod </span>name (<kbd>hdfs-datanode-1</kbd>) has been assigned. Approximately after 10 minutes (due to HDFS's heart beat interval), HDFS web console shows that the old <span>Pod </span>has been marked as dead and the new <span>Pod </span>has the in service state, shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-977 image-border" src="assets/8bb7096f-30a6-4cff-9aac-c1d831eb27e4.png" style="width:166.58em;height:108.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Status when one DataNode is dead</span></div>
<p class="NormalPACKT">Note that this is not a perfect ideal case for HDFS, because DataNode-1 lost data and expects to re-sync from other DataNodes. If the data size is bigger, it may take a long time to complete re-sync. </p>
<p>Fortunately, StatefulSets has an capability that preserve a persistent volume while replacing a Pod. Let's see how HDFS DataNode can preserve data during <span>Pod </span>recreation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>StatefulSet with persistent volume; it requires a <kbd>StorageClass</kbd> that provisions a volume dynamically. Because each <span>Pod </span>is created by StatefulSets, it will create a <strong>persistent volume claim</strong> (<strong>PVC</strong>) with a different identifier. If your StatefulSets specify a static name of PVC, there will be trouble if multiple <span>Pods </span>try to attach the same PVC.</p>
<p>If you have <kbd>StorageClass</kbd> on your cluster, update <kbd>datanode.yaml</kbd> to add <kbd>spec.volumeClaimTemplates</kbd> as follows:</p>
<pre><span>$ </span><span>curl https://raw.githubusercontent.com/kubernetes-cookbook/second-edition/master/chapter3/3-4/datanode-pv.yaml</span><span><br/>...<br/>  </span><span>volumeClaimTemplates</span><span>:<br/></span><span>  </span><span>- </span><span>metadata</span><span>:<br/></span><span>      </span><span>name</span><span>:</span><span> hdfs-data<br/></span><span>    </span><span>spec</span><span>:<br/></span><span>      </span><span>accessModes</span><span>:</span><span> </span><span>[</span><span> </span><span>"ReadWriteOnce"</span><span> </span><span>]<br/></span><span>      </span><span>resources</span><span>:<br/></span><span>        </span><span>requests</span><span>:<br/></span><span>          </span><span>storage</span><span>:</span><span> 10Gi</span></pre>
<p>This tells Kubernetes to create a PVC and PV when a new <span>Pod </span>is created by StatefulSet. So, that <span>Pod </span>template (<kbd>spec.template.spec.containers.volumeMounts</kbd>) should specify <kbd>hdfs-data</kbd>, as follows:</p>
<pre><span>$ </span><span>curl https://raw.githubusercontent.com/kubernetes-cookbook/second-edition/master/chapter3/3-4/datanode-pv.yaml</span><span><br/>...<br/>          volumeMounts:<br/></span><span>          - mountPath: /hadoop/dfs/data<br/></span><span>            name: hdfs-data</span></pre>
<p>Let's recreate HDFS cluster again:</p>
<pre><span>//delete DataNodes<br/>$ kubectl delete -f https://raw.githubusercontent.com/kubernetes-cookbook/second-edition/master/chapter3/3-4/datanode.yaml<br/></span><span>service "hdfs-datanode-svc" deleted<br/></span><span>statefulset "hdfs-datanode" deleted<br/><br/>//delete NameNode<br/>$ kubectl delete -f https://raw.githubusercontent.com/kubernetes-cookbook/second-edition/master/chapter3/3-4/namenode.yaml <br/>service "hdfs-namenode-svc" deleted<br/>statefulset "hdfs-namenode" deleted<br/><br/><br/>//create NameNode again<br/>$ kubectl create -f https://raw.githubusercontent.com/kubernetes-cookbook/second-edition/master/chapter3/3-4/namenode.yaml<br/>service "hdfs-namenode-svc" created<br/>statefulset "hdfs-namenode" created<br/><br/>//create DataNode which uses Persistent Volume (datanode-pv.yaml)<br/>$ kubectl create -f https://raw.githubusercontent.com/kubernetes-cookbook/second-edition/master/chapter3/3-4/<strong>datanode-pv</strong>.yaml<br/>service "hdfs-datanode-svc" created<br/>statefulset "hdfs-datanode" created<br/><br/>//3 PVC has been created automatically<br/>$ kubectl get pvc<br/>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE<br/>hdfs-data-hdfs-datanode-0 Bound pvc-bc79975d-f5bd-11e7-ac7a-42010a8a00ef 10Gi RWO standard 53s<br/>hdfs-data-hdfs-datanode-1 Bound pvc-c753a336-f5bd-11e7-ac7a-42010a8a00ef 10Gi RWO standard 35s<br/>hdfs-data-hdfs-datanode-2 Bound pvc-d1e10587-f5bd-11e7-ac7a-42010a8a00ef 10Gi RWO standard 17s<br/></span></pre>
<p>To demonstrate, use <kbd>kubectl exec</kbd> to access the NameNode, then copy some dummy files to HDFS:</p>
<pre><span>$ kubectl exec -it hdfs-namenode-0 -- /bin/bash<br/></span><span>root@hdfs-namenode-0:/# hadoop fs -put /lib/x86_64-linux-gnu/* /<br/>root@hdfs-namenode-0:/# exit<br/>command terminated with exit code 255<br/><br/>//delete DataNode-1<br/>$ kubectl delete pod hdfs-datanode-1<br/>pod "hdfs-datanode-1" deleted</span></pre>
<p>At this moment, <kbd>DataNode-1</kbd> is restarting, as shown in the following image. However, the data directory of <kbd>DataNode-1</kbd> is kept by PVC as <span><kbd>hdfs-data-hdfs-datanode-1</kbd>. The new Pod <kbd>hdfs-datanode-1</kbd> will take over this PVC again:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d97460f9-8986-43df-bf2b-3311fc0f4aca.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>StatefulSet keeps PVC/PV while restarting</span></div>
<p><span>Therefore, when you access HDFS after <kbd>hdfs-datanode-1</kbd> has recovered, you don't see any data loss or re-sync processes:</span></p>
<pre>$ kubectl exec -it hdfs-namenode-0 -- /bin/bash<br/>root@hdfs-namenode-0:/# hdfs fsck /<br/>Connecting to namenode via http://hdfs-namenode-0.hdfs-namenode-svc.default.svc.cluster.local:50070/fsck?ugi=root&amp;path=%2F<br/>FSCK started by root (auth:SIMPLE) from /10.52.1.13 for path / at Wed Jan 10 04:32:30 UTC 2018<br/>....................................................................................................<br/>.................................................................Status: HEALTHY<br/> Total size: 22045160 B<br/> Total dirs: 2<br/> Total files: 165<br/> Total symlinks: 0<br/> Total blocks (validated): 165 (avg. block size 133607 B)<br/> Minimally replicated blocks: 165 (100.0 %)<br/> Over-replicated blocks: 0 (0.0 %)<br/> Under-replicated blocks: 0 (0.0 %)<br/> Mis-replicated blocks: 0 (0.0 %)<br/> Default replication factor: 3<br/> Average block replication: 3.0<br/> Corrupt blocks: 0<br/> Missing replicas: 0 (0.0 %)<br/> Number of data-nodes: 3<br/> Number of racks: 1<br/>FSCK ended at Wed Jan 10 04:32:30 UTC 2018 in 85 milliseconds<br/><br/><br/>The filesystem under path '/' is HEALTHY</pre>
<p>As you see, the <span>Pod </span>and PV pair is fully managed by StatefulSets. It is convenient if you want to scale more HDFS DataNode using just the <kbd>kubectl scale</kbd> command to make it double or hundreds—whatever you need:</p>
<pre>//make double size of HDFS DataNodes<br/>$ kubectl scale statefulset hdfs-datanode --replicas=6<br/>statefulset "hdfs-datanode" scaled<br/><br/>$ kubectl get pods<br/>NAME READY STATUS RESTARTS AGE<br/>hdfs-datanode-0 1/1 Running 0 20m<br/>hdfs-datanode-1 1/1 Running 0 13m<br/>hdfs-datanode-2 1/1 Running 0 20m<br/>hdfs-datanode-3 1/1 Running 0 56s<br/>hdfs-datanode-4 1/1 Running 0 38s<br/>hdfs-datanode-5 1/1 Running 0 21s<br/>hdfs-namenode-0 1/1 Running 0 21m<br/><br/>$ kubectl get pvc<br/>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE<br/>hdfs-data-hdfs-datanode-0 Bound pvc-bc79975d-f5bd-11e7-ac7a-42010a8a00ef 10Gi RWO standard 21m<br/>hdfs-data-hdfs-datanode-1 Bound pvc-c753a336-f5bd-11e7-ac7a-42010a8a00ef 10Gi RWO standard 21m<br/>hdfs-data-hdfs-datanode-2 Bound pvc-d1e10587-f5bd-11e7-ac7a-42010a8a00ef 10Gi RWO standard 21m<br/>hdfs-data-hdfs-datanode-3 Bound pvc-888b6e0d-f5c0-11e7-ac7a-42010a8a00ef 10Gi RWO standard 1m<br/>hdfs-data-hdfs-datanode-4 Bound pvc-932e6148-f5c0-11e7-ac7a-42010a8a00ef 10Gi RWO standard 1m<br/>hdfs-data-hdfs-datanode-5 Bound pvc-9dd71bf5-f5c0-11e7-ac7a-42010a8a00ef 10Gi RWO standard 1m</pre>
<div class="packt_infobox">You can also use PV to NameNode to persist metadata. However, <kbd>kubectl</kbd> scale does not work well due to HDFS architecture. In order to have high availability or scale out HDFS NameNode, please visit the HDFS Federation document at : <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/Federation.html">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/Federation.html</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>In this recipe, we went deeply into Kubernetes <span>Pod </span>management through DaemonSets and StatefulSet. It manages <span>Pod </span>in a particular way, such as <span>Pod </span>per node and consistent <span>Pod </span>names. It is useful when the Deployments/ReplicaSets stateless <span>Pod </span>management style can't cover your application use cases. For further information, consider the following:</p>
<ul>
<li>The <em>Working with Pods<span> </span></em>recipe in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml">Chapter 2</a>, <em>Walking through Kubernetes Concepts</em></li>
<li>Working with configuration files</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Submitting Jobs on Kubernetes</h1>
                </header>
            
            <article>
                
<p>Your container application is designed not only for daemon processes such as nginx, but also for some batch <span>J</span>obs which eventually exit when the task is complete. Kubernetes supports this scenario; you can submit a container as a Job and Kubernetes will dispatch to an appropriate node and execute your <span>J</span>ob.</p>
<p>In this recipe, we will discuss two new features: <strong>Jobs</strong> and <strong>CronJob</strong>. These two features can make another usage of Pods to utilize your resources.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Since Kubernetes version 1.2, Kubernetes Jobs has been introduced as a stable feature (<kbd>apiVersion: batch/v1</kbd>). In addition, CronJob is a beta feature (<kbd>apiVersion: batch/v1beta1</kbd>) as of Kubernetes version 1.10.</p>
<p>Both work well on <strong>minikube,</strong> which was introduced at <a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml">Chapter 1</a>, <em>Building Your Own Kubernetes <span>Cluster</span>. T</em>herefore, this recipe will use minikube version 0.24.1.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>When submitting a Job to Kubernetes, you have three types of Job that you can define:</p>
<ul>
<li>Single Job</li>
<li>Repeat Job</li>
<li>Parallel Job</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pod as a single Job</h1>
                </header>
            
            <article>
                
<p>A <span>J</span>ob-like <span>Pod </span>is suitable for testing your containers, which can be used for unit test or integration test; alternatively, it can be used for batch programs:</p>
<ol>
<li>In the following example, we will write a <span>Job </span>template to check the packages installed in image Ubuntu:</li>
</ol>
<pre style="padding-left: 90px">$ cat job-dpkg.yaml<br/>apiVersion: batch/v1<br/>kind: Job<br/>metadata:<br/>  name: package-check<br/>spec:<br/>  template:<br/>    spec:<br/>      containers:<br/>      - name: package-check<br/>        image: ubuntu<br/>        command: ["dpkg-query", "-l"]<br/>      restartPolicy: Never</pre>
<p style="padding-left: 60px">Note that restart policy for Pods created in a <span>Job </span>should be set to <kbd>Never</kbd> or <kbd>OnFailure</kbd>, since a <span>Job </span>goes to termination once it is completed successfully.</p>
<ol start="2">
<li>Now, you are ready to create a <kbd>job</kbd> using your template:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px">$ kubectl create -f job-dpkg.yaml<br/>job.batch "package-check" created</pre>
<ol start="3">
<li>After creating a <kbd>job</kbd> object, it is possible to verify the status of both the <span>Pod </span>and Job:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px">$ kubectl get jobs<br/>NAME            DESIRED   SUCCESSFUL   AGE<br/>package-check   1         1            26s</pre>
<ol start="4">
<li>This result indicates that <span>Job </span>is already done, executed (by <kbd>SUCCESSFUL = 1</kbd>) in <kbd>26</kbd> seconds. In this case, Pod has already disappeared:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl get pods<br/>No resources found, use --show-all to see completed objects.</pre>
<ol start="5">
<li>As you can see, the <kbd>kubectl</kbd> command hints to us that we can use <kbd>--show-all</kbd> or <kbd>-a</kbd> option to find the completed Pod, as follows:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl get pods --show-all<br/>NAME                  READY     STATUS      RESTARTS   AGE<br/>package-check-hmjxj   0/1       Completed   0          3m</pre>
<p style="padding-left: 60px">Here you go. So why does the <kbd>Completed</kbd> Pod object remain? Because you may want to see the result after your program has ended. You will find that a <span>Pod </span>is booting up for handling this task. This <span>Pod </span>is going to be stopped very soon at the end of the process.</p>
<ol start="6">
<li>Use the subcommand <kbd>kubectl logs</kbd> to get the result:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px">$ kubectl logs package-check-hmjxj<br/>Desired=Unknown/Install/Remove/Purge/Hold<br/>| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend<br/>|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)<br/>||/ Name                     Version                       Architecture Description<br/>+++-========================-=============================-============-======================================================================<br/>ii  adduser                  3.113+nmu3ubuntu4             all          add and remove users and groups<br/>ii  apt                      1.2.24                        amd64        commandline package manager<br/>ii  base-files               9.4ubuntu4.5                  amd64        Debian base system miscellaneous files<br/>ii  base-passwd              3.5.39                        amd64        Debian base system master password and group files<br/>ii  bash                     4.3-14ubuntu1.2               amd64        GNU Bourne Again SHell<br/>.<br/>.<br/>.</pre>
<ol start="7">
<li>Please go ahead and check the <kbd>job package-check</kbd> using the subcommand <kbd>kubectl describe</kbd>; the confirmation for <span>Pod </span>completion and other messages are shown as system information:</li>
</ol>
<pre class="NormalPACKT" style="padding-left: 90px">$ kubectl describe job package-check<br/>Name:           package-check<br/>Namespace:      default<br/>Selector:       controller-uid=9dfd1857-f5d1-11e7-8233-ae782244bd54<br/>Labels:         controller-uid=9dfd1857-f5d1-11e7-8233-ae782244bd54<br/>                job-name=package-check<br/>Annotations:    &lt;none&gt;<br/>Parallelism:    1<br/>Completions:    1<br/>Start Time:     Tue, 09 Jan 2018 22:43:50 -0800<br/>Pods Statuses:  0 Running / 1 Succeeded / 0 Failed<br/>.<br/>.<br/>.</pre>
<ol start="8">
<li>Later, to remove the <kbd>job</kbd> you just created, delete it with the name. This also removes the completed Pod as well:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl delete jobs package-check<br/>job.batch "package-check" deleted<br/><br/>$ kubectl get pods --show-all<br/>No resources found.</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Create a repeatable Job</h1>
                </header>
            
            <article>
                
<p>Users can also decide the number of tasks that should be finished in a single <span>J</span>ob. It is helpful to solve some random and sampling problems. Let's try it on the same template in the previous example:</p>
<ol>
<li>Add the <kbd>spec.completions</kbd> item to indicate the <span>Pod </span>number:</li>
</ol>
<pre style="padding-left: 90px">$ cat job-dpkg-repeat.yaml<br/>apiVersion: batch/v1<br/>kind: Job<br/>metadata:<br/>  name: package-check<br/>spec:<br/>  <strong>completions: 3<br/></strong>  template:<br/>    spec:<br/>      containers:<br/>      - name: package-check<br/>        image: ubuntu<br/>        command: ["dpkg-query", "-l"]<br/>      restartPolicy: Never</pre>
<ol start="2">
<li>After creating this Job, check how the Pod looks with the subcommand <kbd>kubectl describe</kbd>:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl create -f job-dpkg-repeat.yaml <br/>job.batch "package-check" created<br/><br/>$ kubectl describe jobs package-check<br/>Name:           package-check<br/>Namespace:      default<br/>...<br/>...<br/>Annotations:    &lt;none&gt;<br/>Parallelism:    1<br/><strong>Completions:    3</strong><br/>Start Time:     Tue, 09 Jan 2018 22:58:09 -0800<br/>Pods Statuses:  0 Running / 3 Succeeded / 0 Failed<br/>...<br/>...<br/>Events:<br/>  Type    Reason            <strong>Age</strong>   From            Message<br/>  ----    ------            ----  ----            -------<br/>  Normal  SuccessfulCreate  <strong>42s</strong>   job-controller  Created pod: package-check-f72wk<br/>  Normal  SuccessfulCreate  <strong>32s</strong>   job-controller  Created pod: package-check-2mnw8<br/>  Normal  SuccessfulCreate  <strong>27s</strong>   job-controller  Created pod: package-check-whbr6</pre>
<p>As you can see, three Pods are created to complete this <span>J</span>ob. This is useful if you need to run your program repeatedly at particular times. However, as you may have noticed from the <kbd>Age</kbd> column in preceding result, these Pods ran sequentially, one by one. This means that the 2nd <span>Job </span>was started after the 1st <span>Job </span>was completed, and the 3rd <span>Job </span>was started after the 2nd <span>Job </span>was completed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Create a parallel Job</h1>
                </header>
            
            <article>
                
<p>If your batch <span>Job </span>doesn't have a state or dependency between <span>J</span>obs, you may consider submitting Jobs in parallel. Similar to the <kbd>spec.completions</kbd> parameter, the Job template has a <kbd>spec.parallelism</kbd> parameter to specify how many <span>J</span>obs you want to run in parallel:</p>
<p style="padding-left: 60px">1. Re-use a repeatable <span>Job </span>but change it to specify <kbd>spec.parallelism: 3</kbd> as follows:</p>
<pre style="padding-left: 90px">$ cat job-dpkg-parallel.yaml <br/>apiVersion: batch/v1<br/>kind: Job<br/>metadata:<br/>  name: package-check<br/>spec:<br/>  <strong>parallelism: 3<br/></strong>  template:<br/>    spec:<br/>      containers:<br/>      - name: package-check<br/>        image: ubuntu<br/>        command: ["dpkg-query", "-l"]<br/>      restartPolicy: Never</pre>
<ol start="2">
<li>The result is similar to <kbd>spec.completions=3</kbd><em>,</em> which made <kbd>3</kbd> Pods to run your application:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl get pods --show-all<br/>NAME                  READY     STATUS      RESTARTS   AGE<br/>package-check-5jhr8   0/1       Completed   0          1m<br/>package-check-5zlmx   0/1       Completed   0          1m<br/>package-check-glkpc   0/1       Completed   0          1m</pre>
<ol start="3">
<li>However, if you see an <kbd>Age</kbd> column through the <kbd>kubectl describe</kbd> command, it indicates that <kbd>3</kbd> Pods ran at the same time:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl describe jobs package-check<br/>Name:           package-check<br/>Namespace:      default<br/>Selector:       controller-uid=de41164e-f5d6-11e7-8233-ae782244bd54<br/>Labels:         controller-uid=de41164e-f5d6-11e7-8233-ae782244bd54<br/>                job-name=package-check<br/>Annotations:    &lt;none&gt;<br/><strong>Parallelism:    3</strong><br/>Completions:    &lt;unset&gt;<br/>…<br/>Events:<br/>  Type    Reason            <strong>Age</strong>   From            Message<br/>  ----    ------            ----  ----            -------<br/>  Normal  SuccessfulCreate  <strong>24s</strong>   job-controller  Created pod: package-check-5jhr8<br/>  Normal  SuccessfulCreate  <strong>24s</strong>   job-controller  Created pod: package-check-glkpc<br/>  Normal  SuccessfulCreate  <strong>24s</strong>   job-controller  Created pod: package-check-5zlmx</pre>
<p>In this setting, Kubernetes can dispatch to an available node to run your application and that easily scale your Jobs. It is useful if you want to run something like a worker application to distribute a bunch of Pods to different nodes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Schedule to run Job using CronJob</h1>
                </header>
            
            <article>
                
<p>If you are familiar with <strong>UNIX <span>CronJob</span><span> </span></strong>or <strong>Java Quartz</strong> (<a href="http://www.quartz-scheduler.org">http://www.quartz-scheduler.org</a>), Kubernetes CronJob is a very straightforward tool that you can define a particular timing to run your Kubernetes Job repeatedly.</p>
<p>The scheduling format is very simple; it specifies the following five items:</p>
<ul>
<li>Minutes (0 – 59)</li>
<li>Hours (0 – 23)</li>
<li>Day of Month (1 – 31)</li>
<li>Month (1 – 12)</li>
<li>Day of week (0: Sunday – 6: Saturday)</li>
</ul>
<p>For example, if you want to run your <span>Job </span>only at 9:00am on November 12th, every year, to send a birthday greeting to me :-), the schedule format could be <kbd>0 9 12 11 *</kbd>.</p>
<p>You may also use slash (<kbd>/</kbd>) to specify a step value; a <kbd>run every 5 minutes</kbd> interval for the previous Job example would have the following schedule format: <kbd>*/5 * * * *</kbd>. </p>
<p>In addition, there is an optional parameter, <kbd>spec.concurrencyPolicy</kbd>, <span>that you can specify a behavior if the previous Job is not finished but the next Job schedule is approaching, to determine how the next Job runs. You can set either:</span></p>
<ul>
<li><strong>Allow</strong>: Allow execution of the next <span>J</span>ob</li>
<li><strong>Forbid</strong>: Skip execution of the next <span>J</span>ob</li>
<li><strong>Replace</strong>: Delete the current <span>J</span>ob, then execute the next <span>J</span>ob</li>
</ul>
<p>If you set as <kbd>Allow</kbd>, there might be a potential risk of accumulating some unfinished <span>J</span>obs in the Kubernetes cluster. Therefore, during the testing phase, you should set either <kbd>Forbid</kbd> or <kbd>Replace</kbd> to monitor <span>Job </span>execution and completion:</p>
<pre>$ cat cron-job.yaml <br/>apiVersion: batch/v1beta1<br/>kind: CronJob<br/>metadata:<br/>  name: package-check<br/>spec:<br/>  <strong>schedule: "*/5 * * * *"<br/>  concurrencyPolicy: "Forbid"</strong><br/>  jobTemplate:<br/>    spec:<br/>      template:<br/>        spec:<br/>          containers:<br/>          - name: package-check<br/>            image: ubuntu<br/>            command: ["dpkg-query", "-l"]<br/>          restartPolicy: Never<br/><br/>//create CronJob<br/>$ kubectl create -f cron-job.yaml <br/>cronjob.batch "package-check" created<br/><br/>$ kubectl get cronjob<br/>NAME            SCHEDULE      SUSPEND   ACTIVE    LAST SCHEDULE   AGE<br/>package-check   */5 * * * *   False     0         &lt;none&gt;</pre>
<p>After a few moments, the <span>Job </span> will be triggered by your desired timing—in this case, every 5 minutes. You may then see the <span>Job </span>entry through the <kbd>kubectl get jobs</kbd> and <kbd>kubectl get pods -a</kbd> commands, as follows:</p>
<pre>//around 9 minutes later, 2 jobs have been submitted already<br/>$ kubectl get jobs<br/>NAME                       DESIRED   SUCCESSFUL   AGE<br/>package-check-1515571800   1         1            7m<br/>package-check-1515572100   1         1            2m<br/><br/><br/>//correspond Pod are remain and find by -a option<br/>$ kubectl get pods -a<br/>NAME                             READY     STATUS      RESTARTS   AGE<br/>package-check-1515571800-jbzbr   0/1       Completed   0          7m<br/>package-check-1515572100-bp5fz   0/1       Completed   0          2m</pre>
<p>CronJob will keep remaining until you delete; this means that, every 5 minutes, CronJob will create a new Job entry and related Pods will also keep getting created. This will impact the consumption of Kubernetes resources. Therefore, by default, CronJob will keep up to <kbd>3</kbd> successful Jobs (by <kbd>spec.successfulJobsHistoryLimit</kbd>) and one failed Job (by <kbd>spec.failedJobsHistoryLimit</kbd>). You can change these parameters based on your requirements.</p>
<p>Overall, CronJob supplement allows Jobs to automatically to run in your application with the desired timing. You can utilize CronJob to run some report generation <span>J</span>obs, daily or weekly batch <span>J</span>obs, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Although Jobs and CronJob are the special utilities of P<span>ods</span>, the Kubernetes system has different management systems between them and <span>P</span><span>ods</span>.</p>
<p>For <span>J</span>ob, its selector cannot point to an existing pod. It is a bad idea to take a <span>Pod </span>controlled by the deployment/ReplicaSets as a <span>J</span>ob. The deployment/ReplicaSets have a desired number of <span>P</span><span>ods </span>running, which is against <span>J</span>ob's ideal situation: <span>P</span><span>ods </span>should be deleted once they finish their tasks. The <span>Pod </span>in the Deployments/ReplicaSets won't reach the state of end. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">In this recipe, we executed Jobs and CronJob, demonstrating another usage of Kubernetes <span>Pod </span>that has a completion state. Even once a Pod is completed, Kubernetes can preserve the logs and Pod object so that you can retrieve the result easily. For further information, consider:</p>
<ul>
<li>The <em>Working with <span>P</span><span>ods </span></em>recipe in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml">Chapter 2</a>, <em>Walking through Kubernetes Concepts</em></li>
<li><em>Working with configuration files </em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with configuration files</h1>
                </header>
            
            <article>
                
<p>Kubernetes supports two different file formats, <em>YAML</em> and <em>JSON</em>. Each format can describe the same function of Kubernetes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Before we study how to write a Kubernetes configuration file, learning how to write a correct template format is important. We can learn the standard format of both YAML and JSON from their official websites.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">YAML</h1>
                </header>
            
            <article>
                
<p>The YAML format is very simple, with few syntax rules; therefore, it is easy to read and write, even for users. To know more about YAML, you can refer to the following website link: <a href="http://www.yaml.org/spec/1.2/spec.html">http://www.yaml.org/spec/1.2/spec.html</a>. The following example uses the YAML format to set up the <kbd>nginx</kbd> Pod:</p>
<pre>$ cat nginx-pod.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: my-nginx<br/>  labels:<br/>    env: dev<br/>spec:<br/>  containers:<br/>  - name: my-nginx<br/>    image: nginx<br/>    ports:<br/>    - containerPort: 80</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">JSON</h1>
                </header>
            
            <article>
                
<p>The JSON format is also simple and easy to read for users, but more program-friendly. Because it has data types (number, string, Boolean, and object), it is popular to exchange the data between systems. <span>Technically, YAML is a superset of JSON, so JSON is a valid YAML, but not the other way around. </span>To know more about JSON, you can refer to the following website link: <a href="http://json.org/">http://json.org/</a>. </p>
<p>The following example of the <span>Pod </span>is the same as the preceding YAML format, but using the JSON format:</p>
<pre>$ cat nginx-pod.json<br/>{<br/>  "apiVersion": "v1",<br/>  "kind": "Pod",<br/>  "metadata": {<br/>       "name": "my-nginx",<br/>       "labels": {<br/>              "env": "dev"<br/>       }<br/>  },<br/>  "spec": {<br/>    "containers": [<br/>      {<br/>        "name": "my-nginx",<br/>        "image": "nginx",<br/>        "ports": [<br/>          {<br/>            "containerPort": 80    <br/>          }<br/>        ]<br/>      } <br/>    ]<br/>  }<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Kubernetes has a schema that is defined using a verify configuration format; schema can be generated after the first instance of running the subcommand <kbd>create</kbd> with a configuration file. The cached schema will be stored under the <kbd>.kube/cache/discovery/&lt;SERVICE_IP&gt;_&lt;PORT&gt;</kbd>, based on the version of API server you run:</p>
<pre>// create the resource by either YAML or JSON file introduced before<br/>$ kubectl create -f nginx-pod.yaml<br/>// or<br/>$ kubectl create -f nginx-pod.json<br/>// as an example of v1.10.0, the content of schema directory may look like following<br/>// you would have different endpoint of server<br/>ll ~/.kube/cache/discovery/192.168.99.100_8443/<br/>total 76<br/>drwxr-xr-x 18 nosus nosus 4096 May 6 10:10 ./<br/>drwxr-xr-x 4 nosus nosus 4096 May 6 10:00 ../<br/>drwxr-xr-x 3 nosus nosus 4096 May 6 10:00 admissionregistration.k8s.io/<br/>drwxr-xr-x 3 nosus nosus 4096 May 6 10:00 apiextensions.k8s.io/<br/>drwxr-xr-x 4 nosus nosus 4096 May 6 10:00 apiregistration.k8s.io/<br/>drwxr-xr-x 5 nosus nosus 4096 May 6 10:00 apps/<br/>drwxr-xr-x 4 nosus nosus 4096 May 6 10:00 authentication.k8s.io/<br/>drwxr-xr-x 4 nosus nosus 4096 May 6 10:00 authorization.k8s.io/<br/>drwxr-xr-x 4 nosus nosus 4096 May 6 10:00 autoscaling/<br/>drwxr-xr-x 4 nosus nosus 4096 May 6 10:00 batch/<br/>drwxr-xr-x 3 nosus nosus 4096 May 6 10:00 certificates.k8s.io/<br/>drwxr-xr-x 3 nosus nosus 4096 May 6 10:00 events.k8s.io/<br/>drwxr-xr-x 3 nosus nosus 4096 May 6 10:00 extensions/<br/>drwxr-xr-x 3 nosus nosus 4096 May 6 10:00 networking.k8s.io/<br/>drwxr-xr-x 3 nosus nosus 4096 May 6 10:00 policy/<br/>drwxr-xr-x 4 nosus nosus 4096 May 6 10:00 rbac.authorization.k8s.io/<br/>-rwxr-xr-x 1 nosus nosus 3898 May 6 10:10 servergroups.json*<br/>drwxr-xr-x 4 nosus nosus 4096 May 6 10:00 storage.k8s.io/<br/>drwxr-xr-x 2 nosus nosus 4096 May 6 10:10 v1/</pre>
<p>Each directory listed represents an API category. You will see a file named <kbd>serverresources.json</kbd> under the last layer of each directory, which clearly defines every resource covered by this API category. However, there are some alternative and easier ways to check the schema. From the website of Kubernetes, we can get any details of how to write a configuration file of specific resources. Go ahead and check the official API documentation of the latest version: <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/">https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/</a>. In the webpage, there are three panels: from left to right, they are the resource list, description, and the input and output of HTTP requests or the command kubectl. Taking Deployment as an example, you may click <span class="packt_screen">Deployment v1 app</span> at the resource list, the leftmost panel, and the following screenshot will show up:</p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><img src="assets/cc7417bc-6efa-409f-bd4f-24b297969fd7.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Documentation of Kubernetes Deployment API</div>
<p>But, how do we know the details of setting the container part at the marked place on the preceding image? In the field part of object description, there are two values. The first one, like <span class="packt_screen">apiVersion</span>, means the name, and the second one, like <span class="packt_screen">string</span>, is the type. Type could be integer, string, array, or the other resource object. Therefore, for searching the containers configuration of deployment, we need to know the structure of layers of objects. First, according to the example configuration file on web page, the layer of objects to containers is <kbd>spec.template.spec.containers.</kbd> So, start by clicking the hyperlink <span class="packt_screen">spec DeploymentSpec</span> under Deployment's fields, which is the type of resource object, and go searching hierarchically. Finally, you can find the details listed on this page: <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core">https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core</a>.</p>
<div class="packt_tip"><span class="packt_screen">Solution for tracing the configuration of containers of Deployment</span><strong><br/></strong><span>Here comes the solution for the preceding example:</span>
<ul>
<li><span>Click <span class="packt_screen">spec DeploymentSpec</span></span></li>
<li><span>Click <span class="packt_screen">template PodTemplateSpec</span></span></li>
<li><span>Click <span class="packt_screen">spec PodSpec</span></span></li>
<li><span>Click <span class="packt_screen">containers Container array</span></span></li>
</ul>
<span>Now you got it!</span></div>
<p>Taking a careful look at the definition of container configuration. The following are some common descriptions you should pay attention to:</p>
<ul>
<li><strong>Type</strong>: The user should always set the corresponding type for an item.</li>
<li><strong>Optional or not</strong>: Some items are indicated as optional, which means not necessary, and can be applied as a default value, or not set if you don't specify it.</li>
<li><strong>Cannot be updated</strong>: If the item is indicated as failed to be updated, it is fixed when the resource is created. You need to recreate a new one instead of updating it.</li>
<li><strong>Read-only</strong>: Some of the items are indicated as <kbd>read-only</kbd>, such as UID. Kubernetes generates these items. If you specify this in the configuration file, it will be ignored.</li>
</ul>
<p>Another method for checking the schema is through swagger UI. Kubernetes uses swagger (<a href="https://swagger.io">https://</a><a href="https://swagger.io">swagger.io/</a>) and OpenAPI (<a href="https://www.openapis.org">https://www.openapis.org</a>) to generate the REST API. Nevertheless, the web console for swagger is by default disabled in the API server. To enable the swagger UI of your own Kubernetes API server, just add the flag <kbd>--enable-swagger-ui=ture</kbd> when you start the API server. Then, by accessing the endpoint <kbd>https://&lt;KUBERNETES_MASTER&gt;:&lt;API_SERVER_PORT&gt;/swagger-ui</kbd>, you can successfully browse the API document through the web console:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6a34175a-0a30-4e6a-a20b-70392e605d69.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The swagger web console of Kubernetes API</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Let's introduce some necessary items in configuration files for creating Pod, Deployment, and Service.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pod</h1>
                </header>
            
            <article>
                
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Item</strong></p>
</td>
<td>
<p><strong>Type</strong></p>
</td>
<td>
<p><strong>Example</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>apiVersion</kbd></p>
</td>
<td>
<p>String</p>
</td>
<td>
<p><kbd>v1</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>kind</kbd></p>
</td>
<td>
<p>String</p>
</td>
<td>
<p><kbd>Pod</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>metadata.name</kbd></p>
</td>
<td>
<p>String</p>
</td>
<td>
<p><kbd>my-nginx-pod</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>spec</kbd></p>
</td>
<td>
<p><kbd>v1.PodSpec</kbd></p>
</td>
<td>
<p> </p>
</td>
</tr>
<tr>
<td>
<p><kbd>v1.PodSpec.containers</kbd></p>
</td>
<td>
<p>Array[<kbd>v1.Container</kbd>]</p>
</td>
<td>
<p> </p>
</td>
</tr>
<tr>
<td>
<p><kbd>v1.Container.name</kbd></p>
</td>
<td>
<p>String</p>
</td>
<td>
<p><kbd>my-nginx</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>v1.Container.image</kbd></p>
</td>
<td>
<p>String</p>
</td>
<td>
<p><kbd>nginx</kbd></p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Item</strong></p>
</td>
<td>
<p><strong>Type</strong></p>
</td>
<td>
<p><strong>Example</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>apiVersion</kbd></p>
</td>
<td>
<p>String</p>
</td>
<td>
<p><kbd>apps</kbd>/<kbd>v1beta1</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>kind</kbd></p>
</td>
<td>
<p>String</p>
</td>
<td>
<p><kbd>Deployment</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>metadata.name</kbd></p>
</td>
<td>
<p>String</p>
</td>
<td>
<p><kbd>my-nginx-deploy</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>spec</kbd></p>
</td>
<td>
<p><kbd>v1.DeploymentSpec</kbd></p>
</td>
<td>
<p> </p>
</td>
</tr>
<tr>
<td>
<p><kbd>v1.DeploymentSpec.template</kbd></p>
</td>
<td>
<p><kbd>v1.PodTemplateSpec</kbd></p>
</td>
<td>
<p> </p>
</td>
</tr>
<tr>
<td>
<p><kbd>v1.PodTemplateSpec.metadata.labels</kbd></p>
</td>
<td>
<p>Map of string</p>
</td>
<td>
<p><kbd>env: test</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>v1.PodTemplateSpec.spec</kbd></p>
</td>
<td>
<p><kbd>v1.PodSpec</kbd></p>
</td>
<td>
<p><kbd>my-nginx</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>v1.PodSpec.containers</kbd></p>
</td>
<td>
<p>Array[<kbd>v1.Container</kbd>]</p>
</td>
<td>
<p>As same as Pod</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Service</h1>
                </header>
            
            <article>
                
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Item</strong></p>
</td>
<td>
<p><strong>Type</strong></p>
</td>
<td>
<p><strong>Example</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>apiVersion</kbd></p>
</td>
<td>
<p>String</p>
</td>
<td>
<p><kbd>v1</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>kind</kbd></p>
</td>
<td>
<p>String</p>
</td>
<td>
<p><kbd>Service</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>metadata.name</kbd></p>
</td>
<td>
<p>String</p>
</td>
<td>
<p><kbd>my-nginx-svc</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>spec</kbd></p>
</td>
<td>
<p><kbd>v1.ServiceSpec</kbd></p>
</td>
<td>
<p> </p>
</td>
</tr>
<tr>
<td>
<p><kbd>v1.ServiceSpec.selector</kbd></p>
</td>
<td>
<p>Map of string</p>
</td>
<td>
<p><kbd>env: test</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>v1.ServiceSpec.ports</kbd></p>
</td>
<td>
<p>Array[<kbd>v1.ServicePort</kbd>]</p>
</td>
<td>
<p> </p>
</td>
</tr>
<tr>
<td>
<p><kbd>v1.ServicePort.protocol</kbd></p>
</td>
<td>
<p>String</p>
</td>
<td>
<p><kbd>TCP</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>v1.ServicePort.port</kbd></p>
</td>
<td>
<p>Integer</p>
</td>
<td>
<p><kbd>80</kbd></p>
</td>
</tr>
</tbody>
</table>
<p>Please check the code bundle file <kbd>minimal-conf-resource.yaml</kbd> to find these three resources with minimal configuration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>This recipe described how to find and understand a configuration syntax. Kubernetes has some detailed options to define containers and components. For more details, the following recipes will describe how to define <span>P</span><span>ods</span>, Deployments, and Services:</p>
<ul>
<li>The <em>Working with Pods</em>, <em>Deployment API</em>, and <em>Working with Services</em> recipes in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml" target="_blank">Chapter 2</a>, <em>Walking through Kubernetes Concepts</em></li>
</ul>


            </article>

            
        </section>
    </body></html>