- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Orchestrating Containers across Clouds with Tanzu Kubernetes Grid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we learned about Harbor, a container registry, that
    is covered as a part of the Tanzu product bundle. After learning about hosting
    our container images with Harbor, let’s learn how to deploy them on Kubernetes
    in this chapter with Tanzu Kubernetes Grid, a multi-cloud and enterprise-ready
    Kubernetes distribution of Tanzu. Kubernetes has become widely accepted and the
    default norm in the industry to run containers in the past few years. As per a
    recent survey by the **Cloud Native Computing Foundation** (**CNCF**), 96% of
    the responding organizations were either evaluating or already using Kubernetes
    (source: [https://www.cncf.io/wp-content/uploads/2022/02/CNCF-Annual-Survey-2021.pdf](https://www.cncf.io/wp-content/uploads/2022/02/CNCF-Annual-Survey-2021.pdf))!
    Additionally, a large sum of them used Kubernetes on different cloud platforms
    for reasons such as risk mitigation, avoiding vendor lock-ins, and efficient operational
    expenditure. But operating large Kubernetes platforms on one or more clouds is
    a non-trivial effort for various reasons. Each cloud platform has APIs and distinct
    ways to manage Kubernetes services. The complexity increases when the organizations
    also need to run Kubernetes services on-premises. For such on-premises Kubernetes
    deployments, many enterprises build their own platforms with custom automation.
    While building such custom platforms using open source tools brings initial cost
    savings, maintaining them for a long time is very difficult when the original
    people who built the platforms move out of the organizations. Rather, the organizations
    could better utilize their talents to build more revenue-generating custom applications
    for their businesses by using an enterprise-grade product such as **Tanzu Kubernetes
    Grid** (**TKG**) for such below-value-line concerns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TKG is VMware’s Kubernetes distribution that comes with all the bells and whistles
    to deploy enterprise-grade container platforms on vSphere-based on-premises data
    centers, **Amazon Web Services** (**AWS**), and Azure public cloud infrastructure.
    In this chapter, we will learn about this product in detail and will cover the
    following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Why Tanzu Kubernetes Grid?**: A walkthrough of the features and capabilities
    of TKG'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unboxing Tanzu Kubernetes Grid**: A detailed overview of the components and
    the concepts of TKG'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Getting started with Tanzu Kubernetes Grid**: Learn how to install and configure
    TKG'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Common day-2 operations with Tanzu Kubernetes Grid**: Learn how to perform
    various cluster life cycle activities with TKG'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started by learning about the background of Tanzu Kubernetes Grid.
  prefs: []
  type: TYPE_NORMAL
- en: Why Tanzu Kubernetes Grid?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a nutshell, TKG is an enterprise-supported flavor of the open source Kubernetes
    platform. Like many other distributions, TKG uses the upstream Kubernetes distributions
    without modifying the open source code base. However, there are a few good reasons
    why an enterprise would like to use TKG over the open source community – *vanilla*
    – distribution of Kubernetes. We’ll explore those reasons in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-cloud application deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As per a survey done by Gartner ([https://www.gartner.com/smarterwithgartner/why-organizations-choose-a-multicloud-strategy](https://www.gartner.com/smarterwithgartner/why-organizations-choose-a-multicloud-strategy)),
    over 81% of respondents said that they have a multi-cloud deployment or strategy.
    Enterprises have multi-cloud strategies for reasons such as avoiding vendor lock-ins
    and using the best services offered by the respective cloud provider. Additionally,
    the enterprises have applications that may not be deployed on a public cloud platform
    and hence deployed on-premises. Kubernetes offers a great option to allow application
    deployments in multi-cloud and hybrid cloud platforms. Once an application is
    ready to run in Kubernetes wrapped in a container, it can be deployed on any upstream
    conformant Kubernetes platform deployed in any cloud or data center. Kubernetes
    made the multi-cloud deployment of applications almost trivial. Kubernetes manages
    containerized applications well, but at its core, it does not know how to manage
    the infrastructure on which it is deployed. That makes the platform and infrastructure
    management an external concern. All major cloud providers offer Kubernetes as
    a service in addition to the option of just using their infrastructure to deploy
    a self-managed Kubernetes platform. However, every platform has its own interfaces
    and hence a different user experience. Additionally, every platform has its own
    flavors of operating systems, networks, storage, and compute-level differences.
    Managing large Kubernetes platforms itself is a hard problem to solve, which is
    amplified even more when we need to manage a multi-cloud environment. While learning
    the details of one cloud platform is not complex enough, now, the infrastructure
    and the platform teams need to learn the same for more than one cloud platform
    if they aim to deploy Kubernetes in a multi-cloud fashion. Because of these reasons
    and the complexities involved, several enterprises avoid going multi-cloud despite
    the involved risks in using just one platform.
  prefs: []
  type: TYPE_NORMAL
- en: TKG addresses these challenges by providing a uniform experience to operate
    Kubernetes on vSphere (for on-premises deployment), AWS, and Azure cloud platforms.
    One of the core components of TKG is the Cluster API ([https://cluster-api.sigs.k8s.io/](https://cluster-api.sigs.k8s.io/)),
    a CNCF open source project that provides an abstraction layer on top of the infrastructure.
    The Cluster API deploys and manages Kubernetes nodes with the required storage
    and network configuration for the selected cloud platform. Additionally, TKG exposes
    its interfaces using the `tanzu` CLI by providing the same user interface to perform
    different Kubernetes cluster life cycle operations, irrespective of the infrastructure
    provider. TKG also bundles VMware-supported operating system layers for the three
    supported cloud providers. With all these characteristics, TKG provides an easy-to-consume
    multi-cloud Kubernetes platform.
  prefs: []
  type: TYPE_NORMAL
- en: Open source alignment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another of the main benefits of using TKG is its usage of open source tools
    for different use cases. As its core component, TKG uses the upstream open source
    Kubernetes distribution maintained by the CNCF community. TKG does not fork the
    source code of any of its open source components to add its own flavor and customization.
    This characteristic provides TKG consumers with all the benefits of using open
    source software, including avoidance of vendor lock-ins via portability, and an
    avenue to enrich the functionalities to their needs via open source contributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering these benefits, all enterprises have wanted to use open source
    solutions as much as possible in the past few years. However, the cloud-native
    ecosystem is very crowded with open source products that solve similar problems
    and have different levels of maturity to be used in production setups. *Figure
    7**.1* is a screenshot taken from the CNCF website ([https://landscape.cncf.io/](https://landscape.cncf.io/))
    showing an extremely crowded space with tools solving different problems running
    containerized applications. Picking the right tool for the right problem with
    acceptable maturity, a vibrant community, and enterprise-level support is a challenging
    task. And the landscape is constantly changing by adding new solutions to the
    list very frequently. Adding more to the complexity, the compatibility of one
    tool working with another is not always the case. So, there are three possible
    solutions to this issue:'
  prefs: []
  type: TYPE_NORMAL
- en: Enterprises would build their custom container platforms by carefully evaluating
    their options in this diverse and crowded landscape by running long proof-of-concept
    projects to narrow down their choices. While this would provide all the control
    and possible short-term cost benefits, there are some considerable drawbacks to
    this approach. One of them is that enterprises may need to spend a lot of productive
    time with their engineers to build a custom container platform. Finding right
    talents from the market and building required skills internally are hard and expensive
    challenges. Additionally, such skilled people would be in high demand in the market
    and may not stay with the enterprise for a long time to support what they have
    built internally. And then, such custom-built solutions quickly become an expensive
    liability for the organization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enterprises would choose all the solutions provided by a cloud provider, where
    the enterprises no longer need to spend time selecting the tools and worry about
    the support and compatibility of those tools. This way, an enterprise can build
    a robust and production-grade container platform using all the services provided
    by a single hyper-scaler such as AWS, Azure, or Google Cloud Platform. However,
    this approach could result in potential vendor lock-ins. The *divorce* would be
    very painful in the long term and it would be an expensive *marriage* with one
    cloud provider:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.1 – CNCF landscape](img/B18145_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – CNCF landscape
  prefs: []
  type: TYPE_NORMAL
- en: Enterprises would pick a multi-cloud platform such as Tanzu that would abstract
    an infrastructure provider, resulting in the portability of workloads deployed
    on the platform. Like the first two approaches, this one also has a drawback.
    Using such multi-cloud platforms might result in a vendor lock-in situation for
    the vendor of the multi-cloud platform provider itself. There is no foolproof
    solution to avoid vendor lock-ins when we use any third-party product. However,
    this risk could be somewhat mitigated if the third-party solution is fully backed
    by open source components, which is exactly the case with TKG.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TKG is a bundle of many different popular open source projects, including Kubernetes
    as a container runtime, the Cluster API for Kubernetes cluster life cycle management,
    Antrea for container overlay networking, Ubuntu and Photon as the node operating
    systems, Fluent Bit for logging, Prometheus and Grafana for monitoring, and many
    others. We will cover them all in the next section. VMware supplies signed binaries
    for all these open source tools that work together for a given release of TKG.
    This way, TKG helps enterprises avoid going through the painful process of tool
    selection, makes them work together, gets them supported, and avoids vendor lock-ins
    to a good extent.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have answered the question *Why Tanzu Kubernetes Grid?* Now, let’s
    understand what is included in the TKG bundle and its core concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Unboxing Tanzu Kubernetes Grid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will review all the building blocks of TKG, including its
    interface and core and extension components. After that, we will understand the
    core concepts of this platform to understand how it works. We have a long way
    to go, so let’s start.
  prefs: []
  type: TYPE_NORMAL
- en: Building blocks of Tanzu Kubernetes Grid
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned in the previous section, TKG is a collection of many open source
    tools that solve different problems that, together, make an enterprise-grade Kubernetes
    platform. We can distribute these components into three categories – interface,
    core, and extensions – as shown in *Figure 7**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Tanzu Kubernetes Grid bundle](img/B18145_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Tanzu Kubernetes Grid bundle
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review all these components to learn about their roles in the TKG bundle.
  prefs: []
  type: TYPE_NORMAL
- en: Interface components of Tanzu Kubernetes Grid
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name suggests, these components include TKG’s interfaces with users and
    infrastructure providers, including vSphere, AWS, and Azure. The following section
    specifies these tools in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Tanzu command-line interface (CLI)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Like a few other products in the Tanzu product portfolio, TKG also uses the
    `tanzu` CLI as its primary user interface. The `tanzu` CLI has a plugin structure
    that allows different Tanzu products to use the same interface. TKG uses the `tanzu`
    CLI for all cluster operations, such as viewing, creating, scaling, deleting,
    and upgrading TKG clusters. We will use this CLI in the next section when we set
    up our TKG foundation. The `tanzu` CLI is a part of the broader open source project
    named Tanzu Framework ([https://github.com/vmware-tanzu/tanzu-framework](https://github.com/vmware-tanzu/tanzu-framework)).
    You can learn more about this CLI here: [https://github.com/vmware-tanzu/tanzu-framework/tree/main/docs/cli](https://github.com/vmware-tanzu/tanzu-framework/tree/main/docs/cli).'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this CLI, the Tanzu portfolio includes a **graphical user interface**
    (**GUI**) tool named **Tanzu Mission Control** for all TKG cluster operations.
    We will learn about this tool in detail in *Chapter 9*, *Managing and Controlling
    Kubernetes Clusters with Tanzu* *Mission Control*.
  prefs: []
  type: TYPE_NORMAL
- en: Tanzu Kubernetes Grid installer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The TKG installer is a GUI component that provides a wizard for installing TKG
    on a selected infrastructure, which could be either vSphere, AWS, or Azure at
    the time of writing. During the initial setup of TKG, a very small bootstrapped
    Kubernetes cluster is deployed on the operator’s workstation (also called a **bootstrap
    machine**). The TKG installer pods get deployed on that local Kubernetes cluster
    to deploy and start the GUI in the bootstrap machine. The operator then uses the
    locally running TKG installer’s GUI to deploy the actual TKG foundation on the
    targeted cloud infrastructure. It seems to be confusing because here, TKG uses
    a (small) Kubernetes platform to deploy a (large) Kubernetes platform. We will
    use this GUI during our TKG installation steps in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that TKG also provides a way to install the foundation using
    the `tanzu` CLI as well, along with the GUI experience. However, it is recommended
    to use the GUI for the first installation as the wizard that’s used in the GUI
    generates an installation configuration file that can later be used with little
    modifications to quickly install other similar TKG foundations using the `tanzu`
    CLI.
  prefs: []
  type: TYPE_NORMAL
- en: kind
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**kind** stands for **Kubernetes inside Docker**. kind is an open source project
    under the umbrella of Kubernetes **Special Interest Groups** (**SIGs**) ([https://kind.sigs.k8s.io/](https://kind.sigs.k8s.io/))
    that allows you to deploy a very tiny Kubernetes cluster as a container running
    inside a Docker environment. kind is a CNCF-conformant Kubernetes installer and
    typically gets deployed in local desktop environments to deploy a small Kubernetes
    cluster. The TKG installer pods get deployed in a kind cluster for bootstrap purposes
    only. Once the actual TKG foundation has been installed, the kind cluster is destroyed,
    along with the running installer components, since their purpose has come to an
    end.'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster API
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kubernetes includes a tool named **Kubeadm** ([https://kubernetes.io/docs/reference/setup-tools/kubeadm/](https://kubernetes.io/docs/reference/setup-tools/kubeadm/))
    that helps configure a server to make it a Kubernetes cluster node. It does this
    by installing the required Kubernetes-specific components. Kubeadm helps the node
    join the cluster as best practice *fast paths* for creating Kubernetes clusters.
    However, Kubeadm does not know how to provision the required infrastructure for
    the given cloud provider. Kubeadm requires the servers to be created with the
    required compute, storage, and networking setup before it can convert those servers
    into Kubernetes nodes. This gap in infrastructure management in Kubeadm is filled
    by the **Cluster API**.
  prefs: []
  type: TYPE_NORMAL
- en: A TKG user would never directly use the Cluster API, but it is one of the most
    important building blocks of TKG that interfaces with and abstracts the underlying
    cloud infrastructure. The term **API** stands for **application program interface**
    – a well-known term in the field of computer programming. The Cluster API is also
    a SIG project ([https://cluster-api.sigs.k8s.io/](https://cluster-api.sigs.k8s.io/)).
    The purpose of this project is to provide an interface layer to platforms such
    as TKG to perform various Kubernetes life cycle operations, including cluster
    provisioning, upgrading, and operating. As we know, different cloud providers
    have different ways of operating their infrastructure. A virtual machine in the
    world of vSphere is called an EC2 machine in the world of AWS. A virtual network
    boundary in AWS is known as a **Virtual Private Cloud** (**VPC**) but it is called
    a **Virtual Network** (**VNet**) in Azure. The implementation of the Cluster API
    interfaces abstracts such cloud-specific terminologies and operational differences.
  prefs: []
  type: TYPE_NORMAL
- en: To provide fully automated Kubernetes cluster life cycle management, TKG uses
    the Cluster API; the Cluster API uses Kubeadm internally. This way, TKG fully
    leverages different open source projects to provide a uniform multi-cloud Kubernetes
    cluster management experience.
  prefs: []
  type: TYPE_NORMAL
- en: Carvel
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Carvel** ([https://carvel.dev/](https://carvel.dev/)) is one additional open
    source package management toolkit that TKG uses for installing itself and other
    optional packages. Carvel is a very powerful and flexible packaging tool for Kubernetes
    deployments. Carvel contains multiple tools for different package management and
    deployment tasks, as listed in the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: '*kapp-controller*: To provide continuous delivery for apps and packages deployed
    on Kubernetes clusters in a GitOps way'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ytt*: To create and use templatized YAML configurations for package deployment,
    allowing package configuration customization during their installations on Kubernetes
    clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*kapp*: To bundle multiple Kubernetes resources as one application package
    that can be installed, upgraded, or deleted as one unit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*kbld*: To build or reference container images used in the Kubernetes resource
    configuration in an immutable way'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*imgpkg*: To package, distribute, and relocate Kubernetes configuration and
    the associated container images as one bundle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*vendir*: To declaratively state a directory’s contents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*secretgen-controller*: To manage various types of secrets used by the packages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After learning about the interface related components of TKG, let’s review the
    core components of TKG now.
  prefs: []
  type: TYPE_NORMAL
- en: Core components of Tanzu Kubernetes Grid
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the interfacing components that we saw previously, TKG contains
    a set of core components that are the most basic building blocks for a Kubernetes
    platform. Let’s learn about them here.
  prefs: []
  type: TYPE_NORMAL
- en: Operating systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the many advantages of using TKG is that you get a supported and hardened
    operating system from VMware for Kubernetes nodes. TKG supports Photon and Ubuntu
    Linux-based operating systems. Additionally, TKG allows you to build among the
    supported and a few other flavors of Linux and Windows operating systems with
    a certain level of customization.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Kubernetes is the main ingredient of TKG. Each version of TKG includes one
    or more versions of upstream open source Kubernetes distributions without any
    modifications. This component is the main Kubernetes platform, including the main
    set of tools that make the platform. It includes the following tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '*kube-apiserver*: An API interface for the Kubernetes control plane'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*etcd*: A key-value store to persist the state of the Kubernetes cluster and
    its workload configuration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*kube-scheduler*: To host newly created pods in a node based on various selection
    criteria'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*kube-controller-manager*: To run different control processes to manage nodes,
    jobs, service endpoints, and service accounts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*cloud-controller-manager*: To run different cloud/infrastructure-specific
    processes to manage node health checks, routes, and cloud-specific service load
    balancers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*kubelet*: A node-residing agent to ensure pods’ running status and report
    back to the control plane'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*kube-proxy*: A node-residing network proxy that ensures service routes to
    their endpoint pods running on the node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Container runtime*: A node-residing software (**containerd** in the case of
    TKG) that runs and manages containers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these core components of Kubernetes, there are a few more core
    components that TKG includes that are required for the platform to operate. Let’s
    take a look.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics Server
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Metrics Server aggregates resource usage data, such as container, node CPU,
    and memory usage, in a Kubernetes cluster and makes it available via the Metrics
    API defined in Kubernetes. This component is required to pull details after using
    the `kubectl top node` or `kubectl top` `pod` command.
  prefs: []
  type: TYPE_NORMAL
- en: Container Storage Interface (CSI)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Container Storage Interface** (**CSI**) is a Kubernetes specification that
    requires implementation from the storage infrastructure provider. This will be
    used in the Kubernetes platform to provide persistent volumes for the workloads
    that need them. It provides Kubernetes users with more options for different storage
    solutions. One Kubernetes cluster may have different types of storage, including
    **solid-state drives** (**SSDs**), **magnetic drives** (**HDDs**), and other variants
    that provide different rates of data input and output. TKG uses the infrastructure-specific
    storage driver for vSphere, AWS, or Azure.'
  prefs: []
  type: TYPE_NORMAL
- en: CoreDNS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**CoreDNS** ([https://coredns.io/](https://coredns.io/)) is an open source
    DNS server that is used to provide Kubernetes service name resolution. The open
    source Kubernetes installs kube-dns for this purpose but allows you to replace
    kube-dns with CoreDNS, which is a more enhanced DNS server. TKG clusters get deployed
    with CoreDNS.'
  prefs: []
  type: TYPE_NORMAL
- en: Container Network Interface (CNI)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As per the Kubernetes networking specification, every pod in a cluster should
    have a unique IP address and should be able to communicate with other pods on
    any other node of the cluster without any **network address translation** (**NAT**).
    Additionally, all the agents running in nodes, such as kubelet, should be able
    to communicate with each pod running on their respective node. These requirements
    ensure smooth communication between apps deployed on the same cluster. However,
    we need a specific networking arrangement to implement this specification. This
    is the CNI implementation, which is also known as the overlay network for Kubernetes
    clusters. There are many CNI implementations that we can choose from to be used
    in Kubernetes clusters. Out of them, TKG supports **Antrea** ([https://antrea.io/](https://antrea.io/))
    and **Calico** ([https://www.tigera.io/project-calico/](https://www.tigera.io/project-calico/)),
    which we can choose from during platform setup.
  prefs: []
  type: TYPE_NORMAL
- en: Control plane load balancers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: TKG can create multi-master Kubernetes clusters for high availability of the
    control plane objects of a Kubernetes cluster. Such clusters typically have three
    Kubernetes control plane (master) nodes serving Kubernetes API traffic and performing
    crucial workload management activities. For a TKG deployment on AWS and Azure,
    TKG creates their respective virtual load balancer objects to front these control
    plane nodes for the API server traffic distribution. For vSphere, TKG includes
    **NSX Advanced Load Balancer** ([https://www.vmware.com/products/nsx-advanced-load-balancer.html](https://www.vmware.com/products/nsx-advanced-load-balancer.html))
    to create a virtual load balancer for the same purpose. However, if TKG is not
    configured with NSX Advanced Load Balancer on vSphere, it uses an open source
    and lightweight virtual load balancer named **kube-vip** ([https://kube-vip.io/](https://kube-vip.io/)).
    kube-vip is also a CNCF-governed project.
  prefs: []
  type: TYPE_NORMAL
- en: Extension packages of Tanzu Kubernetes Grid
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The open source Kubernetes distribution comes with the minimal components required
    to deploy a Kubernetes platform. However, to deploy a production-grade Kubernetes
    environment, we need several other capabilities, such as logging, monitoring,
    access control, backup and restore, and more. Since TKG is an enterprise-grade
    Kubernetes distribution, it also comes with many such open source extension packages
    with VMware-signed binaries. Let’s check these components out.
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Fluent Bit** ([https://fluentbit.io/](https://fluentbit.io/)) is a high-performance-focused
    open source log forwarding tool for different flavors of Linux and Windows operating
    systems. Fluent Bit is also a CNCF sub-project. The purpose of this tool is to
    process logs emitted from Kubernetes nodes and the workload pods and can be configured
    to plumb them to a long list of possible log aggregation destinations, including
    Splunk, Elasticsearch, Amazon CloudWatch, Kafka, and many more.'
  prefs: []
  type: TYPE_NORMAL
- en: Ingress controller
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: TKG supplies **Contour** ([https://projectcontour.io/](https://projectcontour.io/))
    binaries as an extended package to provide an ingress type of routing for the
    external-facing services deployed on Kubernetes clusters. Contour is an open source
    project under the CNCF umbrella that internally uses the **Envoy proxy** ([https://www.envoyproxy.io/](https://www.envoyproxy.io/)),
    another CNCF open source project. Together with Envoy (as the data plane), Contour
    (as the control plane) provides the required implementation of the ingress controller
    to provide the HTTP-level (network layer 7) service routing defined using the
    ingress resources of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Identity and authentication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: TKG includes **Pinniped** ([https://pinniped.dev/](https://pinniped.dev/)),
    another CNCF open source project backed by VMware to provide an easy button for
    the Kubernetes cluster’s user identity and authentication management. The upstream
    Kubernetes distribution does not include any authentication mechanism and only
    provides the configuration for authorization. Hence, to allow cluster users to
    get authenticated using existing identity providers based on a **Lightweight Directory
    Access Protocol** (**LDAP**) server or **Open ID Connect** (**OIDC**), TKG supplies
    Pinniped as an extension package.
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For cluster observability, TKG also supplies the signed binaries for **Prometheus**
    ([https://prometheus.io/](https://prometheus.io/)) and **Grafana** ([https://github.com/grafana/grafana](https://github.com/grafana/grafana)),
    two very popular open source monitoring tools for the Kubernetes ecosystem. Here,
    Prometheus is a metrics aggregator engine and Grafana is a metrics rendering tool
    for visualization. In addition to these *batteries included* monitoring tools,
    TKG also supports first-class integration with VMware Aria operations for Applications,
    a **Software-as-a-Service** (**SaaS**) platform in the VMware portfolio, for more
    capabilities around scaling, functionality, and power for full stack observability.
    We will cover this product in detail in [*Chapter 10*](B18145_10.xhtml#_idTextAnchor193),
    *Realizing Full-Stack Visibility with VMware Aria Operations* *for Applications*.
  prefs: []
  type: TYPE_NORMAL
- en: Container registry
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: TKG also comes with **Harbor**, a purpose-built container registry, as an extended
    package that can be installed in the cluster if required. We covered Harbor extensively
    in [*Chapter 6*](B18145_06.xhtml#_idTextAnchor112)*, Managing Container Images*
    *with Harbor*.
  prefs: []
  type: TYPE_NORMAL
- en: Backup and restore
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Disaster recovery is a very important aspect of platform-supporting production
    applications. A Kubernetes cluster is not any different running critical production
    workloads. However, Kubernetes does not include anything to back up and restore
    the state of the workloads running on its clusters. To fill this gap, TKG includes
    **Velero** ([https://velero.io/](https://velero.io/)) as an extension package
    in the bundle. Velero is also an open source project under the CNCF umbrella.
    Velero provides ways to take backups and restore them later at the cluster level,
    Kubernetes namespace level, or for specific workloads based on their attached
    metadata. Velero can also take backups of the persistent volumes containing stateful
    application data and restore them when required. This is the tool that is used
    under the hood of Tanzu Mission Control for backup and recovery features. We will
    cover this in detail in *Chapter 9**, Managing and Controlling Kubernetes Clusters
    with Tanzu* *Mission Control*.
  prefs: []
  type: TYPE_NORMAL
- en: ExternalDNS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: TKG also supplies **ExternalDNS** ([https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns)),
    an open source Kubernetes SIG project, as an extension package. ExternalDNS allows
    you to control the DNS records dynamically for the external-facing services deployed
    on the cluster using a Kubernetes resource definition file in a way that is agnostic
    to a DNS provider. The external services running on a Kubernetes cluster can get
    the required DNS record binding handled by ExternalDNS on a linked DNS server
    such as AWS Route53 or Google Cloud DNS. In a way, it provides a way to control
    external DNS configurations using Kubernetes resource definitions.
  prefs: []
  type: TYPE_NORMAL
- en: cert-manager
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**cert-manager** ([https://cert-manager.io/](https://cert-manager.io/)) is
    another CNCF open source project that TKG includes as an extension to manage X.509
    (identity) certificates used in a Kubernetes cluster. cert-manager obtains certificates
    from a variety of configured certificate issuers, ensures certificate validity,
    and tries to renew them before expiry.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen what components TKG contains, let’s learn about some of
    the important concepts of this platform.
  prefs: []
  type: TYPE_NORMAL
- en: Important concepts of Tanzu Kubernetes Grid
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TKG is a distributed system with several moving parts. To understand how TKG
    works, we need to learn a few concepts of this system. Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed earlier in this chapter, TKG uses a kind cluster to deploy a TKG
    foundation on the selected infrastructure. This kind cluster is very small and
    runs in a Docker container in the operator’s workstation, which is typically a
    personal computer. This kind cluster contains the required machinery, including
    the Tanzu installation portal and other components that help bootstrap a TKG foundation.
    Because of this, this kind cluster is also known as a bootstrap cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Tanzu Kubernetes releases (TKrs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A TKG deployment may support multiple different versions of Kubernetes. A TKr
    is a custom resource definition under TKG that contains a reference to one such
    Kubernetes version that TKG can deploy and manage. TKrs include components such
    as Antrea with its linked version definition for the given Kubernetes version.
    The management cluster of TKG runs a TKr controller that keeps checking the public
    registry for a new Kubernetes version availability. Once a new version is available,
    the TKr controller downloads the required artifacts on the TKG management cluster
    to make it available for use. This way, one TKG management cluster may deploy
    and manage multiple versions of Kubernetes clusters supported under that TKG version.
    This arrangement provides flexibility to different teams wanting to run their
    applications on different Kubernetes versions that are still managed by the same
    TKG control plane.
  prefs: []
  type: TYPE_NORMAL
- en: Management cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Every TKG foundation has one management cluster. A management cluster is nothing
    but a Kubernetes cluster running specific workloads used to life cycle other Kubernetes
    clusters. A TKG foundation and thus its management cluster is infrastructure specific.
    Because of that, we need one management cluster for vSphere, one for AWS, and
    one for Azure if we want to deploy TKG clusters on all the platforms. This is
    because a management cluster contains underlying cloud-specific Cluster API configuration.
    This way, a management cluster can create multiple Kubernetes workload clusters
    on the same cloud platform where it is deployed. In addition to creating, scaling,
    upgrading, and deleting a Kubernetes cluster, the management cluster also keeps
    track of the actual versus the desired state of the Kubernetes cluster nodes if
    it is configured to do so. The management cluster restarts or recreates an unhealthy
    or missing node from a cluster it manages. A management cluster is just a normal
    Kubernetes cluster and can also run any custom app, but it should only be used
    for its main purpose, which is to operate a large number of Kubernetes clusters.
    Its access and permission should be very much restrictive to the TKG platform
    operations team considering the level of access it has over other Kubernetes clusters
    managed by it. The TKG platform operators may give limited access to a management
    cluster using Kubernetes namespaces under the management cluster. This way, different
    teams can use the same management cluster to life cycle their Kubernetes clusters
    linked with a specific namespace of the management cluster. In addition to the
    upstream Kubernetes components, a TKG management cluster has the following TKG-specific
    components deployed into it:'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster API components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cert-manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: secretgen-controller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kapp-controller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tkr-controller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workload cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A workload cluster is a normal Kubernetes cluster created by a management cluster.
    A workload cluster is where we deploy our apps. Depending on the organization’s
    practices and scale, different teams and their application environments may use
    separate workload clusters. The size in terms of the number of nodes in a workload
    cluster is only limited to the infrastructure availability. However, it is recommended
    to keep a cluster size as small as possible to reduce the blast radius if something
    goes wrong and for quicker maintenance time. TKG makes it very easy to manage
    many Kubernetes clusters. As we discussed previously, workload clusters under
    a management cluster may have different supported versions of Kubernetes based
    on the requirements of the teams using them.
  prefs: []
  type: TYPE_NORMAL
- en: Node pool
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A TKG cluster may have different types of worker nodes that have different configurations
    and resources attached to a cluster. Such heterogeneous node types allow different
    kinds of workloads to leverage them for specific resource requirements or just
    for isolation purposes. For example, a workload cluster may have some number of
    nodes with a consumable **graphical processing unit** (**GPU**) that can be utilized
    by extremely compute-hungry machine learning workloads deployed on the cluster.
    We can add such different types of nodes to a workload cluster using **node pools**
    in TKG. Later, we can configure such nodes with taints and tolerations to only
    allow the workloads that need to use certain types of nodes to be scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment topologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TKG supports two different deployment topologies for creating the management
    and workload clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dev Plan**: In this topology, TKG creates a single control plane node and
    the required number of worker nodes. This plan is used for non-critical deployments
    that can tolerate the unavailability of the Kubernetes API server, etcd, and other
    control plane functions. This topology requires fewer resources and is typically
    used for lab environments.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prod Plan**: In this topology, TKG creates three control plane nodes and
    fronts them with a load balancer to provide a highly available Kubernetes control
    plane. As the name suggests, it is used for clusters that will host critical workloads
    and may not tolerate any control plane downtime for cluster operations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 7**.3* shows how TKG works at a high level. As shown in this figure,
    an operator uses either the `tanzu` CLI or the TKG bootstrap UI to supply the
    required configuration for the foundation. Once the management cluster has been
    created, the operator can directly use it to create the required number of workload
    clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – TKG layout](img/B18145_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – TKG layout
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to create a TKG management cluster on
    AWS and understand the operation flow in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Tanzu Kubernetes Grid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TKG, being a multi-cloud solution, can be installed on a vSphere-based on-premises
    environment, or Microsoft Azure and **Amazon Web Services** (**AWS**)-based public
    cloud platforms. To keep this chapter to an acceptable length, we will only cover
    how to install and configure a TKG foundation on AWS. You may find additional
    TKG installation and configuration details here: [https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-mgmt-clusters-prepare-deployment.html](https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-mgmt-clusters-prepare-deployment.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will perform the following tasks in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: Configure the bootstrap machine – the operator workstation from where the installation
    will be triggered
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy the TKG management cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a TKG workload cluster using the management cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain access to the workload cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But before we do that, we need to ensure that the following prerequisites are
    met to complete these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the prerequisites to follow the TKG installation instructions
    given in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'An AWS account with the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An access key and an access key secret
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An SSH key pair registered with the account for the region where TKG is being
    installed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Permission to create a CloudFormation stack that defines **Identity and Access
    Management** (**IAM**) resources and their permissions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A sufficient resource quota that’s allowed to create two **Virtual Private Clouds**
    (**VPCs**), nine subnets (two internet-facing and one internal per availability
    zone) in the VPC, four EC2 security groups, two internet gateways, three NAT gateways,
    and three Elastic IP addresses in the selected region for TKG deployment
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Linux or Mac workstation with the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Internet access
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Command-line access
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Web browser access
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Port `6443` access to all the EC2 instances to access Kubernetes APIs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Desktop installed and running with 6 GB allocated
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 2-core CPU
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The kubectl CLI v1.22 or higher
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a **Network Time Protocol** (**NTP**) server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to [https://my.vmware.com/](https://my.vmware.com/) with an account set
    up to download the required binaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with the first task, which is to prepare the bootstrap machine that
    will be used for this installation. We will need a few tools and environmental
    configurations before we begin the installation.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the bootstrap machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following sub-tasks prepare a bootstrap machine with the required tools
    and configuration for TKG setup on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Tanzu and the Kubectl CLI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a directory on your local machine where you will store the required
    artifacts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Go to [https://my.vmware.com/](https://my.vmware.com/) and log in using your
    My VMware credentials.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to [https://customerconnect.vmware.com/downloads/details?downloadGroup=TKG-154&productId=1162](https://customerconnect.vmware.com/downloads/details?downloadGroup=TKG-154&productId=1162)
    to download the required artifacts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make sure that the selected version in the dropdown is **1.5.4**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Selecting a download version](img/B18145_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Selecting a download version
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the **Product Downloads** tab, scroll to the **VMware Tanzu CLI 1.5.4**
    section and download the binary for your operating system. Note that the procedure
    followed in this chapter is being done on a macOS machine. While most of the commands
    listed in this chapter should work on the other platforms, there could be some
    minor differences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Downloading the appropriate Tanzu CLI](img/B18145_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Downloading the appropriate Tanzu CLI
  prefs: []
  type: TYPE_NORMAL
- en: 'Accept the End User License Agreement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Accepting the End User License Agreement](img/B18145_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Accepting the End User License Agreement
  prefs: []
  type: TYPE_NORMAL
- en: Download the binary into the `$HOME/tkg-154` directory that we created in *step
    1*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, on the same page when you downloaded the binaries, go to the **Kubectl
    1.22.9 for VMware Tanzu Kubernetes Grid** **1.5.4** section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Downloading the appropriate Kubectl CLI](img/B18145_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Downloading the appropriate Kubectl CLI
  prefs: []
  type: TYPE_NORMAL
- en: Download the binary into the same `$``HOME/tkg-154` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go into the `$HOME/tkg-154` directory and extract the CLI binaries you downloaded
    previously. Run the following commands to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the Tanzu CLI on you local system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify the installation by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see version 0.11.6 in the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the Tanzu CLI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Clean up any pre-existing Tanzu plugins for a clean start:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make sure you are under the `$HOME/tkg-154` directory, which contains the extracted
    `cli` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following command to install all the required plugins for this TKG
    release:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should be able to see the following output for the command’s execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the plugin’s installation status by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should be able to see all the plugins listed, along with their versions
    and statuses, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Installed TKG plugin list](img/B18145_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Installed TKG plugin list
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s install the Kubectl CLI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands from the `$HOME/tkg-154` directory, which is where
    we downloaded and extracted the Kubectl CLI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Verify the installation by running the `kubectl version` command. You should
    see the client version as v1.22.9+vmware.1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing Carvel tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed in the previous section, TKG uses the Carvel toolkit for its
    packaging and installation. For that reason, we will need some of the Carvel tool’s
    CLI binaries installed in the bootstrap machine. The Tanzu CLI bundle that we
    previously downloaded and extracted contains all these tools. The following steps
    describe the procedure to install them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the `cli` directory under `$HOME/tkg-154`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install **ytt** with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Verify the `ytt --``version` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install **kapp** with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Verify the `kapp --``version` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install **kbld** with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Verify the `kbld --``version` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install **imgpkg** with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Verify the `imgpkg --``version` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing AWS-specific tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deploying a TKG foundation on the AWS platform requires the **aws** CLI to
    be installed on the bootstrap machine. Let’s configure this:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the **aws** CLI using the instructions provided at [https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verify the installation of the **aws** CLI by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should be able to see the CLI version listed, as shown in the following
    output. The version could be different in your case, depending on when it is installed,
    but it should be v2.x:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command to configure the access profile for your AWS account
    to be used for this installation with the previously defined permissions and quotas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Supply the values of the access key, secret access key, region, and command
    output format, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, you may replace `us-east-1` with any other AWS region of your choice with
    the previously listed prerequisites fulfilled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to ensure you can see the existing SSH key pair in
    the region as it was listed in the prerequisites:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command’s output should show at least one key pair listed.
  prefs: []
  type: TYPE_NORMAL
- en: We now have all the required tools configured in the bootstrap machine to begin
    installing the TKG management cluster. Let’s begin.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the management cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will use the Tanzu installer UI to deploy the management
    cluster on the configured AWS account.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the upstream Kubernetes distribution does not come with user authentication
    capabilities and allows the user to have admin-level access. To fill this gap,
    TKG comes with Pinniped to integrate an external LDAP or OIDC identity provider.
    To minimize installation prerequisites and complexity, we will not use such an
    integration, which requires a pre-existing LDAP or OIDC setup access. A real-life
    TKG cluster should never be configured without such integration with an external
    identity provider. You can learn more about this topic here: [https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-iam-configure-id-mgmt.html](https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-iam-configure-id-mgmt.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, despite aiming for minimal complexity and an infrastructure footprint,
    following this **TKG configuration on AWS will incur cloud service usage charges
    in your AWS account** as it will use the EC2 instance types that are not qualified
    for the free-tier credits, along with some other chargeable services such as Elastic
    IP, NAT gateways, EBS volumes, Elastic Load Balancers, and a few others. If you
    plan to follow this guide to install TKG on AWS, it is recommended that you also
    clean up the resources using the procedure described later in this chapter, followed
    by a manual inspection to verify the removal of all provisioned resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps outline the installation procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure the local Docker daemon is running to deploy and run containers on the
    bootstrap machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following command to start the installer UI in the default browser
    window of the bootstrap workstation. This UI supports Chrome, Safari, Firefox,
    Internet Explorer, and Edge, along with their considerably newer versions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command should automatically open the browser window with the UI running.
    Otherwise, it can be accessed using `http://127.0.0.1:8080/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **DEPLOY** button under **Amazon Web Services**, as highlighted
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Selecting Amazon Web Services for deployment](img/B18145_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Selecting Amazon Web Services for deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the necessary AWS account details by following these sub-steps; these
    are highlighted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set **AWS CREDENTIAL TYPE** to **Credential Profile**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **tkg** from the **AWS CREDENTIAL PROFILE** dropdown and its associated
    **REGION** that we configured during the **aws** CLI setup earlier in this chapter.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **CONNECT** button to ensure the UI can connect to the select AWS
    account using the previously created configuration.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click the **NEXT** button to move to the next configuration section:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Selecting an AWS account for the installation](img/B18145_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Selecting an AWS account for the installation
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **NEXT** button to choose the default VPC configuration, as shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Choosing the default VPC configuration](img/B18145_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Choosing the default VPC configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the management cluster deployment plan to **Development** and **INSTANCE
    TYPE** to **t3.large**. This instance type has 2 vCPUs and 8 GiB memory, which
    is just good enough for a lab-like setup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Selecting the management cluster’s deployment plan and instance
    type](img/B18145_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – Selecting the management cluster’s deployment plan and instance
    type
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter other details of the management cluster, as listed in the following sub-steps
    and shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a short name under `tkg-aws-mgmt-cluster`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the name of the SSH key-pair that’s available to the account you obtained
    in *step 5* of the **aws** CLI configuration under **EC2** **KEY PAIR**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Leave the default selection as-is for the checkbox options.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select one of the availability zones from the **AVAILABILITY ZONE 1** dropdown
    for the selected region to deploy the management cluster into. For the **Production**
    deployment plan, we need to select three availability zones for the three control
    plane nodes of the management cluster.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **t3.large** as the worker node instance type from the **AZ1 WORKER NODE
    INSTANCE** **TYPE** dropdown.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click the **NEXT** button to move on:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Entering the management cluster’s details](img/B18145_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Entering the management cluster’s details
  prefs: []
  type: TYPE_NORMAL
- en: 'Optionally, enter management cluster metadata and click the **NEXT** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Entering the management cluster’s metadata](img/B18145_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – Entering the management cluster’s metadata
  prefs: []
  type: TYPE_NORMAL
- en: 'Leave the default Kubernetes container network configuration as-is and click
    the **NEXT** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Leaving the default Kubernetes network configuration as-is](img/B18145_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Leaving the default Kubernetes network configuration as-is
  prefs: []
  type: TYPE_NORMAL
- en: 'Disable the identity management settings and click the **NEXT** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Disabling identity management settings](img/B18145_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Disabling identity management settings
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the management cluster’s operating system image, as highlighted in the
    screenshot, and click the **NEXT** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Selecting the management cluster’s operating system](img/B18145_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – Selecting the management cluster’s operating system
  prefs: []
  type: TYPE_NORMAL
- en: 'Optionally, choose to participate in VMware’s **Customer Experience Improvement
    Program** (**CEIP**) and click the **NEXT** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Choosing to participate in the Customer Experience Improvement
    Program](img/B18145_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Choosing to participate in the Customer Experience Improvement
    Program
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **REVIEW CONFIGURATION** button to verify the inputs before triggering
    the management cluster creation on AWS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Opening the configuration summary for review](img/B18145_07_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – Opening the configuration summary for review
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the bottom of the verification summary page.
    The command that’s displayed can be used in the future to trigger the same deployment
    again using the Tanzu CLI. The **EXPORT CONFIGURATION** link at the bottom-right
    corner allows us to export this configuration in a file that can be used as a
    reference to deploy other management clusters on AWS with the required modifications.
    Finally, click the **DEPLOY MANAGEMENT CLUSTER** button to trigger the deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.20 – Triggering the management cluster’s deployment](img/B18145_07_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – Triggering the management cluster’s deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see various deployment logs and their installation statuses, as shown
    in the following screenshot. As you can see in the logs, the Cluster API is creating
    the required infrastructure component to deploy the management cluster on AWS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Deployment status with logs](img/B18145_07_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 – Deployment status with logs
  prefs: []
  type: TYPE_NORMAL
- en: 'The installation should ideally be completed in about 10 to 15 minutes and
    you should see a success message, as shown in the following screenshot. As highlighted
    in this screenshot, the logs also highlight how to access the management cluster
    from the bootstrap machine to perform different TKG operations. You should also
    be able to see these logs on the command line from where you fired the `tanzu
    mc create --ui` command, which brought up the installation browser window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.22 – Successful management cluster installation](img/B18145_07_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.22 – Successful management cluster installation
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have finished installing the management cluster, let’s learn how
    to use it to create our first TKG workload cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a workload cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps outline the procedure to access the newly created TKG management
    cluster using the bootstrap machine from where we triggered the management cluster
    installation. As a part of the installation steps, TKG adds the `kubeconfig` details
    to the bootstrap machine to allow administrator-level access to the management
    cluster and hence the Tanzu CLI pointing to the management cluster. Let’s use
    the management cluster and the Tanzu CLI to create our first workload cluster.
    The following steps will be performed on the bootstrap machine:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to set the kubectl context pointing to the newly
    created TKG management cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the `tanzu mc get` command to view the details of the management cluster.
    The command will show that the management cluster has one control plane and one
    worker node created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.23 – Management cluster details](img/B18145_07_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.23 – Management cluster details
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a workload cluster, we need to supply the workload cluster configuration
    file to the management cluster. It contains the following details:'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster deployment plan (Development or Production)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worker node count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worker node EC2 type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS-specific configurations such as region, AZ, network, and SSH key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node operating system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node health check configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node-level auto-scaling configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do not need to include all the attributes in the configuration file and may
    specify only the required attributes. For a broader list of attributes for the
    configuration file, visit [https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-tanzu-k8s-clusters-aws.html#tanzu-kubernetes-cluster-template-0](https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-tanzu-k8s-clusters-aws.html#tanzu-kubernetes-cluster-template-0).
  prefs: []
  type: TYPE_NORMAL
- en: We will download a preconfigured file from this book’s GitHub repository and
    use it to create a workload cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the workload cluster configuration file into the bootstrap machine using
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The config file should be available in the `$HOME/tkg-154/` directory and be
    called `tkg-workload-cluster-config.yaml`. Open the file in your choice of editor
    and update the following parameter values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update **AWS_NODE_AZ**, **AWS_NODE_AZ_1** and **AWS_NODE_AZ_2** based on the
    selected region if you have used any AWS region other than **us-east-1**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update **AWS_REGION** as per your **AWS_PROFILE** configuration if applicable.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update **AWS_SSH_KEY_NAME** to use the SSH key in your AWS account in the selected
    region. This is a must-change.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following command to create the workload cluster using the configuration
    file we prepared in the previous step. Here, we are creating the cluster with
    Kubernetes v1.21.11 using the `--tkr` option so that we can learn how to upgrade
    the cluster later to v.1.22.9:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The workload cluster should be created in about 10 to 15 minutes if all the
    configuration changes are done correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify the cluster’s creation status using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see that the cluster is running, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.24 – Verifying the workload cluster’s creation](img/B18145_07_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.24 – Verifying the workload cluster’s creation
  prefs: []
  type: TYPE_NORMAL
- en: With that, the TKG workload cluster has been created. Now, let’s access the
    workload cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to obtain `kubeconfig` for the workload cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following command to switch the `kubectl` context so that it points
    to the workload cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following command to list the nodes of the workload cluster to ensure
    connectivity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should be able to see the list of Kubernetes nodes, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.25 – Verifying the workload cluster’s connectivity](img/B18145_07_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.25 – Verifying the workload cluster’s connectivity
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have completed all the tasks required to get started with TKG.
    We started with the bootstrap machine’s configuration by installing all the required
    tools and CLIs. After that, we created a TKG management cluster on AWS using the
    Tanzu installer UI. And finally, we created a TKG workload cluster using the management
    cluster that we created. It is worth noting that one bootstrap machine may have
    a reference to more than one TKG management cluster. The operator may use different
    management clusters to manage the workload clusters under them by just switching
    the `kubectl` context to an appropriate management cluster config, followed by
    using the `tanzu login` command to get authenticated for the management cluster
    usage. Now, in the next and the last section of this chapter, we will learn about
    some of the most common day-2 activities around TKG.
  prefs: []
  type: TYPE_NORMAL
- en: Common day-2 operations with Tanzu Kubernetes Grid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a fully configured and running TKG foundation on AWS, let’s
    learn how to perform some of the day-2 operations on it. TKG makes these operations
    very trivial as they do all the heavy lifting behind the scenes:'
  prefs: []
  type: TYPE_NORMAL
- en: Scale a cluster to add or remove nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrade a cluster to bump up the Kubernetes version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete the entire TKG foundation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by scaling the workload cluster so that it has three worker nodes
    instead of just one.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling a Tanzu Kubernetes Grid cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Run the following commands to scale the workload cluster we created:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Switch the `kubectl` context so that it’s pointing to the management cluster
    that we previously created, which we used to create the workload cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Ensure that the workload cluster has only one worker node by running the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output, which shows 1/1 worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.26 – Ensuring the worker node count](img/B18145_07_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.26 – Ensuring the worker node count
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to add two more worker nodes, creating the desired
    total count of three:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following message, showing that the scaling is in progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.27 – Worker node scaling in progress](img/B18145_07_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.27 – Worker node scaling in progress
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that the scaling has been done by running the following cluster listing
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should now see 3/3 worker nodes in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.28 – Confirming that the cluster scaling has been done](img/B18145_07_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.28 – Confirming that the cluster scaling has been done
  prefs: []
  type: TYPE_NORMAL
- en: These steps showed you how to scale up a TKG cluster. The same procedure is
    also applicable to scale down a cluster. The `-w` option of the `scale` command
    declares the desired count of the worker nodes. And depending on the changes in
    the desired count, TKG adds or removes the worker nodes. The scaling command also
    has options to scale the control plane nodes or the nodes of a specific node pool.
    You can learn more about scaling by running the `tanzu cluster scale --``help`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading a Tanzu Kubernetes Grid cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we’ve scaled, let’s learn how to upgrade the TKG workload cluster
    to deploy a newer version of Kubernetes. TKG allows such on-demand upgrades of
    selected clusters under a management cluster. Here, the owners of the cluster
    have the choice of which Kubernetes version they need so that they have enough
    time to prepare to upgrade their workload clusters. The following steps outline
    the upgrade procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Switch the `kubectl` context so that it’s pointing to the management cluster
    that we previously created, which we used to create the workload cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Ensure that the workload cluster is deployed with Kubernetes v1.21.11 by using
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output, showing that the cluster has been deployed
    with Kubernetes version 1.21.11:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.29 – Checking the current Kubernetes version of the cluster](img/B18145_07_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.29 – Checking the current Kubernetes version of the cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to check the available Kubernetes version(s) for
    the upgrade:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output, which shows v1.22.9 as an option to upgrade
    v1.21.11, as highlighted in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.30 – Checking the available version upgrade options](img/B18145_07_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.30 – Checking the available version upgrade options
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to upgrade the workload cluster to Kubernetes version
    1.22.9:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `--tkr` option mentions the target version for the upgrade that we picked
    from the available version list in the previous step. This command will upgrade
    the workload cluster in a rolling manner, one node at a time, to minimize workload
    downtime. The applications running with more than one pod would not face any downtime
    during such rolling upgrades. After firing the preceding command, you should see
    the following confirmation message in about 15 to 20 minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.31 – Cluster upgrade log](img/B18145_07_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.31 – Cluster upgrade log
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the recycling of the workload cluster nodes
    to create the new version on the AWS EC2 console. Here, you can see that the old
    nodes got terminated and that the new nodes with newer versions were created to
    replace them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.32 – Cluster node recycling on the AWS EC2 console](img/B18145_07_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.32 – Cluster node recycling on the AWS EC2 console
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to ensure the workload cluster is running with the
    newer Kubernetes version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following screenshot shows that the workload cluster is running on Kubernetes
    version 1.22.9:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.33 – Confirming the workload cluster’s upgrade](img/B18145_07_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.33 – Confirming the workload cluster’s upgrade
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to allowing the Kubernetes release to be upgraded, the `tanzu cluster
    upgrade` command also allows you to upgrade the cluster for a specific operating
    system and its versions. Run the `tanzu cluster upgrade –help` command to learn
    more about it. In addition to upgrading a TKG cluster for these reasons, there
    is also another dimension for the upgrades – upgrading the TKG version itself.
    Upgrading a TKG version (from TKG v1.5.x to v1.5.y or from v1.4.x to v1.5.y) is
    beyond the scope of this book. However, you can learn more about that here: [https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-upgrade-tkg-index.html](https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.5/vmware-tanzu-kubernetes-grid-15/GUID-upgrade-tkg-index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a Tanzu Kubernetes Grid workload cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Destructions are always easier than constructions. This is the same case with
    TKG workload clusters. The following simple steps outline the procedure to delete
    the TKG workload cluster that we have used so far in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Switch the `kubectl` context so that it’s pointing to the management cluster
    that we previously created, which we used to create the workload cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following command to delete the workload cluster, along with its resources
    on your AWS account:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following confirmation message on the console for the cluster
    deletion in progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.34 – Cluster deletion in progress](img/B18145_07_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.34 – Cluster deletion in progress
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot from the AWS EC2 console shows that all the nodes
    for the workload clusters have been terminated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.35 – Terminated cluster nodes on the AWS EC2 console](img/B18145_07_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.35 – Terminated cluster nodes on the AWS EC2 console
  prefs: []
  type: TYPE_NORMAL
- en: 'Along with the EC2 instances, TKG (with the help of the Cluster API) also deletes
    other network resources that have been created for the deleted cluster on your
    AWS account. As you may have assumed, deleting a Kubernetes cluster is an extremely
    sensitive operation that could result in extended downtime for the applications
    running on it. Extensive measures should be taken to prevent access to such operations
    from environments other than a lab. Now, let’s look at an even more destructive
    operation: deleting the entire TKG foundation, including the management cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a Tanzu Kubernetes Grid foundation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You would rarely need to delete a TKG foundation from its roots except in such
    a lab environment. Nevertheless, we will cover this TKG life cycle activity in
    this chapter. To delete a TKG foundation, we just need to delete the management
    cluster that we created for the same. And like deleting a workload cluster, deleting
    a management cluster is also a simple but highly destructive process. The following
    steps outline the procedure for this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure you are pointing to the right Kubernetes cluster for the `kubectl` context
    by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following command to delete the management cluster, along with its
    resources on your AWS account:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You may need to replace the region name in the command based on the deployed
    region of the management cluster. Upon command execution, you should see the following
    logs on the console for the cluster deletion in progress because of the `--verbose`
    option, followed by the logging detail level. This command takes the log verbose
    level from 0 to 9, with 9 being the most detailed log:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.36 – Management cluster deletion logs](img/B18145_07_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.36 – Management cluster deletion logs
  prefs: []
  type: TYPE_NORMAL
- en: You may have figured out from the logs that TKG created a kind cluster on the
    bootstrap machine to do all the required cleanup to delete the TKG foundation
    from the AWS account. This is the same approach that TKG uses while creating a
    management cluster, as we saw earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have completed some of the important day-2 activities around TKG
    clusters. Now, let’s wrap up this chapter with a quick summary of what we have
    learned.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of this chapter, we discussed some of the reasons why TKG could
    be a good choice for being a Kubernetes-based container platform. As we saw during
    the hands-on activities, TKG makes Kubernetes platform deployment and management
    very easy and operationally efficient by providing a uniform interface – the Tanzu
    CLI. All the Tanzu CLI-based operations we performed in this chapter were infrastructure-agnostic,
    providing the required muti-cloud ease of operations.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the limited scope of TKG in this book, we could not install and use
    all the optional extensions that TKG provides, but we covered them briefly to
    understand their applications. We saw how extensively TKG uses various cherry-picked
    open source tools from the CNCF ecosystem. This way, TKG is a solution completely
    backed by the open source community.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned about the common day-1 and day-2 activities on the TKG platform,
    starting with installing a platform on AWS and creating a workload cluster to
    host actual application containers. Following this, we learned how to add more
    capacity for that workload cluster with on-demand scaling. We also learned how
    easily we can upgrade the cluster for different reasons and finally how to delete
    the cluster and the foundation if required.
  prefs: []
  type: TYPE_NORMAL
- en: As you may have assumed, we have not covered many topics around this subject
    to keep this chapter’s length concise. However, we will learn about TKG clusters’
    backup and restore, compliance scanning, and governance policy configurations
    in *Chapter 9**, Managing and Controlling Kubernetes Clusters with Tanzu Mission
    Control,* for Tanzu Mission Control, a single pane of glass that controls hundreds
    of Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: TKG is a commercially available licensed software provided by VMware. In the
    next chapter we will go deep into the Tanzu developer experience with **Tanzu**
    **Application Platform**.
  prefs: []
  type: TYPE_NORMAL
