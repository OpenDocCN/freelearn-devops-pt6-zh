- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Running Kubernetes in Production
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在生产环境中运行Kubernetes
- en: In the previous chapter, we discussed governance and policy engines. This is
    an important part of managing large-scale Kubernetes-based systems in production.
    However, it is only one part. In this chapter, we will turn our attention to the
    overall management of Kubernetes in production. The focus will be on running multiple
    Managed Kubernetes clusters in the cloud.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了治理和策略引擎。这是管理大规模基于Kubernetes的生产系统的一个重要部分。然而，它只是其中的一部分。在本章中，我们将重点关注Kubernetes在生产环境中的整体管理。重点将放在如何在云中运行多个托管Kubernetes集群。
- en: 'The topics we will cover are:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖的主题包括：
- en: Understanding Managed Kubernetes in the cloud
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解云中的托管Kubernetes
- en: Managing multiple clusters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理多个集群
- en: Building effective processes for large-scale Kubernetes deployments
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为大规模Kubernetes部署构建有效的流程
- en: Handling infrastructure at scale
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理大规模基础设施
- en: Managing clusters and node pools
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理集群和节点池
- en: Upgrading Kubernetes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 升级Kubernetes
- en: Troubleshooting
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障排除
- en: Cost management
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本管理
- en: Understanding Managed Kubernetes in the cloud
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解云中的托管Kubernetes
- en: '**Managed Kubernetes** is a service provided by cloud providers such as **Amazon
    Web Services** (**AWS**), **Google Cloud Platform** (**GCP**), and **Microsoft
    Azure** that simplifies the deployment, management, and scaling of containerized
    applications in the cloud. With Managed Kubernetes, organizations can focus on
    developing and deploying their applications without worrying too much about the
    underlying infrastructure.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**托管Kubernetes**是由云服务提供商（如**亚马逊Web服务**（**AWS**）、**谷歌云平台**（**GCP**）和**微软Azure**）提供的服务，简化了在云中部署、管理和扩展容器化应用程序的过程。通过托管Kubernetes，组织可以专注于开发和部署应用程序，而无需过多担心底层的基础设施。'
- en: Managed Kubernetes provides a pre-configured and optimized environment for deploying
    containers, eliminating the need for the manual setup and maintenance of a Kubernetes
    cluster. This allows organizations to quickly deploy and scale their applications,
    reducing time to market and freeing up valuable resources.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 托管Kubernetes提供了一个预配置且优化的环境，用于部署容器，消除了手动设置和维护Kubernetes集群的需求。这使得组织能够快速部署和扩展应用程序，减少市场响应时间，并释放宝贵的资源。
- en: Additionally, Managed Kubernetes integrates with the cloud providers’ other
    services, such as databases, networking, storage solutions, security, identity,
    and observability features, making it easier to manage and secure the entire application
    stack. This also enables organizations to leverage the providers’ expertise in
    managing large-scale infrastructure, ensuring high availability, and reducing
    downtime.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，托管Kubernetes与云服务提供商的其他服务（如数据库、网络、存储解决方案、安全性、身份验证和可观测性功能）集成，使得管理和保护整个应用堆栈变得更加容易。这也使得组织能够利用服务提供商在管理大规模基础设施、确保高可用性和减少停机时间方面的专业知识。
- en: Overall, Managed Kubernetes provides a simplified and efficient way to deploy
    and manage containerized applications in the cloud, reducing operational overhead
    and improving time to market. This makes it an attractive option for organizations
    of all sizes looking to take advantage of the benefits of containers and the cloud.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，托管Kubernetes提供了一种简化且高效的方式来部署和管理云中的容器化应用程序，减少了运营开销，并提高了市场响应时间。这使得它成为各类组织的一个有吸引力的选择，帮助他们利用容器和云计算的优势。
- en: Deep integration
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度集成
- en: Cloud providers utilize the extensibility of Kubernetes to offer deep integration
    of their Managed Kubernetes solutions with their cloud services via the CNI, the
    CSI, and authentication/authorization plugins. The cloud providers also implement
    the **Cloud Controller Interface** (**CCI**) to allow their compute infrastructure
    to serve Kubernetes nodes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商利用Kubernetes的扩展性，通过CNI、CSI和身份验证/授权插件，提供其托管Kubernetes解决方案与云服务的深度集成。云服务提供商还实现了**云控制器接口**（**CCI**），使其计算基础设施能够为Kubernetes节点提供服务。
- en: However, the integration runs deeper. The cloud providers often configure the
    kubelet, control the container runtime that runs on every node, and deploy various
    DaemonSets on every node.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，集成更深层次。云服务提供商通常会配置kubelet，控制每个节点上运行的容器运行时，并在每个节点上部署各种DaemonSets。
- en: 'For example, AKS leverages many Azure services:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，AKS利用了许多Azure服务：
- en: '**Azure Compute**: AKS leverages Azure Compute resources such as **virtual**
    **machines** (**VMs**), availability sets, and scale sets to provide a managed
    Kubernetes experience.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Compute**：AKS 利用 Azure Compute 资源，如 **虚拟机** (**VMs**)、可用性集和扩展集，提供托管的
    Kubernetes 体验。'
- en: '**Azure Virtual Network**: AKS integrates with Azure Virtual Network, allowing
    users to create and manage their own virtual networks and subnets. This provides
    users with control over their network layout and the ability to tightly control
    network traffic.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Virtual Network**：AKS 与 Azure Virtual Network 集成，使用户能够创建和管理自己的虚拟网络和子网。这为用户提供了对网络布局的控制，并能够紧密控制网络流量。'
- en: '**Azure Blob Storage**: AKS integrates with Azure Blob Storage, allowing users
    to store and manage their application data in the cloud. This provides users with
    scalable, secure, and highly available storage for their applications.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Blob Storage**：AKS 与 Azure Blob Storage 集成，使用户能够在云中存储和管理其应用程序数据。这为用户提供了可扩展、安全且高可用的存储解决方案。'
- en: '**Azure Key Vault**: AKS integrates with Azure Key Vault, allowing users to
    securely manage and store secrets such as passwords, keys, and certificates. This
    provides users with secure storage for their application secrets.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Key Vault**：AKS 与 Azure Key Vault 集成，允许用户安全地管理和存储密码、密钥和证书等机密。这为用户提供了安全存储应用程序机密的解决方案。'
- en: '**Azure Monitor**: AKS integrates with Azure Monitor, allowing users to collect
    and analyze metrics, logs, and traces from their applications. This provides users
    with the ability to monitor and troubleshoot their workloads.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Monitor**：AKS 与 Azure Monitor 集成，使用户能够收集和分析来自其应用程序的度量、日志和跟踪信息。这为用户提供了监控和故障排除其工作负载的能力。'
- en: '**Azure Active Directory** (**AAD**): AKS integrates with AAD to provide a
    secure, reliable, and highly available platform for running Kubernetes clusters.
    AAD provides an efficient and secure way to authenticate and authorize users and
    applications to access the cluster. AAD can also be integrated with Kubernetes
    **RBAC** (**role-based access control**) to provide granular control over access
    to cluster resources.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Active Directory** (**AAD**)：AKS 与 AAD 集成，提供一个安全、可靠且高可用的平台来运行 Kubernetes
    集群。AAD 提供了一种高效且安全的方式来验证和授权用户及应用程序访问集群。AAD 还可以与 Kubernetes **RBAC**（**基于角色的访问控制**）集成，为集群资源访问提供细粒度的控制。'
- en: Let’s move on and discuss one of the key elements of successfully managing a
    production Kubernetes-based system in the cloud.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论成功管理基于 Kubernetes 的生产系统的关键要素之一。
- en: Quotas and limits
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配额和限制
- en: Cloud infrastructure has revolutionized the way organizations store and manage
    their data and run their workloads. However, one major issue that requires consideration
    and attention is the use of quotas and limits by cloud service providers. These
    quotas and limits, while necessary for ensuring the stability and security of
    the cloud infrastructure, can be a major source of frustration and even outages
    for users.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 云基础设施彻底改变了组织存储和管理数据及运行工作负载的方式。然而，云服务提供商使用配额和限制是一个需要考虑和注意的重要问题。这些配额和限制虽然对确保云基础设施的稳定性和安全性至关重要，但也可能是用户面临的主要挫折来源，甚至可能导致宕机。
- en: Quotas and limits are restrictions placed on the number of resources that a
    user can consume. For example, there may be a limit on the number of VMs of a
    particular type that can be created in each region environment, or a quota on
    the amount of storage space that can be used. These quotas and limits are put
    in place to prevent a single user from consuming too many resources and potentially
    disrupting the overall performance of the cloud infrastructure. It also protects
    users from inadvertently provisioning a huge quantity of resources that they don’t
    really need but will have to pay for.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 配额和限制是对用户可消耗资源数量的限制。例如，可能会限制在每个区域环境中可以创建的特定类型的虚拟机数量，或对可用的存储空间数量设定配额。这些配额和限制旨在防止单个用户消耗过多资源，从而可能影响云基础设施的整体性能。它们还保护用户避免无意中配置大量不必要的资源，从而产生不必要的费用。
- en: The cloud is in theory infinitely scalable and elastic. In practice, this is
    true only within the quotas and limits.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，云是无限可扩展和弹性的。但在实际操作中，这只有在配额和限制范围内才成立。
- en: Let’s look at some real-world examples in the next sections.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的章节中查看一些实际示例。
- en: Real-world examples of quotas and limits
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配额和限制的实际示例
- en: On GCP, quotas can generally be increased, while limits are fixed. Also, each
    service has its own page of quotas and limits. The **virtual private cloud** (**VPC**)
    page is found at [https://cloud.google.com/vpc/docs/quota](https://cloud.google.com/vpc/docs/quota).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCP上，配额通常可以增加，而限制是固定的。此外，每个服务都有自己独立的配额和限制页面。**虚拟专用云**（**VPC**）页面可以在[https://cloud.google.com/vpc/docs/quota](https://cloud.google.com/vpc/docs/quota)找到。
- en: 'You can view the quotas in the GCP console as well as request increases: [https://console.cloud.google.com/iam-admin/quotas](https://console.cloud.google.com/iam-admin/quotas).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GCP控制台中查看配额，并请求增加配额：[https://console.cloud.google.com/iam-admin/quotas](https://console.cloud.google.com/iam-admin/quotas)。
- en: There are currently 9,441 quotas!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当前有9,441个配额！
- en: 'Here is a screenshot that shows some quotas for the GCP Compute Engine service:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是显示GCP计算引擎服务一些配额的截图：
- en: '![](img/B18998_17_01.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_17_01.png)'
- en: 'Figure 17.1: Screenshot of GCP compute engine quotas'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1：GCP计算引擎配额截图
- en: Now that we understand what quotas and limits are, let’s discuss capacity planning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了配额和限制是什么，让我们来讨论容量规划。
- en: Capacity planning
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容量规划
- en: In the olden days, capacity planning meant thinking about how many servers you
    needed in your data centers, how big the disks should be, and the bandwidth of
    your network. This was based on the usage of your workloads as well as keeping
    healthy headroom for redundancy as well as growth. Then, you had to consider upgrades
    and how to phase out obsolete hardware. In the cloud, you don’t need to worry
    about hardware. However, you do need to plan around the quotas and limits. This
    means you need to monitor the quotas and limits and, whenever you get close to
    the current quota, request an increase. For quotas such as the number of VM instances
    of a particular VM family, I recommend staying below 50%-60% if possible. This
    should give you ample room for disaster recovery and growth, as well blue-green
    deployments where you run your new version and old version side by side for a
    while.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，容量规划意味着思考你在数据中心需要多少台服务器、硬盘应该多大，以及网络带宽是多少。这些都基于工作负载的使用情况，并确保有足够的冗余空间和增长空间。然后，你需要考虑硬件升级，以及如何逐步淘汰过时的硬件。在云中，你不需要担心硬件。然而，你需要围绕配额和限制进行规划。这意味着你需要监控配额和限制，并且每当接近当前配额时，提出增加请求。对于像某个VM系列的虚拟机实例数量这样的配额，我建议尽量保持在50%-60%以下。这应该为你提供足够的灾难恢复空间和增长空间，同时也适用于蓝绿部署，即同时运行新版本和旧版本一段时间。
- en: When should you not use Managed Kubernetes?
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么时候不应该使用托管Kubernetes？
- en: 'Managed Kubernetes is great, but it is not a panacea. There are several situations
    and use cases where you may prefer to manage Kubernetes yourself, such as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 托管Kubernetes很好，但它不是万灵药。有几种情况和使用场景，你可能更愿意自行管理Kubernetes，例如：
- en: The obvious use case is if you run Kubernetes on-prem and a managed solution
    is simply not available. However, you can run a similar stack to cloud-managed
    Kubernetes via platforms like GKE Anthos, AWS Outposts, and Azure Arc.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显而易见的使用场景是，如果你在本地运行Kubernetes，而托管解决方案根本不可用。然而，你可以通过像GKE Anthos、AWS Outposts和Azure
    Arc等平台，运行类似于云托管Kubernetes的技术栈。
- en: You require extreme control over the control plane, the node components, and
    the daemonsets that run on each node.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要对控制平面、节点组件和运行在每个节点上的守护进程进行极端控制。
- en: You already have in-house expertise in running Kubernetes yourself and using
    Managed Kubernetes will require a steep learning curve and might cost more.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你已经具备了自己运行Kubernetes的内部专业知识，而使用托管Kubernetes将需要陡峭的学习曲线，并且可能成本更高。
- en: You manage highly sensitive information that you must fully control and can’t
    trust the cloud provider with.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你管理着高度敏感的信息，必须完全控制这些信息，且无法将其交给云服务提供商。
- en: You run Kubernetes on multiple cloud providers and/or hybrid environments and
    prefer to have a uniform way to manage Kubernetes in all environments.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在多个云提供商和/或混合环境中运行Kubernetes，并且希望在所有环境中以统一的方式管理Kubernetes。
- en: You want to make sure you are not locked into a particular cloud provider.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你希望确保不会被锁定到某个特定的云服务提供商。
- en: In short, there are various situations where you may take on managing Kubernetes
    yourself. Let’s look at the various ways you may deploy and manage multiple clusters
    of Kubernetes in different environments.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在各种情况下，你可能会选择自行管理Kubernetes。让我们看看你可能如何在不同环境中部署和管理多个Kubernetes集群。
- en: Managing multiple clusters
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理多个集群
- en: A Kubernetes cluster is powerful and can manage a lot of workloads (thousands
    of nodes, and hundreds of thousands of pods). As a startup, you may get pretty
    far with just one cluster. However, at enterprise scale, you’ll need more than
    one cluster. Let’s consider some use cases.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Kubernetes集群非常强大，可以管理大量的工作负载（成千上万的节点，和数十万个Pod）。作为初创公司，您可能只需一个集群就能走得很远。然而，在企业级规模下，您将需要不止一个集群。让我们来看看一些用例。
- en: Geo-distributed clusters
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 地理分布式集群
- en: 'Geo-distributed clusters are clusters that run in different locations. There
    are three main reasons for using geo-distributed clusters:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 地理分布式集群是指在不同位置运行的集群。使用地理分布式集群的主要原因有三：
- en: Keeping your data and workloads close to their consumers.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将您的数据和工作负载靠近其消费者。
- en: Compliance and data privacy laws where data must remain in its country of origin.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合规性和数据隐私法律要求数据必须保留在其原始国家。
- en: High availability and disaster recovery in case of a regional outage.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在地区性停机的情况下实现高可用性和灾难恢复。
- en: Multi-cloud
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多云
- en: If you run on multiple clouds, then naturally you need at least one cluster
    per cloud provider.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在多个云平台上运行，那么您自然需要每个云提供商至少一个集群。
- en: Running on multiple clouds can be complicated, but at enterprise scale, it may
    be unavoidable and sometimes desirable. For example, your company may run Kubernetes
    on cloud X and acquire a company that runs Kubernetes on cloud Y. Migrating from
    Y to X might be too risky and expensive.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个云平台上运行可能会很复杂，但在企业级规模下，这可能是不可避免的，有时也是必要的。例如，您的公司可能在云X上运行Kubernetes，而收购的公司在云Y上运行Kubernetes。从Y迁移到X可能风险太大且费用过高。
- en: Another valid reason to run on multiple clouds is to have leverage against the
    cloud providers to secure better discounts and ensure you are not fully locked
    in. Finally, it may allow you to tolerate a complete cloud provider outage (this
    is not trivial to pull off).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有效的在多个云平台上运行的理由是，通过与云提供商的谈判，争取更好的折扣，并确保您不会被完全锁定。最后，这也可能让您在完全云服务提供商停机的情况下，仍能保持容错（这可不是简单的事情）。
- en: Hybrid
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合云
- en: Hybrid Kubernetes means running some Kubernetes clusters in the cloud (with
    a single or multiple cloud providers) and some Kubernetes clusters on-prem.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 混合Kubernetes意味着在云中（使用一个或多个云提供商）运行一些Kubernetes集群，同时在本地运行一些Kubernetes集群。
- en: This situation may arise as before because of an acquisition or even if you
    are in the process of migrating from on-prem Kubernetes to the cloud. Large systems
    might take years to migrate and during the migration, you’ll have to run a mix
    of Kubernetes clusters running in a hybrid environment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况可能会像之前一样发生，因为收购，或者即使您正在将本地Kubernetes迁移到云端。大型系统的迁移可能需要几年时间，而在迁移过程中，您将不得不运行一个混合环境中的Kubernetes集群。
- en: You may also adopt patterns like burst to cloud where most of your Kubernetes
    clusters run on-prem, but you have the flexibility to deploy workloads to Kubernetes
    clusters running in the cloud, which can scale quickly if you are hit with unanticipated
    load or if your on-prem infrastructure is having issues.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以采用像“突发到云”这样的模式，其中大部分Kubernetes集群运行在本地，但您可以灵活地将工作负载部署到云中的Kubernetes集群，这样在遇到无法预料的负载时，或者当您的本地基础设施出现问题时，云端的集群可以迅速扩展。
- en: Kubernetes on the edge
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 边缘上的Kubernetes
- en: Most enterprise data (around 90%) is generated in the cloud and private data
    centers; however, that number will drop to just 25% by 2025 according to Gartner.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数企业数据（约90%）是在云端和私有数据中心生成的；然而，根据Gartner的预测，到2025年，这一比例将降至仅25%。
- en: This is mind-blowing. Edge computing (AKS IoT) will be responsible for this
    massive shift.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这简直让人震撼。边缘计算（AKS IoT）将是这种巨大转变的主要推动力。
- en: A lot of devices spread all over the place will generate constant streams of
    data. Some of that data will be sent back to the backend for processing, aggregation,
    and storage. However, it makes a lot of sense to perform various forms of data
    processing close to the data instead of sending the raw data as it is. In some
    cases, you can even run workloads locally close to the data and serve users completely
    on the edge.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 很多设备分布在各个地方，将会产生不断的数据流。这些数据中的一部分将被送回后端进行处理、聚合和存储。然而，将各种数据处理方式靠近数据本地进行而不是将原始数据直接发送回来，是非常有意义的。在某些情况下，您甚至可以在本地近距离运行工作负载并完全在边缘服务用户。
- en: This is the promise of edge computing. Running Kubernetes at the edge allows
    organizations to bring the processing of data closer to the source of data generation,
    reducing the latency and bandwidth requirements of sending data to a centralized
    data center or cloud. This results in improved response times and real-time processing
    of data, making it an ideal solution for use cases such as industrial IoT, autonomous
    vehicles, and other real-time data processing applications.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是边缘计算的承诺。在边缘运行 Kubernetes 使组织能够将数据处理更接近数据生成源，从而减少将数据发送到集中数据中心或云的延迟和带宽需求。这将提高响应时间并实现数据的实时处理，使其成为工业物联网、自动驾驶车辆以及其他实时数据处理应用场景的理想解决方案。
- en: However, running Kubernetes at the edge comes with its own set of challenges.
    Edge devices are typically resource-constrained, making it necessary to optimize
    the deployment of Kubernetes for the edge environment. Organizations must also
    consider the network connectivity and reliability of edge devices, as well as
    the security and privacy implications of deploying a Kubernetes cluster at the
    edge.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在边缘运行 Kubernetes 会带来一系列挑战。边缘设备通常资源受限，因此必须优化 Kubernetes 的部署以适应边缘环境。组织还必须考虑边缘设备的网络连接性和可靠性，以及在边缘部署
    Kubernetes 集群的安全性和隐私问题。
- en: Projects like CNCF KubeEdge ([https://kubeedge.io](https://kubeedge.io)) can
    get you started.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 像 CNCF KubeEdge ([https://kubeedge.io](https://kubeedge.io)) 这样的项目可以帮助你入门。
- en: However, we will focus for the rest of this chapter on large-scale Kubernetes-based
    systems in the cloud.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，本章接下来的部分将专注于云中基于 Kubernetes 的大规模系统。
- en: Building effective processes for large-scale Kubernetes deployments
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为大规模 Kubernetes 部署构建有效的流程
- en: To run multi-cluster Kubernetes systems in production, you must develop a set
    of effective processes and best practices that encompass every aspect of the operation.
    Here are some of the critical areas to address.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要在生产环境中运行多集群 Kubernetes 系统，你必须制定一套有效的流程和最佳实践，涵盖操作的各个方面。以下是一些需要解决的关键领域。
- en: The development lifecycle
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发生命周期
- en: The development lifecycle of a multi-cluster Kubernetes-based system in production
    can be a complex process, but it is possible to streamline it with the right approach.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，多集群 Kubernetes 系统的开发生命周期可能是一个复杂的过程，但通过正确的方法，可以将其简化。
- en: You should absolutely implement a CI/CD pipeline that automatically builds,
    tests, and deploys code changes. This pipeline should be integrated with version
    control systems such as Git, and it should also include automated testing to ensure
    code quality.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该绝对实施一个 CI/CD 流水线，自动构建、测试和部署代码变更。该流水线应与版本控制系统（如 Git）集成，并且还应包含自动化测试以确保代码质量。
- en: It’s important to manage the ownership and approval process for different areas
    of the code base.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 管理不同代码库区域的所有权和审批流程是非常重要的。
- en: Kubernetes namespaces can provide a convenient way to organize workloads and
    corresponding software assets and associate them with teams and stakeholders.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 命名空间可以提供一种方便的方式来组织工作负载及其相应的软件资产，并将它们与团队和相关方关联起来。
- en: You should have a complete track of changes, ongoing builds, and deployments,
    and the ability to freeze activity and roll back changes of each workload.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该对每个工作负载的变更、正在进行的构建和部署有完整的跟踪，并能够冻结活动并回滚变更。
- en: It’s also important to control the gradual deployment to different clusters
    and regions to avoid a situation where a bad change is deployed simultaneously
    across the board and brings the entire system down.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是控制逐步部署到不同集群和区域，以避免在所有系统中同时部署坏的变更，导致整个系统崩溃。
- en: Environments
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境
- en: Code review and careful incremental deployment while monitoring the outcome
    are required, but they are insufficient for large enterprise systems with multiple
    Kubernetes clusters. Some changes might display a negative impact only after running
    for a while or under certain conditions, which will escape the control mechanisms
    we mentioned earlier. The best practice is to have multiple runtime environments
    such as production, staging, and development. The exact division of environments
    can vary, but you typically need at least a production environment, which is the
    actual system that manages all the data and your users interact with, and a staging
    environment, which mimics the production system and where you can test changes
    and new versions without impacting your users and risking bringing the production
    environment down.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 代码审查和小心的增量部署，同时监控结果，是必要的，但对于拥有多个Kubernetes集群的大型企业系统来说，这些措施并不足够。某些变更可能只有在运行一段时间后或在特定条件下才会显示出负面影响，这将逃脱我们前面提到的控制机制。最佳实践是拥有多个运行时环境，例如生产环境、预发布环境和开发环境。环境的具体划分可以有所不同，但通常至少需要一个生产环境，这个环境是实际管理所有数据并且用户进行交互的系统，以及一个预发布环境，它模拟生产系统，你可以在其中测试变更和新版本，而不会影响用户并且避免将生产环境置于风险之中。
- en: Let’s consider some aspects of using multiple environments.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑使用多个环境的一些方面。
- en: Separated environments
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分离的环境
- en: It is critical that the staging environment can’t accidentally contaminate and
    impact the production environment. For example, if you run a stress test in staging
    and some workloads in the staging environment are misconfigured and hit production
    endpoints, you will have a very unpleasant time untangling the mess.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 至关重要的是，预发布环境不能意外污染并影响生产环境。例如，如果你在预发布环境中运行压力测试，而某些工作负载在预发布环境中配置错误并访问生产端点，那么你将会经历一段非常不愉快的时间，试图解开这一混乱。
- en: Rigid network segmentation where the staging environment is unable to reach
    the production environment is a good first step. You will still need to be mindful
    of the interaction between staging and production through public endpoints. The
    staging workloads should not have secrets and identities that allow production
    access.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 严格的网络分段使得预发布环境无法访问生产环境，是一个很好的第一步。你仍然需要注意通过公共端点在预发布和生产环境之间的交互。预发布的工作负载不应包含能够访问生产环境的密钥和身份。
- en: Staging environment fidelity
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预发布环境的保真度
- en: The primary motivation for the staging environment is to test changes and interactions
    with other sub-systems before deploying a change to production. This means that
    the staging environment should mimic the production environment as much as possible.
    However, running an exact replica of the production environment is prohibitively
    expensive.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 预发布环境的主要目的是在将变更部署到生产环境之前，测试与其他子系统的交互及变更。这意味着预发布环境应尽可能地模拟生产环境。然而，运行一个完全与生产环境相同的副本成本极高。
- en: The staging environment should be configured and set up using the same automated
    CI/CD pipeline that is able to deploy staging and production.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 预发布环境应使用相同的自动化CI/CD流水线进行配置和设置，该流水线能够部署预发布和生产环境。
- en: Staging infrastructure and resources should also be provisioned using the same
    tools as production although there will typically be fewer resources allocated
    to staging.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 预发布基础设施和资源也应使用与生产环境相同的工具进行配置，尽管通常分配给预发布的资源较少。
- en: You may want to be able to temporarily scale down or even completely shut down
    some parts of the staging environment and be able to bring them back up only when
    necessary for running large-scale tests in staging.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能希望能够暂时缩减，甚至完全关闭预发布环境中的某些部分，并且仅在需要进行大规模测试时再将其重新启动。
- en: Resource quotas
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 资源配额
- en: Resource quotas in staging and production can ensure that misconfiguration or
    even an attack doesn’t cause excessive resource usage.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 预发布和生产环境中的资源配额可以确保配置错误或甚至攻击不会导致资源使用过度。
- en: Promotion process
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升过程
- en: Once a change has been thoroughly tested in staging, there should be a clear
    promotion process for deploying it to production. The process may be different
    for different components depending on the scope and impact of the change. The
    promotion may be completely automatic where the CI/CD pipeline detects that staging
    tests are completed successfully and moves ahead with production deployment or
    involve extra steps and possibly another explicit deployment to production.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在暂存环境中彻底测试了更改，就应该有一个明确的推广过程将其部署到生产环境中。这个过程可能因更改的范围和影响而异。推广过程可能完全自动化，CI/CD管道检测到暂存测试成功完成并继续进行生产部署，或者涉及额外的步骤和可能另一个明确的生产部署。
- en: Permissions and access control
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权限和访问控制
- en: When you manage a constellation of Kubernetes clusters running on cloud infrastructure,
    you need to pay a lot of attention to the permission model and your access control.
    This builds on the previous best practices of the development lifecycle and environments.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当您管理运行在云基础设施上的Kubernetes集群星座时，需要特别注意权限模型和访问控制。这是在之前开发生命周期和环境的最佳实践基础上构建的。
- en: The principle of least privilege
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小权限原则
- en: The principle of least privilege comes from the security field, but it is useful
    even beyond security for reliability, performance, and cost. Actors – either humans
    or workloads – should not have more permissions than necessary to accomplish their
    tasks. By reducing access to the bare minimum, you ensure that no accidental or
    malicious activity occurs for forbidden resources.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最小权限原则源自安全领域，但即使超越安全性，也对可靠性、性能和成本有用。人员或工作负载不应该比完成任务所需的权限更多。通过将访问权限降至最低限度，确保不会发生禁止资源的意外或恶意活动。
- en: Also, when investigating an incident, it automatically narrows down the possible
    culprits to those who had the permissions to act on the misconfigured resource
    or take the invalid action.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，在调查事故时，它会自动将可能的罪魁祸首缩小到那些具有操作不当配置资源或执行无效操作权限的人员。
- en: If you follow the GitOps model, it is possible to create a workflow where every
    change to your clusters and your infrastructure is done by CI/CD and dedicated
    tooling. Human engineers have only read-only access. Some special exceptions can
    be made (see the *Break glass* section in this chapter).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遵循GitOps模型，可以创建一种工作流程，其中对集群和基础设施的每一次更改都由CI/CD和专用工具完成。人类工程师只有只读访问权限。在本章的*打破玻璃*部分可以做一些特殊例外。
- en: Assign permissions to groups
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将权限分配给组
- en: It is highly recommended to assign permissions to groups or teams as opposed
    to individuals. Even if just a single person is currently carrying out a task
    that requires some set of permissions, you should define a group where this person
    is the single member. That will make it easier to add other people later or replace
    the person.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议将权限分配给组或团队，而不是个人。即使只有一个人当前正在执行需要某些权限集的任务，也应定义一个该人员是唯一成员的组。这将使以后添加其他人员或替换该人员更加容易。
- en: Fine-tune your permission model
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化你的权限模型
- en: However, sometimes too strict a permission model can be detrimental. You’ll
    have to maintain a very fine-grained set of permissions to a large set of resources.
    Whenever the slightest change occurs such that another action is needed on some
    resource, you’ll have to modify the permissions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时候过于严格的权限模型可能是有害的。您将不得不对大量资源维护非常细粒度的权限设置。每当发生最轻微的更改以至于需要在某些资源上执行其他操作时，您将不得不修改权限。
- en: Find the golden path between granting super-admin permissions to everyone and
    painstakingly creating hundreds and thousands of roles for each and every resource.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 找到在授予每个人超级管理员权限和为每个资源创建数百甚至数千个角色之间的黄金平衡点。
- en: In particular, consider relaxing the permission model in the development environment
    and potentially in the staging environment too. This is where a lot of experiments
    take place and where you discover what actions you need to perform and what permissions
    are actually necessary and then you can tweak your permissions model before deploying
    to production.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是考虑在开发环境甚至是暂存环境中放宽权限模型。这是许多实验进行的地方，您可以在这里发现需要执行的操作和实际所需的权限，然后在部署到生产环境之前调整您的权限模型。
- en: Break glass
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 打破玻璃
- en: Sometimes, your CI/CD pipeline itself will be broken or, due to incomplete coverage,
    some resources may have been provisioned manually. In these cases, human engineers
    must intervene and, so to speak, “*break the glass*” and take direct action against
    Kubernetes or cloud infrastructure.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，CI/CD 流水线本身可能会出现故障，或者由于覆盖不全，一些资源可能是手动配置的。在这些情况下，人工工程师必须介入，可以说是“*打破玻璃*”，直接对
    Kubernetes 或云基础设施进行操作。
- en: It is recommended to have a formal process of acquiring **break glass** access,
    who is allowed to have it, how long it lasts, and record who had it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 建议制定正式的流程来获取**紧急访问**权限，明确谁可以获得该权限、权限持续时间以及记录谁曾经拥有该权限。
- en: This brings us to the next section about observability.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了我们下一部分的内容——可观察性。
- en: Observability
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可观察性
- en: A comprehensive observability stack is an absolute must. Complex systems composed
    of multiple Kubernetes clusters can be reasoned about and understood theoretically.
    You must have a complete record of events from cloud providers, Kubernetes itself,
    and workloads. Your CI/CD pipeline and other tools must also be fully observable.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一个全面的可观察性栈是绝对必不可少的。由多个 Kubernetes 集群组成的复杂系统可以从理论上推理并理解。你必须拥有来自云服务提供商、Kubernetes
    本身以及工作负载的完整事件记录。你的 CI/CD 流水线及其他工具也必须具备完全的可观察性。
- en: Let’s look at some elements of multi-cluster observability.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看多集群可观察性的一些元素。
- en: One-stop shop observability
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一站式可观察性
- en: Cloud providers and Kubernetes itself provide a lot of observability in the
    form of logs and metrics out of the box. However, those are typically organized
    at the cluster level. If you are dealing with some widespread issue across multiple
    clusters, it is very difficult, and at a certain scale impossible, to go into
    each individual cluster, extract the observability data, and try to make sense
    of it. You must ship all observability data into a single centralized system where
    it can be aggregated, summarized, and be ready for multi-cluster analysis and
    response.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商和 Kubernetes 本身提供了大量的日志和度量指标作为开箱即用的可观察性。然而，这些通常是按集群级别组织的。如果你正在处理跨多个集群的广泛问题，那么进入每个集群、提取可观察性数据并尝试理解它将变得非常困难，甚至在某些规模下是不可能的。你必须将所有可观察性数据传输到一个单一的集中系统中，在那里它可以被汇总、总结，并准备好进行多集群分析和响应。
- en: Troubleshooting your observability stack
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排查你的可观察性栈
- en: Your observability stack is an indispensable component of your system. If it
    is down or degraded, you may be flying blind and unable to effectively respond
    to issues. Moreover, a cross-cluster issue may impact your observability stack
    as it impacts your entire system. Consider this scenario very carefully and make
    sure you have plenty of redundancies and observability alternatives if your primary
    observability stack is not up to the task temporarily. For example, you may rely
    on in-cluster observability solutions if your centralized observability stack
    is compromised. If you want complete redundancy, you may have a parallel observability
    stack on two cloud providers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你的可观察性栈是你系统中不可或缺的组成部分。如果它出现故障或性能下降，你可能会处于“盲飞”状态，无法有效地应对问题。而且，跨集群问题可能会影响你的可观察性栈，进而影响整个系统。非常仔细地考虑这个场景，确保在你的主要可观察性栈暂时无法应对任务时，拥有足够的冗余和可观察性替代方案。例如，如果你的集中式可观察性栈出现问题，你可以依赖集群内部的可观察性解决方案。如果你想要完全冗余，可以在两个云服务提供商上部署并行的可观察性栈。
- en: Consider testing these harsh scenarios of CI/CD pipeline and observability stack
    outages and see how you operate.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑测试这些严苛场景：CI/CD 流水线和可观察性栈出现故障时，看看你的操作流程如何。
- en: Let’s look more specifically into different types of infrastructure and how
    to handle them at scale.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更具体地看看不同类型的基础设施，以及如何在大规模环境中处理它们。
- en: Handling infrastructure at scale
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模处理基础设施
- en: One of the most demanding tasks when running large-scale multi-cluster Kubernetes
    in the cloud is dealing with the cloud infrastructure. In some respects, it is
    much better than being responsible for low-level compute, network, and storage
    infrastructure. However, you lose a lot of control, and troubleshooting issues
    is challenging.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在云端运行大规模多集群 Kubernetes 的任务中，处理云基础设施是最具挑战性的任务之一。从某些方面来说，它比负责底层计算、网络和存储基础设施要好得多。然而，你会失去很多控制权，排查问题变得非常具有挑战性。
- en: Before diving into each category of infrastructure, let’s look at some general
    cloud-level considerations.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入每个基础设施类别之前，先来看一些通用的云级别考虑事项。
- en: Cloud-level considerations
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云级别考虑事项
- en: In the cloud, you organize your resources in entities such as AWS accounts,
    GCP projects, and Azure subscriptions. An organization may have multiple such
    groups, and each one has its own limits and quotas. For the sake of brevity, let’s
    call them accounts. Enterprise organizations’ infrastructure requirements will
    exceed the capacity of a single account. It’s critical to decide how to break
    down your infrastructure into different accounts. One good heuristic is to separate
    environments – production, staging, and development – into separate accounts.
    Account-level isolation is beneficial for these environments. However, this may
    not be sufficient and within a single environment, you might need more resources
    than can fit in one account.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在云中，你会将资源组织为如AWS账户、GCP项目和Azure订阅这样的实体。一个组织可能拥有多个这样的组，每个组都有自己的限制和配额。为了简便起见，我们称之为账户。企业组织的基础设施需求将超过单一账户的容量。决定如何将基础设施拆分为不同账户至关重要。一种好的启发式方法是将不同的环境——生产、预发布和开发——拆分到不同的账户中。账户级别的隔离对于这些环境是有益的。然而，这可能还不够，在单一环境中，你可能需要更多的资源，单一账户无法容纳。
- en: Having a solid account management strategy is key. Accounts can also be a boundary
    of access control as even account administrators can’t access other accounts.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个健全的账户管理策略是关键。账户也可以作为访问控制的边界，因为即使是账户管理员也无法访问其他账户。
- en: Consult with your security team about security-motivated account breakdowns.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请与你的安全团队协商关于基于安全考虑的账户划分。
- en: Another important aspect is the breakdown of regions. If you manage infrastructure
    across multiple regions and workloads in these regions communicate with each other,
    then this has severe latency and cost implications. In particular, cross-region
    egress is typically not free.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要方面是区域划分。如果你在多个区域管理基础设施，并且这些区域中的工作负载需要相互通信，那么这将带来严重的延迟和成本影响。特别是，跨区域出口通常不是免费的。
- en: Let’s look at each category of infrastructure.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下每种基础设施类别。
- en: Compute
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算
- en: Compute infrastructure for Managed Kubernetes includes the Kubernetes clusters
    themselves and their worker nodes. The worker nodes are typically grouped into
    node pools, which is not a Kubernetes concept. How you break down your system
    into Kubernetes clusters and what types of node pools exist in each cluster will
    greatly impact your ability to manage the system at scale.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 管理Kubernetes的计算基础设施包括Kubernetes集群本身及其工作节点。工作节点通常被分组到节点池中，这不是Kubernetes的概念。如何将系统拆分为Kubernetes集群以及每个集群中存在的节点池类型，将极大地影响你在规模化管理系统时的能力。
- en: Ideally, you can treat clusters like cattle, provision identical clusters, and
    easily add or remove clusters in different locations. Each cluster will have the
    same node pools.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你可以像管理牲畜一样管理集群，配置相同的集群，并且可以轻松地在不同位置添加或移除集群。每个集群将拥有相同的节点池。
- en: This uniform and consistent organization of clusters is not always possible.
    There are sometimes reasons to have different clusters for particular purposes.
    You should still strive for a small number of cluster types that can be replicated
    easily.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这种统一一致的集群组织方式并非总是可能的。有时为了特定目的，可能需要不同的集群。你仍然应该尽量保持少数几种集群类型，以便轻松复制。
- en: Design your cluster breakdown
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设计你的集群划分
- en: Clusters in the cloud typically allocate private IP addresses to nodes and pods
    from a virtual network that resides in one region. Yes, it’s possible to have
    wide clusters that cross regions, but this is the exception and not the rule.
    It is highly recommended to manage clusters as cattle if possible and automatically
    provision clusters across all regions of operations.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 云中的集群通常从一个区域的虚拟网络中为节点和Pod分配私有IP地址。是的，跨区域的大型集群是可能的，但这是例外而非常规。强烈建议在可能的情况下将集群作为牲畜来管理，并在所有操作区域自动配置集群。
- en: Design your node pool breakdown
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设计你的节点池划分
- en: Node pools are groups of nodes that have the same instance type. They can typically
    autoscale to accommodate the needs of the cluster with the help of the cluster
    autoscaler. How you choose what node pools to provision in your clusters is a
    fundamental decision that impacts performance, cost, and operational complexity.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 节点池是具有相同实例类型的节点组。它们通常可以通过集群自动扩展器自动扩展，以满足集群的需求。选择在集群中配置哪些节点池是一个基础性决策，它会影响性能、成本和操作复杂度。
- en: We will dive into a deeper discussion on this later in the chapter.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后深入讨论这一点。
- en: Networking
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络
- en: Networking is a very dynamic area of infrastructure. There are many degrees
    of freedom. The interplay between latency and bandwidth has nuances. Workloads
    can’t request a certain amount of bandwidth or guaranteed latency. In addition,
    there are a lot of external factors that impact the connectivity, reachability,
    and performance of your network. Let’s look at some of the important topics we
    have to consider and plan for.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是基础设施中一个非常动态的领域。它有许多自由度。延迟与带宽之间的相互作用有其细微之处。工作负载不能请求一定的带宽或保证的延迟。此外，还有许多外部因素会影响网络的连接性、可达性和性能。让我们来看看一些我们必须考虑和规划的重要议题。
- en: IP address space management
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IP地址空间管理
- en: 'When you run a multi-cluster Kubernetes-based system, every pod in a cluster
    gets a unique private IP address. However, if you want to connect multiple clusters
    and have workloads in one cluster reach workloads in other clusters via their
    private IP address, then two conditions must exist:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行一个基于多集群的Kubernetes系统时，集群中的每个pod都会获得一个唯一的私有IP地址。然而，如果你想连接多个集群，并且让一个集群中的工作负载通过私有IP地址访问其他集群中的工作负载，那么必须满足以下两个条件：
- en: The networks of the different clusters must be peered together.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同集群的网络必须进行对等连接。
- en: The private IP address of pods must be unique across all clusters.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: pods的私有IP地址在所有集群中必须唯一。
- en: This requires centralized management of the private IP address space and carefully
    assigning IP ranges to different clusters.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要集中管理私有IP地址空间，并且要小心地为不同的集群分配IP范围。
- en: Cloud providers differ a little in their approach to assigning IP address ranges
    to clusters. AKS requires that each cluster belongs to a VNet with its own IP
    address range and then subnets are assigned to nodes and pods with sub-ranges
    from the VNet IP address range. GKE comes with a default network that has no IP
    address range of its own. Clusters are provisioned with subnets that have their
    own IP address ranges.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商在为集群分配IP地址范围的方式上有所不同。AKS要求每个集群都属于一个具有自己IP地址范围的VNet，然后子网被分配给节点和pods，子网的IP地址范围来自VNet的IP地址范围。GKE则提供一个没有自己IP地址范围的默认网络。集群是通过具有自己IP地址范围的子网来配置的。
- en: In addition, services require their own IP addresses too, and possibly some
    other components.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，服务也需要自己的IP地址，以及可能的其他组件。
- en: 'The entire private IPv4 address space consists of several blocks:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 整个私有IPv4地址空间由多个块组成：
- en: '`10.0.0.0/8` (class A)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`10.0.0.0/8`（A类）'
- en: '`172.16.0.0/12` (class B)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`172.16.0.0/12`（B类）'
- en: '`192.168.0.0/8` (class C)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`192.168.0.0/8`（C类）'
- en: At scale, the most important one is `10.0.0.0/8`, which consists of 2^24 IP
    addresses, which is more than 16 million addresses. That is a lot of IP addresses,
    but if you don’t plan carefully, you may cause fragmentation and run out of large
    blocks even if you have plenty of unused IP addresses.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模部署中，最重要的是`10.0.0.0/8`，它包含了2^24个IP地址，总数超过1600万个地址。这是一个非常庞大的IP地址范围，但如果不仔细规划，可能会导致碎片化，即使有很多未使用的IP地址，也可能会耗尽大块地址空间。
- en: 'Here are some best practices for managing the IP address space:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是管理IP地址空间的一些最佳实践：
- en: Allocate CIDR blocks to pods, nodes, and services that will be sufficient and
    utilized without too much space.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为pods、nodes和services分配足够并且不会浪费过多空间的CIDR块。
- en: Be aware of Kubernetes and cloud provider limits in terms of the number of supported
    nodes and pods.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解Kubernetes和云服务提供商在支持的节点和pods数量方面的限制。
- en: Consider network peering and service meshes spanning clusters.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑跨集群的网络对等连接和服务网格。
- en: Make sure you use non-overlapping CIDR blocks for connected clusters.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保连接的集群使用不重叠的CIDR块。
- en: Use proper tools to manage the address space.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用适当的工具来管理地址空间。
- en: Consider the impact of pod density on IP address space (e.g., on AKS, IP addresses
    are pre-allocated to the max number of pods on a node even if not utilized).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑pod密度对IP地址空间的影响（例如，在AKS上，即使没有使用，IP地址也会预先分配给节点上pod的最大数量）。
- en: Be aware of limits such as the maximum number of IP addresses available in a
    region.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意一些限制，例如一个区域中可用的最大IP地址数量。
- en: Network topology
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络拓扑
- en: All cloud providers offer a virtual network or VPC concept. All cloud providers
    also have the concept of a region, which is a geographical area where cloud providers
    host resources. Specifically, virtual networks are always confined to a single
    region. Since a Kubernetes cluster in the cloud is associated with a virtual network,
    it follows that a single Kubernetes cluster can’t span more than one region. This
    has implications for availability and reliability. If you want to survive a regional
    outage, you need to run each critical workload across multiple clusters in different
    regions. Moreover, all these clusters typically need to be connected to each other.
    We will discuss this more in the *Cross-cluster communication* section that follows.
    However, as far as network topology goes, it may be better to have multiple clusters
    in the same region share the same virtual network.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 所有云提供商都提供虚拟网络或 VPC 概念。所有云提供商也都有地区的概念，地区是云提供商托管资源的地理区域。具体来说，虚拟网络总是局限于一个单一的地区。由于云中的
    Kubernetes 集群与虚拟网络相关联，因此一个单一的 Kubernetes 集群不能跨越多个地区。这对可用性和可靠性有影响。如果你想在地区性故障中存活下来，你需要在不同地区的多个集群中运行每个关键工作负载。此外，所有这些集群通常需要相互连接。我们将在接下来的
    *跨集群通信* 部分中进一步讨论这个问题。然而，就网络拓扑而言，将同一区域内的多个集群共享同一虚拟网络可能会更好。
- en: Network segmentation
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络分段
- en: Network segmentation is about dividing a network into smaller subnets. In the
    context of Kubernetes, the most important subnets are the subnets for nodes, pods,
    and services. In some cases, the nodes and pods share the same subnet and in other
    cases, there are separate subnets for nodes and pods. Regardless, you need to
    plan and understand how many nodes and pods your cluster can accommodate and size
    your cluster subnets accordingly.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 网络分段是将一个网络划分为更小的子网。在 Kubernetes 的上下文中，最重要的子网是节点、Pod 和服务的子网。在某些情况下，节点和 Pod 共享同一子网，而在其他情况下，节点和
    Pod 会有单独的子网。无论如何，你需要规划并理解你的集群能够容纳多少个节点和 Pod，并据此调整集群子网的规模。
- en: Cross-cluster communication
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨集群通信
- en: 'When running multiple Kubernetes clusters, it is often beneficial to consider
    groups of clusters as a single conceptual cluster. This means that pods in any
    cluster can directly reach pods in other clusters via their private IP address.
    This flat IP address model is an extension of the standard Kubernetes networking
    model within a single cluster to multiple clusters. This requires two elements
    we discussed earlier:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行多个 Kubernetes 集群时，考虑将集群组视为一个单一的概念集群通常是有益的。这意味着，任何集群中的 Pod 都可以通过它们的私有 IP 地址直接访问其他集群中的
    Pod。这种扁平化的 IP 地址模型是将标准 Kubernetes 网络模型从单一集群扩展到多个集群。这需要我们之前讨论过的两个要素：
- en: Non-conflicting IP address ranges for pods across all connected clusters.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有连接的集群之间的 Pod 应该使用不冲突的 IP 地址范围。
- en: Network peering between clusters.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群之间的网络对等连接。
- en: The network peering requirement might be tedious as clusters come and go. Having
    a single regional virtual network reduces the overhead and allows peering just
    the regional virtual networks. However, if you run a lot of clusters in the same
    region sharing the same virtual network, you might run into cloud provider limits
    that will stunt your growth. For example, on Azure, a VNet can have at most 64K
    unique IP addresses.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 网络对等连接要求可能会很繁琐，因为集群会不断地变动。拥有一个单一的区域虚拟网络可以减少开销，并且只需要对区域虚拟网络进行对等连接。然而，如果你在同一地区运行大量共享同一虚拟网络的集群，你可能会遇到云服务商的限制，这会限制你的发展。例如，在
    Azure 上，一个虚拟网络最多可以有 64K 个唯一的 IP 地址。
- en: Cross-cloud communication
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨云通信
- en: If your system spans multiple cloud providers, you need to consider how to connect
    your Kubernetes clusters across cloud providers. There are several ways, with
    different pros and cons.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的系统跨多个云提供商，你需要考虑如何在不同云提供商之间连接你的 Kubernetes 集群。有几种方法，各有优缺点。
- en: First, you may decide to avoid direct communication between clusters on different
    cloud providers. If Kubernetes clusters deployed on different clouds need to communicate
    with each other, they can do so through public APIs. This approach is simple but
    eliminates the idea of a unified conceptual cluster where pods can communicate
    directly with each other regardless of where they are.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可能决定避免在不同云提供商上的集群之间进行直接通信。如果部署在不同云上的 Kubernetes 集群需要相互通信，它们可以通过公共 API 实现。这种方法简单，但会排除统一概念集群的想法，即
    Pod 可以直接相互通信，无论它们位于何处。
- en: A site-to-site VPN is a communication method where different cloud providers
    can connect systems via BGP and establish a VPN connection to networks managed
    by another cloud provider via a VPN gateway that sits in front of the virtual
    networks. This establishes a secure channel; however, a VPN gateway is not trivial
    to set up and incurs a significant overhead.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 点对点VPN是一种通信方法，通过BGP连接不同的云提供商系统，并通过位于虚拟网络前面的VPN网关与另一云提供商管理的网络建立VPN连接。这建立了一个安全通道；然而，设置VPN网关并不是件小事，并会产生显著的开销。
- en: Direct connect (AKA direct peering) is another option that requires installing
    a router in a cloud provider point of presence. This method allows connecting
    Kubernetes clusters running in private data centers to clusters in the cloud.
    In addition, the performance is much better because there is no VPN gateway in
    the middle. The downside is that it is quite complicated to set up and you might
    have to comply with various requirements. It’s a good option for organizations
    with deep low-level networking expertise.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 直接连接（也称为直接对等连接）是另一种选项，需要在云提供商的点对点位置安装路由器。该方法允许将运行在私有数据中心中的Kubernetes集群连接到云中的集群。此外，性能要好得多，因为中间没有VPN网关。缺点是设置相当复杂，您可能需要遵守各种要求。对于具有深入低级网络专业知识的组织来说，这是一个不错的选择。
- en: Carrier or partner peering is similar to direct connect; however, you take advantage
    of the expertise of an established third party that specializes in providing this
    service and already has an established relationship and is certified with the
    cloud provider. You will have to pay for the service, of course.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 运营商或合作伙伴对等连接类似于直连；但是，您可以利用专业提供此服务的第三方的专业知识，该第三方已经与云提供商建立了关系并获得了认证。当然，您需要为此服务付费。
- en: Cross-cluster service meshes
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨集群服务网格
- en: Service meshes bring tremendous value to Kubernetes, as we discussed in *Chapter
    14*, *Utilizing Service Meshes*. When running multiple Kubernetes clusters in
    production, it is arguably even more important to connect all the clusters via
    a service mesh. The advanced capabilities and policies of a service mesh can be
    applied and manage connectivity and routing across all clusters.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*第14章* *利用服务网格*中所讨论的那样，服务网格为Kubernetes带来了巨大的价值。在生产环境中运行多个Kubernetes集群时，通过服务网格连接所有集群可能更加重要。服务网格的高级功能和策略可以应用于管理所有集群之间的连接和路由。
- en: 'There are two approaches to consider here:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在此需要考虑两种方法：
- en: A single fully connected service mesh.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单个完全连接的服务网格。
- en: Divide your clusters into multiple meshes.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的集群划分为多个网格。
- en: The single fully connected single mesh aligns conceptually with the single conceptual
    Kubernetes cluster approach. Everything is straightforward. New clusters just
    join the mesh, and the mesh is configured to allow every pod to talk to every
    other pod (as long as the routing policies allow it).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 单个完全连接的单一网格在概念上与单一概念的Kubernetes集群方法一致。一切都很简单。新集群只需加入网格，并配置网格以允许每个Pod与其他所有Pod通信（只要路由策略允许）。
- en: However, you might eventually run into scalability barriers as a single mesh
    means that the mesh control plane needs to handle policies for all workloads in
    all the clusters, and updating sidecars for all pods can put a lot of burden on
    your clusters.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单个网格可能会遇到可扩展性障碍，因为网格控制平面需要处理所有集群中所有工作负载的策略，并且为所有Pod更新Sidecar可能会给您的集群带来很大负担。
- en: An alternative approach is to have multiple independent meshes. Pods in clusters
    that belong to a particular mesh can directly talk to pods in all other clusters
    in the same service mesh but must go through public endpoints to access clusters
    in other service meshes.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是拥有多个独立的网格。属于特定网格的集群中的Pod可以直接与同一服务网格中所有其他集群中的Pod通信，但必须通过公共端点访问其他服务网格中的集群。
- en: The multi-mesh approach is more scalable but much more complicated. You need
    to consider how to divide your system into different service meshes and when new
    clusters join or leave the system, and how it impacts your overall architecture.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 多网格方法更具可扩展性，但更为复杂。您需要考虑如何将系统划分为不同的服务网格，以及新集群加入或离开系统时对整体架构的影响。
- en: The private IP address space management in the multi-service mesh case can be
    more nuanced too where different service meshes can have conflicting IP addresses.
    This means that you can manage the IP address space for each mesh separately.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在多服务网格的情况下，私有IP地址空间管理可能会更加复杂，因为不同的服务网格可能会有冲突的IP地址。这意味着你可以为每个网格单独管理IP地址空间。
- en: Service meshes offer another interesting solution to the cross-cluster connectivity
    story, which is the east-west gateway. With the east-west gateway approach, workloads
    in different clusters communicate indirectly through dedicated gateways in each
    cluster. This means that the private IP addresses of each cluster are unknown
    and there is an extra hop for each cross-cluster communication.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格提供了另一种有趣的解决方案来处理跨集群的连接问题，那就是东西向网关。通过东西向网关方法，不同集群中的工作负载通过每个集群中的专用网关间接通信。这意味着每个集群的私有IP地址是未知的，并且每次跨集群通信都会多出一个跳跃。
- en: Managing egress at scale
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大规模管理出口流量
- en: Some systems need to access external systems aggressively. Maybe you frequently
    fetch data from external systems or maybe the purpose of your system is to manage
    some external systems via APIs.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一些系统需要积极地访问外部系统。也许你经常从外部系统获取数据，或者你的系统的目的是通过API管理一些外部系统。
- en: There may be unique issues for egress traffic that require special attention.
    Some third-party organizations or even countries may have policies that block
    or throttle traffic coming from certain geographical areas or specific IP CIDR
    blocks. For example, China and its great firewall are famous for blocking and
    censoring a vast number of companies, such as Google and Facebook. If you run
    on GCP and need to access China, it might be a serious issue.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于出口流量，可能会有一些特殊问题需要关注。一些第三方组织或甚至某些国家可能会有政策，阻止或限制来自特定地理区域或特定IP地址范围（CIDR块）的流量。例如，中国及其“长城防火墙”因屏蔽和审查大量公司（如谷歌和Facebook）而闻名。如果你在GCP上运行并且需要访问中国，这可能会是一个严重的问题。
- en: Beyond total blocking, there may be limits and throttling in place if you try
    to access some third-party APIs at scale.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 除了完全阻止之外，如果你试图大规模访问某些第三方API，可能还会有流量限制和限速措施。
- en: If you persist in accessing those third-party APIs, you could even be reported,
    and your cloud provider could potentially impose various sanctions.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你持续访问这些第三方API，你甚至可能会被举报，云服务提供商也有可能对你采取各种制裁措施。
- en: Let’s consider some solutions to deal with these real-world problems.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些解决方案来应对这些现实世界中的问题。
- en: If your current cloud provider is not allowed to access your target destination,
    then you must establish an egress presence outside your cloud provider. This can
    be on another cloud provider or via an intermediate organization in good standing.
    This proxy approach can take many shapes, which is beyond the scope of this section.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你当前的云服务提供商无法访问你的目标地址，那么你必须在云服务提供商之外建立一个出口连接。这可以是在另一个云提供商上，或通过一个信誉良好的中介组织。这种代理方法有许多形式，超出了本节的讨论范围。
- en: If you are getting throttled, then the issue may be that you send too many requests
    from the same source IP address. A good solution here is to create a pool of egress
    nodes with different public IP addresses and distribute your requests through
    multiple different IP addresses. It can also help if you rotate your public IP
    addresses periodically, which is pretty easy in the cloud by just re-creating
    instances, which receive new public IP addresses.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到限流问题，那么可能是因为你从同一个源IP地址发送了过多请求。一个有效的解决方案是创建一个具有不同公网IP地址的出口节点池，并通过多个不同的IP地址分发请求。如果定期轮换公网IP地址也会有所帮助，在云平台中通过重新创建实例就可以很容易地获得新的公网IP地址。
- en: The opposite issue is if you have an agreement with some third-party company
    and they specifically allow traffic from your organization by whitelisting some
    IP addresses you provide. In this case, you need to manage static public IP addresses
    that don’t change and ensure that all requests to that third-party organization
    go out through the whitelisted IP addresses.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种问题是，如果你与某个第三方公司有协议，并且他们通过白名单允许你提供的某些IP地址访问流量，那么你需要管理那些不会变化的静态公网IP地址，并确保所有发往该第三方组织的请求都通过这些白名单IP地址发送。
- en: Finally, to address the risk of being reported and flagged by your cloud provider,
    you may need to isolate your egress access to a separate account. Most cloud provider
    sanctions are at the account level. If your egress account is disabled by your
    cloud provider, at least it will not bring down the entire enterprise.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了应对被云服务提供商报告并标记的风险，你可能需要将出口访问隔离到一个单独的账户。大多数云服务提供商的制裁是基于账户级别的。如果你的出口账户被云服务提供商禁用，至少不会影响整个企业。
- en: Managing the DNS at the cluster level
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群级别的 DNS 管理
- en: Large-scale clusters with lots of pods and services may put a high load on CoreDNS,
    which is the internal DNS server of Kubernetes. It’s important to ensure sufficient
    DNS capacity since most internal communication between workloads in the cluster
    uses DNS names for addressing and not direct IP addresses.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模的集群，包含大量的 pod 和服务，可能会给 CoreDNS（Kubernetes 的内部 DNS 服务器）带来较高的负载。确保足够的 DNS 容量非常重要，因为集群内大多数工作负载之间的内部通信是通过
    DNS 名称进行寻址，而不是直接使用 IP 地址。
- en: It is recommended to use DNS autoscaling, which is often not enabled by default.
    See [https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/](https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/)
    for more details.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 建议使用 DNS 自动扩展，默认情况下通常不会启用。详情请参见 [https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/](https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/)。
- en: Storage
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储
- en: Storage is arguably the most critical element of your infrastructure. This is
    where your persistent data lives, which is the long-term memory of organizations.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 存储可以说是你基础设施中最为关键的元素。这里存储着你的持久化数据，它是组织的长期记忆。
- en: Choose the right storage solutions
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择合适的存储解决方案
- en: There are many storage solutions available for Kubernetes clusters in the cloud,
    such as cloud-native blob storage, managed storage services, managed databases,
    and managed file systems. You should develop a deep understanding of the performance,
    durability, and cost of each storage solution and match them against your storage
    use cases.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 云中有许多适用于 Kubernetes 集群的存储解决方案，如云原生对象存储、托管存储服务、托管数据库和托管文件系统。你应该深入了解每种存储解决方案的性能、耐用性和成本，并根据存储使用场景进行匹配。
- en: Your baseline should always be cloud-native blob storage (AKA buckets) like
    AWS S3, GCP Google Cloud Storage, or Azure Blob Storage. It’s hard to imagine
    a large-scale Managed Kubernetes enterprise that doesn’t use buckets.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你的基础设施应该始终以云原生的对象存储（即桶存储），如 AWS S3、GCP Google Cloud Storage 或 Azure Blob Storage
    为基础。很难想象一个大型的托管 Kubernetes 企业不使用桶存储。
- en: Then, consider more structured or high-level storage solutions. If you aim to
    stay cloud-agnostic, you may ignore cloud-based managed storage solutions and
    deploy your own solutions, as we saw in *Chapter 6*, *Managing Storage*.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，考虑更为结构化或高层次的存储解决方案。如果你希望保持云中立，你可以忽略基于云的托管存储解决方案，部署你自己的解决方案，就像我们在*第 6 章*《管理存储》中所看到的那样。
- en: At an enterprise scale, it may be worthwhile considering different levels of
    access speed and cost for data at different levels of importance.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业规模下，考虑不同重要性层级的数据存取速度和成本可能是值得的。
- en: Data backup and recovery
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据备份与恢复
- en: Plan for data backup and recovery. Your data is valuable. Data backup and recovery
    are crucial for production environments. Consider implementing data backup and
    recovery processes that are reliable and scalable, and make sure they are regularly
    tested and updated.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 规划数据备份与恢复。你的数据是非常宝贵的，数据备份与恢复对于生产环境至关重要。考虑实施可靠且可扩展的数据备份与恢复流程，并确保它们定期经过测试与更新。
- en: You should also consider data retention policies and not automatically assume
    that all data must be kept forever.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该考虑数据保留策略，并且不要自动假设所有数据都必须永久保存。
- en: Of course, to comply with data privacy laws and regulations like the GDPR, you
    will need to build the ability to selectively delete data too.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，为了遵守数据隐私法和类似 GDPR 的法规，你还需要具备选择性删除数据的能力。
- en: Storage monitoring
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储监控
- en: Set up storage monitoring. Period. Monitoring storage performance, usage, and
    capacity is essential for identifying and resolving issues before they impact
    the availability or performance of your applications. Set up monitoring and alerting
    for storage utilization, latency, and throughput. This is important for managed
    storage, but also for node storage where logs can easily accumulate and render
    a node un-operational.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 设置存储监控。仅此而已。监控存储性能、使用情况和容量对于在问题影响应用程序的可用性或性能之前识别并解决问题至关重要。设置监控和警报，以便及时了解存储使用、延迟和吞吐量。这对托管存储非常重要，也适用于节点存储，因为日志容易积累并导致节点无法正常工作。
- en: Data security
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据安全
- en: Implement data security measures. Protecting sensitive data and ensuring compliance
    with data protection regulations is critical for production environments. Implement
    access controls, encryption, and data security policies to safeguard your data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 实施数据安全措施。保护敏感数据并确保符合数据保护法规对于生产环境至关重要。实施访问控制、加密和数据安全策略以保护数据。
- en: Optimize storage usage
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化存储使用
- en: Kubernetes clusters in the cloud can be expensive, and storage costs can add
    up quickly. Optimize your storage usage by deleting unused data, using data compression
    or deduplication, and setting up storage tiering.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 云中的Kubernetes集群可能会很昂贵，存储成本也可能迅速累积。通过删除未使用的数据、使用数据压缩或去重，并设置存储分层，优化存储使用。
- en: Test and validate storage performance
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试并验证存储性能
- en: Before deploying applications in a production environment, test and validate
    the performance of your storage solution to ensure it meets the performance requirements
    of your workloads.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中部署应用程序之前，请测试并验证您的存储解决方案的性能，以确保它满足工作负载的性能要求。
- en: By considering these factors and implementing best practices for managing storage
    in production for Kubernetes clusters in the cloud, you can ensure reliable and
    scalable storage performance for your applications.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 通过考虑这些因素并实施最佳实践来管理云中Kubernetes集群的生产环境存储，您可以确保应用程序的存储性能既可靠又可扩展。
- en: Now that we have covered a lot of guidelines and best practices for managing
    cloud infrastructure at scale for Kubernetes, let’s shine a spotlight on the management
    of clusters and node pools, which is at the heart of managing multi-cluster Kubernetes
    in production.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了大规模管理Kubernetes云基础设施的许多指南和最佳实践，让我们把重点放在集群和节点池的管理上，这是在生产环境中管理多集群Kubernetes的核心。
- en: Managing clusters and node pools
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理集群和节点池
- en: Managing your clusters and node pools is the top infrastructure administration
    activity for a large-scale Kubernetes-based enterprise. In this section, we will
    look at several crucial aspects, including provisioning, bin packing and utilization,
    upgrades, troubleshooting, and cost management.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 管理您的集群和节点池是大规模Kubernetes企业基础设施管理的核心活动。在本节中，我们将讨论几个重要方面，包括配置、资源利用率、升级、故障排除和成本管理。
- en: Provisioning managed clusters and node pools
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置托管集群和节点池
- en: There are different methods for provisioning clusters and node pools. You should
    choose the method that is best for your use case wisely because failure here can
    result in devastating outages. Let’s review some options. All cloud providers
    offer cluster and node pool provisioning via APIs, CLIs, and UIs. I highly recommend
    avoiding directly using any of these methods and instead using GitOps-based declarative
    approaches. Here are some solid options to consider.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 配置集群和节点池有不同的方法。您应根据实际情况明智地选择最适合的方法，因为在这一环节的失败可能导致灾难性的停机。让我们来回顾一下几种选项。所有云服务提供商都通过API、CLI和UI提供集群和节点池的配置。我强烈建议避免直接使用这些方法，而应使用基于GitOps的声明式方法。这里有一些值得考虑的可靠选项。
- en: The Cluster API
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群API
- en: The Cluster API is an open-source project from the Cluster Lifecycle SIG. Its
    goal is to make provisioning, upgrading, and operating multiple Kubernetes clusters
    easier. It is focused on clusters’ and node pools’ lifecycles. However, it started
    mostly as a way to provision clusters using kubeadm. Managed cluster support on
    different cloud providers was added later, and it is still young. In particular,
    GKE is not supported (although you can provision Kubernetes clusters on GCP as
    an infrastructure layer). AKS and EKS are supported.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Cluster API是来自Cluster Lifecycle SIG的开源项目。它的目标是使多个Kubernetes集群的配置、升级和操作更加简单。它专注于集群和节点池的生命周期。然而，它最初主要是使用kubeadm来配置集群的一种方法。稍后添加了对不同云提供商的托管集群支持，但它仍然很年轻。特别是，GKE不受支持（尽管您可以作为基础设施层在GCP上配置Kubernetes集群）。AKS和EKS是支持的。
- en: The Cluster API has a lot of momentum, and if you don’t operate GKE, you should
    definitely look into it.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Cluster API有很大的动力，如果您不操作GKE，您应该确实研究一下它。
- en: Terraform/Pulumi
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Terraform/Pulumi
- en: Terraform and Pulumi are similar in their approach. They can provision clusters
    and node pools on all cloud providers. However, these tools on their own can’t
    respond to out-of-band changes and don’t monitor the state of the infrastructure
    after provisioning. Their internal state can deviate from the real world and that
    can cause difficult to recover from situations that require careful “surgery.”
    In particular, node pools often need to be provisioned or updated, and Terraform
    or Pulumi might not be up to the task. If you have a lot of experience with these
    tools and are aware of their quirks and special requirements, they may still be
    a good option for you.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform和Pulumi在方法上类似。它们可以在所有云提供商上为您配置集群和节点池。然而，这些工具本身无法响应带外变更，并且在配置后不监控基础设施的状态。它们的内部状态可能会偏离真实世界，这可能导致需要小心“手术”的难以恢复的情况。特别是，节点池经常需要配置或更新，而Terraform或Pulumi可能无法胜任。如果您对这些工具有很多经验，并且了解它们的怪癖和特殊要求，它们仍然可能是一个不错的选择。
- en: Kubernetes operators
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubernetes运算符
- en: Another alternative is to use Kubernetes operators that reconcile CRDs with
    cluster and node pool specs with the cloud provider. Under the covers, the operator
    will invoke Managed Kubernetes APIs from the cloud provider. This requires non-trivial
    work and expertise in writing Kubernetes operators but gives you the ultimate
    control.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是使用Kubernetes运算符，它们可以将CRD与集群和节点池规范与云提供商进行协调。在幕后，运算符将调用云提供商的托管Kubernetes
    API。这需要非平凡的工作和编写Kubernetes运算符的专业知识，但可以给您带来最终的控制。
- en: You may try to use Crossplane instead of writing your own operator; however,
    Crossplane support seems pretty basic and incomplete at the moment. One option
    to expand the scope is to use the Upjet project ([https://github.com/upbound/upjet](https://github.com/upbound/upjet))
    to generate Crossplane controllers from Terraform providers.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试使用Crossplane而不是编写自己的运算符；然而，目前Crossplane的支持似乎相当基础和不完整。扩展范围的一个选择是使用Upjet项目（[https://github.com/upbound/upjet](https://github.com/upbound/upjet)）从Terraform提供程序生成Crossplane控制器。
- en: Utilizing managed nodes
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用托管节点
- en: You can also try to use managed nodes, so you never need to deal with provisioning
    node pools and nodes directly. All cloud providers offer some form of managed
    nodes such as GKE AutoPilot, EKS + Fargate, and AKS + ACI. For enterprise use
    cases, I believe you will need more control than fully managed node pools provide.
    It may be a good option for a subset of your workloads. However, at scale, you
    will want to optimize your resource usage and performance and the limitations
    of managed node pools might be too severe.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以尝试使用托管节点，这样您就永远不需要直接处理节点池和节点的配置。所有云提供商都提供某种形式的托管节点，如GKE AutoPilot、EKS +
    Fargate和AKS + ACI。对于企业使用情况，我认为您可能需要比完全托管的节点池提供更多的控制。这可能是您工作负载的一部分的一个不错选择。然而，在规模化之后，您将希望优化资源使用和性能，而托管节点池的限制可能会过于严格。
- en: Once you have figured out how to provision and manage your clusters and node
    pools, you should turn your attention to effectively using the resources you provisioned.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您找出如何配置和管理您的集群和节点池，您应该将注意力转向有效地使用您配置的资源。
- en: Bin packing and utilization
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 装箱和利用率
- en: 'Cloud resources are expensive. Efficient usage of resources on Kubernetes has
    two parts: efficiently scheduling pods to nodes based on their resource requests,
    and pods actually using the resources they requested.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 云资源是昂贵的。在Kubernetes上有效使用资源有两个部分：根据其资源请求有效地调度pod到节点，以及实际使用它们请求的资源的pod。
- en: Bin packing means ensuring that the total sum of resource requests is as close
    as possible to the allocatable resources on the target node. Once a workload is
    scheduled to a node, it will not be evicted under normal conditions even if the
    node is highly underutilized, but components like the cluster autoscaler can help
    here.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Resource utilization measures what percentage of the requested resource is actually
    used. Resource utilization is in general not fixed as the resource usage of workloads
    may vary widely throughout their lifetimes.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of nuances to bin packing, resource utilization, and the interplay
    between them. For example, there are different resources such as CPU, memory,
    disk, and network. A node may have 100% bin packing for CPU, but only 20% bin
    packing memory. Network and non-ephemeral disks on the node are shared resources
    that pods can request to ensure they will always have a certain amount. This complicates
    operation and reliability. Let’s discuss some principles and concepts that can
    assist in navigating this complicated topic.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Understanding workload shape
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Workload shape is the ratio between the workload CPU requests and its memory
    requests. In the cloud, there is a standard ratio of 1 CPU to 4 GiB of memory.
    As a result, most VM types that you can choose offer resource capacities with
    this ratio. Some workloads need more memory or more CPU than this ratio. All cloud
    providers also offer high-memory VM types with a ratio of 1 CPU to 8 GiB of memory
    as well as high-CPU VM types with 1 CPU to 2 GiB of memory.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the resource shape of your workloads is necessary to inform the
    VM types you choose to optimize your resource usage.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: For example, if a workload requires 1 CPU and 8 GiB of memory and you schedule
    it on a VM type with a ratio of 1:4, you will need to run it on a node that has
    2 CPUs and 8 GiB of memory. No other pod can run on this node since the original
    workload uses all the 8 GiB of memory. However, 1 CPU out of 2 is not used at
    all. It would have been much better to schedule the workload on a node with a
    VM type of 1:8 ratio, which ensures optimal bin packing.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Setting requests and limits
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting requests and limits for your workloads is a key for proper resource
    utilization. As you recall when you set requests for your pod’s containers, it
    will be scheduled to a node that has at least the requested number of resources
    available for the total sum of the requests of all containers. The requested resources
    are allocated for the exclusive use of each container for as long as the pod running
    on the node. The containers may use more resources than the requests if available.
    If you specify CPU limits and the container tries to use more CPU than the limit,
    then the pod may get throttled. If you specify a memory limit and the container
    tries to use more memory than the limit, then the container will be OOMKilled
    and restarted.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: It is best practice to set resource requests for CPU, memory, and even ephemeral
    storage if the container uses any. How do you know how much to request? You can
    start with a rough estimate and monitor the actual resource usage over time and
    fine-tune it later.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是为CPU、内存，甚至是短暂存储（如果容器使用的话）设置资源请求。你如何知道应该请求多少资源？你可以从一个粗略的估算开始，随着时间的推移监控实际的资源使用情况，并在后续进行微调。
- en: But even this straightforward method has some subtleties. Suppose a workload
    uses between 2 CPUs and 4 CPUs with an average of 3 CPUs. Should you request 4
    CPUs and know for sure that the workload will never get throttled? But then, you
    waste a whole CPU because the average usage is just 3 CPUs. If you request 3 CPUs,
    are you going to get throttled every time the workload needs more than 3 CPUs?
    That depends on the available CPU on the node the pod is scheduled to. If the
    overall CPU on the node is saturated because all the pods need a lot of CPU, then
    it is possible.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 但即便是这种直接的方法也有一些微妙之处。假设一个工作负载使用2个至4个CPU，平均为3个CPU。你应该请求4个CPU并确保工作负载永远不会被限速吗？但这样一来，你就浪费了一个CPU，因为平均使用量只有3个CPU。如果你请求3个CPU，每当工作负载需要超过3个CPU时，是否会被限速？这取决于Pod调度到的节点上的可用CPU。如果节点上的总体CPU已经饱和，因为所有Pod都需要大量的CPU，那么这种情况是可能发生的。
- en: On top of plain requests, you can also assign priorities to workloads, which
    allow you to control the destiny of high-priority workloads and ensure they take
    precedence over non-prioritized or low-priority workloads.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 除了普通的资源请求，你还可以为工作负载分配优先级，这样可以控制高优先级工作负载的调度顺序，并确保它们优先于那些没有优先级或低优先级的工作负载。
- en: Yes, scheduling is far from trivial. If you need a refresher, check out the
    *Understanding the design of the Kubernetes scheduler* section in *Chapter 15*,
    *Extending Kubernetes*.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，调度远非简单。如果你需要复习，可以查看*第15章*中*理解Kubernetes调度器的设计*部分，章节来自于*扩展Kubernetes*。
- en: Let’s turn our attention to limits. A simple approach is to set limits equal
    to the requests. This ensures in general that containers will not use more resources
    than they requested, which makes bin packing easy. However, in real-world situations,
    the resource usage of workloads varies. It is often more economical to request
    less than the maximal usage or sometimes even less than the average usage. In
    this case, you may opt not to set limits at all or set the limits higher than
    the requests. For example, if a workload uses 1 to 4 CPUs, then you may decide
    to request 2 CPUs and set the limits to 4 CPUs. Requesting just 2 CPUs will allow
    packing more pods into the same node or schedule the pod into a smaller node.
    So, why set limits at all? Well, setting some limits ensures the pods don’t get
    out of control, hog all the CPU, and starve all other pods that may also set lower
    requests but actually need additional CPU.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将注意力转向限制。一个简单的方法是将限制设置为与请求相等。这通常可以确保容器不会使用超过请求的资源，这使得容器打包变得容易。然而，在实际情况中，工作负载的资源使用量是变化的。通常，申请少于最大使用量，或者有时甚至少于平均使用量，是更经济的。在这种情况下，你可以选择不设置限制，或者将限制设置得比请求值更高。例如，如果一个工作负载使用1到4个CPU，你可以决定请求2个CPU，并将限制设置为4个CPU。只请求2个CPU将允许在同一个节点上容纳更多的Pod，或者将Pod调度到较小的节点上。那么，为什么还要设置限制呢？嗯，设置一些限制可以确保Pods不会失控，霸占所有CPU，导致其他Pods的请求资源较低但实际上需要更多CPU的Pods受到资源饥饿。
- en: Setting memory high limits is even more important, especially for workloads
    that are more sensitive and that shouldn’t be restarted often since any attempt
    to use more allocated memory than the limit will result in a container restart.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 设置内存高限制更加重要，特别是对于那些更敏感且不应频繁重启的工作负载，因为任何超出分配内存限制的尝试都会导致容器重启。
- en: Utilizing init containers
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用初始化容器
- en: Some workloads need to do a lot of work when they just start and then their
    resource requirements are lower. For example, a workload needs 10 GiB of memory
    and 4 CPUs to fetch some data and process it in memory before it is ready to handle
    requests. However, once it’s running, it doesn’t need more than 1 CPU and 4 GiB.
    It would be pretty wasteful to request 4 CPUs and 10 GiB if the pod is a long-running
    one. This is where init containers are very useful. You can split your workload
    into two containers. All the initialization work that requires 4 CPUs and 10 GiB
    can be done by an init container and then the main container can request just
    1 CPU and 4 GiB.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Shared nodes vs. dedicated nodes
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When designing your node pools, you have two fundamental choices to make. Shared
    node pools have multiple different workloads scheduled and run side by side on
    the same node. Dedicated node pools have a single workload taking over a single
    node (possibly multiple instances of the same workload).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Shared node pools are simple. The extreme case is that you have just a single
    node pool and all pods are scheduled to nodes from this node pool. If you have
    multiple shared node pools (e.g., one with regular nodes and one with spot instances),
    then you need to assign taints to node pools and tolerations to workloads as well
    as dealing with node and pod affinity.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Since you don’t know exactly which combination of pods will end up on which
    node, there might be inefficiencies with bin packing. However, as long as the
    overall average workload shape matches the resource ratio of your nodes, bin packing
    at a large scale should be close to optimal.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Workloads can request the CPU, memory, and ephemeral storage they need. However,
    there are some shared resources on the node, like network and disk I/O, that you
    can’t easily carve out for your workload when other workloads on the same node
    might try to use the same resource.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: This is where dedicated node pools come in. Critical workloads like databases
    or event queues require predictable network and disk I/O. Scheduling such workloads
    on a dedicated node ensures the workload doesn’t have to worry about other workloads
    cannibalizing the shared resources.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: It makes sense in this case for the workload to request more than 50% of the
    standard resources like CPU or memory to ensure exactly one pod of the critical
    workload is scheduled on the node.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Remember that system daemons will also run on the node and have higher priority.
    If your dedicated workload requests too many resources, it might become unschedulable.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: I have run into this issue after an upgrade where the daemonsets on the node
    required more resources and caused the dedicated workload to be unschedulable
    until it reduced its resource requests.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Large nodes vs, small nodes
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the cloud, nodes come in a variety of sizes, from 1 core to tens or even
    hundreds of cores. Should you have lots of small nodes or fewer large nodes?
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: First and foremost, you must have nodes that your largest workloads fit into.
    For example, if a workload requests 8 CPUs, then you must have a node with at
    least 8 CPUs available.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: But what about much bigger nodes? There are advantages in terms of efficiency
    for larger nodes. In the cloud, when you provision (for example) a node with 1
    CPU core and 4 GIB of memory, you don’t really get all these resources. First,
    the OS, the container runtime, and kube-proxy take their resources, then the additional
    processes the cloud provider decides to run on each node, then various sys daemonsets
    and your own daemonsets. Finally, what’s left is available for your workloads.
    All these processes and workloads that always run on every node need a lot of
    resources. However, the resources they require are not proportional to the size
    of the node. This means that on small nodes, a much smaller percentage of the
    resources you pay for will be available for your pods. Let’s look at an example.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the resource breakdown for a real node running on an AKS production
    cluster. It has a VM type of **Standard_F2s_v2** ([https://learn.microsoft.com/en-us/azure/virtual-machines/fsv2-series](https://learn.microsoft.com/en-us/azure/virtual-machines/fsv2-series)).
    It has 2 CPUs and 4 GIB of memory. However, the allocatable CPU and memory is
    1.9 CPU and 2.1 GiB. Yes, this is correct. You barely get a little more than 50%
    of the memory available on the node:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'But the story doesn’t end here. There are system daemonsets running in kube-system.
    You can find them with the following command:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s look at the resources requested by these workloads on our node:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That’s a total of 0.56 CPU and 520Mi of memory. If we subtract it from the allocatable
    CPU and memory, we end up with 1.4 CPU and 1.58 GiB of memory.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: This is quite eye-opening. On a small node with 2 CPUs and 4 GIB of memory,
    we end up with 70% of the CPU and less than 40% of the memory. Beyond the cost
    implications, if you miscalculate and assume you can schedule, for example, a
    pod that requests 2 GIB of memory on a 4 GiB node, you’ll have an unpleasant surprise
    when your pod remains pending because it doesn’t fit on this node.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at large nodes. A Standard_D64ads_v5 Azure VM has a whopping 64
    cores and 256 GiB of memory. It is undoubtedly a beast. Let’s look at its capacity
    and allocatable resources:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, we lost 740 mcpu (as opposed to 100 mcpu on the small node) and 17 GiB
    of memory. This sounds like a lot, but proportionally, it is much better. Let’s
    look at system workloads to get the full picture:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: That’s a total of 0.365 CPU and 520Mi of memory. Surprisingly, less CPU is requested
    than the small node and the memory requests are the same. If we subtract it from
    the allocatable CPU and memory, we end up with 62.9 CPU and 238.48 GiB of memory.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: On a large node with 64 cores and 256 GiB of memory, we end up with more than
    98% of the CPU and more than 93% of the memory.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: This is a pretty clear victory for large nodes in terms of resource provisioning
    efficiency and getting more resources available for your workloads.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: However, there are additional nuances and considerations to consider. Let’s
    consider small and short-lived workloads.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Small and short-lived workloads
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we use large nodes, and our cluster is bin-packed very efficiently.
    Some deployment needs to scale up and create a new pod. If there is no room for
    the new pod in any of the existing nodes, then a new node must be provisioned.
    However, if the new pod is small, then we actually waste a lot of resources by
    running just one small pod on a large node. At scale, and when a lot of pods come
    and go pretty quickly, this may not be a problem. However, consider the following
    scenario – our cluster is normally running on 100 large nodes. During a temporary
    spike of activity, our clusters scaled up to 200 large nodes and then the activity
    went back to normal. Our resource utilization is now 50% (the cluster needs 100
    nodes out of 200). In an ideal world, the cluster autoscaler will eventually scale
    down empty nodes until we have 100 properly bin-packed nodes. But, in the real
    world, especially in the presence of small short-lived pods, new pods may get
    scheduled arbitrarily to all 200 nodes and the autoscaler might have a difficult
    time scaling down. We will see later, in the custom scheduling section, some options.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Another issue with short-lived workloads is that even if they have room on an
    existing node, they can still waste resources if they take a while to get ready.
    Consider a pod that takes 1 minute to get ready and runs, on average, for 1 minute.
    This pod with optimal utilization of its resources still can do better than 50%
    because after it gets scheduled, it reserves its resources on the node for 2 minutes,
    but actually does work for only 1 minute. If the pod needs to pull its image,
    then it can easily take several minutes to get ready.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes scheduler is very sophisticated and can be extended too, as we
    covered in *Chapter 15*, *Extending Kubernetes*. The issues with the inefficient
    scheduling of pods in the different use cases we mentioned could potentially be
    addressed by choosing a different scoring strategy. The default scoring strategy
    of **RequestedToCapacityRatio** is intended to evenly distribute workloads across
    all nodes. This is not ideal for tight bin packing. The **MostAllocated** scoring
    strategy may be preferable here.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Check out [https://kubernetes.io/docs/concepts/scheduling-eviction/resource-bin-packing](https://kubernetes.io/docs/concepts/scheduling-eviction/resource-bin-packing)
    for more details.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Pod density
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pod density is the maximum number of pods per node (the Kubernetes default is
    110). As mentioned earlier, some resources like private IP addresses or system
    daemon CPU and memory may be correlated with the pod density. If your pod density
    is too high, then you may waste the resources that were pre-allocated to support
    many pods on each node. However, if you set the pod density too low, then you
    may not be able to schedule enough pods to run on the node. Let’s consider a large
    node with 64 CPU cores and 256 GiB of memory. If the pod density is 100, then
    at most 100 pods can run on this node. Suppose we have a lot of small pods that
    use only 10 mcpu and 100 MiB of memory. 100 pods need only 10 CPU cores and 10
    GiB of memory combined. If 100 such pods get scheduled to one large node, the
    node will be highly underutilized. 54 CPU cores and 246 GIB of memory will be
    wasted.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: If you go with the shared node pool model, then it’s an arbitrary mix of pods
    with different workload shapes, and resource requirements can get scheduled to
    nodes.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Fallback node pools
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cloud providers suffer from temporary capacity issues from time to time, and
    as a result, are unable to provision new nodes. In addition, spot instances may
    disappear at any time if there is a lot of demand for regular nodes. The good
    news is that these outages are a zonal affair and also are typically limited to
    a specific instance type or VM family.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: A good strategy to address this issue is to use fallback node pools.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: A fallback node pool is an empty node pool with autoscaling disabled that has
    the same labels and taints as another active node pool but with a different VM
    family or a different node type (e.g., regular vs. spot). If the active node pool
    is unable to provision more nodes and there are pending pods, then the back node
    pool can be resized and/or become auto-scaling. This will allow the pending pods
    to be scheduled to the backup node pool until the situation with the native node
    pool is resolved.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: If you choose this path, you need to come up with a proper procedure to activate
    the backup node pool, which includes detection of issues in the active node pool,
    a manual or automated process for backup node pool activation, and a scale-back
    process when the active node pool is back to normal.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: It is very important to ensure the backup node pool has enough quota to replace
    its active node pool when needed.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: This was a very thorough treatment of bin packing and resource utilization.
    Let’s turn our attention to upgrades.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading Kubernetes
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Upgrading Kubernetes can be a very stressful operation. A hasty upgrade might
    remove support for resource versions, and if you have unsupported resources deployed,
    you will encounter catastrophic failures. Using Managed Kubernetes has its pros
    and cons. When it comes to upgrades, there is, at any point in time, a range of
    supported versions.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: You may upgrade to more recent supported versions. However, if you delay and
    neglect to upgrade, then the cloud provider will upgrade your clusters and node
    pools automatically once you fall behind the cutting edge of supported versions.
    Let’s look at the various elements of upgrading Kubernetes you must be on top
    of.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Know the lifecycle of your cloud provider
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cloud providers can’t support just any Kubernetes version in existence. It
    is critical to know when the current version of your clusters and node pools is
    going to be defunct. All cloud providers have a methodical process and share the
    information broadly. Here are the locations for each of the three major cloud
    providers:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS EKS: [https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html](https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Azure AKS: [https://learn.microsoft.com/en-us/azure/aks/supported-kubernetes-versions](https://learn.microsoft.com/en-us/azure/aks/supported-kubernetes-versions)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google GKE: [https://cloud.google.com/kubernetes-engine/docs/release-schedule](https://cloud.google.com/kubernetes-engine/docs/release-schedule)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, AKS, at the time of writing, supports versions 1.23 through 1.26\.
    In addition, each version has an official end-of-life date. For example, the end-of-life
    date for 1.23 is April 2023\. If your cluster is still on 1.23, then AKS may upgrade
    your cluster automatically to version 1.24\. The process of cloud provider upgrade
    is gradual, done region by region, and might take several weeks.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: All cloud providers offer an API and CLI to check the exact list of versions
    (including patch versions) in every region.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, at the moment, these are versions supported on AKS for the Central
    US region:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, for each minor version, there are several patch versions. It
    is even nice enough to mention which versions you may upgrade to yourself. Due
    to security concerns, the cloud provider may drop support for patch versions at
    any time.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Let’s talk about the upgrade process of the control plane.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading clusters
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When using Managed Kubernetes in the cloud, you are not responsible for the
    operation of the control plane, but you still need to manage the upgrade process.
    You have two options:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Auto upgrade
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual upgrade
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In an auto upgrade, the cloud provider will update your cluster according to
    their schedule, but you still must ensure that the versions of resources in your
    cluster are compatible with the new version. A manual upgrade requires you to
    upgrade yourself but gives you more control over timing. For example, you may
    choose to update earlier to benefit from some new features.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Remember that a manual upgrade doesn’t mean you can stay on the same Kubernetes
    version forever. The cloud provider will forcefully upgrade you if you fall behind
    the minimal supported version.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes releases a new version roughly every 3 months. Cloud providers support
    roughly 4 versions. This means that if you just upgraded to the latest supported
    version, you may hold off for about a year on upgrades, but then you will be on
    the minimal supported version, which means you will now have to upgrade every
    3 months.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Note that you should upgrade the control plane one minor version at a time.
    If you are on version 1.24, and you want to upgrade to 1.26, you have to upgrade
    to 1.25 first and then from 1.25 to 1.26.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: The bottom line is that upgrading the Kubernetes control plane is a standard
    operation that takes place multiple times per year. You should have a streamlined
    process for it.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at what is involved.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Planning an upgrade
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You should plan your upgrades and coordinate them with cluster users. Control
    plane upgrades typically take 20-45 minutes. This is a non-disruptive operation
    for your workloads. Your workloads will keep running, and new pods will be scheduled
    to existing nodes. However, node pool operations might be blocked during the control
    plane upgrade.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: If you’re running multiple clusters with a redundancy scheme, it is best to
    perform the upgrades gradually and start with non-critical clusters (e.g., a development
    or staging environment).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: I recommend having owners (engineers or teams) for every namespace. Notify all
    owners about upcoming upgrades so they can reserve time for converting incompatible
    resources.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Detecting incompatible resources
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main concern with an upgrade is that the functionality of your system will
    be compromised or completely broken because it uses resources that are not supported
    anymore. In most cases, a specific version of a resource will be removed, and
    a newer version will be available.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: But you don’t have to wait until the last minute to scramble and replace removed
    resources or versions. Kubernetes has a deprecation policy and resources will
    be deprecated for several versions before they are fully removed. I suggest making
    sure before each upgrade that all deprecated resources are updated or replaced.
    This will ensure that the upgrade process is not stressful because, even if you
    didn’t manage to update all resources, the deprecated resources are still going
    to be supported by the new version and you will have some extra time to update
    them before they are fully removed.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes publishes a migration guide with details about deprecated and removed
    APIs in each version. See [https://kubernetes.io/docs/reference/using-api/deprecation-guide](https://kubernetes.io/docs/reference/using-api/deprecation-guide).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: For example, Kubernetes 1.25 stopped serving the `CronJob` resource with the
    API version of `batch/v1beta1`. Instead, the `batch/v1` `CronJob` resource has
    been available since Kubernetes 1.21\. Ideally, after you upgrade to Kubernetes
    1.21, you have updated all your `CronJob` resources to use `batch/v1`, and by
    the time you upgrade to Kubernetes 1.25, the fact that `batch/v1beta1` is removed
    is not an issue because you are already on the supported version.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to make sure you detect all deprecated and/or removed
    resources that you currently use. You can use the manual method of reading the
    deprecation guide and just scanning your code and detecting incompatible resources.
    Most releases don’t have a lot of deprecations or removals. However, some releases
    may have up to ten different resources that are being deprecated or removed. For
    example, Kubernetes 1.25 stopped serving seven different resource versions.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: A more systematic way is to use a tool like `kube-no-trouble` ([https://github.com/doitintl/kube-no-trouble](https://github.com/doitintl/kube-no-trouble)),
    which scans your clusters and can output a list of deprecated resources.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to install it:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'I have a 1.25 cluster that doesn’t contain any deprecated resources at the
    moment. However, in Kubernetes 1.26, the **HorizontalPodAutoscaler** of version
    autoscaling/v2beta2 will be removed as it has been deprecated since Kubernetes
    1.23\. Let’s create such a resource. There is a `kyverno` deployment in the cluster:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is an HPA that sets the min replicas to 1 and the max replicas to 3:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, kubectl gives a very nice warning when you create a deprecated
    resource version that tells when the resource was deprecated (1.23), when it will
    be removed (1.26), and which version to replace it with (autoscaling/v2).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: This is nice, but it is not sufficient. You probably create your resources through
    CI/CD, which might not receive the same warning, and even if it does, might not
    surface it, because it is not an error. However, if you created the HPA when your
    cluster was on an earlier version of Kubernetes than 1.23, then you wouldn’t get
    any warning because at the time it wasn’t deprecated.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see if kubent can detect the deprecated HPA:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Yes, it does. You get the same information: when it will be removed, when it
    was deprecated, and what to replace it with.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Updating incompatible resources
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Updating an incompatible resource may require some changes to your manifests.
    If the API change just adds new fields, then you may just change the API version
    and be done with it. However, sometimes it may require additional changes.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'OK. We’re about to upgrade our cluster, and we detected some incompatible resources.
    Kubectl and the kubectl-convert plugin can help here. Follow the instructions
    here to install the plugin: [https://kubernetes.io/docs/tasks/tools/#kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl).
    Let’s convert our HPA manifest and see what it looks like:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The conversion succeeded but created a few unnecessary fields. The `creationTimestamp:
    null` is useless as it will be updated on a live resource. Also, the status is
    useless as this is just a manifest file, and the status will be updated at runtime.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the main differences are that `apiVersion` was changed to `apiVersion:
    autoscaling/v1` and that the target CPU percentage is now specified as a single
    field:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Using kubectl-convert saves time, and it is a well-tested tool.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with removed features
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is one other situation we need to address, which is the complete removal
    of a feature without an upgrade path. Kubernetes 1.25 completely removed support
    for **Pod Security Policies** (**PSPS**). The application of PSPs to pods has
    caused confusion for many users who have attempted to utilize them. Check this
    link for more details: [https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/](https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'If you used PSPs, then when the time comes to upgrade to Kubernetes 1.25, your
    PSPs will no longer work. The Kubernetes developers didn’t just remove the feature
    with no alternative. There are two alternatives to PSPs:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Pod security admission
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A third-party admission controller
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pod security admin is a simplified solution that may or may not be a complete
    replacement for PSPs. The Kubernetes developers published a detailed guide for
    migration. Check it out: [https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/](https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/).'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: If you choose a third party (e.g., Kyverno), then you should check its documentation.
    Kyverno comes with a lot of sample policies for pod security and the transition
    is pretty straightforward.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading node pools
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Upgrading the node pools of multiple clusters can be a major undertaking. If
    you have tens to hundreds of clusters and in each cluster, you have multiple node
    pools (5-20 is not uncommon), then be ready for a serious adventure. Control plane
    upgrades (once you have ensured your workloads are compatible with the new version)
    are pretty quick and painless. Node pool upgrades are very difficult. Realistically,
    it can take several weeks to upgrade all the node pools in a large Kubernetes-based
    system with tens to hundreds of node pools with thousands of nodes.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Syncing with control plane upgrades
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes imposes constraints on the versions of the control plane and the
    worker nodes. The Kubernetes node components may be two minor versions behind
    the control plane. If the control plane is on version N, then the node pools may
    be on version N-2\. Since node pool upgrades are much more disruptive and labor-intensive
    than control plane upgrades, I recommend upgrading node pools only to every other
    version of Kubernetes. For example, suppose we start with a Kubernetes cluster
    where both the control plane and the nodes are on version 1.24\. When we upgrade
    to version 1.25, we upgrade only the control plane to 1.25 and keep the node pools
    on 1.24, which is compatible. Then, when it’s time to upgrade to 1.26, we upgrade
    the control plane first from 1.25 to 1.26, and then we start upgrading all the
    node pools from 1.24 directly to 1.26\. Let’s see how to go about upgrading node
    pools.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: How to perform node pool upgrades
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Node pool upgrades require a new node pool. It is not possible to upgrade nodes
    within the node pool. It is not even possible to add new nodes with a new version
    to an existing node pool. The node version is one of the essential properties
    of a node pool. What it means is that you actually don’t upgrade an existing node
    pool. You replace your node pool. First, you create a new node pool, scale it
    up, and start draining nodes from the old node pool until all the pods run on
    the new node pool and then you can delete the old (now empty) node pool.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'If your original node pool was an autoscaling node pool, then before starting
    the upgrade process, you must turn off autoscaling; otherwise, the pods you evicted
    from a node in the old node pool might get scheduled right back to the old node
    pool. Let’s list the exact steps you need to take to upgrade a node pool:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Create a new node pool with the exact same specifications (instance type, labels,
    tolerations) as the existing node with the new version.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may pre-allocate some instances to the new pool, so they are ready to schedule
    pods from the old node pool.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turn autoscaling off in the old node pool.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cordon all the nodes in the old node pool to prevent the scheduling of new pods
    to the old pool.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drain the nodes of the old node pool.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observe and deal with problems.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wait for the cluster autoscaler to delete empty old pool nodes or delete them
    yourself to expedite the process.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at some problems that can delay or even hold up the upgrade process.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent draining
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you need to upgrade many node pools with lots of nodes in each, you may decide
    to provision new node pools and start draining all your node pools at once or
    in large batches. This can cause you to exceed your quota or hit cloud provider
    capacity issues.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: You should pay attention to your quota and ensure you have a sufficient quota
    regardless of upgrades. If you’re getting close to your quota ceiling, I suggest
    bumping it before engaging in a complex operation like a node pool upgrade. The
    last thing you want is to be in the middle of an upgrade when you need to scale
    your capacity due to business needs and realize you maxed out your quota.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: A good strategy for handling capacity issues and ramping up speed (how fast
    the cloud provider provision can instances for your new nodes) is to pre-allocate
    those instances. Again, this requires that you have a sufficient quota for the
    old node pool and the new node pool at the same time.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand what happens if you don’t pre-allocate nodes in the new node
    pool. When you drain a node from the old node pool, all the pods are evicted from
    the node and become pending pods. Kubernetes will try to schedule these pods to
    existing nodes if any are available. The old node pool is cordoned, so either
    Kubernetes can find suitable nodes on other existing node pools (it’s a good thing
    that improves bin packing) or the cluster autoscaler will need to provision a
    new node. That takes several minutes. If you drain multiple nodes at the same
    time, then all the pods from these nodes will be pending for a few minutes until
    new nodes can be provisioned and your system’s capacity is degraded. In addition,
    if the cloud provider has capacity issues, maybe it can’t provision new nodes
    and your pods will remain pending until then.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Pre-allocating nodes means that the new node pool will have nodes ready to go.
    The moment a pod is evicted from the old node pool, it will immediately be scheduled
    to an available node in the new node pool.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with workloads with no PDB
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When draining a node, Kubernetes is mindful of **pod disruption budgets** (**PDBs**).
    If a deployment has a PDB that says only one pod can be unavailable and there
    are two pods of this deployment on the drained node, then Kubernetes will evict
    just one pod and wait until it is eventually scheduled before evicting the other
    pod. However, if you have workloads without PDBs, then that means Kubernetes is
    allowed to evict all the pods of those workloads at the same time. For most workloads,
    this is unacceptable. You should identify these workloads and work with their
    owner to add a PDB. Note that in the scenario of draining all nodes at once, workloads
    with no PDB are vulnerable even if they have many pods running on different nodes.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with workloads with PDB zero
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: However, the opposite problem of unevictable pods is the bane of node pool upgrades.
    If a node contains an unevictable pod, then it can’t be drained, and Kubernetes
    will wait forever (or until the pod is manually deleted) before it fully drains
    the node. This can halt the upgrade process indefinitely and typically requires
    coordinating with the workload owner to resolve it.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 'If a workload has a PDB with `minUnavailable: 0`, it means that Kubernetes
    is not allowed to evict even a single pod from the workload regardless of how
    many replicas the workload has.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Some workloads (usually stateful) are more sensitive than others and prefer
    not to be disrupted at all. This is, of course, an unrealistic expectation because
    the node itself might go bad or the underlying VM might disappear due to cloud
    provider issues, and then the pods scheduled on it will have to be evicted. It’s
    best to work with workload owners and come up with a solution that minimizes disruption,
    but still allows upgrades to progress. This has to be worked out before the upgrade
    process starts. You don’t want to be in a situation where a single workload holds
    a node pool upgrade process hostage, and you have to beg the workload owner to
    allow you to evict a pod.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'But, in addition to strict PDB-zero workloads, you might run into effective
    PDB-zero situations. Consider a workload with a PDB of `minUnavailable: 1`. This
    is pretty common and means the workload allows one pod at a time to be unavailable.
    When draining the pods of this workload, Kubernetes is allowed to evict one pod
    as long as all the other pods are running. However, if even one of the pods of
    this workload is pending or unable to be ready due to any reason, then effectively
    the workload already has one pod unavailable, and the upgrade process will be
    halted again.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: The best practice here is to identify these workloads before the upgrade process
    starts and ensure that all workloads are healthy and can participate in the node
    pool upgrade process.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: However, even if you did all the preparation work ahead of time, some workloads
    might get into an unhealthy state during the upgrade process (remember we’re talking
    about a process that can take weeks).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: I recommend having strong monitoring of the progress of the upgrade process,
    detecting stuck pods, and working with owners to resolve issues. In the case of
    pods that are scheduled on the old node pool and can’t be ready, it is a simple
    solution to just delete the pod, see it is scheduled to the new node pool, and
    let the workload owner resolve the problem on the new node pool.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Other cases might require more creative solutions.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Let’s turn our attention to various problems that can occur in a cluster and
    how to handle them, especially at scale.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover the troubleshooting process in a production cluster
    and the logical procession of actions to take. The pod lifecycle involves multiple
    phases and failures can occur at each phase. In addition, pod containers go through
    their own mini lifecycle where init containers are running to completion and then
    the main containers start running. Let’s see what can go wrong along the way and
    how to handle it.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s look at pending pods.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: Handling pending pods
  id: totrans-399
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a new pod was created, Kubernetes used to place it in the Pending state
    and try to find a node to schedule it on. However, since Kubernetes 1.26, there
    is an even earlier state where a pod can’t be scheduled.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a new 1.26 kind cluster called “`trouble`" and enable the pod
    scheduling readiness feature. Here is the configuration file (`cluster-config.yaml`):'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And here is how to create the kind cluster:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Next, we’ll create a new namespace called `trouble` and take it from there.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s create a pod with a scheduling gate called `no-scheduling-yet`:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, the pod has a status of `SchedulingGated`.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of the scheduling gate is that if the pod can’t be scheduled yet
    due to issues like the quota, which need to be resolved externally, then the pod
    in this state will not cause a lot of churns to the Kubernetes scheduler, which
    will ignore it. After the external issue is resolved, you (or more likely an operator)
    can remove the feature gate and the pod will become a pending pod ready to be
    scheduled.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s turn our attention to pending pods. It’s okay for a pod to be pending;
    however, if a pod is pending for more than a few minutes, something is wrong and
    we need to investigate it. I suggest having an alert set up for pods pending for
    more than X minutes (reasonable values for X can be between 10 and 60 minutes).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of pending pods: temporarily pending pods and permanent
    pending pods. Temporarily pending pods may be scheduled to one of the existing
    node pools; however, there is currently no room on any of the nodes. If the node
    pool has autoscaling enabled, the cluster autoscaler will try to provision a new
    node. If the node pool has autoscaling disabled, then the pod will remain pending
    until some other pods complete or are evicted from a node to make room. Another
    category of temporarily unschedulable pods is if the target namespace has a resource
    quota that is maxed out at the moment. Here is an example, where the namespace
    has a resource quota of 1 CPU and a deployment with 3 replicas is created where
    each pod requests 0.5 CPU. Only 2 pods can fit with the namespace quota. The third
    pod will be pending:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, let’s create the deployment and see what happens:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We end up with just two running pods as expected:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note, that there is no third pending pod in this case. Kubernetes is smart enough
    to create only two pods in this case.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason is the namespace quota:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Permanent pending pods are pods that can’t be scheduled on any of the available
    node pools, so provisioning a new node will not help. There are several categories
    of such permanently unschedulable pods:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: All the node pools have taints, and the pod doesn’t have the proper tolerations.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pod requests more resources than are available on any of the node pools.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pod is waiting for a persistent volume.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pod has incorrect `nodeSelector` or `nodeAffinity`.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s delete the previous deployment and resource quota and look at an example
    of a pod that just requests way too many resources (666 CPU cores):'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If we look at the pod that was created now, we can see it is indeed pending:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To understand why it is pending, we can look at the status of the pod:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The message is pretty clear and explains that 0 out of 1 node is available to
    schedule. It even says that 1 node has insufficient CPU. If there were other nodes
    in the cluster with other reasons, it would list them too.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: Pending pods don’t use resources and don’t take up space in nodes; however,
    they put some pressure on the API server and also it means that some workloads
    don’t get to do their work and they are waiting for a node to be scheduled on,
    which might be very serious in production. Mind your pending pods and make sure
    to resolve any issues quickly.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: The next category of problems is about pods that are scheduled to a node but
    are unable to run.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Handling scheduled pods that are not running
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There may be several reasons why a scheduled pod is not running. One of the
    most common ones is failure to pull an image required by one of the pod’s containers.
    The kubelet will just keep trying and the pod’s status will show as `ErrImagePull`:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s check the pods:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To see a more elaborate message, we can check the `containerStatuses` field
    of the status:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Image pull errors could relate to a misconfigured image. The image name or the
    image tag might be wrong. However, the image may be correct but might have been
    deleted accidentally from the registry. If you try to pull from a private registry,
    then possibly you don’t have the correct permissions. Finally, the image registry
    may be unavailable. For example, Docker Hub often has rate limits.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: You may prefer to pull all your images from a single source you control, where
    you can scan and curate the images and ensure that images don’t disappear from
    you. If you’re on the cloud, then every cloud provider offers their image registry.
    This should be the preferred option in most cases. You may save some money by
    using a private registry, and you may prefer a different solution in hybrid cloud
    scenarios.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: 'Another reason that a pod doesn’t start running is an init container that doesn’t
    complete:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Checking the pod status, we can see that it is stuck in the init phase because
    our init container is in the pause container, which never completes:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Sometimes the pod starts running, but the container keeps failing. In this
    case, you need to check your pod’s logs or your Dockerfile for the reason. Here
    is a pod that keeps crashing because its Dockerfile command just exists with exit
    code 1:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The result is that the pod will have a status of `RunContainerError`. Kubernetes
    will keep restarting the pod (assuming the default restart policy of always):'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Getting your pods and containers to a running state is a good start, but it
    is not enough. In order for Kubernetes to send requests to your pods, all the
    containers must be ready. Let’s see what problems you might run into.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: Handling running pods that are not ready
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If all your init containers are completed and all your main containers are running
    with no errors, then probes come into play. Kubernetes considers a pod with running
    containers ready to receive requests if no probes are defined or if all probes
    for all containers succeed. The startup probe is checked initially until it succeeds.
    If it fails, the pod is not considered ready. If your container has a hung startup
    pod, it will never be ready.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes will eventually kill and restart your container and the startup probe
    will have another chance.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a deployment where the main container has a startup probe that will
    always fail (the pause container doesn’t even listen on port 80). The pod will
    never get to the ready state. After some retries and delays defined by the startup
    probe, Kubernetes will restart the container and the cycle will repeat:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Checking the pod shows that the pod is in `CrashloopBackoff` and Kubernetes
    keeps restarting the container:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note that the delay between restarts grows exponentially to avoid a bad container
    putting a lot of stress on the API server and the kubelet having to keep restarting
    often.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: If there is no startup probe or it succeeds, the hurdle is the readiness probe.
    It works very similar to a startup probe, except Kubernetes will not restart the
    container. It will just keep checking the readiness probe. When a readiness probe
    fails, the pod will be removed from the endpoints list of any service that matches
    its labels, so it doesn’t get to handle any requests. However, the pod remains
    alive and consumes resources on the node.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see it in action:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As you can see, the pod is running for an hour, it never gets ready, and it
    is not restarted:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The last type of probe is the liveness probe. It works just like the startup
    probe (the container will get restarted and the pod will be in `CrashloopBackoff`),
    except it is checked periodically, even if it succeeds while the startup probe
    is a one-time deal. Once it succeeds, it is never checked again.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: The reason both startup and liveness probes are needed is that some containers
    need a longer startup period, but once they are initialized, then periodic liveness
    checks should be shorter.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: That covers troubleshooting the pod lifecycle. When pods are scheduled but are
    not running or are not ready, it has cost implications. Let’s move on and consider
    cost management.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: Cost management
  id: totrans-469
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When running Kubernetes at a large scale in the cloud, one of the major concerns
    is the cost of the infrastructure. Cloud providers offer a variety of infrastructure
    options and services for your Kubernetes clusters. These are expensive. To harness
    your costs, make sure you follow best practices such as:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: Having a cost mindset
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost observability
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The smart selection of resources
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient usage of resources
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discounts, reserved instances, and spot instances
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invest in local environments
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s review them one by one.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: Cost mindset
  id: totrans-478
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Engineers often neglect cost or put it way down the priority list. I often
    think in this order: make it work, make it fast, make it last, make it secure,
    and only then make it cheap. This is not necessarily a bad thing, especially for
    startup companies or new projects. Growth and velocity are often the top priorities.
    After all, if you don’t have a good product, and you don’t have customers, then
    the business will fail even if your costs are zero.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: In addition, when the system is small, the absolute cost might be relatively
    small even if there is a lot of waste. Add to that the fact that cloud providers
    lure companies in with generous credits.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: However, if you are part of a large enterprise or your startup succeeds and
    grows, at some point, cost will become a significant concern. At this point, you
    need to shift your thinking and have cost as the primary concern for everything
    you do. Cost may or may not be aligned with other initiatives. For example, everybody
    loves Pareto improvements. If you just manage to use 20% fewer VMs to accomplish
    the same task, then you will automatically save a lot of money without negatively
    impacting any other aspect.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: But those easy wins and low-hanging fruit will eventually dry up. Then, you
    get to harder decisions. For example, caching the last week of data in memory
    will give you excellent response times, but at a large cost. What if you just
    cache one day?
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: Availability and redundancy are often at odds with cost as well. Do you really
    need a full-fledged zero-downtime active-active setup across multiple availability
    zones, regions, and cloud providers or can you get by with some downtime and recovery
    from backups in the event of catastrophic failure?
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: You may end up choosing the more expensive option, but you should do it with
    an explicit understanding of how much you pay for it and ensure the value you
    receive is greater.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: That takes us to the next topic of cost observability.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: Cost observability
  id: totrans-486
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To manage your infrastructure cost on Kubernetes and in the cloud in general,
    you must have strong cost-oriented observability. Let’s look at some of the ways
    to accomplish it.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: Tagging
  id: totrans-488
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tagging is associating every resource with a set of tags or labels. From a cost
    perspective, tags should enable you to attribute the cost of any infrastructure
    resource to the relevant stakeholders. For example, you may have a team tag or
    an owner tag. If the resources provisioned by a specific team suddenly grow rapidly,
    you can narrow down the issue more quickly. The specific tags are up to you. Common
    tags may include environment (production, staging, and development), release,
    and git sha. Many resources in the cloud come with cloud-provider tags that you
    can take advantage of.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: Policies and budgets
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Policies and budgets let you rein in wild spending on infrastructure. Some
    policies are implicit, such as a namespace resource quota that will block the
    provisioning of excess resources. However, other policies may be more cost-specific
    and be informed by cost tracking. Budgets let you set a hard limit on spending
    at different scopes. All cloud providers offer budgets as part of their cost management
    solutions:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: 'Tutorial: Create and manage Azure budgets: [https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/tutorial-acm-create-budgets](https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/tutorial-acm-create-budgets)'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Managing your costs with AWS Budgets: [https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html](https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html)'
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create, edit, or delete budgets and budget alerts in GCP: [https://cloud.google.com/billing/docs/how-to/budgets](https://cloud.google.com/billing/docs/how-to/budgets)'
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Policies and budgets are great, but sometimes they are not sufficient, or you
    don’t have the cycles to specify and update them. This is where alerting comes
    into play.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: Alerting
  id: totrans-496
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Budgets are often the last resort to mitigate against rogue infrastructure provisioning
    or accidental runaway provisioning. For example, you may set broad budgets for
    several overall categories, like no more than $500,000 of compute spending. These
    budgets of course need to align with business growth and make sure that they don’t
    cause an incident if you temporarily need to provision more infrastructure to
    handle a temporary spike in demand. Budgets are often set or modified only with
    top management approval.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: Fine-grained and day-to-day cost management alerts are much more nimble and
    practical. If you cross or come close to some cost limit, you can set alerts that
    will let you know and escalate as necessary. The alerts rely on proper tagging,
    so you can have meaningful information to evaluate the cause of the cost increase
    and the responsible party.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, managing costs is a dynamic and complex activity. You need good
    tools to help you.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: Tools
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All the cloud providers have strong cost management tools. Check out:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS Cost Explorer: [https://aws.amazon.com/aws-cost-management/aws-cost-explorer/](https://aws.amazon.com/aws-cost-management/aws-cost-explorer/)'
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Azure Cost Analyzer: [https://www.serverless360.com/azure-cost-analysis](https://www.serverless360.com/azure-cost-analysis)'
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'GCP Cost Management: [https://cloud.google.com/cost-management/](https://cloud.google.com/cost-management/)'
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In addition, you may opt to use a multi-cloud open-source tool like kubecost
    ([https://www.kubecost.com](https://www.kubecost.com)) or a paid product like
    cast.ai ([https://cast.ai](https://cast.ai)). Of course, you can do everything
    yourself, ingest cost metrics from the cloud provider into Prometheus, and build
    your Grafana dashboards and alerts on top of them.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: Remember that picking the right set of tools can literally pay for itself very
    quickly.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: The smart selection of resources
  id: totrans-507
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The cloud offers a plethora of choices and combinations for resources like VMs,
    networking, and disks. We covered all the considerations in the *Bin packing and
    utilization* section earlier in this chapter. However, with a focus on cost, you
    should ensure that you understand that you may be able to get the job done with
    a cheaper alternative and reduce your costs significantly. Familiarize yourself
    with the inventory of resources and stay up to date as cloud providers update
    their offerings and may change prices too.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: Efficient usage of resources
  id: totrans-509
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you’re cutting costs, any unused resources are a red flag. You leave money
    on the table. It is often necessary to build in flexibility. For example, the
    industry average CPU utilization is in the range of 40%-60%. This might seem low,
    but it is not easy to improve on that in a very dynamic environment with constraints
    like high availability, quick recovery, and the ability to scale up quickly.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: Discounts, reserved instances, and spot instances
  id: totrans-511
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the best ways to reduce costs is to just pay less money to the cloud
    providers for the same resources. The common ways to accomplish this are discounts,
    reserved instances, and spot instances.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: Discounts and credits
  id: totrans-513
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Discounts are the best. They only have an upside. You simply pay less than the
    sticker price. That’s it. Well, it’s not that easy. You will need to negotiate
    to get the best prices and often show promise for growth and commitment to stay
    longer on the platform.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: Credits are another great way to offset initial cloud spending. All the major
    cloud providers offer various credit programs, and you may be able to negotiate
    more credits too.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: AWS has the Activate program, targeted mostly at startups, where you can get
    up to $100,000 of AWS credit. See [https://aws.amazon.com/activate/](https://aws.amazon.com/activate/).
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: Azure has the Microsoft for Startups program, which offers up to $150,000 of
    Azure credit. See [https://www.microsoft.com/en-us/startups](https://www.microsoft.com/en-us/startups).
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: GCP has the Google for Startups cloud program, which offers (like AWS) up to
    $100,000 of GCP credit. See [https://cloud.google.com/startup](https://cloud.google.com/startup).
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: These programs are designed to boost young startups without a lot of revenue.
    Let’s move on to options for established enterprise organizations that still want
    to reduce their cloud spending.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: Reserved instances
  id: totrans-520
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reserved instances are a very good way to reduce your costs. They require that
    you purchase capacity in bulk and commit for a long period (one year or three
    years). The longer period carries better discounts. Overall, the discounts are
    significant and can vary from 30% to 75% compared to on-demand prices.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the price, reserved instances also ensure capacity compared to on-demand
    instances, which may temporarily be unavailable for particular instance types
    in a particular availability zone.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: The downsides of reserved instances are that you typically have to prepay, and
    the commitment is often tied to specific instance types and regions. You may be
    able to exchange equivalent reservations, but you’ll have to check the terms and
    conditions of the cloud provider. Also, if you are unable to use all your reserved
    capacity, you still pay for it (although at a very discounted price).
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: If you consider reserved instances (RIs), you may opt for a limited capacity
    of reserved instances, which you know you can always utilize, and then use on-demand
    and spot instances to handle spikes and take advantage of the elasticity of the
    cloud. If you discover later that you spend too much on on-demand, you can always
    reserve more instances and come up with a mix of three-year reserved instances,
    one-year reserved instances, on-demand, and spot instances. This is a great segue
    to the next item on the list, which is spot instances.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: Spot instances
  id: totrans-525
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cloud providers love reserved instances. They sell them, provision them, make
    their profit, and can forget about them (except for making sure they’re up and
    running). The on-demand side is very different. Cloud providers have to make sure
    they can reasonably provision more capacity when their customers demand it. In
    particular, cloud providers should roughly have enough capacity to handle the
    quota of each customer even if the customer significantly underutilizes their
    quota. That means that, in practice, under normal conditions, cloud providers
    have a lot of idle or underutilized capacity. This is where spot instances come
    in. Cloud providers can sell this excess capacity, which is in theory allocated
    for on-demand use. If it is needed, then the cloud provider just takes away the
    spot instances and provides them to the on-demand customers.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: Why should you use spot instances? Well, because they are much cheaper. Due
    to their potentially ephemeral nature, they carry significant discounts of up
    to 90%. Remember, from the cloud provider’s point of view, this is free money.
    Those instances are already accounted for and paid for by the markup of on-demand
    instances in use and the quota ratio of each customer.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: In practice, spot instances don’t disappear from under you very quickly. The
    Kubernetes way advocates that workloads shouldn’t care too much about specific
    nodes. If you run on a spot instance and it goes away, all your pods will be evicted
    and scheduled to other nodes. If you have sensitive workloads that don’t handle
    eviction from their node well, then these workloads are not good Kubernetes citizens
    in the first place. Nodes become unhealthy all the time, regardless, and your
    workloads should be able to handle eviction.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: However, there is one situation that needs to be addressed, specifically if
    you run critical workloads on spot instances. In case of a zonal outage of a specific
    instance type, it is possible that many spot instances will be taken by the cloud
    provider. I suggest having empty fallback on-demand node pools with the same or
    similar instance type and the same labels and taints. If a node pool using spot
    instances suddenly loses a lot of nodes and is unable to scale up (because the
    spot instances are unavailable), then you can scale up your empty on-demand node
    pool and your pods will be scheduled there until spot instances become available
    again.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you have enough quota for the fallback node pools to pick up the slack
    if the equivalent spot instances are unavailable.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s talk about local environments and how they can help us save money.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: Invest in local environments
  id: totrans-532
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Organizations practice different protocols of development and testing. Some
    organizations do a lot of testing in the cloud in staging and development environments.
    Sometimes, engineers provision infrastructure for various experiments and tests.
    Such development and test environments can be difficult to manage effectively
    as infrastructure is often provisioned in an ad hoc manner. There are solutions
    like disposable Kubernetes clusters and virtual clusters. Another direction is
    to invest in local development environments that engineers can run on their local
    machines. The advantages are that these environments are often very quick to set
    up and discard, and they don’t incur any expensive cloud costs. The downside is
    that such environments might not be fully representative of the staging or production
    environments. I suggest looking into local environments and finding use cases
    that will save cloud costs without compromising other critical aspects of the
    system.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-534
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered in depth what it takes to run large-scale Managed
    Kubernetes systems in production. We looked at managing multiple clusters, building
    effective processes, handling infrastructure at scale, managing clusters and node
    pools, bin packing and utilization, upgrading Kubernetes, and troubleshooting
    and cost management. That’s a lot, but even that is just the tip of the iceberg.
    There is no substitute for in-depth familiarity with your use cases and special
    concerns. The bottom line is that large-scale enterprise systems are complex to
    manage, but Kubernetes gives you a lot of industrial-strength tools to accomplish
    it.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: 'The next chapter will conclude the book. We will look at the future of Kubernetes
    and the road ahead. Spoiler alert: the future is very bright. Kubernetes has established
    itself as the gold standard for cloud-native computing. It is being used across
    the board, and it keeps evolving responsibly. An entire support system has developed
    around Kubernetes, including training, open-source projects, tools, and products.
    The community is amazing, and the momentum is very strong.'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: Join us on Discord!
  id: totrans-537
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read this book alongside other users, cloud experts, authors, and like-minded
    professionals.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: Ask questions, provide solutions to other readers, chat with the authors via.
    Ask Me Anything sessions and much more.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: Scan the QR code or visit the link to join the community now.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code844810820358034203.png)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
