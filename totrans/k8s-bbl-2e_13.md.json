["```\nminikube Kubernetes cluster:\n```", "```\n$ minikube start \\\n  --driver=virtualbox \\\n  --nodes 3 \\\n  --cni calico \\\n  --cpus=2 \\\n  --memory=2g \\\n  --kubernetes-version=v1.31.0 \\\n  --container-runtime=containerd \n```", "```\n$ kubectl get nodes\nNAME           STATUS   ROLES           AGE     VERSION\nminikube       Ready    control-plane   3m28s   v1.31.0\nminikube-m02   Ready    <none>          2m29s   v1.31.0\nminikube-m03   Ready    <none>          91s     v1.31.0 \n```", "```\n$ kubectl get daemonsets -A\nNAMESPACE     NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\nkube-system   calico-node   3         3         3       3            3           kubernetes.io/os=linux   63m\nkube-system   kube-proxy    3         3         3       3            3           kubernetes.io/os=linux   63m \n-A or --all namespaces lists the requested objects across all namespaces.\n```", "```\n$ kubectl get po -n kube-system -o wide|grep calico\ncalico-kube-controllers-ddf655445-jx26x   1/1     Running   0          82m   10.244.120.65    minikube       <none>           <none>\ncalico-node-fkjxb                         1/1     Running   0          82m   192.168.59.126   minikube       <none>           <none>\ncalico-node-nrzpb                         1/1     Running   0          81m   192.168.59.128   minikube-m03   <none>           <none>\ncalico-node-sg66x                         1/1     Running   0          82m   192.168.59.127   minikube-m02   <none>           <none> \n```", "```\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: logging \n```", "```\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd-elasticsearch\n  namespace: kube-system\n  labels:\n    k8s-app: fluentd-logging\nspec:\n  selector:\n    matchLabels:\n      name: fluentd-elasticsearch\n# (to be continued in the next paragraph) \n```", "```\n# (continued)\n  template:\n    metadata:\n      labels:\n        name: fluentd-elasticsearch\n    spec:\n      containers:\n        - name: fluentd-elasticsearch\n          image: quay.io/fluentd_elasticsearch/fluentd:v4.7\n          resources:\n            limits:\n              memory: 200Mi\n            requests:\n              cpu: 100m\n              memory: 200Mi\n          volumeMounts:\n            - name: varlog\n              mountPath: /var/log\n      terminationGracePeriodSeconds: 30\n      volumes:\n        - name: varlog\n          hostPath:\n            path: /var/log \n```", "```\n env:\n            - name:  FLUENT_ELASTICSEARCH_HOST\n              value: \"elasticsearch-logging\"\n            - name:  FLUENT_ELASTICSEARCH_PORT\n              value: \"9200\" \n```", "```\n    $ kubectl apply -f fluentd-daemonset.yaml\n    namespace/logging created \n    daemonset.apps/fluentd-elasticsearch created \n    ```", "```\n    $ kubectl describe daemonset fluentd-elasticsearch -n logging \n    ```", "```\n    $ kubectl get po -n logging -o wide\n    NAME                          READY   STATUS    RESTARTS   AGE     IP               NODE           NOMINATED NODE   READINESS GATES\n    fluentd-elasticsearch-cs4hm   1/1     Running   0          3m48s   10.244.120.68    minikube       <none>           <none>\n    fluentd-elasticsearch-stfqs   1/1     Running   0          3m48s   10.244.205.194   minikube-m02   <none>           <none>\n    fluentd-elasticsearch-zk6pt   1/1     Running   0          3m48s   10.244.151.2     minikube-m03   <none>           <none> \n    ```", "```\n$ kubectl exec -n logging -it fluentd-elasticsearch-cs4hm -- /bin/bash\nroot@fluentd-elasticsearch-cs4hm:/# ls -l /var/log/\ntotal 20\ndrwxr-xr-x  3 root root 4096 May 29 10:56 calico\ndrwxr-xr-x  2 root root 4096 May 29 12:40 containers\ndrwx------  3 root root 4096 May 29 10:55 crio\ndrwxr-xr-x  2 root root 4096 May 29 11:53 journal\ndrwxr-x--- 12 root root 4096 May 29 12:40 pods\nroot@fluentd-elasticsearch-cs4hm:/# \n```", "```\n# priorityclass.yaml\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: fluentd-priority\nvalue: 100000  # A high value for criticality\nglobalDefault: false  # Not a default class for all Pods\ndescription: \"Fluentd Daemonset priority class\" \n```", "```\nspec:\n  template:\n    spec:\n      priorityClassName: fluentd-priority \n```", "```\n    ...\n          containers:\n            - name: fluentd-elasticsearch\n              **image: quay.io/fluentd_elasticsearch/fluentd:v4.7.5**\n    ... \n    ```", "```\n    $ kubectl apply -f fluentd-daemonset.yaml\n    namespace/logging unchanged\n    daemonset.apps/fluentd-elasticsearch configured \n    ```", "```\n    $ kubectl rollout status ds -n logging\n    Waiting for daemon set \"fluentd-elasticsearch\" rollout to finish: 2 out of 3 new pods have been updated...\n    Waiting for daemon set \"fluentd-elasticsearch\" rollout to finish: 2 out of 3 new pods have been updated...\n    Waiting for daemon set \"fluentd-elasticsearch\" rollout to finish: 2 of 3 updated pods are available...\n    daemon set \"fluentd-elasticsearch\" successfully rolled out \n    ```", "```\n    $ kubectl describe ds -n logging\n    Name:           fluentd-elasticsearch\n    Selector:       name=fluentd-elasticsearch\n    ...<removed for brevity>...\n    Events:\n      Type    Reason            Age    From                  Message\n      ----    ------            ----   ----                  -------\n    ...<removed for brevity>...\n    -elasticsearch-24v2z\n      Normal  SuccessfulDelete  5m52s  daemonset-controller  Deleted pod: fluentd-elasticsearch-zk6pt\n      Normal  SuccessfulCreate  5m51s  daemonset-controller  Created pod: fluentd-elasticsearch-fxffp \n    ```", "```\n$ kubectl rollout undo daemonset fluentd-elasticsearch -n logging \n```", "```\n$ kubectl delete ds fluentd-elasticsearch -n logging \n```", "```\n$ kubectl delete ds fluentd-elasticsearch -n logging --cascade=orphan \n```"]