<html><head></head><body>
		<div id="_idContainer130">
			<h1 id="_idParaDest-179" class="chapter-number"><a id="_idTextAnchor178"/>12</h1>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor179"/>Summarizing What We Have Learned and the Next Steps</h1>
			<p>Throughout the book, you learned about and practiced various concepts of Service Mesh and how to apply them using Istio. It is strongly recommended that you practice the hands-on examples in each chapter. Don’t just limit yourself to the scenarios presented in this book but rather explore, tweak, and extend the examples and apply them to real-world problems you are facing in <span class="No-Break">your organizations.</span></p>
			<p>In this chapter, we will revise the concepts discussed in this book by implementing Istio for an Online Boutique application. It will be a good idea to look at scenarios presented in this chapter and try to implement them yourself before looking at code examples. I hope reading this last chapter provides you with more confidence in using Istio. We will go through the following topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Enforcing best practices using <span class="No-Break">OPA Gatekeeper</span></li>
				<li>Applying the learnings of this book to a sample Online <span class="No-Break">Boutique application</span></li>
				<li>Istio roadmap, vision, and documentation, and how to engage with <span class="No-Break">the community</span></li>
				<li>Certification, learning resources, and various pathways <span class="No-Break">to learning</span></li>
				<li>The Extended Berkeley <span class="No-Break">Packet Filter</span></li>
			</ul>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor180"/>Technical requirements</h1>
			<p>The technical requirements in this chapter are similar to <span class="No-Break"><em class="italic">Chapter 4</em></span>. We will be using AWS EKS to deploy a website for an online boutique store, which is an open source application available under Apache License 2.0 <span class="No-Break">at </span><a href="https://github.com/GoogleCloudPlatform/microservices-demo"><span class="No-Break">https://github.com/GoogleCloudPlatform/microservices-demo</span></a><span class="No-Break">.</span></p>
			<p>Please check <span class="No-Break"><em class="italic">Chapter 4</em></span>’s <em class="italic">Technical requirements</em> section to set up the infrastructure in AWS using Terraform, set up kubectl, and install Istio including observability add-ons. To deploy the Online Boutique store application, please use the deployment artifacts in the <strong class="source-inline">Chapter12/online-boutique-orig</strong> file <span class="No-Break">on GitHub.</span></p>
			<p>You can deploy the Online Boutique store application using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter12/online-boutique-orig/00-online-boutique-shop-ns.yaml
namespace/online-boutique created
$ kubectl apply -f Chapter12/online-boutique-orig</pre>
			<p>The last command should deploy the Online Boutique application. After some time, you should be able to see all the <span class="No-Break">Pods running:</span></p>
			<pre class="console">
$ kubectl get po -n online-boutique
NAME                        READY   STATUS    RESTARTS   AGE
adservice-8587b48c5f-v7nzq               1/1     Running   0          48s
cartservice-5c65c67f5d-ghpq2             1/1     Running   0          60s
checkoutservice-54c9f7f49f-9qgv5         1/1     Running   0          73s
currencyservice-5877b8dbcc-jtgcg         1/1     Running   0          57s
emailservice-5c5448b7bc-kpgsh            1/1     Running   0          76s
frontend-67f6fdc769-r8c5n                1/1     Running   0          68s
paymentservice-7bc7f76c67-r7njd          1/1     Running   0          65s
productcatalogservice-67fff7c687-jrwcp   1/1     Running   0          62s
recommendationservice-b49f757f-9b78s     1/1     Running   0          70s
redis-cart-58648d854-jc2nv               1/1     Running   0          51s
shippingservice-76b9bc7465-qwnvz         1/1     Running   0          55s</pre>
			<p>The name of the workloads also reflects their role in the Online Boutique application, but you can find more about this freely available open source application <span class="No-Break">at </span><a href="https://github.com/GoogleCloudPlatform/microservices-demo"><span class="No-Break">https://github.com/GoogleCloudPlatform/microservices-demo</span></a><span class="No-Break">.</span></p>
			<p>For now, you can access the application via the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl port-forward svc/frontend 8080:80 -n online-boutique
Forwarding from 127.0.0.1:8080 -&gt; 8079
Forwarding from [::1]:8080 -&gt; 8079</pre>
			<p>You can then open it on the browser using <strong class="source-inline">http://localhost:8080</strong>. You should see something like <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/Figure_12.01_B17989.jpg" alt="Figure 12.1 – Online Boutique application by Google"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Online Boutique application by Google</p>
			<p>This completes the technical setup required for code examples in this chapter. Let’s get into the main topics of the chapter. We will begin with setting up the OPA Gatekeeper to enforce Istio deployment <span class="No-Break">best practices.</span></p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor181"/>Enforcing workload deployment best practices using OPA Gatekeeper</h1>
			<p>In this section, we <a id="_idIndexMarker983"/>will deploy<a id="_idIndexMarker984"/> OPA Gatekeeper using our knowledge from <span class="No-Break"><em class="italic">Chapter 11</em></span>. We will then configure OPA policies to enforce that every deployment has <strong class="source-inline">app</strong> and <strong class="source-inline">version</strong> as labels, and all port names have protocol names as <span class="No-Break">a prefix:</span></p>
			<ol>
				<li>Install OPA Gatekeeper. Deploy it by following the instructions in <span class="No-Break"><em class="italic">Chapter 11</em></span>, in the <em class="italic">Automating best practices using OPA </em><span class="No-Break"><em class="italic">Gatekeeper</em></span><span class="No-Break"> section:</span><pre class="console">
<strong class="bold">% kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml</strong></pre></li>
				<li>After deploying OPA Gatekeeper, you need to configure it to sync namespaces, Pods, services and Istio CRD gateways, virtual services, destination rules, and policy and service role bindings into its cache. We will make use of the configuration file we created in <span class="No-Break"><em class="italic">Chapter 11</em></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">$ kubectl apply -f Chapter11/05-GatekeeperConfig.yaml</strong>
<strong class="bold">config.config.gatekeeper.sh/config created</strong></pre></li>
				<li>Configure OPA Gatekeeper to apply the constraints. In <span class="No-Break"><em class="italic">Chapter 11</em></span>, we configured constraints to enforce that Pods should have <strong class="source-inline">app</strong> and <strong class="source-inline">version</strong> numbers as labels (defined in <strong class="source-inline">Chapter11/gatekeeper/01-istiopodlabelconstraint_template.yaml</strong> and <strong class="source-inline">Chapter11/gatekeeper/01-istiopodlabelconstraint.yaml</strong>), and all port names should have a protocol name as a prefix (defined in <strong class="source-inline">Chapter11/gatekeeper/02-istioportconstraints_template.yaml</strong> and <strong class="source-inline">Chapter11/gatekeeper/02-istioportconstraints.yaml</strong>). Apply the constraints using the <span class="No-Break">following commands:</span><pre class="console">
<strong class="bold">$ kubectl apply -f Chapter11/gatekeeper/01-istiopodlabelconstraint_template.yaml</strong>
<strong class="bold">constrainttemplate.templates.gatekeeper.sh/istiorequiredlabels created</strong>
<strong class="bold">$ kubectl apply -f Chapter11/gatekeeper/01-istiopodlabelconstraint.yaml</strong>
<strong class="bold">istiorequiredlabels.constraints.gatekeeper.sh/mesh-pods-must-have-app-and-version created</strong>
<strong class="bold">$ kubectl apply -f Chapter11/gatekeeper/02-istioportconstraints_template.yaml</strong>
<strong class="bold">constrainttemplate.templates.gatekeeper.sh/allowedistioserviceportname created</strong>
<strong class="bold">$ kubectl apply -f Chapter11/gatekeeper/02-istioportconstraints.yaml</strong>
<strong class="bold">allowedistioserviceportname.constraints.gatekeeper.sh/port-name-constraint created</strong></pre></li>
			</ol>
			<p>This <a id="_idIndexMarker985"/>completes the <a id="_idIndexMarker986"/>deployment and configuration of OPA Gatekeeper. You should extend the constraints with anything else you might like to be included to ensure good hygiene of deployment descriptors of <span class="No-Break">the workloads.</span></p>
			<p>In the next section, we will redeploy the Online Boutique application and enable istio sidecar injection and then discover the configurations that are in violation of OPA constraints and resolve them one <span class="No-Break">by one.</span></p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor182"/>Applying our learnings to a sample application</h1>
			<p>In this section, we will apply the learnings of the book – specifically, the knowledge from <em class="italic">Chapters 4</em> to <em class="italic">6</em> – to our Online Boutique application. Let’s dive <span class="No-Break">right in!</span></p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor183"/>Enabling Service Mesh for the sample application</h2>
			<p>Now<a id="_idIndexMarker987"/> that OPA Gatekeeper is in place with all the constraints we want it to enforce on deployments, it’s time to deploy a sample application. We will first start with un-deploying the <strong class="source-inline">online-boutique</strong> application and redeploying with istio-injection enabled at the <span class="No-Break">namespace level.</span></p>
			<p>Undeploy the Online Boutique application by deleting the <span class="No-Break"><strong class="source-inline">online-boutique</strong></span><span class="No-Break"> namespace:</span></p>
			<pre class="console">
% kubectl delete ns online-boutique
namespace " online-boutique " deleted</pre>
			<p>Once undeployed, let’s modify the namespace and add an <strong class="source-inline">istio-injection:enabled</strong> label and redeploy the application. The updated namespace configuration will be <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
apiVersion: v1
kind: Namespace
metadata:
  name: online-boutique
  labels:
    <strong class="bold">istio-injection: enabled</strong></pre>
			<p>The sample file is available at <strong class="source-inline">Chapter12/OPAGatekeeper/automaticsidecarinjection/00-online-boutique-shop-ns.yaml</strong> <span class="No-Break">on GitHub.</span></p>
			<p>With automatic sidecar injection enabled, let’s try to deploy the application using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter12/OPAGatekeeper/automaticsidecarinjection
namespace/online-boutiquecreated
$ kubectl apply -f Chapter12/OPAGatekeeper/automaticsidecarinjection
Error from server (Forbidden): error when creating "Chapter12/OPAGatekeeper/automaticsidecarinjection/02-carts-svc.yml": admission webhook "validation.gatekeeper.sh" denied the request: [port-name-constraint] All services declaration must have port name with one of following  prefix http-, http2-,https-,grpc-,grpc-web-,mongo-,redis-,mysql-,tcp-,tls-</pre>
			<p>There will <a id="_idIndexMarker988"/>be errors caused by constraint violations imposed by OPA Gatekeeper. The output in the preceding example is truncated to avoid repetitions but from the output in your terminal, you must notice that all deployments are in violation and hence no resource is deployed to the <span class="No-Break">online-boutique namespace.</span></p>
			<p>Try to fix the constraint violation by applying the correct labels and naming ports correctly as suggested by Istio <span class="No-Break">best practices.</span></p>
			<p>You need to apply <strong class="source-inline">app</strong> and <strong class="source-inline">version</strong> labels to all deployments. The following is an example for a <span class="No-Break"><strong class="source-inline">frontend</strong></span><span class="No-Break"> deployment:</span></p>
			<pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: online-boutique
spec:
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
        version: v1</pre>
			<p>Similarly, you need to add <strong class="source-inline">name</strong> to all port definitions in the service declaration. The following is an example of a <span class="No-Break"><strong class="source-inline">carts</strong></span><span class="No-Break"> service:</span></p>
			<pre class="source-code">
apiVersion: v1
kind: Service
metadata:
  name: frontend
  namespace: online-boutique
spec:
  type: ClusterIP
  selector:
    app: frontend
  ports:
  - <strong class="bold">name</strong>: http-frontend
    port: 80
    targetPort: 8080</pre>
			<p>For your <a id="_idIndexMarker989"/>convenience, the updated files are available in <strong class="source-inline">Chapter12/OPAGatekeeper/automaticsidecarinjection</strong>. Deploy the Online Boutique application using the <span class="No-Break">following command:</span></p>
			<pre class="console">
% kubectl apply -f Chapter12/OPAGatekeeper/automaticsidecarinjection</pre>
			<p>With that, we have practiced the deployment of the Online Boutique application in the Service Mesh. You should have the Online Boutique application along with automatic sidecar injection deployed in your cluster. The Online Boutique application is part of the Service Mesh but not yet completely ready for it. In the next section, we will apply the learning from <span class="No-Break"><em class="italic">Chapter 5</em></span> on managing <span class="No-Break">application traffic.</span></p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor184"/>Configuring Istio to manage application traffic</h2>
			<p>In this section, using the <a id="_idIndexMarker990"/>learnings from <span class="No-Break"><em class="italic">Chapter 4</em></span>, we will configure the Service Mesh to manage application traffic for the Online Boutique application. We will first start with configuring the Istio Ingress gateway to allow traffic inside <span class="No-Break">the mesh.</span></p>
			<h3>Configuring Istio Ingress Gateway</h3>
			<p>In <span class="No-Break"><em class="italic">Chapter 4</em></span>, we <a id="_idIndexMarker991"/>read that a gateway is like a load balancer on the edge of the mesh that accepts incoming traffic that is then routed to <span class="No-Break">underlying workloads.</span></p>
			<p>In the following source code block, we have defined the <span class="No-Break">gateway configuration:</span></p>
			<pre class="source-code">
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: online-boutique-ingress-gateway
  namespace: online-boutique
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "onlineboutique.com"</pre>
			<p>The file is also available in <strong class="source-inline">Chapter12/trafficmanagement/01-gateway.yaml</strong> on GitHub. Apply the configuration using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter12/trafficmanagement/01-gateway.yaml
gateway.networking.istio.io/online-boutique-ingress-gateway created</pre>
			<p>Next, we<a id="_idIndexMarker992"/> need to configure <strong class="source-inline">VirtualService</strong> to route traffic for the <strong class="source-inline">onlineboutique.com</strong> host to the corresponding <span class="No-Break"><strong class="source-inline">frontend</strong></span><span class="No-Break"> service.</span></p>
			<h3>Configuring VirtualService</h3>
			<p><strong class="source-inline">VirtualService</strong> is <a id="_idIndexMarker993"/>used to define route rules for every host as specified in the gateway configuration. <strong class="source-inline">VirtualService</strong> is associated with the gateway and the hostname is managed by that gateway. In <strong class="source-inline">VirtualService</strong>, you can define rules on how a traffic/route can be matched and, if matched, then where it should be <span class="No-Break">routed to.</span></p>
			<p>The following source code block defines <strong class="source-inline">VirtualService</strong> that matches any traffic handled by <strong class="source-inline">online-boutique-ingress-gateway</strong> with a hostname of <strong class="source-inline">onlineboutique.com</strong>. If matched, the traffic is routed to subset <strong class="source-inline">v1</strong> of the destination service <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">frontend</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
apiVersion: networking.istio.io/v1alpha3
kind: <strong class="bold">VirtualService</strong>
metadata:
  name: onlineboutique-frontend-vs
  namespace: online-boutique
spec:
  hosts:
  - <strong class="bold">"onlineboutique.com"</strong>
  gateways:
  - <strong class="bold">online-boutique-ingress-gateway</strong>
  http:
  - route:
    - destination:
        host: frontend
        subset: v1</pre>
			<p>The <a id="_idIndexMarker994"/>configuration is available in <strong class="source-inline">Chapter12/trafficmanagement/02-virtualservice-frontend.yaml</strong> <span class="No-Break">on GitHub.</span></p>
			<p>Next, we will configure <strong class="source-inline">DestinationRule</strong>, which defines how the request will be handled by <span class="No-Break">the destination.</span></p>
			<h3>Configuring DestinationRule</h3>
			<p>Though<a id="_idIndexMarker995"/> they might appear unnecessary, when you have more than one version of the workload, then <strong class="source-inline">DestinationRule</strong> is used for defining traffic policies such as a load balancing policy, connection pool policy, outlier detection policy, and so on. The following code block configures <strong class="source-inline">DestinationRule</strong> for the <span class="No-Break"><strong class="source-inline">frontend</strong></span><span class="No-Break"> service:</span></p>
			<pre class="source-code">
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: frontend
  namespace: online-boutique
spec:
  host: frontend
  subsets:
  - name: v1
    labels:
      app: frontend</pre>
			<p>The configuration is available along with the <strong class="source-inline">VirtualService</strong> configuration in <strong class="source-inline">Chapter12/trafficmanagement/02-virtualservice-frontend.yaml </strong><span class="No-Break">on GitHub.</span></p>
			<p>Next, let’s<a id="_idIndexMarker996"/> create <strong class="source-inline">VirtualService</strong> and <strong class="source-inline">DestinationRule</strong> by using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter12/trafficmanagement/02-virtualservice-frontend.yaml
virtualservice.networking.istio.io/onlineboutique-frontend-vs created
destinationrule.networking.istio.io/frontend created</pre>
			<p>You should now be able to access the Online Boutique store site from the web browser. You need to find the public IP of the AWS load balancer exposing the Ingress gateway service – do not forget to add a <strong class="bold">Host</strong> header using<a id="_idIndexMarker997"/> the <strong class="bold">ModHeader</strong> extension to Chrome, as discussed in <span class="No-Break"><em class="italic">Chapter 4</em></span> and as seen in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/Figure_12.02_B17989.jpg" alt="Figure 12.2 – ModHeader extension with Host header"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – ModHeader extension with Host header</p>
			<p>Once the correct Host header is added, you can access the Online Boutique from Chrome using the AWS load balancer <span class="No-Break">public DNS:</span></p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/Figure_12.03_B17989.jpg" alt="Figure 12.3 – Online Boutique landing page"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – Online Boutique landing page</p>
			<p>So far, we<a id="_idIndexMarker998"/> have created only one virtual service to route traffic from the Ingress gateway to the <strong class="source-inline">frontend</strong> service in the mesh. By default, Istio will send traffic to all respective microservices in the mesh, but as we discussed In the previous chapter, the best practice is to define routes via <strong class="source-inline">VirtualService</strong> and how the request should be routed via destination rules. Following the best practice, we need to define <strong class="source-inline">VirtualService</strong> and <strong class="source-inline">DestinationRule</strong> for the remaining microservices. Having <strong class="source-inline">VirtualService</strong> and <strong class="source-inline">DestinationRule</strong> helps you manage traffic when there is more than one version of <span class="No-Break">underlying workloads.</span></p>
			<p>For your convenience, <strong class="source-inline">VirtualService</strong> and <strong class="source-inline">DestinationRule</strong> are already defined in the <strong class="source-inline">Chapter12/trafficmanagement/03-virtualservicesanddr-otherservices.yaml</strong> file on GitHub. You can apply the configuration using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter12/trafficmanagement/03-virtualservicesanddr-otherservices.yaml</pre>
			<p>After applying the configuration and generating some traffic, check out the <span class="No-Break">Kiali dashboard:</span></p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/Figure_12.04_B17989.jpg" alt="Figure 12.4 – Versioned app graph for the Online Boutique shop"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Versioned app graph for the Online Boutique shop</p>
			<p>In the<a id="_idIndexMarker999"/> Kiali dashboard, you can observe the Ingress gateway, all virtual services, and <span class="No-Break">underlying workloads.</span></p>
			<h3>Configuring access to external services</h3>
			<p>Next, we <a id="_idIndexMarker1000"/>quickly revise concepts on routing traffic to destinations outside the mesh. In <span class="No-Break"><em class="italic">Chapter 4</em></span>, we learned about <strong class="source-inline">ServiceEntry</strong>, which enables us to add additional entries to Istio’s internal service registry so that service in the mesh can route traffic to these endpoints that are not part of the Istio service registry. The following is an example of <strong class="source-inline">ServiceRegistry</strong> adding <strong class="source-inline">xyz.com</strong> to the Istio <span class="No-Break">service registry:</span></p>
			<pre class="source-code">
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: allow-egress-to-xyv.com
spec:
  hosts:
  - "xyz.com"
  ports:
  - number: 80
    protocol: HTTP
    name: http
  - number: 443
    protocol: HTTPS
    name: https</pre>
			<p>This <a id="_idIndexMarker1001"/>concludes the section on managing application traffic, in which we exposed <strong class="source-inline">onlineboutique.com</strong> via Istio Ingress Gateway and defined <strong class="source-inline">VirtualService</strong> and <strong class="source-inline">DestinationRule</strong> for routing and handling traffic in <span class="No-Break">the mesh.</span></p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor185"/>Configuring Istio to manage application resiliency</h2>
			<p>Istio <a id="_idIndexMarker1002"/>provides various capabilities to manage application resiliency, and we discussed them in great detail in <span class="No-Break"><em class="italic">Chapter 5</em></span>. We will apply some of the concepts from that chapter to the Online <span class="No-Break">Boutique application.</span></p>
			<p>Let’s start with timeouts <span class="No-Break">and retries!</span></p>
			<h3>Configuring timeouts and retries</h3>
			<p>Let’s assume<a id="_idIndexMarker1003"/> that the email service suffers from intermittent failures, and it is prudent to timeout after 5 seconds if a response is not received from the email service, and then retry sending the email a few times rather than aborting it. We will configure retries and timeout for the email service to revise application <span class="No-Break">resiliency concepts.</span></p>
			<p>Istio provides a provision to configure timeouts, which is the amount of time that an Istio-proxy sidecar should wait for replies from a given service. In the following configuration, we <a id="_idIndexMarker1004"/>have applied a timeout of 5 seconds for the <span class="No-Break">email service:</span></p>
			<pre class="source-code">
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  namespace: online-boutique
  name: emailvirtualservice
spec:
  hosts:
  - emailservice
  http:
  - <strong class="bold">timeout: 5s</strong>
    route:
    - destination:
        host: emailservice
        subset: v1</pre>
			<p>Istio also provides provision for automated retries that are implemented as part of the <strong class="source-inline">VirtualService</strong> configuration. In the following source code block, we have configured Istio to retry the request to the email service twice, with each retry to timeout after 2 seconds and a retry to happen only if <strong class="source-inline">5xx,gateway-error,reset,connect-failure,refused-stream,retriable-4xx</strong> errors are returned <span class="No-Break">from downstream:</span></p>
			<pre class="source-code">
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  namespace: online-boutique
  name: emailvirtualservice
spec:
  hosts:
  - emailservice
  http:
  - timeout: 5s
    route:
    - destination:
        host: emailservice
        subset: v1
    retries:
      <strong class="bold">attempts: 2</strong>
<strong class="bold">      perTryTimeout: 2s</strong>
<strong class="bold">      retryOn: 5xx,gateway-error,reset,connect-failure,refused-stream,retriable-4xx</strong></pre>
			<p>We <a id="_idIndexMarker1005"/>have configured <strong class="source-inline">timeout</strong> and <strong class="source-inline">retries</strong> via the <strong class="source-inline">VirtualService</strong> configuration. With the assumption that the email service is fragile and suffers interim failure, let’s try to alleviate this issue by mitigating any potential issue caused by a traffic surge <span class="No-Break">or spike.</span></p>
			<h3>Configuring rate limiting</h3>
			<p>Istio<a id="_idIndexMarker1006"/> provides controls to handle a surge of traffic from consumers, as well as to control the traffic to match consumers’ capability to handle <span class="No-Break">the traffic.</span></p>
			<p>In the following destination rule, we are defining rate-limiting controls for the email service. We have defined that the number of active requests to the email service will be <strong class="source-inline">1</strong> (as per <strong class="source-inline">http2MaxRequests</strong>), there will be only 1 request per connection (as defined in <strong class="source-inline">maxRequestsPerConnection</strong>), and there will be 0 requests queued while waiting for connection from the connection pool (as defined <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">http1MaxPendingRequests</strong></span><span class="No-Break">):</span></p>
			<pre class="source-code">
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  namespace: online-boutique
  name: emaildr
spec:
  host: emailservice
  <strong class="bold">trafficPolicy</strong>:
      connectionPool:
        http:
          <strong class="bold">http2MaxRequests: 1</strong>
          <strong class="bold">maxRequestsPerConnection: 1</strong>
          <strong class="bold">http1MaxPendingRequests: 0</strong>
  subsets:
  - name: v1
    labels:
      version: v1
      app: emailservice</pre>
			<p>Let’s <a id="_idIndexMarker1007"/>make some more assumptions and assume that there are two versions of the email service, with <strong class="source-inline">v1</strong> being more rogue than the other, <strong class="source-inline">v2</strong>. In such scenarios, we need to apply outlier detection policies to perform circuit breakers. Istio provides good control for outlier detection. The following code block describes the config you need to add to <strong class="source-inline">trafficPolicy</strong> in the corresponding destination rule for the <span class="No-Break">email service:</span></p>
			<pre class="source-code">
      outlierDetection:
        <strong class="bold">baseEjectionTime: 5m</strong>
        <strong class="bold">consecutive5xxErrors: 1</strong>
        <strong class="bold">interval: 90s</strong>
        <strong class="bold">maxEjectionPercent: 50</strong></pre>
			<p>In the<a id="_idIndexMarker1008"/> outlier detection, we have defined <strong class="source-inline">baseEjectionTime</strong> with a value of <strong class="source-inline">5</strong> minutes, which is the minimum duration per ejection. It is then also multiplied by the number of times an email service is found to be unhealthy. For example, if the <strong class="source-inline">v1</strong> email service is found to be an outlier 5 times, then it will be ejected from the connection pool for <strong class="source-inline">baseEjectionTime*5</strong>. Next, we have defined <strong class="source-inline">consecutive5xxErrors</strong> with a value of <strong class="source-inline">1</strong>, which is the number of <strong class="source-inline">5x</strong> errors that need to occur to qualify the upstream to be an outlier. Then, we have defined <strong class="source-inline">interval</strong> with a value of <strong class="source-inline">90s</strong>, which is the time between the checks when Istio scans the upstream for the health status. Finally, we have defined <strong class="source-inline">maxEjectionPercent</strong> with a value of <strong class="source-inline">50%</strong>, which is the maximum number of hosts in the connection pool that can <span class="No-Break">be ejected.</span></p>
			<p>With that, we revised and applied various controls for managing application resiliency for the Online Boutique application. Istio provides various controls for managing application resiliency without needing to modify or build anything specific in your application. In the next section, we will apply the learning of <span class="No-Break"><em class="italic">Chapter 6</em></span> to our Online <span class="No-Break">Boutique application.</span></p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor186"/>Configuring Istio to manage application security</h2>
			<p>Now that <a id="_idIndexMarker1009"/>we have created Ingress via Istio Gateway, routing rules via Istio <strong class="source-inline">VirtualService</strong>, and <strong class="source-inline">DestinationRules</strong> to handle how traffic will be routed to the end destination, we can move on to the next step of securing the traffic in the mesh. The following policy enforces that all traffic in the mesh should strictly happen <span class="No-Break">over mTLS:</span></p>
			<pre class="source-code">
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: strictmtls-online-boutique
  namespace: online-boutique
spec:
  mtls:
    mode: STRICT</pre>
			<p>The configuration is available in the <strong class="source-inline">Chapter12/security/strictMTLS.yaml</strong> file on GitHub. Without this configuration, all the traffic in the mesh is happening in <em class="italic">PERMISSIVE</em> mode, which means that the traffic can happen over mTLS as well as plain text. You can validate that by deploying a <strong class="source-inline">curl</strong> Pod and making an HTTP call to any of the microservices in the mesh. But once you apply the policy, Istio will enforce <em class="italic">STRICT</em> mode, which means mTLS will be strictly enforced for all traffic. Apply the configuration using <span class="No-Break">the following:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter12/security/strictMTLS.yaml
peerauthentication.security.istio.io/strictmtls-online-boutique created</pre>
			<p>You can check in Kiali that all traffic in the mesh is happening <span class="No-Break">over mTLS:</span></p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/Figure_12.05_B17989.jpg" alt="Figure 12.5 – App graph showing mTLS communication between services"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – App graph showing mTLS communication between services</p>
			<p>Next, we <a id="_idIndexMarker1010"/>will be securing Ingress traffic using <strong class="source-inline">https</strong>. This step is important to revise but the outcome of it creates a problem in accessing the application, so we will perform the steps to revise the concepts and then revert them back so that we can continue accessing <span class="No-Break">the application.</span></p>
			<p>We will use the learning from <span class="No-Break"><em class="italic">Chapter 4</em></span>’s <em class="italic">Exposing Ingress over HTTPS</em> section. The steps are much easier if you <a id="_idIndexMarker1011"/>have a <strong class="bold">Certificate Authority</strong> (<strong class="bold">CA</strong>) and registered DNS name, but if not, simply follow these steps to create a certificate to be used for the <span class="No-Break"><strong class="source-inline">onlineboutique.com</strong></span><span class="No-Break"> domain:</span></p>
			<ol>
				<li value="1">Create a CA. Here, we are creating a CA <a id="_idIndexMarker1012"/>with a <strong class="bold">Common Name</strong> (<strong class="bold">CN</strong>) <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">onlineboutique.inc</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">$ openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -subj '/O=Online Boutique./CN=onlineboutique.inc' -keyout onlineboutique.inc.key -out onlineboutique.inc.crt</strong>
<strong class="bold">Generating a 2048 bit RSA private key</strong>
<strong class="bold">writing new private key to 'onlineboutique.inc.key'</strong></pre></li>
				<li>Generate<a id="_idIndexMarker1013"/> a <strong class="bold">Certificate Signing Request</strong> (<strong class="bold">CSR</strong>) for the Online Boutique. Here, we<a id="_idIndexMarker1014"/> are generating a CSR for <strong class="source-inline">onlineboutique.com</strong>, which also generates a <span class="No-Break">private key:</span><pre class="console">
<strong class="bold">$ openssl req -out onlineboutique.com.csr -newkey rsa:2048 -nodes -keyout onlineboutique.com.key -subj "/CN=onlineboutique.com/O=onlineboutique.inc"</strong>
<strong class="bold">Generating a 2048 bit RSA private key</strong>
<strong class="bold">...........................................................................+++</strong>
<strong class="bold">.........+++</strong>
<strong class="bold">writing new private key to 'onlineboutique.com.key'</strong></pre></li>
				<li>Sign the CSR using the CA using the <span class="No-Break">following command:</span><pre class="console">
<strong class="bold">$ openssl x509 -req -sha256 -days 365 -CA onlineboutique.inc.crt -CAkey onlineboutique.inc.key -set_serial 0 -in onlineboutique.com.csr -out onlineboutique.com.crt</strong>
<strong class="bold">Signature ok</strong>
<strong class="bold">subject=/CN= onlineboutique.com/O= onlineboutique.inc</strong></pre></li>
				<li>Load the certificate and private key as a <span class="No-Break">Kubernetes Secret:</span><pre class="console">
<strong class="bold">$ kubectl create -n istio-system secret tls onlineboutique-credential --key=onlineboutique.com.key --cert=onlineboutique.com.crt</strong>
<strong class="bold">secret/onlineboutique-credential created</strong></pre></li>
			</ol>
			<p>We <a id="_idIndexMarker1015"/>have created the certificate and stored them as Kubernetes Secret. In the next steps, we will modify the Istio Gateway configuration to expose the traffic over HTTPS using <span class="No-Break">the certificates.</span></p>
			<ol>
				<li value="5">Create the Gateway configuration as described in the <span class="No-Break">following command:</span><pre class="console">
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: online-boutique-ingress-gateway
  namespace: online-boutique
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: onlineboutique-credential
    hosts:
    - "onlineboutique.com"</pre></li>
			</ol>
			<p>Apply the <span class="No-Break">following configuration:</span></p>
			<pre class="console">
<strong class="bold">$ kubectl apply -f Chapter12/security/01-istio-gateway.yaml</strong></pre>
			<p>You can<a id="_idIndexMarker1016"/> access and check the certificate using the following commands. Please note that the output is truncated to highlight relevant <span class="No-Break">sections only:</span></p>
			<pre class="console">
$ curl -v -HHost:onlineboutique.com --connect-to "onlineboutique.com:443:aced3fea1ffaa468fa0f2ea6fbd3f612-390497785.us-east-1.elb.amazonaws.com" --cacert onlineboutique.inc.crt --head  https://onlineboutique.com:443/
..
* Connected to aced3fea1ffaa468fa0f2ea6fbd3f612-390497785.us-east-1.elb.amazonaws.com (52.207.198.166) port 443 (#0)
--
* Server certificate:
*  subject: CN=onlineboutique.com; O=onlineboutique.inc
*  start date: Feb 14 23:21:40 2023 GMT
*  expire date: Feb 14 23:21:40 2024 GMT
*  common name: onlineboutique.com (matched)
*  issuer: O=Online Boutique.; CN=onlineboutique.inc
*  SSL certificate verify ok.
..</pre>
			<p>The configuration will secure the Ingress traffic to the <strong class="source-inline">online-boutique</strong> store, but it also means that you will not be able to access it from the browser because of a mismatch of the FQDN being used in the browser and the CN configured in the certificates. You can alternatively register DNS names against the AWS load balancer but for now, you might find it easier to remove the HTTPS configuration and revert to using the <strong class="source-inline">Chapter12/trafficmanagement/01-gateway.yaml</strong> file <span class="No-Break">on GitHub.</span></p>
			<p>Let’s dive<a id="_idIndexMarker1017"/> deeper into security and perform <strong class="source-inline">RequestAuthentication</strong> and authorization for the Online Boutique store. In <span class="No-Break"><em class="italic">Chapter 6</em></span>, we did an elaborate exercise of building authentication and authorization using Auth0. Along the same lines, we will be building an authentication and authorization policy for the <strong class="source-inline">frontend</strong> service but this time, we will use a dummy JWKS endpoint, which is shipped in <span class="No-Break">with Istio.</span></p>
			<p>We will start with creating a <strong class="source-inline">RequestAuthentication</strong> policy to define the authentication method supported by the <span class="No-Break"><strong class="source-inline">frontend</strong></span><span class="No-Break"> service:</span></p>
			<pre class="source-code">
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
 name: frontend
 namespace: online-boutique
spec:
  selector:
    matchLabels:
      app: frontend
  jwtRules:
  - issuer: "testing@secure.istio.io"
    jwksUri: "https://raw.githubusercontent.com/istio/istio/release-1.17/security/tools/jwt/samples/jwks.json"</pre>
			<p>We are making use of dummy <strong class="source-inline">jwksUri</strong>, which comes along with Istio for testing purposes. Apply the <strong class="source-inline">RequestAuthentication</strong> policy using <span class="No-Break">the following:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter12/security/requestAuthentication.yaml
requestauthentication.security.istio.io/frontend created</pre>
			<p>After applying the <strong class="source-inline">RequestAuthentication</strong> policy, you can test that by providing a dummy token to the <span class="No-Break"><strong class="source-inline">frontend</strong></span><span class="No-Break"> service:</span></p>
			<ol>
				<li value="1">Fetch<a id="_idIndexMarker1018"/> the dummy token and set it as an environment variable to be used in <span class="No-Break">requests later:</span><pre class="console">
TOKEN=$(curl -k <strong class="bold">https://raw.githubusercontent.com/istio/istio/release-1.17/security/tools/jwt/samples/demo.jwt -s); echo $TOKEN</strong>
<strong class="bold">eyJhbGciOiJSUzI1NiIsImtpZCI6IkRIRmJwb0lVcXJZOHQyen BBMnFYZkNtcjVWTzVaRXI0UnpIVV8tZW52dlEiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjQ2ODU5ODk3MDAsImZvbyI6ImJhciIsImlhdCI6MTUzMjM4OTcwMCwiaXNzIjoidGVzdGluZ0BzZWN1cmUuaXN0aW8uaW8iLCJzdWIiOiJ0ZXN0aW5nQHNlY3VyZS5pc3Rpby5pbyJ9. CfNnxWP2tcnR9q0vxyxweaF3ovQYHYZl82hAUsn21bwQd9zP7c-LS9qd_vpdLG4Tn1A15NxfCjp5f7QNBUo-KC9PJqYpgGbaXhaGx7bEdFWjcwv3nZzvc7M__ZpaCERdwU7igUmJqYGBYQ51vr2njU9ZimyKkfDe3axcyiBZde7G6dabliUosJvvKOPcKIWPccCgefSj_GNfwIip3-SsFdlR7BtbVUcqR-yv-XOxJ3Uc1MI0tz3uMiiZcyPV7sNCU4KRnemRIMHVOfuvHsU60_GhGbiSFzgPTAa9WTltbnarTbxudb_YEOx12JiwYToeX0DCPb43W1tzIBxgm8NxUg</strong></pre></li>
				<li>Test <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">$ curl -HHost:onlineboutique.com http://aced3fea1ffaa468fa0f2ea6fbd3f612-390497785.us-east-1.elb.amazonaws.com/ -o /dev/null --header "Authorization: Bearer $TOKEN" -s -w '%{http_code}\n'</strong>
<strong class="bold">200</strong></pre></li>
			</ol>
			<p>Notice that you received a <span class="No-Break"><strong class="source-inline">200</strong></span><span class="No-Break"> response.</span></p>
			<ol>
				<li value="3">Now try testing with an <span class="No-Break">invalid token:</span><pre class="console">
<strong class="bold">$ curl -HHost:onlineboutique.com http://aced3fea1ffaa468fa0f2ea6fbd3f612-390497785.us-east-1.elb.amazonaws.com/ -o /dev/null --header "Authorization: Bearer BLABLAHTOKEN" -s -w '%{http_code}\n'</strong>
<strong class="bold">401</strong></pre></li>
			</ol>
			<p>The <strong class="source-inline">RequestAuthentication</strong> policy plunged into action and denied <span class="No-Break">the request.</span></p>
			<ol>
				<li value="4">Test without <span class="No-Break">any token:</span><pre class="console">
<strong class="bold">% curl -HHost:onlineboutique.com http://aced3fea1ffaa468fa0f2ea6fbd3f612-390497785.us-east-1.elb.amazonaws.com/ -o /dev/null  -s -w '%{http_code}\n'</strong>
<strong class="bold">200</strong></pre></li>
			</ol>
			<p>The<a id="_idIndexMarker1019"/> outcome of the request is not desired but is expected because the <strong class="source-inline">RequestAuthentication</strong> policy is only responsible for validating a token if a token is passed. If there is no <strong class="source-inline">Authorization</strong> header in the request, then the <strong class="source-inline">RequestAuthentication</strong> policy will not be invoked. We can solve this problem using <strong class="source-inline">AuthorizationPolicy</strong>, which enforces an access control policy for workloads in <span class="No-Break">the mesh.</span></p>
			<p>Let’s build <strong class="source-inline">AuthorizationPolicy</strong>, which enforces that a principal must be present in <span class="No-Break">the request:</span></p>
			<pre class="console">
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: require-jwt
  namespace: online-boutique
spec:
  selector:
    matchLabels:
      app: frontend
  action: ALLOW
  rules:
  - from:
    - source:
       requestPrincipals: ["testing@secure.istio.io/testing@secure.istio.io"]</pre>
			<p>The configuration is available in the <strong class="source-inline">Chapter12/security/requestAuthorizationPolicy.yaml</strong> file in GitHub. Apply <a id="_idIndexMarker1020"/>the configuration using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter12/security/requestAuthorizationPolicy.yaml
authorizationpolicy.security.istio.io/frontend created</pre>
			<p>After applying the configuration test using <em class="italic">Steps 1</em> to <em class="italic">4</em>, which we performed after applying the <strong class="source-inline">RequestAuthentication</strong> policy, you will notice that all steps work as expected, but for <em class="italic">Step 4</em>, we are getting <span class="No-Break">the following:</span></p>
			<pre class="console">
$ curl -HHost:onlineboutique.com http://aced3fea1ffaa468fa0f2ea6fbd3f612-390497785.us-east-1.elb.amazonaws.com/ -o /dev/null  -s -w '%{http_code}\n'
403</pre>
			<p>That is because the authorization policy enforces the required presence of a JWT with the <strong class="source-inline">["</strong><span class="No-Break"><strong class="source-inline">testing@secure.istio.io/testing@secure.istio.io"]</strong></span><span class="No-Break"> principal.</span></p>
			<p>This concludes the security configuration for our Online Boutique application. In the next section, we will read about various resources that will help you become an expert and certified in using and <span class="No-Break">operating Istio.</span></p>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor187"/>Certification and learning resources for Istio</h1>
			<p>The primary resource for learning Istio is the Istio <a id="_idIndexMarker1021"/>website (<a href="https://istio.io/latest/">https://istio.io/latest/</a>). There is elaborative documentation on performing basic to multi-cluster setups. There are resources for beginners and advanced users, and various exercises on performing traffic management, security, observability, extensibility, and policy enforcement. Outside of the Istio documentation, the other organization providing lots of supportive content on Istio is <a id="_idIndexMarker1022"/>Tetrate (<a href="https://tetrate.io/">https://tetrate.io/</a>), which also provides<a id="_idIndexMarker1023"/> labs and certification courses. One such certification provided by<a id="_idIndexMarker1024"/> Tetrate Academy<a id="_idIndexMarker1025"/> is <strong class="bold">Certified Istio Administrator</strong>. Details about the course and exam are available at <a href="https://academy.tetrate.io/courses/certified-istio-administrator">https://academy.tetrate.io/courses/certified-istio-administrator</a>. Tetrate Academy <a id="_idIndexMarker1026"/>also provides a free course to learn about Istio fundamentals. You can find the details of the course at <a href="https://academy.tetrate.io/courses/istio-fundamentals">https://academy.tetrate.io/courses/istio-fundamentals</a>. Similarly, there is a course from Solo.io named <strong class="bold">Get Started with Istio</strong>; you can find details of the course at <a href="https://academy.solo.io/get-started-with-istio">https://academy.solo.io/get-started-with-istio</a>. Another good course from The Linux Foundation is named <strong class="bold">Introduction to Istio</strong>, and you can find the details of the course <span class="No-Break">at </span><a href="https://training.linuxfoundation.org/training/introduction-to-istio-lfs144x/"><span class="No-Break">https://training.linuxfoundation.org/training/introduction-to-istio-lfs144x/</span></a><span class="No-Break">.</span></p>
			<p>I personally enjoy the<a id="_idIndexMarker1027"/> learning <a id="_idIndexMarker1028"/>resources available at <a href="https://istiobyexample.dev/">https://istiobyexample.dev/</a>; the site explains various use cases of Istio (such as canary deployment, managing Ingress, managing gRPC traffic, and so on) in great detail, along with configuration examples. For any technical questions, you can always head to <a id="_idIndexMarker1029"/>StackOverflow at <a href="https://stackoverflow.com/questions/tagged/istio">https://stackoverflow.com/questions/tagged/istio</a>. There is an energetic and enthusiastic community of Istio users and builders who are discussing various topics about Istio at <a href="https://discuss.istio.io/">https://discuss.istio.io/</a>; feel free to sign up for the <span class="No-Break">discussion board.</span></p>
			<p>Tetrate Academy <a id="_idIndexMarker1030"/>also provides a free course on Envoy fundamentals; the course is very helpful to understand the fundamentals of Envoy and, in turn, the Istio data plane. You can find the details of this course at <a href="https://academy.tetrate.io/courses/envoy-fundamentals">https://academy.tetrate.io/courses/envoy-fundamentals</a>. The course is full of practical labs and quizzes that are very helpful in mastering your <span class="No-Break">Envoy skills.</span></p>
			<p>The Istio website has compiled a list of helpful resources to keep you updated with Istio and engage with the<a id="_idIndexMarker1031"/> Istio community; you can find the list at <a href="https://istio.io/latest/get-involved/">https://istio.io/latest/get-involved/</a>. The list also provides you with details on how to report bugs <span class="No-Break">and issues.</span></p>
			<p>To summarize, there are not many resources except a few books and websites, but you will find most of the answers to your <a id="_idIndexMarker1032"/>questions at <a href="https://istio.io/latest/docs/">https://istio.io/latest/docs/</a>. It is also a great idea to follow <strong class="bold">IstioCon</strong>, which is the Istio Community conference and happens on a yearly cadence. You can find a <a id="_idIndexMarker1033"/>session of IstioCon 2022 at <a href="https://events.istio.io/istiocon-2022/sessions/">https://events.istio.io/istiocon-2022/sessions/</a> and 2021 <span class="No-Break">at </span><a href="https://events.istio.io/istiocon-2021/sessions/"><span class="No-Break">https://events.istio.io/istiocon-2021/sessions/</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor188"/>Understanding eBPF</h1>
			<p>As we are at the end of this book, it is important to also look at other technologies that are relevant to Service Mesh. One such technology is the <strong class="bold">Extended Berkeley Packet Filter</strong> (<strong class="bold">eBPF</strong>). In <a id="_idIndexMarker1034"/>this section, we will read about eBPF and its role in Service <span class="No-Break">Mesh evolution.</span></p>
			<p>eBPF is a framework that allows users to run custom programs within the kernel of the operating system without needing to change kernel source code or load kernel modules. The custom programs are<a id="_idIndexMarker1035"/> called <strong class="bold">eBPF programs</strong> and are used to add additional capabilities to the operating system at runtime. The eBPF programs are safe and efficient and, like the kernel modules, they are like lightweight sandbox virtual machines run in a privileged context by the <span class="No-Break">operating system.</span></p>
			<p>eBPF programs are triggered based on events happening at the kernel level, which is achieved by associating them to hook points. Hooks are predefined at kernel levels and include system calls, network events, function entry and exit, and so on. In scenarios where an appropriate hook doesn’t exist, then users can make use of kernel probes, also called kprobes. The kprobes are inserted into the kernel routine; ebPF programs are defined as a handler to kprobes and are executed whenever a particular breakpoint is hit in the kernel. Like hooks and kprobes, eBPF programs can also be attached to uprobes, which are probes at user space levels and are tied to an event at the user application level, thus eBPF programs can be executed at any level from the kernel to the user application.  When executing programs at the kernel level, the biggest concern is the security of the program. In eBPF, that is assured by BPF libraries. The BPF libraries handle the<a id="_idIndexMarker1036"/> system call to load the eBPF programs in two steps. The first step is the verification step, during which the eBPF program is validated to ensure that it will run to completion and will not lock up the kernel, the process loading the eBPF program has correct privileges, and the eBPF program will not harm the kernel in any way. The second step is<a id="_idIndexMarker1037"/> a <strong class="bold">Just-In-Time</strong> (<strong class="bold">JIT</strong>) compilation step, which translates the generic bytecode of the program into the machine-specific instruction and optimizes it to get the maximum execution speed of the program. This makes eBPF programs run as efficiently as natively compiled kernel code as if it was loaded as a kernel module. Once the two steps are complete, the eBPF program is loaded and compiled into the kernel waiting for the hook or kprobes to trigger <span class="No-Break">the execution.</span></p>
			<p>BPF has been <a id="_idIndexMarker1038"/>widely used as an add-on to the kernel. Most of the applications have been at the network level and mostly in observability space. eBPF has been used to provide visibility into system calls at packet and socket levels, which are then used for building security solution systems that can operate with low-level context from the kernel. eBPF programs are also used for introspection of user applications along with the part of the kernel running the application, which provides a consolidated insight to troubleshoot application performance issues. You might be wondering why we are discussing eBPF in the context of Service Mesh. The programmability and plugin model of eBPF is particularly useful in networking. eBPF can be used to perform IP routing, packet filtering, monitoring, and so on at native speeds of kernel modules. One of the drawbacks of the Istio architecture is its model of deploying a sidecar with every workload, as we discussed in <span class="No-Break"><em class="italic">Chapter 2</em></span> – the sidecar basically works by intercepting network traffic, making use of iptables to configure the kernel’s netfilter packet filter functionality. The drawback of this approach is less optimal performance, as the data path created for service traffic is much longer than what it would have been if the workload was just by itself without any sidecar traffic interception. With eBPF socket-related program types, you can filter socket data, redirect socket data, and monitor socket events. These programs have the potential for replacing the iptables-based traffic interception; using eBPF, there are options to intercept and manage network traffic without incurring any negative impacts on the data <span class="No-Break">path performance.</span></p>
			<p><strong class="bold">Isovalent</strong> (at <a href="https://isovalent.com/">https://isovalent.com/</a>) is <a id="_idIndexMarker1039"/>one such organization that is revolutionizing the architecture of API Gateway and Service Mesh. <strong class="bold">Cilium</strong> is a product<a id="_idIndexMarker1040"/> from Isovalent, and it provides a variety of functionality, including API Gateway function, Service Mesh, observability, and networking. Cilium is built with eBPF as its core technology where it injects eBPF programs at various points in the Linux kernel to achieve application networking, security, and observability functions. Cilium is getting adopted in Kubernetes networking to solve performance degradation caused by packets needing to traverse the same network stack multiple times between the host and the Pod. Cilium is solving this problem by bypassing iptables in the networking stacking, avoiding net filters and other overheads caused by iptables, which has led to significant gains in network performance. You can read more about the Cilium product stack at <a href="https://isovalent.com/blog/post/cilium-release-113/">https://isovalent.com/blog/post/cilium-release-113/</a>; you will be amazed to see how eBPF is revolutionizing the application <span class="No-Break">networking space.</span></p>
			<p>Istio has also created an open source project called Merbridge, which replaces iptables with eBPF programs to allow the transporting of data directly between inbound and outbound sockets of sidecar containers and application containers to shorten the overall data path. Merbridge is in its early days but has produced some promising results; you can find the open source project <span class="No-Break">at </span><a href="https://github.com/merbridge/merbridge"><span class="No-Break">https://github.com/merbridge/merbridge</span></a><span class="No-Break">.</span></p>
			<p>With eBPF and products like Cilium, it is highly likely that there will be an advancement in how network proxy-based products will be designed and operated in the future. eBPF is being actively explored by various Service Mesh technologies, including Istio, on how it can be used to overcome drawbacks and improve the overall performance and experience of using Istio. eBPF is a very promising technology and is already being used for doing awesome things with products such as Cilium <span class="No-Break">and Calico.</span></p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor189"/>Summary</h1>
			<p>I hope this book has provided you with a good insight into Istio. <em class="italic">Chapters 1</em> to <em class="italic">3</em> set the context on why Service Mesh is needed and how Istio the control and data planes operate. The information in these three chapters is important to appreciate Istio and to build an understanding of Istio architecture. <em class="italic">Chapters 4</em> to <em class="italic">6</em> then provided details on how to use Istio for building the application network that we discussed in the <span class="No-Break">earlier chapters.</span></p>
			<p>Then, in <span class="No-Break"><em class="italic">Chapter 7</em></span>, you learned about observability and how Istio provides integration into various observation tools, as the next steps you should explore integration with other observability and monitoring tools such as Datadog. Following that, <span class="No-Break"><em class="italic">Chapter 8</em></span> showed practices on how to deploy Istio across multiple Kubernetes clusters, which should have given you confidence on how to install Istio in production environments. <span class="No-Break"><em class="italic">Chapter 9</em></span> then provided details on how Istio can be extended using WebAssembly and its applications, while <span class="No-Break"><em class="italic">Chapter 10</em></span> discussed how Istio helps bridge the old world of virtual machines with the new world of Kubernetes by discussing how the Service Mesh can be extended to include workloads deployed on virtual machines. Lastly, <span class="No-Break"><em class="italic">Chapter 11</em></span> covered the best practices for operating Istio and how tools such as OPA Gatekeeper can be used to automate some of the <span class="No-Break">best practices.</span></p>
			<p>In this chapter, we managed to revise the concepts of <em class="italic">Chapters 4</em> to <em class="italic">6</em> by deploying and configuring another open source demo application, which should have provided you with the confidence and experience to take on the learnings from the book to real-life applications and to take advantage of application networking and security provided <span class="No-Break">by Istio.</span></p>
			<p>You also read about eBPF and what a game-changing technology it is, making it possible to write code at the kernel level without needing to understand or experience the horrors of the kernel. eBPF will possibly bring lots of changes to how Service Mesh, API Gateway, and networking solutions in general operate. In the Appendix of this book, you will find information about other Service Mesh technologies: Consul Connect, Kuma Mesh, Gloo Mesh, and Linkerd. The Appendix provides a good overview of these technologies and helps you appreciate their strength <span class="No-Break">and limitations.</span></p>
			<p>I hope you enjoyed learning about Istio. To establish your knowledge of Istio, you can also explore taking the Certified Istio Administrator exam provided by Tetrate. You can also explore the other learning avenues provided in this chapter. I hope reading this book was an endeavor that will take you to the next level in your career and experience of building scalable, resilient, and secure applications <span class="No-Break">using Istio.</span></p>
			<p>BEST <span class="No-Break">OF LUCK!</span></p>
		</div>
	

		<div id="_idContainer140">
			<h1 id="_idParaDest-191"><a id="_idTextAnchor190"/>Appendix – Other Service Mesh Technologies</h1>
			<p>In this appendix, we will learn about the following Service <span class="No-Break">Mesh implementations:</span></p>
			<ul>
				<li><span class="No-Break">Consul Connect</span></li>
				<li><span class="No-Break">Gloo Mesh</span></li>
				<li><span class="No-Break">Kuma</span></li>
				<li><span class="No-Break">Linkerd</span></li>
			</ul>
			<p>These Service Mesh technologies are popular and are gaining recognition and adoption by organizations. The information provided in this <em class="italic">Appendix</em> about these Service Mesh technologies is not exhaustive; rather, the goal here is to make you familiar with and aware of the alternatives to Istio. I hope reading this <em class="italic">Appendix</em> will provide some basic awareness of these alternative technologies and help you understand how these technologies fare in comparison to Istio. Let’s <span class="No-Break">dive in!</span></p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor191"/>Consul Connect</h1>
			<p>Consul Connect is<a id="_idIndexMarker1041"/> a Service Mesh solution offered by HashiCorp. It is also known as Consul Service Mesh. On<a id="_idIndexMarker1042"/> the HashiCorp website, you will find that the terms Consul Connect and Consul Service Mesh are used interchangeably. It is built upon Consul, which is a service discovery solution and a key-value store. Consul is a very popular and long-established service discovery solution; it provides and manages service identities for every type of workload, which are then used by Service Mesh to manage traffic between Services in Kubernetes. It also supports using ACLs to implement zero-trust networking and provides granular control over traffic flow in <span class="No-Break">the mesh.</span></p>
			<p>Consul uses Envoy as its data plane and injects it into workload Pods as sidecars. The injection can be based on annotations as well as global configurations to automatically inject sidecar proxies into all workloads in specified namespaces. We will start by installing Consul Service Mesh on your workstation, followed by some exercises to practice the basics of using Consul <span class="No-Break">Service Mesh.</span></p>
			<p>Let’s begin<a id="_idIndexMarker1043"/> by <span class="No-Break">installing Consul:</span></p>
			<ol>
				<li>Clone the <span class="No-Break">Consul repository:</span><pre class="console">
<strong class="bold">% git clone https://github.com/hashicorp-education/learn-consul-get-started-kubernetes.git</strong>
<strong class="bold">…..</strong>
<strong class="bold">Resolving deltas: 100% (313/313), done.</strong></pre></li>
				<li>Install the <span class="No-Break">Consul CLI:</span><ul><li>For MacOS, follow <span class="No-Break">these steps:</span><ol><li>Install the <span class="No-Break">HashiCorp tap:</span></li></ol><pre class="console">
<strong class="bold">% brew tap hashicorp/tap</strong></pre></li></ul><ol><li value="2">Install the Consul <span class="No-Break">Kubernetes CLI:</span></li></ol><pre class="console">
<strong class="bold">% brew install hashicorp/tap/consul-k8s</strong>
<strong class="bold">Running `brew update --auto-update`...</strong>
<strong class="bold">==&gt; Auto-updated Homebrew!</strong>
<strong class="bold">Updated 1 tap (homebrew/core).</strong>
<strong class="bold">You have 4 outdated formulae installed.</strong>
<strong class="bold">You can upgrade them with brew upgrade</strong>
<strong class="bold">or list them with brew outdated.</strong>
<strong class="bold">==&gt; Fetching hashicorp/tap/consul-k8s</strong>
<strong class="bold">==&gt; Downloading https://releases.hashicorp.com/consul-k8s/1.0.2/consul-k8s_1.0.2_darwin_arm64.zip</strong>
<strong class="bold">######################################################################## 100.0%</strong>
<strong class="bold">==&gt; Installing consul-k8s from hashicorp/tap</strong>
<strong class="bold">/opt/homebrew/Cellar/consul-k8s/1.0.2: 3 files, 64MB, built in 2 seconds</strong></pre><ol><li value="3">Check the version of <strong class="source-inline">consul-k8s</strong> on the <span class="No-Break">Consul CLI:</span></li></ol><pre class="console">
<strong class="bold">% consul-k8s version</strong>
<strong class="bold">    consul-k8s v1.0.2</strong></pre><ul><li>For Linux <a id="_idIndexMarker1044"/>Ubuntu/Debian, follow <span class="No-Break">these steps:</span></li></ul><ol><li value="4">Add the HashiCorp <span class="No-Break">GPG key:</span></li></ol><pre class="console">
<strong class="bold">% curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add –</strong></pre><ol><li value="5">Add the HashiCorp <span class="No-Break">apt repository:</span></li></ol><pre class="console">
<strong class="bold">% sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"</strong></pre><ol><li value="6">Run <strong class="source-inline">apt-get install</strong> to install the <span class="No-Break"><strong class="source-inline">consul-k8s</strong></span><span class="No-Break"> CLI:</span></li></ol><pre class="console">
<strong class="bold">% sudo apt-get update &amp;&amp; sudo apt-get install consul-k8s</strong></pre><ul><li>For CentOS/RHEL, follow <span class="No-Break">these steps:</span></li></ul><ol><li value="7">Install yum-config-manager to manage <span class="No-Break">your repositories:</span></li></ol><pre class="console">
<strong class="bold">% sudo yum install -y yum-utils</strong></pre><ol><li value="8">Use yum-config-manager to add the official HashiCorp <span class="No-Break">Linux repository:</span></li></ol><pre class="console">
<strong class="bold">% sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo</strong></pre><ol><li value="9">Install the <span class="No-Break"><strong class="source-inline">consul-k8s</strong></span><span class="No-Break"> CLI:</span></li></ol><pre class="console">
<strong class="bold">% sudo yum -y install consul-k8s</strong></pre></li>
				<li><span class="No-Break">Start minikube:</span><pre class="console">
<strong class="bold">% minikube start --profile dc1 --memory 4096 --kubernetes-version=v1.24.0</strong></pre></li>
				<li>Install <a id="_idIndexMarker1045"/>Consul on minikube using the <span class="No-Break">Consul CLI.</span></li>
			</ol>
			<p>Run the following <span class="No-Break">in learn-consul-get-started-kubernetes/local:</span></p>
			<pre class="console">
<strong class="bold">% consul-k8s install -config-file=helm/values-v1.yaml -set global.image=hashicorp/consul:1.14.0</strong>
<strong class="bold">==&gt; Checking if Consul can be installed</strong>
<strong class="bold"> </strong><strong class="bold">✓</strong><strong class="bold"> No existing Consul installations found.</strong>
<strong class="bold"> </strong><strong class="bold">✓</strong><strong class="bold"> No existing Consul persistent volume claims found</strong>
<strong class="bold"> </strong><strong class="bold">✓</strong><strong class="bold"> No existing Consul secrets found.</strong>
<strong class="bold">==&gt; Consul Installation Summary</strong>
<strong class="bold">    Name: consul</strong>
<strong class="bold">    Namespace: consul</strong>
<strong class="bold">….</strong>
<strong class="bold">--&gt; Starting delete for "consul-server-acl-init-cleanup" Job</strong>
<strong class="bold"> </strong><strong class="bold">✓</strong><strong class="bold"> Consul installed in namespace "consul".</strong></pre>
			<ol>
				<li value="5">Check the Consul Pods in <span class="No-Break">the namespace:</span><pre class="console">
<strong class="bold">% kubectl get po -n consul</strong>
<strong class="bold">NAME                    READY   STATUS    RESTARTS    AGE</strong>
<strong class="bold">consul-connect-injector-57dcdd54b7-hhxl4       1/1     Running   1 (21h ago)   21h</strong>
<strong class="bold">consul-server-0       1/1     Running   0             21h</strong>
<strong class="bold">consul-webhook-cert-manager-76bbf7d768-2kfhx   1/1     Running   0             21h</strong></pre></li>
				<li>Configure<a id="_idIndexMarker1046"/> the Consul CLI to be able to communicate <span class="No-Break">with Consul.</span></li>
			</ol>
			<p>We will set environment variables so that the Consul CLI can communicate with your <span class="No-Break">Consul cluster.</span></p>
			<p>Set <strong class="source-inline">CONSUL_HTTP_TOKEN</strong> from <strong class="source-inline">secrets/consul-bootstrap-acl-token</strong> and set it as an <span class="No-Break">environment variable:</span></p>
			<pre class="console">
<strong class="bold">% export </strong><strong class="source-inline">CONSUL_HTTP_TOKEN</strong><strong class="bold">=$(kubectl get --namespace consul </strong><strong class="source-inline">secrets/consul-bootstrap-acl-token</strong><strong class="bold"> --template={{.data.token}} | base64 -d)</strong></pre>
			<p>Set the Consul destination address. By default, Consul runs on port 8500 for HTTP and 8501 <span class="No-Break">for HTTPS:</span></p>
			<pre class="console">
<strong class="bold">% export CONSUL_HTTP_ADDR=https://127.0.0.1:8501</strong></pre>
			<p>Remove SSL verification checks to simplify communication with your <span class="No-Break">Consul cluster:</span></p>
			<pre class="console">
<strong class="bold">% export CONSUL_HTTP_SSL_VERIFY=false</strong></pre>
			<ol>
				<li value="7">Access the Consul dashboard using the <span class="No-Break">following command:</span><pre class="console">
<strong class="bold">% kubectl port-forward pods/consul-server-0 8501:8501 --namespace consul</strong></pre></li>
			</ol>
			<p>Open <strong class="source-inline">localhost:8501</strong> in your browser to access the Consul dashboard, as shown in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/Figure_13.01_B17989.jpg" alt="Figure A.1 – Consul Dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure A.1 – Consul Dashboard</p>
			<p>Now that we<a id="_idIndexMarker1047"/> have installed Consul Service Mesh on minikube, let’s deploy an example application and go through the fundamentals of Consul <span class="No-Break">Service Mesh.</span></p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor192"/>Deploying an example application</h2>
			<p>In this section, we<a id="_idIndexMarker1048"/> will deploy envoydummy along with a curl application. The <strong class="source-inline">sampleconfiguration</strong> file is available <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">AppendixA/envoy-proxy-01.yaml</strong></span><span class="No-Break">.</span></p>
			<p>In the configuration file, you will notice the <span class="No-Break">following annotation:</span></p>
			<pre class="source-code">
      annotations:
        consul.hashicorp.com/connect-inject: "true"</pre>
			<p>This annotation allows Consul to automatically inject a proxy for each service. The proxies create a data plane to handle requests between services based on the configuration <span class="No-Break">from Consul.</span></p>
			<p>Apply the configuration to create <strong class="source-inline">envoydummy</strong> and the <span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break"> Pods:</span></p>
			<pre class="console">
% kubectl create -f AppendixA/Consul/envoy-proxy-01.yaml -n appendix-consul
configmap/envoy-dummy created
service/envoydummy created
deployment.apps/envoydummy created
servicedefaults.consul.hashicorp.com/envoydummy created
serviceaccount/envoydummy created
servicedefaults.consul.hashicorp.com/curl created
serviceaccount/curl created
pod/curl created
service/curl created</pre>
			<p>In a few <a id="_idIndexMarker1049"/>seconds, you will notice that Consul automatically injects a sidecar into <span class="No-Break">the Pods:</span></p>
			<pre class="console">
% % kubectl get po -n appendix-consul
NAME                          READY   STATUS    RESTARTS   AGE
curl                          2/2     Running   0          16s
envoydummy-77dfb5d494-2dx5w   2/2     Running   0          17s</pre>
			<p>To find out more about the sidecar, please inspect the <strong class="source-inline">envoydummy</strong> Pod using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
% kubectl get po/envoydummy-77dfb5d494-pcqs7 -n appendix-consul -o json | jq '.spec.containers[].image'
"envoyproxy/envoy:v1.22.2"
"<strong class="source-inline">hashicorp/consul-dataplane:1.0.0</strong>"
% kubectl get po/envoydummy-77dfb5d494-pcqs7 -n appendix-consul -o json | jq '.spec.containers[].name'
"envoyproxy"
"<strong class="source-inline">consul-dataplane</strong>"</pre>
			<p>In the output, you can see a container named <strong class="source-inline">consul-dataplane</strong> created from an image called <strong class="source-inline">hashicorp/consul-dataplane:1.0.0</strong>. You can inspect the image at <a href="https://hub.docker.com/layers/hashicorp/consul-dataplane/1.0.0-beta1/images/sha256-f933183f235d12cc526099ce90933cdf43c7281298b3cd34a4ab7d4ebeeabf84?context=explore">https://hub.docker.com/layers/hashicorp/consul-dataplane/1.0.0-beta1/images/sha256-f933183f235d12cc526099ce90933cdf43c7281298b3cd34a4ab7d4ebeeabf84?context=explore</a> and you will notice that it is made up of <span class="No-Break">envoy proxy.</span></p>
			<p>Let’s try<a id="_idIndexMarker1050"/> to access <strong class="source-inline">envoydummy</strong> from the <span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break"> Pod:</span></p>
			<pre class="console">
% kubectl exec -it pod/curl -n appendix-consul -- curl http://envoydummy:80
curl: (52) Empty reply from server
command terminated with exit code 52</pre>
			<p>So far, we have successfully deployed the <strong class="source-inline">envoydummy</strong> Pod along with <strong class="source-inline">consul-dataplane</strong> as a sidecar. We have observed Consul Service Mesh security in action by seeing that the <strong class="source-inline">curl</strong> Pod, while deployed in the same namespace, is unable to access the <strong class="source-inline">envoydummy</strong> Pod. In the next section, we will understand this behavior and learn how to configure Consul to perform <span class="No-Break">zero-trust networking.</span></p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor193"/>Zero-trust networking</h2>
			<p>Consul manages<a id="_idIndexMarker1051"/> inter-service authorization with Consul constructs called intentions. Using Consul CRDs, you need to define intentions that prescribe what services are allowed to communicate with each other. Intentions are the cornerstones of zero-trust networking <span class="No-Break">in Consul.</span></p>
			<p>Intentions are enforced by the sidecar proxy on inbound connections. The sidecar proxy identifies the inbound service using its TLS client certificate. After identifying the inbound service, the sidecar proxy then checks if an intention exists to allow the client to communicate with the <span class="No-Break">destination service.</span></p>
			<p>In the following code block, we are defining an intention to allow traffic from the <strong class="source-inline">curl</strong> service to the <span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break"> service:</span></p>
			<pre class="source-code">
apiVersion: consul.hashicorp.com/v1alpha1
kind: ServiceIntentions
metadata:
  name: curl-to-envoydummy-api
  namespace: appendix-consul
spec:
  destination:
    name: envoydummy
  sources:
    - name: curl
      action: <strong class="bold">allow</strong></pre>
			<p>In the<a id="_idIndexMarker1052"/> configuration, we have specified the names of the destination service and the source service. In <strong class="source-inline">action</strong>, we have specified <strong class="source-inline">allow</strong> to allow traffic from source to destination. Another possible value of <strong class="source-inline">action</strong> is <strong class="source-inline">deny</strong>, which denies traffic from source to destination. If you do not want to specify the name of a service, you will need to use <strong class="source-inline">*</strong>. For example, if the service name in <strong class="source-inline">sources</strong> is <strong class="source-inline">*</strong> then it will allow traffic from all services <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break">.</span></p>
			<p>Let’s apply intentions using the <span class="No-Break">following command:</span></p>
			<pre class="console">
% kubectl create -f AppendixA/Consul/curl-to-envoy-dummy-intentions.yaml
serviceintentions.consul.hashicorp.com/curl-to-envoydummy-api created</pre>
			<p>You can verify the created intentions in the <span class="No-Break">Consul dashboard:</span></p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/Figure_13.02_B17989.jpg" alt="Figure A.2 – Consul intentions"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure A.2 – Consul intentions</p>
			<p>Considering that we have created the intention to allow traffic from the <strong class="source-inline">curl</strong> service to the <strong class="source-inline">envoydummy</strong> service, let’s test that the <strong class="source-inline">curl</strong> Pod is able to communicate with the <strong class="source-inline">envoydummy</strong> Pod using the <span class="No-Break">following command:</span></p>
			<pre class="console">
% kubectl exec -it pod/curl -n appendix-consul -- curl http://envoydummy
V1----------Bootstrap Service Mesh Implementation with Istio----------V1%</pre>
			<p>Using intentions, we <a id="_idIndexMarker1053"/>were able to define rules to control traffic between services without needing to configure a firewall or any changes in the cluster. Intentions are key building blocks of Consul for creating <span class="No-Break">zero-trust networks.</span></p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor194"/>Traffic management and routing</h2>
			<p>Consul provides a <a id="_idIndexMarker1054"/>comprehensive set of service discovery and traffic management features. The service discovery comprises three stages: routing, splitting, and resolution. These three stages are also referred to as the service discovery chain, and it can be used to implement traffic controls based on HTTP headers, path, query strings, and <span class="No-Break">workload version.</span></p>
			<p>Let’s go through each<a id="_idIndexMarker1055"/> stage of the service <span class="No-Break">discovery chain.</span></p>
			<h3>Routing</h3>
			<p>This is the<a id="_idIndexMarker1056"/> first stage of the service discovery chain, and it is used to intercept traffic using Layer 7 constructs such as HTTP header and path. This is achieved via service-router config entry through which you can control the traffic routing using various criteria. For example, for <strong class="source-inline">envoydummy</strong>, let’s say we want to enforce that any request send to <strong class="source-inline">envoydummy</strong> version v1 with <strong class="source-inline">/latest</strong> in the URI should be routed to <strong class="source-inline">envoydummy</strong> version v2 instead, and any request to version v2 of the <strong class="source-inline">envoydummy</strong> app but with <strong class="source-inline">/old</strong> in the path should be routed to version v1 of the <strong class="source-inline">envoydummy</strong> app. This can be achieved using the following <span class="No-Break"><strong class="source-inline">ServiceRouter</strong></span><span class="No-Break"> configuration:</span></p>
			<pre class="source-code">
apiVersion: consul.hashicorp.com/v1alpha1
kind: ServiceRouter
metadata:
  name: <strong class="bold">envoydummy</strong>
spec:
  routes:
    - match:
        http:
          <strong class="bold">pathPrefix</strong>: <strong class="bold">'/latest'</strong>
      destination:
        service: 'envoydummy2'</pre>
			<p>In the<a id="_idIndexMarker1057"/> configuration, we are specifying that any request destined for the <strong class="source-inline">envoydummy</strong> service but with <strong class="source-inline">pathPrefix</strong> set to <strong class="source-inline">'/latest'</strong> will be routed to <strong class="source-inline">envoydummy2</strong>. And in the following configuration, we are specifying that any request destined for the <strong class="source-inline">envoydummy2</strong> service but with <strong class="source-inline">pathPrefix</strong> set to <strong class="source-inline">'/old'</strong> will be routed <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
apiVersion: consul.hashicorp.com/v1alpha1
kind: ServiceRouter
metadata:
  name: <strong class="bold">envoydummy2</strong>
spec:
  routes:
    - match:
        http:
          pathPrefix: <strong class="bold">'/old'</strong>
      destination:
        service: <strong class="bold">'envoydummy'</strong></pre>
			<p>Both <strong class="source-inline">ServiceRouter</strong> configurations are saved in <strong class="source-inline">AppendixA/Consul/routing-to-envoy-dummy.yaml</strong>. A deployment descriptor for <strong class="source-inline">envoydummy</strong> version v2 and the intentions that allow traffic from the <strong class="source-inline">curl</strong> Pod are also available in <strong class="source-inline">AppendixA/Consul/envoy-proxy-02.yaml</strong> <span class="No-Break">on GitHub.</span></p>
			<p>Go ahead <a id="_idIndexMarker1058"/>and deploy version v2 of <strong class="source-inline">envoydummy</strong> along with the ServiceRouter configuration using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
% kubectl apply -f AppendixA/Consul/envoy-proxy-02.yaml
% kubectl apply -f AppendixA/Consul/routing-to-envoy-dummy.yaml -n appendix-consul
servicerouter.consul.hashicorp.com/envoydummy configured
servicerouter.consul.hashicorp.com/envoydummy2 configured</pre>
			<p>You can check the configuration using the Consul dashboard. The following two screenshots show the <a id="_idIndexMarker1059"/>two <strong class="source-inline">ServiceRouter</strong> configurations we <span class="No-Break">have applied:</span></p>
			<ul>
				<li><strong class="source-inline">ServiceRouter</strong> configuration to send traffic with the prefix <strong class="source-inline">/latest</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">envoydummy2</strong></span><span class="No-Break">:</span></li>
			</ul>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/Figure_13.03_B17989.jpg" alt="Figure A.3"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure A.3</p>
			<ul>
				<li><strong class="source-inline">ServiceRouter</strong> configuration to send traffic with the prefix <strong class="source-inline">/old</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break">:</span></li>
			</ul>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/Figure_13.04_B17989.jpg" alt="Figure A.4"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure A.4</p>
			<p>Now that we<a id="_idIndexMarker1060"/> have configured the service routes, let’s test the <span class="No-Break">routing behavior:</span></p>
			<ol>
				<li value="1">Make any request to version v1 of <strong class="source-inline">envoydummy</strong> with a URI that is <span class="No-Break">not </span><span class="No-Break"><strong class="source-inline">/latest</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% kubectl exec -it pod/curl -n appendix-consul -- curl http://envoydummy/new</strong>
<strong class="bold">V1----------Bootstrap Service Mesh Implementation with Istio----------V1%</strong></pre></li>
			</ol>
			<p>The output is as expected: the request should be routed to version v1 <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="2">Make a<a id="_idIndexMarker1061"/> request to version v1 of <strong class="source-inline">envoydummy</strong> with a URI that <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">/latest</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% kubectl exec -it pod/curl -n appendix-consul -- curl http://envoydummy/latest</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2%</strong></pre></li>
			</ol>
			<p>The output is as expected: the request, although addressed to version v1 of <strong class="source-inline">envoydummy</strong>, is routed to version v2 <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="3">Make any request to version v2 of <strong class="source-inline">envoydummy</strong> with a URI that is <span class="No-Break">not </span><span class="No-Break"><strong class="source-inline">/old</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% kubectl exec -it pod/curl -n appendix-consul -- curl http://envoydummy2/new</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2%</strong></pre></li>
			</ol>
			<p>The output is as expected: the request should be routed to version v2 <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="4">Make <a id="_idIndexMarker1062"/>a request to version v2 of <strong class="source-inline">envoydummy</strong> with a URI that <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">/old</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% kubectl exec -it pod/curl -n appendix-consul -- curl http://envoydummy2/old</strong>
<strong class="bold">V1----------Bootstrap Service Mesh Implementation with Istio----------V1%</strong></pre></li>
			</ol>
			<p>The output is as expected: the request although addressed to version v2 of <strong class="source-inline">envoydummy</strong> is routed to version v1 <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break">.</span></p>
			<p>In these examples, we made use of path prefixes as criteria for routing. The other options are query parameters and HTTP headers. <strong class="source-inline">ServiceRouter</strong> also supports retry logic, which can be added to the destination configuration. Here is an example of retry logic added to the <span class="No-Break"><strong class="source-inline">ServiceRouter</strong></span><span class="No-Break"> config:</span></p>
			<pre class="source-code">
apiVersion: consul.hashicorp.com/v1alpha1
kind: ServiceRouter
metadata:
  name: envoydummy2
spec:
  routes:
    - match:
        http:
          pathPrefix: '/old'
<strong class="bold">      destination:</strong>
<strong class="bold">        service: 'envoydummy'</strong>
<strong class="bold">        requestTimeout = "20s"</strong>
<strong class="bold">        numRetries = 3</strong>
<strong class="bold">        retryOnConnectFailure = true</strong></pre>
			<p>You can <a id="_idIndexMarker1063"/>read more about <strong class="source-inline">ServiceRouter</strong> configuration on the HashiCorp <span class="No-Break">website: https://developer.hashicorp.com/consul/docs/connect/config-entries/service-router.</span></p>
			<p>Next in the service discovery chain is splitting, which we will learn about in the <span class="No-Break">following section.</span></p>
			<h3>Splitting</h3>
			<p>Service<a id="_idIndexMarker1064"/> splitting is the second stage in the Consul service discovery chain and is configured via the <strong class="source-inline">ServiceSplitter</strong> configuration. <strong class="source-inline">ServiceSplitter</strong> allows you to split a request to a service to multiple subset workloads. Using this configuration, you can also perform canary deployments. Here is an example where traffic for the <strong class="source-inline">envoydummy</strong> service is routed in a 20:80 ratio to version v1 and v2 of the <span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break"> application:</span></p>
			<pre class="source-code">
apiVersion: consul.hashicorp.com/v1alpha1
kind: ServiceSplitter
metadata:
  name: envoydummy
spec:
  splits:
    - weight: 20
      service: envoydummy
    - weight: 80
      service: envoydummy2</pre>
			<p>In the <strong class="source-inline">ServiceSplitter</strong> configuration, we have configured 80% of the traffic to <strong class="source-inline">envoydummy</strong> to be routed to the <strong class="source-inline">envoydummy2</strong> service and the remaining 20% of the traffic to be routed to the <strong class="source-inline">envoydummy</strong> service. The configuration is available in <strong class="source-inline">AppendixA/Consul/splitter.yaml</strong>. Apply the configuration using the <span class="No-Break">following command:</span></p>
			<pre class="console">
% kubectl apply -f AppendixA/Consul/splitter.yaml -n appendix-consul
servicesplitter.consul.hashicorp.com/envoydummy created</pre>
			<p>After<a id="_idIndexMarker1065"/> applying the configuration, you can check out the routing config on the Consul dashboard. In the following screenshot, we can see that all traffic to <strong class="source-inline">envoydummy</strong> is routed to <strong class="source-inline">envoydummy</strong> and <strong class="source-inline">envoydummy2</strong>. The following screenshot doesn’t show the percentage, but you can hover the mouse over the arrows connecting the splitters and resolvers and you should be able to see <span class="No-Break">the percentage:</span></p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/Figure_13.05_B17989.jpg" alt="Figure A.5 – Split of traffic to envoydummy2"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure A.5 – Split of traffic to envoydummy2</p>
			<p>The following screenshot shows the split of traffic <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/Figure_13.06_B17989.jpg" alt="Figure A.6 – Split of traffic to envoydummy service"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure A.6 – Split of traffic to envoydummy service</p>
			<p>Now that the <strong class="source-inline">ServiceSplitter</strong> configuration is in place, test that traffic to our services is routed in the ratio specified in the <span class="No-Break">config file:</span></p>
			<pre class="console">
% for ((i=0;i&lt;10;i++)); do kubectl exec -it pod/curl -n appendix-consul -- curl http://envoydummy/new ;done
V2----------Bootstrap Service Mesh Implementation with Istio----------V2
V2----------Bootstrap Service Mesh Implementation with Istio----------V2
V2----------Bootstrap Service Mesh Implementation with Istio----------V2
V1----------Bootstrap Service Mesh Implementation with Istio----------V1
V2----------Bootstrap Service Mesh Implementation with Istio----------V2
V1----------Bootstrap Service Mesh Implementation with Istio----------V1
V2----------Bootstrap Service Mesh Implementation with Istio----------V2
V2----------Bootstrap Service Mesh Implementation with Istio----------V2
V2----------Bootstrap Service Mesh Implementation with Istio----------V2
V2----------Bootstrap Service Mesh Implementation with Istio----------V2</pre>
			<p>You <a id="_idIndexMarker1066"/>will observe that the traffic is routed in a 20:80 ratio between the two services. <strong class="source-inline">ServiceSplitter</strong> is a powerful feature that can be used for A/B testing, as well as canary and blue/green deployments. Using <strong class="source-inline">ServiceSplitter</strong>, you can also perform weight-based routing between subsets of the same service. It also allows you to add HTTP headers while routing the service. You can read more about <strong class="source-inline">ServiceSplitter</strong> <span class="No-Break">at </span><a href="https://developer.hashicorp.com/consul/docs/connect/config-entries/service-splitter"><span class="No-Break">https://developer.hashicorp.com/consul/docs/connect/config-entries/service-splitter</span></a><span class="No-Break">.</span></p>
			<p>We have looked at two of the three steps in Consul’s service discovery chain. The final stage is resolution, which we will cover in the <span class="No-Break">next section.</span></p>
			<h3>Resolution</h3>
			<p>Consul <a id="_idIndexMarker1067"/>has another config type called <strong class="source-inline">ServiceResolver</strong>, which is used to define which instances of a service map to the service name requested by the client. They control the service discovery and decide where the request is finally routed to. Using <strong class="source-inline">ServiceResolver</strong>, you can control the resilience of your system by routing the request to healthy upstreams. <strong class="source-inline">ServiceResolver</strong> distributes load to services when they are spread across more than one data center and provides failover when the services are suffering from outages. More details about <strong class="source-inline">ServiceResolver</strong> can be found <span class="No-Break">at </span><a href="https://developer.hashicorp.com/consul/docs/connect/config-entries/service-resolver"><span class="No-Break">https://developer.hashicorp.com/consul/docs/connect/config-entries/service-resolver</span></a><span class="No-Break">.</span></p>
			<p>Consul Service Mesh also has provision for gateways to manage traffic from outside the mesh. It supports three kinds <span class="No-Break">of gateway:</span></p>
			<ul>
				<li><strong class="bold">Mesh gateways</strong> are<a id="_idIndexMarker1068"/> used to enable and secure<a id="_idIndexMarker1069"/> communication between data centers. It acts as a proxy providing Ingress to Service Mesh while at the same time securing the traffic using mTLS. Mesh gateways are used to communicate between Consul Service Mesh instances deployed in different data centers and/or Kubernetes clusters. A <a id="_idIndexMarker1070"/>good hands-on exercise on mesh gateways is available <span class="No-Break">at </span><a href="https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-mesh-gateways"><span class="No-Break">https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-mesh-gateways</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Ingress gateways</strong> are <a id="_idIndexMarker1071"/>used to provide access<a id="_idIndexMarker1072"/> to services in the mesh to clients outside the mesh. The client can be outside the mesh but in the same Kubernetes cluster, or completely outside the cluster but within or beyond the network perimeter of the organization. You can <a id="_idIndexMarker1073"/>read more about Ingress Gateway <span class="No-Break">at </span><a href="https://developer.hashicorp.com/consul/docs/k8s/connect/ingress-gateways"><span class="No-Break">https://developer.hashicorp.com/consul/docs/k8s/connect/ingress-gateways</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Terminating gateways</strong>, like<a id="_idIndexMarker1074"/> ServiceEntry in Istio, are<a id="_idIndexMarker1075"/> used to allow workloads within the mesh to access services outside the mesh. To use a terminating gateway, users<a id="_idIndexMarker1076"/> need to also use a configuration called <strong class="source-inline">ServiceDefault</strong>. This is where the details about the external service are defined, and it is referred to by the terminating gateway. You can read <a id="_idIndexMarker1077"/>more<a id="_idIndexMarker1078"/> about terminating gateways <span class="No-Break">at </span><a href="https://developer.hashicorp.com/consul/docs/k8s/connect/terminating-gateways"><span class="No-Break">https://developer.hashicorp.com/consul/docs/k8s/connect/terminating-gateways</span></a><span class="No-Break">.</span></li>
			</ul>
			<p>Finally, Consul Service Mesh<a id="_idIndexMarker1079"/> also provides comprehensive observability of the mesh. The sidecar proxies collect and expose data about the traffic traversing the mesh. The metrics data exposed by the sidecar proxies are then scraped by Prometheus. The data includes Layer 7 metrics such as HTTP status code, request latency, and throughput. The Consul control plane also provides some metrics such as config synchronization status, exceptions, and errors, like the Istio control plane. The tech stack for observability is also like Istio; like Istio, Consul also supports integration with various other observability tools, such as Datadog, to get insight into Consul Service Mesh health and performance. You can read more about Consul Service Mesh observability <span class="No-Break">at </span><a href="https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-layer7-observability"><span class="No-Break">https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-layer7-observability</span></a><span class="No-Break">.</span></p>
			<p>I hope this section provided a brief but informative rundown of how Consul Service Mesh operates, what the various constructs in Consul Service Mesh are, and how they operate. I am sure you must have noticed the similarities between Consul Service Mesh and Istio: they both use Envoy as a sidecar proxy, and the Consul service discovery chain closely resembles Istio virtual services and destination rules; Consul Service Mesh gateways are very similar to Istio gateways. The main difference is how the control plane is implemented and the use of an agent on each node of the cluster. Consul Service Mesh can run on VMs to provide benefits of Service Mesh on legacy workloads. Consul Service Mesh is backed by HashiCorp and is tightly integrated with HashiCorp’s other products, including HashiCorp Vault. It is also offered as a freemium product. There is also an Enterprise version for organizations needing enterprise support and a SaaS offering called HCP Consul that provides a fully managed cloud service to customers who want one-click <span class="No-Break">mesh deployments.</span></p>
			<p class="callout-heading">Uninstalling Consul Service Mesh</p>
			<p class="callout">You can use <a id="_idIndexMarker1080"/>consul-k8s to uninstall Consul Service Mesh using the <span class="No-Break">following commands:</span></p>
			<p class="callout-heading"><strong class="source-inline">% consul-k8s uninstall -auto-approve=true -wipe-data=true</strong></p>
			<p class="callout-heading"><strong class="source-inline">..</strong></p>
			<p class="callout-heading"><strong class="source-inline">    Deleting data for installation:</strong></p>
			<p class="callout-heading"><strong class="source-inline">    Name: consul</strong></p>
			<p class="callout-heading"><strong class="source-inline">    Namespace consul</strong></p>
			<p class="callout-heading"><strong class="source-inline"> </strong><strong class="source-inline">✓</strong><strong class="source-inline"> Deleted PVC =&gt; data-consul-consul-server-0</strong></p>
			<p class="callout-heading"><strong class="source-inline"> </strong><strong class="source-inline">✓</strong><strong class="source-inline"> PVCs deleted.</strong></p>
			<p class="callout-heading"><strong class="source-inline"> </strong><strong class="source-inline">✓</strong><strong class="source-inline"> Deleted Secret =&gt; consul-bootstrap-acl-token</strong></p>
			<p class="callout-heading"><strong class="source-inline"> </strong><strong class="source-inline">✓</strong><strong class="source-inline"> Consul secrets deleted.</strong></p>
			<p class="callout-heading"><strong class="source-inline"> </strong><strong class="source-inline">✓</strong><strong class="source-inline"> Deleted Service Account =&gt; consul-tls-init</strong></p>
			<p class="callout-heading"><strong class="source-inline"> </strong><strong class="source-inline">✓</strong><strong class="source-inline"> Consul service accounts deleted.</strong></p>
			<p class="callout">You can uninstall consul-k8s CLI using Brew <span class="No-Break">on macOS:</span></p>
			<p class="callout"><strong class="source-inline">% brew </strong><span class="No-Break"><strong class="source-inline">uninstall consul-k8s</strong></span></p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor195"/>Gloo Mesh</h1>
			<p>Gloo Mesh is a <a id="_idIndexMarker1081"/>Service Mesh offering from Solo.io. There is an open source version called Gloo Mesh and an enterprise offering called Gloo Mesh Enterprise. Both are based on Istio Service Mesh and claim to have a better control plane and added functionality on top of open source Istio. Solo.io provides a feature comparison on its website outlining the differences between Gloo Mesh Enterprise, Gloo Mesh Open Source, and Istio, which you <a id="_idIndexMarker1082"/>can access at <a href="https://www.solo.io/products/gloo-mesh/">https://www.solo.io/products/gloo-mesh/</a>. Gloo Mesh is primarily focused on providing a Kubernetes-native management plane through which users can configure and operate multiple heterogeneous Service Mesh instances across multiple clusters. It comes with an API that abstracts the complexity of managing and operating multiple meshes without the user needing to know the complexity under the hood caused by multiple Service Meshes. You can find details about Gloo Mesh at <a href="https://docs.solo.io/gloo-mesh-open-source/latest/getting_started/">https://docs.solo.io/gloo-mesh-open-source/latest/getting_started/</a>. This is a comprehensive resource on how to install and try Gloo Mesh. Solo.io has another product called Gloo Edge, which acts as a Kubernetes Ingress controller as well as an API gateway. Gloo Mesh Enterprise is deployed along with Gloo Edge, which provides many comprehensive API management and Ingress capabilities. Gateway Gloo Mesh Enterprise adds support for external authentication using OIDC, OAuth, API key, LDAP, and OPA. These policies are implemented via a custom CRD called ExtAuthPolicy, which can apply these authentications when routes and destinations match <span class="No-Break">certain criteria.</span></p>
			<p>Gloo Mesh Enterprise provides WAF policies to monitor, filter, and block any harmful HTTP traffic. It also provides support for data loss prevention by doing a series of regex replacements on the response body and content that is logged by Envoy. This is a very important feature from a security point of view and stops sensitive data from being logged into the log files. DLP filters can be configured on listeners, virtual services, and routes. Gloo Mesh also provides support for connecting to legacy applications via the SOAP message format. There are options for building data transformation policies to apply XSLT transformation to modernize SOAP/XML endpoints. The data transformation policies can be applied to transform request or response payloads. It also supports special transformations such as via Inja templates. With Inja, you can write loops, conditional logic, and other functions to transform requests <span class="No-Break">and responses.</span></p>
			<p>There is also extensive support for WASM filters. Solo.io provides custom tooling that speeds up the development and deployment of web assemblies. To store WASM files, solo.io provides WebAssembly Hub, available at <a href="https://webassemblyhub.io/">https://webassemblyhub.io/</a>, and <a id="_idIndexMarker1083"/>an open source CLI tool called wasme. You can read more about how to use Web Assembly Hub and the wasme CLI <span class="No-Break">at </span><a href="https://docs.solo.io/web-assembly-hub/latest/tutorial_code/getting_started/"><span class="No-Break">https://docs.solo.io/web-assembly-hub/latest/tutorial_code/getting_started/</span></a><span class="No-Break">.</span></p>
			<p>As Gloo Mesh <a id="_idIndexMarker1084"/>and other products from Solo.io are closely integrated with the Enterprise Service Mesh offering, you get a plethora of other features, and one such feature is a <a id="_idIndexMarker1085"/>global API portal. The API portal is a self-discovery portal for publishing, sharing, and monitoring API usage for internal and external monetization. When using a multi-heterogeneous mesh, users don’t need to worry about managing observability tools for every mesh; instead, Gloo Mesh Enterprise provides aggregated metrics across every mesh, providing a seamless experience of managing and observing <span class="No-Break">multiple meshes.</span></p>
			<p>In enterprise environments, it is important that multiple teams and users can access and deploy services in the mesh without stepping on each others’ toes. Users need to know what services are available to consume and what services they have published. Users should be able to confidently perform mesh operations without impacting the services of other teams. Gloo Mesh uses the concept of workspaces, which are logical boundaries for a team, limiting team Service Mesh operations within the confines of the workspace so that multiple teams can concurrently use the mesh. Workspaces provide security isolation between configurations published by every team. Through workspaces, Gloo Mesh addresses the complexity of muti-tenancy in Enterprise environments, making it simpler for multiple teams to adopt Service Mesh with config isolation from each other and strict access control for safe muti-tenant usage of <span class="No-Break">the mesh.</span></p>
			<p>Gloo Mesh is also integrated with another Service Mesh based on a different architecture than Istio. The mesh is called Istio Ambient Mesh, which, rather than adding a sidecar proxy per workload, adds a proxy at the per-node level. Istio Ambient Mesh is integrated with Gloo Mesh, and users can run their sidecar proxy-based mesh along with their per-node proxy Istio <span class="No-Break">Ambient Mesh.</span></p>
			<p>Gloo Enterprise Mesh, with integration with Solo.io products such as Gloo Edge, makes it a strong contender among Service Mesh offerings. The ability to support multi-cluster and multi-mesh deployments, multi-tenancy via workspaces, strong support for authentication, zero-trust networking, and mature Ingress management via Gloo Edge makes it a comprehensive Service <span class="No-Break">Mesh offering.</span></p>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor196"/>Kuma</h1>
			<p>Kuma is an <a id="_idIndexMarker1086"/>open source CNCF sandbox project donated to CNCF by Kong Inc. Like Istio, Kuma also uses Envoy as the data plane. It supports multi-cluster and multi-mesh deployments, providing one global control plane to manage them all. At the time of writing this book, Kuma is one single executable written in GoLang. It can be deployed on Kubernetes as well as on VMs. When deployed in non-Kubernetes environments, it requires a PostgreSQL database to store <span class="No-Break">its configurations.</span></p>
			<p>Let’s start by <a id="_idIndexMarker1087"/>downloading and installing Kuma, followed by hands-on exercises on <span class="No-Break">this topic:</span></p>
			<ol>
				<li value="1">Download Kuma for your <span class="No-Break">operating system:</span><pre class="console">
<strong class="bold">% curl -L https://kuma.io/installer.sh | VERSION=2.0.2 sh -</strong>
<strong class="bold">INFO Welcome to the Kuma automated download!</strong>
<strong class="bold">INFO Kuma version: 2.0.2</strong>
<strong class="bold">INFO Kuma architecture: arm64</strong>
<strong class="bold">INFO Operating system: Darwin</strong>
<strong class="bold">INFO Downloading Kuma from: https://download.konghq.com/mesh-alpine/kuma-2.0.2-darwin-arm64.tar.gz</strong></pre></li>
				<li>Install Kuma on minikube. Unzip the download file and, in the unzipped folder’s <strong class="source-inline">bin</strong> directory, run the following commands to install Kuma <span class="No-Break">on Kubernetes:</span><pre class="console">
<strong class="bold">% kumactl install control-plane | kubectl apply -f -</strong></pre></li>
			</ol>
			<p>This will create a namespace called <strong class="source-inline">kuma-system</strong> and install the Kuma control plane in that namespace, along with configuring various CRDs and <span class="No-Break">admission controllers.</span></p>
			<ol>
				<li value="3">At this point, we can access Kuma’s GUI using the <span class="No-Break">following command:</span><pre class="console">
<strong class="bold">% kubectl port-forward svc/kuma-control-plane -n kuma-system 5681:5681</strong></pre></li>
			</ol>
			<p>Open <strong class="source-inline">localhost:5681/gui</strong> in your browser and you will see the <span class="No-Break">following dashboard:</span></p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/Figure_13.07_B17989.jpg" alt="Figure A.7 – Kuma dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure A.7 – Kuma dashboard</p>
			<p>The<a id="_idIndexMarker1088"/> Kuma GUI <a id="_idIndexMarker1089"/>provides comprehensive details about the mesh. We will use this to check the configuration as we build policies and add applications to the mesh. On the home page of the GUI, you will notice that it shows one mesh called <strong class="bold">default</strong>. A mesh in Kuma is a Service Mesh that is logically isolated from other Service Meshes in Kuma. You can have one Kuma installation in a Kubernetes cluster, which can then manage multiple Service Meshes perhaps for each team or department deploying their apps in that Kubernetes cluster. This is a very important concept and a key differentiator of Kuma from other Service <span class="No-Break">Mesh technologies.</span></p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor197"/>Deploying envoydemo and curl in Kuma mesh</h2>
			<p>The<a id="_idIndexMarker1090"/> deployment file is available at <strong class="source-inline">AppendixA/Kuma/envoy-proxy-01.yaml</strong>. The noticeable difference compared to Istio in the <a id="_idIndexMarker1091"/>deployment file is the addition of the following label, which instructs Kuma to inject its sidecar proxies <span class="No-Break">into </span><span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
        <strong class="bold">kuma.io/sidecar-injection: enabled</strong></pre>
			<p>The<a id="_idIndexMarker1092"/> following commands will deploy the <strong class="source-inline">envoydummy</strong> and <span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break"> applications:</span></p>
			<pre class="console">
% kubectl create ns appendix-kuma
namespace/appendix-kuma created
% kubectl apply -f AppendixA/Kuma/envoy-proxy-01.yaml
configmap/envoy-dummy created
service/envoydummy created
deployment.apps/envoydummy created
serviceaccount/envoydummy created</pre>
			<p>After a few <a id="_idIndexMarker1093"/>seconds, check whether the Pods have been deployed and the sidecars injected using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
% kubectl get po -n appendix-kuma
NAME                          READY   STATUS    RESTARTS   AGE
curl                          2/2     Running   0          71s
envoydummy-767dbd95fd-tp6hr   2/2     Running   0          71s
serviceaccount/curl created
pod/curl created</pre>
			<p>The sidecars are also<a id="_idIndexMarker1094"/> called <strong class="bold">data plane proxies</strong> (<strong class="bold">DPPs</strong>), and they run along with every workload in the mesh. DPP comprises a data plane entity that defines the configuration of the DPP and a kuma-dp binary. During startup, kuma-dp retrieves the startup configuration for Envoy from the Kuma control plane (kuma-cp) and uses that to spawn the Envoy process. Once Envoy starts, it connects to kuma-cp using XDS. kuma-dp also spawns a core-dns process <span class="No-Break">at startup.</span></p>
			<p>It is worth noticing that installing Kuma and deploying an application has been a breeze. It is very simple, and the GUI is very intuitive, even <span class="No-Break">for beginners.</span></p>
			<p>Using the GUI, let’s check the overall status of <span class="No-Break">the mesh.</span></p>
			<p>From <strong class="bold">MESH</strong> | <strong class="bold">Overview</strong>, you can see newly <span class="No-Break">added DPPs:</span></p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/Figure_13.08_B17989.jpg" alt="Figure A.8 – Mesh overview in the Kuma GUI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure A.8 – Mesh overview in the Kuma GUI</p>
			<p>From <strong class="bold">MESH</strong> | <strong class="bold">Data Plane Proxies</strong>, you can find details about <span class="No-Break">the workloads:</span></p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/Figure_13.09_B17989.jpg" alt="Figure A.9 – Data plane proxies"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure A.9 – Data plane proxies</p>
			<p>Now that <a id="_idIndexMarker1095"/>we have installed apps, we will perform some<a id="_idIndexMarker1096"/> hands-on exercises with Kuma policies to get experience <span class="No-Break">with Kuma.</span></p>
			<p>We will start by accessing the <strong class="source-inline">envoydummy</strong> service from the <span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break"> Pod:</span></p>
			<pre class="console">
% kubectl exec -it pod/curl -n appendix-kuma -- curl http://envoydummy:80
V1----------Bootstrap Service Mesh Implementation with Istio----------V1%</pre>
			<p>The output is as expected. By default, Kuma allows traffic in and out of the mesh. By default, all traffic is unencrypted in the mesh. We will enable mTLS and deny all traffic in the mesh to establish zero-trust networking. First, we will delete the policy that allows all traffic within the mesh using the <span class="No-Break">following command:</span></p>
			<pre class="console">
% kubectl delete trafficpermissions/allow-all-traffic</pre>
			<p><strong class="source-inline">allow-all-traffic</strong> is a traffic permission policy that allows all traffic within the mesh.  The previous command deletes the policy, thereby restricting all traffic in <span class="No-Break">the mesh.</span></p>
			<p>Next, we<a id="_idIndexMarker1097"/> will enable mTLS within the mesh to enable secure communication <a id="_idIndexMarker1098"/>and let <strong class="source-inline">kong-dp</strong> correctly identify a service by comparing the service identity with the DPP certificate. Without enabling mTLS, Kuma cannot enforce traffic permissions. The following policy enables mTLS in the default mesh. It makes use of an inbuilt CA, but in case you want to use an external CA then there are also provisions to provide externally generated root CA and key. Kuma automatically generates certificates for every workload with SAN in <span class="No-Break">SPIFEE format.</span></p>
			<pre class="source-code">
apiVersion: kuma.io/v1alpha1
kind: Mesh
metadata:
  name: <strong class="bold">default</strong>
spec:
  mtls:
    enabledBackend: <strong class="bold">ca-1</strong>
    backends:
    - name: <strong class="bold">ca-1</strong>
      type: <strong class="bold">builtin</strong></pre>
			<p>In the config file, we have defined that this policy applies to the <strong class="source-inline">default</strong> mesh. We have declared a CA named <strong class="source-inline">ca-1</strong> of the <strong class="source-inline">builtin</strong> type and have configured it to be used as the root CA for mTLS by defining <strong class="source-inline">enabledBackend</strong>. The configuration file is available at <strong class="source-inline">AppendixA/Kuma/enablemutualTLS.yaml</strong>. You can apply the configuration using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
% kubectl apply -f AppendixA/Kuma/enablemutualTLS.yaml</pre>
			<p>After enabling mTLS, let’s try to access <strong class="source-inline">envoydummy</strong> <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
% kubectl exec -it pod/curl -n appendix-kuma -- curl http://envoydummy:80
curl: (52) Empty reply from server
command terminated with exit code 52</pre>
			<p>The output<a id="_idIndexMarker1099"/> is as expected because mTLS is enabled and there<a id="_idIndexMarker1100"/> is no <strong class="source-inline">TrafficPermission</strong> policy allowing the traffic between <strong class="source-inline">curl</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break">.</span></p>
			<p>To allow traffic, we need to create the following <span class="No-Break"><strong class="source-inline">TrafficPermission</strong></span><span class="No-Break"> policy:</span></p>
			<pre class="source-code">
apiVersion: kuma.io/v1alpha1
kind: TrafficPermission
mesh: default
metadata:
  name: allow-all-traffic-from-curl-to-envoyv1
spec:
  sources:
    - match:
        kuma.io/service: 'curl_appendix-kuma_svc'
  destinations:
    - match:
        kuma.io/service: 'envoydummy_appendix-kuma_svc_80'</pre>
			<p>Note that the <strong class="source-inline">kuma.io/service</strong> fields contain the values of the corresponding tags. Tags are sets of key-value pairs that contain details of the service that the DPP is part of and metadata about the exposed service. The following are tags applied to DPP <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
% kubectl get dataplane/envoydummy-767dbd95fd-tp6hr -n appendix-kuma -o json | jq '.spec.networking.inbound[].tags'
{
  "k8s.kuma.io/namespace": "appendix-kuma",
  "k8s.kuma.io/service-name": "envoydummy",
  "k8s.kuma.io/service-port": "80",
  "kuma.io/protocol": "http",
  "kuma.io/service": "envoydummy_appendix-kuma_svc_80",
  "name": "envoydummy",
  "pod-template-hash": "767dbd95fd",
  "version": "v1"
}</pre>
			<p>Similarly, you<a id="_idIndexMarker1101"/> can fetch the value of the <strong class="source-inline">curl</strong> DPP. The <a id="_idIndexMarker1102"/>configuration file is available at <strong class="source-inline">AppendixA/Kuma/allow-traffic-curl-to-envoyv1.yaml</strong>. Apply the configuration using the <span class="No-Break">following command:</span></p>
			<pre class="console">
% kubectl apply -f AppendixA/Kuma/allow-traffic-curl-to-envoyv1.yaml
trafficpermission.kuma.io/allow-all-traffic-from-curl-to-envoyv1 created</pre>
			<p>After applying the configuration, test that you can access <strong class="source-inline">envoydummy</strong> <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
% kubectl exec -it pod/curl -n appendix-kuma -- curl http://envoydummy:80
V1----------Bootstrap Service Mesh Implementation with Istio----------V1%</pre>
			<p>We have just experienced how you can control traffic between workloads in the mesh. You will find this very similar to <strong class="source-inline">ServiceIntentions</strong> in Consul <span class="No-Break">Service Mesh.</span></p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor198"/>Traffic management and routing</h2>
			<p>Now we will <a id="_idIndexMarker1103"/>explore traffic routing in Kuma. We will deploy version v2 of the <strong class="source-inline">envoydummy</strong> service and route certain requests between version v1 <span class="No-Break">and v2.</span></p>
			<p>The first step is to deploy version v2 of <strong class="source-inline">envoydummy</strong>, followed by defining traffic permission to allow traffic between the <strong class="source-inline">curl</strong> Pod and the <strong class="source-inline">envoydummy</strong> v2 Pod. The files are at <strong class="source-inline">AppendixA/Kuma/envoy-proxy-02.yaml</strong> and <strong class="source-inline">AppendixA/Kuma/allow-traffic-curl-to-envoyv2.yaml</strong>. Apply the configuration, and once you have applied both files, test that <strong class="source-inline">curl</strong> is able to reach both v1 and v2 of the <span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break"> Pod:</span></p>
			<pre class="console">
% for ((i=0;i&lt;2;i++)); do kubectl exec -it pod/curl -n appendix-kuma -- curl http://envoydummy ;done
V2----------Bootstrap Service Mesh Implementation with Istio----------V2
V1----------Bootstrap Service Mesh Implementation with Istio----------V1</pre>
			<p>Next, we will configure<a id="_idIndexMarker1104"/> the routing by using a <a id="_idIndexMarker1105"/>Kuma policy called <strong class="source-inline">TrafficRoute</strong>. This policy allows us to configure rules for traffic routing in <span class="No-Break">the mesh.</span></p>
			<p>The policy can be split into four parts to make it easier <span class="No-Break">to understand:</span></p>
			<ol>
				<li value="1">In the first part, we are declaring<a id="_idIndexMarker1106"/> the <strong class="source-inline">TrafficRoute</strong> policy. The basic usage of the policy is documented at <a href="https://kuma.io/docs/2.0.x/policies/traffic-route/">https://kuma.io/docs/2.0.x/policies/traffic-route/</a>. Here, we are declaring that the policy applies to the default mesh and  to any request in the mesh originating from <strong class="source-inline">curl_appendix-kuma_svc</strong> with a destination <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">envoydummy_appendix-kuma_svc_80</strong></span><span class="No-Break">:</span><pre class="console">
apiVersion: kuma.io/v1alpha1
kind: <strong class="bold">TrafficRoute</strong>
mesh: <strong class="bold">default</strong>
metadata:
  name: trafficroutingforlatest
spec:
  <strong class="bold">sources</strong>:
    - match:
        kuma.io/service: <strong class="bold">curl_appendix-kuma_svc</strong>
  <strong class="bold">destinations</strong>:
    - match:
        kuma.io/service: <strong class="bold">envoydummy_appendix-kuma_svc_80</strong></pre></li>
				<li>Next, we <a id="_idIndexMarker1107"/>are configuring any request with a prefix of <strong class="source-inline">'/latest'</strong> to be routed to the DPP with the tags that are highlighted <span class="No-Break">under </span><span class="No-Break"><strong class="source-inline">destination</strong></span><span class="No-Break">:</span><pre class="console">
 conf:
    http:
    - match:
        path:
          <strong class="bold">prefix: "/latest"</strong>
      destination:
        <strong class="bold">kuma.io/service:</strong> <strong class="bold">envoydummy_appendix-kuma_svc_80</strong>
        <strong class="bold">version: 'v2'</strong></pre></li>
				<li>Then, we are configuring request with a prefix of <strong class="source-inline">'/old'</strong> to be routed to the data plane with the tags that are highlighted <span class="No-Break">under </span><span class="No-Break"><strong class="source-inline">destination</strong></span><span class="No-Break">:</span><pre class="console">
    - match:
        path:
          <strong class="bold">prefix: "/old"</strong>
      destination:
        <strong class="bold">kuma.io/service: envoydummy_appendix-kuma_svc_80</strong>
        <strong class="bold">version: 'v1'</strong></pre></li>
				<li>Finally, we are declaring the default destination for requests that do not match any of the paths defined in previous parts of the config. The default destination will be the DPP with<a id="_idIndexMarker1108"/> the tags highlighted in the <span class="No-Break">following code:</span><pre class="console">
    destination:
      <strong class="bold">kuma.io/service: envoydummy_appendix-kuma_svc_80</strong></pre></li>
			</ol>
			<p>The configuration file is available at <strong class="source-inline">AppendixA/Kuma/trafficRouting01.yaml</strong>. Apply the configuration and test the <span class="No-Break">following scenarios:</span></p>
			<ul>
				<li>All requests with <strong class="source-inline">'/latest'</strong> should be routed to <span class="No-Break">version v2:</span><pre class="console">
<strong class="bold">% for ((i=0;i&lt;4;i++)); do kubectl exec -it pod/curl -n appendix-kuma -- curl http://envoydummy/latest ;done</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong></pre></li>
				<li>All request with <strong class="source-inline">'/old'</strong> should be routed to <span class="No-Break">version v1:</span><pre class="console">
<strong class="bold">% for ((i=0;i&lt;4;i++)); do kubectl exec -it pod/curl -n appendix-kuma -- curl http://envoydummy/old ;done</strong>
<strong class="bold">V1----------Bootstrap Service Mesh Implementation with Istio----------V1</strong>
<strong class="bold">V1----------Bootstrap Service Mesh Implementation with Istio----------V1</strong>
<strong class="bold">V1----------Bootstrap Service Mesh Implementation with Istio----------V1</strong>
<strong class="bold">V1----------Bootstrap Service Mesh Implementation with Istio----------V1</strong></pre></li>
				<li>All other<a id="_idIndexMarker1109"/> requests should follow the <span class="No-Break">default behavior:</span><pre class="console">
<strong class="bold">% for ((i=0;i&lt;4;i++)); do kubectl exec -it pod/curl -n appendix-kuma -- curl http://envoydummy/xyz ;done</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong>
<strong class="bold">V1----------Bootstrap Service Mesh Implementation with Istio----------V1</strong>
<strong class="bold">V1----------Bootstrap Service Mesh Implementation with Istio----------V1</strong></pre></li>
			</ul>
			<p>The request routing works as expected, and it is similar to how you would configure the same behavior using Istio. Now, let’s look at the load balancing properties of Kuma Mesh. We will build another traffic routing policy to do weighted routing between version v1 and v2 of <strong class="source-inline">envoydummy</strong>. Here is a snippet of the configuration available <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">AppendixA/Kuma/trafficRouting02.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
  conf:
    split:
      - weight: 10
        destination:
          kuma.io/service: envoydummy_appendix-kuma_svc_80
          version: 'v1'
      - weight: 90
        destination:
          kuma.io/service: envoydummy_appendix-kuma_svc_80
          version: 'v2'</pre>
			<p>After applying<a id="_idIndexMarker1110"/> the configuration, you can test the traffic distribution using the <span class="No-Break">following command:</span></p>
			<pre class="console">
% for ((i=0;i&lt;10;i++)); do kubectl exec -it pod/curl -n appendix-kuma -- curl http://envoydummy/xyz ;done</pre>
			<p>The traffic should be distributed between the two versions in approximately a 1:9 ratio. You can perform traffic routing, traffic modification, traffic splitting, load balancing, canary deployments, and locality-aware load balancing <a id="_idIndexMarker1111"/>using a <strong class="source-inline">TrafficRoute</strong> policy. To read more about <strong class="source-inline">TrafficRoute</strong>, please use the comprehensive documentation available <span class="No-Break">here: https://kuma.io/docs/2.0.x/policies/traffic-route.</span></p>
			<p>Kuma also provides policies for circuit breaking, fault injection, timeout, rate limiting, and many more things. A comprehensive list of <a id="_idIndexMarker1112"/>Kuma policies is available here: <a href="https://kuma.io/docs/2.0.x/policies/introduction/">https://kuma.io/docs/2.0.x/policies/introduction/</a>. These out-of-the-box policies make Kuma very easy to use with a very shallow <span class="No-Break">learning curve.</span></p>
			<p>In the hands-on example so far, we have been deploying all workloads in the default mesh. We discussed earlier that Kuma allows you to create different isolated meshes, allowing teams to have isolated mesh environments within the same Kuma cluster. You can create a new mesh using the <span class="No-Break">following configuration:</span></p>
			<pre class="source-code">
apiVersion: kuma.io/v1alpha1
kind: Mesh
metadata:
  name: team-digital</pre>
			<p>The configuration is available in <strong class="source-inline">AppendixA/Kuma/team-digital-mesh.yaml</strong>. Apply the configuration using the <span class="No-Break">following command:</span></p>
			<pre class="console">
% kubectl apply -f AppendixA/Kuma/team-digital-mesh.yaml
mesh.kuma.io/team-digital created</pre>
			<p>Once you have created the mesh, you can create all the resources within the mesh by adding the following annotations to the workload <span class="No-Break">deployment configurations:</span></p>
			<pre class="source-code">
kuma.io/mesh: team-digital</pre>
			<p>And add the following to the <span class="No-Break">Kuma policies:</span></p>
			<pre class="source-code">
mesh: team-digital</pre>
			<p>The ability<a id="_idIndexMarker1113"/> to create a mesh is a very useful feature for enterprise environments and a key differentiator of Kuma compared <span class="No-Break">to Istio.</span></p>
			<p>Kuma also provides built-in Ingress capabilities to handle north-south traffic as well as east-west traffic. The Ingress is managed as a Kuma resource called a gateway, which in turn is an instance of kuma-dp. You have the flexibility to deploy as many Kuma gateways as you want, but ideally, one gateway per mesh is recommended. Kuma also supports integration with non-Kuma gateways, also called delegated gateways. For now, we will talk about built-in Kuma gateways and, later, briefly discuss <span class="No-Break">delegated gateways.</span></p>
			<p>To create a built-in gateway, you first need to<a id="_idIndexMarker1114"/> define <strong class="source-inline">MeshGatewayInstance</strong> along with a matching <strong class="source-inline">MeshGateway</strong>. <strong class="source-inline">MeshGatewayInstance</strong> provides the details of how a gateway instance should be instantiated. Here is an example configuration of <strong class="source-inline">MeshGatewayInstance</strong>, which is also available <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">AppendixA/Kuma/envoydummyGatewayInstance01.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
apiVersion: kuma.io/v1alpha1
kind: MeshGatewayInstance
metadata:
  name: envoydummy-gateway-instance
  namespace: appendix-kuma
spec:
  <strong class="bold">replicas: 1</strong>
  <strong class="bold">serviceType: LoadBalancer</strong>
  tags:
    <strong class="bold">kuma.io/service: envoydummy-edge-gateway</strong></pre>
			<p>In the config, we are setting that there will be <strong class="source-inline">1 replica</strong> and a <strong class="source-inline">serviceType</strong> of <strong class="source-inline">LoadBalancer</strong>, and we have applied a tag, <strong class="source-inline">kuma.io/service: envoydummy-edge-gateway</strong>, which will be used to build the association <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">MeshGateway</strong></span><span class="No-Break">.</span></p>
			<p>In the following <a id="_idIndexMarker1115"/>configuration, we are creating a <strong class="source-inline">MeshGateway</strong> named <strong class="source-inline">envoydummy-edge-gateway</strong>. The configuration is available <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">AppendixA/Kuma/envoydummyGateway01.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
apiVersion: kuma.io/v1alpha1
kind: MeshGateway
mesh: default
metadata:
  name: <strong class="bold">envoydummy-edge-gateway</strong>
  namespace: appendix-kuma
spec:
  selectors:
  - match:
      kuma.io/service: <strong class="bold">envoydummy-edge-gateway</strong>
  conf:
    <strong class="bold">listeners</strong>:
      - <strong class="bold">port: 80</strong>
        <strong class="bold">protocol: HTTP</strong>
        <strong class="bold">hostname: mockshop.com</strong>
        tags:
          port: http/80</pre>
			<p>The <strong class="source-inline">MeshGateway</strong> resource<a id="_idIndexMarker1116"/> specifies the listeners, which are endpoints that accept network traffic. In the configuration, you specify ports, protocols, and an optional hostname. Under <strong class="source-inline">selectors</strong>, we are also specifying the <strong class="source-inline">MeshGatewayInstance</strong> tags with which the <strong class="source-inline">MeshGateway</strong> configuration is associated. Notice that we are specifying the same <a id="_idIndexMarker1117"/>tags we defined in the <span class="No-Break"><strong class="source-inline">MeshGatewayInstance</strong></span><span class="No-Break"> configuration.</span></p>
			<p>Next, we will define <strong class="source-inline">MeshGatewayRoute</strong>, which<a id="_idIndexMarker1118"/> describes how a request is routed from <strong class="source-inline">MeshGatewayInstance</strong> to the workload service. An example configuration is available at <strong class="source-inline">AppendixA/Kuma/envoydummyGatewayRoute01.yaml</strong>. Here are some snippets from <span class="No-Break">the file:</span></p>
			<ul>
				<li>Under <strong class="source-inline">selectors</strong>, we are specifying the details of the gateway and the listener to which this route should be attached. The details are specified by providing the tags of the corresponding gateway <span class="No-Break">and listeners:</span><pre class="console">
spec:
  selectors:
    - match:
        <strong class="bold">kuma.io/service: envoydummy-edge-gateway</strong>
        <strong class="bold">port: http/80</strong></pre></li>
				<li>In the <strong class="source-inline">conf</strong> part, we provide Layer 7 matching criteria for the request, such as the path and HTTP <a id="_idIndexMarker1119"/>headers, and the <span class="No-Break">destination details:</span><pre class="console">
  conf:
    http:
      rules:
        - matches:
            - path:
                <strong class="bold">match: PREFIX</strong>
                <strong class="bold">value: /</strong>
          backends:
            - <strong class="bold">destination</strong>:
                <strong class="bold">kuma.io/service: envoydummy_appendix-kuma_svc_80</strong></pre></li>
				<li>And last but not least, we allow traffic between the edge gateway and the envoy dummy <a id="_idIndexMarker1120"/>service by configuring <strong class="source-inline">TrafficPermission</strong> as described in following snippet. You can find the configuration <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">AppendixA/Kuma/allow-traffic-edgegateway-to-envoy.yaml</strong></span><span class="No-Break">:</span><pre class="console">
kind: TrafficPermission
mesh: default
metadata:
  name: allow-all-traffic-from-curl-to-envoyv1
spec:
  sources:
    - match:
        <strong class="bold">kuma.io/service: 'envoydummy-edge-gateway'</strong>
  destinations:
    - match:
        kuma<strong class="bold">.io/service: 'envoydummy_appendix-kuma_svc_80'</strong></pre></li>
			</ul>
			<p>With traffic<a id="_idIndexMarker1121"/> permission in place, we are now ready to apply the configuration using the following set <span class="No-Break">of commands:</span></p>
			<ol>
				<li value="1"><span class="No-Break">Create </span><span class="No-Break"><strong class="source-inline">MeshGatewayInstance</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% kubectl apply -f AppendixA/Kuma/envoydummyGatewayInstance01.yaml</strong>
<strong class="bold">meshgatewayinstance.kuma.io/envoydummy-gateway-instance created</strong></pre></li>
				<li><span class="No-Break">Create </span><span class="No-Break"><strong class="source-inline">MeshGateway</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% kubectl apply -f AppendixA/Kuma/envoydummyGateway01.yaml</strong>
<strong class="bold">meshgateway.kuma.io/envoydummy-edge-gateway created</strong></pre></li>
				<li><span class="No-Break">Create </span><span class="No-Break"><strong class="source-inline">MeshGatewayRoute</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% kubectl apply -f AppendixA/Kuma/envoydummyGatewayRoute01.yaml</strong>
<strong class="bold">meshgatewayroute.kuma.io/envoydummy-edge-gateway-route created</strong></pre></li>
				<li><span class="No-Break">Create </span><span class="No-Break"><strong class="source-inline">TrafficPermissions</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">$ kubectl apply -f AppendixA/Kuma/allow-traffic-edgegateway-to-envoy.yaml</strong>
<strong class="bold">trafficpermission.kuma.io/allow-all-traffic-from-curl-to-envoyv1 configured</strong></pre></li>
			</ol>
			<p>You can verify <a id="_idIndexMarker1122"/>that Kuma has created a gateway instance using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
% kubectl get po -n appendix-kuma
NAME                         READY   STATUS    RESTARTS   AGE
curl                         2/2     Running   0          22h
envoydummy-767dbd95fd-br2m6                    2/2     Running   0          22h
envoydummy-gateway-instance-75f87bd9cc-z2rx6   1/1     Running   0          93m
envoydummy2-694cbc4f7d-hrvkd                   2/2     Running   0          22h</pre>
			<p>You can also check the corresponding service using the <span class="No-Break">following command:</span></p>
			<pre class="console">
% kubectl get svc -n appendix-kuma
NAME                  TYPE   CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
envoydummy       ClusterIP      10.102.50.112   &lt;none&gt;        80/TCP         22h
envoydummy-gateway-instance   LoadBalancer   10.101.49.118   &lt;pending&gt;     80:32664/TCP   96m</pre>
			<p>We are now all set to access <strong class="source-inline">envoydummy</strong> using the built-in Kuma gateway. But first, we need to find an IP address through which we can access the Ingress gateway service on minikube. Use<a id="_idIndexMarker1123"/> the following command to find the <span class="No-Break">IP address:</span></p>
			<pre class="console">
% minikube service envoydummy-gateway-instance --url -n appendix-kuma
http://127.0.0.1:52346</pre>
			<p>Now, using http://127.0.0.1:52346, you can access the <strong class="source-inline">envoydummy</strong> service by performing <strong class="source-inline">curl</strong> from <span class="No-Break">your terminal:</span></p>
			<pre class="console">
% curl -H "host:mockshop.com" http://127.0.0.1:52346/latest
V1----------Bootstrap Service Mesh Implementation with Istio----------V1</pre>
			<p>You have learned how to create a <strong class="source-inline">MeshGatewayInstance</strong>, which is then associated with <strong class="source-inline">MeshGateway</strong>. After the association, kuma-cp created a gateway instance of the built-in Kuma gateway. We then created a <strong class="source-inline">MeshGatewayRoute</strong> that specifies how the request will be routed from the gateway to the workload service. Later, we created a <strong class="source-inline">TrafficPermission</strong> resource to allow traffic flow from <strong class="source-inline">MeshGateway</strong> to the <span class="No-Break"><strong class="source-inline">EnvoyDummy</strong></span><span class="No-Break"> workload.</span></p>
			<p>Kuma also provides options for using <a id="_idIndexMarker1124"/>an external gateway as Ingress, also called a delegated gateway. In a delegated gateway, Kuma supports integrations with various API gateways, but Kong Gateway is the preferred and most well-documented option. You can read more <a id="_idIndexMarker1125"/>about delegated gateways <span class="No-Break">at </span><a href="https://kuma.io/docs/2.0.x/explore/gateway/#delegated"><span class="No-Break">https://kuma.io/docs/2.0.x/explore/gateway/#delegated</span></a><span class="No-Break">.</span></p>
			<p>Like Istio, Kuma also provides native support for both Kubernetes and VM-based workloads. Kuma provides extensive support for running advanced configurations of Service Mesh spanning multiple Kubernetes clusters, data centers, and cloud providers. Kuma has a concept of zones, which are logical aggregations of DPPs that can communicate with each other. Kuma supports running Service Mesh in multiple zones and the separation of control planes in a multi-zone deployment. Each zone is allocated its own horizontally scalable control plane providing complete isolation between every zone. All zones are then also managed by a centralized global control plane, which manages the creation of and changes to policies that are applied to DPPs and the transmission of zone-specific policies and configurations to respective control planes of underlying zones. The global control plane is a single pane of glass providing an inventory of all DPPs across <span class="No-Break">all zones.</span></p>
			<p>As mentioned <a id="_idIndexMarker1126"/>earlier, Kuma is an open source project that was donated by Kong to CNCF. Kong also provides Kong Mesh, which is an enterprise version of Kuma built on top of Kuma, extending it to include capabilities required for running critical functionality for enterprise workloads. Kong Mesh provides a turnkey Service Mesh solution with capabilities such as integration with OPA, FIPS 140-2 compliance, and role-based access control. Coupled with Kong Gateway as an Ingress gateway, a Service Mesh based on Kuma, additional enterprise-grade add-ons and reliable enterprise support makes Kong Mesh a turnkey Service <span class="No-Break">Mesh technology.</span></p>
			<p class="callout-heading">Uninstalling Kuma</p>
			<p class="callout">You can <a id="_idIndexMarker1127"/>uninstall Kuma Mesh using the <span class="No-Break">following command:</span></p>
			<p class="callout"><strong class="source-inline">% kumactl install control-plane | kubectl delete -</strong><span class="No-Break"><strong class="source-inline">f -</strong></span></p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor199"/>Linkerd</h1>
			<p>Linkerd is a<a id="_idIndexMarker1128"/> CNCF graduated project licensed under Apache v2. Buoyant (<a href="https://buoyant.io/">https://buoyant.io/</a>) is<a id="_idIndexMarker1129"/> the <a id="_idIndexMarker1130"/>major contributor to Linkerd. Out of all Service Mesh technologies, Linkerd is probably one of the earliest, if not the oldest. It was initially made public in 2017 by Buoyant. It had initial success, but then it was criticized for being very resource hungry. The proxy used in Linkerd was written using the Scala and Java networking ecosystem, which uses <a id="_idIndexMarker1131"/>the <strong class="bold">Java Virtual Machine</strong> (<strong class="bold">JVM</strong>) at runtime, causing significant resource consumption. In 2018, Buoyant released a new version of Linkerd called Conduit. Conduit<a id="_idIndexMarker1132"/> was later<a id="_idIndexMarker1133"/> renamed Linkerd v2. The Linkerd v2 data plane is made up of <a id="_idIndexMarker1134"/>Linkerd2-proxy, which is written in Rust and has a small resource consumption footprint.  Linkerd2- proxy is purpose built for proxying as a sidecar in Kubernetes Pods. While Linkerd2-proxy is written in Rust, the Linkerd control plane is developed <span class="No-Break">in Golang.</span></p>
			<p>Like other open source Service Mesh technologies discussed in this <em class="italic">Appendix</em>, we will discover Linkerd by playing around with it and observing how it is similar to or different to Istio. Let’s start by<a id="_idIndexMarker1135"/> installing Linkerd <span class="No-Break">on minikube:</span></p>
			<ol>
				<li value="1">Install Linkerd on minikube using the <span class="No-Break">following command:</span><pre class="console">
<strong class="bold">% curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh</strong>
<strong class="bold">Downloading linkerd2-cli-stable-2.12.3-darwin...</strong>
<strong class="bold">Linkerd stable-2.12.3 was successfully installed</strong>
<strong class="bold">Add the linkerd CLI to your path with:</strong>
<strong class="bold">  export PATH=$PATH:/Users/arai/.linkerd2/bin</strong></pre></li>
				<li>Follow the suggestion to include linkerd2 in <span class="No-Break">your path:</span><pre class="console">
<strong class="bold">export PATH=$PATH:/Users/arai/.linkerd2/bin</strong></pre></li>
				<li>Linkerd provides an option to check and validate that the Kubernetes cluster meets all the prerequisites required to <span class="No-Break">install Linkerd:</span><pre class="console">
<strong class="bold">% linkerd check --pre</strong></pre></li>
				<li>If the output contains the following, then you are good to go with <span class="No-Break">the installation:</span><pre class="console">
Status check results are √</pre></li>
			</ol>
			<p>If not, then you need to resolve the issues by going through suggestions <span class="No-Break">at </span><a href="https://linkerd.io/2.12/tasks/troubleshooting/#pre-k8s-cluster-k8s%20for%20hints"><span class="No-Break">https://linkerd.io/2.12/tasks/troubleshooting/#pre-k8s-cluster-k8s%20for%20hints</span></a><span class="No-Break">.</span></p>
			<ol>
				<li value="5">Next, we will Install Linkerd in <span class="No-Break">two steps:</span><ol><li>First, we install <span class="No-Break">the CRDs:</span></li></ol><pre class="console">
<strong class="bold">% linkerd install --crds | kubectl apply -f -</strong></pre><ol><li value="2">The, we install the Linkerd control plane in the <span class="No-Break"><strong class="source-inline">linkerd</strong></span><span class="No-Break"> namespace:</span></li></ol><pre class="console">
<strong class="bold">% linkerd install --set proxyInit.runAsRoot=true | kubectl apply -f -</strong></pre></li>
				<li>After<a id="_idIndexMarker1136"/> installing the control plane, check that Linkerd is fully installed using the <span class="No-Break">following commands:</span><pre class="console">
<strong class="bold">% linkerd check</strong></pre></li>
			</ol>
			<p>If Linkerd is successfully installed, then you should see the <span class="No-Break">following message:</span></p>
			<pre class="console">
Status check results are √</pre>
			<p>That complete the setup of Linkerd! Let’s now analyze what has <span class="No-Break">been installed:</span></p>
			<pre class="console">
% kubectl get pods,services -n linkerd
NAME                    READY   STATUS    RESTARTS   AGE
pod/linkerd-destination-86d68bb57-447j6       4/4     Running   0          49m
pod/linkerd-identity-5fbdcccbd5-lzfkj         2/2     Running   0          49m
pod/linkerd-proxy-injector-685cd5988b-5lmxq   2/2     Running   0          49m
NAME   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/linkerd-dst           ClusterIP    10.102.201.182   &lt;none&gt;        8086/TCP   49m
service/linkerd-dst-headless        ClusterIP   None             &lt;none&gt;        8086/TCP   49m
service/linkerd-identity        ClusterIP   10.98.112.229    &lt;none&gt;       8080/TCP   49m
service/linkerd-identity-headless       ClusterIP   None             &lt;none&gt;        8090/TCP   49m
service/linkerd-policy           ClusterIP    None             &lt;none&gt;        8090/TCP   49m
service/linkerd-policy-validator    ClusterIP   10.102.142.68    &lt;none&gt;        443/TCP    49m
service/linkerd-proxy-injector      ClusterIP   10.101.176.198   &lt;none&gt;        443/TCP    49m
service/linkerd-sp-validator        ClusterIP    10.97.160.235    &lt;none&gt;        443/TCP    49m</pre>
			<p>It’s worth <a id="_idIndexMarker1137"/>noticing here that the control plane comprises many Pods and Services. The <strong class="source-inline">linkerd-identity</strong> service is a CA for generating signed certificates for Linkerd proxies. <strong class="source-inline">linkerd-proxy-injector</strong> is the Kubernetes admission controller responsible for modifying Kubernetes Pod specifications to add linkerd-proxy and proxy-init containers. The <strong class="source-inline">destination</strong> service is the brains of the Linkerd control plane and maintains service discovery and identity information about the services, along with policies for securing and managing the traffic in <span class="No-Break">the mesh.</span></p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor200"/>Deploying envoydemo and curl in Linkerd</h2>
			<p>Now let’s <a id="_idIndexMarker1138"/>deploy envoydummy and curl apps and check how Linkerd <a id="_idIndexMarker1139"/>performs Service Mesh functions. Follow these steps to install <span class="No-Break">the application:</span></p>
			<ol>
				<li value="1">Like most Service Mesh solutions, we need to annotate the deployment descriptors with the <span class="No-Break">following annotations:</span><pre class="console">
      annotations:
        linkerd.io/inject: enabled</pre></li>
			</ol>
			<p>The configuration file for the <strong class="source-inline">envoydummy</strong> and <strong class="source-inline">curl</strong> apps along with annotations is available <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">AppendixA/Linkerd/envoy-proxy-01.yaml</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="2">After preparing the deployment descriptors, you can apply <span class="No-Break">the configurations:</span><pre class="console">
<strong class="bold">% kubectl create ns appendix-linkerd</strong>
<strong class="bold">% kubectl apply -f AppendixA/Linkerd/envoy-proxy-01.yaml</strong></pre></li>
				<li>That <a id="_idIndexMarker1140"/>should deploy the Pod. Once the Pod is deployed, you<a id="_idIndexMarker1141"/> can check what has been injected into the Pods via the <span class="No-Break">following commands:</span><pre class="console">
% kubectl get po/curl -n appendix-linkerd -o json | jq '.spec.initContainers[].image, .spec.initContainers[].name'
"cr.l5d.io/linkerd/proxy-init:v2.0.0"
"linkerd-init"
% kubectl get po/curl -n appendix-linkerd -o json | jq '.spec.containers[].image, .spec.containers[].name'
"cr.l5d.io/linkerd/proxy:stable-2.12.3"
"curlimages/curl"
"linkerd-proxy"
"curl"</pre></li>
			</ol>
			<p>From the preceding output, observe that Pod initialization was performed by a container named <strong class="source-inline">linkerd-init</strong> of the <strong class="source-inline">cr.l5d.io/linkerd/proxy-init:v2.0.0</strong> type, and the Pod has two running containers, <strong class="source-inline">curl</strong> and <strong class="source-inline">linkerd-proxy</strong>, of the <strong class="source-inline">cr.l5d.io/linkerd/proxy:stable-2.12.3</strong> type. The <strong class="source-inline">linkerd-init</strong> container runs during the initialization phase of the Pod and modifies iptables rules to route all network traffic from <strong class="source-inline">curl</strong> to <strong class="source-inline">linkerd-proxy</strong>. As you may recall, in Istio we have <strong class="source-inline">istio-init</strong> and <strong class="source-inline">istio-proxy</strong> containers, which are similar to Linkerd containers. <strong class="source-inline">linkerd-proxy</strong> is ultra-light and ultra-fast in comparison to Envoy. Being written in Rust makes its performance predictable and it doesn’t need garbage collection, which often causes high latency during garbage collection passes. Rust is arguably much more memory safe than C++ and C, which makes it less susceptible to memory safety bugs. You can read more about why <strong class="source-inline">linkerd-proxy</strong> is better than <a id="_idIndexMarker1142"/>envoy <span class="No-Break">at </span><a href="https://linkerd.io/2020/12/03/why-linkerd-doesnt-use-envoy/"><span class="No-Break">https://linkerd.io/2020/12/03/why-linkerd-doesnt-use-envoy/</span></a><span class="No-Break">.</span></p>
			<p>Verify that <strong class="source-inline">curl</strong> is able to communicate with the <strong class="source-inline">envoydummy</strong> Pod <span class="No-Break">as follows:</span></p>
			<pre class="console">
% kubectl exec -it pod/curl -c curl -n appendix-linkerd -- curl http://envoydummy:80
V1----------Bootstrap Service Mesh Implementation with Istio----------V1%</pre>
			<p>Now that <a id="_idIndexMarker1143"/>we <a id="_idIndexMarker1144"/>have installed the <strong class="source-inline">curl</strong> and <strong class="source-inline">envoydummy</strong> Pods, let’s explore Linkerd Service Mesh functions. Let’s start by exploring how we can restrict traffic within the mesh <span class="No-Break">using Linkerd.</span></p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor201"/>Zero-trust networking</h2>
			<p>Linkerd provides <a id="_idIndexMarker1145"/>comprehensive policies to restrict traffic in the mesh. Linkerd provides a set of CRDs through which policies can be defined to control the traffic in the mesh. Let’s explore these policies by implementing policies to control traffic to the <span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break"> Pod:</span></p>
			<ol>
				<li value="1">We will first lock down all traffic in the cluster <span class="No-Break">using following:</span><pre class="console">
<strong class="bold">% linkerd upgrade --default-inbound-policy deny --set proxyInit.runAsRoot=true | kubectl apply -f -</strong></pre></li>
			</ol>
			<p>We used the <strong class="source-inline">linkerd upgrade</strong> command to apply a <strong class="source-inline">default-inbound-policy</strong> of <strong class="source-inline">deny</strong>, which prohibits all traffic to ports exposed by workloads in the mesh unless there is a server resource attached to <span class="No-Break">the port.</span></p>
			<p>After applying the policy, all access to the <strong class="source-inline">envoydummy</strong> service <span class="No-Break">is denied:</span></p>
			<pre class="console">
<strong class="bold">% kubectl exec -it pod/curl -c curl -n appendix-linkerd -- curl --head http://envoydummy:80</strong>
<strong class="bold">HTTP/1.1 403 Forbidden</strong>
<strong class="bold">content-length: 0</strong>
<strong class="bold">l5d-proxy-error: unauthorized request on route</strong></pre>
			<ol>
				<li value="2">Next, we create a server resource to describe the <strong class="source-inline">envoydummy</strong> port. A Server resource is a means of instructing Linkerd that only authorized clients can access the resource. We<a id="_idIndexMarker1146"/> do that by declaring the following <span class="No-Break">Linkerd policy:</span><pre class="console">
<strong class="bold">apiVersion: policy.linkerd.io/v1beta1</strong>
<strong class="bold">kind: Server</strong>
<strong class="bold">metadata:</strong>
<strong class="bold">  namespace: appendix-linkerd</strong>
<strong class="bold">  name: envoydummy</strong>
<strong class="bold">  labels:</strong>
<strong class="bold">    name: envoydummy</strong>
<strong class="bold">spec:</strong>
<strong class="bold">  podSelector:</strong>
<strong class="bold">    matchLabels:</strong>
<strong class="bold">      name: envoydummy</strong>
<strong class="bold">  port: envoydummy-http</strong>
<strong class="bold">  proxyProtocol: HTTP/1</strong></pre></li>
			</ol>
			<p>The configuration file is available at <strong class="source-inline">AppendixA/Linkerd/envoydummy-server.yaml</strong>. The server resource is defined in the same namespace as the workload. In the configuration file, we also define <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">podSelector</strong>: Criteria for selecting <span class="No-Break">the workload</span></li>
				<li><strong class="source-inline">port</strong>: Name or number of the port for which this server configuration is <span class="No-Break">being declared</span></li>
				<li><strong class="source-inline">proxyProtocol</strong>: Configures protocol discovery for inbound connections and must be one of the following: unknown, <strong class="source-inline">HTTP/1</strong>, <strong class="source-inline">HTTP/2</strong>, <strong class="source-inline">gRPC</strong>, <strong class="source-inline">opaque</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">TLS</strong></span></li>
			</ul>
			<p>Apply the server resource using the <span class="No-Break">following command:</span></p>
			<pre class="console">
<strong class="bold">% kubectl apply -f AppendixA/Linkerd/envoydummy-server.yaml</strong>
<strong class="bold">server.policy.linkerd.io/envoydummy created</strong></pre>
			<p>Although we have applied the server resource, the <strong class="source-inline">curl</strong> Pod still can’t access the <strong class="source-inline">envoydummy</strong> service unless we <span class="No-Break">authorize it.</span></p>
			<ol>
				<li value="3">In this step, we<a id="_idIndexMarker1147"/> will create an authorization policy that authorizes <strong class="source-inline">curl</strong> to access <strong class="source-inline">envoydummy</strong>. The authorization policy is configured by providing server details of the target destination and service account details being used to run the originating service. We created a server resource named <strong class="source-inline">envoydummy</strong> in the previous step and, as per <strong class="source-inline">AppendixA/Linkerd/envoy-proxy-01.yaml</strong>, we are using a service account named <strong class="source-inline">curl</strong> to run the <strong class="source-inline">curl</strong> Pod. The policy is defined as follows and is also available <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">AppendixA/Linkerd/authorize-curl-access-to-envoydummy.yaml</strong></span><span class="No-Break">:</span><pre class="console">
apiVersion: policy.linkerd.io/v1alpha1
kind: <strong class="bold">AuthorizationPolicy</strong>
metadata:
  name: authorise-curl
  namespace: appendix-linkerd
spec:
  targetRef:
    group: policy.linkerd.io
    kind: Server
    name: <strong class="bold">envoydummy</strong>
  requiredAuthenticationRefs:
    - name: <strong class="bold">curl</strong>
      kind: ServiceAccount</pre></li>
				<li>Apply the configuration <span class="No-Break">as follows:</span><pre class="console">
<strong class="bold">% kubectl apply -f AppendixA/Linkerd/authorize-curl-access-to-envoydummy.yaml</strong>
<strong class="bold">authorizationpolicy.policy.linkerd.io/authorise-curl created</strong></pre></li>
			</ol>
			<p>Once the <strong class="source-inline">AuthorizationPolicy</strong> is in place, it will authorize all traffic to the Envoy server from any workload running using a <strong class="source-inline">curl</strong> <span class="No-Break">service account.</span></p>
			<ol>
				<li value="5">You can verify <a id="_idIndexMarker1148"/>the access between the <strong class="source-inline">curl</strong> and <strong class="source-inline">envoydummy</strong> Pods using the <span class="No-Break">following command:</span><pre class="console">
<strong class="bold">% kubectl exec -it pod/curl -c curl -n appendix-linkerd – curl  </strong>http://envoydummy<strong class="bold">:80</strong>
<strong class="bold">V1----------Bootstrap Service Mesh Implementation with Istio----------V1</strong></pre></li>
			</ol>
			<p>Using <strong class="source-inline">AuthorizationPolicy</strong>, we have controlled access to ports presented as servers from other clients in the mesh. Granular access control, such as controlling access to an HTTP resource, can be managed by another policy <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">s</strong></span><span class="No-Break">.</span></p>
			<p>We can understand this concept better via an example, so let’s make a requirement that only requests whose URI start with <strong class="source-inline">/dummy</strong> can be accessible from <strong class="source-inline">curl</strong>; requests to any other URI must be denied. Let’s <span class="No-Break">get started:</span></p>
			<ol>
				<li value="1">We need to first define an <strong class="source-inline">HTTPRoute</strong> policy as described in the following <span class="No-Break">code snippet:</span><pre class="console">
apiVersion: policy.linkerd.io/v1beta1
kind: HTTPRoute
metadata:
  name: envoydummy-dummy-route
  namespace: appendix-linkerd
spec:
  parentRefs:
    - name: envoydummy
      kind: Server
      group: policy.linkerd.io
      namespace: appendix-linkerd
  rules:
    - matches:
      - path:
          value: "/dummy/"
          type: "PathPrefix"
        method: GET</pre></li>
			</ol>
			<p>The <a id="_idIndexMarker1149"/>configuration is also available at <strong class="source-inline">AppendixA/Linkerd/HTTPRoute.yaml</strong>. This will create an HTTP route targeting the <strong class="source-inline">envoydummy</strong> server resource. In the <strong class="source-inline">rules</strong> section, we define the criteria for identifying requests that will be used to identify the HTTP request for this route. Here, we have defined to rule to match any request with the <strong class="source-inline">dummy</strong> prefix and the <strong class="source-inline">GET</strong> method. <strong class="source-inline">HTTPRoute</strong> also supports route matching using headers and query parameters. You can also apply other filters in <strong class="source-inline">HTTPRoute</strong> to specify how the request should be processed during the request or response cycle; you can modify inbound request headers, redirect requests, modify request paths, and <span class="No-Break">so on.</span></p>
			<ol>
				<li value="2">Once we have defined <strong class="source-inline">HTTPRoute</strong>, we can modify the <strong class="source-inline">AuthorizationPolicy</strong> to associate with <strong class="source-inline">HTTPRoute</strong> instead of the server, as listed in the following code snippet and also available <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">AppendixA/Linkerd/HttpRouteAuthorization.yaml</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">apiVersion: policy.linkerd.io/v1alpha1</strong>
<strong class="bold">kind: AuthorizationPolicy</strong>
<strong class="bold">metadata:</strong>
<strong class="bold">  name: authorise-curl</strong>
<strong class="bold">  namespace: appendix-linkerd</strong>
<strong class="bold">spec:</strong>
<strong class="bold">  targetRef:</strong>
<strong class="bold">    group: policy.linkerd.io</strong>
<strong class="bold">    kind: </strong><strong class="source-inline">HTTPRoute</strong>
<strong class="bold">    name: </strong><strong class="source-inline">envoydummy-dummy-route</strong>
<strong class="bold">  requiredAuthenticationRefs:</strong>
<strong class="bold">    - name: curl</strong>
<strong class="bold">      kind: ServiceAccount</strong></pre></li>
			</ol>
			<p>The <a id="_idIndexMarker1150"/>configuration updates <strong class="source-inline">AuthorizationPolicy</strong> and, instead of referencing the server (<strong class="source-inline">envoydummy</strong> configured in <strong class="source-inline">AppendixA/Linkerd/authorize-curl-access-to-envoydummy.yaml</strong>) as the target, the policy is now referencing <strong class="source-inline">HTTPRoute</strong> (<span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">envoydummy-dummy-route</strong></span><span class="No-Break">).</span></p>
			<p>Apply both configurations and test that you are able to make requests with the <strong class="source-inline">/dummy</strong> prefix in the URI. Any other request will be denied <span class="No-Break">by Linkerd.</span></p>
			<p>So far in <strong class="source-inline">AuthorizationPolicy</strong> we have used <strong class="source-inline">ServiceAccount</strong> authentication. <strong class="source-inline">AuthorizationPolicy</strong> also supports <strong class="source-inline">MeshTLSAuthentication</strong> and <strong class="source-inline">NetworkAuthentication</strong>. Here is a brief overview of these <span class="No-Break">authentication types:</span></p>
			<ul>
				<li><strong class="source-inline">MeshTLSAuthentication</strong> is used to identify a client based on its mesh identity. For example, the <strong class="source-inline">curl</strong> Pod will be represented <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">curl.appendix-linkerd.serviceaccount.identity.linkerd.local</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">NetworkAuthentication</strong> is used to identify a client based on its network location <a id="_idIndexMarker1151"/>using <strong class="bold">Classless Inter-Domain Routing</strong> (<span class="No-Break"><strong class="bold">CIDR</strong></span><span class="No-Break">) blocks.</span></li>
			</ul>
			<p>Linkerd also provides retries and timeouts to provide application resilience when systems are under stress or suffering partial failures. Apart from support for usual retry strategies, there is also a provision for specifying retry budgets so that retries do not end up amplifying resilience problems. Linkerd provides automated load balancing of requests to all destination endpoints using the <strong class="bold">exponentially weighted moving average</strong> (<strong class="bold">EWMA</strong>) algorithm. Linkerd<a id="_idIndexMarker1152"/> supports weight-based traffic splitting, which is useful for performing canary and blue/green deployments. Traffic splitting in Linkerd uses<a id="_idIndexMarker1153"/> the <strong class="bold">Service Mesh Interface</strong> (<strong class="bold">SMI</strong>) Traffic Split API, allowing users to incrementally shift traffic between blue and green services. You can read about the Traffic Split API at <a href="https://github.com/servicemeshinterface/smi-spec/blob/main/apis/traffic-split/v1alpha4/traffic-split.md">https://github.com/servicemeshinterface/smi-spec/blob/main/apis/traffic-split/v1alpha4/traffic-split.md</a> and SMI at <a href="https://smi-spec.io">https://smi-spec.io</a>. Linkerd provides a well-defined and documented integration with Flagger to perform automatic traffic shifting when performing canary and <span class="No-Break">blue/green deployments.</span></p>
			<p>There is a lot <a id="_idIndexMarker1154"/>more to learn and digest about Linkerd. You can read about it at <a href="https://linkerd.io/2.12">https://linkerd.io/2.12</a>. Linkerd is ultra-performant because of its ultra-light service proxy build using Rust. It is carefully designed to solve application networking problems. The ultra-light proxy performs most Service Mesh functions but lacks in features such as circuit breaking and rate limiting. Let’s hope that the Linkerd creators bridge the gap <span class="No-Break">with Envoy.</span></p>
			<p>Hopefully, you are now familiar with the various alternatives to Istio and how they implement Service Mesh. Consul, Linkerd, Kuma, and Gloo Mesh have lots of similarities, and all of them are powerful, but Istio is the one that has the biggest community behind it and the support of various well-known organizations. Also, there are various organizations that provide enterprise support for Istio, which is a very important consideration when deploying Istio <span class="No-Break">to production.</span></p>
		</div>
	</body></html>