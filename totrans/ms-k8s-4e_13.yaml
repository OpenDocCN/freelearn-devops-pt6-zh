- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Monitoring Kubernetes Clusters
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控Kubernetes集群
- en: In the previous chapter, we looked at serverless computing and its manifestations
    on Kubernetes. A lot of innovation happens in this space, and it is both super
    useful and fascinating to follow the evolution.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了无服务器计算及其在Kubernetes上的表现。这个领域充满了创新，跟踪其发展既非常有用，又令人着迷。
- en: In this chapter, we’re going to talk about how to make sure your systems are
    up and running and performing correctly, and how to respond when they’re not.
    In *Chapter 3*, *High Availability and Reliability*, we discussed related topics.
    The focus here is on knowing what’s going on in your system and what practices
    and tools you can use.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论如何确保你的系统正常运行并且表现良好，以及当它们出现问题时该如何响应。在*第3章*，*高可用性和可靠性*中，我们讨论了相关话题。这里的重点是了解你的系统发生了什么，并且可以使用哪些实践和工具。
- en: There are many aspects to monitoring, such as logging, metrics, distributed
    tracing, error reporting, and alerting. Practices like auto-scaling and self-healing
    depend on monitoring to detect that there is a need to scale or to heal.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 监控有很多方面，比如日志记录、度量、分布式追踪、错误报告和警报。像自动扩展和自我修复等实践依赖于监控来检测是否需要扩展或修复。
- en: 'The topics we will cover in this chapter include:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论的内容包括：
- en: Understanding observability
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解可观察性
- en: Logging with Kubernetes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kubernetes记录日志
- en: Recording metrics with Kubernetes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kubernetes记录度量
- en: Distributed tracing with Jaeger
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Jaeger进行分布式追踪
- en: Troubleshooting problems
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排查问题
- en: 'The Kubernetes community recognizes the importance of monitoring and has put
    a lot of effort to make sure Kubernetes has a solid monitoring story. The **Cloud
    Native Computing Foundation** (**CNCF**) is the de-facto curator of cloud-native
    infrastructure projects. It has graduated twenty projects so far. Kubernetes was
    the first project to graduate, and out of the early graduated projects, three
    other projects that graduated more than two years ago are focused on monitoring:
    Prometheus, Fluentd, and Jaeger. This means that monitoring and observability
    are the foundation for a large-scale Kubernetes-based system. Before we dive into
    the ins and out of Kubernetes monitoring and specific projects and tools, we should
    get a better understanding of what monitoring is all about. A good framework for
    thinking about monitoring is how observable your system is.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes社区认识到监控的重要性，并投入了大量精力确保Kubernetes拥有一个完善的监控方案。**云原生计算基金会**（**CNCF**）是云原生基础设施项目的事实上的管理机构。到目前为止，它已经毕业了二十个项目。Kubernetes是第一个毕业的项目，在早期毕业的项目中，另外三个超过两年毕业的项目专注于监控：Prometheus、Fluentd和Jaeger。这意味着监控和可观察性是大规模基于Kubernetes的系统的基础。在深入探讨Kubernetes监控以及具体项目和工具之前，我们应该更好地理解监控的含义。一个好的思考监控的框架是：你的系统有多可观察。
- en: Understanding observability
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解可观察性
- en: Observability is a big word. What does it mean in practice? There are different
    definitions out there and big debates about how monitoring and observability are
    similar and different. I take the stance that observability is the property of
    the system that defines what we can tell about the state and behavior of the system
    right now and historically. In particular, we are interested in the health of
    the system and its components. Monitoring is the collection of tools, processes,
    and techniques that we use to increase the observability of the system.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察性是一个大词。它在实践中意味着什么？有不同的定义，并且关于监控和可观察性之间的相似性与差异存在广泛的争论。我认为可观察性是系统的一个属性，它定义了我们现在以及历史上能知道系统的状态和行为。特别是，我们关注的是系统及其组件的健康状况。监控是我们用来提高系统可观察性的工具、过程和技术的集合。
- en: There are different facets of information that we need to collect, record, and
    aggregate in order to get a good sense of what our system is doing. Those facets
    include logs, metrics, distributed traces, and errors. The monitoring or observability
    data is multidimensional and crosses many levels. Just collecting it doesn’t help
    much. We need to be able to query it, visualize it, and alert other systems when
    things go wrong. Let’s review the various components of observability.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要收集、记录和聚合不同方面的信息，以便更好地了解系统的运行情况。这些方面包括日志、度量、分布式追踪和错误。监控或可观察性数据是多维的，跨越多个层次。仅仅收集数据并不能带来太大帮助。我们需要能够查询、可视化这些数据，并在出现问题时向其他系统发出警报。让我们回顾一下可观察性的各个组件。
- en: Logging
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志记录
- en: Logging is a key monitoring tool. Every self-respecting long-running software
    must have logs. Logs capture timestamped events. They are critical for many applications
    like business intelligence, security, compliance, audits, debugging, and troubleshooting.
    It’s important to understand that a complicated distributed system will have different
    logs for different components, and extracting insights from logs is not a trivial
    undertaking.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 日志是一个关键的监控工具。每个自尊的长期运行的软件都必须有日志。日志捕捉带时间戳的事件。它们对许多应用程序至关重要，如商业智能、安全性、合规性、审计、调试和故障排除。重要的是要理解，复杂的分布式系统会为不同组件生成不同的日志，从日志中提取洞察不是一件简单的事。
- en: 'There are several key attributes to logs: format, storage, and aggregation.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 日志有几个关键属性：格式、存储和聚合。
- en: Log format
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志格式
- en: Logs may come in various formats. Plain text is very common and human-readable
    but requires a lot of work to parse and merge with other logs. Structured logs
    are better suitable for large systems because they can be processed at scale.
    Binary logs make sense for systems that generate a lot of logs as they are more
    space efficient, but require custom tools and processing to extract their information.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 日志可能有多种格式。纯文本格式非常常见且人类可读，但需要大量工作来解析和与其他日志合并。结构化日志更适合大规模系统，因为它们可以进行大规模处理。二进制日志适用于生成大量日志的系统，因为它们更节省空间，但需要自定义工具和处理才能提取其信息。
- en: Log storage
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志存储
- en: Logs can be stored in memory, on the file system, in a database, in cloud storage,
    sent to a remote logging service, or any combination of these. In the cloud-native
    world, where software runs in containers, it’s important to pay special attention
    to where logs are stored and how to fetch them when necessary.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 日志可以存储在内存中、文件系统上、数据库中、云存储中、发送到远程日志服务，或这些方式的任何组合。在云原生环境中，软件运行在容器中，因此需要特别关注日志存储位置以及在必要时如何提取它们。
- en: Questions like durability come to mind when containers can come and go. In Kubernetes,
    the standard output and standard error streams of containers are automatically
    logged and available, even when the pod terminates. But, issues like having enough
    space for logs and log rotation are always relevant.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当容器可能随时消失时，持久性等问题就会浮现。在Kubernetes中，容器的标准输出和标准错误流会自动记录并保持可用，即使pod终止。但是，像日志空间不足和日志轮换等问题总是值得关注的。
- en: Log aggregation
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志聚合
- en: In the end, the best practice is to send local logs to a centralized logging
    service that is designed to handle various log formats, persist them as necessary,
    and aggregate many types of logs in a way that can be queried and reasoned about.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是将本地日志发送到一个集中式日志服务，这个服务设计用来处理各种日志格式，根据需要持久化它们，并以可查询和可推理的方式聚合多种日志。
- en: Metrics
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指标
- en: Metrics measure some aspects of the system over time. Metrics are time series
    of numerical values (typically floating point numbers). Each metric has a name
    and often a set of labels that help later in slicing and dicing. For example,
    the CPU utilization of a node or the error rate of a service are metrics.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 指标衡量系统随时间变化的某些方面。指标是数值值的时间序列（通常是浮动点数）。每个指标都有一个名称，通常还会有一组标签，方便后续的切片和分解。例如，节点的CPU利用率或服务的错误率就是指标。
- en: Metrics are much more economical than logs. They require a fixed amount of space
    per time period that doesn’t ebb and flow with incoming traffic like logs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 指标比日志更经济。它们每个时间段需要固定的存储空间，不会像日志那样随着流量的变化而波动。
- en: Also, since metrics are numerical in nature, they don’t need parsing or transformations.
    Metrics can be easily combined and analyzed using statistical methods and serve
    as triggers for events and alerts.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于指标本质上是数值型的，因此无需解析或转换。指标可以通过统计方法轻松组合和分析，并作为事件和警报的触发器。
- en: A lot of metrics at different levels (node, container, process, network, and
    disk) are often collected for you automatically by the OS, cloud provider, or
    Kubernetes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 不同层次的许多指标（如节点、容器、进程、网络和磁盘）通常会由操作系统、云服务提供商或Kubernetes自动为你收集。
- en: But you can also create custom metrics that map to high-level concerns of your
    system and can be configured with application-level policies.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但你也可以创建自定义指标，映射到系统的高层次关注点，并可以与应用程序级别的策略一起配置。
- en: Distributed tracing
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式追踪
- en: Modern distributed systems often use microservice-based architecture, where
    an incoming request is bounced between multiple microservices, waits in queues,
    and triggers serverless functions. When you try to analyze errors, failures, data
    integrity issues, or performance issues, it is critical to be able to follow the
    path of a request. This is where distributed tracing comes in.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现代分布式系统通常采用基于微服务的架构，其中传入的请求在多个微服务之间传递，等待队列，并触发无服务器函数。当你尝试分析错误、故障、数据完整性问题或性能问题时，能够追踪请求的路径是至关重要的。这就是分布式跟踪的作用。
- en: A distributed trace is a collection of spans and references. You can think of
    a trace as a **directed acyclic graph** (**DAG**) that represents a request’s
    traversal through the components of a distributed system. Each span records the
    time the request spent in a given component and references are the edges of the
    graph that connect one span to the following spans.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式跟踪是由多个 span 和引用组成的集合。你可以将跟踪看作一个**有向无环图**（**DAG**），它表示一个请求在分布式系统各个组件中的传递过程。每个
    span 记录请求在特定组件中花费的时间，而引用则是连接一个 span 与下一个 span 的图边。
- en: 'Here is an example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子：
- en: '![](img/B18998_13_01.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_13_01.png)'
- en: 'Figure 13.1: The path of a sample distributed trace'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1：一个样本分布式跟踪的路径
- en: Distributed tracing is indispensable for understanding complex distributed systems.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式跟踪在理解复杂分布式系统中是不可或缺的。
- en: Application error reporting
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用程序错误报告
- en: Error and exception reporting is sometimes done as part of logging. You definitely
    want to log errors, and looking at logs when things go wrong is a time-honored
    tradition. However, there are levels for capturing error information that go beyond
    logging. When an error occurs in one of your applications, it is useful to capture
    an error message, the location of the error in the code, and the stack trace.
    This is pretty standard and most programming languages can provide all this information,
    although stack traces are multi-line and don’t fit well with line-based logs.
    A useful piece of additional information to capture is the local state in each
    level of the stack trace. This helps when a problem occurs in a central place
    but local states, like the number and size of entries in some lists can help identify
    the root cause.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 错误和异常报告有时是作为日志的一部分进行的。你肯定需要记录错误，查看日志在出现问题时是一项历史悠久的传统。然而，捕获错误信息的层级超出了日志记录。当应用程序发生错误时，捕获错误信息、错误在代码中的位置以及堆栈跟踪非常有用。这是非常标准的，大多数编程语言都可以提供这些信息，尽管堆栈跟踪通常是多行的，不太适合基于行的日志。一个有用的额外信息是捕获堆栈跟踪每一层的本地状态。当问题发生在一个核心位置时，本地状态（比如某些列表中的条目数量和大小）可以帮助识别根本原因。
- en: A central error reporting service like Sentry or Rollbar provides a lot of value
    specific to errors beyond logging, such as rich error information, context, and
    user information.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Sentry 或 Rollbar 这样的中央错误报告服务提供了超出日志记录的错误特定价值，比如丰富的错误信息、上下文和用户信息。
- en: Dashboards and visualization
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仪表盘和可视化
- en: OK. You’ve done a great job of collecting logs, defining metrics, tracing your
    requests, and reporting rich errors. Now, you want to figure out what your system
    or parts of it, are doing. What is the baseline? How does traffic fluctuate throughout
    the day, week, and on holidays? When the system is under stress, what parts are
    the most vulnerable?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，你已经成功地收集了日志，定义了指标，跟踪了请求，并报告了丰富的错误信息。现在，你想弄清楚你的系统或其部分正在做什么。基准是什么？流量在一天、一周以及节假日之间是如何波动的？当系统承受压力时，哪些部分最脆弱？
- en: In a complicated system that involves hundreds and thousands of services and
    data stores and integrates with external systems, you can’t just look at the raw
    log files, metrics, and traces.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个涉及数百个服务和数据存储并与外部系统集成的复杂系统中，你不能仅仅依赖原始日志文件、指标和跟踪。
- en: You need to be able to combine a lot of information and build system health
    dashboards, visualize your infrastructure, and create business-level reports and
    diagrams.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要能够整合大量信息，构建系统健康仪表盘，可视化基础设施，并创建业务级别的报告和图表。
- en: You may get some of it (especially for infrastructure) automatically if you’re
    using cloud platforms. But you should expect to do some serious work around visualization
    and dashboards.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是云平台，你可能会自动获得其中的一部分（尤其是基础设施方面的信息）。但你应该预期需要在可视化和仪表盘方面做一些深入的工作。
- en: Alerting
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 警报
- en: Dashboards are great for humans that want to get a broad view of the system
    and be able to drill down and understand how it behaves. Alerting is all about
    detecting abnormal situations and triggering some action. Ideally, your system
    is self-healing and can recover on its own from most situations. But you should
    at least report it, so humans can review what happened at their leisure and decide
    if further action is needed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表盘非常适合那些想要从全局角度了解系统并能够深入分析其行为的人。告警则是检测异常情况并触发某些操作。理想情况下，你的系统应该是自愈的，能够从大多数情况中自动恢复。但至少，你应该报告异常，以便人类可以随时查看发生了什么，并决定是否需要进一步的操作。
- en: Alerting can be integrated with emails, chat rooms, and on-call systems. It
    is often linked to metrics, and when certain conditions apply, an alert is raised.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 告警可以与电子邮件、聊天室和值班系统集成。它通常与指标关联，当特定条件满足时，告警就会触发。
- en: Now that we have covered, in general, the different elements involved in monitoring
    complex systems, let’s see how to do it with Kubernetes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经概述了监控复杂系统中涉及的不同元素，让我们看看如何在 Kubernetes 中实现这些操作。
- en: Logging with Kubernetes
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 进行日志记录
- en: We need to consider carefully our logging strategy with Kubernetes. There are
    several types of logs that are relevant for monitoring purposes. Our workloads
    run in containers, of course, and we care about these logs, but we also care about
    the logs of Kubernetes components like the API server, kubelet, and container
    runtime.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要仔细考虑在 Kubernetes 中的日志策略。有几种类型的日志与监控相关。我们的工作负载当然运行在容器中，我们关心这些日志，但我们也关心 Kubernetes
    组件的日志，如 API 服务器、kubelet 和容器运行时的日志。
- en: In addition, chasing logs across multiple nodes and containers is a non-starter.
    The best practice is to use central logging (a.k.a. log aggregation). There are
    several options here, which we will explore soon.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，跨多个节点和容器追踪日志是不可行的。最佳实践是使用中央日志记录（也称为日志聚合）。这里有几个选项，我们很快会探讨。
- en: Container logs
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器日志
- en: Kubernetes stores the standard output and standard error of every container.
    They are available through the `kubectl logs` command.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 会存储每个容器的标准输出和标准错误。通过 `kubectl logs` 命令可以访问这些日志。
- en: 'Here is a pod manifest that prints the current date and time every 10 seconds:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个打印当前日期和时间的 Pod 清单，每 10 秒钟打印一次：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can save it to a file called `now-pod.yaml` and create it:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其保存到一个名为 `now-pod.yaml` 的文件中并创建它：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To check out the logs, we use the `kubectl logs` command:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看日志，我们使用 `kubectl logs` 命令：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'A few points about container logs. The `kubectl logs` command expects a pod
    name. If the pod has multiple containers, you need to specify the container name
    too:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 关于容器日志的几点说明。`kubectl logs` 命令需要指定 Pod 名称。如果该 Pod 有多个容器，你还需要指定容器名称：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If a deployment or replica set creates multiple copies of the same pod, you
    can query the logs of all pods in a single call by using a shared label:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个部署或副本集创建了多个相同 Pod 的副本，你可以通过使用共享标签，单次查询所有 Pod 的日志：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If a container crashes for some reason, you can use the `kubectl logs -p` command
    to look at logs from the crashed container.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个容器因某种原因崩溃了，你可以使用 `kubectl logs -p` 命令查看崩溃容器的日志。
- en: Kubernetes component logs
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 组件日志
- en: If you run Kubernetes in a managed environment like GKE, EKS, or AKS, you won’t
    be able to access Kubernetes component logs directly, but this is expected. You’re
    not responsible for the Kubernetes control plane. However, the logs of control
    plane components (like the API server and cluster autoscaler), as well as node
    components (like the kubelet and container runtime), may be important for troubleshooting
    issues. Cloud providers often offer proprietary ways to access these logs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在像 GKE、EKS 或 AKS 这样的托管环境中运行 Kubernetes，你将无法直接访问 Kubernetes 组件日志，但这是预期的。你不需要负责
    Kubernetes 控制平面。然而，控制平面组件（如 API 服务器和集群自动扩展器）以及节点组件（如 kubelet 和容器运行时）的日志，对于故障排除可能非常重要。云服务提供商通常提供专有的方法来访问这些日志。
- en: 'Here are the standard control plane components and their log location if you
    run your own Kubernetes control plane:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你自己运行 Kubernetes 控制平面，以下是标准控制平面组件及其日志位置：
- en: 'API server: `/var/log/kube-apiserver.log`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'API 服务器: `/var/log/kube-apiserver.log`'
- en: 'Scheduler: `/var/log/kube-scheduler.log`'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '调度器: `/var/log/kube-scheduler.log`'
- en: 'Controller manager: `/var/log/kube-controller-manager.log`'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '控制器管理器: `/var/log/kube-controller-manager.log`'
- en: 'The worker node components and their log locations are:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点组件及其日志位置如下：
- en: 'Kubelet: `/var/log/kubelet.log`'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kubelet: `/var/log/kubelet.log`'
- en: 'Kube proxy: `/var/log/kube-proxy.log`'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kube proxy: `/var/log/kube-proxy.log`'
- en: Note that on a systemd-based system, you’ll need to use `journalctl` to view
    the worker node logs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在基于 systemd 的系统中，你需要使用 `journalctl` 来查看工作节点的日志。
- en: Centralized logging
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集中式日志
- en: Reading container logs is fine for quick and dirty troubleshooting problems
    in a single pod. To diagnose and debug system-wide issues, we need centralized
    logging (a.k.a. log aggregation). All the logs from our containers should be sent
    to a central repository and made accessible for slicing and dicing using filters
    and queries.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读容器日志对于在单个 Pod 中进行快速且简单的故障排除是可以的。但要诊断和调试系统级别的问题，我们需要集中式日志（即日志聚合）。所有来自我们容器的日志都应该发送到一个中央存储库，并通过过滤器和查询进行切片和切割。
- en: 'When deciding on your central logging approach, there are several important
    decisions:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定你的中央日志方法时，有几个重要的决策：
- en: How to collect the logs
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何收集日志
- en: Where to store the logs
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志存储的位置
- en: How to handle sensitive log information
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何处理敏感日志信息
- en: We will answer these questions in the following sections.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中回答这些问题。
- en: Choosing a log collection strategy
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择日志收集策略
- en: Logs are collected typically by an agent that is running close to the process
    generating the logs and making sure to deliver them to the central logging service.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 日志通常是通过一个运行在接近生成日志的进程旁边的代理收集的，并确保将其发送到中央日志服务。
- en: Let’s look at the common approaches.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下常见的方法。
- en: Direct logging to a remote logging service
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 直接将日志记录到远程日志服务
- en: In this approach, there is no log agent. It is the responsibility of each application
    container to send logs to the remote logging service. This is typically done through
    a client library. It is a high-touch approach and applications need to be aware
    of the logging target as well as be configured with proper credentials.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，没有日志代理。每个应用程序容器的责任是将日志发送到远程日志服务。这通常通过客户端库完成。这是一种高接触式方法，应用程序需要知道日志目标，并配置适当的凭证。
- en: '![Direct logging](img/B18998_13_02.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![直接日志记录](img/B18998_13_02.png)'
- en: 'Figure 13.2: Direct logging'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2：直接日志记录
- en: If you ever want to change your log collection strategy, it will require changes
    to each and every application (at least bumping to a new version of the library).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要更改日志收集策略，将需要对每个应用程序进行修改（至少升级到新版本的库）。
- en: Node agent
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 节点代理
- en: The node agent approach is best when you control the worker nodes, and you want
    to abstract away the act of log aggregation from your applications. Each application
    container can simply write to standard output and standard error and the agent
    running on each node will intercept the logs and deliver them to the remote logging
    service.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 节点代理方法在你控制工作节点时最为合适，并且你希望将日志聚合的任务从应用程序中抽象出来。每个应用程序容器可以简单地写入标准输出和标准错误，运行在每个节点上的代理将拦截日志并将其发送到远程日志服务。
- en: Typically, you deploy the node agent as a DaemonSet so, as nodes are added or
    removed from the cluster, the log agent will always be present without additional
    work.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你会将节点代理作为 DaemonSet 部署，因此，随着节点的添加或移除，日志代理将始终存在，无需额外操作。
- en: '![](img/B18998_13_03.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_13_03.png)'
- en: 'Figure 13.3: Using a node agent for logging'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3：使用节点代理进行日志记录
- en: Sidecar container
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Sidecar 容器
- en: The sidecar container is best when you don’t have control over your cluster
    nodes, or if you use some serverless computing infrastructure to deploy containers
    but don’t want to use the direct logging approach. The node agent approach is
    out of the question if you don’t control the node and can’t install the agent,
    but you can attach a sidecar container that will collect the logs and deliver
    them to the central logging service. It is not as efficient as the node agent
    approach because each container will need its own logging sidecar container, but
    it can be done at the deployment stage without requiring code changes and application
    knowledge.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当你无法控制集群节点，或者使用某些无服务器计算基础设施来部署容器但又不想使用直接日志记录方法时，sidecar 容器是最好的选择。如果你无法控制节点且无法安装代理，那么节点代理方法就不适用了，但你可以附加一个
    sidecar 容器，它将收集日志并将其发送到中央日志服务。它的效率不如节点代理方法，因为每个容器都需要有自己的日志 sidecar 容器，但可以在部署阶段完成，而无需修改代码和应用程序知识。
- en: '![](img/B18998_13_04.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_13_04.png)'
- en: 'Figure 13.4: Using a sidecar container for logging'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4：使用 sidecar 容器进行日志记录
- en: Now that we covered the topic of log collection, let’s consider how to store
    and manage those logs centrally.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了日志收集的主题，让我们考虑一下如何集中存储和管理这些日志。
- en: Cluster-level central logging
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群级别的集中日志
- en: If your entire system is running in a single Kubernetes cluster, then cluster-level
    logging may be a great choice. You can install a central logging service like
    Grafana Loki, ElasticSearch, or Graylog in your cluster and enjoy a cohesive log
    aggregation experience without sending your log data elsewhere.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的整个系统运行在一个Kubernetes集群中，那么集群级别的日志记录可能是一个很好的选择。您可以在集群中安装一个中央日志服务，如Grafana
    Loki、ElasticSearch或Graylog，享受一个统一的日志聚合体验，而无需将日志数据发送到其他地方。
- en: Remote central logging
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 远程中央日志记录
- en: 'There are situations where in-cluster central logging doesn’t cut it for various
    reasons:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有些情况下，由于各种原因，集群内的中央日志记录无法满足需求：
- en: Logs are used for audit purposes; it may be necessary to log to a separate and
    controlled location (e.g., on AWS, it is common to log to a separate account).
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志用于审计目的；可能需要将日志记录到一个单独且受控的位置（例如，在AWS上，通常将日志记录到一个单独的账户）。
- en: Your system runs on multiple clusters and logging in each cluster is not really
    central.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的系统运行在多个集群上，每个集群的日志记录并不真正是中央化的。
- en: You run on a cloud provider and prefer to log into the cloud platform logging
    service (e.g., StackDriver on GCP or CloudWatch on AWS).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您运行在云服务提供商上，并且更喜欢将日志记录到云平台的日志服务（例如，GCP上的StackDriver或AWS上的CloudWatch）。
- en: You already work with a remote central logging service like SumoLogic or Splunk
    and you prefer to continue using them.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您已经在使用像SumoLogic或Splunk这样的远程中央日志服务，并且希望继续使用它们。
- en: You just don’t want the hassle of collecting and storing log data.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您只是希望避免收集和存储日志数据的麻烦。
- en: Cluster-wide issues can impact your log collection, storage, or access and impede
    your ability to troubleshoot them.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群范围的问题可能会影响您的日志收集、存储或访问，并妨碍您排查故障。
- en: 'Logging to a remote central location can be done by all methods: direct logging,
    node agent logging, or sidecar logging. In all cases, the endpoint and credentials
    to the remote logging service must be provided, and the logging is done against
    that endpoint. In most cases, this will be done via a client library that hides
    the details from the applications. As for system-level logging, the common method
    is to collect all the necessary logs by a dedicated logging agent and forward
    them to the remote logging service.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 将日志记录到远程中央位置可以通过所有方法完成：直接日志记录、节点代理日志记录或Sidecar日志记录。在所有情况下，都必须提供远程日志服务的端点和凭证，日志记录是针对该端点进行的。在大多数情况下，这将通过客户端库完成，应用程序无需了解其细节。至于系统级日志记录，常见的方法是通过专用的日志代理收集所有必要的日志，并将它们转发到远程日志服务。
- en: Dealing with sensitive log information
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理敏感日志信息
- en: OK. We can collect the logs and send them to a central logging service. If the
    central logging service is remote, you might need to be selective about which
    information you log.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们可以收集日志并将它们发送到中央日志服务。如果中央日志服务是远程的，您可能需要选择性地记录某些信息。
- en: For example, **personally identifiable information** (**PII**) and **protected
    health information** (**PHI**) are two categories of information that you probably
    shouldn’t log without making sure access to the logs is properly controlled.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，**个人身份信息**（**PII**）和**受保护的健康信息**（**PHI**）是两类您可能不应记录的信息，除非确保访问日志的权限得到妥善控制。
- en: It is common to redact or remove PII like user names and emails from log statements.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会删除或屏蔽如用户名和电子邮件等个人身份信息（PII）日志记录。
- en: Using Fluentd for log collection
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Fluentd进行日志收集
- en: 'Fluentd ([https://www.fluentd.org](https://www.fluentd.org)) is an open source
    CNCF-graduated project. It is considered best in class in Kubernetes, and it can
    integrate with pretty much every logging backend you want. If you set up your
    own centralized logging solution, I recommend using Fluentd. Fluentd operates
    as a node agent. The following diagram shows how Fluentd can be deployed as a
    DaemonSet in a Kubernetes cluster:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd ([https://www.fluentd.org](https://www.fluentd.org))是一个开源的CNCF毕业项目。它被认为是Kubernetes中最好的选择，并且几乎可以与任何您想要的日志后端集成。如果您自己搭建集中式日志解决方案，我推荐使用Fluentd。Fluentd作为节点代理运行。以下图示展示了Fluentd如何作为DaemonSet在Kubernetes集群中部署：
- en: '![](img/B18998_13_05.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_13_05.png)'
- en: 'Figure 13.5: Deploying Fluentd as a DaemonSet in a Kubernetes cluster'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5：在Kubernetes集群中作为DaemonSet部署Fluentd
- en: One of the most popular DIY centralized logging solutions is ELK, where E stands
    for ElasticSearch, L stands for LogStash, and K stands for Kibana. On Kubernetes,
    EFK (where Fluentd replaces LogStash) is very common.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的DIY集中式日志解决方案之一是ELK，其中E代表ElasticSearch，L代表LogStash，K代表Kibana。在Kubernetes上，EFK（Fluentd替代LogStash）非常常见。
- en: Fluentd has plugin-based architecture so don’t feel limited to EFK. Fluentd
    doesn’t require a lot of resources, but if you really need a high-performance
    solution, Fluentbit ([http://fluentbit.io/](http://fluentbit.io/)) is a pure forwarder
    that uses barely 450 KB of memory.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 具有基于插件的架构，因此不要觉得只限于 EFK。Fluentd 不需要很多资源，但如果你真的需要一个高性能的解决方案，Fluentbit（[http://fluentbit.io/](http://fluentbit.io/)）是一个纯粹的转发器，只需要不到
    450 KB 的内存。
- en: We have covered a lot of ground about logging. Let’s look at the next piece
    of the observability story, which is metrics.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经覆盖了很多关于日志的内容。接下来，我们来看看可观测性故事的下一个部分，即指标。
- en: Collecting metrics with Kubernetes
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 收集指标
- en: Kubernetes has a Metrics API. It supports node and pod metrics out of the box.
    You can also define your own custom metrics.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 有一个指标 API，它支持开箱即用的节点和 Pod 指标。你还可以定义自定义指标。
- en: 'A metric contains a timestamp, a usage field, and the time range in which the
    metric was collected (many metrics are accumulated over a time period). Here is
    the API definition for node metrics:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个指标包含一个时间戳、一个使用字段和收集该指标的时间范围（许多指标是按时间段积累的）。以下是节点指标的 API 定义：
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The usage field type is `ResourceList`, but it’s actually a map of a resource
    name to a quantity:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用字段类型是 `ResourceList`，但它实际上是一个资源名称到数量的映射：
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'A `Quantity` represents a fixed-point number. It allows easy marshaling/unmarshaling
    in JSON and YAML, and accessors such as `String()` and `Int64()`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`Quantity` 表示一个定点数。它支持 JSON 和 YAML 中的轻松序列化/反序列化，并提供如 `String()` 和 `Int64()`
    等访问方法：'
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Monitoring with the Metrics Server
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用指标服务器进行监控
- en: The Kubernetes Metrics Server implements the Kubernetes Metrics API.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 指标服务器实现了 Kubernetes 指标 API。
- en: 'You can deploy it with Helm:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 Helm 部署它：
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'On minikube, you can just enable it as an add-on:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在 minikube 上，你可以通过启用它作为插件来使用：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that at the time of writing, there was an issue with the Metrics Server
    on minikube, which was fixed in Kubernetes 1.27 (see [https://github.com/kubernetes/minikube/issues/13969](https://github.com/kubernetes/minikube/issues/13969)).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在撰写本文时，minikube 上的指标服务器存在一个问题，该问题已在 Kubernetes 1.27 中修复（参见 [https://github.com/kubernetes/minikube/issues/13969](https://github.com/kubernetes/minikube/issues/13969)）。
- en: We will use a kind cluster to deploy the `metrics-server`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个 kind 集群来部署 `metrics-server`。
- en: 'After a few minutes to let the metrics server collect some data, you can query
    it using these commands for node metrics:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 等待几分钟，让指标服务器收集一些数据后，你可以使用以下命令查询节点指标：
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In addition, the `kubectl top` command gets its information from the metrics
    server:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`kubectl top` 命令从指标服务器获取信息：
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can also get metrics for pods:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以获取 Pod 的指标：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The metrics server is also the source of performance information in the Kubernetes
    dashboard.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 指标服务器也是 Kubernetes 仪表盘中的性能信息来源。
- en: The rise of Prometheus
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Prometheus 的崛起
- en: Prometheus ([https://prometheus.io/](https://prometheus.io/)) is yet another
    graduated CNCF open source project. It focuses on metrics collection and alert
    management. It has a simple yet powerful data model for managing time-series data
    and a sophisticated query language. It is considered best in class in the Kubernetes
    world. Prometheus lets you define recording rules that are fired at regular intervals
    and collect data from targets. In addition, you can define alerting rules that
    evaluate a condition and trigger alerts if the condition is satisfied.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus（[https://prometheus.io/](https://prometheus.io/)）是另一个成熟的 CNCF 开源项目。它专注于指标收集和告警管理。它具有一个简单但强大的数据模型，用于管理时间序列数据，并提供一个复杂的查询语言。它被认为是
    Kubernetes 领域的最佳选择。Prometheus 允许你定义在固定间隔触发的记录规则，并从目标收集数据。此外，你可以定义告警规则来评估某个条件，并在满足条件时触发告警。
- en: 'It has several unique features compared to other monitoring solutions:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他监控解决方案相比，它具有几个独特的特点：
- en: The collection system is pulled over HTTP. Nobody has to push metrics to Prometheus
    (but push is supported via a gateway).
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集系统是通过 HTTP 拉取的。没有人需要将指标推送到 Prometheus（但通过网关支持推送）。
- en: A multidimensional data model (each metric is a named time series with a set
    of key/value pairs attached to each data point).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个多维数据模型（每个指标都是一个命名的时间序列，并且每个数据点附有一组键/值对）。
- en: 'PromQL: A powerful and flexible query language to slice and dice your metrics.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PromQL：一种强大而灵活的查询语言，用于切割和分析你的指标。
- en: Prometheus server nodes are independent and don’t rely on shared storage.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prometheus 服务器节点是独立的，不依赖于共享存储。
- en: Target discovery can be dynamic or via static configuration.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标发现可以是动态的，也可以通过静态配置进行。
- en: Built-in time series storage, but supports other backends if necessary.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Built-in alert manager and the ability to define alerting rules.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the entire system:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_13_06.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.6: Prometheus architecture'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Installing Prometheus
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prometheus is a complex beast, as you can see. The best way to install it is
    using the Prometheus operator ([https://github.com/prometheus-operator/](https://github.com/prometheus-operator/)).
    The kube-prometheus ([https://github.com/prometheus-operator/kube-prometheus](https://github.com/prometheus-operator/kube-prometheus))
    sub-project installs the operator itself, and a lot of additional components,
    and configures them in a robust manner.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is cloning the git repo:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, the setup manifests install several CRDs and creates a namespace called
    `monitoring`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we can install the manifests:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is too long to display, but let’s examine what was actually installed.
    It turns out that it installed multiple deployments, StatefulSets, a DaemonSet,
    and many services:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This is a high-availability setup. As you can see, Prometheus itself is deployed
    as a StatefulSet with two replicas, and the alert manager is deployed as a StatefulSet
    with three replicas.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The deployments include the `blackbox-exporter`, Grafana for visualizing the
    metrics, `kube-state-metrics` to collect Kubernetes-specific metrics, the Prometheus
    adapter (a compatible replacement to the standard Kubernetes Metrics Server),
    and finally, the Prometheus operator.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with Prometheus
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Prometheus has a basic web UI that you can use to explore its metrics. Let’s
    do port forwarding to `localhost`:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, you can browse to `http://localhost:9090`, where you can select different
    metrics and view raw data or graphs:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![Prometheus UI](img/B18998_13_07.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.7: Prometheus UI'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus records an outstanding number of metrics (`9090` in my current setup).
    The most relevant metrics on Kubernetes are the metrics exposed by the `kube-state-metrics`
    and node exporters.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating kube-state-metrics
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Prometheus operator already installs `kube-state-metrics`. It is a service
    that listens to Kubernetes events and exposes them through a `/metrics` HTTP endpoint
    in the format Prometheus expects. So, it is a Prometheus exporter.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: This is very different from the Kubernetes metrics server, which is the standard
    way Kubernetes exposes metrics for nodes and pods and allows you to expose your
    own custom metrics too. The Kubernetes metrics server is a service that periodically
    queries Kubernetes for data and stores it in memory. It exposes its data through
    the Kubernetes Metrics API. The Prometheus adapter adapts the Kubernetes metrics
    server information and exposes it in Prometheus format.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'The metrics exposed by `kube-state-metrics` are vast. Here is the list of the
    groups of metrics, which is pretty massive on its own. Each group corresponds
    to a Kubernetes API object and contains multiple metrics:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '`CertificateSigningRequest` Metrics'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CertificateSigningRequest` 指标'
- en: '`ConfigMap` Metrics'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ConfigMap` 指标'
- en: '`CronJob` Metrics'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CronJob` 指标'
- en: '`DaemonSet` Metrics'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DaemonSet` 指标'
- en: '`Deployment` Metrics'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Deployment` 指标'
- en: '`Endpoint` Metrics'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Endpoint` 指标'
- en: '`HorizontalPodAutoscaler` Metrics'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HorizontalPodAutoscaler` 指标'
- en: '`Ingress` Metrics'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Ingress` 指标'
- en: '`Job` Metrics'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Job` 指标'
- en: '`LimitRange` Metrics'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LimitRange` 指标'
- en: '`MutatingWebhookConfiguration` Metrics'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MutatingWebhookConfiguration` 指标'
- en: '`Namespace` Metrics'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Namespace` 指标'
- en: '`NetworkPolicy` Metrics'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NetworkPolicy` 指标'
- en: '`Node` Metrics'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Node` 指标'
- en: '`PersistentVolume` Metrics'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PersistentVolume` 指标'
- en: '`PersistentVolumeClaim` Metrics'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PersistentVolumeClaim` 指标'
- en: '`PodDisruptionBudget` Metrics'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PodDisruptionBudget` 指标'
- en: '`Pod` Metrics'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pod` 指标'
- en: '`ReplicaSet` Metrics'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReplicaSet` 指标'
- en: '`ReplicationController` Metrics'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReplicationController` 指标'
- en: '`ResourceQuota` Metrics'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ResourceQuota` 指标'
- en: '`Secret` Metrics'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Secret` 指标'
- en: '`Service` Metrics'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Service` 指标'
- en: '`StatefulSet` Metrics'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StatefulSet` 指标'
- en: '`StorageClass` Metrics'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StorageClass` 指标'
- en: '`ValidatingWebhookConfiguration` Metrics'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ValidatingWebhookConfiguration` 指标'
- en: '`VerticalPodAutoscaler` Metrics'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VerticalPodAutoscaler` 指标'
- en: '`VolumeAttachment` Metrics'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VolumeAttachment` 指标'
- en: 'For example, here are the metrics collected for Kubernetes services:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下是 Kubernetes 服务收集的指标：
- en: '`kube_service_info`'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube_service_info`'
- en: '`kube_service_labels`'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube_service_labels`'
- en: '`kube_service_created`'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube_service_created`'
- en: '`kube_service_spec_type`'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube_service_spec_type`'
- en: Utilizing the node exporter
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用节点导出器
- en: '`kube-state-metrics` collects node information from the Kubernetes API server,
    but this information is pretty limited. Prometheus comes with its own node exporter,
    which collects tons of low-level information about the nodes. Remember that Prometheus
    may be the de-facto standard metrics platform on Kubernetes, but it is not Kubernetes-specific.
    For other systems that use Prometheus, the node exporter is super important. On
    Kubernetes, if you manage your own nodes, this information can be invaluable too.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-state-metrics` 从 Kubernetes API 服务器收集节点信息，但这些信息相当有限。Prometheus 自带的节点导出器可以收集大量的节点底层信息。请记住，Prometheus
    可能是 Kubernetes 上的事实标准指标平台，但它并非 Kubernetes 专用。对于使用 Prometheus 的其他系统，节点导出器非常重要。在
    Kubernetes 上，如果你管理自己的节点，这些信息也非常宝贵。'
- en: 'Here is a small subset of the metrics exposed by the node exporter:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是节点导出器暴露的一小部分指标：
- en: '![Node exporter metrics](img/B18998_13_08.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![Node exporter metrics](img/B18998_13_08.png)'
- en: 'Figure 13.8: Node exporter metrics'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8：节点导出器指标
- en: Incorporating custom metrics
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成自定义指标
- en: 'The built-in metrics, node metrics, and Kubernetes metrics are great, but very
    often, the most interesting metrics are domain-specific and need to be captured
    as custom metrics. There are two ways to do it:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 内建指标、节点指标和 Kubernetes 指标很好，但通常情况下，最有趣的指标是特定领域的，需要作为自定义指标来捕获。有两种方式可以做到：
- en: Write your own exporter and tell Prometheus to scrape it
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写自己的导出器并告诉 Prometheus 去抓取它
- en: Use the Push gateway, which allows you to push metrics into Prometheus
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Push 网关，将指标推送到 Prometheus
- en: In my book *Hands-On Microservices with Kubernetes* ([https://www.packtpub.com/product/hands-on-microservices-with-kubernetes/9781789805468](https://www.packtpub.com/product/hands-on-microservices-with-kubernetes/9781789805468)),
    I provide a full-fledged example of how to implement your own exporter from a
    Go service.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的书《*Kubernetes 微服务实战*》([https://www.packtpub.com/product/hands-on-microservices-with-kubernetes/9781789805468](https://www.packtpub.com/product/hands-on-microservices-with-kubernetes/9781789805468))中，我提供了一个完整的示例，展示如何从
    Go 服务实现自己的导出器。
- en: The Push gateway is more appropriate if you already have a push-based metrics
    collector in place, and you just want to have Prometheus record those metrics.
    It provides a convenient migration path from other metrics collection systems
    to Prometheus.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经有了基于推送的指标收集器，并且只想让 Prometheus 记录这些指标，那么 Push 网关更为合适。它提供了一个从其他指标收集系统到 Prometheus
    的便捷迁移路径。
- en: Alerting with Alertmanager
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Alertmanager 进行告警
- en: Collecting metrics is great, but when things go south (or ideally, BEFORE things
    go south), you want to get notified. In Prometheus, this is the job of the `Alertmanager`.
    You can define rules as expressions-based metrics, and when those expressions
    become true, they trigger an alert.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 收集指标很好，但当问题发生时（或者理想情况下，在问题发生之前），你希望收到通知。在 Prometheus 中，这是 `Alertmanager` 的任务。你可以定义基于表达式的指标规则，当这些表达式成立时，它们会触发告警。
- en: Alerts can serve multiple purposes. They can be handled automatically by a controller
    that is responsible for mitigating specific problems, they can wake up a poor
    on-call engineer at 3 am, they can result in an email or group chat message, or
    any combination of these.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 警报可以担任多种角色。它们可以由负责缓解特定问题的控制器自动处理，也可以在凌晨3点叫醒可怜的值班工程师，还可以触发电子邮件或群聊消息，或者以上述任意组合。
- en: The `Alertmanager` lets you group similar alerts into a single notification,
    inhibiting notifications if other alerts are already firing and silencing alerts.
    All those features are useful when a large-scale system is in trouble. The stakeholders
    are aware of the situation and don’t need repeated alerts or multiple variations
    of the same alert to fire constantly while troubleshooting and trying to find
    the root cause.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`Alertmanager` 允许您将类似的警报分组为单个通知，抑制正在触发的其他警报的通知，并消除警报。在大规模系统出现问题时，所有这些功能都非常有用。利益相关者了解情况，并且在排除故障和寻找根本原因时，不需要重复警报或同一警报的多个变体不断触发。'
- en: 'One of the cool things about the Prometheus operator is that it manages everything
    in CRDs. That includes all the rules, including the alert rules:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus operator 的一个很酷的功能是它在自定义资源定义 (CRD) 中管理所有内容。这包括所有规则，包括警报规则：
- en: '[PRE18]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is the `NodeFilesystemAlmostOutOfSpace` alert that checks if available
    disk space for the file system on the node is less than a threshold for 30 minutes.
    If you notice, there are two almost identical alerts. When the available space
    drops below 5%, a warning alert is triggered. However, if the space drops below
    3%, then a critical alert is triggered. Note the `runbook_url` field, which points
    to a page that explains more about the alert and how to mitigate the problem:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 `NodeFilesystemAlmostOutOfSpace` 警报，检查节点上文件系统的可用磁盘空间是否低于30分钟的阈值。如果注意到，有两个几乎相同的警报。当可用空间低于5%时，将触发警告警报。但是，如果空间低于3%，则触发严重警报。请注意
    `runbook_url` 字段，它指向一个页面，详细说明了警报的更多信息以及如何解决问题：
- en: '[PRE19]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Alerts are very important, but there are cases where you want to visualize the
    overall state of your system or drill down into specific aspects. This is where
    visualization comes into play.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 警报非常重要，但有些情况下，您希望可视化系统的整体状态或深入了解特定方面。这就是可视化发挥作用的地方。
- en: Visualizing your metrics with Grafana
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Grafana 可视化您的指标
- en: You’ve already seen the Prometheus Expression browser, which can display your
    metrics as a graph or in table form. But we can do much better. Grafana ([https://grafana.com](https://grafana.com))
    is an open source monitoring system that specializes in stunningly beautiful visualizations
    of metrics. It doesn’t store the metrics itself but works with many data sources,
    and Prometheus is one of them. Grafana has alerting capabilities, too. When working
    with Prometheus, you may prefer to rely on its `Alertmanager`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经看过 Prometheus 表达式浏览器，它可以将您的指标显示为图形或表格形式。但我们可以做得更好。Grafana ([https://grafana.com](https://grafana.com))
    是一个专注于呈现令人惊叹的美观指标可视化的开源监控系统。它本身不存储指标，但可以与多个数据源一起工作，其中包括 Prometheus。Grafana 也具有警报功能。在使用
    Prometheus 时，您可能更倾向于依赖它的 `Alertmanager`。
- en: 'The Prometheus operator installs Grafana and configures a large number of useful
    Kubernetes dashboards. Check out this beautiful dashboard of Kubernetes capacity:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus operator 安装 Grafana 并配置了大量有用的 Kubernetes 仪表板。看看这个 Kubernetes 容量的漂亮仪表板：
- en: '![](img/B18998_13_09.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_13_09.png)'
- en: 'Figure 13.9: Grafana dashboard'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9：Grafana 仪表板
- en: 'To access Grafana, type the following commands:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问 Grafana，请输入以下命令：
- en: '[PRE20]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Then you can browse to `http://localhost:3000` and have some fun with Grafana.
    Grafana requires a username and password. The default credentials are *admin*
    for the user and *admin* for the password.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您可以浏览 `http://localhost:3000` 并在 Grafana 中玩得开心。Grafana 需要用户名和密码。默认凭据为 *admin*
    作为用户名和 *admin* 作为密码。
- en: 'Here are some of the default dashboards that are configured when deploying
    Grafana via kube-prometheus:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过 kube-prometheus 部署 Grafana 时，这里有一些默认仪表板配置：
- en: '![](img/B18998_13_10.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_13_10.png)'
- en: 'Figure 13.10: Default Grafana Dashboards'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.10：默认 Grafana 仪表板
- en: As you can see, the list is pretty extensive, but you can define your own dashboards
    if you want. There are a lot of fancy visualizations you can create with Grafana.
    I encourage you to explore it further. Grafana dashboards are stored as config
    maps. If you want to add a custom dashboard, just add a config map that contains
    your dashboard spec. There is a dedicated sidecar container watching new config
    maps being added and it will make sure to add your custom dashboard.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: You can also add dashboards via the Grafana UI.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Considering Loki
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you like Prometheus and Grafana and you didn’t settle on a centralized logging
    solution yet (or if you’re unhappy with your current logging solution), then you
    should consider Grafana Loki ([https://grafana.com/oss/loki/](https://grafana.com/oss/loki/)).
    Loki is an open source project for log aggregation inspired by Prometheus. Unlike
    most log aggregation systems, it doesn’t index the log contents but rather a set
    of labels applied to the log. That makes it very efficient. It is still relatively
    new (started in 2018), so you should evaluate if it fits your needs before making
    the decision to adopt it. One thing is sure: Loki has excellent Grafana support.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: There are several advantages for Loki compared to a solution like EFK when Prometheus
    is used as the metrics platform. In particular, the set of labels you use to tag
    your metrics will serve just as well to tag your logs. Also, the fact that Grafana
    is used as a uniform visualization platform for both logs and metrics is useful.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: We have dedicated a lot of time to discussing metrics on Kubernetes. Let’s talk
    about distributed tracing and the Jaeger project.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Distributed tracing with Kubernetes
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a microservice-based system, every request may travel between multiple microservices
    calling each other, wait in queues, and trigger serverless functions. To debug
    and troubleshoot such systems, you need to be able to keep track of requests and
    follow them along their path.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributed tracing provides several capabilities that allow the developers
    and operators to understand their distributed systems:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Distributed transaction monitoring
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance and latency tracking
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Root cause analysis
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service dependency analysis
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed context propagation
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed tracing often requires the participation of the applications and
    services instrumenting endpoints. Since the microservices world is polyglot, multiple
    programming languages may be used. It makes sense to use a shared distributed
    tracing specification and framework that supports many programming languages.
    Enter OpenTelemetry.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: What is OpenTelemetry?
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenTelemetry ([https://opentelemetry.io](https://opentelemetry.io)) is an
    API specification and a set of frameworks and libraries in different languages
    to instrument, collect, and export logs, metrics, and traces. It was born by merging
    the OpenCensus and OpenTracing projects in May 2019\. It is also an incubating
    CNCF project. OpenTelemetry is supported by multiple products and became a de-facto
    standard. It can collect data from a variety of open source and commercial sources.
    Check out the full list here: [https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: OpenTelemetry（[https://opentelemetry.io](https://opentelemetry.io)）是一个API规范及一套用于仪表化、收集和导出日志、指标和追踪的框架和库，涵盖不同语言。它诞生于2019年5月，当时OpenCensus和OpenTracing项目合并。它也是一个孵化中的CNCF项目。OpenTelemetry得到了多个产品的支持，并成为事实上的标准。它可以从各种开源和商业来源收集数据。查看完整列表：[https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver)。
- en: By using a product that complies with OpenTelemetry, you are not locked in,
    and you will work with an API that may be familiar to your developers.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用符合OpenTelemetry标准的产品，您不会被锁定，并且您将使用一个可能对您的开发人员来说很熟悉的API。
- en: 'There are instrumentation libraries for pretty much all the mainstream programming
    languages:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 针对几乎所有主流编程语言都有仪表化库：
- en: C++
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C++
- en: .NET
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: .NET
- en: Erlang/Elixir
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erlang/Elixir
- en: Go
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Go
- en: Java
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java
- en: JavaScript
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JavaScript
- en: PHP
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PHP
- en: Python
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python
- en: Ruby
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruby
- en: Rust
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rust
- en: Swift
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swift
- en: OpenTelemetry tracing concepts
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenTelemetry追踪概念
- en: We will focus here on the tracing concept of OpenTelemetry and skip the logging
    and metrics concepts we covered earlier.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里将重点讨论OpenTelemetry的追踪概念，跳过之前我们涉及的日志记录和指标概念。
- en: The two main concepts are **Span** and **Trace**.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个主要概念是**Span**和**Trace**。
- en: 'A **Span** is the basic unit of work or operation. It has a name, start time,
    and duration. Spans can be nested if one operation starts another operation. Spans
    propagate with a unique ID and context. The **Trace** is an acyclic graph of Spans
    that originated from the same request and shares the same context. A **Trace**
    represents the execution path of a request throughout the system. The following
    diagram illustrates the relationship between a Trace and Spans:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '**Span**是工作或操作的基本单元。它有一个名称、开始时间和持续时间。如果一个操作启动另一个操作，Span可以嵌套。Span通过唯一的ID和上下文传播。**Trace**是由同一个请求发起的Spans构成的非循环图，这些Spans共享相同的上下文。**Trace**代表了请求在系统中执行的路径。下图说明了Trace与Spans之间的关系：'
- en: '![](img/B18998_13_11.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_13_11.png)'
- en: 'Figure 13.11: The relationship between Traces and Spans in OpenTelemetry'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.11：OpenTelemetry中Trace与Span的关系
- en: Now that we understand what OpenTelemetry is about, let’s take a look at the
    Jaeger project.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了OpenTelemetry的概况，让我们来看看Jaeger项目。
- en: Introducing Jaeger
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍Jaeger
- en: Jaeger ([https://www.jaegertracing.io/](https://www.jaegertracing.io/)) is yet
    another CNCF-graduated project, just like Fluentd and Prometheus. It completes
    the trinity of CNCF-graduated observability projects for Kubernetes. Jaeger was
    developed originally by Uber and quickly became the forerunner distributed tracing
    solution for Kubernetes.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger（[https://www.jaegertracing.io/](https://www.jaegertracing.io/)）是另一个CNCF毕业项目，就像Fluentd和Prometheus一样。它完成了Kubernetes的CNCF毕业可观察性项目的三位一体。Jaeger最初由Uber开发，并迅速成为Kubernetes的领先分布式追踪解决方案。
- en: 'There are other open-source distributed tracing systems like Zipkin ([https://zipkin.io](https://zipkin.io))
    and SigNoz ([https://signoz.io](https://signoz.io)). The inspiration for most
    of these systems (as well as Jaeger) is Google’s Dapper ([https://research.google.com/pubs/pub36356.html](https://research.google.com/pubs/pub36356.html)).
    Cloud platforms provide their own tracers, like AWS X-Ray. There are also multiple
    commercial products in the space:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他开源分布式追踪系统，如Zipkin（[https://zipkin.io](https://zipkin.io)）和SigNoz（[https://signoz.io](https://signoz.io)）。这些系统（以及Jaeger）的灵感来自Google的Dapper（[https://research.google.com/pubs/pub36356.html](https://research.google.com/pubs/pub36356.html)）。云平台提供了自己的追踪器，如AWS
    X-Ray。这个领域也有多个商业产品：
- en: Aspecto ([https://www.aspecto.io](https://www.aspecto.io))
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aspecto（[https://www.aspecto.io](https://www.aspecto.io)）
- en: Honeycomb ([https://www.honeycomb.io](https://www.honeycomb.io))
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蜂窝（[https://www.honeycomb.io](https://www.honeycomb.io)）
- en: Lightstep ([http://lightstep.com](http://lightstep.com))
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lightstep（[http://lightstep.com](http://lightstep.com)）
- en: 'Jaeger’s strong points are:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger的优点包括：
- en: Scalable design
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展设计
- en: Supports multiple protocols – OpenTelemetry, OpenTracing, and Zipkin
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持多种协议——OpenTelemetry、OpenTracing和Zipkin
- en: Light memory footprint
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轻量级内存占用
- en: Agents collect metrics over UDP
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理通过 UDP 收集指标
- en: Advanced sampling control
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级采样控制
- en: Jaeger architecture
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Jaeger 架构
- en: Jaeger is a scalable system. It can be deployed as a single binary with all
    its components and store the data in memory, but also as a distributed system
    where spans and traces are stored in persistent storage.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger 是一个可扩展的系统。它可以作为一个包含所有组件的单一二进制文件部署，并将数据存储在内存中；也可以作为一个分布式系统，其中跨度和跟踪信息存储在持久化存储中。
- en: 'Jaeger has several components that collaborate to provide a world-class distributed
    tracing experience. The following diagram illustrates the architecture:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger 拥有多个组件，它们协同工作，提供世界级的分布式追踪体验。以下图示展示了其架构：
- en: '![](img/B18998_13_12.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_13_12.png)'
- en: 'Figure 13.12: Jaeger architecture'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12：Jaeger 架构
- en: Let’s understand the purpose of each component.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解每个组件的目的。
- en: Client libraries
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 客户端库
- en: Originally, Jaeger had its own client libraries that implemented the OpenTracing
    API in order to instrument a service or application for distributed tracing. Now,
    Jaeger recommends using the OpenTelemetry client libraries. The Jaeger client
    libraries have been retired.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，Jaeger 有自己的客户端库，实施了 OpenTracing API，用于在服务或应用程序中实现分布式追踪。现在，Jaeger 推荐使用 OpenTelemetry
    客户端库，Jaeger 客户端库已经被淘汰。
- en: Jaeger agent
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Jaeger 代理
- en: The agent is deployed locally to each node. It listens to spans over UDP – which
    makes it pretty performant – batches them and sends them in bulk to the collector.
    This way, services don’t need to discover the collector or worry about connecting
    to it. Instrumented services simply send their spans to the local agent. The agent
    can also inform the client about sampling strategies.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 代理部署在每个节点本地。它通过 UDP 监听跨度信息——这使得它的性能非常好——将它们批量处理后发送到收集器。这样，服务无需发现收集器或担心与其连接。被仪表化的服务只需将其跨度发送给本地代理。代理还可以通知客户端关于采样策略的信息。
- en: Jaeger collector
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Jaeger 收集器
- en: The collector receives traces from all the agents. It is responsible for validating
    and transforming the traces. It then sends the traces to a data store to a Kafka
    instance, which enables async processing of traces.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 收集器接收来自所有代理的跟踪信息。它负责验证和转换跟踪信息。然后将这些跟踪信息发送到数据存储或 Kafka 实例，从而实现异步处理跟踪。
- en: Jaeger ingester
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Jaeger ingester
- en: The ingester indexes the traces for easy and performant queries later and stores
    them in the data store, which can be a Cassandra or Elasticsearch cluster.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: Ingester 为后续的查询对跟踪信息进行索引，以便更高效地查询，并将其存储在数据存储中，数据存储可以是 Cassandra 或 Elasticsearch
    集群。
- en: Jaeger query
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Jaeger 查询
- en: The Jaeger query service is responsible for presenting a UI to query the traces
    and the spans that the collector put in storage.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger 查询服务负责呈现一个用户界面，用于查询收集器存储的跟踪和跨度。
- en: That covers Jaeger’s architecture and its components. Let’s see how to install
    and work with it.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了 Jaeger 的架构及其组件。接下来，我们来看如何安装并使用它。
- en: Installing Jaeger
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 Jaeger
- en: 'There are Helm charts to install Jaeger and the Jaeger operator:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 有 Helm 图表可供安装 Jaeger 和 Jaeger 操作员：
- en: '[PRE21]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The Jaeger operator requires `cert-manager`, but doesn’t install it automatically.
    Let’s install it first:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger 操作员需要 `cert-manager`，但不会自动安装它。让我们先安装它：
- en: '[PRE22]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, we can install the Jaeger operator into the observability namespace:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将 Jaeger 操作员安装到可观察性命名空间：
- en: '[PRE23]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The deployment is called `jaeger-jaeger-operator`:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 部署被称为`jaeger-jaeger-operator`：
- en: '[PRE24]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we can create a Jaeger instance using the Jaeger CRD. The operator watches
    for this custom resource and creates all the necessary resources. Here is the
    simplest possible Jaeger configuration. It uses the default `AllInOne` strategy,
    which deploys a single pod that contains all the components (agent, collector,
    query, ingester, and Jaeger UI) and uses in-memory storage. This is suitable for
    local development and testing purposes:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 Jaeger CRD 创建一个 Jaeger 实例。操作员会监控这个自定义资源并创建所有必要的资源。这里是最简单的 Jaeger 配置。它使用默认的
    `AllInOne` 策略，部署一个包含所有组件（代理、收集器、查询、Ingester 和 Jaeger UI）的单一 Pod，并使用内存存储。这适用于本地开发和测试：
- en: '[PRE25]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s bring up the Jaeger UI:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动 Jaeger 用户界面：
- en: '[PRE26]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we can browse to `http://localhost:8080` and see the Jaeger UI:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以浏览到`http://localhost:8080`并查看 Jaeger 用户界面：
- en: '![](img/B18998_13_13.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_13_13.png)'
- en: 'Figure 13.13: Jaeger UI'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13：Jaeger 用户界面
- en: In the next chapter, *Chapter 14*, *Utilizing Service Meshes*, we will see more
    of Jaeger and how to use it specifically to trace requests going through the mesh.
    Now, let’s turn our attention to troubleshooting using all the monitoring and
    observability mechanisms we discussed.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，*第14章*，*利用服务网格*中，我们将更多地了解Jaeger以及如何具体使用它来追踪通过网格的请求。现在，让我们将注意力转向使用我们讨论过的所有监控和可观察性机制来进行故障排除。
- en: Troubleshooting problems
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除问题
- en: Troubleshooting a complex distributed system is no picnic. Abstractions, separation
    of concerns, information hiding, and encapsulation are great during development,
    testing, and when making changes to the system. But when things go wrong, you
    need to cross all those boundaries and layers of abstraction from the user action
    in their app, through the entire stack, all the way to the infrastructure, crossing
    all the business logic, asynchronous processes, legacy systems, and third-party
    integrations. This is a challenge even with large monolithic systems, but even
    more so with microservice-based distributed systems. Monitoring will assist you,
    but let’s talk first about preparation, processes, and best practices.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 排查一个复杂的分布式系统问题并非易事。抽象、关注点分离、信息隐藏和封装在开发、测试和系统变更时非常有效。但当问题出现时，你需要跨越所有这些边界和抽象层，从用户在应用中的操作，通过整个技术栈，一直到基础设施，跨越所有业务逻辑、异步处理、遗留系统和第三方集成。这对于大型单体系统来说已经是一个挑战，但对于基于微服务的分布式系统来说更是如此。监控会为你提供帮助，但我们首先需要讨论准备、流程和最佳实践。
- en: Taking advantage of staging environments
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用预生产环境
- en: When building a large system, developers work on their local machines (ignoring
    the cloud development environment here) and, eventually, the code is deployed
    to the production environment. But there are a few steps between those two extremes.
    Complex systems operate in an environment that is not easy to duplicate locally.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建大型系统时，开发人员在本地机器上工作（这里暂时忽略云开发环境），最终，代码会部署到生产环境。但在这两者之间有一些步骤。复杂的系统运行在一个不容易在本地复制的环境中。
- en: You should test changes to code or configuration in an environment that is similar
    to your production environment. This is your staging environment, where you should
    catch most problems that can’t be caught by the developer running tests locally
    in their development environment.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在一个与生产环境相似的环境中测试代码或配置的变更。这就是你的预生产环境，在这里你应该能够发现大多数无法通过开发人员在本地开发环境中运行测试捕获到的问题。
- en: The software delivery process should accommodate the detection of bad code and
    configuration as early as possible. But, sometimes, bad changes will be detected
    only in production and cause an incident. You should have an incident management
    process in place as well, which typically involves reverting to the previous version
    of whatever component caused the issue and then trying to find the root cause
    by looking at logs, metrics, and traces – sometimes, by debugging in the staging
    environment too.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 软件交付过程应该尽早检测到不良代码和配置。但有时候，不良变更只能在生产环境中被检测到，并导致事故。你应该建立一个事故管理流程，通常包括恢复到引发问题的组件的先前版本，然后通过查看日志、指标和追踪——有时也需要在预生产环境中调试——来寻找根本原因。
- en: But, sometimes, the problem is not with your code or configuration. In the end,
    your Kubernetes cluster runs on nodes (yes, even if it’s managed), and those nodes
    can suffer many issues.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，有时候问题并不在于你的代码或配置。最终，你的Kubernetes集群运行在节点上（即便它是托管的），这些节点可能会遇到许多问题。
- en: Detecting problems at the node level
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在节点级别检测问题
- en: 'In Kubernetes’ conceptual model, the unit of work is the pod. However, pods
    are scheduled on nodes. When it comes to monitoring and the reliability of the
    infrastructure, the nodes are what require the most attention, as Kubernetes itself
    (the scheduler, replica sets, and horizontal pod autoscalers) takes care of the
    pods. The kubelet is aware of many issues on the nodes and it will update the
    API server. You can see the node status and if it’s ready using this command:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes的概念模型中，工作单元是pod。然而，pod是被调度到节点上的。在基础设施的监控和可靠性方面，节点是最需要关注的部分，因为Kubernetes本身（调度器、副本集和水平pod自动扩展器）会管理pod。kubelet能够识别节点上的许多问题，并会更新API服务器。你可以使用以下命令查看节点状态及其是否准备就绪：
- en: '[PRE27]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note the last condition, `Ready`. This means that Kubernetes can schedule pending
    pods to this node.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 注意最后一个条件，`Ready`。这意味着Kubernetes可以将待调度的pod安排到此节点上。
- en: 'But, there might be problems that the kubelet can’t detect. Some of the problems
    are:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，可能会有一些问题是 kubelet 无法检测到的。以下是一些问题：
- en: Bad CPU
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坏 CPU
- en: Bad memory
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坏内存
- en: Bad disk
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坏磁盘
- en: Kernel deadlock
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核死锁
- en: Corrupt filesystem
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件系统损坏
- en: Problems with the container runtime (e.g., Docker daemon)
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器运行时的问题（例如，Docker 守护进程）
- en: We need another solution. Enter the node problem detector.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要另一个解决方案。引入节点问题检测器。
- en: The node problem detector is a pod that runs on every node. It needs to solve
    a difficult problem. It must be able to detect various low-level problems across
    different environments, different hardware, and different operating systems. It
    must be reliable enough not to be affected itself (otherwise, it can’t report
    on problems), and it needs to have a relatively low overhead to avoid spamming
    the control plane. The source code is at [https://github.com/kubernetes/node-problem-detector](https://github.com/kubernetes/node-problem-detector).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 节点问题检测器是一个在每个节点上运行的 Pod。它需要解决一个困难的问题。它必须能够在不同的环境、不同的硬件和不同的操作系统中检测各种低级问题。它必须足够可靠，不受自身影响（否则，它无法报告问题），并且需要具有相对较低的开销，以避免对控制平面造成垃圾邮件影响。源代码位于
    [https://github.com/kubernetes/node-problem-detector](https://github.com/kubernetes/node-problem-detector)。
- en: The most natural way is to deploy the node problem detector as a DaemonSet,
    so every node always has a node problem detector running on it. On Google’s GKE
    clusters, it runs as an add-on.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 最自然的方式是将节点问题检测器部署为 DaemonSet，这样每个节点上总会有一个节点问题检测器在运行。在 Google 的 GKE 集群中，它作为附加组件运行。
- en: Problem daemons
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题守护进程
- en: The problem with the node problem detector (pun intended) is that there are
    too many problems that it needs to handle. Trying to cram all of them into a single
    codebase can lead to a complex, bloated, and never-stabilizing codebase. The design
    of the node problem detector calls for the separation of the core functionality
    of reporting node problems to the master from the specific problem detection.
    The reporting API is based on generic conditions and events. The problem detection
    should be done by separate problem daemons (each in its own container). This way,
    it is possible to add and evolve new problem detectors without impacting the core
    node problem detector. In addition, the control plane may have a remedy controller
    that can resolve some node problems automatically, therefore implementing self-healing.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 节点问题检测器（字面上的双关语）的问题在于，它需要处理的“问题”太多了。试图将所有这些问题塞进一个单一的代码库中可能导致一个复杂、臃肿、永远无法稳定的代码库。节点问题检测器的设计要求将报告节点问题的核心功能与特定问题检测分开。报告
    API 基于通用条件和事件。问题检测应该由独立的问题守护进程（每个进程在自己的容器中）来完成。这样，可以在不影响核心节点问题检测器的情况下，添加和发展新的问题检测器。此外，控制平面可能有一个补救控制器，能够自动解决一些节点问题，从而实现自愈。
- en: At this time, problem daemons are baked into the node problem detector binary,
    and they execute as Goroutines, so you don’t get the benefits of the loosely-coupled
    design just yet. In the future, each problem daemon will run in its own container.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，问题守护进程被嵌入到节点问题检测器的二进制文件中，并以 Goroutine 形式执行，因此你还不能享受到松耦合设计的好处。未来，每个问题守护进程将运行在自己的容器中。
- en: In addition to problems with nodes, the other area where things can break down
    is networking. The various monitoring tools we discussed earlier can help us identify
    problems across the infrastructure, in our code, or with third-party dependencies.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 除了节点问题，另一个可能发生故障的领域是网络。我们之前讨论过的各种监控工具可以帮助我们识别基础设施、代码或第三方依赖中的问题。
- en: Let’s talk about the various options in our toolbox, how they compare, and how
    to utilize them for maximal effect.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来谈谈我们工具箱中的各种选项，它们如何比较，并且如何最大化它们的效果。
- en: Dashboards vs. alerts
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仪表盘 vs. 警报
- en: Dashboards are purely for humans. The idea of a good dashboard is to provide,
    at one glance, a lot of useful information about the state of the system or a
    particular component. There are many user experience elements to designing good
    dashboards, just like designing any user interface. Monitoring dashboards can
    cover a lot of data across many components, over long time periods, and may support
    drilling down into finer and finer levels of detail.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表盘纯粹是为人类设计的。一个好的仪表盘的设计理念是通过一眼就能提供大量关于系统或特定组件状态的有用信息。设计一个好的仪表盘有很多用户体验的元素，和设计任何用户界面一样。监控仪表盘可以覆盖许多组件的数据，跨越长时间段，并且可能支持深入挖掘到更细节的层次。
- en: Alerts, on the other hand, are periodically checking certain conditions (often
    based on metrics) and, when triggered, can either result in automatic resolution
    of the cause of the alert or eventually notify a human, who will probably start
    the investigation by looking at some dashboards.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 警报则是定期检查某些条件（通常基于指标），一旦触发，可能会自动解决警报的原因，或者最终通知人类，后者通常会通过查看一些仪表板开始调查。
- en: Self-healing systems can handle certain alerts automatically (or ideally, resolve
    the issue before an alert is even raised). Humans will typically be involved in
    troubleshooting. Even in cases where the system automatically recovered from a
    problem at some point, a human will review the actions the system took and verify
    that the current behavior, including the automatic recovery from problems, is
    adequate.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 自愈系统可以自动处理某些警报（或者理想情况下，在警报被触发之前解决问题）。人类通常会参与故障排除。即使系统在某些情况下自动恢复了问题，也会有人类回顾系统所采取的措施，并验证当前行为，包括自动恢复是否足够。
- en: In many cases, severe problems (a.k.a. incidents) discovered by humans looking
    at dashboards (not scalable) or notified by alerts will require some investigation,
    remediation, and later, post-mortem. In all those stages, the next layer of monitoring
    comes into play.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，由人类通过查看仪表板（不可扩展）或通过警报通知发现的严重问题（即事件）将需要一些调查、修复，并在之后进行事后分析。在这些阶段，下一层的监控发挥作用。
- en: Logs vs metrics vs. error reports
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志与指标与错误报告
- en: 'Let’s understand where each of these tools excels and how to best combine their
    strengths to debug difficult problems. Let’s assume we have good test coverage
    and our business/domain logic code is by and large correct. We run into problems
    in the production environment. There could be several types of problems that happen
    only in production:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解每个工具的优势，并如何将它们的优点结合起来以调试难题。假设我们有良好的测试覆盖率，并且我们的业务/领域逻辑代码大体上是正确的。我们遇到的问题发生在生产环境中。可能会有几种只在生产环境中发生的问题：
- en: Misconfiguration (production configuration is incorrect or out of date)
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置错误（生产环境配置错误或过时）
- en: Infrastructure provisioning
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施的配置
- en: Insufficient permissions and access to data, services, or third-party integrations
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权限不足，无法访问数据、服务或第三方集成
- en: Environment-specific code
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境特定代码
- en: Software bugs that are exposed by production inputs
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产输入暴露的软件漏洞
- en: Scalability and performance issues
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展性和性能问题
- en: 'That’s quite a list and it’s probably not even complete. Typically, when something
    goes wrong, it is in response to some change. What kind of changes are we talking
    about? Here are a few:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实是一个长长的清单，而且可能还不完整。通常，当出现问题时，都是响应某些变化。那么我们在说什么样的变化呢？以下是一些例子：
- en: Deployment of a new version of the code
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署新版本的代码
- en: Dynamic re-configuration of a deployed application
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署应用程序的动态重新配置
- en: New users or existing users changing the way they interact with the system
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新用户或现有用户改变与系统交互的方式
- en: Changes to the underlying infrastructure (e.g., by the cloud provider)
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底层基础设施的变化（例如，由云服务提供商引起）
- en: A new path in the code is utilized for the first time (e.g., fallback to another
    region)
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码中新路径首次被使用（例如，回退到另一个区域）
- en: Since there is such a broad spectrum of problems and causes, it is difficult
    to suggest a linear path to resolution. For example, if the failure caused an
    error, then looking at an error report might be the best starting point. But,
    if the issue is that some action that was supposed to happen didn’t happen, then
    there is no error to look at. In this case, it might make sense to look at the
    logs and compare them to the logs from a previous successful request. In case
    of infrastructure or scalability problems, metrics may give us the best initial
    insight.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 由于问题和原因的范围非常广泛，因此很难建议一个线性的解决路径。例如，如果故障导致错误，那么查看错误报告可能是最好的起点。但如果问题是某个本应发生的动作没有发生，那么就没有错误可供查看。在这种情况下，可能需要查看日志并将其与之前成功请求的日志进行比较。在基础设施或扩展性问题的情况下，指标可能为我们提供最初的洞察。
- en: The bottom line is that debugging distributed systems requires using multiple
    tools together in the pursuit of the ever-elusive root cause.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 底线是，调试分布式系统需要将多个工具结合使用，以追寻那永远难以捉摸的根本原因。
- en: Of course, in distributed systems with lots of components and microservices,
    it is not even clear where to look. This is where distributed tracing shines and
    can help us narrow down and identify the culprit.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Detecting performance and root cause with distributed tracing
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With distributed tracing in place, every request will generate a trace with
    a graph of spans. Jaeger uses sampling of 1/1000 by default so, once in a blue
    moon, issues might escape it, but for persistent problems, we will be able to
    follow the path of a request, see how long each span takes, and if the processing
    of a request bails out for some reason, it will be very easy to notice. At this
    point, you’re back to the logs, metrics, and errors to hunt the root cause.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, troubleshooting problems in a complex system like Kubernetes
    is far from trivial. You need comprehensive observability in place including logging,
    metrics, and distributed tracing. You also need deep knowledge and understanding
    of your system to be able to configure, monitor, and mitigate issues quickly and
    reliably.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we covered the topics of monitoring, observability, and troubleshooting.
    We started with a review of the various aspects of monitoring: logs, metrics,
    error reporting, and distributed tracing. Then, we discussed how to incorporate
    monitoring capabilities into your Kubernetes cluster. We looked at several CNCF
    projects, like Fluentd for log aggregation, Prometheus for metrics collection
    and alert management, Grafana for visualization, and Jaeger for distributed tracing.
    Then, we explored troubleshooting large distributed systems. We realized how difficult
    it can be and why we need so many different tools to conquer the issues.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take it to the next level and dive into service
    meshes. I’m super excited about service meshes because they take much of the complexity
    related to cloud-native microservice-based applications and externalize them outside
    of the microservices. That has a lot of real-world value.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Join us on Discord!
  id: totrans-383
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read this book alongside other users, cloud experts, authors, and like-minded
    professionals.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Ask questions, provide solutions to other readers, chat with the authors via.
    Ask Me Anything sessions and much more.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Scan the QR code or visit the link to join the community now.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code844810820358034203.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
