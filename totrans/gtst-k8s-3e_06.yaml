- en: Application Updates, Gradual Rollouts, and Autoscaling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序更新、渐进式发布和自动扩展
- en: This chapter will expand upon the core concepts, and show you how to roll out
    updates and test new features of your application with minimal disruption to uptime.
    It will cover the basics of doing application updates, gradual rollouts, and A/B
    testing. In addition, we will look at scaling the Kubernetes cluster itself.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将扩展核心概念，向您展示如何推出更新并测试应用程序的新功能，同时尽量减少对正常运行时间的干扰。它将涵盖应用更新、渐进式发布和 A/B 测试的基础知识。此外，我们还将研究如何扩展
    Kubernetes 集群本身。
- en: In version 1.2, Kubernetes released a Deployments API. Deployments are the recommended
    way to deal with scaling and application updates going forward. As mentioned in
    previous chapters, `ReplicationControllers` are no longer the recommended manner
    for managing application updates. However, as they're still core functionality
    for many operators, we will explore rolling updates in this chapter as an introduction
    to the scaling concept and then dive into the preferred method of using Deployments
    in the next chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在版本 1.2 中，Kubernetes 发布了部署（Deployments）API。部署是今后处理扩展和应用程序更新的推荐方式。如前几章所述，`ReplicationControllers`
    已不再是管理应用更新的推荐方式。然而，由于它们仍然是许多操作员的核心功能，我们将在本章中探索滚动更新，以介绍扩展概念，然后在下一章深入探讨使用部署的首选方法。
- en: We'll also investigate the functionality of Helm and Helm Charts that will help
    you manage Kubernetes resources. Helm is a way to manage packages in Kubernetes
    much in the same way that `apt`/`yum` manage code in the Linux ecosystem. Helm
    also lets you share your applications with others, and most importantly create
    reproducible builds of Kubernetes applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将调查 Helm 和 Helm Charts 的功能，这将帮助您管理 Kubernetes 资源。Helm 是管理 Kubernetes 包的一种方式，就像
    `apt`/`yum` 在 Linux 生态系统中管理代码一样。Helm 还允许您与他人共享您的应用程序，最重要的是创建可重现的 Kubernetes 应用程序构建。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Application scaling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序扩展
- en: Rolling updates
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动更新
- en: A/B testing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A/B 测试
- en: Application autoscaling
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序自动扩展
- en: Scaling up your cluster
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展您的集群
- en: Using Helm
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Helm
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You''ll need to have your Google Cloud Platform account enabled and logged
    in, or you can use a local Minikube instance of Kubernetes. You can also use Play
    with Kubernetes over the web: [https://labs.play-with-k8s.com/](https://labs.play-with-k8s.com/).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要启用并登录到您的 Google Cloud Platform 帐户，或者可以使用本地的 Minikube 实例来运行 Kubernetes。您还可以通过网络使用
    Play with Kubernetes：[https://labs.play-with-k8s.com/](https://labs.play-with-k8s.com/)。
- en: 'Here''s the GitHub repository for this chapter: [https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter06](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter06)[.](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code%20files/Chapter%2006)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章的 GitHub 仓库：[https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter06](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter06)[.](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code%20files/Chapter%2006)
- en: Example setup
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例设置
- en: 'Before we start exploring the various capabilities built into Kubernetes for
    scaling and updates, we will need a new example environment. We are going to use
    a variation of our previous container image with a blue background (refer to the
    *v0.1 and v0.2 (side by side)* image, later in this chapter, for a comparison).
    We have the following code in the `pod-scaling-controller.yaml` file:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始探索 Kubernetes 中内置的各种扩展和更新功能之前，我们需要一个新的示例环境。我们将使用之前容器镜像的一个变种，背景为蓝色（请参见本章后面的
    *v0.1 和 v0.2（并排）* 图像进行比较）。我们在 `pod-scaling-controller.yaml` 文件中有以下代码：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Save the following code as `pod-scaling-service.yaml` file:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下代码保存为 `pod-scaling-service.yaml` 文件：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create these services with the following commands:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令创建这些服务：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The public IP address for the service may take a moment to create.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的公共 IP 地址可能需要一些时间来创建。
- en: Scaling up
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展
- en: Over time, as you run your applications in the Kubernetes cluster, you will
    find that some applications need more resources, whereas others can manage with
    fewer resources. Instead of removing the entire `ReplicationControllers` (and
    associated pods), we want a more seamless way to scale our application up and
    down.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，当你在Kubernetes集群中运行应用程序时，你会发现一些应用程序需要更多资源，而另一些则能以较少的资源运行。与其删除整个`ReplicationControllers`（及相关Pods），我们更希望有一种无缝的方法来扩展或收缩我们的应用程序。
- en: 'Thankfully, Kubernetes includes a `scale` command, which is suited specifically
    for this purpose. The `scale` command works both with `ReplicationControllers`
    and the new Deployments abstraction. For now, we will explore its use with `ReplicationControllers`.
    In our new example, we have only one replica running. You can check this with
    a `get pods` command:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Kubernetes包含一个专门用于此目的的`scale`命令。`scale`命令既适用于`ReplicationControllers`，也适用于新的部署抽象。现在，我们将探讨它与`ReplicationControllers`一起使用的方式。在我们的新示例中，只有一个副本在运行。你可以通过`get
    pods`命令检查这一点：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s try scaling that up to three with the following command:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着用以下命令将其扩展到三个：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If all goes well, you'll simply see the `scaled` word on the output of your
    Terminal window.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你将只在终端窗口的输出中看到`scaled`这个词。
- en: Optionally, you can specify the `--current-replicas` flag as a verification
    step. The scaling will only occur if the actual number of replicas currently running
    matches this count.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，你可以指定`--current-replicas`标志作为验证步骤。只有当前运行的副本数量与该计数匹配时，扩展操作才会发生。
- en: After listing our pods once again, we should now see three pods running with
    a name similar to `node-js-scale-XXXXX`, where the `X` characters are a random
    string.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在再次列出我们的Pods之后，我们应该现在能看到三个名称类似于`node-js-scale-XXXXX`的Pods，其中`X`字符是一个随机字符串。
- en: You can also use the `scale` command to reduce the number of replicas. In either
    case, the `scale` command adds or removes the necessary pod replicas, and the
    service automatically updates and balances across new or remaining replicas.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用`scale`命令减少副本的数量。在任何一种情况下，`scale`命令都会添加或移除必要的Pod副本，并且服务会自动更新并平衡到新的或剩余的副本上。
- en: Smooth updates
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平滑更新
- en: The scaling of our application up and down as our resource demands change is
    useful for many production scenarios, but what about simple application updates?
    Any production system will have code updates, patches, and feature additions.
    These could be occurring monthly, weekly, or even daily. Making sure that we have
    a reliable way to push out these changes without interruption to our users is
    a paramount consideration.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的资源需求变化，我们的应用程序的扩展和收缩对于许多生产场景来说非常有用，但对于简单的应用更新怎么办呢？任何生产系统都会进行代码更新、修复补丁和功能添加。这些更新可能是每月、每周，甚至每天发生。确保我们有一种可靠的方法来推送这些更改而不会中断用户使用是至关重要的。
- en: Once again, we benefit from the years of experience the Kubernetes system is
    built on. There is built-in support for rolling updates with the 1.0 version.
    The `rolling-update` command allows us to update entire `ReplicationControllers`
    or just the underlying Docker image used by each replica. We can also specify
    an update interval, which will allow us to update one pod at a time and wait until
    proceeding to the next.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 再次受益于Kubernetes系统多年的经验。1.0版本中内置了对滚动更新的支持。`rolling-update`命令允许我们更新整个`ReplicationControllers`，或者仅更新每个副本使用的底层Docker镜像。我们还可以指定更新间隔，这样可以一次更新一个Pod，并在继续进行到下一个之前等待。
- en: 'Let''s take our scaling example and perform a rolling update to the 0.2 version
    of our container image. We will use an update interval of 2 minutes, so we can
    watch the process as it happens in the following way:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以扩展示例为例，执行一个滚动更新，将容器镜像更新到0.2版本。我们将使用2分钟的更新间隔，这样可以通过以下方式观察整个过程：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You should see some text about creating a new `ReplicationControllers` named
    `node-js-scale-XXXXX`, where the `X` characters will be a random string of numbers
    and letters. In addition, you will see the beginning of a loop that starts one
    replica of the new version and removes one from the existing `ReplicationControllers`.
    This process will continue until the new `ReplicationControllers` has the full
    count of replicas running.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到一些关于创建一个新的名为`node-js-scale-XXXXX`的`ReplicationControllers`的文本，其中`X`字符是一个由数字和字母组成的随机字符串。此外，你还会看到一个循环的开始，它会启动新版本的一个副本，并移除现有`ReplicationControllers`中的一个副本。这个过程将继续，直到新的`ReplicationControllers`拥有完整数量的副本。
- en: 'If we want to follow along in real time, we can open another Terminal window
    and use the `get pods` command, along with a label filter, to see what''s happening:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想实时跟踪，可以打开另一个终端窗口，使用`get pods`命令和标签过滤器来查看发生了什么：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This command will filter for pods with `node-js-scale` in the name. If you run
    this after issuing the `rolling-update` command, you should see several pods running
    as it creates new versions and removes the old ones one by one.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将筛选出名称中包含`node-js-scale`的pod。如果你在执行`rolling-update`命令后运行这个命令，你应该会看到几个pod在运行，因为它在创建新版本并逐个删除旧版本。
- en: 'The full output of the previous `rolling-update` command should look something
    like this screenshot:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个`rolling-update`命令的完整输出应该类似于下面这张截图：
- en: '![](img/38831f58-cd3a-4522-8d71-aa308a1aca23.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/38831f58-cd3a-4522-8d71-aa308a1aca23.png)'
- en: The scaling output
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展输出
- en: As we can see here, Kubernetes is first creating a new `ReplicationController` named
    `node-js-scale-10ea08ff9a118ac6a93f85547ed28f6`. K8s then loops through one by
    one, creating a new pod in the new controller and removing one from the old. This
    continues until the new controller has the full replica count and the old one
    is at zero. After this, the old controller is deleted and the new one is renamed
    with the original controller's name.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，Kubernetes首先创建了一个名为`node-js-scale-10ea08ff9a118ac6a93f85547ed28f6`的新`ReplicationController`。然后K8s逐个循环，先在新控制器中创建一个新的pod，再从旧控制器中删除一个pod。这个过程一直持续，直到新控制器的副本数量达到完整，旧控制器的副本数量为零。之后，旧控制器被删除，新控制器被重命名为原控制器的名称。
- en: 'If you run a `get pods` command now, you''ll notice that the pods still all
    have a longer name. Alternatively, we could have specified the name of a new controller
    in the command, and Kubernetes will create a new `ReplicationControllers` and
    pods using that name. Once again, the controller of the old name simply disappears
    after the update is completed. I recommend that you specify a new name for the
    updated controller to avoid confusion in your pod naming down the line. The same
    `update` command with this method will look like this:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在运行`get pods`命令，你会注意到这些pod仍然有较长的名称。或者，我们可以在命令中指定一个新的控制器名称，Kubernetes会使用该名称创建新的`ReplicationControllers`和pod。再次强调，旧名称的控制器在更新完成后会自动消失。我建议你为更新后的控制器指定一个新的名称，以避免将来在pod命名时产生混淆。使用这种方法的相同`update`命令如下所示：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Using the static external IP address from the service we created in the first
    section, we can open the service in a browser. We should see our standard container
    information page. However, you''ll notice that the title now says Pod Scaling
    v0.2 and the background is light yellow:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们在第一部分创建的服务的静态外部IP地址，我们可以在浏览器中打开该服务。我们应该能看到标准的容器信息页面。然而，你会注意到标题现在显示为Pod Scaling
    v0.2，背景为浅黄色：
- en: '![](img/a03d2bc3-d062-45b6-8bde-9fa33024773b.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a03d2bc3-d062-45b6-8bde-9fa33024773b.png)'
- en: v0.1 and v0.2 (side by side)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: v0.1和v0.2（并排显示）
- en: It's worth noting that, during the entire update process, we've only been looking
    at pods and `ReplicationControllers`. We didn't do anything with our service,
    but the service is still running fine and now directing to the new version of
    our pods. This is because our service is using label selectors for membership.
    Because both our old and new replicas use the same labels, the service has no
    problem using the new pods to service requests. The updates are done on the pods
    one by one, so it's seamless for the users of the service.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在整个更新过程中，我们只关注了pod和`ReplicationControllers`。我们没有对服务做任何操作，但服务依然正常运行，并且现在指向新版本的pod。这是因为我们的服务使用标签选择器来进行成员管理。由于我们的旧副本和新副本使用相同的标签，服务没有问题，可以使用新pod来处理请求。更新是在pod上逐个进行的，因此对服务的用户来说是无缝的。
- en: Testing, releases, and cutovers
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试、发布和切换
- en: The rolling update feature can work well for a simple blue-green deployment
    scenario. However, in a real-world blue-green deployment with a stack of multiple
    applications, there can be a variety of inter-dependencies that require in-depth
    testing. The `update-period` command allows us to add a `timeout` flag where some
    testing can be done, but this will not always be satisfactory for testing purposes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新功能在简单的蓝绿部署场景中可以很好地工作。然而，在实际的蓝绿部署中，涉及多个应用程序的堆栈可能会有各种依赖关系，需要进行深入的测试。`update-period`命令允许我们添加一个`timeout`标志，在这里可以进行一些测试，但这对于测试目的来说并不总是足够的。
- en: Similarly, you may want partial changes to persist for a longer time and all
    the way up to the load balancer or service level. For example, you may wish to
    run an A/B test on a new user interface feature with a portion of your users.
    Another example is running a canary release (a replica in this case) of your application
    on new infrastructure, such as a newly added cluster node.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，您可能希望将部分更改持续较长时间，并一直持续到负载均衡器或服务级别。例如，您可能希望对一部分用户运行新用户界面功能的A/B测试。另一个例子是将您的应用程序以金丝雀发布（在这种情况下是一个副本）的方式运行在新基础设施上，比如新增的集群节点。
- en: 'Let''s take a look at an A/B testing example. For this example, we will need
    to create a new service that uses `sessionAffinity`. We will set the affinity
    to `ClientIP`, which will allow us to forward clients to the same backend pod.
    The following listing `pod-AB-service.yaml` is the key if we want a portion of
    our users to see one version while others see another:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个A/B测试的示例。在这个示例中，我们需要创建一个使用`sessionAffinity`的新服务。我们将亲和性设置为`ClientIP`，这样可以将客户端引导到相同的后端Pod。以下列出的`pod-AB-service.yaml`文件是关键，如果我们希望一部分用户看到一个版本，而其他用户看到另一个版本：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create this service as usual with the `create` command, as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下方式，像往常一样使用`create`命令创建此服务：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This will create a service that will point to our pods running both version
    0.2 and 0.3 of the application. Next, we will create the two `ReplicationControllers` that create
    two replicas of the application. One set will have version 0.2 of the application,
    and the other will have version 0.3, as shown in the listing `pod-A-controller.yaml`and `pod-B-controller.yaml`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个服务，指向运行应用程序版本0.2和0.3的Pod。接下来，我们将创建两个`ReplicationControllers`，它们会创建应用程序的两个副本。一组将运行版本0.2，另一组将运行版本0.3，如`pod-A-controller.yaml`和`pod-B-controller.yaml`文件所示：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note that we have the same service label, so these replicas will also be added
    to the service pool based on this selector. We also have `livenessProbe` and `readinessProbe`
    defined to make sure that our new version is working as expected. Again, use the
    `create` command to spin up the controller:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们有相同的服务标签，因此这些副本也将根据此选择器添加到服务池中。我们还定义了`livenessProbe`和`readinessProbe`，以确保我们的新版本按预期工作。同样，使用`create`命令启动控制器：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, we have a service balancing both versions of our app. In a true A/B test,
    we would now want to start collecting metrics on the visits to each version. Again,
    we have `sessionAffinity` set to `ClientIP`, so all requests will go to the same
    pod. Some users will see v0.2, and some will see v0.3.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个平衡两个版本应用的服务。在真正的A/B测试中，我们现在希望开始收集每个版本的访问指标。同样，我们将`sessionAffinity`设置为`ClientIP`，因此所有请求将指向同一Pod。一部分用户将看到v0.2，另一部分用户将看到v0.3。
- en: Because we have `sessionAffinity` turned on, your test will likely show the
    same version every time. This is expected, and you would need to attempt a connection
    from multiple IP addresses to see both user experiences with each version.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们开启了`sessionAffinity`，所以您的测试很可能每次都会显示相同的版本。这是预期的，您需要尝试从多个IP地址连接，以查看每个版本的用户体验。
- en: Since the versions are each on their own pod, one can easily separate logging
    and even add a logging container to the pod definition for a sidecar logging pattern.
    For brevity, we will not cover that setup in this book, but we will look at some
    of the logging tools in Chapter 8, *Monitoring and Logging*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个版本都运行在独立的Pod上，您可以轻松分离日志记录，甚至为Pod定义添加一个日志容器，采用边车日志模式。为了简洁起见，本书不讨论此设置，但我们将在第8章《监控与日志》中讨论一些日志工具。
- en: We can start to see how this process will be useful for a canary release or
    a manual blue-green deployment. We can also see how easy it is to launch a new
    version and slowly transition over to the new release.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以开始看到这个过程如何对金丝雀发布或手动蓝绿部署有所帮助。我们还可以看到推出新版本并逐步过渡到新版本是多么容易。
- en: 'Let''s look at a basic transition quickly. It''s really as simple as a few
    `scale` commands, which are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看看一个基本的过渡。实际上，只需几个`scale`命令，命令如下：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Use the `get pods` command combined with the `-l` filter in between the `scale`
    commands to watch the transition as it happens.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`get pods`命令，并结合`-l`过滤器，在`scale`命令之间观察过渡过程。
- en: 'Now, we have fully transitioned over to version 0.3 (`node-js-scale-b`). All
    users will now see version 0.3 of the site. We have four replicas of version 0.3
    and none of 0.2\. If you run a `get rc` command, you will notice that we still
    have an `ReplicationControllers` for 0.2 (`node-js-scale-a`). As a final cleanup,
    we can remove that controller completely, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经完全过渡到版本 0.3（`node-js-scale-b`）。所有用户现在都会看到版本 0.3 的网站。我们有四个版本 0.3 的副本，没有版本
    0.2 的副本。如果你运行 `get rc` 命令，你会注意到我们仍然有一个版本 0.2 的 `ReplicationControllers`（`node-js-scale-a`）。作为最终的清理，我们可以完全移除这个控制器，如下所示：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Application autoscaling
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序自动扩缩
- en: A recent feature addition to Kubernetes is that of the Horizontal Pod Autoscaler.
    This resource type is really useful as it gives us a way to automatically set
    thresholds for scaling our application. Currently, that support is only for CPU,
    but there is alpha support for custom application metrics as well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 最近新增了一个特性，那就是水平 pod 自动扩缩器。这个资源类型非常有用，因为它为我们提供了一种自动设置应用程序扩展阈值的方法。目前，支持的阈值仅限于
    CPU，但也有对自定义应用程序指标的 alpha 支持。
- en: 'Let''s use the `node-js-scale` `ReplicationController` from the beginning of
    the chapter and add an autoscaling component. Before we start, let''s make sure
    we are scaled back down to one replica using the following command:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用章节开头的 `node-js-scale` `ReplicationController`，并添加自动扩缩组件。在我们开始之前，让我们确保使用以下命令将副本缩减回一个：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we can create a Horizontal Pod Autoscaler, `node-js-scale-hpa.yaml` with
    the following `hpa` definition:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个水平 pod 自动扩缩器，`node-js-scale-hpa.yaml`，并定义以下 `hpa` 配置：
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Go ahead and create this with the `kubectl create -f` command. Now, we can
    list the Horizontal Pod Autoscaler and get a description as well:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用 `kubectl create -f` 命令创建它。现在，我们可以列出水平 pod 自动扩缩器并获取描述：
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can also create autoscaling in the command line with the `kubectl autoscale`
    command. The preceding YAML will look like the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过命令行使用 `kubectl autoscale` 命令创建自动扩缩。之前的 YAML 文件将如下所示：
- en: '`$ kubectl autoscale rc/node-js-scale --min=1 --max=3 --cpu-percent=20`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl autoscale rc/node-js-scale --min=1 --max=3 --cpu-percent=20`'
- en: 'This will show us an autoscaler on `node-js-scale` `ReplicationController`
    with a target CPU of 30%. Additionally, you will see that minimum pods is set
    to 1 and maximum to 3:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示我们一个 `node-js-scale` `ReplicationController` 的自动扩缩器，目标 CPU 使用率为 30%。另外，你会看到最小
    pod 数为 1，最大为 3：
- en: '![](img/e4495140-c49f-42c1-bf55-033b0f187cc3.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4495140-c49f-42c1-bf55-033b0f187cc3.png)'
- en: Horizontal pod autoscaler with no load
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 没有负载时的水平 pod 自动扩缩器
- en: 'Let''s also query our pods to see how many are running right now:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查询一下我们的 pod，看看现在有多少个正在运行：
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We should see only one `node-js-scale` pod because our Horizontal Pod Autoscaler
    is showing 0% utilization, so we will need to generate some load. We will use
    the popular `boom` application common in many container demos. The following listing `boomload.yaml`
    will help us create continuous load until we can hit the CPU threshold for the
    autoscaler:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该只看到一个 `node-js-scale` pod，因为我们的水平 pod 自动扩缩器显示 CPU 使用率为 0%，所以我们需要生成一些负载。我们将使用在许多容器演示中常见的流行应用程序
    `boom`。以下清单 `boomload.yaml` 将帮助我们创建持续的负载，直到我们达到自动扩缩器的 CPU 阈值：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Use the `kubectl create -f` command with this listing and then be ready to start
    monitoring the `hpa`. We can do this with the `kubectl get hpa` command we used
    earlier.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl create -f`命令和这个清单，然后准备开始监控`hpa`。我们可以使用之前用过的`kubectl get hpa`命令来做到这一点。
- en: 'It may take a few moments, but we should start to see the current CPU utilization
    increase. Once it goes above the 20% threshold we set, the autoscaler will kick
    in:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要一些时间，但我们应该开始看到当前的 CPU 使用率增加。一旦超过我们设定的 20% 阈值，自动扩缩器将开始工作：
- en: '![](img/5b59c037-47ee-4d31-91c8-9df165196eb2.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b59c037-47ee-4d31-91c8-9df165196eb2.png)'
- en: Horizontal pod autoscaler after load starts
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 负载开始后，水平 pod 自动扩缩器
- en: 'Once we see this, we can run `kubectl get pod` again and see there are now
    several `node-js-scale` pods:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦看到这一点，我们可以再次运行 `kubectl get pod`，看到现在有几个 `node-js-scale` pod：
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can clean up now by killing our load generation pod:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过终止我们的负载生成 pod 来清理：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now, if we watch the `hpa`, we should start to see the CPU usage drop. It may
    take a few minutes, but eventually we will go back down to 0% CPU load.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们观察 `hpa`，我们应该开始看到 CPU 使用率下降。可能需要几分钟，但最终我们会回到 0% 的 CPU 负载。
- en: Scaling a cluster
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展集群
- en: All these techniques are great for scaling the application, but what about the
    cluster itself? At some point, you will pack the nodes full and need more resources
    to schedule new pods for your workloads.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些技术对于扩展应用程序都非常有用，但集群本身呢？在某个时刻，你会将节点填满，并需要更多资源来调度新 pod 以容纳你的工作负载。
- en: Autoscaling
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动扩展
- en: When you create your cluster, you can customize the starting number of nodes
    (minions) with the `NUM_MINIONS` environment variable. By default, it is set to
    4.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建集群时，可以通过 `NUM_MINIONS` 环境变量自定义启动时的节点（minion）数量。默认值设置为 4。
- en: Additionally, the Kubernetes team has started to build autoscaling capability
    into the cluster itself. Currently, this is only supported on GCE and GKE, but
    work is being done on other providers. This capability utilizes the `KUBE_AUTOSCALER_MIN_NODES`,
    `KUBE_AUTOSCALER_MAX_NODES`, and `KUBE_ENABLE_CLUSTER_AUTOSCALER` environment
    variables.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Kubernetes 团队已经开始在集群本身中构建自动扩展功能。目前，这项功能仅在 GCE 和 GKE 上受支持，但其他云提供商也在进行相关工作。此功能使用了
    `KUBE_AUTOSCALER_MIN_NODES`、`KUBE_AUTOSCALER_MAX_NODES` 和 `KUBE_ENABLE_CLUSTER_AUTOSCALER`
    环境变量。
- en: 'The following example shows how to set the environment variables for autoscalingbefore
    running `kube-up.sh`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何在运行 `kube-up.sh` 之前设置自动扩展的环境变量：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Also, bear in mind that changing this after the cluster is started will have
    no effect. You would need to tear down the cluster and create it once again. Thus,
    this section will show you how to add nodes to an existing cluster without rebuilding
    it.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，请记住，集群启动后更改这些设置将没有效果。你需要拆除集群并重新创建。因此，本节将展示如何在不重建集群的情况下向现有集群添加节点。
- en: Once you start a cluster with these settings, your cluster will automatically
    scale up and down with the minimum and maximum limits based on compute resource
    usage in the cluster.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你使用这些设置启动集群，你的集群将根据集群中计算资源的使用情况，自动根据最小和最大限制进行扩展和缩减。
- en: GKE clusters also support autoscaling when launched, when using the alpha features.
    The preceding example will use a flag such as `--enable-autoscaling --min-nodes=2 --max-nodes=5`
    in a command-line launch.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: GKE 集群在启动时也支持自动扩展，前提是使用了 alpha 功能。前面的示例将使用类似 `--enable-autoscaling --min-nodes=2
    --max-nodes=5` 的标志进行命令行启动。
- en: Scaling up the cluster on GCE
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 GCE 上扩展集群
- en: If you wish to scale out an existing cluster, we can do it with a few steps.
    Manually scaling up your cluster on GCE is actually quite easy. The existing plumbing
    uses managed instance groups in GCE, which allow you to easily add more machines
    of a standard configuration to the group via an instance template.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望扩展现有集群，可以通过几个步骤实现。在 GCE 上手动扩展集群其实非常简单。现有的基础设施使用了 GCE 中的托管实例组，这使得你可以通过实例模板轻松地向组中添加更多标准配置的机器。
- en: You can see this template easily in the GCE console. First, open the console;
    by default, this should open your default project console. If you are using another
    project for your Kubernetes cluster, simply select it from the project drop-down
    at the top of the page.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GCE 控制台中轻松查看该模板。首先，打开控制台；默认情况下，这将打开你的默认项目控制台。如果你使用其他项目来管理 Kubernetes 集群，只需从页面顶部的项目下拉菜单中选择它。
- en: 'On the side panel, look under Compute and then Compute Engine, and select Instance
    templates. You should see a template titled kubernetes-minion-template. Note that
    the name could vary slightly if you''ve customized your cluster naming settings.
    Click on that template to see the details. Refer to the following screenshot:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在侧边面板中，找到“计算”部分，然后选择“计算引擎”，接着选择“实例模板”。你应该能看到一个名为 kubernetes-minion-template
    的模板。如果你自定义过集群命名设置，名称可能会略有不同。点击该模板查看详细信息。参考以下截图：
- en: '![](img/e6f1d645-a0d4-4a58-b7a3-5ab5f5da1f77.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6f1d645-a0d4-4a58-b7a3-5ab5f5da1f77.png)'
- en: The GCE Instance template for minions
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: GCE 的 minion 实例模板
- en: You'll see a number of settings, but the meat of the template is under the Custom
    metadata. Here, you will see a number of environment variables and also a startup
    script that is run after a new machine instance is created. These are the core
    components that allow us to create new machines and have them automatically added
    to the available cluster nodes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到一系列设置，但模板的核心部分位于“自定义元数据”下。在这里，你会看到一系列环境变量，以及在创建新机器实例后运行的启动脚本。这些是允许我们创建新机器并将其自动添加到可用集群节点中的核心组件。
- en: 'Because the template for new machines is already created, it is very simple
    to scale out our cluster in GCE. Once in the Compute section of the console, simply
    go to Instance groups located right above the Instance templates link on the side
    panel. Again, you should see a group titled kubernetes-minion-group or something
    similar. Click on that group to see the details, as shown in the following screenshot:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于新机器的模板已创建，因此在 GCE 中扩展集群非常简单。进入控制台的 Compute 部分后，只需前往侧边面板上方的实例组链接。您应该会看到一个名为
    kubernetes-minion-group 或类似名称的组。点击该组查看详细信息，如下图所示：
- en: '![](img/36de5070-d467-47cb-ba5e-eb62f9b46575.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36de5070-d467-47cb-ba5e-eb62f9b46575.png)'
- en: The GCE instance group for minions
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: GCE 实例组中的从节点
- en: 'You''ll see a page with a CPU metrics graph and three instances listed here.
    By default, the cluster creates three nodes. We can modify this group by clicking
    on the EDIT GROUP button at the top of the page:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到一个包含 CPU 指标图表和三个实例的页面。默认情况下，集群会创建三个节点。我们可以通过点击页面顶部的 **EDIT GROUP** 按钮来修改此组：
- en: '![](img/7df4a1d2-5706-4ac1-bc8e-a6e40c11d4b7.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7df4a1d2-5706-4ac1-bc8e-a6e40c11d4b7.png)'
- en: The GCE instance group edit page
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: GCE 实例组编辑页面
- en: You should see kubernetes-minion-template selected in the Instance template
    that we reviewed a moment ago. You'll also see an Autoscaling setting, which is
    Off by default, and an instance count of `3`. Simply increment this to `4` and
    click on Save. You'll be taken back to the group details page and you'll see a
    pop-up dialog showing the pending changes.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会看到在我们之前查看的 **Instance template** 中选择了 kubernetes-minion-template。您还会看到一个自动扩展设置，默认情况下是关闭的，并且实例数量为
    `3`。只需将其增加到 `4` 并点击保存。此时，您将被带回到组详情页面，并且会看到一个弹出对话框，显示待处理的更改。
- en: You'll also see some auto healing properties on the Instance groups edit page.
    This recreates failed instances and allows you to set health checks, as well as
    an initial delay period before an action is taken.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您还会在实例组编辑页面看到一些自动修复属性。这些属性会重新创建失败的实例，并允许您设置健康检查以及在采取操作前的初始延迟时间。
- en: 'In a few minutes, you''ll have a new instance listed on the details page. We
    can test that this is ready using the `get nodes` command from the command line:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，您将在详情页中看到一个新实例。我们可以使用命令行中的 `get nodes` 命令来测试它是否已经准备好：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '*A word of caution on autoscaling and scaling down in general*:First, if we
    repeat the earlier process and decrease the countdown to four, GCE will remove
    one node. However, it will not necessarily be the node you just added. The good
    news is that pods will be rescheduled on the remaining nodes. However, it can
    only reschedule where resources are available. If you are close to full capacity
    and shut down a node, there is a good chance that some pods will not have a place
    to be rescheduled. In addition, this is not a live migration, so any application
    state will be lost in the transition. The bottom line is that you should carefully
    consider the implications before scaling down or implementing an autoscaling scheme.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*关于自动扩展和缩容的警告*：首先，如果我们重复前面的操作并将倒计时减少到四个，GCE 将删除一个节点。然而，它不一定是您刚刚添加的节点。好消息是，Pods
    将被重新调度到剩余的节点上。但它只能在资源可用的地方重新调度。如果您接近满负荷并关闭了一个节点，那么很可能会有一些 Pods 无法重新调度到其它地方。此外，这不是实时迁移，因此在过渡过程中，任何应用程序的状态都将丢失。归根结底，在进行缩容或实施自动扩展方案之前，您应该仔细考虑其潜在影响。'
- en: For more information on general autoscaling in GCE, refer to the [https://cloud.google.com/compute/docs/autoscaler/?hl=en_US#scaling_based_on_cpu_utilization](https://cloud.google.com/compute/docs/autoscaler/?hl=en_US#scaling_based_on_cpu_utilization) link.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 GCE 中一般自动扩展的更多信息，请参考 [https://cloud.google.com/compute/docs/autoscaler/?hl=zh_CN#scaling_based_on_cpu_utilization](https://cloud.google.com/compute/docs/autoscaler/?hl=zh_CN#scaling_based_on_cpu_utilization) 链接。
- en: Scaling up the cluster on AWS
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 AWS 上扩展集群
- en: The AWS provider code also makes it very easy to scale up your cluster. Similar
    to GCE, the AWS setup uses autoscaling groups to create the default four minion
    nodes. In the future, the autoscaling groups will hopefully be integrated into
    the Kubernetes cluster autoscaling functionality. For now, we will walk though
    a manual setup.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供者代码同样使得扩展集群变得非常简单。与 GCE 类似，AWS 设置使用自动扩展组来创建默认的四个从节点。未来，自动扩展组有望与 Kubernetes
    集群的自动扩展功能集成。现在，我们将手动设置操作。
- en: 'This can also be easily modified using the CLI or the web console. In the console,
    from the EC2 page, simply go to the Auto Scaling Groups section at the bottom
    of the menu on the left. You should see a name similar to kubernetes-minion-group.
    Select this group and you will see the details shown in the following screenshot:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以通过命令行接口或网页控制台轻松修改。在控制台中，从 EC2 页面，简单地进入左侧菜单底部的自动扩展组部分。你应该会看到一个类似于 kubernetes-minion-group
    的名称。选择此组，你将看到以下截图所示的详细信息：
- en: '![](img/e22e5462-6ef2-494e-8d67-cf585c7b2309.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e22e5462-6ef2-494e-8d67-cf585c7b2309.png)'
- en: Kubernetes minion autoscaling details
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 节点自动扩展详细信息
- en: We can scale this group up easily by clicking on Edit. Then, change the Desired,
    Min, and Max values to `5` and click on Save. In a few minutes, you'll have the
    fifth node available. You can once again check this using the `get nodes` command.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过点击“编辑”轻松地扩展此组。然后，将所需的、最小值和最大值更改为`5`，并点击“保存”。几分钟后，你将看到第五个节点可用。你可以再次使用`get
    nodes`命令来检查。
- en: Scaling down is the same process, but remember that we discussed the same considerations
    in the previous *Scaling up the cluster on GCE *section. Workloads could get abandoned
    or, at the very least, unexpectedly restarted.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 缩减规模的过程是相同的，但请记住，我们在之前的*在 GCE 上扩展集群*部分中讨论了相同的注意事项。工作负载可能会被放弃，或者至少会意外重启。
- en: Scaling manually
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动扩展
- en: For other providers, creating new minions may not be an automated process. Depending
    on your provider, you'll need to perform various manual steps. It can be helpful
    to look at the provider-specific scripts in the `cluster` directory.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他提供商，创建新节点可能不是一个自动化的过程。根据你的提供商，你可能需要执行各种手动步骤。查看`cluster`目录中的特定提供商脚本可能会有所帮助。
- en: Managing applications
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理应用程序
- en: At the time of this book's writing, new software has emerged that hopes to tackle
    the problem of managing Kubernetes applications from a holistic perspective. As
    application installation and continued management grows more complex, software
    such as Helm hopes to ease the pain for cluster operators creating, versioning,
    publishing, and exporting application installation and configuration for other
    operators. You may have also heard the term GitOps, which uses Git as the source
    of truth from which all Kubernetes instances can be managed.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书写作时，出现了新的软件，它希望从整体角度解决管理 Kubernetes 应用程序的问题。随着应用程序安装和持续管理变得越来越复杂，像 Helm 这样的软件希望减轻集群操作员在创建、版本控制、发布和导出应用程序安装及配置时的压力。你可能还听说过
    GitOps 这个术语，它使用 Git 作为真理源，从中可以管理所有 Kubernetes 实例。
- en: While we'll jump deeper into **Continuous Integration and Continuous Delivery**
    (**CI/CD**) in the next chapter, let's see what advantages can be gained by taking
    advantage of package management within the Kubernetes ecosystem. First, it's important
    to understand what problem we're trying to solve when it comes to package management
    within the Kubernetes ecosystem. Helm and programs like it have a lot in common
    with package managers such as `apt`, `yum`, `rpm`, `dpgk`, Aptitude, and Zypper.
    These pieces of software helped users cope during the early days of Linux, where
    programs were simply distributed as source code, with installation documents,
    configuration files, and the necessary moving pieces left to the operator to set
    up. These days of course Linux distributions use a great many pre-built packages,
    which are made available to the user community for consumption on their operating
    system of choice. In many ways, we're in those early days of software management
    for Kubernetes, with many different methods for installing software within many
    different layers of the Kubernetes system. But are there other reasons for  wanting
    a GNU Linux-style package manager for Kubernetes? Perhaps you feel confident that
    by using containers, or Git and configuration management, you can manage on your
    own.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们将在下一章深入探讨**持续集成与持续交付**（**CI/CD**），但首先让我们看看通过利用 Kubernetes 生态系统中的软件包管理可以获得哪些优势。首先，理解我们在
    Kubernetes 生态系统中的软件包管理所要解决的问题是很重要的。Helm 和类似的程序与 `apt`、`yum`、`rpm`、`dpgk`、Aptitude
    和 Zypper 等软件包管理器有很多相似之处。这些软件帮助用户度过了 Linux 的早期阶段，那时程序通常以源代码形式发布，安装文档、配置文件和必要的组件需要操作员自行设置。当然，现如今
    Linux 发行版使用了大量预构建的包，这些包为用户社区提供，供用户在他们选择的操作系统中使用。在许多方面，我们正处于 Kubernetes 软件管理的早期阶段，存在许多不同的方法来安装软件，覆盖
    Kubernetes 系统的不同层次。但是否还有其他原因让你希望为 Kubernetes 引入类似 GNU Linux 风格的软件包管理器呢？也许你认为通过使用容器、Git
    和配置管理，你能够独立管理。
- en: 'Keep in mind the that there several important dimensions to consider when it
    comes to application management in a Kubernetes cluster:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 集群的应用管理中，有几个重要的维度需要考虑，请牢记：
- en: You want to be able to leverage the experience of others. When you install software
    in your cluster, you want to be able to take advantage of the expertise of the
    teams that built the software you're running, or experts who've set it up in a
    way to perform best.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你希望能够借鉴他人的经验。当你在集群中安装软件时，你希望能够利用构建该软件的团队的专业知识，或者已经以最佳性能配置好软件的专家的经验。
- en: You want a repeatable, auditable method of maintaining the application-specific
    configuration of your cluster across environments. It's difficult to build in
    environment-specific memory settings, for example, across environments using simpler
    tools such as cURL, or within a `makefile` or other package compilation tools.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要一种可重复、可审计的方法来维护集群在不同环境中的应用特定配置。例如，在使用简单工具如 cURL，或在 `makefile` 或其他软件包编译工具中，很难为不同环境构建特定的内存设置。
- en: In short, we want to take advantage of the expertise of the ecosystem when deploying
    technologies such as databases, caching layers, web servers, key/value stores,
    and other technologies that you're likely to run on your Kubernetes cluster. There
    are a lot of potential players in this part of the ecosystem, such as Landscaper
    ([https://github.com/Eneco/landscaper](https://github.com/Eneco/landscaper)),
    Kubepack ([https://github.com/kubepack/pack](https://github.com/kubepack/pack)),
    Flux ([https://github.com/weaveworks/flux](https://github.com/weaveworks/flux)),
    Armada ([https://github.com/att-comdev/armada](https://github.com/att-comdev/armada)),
    and helmfile ([https://cdp.packtpub.com/getting_started_with_kubernetes__third_edition/wp-admin/post.php?post=29&action=pdfpreview](https://cdp.packtpub.com/getting_started_with_kubernetes__third_edition/wp-admin/post.php?post=29&action=pdfpreview)).
    In this section in particular, we're going to look at Helm ([https://github.com/helm/helm](https://github.com/helm/helm)),
    which has recently been accepted into the CNCF as an incubating project, and its
    approach to the problems we've described here.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们希望在部署像数据库、缓存层、Web 服务器、键/值存储和其他你可能在 Kubernetes 集群中运行的技术时，利用生态系统的专业知识。这个生态系统中有许多潜在的参与者，例如
    Landscaper ([https://github.com/Eneco/landscaper](https://github.com/Eneco/landscaper))、Kubepack
    ([https://github.com/kubepack/pack](https://github.com/kubepack/pack))、Flux ([https://github.com/weaveworks/flux](https://github.com/weaveworks/flux))、Armada
    ([https://github.com/att-comdev/armada](https://github.com/att-comdev/armada))
    和 helmfile ([https://cdp.packtpub.com/getting_started_with_kubernetes__third_edition/wp-admin/post.php?post=29&action=pdfpreview](https://cdp.packtpub.com/getting_started_with_kubernetes__third_edition/wp-admin/post.php?post=29&action=pdfpreview))。在这一部分，我们将特别关注
    Helm ([https://github.com/helm/helm](https://github.com/helm/helm))，它最近已被 CNCF
    接纳为孵化项目，并且它在解决我们这里描述的问题时的方法。
- en: Getting started with Helm
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 Helm
- en: We'll see how Helm makes it easier to manage Kubernetes applications using charts,
    which are packages that contain a description of the package in the form of `chart.yml`,
    and several templates that contain manifests Kubernetes can use to manipulate
    objects within its systems.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到 Helm 如何通过使用 charts 来简化 Kubernetes 应用程序的管理，charts 是包含 `chart.yml` 文件的包，其中描述了包的内容，并且包含多个模板，这些模板包含
    Kubernetes 可以用来操作其系统内对象的清单。
- en: 'Note: Kubernetes is built with a philosophy of the operator defining a desired
    end state, with Kubernetes working over time and eventual consistency to enforce
    that state. Helm''s approach to application management follows the same principles.
    Just as you can manage objects via `kubectl` with imperative commands, imperative
    objective configuration, and declarative object configuration, Helm takes advantage
    of the declarative object style, which has the highest functionality curve and
    highest difficulty.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Kubernetes 是基于一种理念构建的，即操作员定义所需的最终状态，Kubernetes 会随着时间的推移，通过最终一致性来强制执行该状态。Helm
    的应用程序管理方法遵循相同的原则。就像你可以通过 `kubectl` 使用命令式命令、命令式目标配置和声明式对象配置来管理对象一样，Helm 利用声明式对象风格，这种风格具有最高的功能曲线和最高的难度。
- en: 'Let''s get started quickly with Helm. First, make sure that you SSH into your
    Kubernetes cluster that we''ve been using. You''ll notice that as with many Kubernetes
    pieces, we''re going to use Kubernetes to install Helm and its components. You
    can also use a local installation of Kubernetes from Minikube. First, check and
    make sure that `kubectl` is set to use the correct cluster:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速开始使用 Helm。首先，确保你已经通过 SSH 连接到我们之前使用的 Kubernetes 集群。你会注意到，与许多 Kubernetes
    组件一样，我们将使用 Kubernetes 来安装 Helm 及其组件。你也可以使用 Minikube 上的本地 Kubernetes 安装。首先，检查并确保
    `kubectl` 已设置为使用正确的集群：
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Next up, let's grab the Helm install script and install it locally. Make sure
    to read the script through first so you're comfortable with what it does!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们获取 Helm 安装脚本并在本地安装它。确保先阅读脚本，以便你了解它的功能！
- en: You can read through the script contents here: [https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get](https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读脚本内容：[https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get](https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get)。
- en: 'Now, let''s run the install script and grab the pieces:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行安装脚本并获取相关组件：
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now that we''ve pulled and installed Helm, we can install Tiller on the cluster
    using `helm init`. You can also run Tiller locally for development, but for production
    installations and this demo, we''ll run Tiller inside the cluster directly as
    a component itself. Tiller will use the previous context when configuring itself,
    so make sure that you''re using the correct endpoint:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拉取并安装了Helm，我们可以使用`helm init`在集群中安装Tiller。你也可以在本地运行Tiller进行开发，但对于生产安装和本次演示，我们将在集群内部直接作为组件运行Tiller。Tiller将在配置时使用之前的上下文，所以请确保你使用的是正确的端点：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now that we''ve installed Helm, let''s see what it''s like to manage applications
    directly by installing MySQL using one of the official stable charts. We''ll make
    sure we have the latest repositories and then install it:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了Helm，让我们看看如何通过安装MySQL来直接管理应用程序，使用其中一个官方稳定版charts。我们会确保获取到最新的repositories，然后进行安装：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You can get a sneak preview of the power of Helm managed MySQL by running the
    `install` command, `helm install stable/mysql`, which is helm''s version of man
    pages for the application install:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行`install`命令，`helm install stable/mysql`，预览Helm管理MySQL的强大功能，这是Helm版的手册页，用于应用程序安装：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Helm installs a number of pieces here, which we recognize as Kubernetes objects,
    including Deployment, Secret, and ConfigMap. You can view your installation of
    MySQL with `helm ls`, and delete your MySQL installation with `helm delete <cluster_name>`.
    You can also create your own charts with `helm init <chart_name>` and lint those
    charts with Helm lint.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Helm在这里安装了许多我们识别为Kubernetes对象的组件，包括Deployment、Secret和ConfigMap。你可以通过`helm ls`查看你的MySQL安装，使用`helm
    delete <cluster_name>`删除你的MySQL安装。你也可以通过`helm init <chart_name>`创建自己的charts，并使用Helm
    lint对这些charts进行lint检查。
- en: 'If you''d like to learn more about the powerful tools available to you with
    Helm, check out the docs: [https://docs.helm.sh/](https://docs.helm.sh/). We''ll
    also dive into more comprehensive examples in the next chapter when we look at
    CI/CD.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于Helm强大工具的使用，可以查看文档：[https://docs.helm.sh/](https://docs.helm.sh/)。在下一个章节中，我们还会深入探讨更多的综合示例，重点讲解CI/CD。
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We should now be a bit more comfortable with the basics of application scaling
    in Kubernetes. We also looked at the built-in functions in order to roll updates
    as well as a manual process for testing and slowly integrating updates. We took
    a look at how to scale the nodes of our underlying cluster and increase the overall
    capacity for our Kubernetes resources. Finally, we explored some of the new autoscaling
    concepts for both the cluster and our applications themselves.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该对Kubernetes中应用程序扩展的基础知识有了更深入的了解。我们还研究了内置功能，以便进行更新发布，并探讨了测试和逐步集成更新的手动过程。我们查看了如何扩展底层集群的节点，并增加Kubernetes资源的整体容量。最后，我们探索了一些新的自动扩展概念，既适用于集群，也适用于我们的应用程序本身。
- en: In the next chapter, we will look at the latest techniques for scaling and updating
    applications with the new `deployments` resource type, as well as some of the
    other types of workloads we can run on Kubernetes.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个章节中，我们将探讨最新的扩展和更新应用程序的技术，特别是新的`deployments`资源类型，以及我们可以在Kubernetes上运行的其他类型的工作负载。
- en: Questions
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the name of the command that allows you to increase the number of replication
    controllers and the new Deployments abstraction in order to meet application needs?
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个命令可以增加复制控制器的数量，并通过新的Deployments抽象满足应用程序需求？
- en: What is the name of the strategy for providing smooth rollouts to applications
    without interrupting the user experience?
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供平稳发布以避免中断用户体验的策略叫什么？
- en: What is one type of session affinity available during deployment?
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在部署期间，哪种类型的会话亲和性是可用的？
- en: What is the recent addition to Kubernetes that allows for pods in the cluster
    to scale horizontally?
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最近Kubernetes新增了什么功能，允许集群中的Pods水平扩展？
- en: Which environment variables, if set, allow the cluster to scale Kubernetes nodes
    with demand?
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置了哪些环境变量可以使集群根据需求扩展Kubernetes节点？
- en: Which software tool allows you to install applications and leverage the expertise
    of those product team's installation settings?
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪种软件工具允许你安装应用程序并利用产品团队的安装设置经验？
- en: What is a Helm install file called?
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Helm安装文件叫什么？
- en: Further reading
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'If you''d like to read more about Helm, check out its web page here: [https://www.helm.sh/blog/index.html](https://www.helm.sh/blog/index.html). If
    you''d like to read more about the software behind cluster autoscaling, check
    out the Kubernetes `autoscaler` repository: [https://github.com/kubernetes/autoscaler](https://github.com/kubernetes/autoscaler).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于 Helm 的信息，可以访问它的网页：[https://www.helm.sh/blog/index.html](https://www.helm.sh/blog/index.html)。如果你想了解更多关于集群自动扩缩容背后的软件，可以查看
    Kubernetes 的 `autoscaler` 仓库：[https://github.com/kubernetes/autoscaler](https://github.com/kubernetes/autoscaler)。
