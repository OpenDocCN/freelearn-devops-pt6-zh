- en: Application Updates, Gradual Rollouts, and Autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will expand upon the core concepts, and show you how to roll out
    updates and test new features of your application with minimal disruption to uptime.
    It will cover the basics of doing application updates, gradual rollouts, and A/B
    testing. In addition, we will look at scaling the Kubernetes cluster itself.
  prefs: []
  type: TYPE_NORMAL
- en: In version 1.2, Kubernetes released a Deployments API. Deployments are the recommended
    way to deal with scaling and application updates going forward. As mentioned in
    previous chapters, `ReplicationControllers` are no longer the recommended manner
    for managing application updates. However, as they're still core functionality
    for many operators, we will explore rolling updates in this chapter as an introduction
    to the scaling concept and then dive into the preferred method of using Deployments
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We'll also investigate the functionality of Helm and Helm Charts that will help
    you manage Kubernetes resources. Helm is a way to manage packages in Kubernetes
    much in the same way that `apt`/`yum` manage code in the Linux ecosystem. Helm
    also lets you share your applications with others, and most importantly create
    reproducible builds of Kubernetes applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Application scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A/B testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application autoscaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling up your cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Helm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You''ll need to have your Google Cloud Platform account enabled and logged
    in, or you can use a local Minikube instance of Kubernetes. You can also use Play
    with Kubernetes over the web: [https://labs.play-with-k8s.com/](https://labs.play-with-k8s.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the GitHub repository for this chapter: [https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter06](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter06)[.](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code%20files/Chapter%2006)'
  prefs: []
  type: TYPE_NORMAL
- en: Example setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start exploring the various capabilities built into Kubernetes for
    scaling and updates, we will need a new example environment. We are going to use
    a variation of our previous container image with a blue background (refer to the
    *v0.1 and v0.2 (side by side)* image, later in this chapter, for a comparison).
    We have the following code in the `pod-scaling-controller.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the following code as `pod-scaling-service.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Create these services with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The public IP address for the service may take a moment to create.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over time, as you run your applications in the Kubernetes cluster, you will
    find that some applications need more resources, whereas others can manage with
    fewer resources. Instead of removing the entire `ReplicationControllers` (and
    associated pods), we want a more seamless way to scale our application up and
    down.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thankfully, Kubernetes includes a `scale` command, which is suited specifically
    for this purpose. The `scale` command works both with `ReplicationControllers`
    and the new Deployments abstraction. For now, we will explore its use with `ReplicationControllers`.
    In our new example, we have only one replica running. You can check this with
    a `get pods` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try scaling that up to three with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If all goes well, you'll simply see the `scaled` word on the output of your
    Terminal window.
  prefs: []
  type: TYPE_NORMAL
- en: Optionally, you can specify the `--current-replicas` flag as a verification
    step. The scaling will only occur if the actual number of replicas currently running
    matches this count.
  prefs: []
  type: TYPE_NORMAL
- en: After listing our pods once again, we should now see three pods running with
    a name similar to `node-js-scale-XXXXX`, where the `X` characters are a random
    string.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the `scale` command to reduce the number of replicas. In either
    case, the `scale` command adds or removes the necessary pod replicas, and the
    service automatically updates and balances across new or remaining replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Smooth updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The scaling of our application up and down as our resource demands change is
    useful for many production scenarios, but what about simple application updates?
    Any production system will have code updates, patches, and feature additions.
    These could be occurring monthly, weekly, or even daily. Making sure that we have
    a reliable way to push out these changes without interruption to our users is
    a paramount consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we benefit from the years of experience the Kubernetes system is
    built on. There is built-in support for rolling updates with the 1.0 version.
    The `rolling-update` command allows us to update entire `ReplicationControllers`
    or just the underlying Docker image used by each replica. We can also specify
    an update interval, which will allow us to update one pod at a time and wait until
    proceeding to the next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take our scaling example and perform a rolling update to the 0.2 version
    of our container image. We will use an update interval of 2 minutes, so we can
    watch the process as it happens in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You should see some text about creating a new `ReplicationControllers` named
    `node-js-scale-XXXXX`, where the `X` characters will be a random string of numbers
    and letters. In addition, you will see the beginning of a loop that starts one
    replica of the new version and removes one from the existing `ReplicationControllers`.
    This process will continue until the new `ReplicationControllers` has the full
    count of replicas running.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to follow along in real time, we can open another Terminal window
    and use the `get pods` command, along with a label filter, to see what''s happening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This command will filter for pods with `node-js-scale` in the name. If you run
    this after issuing the `rolling-update` command, you should see several pods running
    as it creates new versions and removes the old ones one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full output of the previous `rolling-update` command should look something
    like this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38831f58-cd3a-4522-8d71-aa308a1aca23.png)'
  prefs: []
  type: TYPE_IMG
- en: The scaling output
  prefs: []
  type: TYPE_NORMAL
- en: As we can see here, Kubernetes is first creating a new `ReplicationController` named
    `node-js-scale-10ea08ff9a118ac6a93f85547ed28f6`. K8s then loops through one by
    one, creating a new pod in the new controller and removing one from the old. This
    continues until the new controller has the full replica count and the old one
    is at zero. After this, the old controller is deleted and the new one is renamed
    with the original controller's name.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run a `get pods` command now, you''ll notice that the pods still all
    have a longer name. Alternatively, we could have specified the name of a new controller
    in the command, and Kubernetes will create a new `ReplicationControllers` and
    pods using that name. Once again, the controller of the old name simply disappears
    after the update is completed. I recommend that you specify a new name for the
    updated controller to avoid confusion in your pod naming down the line. The same
    `update` command with this method will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the static external IP address from the service we created in the first
    section, we can open the service in a browser. We should see our standard container
    information page. However, you''ll notice that the title now says Pod Scaling
    v0.2 and the background is light yellow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a03d2bc3-d062-45b6-8bde-9fa33024773b.png)'
  prefs: []
  type: TYPE_IMG
- en: v0.1 and v0.2 (side by side)
  prefs: []
  type: TYPE_NORMAL
- en: It's worth noting that, during the entire update process, we've only been looking
    at pods and `ReplicationControllers`. We didn't do anything with our service,
    but the service is still running fine and now directing to the new version of
    our pods. This is because our service is using label selectors for membership.
    Because both our old and new replicas use the same labels, the service has no
    problem using the new pods to service requests. The updates are done on the pods
    one by one, so it's seamless for the users of the service.
  prefs: []
  type: TYPE_NORMAL
- en: Testing, releases, and cutovers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The rolling update feature can work well for a simple blue-green deployment
    scenario. However, in a real-world blue-green deployment with a stack of multiple
    applications, there can be a variety of inter-dependencies that require in-depth
    testing. The `update-period` command allows us to add a `timeout` flag where some
    testing can be done, but this will not always be satisfactory for testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you may want partial changes to persist for a longer time and all
    the way up to the load balancer or service level. For example, you may wish to
    run an A/B test on a new user interface feature with a portion of your users.
    Another example is running a canary release (a replica in this case) of your application
    on new infrastructure, such as a newly added cluster node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at an A/B testing example. For this example, we will need
    to create a new service that uses `sessionAffinity`. We will set the affinity
    to `ClientIP`, which will allow us to forward clients to the same backend pod.
    The following listing `pod-AB-service.yaml` is the key if we want a portion of
    our users to see one version while others see another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Create this service as usual with the `create` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a service that will point to our pods running both version
    0.2 and 0.3 of the application. Next, we will create the two `ReplicationControllers` that create
    two replicas of the application. One set will have version 0.2 of the application,
    and the other will have version 0.3, as shown in the listing `pod-A-controller.yaml`and `pod-B-controller.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we have the same service label, so these replicas will also be added
    to the service pool based on this selector. We also have `livenessProbe` and `readinessProbe`
    defined to make sure that our new version is working as expected. Again, use the
    `create` command to spin up the controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have a service balancing both versions of our app. In a true A/B test,
    we would now want to start collecting metrics on the visits to each version. Again,
    we have `sessionAffinity` set to `ClientIP`, so all requests will go to the same
    pod. Some users will see v0.2, and some will see v0.3.
  prefs: []
  type: TYPE_NORMAL
- en: Because we have `sessionAffinity` turned on, your test will likely show the
    same version every time. This is expected, and you would need to attempt a connection
    from multiple IP addresses to see both user experiences with each version.
  prefs: []
  type: TYPE_NORMAL
- en: Since the versions are each on their own pod, one can easily separate logging
    and even add a logging container to the pod definition for a sidecar logging pattern.
    For brevity, we will not cover that setup in this book, but we will look at some
    of the logging tools in Chapter 8, *Monitoring and Logging*.
  prefs: []
  type: TYPE_NORMAL
- en: We can start to see how this process will be useful for a canary release or
    a manual blue-green deployment. We can also see how easy it is to launch a new
    version and slowly transition over to the new release.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a basic transition quickly. It''s really as simple as a few
    `scale` commands, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Use the `get pods` command combined with the `-l` filter in between the `scale`
    commands to watch the transition as it happens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have fully transitioned over to version 0.3 (`node-js-scale-b`). All
    users will now see version 0.3 of the site. We have four replicas of version 0.3
    and none of 0.2\. If you run a `get rc` command, you will notice that we still
    have an `ReplicationControllers` for 0.2 (`node-js-scale-a`). As a final cleanup,
    we can remove that controller completely, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Application autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A recent feature addition to Kubernetes is that of the Horizontal Pod Autoscaler.
    This resource type is really useful as it gives us a way to automatically set
    thresholds for scaling our application. Currently, that support is only for CPU,
    but there is alpha support for custom application metrics as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the `node-js-scale` `ReplicationController` from the beginning of
    the chapter and add an autoscaling component. Before we start, let''s make sure
    we are scaled back down to one replica using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create a Horizontal Pod Autoscaler, `node-js-scale-hpa.yaml` with
    the following `hpa` definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Go ahead and create this with the `kubectl create -f` command. Now, we can
    list the Horizontal Pod Autoscaler and get a description as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also create autoscaling in the command line with the `kubectl autoscale`
    command. The preceding YAML will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$ kubectl autoscale rc/node-js-scale --min=1 --max=3 --cpu-percent=20`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This will show us an autoscaler on `node-js-scale` `ReplicationController`
    with a target CPU of 30%. Additionally, you will see that minimum pods is set
    to 1 and maximum to 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4495140-c49f-42c1-bf55-033b0f187cc3.png)'
  prefs: []
  type: TYPE_IMG
- en: Horizontal pod autoscaler with no load
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also query our pods to see how many are running right now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see only one `node-js-scale` pod because our Horizontal Pod Autoscaler
    is showing 0% utilization, so we will need to generate some load. We will use
    the popular `boom` application common in many container demos. The following listing `boomload.yaml`
    will help us create continuous load until we can hit the CPU threshold for the
    autoscaler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Use the `kubectl create -f` command with this listing and then be ready to start
    monitoring the `hpa`. We can do this with the `kubectl get hpa` command we used
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'It may take a few moments, but we should start to see the current CPU utilization
    increase. Once it goes above the 20% threshold we set, the autoscaler will kick
    in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b59c037-47ee-4d31-91c8-9df165196eb2.png)'
  prefs: []
  type: TYPE_IMG
- en: Horizontal pod autoscaler after load starts
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we see this, we can run `kubectl get pod` again and see there are now
    several `node-js-scale` pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can clean up now by killing our load generation pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now, if we watch the `hpa`, we should start to see the CPU usage drop. It may
    take a few minutes, but eventually we will go back down to 0% CPU load.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All these techniques are great for scaling the application, but what about the
    cluster itself? At some point, you will pack the nodes full and need more resources
    to schedule new pods for your workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you create your cluster, you can customize the starting number of nodes
    (minions) with the `NUM_MINIONS` environment variable. By default, it is set to
    4.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the Kubernetes team has started to build autoscaling capability
    into the cluster itself. Currently, this is only supported on GCE and GKE, but
    work is being done on other providers. This capability utilizes the `KUBE_AUTOSCALER_MIN_NODES`,
    `KUBE_AUTOSCALER_MAX_NODES`, and `KUBE_ENABLE_CLUSTER_AUTOSCALER` environment
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows how to set the environment variables for autoscalingbefore
    running `kube-up.sh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Also, bear in mind that changing this after the cluster is started will have
    no effect. You would need to tear down the cluster and create it once again. Thus,
    this section will show you how to add nodes to an existing cluster without rebuilding
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Once you start a cluster with these settings, your cluster will automatically
    scale up and down with the minimum and maximum limits based on compute resource
    usage in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: GKE clusters also support autoscaling when launched, when using the alpha features.
    The preceding example will use a flag such as `--enable-autoscaling --min-nodes=2 --max-nodes=5`
    in a command-line launch.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up the cluster on GCE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you wish to scale out an existing cluster, we can do it with a few steps.
    Manually scaling up your cluster on GCE is actually quite easy. The existing plumbing
    uses managed instance groups in GCE, which allow you to easily add more machines
    of a standard configuration to the group via an instance template.
  prefs: []
  type: TYPE_NORMAL
- en: You can see this template easily in the GCE console. First, open the console;
    by default, this should open your default project console. If you are using another
    project for your Kubernetes cluster, simply select it from the project drop-down
    at the top of the page.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the side panel, look under Compute and then Compute Engine, and select Instance
    templates. You should see a template titled kubernetes-minion-template. Note that
    the name could vary slightly if you''ve customized your cluster naming settings.
    Click on that template to see the details. Refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6f1d645-a0d4-4a58-b7a3-5ab5f5da1f77.png)'
  prefs: []
  type: TYPE_IMG
- en: The GCE Instance template for minions
  prefs: []
  type: TYPE_NORMAL
- en: You'll see a number of settings, but the meat of the template is under the Custom
    metadata. Here, you will see a number of environment variables and also a startup
    script that is run after a new machine instance is created. These are the core
    components that allow us to create new machines and have them automatically added
    to the available cluster nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the template for new machines is already created, it is very simple
    to scale out our cluster in GCE. Once in the Compute section of the console, simply
    go to Instance groups located right above the Instance templates link on the side
    panel. Again, you should see a group titled kubernetes-minion-group or something
    similar. Click on that group to see the details, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36de5070-d467-47cb-ba5e-eb62f9b46575.png)'
  prefs: []
  type: TYPE_IMG
- en: The GCE instance group for minions
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll see a page with a CPU metrics graph and three instances listed here.
    By default, the cluster creates three nodes. We can modify this group by clicking
    on the EDIT GROUP button at the top of the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7df4a1d2-5706-4ac1-bc8e-a6e40c11d4b7.png)'
  prefs: []
  type: TYPE_IMG
- en: The GCE instance group edit page
  prefs: []
  type: TYPE_NORMAL
- en: You should see kubernetes-minion-template selected in the Instance template
    that we reviewed a moment ago. You'll also see an Autoscaling setting, which is
    Off by default, and an instance count of `3`. Simply increment this to `4` and
    click on Save. You'll be taken back to the group details page and you'll see a
    pop-up dialog showing the pending changes.
  prefs: []
  type: TYPE_NORMAL
- en: You'll also see some auto healing properties on the Instance groups edit page.
    This recreates failed instances and allows you to set health checks, as well as
    an initial delay period before an action is taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a few minutes, you''ll have a new instance listed on the details page. We
    can test that this is ready using the `get nodes` command from the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '*A word of caution on autoscaling and scaling down in general*:First, if we
    repeat the earlier process and decrease the countdown to four, GCE will remove
    one node. However, it will not necessarily be the node you just added. The good
    news is that pods will be rescheduled on the remaining nodes. However, it can
    only reschedule where resources are available. If you are close to full capacity
    and shut down a node, there is a good chance that some pods will not have a place
    to be rescheduled. In addition, this is not a live migration, so any application
    state will be lost in the transition. The bottom line is that you should carefully
    consider the implications before scaling down or implementing an autoscaling scheme.'
  prefs: []
  type: TYPE_NORMAL
- en: For more information on general autoscaling in GCE, refer to the [https://cloud.google.com/compute/docs/autoscaler/?hl=en_US#scaling_based_on_cpu_utilization](https://cloud.google.com/compute/docs/autoscaler/?hl=en_US#scaling_based_on_cpu_utilization) link.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up the cluster on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The AWS provider code also makes it very easy to scale up your cluster. Similar
    to GCE, the AWS setup uses autoscaling groups to create the default four minion
    nodes. In the future, the autoscaling groups will hopefully be integrated into
    the Kubernetes cluster autoscaling functionality. For now, we will walk though
    a manual setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can also be easily modified using the CLI or the web console. In the console,
    from the EC2 page, simply go to the Auto Scaling Groups section at the bottom
    of the menu on the left. You should see a name similar to kubernetes-minion-group.
    Select this group and you will see the details shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e22e5462-6ef2-494e-8d67-cf585c7b2309.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes minion autoscaling details
  prefs: []
  type: TYPE_NORMAL
- en: We can scale this group up easily by clicking on Edit. Then, change the Desired,
    Min, and Max values to `5` and click on Save. In a few minutes, you'll have the
    fifth node available. You can once again check this using the `get nodes` command.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling down is the same process, but remember that we discussed the same considerations
    in the previous *Scaling up the cluster on GCE *section. Workloads could get abandoned
    or, at the very least, unexpectedly restarted.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling manually
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For other providers, creating new minions may not be an automated process. Depending
    on your provider, you'll need to perform various manual steps. It can be helpful
    to look at the provider-specific scripts in the `cluster` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Managing applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of this book's writing, new software has emerged that hopes to tackle
    the problem of managing Kubernetes applications from a holistic perspective. As
    application installation and continued management grows more complex, software
    such as Helm hopes to ease the pain for cluster operators creating, versioning,
    publishing, and exporting application installation and configuration for other
    operators. You may have also heard the term GitOps, which uses Git as the source
    of truth from which all Kubernetes instances can be managed.
  prefs: []
  type: TYPE_NORMAL
- en: While we'll jump deeper into **Continuous Integration and Continuous Delivery**
    (**CI/CD**) in the next chapter, let's see what advantages can be gained by taking
    advantage of package management within the Kubernetes ecosystem. First, it's important
    to understand what problem we're trying to solve when it comes to package management
    within the Kubernetes ecosystem. Helm and programs like it have a lot in common
    with package managers such as `apt`, `yum`, `rpm`, `dpgk`, Aptitude, and Zypper.
    These pieces of software helped users cope during the early days of Linux, where
    programs were simply distributed as source code, with installation documents,
    configuration files, and the necessary moving pieces left to the operator to set
    up. These days of course Linux distributions use a great many pre-built packages,
    which are made available to the user community for consumption on their operating
    system of choice. In many ways, we're in those early days of software management
    for Kubernetes, with many different methods for installing software within many
    different layers of the Kubernetes system. But are there other reasons for  wanting
    a GNU Linux-style package manager for Kubernetes? Perhaps you feel confident that
    by using containers, or Git and configuration management, you can manage on your
    own.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in mind the that there several important dimensions to consider when it
    comes to application management in a Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: You want to be able to leverage the experience of others. When you install software
    in your cluster, you want to be able to take advantage of the expertise of the
    teams that built the software you're running, or experts who've set it up in a
    way to perform best.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You want a repeatable, auditable method of maintaining the application-specific
    configuration of your cluster across environments. It's difficult to build in
    environment-specific memory settings, for example, across environments using simpler
    tools such as cURL, or within a `makefile` or other package compilation tools.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In short, we want to take advantage of the expertise of the ecosystem when deploying
    technologies such as databases, caching layers, web servers, key/value stores,
    and other technologies that you're likely to run on your Kubernetes cluster. There
    are a lot of potential players in this part of the ecosystem, such as Landscaper
    ([https://github.com/Eneco/landscaper](https://github.com/Eneco/landscaper)),
    Kubepack ([https://github.com/kubepack/pack](https://github.com/kubepack/pack)),
    Flux ([https://github.com/weaveworks/flux](https://github.com/weaveworks/flux)),
    Armada ([https://github.com/att-comdev/armada](https://github.com/att-comdev/armada)),
    and helmfile ([https://cdp.packtpub.com/getting_started_with_kubernetes__third_edition/wp-admin/post.php?post=29&action=pdfpreview](https://cdp.packtpub.com/getting_started_with_kubernetes__third_edition/wp-admin/post.php?post=29&action=pdfpreview)).
    In this section in particular, we're going to look at Helm ([https://github.com/helm/helm](https://github.com/helm/helm)),
    which has recently been accepted into the CNCF as an incubating project, and its
    approach to the problems we've described here.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Helm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll see how Helm makes it easier to manage Kubernetes applications using charts,
    which are packages that contain a description of the package in the form of `chart.yml`,
    and several templates that contain manifests Kubernetes can use to manipulate
    objects within its systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Kubernetes is built with a philosophy of the operator defining a desired
    end state, with Kubernetes working over time and eventual consistency to enforce
    that state. Helm''s approach to application management follows the same principles.
    Just as you can manage objects via `kubectl` with imperative commands, imperative
    objective configuration, and declarative object configuration, Helm takes advantage
    of the declarative object style, which has the highest functionality curve and
    highest difficulty.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started quickly with Helm. First, make sure that you SSH into your
    Kubernetes cluster that we''ve been using. You''ll notice that as with many Kubernetes
    pieces, we''re going to use Kubernetes to install Helm and its components. You
    can also use a local installation of Kubernetes from Minikube. First, check and
    make sure that `kubectl` is set to use the correct cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Next up, let's grab the Helm install script and install it locally. Make sure
    to read the script through first so you're comfortable with what it does!
  prefs: []
  type: TYPE_NORMAL
- en: You can read through the script contents here: [https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get](https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s run the install script and grab the pieces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''ve pulled and installed Helm, we can install Tiller on the cluster
    using `helm init`. You can also run Tiller locally for development, but for production
    installations and this demo, we''ll run Tiller inside the cluster directly as
    a component itself. Tiller will use the previous context when configuring itself,
    so make sure that you''re using the correct endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''ve installed Helm, let''s see what it''s like to manage applications
    directly by installing MySQL using one of the official stable charts. We''ll make
    sure we have the latest repositories and then install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You can get a sneak preview of the power of Helm managed MySQL by running the
    `install` command, `helm install stable/mysql`, which is helm''s version of man
    pages for the application install:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Helm installs a number of pieces here, which we recognize as Kubernetes objects,
    including Deployment, Secret, and ConfigMap. You can view your installation of
    MySQL with `helm ls`, and delete your MySQL installation with `helm delete <cluster_name>`.
    You can also create your own charts with `helm init <chart_name>` and lint those
    charts with Helm lint.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''d like to learn more about the powerful tools available to you with
    Helm, check out the docs: [https://docs.helm.sh/](https://docs.helm.sh/). We''ll
    also dive into more comprehensive examples in the next chapter when we look at
    CI/CD.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should now be a bit more comfortable with the basics of application scaling
    in Kubernetes. We also looked at the built-in functions in order to roll updates
    as well as a manual process for testing and slowly integrating updates. We took
    a look at how to scale the nodes of our underlying cluster and increase the overall
    capacity for our Kubernetes resources. Finally, we explored some of the new autoscaling
    concepts for both the cluster and our applications themselves.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the latest techniques for scaling and updating
    applications with the new `deployments` resource type, as well as some of the
    other types of workloads we can run on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the name of the command that allows you to increase the number of replication
    controllers and the new Deployments abstraction in order to meet application needs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the name of the strategy for providing smooth rollouts to applications
    without interrupting the user experience?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is one type of session affinity available during deployment?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the recent addition to Kubernetes that allows for pods in the cluster
    to scale horizontally?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which environment variables, if set, allow the cluster to scale Kubernetes nodes
    with demand?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which software tool allows you to install applications and leverage the expertise
    of those product team's installation settings?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a Helm install file called?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you''d like to read more about Helm, check out its web page here: [https://www.helm.sh/blog/index.html](https://www.helm.sh/blog/index.html). If
    you''d like to read more about the software behind cluster autoscaling, check
    out the Kubernetes `autoscaler` repository: [https://github.com/kubernetes/autoscaler](https://github.com/kubernetes/autoscaler).'
  prefs: []
  type: TYPE_NORMAL
