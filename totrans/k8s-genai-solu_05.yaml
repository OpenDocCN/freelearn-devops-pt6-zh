- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: 'Working with GenAI on K8s: Chatbot Example'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 K8s 上使用 GenAI：聊天机器人示例
- en: In this chapter, we will build on the examples we discussed in [*Chapter 4*](B31108_04.xhtml#_idTextAnchor049)
    and start deploying those examples in **K8s**/**Amazon EKS**. We will start by
    deploying **JupyterHub** ([https://jupyter.org/hub](https://jupyter.org/hub))
    on EKS, which can be used for model experimentation. Next, we will fine-tune the
    **Llama 3 model** within EKS and deploy it. Finally, we’ll set up a **RAG-powered
    chatbot** that will deliver personalized recommendations for an *e-commerce* company
    use case.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将基于[*第四章*](B31108_04.xhtml#_idTextAnchor049)中讨论的示例，开始在 **K8s**/**Amazon
    EKS** 上部署这些示例。我们将从在 EKS 上部署 **JupyterHub** ([https://jupyter.org/hub](https://jupyter.org/hub))
    开始，这可以用于模型实验。接下来，我们将在 EKS 上微调 **Llama 3 模型** 并进行部署。最后，我们将设置一个 **RAG 驱动的聊天机器人**，它将为
    *电商* 公司使用案例提供个性化推荐。
- en: 'We’ll cover the following key topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下关键主题：
- en: GenAI use cases for e-commerce
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电商的 GenAI 使用案例
- en: Experimentation using JupyterHub
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 JupyterHub 进行实验
- en: Fine-tuning Llama 3 in K8s
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 K8s 上微调 Llama 3
- en: Deploying the fine-tuned model on K8s
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 K8s 上部署微调模型
- en: Deploying a RAG application on K8s
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 K8s 上部署 RAG 应用
- en: Deploying a chatbot on K8s
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 K8s 上部署聊天机器人
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be using the following tools, some of which require
    you to set up an account and create an access token:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下工具，其中一些工具需要您注册帐户并创建访问令牌：
- en: '**Hugging** **Face**: [https://huggingface.co/join](https://huggingface.co/join)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging** **Face**: [https://huggingface.co/join](https://huggingface.co/join)'
- en: '**OpenAI**: [https://platform.openai.com/signup](https://platform.openai.com/signup)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI**: [https://platform.openai.com/signup](https://platform.openai.com/signup)'
- en: 'The **Llama 3 model**, which can be accessed via Hugging Face: [https://huggingface.co/meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '可以通过 Hugging Face 访问的 **Llama 3 模型**: [https://huggingface.co/meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)'
- en: An **Amazon EKS cluster**, as illustrated in [*Chapter 3*](B31108_03.xhtml#_idTextAnchor039)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **Amazon EKS 集群**，如[*第三章*](B31108_03.xhtml#_idTextAnchor039)所示
- en: GenAI use cases for e-commerce
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 电商的 GenAI 使用案例
- en: As we discussed in [*Chapter 1*](B31108_01.xhtml#_idTextAnchor015), it is critical
    to think about *KPIs* and *business objectives* as we explore possible deployment
    options for **GenAI applications**. For an e-commerce platform, possible use cases
    could be chatbots to answer customer questions, personalized recommendations,
    content creation for product descriptions, and personalized marketing campaigns.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第一章*](B31108_01.xhtml#_idTextAnchor015)中讨论的那样，在探索**GenAI 应用**的部署选项时，考虑
    *关键绩效指标 (KPIs)* 和 *商业目标* 是至关重要的。对于电商平台，可能的使用案例包括用于回答客户问题的聊天机器人、个性化推荐、产品描述的内容创作和个性化营销活动。
- en: 'Let’s say that we have an e-commerce company called *MyRetail* for which we
    have been given the responsibility to explore and deploy GenAI use cases. The
    company has been growing rapidly and has a clear goal and strong differentiation:
    to provide its customers with a personalized, seamless shopping experience. To
    stay competitive, MyRetail aims to integrate cutting-edge AI technologies into
    its customer service while focusing on the following two features:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个电商公司，名为 *MyRetail*，我们被赋予了探索和部署 GenAI 使用案例的责任。该公司发展迅速，具有明确的目标和强大的差异化竞争力：为客户提供个性化、无缝的购物体验。为了保持竞争力，MyRetail
    旨在将前沿的 AI 技术整合到客户服务中，同时专注于以下两个功能：
- en: '*Creating personalized product recommendations* using a **RAG system**.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 **RAG 系统** *创建个性化产品推荐*。
- en: '*Providing automated responses* to inquiries related to the company’s loyalty
    program through a fine-tuned **GenAI model**.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用微调后的 **GenAI 模型** *提供自动化回应*，解答关于公司忠诚计划的咨询。
- en: MyRetail’s diverse customer base means that generic product recommendations
    and traditional FAQ systems are no longer sufficient. Customers expect personalized
    shopping experiences, and the company’s *MyElite loyalty program* needs to offer
    real-time, detailed information on rewards, points, and account status.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: MyRetail 的客户群体多样化，这意味着通用的产品推荐和传统的 FAQ 系统已不再足够。客户期待个性化的购物体验，而公司的 *MyElite 忠诚计划*
    需要提供有关奖励、积分和帐户状态的实时、详细信息。
- en: 'To achieve these goals, MyRetail has decided to adopt the open source K8s orchestration
    platform and has selected Amazon EKS to deploy it in the cloud. They plan to build
    a chatbot application that utilizes two GenAI models: the first one will be a
    fine-tuned Llama 3 model trained on their MyElite loyalty program FAQ to answer
    user queries, while the second one will be a RAG application that supplements
    user queries with contextual shopping catalog data to enhance their shopping experience.
    The overall architecture of this solution is shown in *Figure 5**.1*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这些目标，MyRetail 决定采用开源的 K8s 编排平台，并选择在云中部署 Amazon EKS。他们计划构建一个聊天机器人应用程序，使用两个
    GenAI 模型：第一个将是经过微调的 Llama 3 模型，该模型基于他们的 MyElite 忠诚计划常见问题解答来回答用户查询；第二个将是一个 RAG
    应用程序，它通过提供上下文购物目录数据来补充用户查询，从而提升他们的购物体验。该解决方案的整体架构如 *图 5.1* 所示：
- en: '![Figure 5.1 – Chatbot architecture](img/B31108_05_1.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 聊天机器人架构](img/B31108_05_1.jpg)'
- en: Figure 5.1 – Chatbot architecture
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 聊天机器人架构
- en: However, before we implement this **chatbot** and **RAG system** in EKS, let’s
    create a **JupyterHub**-based playground for our data scientists to experiment
    with and optimize the GenAI models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们将这个**聊天机器人**和**RAG 系统**部署到 EKS 上之前，让我们先为数据科学家们创建一个基于 **JupyterHub** 的实验环境，以便他们进行实验并优化
    GenAI 模型。
- en: Experimentation using JupyterHub
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 JupyterHub 进行实验
- en: '*Experimentation* plays a vital role in any GenAI project life cycle as it
    enables engineers and researchers to iterate, improve, and refine models while
    optimizing performance. Several tools are available for this; recall that we used
    **Anaconda** and **Google Colab** in [*Chapter 4*](B31108_04.xhtml#_idTextAnchor049).
    Primarily, these tools help us to experiment interactively with GenAI models,
    visualize, monitor, and integrate with popular AI/ML frameworks, integrate with
    cloud services, and collaborate with others. **Jupyter Notebook** ([https://jupyter.org/](https://jupyter.org/))
    has gained widespread adoption among data scientists and ML engineers due to its
    flexibility and easy-to-use web interface. This is evident from the average daily
    downloads of the notebook package (900K to 1 million downloads) ([https://pypistats.org/packages/notebook](https://pypistats.org/packages/notebook)),
    as indicated by the **Python Package Index** ([https://pypi.org/](https://pypi.org/)).
    Traditionally, we install these notebooks on local machines, but they often need
    specialized resources such as GPUs to perform meaningful analysis. To solve this
    issue, we will leverage a K8s cluster to spin up Jupyter notebooks as needed using
    JupyterHub.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*实验* 在任何 GenAI 项目的生命周期中都扮演着至关重要的角色，因为它使工程师和研究人员能够迭代、改进和优化模型，同时提升性能。为此有多种工具可以使用；回想一下我们在
    [*第 4 章*](B31108_04.xhtml#_idTextAnchor049) 中使用了 **Anaconda** 和 **Google Colab**。这些工具主要帮助我们与
    GenAI 模型进行交互式实验，进行可视化、监控，并与流行的 AI/ML 框架进行集成，结合云服务，且与他人协作。由于其灵活性和易用的 Web 界面，**Jupyter
    Notebook** ([https://jupyter.org/](https://jupyter.org/)) 在数据科学家和 ML 工程师中得到了广泛应用。这从笔记本包的平均每日下载量（90
    万到 100 万次下载） ([https://pypistats.org/packages/notebook](https://pypistats.org/packages/notebook))
    中可以看出，数据来源是 **Python 包索引** ([https://pypi.org/](https://pypi.org/))。传统上，我们将这些
    notebook 安装在本地机器上，但它们通常需要专用资源，如 GPU，才能进行有意义的分析。为了解决这个问题，我们将利用 K8s 集群，根据需要通过 JupyterHub
    启动 Jupyter notebook。'
- en: '**JupyterHub** offers a centralized platform for running Jupyter notebooks,
    enabling users to access computational resources without requiring individual
    installations or maintenance. System administrators can manage user access effectively
    and tailor the environment with pre-configured tools and settings to meet user-specific
    preferences.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**JupyterHub** 提供了一个集中式平台，用于运行 Jupyter notebook，使用户能够访问计算资源，而无需单独安装或维护。系统管理员可以有效地管理用户访问权限，并根据用户的特定需求，通过预配置的工具和设置定制环境。'
- en: 'Let’s start by learning how to install JupyterHub on an Amazon EKS cluster:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从学习如何在 Amazon EKS 集群上安装 JupyterHub 开始：
- en: 'First, deploy the `addons.tf` to install the CSI plugin, as well as added the
    necessary **IAM permissions**. The complete code for this is available on GitHub
    at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/addons.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/addons.tf):'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，部署 `addons.tf` 来安装 CSI 插件，并添加必要的 **IAM 权限**。完整的代码可以在 GitHub 上找到，地址是 [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/addons.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/addons.tf)：
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we need to create a default **StorageClass** ([https://kubernetes.io/docs/concepts/storage/storage-classes/](https://kubernetes.io/docs/concepts/storage/storage-classes/))
    for the EBS CSI driver that specifies attributes such as the reclaim policy, storage
    provisioner, and other parameters used in dynamic volume provisioning. Here, we
    are setting the newer gp3 as the default type for EBS CSI driver-created volumes.
    Refer to the Amazon EBS documentation at [https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html](https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html)
    to learn more about different volume types:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要为 EBS CSI 驱动程序创建一个默认的 **StorageClass**（[https://kubernetes.io/docs/concepts/storage/storage-classes/](https://kubernetes.io/docs/concepts/storage/storage-classes/)），该类指定了回收策略、存储提供程序以及动态卷配置中使用的其他参数。在这里，我们将新型的
    gp3 设置为 EBS CSI 驱动程序创建的卷的默认类型。请参考 Amazon EBS 文档 [https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html](https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html)
    以了解更多关于不同卷类型的信息：
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Run the following commands to deploy the EBS CSI driver to the EKS cluster:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令将 EBS CSI 驱动程序部署到 EKS 集群：
- en: '[PRE2]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can verify the installation status of the add-on by running the following
    command:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过运行以下命令来验证附加组件的安装状态：
- en: '[PRE3]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: resource "random_password" "jupyter_pwd" {
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: resource "random_password" "jupyter_pwd" {
- en: length = 16
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: length = 16
- en: special = true
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: special = true
- en: override_special = "_%@"
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: override_special = "_%@"
- en: '}'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE4]'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: resource "kubernetes_namespace" "jupyterhub" {
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: resource "kubernetes_namespace" "jupyterhub" {
- en: metadata {
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: metadata {
- en: name = "jupyterhub"
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: name = "jupyterhub"
- en: '}'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE5]'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: module "jupyterhub_single_user_irsa" {
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: module "jupyterhub_single_user_irsa" {
- en: '...'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: role_name = "${module.eks.cluster_name}-jupyterhub-single-user-sa"
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: role_name = "${module.eks.cluster_name}-jupyterhub-single-user-sa"
- en: role_policy_arns = {
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: role_policy_arns = {
- en: policy = "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: policy = "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
- en: '}'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '}'
- en: '...'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: resource "kubernetes_service_account_v1" "jupyterhub_single_user_sa" {
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: resource "kubernetes_service_account_v1" "jupyterhub_single_user_sa" {
- en: metadata {
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: metadata {
- en: name = "${module.eks.cluster_name}-jupyterhub-single-user"
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: name = "${module.eks.cluster_name}-jupyterhub-single-user"
- en: 'annotations = {"eks.amazonaws.com/role-arn": module.jupyterhub_single_user_irsa.iam_role_arn}'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'annotations = {"eks.amazonaws.com/role-arn": module.jupyterhub_single_user_irsa.iam_role_arn}'
- en: '...'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: '[PRE6]'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: data "http" "jupyterhub_values" {
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: data "http" "jupyterhub_values" {
- en: url = "https://kubernetes-for-genai-models.s3.amazonaws.com/chapter5/jupyterhub-values.yaml"
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: url = "https://kubernetes-for-genai-models.s3.amazonaws.com/chapter5/jupyterhub-values.yaml"
- en: '}'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '}'
- en: module "eks_data_addons" {
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: module "eks_data_addons" {
- en: source = "aws-ia/eks-data-addons/aws"
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: source = "aws-ia/eks-data-addons/aws"
- en: '...'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: enable_jupyterhub = true
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: enable_jupyterhub = true
- en: jupyterhub_helm_config = {
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: jupyterhub_helm_config = {
- en: values = [local.jupyterhub_values_rendered]
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: values = [local.jupyterhub_values_rendered]
- en: '...'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: '[PRE7]'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: $ terraform init
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $ terraform init
- en: $ terraform plan
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $ terraform plan
- en: $ terraform apply -auto-approve
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $ terraform apply -auto-approve
- en: '[PRE8]'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: $ helm list -n jupyterhub
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $ helm list -n jupyterhub
- en: NAME             NAMESPACE       REVISION          STATUS
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: NAME             NAMESPACE       REVISION          STATUS
- en: eks.tf that contains GPU nodes (g6.2xlarge). Here, we are using EC2 Spot Instances
    pricing to minimize AWS charges; please refer to the documentation at https://aws.amazon.com/ec2/spot/pricing/
    for pricing details. We are also adding K8s taints (https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)
    to ensure that only GPU workloads are scheduled to these worker nodes. K8s taints
    are key-value pairs that are applied to nodes that prevent certain Pods from being
    scheduled on them unless the Pods tolerate the taints, allowing for better control
    over workload placement. By applying taints, we can ensure that non-GPU workloads
    are prevented from being scheduled on GPU nodes, reserving those nodes exclusively
    for GPU-optimized Pods. You can download the eks.tf file from https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/eks.tf.
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: eks.tf 文件包含 GPU 节点（g6.2xlarge）。在这里，我们使用 EC2 Spot 实例定价以最小化 AWS 费用；有关定价详情，请参考
    [https://aws.amazon.com/ec2/spot/pricing/](https://aws.amazon.com/ec2/spot/pricing/)。我们还为
    K8s 节点添加了污点（[https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)），确保只有
    GPU 工作负载被调度到这些工作节点。K8s 污点是键值对，应用于节点，以防止某些 Pod 在不容忍污点的情况下调度到这些节点，从而更好地控制工作负载的位置。通过应用污点，我们可以确保非
    GPU 工作负载不会调度到 GPU 节点，从而将这些节点专门保留给 GPU 优化的 Pod。你可以从 [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/eks.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/eks.tf)
    下载 eks.tf 文件。
- en: '[PRE9]'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: $ terraform init
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $ terraform init
- en: $ terraform plan
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $ terraform plan
- en: $ terraform apply -auto-approve
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $ terraform apply -auto-approve
- en: $ kubectl get nodes -l nvidia.com/gpu.present=true
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $ kubectl get nodes -l nvidia.com/gpu.present=true
- en: NAME                                            STATUS
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: NAME                                            STATUS
- en: ip-10-0-17-1.us-west-2.compute.internal         Ready
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: ip-10-0-17-1.us-west-2.compute.internal         Ready
- en: '[PRE11]'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: $ kubectl port-forward svc/proxy-public 8000:80 -n jupyterhub
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $ kubectl port-forward svc/proxy-public 8000:80 -n jupyterhub
- en: '[PRE12]'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: $ terraform output jupyter_pwd
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $ terraform output jupyter_pwd
- en: '[PRE13]'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Figure 5.2 – JupyterHub login page](img/B31108_05_2.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – JupyterHub 登录页面](img/B31108_05_2.jpg)'
- en: Figure 5.2 – JupyterHub login page
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – JupyterHub 登录页面
- en: 'After logging in, you will be presented with three notebook options. Since
    we are using JupyterHub for GenAI tasks that need GPU power, select **Data Science
    (GPU)** and click **Start**, as shown in *Figure 5**.3*:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录后，您将看到三个 notebook 选项。由于我们使用 JupyterHub 进行需要 GPU 支持的 GenAI 任务，请选择**数据科学 (GPU)**并点击**开始**，如*图
    5.3*所示：
- en: '![Figure 5.3 – JupyterHub home page](img/B31108_05_3.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – JupyterHub 主页](img/B31108_05_3.jpg)'
- en: Figure 5.3 – JupyterHub home page
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – JupyterHub 主页
- en: 'As shown in *Figure 5**.4*, this will start a new notebook instance running
    in a K8s Pod and also request a **Persistent Volume Claim** (**PVC**) ([https://kubernetes.io/docs/concepts/storage/persistent-volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes))
    so that the EBS CSI driver will create an Amazon EBS volume and attach it to the
    notebook. We’re using a persistent volume here to preserve the notebook’s state,
    data, and configurations across Pod restarts and terminations. This allows the
    notebook instance to be terminated after periods of inactivity and relaunched
    when the user returns, making the process more cost-efficient:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如*图 5.4*所示，这将启动一个新的 notebook 实例，该实例将在 K8s Pod 中运行，并请求一个**持久卷声明**（**PVC**）（[https://kubernetes.io/docs/concepts/storage/persistent-volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes)），以便
    EBS CSI 驱动程序创建一个 Amazon EBS 卷并将其附加到 notebook。我们在这里使用持久卷，以便在 Pod 重启和终止时保留 notebook
    的状态、数据和配置。这使得 notebook 实例可以在一段时间不活动后终止，并在用户返回时重新启动，从而提高了成本效率：
- en: '[PRE14]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Figure 5.4 – Our Jupyter notebook](img/B31108_05_4.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 我们的 Jupyter notebook](img/B31108_05_4.jpg)'
- en: Figure 5.4 – Our Jupyter notebook
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 我们的 Jupyter notebook
- en: 'You can now import the following notebooks from [*Chapter 4*](B31108_04.xhtml#_idTextAnchor049)
    and execute the necessary commands to test both the **RAG** and **fine-tuning**
    examples:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以从[*第 4 章*](B31108_04.xhtml#_idTextAnchor049)导入以下 notebook，并执行必要的命令来测试
    **RAG** 和 **微调** 示例：
- en: '**RAG** **notebook**: [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_RAG_Example.ipynb](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_RAG_Example.ipynb)'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RAG** **notebook**: [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_RAG_Example.ipynb](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_RAG_Example.ipynb)'
- en: '**Fine-tuning** **notebook**: [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_FineTuning_Example.ipynb](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_FineTuning_Example.ipynb)'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调** **notebook**: [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_FineTuning_Example.ipynb](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_FineTuning_Example.ipynb)'
- en: With that, we have set up JupyterHub on the EKS cluster and used it to launch
    a Jupyter notebook to test our GenAI experimentation scripts. Next, we will containerize
    both the fine-tuning and RAG scripts and run them as K8s Pods on the EKS cluster.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个设置，我们已经在 EKS 集群上部署了 JupyterHub，并用它启动了一个 Jupyter notebook 来测试我们的 GenAI 实验脚本。接下来，我们将把微调和
    RAG 脚本容器化，并作为 K8s Pods 在 EKS 集群上运行。
- en: Fine-tuning Llama 3 in K8s
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 K8s 中微调 Llama 3
- en: As discussed in [*Chapter 3*](B31108_03.xhtml#_idTextAnchor039), running fine-tuning
    workloads on K8s has several advantages, including scalability, efficient resource
    utilization, portability, and monitoring. In this section, we will containerize
    the **Llama 3 fine-tuning job** that we experimented with in the Jupyter notebook
    and deploy it on the EKS cluster. This is essential for automating the end-to-end
    AI/ML pipelines and versioning the models.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[*第 3 章*](B31108_03.xhtml#_idTextAnchor039)中讨论的那样，在 K8s 上运行微调工作负载有多个优势，包括可扩展性、高效的资源利用率、可移植性和监控。在本节中，我们将把在
    Jupyter notebook 中进行的**Llama 3 微调任务**容器化，并将其部署到 EKS 集群上。这对于自动化端到端的 AI/ML 流水线以及模型版本管理至关重要。
- en: 'The following steps are involved in fine-tuning a Llama 3 model:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 微调 Llama 3 模型的步骤如下：
- en: '*Gather training and evaluation datasets* and store them in **Amazon S3**.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*收集训练和评估数据集*并将其存储在**Amazon S3**中。'
- en: '*Create a container image* and upload it to **Amazon ECR**.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*创建容器镜像*并将其上传到**Amazon ECR**。'
- en: '*Deploy the fine-tuning job* in the **EKS cluster**.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在**EKS 集群**中部署微调任务*。'
- en: Data preparation
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'We will utilize two datasets (training and evaluation) to fine-tune and validate
    a `kubernetes-for-genai-models`. **Amazon S3** ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/))
    is an object storage service that’s used to store and retrieve any amount of data
    from anywhere, making it the ideal choice for sharing large datasets for collaboration:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用两个数据集（训练集和评估集）来微调并验证`kubernetes-for-genai-models`。**Amazon S3** ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/))
    是一个对象存储服务，用于存储和检索任何数量的数据，适用于任何地方的访问，因此它是共享大规模数据集进行协作的理想选择：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this section, we explored the datasets that will be used to fine-tune the
    Llama 3 model for our e-commerce use case. We also covered best practices such
    as storing these databases in external storage services such as Amazon S3 rather
    than packaging them in the container image. In the next section, we will focus
    on creating a container image for the fine-tuning job.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了将用于微调 Llama 3 模型的数据库，这些数据库用于我们的电子商务应用场景。我们还介绍了最佳实践，例如将这些数据库存储在 Amazon
    S3 等外部存储服务中，而不是将它们打包到容器镜像中。在下一节中，我们将专注于为微调任务创建容器镜像。
- en: Creating a container image
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建容器镜像
- en: 'To create a container image for the fine-tuning job, we need a **Dockerfile**,
    a **fine-tuning script**, and a **dependency list**. We’ve already created these
    artifacts and made them available on GitHub: [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/llama-finetuning](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/llama-finetuning).
    Let’s start building the container so that we can fine-tune the Llama 3 model
    with these artifacts:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为微调任务创建容器镜像，我们需要一个**Dockerfile**、一个**微调脚本**和一个**依赖项列表**。我们已经创建了这些工件，并将它们上传到
    GitHub： [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/llama-finetuning](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/llama-finetuning)。让我们开始构建容器，以便我们可以使用这些工件微调
    Llama 3 模型：
- en: 'Create a directory named `llama-finetuning`:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建名为`llama-finetuning`的目录：
- en: '[PRE16]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '...'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: train_dataset_file = os.environ.get('TRAIN_DATASET_FILE')
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: train_dataset_file = os.environ.get('TRAIN_DATASET_FILE')
- en: eval_dataset_file = os.environ.get('EVAL_DATASET_FILE')
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: eval_dataset_file = os.environ.get('EVAL_DATASET_FILE')
- en: train_dataset = load_dataset('json', data_files=train_dataset_file, split='train')
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: train_dataset = load_dataset('json', data_files=train_dataset_file, split='train')
- en: eval_dataset = load_dataset('json', data_files=eval_dataset_file, split='train')
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: eval_dataset = load_dataset('json', data_files=eval_dataset_file, split='train')
- en: '[PRE17]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After training, we need to save the model weights, configuration files, and
    tokenizer configuration so that they can be used to create an inference container
    later:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练后，我们需要保存模型权重、配置文件和分词器配置，以便它们可以用于后续创建推理容器：
- en: '[PRE18]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Export the model weights and configuration files to an S3 bucket:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型权重和配置文件导出到 S3 存储桶：
- en: '[PRE19]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Create a Dockerfile where we can build the fine-tuning container image. It
    should start with the **nvidia/cuda** ([https://hub.docker.com/r/nvidia/cuda](https://hub.docker.com/r/nvidia/cuda))
    parent image, install the required Python dependencies, and contain the fine-tuning
    script. The complete file is available at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/Dockerfile](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/Dockerfile):'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 Dockerfile，我们可以用来构建微调容器镜像。它应当以**nvidia/cuda** ([https://hub.docker.com/r/nvidia/cuda](https://hub.docker.com/r/nvidia/cuda))
    父镜像开始，安装所需的 Python 依赖，并包含微调脚本。完整文件可在 [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/Dockerfile](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/Dockerfile)
    获取：
- en: '[PRE20]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Create the container image by running the following command:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令来创建容器镜像：
- en: '[PRE21]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You can verify the container image by using the following docker command:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用以下 docker 命令验证容器镜像：
- en: '[PRE22]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With that, we’ve successfully built the container image so that we can fine-tune
    the Llama 3 model. Next, we will upload this to an Amazon ECR repository and deploy
    it to the EKS cluster.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经成功构建了容器镜像，以便微调 Llama 3 模型。接下来，我们将把它上传到 Amazon ECR 仓库，并将其部署到 EKS 集群中。
- en: Deploying the fine-tuning job
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署微调任务
- en: 'To deploy the fine-tuning job, we need to save the container image in **Amazon
    ECR** and create an **S3 bucket** where we’ll save model assets, as well as create
    a **K8s job** in the **EKS cluster**:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了部署微调任务，我们需要将容器镜像保存到**Amazon ECR**，并创建一个**S3存储桶**，在其中保存模型资产，同时还需在**EKS集群**中创建一个**K8s任务**：
- en: 'Create an `ecr.tf` file in the `genai-eks-demo` directory. The complete code
    is available on GitHub at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf):'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`genai-eks-demo`目录中创建一个`ecr.tf`文件。完整代码可以在GitHub上找到：[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf)：
- en: '[PRE23]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Create an Amazon S3 bucket so that you can store the fine-tuned model assets.
    We’ll create one using Terraform. Download the `model-assets.tf` file from [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/model-assets.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/model-assets.tf):'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个Amazon S3存储桶，以便存储微调后的模型资产。我们将使用Terraform创建一个。请从[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/model-assets.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/model-assets.tf)下载`model-assets.tf`文件：
- en: '[PRE24]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We also need to create a `eks.tf` file you downloaded earlier:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要创建一个之前下载的`eks.tf`文件：
- en: '[PRE25]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Run the following commands to create the **ECR repository** and **S3 bucket**.
    The S3 bucket’s name will be printed in the output:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令以创建**ECR仓库**和**S3存储桶**。S3存储桶的名称将在输出中显示：
- en: '[PRE26]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: $ terraform output -raw my_llama_finetuned_ecr_push_cmds
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $ terraform output -raw my_llama_finetuned_ecr_push_cmds
- en: aws ecr get-login-password --region us-west-2 | docker login --username AWS
    --password-stdin 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: aws ecr get-login-password --region us-west-2 | docker login --username AWS
    --password-stdin 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned
- en: docker tag my-llama-finetuned 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: docker tag my-llama-finetuned 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned
- en: docker push 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: docker push 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned
- en: '[PRE27]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now that we’ve created the required infrastructure, let’s go ahead and deploy
    the fine-tuning job to the **EKS cluster**. To do so, we need to create a **K8s
    Job manifest file** that will run this as a K8s Job. Download the manifest from
    GitHub at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/llama-finetuning-job.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/llama-finetuning-job.yaml)
    and replace the image, the Hugging Face token, and the name of the S3 bucket that
    contains the model assets you created previously:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经创建了所需的基础设施，接下来让我们将微调任务部署到**EKS集群**。为此，我们需要创建一个**K8s任务清单文件**，将其作为K8s任务运行。从GitHub下载清单文件：[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/llama-finetuning-job.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/llama-finetuning-job.yaml)，并替换镜像、Hugging
    Face令牌以及之前创建的包含模型资产的S3存储桶名称：
- en: '[PRE28]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Run the following commands to run the job on the EKS cluster:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令以在EKS集群上运行该任务：
- en: '[PRE29]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'A K8s Pod will be scheduled on a GPU node and start the fine-tuning process.
    You can monitor its process by tailing its logs:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个K8s Pod将被调度到GPU节点上并启动微调过程。你可以通过查看日志来监控其进度：
- en: '[PRE30]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once fine-tuning is complete, the job will automatically upload the model assets
    to the S3 bucket, as shown in *Figure 5**.5*:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调完成后，任务将自动将模型资产上传到S3存储桶，如*图 5.5*所示：
- en: '![Figure 5.5 – An Amazon S3 bucket that contains fine-tuned model assets](img/B31108_05_5.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 一个包含微调模型资产的Amazon S3存储桶](img/B31108_05_5.jpg)'
- en: Figure 5.5 – An Amazon S3 bucket that contains fine-tuned model assets
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 一个包含微调模型资产的Amazon S3存储桶
- en: In this section, we began by modifying our Llama 3 fine-tuning script to make
    it container-ready. Then, we created a container image that includes the script
    and its dependent libraries, using the `nvidia/cuda` image from **DockerHub**
    as the base. Finally, we created an Amazon ECR repository that will store the
    container image and deploy it as a K8s job in the EKS cluster. In the next section,
    we will utilize the fine-tuned model assets to create an inference container image
    and deploy it in the EKS cluster.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先修改了我们的Llama 3精细调整脚本，使其可以用容器方式运行。然后，我们创建了一个包含该脚本及其依赖库的容器镜像，使用**DockerHub**上的`nvidia/cuda`镜像作为基础。最后，我们创建了一个Amazon
    ECR存储容器镜像的存储库，并将其作为K8s作业部署到EKS集群中。在接下来的部分，我们将利用经过精细调整的模型资产创建推理容器镜像，并在EKS集群中部署它。
- en: Deploying the fine-tuned model on K8s
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在K8s上部署经过精细调整的模型
- en: In this section, we will containerize the fine-tuned **Llama 3 model** using
    **Python FastAPI** and deploy the inference endpoint as a K8s deployment in the
    EKS cluster.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用**Python FastAPI**将经过精细调整的**Llama 3模型**容器化，并部署为EKS集群中的K8s部署。
- en: 'Let’s start by creating the inference container using the fine-tuned model
    assets stored in the S3 bucket. We will be using Python FastAPI ([https://fastapi.tiangolo.com/](https://fastapi.tiangolo.com/))
    to expose the model as an **HTTP API**. FastAPI is a modern high-performance web
    framework for building APIs in Python:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从S3桶中的存储的精细调整模型资产开始创建推理容器。我们将使用Python FastAPI ([https://fastapi.tiangolo.com/](https://fastapi.tiangolo.com/))将模型暴露为**HTTP
    API**。FastAPI是一个现代高性能的Python构建API的Web框架：
- en: 'Create a directory called `llama-finetuned-inf`:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建名为`llama-finetuned-inf`的目录：
- en: '[PRE31]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Download the model assets from the S3 bucket to the local directory so that
    we can copy them to the container image:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从S3桶下载模型资产到本地目录，以便我们可以将它们复制到容器镜像中：
- en: '[PRE32]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: FROM nvidia/cuda:12.8.1-runtime-ubuntu24.04
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FROM nvidia/cuda:12.8.1-runtime-ubuntu24.04
- en: '...'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: RUN pip install torch transformers peft accelerate bitsandbytes sentencepiece
    fastapi uvicorn
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RUN pip install torch transformers peft accelerate bitsandbytes sentencepiece
    fastapi uvicorn
- en: COPY model-assets /app/model-assets
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: COPY model-assets /app/model-assets
- en: COPY main.py /app/main.py
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: COPY main.py /app/main.py
- en: CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "80"]
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "80"]
- en: '[PRE33]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we will wrap the fine-tuned Llama 3 model into an API using `/generate`
    that accepts **HTTP POST** requests and returns a response after invoking the
    model. You can download the complete code from [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/main.py](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/main.py):'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`/generate`将精细调整的Llama 3模型包装成一个接受**HTTP POST**请求并在调用模型后返回响应的API。您可以从[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/main.py](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/main.py)下载完整的代码：
- en: '[PRE34]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Create the container image by running the following commands:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令创建容器镜像：
- en: '[PRE35]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We will be reusing the **ECR repository** for this image. Alternatively, you
    can create a new repository using Terraform, as described in [*Chapter 3*](B31108_03.xhtml#_idTextAnchor039).
    Replace the **account number** and **region** values in the following commands
    before running them:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重用**ECR存储库**用于此镜像。或者，您可以根据Terraform创建一个新的存储库，如[*第3章*](B31108_03.xhtml#_idTextAnchor039)所述。在运行这些命令之前，请替换以下命令中的**帐户号码**和**区域**值：
- en: '[PRE36]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'apiVersion: apps/v1'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'apiVersion: apps/v1'
- en: 'kind: Deployment'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'kind: Deployment'
- en: 'metadata:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'metadata:'
- en: 'name: my-llama-finetuned-deployment'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'name: my-llama-finetuned-deployment'
- en: 'spec:'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'spec:'
- en: '...'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: 'containers:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'containers:'
- en: '- name: llama-finetuned-container'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- name: llama-finetuned-container'
- en: 'image: <<Replace your ECR image here>>'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'image: <<替换您的ECR镜像位置>>'
- en: '...'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: 'env:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'env:'
- en: '- name: HUGGING_FACE_HUB_TOKEN'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- name: HUGGING_FACE_HUB_TOKEN'
- en: 'value: "<<Replace your Hugging face token here>>"'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'value: "<<替换您的Hugging Face令牌位置>>"'
- en: '...'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: '[PRE37]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Finally, deploy the model by running the following commands:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过运行以下命令部署模型：
- en: '[PRE38]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'It may take a few minutes for the K8s Pod to be ready since the image needs
    to be downloaded from ECR. Run the following command to verify its status:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于需要从ECR下载镜像，K8s Pod可能需要几分钟准备就绪。运行以下命令来验证其状态：
- en: '[PRE39]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In this section, we took the model assets that were generated via the fine-tuning
    process and wrapped them with Python FastAPI to create an HTTP API. Then, we created
    a container, deployed it to the EKS cluster, and exposed it via the K8s ClusterIP
    service. This demonstrates how you can customize the behavior of general-purpose
    LLMs with your dataset and serve them using K8s. In the next section, we will
    explore how to deploy a RAG application on K8s.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们采用了通过微调过程生成的模型资产，并使用 Python FastAPI 将其包装成 HTTP API。接着，我们创建了一个容器，将其部署到
    EKS 集群，并通过 K8s ClusterIP 服务暴露出来。这演示了如何使用您的数据集自定义通用 LLM 的行为，并通过 K8s 服务它们。在下一节中，我们将探索如何在
    K8s 上部署 RAG 应用程序。
- en: Deploy a RAG application on K8s
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 K8s 上部署 RAG 应用程序。
- en: 'As explained in [*Chapter 4*](B31108_04.xhtml#_idTextAnchor049), RAG allows
    us to integrate external knowledge sources into the **LLM response generation
    process**, leading to more accurate and contextually relevant responses. By providing
    up-to-date and relevant information in the input, RAG also reduces errors such
    as hallucinations ([https://www.ibm.com/topics/ai-hallucinations](https://www.ibm.com/topics/ai-hallucinations)).
    In this section, we will explore how to deploy a RAG application to a K8s cluster.
    The following high-level steps are involved:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [*第 4 章*](B31108_04.xhtml#_idTextAnchor049) 中所述，RAG 使我们能够将外部知识源集成到 **LLM 响应生成过程**
    中，从而生成更准确、更具上下文相关性的响应。通过在输入中提供最新且相关的信息，RAG 还减少了诸如幻觉现象的错误 ([https://www.ibm.com/topics/ai-hallucinations](https://www.ibm.com/topics/ai-hallucinations))。在本节中，我们将探索如何将
    RAG 应用程序部署到 K8s 集群。以下是涉及的高层步骤：
- en: Set up a vector database.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个向量数据库。
- en: Create a RAG application to query the vector store and call the LLM with both
    the input and contextual data.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 RAG 应用程序，用于查询向量存储，并通过输入和上下文数据调用 LLM。
- en: Deploy the RAG application on K8s.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 K8s 上部署 RAG 应用程序。
- en: Load and index the data in the vector store.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在向量存储中加载和索引数据。
- en: 'We’ll begin by setting up a **vector database**, which is a specialized datastore
    that’s used for storing and querying high-dimensional vector embeddings. In RAG,
    such databases play an essential role by allowing a similarity search to be performed
    on the input request and relevant information to be retrieved. There are many
    open source and commercial vector databases available, such as **Pinecone** ([https://www.pinecone.io/](https://www.pinecone.io/)),
    **Qdrant** ([https://qdrant.tech/](https://qdrant.tech/)), **Chroma** ([https://www.trychroma.com/](https://www.trychroma.com/)),
    and **OpenSearch** ([https://opensearch.org/](https://opensearch.org/)). Since
    many of these offerings are available as managed or SaaS models, one question
    naturally arises: when should we opt to run a self-hosted vector database in K8s?
    The main factors we should consider are as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先设置一个 **向量数据库**，它是一个专门用于存储和查询高维向量嵌入的数据存储。在 RAG 中，这些数据库通过允许对输入请求进行相似性搜索并检索相关信息，起着至关重要的作用。市面上有很多开源和商业向量数据库，例如
    **Pinecone** ([https://www.pinecone.io/](https://www.pinecone.io/))、**Qdrant**
    ([https://qdrant.tech/](https://qdrant.tech/))、**Chroma** ([https://www.trychroma.com/](https://www.trychroma.com/))
    和 **OpenSearch** ([https://opensearch.org/](https://opensearch.org/))。由于这些产品很多都可以作为托管或
    SaaS 模型使用，因此一个自然产生的问题是：我们应该什么时候选择在 K8s 中运行自托管的向量数据库？我们需要考虑的主要因素如下：
- en: Performance and latency
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能和延迟
- en: Data sovereignty and compliance
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据主权和合规性
- en: How customizable the configuration is
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置的可定制性
- en: Cost control
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本控制
- en: How to avoid vendor lock-in
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何避免供应商锁定
- en: 'Follow these steps:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤进行：
- en: For our setup, we are using a Qdrant vector database to store MyRetail’s sales
    catalog. Let’s download the `qdrant.tf` file from our GitHub repository at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/qdrant.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/qdrant.tf)
    and run `terraform apply` command. This will install the Qdrant vector database
    as a K8s StatefulSet ([https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/))
    on the EKS cluster in the `qdrant` namespace using a publicly available Helm chart.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于我们的设置，我们使用 Qdrant 向量数据库来存储 MyRetail 的销售目录。让我们从我们的 GitHub 仓库下载`qdrant.tf`文件，地址是
    [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/qdrant.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/qdrant.tf)，并运行
    `terraform apply` 命令。这将安装 Qdrant 向量数据库作为 K8s StatefulSet ([https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/))，并在
    EKS 集群的 `qdrant` 命名空间中使用公开的 Helm chart 进行部署。
- en: '[PRE40]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create an `ecr.tf` file or you can download the complete code from GitHub at
    [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf):'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '创建一个`ecr.tf`文件，或者你可以从GitHub下载完整代码：[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf):'
- en: '[PRE41]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Run the following commands to deploy the Qdrant `kubectl` command. The following
    output shows that a `qdrant` vector database Pod has been deployed and is `Running`:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令部署Qdrant `kubectl`命令。以下输出显示`qdrant`向量数据库Pod已部署并处于`Running`状态：
- en: '[PRE42]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Optionally, you can connect to Qdrant’s Web UI to interact with the vector
    database. Run the following command to connect to the Qdrant Web UI locally. Please
    refer to the Qdrant documentation at [https://qdrant.tech/documentation/interfaces/web-ui/](https://qdrant.tech/documentation/interfaces/web-ui/)
    to learn about its various features:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选地，你可以连接到Qdrant的Web UI与向量数据库进行交互。运行以下命令在本地连接到Qdrant Web UI。请参考Qdrant文档：[https://qdrant.tech/documentation/interfaces/web-ui/](https://qdrant.tech/documentation/interfaces/web-ui/)了解其各种功能：
- en: '[PRE43]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '@app.post("/load_data")'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '@app.post("/load_data")'
- en: 'async def load_data(request: LoadDataModel):'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'async def load_data(request: LoadDataModel):'
- en: '...'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: response = requests.get(request.url)
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: response = requests.get(request.url)
- en: reader = csv.DictReader(file_content)
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: reader = csv.DictReader(file_content)
- en: '...'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: qdrant_store = QdrantVectorStore(
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: qdrant_store = QdrantVectorStore(
- en: embedding=OpenAIEmbeddings(),
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: embedding=OpenAIEmbeddings(),
- en: collection_name=collection_name,
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: collection_name=collection_name,
- en: client=qdrant_client
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: client=qdrant_client
- en: )
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: )
- en: qdrant_store.add_documents(docs)
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: qdrant_store.add_documents(docs)
- en: '[PRE44]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Exposes a Python FastAPI endpoint called `/generate` that accepts a user prompt
    and an **optional session ID**. It creates the embedding for the input prompt
    using **OpenAIEmbeddings** and performs a similarity search against the Qdrant
    vector database to retrieve the relevant context information:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 暴露一个名为`/generate`的Python FastAPI端点，接受用户输入的提示和**可选的会话ID**。它使用**OpenAIEmbeddings**为输入提示生成嵌入，并在Qdrant向量数据库中执行相似性搜索以检索相关的上下文信息：
- en: '[PRE45]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Creates a conversational RAG application that remembers a user’s prior questions
    and answers and applies logic that can be incorporated into the current request.
    Here, we are building `rag_chain` using `history_aware_retriever` ([https://python.langchain.com/api_reference/langchain/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html))
    from the **LangChain** package ([https://python.langchain.com/docs/introduction/](https://python.langchain.com/docs/introduction/)).
    It takes an input prompt and past chat history and calls an LLM (OpenAI’s **GPT-3.5
    Turbo**) to fetch the contextualized input. LangChain is a framework that’s designed
    to build applications powered by LLMs, enabling easy integration, management,
    and orchestration of multiple models and data pipelines for tasks such as creating
    chatbots, generating text, and more:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个对话式RAG应用程序，记住用户的先前问题和回答，并应用可以纳入当前请求的逻辑。在这里，我们使用**LangChain**包中的`history_aware_retriever`([https://python.langchain.com/api_reference/langchain/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html))构建`rag_chain`。它接受输入提示和之前的聊天记录，并调用LLM（OpenAI的**GPT-3.5
    Turbo**）来获取上下文化的输入。LangChain是一个用于构建由LLM驱动的应用程序的框架，旨在简化多个模型和数据管道的集成、管理和协调，用于创建聊天机器人、生成文本等任务：
- en: '[PRE46]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Invokes the RAG chain with the input and returns the response, along with the
    session ID, in **JSON format**:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用RAG链，传入输入并返回响应，以及**JSON格式**的会话ID：
- en: '[PRE47]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '*Figure 5**.6* illustrates the complete RAG application flow. The process begins
    with contextualizing the latest user prompt using an LLM, which reformulates the
    query based on the chat history. Then, the retriever component takes the rephrased
    query to gather relevant context from the conversation. Finally, `question_answer_chain`
    combines the retrieved context, the chat history, and the current user input to
    generate the final answer:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.6* 展示了完整的RAG应用流程。流程开始时使用LLM对最新的用户提示进行上下文化处理，LLM会根据聊天记录重新组织查询。然后，检索组件会将重新措辞的查询用于从对话中收集相关上下文。最后，`question_answer_chain`将检索到的上下文、聊天记录和当前的用户输入结合起来，生成最终答案：'
- en: '![Figure 5.6 – RAG application flow](img/B31108_05_6.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – RAG应用流程](img/B31108_05_6.jpg)'
- en: Figure 5.6 – RAG application flow
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – RAG 应用流程
- en: 'Create a directory named `rag-app`:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `rag-app` 的目录：
- en: '[PRE48]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Download the `main.py` and `requirements.txt` files with our RAG application
    code and dependent libraries from [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/rag-app](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/rag-app)
    and place them in `rag-app` directory.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/rag-app](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/rag-app)
    下载 `main.py` 和 `requirements.txt` 文件，这些文件包含了我们的 RAG 应用程序代码和依赖库，并将它们放入 `rag-app`
    目录中。
- en: 'The next step is to create a Dockerfile for this RAG application. We will start
    with a Python parent image, install the necessary FastAPI dependencies, add the
    Python application code, and run the **uvicorn** ([https://www.uvicorn.org/](https://www.uvicorn.org/))
    command to start a web server for the FastAPI application. The complete Dockerfile
    is available at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/Dockerfile](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/Dockerfile):'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是为这个 RAG 应用程序创建一个 Dockerfile。我们将从一个 Python 父镜像开始，安装必要的 FastAPI 依赖项，添加 Python
    应用程序代码，并运行 **uvicorn**（[https://www.uvicorn.org/](https://www.uvicorn.org/)）命令来启动
    FastAPI 应用程序的 Web 服务器。完整的 Dockerfile 可在 [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/Dockerfile](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/Dockerfile)
    查看：
- en: '[PRE49]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Build the container image and push it to the ECR repository. Replace the **account
    number** and **region** values before running them:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建容器镜像并将其推送到 ECR 仓库。在运行之前，请替换 **账户号** 和 **区域** 值：
- en: '[PRE50]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'apiVersion: apps/v1'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'apiVersion: apps/v1'
- en: 'kind: Deployment'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类型：部署
- en: 'metadata:'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'name: rag-app-deployment'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'name: rag-app-deployment'
- en: '...'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: 'containers:'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 容器：
- en: '- name: rag-app-container'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- name: rag-app-container'
- en: 'image: <<Replace your ECR image here>>'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 镜像：<<在此替换您的 ECR 镜像>>
- en: '...'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: 'env:'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 环境变量：
- en: '- name: OPENAI_API_KEY'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- name: OPENAI_API_KEY'
- en: 'value: "<<Replace your OpenAI API Key here>>"'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 值："<<在此替换您的 OpenAI API 密钥>>"
- en: '...'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: '[PRE51]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'After updating the manifest, you can run the following commands to deploy the
    RAG application to the cluster. This will create a K8s deployment called `rag-app-deploment`
    with one replica that’s exposed via a ClusterIP service on port `80`. You can
    validate this by running the following command:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新清单后，你可以运行以下命令将 RAG 应用程序部署到集群中。这将创建一个名为 `rag-app-deployment` 的 K8s 部署，并创建一个副本，通过
    ClusterIP 服务暴露在端口 `80` 上。你可以通过运行以下命令来验证：
- en: '[PRE52]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: $ kubectl apply -f qdrant-restore-job.yaml
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $ kubectl apply -f qdrant-restore-job.yaml
- en: batch/job qdrant-restore-job created
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批处理/作业 qdrant-restore-job 已创建
- en: '[PRE53]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: In this section, we created a conversational RAG application using the LangChain
    framework, a Qdrant vector database, and OpenAI GenAI models. We containerized
    the application by exposing it as a Python FastAPI and deployed it to the EKS
    cluster. In addition, we created another example RAG application that utilizes
    **Amazon Bedrock** ([https://aws.amazon.com/bedrock/](https://aws.amazon.com/bedrock/))
    and **Anthropic Claude** ([https://aws.amazon.com/bedrock/claude/](https://aws.amazon.com/bedrock/claude/))
    models instead of OpenAI. You can find it at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/bedrock-rag-app](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/bedrock-rag-app)
    and choose either one based on your preference. In the next section, we’ll tie
    everything together using a Chatbot UI.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用 LangChain 框架、Qdrant 向量数据库和 OpenAI GenAI 模型创建了一个对话式 RAG 应用程序。我们将应用程序容器化，将其作为
    Python FastAPI 服务并部署到 EKS 集群中。此外，我们还创建了另一个示例 RAG 应用程序，使用 **Amazon Bedrock**（[https://aws.amazon.com/bedrock/](https://aws.amazon.com/bedrock/)）和
    **Anthropic Claude**（[https://aws.amazon.com/bedrock/claude/](https://aws.amazon.com/bedrock/claude/)）模型代替了
    OpenAI。你可以在 [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/bedrock-rag-app](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/bedrock-rag-app)
    找到它，并根据个人喜好选择一个。在接下来的章节中，我们将使用聊天机器人 UI 将所有内容整合在一起。
- en: Deploying a chatbot on K8s
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 K8s 上部署聊天机器人
- en: So far, we’ve deployed a fine-tuned Llama 3 model that has been trained on the
    MyElite loyalty program’s FAQ and a conversational RAG application using sales
    catalog data. Now, we will build a **Chatbot UI component** that will expose both
    services to MyRetail customers.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经部署了一个经过微调的 Llama 3 模型，该模型已在 MyElite 忠诚度计划的常见问题解答和一个基于销售目录数据的对话式 RAG
    应用程序上进行训练。现在，我们将构建一个 **聊天机器人 UI 组件**，该组件将向 MyRetail 客户展示这两项服务。
- en: We will be building the Chatbot UI using **Gradio** ([https://www.gradio.app/](https://www.gradio.app/)),
    an open source Python package used to build demos or web applications for ML models,
    APIs, and more. You can refer to the QuickStart guide at [https://www.gradio.app/guides/quickstart](https://www.gradio.app/guides/quickstart)
    to learn more about the Gradio framework. Alternatively, you can explore using
    UI frameworks such as **Streamlit** ([https://streamlit.io/](https://streamlit.io/)),
    **NiceGUI** ([https://nicegui.io/](https://nicegui.io/)), **Dash** ([https://github.com/plotly/dash](https://github.com/plotly/dash)),
    and **Flask** ([https://flask.palletsprojects.com/en/stable/](https://flask.palletsprojects.com/en/stable/))
    to build chatbot interfaces.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**Gradio** ([https://www.gradio.app/](https://www.gradio.app/))来构建聊天机器人UI，Gradio是一个开源Python包，用于构建机器学习模型、API等的演示或Web应用程序。您可以参考[https://www.gradio.app/guides/quickstart](https://www.gradio.app/guides/quickstart)上的快速入门指南，了解更多关于Gradio框架的信息。或者，您也可以探索使用UI框架，如**Streamlit**
    ([https://streamlit.io/](https://streamlit.io/))、**NiceGUI** ([https://nicegui.io/](https://nicegui.io/))、**Dash**
    ([https://github.com/plotly/dash](https://github.com/plotly/dash))和**Flask** ([https://flask.palletsprojects.com/en/stable/](https://flask.palletsprojects.com/en/stable/))来构建聊天机器人界面。
- en: 'For your convenience, we’ve already created a chatbot container and made it
    available publicly on DockerHub: [https://hub.docker.com/repository/docker/k8s4genai/chatbot-ui/](https://hub.docker.com/repository/docker/k8s4genai/chatbot-ui/).
    The source code for this application is available on GitHub at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/chatbot](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/chatbot)
    for your reference.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便您，我们已经创建了一个聊天机器人容器，并已将其公开发布在DockerHub上：[https://hub.docker.com/repository/docker/k8s4genai/chatbot-ui/](https://hub.docker.com/repository/docker/k8s4genai/chatbot-ui/)。该应用程序的源代码可以在GitHub上找到：[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/chatbot](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/chatbot)，供您参考。
- en: Important note
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We are creating a public-facing NLB for testing purposes. Feel free to restrict
    access to your IP address by updating the inbound rules of the NLB security group.
    Refer to the AWS NLB documentation at [https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-security-groups.html](https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-security-groups.html)
    for more details.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在创建一个面向公众的NLB进行测试。您可以通过更新NLB安全组的入站规则来限制对您IP地址的访问。有关更多详细信息，请参阅AWS NLB文档：[https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-security-groups.html](https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-security-groups.html)。
- en: 'Let’s deploy the chatbot application on the EKS cluster and configure it with
    both the fine-tuning Llama 3 deployment and the RAG application:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在EKS集群上部署聊天机器人应用程序，并将其与微调的Llama 3部署和RAG应用程序配置在一起：
- en: 'Download the Chatbot UI K8s deployment manifest from [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/chatbot/chatbot-deploy.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/chatbot/chatbot-deploy.yaml)
    and deploy it to the EKS cluster by running the following commands. This will
    create a K8s deployment with one replica of the Chatbot UI application and expose
    it to the public internet via **AWS Network** **Load Balancer**:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/chatbot/chatbot-deploy.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/chatbot/chatbot-deploy.yaml)下载聊天机器人UI
    K8s部署清单，并通过运行以下命令将其部署到EKS集群。这将创建一个包含一个副本的聊天机器人UI应用程序，并通过**AWS网络** **负载均衡器**将其暴露到公共互联网：
- en: '[PRE54]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Fetch the AWS Network Load Balancer endpoint by running the following command:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令获取AWS网络负载均衡器端点：
- en: '[PRE55]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Open the Chatbot UI by launching the Load Balancer URL in a web browser, as
    shown in *Figure 5**.7*. Now, you can interact with the chatbot by selecting the
    **Shopping** assistant (RAG application) or the **Loyalty Program** assistant
    (Llama 3 fine-tuned model) and typing a question in the chatbox – for example,
    *Please suggest walking shoes for a 60 year male in* *tabular format*:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在网页浏览器中启动负载均衡器URL打开聊天机器人UI，如*图 5.7*所示。现在，您可以通过选择**购物**助手（RAG应用程序）或**忠诚度计划**助手（微调的Llama
    3模型）并在聊天框中输入问题来与聊天机器人互动——例如，*请建议适合60岁男性的步行鞋，* *以表格形式展示*：
- en: '![Figure 5.7 – Chatbot UI](img/B31108_05_7.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – 聊天机器人UI](img/B31108_05_7.jpg)'
- en: Figure 5.7 – Chatbot UI
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – 聊天机器人UI
- en: 'Once the results are displayed, go ahead and ask a follow-up question – for
    example, *Can you sort the results in descending order by price*. As shown in
    *Figure 5**.8*, you will see that the results are sorted accordingly. Since the
    shopping assistant was developed with conversational RAG, it considers the user’s
    prior conversational history while answering the current prompt. In this example,
    we asked the chatbot to *Please suggest walking shoes for a 60 year male in tabular
    format*, followed by *Can you sort the results in descending order by price*.
    For the second query, the RAG application considered the user’s prior conversations
    and returned the sorted results:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦结果显示出来，您可以继续提出后续问题——例如，*您能按价格降序排列结果吗*。如*图 5.8*所示，您将看到结果已按要求排序。由于购物助手是通过对话式
    RAG 开发的，它在回答当前提示时会考虑用户的先前对话历史。在这个例子中，我们请求聊天机器人*请建议适合60岁男性的步行鞋，以表格格式展示*，然后是*您能按价格降序排列结果吗*。对于第二个查询，RAG
    应用程序考虑了用户的先前对话，并返回了排序后的结果：
- en: '![Figure 5.8 – Chatbot UI results](img/B31108_05_8.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – 聊天机器人 UI 结果](img/B31108_05_8.jpg)'
- en: Figure 5.8 – Chatbot UI results
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 聊天机器人 UI 结果
- en: 'You can also toggle between the **Shopping** assistant and the **Loyalty Program**
    assistant by using the options under **Choose an assistant** and ask questions
    related to the loyalty program, as shown in *Figure 5**.9*:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以通过使用**选择助手**下的选项，在**购物**助手和**忠诚度计划**助手之间切换，并提出与忠诚度计划相关的问题，如*图 5.9*所示：
- en: '![Figure 5.9 – Choosing the Loyalty Program assistant](img/B31108_05_9.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 选择忠诚度计划助手](img/B31108_05_9.jpg)'
- en: Figure 5.9 – Choosing the Loyalty Program assistant
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 选择忠诚度计划助手
- en: In this section, we deployed a Chatbot UI application that had been developed
    with the Gradio Python package to an EKS cluster and exposed it via the K8s LoadBalancer
    service. This application is connected to both the RAG application and the fine-tuned
    Llama 3 model we developed in this chapter so that it can answer user queries
    about MyRetail’s MyElite loyalty program and shopping catalog.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将一个使用 Gradio Python 包开发的聊天机器人 UI 应用程序部署到 EKS 集群，并通过 K8s LoadBalancer
    服务将其暴露。该应用程序连接到我们在本章中开发的 RAG 应用程序和微调的 Llama 3 模型，从而能够回答用户关于 MyRetail 的 MyElite
    忠诚度计划和购物目录的查询。
- en: Summary
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered how to fine-tune and deploy GenAI models in a K8s
    environment using Amazon EKS. We used a fictional company, MyRetail, as an example
    to highlight GenAI applications in e-commerce/retail business by creating personalized
    shopping experiences for our customers using GenAI models. This allowed us to
    automate responses for the company’s loyalty program and offer product recommendations.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了如何在使用 Amazon EKS 的 K8s 环境中微调和部署 GenAI 模型。我们使用一个虚构公司 MyRetail 作为示例，突出展示了在电子商务/零售业务中如何通过使用
    GenAI 模型为客户创造个性化购物体验。这使我们能够为公司的忠诚度计划自动化响应，并提供产品推荐。
- en: We began by discussing the importance of experimentation in the overall GenAI
    project life cycle and deployed JupyterHub in EKS. JupyterHub enables centralized
    access to computational resources such as GPUs, making it more suitable for large-scale
    AI tasks. Then, we created a Llama 3 fine-tuning container image and deployed
    it to the EKS cluster. The fine-tuning job utilized training and validation datasets
    from Amazon S3 to fine-tune the Llama 3 model and exported the model assets to
    S3\. We containerized the inference container using those model assets and deployed
    it to the EKS cluster as a K8s deployment.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先讨论了在整个 GenAI 项目生命周期中实验的重要性，并在 EKS 上部署了 JupyterHub。JupyterHub 提供了集中式访问计算资源，如
    GPU，使其更适合大规模 AI 任务。然后，我们创建了一个 Llama 3 微调容器镜像，并将其部署到 EKS 集群中。微调任务利用来自 Amazon S3
    的训练和验证数据集对 Llama 3 模型进行微调，并将模型资产导出到 S3。我们使用这些模型资产对推理容器进行了容器化，并将其作为 K8s 部署部署到 EKS
    集群中。
- en: This chapter also outlined how to deploy a RAG application that queries a vector
    database (Qdrant) to retrieve context-relevant information before calling the
    LLM to generate responses. This reduces hallucinations and improves response accuracy
    by incorporating external data. Finally, we deployed a Chatbot UI and connected
    it to both the fine-tuned Llama 3 model and the RAG application to enhance the
    shopping experience for MyRetail customers. In the next chapter, we will explore
    various autoscaling constructs provided by K8s and how we can leverage them to
    optimize GenAI workloads.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还概述了如何部署一个 RAG 应用，该应用查询向量数据库（Qdrant）以获取与上下文相关的信息，然后调用 LLM 生成响应。通过引入外部数据，这样可以减少幻觉现象并提高响应的准确性。最后，我们部署了一个聊天机器人
    UI，并将其与微调后的 Llama 3 模型和 RAG 应用连接，以提升 MyRetail 客户的购物体验。在下一章中，我们将探索 K8s 提供的各种自动扩展构件，并讨论如何利用它们来优化
    GenAI 工作负载。
