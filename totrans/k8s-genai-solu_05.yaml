- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Working with GenAI on K8s: Chatbot Example'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will build on the examples we discussed in [*Chapter 4*](B31108_04.xhtml#_idTextAnchor049)
    and start deploying those examples in **K8s**/**Amazon EKS**. We will start by
    deploying **JupyterHub** ([https://jupyter.org/hub](https://jupyter.org/hub))
    on EKS, which can be used for model experimentation. Next, we will fine-tune the
    **Llama 3 model** within EKS and deploy it. Finally, we’ll set up a **RAG-powered
    chatbot** that will deliver personalized recommendations for an *e-commerce* company
    use case.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll cover the following key topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: GenAI use cases for e-commerce
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimentation using JupyterHub
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning Llama 3 in K8s
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the fine-tuned model on K8s
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a RAG application on K8s
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a chatbot on K8s
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using the following tools, some of which require
    you to set up an account and create an access token:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '**Hugging** **Face**: [https://huggingface.co/join](https://huggingface.co/join)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenAI**: [https://platform.openai.com/signup](https://platform.openai.com/signup)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **Llama 3 model**, which can be accessed via Hugging Face: [https://huggingface.co/meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **Amazon EKS cluster**, as illustrated in [*Chapter 3*](B31108_03.xhtml#_idTextAnchor039)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GenAI use cases for e-commerce
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 1*](B31108_01.xhtml#_idTextAnchor015), it is critical
    to think about *KPIs* and *business objectives* as we explore possible deployment
    options for **GenAI applications**. For an e-commerce platform, possible use cases
    could be chatbots to answer customer questions, personalized recommendations,
    content creation for product descriptions, and personalized marketing campaigns.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say that we have an e-commerce company called *MyRetail* for which we
    have been given the responsibility to explore and deploy GenAI use cases. The
    company has been growing rapidly and has a clear goal and strong differentiation:
    to provide its customers with a personalized, seamless shopping experience. To
    stay competitive, MyRetail aims to integrate cutting-edge AI technologies into
    its customer service while focusing on the following two features:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '*Creating personalized product recommendations* using a **RAG system**.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Providing automated responses* to inquiries related to the company’s loyalty
    program through a fine-tuned **GenAI model**.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MyRetail’s diverse customer base means that generic product recommendations
    and traditional FAQ systems are no longer sufficient. Customers expect personalized
    shopping experiences, and the company’s *MyElite loyalty program* needs to offer
    real-time, detailed information on rewards, points, and account status.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve these goals, MyRetail has decided to adopt the open source K8s orchestration
    platform and has selected Amazon EKS to deploy it in the cloud. They plan to build
    a chatbot application that utilizes two GenAI models: the first one will be a
    fine-tuned Llama 3 model trained on their MyElite loyalty program FAQ to answer
    user queries, while the second one will be a RAG application that supplements
    user queries with contextual shopping catalog data to enhance their shopping experience.
    The overall architecture of this solution is shown in *Figure 5**.1*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Chatbot architecture](img/B31108_05_1.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Chatbot architecture
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: However, before we implement this **chatbot** and **RAG system** in EKS, let’s
    create a **JupyterHub**-based playground for our data scientists to experiment
    with and optimize the GenAI models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Experimentation using JupyterHub
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Experimentation* plays a vital role in any GenAI project life cycle as it
    enables engineers and researchers to iterate, improve, and refine models while
    optimizing performance. Several tools are available for this; recall that we used
    **Anaconda** and **Google Colab** in [*Chapter 4*](B31108_04.xhtml#_idTextAnchor049).
    Primarily, these tools help us to experiment interactively with GenAI models,
    visualize, monitor, and integrate with popular AI/ML frameworks, integrate with
    cloud services, and collaborate with others. **Jupyter Notebook** ([https://jupyter.org/](https://jupyter.org/))
    has gained widespread adoption among data scientists and ML engineers due to its
    flexibility and easy-to-use web interface. This is evident from the average daily
    downloads of the notebook package (900K to 1 million downloads) ([https://pypistats.org/packages/notebook](https://pypistats.org/packages/notebook)),
    as indicated by the **Python Package Index** ([https://pypi.org/](https://pypi.org/)).
    Traditionally, we install these notebooks on local machines, but they often need
    specialized resources such as GPUs to perform meaningful analysis. To solve this
    issue, we will leverage a K8s cluster to spin up Jupyter notebooks as needed using
    JupyterHub.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '**JupyterHub** offers a centralized platform for running Jupyter notebooks,
    enabling users to access computational resources without requiring individual
    installations or maintenance. System administrators can manage user access effectively
    and tailor the environment with pre-configured tools and settings to meet user-specific
    preferences.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by learning how to install JupyterHub on an Amazon EKS cluster:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'First, deploy the `addons.tf` to install the CSI plugin, as well as added the
    necessary **IAM permissions**. The complete code for this is available on GitHub
    at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/addons.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/addons.tf):'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we need to create a default **StorageClass** ([https://kubernetes.io/docs/concepts/storage/storage-classes/](https://kubernetes.io/docs/concepts/storage/storage-classes/))
    for the EBS CSI driver that specifies attributes such as the reclaim policy, storage
    provisioner, and other parameters used in dynamic volume provisioning. Here, we
    are setting the newer gp3 as the default type for EBS CSI driver-created volumes.
    Refer to the Amazon EBS documentation at [https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html](https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html)
    to learn more about different volume types:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Run the following commands to deploy the EBS CSI driver to the EKS cluster:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can verify the installation status of the add-on by running the following
    command:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: resource "random_password" "jupyter_pwd" {
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: length = 16
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: special = true
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: override_special = "_%@"
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: resource "kubernetes_namespace" "jupyterhub" {
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: metadata {
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: name = "jupyterhub"
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: module "jupyterhub_single_user_irsa" {
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '...'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: role_name = "${module.eks.cluster_name}-jupyterhub-single-user-sa"
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: role_policy_arns = {
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: policy = "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '...'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: resource "kubernetes_service_account_v1" "jupyterhub_single_user_sa" {
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: metadata {
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: name = "${module.eks.cluster_name}-jupyterhub-single-user"
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'annotations = {"eks.amazonaws.com/role-arn": module.jupyterhub_single_user_irsa.iam_role_arn}'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '...'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: data "http" "jupyterhub_values" {
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: url = "https://kubernetes-for-genai-models.s3.amazonaws.com/chapter5/jupyterhub-values.yaml"
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: module "eks_data_addons" {
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: source = "aws-ia/eks-data-addons/aws"
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '...'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: enable_jupyterhub = true
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: jupyterhub_helm_config = {
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: values = [local.jupyterhub_values_rendered]
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '...'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: $ terraform init
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: $ terraform plan
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: $ terraform apply -auto-approve
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: $ helm list -n jupyterhub
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: NAME             NAMESPACE       REVISION          STATUS
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: eks.tf that contains GPU nodes (g6.2xlarge). Here, we are using EC2 Spot Instances
    pricing to minimize AWS charges; please refer to the documentation at https://aws.amazon.com/ec2/spot/pricing/
    for pricing details. We are also adding K8s taints (https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)
    to ensure that only GPU workloads are scheduled to these worker nodes. K8s taints
    are key-value pairs that are applied to nodes that prevent certain Pods from being
    scheduled on them unless the Pods tolerate the taints, allowing for better control
    over workload placement. By applying taints, we can ensure that non-GPU workloads
    are prevented from being scheduled on GPU nodes, reserving those nodes exclusively
    for GPU-optimized Pods. You can download the eks.tf file from https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/eks.tf.
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: $ terraform init
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: $ terraform plan
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: $ terraform apply -auto-approve
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: $ kubectl get nodes -l nvidia.com/gpu.present=true
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: NAME                                            STATUS
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: ip-10-0-17-1.us-west-2.compute.internal         Ready
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: $ kubectl port-forward svc/proxy-public 8000:80 -n jupyterhub
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: $ terraform output jupyter_pwd
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Figure 5.2 – JupyterHub login page](img/B31108_05_2.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – JupyterHub login page
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'After logging in, you will be presented with three notebook options. Since
    we are using JupyterHub for GenAI tasks that need GPU power, select **Data Science
    (GPU)** and click **Start**, as shown in *Figure 5**.3*:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.3 – JupyterHub home page](img/B31108_05_3.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – JupyterHub home page
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 5**.4*, this will start a new notebook instance running
    in a K8s Pod and also request a **Persistent Volume Claim** (**PVC**) ([https://kubernetes.io/docs/concepts/storage/persistent-volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes))
    so that the EBS CSI driver will create an Amazon EBS volume and attach it to the
    notebook. We’re using a persistent volume here to preserve the notebook’s state,
    data, and configurations across Pod restarts and terminations. This allows the
    notebook instance to be terminated after periods of inactivity and relaunched
    when the user returns, making the process more cost-efficient:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Figure 5.4 – Our Jupyter notebook](img/B31108_05_4.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Our Jupyter notebook
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now import the following notebooks from [*Chapter 4*](B31108_04.xhtml#_idTextAnchor049)
    and execute the necessary commands to test both the **RAG** and **fine-tuning**
    examples:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**RAG** **notebook**: [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_RAG_Example.ipynb](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_RAG_Example.ipynb)'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tuning** **notebook**: [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_FineTuning_Example.ipynb](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_FineTuning_Example.ipynb)'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With that, we have set up JupyterHub on the EKS cluster and used it to launch
    a Jupyter notebook to test our GenAI experimentation scripts. Next, we will containerize
    both the fine-tuning and RAG scripts and run them as K8s Pods on the EKS cluster.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning Llama 3 in K8s
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in [*Chapter 3*](B31108_03.xhtml#_idTextAnchor039), running fine-tuning
    workloads on K8s has several advantages, including scalability, efficient resource
    utilization, portability, and monitoring. In this section, we will containerize
    the **Llama 3 fine-tuning job** that we experimented with in the Jupyter notebook
    and deploy it on the EKS cluster. This is essential for automating the end-to-end
    AI/ML pipelines and versioning the models.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps are involved in fine-tuning a Llama 3 model:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '*Gather training and evaluation datasets* and store them in **Amazon S3**.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Create a container image* and upload it to **Amazon ECR**.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Deploy the fine-tuning job* in the **EKS cluster**.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data preparation
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will utilize two datasets (training and evaluation) to fine-tune and validate
    a `kubernetes-for-genai-models`. **Amazon S3** ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/))
    is an object storage service that’s used to store and retrieve any amount of data
    from anywhere, making it the ideal choice for sharing large datasets for collaboration:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this section, we explored the datasets that will be used to fine-tune the
    Llama 3 model for our e-commerce use case. We also covered best practices such
    as storing these databases in external storage services such as Amazon S3 rather
    than packaging them in the container image. In the next section, we will focus
    on creating a container image for the fine-tuning job.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Creating a container image
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create a container image for the fine-tuning job, we need a **Dockerfile**,
    a **fine-tuning script**, and a **dependency list**. We’ve already created these
    artifacts and made them available on GitHub: [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/llama-finetuning](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/llama-finetuning).
    Let’s start building the container so that we can fine-tune the Llama 3 model
    with these artifacts:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a directory named `llama-finetuning`:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '...'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: train_dataset_file = os.environ.get('TRAIN_DATASET_FILE')
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: eval_dataset_file = os.environ.get('EVAL_DATASET_FILE')
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: train_dataset = load_dataset('json', data_files=train_dataset_file, split='train')
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: eval_dataset = load_dataset('json', data_files=eval_dataset_file, split='train')
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After training, we need to save the model weights, configuration files, and
    tokenizer configuration so that they can be used to create an inference container
    later:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Export the model weights and configuration files to an S3 bucket:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Create a Dockerfile where we can build the fine-tuning container image. It
    should start with the **nvidia/cuda** ([https://hub.docker.com/r/nvidia/cuda](https://hub.docker.com/r/nvidia/cuda))
    parent image, install the required Python dependencies, and contain the fine-tuning
    script. The complete file is available at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/Dockerfile](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/Dockerfile):'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Create the container image by running the following command:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You can verify the container image by using the following docker command:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With that, we’ve successfully built the container image so that we can fine-tune
    the Llama 3 model. Next, we will upload this to an Amazon ECR repository and deploy
    it to the EKS cluster.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the fine-tuning job
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To deploy the fine-tuning job, we need to save the container image in **Amazon
    ECR** and create an **S3 bucket** where we’ll save model assets, as well as create
    a **K8s job** in the **EKS cluster**:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an `ecr.tf` file in the `genai-eks-demo` directory. The complete code
    is available on GitHub at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf):'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Create an Amazon S3 bucket so that you can store the fine-tuned model assets.
    We’ll create one using Terraform. Download the `model-assets.tf` file from [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/model-assets.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/model-assets.tf):'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We also need to create a `eks.tf` file you downloaded earlier:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Run the following commands to create the **ECR repository** and **S3 bucket**.
    The S3 bucket’s name will be printed in the output:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: $ terraform output -raw my_llama_finetuned_ecr_push_cmds
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: aws ecr get-login-password --region us-west-2 | docker login --username AWS
    --password-stdin 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: docker tag my-llama-finetuned 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: docker push 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now that we’ve created the required infrastructure, let’s go ahead and deploy
    the fine-tuning job to the **EKS cluster**. To do so, we need to create a **K8s
    Job manifest file** that will run this as a K8s Job. Download the manifest from
    GitHub at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/llama-finetuning-job.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/llama-finetuning-job.yaml)
    and replace the image, the Hugging Face token, and the name of the S3 bucket that
    contains the model assets you created previously:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Run the following commands to run the job on the EKS cluster:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'A K8s Pod will be scheduled on a GPU node and start the fine-tuning process.
    You can monitor its process by tailing its logs:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once fine-tuning is complete, the job will automatically upload the model assets
    to the S3 bucket, as shown in *Figure 5**.5*:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.5 – An Amazon S3 bucket that contains fine-tuned model assets](img/B31108_05_5.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – An Amazon S3 bucket that contains fine-tuned model assets
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we began by modifying our Llama 3 fine-tuning script to make
    it container-ready. Then, we created a container image that includes the script
    and its dependent libraries, using the `nvidia/cuda` image from **DockerHub**
    as the base. Finally, we created an Amazon ECR repository that will store the
    container image and deploy it as a K8s job in the EKS cluster. In the next section,
    we will utilize the fine-tuned model assets to create an inference container image
    and deploy it in the EKS cluster.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先修改了我们的Llama 3精细调整脚本，使其可以用容器方式运行。然后，我们创建了一个包含该脚本及其依赖库的容器镜像，使用**DockerHub**上的`nvidia/cuda`镜像作为基础。最后，我们创建了一个Amazon
    ECR存储容器镜像的存储库，并将其作为K8s作业部署到EKS集群中。在接下来的部分，我们将利用经过精细调整的模型资产创建推理容器镜像，并在EKS集群中部署它。
- en: Deploying the fine-tuned model on K8s
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在K8s上部署经过精细调整的模型
- en: In this section, we will containerize the fine-tuned **Llama 3 model** using
    **Python FastAPI** and deploy the inference endpoint as a K8s deployment in the
    EKS cluster.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用**Python FastAPI**将经过精细调整的**Llama 3模型**容器化，并部署为EKS集群中的K8s部署。
- en: 'Let’s start by creating the inference container using the fine-tuned model
    assets stored in the S3 bucket. We will be using Python FastAPI ([https://fastapi.tiangolo.com/](https://fastapi.tiangolo.com/))
    to expose the model as an **HTTP API**. FastAPI is a modern high-performance web
    framework for building APIs in Python:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从S3桶中的存储的精细调整模型资产开始创建推理容器。我们将使用Python FastAPI ([https://fastapi.tiangolo.com/](https://fastapi.tiangolo.com/))将模型暴露为**HTTP
    API**。FastAPI是一个现代高性能的Python构建API的Web框架：
- en: 'Create a directory called `llama-finetuned-inf`:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建名为`llama-finetuned-inf`的目录：
- en: '[PRE31]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Download the model assets from the S3 bucket to the local directory so that
    we can copy them to the container image:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从S3桶下载模型资产到本地目录，以便我们可以将它们复制到容器镜像中：
- en: '[PRE32]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: FROM nvidia/cuda:12.8.1-runtime-ubuntu24.04
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FROM nvidia/cuda:12.8.1-runtime-ubuntu24.04
- en: '...'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: RUN pip install torch transformers peft accelerate bitsandbytes sentencepiece
    fastapi uvicorn
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RUN pip install torch transformers peft accelerate bitsandbytes sentencepiece
    fastapi uvicorn
- en: COPY model-assets /app/model-assets
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: COPY model-assets /app/model-assets
- en: COPY main.py /app/main.py
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: COPY main.py /app/main.py
- en: CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "80"]
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "80"]
- en: '[PRE33]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we will wrap the fine-tuned Llama 3 model into an API using `/generate`
    that accepts **HTTP POST** requests and returns a response after invoking the
    model. You can download the complete code from [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/main.py](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/main.py):'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`/generate`将精细调整的Llama 3模型包装成一个接受**HTTP POST**请求并在调用模型后返回响应的API。您可以从[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/main.py](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/main.py)下载完整的代码：
- en: '[PRE34]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Create the container image by running the following commands:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令创建容器镜像：
- en: '[PRE35]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We will be reusing the **ECR repository** for this image. Alternatively, you
    can create a new repository using Terraform, as described in [*Chapter 3*](B31108_03.xhtml#_idTextAnchor039).
    Replace the **account number** and **region** values in the following commands
    before running them:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重用**ECR存储库**用于此镜像。或者，您可以根据Terraform创建一个新的存储库，如[*第3章*](B31108_03.xhtml#_idTextAnchor039)所述。在运行这些命令之前，请替换以下命令中的**帐户号码**和**区域**值：
- en: '[PRE36]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'apiVersion: apps/v1'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'apiVersion: apps/v1'
- en: 'kind: Deployment'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'kind: Deployment'
- en: 'metadata:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'metadata:'
- en: 'name: my-llama-finetuned-deployment'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'name: my-llama-finetuned-deployment'
- en: 'spec:'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'spec:'
- en: '...'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: 'containers:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'containers:'
- en: '- name: llama-finetuned-container'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- name: llama-finetuned-container'
- en: 'image: <<Replace your ECR image here>>'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'image: <<替换您的ECR镜像位置>>'
- en: '...'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: 'env:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'env:'
- en: '- name: HUGGING_FACE_HUB_TOKEN'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- name: HUGGING_FACE_HUB_TOKEN'
- en: 'value: "<<Replace your Hugging face token here>>"'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'value: "<<替换您的Hugging Face令牌位置>>"'
- en: '...'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: '[PRE37]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Finally, deploy the model by running the following commands:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过运行以下命令部署模型：
- en: '[PRE38]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'It may take a few minutes for the K8s Pod to be ready since the image needs
    to be downloaded from ECR. Run the following command to verify its status:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于需要从ECR下载镜像，K8s Pod可能需要几分钟准备就绪。运行以下命令来验证其状态：
- en: '[PRE39]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In this section, we took the model assets that were generated via the fine-tuning
    process and wrapped them with Python FastAPI to create an HTTP API. Then, we created
    a container, deployed it to the EKS cluster, and exposed it via the K8s ClusterIP
    service. This demonstrates how you can customize the behavior of general-purpose
    LLMs with your dataset and serve them using K8s. In the next section, we will
    explore how to deploy a RAG application on K8s.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们采用了通过微调过程生成的模型资产，并使用 Python FastAPI 将其包装成 HTTP API。接着，我们创建了一个容器，将其部署到
    EKS 集群，并通过 K8s ClusterIP 服务暴露出来。这演示了如何使用您的数据集自定义通用 LLM 的行为，并通过 K8s 服务它们。在下一节中，我们将探索如何在
    K8s 上部署 RAG 应用程序。
- en: Deploy a RAG application on K8s
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 K8s 上部署 RAG 应用程序。
- en: 'As explained in [*Chapter 4*](B31108_04.xhtml#_idTextAnchor049), RAG allows
    us to integrate external knowledge sources into the **LLM response generation
    process**, leading to more accurate and contextually relevant responses. By providing
    up-to-date and relevant information in the input, RAG also reduces errors such
    as hallucinations ([https://www.ibm.com/topics/ai-hallucinations](https://www.ibm.com/topics/ai-hallucinations)).
    In this section, we will explore how to deploy a RAG application to a K8s cluster.
    The following high-level steps are involved:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [*第 4 章*](B31108_04.xhtml#_idTextAnchor049) 中所述，RAG 使我们能够将外部知识源集成到 **LLM 响应生成过程**
    中，从而生成更准确、更具上下文相关性的响应。通过在输入中提供最新且相关的信息，RAG 还减少了诸如幻觉现象的错误 ([https://www.ibm.com/topics/ai-hallucinations](https://www.ibm.com/topics/ai-hallucinations))。在本节中，我们将探索如何将
    RAG 应用程序部署到 K8s 集群。以下是涉及的高层步骤：
- en: Set up a vector database.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个向量数据库。
- en: Create a RAG application to query the vector store and call the LLM with both
    the input and contextual data.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 RAG 应用程序，用于查询向量存储，并通过输入和上下文数据调用 LLM。
- en: Deploy the RAG application on K8s.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 K8s 上部署 RAG 应用程序。
- en: Load and index the data in the vector store.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在向量存储中加载和索引数据。
- en: 'We’ll begin by setting up a **vector database**, which is a specialized datastore
    that’s used for storing and querying high-dimensional vector embeddings. In RAG,
    such databases play an essential role by allowing a similarity search to be performed
    on the input request and relevant information to be retrieved. There are many
    open source and commercial vector databases available, such as **Pinecone** ([https://www.pinecone.io/](https://www.pinecone.io/)),
    **Qdrant** ([https://qdrant.tech/](https://qdrant.tech/)), **Chroma** ([https://www.trychroma.com/](https://www.trychroma.com/)),
    and **OpenSearch** ([https://opensearch.org/](https://opensearch.org/)). Since
    many of these offerings are available as managed or SaaS models, one question
    naturally arises: when should we opt to run a self-hosted vector database in K8s?
    The main factors we should consider are as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先设置一个 **向量数据库**，它是一个专门用于存储和查询高维向量嵌入的数据存储。在 RAG 中，这些数据库通过允许对输入请求进行相似性搜索并检索相关信息，起着至关重要的作用。市面上有很多开源和商业向量数据库，例如
    **Pinecone** ([https://www.pinecone.io/](https://www.pinecone.io/))、**Qdrant**
    ([https://qdrant.tech/](https://qdrant.tech/))、**Chroma** ([https://www.trychroma.com/](https://www.trychroma.com/))
    和 **OpenSearch** ([https://opensearch.org/](https://opensearch.org/))。由于这些产品很多都可以作为托管或
    SaaS 模型使用，因此一个自然产生的问题是：我们应该什么时候选择在 K8s 中运行自托管的向量数据库？我们需要考虑的主要因素如下：
- en: Performance and latency
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能和延迟
- en: Data sovereignty and compliance
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据主权和合规性
- en: How customizable the configuration is
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置的可定制性
- en: Cost control
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本控制
- en: How to avoid vendor lock-in
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何避免供应商锁定
- en: 'Follow these steps:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤进行：
- en: For our setup, we are using a Qdrant vector database to store MyRetail’s sales
    catalog. Let’s download the `qdrant.tf` file from our GitHub repository at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/qdrant.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/qdrant.tf)
    and run `terraform apply` command. This will install the Qdrant vector database
    as a K8s StatefulSet ([https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/))
    on the EKS cluster in the `qdrant` namespace using a publicly available Helm chart.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于我们的设置，我们使用 Qdrant 向量数据库来存储 MyRetail 的销售目录。让我们从我们的 GitHub 仓库下载`qdrant.tf`文件，地址是
    [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/qdrant.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/qdrant.tf)，并运行
    `terraform apply` 命令。这将安装 Qdrant 向量数据库作为 K8s StatefulSet ([https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/))，并在
    EKS 集群的 `qdrant` 命名空间中使用公开的 Helm chart 进行部署。
- en: '[PRE40]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create an `ecr.tf` file or you can download the complete code from GitHub at
    [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf):'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Run the following commands to deploy the Qdrant `kubectl` command. The following
    output shows that a `qdrant` vector database Pod has been deployed and is `Running`:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Optionally, you can connect to Qdrant’s Web UI to interact with the vector
    database. Run the following command to connect to the Qdrant Web UI locally. Please
    refer to the Qdrant documentation at [https://qdrant.tech/documentation/interfaces/web-ui/](https://qdrant.tech/documentation/interfaces/web-ui/)
    to learn about its various features:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '@app.post("/load_data")'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'async def load_data(request: LoadDataModel):'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '...'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: response = requests.get(request.url)
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: reader = csv.DictReader(file_content)
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '...'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: qdrant_store = QdrantVectorStore(
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: embedding=OpenAIEmbeddings(),
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: collection_name=collection_name,
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: client=qdrant_client
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: qdrant_store.add_documents(docs)
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Exposes a Python FastAPI endpoint called `/generate` that accepts a user prompt
    and an **optional session ID**. It creates the embedding for the input prompt
    using **OpenAIEmbeddings** and performs a similarity search against the Qdrant
    vector database to retrieve the relevant context information:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Creates a conversational RAG application that remembers a user’s prior questions
    and answers and applies logic that can be incorporated into the current request.
    Here, we are building `rag_chain` using `history_aware_retriever` ([https://python.langchain.com/api_reference/langchain/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html))
    from the **LangChain** package ([https://python.langchain.com/docs/introduction/](https://python.langchain.com/docs/introduction/)).
    It takes an input prompt and past chat history and calls an LLM (OpenAI’s **GPT-3.5
    Turbo**) to fetch the contextualized input. LangChain is a framework that’s designed
    to build applications powered by LLMs, enabling easy integration, management,
    and orchestration of multiple models and data pipelines for tasks such as creating
    chatbots, generating text, and more:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Invokes the RAG chain with the input and returns the response, along with the
    session ID, in **JSON format**:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '*Figure 5**.6* illustrates the complete RAG application flow. The process begins
    with contextualizing the latest user prompt using an LLM, which reformulates the
    query based on the chat history. Then, the retriever component takes the rephrased
    query to gather relevant context from the conversation. Finally, `question_answer_chain`
    combines the retrieved context, the chat history, and the current user input to
    generate the final answer:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – RAG application flow](img/B31108_05_6.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – RAG application flow
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a directory named `rag-app`:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Download the `main.py` and `requirements.txt` files with our RAG application
    code and dependent libraries from [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/rag-app](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/rag-app)
    and place them in `rag-app` directory.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next step is to create a Dockerfile for this RAG application. We will start
    with a Python parent image, install the necessary FastAPI dependencies, add the
    Python application code, and run the **uvicorn** ([https://www.uvicorn.org/](https://www.uvicorn.org/))
    command to start a web server for the FastAPI application. The complete Dockerfile
    is available at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/Dockerfile](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/Dockerfile):'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Build the container image and push it to the ECR repository. Replace the **account
    number** and **region** values before running them:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'apiVersion: apps/v1'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'kind: Deployment'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'metadata:'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'name: rag-app-deployment'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '...'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'containers:'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- name: rag-app-container'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'image: <<Replace your ECR image here>>'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '...'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'env:'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- name: OPENAI_API_KEY'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'value: "<<Replace your OpenAI API Key here>>"'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '...'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'After updating the manifest, you can run the following commands to deploy the
    RAG application to the cluster. This will create a K8s deployment called `rag-app-deploment`
    with one replica that’s exposed via a ClusterIP service on port `80`. You can
    validate this by running the following command:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: $ kubectl apply -f qdrant-restore-job.yaml
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: batch/job qdrant-restore-job created
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: In this section, we created a conversational RAG application using the LangChain
    framework, a Qdrant vector database, and OpenAI GenAI models. We containerized
    the application by exposing it as a Python FastAPI and deployed it to the EKS
    cluster. In addition, we created another example RAG application that utilizes
    **Amazon Bedrock** ([https://aws.amazon.com/bedrock/](https://aws.amazon.com/bedrock/))
    and **Anthropic Claude** ([https://aws.amazon.com/bedrock/claude/](https://aws.amazon.com/bedrock/claude/))
    models instead of OpenAI. You can find it at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/bedrock-rag-app](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/bedrock-rag-app)
    and choose either one based on your preference. In the next section, we’ll tie
    everything together using a Chatbot UI.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a chatbot on K8s
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve deployed a fine-tuned Llama 3 model that has been trained on the
    MyElite loyalty program’s FAQ and a conversational RAG application using sales
    catalog data. Now, we will build a **Chatbot UI component** that will expose both
    services to MyRetail customers.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: We will be building the Chatbot UI using **Gradio** ([https://www.gradio.app/](https://www.gradio.app/)),
    an open source Python package used to build demos or web applications for ML models,
    APIs, and more. You can refer to the QuickStart guide at [https://www.gradio.app/guides/quickstart](https://www.gradio.app/guides/quickstart)
    to learn more about the Gradio framework. Alternatively, you can explore using
    UI frameworks such as **Streamlit** ([https://streamlit.io/](https://streamlit.io/)),
    **NiceGUI** ([https://nicegui.io/](https://nicegui.io/)), **Dash** ([https://github.com/plotly/dash](https://github.com/plotly/dash)),
    and **Flask** ([https://flask.palletsprojects.com/en/stable/](https://flask.palletsprojects.com/en/stable/))
    to build chatbot interfaces.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'For your convenience, we’ve already created a chatbot container and made it
    available publicly on DockerHub: [https://hub.docker.com/repository/docker/k8s4genai/chatbot-ui/](https://hub.docker.com/repository/docker/k8s4genai/chatbot-ui/).
    The source code for this application is available on GitHub at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/chatbot](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/chatbot)
    for your reference.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: We are creating a public-facing NLB for testing purposes. Feel free to restrict
    access to your IP address by updating the inbound rules of the NLB security group.
    Refer to the AWS NLB documentation at [https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-security-groups.html](https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-security-groups.html)
    for more details.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s deploy the chatbot application on the EKS cluster and configure it with
    both the fine-tuning Llama 3 deployment and the RAG application:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the Chatbot UI K8s deployment manifest from [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/chatbot/chatbot-deploy.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/chatbot/chatbot-deploy.yaml)
    and deploy it to the EKS cluster by running the following commands. This will
    create a K8s deployment with one replica of the Chatbot UI application and expose
    it to the public internet via **AWS Network** **Load Balancer**:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Fetch the AWS Network Load Balancer endpoint by running the following command:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Open the Chatbot UI by launching the Load Balancer URL in a web browser, as
    shown in *Figure 5**.7*. Now, you can interact with the chatbot by selecting the
    **Shopping** assistant (RAG application) or the **Loyalty Program** assistant
    (Llama 3 fine-tuned model) and typing a question in the chatbox – for example,
    *Please suggest walking shoes for a 60 year male in* *tabular format*:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Chatbot UI](img/B31108_05_7.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – Chatbot UI
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the results are displayed, go ahead and ask a follow-up question – for
    example, *Can you sort the results in descending order by price*. As shown in
    *Figure 5**.8*, you will see that the results are sorted accordingly. Since the
    shopping assistant was developed with conversational RAG, it considers the user’s
    prior conversational history while answering the current prompt. In this example,
    we asked the chatbot to *Please suggest walking shoes for a 60 year male in tabular
    format*, followed by *Can you sort the results in descending order by price*.
    For the second query, the RAG application considered the user’s prior conversations
    and returned the sorted results:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Chatbot UI results](img/B31108_05_8.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Chatbot UI results
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also toggle between the **Shopping** assistant and the **Loyalty Program**
    assistant by using the options under **Choose an assistant** and ask questions
    related to the loyalty program, as shown in *Figure 5**.9*:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Choosing the Loyalty Program assistant](img/B31108_05_9.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Choosing the Loyalty Program assistant
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we deployed a Chatbot UI application that had been developed
    with the Gradio Python package to an EKS cluster and exposed it via the K8s LoadBalancer
    service. This application is connected to both the RAG application and the fine-tuned
    Llama 3 model we developed in this chapter so that it can answer user queries
    about MyRetail’s MyElite loyalty program and shopping catalog.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered how to fine-tune and deploy GenAI models in a K8s
    environment using Amazon EKS. We used a fictional company, MyRetail, as an example
    to highlight GenAI applications in e-commerce/retail business by creating personalized
    shopping experiences for our customers using GenAI models. This allowed us to
    automate responses for the company’s loyalty program and offer product recommendations.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: We began by discussing the importance of experimentation in the overall GenAI
    project life cycle and deployed JupyterHub in EKS. JupyterHub enables centralized
    access to computational resources such as GPUs, making it more suitable for large-scale
    AI tasks. Then, we created a Llama 3 fine-tuning container image and deployed
    it to the EKS cluster. The fine-tuning job utilized training and validation datasets
    from Amazon S3 to fine-tune the Llama 3 model and exported the model assets to
    S3\. We containerized the inference container using those model assets and deployed
    it to the EKS cluster as a K8s deployment.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also outlined how to deploy a RAG application that queries a vector
    database (Qdrant) to retrieve context-relevant information before calling the
    LLM to generate responses. This reduces hallucinations and improves response accuracy
    by incorporating external data. Finally, we deployed a Chatbot UI and connected
    it to both the fine-tuned Llama 3 model and the RAG application to enhance the
    shopping experience for MyRetail customers. In the next chapter, we will explore
    various autoscaling constructs provided by K8s and how we can leverage them to
    optimize GenAI workloads.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
