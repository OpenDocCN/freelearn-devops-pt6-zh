<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer028">
			<h1 id="_idParaDest-28" class="chapter-number"><a id="_idTextAnchor027"/>2</h1>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>Kubernetes – Introduction and Integration with GenAI</h1>
			<p>Deploying and managing GenAI workloads at scale presents significant challenges, including building the models, packaging them for distribution, and ensuring effective deployment and scaling. In this chapter, we will discuss the concepts of containers and <strong class="bold">Kubernetes</strong> (<strong class="bold">K8s</strong>) and why <a id="_idIndexMarker094"/>they are emerging as powerful solutions to address these complexities. They are becoming the de facto standard for companies <a id="_idIndexMarker095"/>such as OpenAI (<a href="https://openai.com/index/scaling-kubernetes-to-7500-nodes/">https://openai.com/index/scaling-kubernetes-to-7500-nodes/</a>) and Anthropic (<a href="https://youtu.be/c9NJ6GSeNDM?si=xjei4T9VfZvejD5o&amp;t=2412">https://youtu.be/c9NJ6GSeNDM?si=xjei4T9VfZvejD5o&amp;t=2412</a>) to deploy GenAI workloads. We will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li><span class="No-Break">Understanding containers</span></li>
				<li>Why containers for <span class="No-Break">GenAI models</span></li>
				<li>What is <span class="No-Break">Kubernetes (K8s)?</span></li>
			</ul>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Understanding containers</h1>
			<p>Containers have revolutionized the way we manage applications by standardizing the packaging format. With the portability of containers, applications are packaged as a standard unit <a id="_idIndexMarker096"/>of software that packages all of your code and dependencies to deploy consistently and reliably across various environments, such as on-premises, public, and <a id="_idIndexMarker097"/>private clouds. Containers are also considered an evolution of <strong class="bold">Virtual Machine</strong> (<strong class="bold">VM</strong>) technology, where multiple containers are run on the same operating system, sharing the underlying kernel to increase the <a id="_idIndexMarker098"/>overall server utilization. This is a massive advantage of containers as there is no overhead of multiple <strong class="bold">Operating Systems</strong> (<strong class="bold">OSes</strong>) and other OS-level components. So, containers can be started and stopped a lot faster while <span class="No-Break">providing isolation.</span></p>
			<p>The following figure illustrates the evolution of computing environments, highlighting a shift toward higher levels of abstraction and an increased focus on <span class="No-Break">business logic.</span></p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B31108_02_01.jpg" alt="Figure 2.1 – Evolution of container technology" width="1386" height="720"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Evolution of container technology</p>
			<p>Physical servers offer the least abstraction, with extensive manual configuration, and resource inefficiencies. VMs provide a middle ground, by abstracting underlying hardware resources <a id="_idIndexMarker099"/>using hypervisor technology. This allows you to run multiple VMs on the same physical server, increasing the resource utilization and security. However, they are bulky and slow to boot up. Containers provide the highest level of abstraction by encapsulating applications and dependencies in portable units, allowing us to focus more on developing and optimizing business logic rather than managing infrastructure. <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.2</em> illustrates the high-level differences between VMs <span class="No-Break">and containers.</span></p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B31108_02_02.jpg" alt="Figure 2.2 – Virtual machines versus containers" width="1210" height="431"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Virtual machines versus containers</p>
			<p>Container technology is made possible because of namespaces and cgroups (control groups) in the <a id="_idIndexMarker100"/>Linux kernel. They form the foundational building blocks to <a id="_idIndexMarker101"/>provide isolation and resource limits. A <strong class="bold">Linux Namespace</strong> (<a href="https://man7.org/linux/man-pages/man7/namespaces.7.html">https://man7.org/linux/man-pages/man7/namespaces.7.html</a>) is an abstraction over resources in the operating system. It partitions OS-level resources such that different sets of processes see a different set of resources (network, file system, etc.,) even though <a id="_idIndexMarker102"/>they are running on the same OS kernel. <strong class="bold">Cgroups</strong> (<a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">https://man7.org/linux/man-pages/man7/cgroups.7.html</a>) govern the isolation and usage of system resources, such as CPU, memory, and network, for a group of processes and optionally enforce limits and constraints. These capabilities let containers abstract the operating system components for <span class="No-Break">modern applications.</span></p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Container terminology</h2>
			<p>The following <a id="_idIndexMarker103"/>are some terms associated with containers that will be crucial in following along with <span class="No-Break">this book:</span></p>
			<ul>
				<li><strong class="bold">Container runtime</strong>: This is a host-level process that is responsible for creating, stopping, and <a id="_idIndexMarker104"/>starting containers. It interacts <a id="_idIndexMarker105"/>with low-level container runtimes such as runc to set up namespaces and cgroups for containers. Popular examples include containerd, CRI-O, and <span class="No-Break">so on.</span></li>
				<li><strong class="bold">Container image</strong>: This is <a id="_idIndexMarker106"/>a lightweight, standalone, executable <a id="_idIndexMarker107"/>package that includes everything needed to run a piece of software, including the code, runtime, libraries, environment variables, and configuration files. It is created using a Dockerfile, a plain text definition file that includes a set of instructions to install <a id="_idIndexMarker108"/>dependencies, applications, and so on. Typical characteristics of a container image include <span class="No-Break">the following:</span><ul><li><strong class="bold">Self-contained</strong>: It encapsulates everything needed to run <span class="No-Break">software applications.</span></li><li><strong class="bold">Immutable</strong>: It is read-only in nature; any changes would require a <span class="No-Break">new image.</span></li><li><strong class="bold">Layered</strong>: Images are built in layers, each layer representing a file system. This is what makes images highly efficient as common layers can be shared across <span class="No-Break">multiple images.</span></li><li><strong class="bold">Portable</strong>: As the image packages the application and all its dependencies, they can be run on any system that supports a container runtime, making them highly portable <span class="No-Break">in nature.</span></li></ul></li>
				<li><strong class="bold">Container registry</strong>: This is <a id="_idIndexMarker109"/>a tool used to manage and <a id="_idIndexMarker110"/>distribute container images. Popular <a id="_idIndexMarker111"/>registries include Docker Hub (<a href="https://hub.docker.com/">https://hub.docker.com/</a>), Amazon <a id="_idIndexMarker112"/>Elastic Container Registry (<a href="https://aws.amazon.com/ecr/">https://aws.amazon.com/ecr/</a>), Google <a id="_idIndexMarker113"/>Artifact Registry (<a href="https://cloud.google.com/artifact-registry">https://cloud.google.com/artifact-registry</a>), and so on. Tools such as Artifactory and Harbor can be used to self-host <span class="No-Break">a registry.</span></li>
				<li><strong class="bold">Container</strong>: This is a <a id="_idIndexMarker114"/>running instance or process created from the <a id="_idIndexMarker115"/>container image by the <span class="No-Break">container runtime.</span></li>
			</ul>
			<p>Let’s now understand the high-level container workflow using Docker, a software platform designed to help developers build, share, and run container applications. Docker follows traditional client-server architecture. When you install Docker on a host, it runs a server component called the <a id="_idIndexMarker116"/>Docker daemon and a client component – the Docker CLI. The <strong class="bold">Daemon</strong> is responsible for creating and managing images, running containers using those images, and setting up networking, storage, and so on. As depicted in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.3</em>, the Docker CLI is used to interact with the Docker daemon process to build and run containers. Once the images are built, the Docker daemon <a id="_idIndexMarker117"/>can push and pull those images to/from a container registry. Storing the images in a container registry allows them to be portable and they can be run anywhere a container runtime <span class="No-Break">is available.</span></p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B31108_02_03.jpg" alt="Figure 2.3 – Docker architecture overview" width="1650" height="645"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Docker architecture overview</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Creating a container image</h2>
			<p>Let’s create <a id="_idIndexMarker118"/>our first hello world container image and run it locally. Use the Docker documentation page at <a href="https://docs.docker.com/engine/install/">https://docs.docker.com/engine/install/</a> to install Docker Engine on your machine. The following are the links to install Docker Desktop for different <span class="No-Break">operating systems:</span></p>
			<ul>
				<li>Docker <a id="_idIndexMarker119"/>Desktop for <span class="No-Break">Linux</span><span class="No-Break">: </span><a href="https://docs.docker.com/desktop/install/linux-install/"><span class="No-Break">https://docs.docker.com/desktop/install/linux-install/</span></a></li>
				<li>Docker <a id="_idIndexMarker120"/>Desktop for Mac (<span class="No-Break">macOS): </span><a href="https://docs.docker.com/desktop/install/mac-install/"><span class="No-Break">https://docs.docker.com/desktop/install/mac-install/</span></a></li>
				<li>Docker <a id="_idIndexMarker121"/>Desktop for <span class="No-Break">Windows: </span><a href="https://docs.docker.com/desktop/install/windows-install/"><span class="No-Break">https://docs.docker.com/desktop/install/windows-install/</span></a></li>
			</ul>
			<p>In the following Dockerfile, we are creating a simple hello world application using the <strong class="source-inline">nginx</strong> server. We will use <strong class="source-inline">nginx</strong> as the parent image and customize the <strong class="source-inline">index.html</strong> with our <em class="italic">Hello World!</em> message. Let’s start by following <span class="No-Break">these steps:</span></p>
			<ol>
				<li>Create a Dockerfile with the <span class="No-Break">following content:</span><pre class="source-code">
FROM nginx
RUN echo "Hello World!" &gt; /usr/share/nginx/html/index.html</pre></li>				<li>Build a container image with a <span class="No-Break"><strong class="source-inline">v1</strong></span><span class="No-Break"> tag:</span><pre class="source-code">
$ docker build -t hello-world:v1 .</pre><p class="list-inset">You can <a id="_idIndexMarker122"/>list the local container images using the <span class="No-Break">following command:</span></p><pre class="source-code">$ docker images
REPOSITORY     TAG       IMAGE ID       CREATED       SIZE
hello-world    v1        7a5469eb898f   2 mins ago    273MB</pre></li>				<li>Run a container using the <strong class="source-inline">hello-world</strong> image and bind nginx port <strong class="source-inline">80</strong> to the <strong class="source-inline">8080</strong> <span class="No-Break">host port:</span><pre class="source-code">
$ docker run -p 8080:80 hello-world:v1</pre></li>				<li>Now that we have the image running, we can test the container by calling <span class="No-Break">localhost </span><span class="No-Break"><strong class="source-inline">8080</strong></span><span class="No-Break">:</span><pre class="source-code">
$ curl http://localhost:8080/
Hello World!</pre></li>			</ol>
			<p>As an optional step, you can also push the container image to an <strong class="source-inline">xyz</strong> container registry so that you can run it anywhere. Replace <strong class="source-inline">xyz</strong> with your container repository name. For example, follow the instructions at <a href="https://www.docker.com/blog/how-to-use-your-own-registry-2/">https://www.docker.com/blog/how-to-use-your-own-registry-2/</a> to create the registry in <span class="No-Break">Docker Hub:</span></p>
			<pre class="console">
$ docker tag hello-world:v1 xyz/hello-world:v1
$ docker push xyz/hello-world:v1</pre>			<p>In this section, we learned about the evolution of computing environments, the benefits of using containers over traditional physical servers, and VMs. We gained an understanding <a id="_idIndexMarker123"/>of the overall Docker architecture and various key container terminology and built and ran our first hello-world container application. Let’s explore why containers are a great fit for <span class="No-Break">GenAI models.</span></p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>Why containers for GenAI models?</h2>
			<p>A typical challenge with developing ML or GenAI applications involves using complex and continuously <a id="_idIndexMarker124"/>evolving open source ML frameworks such as PyTorch and TensorFlow, ML tool kits such as Hugging Face Transformers, ever-changing GPU hardware ecosystems from NVIDIA, and custom accelerators from Amazon, Google, and <span class="No-Break">so on.</span></p>
			<p>The following figure illustrates various components involved in creating and running an ML or <span class="No-Break">GenAI container.</span></p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B31108_02_04.jpg" alt="Figure 2.4 – A typical GenAI container image" width="645" height="946"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – A typical GenAI container image</p>
			<p>At the top layers, the container encapsulates various software libraries, deep learning frameworks, and user-supplied code. The next set of layers contains hardware-layer-specific libraries <a id="_idIndexMarker125"/>to interact with GPUs or custom accelerators on the host. The container runtime can be used to launch the containers from the container image. Let’s dive into the significant benefits of using containers for <span class="No-Break">GenAI workloads:</span></p>
			<ul>
				<li><strong class="bold">Dependency management</strong>: This could become crucial due to evolving frameworks <a id="_idIndexMarker126"/>and interdependencies on specific versions. With containers, we can encapsulate the GenAI application code and its dependencies in a container image and use it on a developer machine or in the test/production <span class="No-Break">environment consistently.</span></li>
				<li><strong class="bold">Resource access</strong>: GenAI/ML apps are computationally intensive, need access to single <a id="_idIndexMarker127"/>or multiple GPUs or custom accelerators, and adjust resource allocations dynamically based on the workload demand. Containers allow for fine-grained control over resource allocation, enabling efficient utilization of the available resources and avoiding noisy neighbor situations. Containers can also be scaled horizontally or vertically to handle increased demand in <span class="No-Break">the applications.</span></li>
				<li><strong class="bold">Model versioning and updates</strong>: Managing different versions of the model and <a id="_idIndexMarker128"/>keeping respective dependencies up to date without disrupting applications could be challenging. With containers, different images can be created and versioned, making it easy to track changes, manage different model versions, and seamlessly execute rollbacks if needed. We will also explore how to use container orchestration engines to automate these updates later, in <a href="B31108_11.xhtml#_idTextAnchor145"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Security</strong>: Protecting <a id="_idIndexMarker129"/>data during the training and inference stages is crucial when developing GenAI apps. With containers, we can enforce strict access controls and policies for data access, minimize the attack surface by including only the necessary components in the container image, and they also provide a layer of isolation between the containers and the <span class="No-Break">underlying host.</span></li>
			</ul>
			<p>By providing isolated, consistent, and reproducible environments, containers can simplify dependency management, optimize resource efficiency, streamline model deployments, and improve overall system security. Container technology comprehensively addresses all challenges presented by GenAI application development, thus making it a de <span class="No-Break">facto choice.</span></p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>Building a GenAI container image</h2>
			<p>Let’s get <a id="_idIndexMarker130"/>a first-hand experience of building our first GenAI container image and deploying it locally. To get started, we will download the model <a id="_idIndexMarker131"/>files from <strong class="bold">Hugging Face</strong> (<a href="https://huggingface.co/">https://huggingface.co/</a>), an AI/ML platform that helps users build, deploy, and test machine learning <a id="_idIndexMarker132"/>models. Hugging Face operates the <strong class="bold">Model Hub</strong>, where developers and researchers can share thousands of pre-trained models. It also supports various frameworks, including TensorFlow, PyTorch, <span class="No-Break">and ONNX.</span></p>
			<p>In this walk-through, we will <a id="_idIndexMarker133"/>take one of the popular open source models, <strong class="bold">Llama 2</strong> (<a href="https://llama.meta.com/llama2/">https://llama.meta.com/llama2/</a>), by <strong class="bold">Meta</strong>. However, you have to read <a id="_idIndexMarker134"/>and comply with the terms and conditions of the model. Navigate to the Hugging Face Llama model page (<a href="https://huggingface.co/meta-llama/Llama-2-7b">https://huggingface.co/meta-llama/Llama-2-7b</a>) to request access to the model. The Llama 2 model comes in multiple sizes: 7B, 13B, and 70B where B stands for billion parameters. The bigger the size of the model, the greater the resources needed to host it. Given not all personal laptops are equipped with specialized hardware such as GPUs, we will be <a id="_idIndexMarker135"/>using the CPU version of the Llama 2 model. This was made possible because of <strong class="bold">llama.cpp</strong> (<a href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a>), an open source project aimed at providing an efficient and portable implementation of Llama models. It enables us to deploy these models on various platforms, including personal computers, without requiring GPUs. llama.cpp <a id="_idIndexMarker136"/>applies a custom quantization approach to compress the models in a GGUF format, to reduce the size and <span class="No-Break">resource requirements.</span></p>
			<p>An inference <a id="_idIndexMarker137"/>endpoint is created using the <strong class="bold">Python Flask API</strong> (<a href="https://flask.palletsprojects.com/">https://flask.palletsprojects.com/</a>), which provides an HTTP POST API on port <strong class="source-inline">5000</strong> and accepts a JSON request with an input prompt, system message, and so on. Input parameters are then passed to the Llama model and model output is returned in the JSON format. Let’s begin <span class="No-Break">the process:</span></p>
			<ol>
				<li>Install <span class="No-Break">the pre-requisites:</span><ul><li><strong class="source-inline">huggingface-cli</strong> <span class="No-Break">from </span><a href="https://huggingface.co/docs/huggingface_hub/en/installation"><span class="No-Break">https://huggingface.co/docs/huggingface_hub/en/installation</span></a></li><li><strong class="source-inline">jq</strong> <span class="No-Break">from </span><a href="https://jqlang.github.io/jq/download/"><span class="No-Break">https://jqlang.github.io/jq/download/</span></a></li><li><strong class="bold">Docker Engine</strong> <span class="No-Break">from </span><a href="https://docs.docker.com/engine/install/"><span class="No-Break">https://docs.docker.com/engine/install/</span></a></li></ul></li>
				<li>Authenticate to the Hugging Face platform by following the instructions <span class="No-Break">at </span><a href="https://huggingface.co/docs/huggingface_hub/en/guides/cli"><span class="No-Break">https://huggingface.co/docs/huggingface_hub/en/guides/cli</span></a><span class="No-Break">.</span></li>
				<li>Download the Llama 2 model <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">huggingface-cli</strong></span><span class="No-Break">:</span><pre class="source-code">
$ huggingface-cli download TheBloke/Llama-2-7B-Chat-GGUF llama-2-7b-chat.Q2_K.gguf --local-dir . --local-dir-use-symlinks False</pre></li>				<li>The following message is displayed when the model download <span class="No-Break">is complete:</span><pre class="source-code">
$ Download complete. Moving file to llama-2-7b-chat.Q2_K.gguf
llama-2-7b-chat.Q2_K.gguf</pre></li>				<li>Create an <strong class="source-inline">app.py</strong> with our Flask code. This code block sets up a Python Flask app with a single route, <strong class="source-inline">/predict</strong>, that accepts HTTP POST requests. It uses the <strong class="source-inline">llama_cpp</strong> library to load the Llama 2 model, generates a response based on the input <a id="_idIndexMarker138"/>prompt and system message from the request, and returns the model’s response as a JSON object. You can download the <strong class="source-inline">app.py</strong> code from GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/app.py"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/app.py</span></a><span class="No-Break">:</span><pre class="source-code">
from flask import Flask, request, jsonify
import llama_cpp
app = Flask(__name__)
model = llama_cpp.Llama("llama-2-7b-chat.Q2_K.gguf")
@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    prompt = f"""&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;{data.get('sys_msg', '')}&lt;&lt;/SYS&gt;&gt;{data.get('prompt', '')} [/INST]"""
    response = model(prompt, max_tokens=1000)
    return jsonify({'response': response})
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)</pre></li>				<li>Create a <a id="_idIndexMarker139"/>Dockerfile that packages the Python source code, Llama 2 model, and other dependencies. You can download the Dockerfile code from GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/Dockerfile"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/Dockerfile</span></a><span class="No-Break">:</span><pre class="source-code">
# Use an official Python runtime as a parent image
FROM python
# Set the working directory
WORKDIR /app
# Expose the application on port 5000
EXPOSE 5000
# Define environment variable
ENV FLASK_APP=app.py
# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir flask llama-cpp-python
# Copy the current dir contents into the container at /app
COPY . /app
# Run app.py when the container launches
CMD ["python", "app.py"]</pre></li>				<li>Build the <span class="No-Break">container image:</span><pre class="source-code">
<strong class="bold">$ docker build -t my-llama .</strong></pre></li>				<li>Run the container and bind the Flask API server to host <span class="No-Break">port </span><span class="No-Break"><strong class="source-inline">8000</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">$ docker run -p 8000:5000 my-llama</strong></pre></li>				<li>Invoke the Llama 2 model by <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">$ curl -X POST http://localhost:8000/predict -H "Content-Type: application/json" -d '{"prompt":"Create a poem about humanity?","sys_msg":"You are a helpful, respectful, and honest assistant. Always provide safe, unbiased, and positive responses. Avoid harmful, unethical, or illegal content. If a question is unclear or incorrect, explain why. If unsure, do not provide false information."}' | jq .</strong></pre></li>			</ol>
			<p>This will run <a id="_idIndexMarker140"/>the inference against our local <strong class="source-inline">my-llama</strong> container and return the following <span class="No-Break">LLM response:</span></p>
			<pre class="console">
……
Of course, I'm here to assist you with a safe and positive response! Here's a poem about humanity:
Humanity, oh humanity, so diverse and wide,
A mix of cultures, beliefs, and ideals inside.
We may come from different places, with different views,
But beneath our differences, we share a common brew.
……
So here's to humanity, in all its grace and might,
A masterpiece of diversity, a work of art in sight.
Let us cherish each other, with kindness and respect,
And create a world where love is the only text.</pre>			<p>The input request contains two <span class="No-Break">key attributes:</span></p>
			<ul>
				<li><strong class="bold">Prompt</strong>: Input to <span class="No-Break">the LLM</span></li>
				<li><strong class="bold">System message</strong>: To set the context and guide the behavior of the LLM during <span class="No-Break">the request</span></li>
			</ul>
			<p>Optionally, we can pass additional attributes such as <strong class="source-inline">max_tokens</strong> to limit the length of the <span class="No-Break">generated output.</span></p>
			<p>If you notice the size of the image we built, it will be around 7+ GB. That is mainly attributed to the model file and other dependencies. We will explore techniques for reducing the size of the image and optimizing the container startup time later, in <a href="B31108_09.xhtml#_idTextAnchor113"><span class="No-Break"><em class="italic">Chapter 9</em></span></a><span class="No-Break">.</span></p>
			<p>It was easy to <a id="_idIndexMarker141"/>create and run a single container instance on our local computer. Just imagine operating hundreds or thousands of these containers running across hundreds or thousands of VMs, and making sure these containers are highly available, scalable, load balanced, managing resource allocations, and implementing <a id="_idIndexMarker142"/>automated deployments and rollbacks. This is where <strong class="bold">Container Orchestrators</strong> come to <span class="No-Break">the rescue.</span></p>
			<p>Container orchestrators play a pivotal role in modern software development and deployment, addressing all the preceding concerns and managing containerized applications at scale. Some of their key benefits are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">High availability and fault tolerance</strong>: They continuously monitor the health of the <a id="_idIndexMarker143"/>containers at regular intervals and automatically restart/replace failed containers, thus ensuring the desired number of containers are always up <span class="No-Break">and running.</span></li>
				<li><strong class="bold">Scaling</strong>: They enable automatic scaling of the applications to match the user load/demand. Container instances are automatically created and removed in response to the application load. As applications are scaled, underlying compute resources are also scaled to accommodate the <span class="No-Break">new containers.</span></li>
				<li><strong class="bold">Automated deployments</strong>: They automate the deployment of containerized applications across multiple hosts and rollbacks in case of any failures. We can also implement advanced traffic routing patterns such as canary releases and blue/green deployments to safely roll out <span class="No-Break">new changes.</span></li>
				<li><strong class="bold">Load balancing</strong>: They provide built-in load balancing to distribute incoming traffic across multiple container instances, improving performance and reliability. They can also integrate with external load-balancing solutions to load-balance <span class="No-Break">the traffic.</span></li>
				<li><strong class="bold">Service discovery</strong>: As containers are ephemeral in nature, orchestrators provide service discovery features to dynamically discover the container endpoints to <a id="_idIndexMarker144"/>facilitate inter-service communication. They also manage the container networking, including IP address management, DNS resolution, and <span class="No-Break">network segmentation.</span></li>
				<li><strong class="bold">Observability</strong>: They can integrate with monitoring and logging tools, providing visibility into the health and performance of <span class="No-Break">containerized applications.</span></li>
				<li><strong class="bold">Resource management and advanced scheduling</strong>: They can also manage the resource allocation for the containers, including CPU, memory, GPUs, and so on. They enforce the resource limits and reservations, preventing noisy neighbor situations. Advanced scheduling policies can be used to schedule the applications with special hardware needs such as GPUs to the <span class="No-Break">specific hosts.</span></li>
			</ul>
			<p>Now that <a id="_idIndexMarker145"/>we understand the need for a container <a id="_idIndexMarker146"/>orchestrator, the next question is which <a id="_idIndexMarker147"/>orchestrator should we pick? There are a number of <a id="_idIndexMarker148"/>different open source and proprietary orchestrators on the market. Some notable ones are <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Amazon Elastic Container </strong><span class="No-Break"><strong class="bold">Service</strong></span><span class="No-Break"> (</span><a href="https://aws.amazon.com/ecs/"><span class="No-Break">https://aws.amazon.com/ecs/</span></a><span class="No-Break">)</span></li>
				<li><strong class="bold">Azure Container </strong><span class="No-Break"><strong class="bold">Apps</strong></span><span class="No-Break"> (</span><a href="https://azure.microsoft.com/en-us/products/container-apps"><span class="No-Break">https://azure.microsoft.com/en-us/products/container-apps</span></a><span class="No-Break">)</span></li>
				<li><strong class="bold">Docker </strong><span class="No-Break"><strong class="bold">Swarm</strong></span><span class="No-Break"> (</span><a href="https://docs.docker.com/engine/swarm/"><span class="No-Break">https://docs.docker.com/engine/swarm/</span></a><span class="No-Break">)</span></li>
				<li><strong class="bold">Apache </strong><span class="No-Break"><strong class="bold">Mesos</strong></span><span class="No-Break"> (</span><a href="https://mesos.apache.org/"><span class="No-Break">https://mesos.apache.org/</span></a><span class="No-Break">)</span></li>
				<li>Kubernetes (<a href="https://kubernetes.io/">https://kubernetes.io/</a>), and <a id="_idIndexMarker149"/>managed <a id="_idIndexMarker150"/>Kubernetes offerings <a id="_idIndexMarker151"/>such as <strong class="bold">Amazon Elastic Kubernetes </strong><span class="No-Break"><strong class="bold">Service</strong></span><span class="No-Break"> (</span><a href="https://aws.amazon.com/eks/"><span class="No-Break">https://aws.amazon.com/eks/</span></a><span class="No-Break">)</span></li>
				<li><strong class="bold">Google Kubernetes Engine (</strong><span class="No-Break"><strong class="bold">GKE)</strong></span><span class="No-Break"> (</span><a href="https://cloud.google.com/kubernetes-engine"><span class="No-Break">https://cloud.google.com/kubernetes-engine</span></a><span class="No-Break">)</span></li>
				<li><strong class="bold">Azure Kubernetes Service (</strong><span class="No-Break"><strong class="bold">AKS)</strong></span><span class="No-Break"> (</span><a href="https://azure.microsoft.com/en-us/products/kubernetes-service"><span class="No-Break">https://azure.microsoft.com/en-us/products/kubernetes-service</span></a><span class="No-Break">)</span></li>
				<li><strong class="bold">Red Hat </strong><span class="No-Break"><strong class="bold">OpenShift</strong></span><span class="No-Break"> (</span><a href="https://www.redhat.com/en/technologies/cloud-computing/openshift"><span class="No-Break">https://www.redhat.com/en/technologies/cloud-computing/openshift</span></a><span class="No-Break">)</span></li>
			</ul>
			<p>In this section, we learned about the typical challenges of building GenAI models and how we can <a id="_idIndexMarker152"/>use container technology to package GenAI code/models, AI/ML frameworks, hardware-specific libraries, and other <a id="_idIndexMarker153"/>dependencies into an <a id="_idIndexMarker154"/>image for consistency, reusability, and portability. We also built our first GenAI container image using the open source Llama 2 model and ran inference by deploying it locally. Finally, we discussed the challenges of managing containerized applications at scale and how container orchestrators such as K8s can solve those. In the next section, let’s dive into K8s and its architecture, and why it is a great fit to run <span class="No-Break">GenAI models.</span></p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>What is Kubernetes (K8s)?</h1>
			<p>Kubernetes, commonly referred to as K8s, is an open source container orchestration platform that <a id="_idIndexMarker155"/>automates the deployment, scaling, and management of containerized applications. It is the most widely used container orchestration platform (<a href="https://www.cncf.io/reports/kubernetes-project-journey-report/">https://www.cncf.io/reports/kubernetes-project-journey-report/</a>) and has become the de facto choice for many enterprises to run a wide variety <span class="No-Break">of workloads.</span></p>
			<p>Originally developed by Google engineers Joe Beda, Brendan Burns, and Craig McLuckie in 2014 and <a id="_idIndexMarker156"/>now maintained by the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>), its name came from the Ancient Greek for a pilot or helmsman (the person at the helm who steers the ship). It has become the second largest open source project in the world, after Linux, and is the primary orchestrator tool for 71% of Fortune 100 companies (<a href="https://www.cncf.io/reports/kubernetes-project-journey-report/">https://www.cncf.io/reports/kubernetes-project-journey-report/</a>). According to Gartner’s <em class="italic">The CTO’s Guide to Containers and Kubernetes</em> (<a href="https://www.gartner.com/en/documents/5128231">https://www.gartner.com/en/documents/5128231</a>), by 2027, more than 90% of global organizations will be running containerized applications <span class="No-Break">in production.</span></p>
			<p>Some notable <a id="_idIndexMarker157"/>factors in K8s becoming so popular are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Rich community and ecosystem</strong>: With 77K+ contributors from 44 different countries and contributed to by 8K+ companies, K8s has the most vibrant and active community (<a href="https://www.cncf.io/reports/kubernetes-project-journey-report/">https://www.cncf.io/reports/kubernetes-project-journey-report/</a>). The CNCF survey indicates that Kubernetes-related projects (such as Helm, Prometheus, and Istio) are also widely adopted, further strengthening <span class="No-Break">its ecosystem.</span></li>
				<li><strong class="bold">Comprehensive features</strong>: K8s offers a rich set of features including automated rollouts and rollbacks, self-healing, horizontal scaling, service discovery, and load balancing. These capabilities make it a versatile and powerful tool for managing <span class="No-Break">containerized applications.</span></li>
				<li><strong class="bold">Portability</strong>: K8s abstracts away the underlying infrastructure, enabling applications to be deployed consistently whether on-premises, in public, private, or hybrid clouds, or <span class="No-Break">edge locations.</span></li>
				<li><strong class="bold">Managed K8s services</strong>: The availability of the managed K8s offerings from major cloud providers such as Amazon EKS, GKE, and AKS has significantly lowered the barrier to entry. These offerings take away the operational complexities of running and operating K8s clusters, allowing enterprises to focus on their core <span class="No-Break">business objectives.</span></li>
				<li><strong class="bold">Strong governance</strong>: K8s has been governed by CNCF since 2016, which fosters collaborative development and community participation, enabling contributions from a diverse group of developers, organizations, and end users. It follows <a id="_idIndexMarker158"/>an open source model with well-defined governance structures, including <strong class="bold">special interest groups</strong> (<strong class="bold">SIGs</strong>) that focus on specific areas of development <span class="No-Break">and operations.</span></li>
				<li><strong class="bold">Declarative configuration</strong>: K8s uses a declarative approach for configuration management, allowing users to define the desired state of their applications and infrastructure using YAML or JSON files. As depicted in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.5</em>, K8s controllers continuously monitor the current state of a resource and automatically reconcile to match the desired state, simplifying operations and <span class="No-Break">ensuring consistency.</span></li>
			</ul>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B31108_02_05.jpg" alt="Figure 2.5 – How K8s controllers work" width="1194" height="226"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – How K8s controllers work</p>
			<ul>
				<li><strong class="bold">Extensibility</strong>: This is probably the most significant reason why K8 became so popular. K8s is <a id="_idIndexMarker159"/>designed with a modular architecture, supporting custom plugins and extensions through well-defined APIs. This enabled developers and companies to extend or customize the K8s functionality without modifying the upstream code, fostering innovation <span class="No-Break">and adaptability.</span></li>
			</ul>
			<p>Next, let’s look at the architecture <span class="No-Break">of Kubernetes.</span></p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Kubernetes architecture</h2>
			<p>K8s architecture <a id="_idIndexMarker160"/>is based on running clusters that allow your applications/containers to run across multiple hosts. Each cluster consists of two types <span class="No-Break">of nodes:</span></p>
			<ul>
				<li><strong class="bold">Control plane</strong>: The K8s <a id="_idIndexMarker161"/>control plane is the brain behind cluster operations. It consists of critical components such as kube-apiserver, etcd, the scheduler manager, the controller manager, and the cloud <span class="No-Break">controller manager.</span></li>
				<li><strong class="bold">Data plane/worker nodes</strong>: The K8s <a id="_idIndexMarker162"/>data plane running on the worker nodes is composed of several key components, such <a id="_idIndexMarker163"/>as kube-proxy, the kubelet, and the <strong class="bold">Container Network Interface</strong> (<span class="No-Break"><strong class="bold">CNI</strong></span><span class="No-Break">) plugin.</span></li>
			</ul>
			<p><span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.6</em> depicts the high-level K8s cluster architecture. You will notice kube-apiserver is the frontend of the K8s control plane and interacts with the other control plane components, such as controller managers, etcd, and so on, to fulfil the requests. K8s worker nodes <a id="_idIndexMarker164"/>host several key components, responsible for taking instructions from kube-apiserver, executing them on the worker node, and reporting back on <span class="No-Break">the status.</span></p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B31108_02_06.jpg" alt="Figure 2.6 – Kubernetes cluster architecture" width="1044" height="587"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – Kubernetes cluster architecture</p>
			<h3>Control plane components</h3>
			<p>The <a id="_idIndexMarker165"/>primary components <a id="_idIndexMarker166"/>of a control plane are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">kube-apiserver</strong>: This serves as the entry point or frontend into the K8s cluster and the <a id="_idIndexMarker167"/>central management component <a id="_idIndexMarker168"/>that exposes the K8s API. K8s users, administrators, and other components use this API to communicate with the cluster. It also communicates with the etcd component to save the state of K8s objects. It also handles authentication and authorization, validation, and request processing, and communicates with other control plane and data plane components to manage the <span class="No-Break">clusters’ states.</span></li>
				<li><strong class="bold">etcd</strong>: This is the distributed key-value store and is used as the K8s backing store for all cluster <a id="_idIndexMarker169"/>state. It stores the configuration data of all K8s objects and any updates made to them and ensures the cluster state is always reliable and accessible. It’s critical to take regular backups of the etcd database so that clusters can be restored in case of <span class="No-Break">any disruptions.</span></li>
				<li><strong class="bold">kube-controller-manager</strong>: This is responsible for managing various controllers <a id="_idIndexMarker170"/>in the cluster. This includes the default upstream controllers and any custom-built ones. Some examples include <span class="No-Break">the following:</span></li>
				<li><strong class="bold">Deployment controller</strong>: This <a id="_idIndexMarker171"/>watches for K8s deployment objects and manages the updates to <span class="No-Break">K8s Pods.</span></li>
				<li><strong class="bold">kube-scheduler</strong>: This is responsible for scheduling the K8s Pods on the worker <a id="_idIndexMarker172"/>nodes. It monitors the API server for newly created Pods and assigns a worker node based on the resource availability, scheduling requirements defined in the Pod configuration such as nodeSelectors, Pod/node affinity, topology spreads, and so on. When unable to schedule a Pod due to resource exhaustion and so on, it will mark the Pods as <em class="italic">Pending</em> so that other operational add-ons such as the cluster autoscaler can kick in and add/remove compute capacity (worker nodes) to <span class="No-Break">the cluster.</span></li>
				<li><strong class="bold">cloud-controller-manager</strong>: This manages the cloud-specific controllers to handle <a id="_idIndexMarker173"/>the cloud provider <a id="_idIndexMarker174"/>API calls for resource management. It’s the gateway to the Cloud Provider API from the K8s core and is responsible for creating and managing cloud-provider-specific resources (such as nodes, LoadBalancers, and so on) based on changes to K8s objects (such as nodes, Services, and so on). Some examples include <span class="No-Break">the following:</span><ul><li><strong class="bold">Node controller</strong>: This is responsible for monitoring the health of the worker <a id="_idIndexMarker175"/>nodes and handling <a id="_idIndexMarker176"/>the addition or removal of nodes in <span class="No-Break">the cluster</span></li><li><strong class="bold">Service controller</strong>: This <a id="_idIndexMarker177"/>watches <a id="_idIndexMarker178"/>for service and node object changes, and creates, updates, and deletes cloud provider load <span class="No-Break">balancers accordingly</span></li></ul></li>
			</ul>
			<h3>Data plane components</h3>
			<p>The primary <a id="_idIndexMarker179"/>components of a <a id="_idIndexMarker180"/>data plane are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">kubelet</strong>: This is an agent that runs on every worker node in the cluster. It’s responsible <a id="_idIndexMarker181"/>for taking instructions from kube-apiserver, executing them on the respective worker node, and reporting the updates <a id="_idIndexMarker182"/>on the node components back to the cluster control plane. It interacts with other node components such as the container runtime to launch container processes, the CNI plugin to set up the container networking, and CSI plugins to manage the persistent volumes and <span class="No-Break">so on.</span></li>
				<li><strong class="bold">kube-proxy</strong>: This is a <a id="_idIndexMarker183"/>network proxy that <a id="_idIndexMarker184"/>runs on each worker node in the cluster, implementing the K8s service concept. It maintains the network routing rules on the worker node, to allow the network communications to and from your Pods from within/outside the cluster. It uses the operating system packet filtering layer, such as IP tables, IPVS, and so on, to route the traffic to other endpoints in <span class="No-Break">the cluster.</span></li>
				<li><strong class="bold">Container runtime</strong>: containerd <a id="_idIndexMarker185"/>is the de <a id="_idIndexMarker186"/>facto container runtime responsible for launching the containers on the worker node. It is responsible for managing the lifecycle of containers in the K8s environment. K8s also supports other container runtimes such <span class="No-Break">as CRI-O.</span></li>
			</ul>
			<p>Apart from these components, it’s essential to deploy additional add-on software on the K8s cluster for production operations. These add-ons add capabilities such as monitoring, security, <span class="No-Break">and networking.</span></p>
			<h3>Add-on software components</h3>
			<p>Here <a id="_idIndexMarker187"/>are a few examples <a id="_idIndexMarker188"/>of <span class="No-Break">add-on software:</span></p>
			<ul>
				<li><strong class="bold">CNI plugin</strong>: This is <a id="_idIndexMarker189"/>a software add-on that <a id="_idIndexMarker190"/>implements container network specifications. They adhere to the K8s networking tenets and are responsible for allocating IP addresses to K8s Pods (<a href="https://kubernetes.io/docs/concepts/workloads/pods/">https://kubernetes.io/docs/concepts/workloads/pods/</a>) and enabling them to communicate with each other within the cluster. Popular add-ons in this space are Cilium (<a href="https://github.com/cilium/cilium">https://github.com/cilium/cilium</a>), Calico (<a href="https://github.com/projectcalico/calico">https://github.com/projectcalico/calico</a>), and Amazon VPC <span class="No-Break">CNI (</span><a href="https://github.com/aws/amazon-vpc-cni-k8s"><span class="No-Break">https://github.com/aws/amazon-vpc-cni-k8s</span></a><span class="No-Break">).</span></li>
				<li><strong class="bold">CSI plugin</strong>: This is <a id="_idIndexMarker191"/>a software add-on that <a id="_idIndexMarker192"/>implements container storage interface specifications. They are responsible for providing persistent storage volumes to K8s Pods and managing the lifecycle of those volumes. A couple of notable add-ons are Amazon EBS CSI driver (<a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver">https://github.com/kubernetes-sigs/aws-ebs-csi-driver</a>) and Portworx CSI <span class="No-Break">Driver (</span><a href="https://docs.portworx.com/portworx-enterprise/operations/operate-kubernetes/storage-operations/csi"><span class="No-Break">https://docs.portworx.com/portworx-enterprise/operations/operate-kubernetes/storage-operations/csi</span></a><span class="No-Break">).</span></li>
				<li><strong class="bold">CoreDNS</strong>: This is <a id="_idIndexMarker193"/>an essential software <a id="_idIndexMarker194"/>add-on that provides DNS resolution within the cluster. Containers launched in K8s worker nodes automatically include this DNS server in their <span class="No-Break">DNS searches.</span></li>
				<li><strong class="bold">Monitoring plugins</strong>: This <a id="_idIndexMarker195"/>is a software <a id="_idIndexMarker196"/>add-on that provides observability into the cluster infrastructure and workloads. They extract essential observability details such as logs, metrics, and traces and write to monitoring platforms such as Prometheus, Amazon CloudWatch, Splunk, Datadog, and <span class="No-Break">New Relic.</span></li>
				<li><strong class="bold">Device plugins</strong>: Modern <a id="_idIndexMarker197"/>AI/ML apps use <a id="_idIndexMarker198"/>specialized hardware devices such as GPUs from NVIDIA, Intel, and AMD and custom accelerators from Amazon, Google, and Meta. K8s provides a device plugin framework that you can use to advertise system hardware resources to the kubelet and control plane so that you can make scheduling decisions based on <span class="No-Break">their availability.</span></li>
			</ul>
			<p>This is not an exhaustive list of all K8s components and add-ons. We will dive into AI/ML-related add-ons in a later part of <span class="No-Break">the book.</span></p>
			<p>In this section, we dove into K8s architecture, learned about various control plane and data plane components, and explored the advantages of the K8s platform and why it became the de facto standard in the community. Let’s understand why K8s is a great fit for running GenAI <span class="No-Break">models next.</span></p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>Why K8s is a great fit for GenAI models</h2>
			<p>Now that <a id="_idIndexMarker199"/>we understand the K8s architecture, its components, and the advantages of the platform, let’s discuss how we apply those to solve common challenges with operating <span class="No-Break">GenAI models.</span></p>
			<h3>Challenges of running GenAI models</h3>
			<p>Some <a id="_idIndexMarker200"/>common challenges of running GenAI models are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Computational requirements</strong>: GenAI models are increasingly becoming large and complex, thus requiring substantial computational resources, including GPUs, TPUs, and custom accelerators for training and inference. Managing these resources efficiently is crucial to ensure performance <span class="No-Break">and cost-efficiency.</span></li>
				<li><strong class="bold">Scalability</strong>: As the demand for AI/ML services increases, scaling GenAI models to handle the demand is essential. This requires seamless scaling of computational resources without sacrificing the performance and cost of <span class="No-Break">the models.</span></li>
				<li><strong class="bold">Observability</strong>: As GenAI models proliferate, it’s critical to understand their performance <a id="_idIndexMarker201"/>by monitoring both business-level KPIs and the health of the overall system using logs <span class="No-Break">and metrics.</span></li>
				<li><strong class="bold">Data management</strong>: GenAI models rely on vast amounts of data for both training and inference. Data preparation, security, and management are critical in increasing the model’s accuracy <span class="No-Break">and performance.</span></li>
				<li><strong class="bold">Deployment complexity</strong>: As we learned earlier in this chapter, all GenAI models require custom frameworks, plugin libraries, and other dependencies to deploy them. This complexity can lead to deployment issues, delays, and <span class="No-Break">increased errors.</span></li>
			</ul>
			<h3>K8s advantages</h3>
			<p>K8s offers <a id="_idIndexMarker202"/>several advantages for addressing the challenges of running GenAI models, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Efficient resource management</strong>: K8s has a robust resource management system built into kube-scheduler. It automates the distribution of K8s Pods into the worker nodes while meeting different scheduling requirements/constraints. Schedulers can be configured to operate in lowest-cost, random, or bin-pack modes for flexibility. With the K8s extensibility, you can develop custom schedulers and use them for scheduling workloads. A common application of this is to schedule training or inference workloads on devices with custom devices such as AWS Trainium and Inferentia. By using K8s, we can implement dynamic resource allocation based on model requirements, thus optimizing costs and <span class="No-Break">improving performance.</span></li>
				<li><strong class="bold">Seamless scalability</strong>: Training or fine-tuning a GenAI model requires a significant number of computational resources. Inference endpoints of these models also <a id="_idIndexMarker203"/>need to scale horizontally based on the workload demand. This will be achieved seamlessly using K8s <a id="_idIndexMarker204"/>autoscaling mechanisms such as <strong class="bold">Horizontal Pod Autoscaling</strong> (<strong class="bold">HPA</strong>) (<a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/</a>), <strong class="bold">Vertical Pod Autoscaling</strong> (<strong class="bold">VPA</strong>) (<a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler</a>), and <strong class="bold">Cluster Autoscaling</strong> (<a href="https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/">https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/</a>). HPA <a id="_idIndexMarker205"/>automatically scales a workload <a id="_idIndexMarker206"/>resource (such as a Deployment or StatefulSet) to match the workload demand. It does this by creating and deploying new K8s Pods in response to the demand. VPA automatically adjusts the resource limits (such as CPU, memory, and so on) of Pods to match their actual usage. It helps in optimizing resource allocation, ensuring workloads are run efficiently. Cluster autoscaling is responsible for ensuring the right number of resources are attached to the cluster at all times. We will take a deeper look at these mechanisms in <a href="B31108_06.xhtml#_idTextAnchor075"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Extensibility</strong>: Extensibility plays a crucial role in running GenAI workloads on K8s. It allows you to extend the functionality of K8s in a scalable manner without <a id="_idIndexMarker207"/>modifying the upstream code. We can use custom-built add-ons such as <strong class="bold">Kubeflow</strong> (<a href="https://www.kubeflow.org/">https://www.kubeflow.org/</a>), an AI/ML platform that provides custom resources for managing ML pipelines, model training, and deployment. Hardware companies can also leverage this by developing device plugins to manage GPU resources in K8s so that GenAI training and <a id="_idIndexMarker208"/>inference workloads are scheduled accordingly. GenAI workloads often require specific frameworks such as <strong class="bold">PyTorch</strong>, <strong class="bold">TensorFlow</strong>, and <strong class="bold">Jupyter Notebook</strong>. We can use custom-built operators to integrate <a id="_idIndexMarker209"/>these frameworks and tools for seamless development <a id="_idIndexMarker210"/><span class="No-Break">and deployment.</span></li>
				<li><strong class="bold">Security</strong>: K8s has many in-built security mechanisms to secure GenAI workloads. One <a id="_idIndexMarker211"/>can use <strong class="bold">Role-Based Access Control</strong> (<strong class="bold">RBAC</strong>) (<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">https://kubernetes.io/docs/reference/access-authn-authz/rbac/</a>) to limit access to resources, ensuring that only authorized users or applications can access sensitive data. K8s Secrets or external secret management solutions can be used to safeguard sensitive information. K8s network <a id="_idIndexMarker212"/>policies can be used to implement network segmentation so that only authorized Pods can access data stores. On top of this, we can enable security controls such <a id="_idIndexMarker213"/>as encryption, audit logging, security scanning, and <strong class="bold">Pod Security Standards</strong> (<strong class="bold">PSS</strong>) (<a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">https://kubernetes.io/docs/concepts/security/pod-security-standards/</a>) to ensure a robust security posture. We will explore all these features in detail in <a href="B31108_09.xhtml#_idTextAnchor113"><span class="No-Break"><em class="italic">Chapter 9</em></span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">High Availability (HA) and fault tolerance</strong>: These play a critical role in running GenAI workloads efficiently and reliably. Foundational model training often takes weeks or months. If a node or Pod fails, K8s’s in-built self-healing mechanism can automatically schedule the job to another node, thus minimizing interruptions. AI frameworks can be used along with this to implement a checkpointing strategy, to save the training state periodically. For model inferencing, K8s can automatically scale inference Pods based on the demand and recover the failed ones by launching replacement Pods. K8s can also perform rolling blue/green updates to deploy new model versions and seamlessly roll back in case of failures. We will take a deeper look at this topic in <a href="B31108_13.xhtml#_idTextAnchor176"><span class="No-Break"><em class="italic">Chapter 13</em></span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Rich ecosystem and add-ons</strong>: The Cloud Native Artificial Intelligence Whitepaper (<a href="https://www.cncf.io/wp-content/uploads/2024/03/cloud_native_ai24_031424a-2.pdf">https://www.cncf.io/wp-content/uploads/2024/03/cloud_native_ai24_031424a-2.pdf</a>) underscores the growing adoption of Kubernetes-native tools and frameworks to streamline the development, training, and deployment of AI models. Notable examples include Kubeflow <a id="_idIndexMarker214"/>and MLflow for operating end-to-end ML platforms on K8s; KServe, Seldon, and RayServe for model serving and scaling; and OpenLLMetry, TruLens, and Deepchecks for model observability. This list will continue to grow as the industry matures around GenAI <span class="No-Break">use cases.</span></li>
			</ul>
			<p>In this section, we learned about the typical challenges of operating GenAI models and looked at the advantages of using K8s to address them. K8s extensibility, efficient resource management, security, HA, and fault tolerance capabilities make it a great fit to run GenAI models <span class="No-Break">at scale.</span></p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>Summary</h1>
			<p>In this chapter, we started with the evolution of compute technologies and how containers emerged as a standard to package and ship applications and abstract away infrastructure complexities for developers. We discussed the benefits of using containers for GenAI models and built and ran our first hello-world, GenAI container images. Then we looked at the challenges of running and managing containers at scale and how container orchestrator engines such as K8s can help <span class="No-Break">simplify that.</span></p>
			<p>We dove into the high-level K8s architecture and various components that made up the control plane and data plane. We also learned how the extensibility, portability, declarative nature, and rich community behind K8s made it popular and the de facto container orchestrator in <span class="No-Break">the market.</span></p>
			<p>Finally, we discussed the typical challenges of operating GenAI workloads at scale and how K8s is a great fit to address those challenges with its efficient resource management, seamless scaling, extensibility, and security capabilities. In the next chapter, we will explore how to build a K8s cluster in a cloud environment, leverage popular open source tooling to manage GenAI workloads, and deploy our <em class="italic">my-llama</em> container <span class="No-Break">in it.</span></p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>Appendix</h2>
			<p>Following are some great resources for in-depth training <span class="No-Break">on Kubernetes:</span></p>
			<ul>
				<li>Kubernetes training <span class="No-Break">website: </span><a href="https://kubernetes.io/training/"><span class="No-Break">https://kubernetes.io/training/</span></a></li>
				<li>Kubernetes course on the Linux <span class="No-Break">Foundation: </span><a href="https://training.linuxfoundation.org/full-catalog/?_sft_product_type=training&amp;_sft_technology=kubernetes"><span class="No-Break">https://training.linuxfoundation.org/full-catalog/?_sft_product_type=training&amp;_sft_technology=kubernetes</span></a></li>
				<li>Kubernetes Learning Path at <span class="No-Break">KodeKloud: </span><a href="https://kodekloud.com/learning-path/kubernetes/"><span class="No-Break">https://kodekloud.com/learning-path/kubernetes/</span></a></li>
			</ul>
		</div>
	</div></div></body></html>