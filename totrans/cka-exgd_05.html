<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer112">
<h1 class="chapter-number" id="_idParaDest-105"><a id="_idTextAnchor149"/>5</h1>
<h1 id="_idParaDest-106"><a id="_idTextAnchor150"/>Demystifying Kubernetes Storage</h1>
<p>In this chapter, we will discuss the core concept of Kubernetes storage for stateful workloads and shows how to configure applications with mounted storage and dynamically persistent storage. This chapter covers 10% of the <strong class="bold">Certified Kubernetes Administrator</strong> (<strong class="bold">CKA</strong>) exam content.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>Stateful versus stateless workloads </li>
<li>Kubernetes volumes</li>
<li>Kubernetes StorageClasses</li>
<li>Volume modes, access modes, and reclaim policies for volumes</li>
<li>Configuring an application with mounted storage</li>
<li>Configuring an application with persistent storage</li>
</ul>
<h1 id="_idParaDest-107"><a id="_idTextAnchor151"/>Technical requirements  </h1>
<p>To get started, we need to make sure your local machine meets the following technical requirements:</p>
<ul>
<li>A compatible Linux host – we recommend a Debian-based Linux distribution such as Ubuntu 18.04 or later</li>
<li>Make sure your host machine has at least 2 GB RAM, 2 CPU cores, and about 20 GB of free disk space</li>
</ul>
<h1 id="_idParaDest-108"><a id="_idTextAnchor152"/>Stateful versus stateless workloads</h1>
<p>Kubernetes is <a id="_idIndexMarker329"/>designed for both stateful and stateless applications. To maintain stateless workloads in Kubernetes, we can freely delete and replace containers without any additional concerns. The stateful application usually has storage attached either locally or in a remote location, as it needs to hold client data. That data could be short-lived or <em class="italic">non-persistent</em> storage, which means that it is just maintained until the expiration of a session. An example of this is the Redis cache on Kubernetes. Another use case is when the data needs to be held for long enough by using <a id="_idIndexMarker330"/>persistent storage so that it can be used on-demand. An example of the latter is the MongoDB operator for Kubernetes. The whole story is much more complicated than it seems but it all starts with Kubernetes volumes. </p>
<p>Kubernetes volumes represent <a id="_idIndexMarker331"/>the concept of storage in Kubernetes. As mentioned in <a href="B18201_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Kubernetes Overview</em>, the volumes in Kubernetes are managed by storage drivers tailored by storage vendors. This part is no <a id="_idIndexMarker332"/>longer part of Kubernetes source code after the <strong class="bold">Container Storage Interface </strong>(<strong class="bold">CSI</strong>) was introduced. </p>
<p>A volume can support <a id="_idIndexMarker333"/>local storage, on-premises software-defined storage, cloud-based storage (such as blob, block, or file storage), or a <strong class="bold">network file system</strong> (<strong class="bold">NFS</strong>) as shown in <em class="italic">Figure 5.1</em>: </p>
<div>
<div class="IMG---Figure" id="_idContainer097">
<img alt="Figure 5.1 – A CSI " height="389" src="image/Figure_5.01_B18201.jpg" width="542"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.1 – A CSI</p>
<p>Then, users can use CSI-compatible volume drivers and CSI volumes to attach or directly mount the pods up and running in the Kubernetes cluste<a id="_idTextAnchor153"/>r. </p>
<h1 id="_idParaDest-109"><a id="_idTextAnchor154"/>Kubernetes volumes </h1>
<p>Ephemeral volumes and <a id="_idIndexMarker334"/>persistent volumes are two main types of volumes in Kubernetes. We’ll take a look at each of them. Some of them may not be covered in the CKA exam, but it is important to know, as whichever organization you work in will have embarked on its journey with one of those public cloud provide<a id="_idTextAnchor155"/>rs. </p>
<h2 id="_idParaDest-110"><a id="_idTextAnchor156"/>Ephemeral storage</h2>
<p>Ephemeral volumes targeted to the application need to hold the data, but they don’t care about data loss <a id="_idIndexMarker335"/>in the case that the pod fails <a id="_idIndexMarker336"/>or restarts – the lifecycle of the ephemeral volume is aligned with the pod lifecycle. With that in mind, mounted storage is usually ephemeral, as it shares the same lifecycle as your containers. As long as the container is stopped or destroyed during the process of restarting the pod, any internal storage is completely removed.</p>
<p>Another use case is when a pod contains multiple containers. It is possible to mount that storage to the containers and allow those containers to share the same volume so that they interact with the same shared filesystem. </p>
<p>Ephemeral volumes have several types, which we will cover one by<a id="_idTextAnchor157"/> one. </p>
<h3>emptyDir</h3>
<p>emptyDir is one of the most common types of ephemeral storage and will appear in the CKA exam. It usually <a id="_idIndexMarker337"/>serves as an empty directory when the pod <a id="_idIndexMarker338"/>starts, and it shares the same lifecycle with the Pod, meaning it only exists as long as a pod is up and running, and the data in the emptyDir is deleted permanently when the pod stops or restarts. </p>
<p>When it comes to multi-containers in the same pod, it can be shared across containers, although each container can mount the emptyDir in a different repository, as shown in <em class="italic">Figure 5.2</em>: </p>
<div>
<div class="IMG---Figure" id="_idContainer098">
<img alt="Figure 5.2 – Multi-containers in a pod sharing storage volumes " height="518" src="image/Figure_5.02_B18201.jpg" width="927"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.2 – Multi-containers in a pod sharing storage volumes</p>
<p>The following is <a id="_idIndexMarker339"/>an example YAML definition of an emptyDir <a id="_idIndexMarker340"/>mounted to a pod: </p>
<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
  name: multi-containers
spec:
  restartPolicy: Never
<strong class="bold">  volumes:</strong>
<strong class="bold">  - name: shared-data</strong>
<strong class="bold">    emptyDir: {}</strong>
  containers:
  - name: busybox-pod
    image: busybox
    command: ["/bin/sh","-c","while true; do sleep 3600; done"]
    <strong class="bold">volumeMounts:</strong>
<strong class="bold">    - name: shared-data</strong>
<strong class="bold">      mountPath: /tmp/data</strong>
  - name: nginx-pod
    image: nginx
    <strong class="bold">volumeMounts:</strong>
<strong class="bold">    - name: shared-data</strong>
<strong class="bold">      mountPath: /data</strong></pre>
<p>Through the <a id="_idIndexMarker341"/>preceding example, you can see how to mount shared <a id="_idIndexMarker342"/>volumes between two containers, which would come in handy when you want those two containers to consume the same da<a id="_idTextAnchor158"/>ta source. </p>
<h3>CSI ephemeral volumes </h3>
<p>CSI ephemeral volumes are <a id="_idIndexMarker343"/>CSI driver-compatible <a id="_idIndexMarker344"/>volumes that serve as temporary storage. For a very long time in the past, CSI volumes provided by an external storage driver in Kubernetes were used as persistent volumes, with the goal of not sharing a lifecycle with the pod. Starting from Kubernetes 1.15, CSI drivers can also be used for such ephemeral inline volumes. The following is an example of using CSI ephemeral volumes: </p>
<pre class="source-code">
kind: Pod
apiVersion: v1
metadata:
  name: my-csi-pod
spec:
  containers:
    - name: my-frontend
      image: busybox
      volumeMounts:
      - mountPath: "/data"
        name: my-csi-vol
      command: ["sleep", "1000000"]
  volumes:
    - name: my-csi-vol
      <strong class="bold">csi:</strong>
<strong class="bold">        driver:</strong> inline.storage.kubernetes.io
        <strong class="bold">volumeAttributes:</strong>
          foo: bar</pre>
<p>These CSI storage <a id="_idIndexMarker345"/>drivers are generally third-party, such <a id="_idIndexMarker346"/>as Azure Disk, Azure File, AWS EBS, and DellEMC unity – you <a id="_idIndexMarker347"/>can find a complete list of CSI drivers at <a href="https://kubernetes-csi.github.io/docs/drivers.xhtml">https://kubernetes-csi.github.io/do<span id="_idTextAnchor159"/>cs/drivers.xhtml</a>.</p>
<h3>Generic ephemeral volumes </h3>
<p>Generic ephemeral volumes <a id="_idIndexMarker348"/>are general drivers with some additional features <a id="_idIndexMarker349"/>available such as snapshotting, storage cloning, storage resizing, and storage capacity tracking. The following is an example of using CSI ephemeral volumes: </p>
<pre class="source-code">
kind: Pod
apiVersion: v1
metadata:
  name: my-app
spec:
  containers:
    - name: my-frontend
      image: busybox
      volumeMounts:
      - mountPath: "/cache"
        name: cache-volume
      command: [ "sleep", "1000000" ]
  volumes:
    - name: scratch-volume
<strong class="bold">      ephemeral:</strong>
        volumeClaimTemplate:
          metadata:
            labels:
              type: my-cache-volume
          spec:
            accessModes: [ "ReadWriteOnce" ]
            storageClassName: "my-cache-storage-class"
            resources:
              requests:
                storage: 1Gi</pre>
<p>Generic ephemeral volumes work with all storage drivers that support dynamic provisioning, including <a id="_idIndexMarker350"/>some third-party CSI storage drivers. Now that we <a id="_idIndexMarker351"/>have a good understanding of ephemeral volumes, we’ll have a look at projected volumes and see how they wo<a id="_idTextAnchor160"/>rk with Kubernetes. </p>
<h3>Projected volumes </h3>
<p>Configuration <a id="_idIndexMarker352"/>data is mounted to the Kubernetes pods – this data <a id="_idIndexMarker353"/>was injected into a pod through the sidecar pattern. We covered <strong class="source-inline">ConfigMap</strong> and <strong class="source-inline">Secret</strong> objects in <a href="B18201_04.xhtml#_idTextAnchor080"><em class="italic">Chapter 4</em></a>, <em class="italic">Application Scheduling and Lifecycle Management,</em> which fall under this category. More specifically, they are also called <strong class="bold">projected volumes</strong> because they represent a volume that maps several existing volumes into the same directory. </p>
<p>Besides ConfigMap and <a id="_idIndexMarker354"/>Secret, projected volumes also consist of <strong class="bold">downwardAPI</strong> volumes and <strong class="bold">service account tokens</strong>. We’ll take a closer look at them here with <a id="_idIndexMarker355"/>some examples. </p>
<p>A <strong class="source-inline">downwardAPI</strong> volume is designed to make downward API data available to applications. Similarly, it also mounts as a directory and then writes the data in plain-text files. The downward API allows <a id="_idIndexMarker356"/>containers to consume cluster or pod information without using the Kubernetes API server or through the client. </p>
<p>The following example shows you how to mount <strong class="source-inline">downwardAPI</strong> as a projected volume:</p>
<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
<strong class="bold">    volumeMounts:</strong>
<strong class="bold">    - name: all-in-one</strong>
<strong class="bold">      mountPath: "/projected-volume"</strong>
<strong class="bold">      readOnly: true</strong>
  volumes:
  - name: all-in-one
<strong class="bold">    projected:</strong>
<strong class="bold">      sources:</strong>
<strong class="bold">      - downwardAPI:</strong>
          items:
            - path: "labels"
              fieldRef:
                fieldPath: metadata.labels
            - path: "cpu_limit"
              resourceFieldRef:
                containerName: container-test
                resource: limits.cpu</pre>
<p>A service account token type of projected volume is designed to make downward API data available to <a id="_idIndexMarker357"/>applications. Similarly, it also mounts as a directory and then writes the data in plain-text files. </p>
<p>The following <a id="_idIndexMarker358"/>example shows you how to mount a service account <a id="_idIndexMarker359"/>token as a projected volume: </p>
<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
<strong class="bold">    volumeMounts:</strong>
<strong class="bold">    - name: all-in-one</strong>
<strong class="bold">      mountPath: "/projected-volume"</strong>
<strong class="bold">      readOnly: true</strong>
  volumes:
  - name: all-in-one
<strong class="bold">    projected:</strong>
<strong class="bold">      sources:</strong>
<strong class="bold">      - serviceAccountToken:</strong>
          audience: api
          expirationSeconds: 3600
          path: token</pre>
<p>Let’s wrap up what we covered in this section about <strong class="source-inline">downwardAPI</strong> and service account token volumes, as well as recall what we learned about ConfigMap and Secret objects in <a href="B18201_04.xhtml#_idTextAnchor080"><em class="italic">Chapter 4</em></a>, <em class="italic">Application Scheduling and Lifecycle Management</em>, by looking at <a id="_idIndexMarker360"/>the following. This is an all-in-one example to help you understand <a id="_idIndexMarker361"/>how to work with all of them in one encounter: </p>
<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
<strong class="bold">    volumeMounts:</strong>
<strong class="bold">    - name: all-in-one</strong>
<strong class="bold">      mountPath: "/projected-volume"</strong>
<strong class="bold">      readOnly: true</strong>
  volumes:
  - name: all-in-one
<strong class="bold">    projected:</strong>
<strong class="bold">      sources:</strong>
<strong class="bold">      - secret:</strong>
          name: mysecret
          items:
            - key: username
              path: my-group/my-username
<strong class="bold">      - downwardAPI:</strong>
          items:
            - path: "labels"
              fieldRef:
                fieldPath: metadata.labels
            - path: "cpu_limit"
              resourceFieldRef:
                containerName: container-test
                resource: limits.cpu
<strong class="bold">      - configMap:</strong>
          name: myconfigmap
          items:
            - key: config
              path: my-group/my-config
<strong class="bold">      - serviceAccountToken:</strong>
          audience: api
          expirationSeconds: 3600
          path: token</pre>
<p>All the projected volumes, <strong class="source-inline">configMap</strong>, <strong class="source-inline">downwardAPI</strong>, <strong class="source-inline">secret</strong>, plus <strong class="source-inline">emptyDir</strong>, are provided as local <a id="_idIndexMarker362"/>ephemeral storage. On each node, <strong class="source-inline">kubelet</strong> is in charge <a id="_idIndexMarker363"/>of provisioning and managing pods, and managing the local ephemeral storage. </p>
<p>Aside from the mounted storage serving as internal storage, in some use cases, we also need persistent data outside the life of the container itself that continues to exist even if the container stops or is replaced. This raises the requirement to have permanent external storage assigned to our pods. We’ll take a look at persiste<a id="_idTextAnchor161"/>nt volumes in the next section. </p>
<h2 id="_idParaDest-111"><a id="_idTextAnchor162"/>Persistent storage</h2>
<p>Compared to ephemeral volumes, persistent volumes have a lifecycle that is independent of the <a id="_idIndexMarker364"/>Kubernetes pods. State persistence means keeping some <a id="_idIndexMarker365"/>data or information to continue beyond the life of the container when the container is deleted or replaced. However, it can be modified or updated by the containers while it’s running. </p>
<p>The mechanism of working with persistent volume in Kubernetes takes advantage of the exposed API, which abstracts technical details of how the external storage is provided, as well as how it is consumed. Kubernetes allows us to work with persistent storage through the notion of persistent volumes and persistent volume claims:</p>
<ul>
<li>A <strong class="bold">PersistentVolume</strong> (<strong class="bold">PV</strong>) is a storage resources provisioned dynamically based on the <a id="_idIndexMarker366"/>storage classes with a set of features to fulfill the user’s requirements. </li>
<li>A <strong class="bold">PersistentVolumeClaim</strong> (<strong class="bold">PVC</strong>) is the abstraction layer between the pod and the PV requested <a id="_idIndexMarker367"/>by the user, with a set of requirements including the specific level of resources and the access modes.</li>
</ul>
<p>As shown in the following, <em class="italic">Figure 5.3</em>,  the PV and PVC are defined in the Kubernetes cluster, while the physical storage is outside of the Kubernetes cluster: </p>
<div>
<div class="IMG---Figure" id="_idContainer099">
<img alt="Figure 5.3 – The PV and PVC " height="502" src="image/Figure_5.03_B18201.jpg" width="1582"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.3 – The PV and PVC</p>
<p>Equally, note that the <a id="_idIndexMarker368"/>PV can be bound to a PVC, and it is a <a id="_idIndexMarker369"/>cluster-wide resource, while the PVC is namespaced. </p>
<p>Let’s cover some other important concepts with regards to working with <a id="_idTextAnchor163"/>a PV and PVC before we dive into <em class="italic">how</em>.</p>
<h3>The StorageClass</h3>
<p>The <strong class="source-inline">StorageClass</strong> resource <a id="_idIndexMarker370"/>in Kubernetes classifies the Kubernetes storage <a id="_idIndexMarker371"/>class. As a matter of fact, a <strong class="source-inline">StorageClass</strong> contains a <strong class="source-inline">provisioner</strong>, <strong class="source-inline">parameters</strong>, and <strong class="source-inline">reclaimPolicy</strong> field. </p>
<p>The provisioner represents which CSI volume plugin is being used to provision the PVs. Examples of different provisioners are Azure Disk, AWS EBS, and Glusterfs. You can find a complete list <a id="_idIndexMarker372"/>of supported <strong class="source-inline">StorageClass</strong> resources here: <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">https://kubernetes.io/docs/concepts/storage/storage-classes/</a>.</p>
<p>We need to define the storage class in the PVC and the definition of storage classes includes the provisioner and the reclaim policy. Their relationship is shown in <em class="italic">Figure 5.3</em>: </p>
<div>
<div class="IMG---Figure" id="_idContainer100">
<img alt="Figure 5.4 – A StorageClass resource " height="739" src="image/Figure_5.04_B18201.jpg" width="1582"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.4 – A StorageClass resource</p>
<p>Notice that when the reclaim policy is not specified, it defaults to <strong class="source-inline">Delete</strong>, which means if a user deletes the PVC that is bound to this PV, the PVC itself gets deleted too. You can also set it to <strong class="source-inline">Retain</strong>, which means it will be retained and that you will need to manually delete <a id="_idIndexMarker373"/>the data that resides in it. Another case would <a id="_idIndexMarker374"/>be to set it to <strong class="source-inline">Recycle</strong>. In this case, the PV will be recycled, deprecated, and replaced by dynamic provisioning, which will depend on the provisioner. The DefaultStorageClass admission controller on the Kubernetes API server will also need to be enabled – this is out of the scope of the CKA exam but I think it’s still worth a mention here. </p>
<p>The following is an example <strong class="source-inline">StorageClass</strong> definition, using an Azure Disk-managed disk to define a <strong class="source-inline">StorageClass</strong> resource with a YAML definition: </p>
<pre class="source-code">
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
<strong class="bold">provisioner: kubernetes.io/azure-disk</strong>
parameters:
  storage<a id="_idTextAnchor164"/>accounttype: Standard_LRS
  kind: managed</pre>
<p>Interestingly, despite the fact that local volumes don’t support dynamic provisioning, they can still be created and bound when the pod is scheduled. We can set <strong class="source-inline">volumeBindingMode</strong> to <strong class="source-inline">WaitForFirstConsumer</strong>, which is shown as follows: </p>
<pre class="source-code">
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
commissions: kubernetes.io/no commissions
<strong class="bold">volumeBindingMode: WaitForFirstConsumer</strong></pre>
<p>Learning about <a id="_idIndexMarker375"/>storage class in Kubernetes will help you work <a id="_idIndexMarker376"/>with different storage in real life, going above and beyond what’s required in the current CKA exam. Please feel free to check out the official documentation – it will be updated whenever a new supported storage class is added and will provide useful examples: <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">https://kubernetes.io/docs/concepts/storage/storage-classes/</a>.</p>
<p>Now, let’s take a look at another i<a id="_idTextAnchor165"/>m<a id="_idTextAnchor166"/>portant concept called volume modes next.  </p>
<h3>Volume modes </h3>
<p>Volume modes indicate the type of consumption of the volume – this can either be a filesystem or a <a id="_idIndexMarker377"/>block device. When <strong class="source-inline">volumeMode</strong> is <a id="_idIndexMarker378"/>set to <strong class="source-inline">Filesystem</strong>, it mounts into the pods as a directory. When <strong class="source-inline">volumeMod<a id="_idTextAnchor167"/><a id="_idTextAnchor168"/>e</strong> is set to <strong class="source-inline">Block</strong>, we use it as a raw block. </p>
<h3>Access modes </h3>
<p>When a PV is mounted to a pod, we can specify different access modes. The access modes represent <a id="_idIndexMarker379"/>the way that the data in the storage <a id="_idIndexMarker380"/>resources is being consumed. They can be summarized as shown in the following table: </p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Access modes</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Definition</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Abbreviated</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">ReadWriteOnce</strong></p>
</td>
<td class="No-Table-Style">
<p>The volume can be mounted as read-write by one node.</p>
</td>
<td class="No-Table-Style">
<p><strong class="source-inline">RWO</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">ReadOnlyMany</strong></p>
</td>
<td class="No-Table-Style">
<p>The volume can be mounted as read-only by multiple nodes.</p>
</td>
<td class="No-Table-Style">
<p><strong class="source-inline">ROX</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">ReadWriteMany</strong></p>
</td>
<td class="No-Table-Style">
<p>The volume can be mounted as read-write by multiple nodes.</p>
</td>
<td class="No-Table-Style">
<p><strong class="source-inline">RWX</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">ReadWriteOncePod</strong></p>
</td>
<td class="No-Table-Style">
<p>The volume can be mounted as read-write by one pod. This is a feature supported by Kubernetes, starting from Kubernetes 1.22.</p>
</td>
<td class="No-Table-Style">
<p><strong class="source-inline">RWOP</strong></p>
</td>
</tr>
</tbody>
</table>
<p>To learn more <a id="_idIndexMarker381"/>about access modes, you can find the official documentation here: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes</a>.</p>
<p>Knowing the access modes is important, as they’re used all the time when working with Kubernetes storage. Now, let’s take a look at the PV and PVC next, and see how these concepts work together with Kubernetes. <a id="_idTextAnchor169"/></p>
<h3>A PV</h3>
<p>Let’s first take a look <a id="_idIndexMarker382"/>at how to create a PV. You do so using the following YAML definition: </p>
<pre class="source-code">
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: my-pv
  spec:
    storageClassName: local-storage
    capacity:
      storage: 1Gi
    accessModes:
      - ReadWriteOnce
   </pre>
<p>To learn more <a id="_idIndexMarker383"/>about how PVs work with Kubernetes, check out this <a id="_idIndexMarker384"/>article: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes</a>.</p>
<p>Knowing about PVs on their own is not enough – we need to learn about how PVCs work alongside them within Kubernetes storage, which is what we’ll get into ne<a id="_idTextAnchor170"/>xt.</p>
<h3>PVCs</h3>
<p>One of the most <a id="_idIndexMarker385"/>interesting things about the PVC is that users don’t need to worry about the details of where the storage is located. They only need to know about the <strong class="source-inline">StorageClass</strong> and <strong class="source-inline">accessMode</strong>. PVCs will automatically bind themselves to a PV that has a compatible <strong class="source-inline">StorageClass</strong> and <strong class="source-inline">accessMode</strong>. The following is an example of a PVC: </p>
<pre class="source-code">
 apiVersion: v1
 kind: PersistentVolumeClaim
 metadata:
   name: my-pvc
 spec:
   storageClassName: local-storage
   accessModes:
       - ReadWriteOnce
   resources:
     requests:
        storage: 512Mi</pre>
<p>You can <a id="_idIndexMarker386"/>learn more about the PVC from the official Kubernetes documentation here: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#lifecycle-of-a-volume-and-claim">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#lifecycle-of-a-volume-and-claim</a>.</p>
<p>Once you have a PV and PVC that will define the Kubernetes storage, the next step is to assign the storage to your applications deployed on top of Kubernetes. As we explained, Kubernetes is also capable of dealing with stateful workloads, so we’ll have a look at how to mount storage to a stateful application in Kubern<a id="_idTextAnchor171"/>e<a id="_idTextAnchor172"/>tes. </p>
<h1 id="_idParaDest-112"><a id="_idTextAnchor173"/>Cracking stateful applications in Kubernetes</h1>
<p>In this section, we <a id="_idIndexMarker387"/>will learn about how to work with storage <a id="_idIndexMarker388"/>for stateful applications in Kubernetes. The considerations within this part are often seen as high-value and low-effort in terms of the CKA exam. Make sure you keep practicing them until you feel you know them confidently: </p>
<ul>
<li>Mounting storage to a stateful application</li>
<li>Dynamically provisioning storage to a stateful appli<a id="_idTextAnchor174"/><a id="_idTextAnchor175"/>cation </li>
</ul>
<h2 id="_idParaDest-113"><a id="_idTextAnchor176"/>Configuring an application with mounted storage</h2>
<p>You need to <a id="_idIndexMarker389"/>create a new YAML <a id="_idIndexMarker390"/>definition where you write up the specification of the Kubernetes pod and then set up emptyDir volumes for the pod. Kubernetes creates empty storage on a node after the pod is scheduled to a specific worker node: </p>
<ol>
<li>Check whether you currently have any nodes available to schedule a pod by using the following command: <p class="source-code"><strong class="bold">kubectl get nodes</strong></p></li>
</ol>
<p>Alternatively, you can use the simplified version of the previous command: </p>
<p class="source-code"><strong class="bold">alias k=kubectl</strong></p>
<p class="source-code"><strong class="bold">k get no</strong></p>
<p>If the status of any of your nodes shows <strong class="source-inline">Ready</strong>, as in the following figure, that means you can proceed to the next step:</p>
<div>
<div class="IMG---Figure" id="_idContainer101">
<img alt="Figure 5.5 – Checking the available nodes " height="94" src="image/Figure_5.05_B18201.jpg" width="1004"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.5 – Checking the available nodes</p>
<ol>
<li>Use the <a id="_idIndexMarker391"/>Vim editor to <a id="_idIndexMarker392"/>create a new YAML definition file called <strong class="source-inline">pod-volume.yaml</strong>, and when you enter Vim, press the <em class="italic">Insert</em> key on your keyboard and let the current <strong class="source-inline">edit</strong> mode switch to <strong class="source-inline">INSERT</strong>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer102">
<img alt="Figure 5.6 – Inserting a YAML spec with Vim   " height="738" src="image/Figure_5.06_B18201.jpg" width="1028"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.6 – Inserting a YAML spec with Vim  </p>
<ol>
<li>Then, put the following in the YAML definition:<p class="source-code">apiVersion: v1</p><p class="source-code">kind: Pod</p><p class="source-code">metadata:</p><p class="source-code">  name: my-volume-pod</p><p class="source-code">spec:</p><p class="source-code">  containers:</p><p class="source-code">  - image: busybox</p><p class="source-code">    name: busybox</p><p class="source-code">    command: ["/bin/sh","-c","while true; do sleep 3600;  done"]</p><p class="source-code">    volumeMounts:</p><p class="source-code">    - name: my-volume</p><p class="source-code">      mountPath: /tmp/storage</p><p class="source-code">  volumes:</p><p class="source-code">  - name: my-volume</p><p class="source-code">    emptyDir: {}</p></li>
<li>Then, save <a id="_idIndexMarker393"/>your edits and <a id="_idIndexMarker394"/>quit Vim. Press the <em class="italic">Esc</em> key, type <strong class="source-inline">:wq!</strong> at the bottom of the editor, and then press <em class="italic">Enter</em> to take you back to the terminal: </li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer103">
<img alt="Figure 5.7 – Saving the YAML definition in Vim " height="748" src="image/Figure_5.07_B18201.jpg" width="1028"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.7 – Saving the YAML definition in Vim</p>
<ol>
<li>When <a id="_idIndexMarker395"/>you’re on the <a id="_idIndexMarker396"/>terminal, use the following command to deploy the .<strong class="source-inline">yaml</strong> file: <p class="source-code"><strong class="bold">kubectl apply -f pod-volume.yaml</strong></p></li>
</ol>
<p>Then, it should display a message that the pod has been created successfully, something similar to the following: </p>
<p class="source-code">pod/my-volume-pod created</p>
<p>You can go ahead and check whether the pod is now running by using the <strong class="source-inline">kubectl get pods</strong> command and the command comes back with the following output: </p>
<div>
<div class="IMG---Figure" id="_idContainer104">
<img alt="Figure 5.8 – Checking whether the pod is running " height="92" src="image/Figure_5.08_B18201.jpg" width="796"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.8 – Checking whether the pod is running</p>
<p>Now, you <a id="_idIndexMarker397"/>have deployed a pod with <a id="_idIndexMarker398"/>mounted storage. If you run the following command, you’ll be able to check out further details, including configuration information, resource requirements, the labels of the pods, and events information about this pod and the mounted storage:</p>
<p class="source-code">kubectl describe pod my-volume-pod</p>
<p>The output of this command should be similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer105">
<img alt="Figure 5.9 – Checking the pod configurations and status " height="1311" src="image/Figure_5.09_B18201.jpg" width="1274"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.9 – Checking the pod configurations and status</p>
<p>From the <a id="_idIndexMarker399"/>output, we can see the pod has <a id="_idIndexMarker400"/>been mounted on a volume called <strong class="source-inline">my-volume</strong> just as we specified in the YAML definition. <strong class="source-inline">Type</strong> has been specified as <strong class="source-inline">EmptyDir</strong>, so it’s a temporary directory that shares the pod’s lifecycle. The <a id="_idIndexMarker401"/>bottom of the <a id="_idIndexMarker402"/>screenshot also shows the relevant events when provisi<a id="_idTextAnchor177"/>oning this pod. </p>
<h2 id="_idParaDest-114"><a id="_idTextAnchor178"/>Configuring an application with persistent storage</h2>
<p>In this case, you <a id="_idIndexMarker403"/>need to <a id="_idIndexMarker404"/>create a new YAML definition where you write up the specification of the Kubernetes PV – Kubernetes will assign the storage based on the PVC bound to the PV on a node after the pod has been scheduled to a spec<a id="_idTextAnchor179"/>ific worker node. </p>
<h3>Creating your PV </h3>
<p>You can start by <a id="_idIndexMarker405"/>checking whether you currently have any nodes available to schedule a pod by using <strong class="source-inline">kubectl get nodes</strong> or <strong class="source-inline">kubectl get no</strong>. Make sure that the status of one of your nodes is <strong class="source-inline">Ready</strong>, as in the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer106">
<img alt="Figure 5.10 – Checking the available nodes " height="94" src="image/Figure_5.10_B18201.jpg" width="1004"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.10 – Checking the available nodes</p>
<p>From here, we’re creating a new PV by going through the following steps: </p>
<ol>
<li>Use Vim to write up the following YAML definition called <strong class="source-inline">data-pv.yaml</strong>: <p class="source-code">  apiVersion: v1</p><p class="source-code">  kind: PersistentVolume</p><p class="source-code">  metadata:</p><p class="source-code">    name: data-pv</p><p class="source-code">  spec:</p><p class="source-code">    storageClassName: local-storage</p><p class="source-code">    capacity:</p><p class="source-code">      storage: 1Gi</p><p class="source-code">    accessModes:</p><p class="source-code">      - ReadWriteOnce</p><p class="source-code">    hostPath:</p><p class="source-code">      path: "/mnt/data"</p></li>
<li>When you’re on the terminal, use the following command to deploy the .<strong class="source-inline">yaml</strong> file: <p class="source-code"><strong class="bold">kubectl apply -f data-pv.yaml</strong></p></li>
</ol>
<p>Then, it will display a message that the PV has been created successfully, something similar to the following: </p>
<p class="source-code">persistentvolume/data-pv created</p>
<p>The preceding YAML definition means that there is 1 GB of storage allocated as local storage. You can <a id="_idIndexMarker406"/>define a PVC of 1 G storage bound to that PV. However, in the theoretical case that you had two claims of 500 MB each, the PV could also be split during the allocation process. Under the hood, those two PVCs are bound to the same PV, and from there, they share the amount of storage.</p>
<ol>
<li>Use the following command to check the PV’s status: <p class="source-code"><strong class="bold">kubectl get pv</strong></p></li>
</ol>
<p>You’ll get the following output:</p>
<div>
<div class="IMG---Figure" id="_idContainer107">
<img alt="Figure 5.11 – Checking whether the PV is available " height="88" src="image/Figure_5.11_B18201.jpg" width="1524"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.11 – Checking whether the PV is available</p>
<p>Notice that the status is <strong class="source-inline">available</strong>, meaning that this PV is currently not bound to a PVC and is available to be bound with a new PVC, which we’re about to create <a id="_idTextAnchor180"/>in the next step. </p>
<h3>Creating your PVC</h3>
<p>From here, we’re <a id="_idIndexMarker407"/>creating a new PVC by going through the following steps: </p>
<ol>
<li>Use Vim to write up the following YAML definition called <strong class="source-inline">data-pvc.yaml</strong>:<p class="source-code">apiVersion: v1</p><p class="source-code">kind: PersistentVolumeClaim</p><p class="source-code">metadata:</p><p class="source-code">  name: data-pvc</p><p class="source-code">spec:</p><p class="source-code">  storageClassName: local-storage</p><p class="source-code">  accessModes:</p><p class="source-code">    - ReadWriteOnce</p><p class="source-code">  resources:</p><p class="source-code">    requests:</p><p class="source-code">      storage: 512Mi</p></li>
<li>When you’re on the terminal, use the following command to deploy the <strong class="source-inline">yaml</strong> file: <p class="source-code"><strong class="bold">kubectl apply -f data-pvc.yaml</strong></p></li>
</ol>
<p>The PVC is created successfully and gives an output similar to the following: </p>
<p class="source-code">persistentvolumeclaim/data-pvc created</p>
<ol>
<li>Use the <a id="_idIndexMarker408"/>following command to check the PVC’s status: <p class="source-code"><strong class="bold">kubectl get pvc</strong></p></li>
</ol>
<p>You’ll get the following output:</p>
<div>
<div class="IMG---Figure" id="_idContainer108">
<img alt="Figure 5.12 – Checking the PVC   " height="88" src="image/Figure_5.12_B18201.jpg" width="1080"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.12 – Checking the PVC  </p>
<p>You may notice that the status of this PVC is <strong class="source-inline">Bound</strong>, which means that it is bound to a PV. </p>
<p>To double-check whether it is bound to the PV that you desire, you can use <strong class="source-inline">kubectl get pv command</strong> to check back:</p>
<div>
<div class="IMG---Figure" id="_idContainer109">
<img alt="Figure 5.13 – Check whether the PVC is bound to the PV " height="88" src="image/Figure_5.13_B18201.jpg" width="1550"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.13 – Check whether the PVC is bound to the PV</p>
<p>The preceding <a id="_idIndexMarker409"/>figure shows the <strong class="source-inline">Bound</strong> status of our PV, which means it has been b<a id="_idTextAnchor181"/>ound successfully. </p>
<h3>Configuring the pod to consume the PV</h3>
<p>From here, we’re <a id="_idIndexMarker410"/>configuring the pod to consume the PV by going through the following steps: </p>
<ol>
<li>Use Vim to write up the following YAML definition called <strong class="source-inline">data-pod.yaml</strong> where we’re about to create a pod to consume the targeted PV: <p class="source-code"> apiVersion: v1</p><p class="source-code"> kind: Pod</p><p class="source-code"> metadata:</p><p class="source-code">   name: data-pod</p><p class="source-code"> spec:</p><p class="source-code">   containers:</p><p class="source-code">     - name: busybox</p><p class="source-code">       image: busybox</p><p class="source-code">       command: ["/bin/sh", "-c","while true; do sleep 3600;  done"]</p><p class="source-code">       volumeMounts:</p><p class="source-code">       - name: temp-data</p><p class="source-code">         mountPath: /tmp/data</p><p class="source-code">   volumes:</p><p class="source-code">     - name: temp-data</p><p class="source-code">       persistentVolumeClaim:</p><p class="source-code">         claimName: data-pvc</p><p class="source-code">   restartPolicy: Always</p></li>
<li>2. When you’re on the terminal, use the following command to deploy the <strong class="source-inline">yaml</strong> file: <p class="source-code"><strong class="bold">kubectl apply -f data-pod.yaml</strong></p></li>
</ol>
<p>The pod is successfully created with an output similar to the following: </p>
<p class="source-code">pod/data-pod created</p>
<p>You can use the <strong class="source-inline">kubectl get pods</strong> command to verify whether your pod is up and running. If you <a id="_idIndexMarker411"/>want your command to watch the status of the pod, you can use the <strong class="source-inline">-w</strong> flag in your command; it should look as follows:</p>
<p class="source-code">kubectl get pods -w</p>
<p>The output would look as follows: </p>
<div>
<div class="IMG---Figure" id="_idContainer110">
<img alt="Figure 5.14 – Checking whether the pod is up and running   " height="174" src="image/Figure_5.14_B18201.jpg" width="838"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.14 – Checking whether the pod is up and running  </p>
<p>You can use the following command to check out further details, including configuration information, resource requirements, labels of the pods, and event information about this pod and <a id="_idIndexMarker412"/>the dynamically allocated storage:</p>
<p class="source-code">kubectl describe pod data-pod</p>
<p>The output of this command should be similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer111">
<img alt="Figure 5.15 – Checking the pod’s detailed configuration and events " height="1692" src="image/Figure_5.15_B18201.jpg" width="1645"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 5.15 – Checking the pod’s detailed configuration and events</p>
<p>From the output, we can see the pod has been dynamically attached to persistent storage called <strong class="source-inline">temp-data</strong>, which was expected, as we defined it in the YAML definition. The bottom of the screenshot also shows the relevant events while provisioning this pod. </p>
<p>The preceding is an example of using a PVC as a volume – this allows pods to access storage by <a id="_idIndexMarker413"/>using the claim as a volume. In that case, the claim must exist in the same namespace in which the pods will be using them. </p>
<p>We also noticed that, in some cases, people use <strong class="source-inline">hostPath</strong> to mount volumes, which simply allocates local storage of that node of the cluster so that the pod consumes the storage where the pod lives. </p>
<p class="callout-heading">Important note</p>
<p class="callout"><strong class="source-inline">hostPath</strong> also easily causes security issues, so we should avoid using it as much as possible. While using it, we can <a id="_idIndexMarker414"/>specify <strong class="source-inline">volumeMounts</strong> as <strong class="source-inline">ReadOnly</strong> and only make it available to a specific file or repository. </p>
<p>The following is an example of this:</p>
<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
  name: my-pv
  namespace: default
spec:
  restartPolicy: Never
  volumes:
  - name: vol
<strong class="bold">    hostPath:</strong>
<strong class="bold">      path: /tmp/data</strong>
  containers:
  - name: my-pv-hostpath
    image: "busybox"
    command: ["/bin/sh", "-c","while true; do sleep 3600;  done"]
    volumeMounts:
    - name: vol
      mountPath: /scrub</pre>
<p>Note that <strong class="source-inline">hostPath</strong> works for <a id="_idIndexMarker415"/>a single node only, and if you’re on a multi-node cluster, a local volume is the way to go. You can <a id="_idIndexMarker416"/>find more details about local storage at <a href="https://kubernetes.io/docs/concepts/storage/volumes/#local">https://kubernetes.io/docs/concepts<span id="_idTextAnchor182"/>/storage/volumes/#local</a>.</p>
<h1 id="_idParaDest-115"><a id="_idTextAnchor183"/>Summary</h1>
<p>This chapter covers one of the highest-value topics in the CKA exam, which is Kubernetes storage. Over the last three years, the CKA exam has raised more and more attention toward Kubernetes storage, where it previously only scratched the surface and now focuses on various use cases of the stateful application deployment. Learning this part may not seem the most crucial for Kubernetes administrators at the moment, but it will take off more quickly once we have more and more cloud-native databases adopted by enterprise-grade customers. Having a solid knowledge of storage will add value to your existing Kubernetes administration skills. If you can confidently play with the exercises in this chapter, it will increase your success rate in the actual CKA exam, as storage-related questions are usually simpler but higher value compared to other cluster maintenance task-related questions in the previous chapters. </p>
<p>In the next chapter, <em class="italic">Securing Kubernetes</em>, we will dive into some important Kubernetes security concepts, which will help you not only set up a solid foundation for the CKA exam but also potentially help you for the <strong class="bold">Certified Kubernetes Security Specialist</strong> (<strong class="bold">CKS</strong>) exam in <a id="_idTextAnchor184"/>the future – stay tuned!</p>
<h1 id="_idParaDest-116"><a id="_idTextAnchor185"/>Mock CKA scenario-based practice test </h1>
<p>You have two virtual machines, <strong class="source-inline">master-0</strong> and <strong class="source-inline">worker-0</strong>. Please complete the fo<a id="_idTextAnchor186"/>llowing mock scenarios. </p>
<h2 id="_idParaDest-117"><a id="_idTextAnchor187"/>Scenario 1</h2>
<p>Create a new PV called <strong class="source-inline">packt-data-pv</strong> to store 2 GB, and two PVCs each requesting<a id="_idTextAnchor188"/> 1 GB of local storage. </p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor189"/>Scenario 2</h2>
<p>Provision a new pod called <strong class="source-inline">pack-storage-pod</strong> and assign an available PV to this Pod.</p>
<p>You can find all the scenario resolutions in <a id="_idTextAnchor190"/><a href="B18201_Appendix_A.xhtml#_idTextAnchor386"><em class="italic">Appendix</em></a><em class="italic"> - Mock CKA scenario-based practice test resolutions</em> of this book.</p>
<h1 id="_idParaDest-119"><a id="_idTextAnchor191"/>FAQs</h1>
<ul>
<li><em class="italic">Where can I find the latest updates about the supported CSI drivers while working with Kubernetes? </em></li>
</ul>
<p>The Kubernetes CSI <strong class="bold">Special Interest Group</strong> (<strong class="bold">SIG</strong>) has a GitHub-based documentation website where you can find all the latest drivers, with tutorials from their main page: <a href="https://kubernetes-csi.github.io/docs">https://kubernetes-csi.github.io/docs</a>. More specifically, you can find all available supported CSI drivers at the following link: <a href="https://kubernetes-csi.github.io/docs/drivers.xhtml">https://kubernetes-csi.github.io/docs/drivers.xhtml</a>.</p>
<ul>
<li><em class="italic">What is the recommended official Kubernetes article to refer to for configuring ephemeral storage?</em></li>
</ul>
<p>I recommend bookmarking the official documentation about ephemeral volumes: <a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/">https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/</a>.</p>
<ul>
<li><em class="italic">What is the recommended official Kubernetes article to refer to for configuring persistent storage?</em></li>
</ul>
<p>I recommend bookmarking this article, <em class="italic">Configure a Pod to Use a Persistent Volume for Storage</em>, where you can find all the key steps and processes: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/</a>.</p>
</div>
</div></body></html>