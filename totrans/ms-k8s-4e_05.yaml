- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Kubernetes Resources in Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will design a fictional massive-scale platform that will
    challenge Kubernetes’ capabilities and scalability. The Hue platform is all about
    creating an omniscient and omnipotent digital assistant. Hue is a digital extension
    of you. Hue will help you do anything, find anything, and, in many cases, will
    do a lot on your behalf. It will obviously need to store a lot of information,
    integrate with many external services, respond to notifications and events, and
    be smart about interacting with you.
  prefs: []
  type: TYPE_NORMAL
- en: We will take the opportunity in this chapter to get to know kubectl and related
    tools a little better and explore in detail familiar resources we’ve seen before,
    such as pods, as well as new resources, such as jobs. We will explore advanced
    scheduling and resource management.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing the Hue platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Kubernetes to build the Hue platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separating internal and external services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced scheduling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using namespaces to limit access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Kustomization for hierarchical cluster structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launching jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixing non-cluster components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing the Hue platform with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolving the Hue platform with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just to be clear, this is a design exercise! We are not actually going to build
    the Hue platform. The motivation behind this exercise is to showcase the vast
    range of capabilities available with Kubernetes in the context of a large system
    with multiple moving parts.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, you will have a clear picture of how impressive
    Kubernetes is and how it can be used as the foundation for hugely complex systems.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the Hue platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will set the stage and define the scope of the amazing Hue
    platform. Hue is not Big Brother; Hue is Little Brother! Hue will do whatever
    you allow it to do. Hue will be able to do a lot, which might concern some people,
    but you get to pick how much or how little Hue can help you with. Get ready for
    a wild ride!
  prefs: []
  type: TYPE_NORMAL
- en: Defining the scope of Hue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hue will manage your digital persona. It will know you better than you know
    yourself. Here is a list of some of the services Hue can manage and help you with:'
  prefs: []
  type: TYPE_NORMAL
- en: Search and content aggregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical – electronic heath records, DNA sequencing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smart homes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finance – banking, savings, retirement, investing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Office
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Travel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wellbeing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Family
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at some of the capabilities of the Hue platform, such as smart reminders
    and notifications, security, identity, and privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Smart reminders and notifications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s think of the possibilities. Hue will know you, but also know your friends
    and the aggregate of other users across all domains. Hue will update its models
    in real time. It will not be confused by stale data. It will act on your behalf,
    present relevant information, and learn your preferences continuously. It can
    recommend new shows or books that you may like, make restaurant reservations based
    on your schedule and that of your family or friends, and control your house’s
    automation.
  prefs: []
  type: TYPE_NORMAL
- en: Security, identity, and privacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hue is your proxy online. The ramifications of someone stealing your Hue identity,
    or even just eavesdropping on your Hue interactions, are devastating. Potential
    users may even be reluctant to trust the Hue organization with their identity.
    Let’s devise a non-trust system where users have the power to pull the plug on
    Hue at any time. Here are a few ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: Strong identity via a dedicated device with multi-factor authorization, including
    multiple biometric factors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequently rotating credentials
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quick service pause and identity verification of all external services (will
    require original proof of identity for each provider)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hue backend will interact with all external services via short-lived tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecting Hue as a collection of loosely coupled microservices with strong
    compartmentalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GDPR compliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End-to-end encryption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid owning critical data (let external providers manage it)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hue’s architecture will need to support enormous variation and flexibility.
    It will also need to be very extensible where existing capabilities and external
    services are constantly upgraded, and new capabilities and external services are
    integrated into the platform. That level of scale calls for microservices, where
    each capability or service is totally independent of other services except for
    well-defined interfaces via standard and/or discoverable APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Hue components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before embarking on our microservice journey, let’s review the types of components
    we need to construct for Hue.
  prefs: []
  type: TYPE_NORMAL
- en: User profile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The user profile is a major component, with lots of sub-components. It is the
    essence of the user, their preferences, their history across every area, and everything
    that Hue knows about them. The benefit you can get from Hue is affected strongly
    by the richness of the profile. But the more information is managed by the profile,
    the more damage you can suffer if the data (or part of it) is compromised.
  prefs: []
  type: TYPE_NORMAL
- en: A big piece of managing the user profile is the reports and insights that Hue
    will provide to the user. Hue will employ sophisticated machine learning to better
    understand the user and their interactions with other users and external service
    providers.
  prefs: []
  type: TYPE_NORMAL
- en: User graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The user graph component models networks of interactions between users across
    multiple domains. Each user participates in multiple networks: social networks
    such as Facebook, Instagram, and Twitter; professional networks; hobby networks;
    and volunteer communities. Some of these networks are ad hoc and Hue will be able
    to structure them to benefit users. Hue can take advantage of the rich profiles
    it has of user connections to improve interactions even without exposing private
    information.'
  prefs: []
  type: TYPE_NORMAL
- en: Identity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Identity management is critical, as mentioned previously, so it merits a separate
    component. A user may prefer to manage multiple mutually exclusive profiles with
    separate identities. For example, maybe users are not comfortable with mixing
    their health profile with their social profile at the risk of inadvertently exposing
    personal health information to their friends. While Hue can find useful connections
    for you, you may prefer to trade off capabilities for more privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Authorizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The authorizer is a critical component where the user explicitly authorizes
    Hue to perform certain actions or collect various data on their behalf. This involves
    access to physical devices, accounts of external services, and levels of initiative.
  prefs: []
  type: TYPE_NORMAL
- en: External services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hue is an aggregator of external services. It is not designed to replace your
    bank, your health provider, or your social network. It will keep a lot of metadata
    about your activities, but the content will remain with your external services.
    Each external service will require a dedicated component to interact with the
    external service API and policies. When no API is available, Hue emulates the
    user by automating the browser or native apps.
  prefs: []
  type: TYPE_NORMAL
- en: Generic sensor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A big part of Hue’s value proposition is to act on the user’s behalf. In order
    to do that effectively, Hue needs to be aware of various events. For example,
    if Hue reserved a vacation for you but it senses that a cheaper flight is available,
    it can either automatically change your flight or ask you for confirmation. There
    is an infinite number of things to sense. To reign in sensing, a generic sensor
    is needed. The generic sensor will be extensible but exposes a generic interface
    that the other parts of Hue can utilize uniformly even as more and more sensors
    are added.
  prefs: []
  type: TYPE_NORMAL
- en: Generic actuator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the counterpart of the generic sensor. Hue needs to perform actions
    on your behalf; for example, reserving a flight or a doctor’s appointment. To
    do that, Hue needs a generic actuator that can be extended to support particular
    functions but can interact with other components, such as the identity manager
    and the authorizer, in a uniform fashion.
  prefs: []
  type: TYPE_NORMAL
- en: User learner
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the brain of Hue. It will constantly monitor all your interactions (that
    you authorize) and update its model of you and other users in your networks. This
    will allow Hue to become more and more useful over time, predict what you need
    and what will interest you, provide better choices, surface more relevant information
    at the right time, and avoid being annoying and overbearing.
  prefs: []
  type: TYPE_NORMAL
- en: Hue microservices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The complexity of each of the components is enormous. Some of the components,
    such as the external service, the generic sensor, and the generic actuator, will
    need to operate across hundreds, thousands, or even more external services that
    constantly change outside the control of Hue. Even the user learner needs to learn
    the user’s preferences across many areas and domains. Microservices address this
    need by allowing Hue to evolve gradually and grow more isolated capabilities without
    collapsing under its own complexity. Each microservice interacts with generic
    Hue infrastructure services through standard interfaces and, optionally, with
    a few other services through well-defined and versioned interfaces. The surface
    area of each microservice is manageable and the orchestration between microservices
    is based on standard best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Plugins
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Plugins are the key to extending Hue without a proliferation of interfaces.
    The thing about plugins is that often, you need plugin chains that cross multiple
    abstraction layers. For example, if you want to add a new integration for Hue
    with YouTube, then you can collect a lot of YouTube-specific information – your
    channels, favorite videos, recommendations, and videos you have watched. To display
    this information to users and allow them to act on it, you need plugins across
    multiple components and, eventually, in the user interface as well. Smart design
    will help by aggregating categories of actions such as recommendations, selections,
    and delayed notifications to many services.
  prefs: []
  type: TYPE_NORMAL
- en: The great thing about plugins is that they can be developed by anyone. Initially,
    the Hue development team will have to develop the plugins, but as Hue becomes
    more popular, external services will want to integrate with Hue and build Hue
    plugins to enable their service. That will lead, of course, to a whole ecosystem
    of plugin registration, approval, and curation.
  prefs: []
  type: TYPE_NORMAL
- en: Data stores
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hue will need several types of data stores, and multiple instances of each
    type, to manage its data and metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: Relational database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time-series database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-memory caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blob storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the scope of Hue, each one of these databases will have to be clustered,
    scalable, and distributed. In addition, Hue will use local storage on edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: Stateless microservices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The microservices should be mostly stateless. This will allow specific instances
    to be started and killed quickly and migrated across the infrastructure as necessary.
    The state will be managed by the stores and accessed by the microservices with
    short-lived access tokens. Hue will store frequently accessed data in easily hydrated
    fast caches when appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A big part of Hue’s functionality per user will involve relatively short interactions
    with external services or other Hue services. For those activities, it may not
    be necessary to run a full-fledged persistent microservice that needs to be scaled
    and managed. A more appropriate solution may be to use a serverless function that
    is more lightweight.
  prefs: []
  type: TYPE_NORMAL
- en: Event-driven interactions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All these microservices need to talk to each other. Users will ask Hue to perform
    tasks on their behalf. External services will notify Hue of various events. Queues
    coupled with stateless microservices provide the perfect solution.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple instances of each microservice will listen to various queues and respond
    when relevant events or requests are popped from the queue. Serverless functions
    may be triggered as a result of particular events too. This arrangement is very
    robust and easy to scale. Every component can be redundant and highly available.
    While each component is fallible, the system is very fault-tolerant.
  prefs: []
  type: TYPE_NORMAL
- en: A queue can be used for asynchronous RPC or request-response style interactions
    too, where the calling instance provides a private queue name and the response
    is posted to the private queue.
  prefs: []
  type: TYPE_NORMAL
- en: That said, sometimes direct service-to-service interaction (or serverless function-to-service
    interaction) through a well-defined interface makes more sense and simplifies
    the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Planning workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hue often needs to support workflows. A typical workflow will take a high-level
    task, such as making a dentist appointment. It will extract the user’s dentist’s
    details and schedule, match it with the user’s schedule, choose between multiple
    options, potentially confirm with the user, make the appointment, and set up a
    reminder. We can classify workflows into fully automatic workflows and human workflows
    where humans are involved. Then there are workflows that involve spending money
    and might require an additional level of approval.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic workflows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatic workflows don’t require human intervention. Hue has full authority
    to execute all the steps from start to finish. The more autonomy the user allocates
    to Hue, the more effective it will be. The user will be able to view and audit
    all workflows, past and present.
  prefs: []
  type: TYPE_NORMAL
- en: Human workflows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Human workflows require interaction with a human. Most often it will be the
    user that needs to make a choice from multiple options or approve an action. But
    it may involve a person on another service. For example, to make an appointment
    with a dentist, Hue may have to get a list of available times from the secretary.
    In the future, Hue will be able to handle conversations with humans and possibly
    automate some of these workflows too.
  prefs: []
  type: TYPE_NORMAL
- en: Budget-aware workflows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some workflows, such as paying bills or purchasing a gift, require spending
    money. While, in theory, Hue can be granted unlimited access to the user’s bank
    account, most users will probably be more comfortable setting budgets for different
    workflows or just making spending a human-approved activity. Potentially, the
    user could grant Hue access to a dedicated account or set of accounts and, based
    on reminders and reports, allocate more or fewer funds to Hue as needed.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have covered a lot of ground and looked at the different components
    that comprise the Hue platform and its design. Now is a good time to see how Kubernetes
    can help with building a platform like Hue.
  prefs: []
  type: TYPE_NORMAL
- en: Using Kubernetes to build the Hue platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at various Kubernetes resources and how they
    can help us build Hue. First, we’ll get to know the versatile kubectl a little
    better, then we will look at how to run long-running processes in Kubernetes,
    exposing services internally and externally, using namespaces to limit access,
    launching ad hoc jobs, and mixing in non-cluster components. Obviously, Hue is
    a huge project, so we will demonstrate the ideas on a local cluster and not actually
    build a real Hue Kubernetes cluster. Consider it primarily a thought experiment.
    If you wish to explore building a real microservice-based distributed system on
    Kubernetes, check out *Hands-On Microservices with Kubernetes*: [https://www.packtpub.com/product/hands-on-microservices-with-kubernetes/9781789805468](https://www.packtpub.com/product/hands-on-microservices-with-kubernetes/9781789805468).'
  prefs: []
  type: TYPE_NORMAL
- en: Using kubectl effectively
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'kubectl is your Swiss Army knife. It can do pretty much anything around a cluster.
    Under the hood, kubectl connects to your cluster via the API. It reads your `~/.kube/config`
    file (by default, this can be overridden with the `KUBECONFIG` environment variable
    or the `--kubeconfig` command-line argument), which contains the information necessary
    to connect to your cluster or clusters. The commands are divided into multiple
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generic commands**: Deal with resources in a generic way: `create`, `get`,
    `delete`, `run`, `apply`, `patch`, `replace`, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster management commands**: Deal with nodes and the cluster at large:
    `cluster-info`, `certificate`, `drain`, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Troubleshooting commands**: `describe`, `logs`, `attach`, `exec`, and so
    on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment commands**: Deal with deployment and scaling: `rollout`, `scale`,
    `auto-scale`, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Settings commands**: Deal with labels and annotations: `label`, `annotate`,
    and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Misc commands**: `help`, `config`, and `version`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customization commands**: Integrate the kustomize.io capabilities into kubectl'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configuration commands:** Deal with contexts, switch between clusters and
    namespaces, set current context and namespace, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can view the configuration with Kubernetes’ `config view` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the configuration for my local KinD cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Your `kubeconfig` file may or may not be similar to the code sample above, but
    as long as it points to a running Kubernetes cluster, you will be able to follow
    along. Let’s take an in-depth look into the kubectl manifest files.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding kubectl manifest files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many kubectl operations, such as `create`, require a complicated hierarchical
    structure (since the API requires this structure). kubectl uses YAML or JSON manifest
    files. YAML is more concise and human-readable so we will use YAML mostly. Here
    is a YAML manifest file for creating a pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let’s examine the various fields of the manifest.
  prefs: []
  type: TYPE_NORMAL
- en: apiVersion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The very important Kubernetes API keeps evolving and can support different versions
    of the same resource via different versions of the API.
  prefs: []
  type: TYPE_NORMAL
- en: kind
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`kind` tells Kubernetes what type of resource it is dealing with; in this case,
    `Pod`. This is always required.'
  prefs: []
  type: TYPE_NORMAL
- en: metadata
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`metadata` contains a lot of information that describes the pod and where it
    operates:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: Identifies the pod uniquely within its namespace'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels`: Multiple labels can be applied'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`namespace`: The namespace the pod belongs to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`annotations`: A list of annotations available for query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: spec
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`spec` is a pod template that contains all the information necessary to launch
    a pod. It can be quite elaborate, so we’ll explore it in multiple parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Container spec
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The pod spec’s `containers` section is a list of container specs. Each container
    spec has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Each container has an `image`, a command that, if specified, replaces the Docker
    image command. It also has arguments and environment variables. Then, there are
    of course the image pull policy, ports, and resource limits. We covered those
    in earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to explore the pod resource, or other Kubernetes resources, further,
    then the following command can be very useful: `kubectl explain`.'
  prefs: []
  type: TYPE_NORMAL
- en: It can explore resources as well as specific sub-resources and fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'Try the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Deploying long-running microservices in pods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Long-running microservices should run in pods and be stateless. Let’s look at
    how to create pods for one of Hue’s microservices – the Hue learner – which is
    responsible for learning the user’s preferences across different domains. Later,
    we will raise the level of abstraction and use a deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Creating pods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s start with a regular pod configuration file for creating a Hue learner
    internal service. This service doesn’t need to be exposed as a public service
    and it will listen to a queue for notifications and store its insights in some
    persistent storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need a simple container that will run in the pod. Here is possibly the simplest
    Docker file ever, which will simulate the Hue learner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It uses the `busybox` base image, prints to standard output `Started...`, and
    then goes into an infinite loop, which is, by all accounts, long-running.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have built two Docker images tagged as `g1g1/hue-learn:0.3` and `g1g1/hue-learn:0.4`
    and pushed them to the Docker Hub registry (`g1g1` is my username):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now these images are available to be pulled into containers inside of Hue’s
    pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use YAML here because it’s more concise and human-readable. Here are
    the boilerplate and metadata labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next comes the important `containers` spec, which defines for each container
    the mandatory name and image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `resources` section tells Kubernetes the resource requirements of the container,
    which allows for more efficient and compact scheduling and allocations. Here,
    the container requests 200 milli-cpu units (0.2 core) and 256 MiB (2 to the power
    of 28 bytes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The environment section allows the cluster administrator to provide environment
    variables that will be available to the container. Here it tells it to discover
    the queue and the store via DNS. In a testing environment, it may use a different
    discovery method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Decorating pods with labels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Labeling pods wisely is key for flexible operations. It lets you evolve your
    cluster live, organize your microservices into groups you can operate on uniformly,
    and drill down on the fly to observe different subsets.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, our Hue learner pod has the following labels (and a few others):'
  prefs: []
  type: TYPE_NORMAL
- en: '`runtime-environment : production`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tier : internal-service`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `runtime-environment` label allows performing global operations on all pods
    that belong to a certain environment. The `tier` label can be used to query all
    pods that belong to a particular tier. These are just examples; your imagination
    is the limit here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to list the labels with the `get pods` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if you want to filter and list only the **kube-dns** pods, type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Deploying long-running processes with deployments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a large-scale system, pods should never be just created and let loose. If
    a pod dies unexpectedly for whatever reason, you want another one to replace it
    to maintain overall capacity. You can create replication controllers or replica
    sets yourself, but that leaves the door open to mistakes, as well as the possibility
    of partial failure. It makes much more sense to specify how many replicas you
    want when you launch your pods in a declarative manner. This is what Kubernetes
    deployments are for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s deploy three instances of our Hue learner microservice with a Kubernetes
    deployment resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The pod spec is identical to the `spec` section from the pod configuration file
    previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create the deployment and check its status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can get a lot more information about the deployment using the `kubectl
    describe` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Updating a deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Hue platform is a large and ever-evolving system. You need to upgrade constantly.
    Deployments can be updated to roll out updates in a painless manner. You change
    the pod template to trigger a rolling update fully managed by Kubernetes. Currently,
    all the pods are running with version 0.3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s update the deployment to upgrade to version 0.4\. Modify the image version
    in the deployment file. Don’t modify labels; it will cause an error. Save it to
    `hue-learn-deployment-0.4.yaml`. Then we can use the `kubectl apply` command to
    upgrade the version and verify that the pods now run 0.4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note that new pods are created and the original 0.3 pods are terminated in a
    rolling update manner.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We’ve covered how kubectl manifest files are structured and how they can be
    applied to deploy and update workloads on our cluster. Let’s see how these workloads
    can discover and call each other via internal services as well as be called from
    outside the cluster via externally exposed services.
  prefs: []
  type: TYPE_NORMAL
- en: Separating internal and external services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Internal services are services that are accessed directly only by other services
    or jobs in the cluster (or administrators that log in and run ad hoc tools). There
    are also workloads that are not accessed at all. These workloads may watch for
    some events and perform their function without exposing any API.
  prefs: []
  type: TYPE_NORMAL
- en: 'But some services need to be exposed to users or external programs. Let’s look
    at a fake Hue service that manages a list of reminders for a user. It doesn’t
    really do much – just returns a fixed list of reminders – but we’ll use it to
    illustrate how to expose services. I already pushed a `hue-reminders` image to
    Docker Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Deploying an internal service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is the deployment, which is very similar to the `hue-learner` deployment,
    except that I dropped the annotations, `env`, and `resources` sections, kept just
    one or two labels to save space, and added a `ports` section to the container.
    That’s crucial because a service must expose a port through which other services
    can access it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the deployment, two `hue-reminders` pods are added to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'OK. The pods are running. In theory, other services can look up or be configured
    with their internal IP address and just access them directly because they are
    all in the same network address space. But this doesn’t scale. Every time a reminder’s
    pod dies and is replaced by a new one, or when we just scale up the number of
    pods, all the services that access these pods must know about it. Kubernetes services
    solve this issue by providing a single stable access point to all the pods that
    share a set of selector labels. Here is the service definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The service has a `selector` that determines the backing pods by their matching
    labels. It also exposes a port, which other services will use to access it. It
    doesn’t have to be the same port as the container’s port. You can define a `targetPort`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `protocol` field can be one of the following: k’TCP, UDP, or (since Kubernetes
    1.12) SCTP.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating the hue-reminders service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create the service and explore it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The service is up and running. Other pods can find it through environment variables
    or DNS. The environment variables for all services are set at pod creation time.
    That means that if a pod is already running when you create your service, you’ll
    have to kill it and let Kubernetes recreate it with the environment variables
    for the new service.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the pod `hue-learn-68d74fd4b7-bxxnm` was created before the `hue-reminders`
    service was created, so it doesn’t have the environment variable for `HUE_REMINDERS_SERVICE`.
    Printing the environment for the pod shows the environment variable doesn’t exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s kill the pod and, when a new pod replaces it, let’s try again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check the `hue-learn` pods again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Great. We have a new fresh pod – `hue-learn-68d74fd4b7-rw4qr`. Let’s see if
    it has the environment variable for the `HUE_REMINDERS_SERVICE` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Yes, it does! But using DNS is much simpler. Kubernetes assigns an internal
    DNS name to every service. The service DNS name is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, every pod in the cluster can access the `hue-reminders` service through
    its service endpoint and port `8080`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Yes, at the moment `hue-reminders` always returns the same two reminders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This is for demonstration purposes only. If `hue-reminders` was a real system
    it would return live and dynamic reminders.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered internal services and how to access them, let’s look
    at external services.
  prefs: []
  type: TYPE_NORMAL
- en: Exposing a service externally
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The service is accessible inside the cluster. If you want to expose it to the
    world, Kubernetes provides several ways to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: Configure `NodePort` for direct access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure a cloud load balancer if you run it in a cloud environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure your own load balancer if you run on bare metal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before you configure a service for external access, you should make sure it
    is secure. We’ve already covered the principles of this in *Chapter 4*, *Securing
    Kubernetes*. The Kubernetes documentation has a good example that covers all the
    gory details here: [https://github.com/kubernetes/examples/blob/master/staging/https-nginx/README.md](https://github.com/kubernetes/examples/blob/master/staging/https-nginx/README.md).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the `spec` section of the `hue-reminders` service when exposed to the
    world through `NodePort`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The main downside of exposing services though `NodePort` is that the port numbers
    are shared across all services. You must coordinate them globally across your
    entire cluster to avoid conflicts. This is not trivial at scale for large clusters
    with lots of developers deploying services.
  prefs: []
  type: TYPE_NORMAL
- en: But there are other reasons that you may want to avoid exposing a Kubernetes
    service directly, such as security and lack of abstraction, and you may prefer
    to use an Ingress resource in front of the service.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ingress is a Kubernetes configuration object that lets you expose a service
    to the outside world and takes care of a lot of details. It can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide an externally visible URL to your service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balance traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terminate SSL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide name-based virtual hosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To use Ingress, you must have an Ingress controller running in your cluster.
    Ingress was introduced in Kubernetes 1.1, and became stable in Kubernetes 1.19\.
    One of the current limitations of the Ingress controller is that it isn’t built
    for scale. As such, it is not a good option for the Hue platform yet. We’ll cover
    the Ingress controller in greater detail in *Chapter 10*, *Exploring Kubernetes
    Networking*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what an Ingress resource looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Note the annotation, which hints that it is an Ingress object that works with
    the Nginx Ingress controller. There are many other Ingress controllers and they
    typically use annotations to encode information they need that is not captured
    by the Ingress object itself and its rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other Ingress controllers include:'
  prefs: []
  type: TYPE_NORMAL
- en: Traefik
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gloo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contour
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS ALB Ingress controller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HAProxy Ingress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voyager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An `IngressClass` resource can be created and specified in the `Ingress` resource.
    If it is not specified, then the default `IngressClass` is used.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at how the different components of the Hue platform
    can discover and talk to each other via services as well as exposing public-facing
    services to the outside world. In the next section, we will look at how Hue workloads
    can be scheduled efficiently and cost-effectively on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced scheduling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the strongest suits of Kubernetes is its powerful yet flexible scheduler.
    The job of the scheduler, put simply, is to choose nodes to run newly created
    pods. In theory, the scheduler could even move existing pods around between nodes,
    but in practice, it doesn’t do that at the moment and instead leaves this functionality
    for other components.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the scheduler follows several guiding principles, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Split pods from the same replica set or stateful set across nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schedule pods to nodes that have enough resources to satisfy the pod requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balance out the overall resource utilization of nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is pretty good default behavior, but sometimes you may want better control
    over specific pod placement. Kubernetes 1.6 introduced several advanced scheduling
    options that give you fine-grained control over which pods are scheduled or not
    scheduled on which nodes as well as which pods are to be scheduled together or
    separately.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review these mechanisms in the context of Hue.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s create a k3d cluster with two worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at the various ways that pods can be scheduled to nodes and when
    each method is appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Node selector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The node selector is pretty simple. A pod can specify which nodes it wants
    to be scheduled on in its spec. For example, the `trouble-shooter` pod has a `nodeSelector`
    that specifies the `kubernetes.io/hostname` label of the `worker-2` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'When creating this pod, it is indeed scheduled to the `k3d-k3s-default-agent-1`
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Taints and tolerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can taint a node in order to prevent pods from being scheduled on the node.
    This can be useful, for example, if you don’t want pods to be scheduled on your
    control plane nodes. Tolerations allow pods to declare that they can “tolerate”
    a specific node taint and then these pods can be scheduled on the tainted node.
    A node can have multiple taints and a pod can have multiple tolerations. A taint
    is a triplet: key, value, effect. The key and value are used to identify the taint.
    The effect is one of:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NoSchedule` (no pods will be scheduled to the node unless they tolerate the
    taint)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PreferNoSchedule` (soft version of `NoSchedule`; the scheduler will attempt
    to not schedule pods that don’t tolerate the taint)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NoExecute` (no new pods will be scheduled, but also existing pods that don’t
    tolerate the taint will be evicted)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s deploy `hue-learn` and `hue-reminders` on our k3d cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Currently, there is a `hue-learn` pod that runs on the control plane node (`k3d-k3s-default-server-0`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s taint our control plane node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now review the taint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Yeah, it worked! There are now no pods scheduled on the master node. The `hue-learn`
    pod on `k3d-k3s-default-server-0` was evicted and a new pod (`hue-learn-67d4649b58-bl8cn`)
    is now running on `k3d-k3s-default-agent-0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'To allow pods to tolerate the taint, add a toleration to their spec such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Node affinity and anti-affinity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Node affinity is a more sophisticated form of the `nodeSelector`. It has three
    main advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Rich selection criteria (`nodeSelector` is just `AND` of exact matches on the
    labels)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rules can be soft
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can achieve anti-affinity using operators like `NotIn` and `DoesNotExist`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that if you specify both `nodeSelector` and `nodeAffinity`, then the pod
    will be scheduled only to a node that satisfies both requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we add the following section to our `trouble-shooter` pod,
    it will not be able to run on any node because it conflicts with the `nodeSelector`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Pod affinity and anti-affinity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pod affinity and anti-affinity provide yet another avenue for managing where
    your workloads run. All the methods we discussed so far – node selectors, taints/tolerations,
    node affinity/anti-affinity – were about assigning pods to nodes. But pod affinity
    is about the relationships between different pods. Pod affinity has several other
    concepts associated with it: namespacing (since pods are namespaced), topology
    zone (node, rack, cloud provider zone, cloud provider region), and weight (for
    preferred scheduling). A simple example is if you want `hue-reminders` to always
    be scheduled with a `trouble-shooter` pod. Let’s see how to define it in the pod
    template spec of the `hue-reminders` deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The topology key is a node label that Kubernetes will treat as identical for
    scheduling purposes. On cloud providers, it is recommended to use `topology.kubernetes.io/zone`
    when workloads should run in proximity to each other. In the cloud, a zone is
    the equivalent of a data center.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, after re-deploying `hue-reminders`, all the `hue-reminders` pods are
    scheduled to run on `k3d-k3s-default-agent-1` next to the `trouble-shooter` pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Pod topology spread constraints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Node affinity/anti-affinity and pod affinity/anti-affinity are sometimes too
    strict. You may want to spread your pods – it’s okay if some pods of the same
    deployment end up on the same node. Pod topology spread constraints give you this
    flexibility. You can specify the max skew, which is how far you can be from the
    optimal spread, as well as the behavior when the constraint can’t be satisfied
    (`DoNotSchedule` or `ScheduleAnyway`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our `hue-reminders` deployment with pod topology spread constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that after applying the manifest, the three pods were spread across
    the two agent nodes (the server node has a taint as you recall):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The descheduler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes is great at scheduling pods to nodes according to sophisticated
    placement rules. But, once a pod is scheduled, Kubernetes will not move it to
    another node if the original conditions changed. Here are some use cases that
    would benefit from moving workloads around:'
  prefs: []
  type: TYPE_NORMAL
- en: Certain nodes are experiencing under-utilization or over-utilization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The initial scheduling decision is no longer valid when taints or labels are
    modified on nodes, causing pod/node affinity requirements to no longer be met.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certain nodes have encountered failures, resulting in the migration of their
    pods to other nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional nodes are introduced to the clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is where the descheduler comes into play. The descheduler is not part of
    vanilla Kubernetes. You need to install it and define policies that determine
    which running pods may be evicted. It can run as a `Job`, `CronJob`, or `Deployment`.
    The descheduler will periodically check the current placement of pods and will
    evict pods that violate some policy. The pods will get rescheduled and then the
    standard Kubernetes scheduler will take care of scheduling them according to the
    current conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check it out here: [https://github.com/kubernetes-sigs/descheduler](https://github.com/kubernetes-sigs/descheduler).'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw how the advanced scheduling mechanisms Kubernetes provides,
    as well as projects like the descheduler, can help Hue schedule its workload in
    an optimal way across the available infrastructure. In the next section, we will
    look at how to divide Hue workloads to a namespace to manage access to different
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: Using namespaces to limit access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Hue project is moving along nicely, and we have a few hundred microservices
    and about 100 developers and DevOps engineers working on it. Groups of related
    microservices emerge, and you notice that many of these groups are pretty autonomous.
    They are completely oblivious to the other groups. Also, there are some sensitive
    areas such as health and finance that you want to control access to more effectively.
    Enter namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create a new service, `hue-finance`, and put it in a new namespace called
    `restricted`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the YAML file for the new `restricted` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create it as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the namespace has been created, we can configure a context for the namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check our cluster configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there are two contexts now and the current context is `restricted`.
    If we wanted to, we could even create dedicated users with their own credentials
    that are allowed to operate in the restricted namespace. Depending on the environment
    this can be easy or difficult and may involve creating certificates via Kubernetes
    certificate authorities. Cloud providers offer integration with their IAM systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'To move along, I’ll use the `admin@k3d-k3s-default` user’s credentials and
    create a user named `restricted@k3d-k3s-default` directly in the `kubeconfig`
    file of the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in this empty namespace, we can create our `hue-finance` service, and
    it will be separate from the other services in the default namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: You don’t have to switch contexts. You can also use the `--namespace=<namespace>`
    and `--all-namespaces` command-line switches, but when operating for a while in
    the same non-default namespace, it’s nice to set the context to that namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Using Kustomization for hierarchical cluster structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is not a typo. Kubectl recently incorporated the functionality of Kustomize
    ([https://kustomize.io/](https://kustomize.io/)). It is a way to configure Kubernetes
    without templates. There was a lot of drama about the way the Kustomize functionality
    was integrated into kubectl itself, since there are other options and it was an
    open question if kubectl should be that opinionated. But, that’s all in the past.
    The bottom line is that `kubectl apply -k` unlocks a treasure trove of configuration
    options. Let’s understand what problem it helps us to solve and take advantage
    of it to help us manage Hue.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basics of Kustomize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kustomize was created as a response to template-heavy approaches like Helm to
    configure and customize Kubernetes clusters. It is designed around the principle
    of declarative application management. It takes a valid Kubernetes YAML manifest
    (base) and specializes it or extends it by overlaying additional YAML patches
    (overlays). Overlays depend on their bases. All files are valid YAML files. There
    are no placeholders.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `kustomization.yaml` file controls the process. Any directory that contains
    a `kustomization.yaml` file is called a root. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Kustomize can work well in a GitOps environment where different Kustomizations
    live in a Git repo and changes to the bases, overlays, or `kustomization.yaml`
    files trigger a deployment.
  prefs: []
  type: TYPE_NORMAL
- en: One of the best use cases for Kustomize is organizing your system into multiple
    namespaces such as staging and production. Let’s restructure the Hue platform
    deployment manifests.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the directory structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we need a base directory that will contain the commonalities of all
    the manifests. Then we will have an `overlays` directory that contains `staging`
    and `production` sub-directories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The `hue-learn.yaml` file in the base directory is just an example. There may
    be many files there. Let’s review it quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'It is very similar to the manifest we created earlier, but it doesn’t have
    the `app: hue` label. It is not necessary because the label is provided by the
    `kustomization.yaml` file as a common label for all the listed resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Applying Kustomizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can observe the results by running the `kubectl kustomize` command on the
    base directory. You can see that the common label `app: hue` was added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: In order to actually deploy the Kustomization, we can run `kubectl -k apply`.
    But, the base is not supposed to be deployed on its own. Let’s dive into the `overlays/staging`
    directory and examine it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `namespace.yaml` file just creates the `staging` namespace. It will also
    benefit from all the Kustomizations as we’ll soon see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The `kustomization.yaml` file adds the common label `environment: staging`.
    It depends on the base directory and adds the `namespace.yaml` file to the resources
    list (which already includes `hue-learn.yaml` from the base):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: But, that’s not all. The most interesting part of Kustomizations is patching.
  prefs: []
  type: TYPE_NORMAL
- en: Patching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Patches add or replace parts of manifests. They never remove existing resources
    or parts of resources. The `hue-learn-patch.yaml` updates the image from `g1g1/hue-learn:0.3`
    to `g1g1/hue-learn:0.4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: This is a strategic merge. Kustomize supports another type of patch called `JsonPatches6902`.
    It is based on RFC 6902 ([https://tools.ietf.org/html/rfc6902](https://tools.ietf.org/html/rfc6902)).
    It is often more concise than a strategic merge. We can use YAML syntax for JSON
    6902 patches. Here is the same patch of changing the image version to version
    0.4 using `JsonPatches6902` syntax`:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Kustomizing the entire staging namespace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here is what Kustomize generates when running it on the `overlays/staging`
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the namespace didn’t inherit the `app: hue` label from the base,
    but only the `environment: staging` label from its local Kustomization file. The
    `hue-learn` pod, on the other hand, got all labels as well the namespace designation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s time to deploy it to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can review the pod in the newly created `staging` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check that the overlay worked and the image version is indeed 0.4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we covered the powerful structuring and reusability afforded
    by the Kustomize option. This is very important for a large-scale system like
    the Hue platform where a lot of workloads can benefit from a uniform structure
    and consistent foundation. In the next section, we will look at launching short-term
    jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Launching jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hue has evolved and has a lot of long-running processes deployed as microservices,
    but it also has a lot of tasks that run, accomplish some goal, and exit. Kubernetes
    supports this functionality via the `Job` resource. A Kubernetes job manages one
    or more pods and ensures that they run until success or failure. If one of the
    pods managed by the job fails or is deleted, then the job will run a new pod until
    it succeeds.
  prefs: []
  type: TYPE_NORMAL
- en: There are also many serverless or function-as-a-service solutions for Kubernetes,
    but they are built-on top of native Kubernetes. We will cover serverless computing
    in depth in *Chapter 12*, *Serverless Computing on Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a job that runs a Python process to compute the factorial of 5 (hint:
    it’s 120):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `restartPolicy` must be either `Never` or `OnFailure`. The default
    value – `Always` – is invalid because a job doesn’t restart after successful completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start the job and check its status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The pods of completed tasks are displayed with a status of `Completed`. Note
    that job pods have a label called `job-name` with the name of the job, so it’s
    easy to filter just the job pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check out its output in the logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Launching jobs one after another is fine for some use cases, but it is often
    useful to run jobs in parallel. In addition, it’s important to clean up jobs after
    they are done as well as to run jobs periodically. Let’s see how it’s done.
  prefs: []
  type: TYPE_NORMAL
- en: Running jobs in parallel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also run a job with parallelism. There are two fields in the spec called
    `completions` and `parallelism`. The completions are set to 1 by default. If you
    want more than one successful completion, then increase this value. Parallelism
    determines how many pods to launch. A job will not launch more pods than needed
    for successful completions, even if the parallelism number is greater.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run another job that just sleeps for 20 seconds until it has three successful
    completions. We’ll use a parallelism factor of six, but only three pods will be
    launched:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s run the job and wait for all the pods to complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now see that all three pods completed and the pods are not ready because
    they already did their work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Completed pods don’t take up resources on the node, so other pods can get scheduled
    there.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up completed jobs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a job completes, it sticks around – and its pods, too. This is by design,
    so you can look at logs or connect to pods and explore. But normally, when a job
    has completed successfully, it is not needed anymore. It’s your responsibility
    to clean up completed jobs and their pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way is to simply delete the `job` object, which will delete all
    the pods too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Scheduling cron jobs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes cron jobs are jobs that run at a specified time, once or repeatedly.
    They behave as regular Unix cron jobs specified in the `/etc/crontab` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `CronJob` resource became stable with Kubernetes 1.21\. Here is the configuration
    to launch a cron job every minute to remind you to stretch. In the schedule, you
    may replace the ‘`*`'' with ‘`?`'':'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'In the pod `spec`, under the job template, I added the label `cronjob-name:
    cron-demo`. The reason is that cron jobs and their pods are assigned names with
    a random prefix by Kubernetes. The label allows you to easily discover all the
    pods of a particular cron job. The pods will also have the `job-name` label because
    a cron job creates a `job` object for each invocation. However, the job name itself
    has a random prefix, so it doesn’t help us discover the pods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the cron job and observe the results after a minute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, every minute the cron job creates a new job with a different
    name. The pod of each job is labeled with its job name, but also with the cron
    job name – `cronjob-demo` – for easily aggregating all pods originating from this
    cron job.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, you can check the output of a completed job’s pod using the `logs`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: When you delete a cron job it stops scheduling new jobs and deletes all the
    existing `job` objects and all the pods it created.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the designated label (`name=cron-demo` in this case) to locate
    all the `job` objects launched by the cron job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also suspend a cron job so it doesn’t create more jobs without deleting
    completed jobs and pods. You can also manage how many jobs stick around by setting
    it in the spec history limits: `.spec.successfulJobsHistoryLimit` and `.spec.failedJobsHistoryLimit`.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered the important topics of launching jobs and controlling
    them. This is a critical aspect of the Hue platform, which needs to react to real-time
    events and handle them by launching jobs as well as periodically performing short
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Mixing non-cluster components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most real-time system components in the Kubernetes cluster will communicate
    with out-of-cluster components. Those could be completely external third-party
    services accessible through some API, but can also be internal services running
    in the same local network that, for various reasons, are not part of the Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two categories here: inside the cluster network and outside the cluster
    network.'
  prefs: []
  type: TYPE_NORMAL
- en: Outside-the-cluster-network components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These components have no direct access to the cluster. They can only access
    it through APIs, externally visible URLs, and exposed services. These components
    are treated just like any external user. Often, cluster components will just use
    external services, which pose no security issue. For example, in a previous company,
    we had a Kubernetes cluster that reported exceptions to a third-party service
    called Sentry ([https://sentry.io/welcome/](https://sentry.io/welcome/)). It was
    one-way communication from the Kubernetes cluster to the third-party service.
    The Kubernetes cluster had the credentials to access Sentry and that was the extent
    of this one-way communication.
  prefs: []
  type: TYPE_NORMAL
- en: Inside-the-cluster-network components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These are components that run inside the network but are not managed by Kubernetes.
    There are many reasons to run such components. They could be legacy applications
    that have not been “kubernetized” yet, or some distributed data store that is
    not easy to run inside Kubernetes. The reason to run these components inside the
    network is for performance, and to have isolation from the outside world so traffic
    between these components and pods can be more secure. Being part of the same network
    ensures low latency, and the reduced need for opening up the network for communication
    is both convenient and can be more secure.
  prefs: []
  type: TYPE_NORMAL
- en: Managing the Hue platform with Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will look at how Kubernetes can help operate a huge platform
    such as Hue. Kubernetes itself provides a lot of capabilities to orchestrate pods
    and manage quotas and limits, detecting and recovering from certain types of generic
    failures (hardware malfunctions, process crashes, and unreachable services). But,
    in a complicated system such as Hue, pods and services may be up and running but
    in an invalid state or waiting for other dependencies in order to perform their
    duties. This is tricky because if a service or pod is not ready yet but is already
    receiving requests, then you need to manage it somehow: fail (puts responsibility
    on the caller), retry (how many times? for how long? how often?), and queue for
    later (who will manage this queue?).'
  prefs: []
  type: TYPE_NORMAL
- en: It is often better if the system at large can be aware of the readiness state
    of different components, or if components are visible only when they are truly
    ready. Kubernetes doesn’t know Hue, but it provides several mechanisms such as
    liveness probes, readiness probes, startup probes, and init containers to support
    application-specific management of your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Using liveness probes to ensure your containers are alive
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The kubelet watches over your containers. If a container process crashes, the
    kubelet will take care of it based on the restart policy. But this is not enough
    in many cases. Your process may not crash but instead run into an infinite loop
    or a deadlock. The restart policy might not be nuanced enough. With a liveness
    probe, you get to decide when a container is considered alive. If a liveness probe
    fails, Kubernetes will restart your container. Here is a pod template for the
    Hue music service. It has a `livenessProbe` section, which uses the `httpGet`
    probe. An HTTP probe requires a scheme (`http` or `https`, default to `http`),
    a host (default to `PodIp`), a path, and a port. The probe is considered successful
    if the HTTP status is between `200` and `399`. Your container may need some time
    to initialize, so you can specify an `initialDelayInSeconds`. The kubelet will
    not hit the liveness check during this period:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: If a liveness probe fails for any container, then the pod’s restart policy goes
    into effect. Make sure your restart policy is not `Never`, because that will make
    the probe useless.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three other types of liveness probes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TcpSocket` – Just checks that a port is open'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Exec` – Runs a command that returns 0 for success'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gRPC` – Follows the gRPC health-checking protocol ([https://github.com/grpc/grpc/blob/master/doc/health-checking.md](https://github.com/grpc/grpc/blob/master/doc/health-checking.md))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using readiness probes to manage dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Readiness probes are used for a different purpose. Your container may be up
    and running and pass its liveness probe, but it may depend on other services that
    are unavailable at the moment. For example, `hue-music` may depend on access to
    a data service that contains your listening history. Without access, it is unable
    to perform its duties. In this case, other services or external clients should
    not send requests to the `hue-music` service, but there is no need to restart
    it. Readiness probes address this use case. When a readiness probe fails for a
    container, the container’s pod will be removed from any service endpoint it is
    registered with. This ensures that requests don’t flood services that can’t process
    them. Note that you can also use readiness probes to temporarily remove pods that
    are overbooked until they drain some internal queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample readiness probe. I use the `exec` probe here to execute a
    custom command. If the command exits a non-zero exit code, the container will
    be torn down:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: It is fine to have both a readiness probe and a liveness probe on the same container
    as they serve different purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Using startup probes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some applications (mostly legacy) may have long initialization periods. In this
    case, liveness probes may fail and cause the container to restart before it finishes
    initialization. This is where startup probes come in. If a startup probe is configured,
    liveness and readiness checks are skipped until the startup is completed. At this
    point, the startup probe is not invoked anymore and normal liveness and readiness
    probes take over.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the following configuration snippet, the startup probe will
    check for 5 minutes every 10 seconds if the container has started (using the same
    liveness check as the liveness probe). If the startup probe fails 30 times (300
    seconds = 5 minutes) then the container will be restarted and get 5 more minutes
    to try and initialize itself. But, if it passes the startup probe check within
    5 minutes, then the liveness probe is in effect and any failure of the liveness
    check will result in a restart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Employing init containers for orderly pod bring-up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Liveness, readiness, and startup probes are great. They recognize that, at startup,
    there may be a period where the container is not ready yet, but shouldn’t be considered
    failed. To accommodate that, there is the `initialDelayInSeconds` setting where
    containers will not be considered failed. But, what if this initial delay is potentially
    very long? Maybe, in most cases, a container is ready after a couple of seconds
    and ready to process requests, but because the initial delay is set to 5 minutes
    just in case, we waste a lot of time when the container is idle. If the container
    is part of a high-traffic service, then many instances can all sit idle for five
    minutes after each upgrade and pretty much make the service unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: Init containers address this problem. A pod may have a set of init containers
    that run to completion before other containers are started. An init container
    can take care of all the non-deterministic initialization and let application
    containers with their readiness probe have a minimal delay.
  prefs: []
  type: TYPE_NORMAL
- en: Init containers are especially useful for pod-level initialization purposes
    like waiting for volumes to be ready. There is some overlap between init containers
    and startup probes and the choice depends on the specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Init containers came out of beta in Kubernetes 1.6\. You specify them in the
    pod spec as the `initContainers` field, which is very similar to the `containers`
    field. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Pod readiness and readiness gates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pod readiness was introduced in Kubernetes 1.11 and became stable in Kubernetes
    1.14\. While readiness probes allow you to determine at the container level if
    it’s ready to serve requests, the overall infrastructure that supports delivering
    traffic to the pod might not be ready yet. For example, the service, network policy,
    and load balancer might take some extra time. This can be a problem, especially
    during rolling deployments where Kubernetes might terminate the old pods before
    the new pods are really ready, which will cause degradation in service capacity
    and even cause a service outage in extreme cases (all old pods were terminated
    and no new pod is fully ready).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the problem that the pod readiness gates address. The idea is to extend
    the concept of pod readiness to check additional conditions in addition to making
    sure all the containers are ready. This is done by adding a new field to the `PodSpec`
    called `readinessGates`. You can specify a set of conditions that must be satisfied
    for the pod to be considered ready. In the following example, the pod is not ready
    because the `www.example.com/feature-1` condition has the `status: False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Sharing with DaemonSet pods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DaemonSet pods are pods that are deployed automatically, one per node (or a
    designated subset of the nodes). They are typically used for keeping an eye on
    nodes and ensuring they are operational. This is a very important function, which
    we will cover in *Chapter 13*, *Monitoring Kubernetes Clusters*. But they can
    be used for much more. The nature of the default Kubernetes scheduler is that
    it schedules pods based on resource availability and requests. If you have lots
    of pods that don’t require a lot of resources, similarly many pods will be scheduled
    on the same node.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a pod that performs a small task and then, every second, sends
    a summary of all its activities to a remote service. Now, imagine that, on average,
    50 of these pods are scheduled on the same node. This means that, every second,
    50 pods make 50 network requests with very little data. How about we cut it down
    by 50× to just a single network request? With a `DaemonSet` pod, all the other
    50 pods can communicate with it instead of talking directly to the remote service.
    The `DaemonSet` pod will collect all the data from the 50 pods and, once a second,
    will report it in aggregate to the remote service. Of course, that requires the
    remote service API to support aggregate reporting. The nice thing is that the
    pods themselves don’t have to be modified; they will just be configured to talk
    to the `DaemonSet` pod on `localhost` instead of the remote service. The `DaemonSet`
    pod serves as an aggregating proxy. It can also implement retry and other similar
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The interesting part about this configuration file is that the `hostNetwork`,
    `hostPID`, and `hostIPC` options are set to `true`. This enables the pods to communicate
    efficiently with the proxy, utilizing the fact they are running on the same physical
    host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we looked at how to manage the Hue platform on Kubernetes and
    ensure that Hue components are deployed reliably and accessed when they are ready
    using capabilities such as init containers, readiness gates, and DaemonSets. In
    the next section, we’ll see where the Hue platform could potentially go in the
    future.
  prefs: []
  type: TYPE_NORMAL
- en: Evolving the Hue platform with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll discuss other ways to extend the Hue platform and service
    additional markets and communities. The question is always, what Kubernetes features
    and capabilities can we use to address new challenges or requirements?
  prefs: []
  type: TYPE_NORMAL
- en: This is a hypothetical section for thinking big and using Hue as an example
    of a massively complicated system.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing Hue in an enterprise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An enterprise often can’t run in the cloud, either due to security and compliance
    reasons or for performance reasons because the system has to work with data and
    legacy systems that are not cost-effective to move to the cloud. Either way, Hue
    for enterprise must support on-premises clusters and/or bare-metal clusters.
  prefs: []
  type: TYPE_NORMAL
- en: While Kubernetes is most often deployed in the cloud and even has a special
    cloud-provider interface, it doesn’t depend on the cloud and can be deployed anywhere.
    It does require more expertise, but enterprise organizations that already run
    systems on their own data centers may have that expertise or develop it.
  prefs: []
  type: TYPE_NORMAL
- en: Advancing science with Hue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hue is so great at integrating information from multiple sources that it would
    be a boon for the scientific community. Consider how Hue can help with multi-disciplinary
    collaboration between scientists from different disciplines.
  prefs: []
  type: TYPE_NORMAL
- en: A network of scientific communities might require deployment across multiple
    geographically distributed clusters. Enter multi-cluster Kubernetes. Kubernetes
    has this use case in mind and evolves its support. We will discuss it at length
    in *Chapter 11*, *Running Kubernetes on Multiple Clusters*.
  prefs: []
  type: TYPE_NORMAL
- en: Educating the kids of the future with Hue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hue can be utilized for education and provide many services to online education
    systems. But, privacy concerns may prevent deploying Hue for kids as a single,
    centralized system. One possibility is to have a single cluster, with namespaces
    for different schools. Another deployment option is that each school or county
    has its own Hue Kubernetes cluster. In the second case, Hue for education must
    be extremely easy to operate to cater to schools without a lot of technical expertise.
    Kubernetes can help a lot by providing self-healing and auto-scaling features
    and capabilities for Hue, to be as close to zero-administration as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we designed and planned the development, deployment, and management
    of the Hue platform – an imaginary omniscient and omnipotent system – built on
    microservice architecture. We used Kubernetes as the underlying orchestration
    platform, of course, and delved into many of its concepts and resources. In particular,
    we focused on deploying pods for long-running services as opposed to jobs for
    launching short-term or cron jobs, explored internal services versus external
    services, and also used namespaces to segment a Kubernetes cluster. We looked
    into the various workload scheduling mechanisms of Kubernetes. Then, we looked
    at the management of a large system such as Hue with liveness probes, readiness
    probes, startup probes, init containers, and daemon sets.
  prefs: []
  type: TYPE_NORMAL
- en: You should now feel comfortable architecting web-scale systems composed of microservices
    and understand how to deploy and manage them in a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look into the super-important area of storage.
    Data is king, but it is often the least flexible element of the system. Kubernetes
    provides a storage model and many options for integrating with various storage
    solutions.
  prefs: []
  type: TYPE_NORMAL
