<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer040">
<h1 class="chapter-number" id="_idParaDest-36"><a id="_idTextAnchor035"/>2</h1>
<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/>Installing and Configuring Kubernetes Clusters</h1>
<p>This chapter introduces the different configurations of Kubernetes, which is the first step toward working with Kubernetes. We’ll get our hands dirty by setting up a Kubernetes cluster with a single worker node and then multiple worker nodes. This chapter familiarizes you with Kubernetes installations, which is one of the key skills that will serve in your daily job as a Kubernetes administrator. </p>
<p>In this chapter, we’re going to cover the following topics: </p>
<ul>
<li><a id="_idTextAnchor037"/>Hands-on Kubernetes tooling </li>
<li>Installing and configuring a Kubernetes cluster</li>
<li><a id="_idTextAnchor038"/>Using <strong class="source-inline">minikube</strong> to set up a single node Kubernetes cluster</li>
<li><a id="_idTextAnchor039"/>Using <strong class="source-inline">kubeadm</strong> to install a basic Kubernetes cluster</li>
<li><a id="_idTextAnchor040"/>Setting up a highly available cluster with <strong class="source-inline">kubeadm</strong></li>
</ul>
<h1 id="_idParaDest-38"><a id="_idTextAnchor041"/>Technical requirements </h1>
<p>To get started, we need to make sure your local machine meets the technical requirements described as the following:</p>
<ul>
<li>A compatible Linux host – we recommend a Debian-based Linux distribution such as Ubuntu 18.04 or later. </li>
<li>Make sure your host machine has at least 2 GB RAM, 2 CPU cores, and about 20 GB of free disk space.</li>
</ul>
<h1 id="_idParaDest-39"><a id="_idTextAnchor042"/>Hands-on Kubernetes tooling</h1>
<p>There are a handful of Kubernetes tools<a id="_idIndexMarker095"/> on the market – we’ll start by covering some widely used Kubernetes tools to interact with the Kubernetes cluster. We’ll dive into some key tools with hands-on labs later in this chapter. </p>
<h2 id="_idParaDest-40"><a id="_idTextAnchor043"/>Core tools </h2>
<p>In this section, we are going to cover tools<a id="_idIndexMarker096"/> which are required to work with Kubernetes and containers. </p>
<h3>kubectl</h3>
<p><strong class="source-inline">kubectl</strong> is a Kubernetes command-line tool<a id="_idIndexMarker097"/> used to talk to the Kubernetes cluster. It is hands<a id="_idIndexMarker098"/> down the most common and important utility that allows you to run commands against the Kubernetes cluster. There are a handful of <strong class="source-inline">kubectl</strong> commands available that will allow users to work with the Kubernetes cluster, such as deploying a containerized application, managing cluster resources, and monitoring and visualizing events and logs. We’ll cover most of the common <strong class="source-inline">kubectl</strong> commands with examples as we go through the process. </p>
<p>To set up the <strong class="source-inline">kubectl</strong> utility, if you’re on Red Hat-based distributions such as CentOS or Fedora, check out the official<a id="_idIndexMarker099"/> article for further information: <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management">https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management</a>. You can use the following commands: </p>
<p class="source-code">cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo</p>
<p class="source-code">[kubernetes]</p>
<p class="source-code">name=Kubernetes</p>
<p class="source-code">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</p>
<p class="source-code">enabled=1</p>
<p class="source-code">gpgcheck=1</p>
<p class="source-code">repo_gpgcheck=1</p>
<p class="source-code">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</p>
<p class="source-code">EOF</p>
<p class="source-code">sudo yum install -y kubectl </p>
<p>If you’re on Debian-based distributions<a id="_idIndexMarker100"/> such as Ubuntu 18.04, you can follow<a id="_idIndexMarker101"/> the following instructions: </p>
<ol>
<li>Firstly, you need to update the <strong class="source-inline">apt</strong> package index – then, you need to install the packages needed to use the Kubernetes <strong class="source-inline">apt</strong> repository by running the following commands sequentially: <p class="source-code"><strong class="bold">sudo apt-get update</strong></p><p class="source-code"><strong class="bold">sudo apt-get install -y apt-transport-https ca-certificates curl</strong></p></li>
<li>Download the Google Cloud public signing key and add the Kubernetes <strong class="source-inline">apt</strong> repository by using the following command: <p class="source-code"><strong class="bold">sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg</strong></p><p class="source-code"><strong class="bold">echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list</strong></p></li>
<li>Now, you’re ready to go. Make sure you update the <strong class="source-inline">apt</strong> package index with the new repository again and then install the <strong class="source-inline">kubectl</strong> utility using the <strong class="source-inline">apt-get install</strong> command: <p class="source-code"><strong class="bold">sudo apt-get update</strong></p><p class="source-code"><strong class="bold">sudo apt-get install -y kubectl</strong></p></li>
<li>You can verify whether <strong class="source-inline">kubectl</strong> has been successfully installed by running the following command upon the completion of the previous steps: <p class="source-code"><strong class="bold">kubectl version --client</strong></p></li>
</ol>
<p>You’ll see an output similar to the following if you have installed <strong class="source-inline">kubectl</strong> successfully:</p>
<div>
<div class="IMG---Figure" id="_idContainer016">
<img alt="Figure 2.1 – A successful installation of kubectl " height="58" src="image/Figure_2.01_B18201.jpg" width="1401"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.1 – A successful installation of kubectl</p>
<p>For instructions<a id="_idIndexMarker102"/> on installing <strong class="source-inline">kubectl</strong> in different<a id="_idIndexMarker103"/> environments, please<a id="_idIndexMarker104"/> refer to <a href="https://kubernetes.io/docs/tasks/tools/">https://kubernetes.io/docs/tasks/tools/</a>.</p>
<h3>Container runtimes </h3>
<p>Now, we are going<a id="_idIndexMarker105"/> to set up <strong class="source-inline">containerd</strong> as our container runtime by following these instructions:</p>
<ol>
<li>Update the <strong class="source-inline">apt</strong> index, add Docker’s official <strong class="source-inline">GPG</strong> key, and set up the <strong class="source-inline">apt</strong> repository by running the following instructions: <p class="source-code"><strong class="bold">sudo apt-get update</strong></p><p class="source-code"><strong class="bold">sudo apt-get install \</strong></p><p class="source-code"><strong class="bold">    ca-certificates \</strong></p><p class="source-code"><strong class="bold">    curl \</strong></p><p class="source-code"><strong class="bold">    gnupg \</strong></p><p class="source-code"><strong class="bold">    lsb-release</strong></p><p class="source-code"><strong class="bold">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg</strong></p><p class="source-code"><strong class="bold">echo \</strong></p><p class="source-code"><strong class="bold">  "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \</strong></p><p class="source-code"><strong class="bold">  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null</strong></p></li>
<li>Install the Docker engine and <strong class="source-inline">containerd.io</strong>:<p class="source-code"><strong class="bold">sudo apt-get update</strong></p><p class="source-code"><strong class="bold">sudo apt-get install docker-ce docker-ce-cli containerd.io</strong></p></li>
<li>Validate that Docker has been installed successfully<a id="_idIndexMarker106"/> by using the following commands: <p class="source-code"><strong class="bold">sudo docker ps</strong></p><p class="source-code"><strong class="bold">#optional - running your first docker container</strong></p><p class="source-code"><strong class="bold">sudo docker run hello-world</strong></p></li>
</ol>
<p>You’ll see an output similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer017">
<img alt="Figure 2.2 – Docker is up and running " height="118" src="image/Figure_2.02_B18201.jpg" width="1404"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.2 – Docker is up and running</p>
<ol>
<li>If you’re about to configure <strong class="source-inline">containerd</strong> as the container runtime, you can use the following command and set the configuration to <strong class="source-inline">default</strong>: <p class="source-code"><strong class="bold">sudo mkdir -p /etc/containerd</strong></p><p class="source-code"><strong class="bold">containerd config default | sudo tee /etc/containerd/config.toml</strong></p></li>
<li>Restart <strong class="source-inline">containerd</strong> to make sure the changes take effect: <p class="source-code"><strong class="bold">sudo systemctl restart containerd</strong></p></li>
</ol>
<p>If you want to know more about how<a id="_idIndexMarker107"/> to set up CRI-O as a runtime, please<a id="_idIndexMarker108"/> check out the following link: <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o">https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o</a>. It will show you how <strong class="source-inline">containerd</strong> serves as a container runtime in the context of Kubernetes.</p>
<h2 id="_idParaDest-41"><a id="_idTextAnchor044"/>Deployment tools </h2>
<p>To bootstrap a Kubernetes cluster, we rely<a id="_idIndexMarker109"/> on the deployment tools. There are lots of useful tools <a id="_idIndexMarker110"/>on the market to help spin up a Kubernetes cluster, of which a lot of them are vendor-affinity. Here, we will cover what’s requested in the CKA exam. That’s the primary reason that we focus on upstream Kubernetes and these tools will help bootstrap a cluster on-premises. The following tools help you set up a Kubernetes cluster and we’ll cover the detailed instructions while working with each of them in the next chapter: </p>
<ul>
<li><strong class="bold">kubeadm</strong>: <strong class="source-inline">kubeadm</strong> is the most important tool<a id="_idIndexMarker111"/> to help you crack the exam exercises. It helps install and set up the Kubernetes cluster with best practices. With <strong class="source-inline">kubeadm</strong>, you can provision a single node cluster and, more importantly, multi-node clusters. This is the first choice for most large organizations that want to manage their own Kubernetes cluster and use their own on-premises servers. </li>
<li><strong class="bold">minikube</strong>: <strong class="source-inline">minikube</strong> is a popular local Kubernetes<a id="_idIndexMarker112"/> that can be provisioned on your local laptop or a <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>). It’s very lightweight, focusing on making it easy to learn and testing Kubernetes quickly. </li>
<li><strong class="bold">kind</strong>: <strong class="source-inline">kind</strong> is similar to <strong class="source-inline">minikube</strong>. It focuses on provisioning local<a id="_idIndexMarker113"/> Kubernetes clusters and some simple CI scenarios and development. It runs local Kubernetes clusters using a Docker runtime – it can run as a single node Kubernetes cluster or a Kubernetes multi-node cluster. You can test lots of useful, simple scenarios with <strong class="source-inline">kind</strong>. </li>
</ul>
<h2 id="_idParaDest-42"><a id="_idTextAnchor045"/>Other tools </h2>
<p>Some of the other tools are not covered in the CKA exam – however, they will still come in handy in your daily work as a Kubernetes administrator. </p>
<h3>Helm </h3>
<p>Helm is a management tool<a id="_idIndexMarker114"/> for managing packages<a id="_idIndexMarker115"/> of pre-configured Kubernetes objects in the form of charts – we call these Helm charts.</p>
<p>To install <strong class="source-inline">helm</strong>, you can follow the following instructions<a id="_idIndexMarker116"/> for a Debian-based distribution such as Ubuntu 18.04: </p>
<ol>
<li>Update the <strong class="source-inline">apt</strong> package index:<p class="source-code"><strong class="bold">curl https://baltocdn.com/helm/signing.asc | sudo apt-key add -</strong></p><p class="source-code"><strong class="bold">sudo apt-get install apt-transport-https --yes</strong></p></li>
<li>Install the packages to use the Helm <strong class="source-inline">apt</strong> repository with the following command: <p class="source-code"><strong class="bold">echo "deb https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list</strong></p></li>
<li>Make sure you update the <strong class="source-inline">apt</strong> package index with the new repository again and then install Helm using the <strong class="source-inline">apt-get install</strong> command: <p class="source-code"><strong class="bold">sudo apt-get update</strong></p><p class="source-code"><strong class="bold">sudo apt-get install helm</strong></p></li>
<li>Use the following Helm command to validate its successful installation:<p class="source-code"><strong class="bold">helm version</strong></p></li>
</ol>
<p>You’ll see output similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<img alt="Figure 2.3 – Successful installation of Helm " height="41" src="image/Figure_2.03_B18201.jpg" width="1241"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.3 – Successful installation of Helm</p>
<p>To know more<a id="_idIndexMarker117"/> ways to install Helm, check<a id="_idIndexMarker118"/> out the following link: <a href="https://helm.sh/docs/intro/install/">https://helm.sh/docs/intro/install/</a>.</p>
<h3>Kompose</h3>
<p>Most people who work with Docker<a id="_idIndexMarker119"/> will know about Docker Compose. Docker Compose<a id="_idIndexMarker120"/> is a tool used to define and run the multi-container applications containerized by Docker. It also uses a YAML file to define the application specifications. As more and more people are moving away from purely using Docker Swarm or Docker Desktop to take advantage of the enterprise-scale container orchestration system, Kompose comes in handy as a conversion tool for Docker Compose to contain orchestrators such as Kubernetes – the same structure works for Redhat OpenShift too. </p>
<p>You can<a id="_idIndexMarker121"/> install Kompose by running<a id="_idIndexMarker122"/> the following instructions on your Ubuntu 18.04:</p>
<ol>
<li>Fetch the <strong class="source-inline">kompose</strong> binary:<p class="source-code"><strong class="bold">curl -L https://github.com/kubernetes/kompose/releases/download/v1.26.0/kompose-linux-amd64 -o kompose</strong></p><p class="source-code"><strong class="bold">chmod +x kompose</strong></p><p class="source-code"><strong class="bold">sudo mv ./kompose /usr/local/bin/kompose</strong></p></li>
<li>Then, you can fetch a <strong class="source-inline">docker compose</strong> example file from the official website and test the <strong class="source-inline">kompose convert</strong> command as follows: <p class="source-code"><strong class="bold">wget https://raw.githubusercontent.com/kubernetes/kompose/master/examples/docker-compose-v3.yaml -O docker-compose.yaml</strong></p><p class="source-code"><strong class="bold">kompose convert</strong></p></li>
</ol>
<p>Your output will look similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer019">
<img alt="Figure 2.4 – A kompose convert command translating Docker compose into Kubernetes-native YAML-defined files  " height="135" src="image/Figure_2.04_B18201.jpg" width="551"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.4 – A kompose convert command translating Docker compose into Kubernetes-native YAML-defined files </p>
<ol>
<li>Then, deploy those YAML files to your local Kubernetes cluster by using the following command: <p class="source-code"><strong class="bold">kubectl apply -f .</strong></p></li>
</ol>
<p>Your output will look similar<a id="_idIndexMarker123"/> to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<img alt="Figure 2.5 – Kubernetes Pods up and running  " height="103" src="image/Figure_2.05_B18201.jpg" width="578"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.5 – Kubernetes Pods up and running </p>
<p>The preceding screenshot<a id="_idIndexMarker124"/> shows the Redis Pods running in your Kubernetes cluster. </p>
<h3>The dashboard</h3>
<p>You can install a web-based <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>) to your Kubernetes cluster. It<a id="_idIndexMarker125"/> not only displays the cluster status<a id="_idIndexMarker126"/> and shows what’s going on with the Kubernetes<a id="_idIndexMarker127"/> cluster but also allows you to deploy containerized applications, troubleshoot, and manage the cluster and all related resources in the cluster. </p>
<p>The following is a sample dashboard: </p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<img alt="Figure 2.6 – The Kubernetes dashboard " height="768" src="image/Figure_2.06_B18201.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.6 – The Kubernetes dashboard</p>
<p>The dashboard is sometimes<a id="_idIndexMarker128"/> handy for quick monitoring of the cluster<a id="_idIndexMarker129"/> states from the UI and user-friendly for collaborating with people who are not familiar with <strong class="source-inline">kubectl</strong> commands. </p>
<h1 id="_idParaDest-43"><a id="_idTextAnchor046"/>Installing and configuring a Kubernetes cluster </h1>
<p>This section focuses<a id="_idIndexMarker130"/> on the installation of the Kubernetes cluster and the related<a id="_idIndexMarker131"/> configurations for it. With a good understanding gained from <a href="B18201_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, where you learned about the Kubernetes cluster architecture and Kubernetes toolings, you will perform the Kubernetes cluster installation the hard way with <strong class="source-inline">minikube</strong> and <strong class="source-inline">kubeadm</strong>, and then update the cluster version. </p>
<p>Note that using <strong class="source-inline">minikube</strong> to spin up a single node cluster is not covered in the CKA exam but it comes quite handy when you’d like to test out Kubernetes in your local machine. The same goes for using <strong class="source-inline">kubeadm</strong> to install a Kubernetes<a id="_idIndexMarker132"/> multi-node cluster, as well as setting up a <strong class="bold">highly available</strong> (<strong class="bold">HA</strong>) Kubernetes cluster. </p>
<p>We expect you to learn both ways while putting more focus on the hands-on lab working with <strong class="source-inline">kubeadm</strong>. Starting with the next section, we’ll walk you through the process of installing a new Kuberne<a id="_idTextAnchor047"/>tes cluster and configuration. </p>
<h2 id="_idParaDest-44"><a id="_idTextAnchor048"/>Prerequisites for installing a Kubernetes cluster </h2>
<p>To get started, we need to make<a id="_idIndexMarker133"/> sure your local machine meets the following technical requirements for both <strong class="source-inline">minikube</strong> and <strong class="source-inline">kubeadm</strong>: </p>
<ul>
<li>A compatible Linux host – we recommend a Debian-based Linux distribution such as Ubuntu 18.04 or later. </li>
<li>Make sure your host machine has at least 2 GB RAM, 2 CPU cores, and about 20 GB of free disk space.</li>
<li>Internet connectivity, as you will need to download dependencies throughout the process. </li>
<li>A container runtime is needed prior to creating a Kubernetes cluster. During the cluster creation process, the Kubernetes cluster automatically detects an installed container runtime<a id="_idIndexMarker134"/> by scanning through the Unix domain sockets, if there are any, within your local machine. The <strong class="bold">Unix domain socket</strong> uses <strong class="bold">Transmission Control Protocol</strong> (<strong class="bold">TCP</strong>) as the underlying transport<a id="_idIndexMarker135"/> protocol. It is used for bidirectional data communication happening on the same operating system. We talked about how to install and configure container runtime in <a href="B18201_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a> – please follow those instructions. </li>
</ul>
<p>Before we get started, let’s get the following checklist done.</p>
<h3>Checking whether swap is disabled </h3>
<p>For <strong class="source-inline">kubeadm</strong>, we have to disable <strong class="source-inline">swap</strong> in order to make <strong class="source-inline">kubelet</strong> work correctly, you can disable <strong class="source-inline">swap</strong> by doing the following: </p>
<p class="source-code">sudo swapoff -a</p>
<h3>Checking the container runtime </h3>
<p>You can check the path to the Unix domain socket as instructed to verify your container runtime – this path is detectable by Kubernetes. Following the instructions to install Docker covered earlier in this chapter, you will find the Unix domain path under the <strong class="source-inline">/var/run/dockershim.sock</strong> path once you have installed the <strong class="source-inline">kubelet</strong> agent. To validate that Docker has been installed successfully, run the <strong class="source-inline">docker ps</strong> command: </p>
<p class="source-code">sudo docker ps</p>
<p>The outcome of the following command is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<img alt="Figure 2.7 – Checking the Docker runtime " height="31" src="image/Figure_2.07_B18201.jpg" width="505"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.7 – Checking the Docker runtime</p>
<p>If you have installed <strong class="source-inline">containerd</strong> as the container<a id="_idIndexMarker136"/> runtime, which we covered earlier in this chapter under the <em class="italic">Container runtimes</em> section, you will find the Unix domain path under the <strong class="source-inline">/run/containerd/containerd.sock</strong> path as the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer023">
<img alt="Figure 2.8 – Checking the containerd runtime " height="16" src="image/Figure_2.08_B18201.jpg" width="737"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.8 – Checking the containerd runtime</p>
<p><strong class="source-inline">kubeadm</strong> picks <strong class="source-inline">docker</strong> over <strong class="source-inline">containerd</strong> as the container runtime when both the <strong class="source-inline">docker</strong> and <strong class="source-inline">containerd</strong> runtimes are detected. At the time of writing, as announced at the beginning of Jan 2022, Kubernetes is removing <strong class="source-inline">dockershim</strong> in the upcoming v1.24 release. This is not surprising at all since it was first announced in Dec 2020 and Kubernetes’ built-in <strong class="source-inline">dockershim</strong> component was deprecated in Kubernetes v1.20. In most cases, it won’t affect the applications running in Kubernetes or the build process of the containerized applications if the following conditions are satisfied:</p>
<ul>
<li>There’s no privileged root permission applied at the container level while it executes inside the pods using Docker commands and it restarts <strong class="source-inline">docker.service</strong> with <strong class="source-inline">systemctl</strong></li>
<li>Docker configuration files such as /<strong class="source-inline">etc/docker/daemon.json </strong>are modified</li>
</ul>
<p>At this point, the official Kubernetes documentation has published this article to help users check whether <strong class="source-inline">dockershim</strong> deprecation will impact them. Check it out here for more ways to check<a id="_idIndexMarker137"/> the dependencies on Docker: <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/#find-docker-dependencies">https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/#find-docker-dependencies</a>.</p>
<h3>Checking whether the ports required by Kubernetes are opened </h3>
<p>We also need to check<a id="_idIndexMarker138"/> if certain ports are open on your local machines prior to installing <strong class="source-inline">kubeadm</strong>. You can use the <strong class="source-inline">telnet</strong> command to do so: </p>
<p class="source-code">telnet 127.0.0.1 6443</p>
<p>You can check the official<a id="_idIndexMarker139"/> documentation to make sure the ports and protocols used by Kubernetes are available by visiting this link: <a href="https://kubernetes.io/docs/reference/ports-and-protocols/">https://kubernetes.io/docs/reference/ports-and-protocols/</a>.</p>
<h3>Ensuring iptables sees bridged traffic</h3>
<p>Make sure your Linux node’s <strong class="source-inline">iptables</strong> is correctly configured to be able to watch the bridged traffic. You can set the <strong class="source-inline">net.bridge.bridge-nf-call-iptables</strong> parameter to a value of <strong class="source-inline">1</strong>, just as we did here: </p>
<p class="source-code">cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf</p>
<p class="source-code">br_netfilter</p>
<p class="source-code">EOF</p>
<p class="source-code">cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf</p>
<p class="source-code">net.bridge.bridge-nf-call-ip6tables = 1</p>
<p class="source-code">net.bridge.bridge-nf-call-iptables = 1</p>
<p class="source-code">EOF</p>
<p class="source-code">sudo sysctl --system</p>
<p>You’ll see an output similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer024">
<img alt="Figure 2.9 – iptables watching bridged traffic " height="704" src="image/Figure_2.09_B18201.jpg" width="594"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.9 – iptables watching bridged traffic</p>
<p>The preceding screenshot<a id="_idIndexMarker140"/> shows the values in <strong class="source-inline">iptables</strong> have been updated. </p>
<h3>Checking whether you have installed kubectl </h3>
<p><strong class="source-inline">kubectl</strong> is the command-line utility that you can use to talk to the Kubernetes cluster. Using the <strong class="source-inline">kubectl version</strong> command, you can verify whether <strong class="source-inline">kubectl</strong> has been successfully installed: </p>
<p class="source-code">kubectl version --client</p>
<p>A successful installation will show an output similar to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer025">
<img alt="Figure 2.10 – Checking the kubectl version " height="45" src="image/Figure_2.10_B18201.jpg" width="1492"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.10 – Checking the kubectl version</p>
<p>Make sure you have completed the checklist<a id="_idIndexMarker141"/> in this section before moving on to the next section. These tools and requirements are essential and you may use them accordingly in the future. </p>
<h1 id="_idParaDest-45"><a id="_idTextAnchor049"/>Using minikube to set up a single node Kubernetes cluster</h1>
<p>Creating a Kubernetes cluster<a id="_idIndexMarker142"/> using <strong class="source-inline">minikube</strong> is the easiest way to spin<a id="_idIndexMarker143"/> up a local Kubernetes cluster and it can be achieved in a matter of minutes. Here’s what you need to do.</p>
<h3>Installing minikube </h3>
<p>Follow these<a id="_idIndexMarker144"/> steps to install <strong class="source-inline">minikube</strong>:</p>
<ol>
<li>On your local or cloud-based Linux VM, use the <strong class="source-inline">curl</strong> command to retrieve the <strong class="source-inline">minikube</strong> binary, and then install it under <strong class="source-inline">/usr/local/bin/minikube</strong> as follows: <p class="source-code"><strong class="bold">curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64</strong></p><p class="source-code"><strong class="bold">sudo install minikube-linux-amd64 /usr/local/bin/minikube</strong></p></li>
<li>You can go to <strong class="source-inline">/usr/local/bin/minikube</strong> to check whether<a id="_idIndexMarker145"/> you have successfully installed the <strong class="source-inline">minikube</strong> binary before moving to the next steps or you can also check by typing the following command into the terminal:<p class="source-code"><strong class="bold">minikube –-help</strong></p></li>
</ol>
<h3>Using minikube to provision a single node Kubernetes cluster </h3>
<p>Follow these steps to use minikube to provision a single node Kubernetes cluster:</p>
<ol>
<li>When using <strong class="source-inline">minikube</strong> to provision a single node<a id="_idIndexMarker146"/> Kubernetes cluster, you can simply use the <strong class="source-inline">minikube start</strong> command: <p class="source-code"><strong class="bold">minikube start</strong></p></li>
<li>You can also set up the CPU cores and memory to start your <strong class="source-inline">minikube</strong> cluster by adding a <strong class="source-inline">--memory</strong> an<a id="_idTextAnchor050"/>d <strong class="source-inline">--cpus</strong> flag as follows: <p class="source-code"><strong class="bold">minikube start --memory 8192 --cpus 4</strong></p></li>
</ol>
<p>After the command is executed, it kicks off the <strong class="source-inline">minikube</strong> cluster provisioning process. You’ll see an output similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer026">
<img alt="Figure 2.11 – Spinning up a minikube cluster " height="306" src="image/Figure_2.11_B18201.jpg" width="911"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.11 – Spinning up a minikube cluster</p>
<p>By the end, you will see a message<a id="_idIndexMarker147"/> telling you we’re ready to use the <strong class="source-inline">minikube</strong> Kubernetes cluster (as concluded in the preceding screenshot).</p>
<h3>Verifying the minikube cluster installation </h3>
<p>Your <strong class="source-inline">minikube</strong> cluster contains<a id="_idIndexMarker148"/> one node that serves as both the control plane and worker node. That means that once you have it set up, you can start to schedule workloads in your local Kubernetes cluster. You can use the following command to see whether the node is ready to use:</p>
<p class="source-code">kubectl get node </p>
<p>You can also use the shortcut of this command: </p>
<p class="source-code">alias k=kubectl </p>
<p class="source-code">k get no</p>
<p>The output will show you the following:</p>
<ul>
<li>The status of the node and whether it’s ready to use</li>
<li>The role of that node</li>
<li>The Kubernetes version </li>
<li>The age of that node since it’s been deployed </li>
</ul>
<p>Here is the output:</p>
<div>
<div class="IMG---Figure" id="_idContainer027">
<img alt="Figure 2.12 – Checking the Docker runtime " height="55" src="image/Figure_2.12_B18201.jpg" width="550"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.12 – Checking the Docker runtime</p>
<h3>Configuring the minikube cluster </h3>
<p>If you’d like to configure the <strong class="source-inline">minikube</strong> cluster without<a id="_idIndexMarker149"/> reprovisioning a new one, you need to stop the <strong class="source-inline">minikube</strong> cluster using the <strong class="source-inline">minikube stop</strong> command. </p>
<p>The <strong class="source-inline">minikube config set</strong> command will help you apply the settings such as CPU and memory that you’ll allocate to the <strong class="source-inline">minikube</strong> cluster. After configuring the <strong class="source-inline">minikube</strong> cluster, you need to start the <strong class="source-inline">minikube</strong> cluster and from there, you’ll be working on the cluster with the new configurations. </p>
<p>Here’s the process to configure <strong class="source-inline">minikube</strong> using more memory and CPUs: </p>
<p class="source-code">minikube stop</p>
<p class="source-code">minikube config set memory 8192</p>
<p class="source-code">minikube config set cpus 4</p>
<p class="source-code">minikube start</p>
<p>After that, you can continue<a id="_idIndexMarker150"/> to play with the <strong class="source-inline">minikube</strong> cluster. In case you have any questions about how the commands work, use the <strong class="source-inline">minikube config - - help</strong> command to get help. </p>
<h3>Deleting a minikube cluster </h3>
<p>The following command deletes<a id="_idIndexMarker151"/> all local Kubernetes clusters and all profiles:</p>
<p class="source-code">minikube delete --all</p>
<p>What you learned from this section can be used repeatedly every time you need a local Kubernetes cluster. You can replicate what you have learned from this section for quick testing of the latest Kubernetes release for most of the new features featured in the release note: <a href="https://github.com/kubernetes/kubernetes/releases">https://github.com/kubernetes/kubernetes/releases</a>.</p>
<p>However, most enterprise-grade environments will not be satisfied with a single node cluster. They are mostly multi-node setups. In the next section, we will dive into creating a Kubernetes multi-node cluster with <strong class="source-inline">kubeadm</strong>. </p>
<h1 id="_idParaDest-46"><a id="_idTextAnchor051"/>Using kubeadm to install a basic Kubernetes cluster</h1>
<p>In this section, we will create<a id="_idIndexMarker152"/> a multi-node Kubernetes<a id="_idIndexMarker153"/> cluster using <strong class="source-inline">kubeadm</strong>. The following are the steps we need to achieve the goal:</p>
<ol>
<li>Install <strong class="source-inline">kubeadm</strong>.</li>
<li>Bootstrap a master node where your control plane will be located</li>
<li>Install the network plugins (we will get to the detailed supported plugins later in this chapter and use Calico as an example in that section).</li>
<li>Bootstrap the worker nodes.</li>
<li>Join the worker nodes to the control plane.</li>
</ol>
<p>Before getting started, you need<a id="_idIndexMarker154"/> to make sure your master node<a id="_idIndexMarker155"/> meets all the technical requirements listed in this chapter. </p>
<p>We’ll deploy a basic Kubernetes cluster by going through the steps described in this section, as shown in <em class="italic">Figure 2.7</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer028">
<img alt="Figure 2.13 – The workflow of using kubeadm to spin up a basic Kubernetes cluster " height="702" src="image/Figure_2.13_B18201.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.13 – The workflow of using kubeadm to spin up a basic Kubernetes cluster</p>
<p>The Kubernetes cluster will be similar to the architecture featured in <em class="italic">Figure 2.14</em>: </p>
<div>
<div class="IMG---Figure" id="_idContainer029">
<img alt="Figure 2.14 – A standard multi-node Kubernetes cluster  " height="967" src="image/Figure_2.14_B18201.jpg" width="1251"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.14 – A standard multi-node Kubernetes cluster </p>
<p>From now on, you can follow<a id="_idIndexMarker156"/> these instructions to create a multi-node<a id="_idIndexMarker157"/> Kubernetes cluster. To create a Kubernetes cluster using <strong class="source-inline">kubeadm</strong>, its default settings conform to best practices of setting up a standard Kubernetes cluster. This set of best practices is encapsulated as Kubernetes Conformance tests. Check out the details<a id="_idIndexMarker158"/> about the Kubernetes Conformance Program here: <a href="https://kubernetes.io/blog/2017/10/software-conformance-certification/">https://kubernetes.io/blog/2017/10/software-conformance-certification/</a>.</p>
<h3>Installing kubeadm</h3>
<p>We introduced<a id="_idIndexMarker159"/> setting up <strong class="source-inline">docker</strong> or <strong class="source-inline">containerd</strong> as the container runtime – we can then install <strong class="source-inline">kubeadm</strong> by following these instructions:</p>
<ol>
<li>Update the <strong class="source-inline">apt</strong> package index, add the Google<a id="_idIndexMarker160"/> Cloud public signing key, and set up the Kubernetes <strong class="source-inline">apt</strong> repository by running the following instructions: <p class="source-code"><strong class="bold">sudo apt-get update</strong></p><p class="source-code"><strong class="bold">sudo apt-get install -y apt-transport-https ca-certificates curl</strong></p><p class="source-code"><strong class="bold">sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg</strong></p><p class="source-code"><strong class="bold">echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list</strong></p></li>
<li>Start by updating the <strong class="source-inline">ap</strong>t package index and then install <strong class="source-inline">kubelet</strong> and <strong class="source-inline">kubeadm</strong>: <p class="source-code"><strong class="bold">sudo apt-get update</strong></p><p class="source-code"><strong class="bold">sudo apt-get install -y kubelet kubeadm </strong></p></li>
<li>Here, if you haven’t installed <strong class="source-inline">kubectl</strong> yet, you can also install <strong class="source-inline">kubelet</strong>, <strong class="source-inline">kubeadm</strong>, and <strong class="source-inline">kubectl</strong> in one go: <p class="source-code"><strong class="bold">sudo apt-get update</strong></p><p class="source-code"><strong class="bold">sudo apt-get install -y kubelet kubeadm kubectl </strong></p></li>
<li>Use the following command to pin the version of the utilities you’re installing: <p class="source-code"><strong class="bold">sudo apt-mark hold kubelet kubeadm kubectl </strong></p></li>
</ol>
<p>The output shows those packages are set on hold as shown in <em class="italic">Figure 2.9</em>: </p>
<div>
<div class="IMG---Figure" id="_idContainer030">
<img alt="Figure 2.15 – Checking the containerd runtime " height="124" src="image/Figure_2.15_B18201.jpg" width="1118"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.15 – Checking the containerd runtime</p>
<ol>
<li>From here, you can check whether <strong class="source-inline">kubeadm</strong> has been successfully installed by typing <strong class="source-inline">kubeadm</strong> into the command<a id="_idIndexMarker161"/> shell. Here’s the output of the command: </li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer031">
<img alt="Figure 2.16 – Checking the containerd runtime " height="871" src="image/Figure_2.16_B18201.jpg" width="1232"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.16 – Checking the containerd runtime</p>
<ol>
<li>To verify that <strong class="source-inline">kubelet</strong> is present on the master node, you can use the <strong class="source-inline">which kubelet</strong> command, which returns the location of the <strong class="source-inline">kubelet</strong> agent: </li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer032">
<img alt="Figure 2.17 – Checking kubelet’s presence " height="62" src="image/Figure_2.17_B18201.jpg" width="1022"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.17 – Checking kubelet’s presence</p>
<p>As you have successfully installed <strong class="source-inline">kubeadm</strong> and <strong class="source-inline">kubelet</strong>, you can now start initiating a control plane. </p>
<p>Here, we will show an optional operation where you can use <strong class="source-inline">images pull</strong> to pre-pull the images that are required to set up the Kubernetes cluster: </p>
<p class="source-code">sudo kubeadm config images pull</p>
<p>The output should be similar to the following screenshot: </p>
<div>
<div class="IMG---Figure" id="_idContainer033">
<img alt="Figure 2.18 – Pre-pulling the images " height="230" src="image/Figure_2.18_B18201.jpg" width="1022"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.18 – Pre-pulling the images</p>
<p>Note that the preceding<a id="_idIndexMarker162"/> operation is optional – you’re free to skip it and go straight to the next section. </p>
<h3>Bootstrapping a master node </h3>
<p>You can use the <strong class="source-inline">kubeadm init</strong> command<a id="_idIndexMarker163"/> to initiate the control plane as a regular<a id="_idIndexMarker164"/> user and gain <strong class="source-inline">sudo</strong> privileges from your master node machine by using the following command:</p>
<p class="source-code">sudo kubeadm init --pod-network-cidr=192.168.0.0/16</p>
<p>You will see an output similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer034">
<img alt="Figure 2.19 – The control plane initiated successfully " height="427" src="image/Figure_2.19_B18201.jpg" width="1258"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.19 – The control plane initiated successfully</p>
<p>After your Kubernetes <strong class="source-inline">control-plane</strong> is initialized successfully, you can execute the following commands to configure <strong class="source-inline">kubectl</strong>: </p>
<p class="source-code">mkdir -p $HOME/.kube</p>
<p class="source-code">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</p>
<p class="source-code">sudo chown $(id -u):$(id -g) $HOME/.kube/config</p>
<p>If you’re a root user, you can use the following: </p>
<p class="source-code">export KUBECONFIG=/etc/kubernetes/admin.conf</p>
<p>Then, the next step<a id="_idIndexMarker165"/> is to deploy a pod network to the Kubernetes<a id="_idIndexMarker166"/> cluster. </p>
<h3>Installing the networking plugins </h3>
<p>In order for the pods<a id="_idIndexMarker167"/> to talk to each other, you can deploy<a id="_idIndexMarker168"/> the networking by enabling <strong class="bold">Container Network Interface</strong> (<strong class="bold">CNI</strong>) plugin. The CNI plugins conform to the CNI<a id="_idIndexMarker169"/> specification, and as per the official Kubernetes documentation, Kubernetes follows the v0.4.0 release of the CNI specification. </p>
<p>There’s a wide range of networking plugins working with Kubernetes – we will dive into Kubernetes networking in <a href="B18201_07.xhtml#_idTextAnchor235"><em class="italic">Chapter 7</em></a>, <em class="italic">Demystifying Kubernetes Networking</em>. Here are some add-ons options: </p>
<ul>
<li>Calico </li>
<li>Flannel</li>
<li>Weave Net </li>
</ul>
<p>For all the possible<a id="_idIndexMarker170"/> options acknowledged by the Kubernetes community, please check out the official documentation: <a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/">https://kubernetes.io/docs/concepts/cluster-administration/addons/</a>. You can check out the links from this page to get the installation instructions for the respective options. </p>
<p>Here, we’re going to use the Calico plugin as the overlay network for our Kubernetes cluster. It is a Kubernetes CNI networking provider and it allows you to write up the network policies, which means that it supports a set of networking options to suit your different requirements. Here’s how we’ll approach it: </p>
<ol>
<li>Deploy the Tigera Calico <strong class="bold">Custom Resource Definitions</strong> (<strong class="bold">CRDs</strong>) and operator by using<a id="_idIndexMarker171"/> the <strong class="source-inline">kubectl create -f</strong> command: <p class="source-code"><strong class="bold">kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml</strong></p><p class="source-code"><strong class="bold">kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml</strong></p></li>
<li>You can use the <strong class="source-inline">watch</strong> command<a id="_idIndexMarker172"/> to monitor the pod status<a id="_idIndexMarker173"/> in the process: <p class="source-code"><strong class="bold">watch kubectl get pods -n calico-system</strong></p></li>
</ol>
<p>Alternatively, use the following alternative command:</p>
<p class="source-code"><strong class="bold">kubectl get pods -n calico-system -w</strong></p>
<p>Now, you can see the pods have a <strong class="source-inline">Running</strong> status:</p>
<div>
<div class="IMG---Figure" id="_idContainer035">
<img alt="Figure 2.20 – The control plane initiated successfully " height="92" src="image/Figure_2.20_B18201.jpg" width="528"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.20 – The control plane initiated successfully</p>
<ol>
<li>For the Kubernetes cluster created by <strong class="source-inline">kubeadm</strong>, there’s a taint by default for master nodes. Therefore, we need to remove taints so that the master node is available to schedule pods. To remove the taint, you can use the following command: <p class="source-code"><strong class="bold">kubectl taint nodes --all node-role.kubernetes.io/master-</strong></p></li>
</ol>
<p>The following screenshot shows that the taint on the master node has been successfully removed:</p>
<div>
<div class="IMG---Figure" id="_idContainer036">
<img alt="Figure 2.21 – Removing the taint on the master node successfully " height="62" src="image/Figure_2.21_B18201.jpg" width="1286"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.21 – Removing the taint on the master node successfully</p>
<ol>
<li>You can use the following command to check out the current nodes that are available: <p class="source-code"><strong class="bold">kubectl get no</strong></p></li>
<li>To get more information from the node, you can use the following command: <p class="source-code"><strong class="bold">kubectl get no -o wide</strong></p></li>
</ol>
<p>The following screenshot shows the sample output:</p>
<div>
<div class="IMG---Figure" id="_idContainer037">
<img alt="Figure 2.22 – The Kubernetes node status " height="64" src="image/Figure_2.22_B18201.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.22 – The Kubernetes node status</p>
<p>From the preceding command<a id="_idIndexMarker174"/> output, you can see the Kubernetes node<a id="_idIndexMarker175"/> is operational after enabling the CNI networking and it has been assigned an internal IP address. </p>
<h3>Bootstrapping the worker nodes</h3>
<p>To add more worker nodes<a id="_idIndexMarker176"/> to the Kubernetes cluster, we will SSH to the client<a id="_idIndexMarker177"/> machine, and make sure the worker nodes meet the same technical requirements as the master node. Check out the <em class="italic">Prerequisites for installing a Kubernetes cluster</em> section of this chapter and refer to the information on <strong class="source-inline">kubeadm</strong> for more details. Make sure you have installed the container runtime and <strong class="source-inline">kubeadm</strong>, although <strong class="source-inline">kubectl</strong> is optional for worker nodes since we usually use the master node for management. </p>
<h3>Joining the worker nodes to the control plane</h3>
<p>We can go ahead<a id="_idIndexMarker178"/> with installing <strong class="source-inline">kubeadm</strong> for the master node after<a id="_idIndexMarker179"/> making sure that your worker nodes and local environment meet the technical requirements that we set, as we mentioned earlier in this section. As introduced in <a href="B18201_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Kubernetes Overview</em>, the worker nodes are where your containerized workloads are up and running. </p>
<p>You can use the following command to join the worker nodes to the Kubernetes cluster. This command can be used repeatedly each time you have to join new worker nodes:</p>
<p class="source-code">sudo kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;</p>
<p>You can actually go back and copy the output of the master node control plane, which would look similar to the following sample command: </p>
<p class="source-code">sudo kubeadm join 172.16.16.129:6443 --token k626hm.oqwyac35h43x80mg   --discovery-token-ca-cert-hash sha256:889983</p>
<p class="source-code">a6b87643e598b88533dbe3a68643a623b9a0ed9380561c6a7dbb93b3f0</p>
<p>You can use the preceding<a id="_idIndexMarker180"/> command to join the worker node<a id="_idIndexMarker181"/> to the control plane and set up your Kubernetes cluster with multiple worker nodes. </p>
<h1 id="_idParaDest-47"><a id="_idTextAnchor052"/>Setting up a highly available cluster with kubeadm</h1>
<p>In <a href="B18201_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Kubernetes Overview</em>, we introduced the cluster<a id="_idIndexMarker182"/> architecture, which gives us two options: setting up a single node Kubernetes<a id="_idIndexMarker183"/> cluster for dev/test quick testing or setting up a multi-node Kubernetes cluster for more professional use, or even use in production. A standard configuration would be one master with multiple worker nodes. As we stated in the previous chapter, the Kubernetes master node is where the control plane resides. In the event of a master node going down, either the containerized workloads up and running in the worker nodes will still keep running until the worker node is off the grid for some reason or there are no available master nodes, meaning no new workloads will be scheduled to the worker node. </p>
<p>There are two options available to build a HA Kubernetes cluster:</p>
<ul>
<li><strong class="bold">Building multiple master nodes</strong>: This is the option where the control plane nodes and etcd members<a id="_idIndexMarker184"/> co-exist in the same master nodes. <em class="italic">Figure 2.16</em> shows the stacked etcd topology: </li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer038">
<img alt="Figure 2.23 – A stacked etcd topology for a HA kubeadm cluster " height="919" src="image/Figure_2.23_B18201.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.23 – A stacked etcd topology for a HA kubeadm cluster</p>
<p>This topology makes<a id="_idIndexMarker185"/> the cluster more resilient<a id="_idIndexMarker186"/> compared to the basic Kubernetes cluster architecture that we built in this chapter, thanks to the redundancy of the master node. In case one master node goes down, it’s easy to switch to another available master node to ensure the health of the entire Kubernetes cluster. </p>
<p>However, in some cases where we need to manage the cluster and replicate the cluster information, the external etcd typology comes in. </p>
<ul>
<li><strong class="bold">Building an external etcd cluster</strong>: Compared to the previous option, the key idea of this option<a id="_idIndexMarker187"/> is to decouple the etcd store to a separate infrastructure since the etcd, as we mentioned in <a href="B18201_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, is where Kubernetes stores the cluster and the state information about the Kubernetes objects. The <strong class="source-inline">kubeadm</strong> HA topology architecture for an external etcd cluster is shown in <em class="italic">Figure 2.24</em>: </li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer039">
<img alt="Figure 2.24 – The topology for an external etcd HA kubeadm cluster " height="1062" src="image/Figure_2.24_B18201.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 2.24 – The topology for an external etcd HA kubeadm cluster</p>
<p>As shown in <em class="italic">Figure 2.24</em>, the external etcd is a cluster and it communicates with the API server of each control plane. In the event of the control plane node going down, we won’t lose all the information stored in the etcd store. It also makes the control plane more decoupled and manageable, as we only need to add more control plane nodes. A loss of the control plane node<a id="_idIndexMarker188"/> won’t be as impactful as it would<a id="_idIndexMarker189"/> with the stacked etcd topology. </p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor053"/>Summary</h1>
<p>This chapter covers the very first job for most Kubernetes administrators who are setting up a Kubernetes cluster with a single worker node or with multiple worker nodes. The various tools introduced in this chapter will help your daily routine at work beyond the exam. Nevertheless, this is also one of the most time-consuming tasks in the CKA exam. Practice, practice, and more practice will help you get the hang of it. Knowing the HA topology for a Kubernetes cluster will also help you address the requirements of the organization that you’ll be working for as a Kubernetes administrator. As you master the setup process for a basic Kubernetes cluster, it will become easier to apply your skills to different typologies. </p>
<p>In the next chapter, we’ll talk about Kubernetes cluster maintenance, including some important topics such as upgrades to Kubernetes components, which is quite an essential task in the daily work of a Kubernetes administrator. Touching on external etcd typology in this chapter is just a start, as we’ll dive into more interesting work with etcd in the next chapter. Happy learning!</p>
<h1 id="_idParaDest-49"><a id="_idTextAnchor054"/>Mock CKA scenario-based practice test </h1>
<p>You have two VMs, <em class="italic">master-0</em> and <em class="italic">worker-0</em>. Please complete the following mock scenarios. </p>
<h2 id="_idParaDest-50"><strong class="bold"><a id="_idTextAnchor055"/>Scenario 1</strong>: </h2>
<p>Install the latest version of <strong class="source-inline">kubeadm</strong>, then create a basic <strong class="source-inline">kubeadm</strong> cluster on the <strong class="source-inline">master-0</strong> node, and get the node information. </p>
<h2 id="_idParaDest-51"><strong class="bold"><a id="_idTextAnchor056"/>Scenario 2</strong>: </h2>
<p>SSH to <strong class="source-inline">worker-0</strong> and join it to the <strong class="source-inline">master-0</strong> node. </p>
<h2 id="_idParaDest-52"><strong class="bold"><a id="_idTextAnchor057"/>Scenario 3 (optional)</strong>:</h2>
<p>Set up a local <strong class="source-inline">minikube</strong> cluster and schedule your first workload, called <strong class="source-inline">hello Packt</strong> </p>
<p>You can find all the scenario resolutions in <a href="B18201_Appendix_A.xhtml#_idTextAnchor386"><em class="italic">Appendix</em></a><em class="italic"> - Mock CKA scenario-based practice test resolutions</em> of this book.</p>
<h1 id="_idParaDest-53"><a id="_idTextAnchor058"/>FAQs</h1>
<ul>
<li><em class="italic">Where should I start to test the Kubernetes cluster? </em></li>
</ul>
<p>You can start on your local laptop or desktop on Windows, Linux, or Mac OS, and we recommend using VMware player or Hyper-V to spin up multiple VMs so you can test out a multinode scenario. Using Multipass from Canonical is also great for creating Ubuntu VMs and it supports Linux, Mac, and Windows. Check it out here: <a href="https://multipass.run/">https://multipass.run/</a>.</p>
<p>Another option is to get a cloud subscription such as Microsoft Azure, AWS, or GCP, using which you can provision a VM with a click-through experience. </p>
<ul>
<li><em class="italic">Where can I find the latest Kubernetes release to test out? </em></li>
</ul>
<p>The Kubernetes GitHub repository is where you can find all the releases as well as changelogs, and you can get the latest release and build it by yourself: <a href="https://github.com/kubernetes/kubernetes">https://github.com/kubernetes/kubernetes</a>.</p>
<p>We can also use <strong class="source-inline">kubeadm</strong> or <strong class="source-inline">minikube</strong> to get Kubernetes, as they are aligned with the Kubernetes source code delivery cycle and are up to date. </p>
</div>
</div></body></html>