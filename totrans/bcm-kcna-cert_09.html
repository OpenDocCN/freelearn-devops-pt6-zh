<html><head></head><body>
		<div id="_idContainer052">
			<h1 id="_idParaDest-96" class="chapter-number"><a id="_idTextAnchor095"/>9</h1>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor096"/>Understanding Cloud Native Architectures</h1>
			<p>With this chapter, we are moving on to explore further aspects of Cloud Native in more detail. We will see which concepts are a part of Cloud Native and what is the architecture of Cloud Native, talk about <strong class="bold">resiliency</strong> and <strong class="bold">autoscaling</strong>, and get to know some of the best practices. This chapter covers further requirements of the <em class="italic">Cloud Native Architecture</em> domain of the <strong class="bold">Kubernetes and Cloud Native Associate</strong> (<strong class="bold">KCNA</strong>) exam, which makes up a total of 16% <span class="No-Break">of questions.</span></p>
			<p>This chapter has no practical part, so we won’t perform any hands-on exercises, but it is still very important to understand it in order to pass the KCNA exam and advance in the field. We will focus on the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Cloud <span class="No-Break">Native architectures</span></li>
				<li>Resiliency <span class="No-Break">and autoscaling</span></li>
				<li><span class="No-Break">Serverless</span></li>
				<li>Cloud Native <span class="No-Break">best practices</span></li>
			</ul>
			<p>If you’ve skipped the first two chapters of the book and you find some terms discussed here unclear, please go back and read <em class="italic">Chapters 1</em> and <em class="italic">2</em> to cover the <span class="No-Break">gaps first.</span></p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor097"/>Cloud Native architectures</h1>
			<p>In the first two chapters, we’ve already covered the definition<a id="_idIndexMarker529"/> of Cloud Native. Let’s check it one more time for a <span class="No-Break">quick recap.</span></p>
			<p class="callout-heading">Cloud Native</p>
			<p class="callout">This is an approach to building and running<a id="_idIndexMarker530"/> applications on modern, dynamic infrastructures such as clouds. It emphasizes application workloads with high resiliency, scalability, a high degree of automation, ease of management, <span class="No-Break">and observability.</span></p>
			<p>Yet, despite the presence of the word <em class="italic">cloud</em>, a Cloud Native application is not strictly required to run in the cloud. It’s an approach that can be followed when building and running applications also on-premises. And yes—you can also build resilient, scalable, highly automated applications on-premises that are Cloud Native, albeit not running in <span class="No-Break">the cloud.</span></p>
			<p>It is important to understand<a id="_idIndexMarker531"/> that simply picking a well-known public cloud provider and building on top of its service offerings (whether IaaS, PaaS, SaaS, or FaaS) does not mean your application automatically becomes Cloud Native. For example, if your application requires manual intervention to start then it cannot scale automatically, and it won’t be possible to restart it automatically in case of failure, so it’s not <span class="No-Break">Cloud Native.</span></p>
			<p>Another example: you have a web application consisting of a few microservices, but it is deployed and configured manually without any automation and thus cannot be easily updated. This is not a Cloud Native approach. If you forgot about microservices in the meantime, here is a definition one more <span class="No-Break">time .</span></p>
			<p class="callout-heading">Microservices</p>
			<p class="callout">These are small applications<a id="_idIndexMarker532"/> that work together as a part of a larger application or service. Each microservice could be responsible for a single feature of a big application and communicate with other microservices over the network. This is the opposite of monolithic applications, which bundle all functionality and logic in one big deployable piece (<span class="No-Break">tightly coupled).</span></p>
			<p>In practice, to implement a Cloud Native application, you’ll most likely turn to microservices. Microservices are loosely coupled, which makes it possible to scale, develop, and update them independently from each other. Microservices solve many problems but they also add operational overhead as their number grows, and that is why many of the Cloud Native technologies such as Kubernetes and Helm aim to make microservices easy to deploy, manage, <span class="No-Break">and scale.</span></p>
			<p>Again, strictly speaking, you are not required to run microservices managed by Kubernetes to have Cloud Native<a id="_idIndexMarker533"/> architecture. But this combination of containerized, small, loosely coupled applications orchestrated by Kubernetes works very well and paves the way toward Cloud Native. It is much, much easier to implement resiliency, autoscaling, controllable rolling updates, and observability with Kubernetes<a id="_idIndexMarker534"/> and containers than doing so with many <strong class="bold">virtual machines</strong> (<strong class="bold">VMs</strong>) and homegrown shell scripts. Kubernetes is also infrastructure-agnostic, which makes it very attractive for many environments and use cases. It can run on bare-metal servers, on VMs, in public and private clouds, and could be consumed as a managed service or run in a hybrid environment consisting of cloud resources and on-premises <span class="No-Break">data centers.</span></p>
			<p>Now, before diving deeper into some of the aspects, let’s see at a high level the benefits<a id="_idIndexMarker535"/> Cloud <span class="No-Break">Native provides:</span></p>
			<ul>
				<li><strong class="bold">Reduced time to market (TTM) </strong>– A high degree of automation and easy updates make it possible to deliver new Cloud Native application features rapidly, which offers a competitive advantage for <span class="No-Break">many businesses.</span></li>
				<li><strong class="bold">Cost efficiency </strong>– Cloud Native applications scale based on demand, which means paying only for resources required and <span class="No-Break">eliminating waste.</span></li>
				<li><strong class="bold">Higher reliability </strong>– Cloud Native applications can self-heal and automatically recover from failures. This means reduced system downtime, resulting in a better <span class="No-Break">user experience.</span></li>
				<li><strong class="bold">Scalability and flexibility </strong>– Microservices can be scaled, developed, and updated individually allowing us to handle various scenarios and provide more flexibility for <span class="No-Break">development teams.</span></li>
				<li><strong class="bold">No vendor lock-in </strong>– With the right approach and use of open source technologies, a Cloud Native application could be shifted between different infrastructures or cloud providers with <span class="No-Break">minimum effort.</span></li>
			</ul>
			<p>The list can be continued, but it should be enough to give you an idea of why most modern applications follow Cloud Native approaches and practices. Moving on, we will focus on some of the most important aspects of Cloud Native, such as resiliency <span class="No-Break">and autoscaling.</span></p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor098"/>Resiliency and autoscaling</h1>
			<p>As funny as it may<a id="_idIndexMarker536"/> sound, in order to design<a id="_idIndexMarker537"/> and build resilient systems, we need to expect<a id="_idIndexMarker538"/> things to fail<a id="_idIndexMarker539"/> and break apart. In other words, in order to engineer resilient systems, we need to engineer for failure and provide ways for applications and infrastructure to recover from <span class="No-Break">failures automatically.</span></p>
			<p class="callout-heading">Resiliency</p>
			<p class="callout">This characterizes an application and infrastructure that can automatically recover from failures. The ability<a id="_idIndexMarker540"/> to recover without manual intervention is often <span class="No-Break">called </span><span class="No-Break"><em class="italic">self-healing</em></span><span class="No-Break">.</span></p>
			<p>We’ve already seen self-healing in action in <a href="B18970_06.xhtml#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> when Kubernetes detected that the <em class="italic">desired state</em> and the <em class="italic">current state</em> were different and quickly spawned additional application replicas. This is possible thanks to the Kubernetes <span class="No-Break"><em class="italic">reconciliation loop</em></span><span class="No-Break">.</span></p>
			<p>There are, of course, ways to build resilient applications<a id="_idIndexMarker541"/> and infrastructure without Kubernetes. For example, the <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) public cloud offers <strong class="bold">Autoscaling Groups</strong>, which allow you to run a desired number of VMs in a group<a id="_idIndexMarker542"/> or increase and decrease the number automatically based on the load. In case of VM failure, it will be detected and reacted upon with the creation of a new VM. In case CPU utilization in the group reaches a certain, pre-defined threshold, new VMs can be provisioned to share the load. And this brings us to another important <span class="No-Break">concept: autoscaling.</span></p>
			<p class="callout-heading">Autoscaling</p>
			<p class="callout">This is an ability to add or reduce computing resources automatically to meet the <span class="No-Break">current demand.</span></p>
			<p>Needless to say, different cloud providers offer many options for configuring autoscaling today. It can be based on various metrics and conditions, where CPU or RAM utilization are just <span class="No-Break">common examples.</span></p>
			<p>Previously, in the times of traditional IT, applications and infrastructure were designed to account for peak system usage, and this resulted in highly underutilized hardware and high running costs. Autoscaling has been a huge improvement, and it became one of the most important features of <span class="No-Break">Cloud Native.</span></p>
			<p>When we talk about autoscaling, it applies to <em class="italic">both the application and the infrastructure it runs on</em>, because scaling one without another won’t be sufficient. Let’s consider the following example<a id="_idIndexMarker543"/> to explain—you manage multiple microservice<a id="_idIndexMarker544"/> applications running in a Kubernetes cluster that under<a id="_idIndexMarker545"/> typical load require 10 worker nodes to operate. If additional worker<a id="_idIndexMarker546"/> nodes are created and joined into the K8s cluster, there won’t be any pods running on those new nodes until one of the <span class="No-Break">following happens:</span></p>
			<ul>
				<li>The number of replicas of applications is increased so that new pods <span class="No-Break">are created</span></li>
				<li>Some of the existing pods exit and get recreated by a controller (such as Deployment, StatefulSet, and <span class="No-Break">so on)</span></li>
			</ul>
			<p>That is because Kubernetes won’t reschedule already running pods when a new node joins the cluster. So, it would be required to increase the number of microservice replicas besides adding new nodes. <em class="italic">Why won’t simply adding more replicas be enough?</em> Doing so will eventually max out the CPU/RAM utilization and make Kubernetes nodes unresponsive as the payloads will be <em class="italic">fighting</em> <span class="No-Break">for resources.</span></p>
			<p>Scaling in general can also be distinguished into two types, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B18970_09_01.jpg" alt="Figure 9.1 – Comparison of horizontal and vertical scaling"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Comparison of horizontal and vertical scaling</p>
			<p>Let’s look at this in <span class="No-Break">more detail:</span></p>
			<ul>
				<li><strong class="bold">Horizontal</strong>—When we add or reduce the number<a id="_idIndexMarker547"/> of instances (VMs or nodes), as in the preceding<a id="_idIndexMarker548"/> example. Horizontal<a id="_idIndexMarker549"/> scaling is also known as <strong class="bold">scaling out</strong> (adding new VMs) and <strong class="bold">scaling in</strong> (<span class="No-Break">terminating VMs).</span></li>
				<li><strong class="bold">Vertical</strong>—When we keep the number of instances<a id="_idIndexMarker550"/> the same but change their<a id="_idIndexMarker551"/> configuration (such as the number of vCPUs, GB of RAM, size of the disks, and so on). Vertical scaling<a id="_idIndexMarker552"/> is also known as <strong class="bold">scaling up</strong> (adding CPU/RAM capacity) and <strong class="bold">scaling down</strong> (reducing <span class="No-Break">CPU/RAM capacity).</span></li>
			</ul>
			<p>To keep it simple, you can memorize horizontal autoscaling as an automatic increase (or decrease) in numbers and vertical autoscaling as an increase (or decrease) in size. Cloud Native microservice architectures commonly<a id="_idIndexMarker553"/> apply horizontal autoscaling, whereas old monolithic applications were scaled<a id="_idIndexMarker554"/> vertically most of the time. Vertical autoscaling is also more restricting because even the largest flavors of VMs and bare-metal servers available today cannot go beyond certain technological limits. Therefore, scaling horizontally is the <span class="No-Break">preferred way.</span></p>
			<p>From the previous chapters, we already know that we can change the number of Deployment replicas with Kubernetes in just one simple command; however, that is not automatic, but a manual scaling. Apart from that, there are three mechanisms in Kubernetes that allow us to implement <span class="No-Break">automatic scaling:</span></p>
			<ul>
				<li><strong class="bold">Horizontal Pod Autoscaler (HPA)</strong>—This updates the workload resource (such as Deployment, StatefulSet) to add more pods<a id="_idIndexMarker555"/> when the load goes up and removes pods when the load goes down to match demand. HPA requires configuration of lower and upper bounds of replicas (for example, 3 replicas minimum and <span class="No-Break">10 maximum).</span></li>
				<li><strong class="bold">Vertical Pod Autoscaler (VPA)</strong>—This updates the workload resource requests<a id="_idIndexMarker556"/> and limits for containers (CPU, memory, huge page size). It can reduce requests and limits of containers <em class="italic">over-requesting</em> resources and scale up requests and limits for <em class="italic">under-requesting</em> workloads based on historical usage over time. VPA does not change the number of replicas like HPA does, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.2</em>. VPA also optionally allows us to define minimum and maximum <span class="No-Break">request boundaries.</span></li>
				<li><strong class="bold">Cluster Autoscaler</strong>—This adjusts the size of the Kubernetes cluster<a id="_idIndexMarker557"/> by adding or removing worker nodes when either there are pods that fail to run due to insufficient resources or there are underutilized nodes for an extended period of time and their pods can be placed on other nodes in the cluster. It is recommendable to limit the maximum number of nodes in the cluster to protect against making the cluster too large. Cluster Autoscaler also requires integration with your cloud provider <span class="No-Break">to operate:</span></li>
			</ul>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B18970_09_02.jpg" alt="Figure 9.2 – Comparison of Kubernetes HPA and VPA"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Comparison of Kubernetes HPA and VPA</p>
			<p>Those mechanisms are especially<a id="_idIndexMarker558"/> powerful as they get<a id="_idIndexMarker559"/> combined. As you remember, we need to scale both application and the infrastructure where it runs, so using only HPA or only using Cluster Autoscaler won’t be sufficient. In fact, all three mechanisms can be used together, but this is a complex topic that you should not approach without getting enough K8s experience first. For the scope of KCNA, you only need to know what autoscaling is and which autoscaling mechanisms Kubernetes has <span class="No-Break">to offer.</span></p>
			<p>At the end of the day, autoscaling<a id="_idIndexMarker560"/> is a crucial part of Cloud Native that is required<a id="_idIndexMarker561"/> to strike a balance between workload<a id="_idIndexMarker562"/> performance<a id="_idIndexMarker563"/> and infrastructure size <span class="No-Break">and costs.</span></p>
			<p>Moving on, we are going to learn more about Serverless—an evolution of computing that gained adoption over the <span class="No-Break">last years.</span></p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor099"/>Serverless</h1>
			<p>In the very first chapter, we briefly touched<a id="_idIndexMarker564"/> on a definition of Serverless—a newer cloud delivery model<a id="_idIndexMarker565"/> that appeared around 2010 and is known as <em class="italic">Function as a Service</em> <span class="No-Break">or </span><span class="No-Break"><em class="italic">FaaS</em></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Serverless</p>
			<p class="callout">This is a computing model where code is written as small functions that are built and run without the need to manage any servers. Those functions are triggered by events (for example, the user clicked a button on the web page, uploaded a file, and <span class="No-Break">so on).</span></p>
			<p>Despite the name, the truth is that Serverless computing still relies on real hardware servers underneath. However, servers running the functions <strong class="bold">are completely abstracted away</strong> from the application development. In this model, the provider handles all operations that are required to run the code: provisioning, scaling, maintenance, security patching, and so on. Since you don’t need to take care of servers ever, the model is <span class="No-Break">called Serverless.</span></p>
			<p>Serverless brings in several advantages besides a lack of routine server operations. The development team can simply upload code to the Serverless platform, and once deployed, the application and infrastructure it runs on will be scaled automatically as needed based <span class="No-Break">on demand.</span></p>
			<p>When a Serverless function idles, most cloud providers won’t charge anything as with no events, there are no executions of the functions and thus no costs are incurred. If there are 10,000 events that trigger 10,000 function executions, then in most cases, their <em class="italic">exact execution times</em> will be billed by the provider. This is different from a typical <em class="italic">pay-as-you-go</em> VM in the cloud where you pay for all time the VM is running, regardless of the actual CPU/RAM utilization. A sample Serverless architecture is shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B18970_09_03.jpg" alt="Figure 9.3 – Serverless architecture example"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Serverless architecture example</p>
			<p class="callout-heading">Note</p>
			<p class="callout">An API Gateway <a id="_idIndexMarker566"/>is a part of Serverless that allows us to define endpoints for the REST API of an application and connect those endpoints with corresponding functions implementing the actual logic. API Gateway typically handles user authentication and access control and often provides additional features <span class="No-Break">for observability.</span></p>
			<p>It’s worth mentioning that many popular programming<a id="_idIndexMarker567"/> languages (Java, Golang, Python, and Ruby, to name a few) are supported by the cloud providers offering Serverless today, yet not all providers allow the use of our own container images. In fact, most public cloud providers offering Serverless today rely on their own proprietary<a id="_idIndexMarker568"/> technology. So, if you develop a Serverless application for <strong class="bold">AWS Lambda</strong> (the Serverless offering of AWS) then migrating it to <strong class="bold">Google Cloud Functions</strong> (the Serverless offering of Google Cloud) will require<a id="_idIndexMarker569"/> <span class="No-Break">considerable effort.</span></p>
			<p>Besides fully managed cloud Serverless platforms, there are a few open source alternatives available today that reduce the risk of vendor lock-in. For example, the following Serverless frameworks can be installed on top of Kubernetes with functions code packaged and run <span class="No-Break">as containers:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">OpenFaaS</strong></span></li>
				<li><span class="No-Break"><strong class="bold">CloudEvents</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Knative</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Fn</strong></span></li>
			</ul>
			<p>Knative<a id="_idIndexMarker570"/> and CloudEvents are currently curated <strong class="bold">Cloud Native Computing Foundation</strong> (<span class="No-Break"><strong class="bold">CNCF</strong></span><span class="No-Break">) projects.</span></p>
			<p>To wrap it up, FaaS can be seen as an evolution<a id="_idIndexMarker571"/> of cloud computing models (IaaS, PaaS, and SaaS) that fits well for Cloud Native architectures. While the Serverless market share is still growing, it has become apparent that it will not replace common VMs and managed platform offerings due to limitations that we briefly <span class="No-Break">cover here:</span></p>
			<ul>
				<li>To persist data, Serverless applications must interact with other stateful components. So, unless you never need to keep the state, you’ll have to involve databases and other <span class="No-Break">storage options.</span></li>
				<li>In most cases, there is little to no control over runtime configuration. For instance, you won’t be able to change OS or <strong class="bold">Java Virtual Machine</strong> (<strong class="bold">JVM</strong>) parameters when using FaaS offered by a <span class="No-Break">cloud provider.</span></li>
				<li><strong class="bold">Cold start</strong>—Initialization of container or infrastructure where the function code will execute takes some time (typically in the range of tens of seconds). If a particular function has not been invoked for a while, the next invocation will suffer from a cold-start delay. The way to go around it is to call the functions periodically to keep <span class="No-Break">them </span><span class="No-Break"><em class="italic">pre-warmed</em></span><span class="No-Break">.</span></li>
				<li>Monitoring, logging, and debugging Serverless applications is often harder. While you don’t need to take care of servers and metrics such as CPU or disk utilization, the ways to debug functions at runtime are limited. You won’t be able to run code line by line as you would do locally in <span class="No-Break">an IDE.</span></li>
				<li>Risk of vendor lock-in. As already mentioned, FaaS offerings are not standardized, and thus migrating from one provider to another would require <span class="No-Break">significant work.</span></li>
			</ul>
			<p>The list is not exhaustive, but hopefully, it gives you an idea of why you should not necessarily rush your development team to move completely to Serverless. Both cloud provider and open source FaaS offerings have improved a lot in the recent years, so there is a chance that many of the limitations will be resolved in the <span class="No-Break">near future.</span></p>
			<p>Again, we are diving deeper<a id="_idIndexMarker572"/> here than required to pass KCNA. You’ll not be questioned about the limitations of Serverless, but you need to understand the concept of the billing model and be able to name a few projects that let you operate your own FaaS on top <span class="No-Break">of Kubernetes.</span></p>
			<p>In the last section of the chapter, we’re going to summarize some key points we’ve learned about <span class="No-Break">Cloud Native.</span></p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor100"/>Cloud Native best practices</h1>
			<p>As the world is changing fast, users get<a id="_idIndexMarker573"/> more and more demanding, and the IT landscape has to change in order to meet expectations. Today, not many people tolerate waiting for a web page to open if it takes 30 seconds and people complain if online banking is not working for a whole <span class="No-Break">hour long.</span></p>
			<p>Cloud Native has signified major improvements in the field by bringing a new approach to building and running applications. Cloud Native applications are designed to expect failures and automatically recover from most of them. A lot of focus is put on making <strong class="bold">both the application and the infrastructure resilient</strong>. This can be achieved in many ways with or without Kubernetes. If using Kubernetes, make sure to run multiple control plane and worker nodes spread across different failure<a id="_idIndexMarker574"/> domains such as your cloud provider <strong class="bold">availability zones</strong> (<strong class="bold">AZs</strong>). Always run at least two replicas (pods) of an application using a controller such as Deployment and make sure to spread them across the topology of your cluster. In the case of pod failure, the K8s reconciliation loop will kick in and self-heal <span class="No-Break">the application.</span></p>
			<p>Some companies have taken further steps to improve resiliency by introducing random failures across the infrastructure, allowing them to detect weak spots that need improvement or system redesign. For example, Netflix<a id="_idIndexMarker575"/> has become known for its <strong class="bold">Chaos Monkey</strong> tool that randomly terminates VMs and containers in order to incentivize engineers to build highly <span class="No-Break">resilient services.</span></p>
			<p>Next on the list is autoscaling—a crucial element for both performance and cost efficiency. Again, autoscaling must be implemented for <strong class="bold">both the application and the infrastructure</strong>. If running Kubernetes, make sure to set up at least HPA and Cluster Autoscaler. And don’t forget to configure resource requests and limits for all workloads, as it helps K8s to schedule pods in an <span class="No-Break">optimal way.</span></p>
			<p>When it comes to application architecture, apply the principle of <em class="italic">loose coupling</em>—develop microservices performing small tasks working together as a part of a larger application. Consider event-driven Serverless architecture if that fits your scenarios. In many cases, Serverless might be more cost-efficient and require almost <span class="No-Break">zero operations.</span></p>
			<p>When it comes to roles, we’ve already learned in <a href="B18970_02.xhtml#_idTextAnchor026"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> that organizations need to hire the right people for the job. This is not only about hiring DevOps and site reliability engineers to handle infrastructure, but it is also about team collaboration and corporate culture supporting constant change and experimentation. Learnings that come along provide valuable insights and lead to improvements in architecture and <span class="No-Break">system design.</span></p>
			<p>Furthermore, we briefly mentioned<a id="_idIndexMarker576"/> before that Cloud Native applications should feature a high degree of automation and ease of management. In <a href="B18970_11.xhtml#_idTextAnchor112"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, we’ll see in detail how automation helps shipping software faster and more reliably. And in the upcoming <a href="B18970_10.xhtml#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, we’ll discuss telemetry <span class="No-Break">and observability.</span></p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor101"/>Summary</h1>
			<p>In this chapter, we’ve learned about Cloud Native architectures, applications, and their features. Not every application that runs in the cloud automatically becomes Cloud Native. In fact, Cloud Native principles can be successfully applied also on-premises and not just in the cloud. We’ve briefly discussed the benefits of Cloud Native and in-depth about two core features—<strong class="bold">resiliency</strong> and <strong class="bold">autoscaling</strong>. While Cloud Native applications do not strictly require Kubernetes to run them, K8s makes things much easier with its <em class="italic">self-healing</em> capabilities and multiple autoscaling mechanisms: <strong class="bold">HPA</strong>, <strong class="bold">VPA</strong>, and <span class="No-Break"><strong class="bold">Cluster Autoscaler</strong></span><span class="No-Break">.</span></p>
			<p>Next, we covered Serverless or FaaS—a newer, event-driven computing model that comes with autoscaling and requires almost no operations at all. With Serverless, we are not responsible for any OS, security patching, or server life cycle. Serverless is also billed based on the actual usage calculated by the number of actual function invocations and the time they run. Serverless technologies can be leveraged to implement Cloud Native applications; however, be aware of <span class="No-Break">their limitations.</span></p>
			<p>Finally, we summarized the points about Cloud Native that we’ve learned in this chapter and previously, in <a href="B18970_02.xhtml#_idTextAnchor026"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. In the upcoming chapter, we will focus on monitoring Cloud Native applications and see how telemetry and observability can <span class="No-Break">be implemented.</span></p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor102"/>Questions</h1>
			<p>As we conclude, here is a list of questions for you to test your knowledge regarding this chapter’s material. You will find the answers in the <em class="italic">Assessments</em> section of <span class="No-Break">the </span><span class="No-Break"><em class="italic">Appendix</em></span><span class="No-Break">:</span></p>
			<ol>
				<li>Which of the following helps to get better resiliency <span class="No-Break">with Kubernetes?</span><ol><li><span class="No-Break">Resource requests</span></li><li><span class="No-Break">Multi-container pods</span></li><li><span class="No-Break">Reconciliation loop</span></li><li><span class="No-Break">Ingress controller</span></li></ol></li>
				<li>Which of the following Kubernetes autoscalers allows us to automatically increase and decrease the number of pods based on <span class="No-Break">the load?</span><ol><li><span class="No-Break">VPA</span></li><li><span class="No-Break">HPA</span></li><li><span class="No-Break">RPA</span></li><li><span class="No-Break">Cluster Autoscaler</span></li></ol></li>
				<li>Which of the following Kubernetes autoscalers adjusts container resource requests and limits based on <span class="No-Break">statistical data?</span><ol><li><span class="No-Break">VPA</span></li><li><span class="No-Break">HPA</span></li><li><span class="No-Break">RPA</span></li><li><span class="No-Break">Cluster Autoscaler</span></li></ol></li>
				<li>Why is it important to downscale the application <span class="No-Break">and infrastructure?</span><ol><li>To reduce the possible <span class="No-Break">attack surface</span></li><li>To avoid hitting cloud <span class="No-Break">provider limits</span></li><li>To reduce <span class="No-Break">network traffic</span></li><li>To reduce costs when computation resources <span class="No-Break">are idling</span></li></ol></li>
				<li>What best describes <span class="No-Break">horizontal scaling?</span><ol><li>Adding more CPU to the same <span class="No-Break">service instance</span></li><li>Adding more replicas/instances of the <span class="No-Break">same service</span></li><li>Adding more RAM to the same <span class="No-Break">service instance</span></li><li>To schedule pods to different nodes where other pods <span class="No-Break">already running</span></li></ol></li>
				<li>Which scaling approach is preferred for Cloud <span class="No-Break">Native applications?</span><ol><li><span class="No-Break">Cluster scaling</span></li><li><span class="No-Break">Cloud scaling</span></li><li><span class="No-Break">Vertical scaling</span></li><li><span class="No-Break">Horizontal scaling</span></li></ol></li>
				<li>Which of the following projects allow us to operate our own Serverless platform on Kubernetes (<span class="No-Break">pick multiple)?</span><ol><li><span class="No-Break">KubeVirt</span></li><li><span class="No-Break">KEDA</span></li><li><span class="No-Break">Knative</span></li><li><span class="No-Break">OpenFaaS</span></li></ol></li>
				<li>What characterizes Serverless computing (<span class="No-Break">pick multiple)?</span><ol><li>Servers are not <span class="No-Break">needed anymore</span></li><li>It supports all <span class="No-Break">programming languages</span></li><li>It <span class="No-Break">is event-based</span></li><li>The provider takes care of <span class="No-Break">server management</span></li></ol></li>
				<li>What is correct about <span class="No-Break">scaling microservices?</span><ol><li>Individual microservices can be scaled in <span class="No-Break">and out</span></li><li>Only all microservices can be scaled in and out <span class="No-Break">at once</span></li><li>Microservices do not need to be scaled—only the infrastructure needs <span class="No-Break">to be</span></li><li>Microservices are best <span class="No-Break">scaled up</span></li></ol></li>
				<li>Which application design principle works best with <span class="No-Break">Cloud Native?</span><ol><li><span class="No-Break">Self-healing</span></li><li><span class="No-Break">Tight coupling</span></li><li><span class="No-Break">Decoupling</span></li><li><span class="No-Break">Loose coupling</span></li></ol></li>
				<li>What describes a highly resilient application <span class="No-Break">and infrastructure?</span><ol><li>Ability to automatically shut down in case <span class="No-Break">of issues</span></li><li>Ability to automatically recover from <span class="No-Break">most failures</span></li><li>Ability to preserve the state in case <span class="No-Break">of failure</span></li><li>Ability to perform <span class="No-Break">rolling updates</span></li></ol></li>
				<li>What represents the smallest part of a <span class="No-Break">Serverless application?</span><ol><li><span class="No-Break">Gateway</span></li><li><span class="No-Break">Method</span></li><li><span class="No-Break">Container</span></li><li><span class="No-Break">Function</span></li></ol></li>
				<li>Which of the following is a correct statement <span class="No-Break">about Serverless?</span><ol><li>It is only billed for the <span class="No-Break">actual usage</span></li><li>It is free as no servers <span class="No-Break">are involved</span></li><li>It is billed at a constant <span class="No-Break">hourly price</span></li><li>It is billed the same as <span class="No-Break">IaaS services</span></li></ol></li>
				<li>Which of the following features do Cloud Native applications have (<span class="No-Break">pick multiple)?</span><ol><li><span class="No-Break">High scalability</span></li><li><span class="No-Break">High efficiency</span></li><li><span class="No-Break">High resiliency</span></li><li><span class="No-Break">High portability</span></li></ol></li>
				<li>What should normally be scaled in order to accommodate <span class="No-Break">the load?</span><ol><li>The application and the infrastructure it <span class="No-Break">runs on</span></li><li>The load balancer <span class="No-Break">and ingress</span></li><li>The number of <span class="No-Break">application pods</span></li><li>The number of Kubernetes <span class="No-Break">worker nodes</span></li></ol></li>
				<li>Which resiliency testing tool can be used to randomly introduce failures in <span class="No-Break">the infrastructure?</span><ol><li><span class="No-Break">Chaos Monster</span></li><li><span class="No-Break">Chaos Kube</span></li><li><span class="No-Break">Chaos Donkey</span></li><li><span class="No-Break">Chaos Monkey</span></li></ol></li>
			</ol>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor103"/>Further reading</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
			<ul>
				<li><span class="No-Break">Autoscaling: </span><a href="https://glossary.cncf.io/auto-scaling/"><span class="No-Break">https://glossary.cncf.io/auto-scaling/</span></a></li>
				<li>Kubernetes HPA <span class="No-Break">walkthrough: </span><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/"><span class="No-Break">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/</span></a></li>
				<li>Kubernetes Cluster <span class="No-Break">Autoscaler: </span><a href="https://github.com/kubernetes/autoscaler"><span class="No-Break">https://github.com/kubernetes/autoscaler</span></a></li>
				<li>Chaos <span class="No-Break">Monkey: </span><a href="https://github.com/Netflix/chaosmonkey"><span class="No-Break">https://github.com/Netflix/chaosmonkey</span></a></li>
				<li><span class="No-Break">OpenFaaS: </span><a href="https://www.openfaas.com/"><span class="No-Break">https://www.openfaas.com/</span></a></li>
				<li><span class="No-Break">Knative: </span><a href="https://knative.dev/docs/"><span class="No-Break">https://knative.dev/docs/</span></a></li>
			</ul>
		</div>
	</body></html>