- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today’s data-driven world, the ability to process and analyze vast amounts
    of data has become a critical competitive advantage for businesses across industries.
    Big data technologies have emerged as powerful tools to handle the ever-increasing
    volume, velocity, and variety of data, enabling organizations to extract valuable
    insights and drive informed decision-making. However, managing and scaling these
    technologies can be a daunting task, often requiring significant infrastructure
    and operational overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Enter Kubernetes, the open source container orchestration platform that has
    revolutionized the way we deploy and manage applications. By providing a standardized
    and automated approach to container management, Kubernetes has simplified the
    deployment and scaling of complex applications, including big data workloads.
    This book aims to bridge the gap between these two powerful technologies, guiding
    you through the process of implementing a robust and scalable big data architecture
    on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the chapters, you will embark on a comprehensive journey, starting
    with the fundamentals of containers and Kubernetes architecture. You will learn
    how to build and deploy Docker images, understand the core components of Kubernetes,
    and gain hands-on experience in setting up local and cloud-based Kubernetes clusters.
    This solid foundation will prepare you for the subsequent chapters, where you
    will dive into the world of the modern data stack.
  prefs: []
  type: TYPE_NORMAL
- en: The book will introduce you to the most widely adopted tools in the big data
    ecosystem, such as Apache Spark for data processing, Apache Airflow for pipeline
    orchestration, and Apache Kafka for real-time data ingestion. You will not only
    learn the theoretical concepts behind these technologies but also gain practical
    experience in implementing them on Kubernetes. Through a series of hands-on exercises
    and projects, you will develop a deep understanding of how to build and deploy
    data pipelines, process large datasets, and orchestrate complex workflows on a
    Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As the book progresses, you will explore advanced topics such as deploying a
    data consumption layer with tools such as Trino and Elasticsearch and integrating
    generative AI workloads using Amazon Bedrock. These topics will equip you with
    the knowledge and skills necessary to build and maintain a robust and scalable
    big data architecture on Kubernetes, ensuring efficient data processing, analysis,
    and analytics application deployment.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this book, you will have gained a comprehensive understanding
    of the synergy between big data and Kubernetes, enabling you to leverage the power
    of these technologies to drive innovation and business growth. Whether you are
    a data engineer, a DevOps professional, or a technology enthusiast, this book
    will provide you with the practical knowledge and hands-on experience needed to
    successfully implement and manage big data workloads on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are a data engineer, a cloud architect, a DevOps professional, a data
    or science manager, or a technology enthusiast, this book is for you. You should
    have a basic background in Python and SQL programming, and basic knowledge of
    Apache Spark, Apache Kafka, and Apache Airflow. A basic understanding of Docker
    and Git will also be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Chapter 1*](B21927_01.xhtml#_idTextAnchor015), *Getting Started with Containers*,
    embarks on a journey to understand containers and Docker, the foundational technologies
    for modern application deployment. You’ll learn how to install Docker and run
    your first container image, experiencing the power of containerization firsthand.
    Additionally, you’ll dive into the intricacies of Dockerfiles, mastering the art
    of crafting concise and functional container images. Through practical examples,
    including the construction of a simple API and a data processing job with Python,
    you’ll grasp the nuances of containerizing services and jobs. By the end of this
    chapter, you’ll have the opportunity to solidify your newfound knowledge by building
    your own job and API, laying the groundwork for a portfolio of practical container-based
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B21927_02.xhtml#_idTextAnchor031), *Kubernetes Architecture*,
    introduces you to the core components that make up the Kubernetes architecture.
    You will learn about the control plane components such as the API server, etcd,
    scheduler, and controller manager, as well as the worker node components such
    as kubelet, kube-proxy, and container runtime. The chapter will explain the roles
    and responsibilities of each component, and how they interact with each other
    to ensure the smooth operation of a Kubernetes cluster. Additionally, you will
    gain an understanding of the key concepts in Kubernetes, including pods, deployments,
    services, jobs, stateful sets, persistent volumes, ConfigMaps, and secrets. By
    the end of this chapter, you will have a solid foundation in the architecture
    and core concepts of Kubernetes, preparing you for hands-on experience in the
    subsequent chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B21927_03.xhtml#_idTextAnchor053), *Kubernetes – Hands On*, guides
    you through the process of deploying a local Kubernetes cluster using kind, and
    a cloud-based cluster on AWS using Amazon EKS. You will learn the minimal AWS
    account configuration required to successfully deploy an EKS cluster. After setting
    up the clusters, you will have the opportunity to choose between deploying your
    applications on the local or cloud environment. Regardless of your choice, you
    will retake the API and data processing jobs developed in [*Chapter 1*](B21927_01.xhtml#_idTextAnchor015)
    and deploy them to Kubernetes. This hands-on experience will solidify your understanding
    of Kubernetes concepts and prepare you for more advanced topics in the following
    chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B21927_04.xhtml#_idTextAnchor070), *The Modern Data Stack*, introduces
    you to the most well-known data architecture designs, with a focus on the “lambda”
    architecture. You will learn about the tools that make up the modern data stack,
    which is a set of technologies used to implement a data lake(house) architecture.
    Among these tools are Apache Spark for data processing, Apache Airflow for data
    pipeline orchestration, and Apache Kafka for real-time event streaming and data
    ingestion. This chapter will provide a conceptual introduction to these tools
    and how they work together to build the core technology assets of a data lake(house)
    architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B21927_05.xhtml#_idTextAnchor092), *Big Data Processing with
    Apache Spark*, introduces you to Apache Spark, one of the most popular tools for
    big data processing. You will understand the core components of a Spark program,
    how it scales and handles distributed processing, and best practices for working
    with Spark. You will implement simple data processing tasks using both the DataFrames
    API and the Spark SQL API, leveraging Python to interact with Spark. The chapter
    will guide you through installing Spark locally for testing purposes, enabling
    you to gain hands-on experience with this powerful tool before deploying it on
    a larger scale.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B21927_06.xhtml#_idTextAnchor112), *Apache Airflow for Building
    Pipelines*, introduces you to Apache Airflow, a widely adopted open source tool
    for data pipeline orchestration. You will learn how to install Airflow using Docker
    and Astro CLI, making the setup process straightforward. The chapter will familiarize
    you with Airflow’s core features and the most commonly used operators for data
    engineering tasks. Additionally, you will gain insights into best practices for
    building resilient and efficient data pipelines that leverage Airflow’s capabilities
    to the fullest. By the end of this chapter, you will have a solid understanding
    of how to orchestrate complex data workflows using Airflow, a crucial skill for
    any data engineer or data architect working with big data on Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B21927_07.xhtml#_idTextAnchor122), *Apache Kafka for Real-Time
    Events and Data Ingestion*, introduces you to Apache Kafka, a distributed event
    streaming platform that is widely used for building real-time data pipelines and
    streaming applications. You will understand Kafka’s architecture and how it scales
    while being resilient, enabling it to handle high volumes of real-time data with
    low latency. You will learn about Kafka’s distributed topics design, which underpins
    its robust performance for real-time events. The chapter will guide you through
    running Kafka locally with Docker and implementing basic reading and writing operations
    on topics. Additionally, you will explore different strategies for data replication
    and topic distribution, ensuring you can design and implement efficient and reliable
    Kafka clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B21927_08.xhtml#_idTextAnchor134), *Deploying the Big Data Stack
    on Kubernetes*, guides you through the process of deploying the big data tools
    you learned about in the previous chapters on a Kubernetes cluster. You will start
    by building bash scripts to deploy the Spark operator and run `SparkApplications`
    on Kubernetes. Next, you will deploy Apache Airflow to Kubernetes, enabling you
    to orchestrate data pipelines within the cluster. Additionally, you will deploy
    Apache Kafka on Kubernetes using both the ephemeral cluster and JBOD techniques.
    The Kafka Connect cluster will also be deployed, along with connectors to migrate
    data from SQL databases to persistent object storage. By the end of this chapter,
    you will have a fully functional big data stack running on Kubernetes, ready for
    further exploration and development.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B21927_09.xhtml#_idTextAnchor141), *Data Consumption Layer*,
    guides you through the process of securely making data available for business
    analysts in a big data architecture deployed on Kubernetes. You will start by
    gaining an overview of working on a modern approach using a “data lake engine”
    instead of a data warehouse. In this chapter, you will become familiar with Trino
    for data consumption directly from a data lake through Kubernetes. You will understand
    how a data lake engine works, deploy it into Kubernetes, and monitor query execution
    and history. Additionally, for real-time data, you will get familiar with Elasticsearch
    and Kibana for data consumption. You will deploy these tools, and learn how to
    index data in them and how to build a simple data visualization with Kibana.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B21927_10.xhtml#_idTextAnchor154), *Building a Big Data Pipeline
    in Kubernetes*, guides you through the process of deploying and orchestrating
    two complete data pipelines, one for batch processing and another for real-time
    processing, on a Kubernetes cluster. You will connect all the tools you’ve learned
    about throughout the book, such as Apache Spark, Apache Airflow, Apache Kafka,
    and Trino, to build a single, complex solution. You will deploy these tools on
    Kubernetes, write code for data processing and orchestration, and make the data
    available for querying through a SQL engine. By the end of this chapter, you will
    have hands-on experience in building and managing a comprehensive big data pipeline
    on Kubernetes, integrating various components and technologies into a cohesive
    and scalable architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B21927_11.xhtml#_idTextAnchor167), *Generative AI on Kubernetes*,
    guides you through the process of deploying a generative AI application on Kubernetes
    using Amazon Bedrock as a service suite for foundational models. You will learn
    how to connect your application to a knowledge base serving as a **Retrieval-Augmented
    Generation** (**RAG**) layer, which enhances the AI model’s capabilities by providing
    access to external information sources. Additionally, you will discover how to
    automate task execution by the AI models with agents, enabling seamless integration
    of generative AI into your workflows. By the end of this chapter, you will have
    a solid understanding of how to leverage the power of generative AI on Kubernetes,
    unlocking new possibilities for personalized customer experiences, intelligent
    assistants, and automated business analytics.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B21927_12.xhtml#_idTextAnchor183), *Where to Go from Here*,
    guides you through the next steps in your journey toward mastering big data and
    Kubernetes. You will explore crucial concepts and technologies that are essential
    for building robust and scalable solutions on Kubernetes. This includes monitoring
    strategies for both Kubernetes and your applications, implementing a service mesh
    for efficient communication, securing your cluster and applications, enabling
    automated scalability, embracing GitOps and CI/CD practices for streamlined deployment
    and management, and Kubernetes cost control. For each topic, you’ll receive an
    overview and recommendations on the technologies to explore further, empowering
    you to deepen your knowledge and skills in these areas.'
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some basics in Python programming knowledge and experience with Spark, Docker,
    Airflow, Kafka, and Git will help you get the most out of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Software/hardware covered in** **the book** | **Operating** **system requirements**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Python>=3.9 | Windows, macOS, or Linux |'
  prefs: []
  type: TYPE_TB
- en: '| Docker, the latest version available | Linux |'
  prefs: []
  type: TYPE_TB
- en: '| Docker Desktop, the latest version available | Windows or macOS |'
  prefs: []
  type: TYPE_TB
- en: '| Kubectl | Windows, macOS, or Linux |'
  prefs: []
  type: TYPE_TB
- en: '| Awscli | Windows, macOS, or Linux |'
  prefs: []
  type: TYPE_TB
- en: '| Eksctl | Windows, macOS, or Linux |'
  prefs: []
  type: TYPE_TB
- en: '| DBeaver Community Edition | Windows, macOS, or Linux |'
  prefs: []
  type: TYPE_TB
- en: All guidance needed for software installation will be provided in each chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are using the digital version of this book, we advise you to type
    the code yourself or access the code from the book’s GitHub repository (a link
    is available in the next section). Doing so will help you avoid any potential
    errors related to the copying and pasting** **of code.**'
  prefs: []
  type: TYPE_NORMAL
- en: Download the example code files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the example code files for this book from GitHub at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes).
    If there’s an update to the code, it will be updated in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of text conventions used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '`Code in text`: Indicates code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. Here is an example: “This command will pull the `hello-world` image from
    the Docker Hub public repository and run the application in it. “'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Any command-line input or output is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This is how the filename above the code snippet will look:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cjava.py**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For instance, words in menus or dialog boxes appear in **bold**. Here is an example:
    “You should ensure that the **Use WSL 2 instead of Hyper-V** option is selected
    on the **Configuration** page.”'
  prefs: []
  type: TYPE_NORMAL
- en: Tips or important notes
  prefs: []
  type: TYPE_NORMAL
- en: Appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  prefs: []
  type: TYPE_NORMAL
- en: '**General feedback**: If you have questions about any aspect of this book,
    email us at [customercare@packtpub.com](mailto:customercare@packtpub.com) and
    mention the book title in the subject of your message.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)
    and fill in the form.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at [copyright@packt.com](mailto:copyright@packt.com)
    with a link to the material.'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com).'
  prefs: []
  type: TYPE_NORMAL
- en: Share Your Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you’ve read *Big Data on Kubernetes*, we’d love to hear your thoughts!
    Please [click here to go straight to the Amazon review page](https://packt.link/r/1-835-46214-6)
    for this book and share your feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  prefs: []
  type: TYPE_NORMAL
- en: Download a free PDF copy of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thanks for purchasing this book!
  prefs: []
  type: TYPE_NORMAL
- en: Do you like to read on the go but are unable to carry your print books everywhere?
  prefs: []
  type: TYPE_NORMAL
- en: Is your eBook purchase not compatible with the device of your choice?
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry, now with every Packt book you get a DRM-free PDF version of that
    book at no cost.
  prefs: []
  type: TYPE_NORMAL
- en: Read anywhere, any place, on any device. Search, copy, and paste code from your
    favorite technical books directly into your application.
  prefs: []
  type: TYPE_NORMAL
- en: The perks don’t stop there, you can get exclusive access to discounts, newsletters,
    and great free content in your inbox daily
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these simple steps to get the benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Scan the QR code or visit the link below
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Download a free PDF copy of this book'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B21927_QR_Free_PDF.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/free-ebook/978-1-83546-214-0](https://packt.link/free-ebook/978-1-83546-214-0)'
  prefs: []
  type: TYPE_NORMAL
- en: Submit your proof of purchase
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That’s it! We’ll send your free PDF and other benefits to your email directly
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Part 1:Docker and Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you will learn about the fundamentals of containerization and
    Kubernetes. You will start by understanding the basics of containers and how to
    build and run Docker images. This will provide you with a solid foundation for
    working with containerized applications. Next, you will dive into the Kubernetes
    architecture, exploring its components, features, and core concepts such as pods,
    deployments, and services. With this knowledge, you will be well equipped to navigate
    the Kubernetes ecosystem. Finally, you will get hands-on experience by deploying
    local and cloud-based Kubernetes clusters and then deploying applications you
    built earlier onto these clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 1*](B21927_01.xhtml#_idTextAnchor015), Getting Started with Containers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B21927_02.xhtml#_idTextAnchor031), Kubernetes Architecture'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B21927_03.xhtml#_idTextAnchor053), Kubernetes – Hands On'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
