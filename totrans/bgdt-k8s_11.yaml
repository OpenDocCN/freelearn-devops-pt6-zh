- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative** **artificial intelligence** (**GenAI**) has emerged as a transformative
    technology, revolutionizing how we interact with and leverage AI. In this chapter,
    we will explore the exciting world of generative AI and learn how to harness its
    power on Kubernetes. We will dive into the fundamentals of generative AI and understand
    its main differences from traditional AI.'
  prefs: []
  type: TYPE_NORMAL
- en: Our focus will be on leveraging **Amazon Bedrock**, a comprehensive suite of
    services designed to simplify the development and deployment of generative AI
    applications. Through hands-on examples, you will gain practical experience in
    building a generative AI application on Kubernetes using **Streamlit**, a powerful
    Python library for creating interactive data applications. We will cover the entire
    process, from the development to deploying the application on a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we will explore the concept of **retrieval-augmented generation**
    (**RAG**), which combines the power of generative AI with external knowledge bases.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will introduce **Agents for Amazon Bedrock**, a powerful feature
    that allows you to automate tasks and create intelligent assistants. You will
    learn how to build an agent, define its capabilities through an OpenAPI schema,
    and create the underlying Lambda function that serves as the backend for your
    agent.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a solid understanding of generative
    AI, its applications, and the tools and techniques required to build and deploy
    generative AI applications on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What generative AI is and what it is not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Amazon Bedrock to work with foundational models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a generative AI application on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building RAG with **Knowledge Bases for** **Amazon Bedrock**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building action models with agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you will need an AWS account and a running Kubernetes cluster.
    We will also be using **LangChain** and Streamlit libraries. Although it is not
    necessary to have them installed for application deployment in Kubernetes, the
    installation is advised if you want to test the code locally and modify it to
    your own experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Also, it will be necessary to install the **Beautiful Soup** library to get
    data for the RAG exercise (fourth section).
  prefs: []
  type: TYPE_NORMAL
- en: All the code for this chapter is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes)
    under the `Chapter11` folder.
  prefs: []
  type: TYPE_NORMAL
- en: What generative AI is and what it is not
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At its core, generative AI refers to AI systems capable of generating new, original
    content, such as text, images, audio, or code, based on the training data they
    have been exposed to. Generative AI models are trained on large datasets of existing
    content, and they learn the patterns and relationships within that data. When
    prompted, these models can then generate new, original content that resembles
    the training data but is not an exact copy of any specific example.
  prefs: []
  type: TYPE_NORMAL
- en: This contrasts with traditional machine learning models, which are focused on
    making predictions or classifications based on existing data.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional machine learning models, such as those used for image recognition,
    natural language processing, or predictive analytics, are designed to take in
    input data and make predictions or classifications based on that data. Machine
    learning models excel at tasks such as classification (e.g., identifying objects
    in images or topics in texts), regression (e.g., predicting house prices based
    on features such as square footage and location), and clustering (e.g., grouping
    customers based on similar behavior patterns).
  prefs: []
  type: TYPE_NORMAL
- en: For example, an image recognition model might be trained on a large dataset
    of labeled images to learn to recognize and classify objects in new, unseen images.
    Similarly, a natural language processing model might be trained on a corpus of
    text data to perform tasks such as sentiment analysis, named entity recognition,
    or language translation.
  prefs: []
  type: TYPE_NORMAL
- en: In a credit risk assessment scenario, a machine learning model would be trained
    on a dataset containing information about past loan applicants, such as their
    income, credit history, and other relevant features, along with labels indicating
    whether they defaulted on their loans or not. The model would learn the patterns
    and relationships between these features and the loan default outcomes. When presented
    with a new loan application, the trained model can then predict the likelihood
    of the applicant defaulting on the loan.
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, the machine learning model is not generating new content; instead,
    it is using the patterns and relationships it has learned from the training data
    to make informed predictions or decisions about new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, for instance, a generative AI model trained on a vast corpus of
    text can generate human-like writing on any given topic or in any desired style.
    Similarly, models trained on images can create entirely new, realistic-looking
    images based on textual descriptions or other input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the end result of generative AI is the creation of new content, the underlying
    mechanism is still based on the same principles of machine learning: making predictions.
    However, instead of predicting a single output (such as a classification or a
    numerical value), generative AI models are trained to predict the next element
    in a sequence, whether that sequence is a sequence of words, pixels, or any other
    type of data.'
  prefs: []
  type: TYPE_NORMAL
- en: The power of large neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the concept of predicting the next element in a sequence is relatively
    simple, the ability of generative AI models to generate coherent, high-quality
    content lies in the sheer scale and complexity of the neural networks used to
    power these models.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI models typically employ large, deep neural networks with billions
    or even trillions of parameters. These neural networks are trained on vast amounts
    of data, often spanning millions or billions of examples, allowing them to capture
    incredibly nuanced patterns and relationships within the data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the Anthropic models, such as Claude, are trained on an enormous
    corpus of text data, spanning a wide range of topics and domains. This allows
    the models to develop a deep understanding of language, context, and domain-specific
    knowledge, enabling them to generate text that is not only grammatically correct
    but also semantically coherent and relevant to the given context.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While generative AI has demonstrated remarkable capabilities, it is not without
    its challenges and limitations. One of the primary concerns is the potential for
    these models to generate biased, harmful, or misleading content, especially when
    trained on datasets that reflect societal biases or contain inaccurate information.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, generative AI models can sometimes produce outputs that are nonsensical,
    inconsistent, or factually incorrect, even though they may appear coherent and
    plausible on the surface. This is known as the “hallucination” problem, where
    the model generates content that is not grounded in factual knowledge or the provided
    context. Here are two well-known real-life cases. Air Canada’s AI-powered chatbot
    provided misleading information to a passenger regarding the airline’s bereavement
    fare policy. The chatbot incorrectly stated that passengers could apply for reduced
    bereavement fares retroactively, even after travel had already occurred, which
    contradicted Air Canada’s actual policy. The passenger relied on this hallucinated
    response from the chatbot and subsequently filed a successful small claims case
    against Air Canada when the airline refused to honor the chatbot’s advice ([https://www.forbes.com/sites/marisagarcia/2024/02/19/what-air-canada-lost-in-remarkable-lying-ai-chatbot-case/](https://www.forbes.com/sites/marisagarcia/2024/02/19/what-air-canada-lost-in-remarkable-lying-ai-chatbot-case/)).
    Also, a federal judge in Brazil used the ChatGPT AI system to research legal precedents
    for a ruling he was writing. However, the AI provided fabricated information,
    citing non-existent rulings from the Superior Court of Justice as the basis for
    the judge’s decision ([https://g1.globo.com/politica/blog/daniela-lima/post/2023/11/13/juiz-usa-inteligencia-artificial-para-fazer-decisao-e-cita-jurisprudencia-falsa-cnj-investiga-caso.ghtml](https://g1.globo.com/politica/blog/daniela-lima/post/2023/11/13/juiz-usa-inteligencia-artificial-para-fazer-decisao-e-cita-jurisprudencia-falsa-cnj-investiga-caso.ghtml)).
  prefs: []
  type: TYPE_NORMAL
- en: Despite these challenges, generative AI is a rapidly evolving field, and researchers
    and developers are actively working on addressing these issues. Techniques such
    as fine-tuning, prompt engineering, and the use of external knowledge sources
    (e.g., knowledge bases or RAG) are being explored to improve the reliability,
    safety, and factual accuracy of generative AI models.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will dive deeper into the practical aspects of
    building and deploying generative AI applications using Amazon Bedrock and its
    foundational models, knowledge base, and agent-based architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon Bedrock to work with foundational models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Bedrock provides a suite of foundational models that can be used as building
    blocks for your generative AI applications. It’s important to understand the capabilities
    and intended use cases of each model to choose the right one for your application.
  prefs: []
  type: TYPE_NORMAL
- en: The available models in Amazon Bedrock include language models, computer vision
    models, and multimodal models. Language models excel at understanding and generating
    human-like text. They can be employed for tasks such as text summarization, question
    answering, and content generation. Computer vision models, on the other hand,
    are adept at analyzing and understanding visual data, making them ideal for applications
    such as image recognition, object detection, and scene understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal models, as the name suggests, can handle multiple modalities simultaneously.
    This makes it suitable for tasks such as image captioning, visual question answering,
    and data chart analysis.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that each model has its own strengths and limitations,
    and the choice of model should be guided by the specific requirements of your
    application. For example, if your application primarily deals with text-based
    tasks, a language model such as Llama might be the most appropriate choice. However,
    if you need to process both text and images, a multimodal model such as Claude
    would be a better fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'To effectively integrate Amazon Bedrock’s foundational models into our generative
    AI applications, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: To use Amazon Bedrock’s available foundational models, first, we need to activate
    them. Go to the AWS console and search for the Amazon Bedrock page. Then, click
    on **Modify model access** (*Figure 11**.1*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Modifying model access on Amazon Bedrock](img/B21927_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Modifying model access on Amazon Bedrock
  prefs: []
  type: TYPE_NORMAL
- en: On the next page, select the **Claude 3 Sonnet** and **Claude 3 Haiku** Anthropic
    models. Those are the foundational models we will use for our generative AI applications.
    You can select all the available models if you wish to play and experiment with
    different models (*Figure 11**.2*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Requesting access for Anthropic’s Claude 3 models](img/B21927_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Requesting access for Anthropic’s Claude 3 models
  prefs: []
  type: TYPE_NORMAL
- en: Click **Next** and, on the next page, review the changes and click **Submit**.
    Those models can take a few minutes to get access granted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once access has been granted, we have all we need to develop a generative AI
    application. Let’s get to it.
  prefs: []
  type: TYPE_NORMAL
- en: Building a generative AI application on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will build a generative AI application with Streamlit. A
    diagram representing the architecture for this application is shown in *Figure
    11**.3*. In this application, the user will be able to choose which foundational
    model they are going to talk to.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Foundational models’ application architecture](img/B21927_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Foundational models’ application architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the Python code for the application. The complete code is
    available under the [*Chapter 11*](B21927_11.xhtml#_idTextAnchor167)`/streamlit-claude/app`
    folders on GitHub. We will walk through the code, block by block:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a folder named `app` and inside it, create a `main.py` code file. First,
    we import the necessary files and create a client to access Amazon Bedrock runtime
    APIs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define a dictionary of parameters that are important for working with
    Claude:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will configure a function to allow the choosing of the preferred foundational
    model. With the choice, we will return a model object that can access Bedrock
    through Langchain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will add a small function to reset the conversation history:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will begin the development of the `main` function and add some widgets
    to the application interface. The following code creates a sidebar. In it, we
    add a selection box with Claude 3 Haiku and Claude 3 Sonnet as options, we write
    a confirmation message to tell the user which model they are talking to, and we
    add a `choose_model` function to return the class that connects to Bedrock and
    write the title of the application, *Chat with* *Claude 3*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will initialize the chat history as an empty list if it doesn’t already
    exist in `st.session_state`. `st.session_state` is a Streamlit object that persists
    data across app reruns. Then, we iterate over the `messages` list in `st.session_state`
    and display each message in a chat message container. The `st.chat_message` function
    creates a chat message container with the specified role (e.g., `user` or `assistant`).
    The `st.markdown` function displays the message content inside the container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we handle user input and display the conversation. The `st.chat_input`
    function creates an input field where the user can enter their prompt. If the
    user enters a prompt, the following steps are executed: (1) the user’s prompt
    is added to the `messages` list in `st.session_state` with the `user` role; (2)
    the user’s prompt is displayed in a chat message container with the `user` role;
    (3) the `model.stream(prompt)` function is called, which sends the user’s prompt
    to the Bedrock model and streams the response back. The `st.write_stream` function
    displays the streamed response in real-time; (4) the assistant’s response is added
    to the `messages` list in `st.session_state` with the `assistant` role:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we call the main function to start the Streamlit application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you want to run this application locally, here is a `requirements.txt` file:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you have the libraries already installed, authenticate your AWS CLI with
    the `aws configure` command and start the application locally with the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is an awesome way of testing the application before building a container
    image for deployment. You can test and modify the application as you wish.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When it is ready, now, let’s build a container image for deployment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following is a simple **Dockerfile** to build the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dockerfile**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This Dockerfile starts with the Python 3.9 slim base image and sets the working
    directory to `/app`. It then installs various system packages required for the
    application, such as `build-essential`, `curl`, `software-properties-common`,
    and `git`. The application code is copied into the `/app` directory, and the container
    exposes port `8501`. A health check is set up to check whether the Streamlit application
    is running correctly on [http://localhost:8501/_stcore/health](http://localhost:8501/_stcore/health).
    The required Python packages are installed using `pip3` based on the `requirements.txt`
    file. Finally, the `ENTRYPOINT` command starts the Streamlit application by running
    `streamlit run main.py` and specifying the server port and address.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To build the image locally, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remember to change `<YOUR_USERNAME>` to your actual Docker Hub username. Then,
    push the image with the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Remember that this image is going to be publicly available on Docker Hub. Don’t
    put any authentication credentials or sensitive data in the code or as environment
    variables!
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s deploy our application on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the Streamlit app
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have seen before, to deploy our app on Kubernetes, we need a `Deployment`
    and a `Service` `.yaml` definition. We can provide both in a single file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a `deploy_chat_with_claude.yaml` file with this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**deploy_chat_with_claude.yaml**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first part of the code defines a `Deployment` resource named `chat-with-claude`.
    It takes a previously built image (which you can change to your own new image)
    and opens port `8501` in the container to be accessed from outside the pod. The
    `spec.template.spec.containers.env` block mounts AWS credentials as environment
    variables in the container from a secret called `aws-credentials`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The second part of the code defines a `LoadBalancer` service for the pods defined
    in `Deployment`, which listens on port `8501` and directs traffic to port `8501`
    in the container. Don’t forget `---`, which is necessary to separate several resources
    in a single file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are going to create the namespace and the secret and deploy the application
    with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That’s it. Wait a few minutes for `LoadBalancer` to be up and running and check
    its URL with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, paste the URL with `:8501` at the end to define the correct port *et voilà!*
    (*Figure 11**.4*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.4 – The Chat with Claude 3 app UI](img/B21927_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – The Chat with Claude 3 app UI
  prefs: []
  type: TYPE_NORMAL
- en: Now, play a little bit with the assistant. Try Haiku and Sonnet and note their
    differences in speed and quality of answer. After a few shots, you will notice
    that asking specific questions to foundational models leads to a hallucination.
    Ask the model, for instance, who are you. You are going to have a nice surprise
    (and some laughs). This model needs context.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will provide some context using RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Building RAG with Knowledge Bases for Amazon Bedrock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG is a technique used in generative AI models to provide additional context
    and knowledge to foundational models during the generation process. It works by
    first retrieving relevant information from a knowledge base or corpus of documents,
    and then using this retrieved information to augment the input to the generative
    model.
  prefs: []
  type: TYPE_NORMAL
- en: RAG is a good choice for giving context to generative AI models because it allows
    the model to access and utilize external knowledge sources, which can significantly
    improve the quality, accuracy, and relevance of the generated output. Without
    RAG, the model would be limited to the knowledge and patterns it learned during
    training, which may not always be sufficient or up to date, especially for domain-specific
    or rapidly evolving topics.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of RAG is that it enables the model to leverage large
    knowledge bases or document collections, which would be impractical or impossible
    to include in the model’s training data. This allows the model to generate more
    informed and knowledgeable outputs, as it can draw upon a vast amount of relevant
    information. Additionally, RAG can help mitigate issues such as hallucination
    and bias, as the model has access to authoritative and factual sources.
  prefs: []
  type: TYPE_NORMAL
- en: However, RAG also has some limitations. The quality of the generated output
    heavily depends on the relevance and accuracy of the retrieved information, which
    can be influenced by the quality of the knowledge base, the effectiveness of the
    retrieval mechanism, and the ability of the model to properly integrate the retrieved
    information. Additionally, RAG can introduce computational overhead and latency,
    as it requires an additional retrieval step before the generation process.
  prefs: []
  type: TYPE_NORMAL
- en: To build an AI assistant with RAG, we will use the Knowledge Bases for Amazon
    Bedrock service, a feature in Bedrock that allows you to create and manage a knowledge
    base seamlessly. Let’s get to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our exercise, we will build an AI assistant capable of giving information
    about the AWS Competency Program. A visual representation of this assistant’s
    architecture is shown in *Figure 11**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Knowledge Bases for Amazon Bedrock application architecture](img/B21927_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Knowledge Bases for Amazon Bedrock application architecture
  prefs: []
  type: TYPE_NORMAL
- en: The AWS Competency Program is a validation program offered by AWS that recognizes
    partners who have demonstrated technical proficiency and proven customer success
    in specialized solution areas. AWS Competencies are awarded to **AWS Partner Network**
    (**APN**) members who have undergone technical validation related to specific
    AWS services or workloads, ensuring they have the expertise needed to deliver
    consistent, high-quality solutions on AWS. These competencies span various areas
    such as DevOps, migration, data and analytics, machine learning, and security.
    Each competency has its own rules document and can be quite challenging to understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will gather some context information about the program. On GitHub,
    under the [*Chapter 11*](B21927_11.xhtml#_idTextAnchor167)`/claude-kb/knowledge-base/`
    folder, you will find a Python code that will gather information on conversational
    AI, data and analytics, DevOps, education, energy, financial services, machine
    learning, and security programs. After saving this code locally, install the Beautiful
    Soup library with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After a few seconds, data should be saved locally on your machine.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, create an S3 bucket and upload these files. This will be the base for
    our RAG layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, go to the **Bedrock** page in the AWS console. In the side menu, click
    on **Knowledge Bases** and then, click on **Create knowledge base** (*Figure 11**.6*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.6 – The Knowledge bases landing page](img/B21927_11_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – The Knowledge bases landing page
  prefs: []
  type: TYPE_NORMAL
- en: On the next page, choose a name for your knowledge base and select **Create
    and use a new service role** under the **IAM permissions** section. Then, click
    **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, you will configure the data source. Choose a data source name as you wish.
    For **Data source location**, make sure the **This AWS account** box option is
    checked. Then, in the **S3 URI** section, click on **Browse S3** to search for
    your S3 bucket that contains the AWS Competency datasets (the bucket we created
    in *Step 2*). An example of that configuration is shown in *Figure 11**.7*. After
    selecting the S3 bucket, click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Choosing a data source for the knowledge base](img/B21927_11_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Choosing a data source for the knowledge base
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to choose the embeddings model. This embeddings model is
    responsible for transforming text or image files into vector representations called
    **embeddings**. These embeddings capture the semantic and contextual information
    of the input data, allowing for efficient similarity comparisons and retrieval
    operations. One of Bedrock’s embeddings models, Amazon Titan, should be available
    by default. If it is not, do the same process of asking for access in the console.
  prefs: []
  type: TYPE_NORMAL
- en: On the next page, in the **Embeddings model** section, choose **Titan Embeddings
    G1 - Text**. In the **Vector database** section, make sure the **Quick create
    a new vector store** option is checked. This quick creation option creates a vector
    database based on OpenSearch Serverless. Leave the other options unmarked and
    click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: OpenSearch is an open-source distributed search and analytics engine based on
    Apache Lucene and derived from Elasticsearch. It is a great option for a RAG vector
    database because it provides efficient full-text search and nearest-neighbor search
    capabilities for vector embeddings. OpenSearch supports dense vector indexing
    and retrieval, making it suitable for storing and querying large collections of
    vector embeddings, which are essential for the retrieval component of RAG models.
  prefs: []
  type: TYPE_NORMAL
- en: Next, review the information to see whether it was correctly provided. If everything
    looks good, click on **Create knowledge base**. Be patient. This creation will
    take several minutes to complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the knowledge base is up and running, go back to the **Knowledge base**
    page in Bedrock and click on the knowledge base you just created. On the next
    page, scroll until you find the **Data source** section (as shown in *Figure 11**.8*).
    Select the data source and click **Sync** to start the embedding of the text content.
    This will also take a few minutes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Syncing the knowledge base with its data source](img/B21927_11_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Syncing the knowledge base with its data source
  prefs: []
  type: TYPE_NORMAL
- en: After the “sync” is ready, we have everything we need to run our generative
    AI assistant with RAG. Now, it is time to adjust the code to let Claude work with
    the knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting the code for RAG retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will start from the code we developed earlier to work with the pure Claude
    model. As we just need some small modifications, we won’t go through the entire
    code again. We will take a closer look at the necessary modifications. The complete
    code for the RAG application is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/claude-kb/app](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/claude-kb/app)
    folder. If you don’t want to customize your code, you can use the ready-to-go
    docker image I provided for this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need extra imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we import the `os` library to get environment variables. The `Config`
    class will help build a configuration object to access the `bedrock-agent` API.
    All the other imports relate to accessing the knowledge base and merging the retrieved
    documents with AI responses.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will get the ID for the Knowledge Bases for Amazon Bedrock service
    from an environment variable. This can be a very helpful approach. If we need
    to change the knowledge base in the future, there is no need to rebuild the image.
    We just change the environment variable. Then, we set some configurations and
    create a client for the `bedrock-agent-runtime` API (needed for the knowledge
    base):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will configure a prompt template that will help us chain the retrieved
    documents from the knowledge base and the user questions. At the end, we instantiate
    an object that will hold the template and receive the documents and the user questions
    as inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After setting the `choose_model()` function, we need to instantiate a `retriever`
    class that will pull documents from the knowledge base:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, inside the `main` function, we will add `RetrievalQA`. This class is used
    for building question-answering systems that can retrieve relevant information
    from the knowledge base:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will modify the response to give the entire answer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That’s it. The code is ready to be built in a new image. You can rebuild it
    by creating a new Dockerfile with the same code we used before. When running the
    `docker build` command, remember to choose a different image name (or, at least,
    a different version).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will start the deployment. The `.yaml` file is also very similar to
    the one we did in the last section (but remember to change all the names for the
    deployment, services, container, and label to `rag-with-claude`). A full version
    of this code is available in the GitHub repository). We only need to declare the
    environment variable for the knowledge base ID. As this is not a sensitive credential,
    we don’t need to use a Kubernetes secret for that. We will use `ConfigMap`. The
    `spec.template.spec.container.env` section of your `.yaml` file should look like
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we added a new environment variable called `KB_ID` that will be imported
    from `ConfigMap`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To deploy the new application, we run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We run the preceding to deploy the application. Wait a few minutes for `LoadBalancer`
    to be up and use the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use the preceding command to get the URL for `LoadBalancer`. Copy and paste
    the service named `rag-with-claude` in a browser and add `:8501` to connect to
    the exposed port. *Et voilà!* You should see your new application running as shown
    in *Figure 11**.9*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.9 – RAG application UI](img/B21927_11_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – RAG application UI
  prefs: []
  type: TYPE_NORMAL
- en: Try to play a little bit with this application. You will see that if you ask
    questions not related to its scope (AWS Competency program), the assistant will
    say it cannot answer.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will move to the final part of this chapter and learn how to make generative
    AI models execute actions with agents.
  prefs: []
  type: TYPE_NORMAL
- en: Building action models with agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Agents are the newest feature in the generative AI world. They are powerful
    tools that enable the automation of tasks by allowing generative AI models to
    take actions on our behalf. They act as intermediaries between the generative
    AI models and external systems or services, facilitating the execution of tasks
    in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, an agent “understands” what the user wants and calls a backend
    function that performs the action. The scope within which the agent can act is
    defined by an OpenAPI schema that it will use both to “understand” what it does
    and how to properly call the backend function.
  prefs: []
  type: TYPE_NORMAL
- en: So, in summary, to build an agent we need an OpenAPI schema, a backend function,
    and a knowledge base. The knowledge base is optional, but it can greatly improve
    a user’s experience with the AI assistant.
  prefs: []
  type: TYPE_NORMAL
- en: For this section’s exercise, we will build an agent that “knows” the available
    information about the AWS Competency program. A visual representation of the agent’s
    application architecture is shown in *Figure 11**.10*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.10 – Agent application architecture](img/B21927_11_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – Agent application architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'This agent is going to build a simple worksheet with a use case’s information,
    save the worksheet to Amazon S3, and register the information on a DynamoDB table
    for consultation. Let’s get to it:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we need an OpenAPI schema defining the methods available for our agent.
    In this case, we will define two methods. The first one, `generateCaseSheet`,
    registers the use case information and builds the worksheet. The second, `checkCase`,
    takes the use case ID and returns information about it. As this is a long JSON
    file, we will not display it here. The complete code is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent)
    folder. Copy this code and save it in an S3 bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will define a Lambda function that will serve as a backend for the
    agent. The complete Python code for the function is available in the book’s GitHub
    repository under the [*Chapter 11*](B21927_11.xhtml#_idTextAnchor167)`/agent/function`
    folder. In your machine, create a folder named `function` and save this code as
    `lambda_function.py` in the `function` folder. This code defines a Lambda function
    that serves as the backend for a Bedrock agent. The function handles two different
    API paths: `/generateCaseSheet` and `/checkCase`. Let’s go through the code block
    by block. After importing the necessary folders, we define two helper functions
    to extract parameter values from the event object (`get_named_parameter` and `get_named_property`).
    The `generateCaseSheet` function is responsible for creating a new case sheet
    based on the provided information. It extracts the required parameters from the
    event object, generates a unique ID, creates a new Excel workbook using the `CaseTemplate`
    class, fills in the template with the provided parameters, saves the workbook
    to a temporary file, uploads it to an S3 bucket, and stores the case sheet information
    in a DynamoDB table. Finally, it returns a response object containing the case
    details. The `checkCase` function retrieves the case sheet information from the
    DynamoDB table based on the provided `caseSheetId` parameter and returns a response
    object containing the case details. The `lambda_handler` function is the entry
    point for the Lambda function. It determines the appropriate action based on the
    `apiPath` value in the event object. The function constructs the appropriate response
    object based on the action and returns it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, inside the `function` folder, create a new file called `lambda_requirements.txt`
    where we will list the dependencies for the Lambda function code. In the `lambda_requirements.txt`
    file, type `openpyxl==3.0.10` and save it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, before deploying the function, we need to create an IAM role that will
    give Lambda the necessary permissions. On the AWS console, go to the IAM page,
    choose **Roles** in the side menu, and click on **Create a** **new role**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next page, select **AWS service** for the **Trusted entity type** and
    **Lambda** for the **Use case** (as shown in *Figure 11**.11*). Click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.11 – Selecting trusted entity and AWS service](img/B21927_11_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – Selecting trusted entity and AWS service
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will select a permission policy. Choose **Administrator Access** and
    click **Next**. Remember that having such open permissions is *not* a good practice
    for production environments. You should set permissions only for the actions and
    resources needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, choose a name for your IAM role (`BDOK-Lambda-service-role`, for instance)
    and click on **Create role**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, you will see the IAM **Roles** page again. Search for your created role
    and click on it (*Figure 11**.12*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.12 – Selecting your created IAM role](img/B21927_11_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 – Selecting your created IAM role
  prefs: []
  type: TYPE_NORMAL
- en: On the role’s page, you will see the **Amazon Resource Name** (**ARN**) of the
    role. Copy it and save it for later. We will need that name to deploy the Lambda
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, inside the `function` folder you created, create a new folder called `worksheet`.
    Copy two files from [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent/function/worksheet](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent/function/worksheet),
    the first named `__init__.py` and the second named `template.py` and place those
    code files inside the `worksheet` folder. This code contains a class named `CaseTemplate`
    that builds an Excel worksheet with the `openpyxl` Python library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, copy another two files in [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent/scripts](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent/scripts)
    folder named `build_lambda_package.sh` and `create_lambda_function.sh`. Those
    files contain bash code that will install the dependencies for the Lambda function
    and deploy it to AWS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we will deploy our Lambda function. This is a good time to check whether
    your project structure is correct. The folder and file structure should look like
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: sh build_lambda_package.sh
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: sh create_lambda_function.sh "<YOUR_ROLE_ARN>"
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Remember to change `<YOUR_ROLE_ARN>` to the actual ARN of your Lambda IAM role.
    Now, we have some more work to do. Next, we will create the DynamoDB table to
    store information about the use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a DynamoDB table
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DynamoDB is a fully managed NoSQL database service. It is a key-value and document
    database that can deliver single-digit millisecond performance at any scale. DynamoDB
    is optimized for running serverless applications and is designed to scale up or
    down automatically to meet demand, without having to provision or manage servers.
    It is particularly well suited for applications that need low-latency read and
    write access to data at any scale. Its extremely low latency makes it a very good
    choice for an AI assistant application. Let’s get to it:'
  prefs: []
  type: TYPE_NORMAL
- en: In the AWS console, navigate to the **DynamoDB** page. In the side menu, click
    on **Tables** and then, click on **Create table**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next page, fill in `case-sheets` and the `caseSheetId`. Remember to select
    **Number** to indicate that this entry is a number, as shown in *Figure 11**.13*.
    Leave all the other configurations to default and click **Create table**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.13 – Creating a DynamoDB table](img/B21927_11_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – Creating a DynamoDB table
  prefs: []
  type: TYPE_NORMAL
- en: In a few seconds, you should have your DynamoDB table ready for use. Now, we
    will configure the Bedrock agent.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, in the last part of this section, we will configure a Bedrock agent and
    link it to its backend Lambda function and the knowledge base database. Let’s
    get to it:'
  prefs: []
  type: TYPE_NORMAL
- en: First, in the AWS console, search for `Bedrock` and, in the side menu, click
    on **Agents**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the pop-up box, enter the name of your agent (`aws-competency-agent`) and
    click on **Create**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, you will see the agent configuration page. Scroll down to **Select model**
    and choose the Anthropic model **Claude 3 Haiku** (you can also play with the
    other available models as you like).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `You are a friendly AI assistant. Your main goal is to help AWS partner
    companies build case sheets for the AWS Competency program, register those cases,
    and tell the user the information about the registered cases. When you generate
    a case sheet, always show back to the user the ID of the case sheet (id), the
    client's name (client), and the name of the case (casename) and confirm that the
    case was successfully created. Also, answer the questions of the user about what
    you can do and how you can help.` This is a very important part of the agent configuration.
    Play with these instructions as much as you like. An example of this screen is
    shown in *Figure 11**.14*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.14 – Configuring the agent’s instructions](img/B21927_11_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – Configuring the agent’s instructions
  prefs: []
  type: TYPE_NORMAL
- en: After that, click on the **Save** button at the top of the page to make AWS
    create the necessary permission policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, scroll down to the **Action groups** section and click on **Add**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next page, select a name for your action group. For **Action group type**,
    select **Define with API schemas**. In **Action group invocation**, select **Select
    an existing Lambda function** and select the Lambda function we just created (*Figure
    11**.15*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.15 – Selecting a Lambda function for the agent’s action group](img/B21927_11_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 – Selecting a Lambda function for the agent’s action group
  prefs: []
  type: TYPE_NORMAL
- en: Now, in the **Action group schema** section, choose **Select an existing API
    schema** and then, click on **Browse S3** to search for the OpenAPI schema we
    have saved on S3 (*Figure 11**.16*). Then, click on **Create**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.16 – Selecting the OpenAPI schema](img/B21927_11_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 – Selecting the OpenAPI schema
  prefs: []
  type: TYPE_NORMAL
- en: Next, in the **Knowledge base** section, click on **Add**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the knowledge base we have created before and type some instructions
    for the agent on how to use it. For instance: `This knowledge base contains information
    on the following AWS Competency programs: conversational AI, data and analytics,
    DevOps, education, energy, financial services, machine learning, and security`.
    Make sure **Knowledge base status** is set to **Enabled** (*Figure 11**.17*).
    Click **Save** **and exit**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.17 – Attaching a knowledge base to the agent](img/B21927_11_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 – Attaching a knowledge base to the agent
  prefs: []
  type: TYPE_NORMAL
- en: Now, you are back to the agent’s editing page. Nothing else is needed here,
    so you can click on **Prepare** at the top to get your agent ready to run, and
    then click on **Save** **and exit**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, you will be led back to the agent’s main page. Scroll down to the **Aliases**
    section and click on **Create**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type in an alias name (`aws-competency`, for instance) and click on **Create
    Alias**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.18 – Creating an alias](img/B21927_11_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 – Creating an alias
  prefs: []
  type: TYPE_NORMAL
- en: Now, the last thing to do is register a permission on Lambda for this agent
    to trigger the function execution. On the agent’s main page, copy the agent ARN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, go to the **Lambda** page and click on the function we created for this
    exercise. On the function’s main page, scroll down, click on **Configuration**,
    and then click on **Permissions** in the side menu (*Figure 11**.19*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.19 – Lambda permissions](img/B21927_11_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19 – Lambda permissions
  prefs: []
  type: TYPE_NORMAL
- en: Scroll down again to the **Resource-based policy statements** section and click
    on **Add permissions**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next page, fill in the `lambda:InvokeFunction` (*Figure 11**.20*). Then,
    click on **Save**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.20 – Configuring the Lambda permission for the Bedrock agent](img/B21927_11_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.20 – Configuring the Lambda permission for the Bedrock agent
  prefs: []
  type: TYPE_NORMAL
- en: That’s all the configuration we need to get the agent running. Now, it’s time
    for deployment. Let’s get our Streamlit application to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the application on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deploying the agent Streamlit application on Kubernetes follows the same path
    we did for the other two applications deployed before. The only thing different
    is that we must create a new `configmap` with the agent’s ID and its alias ID:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the **Agent** page in the AWS console and copy the agent’s ID (in the
    top section) and the alias ID (in the bottom section).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, create `configmap` with those parameters with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Remember to replace the `<YOUR_ALIAS_ID>` and `<YOUR_AGENT_ID>` placeholders
    with the actual values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Build a custom image if you want to customize the application. If you don’t,
    use the ready image from DockerHub ([https://hub.docker.com/r/neylsoncrepalde/chat-with-claude-agent](https://hub.docker.com/r/neylsoncrepalde/chat-with-claude-agent)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will define a `deploy_agent.yaml` file for the application and service
    deployment. The content for this file is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent)
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With this file copied locally, now run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Wait for a few seconds for `LoadBalancer` to get started. Then, run the following
    to get the URL of `LoadBalancer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Paste it in a browser adding the correct port (`:8501`) to see the magic happening
    (*Figure 11**.21*).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.21 – AWS Competency agent application UI](img/B21927_11_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.21 – AWS Competency agent application UI
  prefs: []
  type: TYPE_NORMAL
- en: Try inserting a prompt for the creation of a new use case as in *Figure 11**.18*.
    Also, you can check specific information about this case passing the case’s ID
    (*Figure 11**.22*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.22 – Checking use case information with the agent](img/B21927_11_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.22 – Checking use case information with the agent
  prefs: []
  type: TYPE_NORMAL
- en: Play a little bit. Ask questions about the Competency program and try registering
    different cases. Also, you can check AWS DynamoDB and see the information ingested
    in our created table and check S3 to see the Excel files the agent created.
  prefs: []
  type: TYPE_NORMAL
- en: That is it! Congratulations! You have just deployed a full generative AI agent
    application that can perform tasks for you using natural language on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the exciting world of generative AI and learned
    how to harness its power on Kubernetes. We started by understanding the fundamental
    concepts of generative AI, its underlying mechanisms, and how it differs from
    traditional machine learning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: We then leveraged Amazon Bedrock, a comprehensive suite of services, to build
    and deploy generative AI applications. We learned how to work with Bedrock’s foundational
    models, such as Claude 3 Haiku and Claude 3 Sonnet, and how to integrate them
    into a Streamlit application for interactive user experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we delved into the concept of RAG, which combines the power of generative
    AI with external knowledge bases. We built a RAG system using Knowledge Bases
    for Amazon Bedrock, enabling our application to access and leverage vast amounts
    of structured data, improving the accuracy and relevance of the generated output.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we explored Agents for Amazon Bedrock, a powerful feature that allows
    generative AI models to automate tasks and take actions on our behalf. We learned
    how to build an agent, define its capabilities through an OpenAPI schema, and
    create the underlying Lambda function that serves as the backend for our agent.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we gained hands-on experience in building and deploying
    generative AI applications on Kubernetes. The skills and knowledge acquired in
    this chapter are invaluable in today’s rapidly evolving technological landscape.
    Generative AI is transforming industries and revolutionizing how we interact with
    and leverage AI. By mastering the tools and techniques presented in this chapter,
    you will be well equipped to build innovative and intelligent applications that
    can generate human-like content, leverage external knowledge sources, and automate
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss some important points needed for a production-ready
    Kubernetes environment, which we did not have space to discuss throughout the
    book.
  prefs: []
  type: TYPE_NORMAL
