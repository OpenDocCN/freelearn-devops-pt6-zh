<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Walking through Kubernetes Concepts</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will cover the following recipes: </p>
<ul>
<li>Linking Pods and containers</li>
<li>Managing Pods with ReplicaSets </li>
<li>Deployment API</li>
<li>Working with Services </li>
<li>Working with Volumes </li>
<li>Working with Secrets </li>
<li>Working with names </li>
<li>Working with Namespaces </li>
<li>Working with labels and selectors </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will start by creating different kinds of resources on the Kubernetes system. In order to realize your application in a microservices structure, reading the recipes in this chapter will be a good start towards understanding the concepts of the Kubernetes resources and consolidating them. After you deploy applications in Kubernetes, you can work on its scalable and efficient container management, and also fulfill the DevOps delivering procedure of microservices.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An overview of Kubernetes</h1>
                </header>
            
            <article>
                
<p>Working with Kubernetes is quite easy, using either a <strong>Command Line Interface</strong> (<strong>CLI</strong>) or API (RESTful). This section will describe Kubernetes control by CLI. The CLI we use in this chapter is version 1.10.2.</p>
<p>After you install Kubernetes master, you can run a <kbd><span>kubectl</span></kbd> command as follows. It shows the <span>kubectl</span> and Kubernetes master versions (both the API Server and CLI are v1.10.2):</p>
<pre><span>$ kubectl version --short<br/></span><span>Client Version: v1.10.2<br/></span><span>Server Version: v1.10.2</span></pre>
<p><kbd><span>kubectl</span></kbd> connects the Kubernetes API server using the RESTful API. By default, it attempts to access the localhost if <kbd>.kube/config</kbd> is not configured, otherwise you need to specify the API server address using the <kbd><span>--server</span></kbd> parameter. Therefore, it is recommended to use <kbd><span>kubectl</span></kbd> on the API server machine for practice. </p>
<div class="packt_tip packt_infobox"><span>If you use</span> <span>kubectl</span> <span>over the network, you need to consider authentication and authorization for the API server. See</span> <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml" target="_blank">Chapter 7</a><span>,</span> <em>Building Kubernetes on GCP</em><span>.</span></div>
<p><kbd><span>kubectl</span></kbd> is the only command for Kubernetes clusters, and it controls the Kubernetes cluster manager. Find more information at <a href="http://kubernetes.io/docs/user-guide/kubectl-overview/"><span>http://kubernetes.io/docs/user-guide/kubectl-overview/</span></a>. Any container, or Kubernetes cluster operation, can be performed by a <kbd><span>kubectl</span></kbd> command. </p>
<p>In addition, <span>kubectl</span> allows the inputting of information via either the command line's optional arguments or a file (use the <kbd><span>-f</span></kbd> option); it is highly recommended to use a file, because you can maintain Kubernetes configuration as code. This will be described in detail in this chapter.</p>
<p>Here is a typical <kbd>kubectl</kbd> command-line argument: </p>
<pre>kubectl [command] [TYPE] [NAME] [flags]</pre>
<p>The attributes of the preceding command are as follows: </p>
<ul>
<li><kbd><span>command</span></kbd>: Specifies the operation that you want to perform on one or more resources. </li>
<li><kbd><span>TYPE</span></kbd>: Specifies the resource type. Resource types are case-sensitive and you can specify the singular, plural, or abbreviated forms. </li>
<li><kbd><span>NAME</span></kbd>: Specifies the name of the resource. Names are case-sensitive. If the name is omitted, details for all resources are displayed. </li>
<li><kbd><span>flags</span></kbd>: Specifies optional flags. </li>
</ul>
<p>For example, if you want to launch <kbd>nginx</kbd>, you can use either the <kbd><span>kubectl run</span></kbd> command or the <kbd>kubectl create -f</kbd> command with the YAML file as follows:</p>
<ol>
<li>Use the <kbd>run</kbd> command:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl run my-first-nginx --image=nginx "my-first-nginx"</pre>
<ol start="2">
<li>Use the <kbd>create -f</kbd> command with the YAML file:</li>
</ol>
<pre style="padding-left: 90px">$ cat nginx.yaml <br/><span>apiVersion: apps/v1<br/></span><span>kind: Deployment<br/></span><span>metadata:<br/></span><span>  name: my-first-nginx<br/></span><span>  labels:<br/></span><span>    app: nginx<br/></span><span>spec:<br/></span><span>  replicas: 1<br/></span><span>  selector:<br/></span><span>    matchLabels:<br/></span><span>      app: nginx<br/></span><span>  template:<br/></span><span>    metadata:<br/></span><span>      labels:<br/></span><span>        app: nginx<br/></span><span>    spec:<br/></span><span>      containers:<br/></span><span>      - name: nginx<br/></span><span>        image: nginx</span> <br/><br/>//specify -f (filename) <br/>$ kubectl create -f nginx.yaml <br/>deployment.apps "my-first-nginx" created</pre>
<ol start="3">
<li>If you want to see the status of the Deployment, type the <kbd><span>kubectl get</span></kbd> command as follows: </li>
</ol>
<pre style="padding-left: 90px"><span>$ kubectl get deployment<br/></span><span>NAME             DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE<br/></span><span>my-first-nginx   1         1         1            1           4s</span></pre>
<ol start="4">
<li>If you also want the support abbreviation, type the following: </li>
</ol>
<pre style="padding-left: 90px"><span>$ kubectl get deploy<br/></span><span>NAME             DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE<br/></span><span>my-first-nginx   1         1         1            1           38s</span></pre>
<ol start="5">
<li>If you want to delete these resources, type the <kbd><span>kubectl delete</span></kbd> command as follows: </li>
</ol>
<pre style="padding-left: 90px"><span>$ kubectl delete deploy my-first-nginx<br/></span><span>deployment.extensions "my-first-nginx" deleted</span></pre>
<ol start="6">
<li>The <kbd><span>kubectl</span></kbd> command supports many kinds of sub-commands; use the <kbd><span>-h</span></kbd> option to see the details, for example:</li>
</ol>
<pre style="padding-left: 90px">//display whole sub command options <br/>$ kubectl -h <br/><br/><br/>//display sub command "get" options <br/>$ kubectl get -h <br/><br/><br/>//display sub command "run" options <br/>$ kubectl run -h </pre>
<p>This section describes how to use the <kbd><span>kubectl</span></kbd> command to control the Kubernetes cluster. The following recipes describe how to set up Kubernetes components: </p>
<ul>
<li><span><em>Setting up a Kubernetes cluster on macOS using minikube</em> and <em>Set up a Kubernetes cluster on Windows</em></span><em> using minikube</em> in <a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml">Chapter 1</a>, <em>Building Your Own Kubernetes <span>Cluster</span></em></li>
<li><span><em>Setting up a Kubernetes cluster on Linux using kubeadm</em> in <a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml">Chapter 1</a>, <em>Building Your Own Kubernetes Cluster</em></span></li>
<li><span><em>Setting up a Kubernetes cluster on Linux using kubespray (Ansible) </em>in <a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml">Chapter 1</a>, <em>Building Your Own Kubernetes Cluster</em></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linking Pods and containers</h1>
                </header>
            
            <article>
                
<p>The Pod is a group of one or more containers and the smallest deployable unit in Kubernetes. Pods are always co-located and co-scheduled, and run in a shared context. Each Pod is isolated by the following Linux namespaces:</p>
<ul>
<li>The <strong>process ID</strong> (<strong>PID</strong>) namespace </li>
<li>The network namespace </li>
<li>The <strong>interprocess communication</strong> (<strong>IPC</strong>) namespace </li>
<li>The <strong>unix time sharing</strong> (<strong>UTS</strong>) namespace </li>
</ul>
<p>In a pre-container world, they would have been executed on the same physical or virtual machine. </p>
<p>It is useful to construct your own application stack Pod (for example, web server and database) that are mixed by different Docker images. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>You must have a Kubernetes cluster and make sure that the Kubernetes node has accessibility to the Docker Hub (<a href="https://hub.docker.com"><span>https://hub.docker.com</span></a>) in order to download Docker images. </p>
<div class="packt_infobox">If you are running minikube, use <kbd>minikube ssh</kbd> to log on to the minikube VM first, then run the <kbd>docker pull</kbd> command.</div>
<p><span>You can simulate downloading a Docker image by using the </span><kbd><span>docker pull</span></kbd><span> command as </span>follows: </p>
<pre>//this step only if you are using minikube<br/>$ <strong>minikube ssh</strong><br/>                         _ _ <br/>            _ _ ( ) ( ) <br/>  ___ ___ (_) ___ (_)| |/') _ _ | |_ __ <br/>/' _ ` _ `\| |/' _ `\| || , &lt; ( ) ( )| '_`\ /'__`\<br/>| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )( ___/<br/>(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)<br/><br/><br/>//run docker pull to download CentOS docker image<br/>$ <strong>docker pull centos</strong><br/>Using default tag: latest<br/>latest: Pulling from library/centos<br/>d9aaf4d82f24: Pull complete <br/>Digest: sha256:4565fe2dd7f4770e825d4bd9c761a81b26e49cc9e3c9631c58cfc3188be9505a<br/>Status: Downloaded newer image for centos:latest</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The following are the steps to create a Pod has 2 containers:</p>
<ol>
<li>Log on to the Kubernetes machine (no need to log on if using minikube) and prepare the following YAML file. It defines the launch <kbd>nginx</kbd> container and the CentOS container. </li>
<li>The <kbd>nginx</kbd> container opens the HTTP port (TCP/<kbd>80</kbd>). On the other hand, the CentOS container attempts to access the <kbd><span>localhost:80</span></kbd> every three seconds using the <kbd><span>curl</span></kbd> command: </li>
</ol>
<div class="page">
<div class="layoutArea">
<div class="column">
<pre style="padding-left: 90px"><span>$ cat my-first-pod.yaml <br/></span><span>apiVersion: v1<br/></span><span>kind: Pod<br/></span><span>metadata: <br/></span><span>  name: my-first-pod<br/></span><span>spec:<br/></span><span>  containers:<br/></span><span>  - name: my-nginx<br/></span><span>    image: nginx<br/></span><span>  - name: my-centos<br/></span><span>    image: centos<br/></span><span>    command: ["/bin/sh", "-c", "while : ;do curl http://localhost:80/; sleep 10; done"]</span></pre></div>
</div>
</div>
<ol start="3">
<li>Then, <span>execute the <kbd>kubectl create</kbd> command </span>to<span> launch <kbd>my-first-pod</kbd> as follows:</span></li>
</ol>
<div class="page">
<div class="layoutArea">
<div class="column">
<pre style="padding-left: 90px"><span>$ kubectl create -f my-first-pod.yaml <br/></span><span>pod "my-first-pod" created </span></pre></div>
</div>
</div>
<div class="page">
<div class="layoutArea">
<div class="column">
<p style="padding-left: 60px"><span>It takes between a few seconds and a few minutes, depending on the network bandwidth</span> <span>of the Docker Hub and Kubernetes node's spec.</span></p>
<ol start="4">
<li>You can check <kbd>kubectl get pods</kbd> to see the status, as follows:</li>
</ol>
<pre style="padding-left: 90px">//still downloading Docker images (0/2)<br/><span>$ kubectl get pods<br/></span><span>NAME           READY     STATUS              RESTARTS   AGE<br/></span><span>my-first-pod   <strong>0/2</strong>       ContainerCreating   0          14s<br/><br/><br/></span>//my-first-pod is running (2/2)<br/><span>$ kubectl get pods<br/></span><span>NAME           READY     STATUS    RESTARTS   AGE<br/></span><span>my-first-pod   2/2       <strong>Running  </strong> 0          1m</span></pre>
<p style="padding-left: 60px"><span>Now both the nginx container (</span><kbd>my-nginx</kbd><span>) and the CentOS container (</span><kbd>my-centos</kbd><span>)</span> <span>are ready.</span></p>
</div>
</div>
</div>
<div class="page">
<div class="layoutArea">
<div class="column">
<ol start="5">
<li>Let's check whether the CentOS container can access <kbd>nginx</kbd> or not. You can run the <kbd>kubectl exec</kbd> command to run bash on the CentOS container, then run the <kbd>curl</kbd> command to access the <kbd>nginx</kbd>, as follows:</li>
</ol>
</div>
<pre style="padding-left: 90px">//run bash on my-centos container<br/>//then access to TCP/80 using curl<br/>$ kubectl exec my-first-pod -it -c my-centos -- /bin/bash<br/>[root@my-first-pod /]# <br/>[root@my-first-pod /]# curl -L http://localhost:80<br/>&lt;!DOCTYPE html&gt;<br/>&lt;html&gt;<br/>&lt;head&gt;<br/>&lt;title&gt;Welcome to nginx!&lt;/title&gt;<br/>&lt;style&gt;<br/>    body {<br/>        width: 35em;<br/>        margin: 0 auto;<br/>        font-family: Tahoma, Verdana, Arial, sans-serif;<br/>    }<br/>&lt;/style&gt;<br/>&lt;/head&gt;<br/>&lt;body&gt;<br/>&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;<br/>&lt;p&gt;If you see this page, the nginx web server is successfully installed and<br/>working. Further configuration is required.&lt;/p&gt;<br/><br/>&lt;p&gt;For online documentation and support please refer to<br/>&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;<br/>Commercial support is available at<br/>&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;<br/><br/>&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;<br/>&lt;/body&gt;<br/>&lt;/html&gt;</pre>
<p>A<span>s you can see, the Pod links two different containers, <kbd>nginx</kbd> and <kbd>CentOS</kbd>, into the </span><span>same Linux network namespace.</span></p>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>When launching a Pod, the Kubernetes scheduler dispatches to the kubelet process to handle all the operations to launch both <kbd>nginx</kbd> and <kbd>CentOS</kbd> containers on one Kubernetes node.</p>
<p>The following diagram illustrates these two containers and the Pod; these two containers can communicate via the localhost network, because within the Pod containers, it share the network interface:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-649 image-border" src="assets/17157006-2419-4268-9291-481e14d19121.png" style="width:37.00em;height:15.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A Pod has two containers, which can communicate via localhost</div>
<p>If you have two or more nodes, you can check the <kbd><span>-o wide</span></kbd> option to find a node which runs a Pod:</p>
<pre>//it indicates Node "minikube" runs my-first-pod <br/><span>$ kubectl get pods -o wide<br/></span><span>NAME           READY     STATUS    RESTARTS   AGE       IP           NODE<br/></span><span>my-first-pod   2/2       Running   0          43m       172.17.0.2   minikube</span></pre>
<p>Log in to that node, then you can check the <kbd><span>docker ps | grep my-first-pod</span></kbd> command to see the running containers as follows: </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-220 image-border" src="assets/73ca0964-dd98-4df3-b84b-868116a59e25.png" style="width:181.33em;height:81.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>List of containers that belong</span> to my-first-pod</div>
<p class="mce-root">You may notice that <kbd>my-first-pod</kbd> contains three containers; <kbd>centos</kbd>, <kbd>nginx</kbd>, and <kbd>pause</kbd> are running instead of two. Because each Pod we need to keep belongs to a particular Linux namespace, if both the CentOS and nginx containers die, the namespace will also destroyed. Therefore, the pause container just remains in the Pod to maintain Linux namespaces. </p>
<p>Let's launch a second Pod, rename it as <span><kbd>my-second-pod</kbd>,</span> and run the <span><kbd>kubectl</kbd> create</span> command as follows: </p>
<pre>//just replace the name from my-first-pod to my-second-pod <br/><span>$ cat my-first-pod.yaml | sed -e 's/my-first-pod/my-second-pod/' &gt; my-second-pod.yaml<br/></span><span><br/><br/>//metadata.name has been changed to my-second-pod<br/>$ cat my-second-pod.yaml <br/></span><span>apiVersion: v1<br/></span><span>kind: Pod<br/></span><span>metadata: <br/></span><span>  name: <strong>my-second-pod</strong><br/></span><span>spec:<br/></span><span>  containers:<br/></span><span>  - name: my-nginx<br/></span><span>    image: nginx<br/></span><span>  - name: my-centos<br/></span><span>    image: centos<br/></span><span>    command: ["/bin/sh", "-c", "while : ;do curl<br/>http://localhost:80/; sleep 10; done"]<br/></span><br/><br/>//create second pod<br/><span>$ kubectl create -f my-second-pod.yaml <br/></span><span>pod "my-second-pod" created<br/><br/><br/>//2 pods are running<br/></span><span>$ kubectl get pods<br/></span><span>NAME            READY     STATUS    RESTARTS   AGE<br/></span><span>my-first-pod    2/2       Running   0          1h<br/></span><span>my-second-pod   2/2       Running   0          43s</span></pre>
<p>Now you have two Pods; each Pod has two containers, <kbd>centos</kbd> and <kbd>nginx</kbd>. So a total of four containers are running on your Kubernetes cluster as in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-650 image-border" src="assets/75affb40-1877-4289-b830-42e63715dad9.png" style="width:29.08em;height:27.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Duplicate Pod from my-first-pod to my-second-pod</div>
<div class="packt_tip"><span>If you would like to deploy more of the same Pod, consider using a Deployment (ReplicaSet) instead. </span></div>
<p>After your testing, you can run the <span><kbd>kubectl</kbd> delete</span> command to delete your Pod from the Kubernetes cluster: </p>
<pre><span>//specify --all option to delete all pods<br/>$ kubectl delete pods --all<br/></span><span>pod "my-first-pod" deleted<br/></span><span>pod "my-second-pod" deleted<br/></span><span><br/><br/>//pods are terminating<br/>$ kubectl get pods<br/></span><span>NAME            READY     STATUS        RESTARTS   AGE<br/></span><span>my-first-pod    2/2       Terminating   0          1h<br/></span><span>my-second-pod   2/2       Terminating   0          3m</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>This recipe from this chapter described how to control Pods. They are the basic components of Kubernetes operation. The following recipes will describe the advanced operation of Pods using Deployments, Services, and so on: </p>
<ul>
<li><em>Managing Pods with ReplicaSets</em></li>
<li><em>Deployment API</em></li>
<li><em>Working with Services </em></li>
<li><em>Working with labels and selectors</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managing Pods with ReplicaSets </h1>
                </header>
            
            <article>
                
<p>A ReplicaSet is a term for API objects in Kubernetes that refer to Pod replicas. The idea is to be able to control a set of Pods' behaviors. The ReplicaSet ensures that the Pods, in the amount of a user-specified number, are running all the time. If some Pods in the ReplicaSet crash and terminate, the system will recreate Pods with the original configurations on healthy nodes automatically, and keep a certain number of processes continuously running. While changing the size of set, users can scale the application <span>out or down</span> easily. According to this feature, no matter whether you need replicas of Pods or not, you can always rely on ReplicaSet for auto-recovery and scalability. In this recipe, you're going to learn how to manage your Pods with ReplicaSet:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-651 image-border" src="assets/940902c6-7d59-4f0e-81c8-b2cebfc002be.png" style="width:41.83em;height:22.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">ReplicaSet and their Pods on two nodes</div>
<p>The ReplicaSet usually handles a tier of applications. As you can see in the preceding diagram, we launch a ReplicaSet with three Pod replicas. Some mechanism details are listed as follows:</p>
<ul>
<li>The <strong>kube-controller-manager</strong> daemon helps to maintain the resource running in its desired state. For example, the desired state of ReplicaSet in the diagram is three Pod replicas.</li>
<li>The <strong>kube-scheduler</strong> daemon on master, the scheduler of Kubernetes, takes charge of assigning tasks to healthy nodes.</li>
<li>The selector of the <span class="packt_screen">ReplicaSet</span> is used for deciding which Pods it covers. If the key-value pairs in the Pod's label include all items in the selector of the ReplicaSet, this Pod belongs to this ReplicaSet. As you will see, the diagram shows three Pods are under the charge of the ReplicaSet. Even though Pod 2 has a different label of <kbd>env</kbd>, it is selected since the other two labels, <kbd>role</kbd> and <kbd>project</kbd>, match the ReplicaSet's selector.</li>
</ul>
<div class="packt_tip"><strong>ReplicationController? ReplicaSet? <br/></strong><span>For experienced Kubernetes players, you may notice ReplicaSet looks quite similar to the ReplicationController. Since version 1.2 of Kubernetes, in order to concentrate on different features, the ReplicationController's functionality has been covered by ReplicaSet and Deployment. ReplicaSet focuses on the Pod replica, keeping certain Pods running in healthy states. On the other hand, Deployment is a higher-level API, which can manage the ReplicaSet, perform application rolling updates, and expose the services. In Kubernetes v1.8.3, users can still create replication controllers. However, using Deployment with ReplicaSet is more recommended because these are up to date and have finer granularity of configuration.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Creating a ReplicaSet is the same as creating any Kubernetes resource; we fire the <kbd>kubectl</kbd> command on the Kubernetes master. Therefore, we ensure your Kubernetes environment is ready to accept your order. More than that, the Kubernetes node should be able to access the Docker Hub. For the demonstration in the following few pages, we would take official <kbd>nginx</kbd> docker image for example, which stores in public docker registry as well.</p>
<div class="packt_tip"><strong>The evaluation of a prepared Kubernetes system<br/></strong><span>You can verify whether your Kuberenetes master is a practical one through checking the items here:</span>
<ul>
<li><span><strong>Check whether the daemons are running or no</strong>t: There should be three working daemon processes on the master node:</span> <kbd>apiserver</kbd>, <kbd>scheduler</kbd>, and <kbd>controller-manager</kbd>.</li>
<li><span><strong>Check whether the command kubectl exists and is workable</strong>: Try the command <kbd>kubectl get cs</kbd> to cover this bullet point and the first one. You can verify not only the status of components but also the feasibility of</span> <kbd>kubectl</kbd>.</li>
<li><span><strong>Check whether the nodes are ready to work</strong>: You can check them by using the command <kbd>kubectl get nodes</kbd> to get their status.</span></li>
</ul>
<span>In the case that some items listed here are invalid, please refer to <a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml"/></span><span><a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml">Chapter 1</a>, <em>Building Your Own Kubernetes Cluster,</em> for proper guidelines based on the installation you chose.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this section, we will demonstrate the life cycle of a ReplicaSet from creation to destruction.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a ReplicaSet</h1>
                </header>
            
            <article>
                
<p>When trying to use the command line to launch a Kubernetes Service immediately, we usually fire <kbd>kubectl run</kbd>. However, it would creates a Deployment by default, and not only taking care of the Pod replica but also providing a container-updating mechanism. To simply create a standalone ReplicaSet, we can exploit a configuration YAML file and run it:</p>
<pre>$ cat my-first-replicaset.yaml<br/>apiVersion: extensions/v1beta1<br/>kind: ReplicaSet<br/>metadata:<br/>  name: my-first-replicaset<br/>  labels:<br/>    version: 0.0.1<br/>spec:<br/>  replicas: 3<br/>  selector:<br/>    matchLabels:<br/>      project: My-Happy-Web<br/>      role: frontend<br/>  template:<br/>    metadata:<br/>      labels:<br/>        project: My-Happy-Web<br/>        role: frontend<br/>        env: dev<br/>    spec:<br/>      containers:<br/>      - name: happy-web<br/>        image: nginx:latest</pre>
<p>The preceding file is the YAML for our first ReplicaSet. It defines a ReplicaSet named <kbd>my-first-replicaset</kbd>, which has three replicas for its Pods. Labels and the selector are the most characteristic settings of ReplicaSet. There are two sets of labels: one for ReplicaSet, the other for Pods. The first label for ReplicaSet is under the metadata of this resource, right beneath the name, which is simply used for description. However, the other label value under the template's metadata, the one for Pods, is also used for identification. ReplicaSet takes charge of the Pods which have the labels covered by its selector.</p>
<p>In our example configuration file, the selector of ReplicaSet looks for Pods with <kbd>project: My-Happy-Web</kbd> and <kbd>role: frontend</kbd> tags. Since we initiate Pods under control of this ReplicaSet, the Pods' labels should definitely include what selector cares. You may get following error message while creating a ReplicaSet with incorrectly labeled Pods: <kbd>`selector` does not match template `labels`</kbd>.</p>
<p>Now, let's create ReplicaSet through this file:</p>
<pre>$ kubectl create -f my-first-replicaset.yaml<br/>replicaset.extensions "my-first-replicaset" created</pre>
<div class="packt_infobox"><strong>The API version of ReplicaSet in Kubernetes v1.9</strong> <br/>
While this book is under construction, Kubernetes v1.9 is released. The API version of ReplicaSet turns to a stable version <kbd>apps/v1</kbd> instead of <kbd>apps/v1beta2</kbd>. If you have an older version Kubernetes, please change the value of <kbd>apiVersion</kbd> to <kbd>apps/v1beta2</kbd>, or you can just update your Kubernetes system.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting the details of a ReplicaSet</h1>
                </header>
            
            <article>
                
<p>After we create the ReplicaSet, the subcommands <kbd>get</kbd> and <kbd>describe</kbd> can help us to capture its information and the status of Pods. In the CLI of Kubernetes, we are able to use the abbreviation rs for resource type, instead of the full name ReplicaSet:</p>
<pre>// use subcommand "get" to list all ReplicaSets<br/>$ kubectl get rs<br/>NAME                  DESIRED   CURRENT   READY     AGE<br/>my-first-replicaset   3         3         3         4s</pre>
<p>This result shows roughly that the Pod replicas of <kbd>my-first-replicaset</kbd> are all running successfully; currently running Pods are of the desired number and all of them are ready for serving requests.</p>
<p>For detailed information, check by using the subcommand <kbd>describe</kbd>:</p>
<pre>// specify that we want to check ReplicaSet called my-first-replicaset<br/>$ kubectl describe rs my-first-replicaset<br/>Name:         my-first-replicaset<br/>Namespace:    default<br/>Selector:     project=My-Happy-Web,role=frontend<br/>Labels:       version=0.0.1<br/>Annotations:  &lt;none&gt;<br/>Replicas:     3 current / 3 desired<br/>Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed<br/>Pod Template:<br/>  Labels:  env=dev<br/>           project=My-Happy-Web<br/>           role=frontend<br/>  Containers:<br/>   happy-web:<br/>    Image:        nginx:latest<br/>    Port:         &lt;none&gt;<br/>    Host Port:    &lt;none&gt;<br/>    Environment:  &lt;none&gt;<br/>    Mounts:       &lt;none&gt;<br/>  Volumes:        &lt;none&gt;<br/>Events:<br/>  Type    Reason            Age   From                   Message<br/>  ----    ------            ----  ----                   -------<br/>  Normal  SuccessfulCreate  9s    replicaset-controller  Created pod: my-first-replicaset-8hg55<br/>  Normal  SuccessfulCreate  9s    replicaset-controller  Created pod: my-first-replicaset-wtphz<br/>  Normal  SuccessfulCreate  9s    replicaset-controller  Created pod: my-first-replicaset-xcrws</pre>
<p>You can see that the output lists ReplicaSet's particulars of the configuration, just like what we requested in the YAML file. Furthermore, the logs for the creation of Pods are shown as part of ReplicaSet, which confirms that the Pod replicas are successfully created and designated with unique names. You can also check Pods by name:</p>
<pre>// get the description according the name of Pod, please look at the Pod name shown on your screen, which should be different from this book.<br/>$ kubectl describe pod my-first-replicaset-xcrws</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Changing the configuration of a ReplicaSet</h1>
                </header>
            
            <article>
                
<p>The subcommands known as <kbd>edit</kbd>, <kbd>patch</kbd>, and <kbd>replace</kbd> can help to update live Kubernetes resources. All these functionalities change the settings by way of modifying a configuration file. Here we just take <kbd>edit</kbd>, for example.</p>
<p>The subcommand edit lets users modify resource configuration through the editor. Try to update your ReplicaSet through the command <kbd>kubectl edit rs $REPLICASET_NAME</kbd>; you will access this resource via the default editor with a YAML configuration file:</p>
<pre>// demonstrate to change the number of Pod replicas.<br/>$ kubectl get rs<br/>NAME                  DESIRED   CURRENT   READY     AGE<br/>my-first-replicaset   3         3         3         2m<br/><br/>// get in the editor, modify the replica number, then save and leave<br/>$ kubectl edit rs my-first-replicaset<br/># Please edit the object below. Lines beginning with a '#' will be ignored,<br/># and an empty file will abort the edit. If an error occurs while saving this file will be<br/># reopened with the relevant failures.<br/>#<br/>apiVersion: extensions/v1beta1<br/>kind: ReplicaSet<br/>metadata:<br/>  creationTimestamp: 2018-05-05T20:48:38Z<br/>  generation: 1<br/>  labels:<br/>    version: 0.0.1<br/>  name: my-first-replicaset<br/>  namespace: default<br/>  resourceVersion: "1255241"<br/>  selfLink: /apis/extensions/v1beta1/namespaces/default/replicasets/my-first-replicaset<br/>  uid: 18330fa8-cd55-11e7-a4de-525400a9d353<br/>spec:<br/>  <strong>replicas: 4<br/></strong>  selector:<br/>    matchLabels:<br/>...<br/>replicaset "my-first-replicaset" edited<br/>$ kubectl get rs<br/>NAME                  DESIRED   CURRENT   READY     AGE<br/>my-first-replicaset   4         4         4         4m</pre>
<p>In the demonstration, we succeed to add one Pod in the set, yet this is not the best practice for auto-scaling the Pod. Take a look at the <em>Working with configuration files</em> recipe in <a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml">Chapter 3</a>, <em>Playing with Containers</em>,<em> </em>for Reference, and try to change the other values.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deleting a ReplicaSet</h1>
                </header>
            
            <article>
                
<p>In order to remove the ReplicaSet from the Kubernetes system, you can rely on the subcommand <kbd>delete</kbd>. When we fire <kbd>delete</kbd> to remove the resource, it removes the target objects forcefully:</p>
<pre>$ time kubectl delete rs my-first-replicaset &amp;&amp; kubectl get pod<br/>replicaset.extensions "my-first-replicaset" deleted<br/>real  0m2.492s<br/>user  0m0.188s<br/>sys   0m0.048s<br/>NAME                        READY     STATUS        RESTARTS   AGE<br/>my-first-replicaset-8hg55   0/1       Terminating   0          53m<br/>my-first-replicaset-b6kr2   1/1       Terminating   0          48m<br/>my-first-replicaset-wtphz   0/1       Terminating   0          53m<br/>my-first-replicaset-xcrws   1/1       Terminating   0          53m</pre>
<p>We find that the response time is quite short and the effect is also instantaneous.</p>
<div class="packt_tip"><strong>Removing the Pod under ReplicaSet<br/></strong><span>As we mentioned previously, it is impossible to scale down the ReplicaSet by deleting the Pod, because while a Pod is removed, the ReplicaSet is out of stable status: if the desired number of Pods is not met, and the controller manager will ask ReplicaSet to create another one. The concept is shown in the following commands:</span>
<pre>// check ReplicaSet and the Pods<br/> $ kubectl get rs,pod<br/> NAME DESIRED CURRENT READY AGE<br/> rs/my-first-replicaset 3 3 3 14s<br/> NAME READY STATUS RESTARTS AGE<br/> po/my-first-replicaset-bxf45 1/1 Running 0 14s<br/> po/my-first-replicaset-r6wpx 1/1 Running 0 14s<br/> po/my-first-replicaset-vt6fd 1/1 Running 0 14s<br/> <br/> // remove certain Pod and check what happened<br/> $ kubectl delete pod my-first-replicaset-bxf45<br/> pod "my-first-replicaset-bxf45" deleted<br/> $ kubectl get rs,pod<br/> NAME DESIRED CURRENT READY AGE<br/> rs/my-first-replicaset 3 3 3 2m<br/> NAME READY STATUS RESTARTS AGE<br/> po/my-first-replicaset-dvbpg 1/1 Running 0 6s<br/> po/my-first-replicaset-r6wpx 1/1 Running 0 2m<br/> po/my-first-replicaset-vt6fd 1/1 Running 0 2m<br/> <br/> // check the event log as well<br/> $ kubectl describe rs my-first-replicaset<br/> (ignored)<br/> :<br/> Events:<br/> Type Reason Age From Message<br/> ---- ------ ---- ---- -------<br/> Normal SuccessfulCreate 2m replicaset-controller Created pod: my-first-replicaset-bxf45<br/> Normal SuccessfulCreate 2m replicaset-controller Created pod: my-first-replicaset-r6wpx<br/> Normal SuccessfulCreate 2m replicaset-controller Created pod: my-first-replicaset-vt6fd<br/> Normal SuccessfulCreate 37s replicaset-controller Created pod: my-first-replicaset-dvbpg</pre>
<span>You will find that although the</span> <kbd>my-first-replicaset-bxf45</kbd> Pod <span>is removed, the</span> <kbd>my-first-replicaset-dvbpg</kbd> P<span>od</span><span> </span><span>is created automatically and attached to this ReplicaSet.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>The</span> ReplicaSet defines a set of Pods by using a Pod template and labels. As in the ideas from previous sections, the ReplicaSet only manages the Pods via their labels. It is possible that the Pod template and the configuration of the Pod are different. This also means that standalone Pods can be added into a set by using label modification.</p>
<p>Let's evaluate this concept of selectors and labels by creating a ReplicaSet si<span>milar to the diagram at the beginning of this recipe:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-652 image-border" src="assets/dd292c2f-714e-431e-a7f0-c134076653fe.png" style="width:38.50em;height:20.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The ReplicaSet would cover Pods which have the same labels describing in its selector</div>
<p>First, we are going to create a CentOS Pod with the labels project: <kbd>My-Happy-Web</kbd>, <kbd>role: frontend</kbd>, and <kbd>env: test</kbd>:</p>
<pre>// use subcommand "run" with tag restart=Never to create a Pod<br/>$ kubectl run standalone-pod --image=centos --labels="project=My-Happy-Web,role=frontend,env=test" --restart=Never --command sleep 3600<br/>pod "standalone-pod" created<br/><br/>// check Pod along with the labels<br/>$ kubectl get pod -L project -L role -L env<br/>NAME             READY     STATUS    RESTARTS   AGE       PROJECT        ROLE       ENV<br/>standalone-pod   1/1       Running   0          3m        My-Happy-Web   frontend   test</pre>
<p>After adding this command, a standalone Pod runs with the labels we specified.</p>
<p>Next, go create your first ReplicaSet example by using the YAML file again:</p>
<pre>$ kubectl create -f my-first-replicaset.yaml<br/>replicaset.apps "my-first-replicaset" created<br/><br/>// check the Pod again<br/>$ kubectl get pod -L project -L role -L env<br/>NAME                        READY     STATUS    RESTARTS   AGE       PROJECT        ROLE       ENV<br/>my-first-replicaset-fgdc8   1/1       Running   0          14s       My-Happy-Web   frontend   dev<br/>my-first-replicaset-flc9m   1/1       Running   0          14s       My-Happy-Web   frontend   dev<br/>standalone-pod              1/1       Running   0          6m        My-Happy-Web   frontend   test</pre>
<p>As in the preceding result, only two Pods are created. It is because the Pod <kbd>standalone-pod</kbd> is considered one of the sets taken by <kbd>my-first-replicaset</kbd>. Remember that <kbd>my-first-replicaset</kbd> takes care of the Pods labeled with project: <kbd>My-Happy-Web</kbd> and <kbd>role:frontend</kbd> (ignore the <kbd>env </kbd><span>tag</span><span>). Go check the standalone Pod; you will find it belongs to a member of the ReplicaSet as well:</span></p>
<pre>$ kubectl describe pod standalone-pod<br/>Name:           standalone-pod<br/>Namespace:      default<br/>Node:           ubuntu02/192.168.122.102<br/>Start Time:     Sat, 05 May 2018 16:57:14 -0400<br/>Labels:         env=test<br/>                project=My-Happy-Web<br/>                role=frontend<br/>Annotations:    &lt;none&gt;<br/>Status:         Running<br/>IP:             192.168.79.57<br/>Controlled By:  <strong>ReplicaSet/my-first-replicaset<br/></strong>...</pre>
<p>Similarly, once we delete the set, the standalone Pod will be removed with the group:</p>
<pre>// remove the ReplicaSet and check pods immediately<br/>$ kubectl delete rs my-first-replicaset &amp;&amp; kubectl get pod<br/>replicaset.extensions "my-first-replicaset" deleted<br/>NAME                        READY     STATUS        RESTARTS   AGE<br/>my-first-replicaset-fgdc8   0/1       Terminating   0          1m<br/>my-first-replicaset-flc9m   0/1       Terminating   0          1m<br/>standalone-pod              0/1       Terminating   0          7m</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>There are multiple Kubernetes resources for Pod management. Users are encouraged to leverage various types of resources to meet different purposes. Let's comparing the resource types listed below with ReplicaSet:</p>
<ul>
<li><strong>Deployment</strong>: In general cases, Kubernetes Deployments are used together with ReplicaSet for complete Pod management: container rolling updates, load balancing, and service exposing.</li>
<li><strong>Job</strong>: Sometimes, we want the Pods run as a job instead of a service. A Kubernetes job is suitable for this situation. You can consider it a ReplicaSet with the constraint of termination.</li>
<li><strong>DaemonSet</strong>: More than ReplicaSet, the Kubernetes DaemonSet guarantees that the specified set is running on every node in the cluster. That said, a subset of ReplicaSet on every node.</li>
</ul>
<p>To get more idea and instruction, you can check the recipe <em>Ensuring flexible usage of your containers</em> in <a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml">Chapter 3</a>, <em>Playing with Containers</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>Now you understand the idea of ReplicaSet. Continue to look up the following recipes in this chapter for more Kubernetes resources, which will allow you to explore the magical effects of ReplicaSet:</p>
<ul>
<li><em>Deployment API</em></li>
<li><em>Working with Services</em></li>
<li><em>Working with labels an selectors</em></li>
</ul>
<p>Moreover, since you have built a simple ReplicaSet by using a configuration file, refer to more details about creating your own configuration files for Kubernetes resources:</p>
<ul>
<li><em>Working with configuration files</em> section in <a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml">Chapter 3</a>, <em><em>Playing with Containers</em></em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment API</h1>
                </header>
            
            <article>
                
<p>The Deployment API was introduced in Kubernetes version 1.2. It is replacing the replication controller. The functionalities of rolling-update and rollback by replication controller, it was achieved with client side (<kbd>kubectl</kbd> command and <kbd>REST API</kbd>), that <kbd>kubectl</kbd> need to keep connect while updating a replication controller. On the other hand, Deployments takes care of the process of rolling-update and rollback <span>at the server side. Once that request is accepted, the </span>client can disconnect immediately. </p>
<p>Therefore, the Deployments API is designed as a higher-level API to manage ReplicaSet objects. This section will explore how to use the Deployments API to manage ReplicaSets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In order to create Deployment objects, as usual, use the <kbd>kubectl run</kbd> command or prepare the YAML/JSON file that describe Deployment configuration. This example is using the <kbd>kubectl run</kbd> command to create a <kbd>my-nginx</kbd> Deployment object:</p>
<pre>//create my-nginx Deployment (specify 3 replicas and nginx version 1.11.0)<br/>$ kubectl run my-nginx --image=nginx:1.11.0 --port=80 --replicas=3<br/>deployment.apps "my-nginx" created<br/><br/><br/>//see status of my-nginx Deployment<br/>$ kubectl get deploy<br/>NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE<br/>my-nginx   3         3         3            3           8s<br/><br/><br/>//see status of ReplicaSet<br/>$ kubectl get rs<br/>NAME                 DESIRED   CURRENT   READY     AGE<br/>my-nginx-5d69b5ff7   3         3         3         11s<br/><br/><br/>//see status of Pod<br/>$ kubectl get pods<br/>NAME                       READY     STATUS    RESTARTS   AGE<br/>my-nginx-5d69b5ff7-9mhbc   1/1       Running   0          14s<br/>my-nginx-5d69b5ff7-mt6z7   1/1       Running   0          14s<br/>my-nginx-5d69b5ff7-rdl2k   1/1       Running   0          14s</pre>
<p>As you can see, a Deployment object <kbd>my-nginx</kbd> creates one <kbd>ReplicaSet</kbd>, which has an identifier: <kbd>&lt;Deployment name&gt;-&lt;hex decimal hash&gt;</kbd>. And then ReplicaSet creates three Pods which have an identifier: <kbd>&lt;ReplicaSet id&gt;-&lt;random id&gt;</kbd>.</p>
<div class="packt_infobox">Until Kubernetes version 1.8, <kbd>&lt;Deployment name&gt;-&lt;pod-template-hash value (number)&gt;</kbd> was used as a ReplicaSet identifier instead of a hex decimal hash.<br/>
<br/>
For more details, look at pull request: <a href="https://github.com/kubernetes/kubernetes/pull/51538">https://github.com/kubernetes/kubernetes/pull/51538</a>.</div>
<p>This diagram illustrates the <strong>Deployment</strong>, <strong>ReplicaSet</strong>, and <strong>Pod</strong> relationship:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-653 image-border" src="assets/48a785dd-5ce1-4a9b-8145-9d8aacdfd6e9.png" style="width:21.17em;height:16.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Relationship diagram for Deployments, ReplicaSets, and Pods</div>
<p>Because of this relationship, if you perform <kbd>delete</kbd> on a <kbd>my-nginx</kbd> Deployment object, it will also <span>attempt </span>to delete ReplicaSet and Pods respectively:</p>
<pre class="NormalPACKT">//delete my-nginx Deployment<br/>$ kubectl delete deploy my-nginx<br/>deployment.extensions "my-nginx" deleted<br/><br/><br/>//see status of ReplicaSet<br/>$ kubectl get rs<br/>No resources found.<br/> <br/><br/>//see status of Pod, it has been terminated<br/>$ kubectl get pods<br/>NAME                       READY     STATUS        RESTARTS   AGE<br/>my-nginx-5d69b5ff7-9mhbc   0/1       Terminating   0          2m<br/>my-nginx-5d69b5ff7-mt6z7   0/1       Terminating   0          2m<br/>my-nginx-5d69b5ff7-rdl2k   0/1       Terminating   0          2m</pre>
<p>This example is just a simple <kbd>create</kbd> and <kbd>delete</kbd>, that easy to understand Deployment object and ReplicaSet object 1:1 relationship at this moment. However, a Deployment object can manage many ReplicaSets to preserve as a history. So the actual relationship is 1:N, as in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-654 image-border" src="assets/48c720e2-16f1-473d-aa22-aad05204a347.png" style="width:29.67em;height:17.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Deployments maintain ReplicaSet history</div>
<p>To understand the 1:N relationship, let's recreate this Deployment object again and perform to make some changes to see how Deployment manages ReplicaSet history.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>You may run the <kbd>kubectl run</kbd> command to recreate <kbd>my-nginx</kbd>, or write a Deployments configuration file that produces the same result. This is a great opportunity to learn about the Deployment configuration file. </p>
<p>This example is an equivalent of <kbd>kubectl run my-nginx --image=nginx:1.11.0 --port=80 --replicas=3</kbd>:</p>
<pre class="NormalPACKT">$ cat deploy.yaml <br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: my-nginx<br/>spec:<br/>  replicas: 3<br/>  selector:<br/>    matchLabels:<br/>      run: my-nginx<br/>  template:<br/>    metadata:<br/>      labels:<br/>        run: my-nginx<br/>    spec:<br/>      containers:<br/>      - name: my-nginx<br/>        image: nginx:1.11.0<br/>        ports:<br/>        - containerPort: 80</pre>
<p>These parameters, sorted by key and value, are described here:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Key</strong></p>
</td>
<td>
<p><strong>Value</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>apiVersion</kbd></p>
</td>
<td>
<p><kbd>apps/v1</kbd></p>
</td>
<td>
<p>Until Kubernetes v1.8, it had been used apps/v1Beta1, v1.8 used apps/v1Beta2, then v1.9 or later use apps/v1</p>
</td>
</tr>
<tr>
<td>
<p><kbd>kind</kbd></p>
</td>
<td>
<p><kbd>deployment</kbd></p>
</td>
<td>
<p>Indicates that this is a set of Deployment configurations</p>
</td>
</tr>
<tr>
<td>
<p><kbd>metadata.name</kbd></p>
</td>
<td>
<p><kbd>my-nginx</kbd></p>
</td>
<td>
<p>Name of Deployment</p>
</td>
</tr>
<tr>
<td>
<p><kbd>spec.replicas</kbd></p>
</td>
<td>
<p><kbd>3</kbd></p>
</td>
<td>
<p>Desire to have three Pods</p>
</td>
</tr>
<tr>
<td>
<p><kbd>spec.selector.matchLabels</kbd></p>
</td>
<td>
<p><kbd>run:my-nginx</kbd></p>
</td>
<td>
<p>Control ReplicaSet/Pods which have this label</p>
</td>
</tr>
<tr>
<td>
<p><kbd>spec.template.metadata.labels</kbd></p>
</td>
<td>
<p><kbd>run:my-nginx</kbd></p>
</td>
<td>
<p>Assigns this label when creating a ReplicaSet/Pod; it must match <kbd>spec.selector.matchLabels</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>spec.template.spec.containers</kbd></p>
</td>
<td>
<p>name: <kbd>my-nginx</kbd></p>
<p>image: <kbd>nginx:1.11.0</kbd></p>
<p>port:</p>
<p><kbd>- containerPort:80</kbd></p>
</td>
<td>
<p>ReplicaSet creates and manages Pods which have:</p>
<ul>
<li>name as <kbd>my-nginx</kbd></li>
<li>Container image as nginx version 1.11.0</li>
<li>Publish port number <kbd>80</kbd></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>If you use this YAML file to create a Deployment, use the <kbd>kubectl create</kbd> command instead of <kbd>kubectl run</kbd>.</p>
<p>Note that, this time, you should also specify <kbd>--save-config</kbd>, which allows you to update the resource using the <kbd>kubectl apply</kbd> command in the future. In addition, specify <kbd>--record</kbd> which can store the command line history. Those two options are not mandatory to manage ReplicaSet history but help you to preserve better information:</p>
<pre class="NormalPACKT">//use -f to specify YAML file<br/>$ kubectl create -f deploy.yaml --save-config --record<br/>deployment.apps "my-nginx" created <br/><br/><br/>//check my-nginx Deployment<br/>$ kubectl get deploy<br/>NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE<br/>my-nginx   3         3         3            3           5s<br/><br/><br/>$ kubectl describe deploy my-nginx<br/><span>Name: <span class="Apple-converted-space">                  </span>my-nginx<br/></span><span>Namespace:<span class="Apple-converted-space">              </span>default<br/></span><span>CreationTimestamp:<span class="Apple-converted-space">      </span>Wed, 09 May 2018 03:40:09 +0000<br/></span><span>Labels: <span class="Apple-converted-space">                </span>&lt;none&gt;<br/></span><span>Annotations:<span class="Apple-converted-space">            </span>deployment.kubernetes.io/revision=1<br/></span><span><span class="Apple-converted-space">                        </span>kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"my-nginx","namespace":"default"},"spec":{"replicas":3,"selector":{"mat...<br/></span><span><span class="Apple-converted-space">                        </span>kubernetes.io/change-cause=kubectl create --filename=deploy.yaml --save-config=true --record=true<br/></span><span>Selector: <span class="Apple-converted-space">              </span>run=my-nginx<br/></span><span>Replicas: <span class="Apple-converted-space">              </span>3 desired | 3 updated | 3 total | 3 available | 0 unavailable<br/></span><span>StrategyType: <span class="Apple-converted-space">          </span>RollingUpdate<br/></span><span>MinReadySeconds:<span class="Apple-converted-space">        </span>0<br/></span><span>RollingUpdateStrategy:<span class="Apple-converted-space">  </span>25% max unavailable, 25% max surge<br/></span><span>Pod Template:<br/></span><span><span class="Apple-converted-space">  </span>Labels:<span class="Apple-converted-space">  </span>run=my-nginx<br/></span><span><span class="Apple-converted-space">  </span>Containers:<br/></span><span><span class="Apple-converted-space">   </span>my-nginx:<br/></span><span><span class="Apple-converted-space">    </span>Image:<span class="Apple-converted-space">        </span>nginx:1.11.0<br/></span><span><span class="Apple-converted-space">    </span>Port: <span class="Apple-converted-space">        </span>80/TCP<br/></span><span><span class="Apple-converted-space">    </span>Host Port:<span class="Apple-converted-space">    </span>0/TCP<br/></span><span><span class="Apple-converted-space">    </span>Environment:<span class="Apple-converted-space">  </span>&lt;none&gt;<br/></span><span><span class="Apple-converted-space">    </span>Mounts: <span class="Apple-converted-space">      </span>&lt;none&gt;<br/></span><span><span class="Apple-converted-space">  </span>Volumes:<span class="Apple-converted-space">        </span>&lt;none&gt;<br/></span><span>Conditions:<br/></span><span><span class="Apple-converted-space">  </span>Type <span class="Apple-converted-space">          </span>Status<span class="Apple-converted-space">  </span>Reason<br/></span><span><span class="Apple-converted-space">  </span>---- <span class="Apple-converted-space">          </span>------<span class="Apple-converted-space">  </span>------<br/></span><span><span class="Apple-converted-space">  </span>Available<span class="Apple-converted-space">      </span>True<span class="Apple-converted-space">    </span>MinimumReplicasAvailable<br/></span><span><span class="Apple-converted-space">  </span>Progressing<span class="Apple-converted-space">    </span>True<span class="Apple-converted-space">    </span>NewReplicaSetAvailable<br/></span><span>OldReplicaSets:<span class="Apple-converted-space">  </span>&lt;none&gt;<br/></span><strong><span>NewReplicaSet: <span class="Apple-converted-space">  </span>my-nginx-54bb7bbcf9 (3/3 replicas created)<br/></span></strong><span>Events:<br/></span><span><span class="Apple-converted-space">  </span>Type<span class="Apple-converted-space">    </span>Reason <span class="Apple-converted-space">            </span>Age <span class="Apple-converted-space">  </span>From <span class="Apple-converted-space">                  </span>Message<br/></span><span><span class="Apple-converted-space">  </span>----<span class="Apple-converted-space">    </span>------ <span class="Apple-converted-space">            </span>----<span class="Apple-converted-space">  </span>---- <span class="Apple-converted-space">                  </span>-------<br/></span><span><span class="Apple-converted-space">  </span>Normal<span class="Apple-converted-space">  </span>ScalingReplicaSet<span class="Apple-converted-space">  </span>34s <span class="Apple-converted-space">  </span>deployment-controller<span class="Apple-converted-space">  </span>Scaled up replica set my-nginx-54bb7bbcf9 to 3</span></pre>
<p>You can see a property <kbd>OldReplicaSets</kbd> and  <kbd>NewReplicaSet</kbd> in the preceding code, which are some association between Deployment and ReplicaSet.</p>
<p>Whenever you update a definition of a container template, for example, changing the nginx image version from 1.11.0 to 1.12.0, then Deployment <kbd>my-nginx</kbd> will create a new ReplicaSet. Then the property <kbd>NewReplicaSet</kbd> will point to the new ReplicaSet which has nginx version 1.12.0.</p>
<p>On the other hand, the <kbd>OldReplicaSets</kbd> property points to an old ReplicaSet which has nginx version 1.11.0 until new ReplicaSet is complete to setup new Pod.</p>
<p>These old/new ReplicaSet associations between Deployment, Kubernetes administrator can easy to achieve rollback operation in case new ReplicaSet has any issues.</p>
<p>In addition, Deployment can keep preserves the history of ReplicaSet which were associated with it before. Therefore, Deployment can anytime to change back (rollback) to any point of older ReplicaSet.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>As mentioned earlier, let's bump the nginx image version from 1.11.0 to 1.12.0. There are two ways to change the container image: use the <kbd>kubectl set</kbd> command, or update YAML then use the <kbd>kubectl apply</kbd> command.</p>
<p>Using the <kbd>kubectl set</kbd> command is quicker and there is better visibility when using the <kbd>--record</kbd> option.</p>
<p>On the other hand, updating YAML and using the <kbd>kubectl apply</kbd> command is better to preserve the entire Deployment YAML configuration file, which is better when using a version control system such as <kbd>git</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using kubectl set to update the container image</h1>
                </header>
            
            <article>
                
<p>Use the <kbd>kubectl set</kbd> command allows us to overwrite the <span><kbd>spec.template.spec.containers[].image</kbd> property that is similar to using the <kbd>kubectl run</kbd> command to specify the image file. The following example specifies</span> <kbd>my-nginx</kbd> <span>deployment to set the container <kbd>my-nginx</kbd> to change the image to nginx version 1.12.0:</span></p>
<pre>$ kubectl set image deployment my-nginx my-nginx=nginx:1.12.0 --record<br/>deployment.apps "my-nginx" image updated<br/><br/><br/>$ kubectl describe deploy my-nginx<br/>Name:                   my-nginx<br/>…<br/>…<br/><span>Conditions:<br/></span><span><span class="Apple-converted-space">  </span>Type <span class="Apple-converted-space">          </span>Status<span class="Apple-converted-space">  </span>Reason<br/></span><span><span class="Apple-converted-space">  </span>---- <span class="Apple-converted-space">          </span>------<span class="Apple-converted-space">  </span>------<br/></span><span><span class="Apple-converted-space">  </span>Available<span class="Apple-converted-space">      </span>True<span class="Apple-converted-space">    </span>MinimumReplicasAvailable<br/></span><span><span class="Apple-converted-space">  </span>Progressing<span class="Apple-converted-space">    </span>True<span class="Apple-converted-space">    </span>ReplicaSetUpdated<br/></span><strong><span>OldReplicaSets:<span class="Apple-converted-space">  </span>my-nginx-54bb7bbcf9 (3/3 replicas created)<br/></span><span>NewReplicaSet: <span class="Apple-converted-space">  </span>my-nginx-77769b7666 (1/1 replicas created)<br/></span></strong><span>Events:<br/></span><span><span class="Apple-converted-space">  </span>Type<span class="Apple-converted-space">    </span>Reason <span class="Apple-converted-space">            </span>Age <span class="Apple-converted-space">  </span>From <span class="Apple-converted-space">                  </span>Message<br/></span><span><span class="Apple-converted-space">  </span>----<span class="Apple-converted-space">    </span>------ <span class="Apple-converted-space">            </span>----<span class="Apple-converted-space">  </span>---- <span class="Apple-converted-space">                  </span>-------<br/></span><span><span class="Apple-converted-space">  </span>Normal<span class="Apple-converted-space">  </span>ScalingReplicaSet<span class="Apple-converted-space">  </span>27s <span class="Apple-converted-space">  </span>deployment-controller<span class="Apple-converted-space">  </span>Scaled up replica set my-nginx-54bb7bbcf9 to 3<br/></span><span><span class="Apple-converted-space">  </span>Normal<span class="Apple-converted-space">  </span>ScalingReplicaSet<span class="Apple-converted-space">  </span>2s<span class="Apple-converted-space">    </span>deployment-controller<span class="Apple-converted-space">  </span>Scaled up replica set my-nginx-77769b7666 to 1</span></pre>
<p>As you can see, <kbd>OldReplicaSets</kbd> becomes the previous <kbd>ReplicaSet</kbd> (<kbd>my-nginx-54bb7bbcf9</kbd>) and <kbd>NewReplicaSet</kbd> becomes <kbd>my-nginx-77769b7666</kbd>. Note that you can see the <kbd>OldReplicaSets</kbd> property until <kbd>NewReplicaSet</kbd> is ready, so once the new <kbd>ReplicaSet</kbd> is successfully launched, <kbd>OldReplicaSet</kbd> becomes <kbd>&lt;none&gt;</kbd>, as follows:</p>
<pre>$ kubectl describe deploy my-nginx<br/>Name:                   my-nginx<br/>…<br/>…<br/>  Type           Status  Reason<br/>  ----           ------  ------<br/>  Available      True    MinimumReplicasAvailable<br/>  Progressing    True    NewReplicaSetAvailable<br/><strong><span>OldReplicaSets:<span class="Apple-converted-space">  </span>&lt;none&gt;<br/></span><span>NewReplicaSet: <span class="Apple-converted-space">  </span>my-nginx-77769b7666 (3/3 replicas created)</span></strong></pre>
<p>If you can see the <kbd>ReplicaSet</kbd> list by <kbd>kubectl get rs</kbd>, you can see two ReplicaSet, as follows:</p>
<pre>$ kubectl get rs<br/><span>NAME                  DESIRED   CURRENT   READY     AGE<br/></span><span>my-nginx-54bb7bbcf9 <span class="Apple-converted-space">  </span>0 <span class="Apple-converted-space">        </span>0 <span class="Apple-converted-space">        </span>0 <span class="Apple-converted-space">        </span>3m<br/></span><span>my-nginx-77769b7666 <span class="Apple-converted-space">  </span>3 <span class="Apple-converted-space">        </span>3 <span class="Apple-converted-space">        </span>3 <span class="Apple-converted-space">        </span>3m</span></pre>
<p>As you can see, in the old <kbd>ReplicaSet</kbd> (<kbd>my-nginx-54bb7bbcf9</kbd>), the numbers of <kbd>DESIRED/CURRENT/READY</kbd> pods are all zero.</p>
<p>In addition, because the preceding example uses the <kbd>--record</kbd> option, you can see the history of the Deployment <kbd>my-nginx</kbd> rollout with the <kbd>kubectl rollout history</kbd> command, as follows:</p>
<pre>$ kubectl rollout history deployment my-nginx<br/>deployments "my-nginx"<br/>REVISION  CHANGE-CAUSE<br/>1         kubectl create --filename=deploy.yaml --save-config=true --record=true<br/>2         kubectl set image deployment/my-nginx my-nginx=nginx:1.12.0 --record=true</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating the YAML and using kubectl apply</h1>
                </header>
            
            <article>
                
<p>For demo purposes, copy <kbd>deploy.yaml</kbd> to <kbd>deploy_1.12.2.yaml</kbd> and change the <kbd>nginx</kbd> version to <kbd>1.12.2</kbd>, as follows:</p>
<pre>        image: nginx:1.12.2</pre>
<p>Then run the <kbd>kubectl apply</kbd> command with the <kbd>--record</kbd> option:</p>
<pre>$ kubectl apply -f deploy_1.12.2.yaml --record<br/>deployment.apps "my-nginx" configured</pre>
<p>This will perform the same thing as the <kbd>kubectl set</kbd> image command, so you can see that the nginx image version has been bumped up to <kbd>1.12.2</kbd>; also, the <kbd>OldReplicaSets</kbd>/<kbd>NewReplicaSet</kbd> combination has been changed as follows:</p>
<pre>$ kubectl describe deploy my-nginx<br/>Name:                   my-nginx<br/>…<br/>…<br/>Pod Template:<br/>  Labels: run=my-nginx<br/>  Containers:<br/>   my-nginx:<br/>    <strong>Image: nginx:1.12.2</strong><br/>...<br/>...<br/>Conditions:<br/>  Type           Status  Reason<br/>  ----           ------  ------<br/>  Available      True    MinimumReplicasAvailable<br/>  Progressing    True    ReplicaSetUpdated<br/><strong>OldReplicaSets: my-nginx-77769b7666 (3/3 replicas created)</strong><br/><strong>NewReplicaSet: my-nginx-69fbc98fd4 (1/1 replicas created)</strong></pre>
<p>After a few moments, <kbd>NewReplicaSet</kbd> will be ready. Then there will be a total of three <kbd>ReplicaSets</kbd> existing on your system:</p>
<pre>$ kubectl get rs<br/><span>NAME                  DESIRED   CURRENT   READY     AGE<br/></span><span>my-nginx-54bb7bbcf9 <span class="Apple-converted-space">  </span>0 <span class="Apple-converted-space">        </span>0 <span class="Apple-converted-space">        </span>0 <span class="Apple-converted-space">        </span>7m<br/></span><span>my-nginx-69fbc98fd4 <span class="Apple-converted-space">  </span>3 <span class="Apple-converted-space">        </span>3 <span class="Apple-converted-space">        </span>3 <span class="Apple-converted-space">        </span>1m<br/></span><span>my-nginx-77769b7666 <span class="Apple-converted-space">  </span>0 <span class="Apple-converted-space">        </span>0 <span class="Apple-converted-space">        </span>0 <span class="Apple-converted-space">        </span>6m</span></pre>
<p>You can also see the rollout history:</p>
<pre>$ kubectl rollout history deployment my-nginx<br/>deployments "my-nginx"<br/>REVISION  CHANGE-CAUSE<br/>1         kubectl create --filename=deploy.yaml --save-config=true --record=true<br/>2         kubectl set image deployment/my-nginx my-nginx=nginx:1.12.0 --record=true<br/>3         kubectl apply --filename=deploy_1.12.2.yaml --record=true</pre>
<p><span>Whenever you want to revert to a previous <kbd>ReplicaSet</kbd>, which means rolling back to the previous nginx version, you can use</span> <kbd>kubectl rollout undo</kbd> <span>with the</span> <kbd>--to-revision</kbd> <span>option. For example, if you want to </span>roll back to revision 2 in your history (<kbd>kubectl set image deployment/my-nginx my-nginx=nginx:1.12.0 --record=true</kbd>), specify <kbd>--to-revision=2</kbd>:</p>
<pre>$ kubectl rollout undo deployment my-nginx --to-revision=2<br/>deployment.apps "my-nginx" rolled back'</pre>
<p>A few moments later, Deployment will deactivate the current <kbd>ReplicaSet</kbd>, which uses the Pod template with <kbd>nginx</kbd> version <kbd>1.12.2</kbd>, and will then activate the <kbd>ReplicaSet</kbd> which uses <kbd>nginx</kbd> version <kbd>1.12</kbd>, as follows:</p>
<pre><span>$ kubectl get rs<br/></span><span>NAME                  DESIRED   CURRENT   READY     AGE<br/></span><span>my-nginx-54bb7bbcf9 <span class="Apple-converted-space">  </span>0 <span class="Apple-converted-space">        </span>0 <span class="Apple-converted-space">        </span>0 <span class="Apple-converted-space">        </span>8m<br/></span><span>my-nginx-69fbc98fd4 <span class="Apple-converted-space">  </span>0 <span class="Apple-converted-space">        </span>0 <span class="Apple-converted-space">        </span>0 <span class="Apple-converted-space">        </span>2m<br/></span><span>my-nginx-77769b7666 <span class="Apple-converted-space">  </span>3 <span class="Apple-converted-space">        </span>3 <span class="Apple-converted-space">        </span>3 <span class="Apple-converted-space">        </span>7m</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>In this section, you learned about the concept of Deployment. It is an important core feature in Kubernetes ReplicaSet life cycle management. It allows us to achieve rollout and rollback functionalities, and can integrate to CI/CD. In the following chapter you will see detailed operations of rollout and rollback:</p>
<ul>
<li><em>Updating live containers</em> <span>section </span>in <a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml">Chapter 3</a>, <em>Playing with Containers</em></li>
<li>
<p><span><em>Setting up a continuous delivery pipeline </em></span>section <span>in <a href="669edaf0-c274-48fa-81d8-61150fa36df5.xhtml">Chapter 5</a>, <em>Building Continuous Delivery Pipelines</em> </span></p>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with Services</h1>
                </header>
            
            <article>
                
<p>The network service is an application that receives requests and provides a solution. Clients access the service by a network connection. They don't have to know the architecture of the service or how it runs. The only thing that clients have to verify is whether the endpoint of the service can be accessed, and then follow its usage policy to get the response of the server. The Kubernetes Service has similar ideas. It is not necessary to understand every Pod before reaching their functionalities. For components outside the Kubernetes system, they just access the Kubernetes Service with an exposed network port to communicate with running Pods. It is not necessary to be aware of the containers' IPs and ports. Behind Kubernetes Services, we can fulfill a zero-downtime update for our container programs without struggling:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-674 image-border" src="assets/feaa67cf-b59c-43b1-87f2-5e0647c966ed.png" style="width:75.25em;height:47.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Kubernetes Service-covered Pods by labels of Pods and their selectors</div>
<p>The preceding diagram shows the basic structure of the <strong>Service</strong> and realizes the following concepts:</p>
<ul>
<li>As with the <strong>D</strong><strong>eployment</strong>, the <strong>Service</strong> directs requests to Pods that have labels containing the Service's selector. In other words, the Pods selected by the <strong>Service</strong> are based on their labels.</li>
<li>The load of requests sent to the Services will distribute to three Pods.</li>
<li>The <strong>Deployment</strong>, along with ReplicaSet, ensures that the number of running Pods meets its desired state. It monitors the Pods for the <strong>Service</strong>, making sure they will be healthy for taking over duties from the <strong>Service</strong>.</li>
<li><strong>Service</strong> is an abstraction layer for grouping Pods, which allows for Pods scaling across nodes.</li>
</ul>
<p>In this recipe, you will learn how to create Services in front of your Pods for the requests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Prior to applying Kubernetes Services, it is important to verify whether all nodes in the system are running <kbd>kube-proxy</kbd>. The daemon <kbd>kube-proxy</kbd> works as a network proxy in a node. It helps to reflect Service settings, such as IPs or ports on each node, and to do network forwarding. To check if <kbd>kube-proxy</kbd> is running or not, we take a look at network connections:</p>
<pre>// check by command netstat with proper tags for showing the information we need, t:tcp, u:udp, l:listening, p:program, n:numeric address<br/>// use root privilege for grabbing all processes<br/>$ sudo netstat -tulpn | grep kube-proxy<br/>tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      2326/kube-proxy<br/>tcp6       0      0 :::31723                :::*                    LISTEN      2326/kube-proxy<br/>tcp6       0      0 :::10256                :::*                    LISTEN      2326/kube-proxy</pre>
<p>Once you see the output, the process ID <kbd>2326</kbd>, <kbd>kube-proxy</kbd>, listening on port <kbd>10249</kbd> on localhost, the node is ready for Kubernetes Services. Go ahead and verify whether all of your nodes in the Kubernetes cluster having <kbd>kube-proxy</kbd> running on them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>As mentioned in the previous section, the Kubernetes Service exposes Pods by selecting them through corresponding labels. However, there is another configuration we have to take care of: the network port. As the following diagram indicates, the Service and Pod have their own key-value pair labels and ports:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-226 image-border" src="assets/28ed7767-9dc7-4b82-9de3-3a6f99d969d0.png" style="width:16.58em;height:28.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Network port mapping between Service and Pod</div>
<p>Therefore, setting the selector of Service and binding the service exposed port to the container port are required to be carried out while creating Services. If either of them fail to be set properly, clients won't get responses or will get connection-refused errors.</p>
<p>We can define and create a new Kubernetes Service through the CLI or a configuration file. Here, we are going to explain how to deploy the Services by command. The subcommands <kbd>expose</kbd> and <kbd>describe</kbd> are utilized in the following commands for various scenarios. For file-format creation, it is recommended to read the <em>Working with configuration files</em> recipe in <a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml">Chapter 3</a>, <em>Playing with Containers</em>, for a detailed discussion.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Service for different resources</h1>
                </header>
            
            <article>
                
<p>You can attach a Service to a Pod, a Deployment, an endpoint outside the Kubernetes system, or even another Service. We will show you these, one by one, in this section. The creation of the Kubernetes Service looks similar to these command formats: <kbd>kubectl expose $RESOURCE_TYPE $RESOURCE_NAME [OTHER TAGS]</kbd> or <kbd>kubectl expose -f $CONFIG_FILE</kbd>. The resource types (Pod, Deployment, and Service) are supported by the subcommand <kbd>expose</kbd>. So is the configuration file, which follows the limitation <span>type</span>. Accordingly, for a later demonstration we will attach the newly created Service to the endpoint by the configuration file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Service for a Pod</h1>
                </header>
            
            <article>
                
<p>Kubernetes Pods covered by Service require labels, so that Service can recognize who is the one it should take charge of. In the following commands, we create a Pod with labels first, and attach a Service on it:</p>
<pre>// using subcommand "run" with "never" restart policy, and without replica, you can get a Pod<br/>// here we create a nginx container with port 80 exposed to outside world of Pod<br/>$ kubectl run nginx-pod --image=nginx --port=80 --restart="Never" --labels="project=My-Happy-Web,role=frontend,env=test"<br/>pod "nginx-pod" created<br/><br/>// expose Pod "nginx-pod" with a Service officially with port 8080, target port would be the exposed port of pod<br/>$ kubectl expose pod nginx-pod --port=8080 --target-port=80 --name="nginx-service"<br/>service "nginx-service" exposed</pre>
<p>You may find that, based on the preceding command, we did not assign any selector to this Service. Nonetheless, since Service <kbd>nginx-service</kbd> takes the port forwarding task of Pod <kbd>nginx-pod</kbd>, it will take the labels of the Pod as its selector. Go ahead and check the details of the Service with the subcommand <kbd>describe</kbd>:</p>
<pre>// "svc" is the abbreviate of Service, for the description's resource type<br/>$ kubectl describe svc nginx-service<br/>Name:              nginx-service<br/>Namespace:         default<br/>Labels:            env=test<br/>                   project=My-Happy-Web<br/>                   role=frontend<br/>Annotations:       &lt;none&gt;<br/>Selector:          env=test,project=My-Happy-Web,role=frontend<br/>Type:              ClusterIP<br/>IP:                10.96.107.213<br/>Port:              &lt;unset&gt;  8080/TCP<br/>TargetPort:        80/TCP<br/>Endpoints:         192.168.79.24:80<br/>Session Affinity:  None<br/>Events:            &lt;none&gt;</pre>
<p>Now you can see that, for guaranteeing the responsibility, this successfully exposed Service just copied the labels of the Pod as its selector. The value list after <kbd>Endpoints</kbd> was the IP of the Pod and its exposed port <kbd>80</kbd>. Furthermore, the Service took the Pod's labels as its own. According to this example, the Pod can be accessed through Service by surfing <kbd>10.96.107.213:8080</kbd>.</p>
<p>Except for the selector of Service, some parameters can be automatically configured if they are bypassed by users. One parameter is the labels of the Pod; another is the name of the Service; and the other is the exposed port of the Service. Let's take a look at how this simple set of Pod and Service can be managed:</p>
<pre>// create a Pod and a Service for it<br/>$ kubectl run nginx-no-label --image=nginx --port=80 --restart="Never" &amp;&amp; kubectl expose pod nginx-no-label<br/>pod "nginx-no-label" created<br/>service "nginx-no-label" exposed<br/>// take a lookat the configurations of the Service<br/>$ kubectl describe svc nginx-no-label<br/>Name:              nginx-no-label<br/>Namespace:         default<br/>Labels:            run=nginx-no-label<br/>Annotations:       &lt;none&gt;<br/>Selector:          <strong>run=nginx-no-label<br/></strong>Type:              ClusterIP<br/>IP:                10.105.96.243<br/>Port:              &lt;unset&gt;  80/TCP<br/>TargetPort:        80/TCP<br/>Endpoints:         192.168.79.10:80<br/>Session Affinity:  None<br/>Events:            &lt;none&gt;</pre>
<p>Here, we can see that the Service inherited the name, label, and port from the Pod. The selector was assigned the dummy label with the key named <kbd>run</kbd> and the value named as Pod's name, which is just the same dummy one of Pod <kbd>nginx-no-label</kbd>. Users should access the Service through port <kbd>80</kbd>, as well. For such simple settings, you can alternatively try the following command to create the Pods and Service at the same time:</p>
<pre>// through leveraging tag "--expose", create the Service along with Pod<br/>$ kubectl run another-nginx-no-label --image=nginx --port=80 --restart="Never" --expose<br/>service "another-nginx-no-label" created<br/>pod "another-nginx-no-label" created</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Service for a Deployment with an external IP</h1>
                </header>
            
            <article>
                
<p>Kubernetes Deployment is the ideal resource type for a Service. For Pods supervised by the ReplicaSet and Deployment, the Kubernetes system has a controller manager to look over the their life cycles. It is also helpful for updating the version or state of the program by binding the existing Services to another Deployment. For the following commands, we create a Deployment first, and attach a Service with an external IP:</p>
<pre>// using subcommand "run" and assign 2 replicas<br/>$ kubectl run nginx-deployment --image=nginx --port=80 --replicas=2 --labels="env=dev,project=My-Happy-Web,role=frontend"<br/>deployment.apps "nginx-deployment" created<br/>// explicitly indicate the selector of Service by tag "--selector", and assign the Service an external IP by tag "--external-ip"<br/>// the IP 192.168.122.102 demonstrated here is the IP of one of the Kubernetes node in system<br/>$ kubectl expose deployment nginx-deployment --port=8080 --target-port=80 --name="another-nginx-service" --selector="project=My-Happy-Web,role=frontend" --external-ip="192.168.122.102"<br/>service "another-nginx-service" exposed</pre>
<p>Let's go ahead and check the details of the newly created Service, <kbd>another-nginx-service</kbd>:</p>
<pre>$ kubectl describe svc another-nginx-service<br/>Name:              another-nginx-service<br/>Namespace:         default<br/>Labels:            env=dev<br/>                   project=My-Happy-Web<br/>                   role=frontend<br/>Annotations:       &lt;none&gt;<br/>Selector:          project=My-Happy-Web,role=frontend<br/>Type:              ClusterIP<br/>IP:                10.100.109.230<br/>External IPs:      192.168.122.102<br/>Port:              &lt;unset&gt;  8080/TCP<br/>TargetPort:        80/TCP<br/>Endpoints:         192.168.79.15:80,192.168.79.21:80,192.168.79.24:80<br/>Session Affinity:  None<br/>Events:            &lt;none&gt;</pre>
<p>Apart from the Service IP (in the case of the preceding command, <kbd>10.100.109.230</kbd>), which can be accessed within the Kubernetes system, the Service can now be connected through an external one (<kbd>192.168.122.102</kbd>, for example) beyond the Kubernetes system. While the Kubernetes master is able to communicate with every node, in this case, we can fire a request to the Service such as the following command:</p>
<pre>$ curl 192.168.122.102:8080<br/>&lt;!DOCTYPE html&gt;<br/>&lt;html&gt;<br/>&lt;head&gt;<br/>&lt;title&gt;Welcome to nginx!&lt;/title&gt;<br/>...</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Service for an Endpoint without a selector</h1>
                </header>
            
            <article>
                
<p>First, we are going to create an Endpoint directing the external service. A Kubernetes Endpoint is an abstraction, making components beyond Kubernetes <span>(for instance, a database in other system)</span> become a part of Kubernetes resources. It provides a feasible use case for a hybrid environment. To create an endpoint, an IP address, along with a port, is required. Please take a look at the following template:</p>
<pre>$ cat k8s-endpoint.yaml<br/>apiVersion: v1<br/>kind: Endpoints<br/>metadata:<br/>  name: k8s-ep<br/>subsets:<br/>  - addresses:<br/>      - hostname: kubernetes-io<br/>        ip: 45.54.44.100<br/>    ports:<br/>      - port: 80</pre>
<p>The template defines an Endpoint named <kbd>k8s-ep</kbd>, which points to the IP of the host of the official Kubernetes website (<a href="https://kubernetes.io">https://kubernetes.io</a>). Never mind that this Endpoint forwards to a plain HTML; we just take this Endpoint as an example. As mentioned, Endpoint is not a resource supported by the Kubernetes API for exposing:</p>
<pre>// Give it a try!<br/>$ kubectl expose -f k8s-endpoint.yaml<br/>error: cannot expose a { Endpoints}</pre>
<p>In Kubernetes, an Endpoint not only represents an external service; an internal Kubernetes Service is also a Kubernetes Endpoint. You can check Endpoint resources with the command <kbd>kubectl get endpoints</kbd>. You will find that there is not a single endpoint <kbd>k8s-ep</kbd> (which you just created), but many endpoints named the same as the Services in previous pages. When a Service is created with a selector and exposes certain resources (such as a Pod, Deployment, or other Service), a corresponding Endpoint with the same name is created at the same time.</p>
<p>Therefore, we still can create a Service associated with the Endpoint using an identical name, as in the following template:</p>
<pre>$ cat endpoint-service.yaml<br/>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: k8s-ep<br/>spec:<br/>  ports:<br/>    - protocol: TCP<br/>      port: 8080<br/>      targetPort: 80</pre>
<p>The relationship between the Endpoints and the Service is built up with the resource name. For the Service <kbd>k8s-ep</kbd>, we didn't indicate the selector, since it did not actually take any Pod in responsibility:</p>
<pre>// go create the Service and the endpoint<br/>$ kubectl create -f endpoint-service.yaml &amp;&amp; kubectl create -f k8s-endpoint.yaml<br/>service "k8s-ep" created<br/>endpoints "k8s-ep" created<br/>// verify the Service k8s-ep<br/>$ kubectl describe svc k8s-ep<br/>Name:              k8s-ep<br/>Namespace:         default<br/>Labels:            &lt;none&gt;<br/>Annotations:       &lt;none&gt;<br/>Selector:          &lt;none&gt;<br/>Type:              ClusterIP<br/>IP:                10.105.232.226<br/>Port:              &lt;unset&gt;  8080/TCP<br/>TargetPort:        80/TCP<br/>Endpoints:         45.54.44.100:80<br/>Session Affinity:  None<br/>Events:            &lt;none&gt;</pre>
<p>Now you can see that the endpoint of the Service is just the one defined in <kbd>k8s-endpoint.yaml</kbd>. It is good for us to access the outside world through the Kubernetes Service! In the case earlier, we can verify the result with the following command:</p>
<pre>$ curl 10.105.232.226:8080</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Service for another Service with session affinity</h1>
                </header>
            
            <article>
                
<p>While building a Service over another, we may think of multiple layers for port forwarding. In spite of redirecting traffic from one port to another, the action of exposing a Service is actually copying the setting of one Service to another. This scenario could be utilized as updating the Service setting, without causing headaches to current clients and servers:</p>
<pre>// create a Service by expose an existed one<br/>// take the one we created for Deployment for example<br/>$ kubectl expose svc another-nginx-service --port=8081 --target-port=80 --name=yet-another-nginx-service --session-affinity="ClientIP"<br/>service "yet-another-nginx-service" exposed<br/>// check the newly created Service<br/>$ kubectl describe svc yet-another-nginx-service<br/>Name:              yet-another-nginx-service<br/>Namespace:         default<br/>Labels:            env=dev<br/>                   project=My-Happy-Web<br/>                   role=frontend<br/>Annotations:       &lt;none&gt;<br/>Selector:          project=My-Happy-Web,role=frontend<br/>Type:              ClusterIP<br/>IP:                10.110.218.136<br/>Port:              &lt;unset&gt;  8081/TCP<br/>TargetPort:        80/TCP<br/>Endpoints:         192.168.79.15:80,192.168.79.21:80,192.168.79.24:80<br/>Session Affinity:  ClientIP<br/>Events:            &lt;none&gt;</pre>
<p>Here we are! We successfully exposed another Service with similar settings to the Service <kbd>another-nginx-service</kbd>. The commands and output can be summarized as follows:</p>
<ul>
<li><strong>A new Service name is required</strong>: Although we can copy the configurations from another Service, the name of the resource type should always be unique. When exposing a Service without the tag <kbd>--name</kbd>, you will get the error message: <kbd>Error from server (AlreadyExists): services "another-nginx-service" already exists</kbd>.</li>
<li><strong>Adding or updating the configuration is workable</strong>: We are able to add a new configuration, like adding session affinity; or we can update the port of the Service, like here, where we change to open port <kbd>8081</kbd> instead of <kbd>8080</kbd>.</li>
<li><strong>Avoid changing target port</strong>: Because the target port is along with the IP of the Pods, once the Service exposing changes the target port, the newly copied Service cannot forward traffic to the same endpoints. In the preceding example, since the new target port is defined, we should point out the container port again. It prevented the new Service from using the target port as the container port and turned out a misleading transaction.</li>
</ul>
<p>With session affinity, the list of description tags session affinity as <kbd>ClientIP</kbd>. For the current Kubernetes version, the client IP is the only option for session affinity. It takes the action as a hash function: with the same IP address, the request will always send to the identical Pod. However, this could be a problem if there is a load balancer or ingress controller in front of the Kubernetes Service: the requests would be considered to come from the same source, and the traffic forwarded to a single Pod. Users have to handle this issue on their own, for example, by building an HA proxy server instead of using the Kubernetes Service.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deleting a Service</h1>
                </header>
            
            <article>
                
<p>If you go through every command in this section, there are definitely some demonstrated Kubernetes Services (we counted six of them) that should be removed. To delete a Service, the same as with any other Kubernetes resource, you can remove the Service with the name or the configuration file through the subcommand <kbd>delete</kbd>. When you try to remove the Service and the Endpoint at the same time, the following situation will happen:</p>
<pre>// the resource abbreviation of endpoint is "ep", separate different resource types by comma<br/>$ kubectl delete svc,ep k8s-ep<br/>service "k8s-ep" deleted<br/>Error from server (NotFound): endpoints "k8s-ep" not found</pre>
<p>This is because a Service is also a Kubernetes Endpoint. That's why, although we created the Service and the endpoint separately, once they are considered to work as a unit, the Endpoint is going to be removed when the Service is removed. Thus, the error message expresses that there is no endpoint called <kbd>k8s-ep</kbd>, since it was already removed with the Service deletion.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>On the network protocol stack, the Kubernetes Service relies on the transport layer, working together with the <strong>overlay network</strong> and <kbd>kube-proxy</kbd>. The overlay network of Kubernetes builds up a cluster network by allocating a subnet lease out of a pre-configured address space and storing the network configuration in <kbd>etcd</kbd>; on the other hand, <kbd>kube-proxy</kbd> helps to forward traffic from the endpoints of Services to the Pods through <kbd>iptables</kbd> settings.</p>
<div class="packt_infobox"><strong>Proxy-mode and Service </strong><kbd>kube-proxy</kbd>  currently has three modes with different implementation methods: <kbd>userspace</kbd>, <kbd>iptables</kbd>, and <kbd>ipvs</kbd>. The modes affect how the requests of clients reach to certain Pods through the Kubernete Service:
<ul>
<li><kbd>userspace</kbd>: <kbd>kube-proxy</kbd> opens a random port, called a proxy port, for each Service on the local node, then updates the <kbd>iptables</kbd> rules, which capture any request sent to the Service and forward it to the proxy port. In the end, any message sent to the proxy port will be passed to the Pods covered by the Service. It is less efficient, since the traffic is required to go to <kbd>kube-proxy</kbd> for routing to the Pod.</li>
<li><kbd>iptables</kbd><span><span>: As with the <kbd>userspace</kbd> mode, there are also required <kbd>iptables</kbd> rules for redirecting the client traffic. But there is no proxy port as mediator. Faster but need to take care the liveness of Pod. By default, there is no way for a request to retry another Pod if the target one fails. To avoid accessing the unhealthy Pod, health-checking Pods and updating <kbd>iptables</kbd> in time is necessary.</span></span></li>
<li><kbd>ipvs</kbd><span>: <kbd>ipvs</kbd> is the beta feature in Kubernetes v1.9. In this mode, <kbd>kube-proxy</kbd> builds up the interface called netlink between the Service and its backend set. The <kbd>ipvs</kbd> mode takes care of the downside in both <kbd>userspace</kbd> and <kbd>iptables</kbd>; it is even faster, since the routing rules stored a hash table structure in the kernel space, and even reliable that <kbd>kube-proxy</kbd> keeps checking the consistency of <kbd>netlinks</kbd>. <kbd>ipvs</kbd> even provides multiple load balancing options.</span></li>
</ul>
The system picks the optimal and stable one as the default setting for <kbd>kube-proxy</kbd>. Currently, it is the mode <kbd>iptables</kbd>.</div>
<p>When a Pod tries to communicate with a Service, it can find the Service through environment variables or a DNS host lookup. Let's give it a try in the following scenario of accessing a service in a Pod:</p>
<pre>// run a Pod first, and ask it to be alive 600 seconds<br/>$ kubectl run my-1st-centos --image=centos --restart=Never sleep 600<br/>pod "my-1st-centos" created<br/>// run a Deployment of nginx and its Service exposing port 8080 for nginx<br/>$ kubectl run my-nginx --image=nginx --port=80<br/>deployment.apps "my-nginx" created<br/>$ kubectl expose deployment my-nginx --port=8080 --target-port=80 --name="my-nginx-service"<br/>service "my-nginx-service" exposed<br/>// run another pod<br/>$ kubectl run my-2nd-centos --image=centos --restart=Never sleep 600<br/>pod "my-2nd-centos" created<br/>//Go check the environment variables on both pods.<br/>$ kubectl exec my-1st-centos -- /bin/sh -c export<br/>$ kubectl exec my-2nd-centos -- /bin/sh -c export</pre>
<p>You will find that the Pod <kbd>my-2nd-centos</kbd> comes out with additional variables showing information for the Service <kbd>my-nginx-service</kbd>, as follows:</p>
<pre>export MY_NGINX_SERVICE_PORT="tcp://10.104.218.20:8080"<br/>export MY_NGINX_SERVICE_PORT_8080_TCP="tcp://10.104.218.20:8080"<br/>export MY_NGINX_SERVICE_PORT_8080_TCP_ADDR="10.104.218.20"<br/>export MY_NGINX_SERVICE_PORT_8080_TCP_PORT="8080"<br/>export MY_NGINX_SERVICE_PORT_8080_TCP_PROTO="tcp"<br/>export MY_NGINX_SERVICE_SERVICE_HOST="10.104.218.20"<br/>export MY_NGINX_SERVICE_SERVICE_PORT="8080"</pre>
<p>This is because the system failed to do a real-time update for Services; only the Pods created subsequently can be applied to accessing the Service through environment variables. With this ordering-dependent constraint, pay attention to running your Kubernetes resources in a proper sequence if they have to interact with each other in this way. The keys of the environment variables representing the Service host are formed as <kbd>&lt;SERVICE NAME&gt;_SERVICE_HOST</kbd>, and the Service port is like <kbd>&lt;SERVICE NAME&gt;_SERVICE_PORT</kbd>. In the preceding example, the dash in the name is also transferred to the underscore:</p>
<pre>// For my-2nd-centos, getting information of Service by environment variables<br/>$ kubectl exec my-2nd-centos -- /bin/sh -c 'curl $MY_NGINX_SERVICE_SERVICE_HOST:$MY_NGINX_SERVICE_SERVICE_PORT'<br/>&lt;!DOCTYPE html&gt;<br/>&lt;html&gt;<br/>&lt;head&gt;<br/>&lt;title&gt;Welcome to nginx!&lt;/title&gt;<br/>...</pre>
<p>Nevertheless, if the <kbd>kube-dns</kbd> add-on is installed, which is a DNS server in the Kubernetes system, any Pod in the same Namespace can access the Service, no matter when the Service was created. The hostname of the Service would be formed as <kbd>&lt;SERVICE NAME&gt;.&lt;NAMESPACE&gt;.svc.cluster.local</kbd>. <kbd>cluster.local</kbd> is the default cluster domain defined in booting <kbd>kube-dns</kbd>:</p>
<pre>// go accessing my-nginx-service by A record provided by kube-dns<br/>$ kubectl exec my-1st-centos -- /bin/sh -c 'curl my-nginx-service.default.svc.cluster.local:8080'<br/>$ kubectl exec my-2nd-centos -- /bin/sh -c 'curl my-nginx-service.default.svc.cluster.local:8080'</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The Kubernetes Service has four types: <kbd>ClusterIP</kbd>, <kbd>NodePort</kbd>, <kbd>LoadBalancer</kbd>, and <kbd>ExternalName</kbd>. In the <em>How to do it...</em> section in this recipe, we only demonstrate the default type, <kbd>ClusterIP</kbd>. The type <kbd>ClusterIP</kbd> indicates that the Kubernetes Service is assigned a unique virtual IP in the overlay network, which also means the identity in this Kubernetes cluster. <kbd>ClusterIP</kbd> guarantees that the Service is accessible internally.</p>
<p>The following diagram expresses the availability coverage of the types, and their entry points:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-227 image-border" src="assets/516627fc-54d3-4283-ba0f-70d54d0d7978.png" style="width:49.08em;height:27.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Four Service types and their entry points </div>
<p>For the <kbd>NodePort</kbd> type, it covers the <kbd>ClusterIP</kbd>'s features, has a peer-accessible virtual IP, and also allows the user to expose Services on each node with the same port. The type <kbd>LoadBalancer</kbd> is on the top of the other two types. The <kbd>LoadBalancer</kbd> Service would be exposed internally and on the node. More than that, if your cloud provider supports external load balancing servers, you can bind the load balancer IP to the Service, and this will become another exposing point. On the other hand, the type <kbd>ExternalName</kbd> is used for the endpoint out of your Kubernetes system. It is similar to the Endpoint we created with the configuration file in a previous section; moreover, a single <kbd>ExternalName</kbd> Service can provide this feature.</p>
<p>We can use the subcommand <kbd>create</kbd> to create Services in different types:</p>
<pre>// create a NodePort Service<br/>// the tag "tcp" is for indicating port configuration: SERVICE_PORT:TARGET_PORT<br/>$ kubectl create service nodeport my-nginx --tcp=8080:80<br/>service "my-nginx" created<br/>$ kubectl describe svc my-nginx<br/>Name:                     my-nginx<br/>Namespace:                default<br/>Labels:                   app=my-nginx<br/>Annotations:              &lt;none&gt;<br/>Selector:                 app=my-nginx<br/>Type:                     NodePort<br/>IP:                       10.105.106.134<br/>Port:                     8080-80  8080/TCP<br/>TargetPort:               80/TCP<br/>NodePort:                 8080-80  <strong>31336</strong>/TCP<br/>Endpoints:                &lt;none&gt;<br/>Session Affinity:         None<br/>External Traffic Policy:  Cluster<br/>Events:                   &lt;none&gt;</pre>
<p>In this example of the <kbd>NodePort</kbd> Service, you can see that it still has the virtual IP (<kbd>10.105.106.134</kbd>) in the cluster, and can be accessed through port <kbd>31336</kbd> of any Kubernetes node:</p>
<pre>// run an nginx Deployment with the label as NodePort Service my-nginx's selector<br/>$ kubectl run test-nodeport --image=nginx --labels="app=my-nginx"<br/>deployment.apps "test-nodeport" created<br/>// check the Kubernetes node with Service port on the node<br/>$ curl ubuntu02:31336<br/>&lt;!DOCTYPE html&gt;<br/>&lt;html&gt;<br/>&lt;head&gt;<br/>&lt;title&gt;Welcome to nginx!&lt;/title&gt;<br/>...</pre>
<p>In the case here, we demonstrate creating an <kbd>ExternalName</kbd> Service which exposes the <kbd>CNAME kubernetes.io</kbd>:</p>
<pre>$ kubectl create service externalname k8s-website --external-name kubernetes.io<br/>service "k8s-website" created<br/>// create a CentOS Pod for testing the Service availability<br/>$ kubectl run my-centos --image=centos --restart=Never sleep 600<br/>pod "my-centos" created<br/>//now you can check the Service by Service's DNS name<br/>$ kubectl exec -it my-centos -- /bin/sh -c 'curl k8s-website.default.svc.cluster.local '<br/>//Check all the Services we created in this section<br/>//ExternalName Service has no cluster IP as defined<br/>$ kubectl get svc<br/>NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)          AGE<br/>k8s-website            ExternalName   <strong>&lt;none&gt;</strong>           kubernetes.io   &lt;none&gt;           31m<br/>kubernetes             ClusterIP      10.96.0.1        &lt;none&gt;          443/TCP          14d<br/>my-nginx               NodePort       10.105.106.134   &lt;none&gt;          8080:31336/TCP   1h</pre>
<p>Yet, we cannot build an <kbd>ExternalName</kbd> Service in CLI with the subcommand <kbd>expose</kbd>, because <kbd>expose</kbd> works on exposing the Kubernetes resources, while the <kbd>ExternalName</kbd> Service is for the resources in the outside world. Then, it is also reasonable that the <kbd>ExternalName</kbd> Service doesn't need to be defined with the selector.</p>
<div class="packt_tip"><strong>Using the subcommand "create" to create Services <br/></strong><span>While using the subcommand <kbd>create</kbd> on Service creation, the command line would look like this: <kbd>kubectl create service &lt;SERVICE TYPE&gt; &lt;SERVICE NAME&gt; [OPTIONS]</kbd>. And we can put the Service types at <kbd>&lt;SERVICE TYPE&gt;</kbd>, such as <kbd>clusterip</kbd>, <kbd>nodeport</kbd>, <kbd>loadbalancer</kbd>, and <kbd>externalname</kbd>. With this method, we cannot specify the selector of the Service. As with the <kbd>NodePort</kbd> Service we created in that section, only a default selector, <kbd>app: my-nginx</kbd>, is created, and we have to assign this label to a later created Deployment <kbd>test-nodeport</kbd>. Except for the type</span> <kbd>ExternalName</kbd><span>, Service types can be created with the subcommand <kbd>expose</kbd> with the tag <kbd>type</kbd>. Try to create the <kbd>NodePort</kbd> service with <kbd>kubectl expose</kbd> for existing resources!</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To get the best practices of Kubernetes Services, the following recipes in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml">Chapter 2</a>, <em>Walking though Kubernetes Concepts</em>, are suggested reading:</p>
<ul>
<li><em>Deployment API</em></li>
<li><em>Working with Secrets</em></li>
<li><em>Working with labels and selectors</em></li>
</ul>
<p>There is more advanced knowledge to make your service more functional and flexible. Stay tuned:</p>
<ul>
<li><em>Forwarding container ports</em> section in <a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml">Chapter 3</a>, <em>Playing with Containers</em></li>
<li><em>Ensuring flexible usage of your containers</em> section in <a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml">Chapter 3</a>, <em>Playing with Containers</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with volumes</h1>
                </header>
            
            <article>
                
<p>Files in a container are ephemeral. When the container is terminated, the files are gone. Docker has introduced data volumes to help us persist data (<a href="https://docs.docker.com/engine/admin/volumes/volumes">https://docs.docker.com/engine/admin/volumes/volumes</a>). However, when it comes to multiple hosts, as a container cluster, it is hard to manage volumes across all the containers and hosts for file sharing or provisioning volume dynamically. Kubernetes introduces volume, which lives with a Pod across a container life cycle. It supports various types of volumes, including popular network disk solutions and storage services in different public clouds. Here are a few:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Volume type</strong></p>
</td>
<td>
<p><strong>Storage provider</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>emptyDir</kbd></p>
</td>
<td>
<p>Localhost</p>
</td>
</tr>
<tr>
<td>
<p><kbd>hostPath</kbd></p>
</td>
<td>
<p>Localhost</p>
</td>
</tr>
<tr>
<td>
<p><kbd>glusterfs</kbd></p>
</td>
<td>
<p>GlusterFS cluster</p>
</td>
</tr>
<tr>
<td>
<p><kbd>downwardAPI</kbd></p>
</td>
<td>
<p>Kubernetes Pod information</p>
</td>
</tr>
<tr>
<td>
<p><kbd>nfs</kbd></p>
</td>
<td>
<p>NFS server</p>
</td>
</tr>
<tr>
<td>
<p><kbd>awsElasticBlockStore</kbd></p>
</td>
<td>
<p>Amazon Web Service Amazon Elastic Block Store</p>
</td>
</tr>
<tr>
<td>
<p><kbd>gcePersistentDisk</kbd></p>
</td>
<td>
<p>Google Compute Engine persistent disk</p>
</td>
</tr>
<tr>
<td>
<p><kbd>azureDisk</kbd></p>
</td>
<td>
<p>Azure disk storage</p>
</td>
</tr>
<tr>
<td>
<p><kbd>projected</kbd></p>
</td>
<td>
<p>Kubernetes resources; currently supports <kbd>secret</kbd>, <kbd>downwardAPI</kbd>, and <kbd>configMap</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>secret</kbd></p>
</td>
<td>
<p>Kubernetes Secret resource</p>
</td>
</tr>
<tr>
<td>
<p><kbd>vSphereVolume</kbd></p>
</td>
<td>
<p>vSphere VMDK volume</p>
</td>
</tr>
<tr>
<td>
<p><kbd>gitRepo</kbd></p>
</td>
<td>
<p>Git repository</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Storage providers are required when you start to use volume in Kubernetes, except for <kbd>emptyDir</kbd>, which will be erased when the Pod is removed. For other storage providers, folders, servers, or clusters have to be built before using them in the Pod definition. Dynamic provisioning was promoted to stable in Kubernetes version 1.6, which allows you to provision storage based on the supported cloud provider.</p>
<p>In this section, we'll walk through the details of <kbd>emptyDir</kbd>, <kbd>hostPath</kbd>, <kbd>nfs</kbd>, <kbd>glusterfs</kbd>, <kbd>downwardAPI</kbd>, and <kbd>gitRepo</kbd>. <kbd>Secret</kbd>, which is used to store credentials, will be introduced in the next section. <kbd>Projected</kbd>, on the other hand, is a way one could group other volume resources under one single mount point. As it only supports <kbd>secret</kbd>, <kbd>downwardAPI</kbd>, and <kbd>configMap</kbd>, we'll be introducing this in the Secret section, as well. The rest of the volume types have similar Kubernetes syntax, just with different backend volume implementations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Volumes are defined in the volumes section of the pod definition with unique names. Each type of volume has a different configuration to be set. Once you define the volumes, you can mount them in the <kbd>volumeMounts</kbd> section in the container specs. <kbd>volumeMounts.name</kbd> and <kbd>volumeMounts.mountPath</kbd> are required, which indicate the name of the volumes you defined and the mount path inside the container, respectively.</p>
<p>We'll use the Kubernetes configuration file with the YAML format to create a Pod with volumes in the following examples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">emptyDir</h1>
                </header>
            
            <article>
                
<p><kbd>emptyDir</kbd> is the simplest volume type, which will create an empty volume for containers in the same Pod to share. When the Pod is removed, the files in <kbd>emptyDir</kbd> will be erased, as well. <kbd>emptyDir</kbd> is created when a Pod is created. In the following configuration file, we'll create a Pod running Ubuntu with commands to sleep for <kbd>3600</kbd> seconds. As you can see, one volume is defined in the volumes section with name data, and the volumes will be mounted under the <kbd>/data-mount</kbd> path in the Ubuntu container:</p>
<pre>// configuration file of emptyDir volume<br/># cat 2-6-1_emptyDir.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: ubuntu<br/>  labels:<br/>    name: ubuntu<br/>spec:<br/>  containers:<br/>    - image: ubuntu<br/>      command:<br/>        - sleep<br/>        - "3600"<br/>      imagePullPolicy: IfNotPresent<br/>      name: ubuntu<br/>      volumeMounts:<br/>        - mountPath: /data-mount<br/>          name: data<br/>      volumes:<br/>        - name: data<br/>       <strong>   emptyDir: {}</strong><br/><br/>// create pod by configuration file emptyDir.yaml<br/># kubectl create -f 2-6-1_emptyDir.yaml<br/>pod "ubuntu" created</pre>
<div class="packt_infobox"><strong>Check which node the Pod is running on<br/></strong>By using the <kbd>kubectl describe pod &lt;Pod name&gt; | grep Node</kbd> command, you can check which node the Pod is running on.</div>
<p>After the Pod is running, you can use <kbd>docker inspect &lt;container ID&gt;</kbd> on the target node and you can see the detailed mount points inside your container:</p>
<pre>  "Mounts": [<br/>     ...<br/>  {<br/>                "Type": "bind",<br/>                "Source": "/var/lib/kubelet/pods/98c7c676-e9bd-11e7-9e8d-080027ac331c/volumes/kubernetes.io~empty-dir/data",<br/>                "Destination": "/data-mount",<br/>                "Mode": "",<br/>                "RW": true,<br/>                "Propagation": "rprivate"<br/>            }<br/>     ...<br/>]</pre>
<p>Kubernetes mounts <kbd>/var/lib/kubelet/pods/&lt;id&gt;/volumes/kubernetes.io~empty-dir/&lt;volumeMount name&gt;</kbd> to <kbd>/data-mount</kbd> for the Pod to use. If you create a Pod with more than one container, all of them will mount the same destination <kbd>/data-mount</kbd> with the same source. The default mount propagation is <kbd>rprivate</kbd>, which means any mount points on the host are invisible in the container, and vice versa.</p>
<p><kbd>emptyDir</kbd> could be mounted as <kbd>tmpfs</kbd> by setting <kbd>emptyDir.medium</kbd> as <kbd>Memory</kbd>.</p>
<p>Taking the previous configuration file <kbd>2-6-1_emptyDir_mem.yaml</kbd> as an example, it would be as follows:</p>
<pre>volumes:<br/>    -<br/>      name: data<br/>      emptyDir:<br/>        medium: Memory</pre>
<p>We could verify whether it's successfully mounted with the <kbd>kubectl exec &lt;pod_name&gt; &lt;commands&gt;</kbd> command. We'll run the <kbd>df</kbd> command in this container:</p>
<pre># kubectl exec ubuntu df<br/>Filesystem 1K-blocks Used Available Use% Mounted on<br/>...<br/>tmpfs 1024036 0 1024036 0% /data-mount<br/>...</pre>
<p>Note that <kbd>tmpfs</kbd> is stored in memory instead of in the filesystem. No file will be created, and it'll be flushed in every reboot. In addition, it is constrained by memory limits in Kubernetes. For more information about container resource constraint, refer to <em>Working with Namespace</em> in this chapter.</p>
<div>
<p>If you have more than one container inside a Pod, the <kbd>Kubectl exec</kbd> command will be <kbd>kubectl exec &lt;pod_name&gt; &lt;container_name&gt; &lt;commands&gt;</kbd>.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">hostPath</h1>
                </header>
            
            <article>
                
<p><kbd>hostPath</kbd> acts as data volume in Docker. The local folder on a node listed in <kbd>hostPath</kbd> will be mounted into the Pod. Since the Pod can run on any nodes, read/write functions happening in the volume could explicitly exist in the node on which the Pod is running. In Kubernetes, however, the Pod should not be node-aware. Please note that the configuration and files might be different on different nodes when using <kbd>hostPath</kbd>. Therefore, the same Pod, created by the same command or configuration file, might act differently on different nodes.</p>
<p>By using <kbd>hostPath</kbd>, you're able to read and write the files between containers and localhost disks of nodes. What we need for volume definition is for <kbd>hostPath.path</kbd> to specify the target mounted folder on the node:</p>
<pre>apiVersion: v1<br/># cat 2-6-2_hostPath.yaml<br/>kind: Pod<br/>metadata:<br/>  name: ubuntu<br/>spec:<br/>  containers:<br/>    -<br/>      image: ubuntu<br/>      command:<br/>        - sleep<br/>        - "3600"<br/>      imagePullPolicy: IfNotPresent<br/>      name: ubuntu<br/>      volumeMounts:<br/>        -<br/>          mountPath: /data-mount<br/>          name: data<br/>  volumes:<br/>    -<br/>      name: data<br/>  <strong>    hostPath:</strong><br/><strong>        path: /tmp/data</strong></pre>
<p>Using <kbd>docker inspect</kbd> to check the volume details, you will see the volume on the host is mounted in the <kbd>/data-mount</kbd> destination:</p>
<pre>"Mounts": [<br/>            {<br/>                "Type": "bind",<br/>                "Source": "/tmp/data",<br/>                "Destination": "/data-mount",<br/>                "Mode": "",<br/>                "RW": true,<br/>                "Propagation": "rprivate"<br/>            },<br/>                          ...<br/>    ]</pre>
<p>If we run <kbd>kubectl exec ubuntu touch /data-mount/sample</kbd>, we should be able to see one empty file, named <kbd>sample under /tmp/data</kbd>, on the host.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NFS</h1>
                </header>
            
            <article>
                
<p>You can mount an <strong>network filesystem</strong> (<strong><span>NFS</span></strong>) to your Pod as <kbd>nfs volume</kbd>. Multiple Pods can mount and share the files in the same <kbd>nfs volume</kbd>. The data stored into <kbd>nfs volume</kbd> will be persistent across the Pod lifetime. You have to create your own NFS server before using <kbd>nfs volume</kbd>, and make sure the <kbd>nfs-utils</kbd> package is installed on Kubernetes minions.</p>
<div class="packt_infobox">Check whether your NFS server works before you go. You should check out the <kbd>/etc/exports</kbd> file with a proper sharing parameter and directory, and use the <kbd>mount -t nfs &lt;nfs server&gt;:&lt;share name&gt; &lt;local mounted point&gt;</kbd> command to check whether it could be mounted locally.</div>
<p>The configuration file of the volume type with NFS is similar to others, but <kbd>nfs.server</kbd> and <kbd>nfs.path</kbd> are required in the volume definition to specify NFS server information and the path mounted from. <kbd>nfs.readOnly</kbd> is an optional field for specifying whether the volume is read-only or not (the default is <kbd>false</kbd>):</p>
<pre># configuration file of nfs volume<br/>$ cat 2-6-3_nfs.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: nfs<br/>spec:<br/>  containers:<br/>    -<br/>      name: nfs<br/>      image: ubuntu<br/>      volumeMounts:<br/>          - name: nfs<br/>            mountPath: "/data-mount"<br/>  volumes:<br/>  - name: nfs<br/>   <strong> nfs:</strong><br/><strong>      server: &lt;your nfs server&gt;</strong><br/><strong>      path: "/"</strong></pre>
<p>After you run <kbd>kubectl create –f 2-6-3_nfs.yaml</kbd>, you can describe your Pod with <kbd>kubectl describe &lt;pod name&gt;</kbd> to check the mounting status. If it's mounted successfully, it should show conditions. Ready as true and the target <kbd>nfs</kbd> you mount:</p>
<pre>Conditions:<br/>  Type Status<br/>  Ready True<br/>Volumes:<br/>  nfs:<br/>    Type: NFS (an NFS mount that lasts the lifetime of a pod)<br/>    Server: &lt;your nfs server&gt;<br/>    Path: /<br/>    ReadOnly: false</pre>
<p>If we inspect the container with the <kbd>docker</kbd> command, we can see the volume information in the <kbd>Mounts</kbd> section:</p>
<pre>"Mounts": [<br/> {<br/>            "Source": "/var/lib/kubelet/pods/&lt;id&gt;/volumes/kubernetes.io~nfs/nfs",<br/>            "Destination": "/data-mount",<br/>            "Mode": "",<br/>            "RW": true<br/>        },<br/>                          ...<br/>     ]</pre>
<p>Actually, Kubernetes just mounts your <kbd>&lt;nfs server&gt;:&lt;share name&gt;</kbd> into <kbd>/var/lib/kubelet/pods/&lt;id&gt;/volumes/kubernetes.io~nfs/nfs</kbd>, and then mounts it into the container as the destination in <kbd>/data-mount</kbd>. You could also use <kbd>kubectl exec</kbd> to touch the file, to test whether it's perfectly mounted.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">glusterfs</h1>
                </header>
            
            <article>
                
<p>GlusterFS (<a href="https://www.gluster.org">https://www.gluster.org</a>) is a scalable, network-attached storage filesystem. The <kbd>glusterfs</kbd> volume type allows you to mount GlusterFS volume into your Pod. Just like NFS volume, the data in <kbd>glusterfs</kbd> volume is persistent across the Pod lifetime. If the Pod is terminated, the data is still accessible in <kbd>glusterfs</kbd> volume. You should build the GlusterFS system before using <kbd>glusterfs</kbd> volume.</p>
<div class="packt_infobox">Check whether <kbd>glusterfs</kbd> works before you go. <span>By using <kbd>glusterfs</kbd> volume information on GlusterFS servers, you can see currently available volumes. By using <kbd>mount -t glusterfs &lt;glusterfs server&gt;:/&lt;volume name&gt; &lt;local mounted point&gt;</kbd> on local, you can check whether the GlusterFS system can be successfully mounted.</span></div>
<p>Since the volume replica in GlusterFS must be greater than <kbd>1</kbd>, let's assume we have two replicas in the servers <kbd>gfs1</kbd> and <kbd>gfs2</kbd>, and the volume name is <kbd>gvol</kbd>.</p>
<p>First, we need to create an endpoint acting as a bridge for <kbd>gfs1</kbd> and <kbd>gfs2</kbd>:</p>
<pre>$ cat 2-6-4_gfs-endpoint.yaml<br/>kind: Endpoints<br/>apiVersion: v1<br/>metadata:<br/>  name: glusterfs-cluster<br/>subsets:<br/>  -<br/>    addresses:<br/>      -<br/>        ip: &lt;gfs1 server ip&gt;<br/>    ports:<br/>      -<br/>        port: 1<br/>  -<br/>    addresses:<br/>      -<br/>        ip: &lt;gfs2 server ip&gt;<br/>    ports:<br/>      -<br/>        port: 1<br/><br/># create endpoints<br/>$ kubectl create –f 2-6-4_gfs-endpoint.yaml</pre>
<p>Then, we can use <kbd>kubectl get endpoints</kbd> to check the endpoint was created properly:</p>
<pre>$kubectl get endpoints<br/>NAME ENDPOINTS AGE<br/>glusterfs-cluster &lt;gfs1&gt;:1,&lt;gfs2&gt;:1 12m</pre>
<p>After that, we should be able to create the Pod with <kbd>glusterfs</kbd> volume by <kbd>glusterfs.yaml</kbd>. The parameters of the <kbd>glusterfs</kbd> volume definition are <kbd>glusterfs.endpoints</kbd>, which specify the endpoint name we just created, and <kbd>glusterfs.path</kbd>, which is the volume name <kbd>gvol</kbd>. <kbd>glusterfs.readOnly</kbd> is used to set whether the volume is mounted in read-only mode:</p>
<pre>$ cat 2-6-4_glusterfs.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: ubuntu<br/>spec:<br/>  containers:<br/>    -<br/>      image: ubuntu<br/>      command:<br/>        - sleep<br/>        - "3600"<br/>      imagePullPolicy: IfNotPresent<br/>      name: ubuntu<br/>      volumeMounts:<br/>        -<br/>          mountPath: /data-mount<br/>          name: data<br/>  volumes:<br/>    -<br/>      name: data<br/>    <strong>  glusterfs:</strong><br/><strong>        endpoints: glusterfs-cluster</strong><br/><strong>        path: gvol</strong></pre>
<p>Let's check the volume setting with <kbd>kubectl describle</kbd>:</p>
<pre>Volumes:<br/>  data:<br/>    Type: Glusterfs (a Glusterfs mount on the host that shares a pod's lifetime)<br/>    EndpointsName: glusterfs-cluster<br/>    Path: gvol<br/>    ReadOnly: false</pre>
<p>Using <kbd>docker inspect</kbd>, you should be able to see that the mounted source is <kbd>/var/lib/kubelet/pods/&lt;id&gt;/volumes/kubernetes.io~glusterfs/data</kbd> to the destination <kbd>/data-mount</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">downwardAPI</h1>
                </header>
            
            <article>
                
<p><kbd>downwardAPI</kbd> volume is used to expose Pod information into a container. The definition of <kbd>downwardAPI</kbd> is a list of items. An item contains a path and <kbd>fieldRef</kbd>. Kubernetes will dump the specified metadata listed in <kbd>fieldRef</kbd> to a file named <kbd>path</kbd> under <kbd>mountPath</kbd> and mount the <kbd>&lt;volume name&gt;</kbd> into the destination you specified. Currently supported metadata for <kbd>downwardAPI</kbd> volume includes:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Field path</strong></p>
</td>
<td>
<p><strong>Scope</strong></p>
</td>
<td>
<p><strong>Definition</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>spec.nodeName</kbd></p>
</td>
<td>
<p>Pod</p>
</td>
<td>
<p>The node that the Pod is running on</p>
</td>
</tr>
<tr>
<td>
<p><kbd>spec.serviceAccountName</kbd></p>
</td>
<td>
<p>Pod</p>
</td>
<td>
<p>The service account associating with the current Pod</p>
</td>
</tr>
<tr>
<td>
<p><kbd>metadata.name</kbd></p>
</td>
<td>
<p>Pod</p>
</td>
<td>
<p>The name of the Pod</p>
</td>
</tr>
<tr>
<td>
<p><kbd>metadata.namespace</kbd></p>
</td>
<td>
<p>Pod</p>
</td>
<td>
<p>The Namespace that the Pod belongs to</p>
</td>
</tr>
<tr>
<td>
<p><kbd>metadata.annotations</kbd></p>
</td>
<td>
<p>Pod</p>
</td>
<td>
<p>The annotations of the Pod</p>
</td>
</tr>
<tr>
<td>
<p><kbd>metadata.labels</kbd></p>
</td>
<td>
<p>Pod</p>
</td>
<td>
<p>The labels of the Pod</p>
</td>
</tr>
<tr>
<td>
<p><kbd>status.podIP</kbd></p>
</td>
<td>
<p>Pod</p>
</td>
<td>
<p>The ip of the Pod</p>
</td>
</tr>
<tr>
<td>
<p><kbd>limits.cpu</kbd></p>
</td>
<td>
<p>Container</p>
</td>
<td>
<p>The CPU limits of the container</p>
</td>
</tr>
<tr>
<td>
<p><kbd>requests.cpu</kbd></p>
</td>
<td>
<p>Container</p>
</td>
<td>
<p>The CPU requests of the container</p>
</td>
</tr>
<tr>
<td>
<p><kbd>limits.memory</kbd></p>
</td>
<td>
<p>Container</p>
</td>
<td>
<p>The memory limits of the container</p>
</td>
</tr>
<tr>
<td>
<p><kbd>requests.memory</kbd></p>
</td>
<td>
<p>Container</p>
</td>
<td>
<p>The memory requests of the container</p>
</td>
</tr>
<tr>
<td>
<p><kbd>limits.ephemeral-storage</kbd></p>
</td>
<td>
<p>Container</p>
</td>
<td>
<p>The ephemeral storage limits of the container</p>
</td>
</tr>
<tr>
<td>
<p><kbd>requests.ephemeral-storage</kbd></p>
</td>
<td>
<p>Container</p>
</td>
<td>
<p>The ephemeral storage requests of the container</p>
</td>
</tr>
</tbody>
</table>
<p>We use <kbd>fieldRef.fieldPath</kbd> if the scope is with a Pod; <kbd>resourceFieldRef</kbd> is used when the scope is with a container. For example, the following configuration file could expose <kbd>metadata.labels</kbd> in <kbd>/data-mount</kbd> volume in an Ubuntu container:</p>
<pre>// pod scope example<br/># cat 2-6-5_downward_api.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: downwardapi<br/>  labels:<br/>    env: demo<br/>spec:<br/>  containers:<br/>    -<br/>      name: downwardapi<br/>      image: ubuntu<br/>      command:<br/>        - sleep<br/>        - "3600"<br/>      volumeMounts:<br/>          - name: podinfo<br/>            mountPath: "/data-mount"<br/>  volumes:<br/>    - name: podinfo<br/>      downwardAPI:<br/>        items:<br/>          - path: metadata<br/>           <strong> fieldRef:</strong><br/><strong>              fieldPath: metadata.labels</strong></pre>
<p>By describing the <kbd>pod</kbd>, we could check that the volume is mounted successfully to <kbd>/data-mount</kbd>, and <kbd>metadata.labels</kbd> is pointed to the <kbd>metadata</kbd> file:</p>
<pre>// describe the pod<br/># kubectl describe pod downwardapi<br/>...<br/>    Mounts:<br/>      /data-mount from podinfo (rw)<br/>...<br/>Volumes:<br/>  podinfo:<br/>    Type: DownwardAPI (a volume populated by information about the pod)<br/>    Items:<br/>      metadata.labels -&gt; metadata</pre>
<p>We could check the file inside the container with <kbd>kubectl exec downwardapi cat /data-mount/metadata</kbd>, and you should be able to see <kbd>env="example" presents</kbd>.</p>
<p>If it's in the container scope, we'll have to specify the container name:</p>
<pre># cat 2-6-5_downward_api_container.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: downwardapi-container<br/>spec:<br/>  containers:<br/>    -<br/>      name: downwardapi<br/>      image: ubuntu<br/>      command:<br/>        - sleep<br/>        - "3600"<br/>      volumeMounts:<br/>          - name: podinfo<br/>            mountPath: "/data-mount"<br/>  volumes:<br/>    - name: podinfo<br/>      downwardAPI:<br/>        items:<br/>          - path: "cpu_limit"<br/>         <strong>   resourceFieldRef:</strong><br/><strong>              containerName: downwardapi</strong><br/><strong>              resource: limits.cpu</strong></pre>
<p>We could use the <kbd>docker inspect &lt;container_name&gt;</kbd> command inside a node to check the implementation:</p>
<pre>{<br/>            "Source": "/var/lib/kubelet/pods/&lt;id&gt;/volumes/kubernetes.io~downward-api/&lt;volume name&gt;",<br/>            "Destination": "/data-mount",<br/>            "Mode": "",<br/>            "RW": true<br/> }</pre>
<p>Kubernetes exposes <kbd>pod</kbd> information in source volume, and mounts it to <kbd>/data-mount</kbd>.</p>
<p>For the IP of the Pod, using environment variable to propagate in Pod spec would be must easier:</p>
<pre>spec:<br/>  containers:<br/>    - name: envsample-pod-info<br/>      env:<br/>        - name: MY_POD_IP<br/>          valueFrom:<br/>            fieldRef:<br/>              fieldPath: status.podIP</pre>
<p>The sample folder in the Kubernetes GitHub (<a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information">https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information</a>) contains more examples for both environment variables and <kbd>downwardAPI</kbd> volume.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">gitRepo</h1>
                </header>
            
            <article>
                
<p><kbd>gitRepo</kbd> is a convenient volume type that clones your existing <span><span>Git</span></span> repository into a container:</p>
<pre>// an example of how to use gitRepo volume type<br/># cat 2-6-6_gitRepo.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: gitrepo<br/>spec:<br/>  containers:<br/>  - image: ubuntu<br/>    name: ubuntu<br/>    command:<br/>      - sleep<br/>      - "3600"<br/>    volumeMounts:<br/>    - mountPath: /app<br/>      name: app-git<br/>  volumes:<br/>  - name: app-git<br/>    <strong>gitRepo:</strong><br/><strong>      repository: "https://github.com/kubernetes-cookbook/second-edition.git"</strong><br/><strong>      revision: "9d8e845e2f55a5c65da01ac4235da6d88ef6bcd0"</strong><br/><br/># kubectl create -f 2-6-6_gitRepo.yaml<br/>pod "gitrepo" created</pre>
<p>In the preceding example, the volume plugin mounts an empty directory and runs the <span>git clone <kbd>&lt;gitRepo.repolist&gt;</kbd> to clone the repository into it. Then the Ubuntu container will be able to access it.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>In the previous cases, the user needs to know the details of the storage provider. Kubernetes provides <kbd>PersistentVolumes</kbd> and <kbd>PersistentVolumeClaim</kbd> to abstract the details of the storage provider and storage consumer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PersistentVolumes</h1>
                </header>
            
            <article>
                
<p>An illustration of <kbd>PersistentVolume</kbd> is shown in the following graph. First, the administrator provisions the specification of a <kbd>PersistentVolume</kbd>. Then the consumer requests for storage with <kbd>PersistentVolumeClaim</kbd>. Finally, the Pod mounts the volume with the reference of <kbd>PersistentVolumeClaim</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-228 image-border" src="assets/f65303f4-59e5-4814-a38b-d6a38bd7977b.png" style="width:20.17em;height:29.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">PersistentVolumeClaims is an abstract layer to decouple volumes for a Pod and physical volume resource</div>
<p>Here is an example using <kbd>NFS</kbd>. The administrator needs to provision and allocate <kbd>PersistentVolume</kbd> first:</p>
<pre># example of PV with NFS<br/>$ cat 2-6-7_pv.yaml<br/>  apiVersion: "v1"<br/>  kind: "PersistentVolume"<br/>  metadata:<br/>    name: "pvnfs01"<br/>  spec:<br/>    capacity:<br/>      storage: "3Gi"<br/>    accessModes:<br/>      - "ReadWriteOnce"<br/>    nfs:<br/>      path: "/"<br/>      server: "&lt;your nfs server&gt;"<br/>    persistentVolumeReclaimPolicy: "Recycle"<br/><br/># create the pv<br/>$ kubectl create -f 2-6-7_pv.yaml<br/>persistentvolume "pvnfs01" created</pre>
<p>We can see that there are three parameters here: <kbd>capacity</kbd>, <kbd>accessModes</kbd>, and <kbd>persistentVolumeReclaimPolicy</kbd>. <kbd>capacity</kbd> is the size of this <kbd>PersistentVolume</kbd>. Now, <kbd>accessModes</kbd> is based on the capability of the storage provider and can be set to a specific mode during provision. For example, NFS supports multiple readers and writers simultaneously—then we can specify the <kbd>accessModes</kbd> as one of <kbd>ReadWriteOnce</kbd>, <kbd>ReadOnlyMany</kbd>, or <kbd>ReadWriteMany</kbd>. Now, <kbd>persistentVolumeReclaimPolicy</kbd> is used to define the behavior when <kbd>PersistentVolume</kbd> is released. The currently supported policy is retain and recycle for <kbd>nfs</kbd> and <kbd>hostPath</kbd>. You have to clean the volume by yourself in retain mode; on the other hand, Kubernetes will scrub the volume in recycle mode.</p>
<p>PV is a resource like a node. We could use <kbd>kubectl get pv</kbd> to see current provisioned PVs:</p>
<pre># list current PVs<br/>$ kubectl get pv<br/>NAME LABELS CAPACITY ACCESSMODES STATUS CLAIM REASON AGE<br/>pvnfs01 &lt;none&gt; 3Gi RWO Bound default/pvclaim01 37m</pre>
<p>Next, we will need to bind <kbd>PersistentVolume</kbd> with <kbd>PersistentVolumeClaim</kbd> in order to mount it as volume into the <kbd>pod</kbd>:</p>
<pre># example of PersistentVolumeClaim<br/>$ cat claim.yaml<br/>apiVersion: "v1"<br/>kind: "PersistentVolumeClaim"<br/>metadata:<br/>  name: "pvclaim01"<br/>spec:<br/>  accessModes:<br/>    - ReadWriteOnce<br/>  resources:<br/>    requests:<br/>      storage: 1Gi<br/><br/># create the claim<br/>$ kubectl create -f claim.yaml<br/>persistentvolumeclaim "pvclaim01" created<br/><br/># list the PersistentVolumeClaim (pvc)<br/>$ kubectl get pvc<br/>NAME LABELS STATUS VOLUME CAPACITY ACCESSMODES AGE<br/>pvclaim01 &lt;none&gt; Bound pvnfs01 3Gi RWO 59m</pre>
<p>The constraints of <kbd>accessModes</kbd> and storage can be set in <kbd>PersistentVolumeClaim</kbd>. If the claim is bound successfully, its status will turn to <kbd>Bound</kbd>; on the other hand, if the status is <kbd>Unbound</kbd>, it means there is no PV currently matching the requests.</p>
<p>Then we are able to mount the PV as volume with the reference of <kbd>PersistentVolumeClaim</kbd>:</p>
<pre># example of mounting into Pod<br/>$ cat nginx.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: nginx<br/>  labels:<br/>    project: pilot<br/>    environment: staging<br/>    tier: frontend<br/>spec:<br/>  containers:<br/>    -<br/>      image: nginx<br/>      imagePullPolicy: IfNotPresent<br/>      name: nginx<br/>      volumeMounts:<br/>      - name: pv<br/>        mountPath: "/usr/share/nginx/html"<br/>      ports:<br/>      - containerPort: 80<br/>  volumes:<br/>    - name: pv<br/>   <strong>   persistentVolumeClaim:</strong><br/><strong>        claimName: "pvclaim01"</strong><br/><br/># create the pod<br/>$ kubectl create -f nginx.yaml<br/>pod "nginx" created</pre>
<p>It will be similar syntax to other volume types. Just add the <kbd>claimName</kbd> of <kbd>persistentVolumeClaim</kbd> in the volume definition. We are all set! Let's check the details to see whether we mounted it successfully:</p>
<pre># check the details of a pod<br/>$ kubectl describe pod nginx<br/>...<br/>Volumes:<br/>  pv:<br/>    Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)<br/>    ClaimName: pvclaim01<br/>    ReadOnly: false<br/>...</pre>
<p>We can see we have a volume mounted in the Pod <kbd>nginx</kbd> with the type <kbd>pv pvclaim01</kbd>. Use <kbd>docker inspect</kbd> to see how it is mounted:</p>
<pre>"Mounts": [<br/>        {<br/>            "Source": "/var/lib/kubelet/pods/&lt;id&gt;/volumes/kubernetes.io~nfs/pvnfs01",<br/>            "Destination": "/usr/share/nginx/html",<br/>            "Mode": "",<br/>            "RW": true<br/>        },<br/>                ...<br/>    ]</pre>
<p>Kubernetes mounts <kbd>/var/lib/kubelet/pods/&lt;id&gt;/volumes/kubernetes.io~nfs/&lt; persistentvolume name&gt;</kbd> into the destination in the Pod.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using storage classes</h1>
                </header>
            
            <article>
                
<p>In the cloud world, people provision storage or data volume dynamically. While <kbd>PersistentVolumeClaim</kbd> is based on existing static <kbd>PersistentVolume</kbd> that is provisioned by administrators, it might be really beneficial if the cloud volume could be requested dynamically when it needs to be. Storage classes are designed to resolve this problem. To make storage classes available in your cluster, three conditions need to be met. First, the <kbd>DefaultStorageClass</kbd> admission controller has to be enabled (refer to <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml">Chapter 7</a>, <em>Building Kubernetes on GCP</em>). Then <kbd>PersistentVolumeClaim</kbd> needs to request a storage class. The last condition is trivial; administrators have to configure a storage class in order to make dynamic provisioning work:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-229 image-border" src="assets/23a069b1-0d06-4c86-8762-e3fe472a7a87.png" style="width:21.17em;height:36.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">StorageClass dynamically allocates a PV and associates it with a PVC</div>
<p>The default storage classes are various, basically based on your underlying cloud provider. Storage classes are the abstract way to define underlying storage providers. They have different syntax based on different types of providers. Default storage classes can be changed, but cannot be deleted. The default storage class has an annotation <kbd>storageclass.beta.kubernetes.io/is-default-class=true</kbd> on. Removing that annotation can disable the dynamic provisioning. Moving the annotation to another storage class can switch the default storage class. If no storage classes have that annotation, dynamic provisioning will not be triggered when there is a new <kbd>PersistentVolumeClaim</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">gcePersistentDisk</h1>
                </header>
            
            <article>
                
<p><kbd>gcePersistentDisk</kbd> volume mounts a <strong>Google Compute Engine</strong> (<strong>GCE</strong>) <strong>Persistent Disk</strong> (<strong>PD</strong>) into a Pod. If you provision it statically, you'll have to create it first with the <kbd>gcloud</kbd> command or in the GCE console. The following is an example:</p>
<pre># cat 2-6-8_gce/static_mount.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: gce-pd-pod<br/>spec:<br/>  containers:<br/>  - image: nginx<br/>    name: gce-pd-example<br/>    volumeMounts:<br/>    - mountPath: /mount-path<br/>      name: gce-pd<br/>      ports:<br/>        - containerPort: 80<br/>  volumes:<br/>  - name: gce-pd<br/> <strong>   gcePersistentDisk:</strong><br/><strong>      pdName: example</strong><br/><strong>      fsType: ext4</strong></pre>
<p>Alternatively, and more cost-effectively, we could use dynamic provisioning. Then we don't need to provision PD beforehand. For enabling dynamic provisioning, the <kbd>DefaultStorageClass</kbd> admission controller has to be enabled on the API server. In some Kubernetes environments, it has been enabled by default, such as in GCE. We could explicitly disable it by setting the <kbd>storageClassName: "" in Pod/Deployment/ReplicaSet</kbd> configuration file.</p>
<p>Next, we'll introduce how to create a non-default <kbd>StorageClass</kbd>:</p>
<pre>// list storageclasses (sc)<br/># kubectl get sc<br/>NAME PROVISIONER<br/>standard (default) kubernetes.io/gce-pd</pre>
<p>We can see we have a default storage class named <kbd>standard</kbd>. If that's the desired provider, then you don't need to create your own storage classes. In the following example, we'll create a new storage class named <kbd>example</kbd>:</p>
<pre>// gce storage class<br/># cat 2-6-8_gce/storageclass.yaml<br/>kind: StorageClass<br/>apiVersion: storage.k8s.io/v1<br/>metadata:<br/>  name: example<br/><strong>provisioner: kubernetes.io/gce-pd</strong><br/><strong>parameters:</strong><br/><strong>  type: pd-standard</strong><br/><strong>  zones: us-central1-a</strong><br/>   <br/>// create storage class<br/># kubectl create -f storageclass.yaml<br/>   storageclass "example" created<br/><br/>// check current storage classes<br/># kubectl get sc<br/>NAME PROVISIONER<br/>example kubernetes.io/gce-pd<br/>   standard (default) kubernetes.io/gce-pd</pre>
<p>For the type, you can specify any storage type that GCE supports, such as <kbd>pd-ssd</kbd>. You can specify zones by changing zone parameters, too. Next, we'll add a <kbd>PersistentVolumeClaim</kbd> for using this storage class:</p>
<pre># 2-6-8_gce/pvc.yaml<br/>apiVersion: v1<br/>kind: PersistentVolumeClaim<br/>metadata:<br/>  name: gce-example<br/>spec:<br/>  accessModes:<br/>    - ReadWriteOnce<br/><strong>  storageClassName: example</strong><br/><strong>  resources:</strong><br/><strong>    requests:</strong><br/><strong>      storage: 5Gi</strong><br/><br/>// create pvc<br/># kubectl create -f pvc.yaml<br/>persistentvolumeclaim "gce-example" created<br/><br/>// check pvc status<br/># kubectl get pvc<br/>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE<br/>gce-example Bound pvc-d04218e3-ede5-11e7-aef7-42010a8001f4 5Gi RWO example 1h</pre>
<p>This configuration file will create a PVC by specifying the storage class named <kbd>example</kbd>. A PV will be created by the claim. When a PVC is in <kbd>Bound</kbd> status, Kubernetes will always bind that PV to the matching PVC. Then, let's have a Pod using this PVC:</p>
<pre># cat 2-6-8_gce/pod.yaml<br/>kind: Pod<br/>apiVersion: v1<br/>metadata:<br/>  name: gce-pd-pod<br/>spec:<br/>  volumes:<br/>    - name: gce-pd<br/><strong>      persistentVolumeClaim:</strong><br/><strong>       claimName: gce-example</strong><br/><strong>  containers:</strong><br/>    - name: gce-pd-example<br/>      image: nginx<br/>      ports:<br/>        - containerPort: 80<br/>      volumeMounts:<br/>        - mountPath: /mount-path<br/>          name: gce-pd<br/><br/>// create a pod<br/># kubectl create -f pod.yaml<br/>pod "gce-pd-pod" created<br/><br/>// check the volume setting in pod<br/># kubectl describe pod gce-pd-pod<br/>...<br/>Containers:<br/>  gce-pd-example:<br/>    Container ID: <br/>    Mounts:<br/>    <strong>  /mount-path from gce-pd (rw)</strong><br/>...<br/>Volumes:<br/>  gce-pd:<br/>    Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)<br/>    ClaimName: gce-example<br/>    ReadOnly: false</pre>
<p>We can see that <kbd>gce-pd</kbd> is mounted under <kbd>/mount-path</kbd>. Let's see if the volume has been provisioned dynamically.</p>
<p>Alternatively, you could use <kbd>gcloud compute disks list. gcloud</kbd> in a command-line tool in GCE.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">awsElasticBlockStore</h1>
                </header>
            
            <article>
                
<p><kbd>awsElasticBlockStore</kbd> volume mounts an <strong>Amazon Web Service Elastic Block Store</strong> (<strong>AWS EBS</strong>) volume. It's a service that provides persistent block storage for Amazon EC2. Just like the GCE persistent disk, we can provision it statically or dynamically.</p>
<p>To provision it statically, administrators have to create an EBS volume by the AWS console or AWS CLI beforehand. The following is an example of how to mount an existing EBS volume to the containers in a Deployment:</p>
<pre>// example of how we used pre-created EBS volume.<br/># cat 2-6-8_aws/static_mount.yaml<br/>kind: Deployment<br/>apiVersion: apps/v1<br/>metadata:<br/>  name: aws-ebs-deployment<br/>spec:<br/>  replicas: 2<br/>  selector:<br/>    matchLabels:<br/>      run: nginx<br/>  template:<br/>    metadata:<br/>      labels:<br/>        run: nginx<br/>    spec:<br/>      volumes:<br/><strong>        - name: aws-ebs</strong><br/><strong>          awsElasticBlockStore:</strong><br/><strong>            volumeID: &lt;ebs volume ID&gt;</strong><br/><strong>            fsType: ext4</strong><br/>      containers:<br/>      - name: aws-ebs-example<br/>        image: nginx<br/>        ports:<br/>          - containerPort: 80<br/>        volumeMounts:<br/>          - mountPath: /mount-path<br/>            name: aws-ebs</pre>
<p>To provision it dynamically, on the other hand, just like how we demonstrated in the GCE persistent disk, we first create a non-default storage class; you're free to use a default storage class as well. Here, our environment is provisioned by kops (<a href="https://github.com/kubernetes/kops">https://github.com/kubernetes/kops</a>; for more information, please refer to <a href="b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml">Chapter 6</a>, <em>Building Kubernetes on AWS</em>). The environment has been bound with the required IAM policies, such as <kbd>ec2:AttachVolume</kbd>, <kbd>ec2:CreateVolume</kbd>, <kbd>ec2:DetachVolume</kbd>, and <kbd>ec2:DeleteVolume</kbd>. If you provision it from scratch, be sure that you have required policies attaching to the masters:</p>
<pre>// declare a storage class<br/># cat 2-6-8_aws/storageclass.yaml<br/>kind: StorageClass<br/>apiVersion: storage.k8s.io/v1<br/>metadata:<br/>  name: example-ebs<br/><strong>provisioner: kubernetes.io/aws-ebs</strong><br/><strong>parameters:</strong><br/><strong>  type: io1</strong><br/><strong>  zones: us-east-1a</strong><br/><br/>// create storage class<br/># kubectl create -f storageclass.yaml<br/>storageclass "example-ebs" created<br/><br/>// check if example-ebs sc is created<br/># kubectl get sc<br/>NAME PROVISIONER<br/>default kubernetes.io/aws-ebs<br/>example-ebs kubernetes.io/aws-ebs<br/>gp2 (default) kubernetes.io/aws-ebs</pre>
<p>Next, we create a PVC with the storage class name we just created:</p>
<pre>// declare a PVC<br/># cat 2-6-8_aws/pvc.yaml<br/>apiVersion: v1<br/>kind: PersistentVolumeClaim<br/>metadata:<br/>  name: aws-example<br/>spec:<br/><strong>  accessModes:</strong><br/><strong>    - ReadWriteOnce</strong><br/><strong>  storageClassName: example-ebs</strong><br/><strong>  resources:</strong><br/><strong>    requests:</strong><br/><strong>      storage: 5Gi</strong><br/><br/>// create a PVC<br/># kubectl create -f pvc.yaml<br/>persistentvolumeclaim "aws-example" created<br/><br/>// check if PVC has been created<br/># kubectl get pvc<br/>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE<br/>aws-example Bound pvc-d1cddc08-ee31-11e7-8582-022bb4c3719e 5Gi RWO example-ebs 5s</pre>
<p>When Kubernetes receives the request of <kbd>PersistentVolumeClaim</kbd>, it'll try to allocate a new <kbd>PersistentVolume</kbd>, or bind to an existing PV, if possible:</p>
<pre>// check if a PV is created by a PVC.<br/># kubectl get pv<br/>NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE<br/>pvc-d1cddc08-ee31-11e7-8582-022bb4c3719e 5Gi RWO Delete Bound default/aws-example example-ebs 36m</pre>
<p>We can check the corresponding PV in the AWS console, as well.</p>
<p>At the end, we create a Deployment with this volume by specifying <kbd>persistentVolumeClaim</kbd> in the spec:</p>
<pre>// create a deployment<br/># cat 2-6-8_aws/deployment.yaml<br/>kind: Deployment<br/>apiVersion: apps/v1<br/>metadata:<br/>  name: aws-ebs-deployment<br/>spec:<br/>  replicas: 2<br/>  selector:<br/>    matchLabels:<br/>      run: nginx<br/>  template:<br/>    metadata:<br/>      labels:<br/>        run: nginx<br/>    spec:<br/>      volumes:<br/>      - name: aws-ebs<br/>      <strong>  persistentVolumeClaim:</strong><br/><strong>          claimName: aws-example</strong><br/>      containers:<br/>      - name: aws-ebs-example<br/>        image: nginx<br/>        ports:<br/>          - containerPort: 80<br/>        volumeMounts:<br/>          - mountPath: /mount-path<br/>            name: aws-ebs</pre>
<p>By specifying <kbd>claimName</kbd> as <kbd>aws-example</kbd>, it'll then use the EBS volume we just create by PVC, which is requested to AWS dynamically. If we take a look at the Pod description with <kbd>kubectl describe pod &lt;pod_name&gt;</kbd>, we can see the details of the volumes:</p>
<pre>// kubectl describe pod &lt;pod_name&gt;<br/># kubectl describe pod aws-ebs-deployment-68bdc6f546-246s7<br/>Containers:<br/>  aws-ebs-example:<br/>    ...<br/>    Mounts:<br/>      <strong>/mount-path from aws-ebs (rw)</strong><br/>Volumes:<br/>  aws-ebs:<br/>    Type: AWSElasticBlockStore (a Persistent Disk resource in AWS)<br/>    VolumeID: vol-0fccc3b0af8c17727<br/>    FSType: ext4<br/>    Partition: 0<br/>    ReadOnly: false<br/>...</pre>
<p>EBS volume <kbd>vol-0fccc3b0af8c17727</kbd> is mounted under <kbd>/mount-path</kbd> inside the container.</p>
<div>
<p>If the volume was dynamically provisioned, the default reclaim policy is set to <kbd>delete</kbd>. Set it to <kbd>retain</kbd> if you want to keep them, even if a PVC is deleted.</p>
</div>
<div class="packt_infobox"><strong>The StorageObjectInUseProtection admission controller<br/></strong><br/>
A PVC might be deleted accidentally by user even if it's used by a Pod. In Kubernetes v1.10, a new admission controller is added to prevent this from happening.<kbd> kubernetes.io/pv-protection</kbd> or <kbd>kubernetes.io/pvc-protection</kbd> finalizer will be added into PV or PVC by <kbd>StorageObjectInUseProtection</kbd> admission controller. Then when object deletion request is sent, admission controller will do pre-delete check and see if there is any Pod are using it. This will prevent data loss.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>Volumes can be mounted on the Pods by declaring in Pods or ReplicaSet spec. Check out the following recipes to jog your memory:</p>
<ul>
<li><em>Working with Pods</em> section in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml">Chapter 2</a>, <em>Walking through Kubernetes Concepts</em></li>
<li><em>Working with replica sets</em> <span>section </span><span>in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml">Chapter 2</a>, </span><em><span>Walking through Kubernetes Concepts</span></em></li>
<li><em>Working with Secrets</em> <span>section </span><span>in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml">Chapter 2</a>, </span><em><span>Walking through Kubernetes Concepts</span></em></li>
<li><em>Setting resource in nodes</em> <span>section </span>in <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml">Chapter 8</a>, <em>Advanced Cluster Administration</em></li>
<li><em>Authentication and authorization</em> <span>section </span><span>in <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml">Chapter 8</a>, <em>Advanced Cluster Administration</em></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with Secrets</h1>
                </header>
            
            <article>
                
<p>Kubernetes Secrets manage information in key-value formats with the value encoded. It can be a password, access key, or token. With Secrets, users don't have to expose sensitive data in the configuration file. Secrets can reduce the risk of credential leaks and make our resource configurations more organized.</p>
<p>Currently, there are three types of Secrets:</p>
<ul>
<li>Generic/Opaque: <a href="https://en.wikipedia.org/wiki/Opaque_data_type">https://en.wikipedia.org/wiki/Opaque_data_type</a></li>
<li>Docker registry</li>
<li>TLS</li>
</ul>
<p>Generic/Opaque is the default type that we're using in our application. Docker registry is used to store the credential of a private Docker registry. TLS Secret is used to store the CA certificate bundle for cluster administration.</p>
<div>
<p>Kubernetes creates built-in Secrets for the credentials that using to access API server.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Before using Secrets, we have to keep in mind that Secret should be always created before dependent Pods, so dependent Pods can reference it properly. In addition, Secrets have a 1 MB size limitation. It works properly for defining a bunch of information in a single Secret. However, Secret is not designed for storing large amounts of data. For configuration data, consider using <kbd>ConfigMaps</kbd>. For large amounts of non-sensitive data, consider using volumes instead.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In the following example, we'll walk through how to create a Generic/Opaque Secret and use it in your Pods by assuming that we have an access token that needs to be used inside a Pod.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Secret</h1>
                </header>
            
            <article>
                
<p>There are two ways to create a Secret. The first one is with <kbd>kubectl create secret</kbd> in the command line, and the other one is with direct resource creation in the configuration file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with kubectl create command line</h1>
                </header>
            
            <article>
                
<p>By using <kbd>kubectl create secret</kbd> command line, you can create a Secret from a file, directory, or literal value. With this method, you don't need to encode the Secret by yourself. Kubernetes will do that for you:</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From a file</h1>
                </header>
            
            <article>
                
<ol>
<li>If a file is the source of Secret, we'll have to create a text file which contains our sensitive data first:</li>
</ol>
<pre style="padding-left: 90px">// assume we have a sensitive credential named access token.<br/># cat 2-7-1_access-token<br/>9S!g0U61699r</pre>
<ol start="2">
<li>Next, we could use <kbd>kubectl create secret</kbd> in the command line to create the Secret. The syntax is:</li>
</ol>
<pre style="padding-left: 90px">Kubectl create secret &lt;secret-type&gt; --from-file &lt;file1&gt; (--from-file &lt;file2&gt; ...)</pre>
<ol start="3">
<li>In our case, we use generic Secret type, since the access token is neither the Docker registry image pull Secrets nor TLS information:</li>
</ol>
<pre style="padding-left: 90px"># kubectl create secret generic access-token --from-file 2-7-1_access-token<br/>secret "access-token" created</pre>
<ol start="4">
<li>You can check the detailed Secret information with the <kbd>kubectl get secret</kbd> command:</li>
</ol>
<pre style="padding-left: 90px">// get the detailed information for a Secret.<br/># kubectl get secret access-token -o yaml<br/>apiVersion: v1<br/>data:<br/>  2-7-1_access-token: OVMhZzBVNjE2OTlyCg==<br/>kind: Secret<br/>metadata:<br/>  creationTimestamp: 2018-01-01T20:26:24Z<br/>  name: access-token<br/>  namespace: default<br/>  resourceVersion: "127883"<br/>  selfLink: /api/v1/namespaces/default/secrets/access-token<br/>  uid: 0987ec7d-ef32-11e7-ac53-080027ac331c<br/>type: Opaque</pre>
<ol start="5">
<li>You can use the <kbd>base64</kbd> command (<a href="https://linux.die.net/man/1/base64">https://linux.die.net/man/1/base64</a>) in Linux to decode the encoded Secret:</li>
</ol>
<pre style="padding-left: 120px">// decode encoded Secret<br/># echo "OVMhZzBVNjE2OTlyCg==" | base64 --decode<br/>9S!g0U61699r</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From a directory</h1>
                </header>
            
            <article>
                
<p>Creating a Secret from a directory is similar to creating from a file, using the same command, but with <kbd>directory</kbd>. Kubernetes will iterate all the files inside that directory and create a Secret for you:</p>
<pre>// show directory structure<br/># tree<br/>.<br/>├── 2-7-1_access-token-dir<br/>│ └── 2-7-1_access-token<br/><br/>// create Secrets from a directory<br/># kubectl create secret generic access-token --from-file 2-7-1_access-token-dir/<br/>secret "access-token" created</pre>
<p>You can check the Secret with the <kbd>kubectl get secret access-token -o yaml</kbd> command again and see if they're identical to the ones from the file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From a literal value</h1>
                </header>
            
            <article>
                
<p>Kubernetes supports creating a Secret with a single command line, as well:</p>
<pre>// create a Secret via plain text in command line<br/># kubectl create secret generic access-token --from-literal=2-7-1_access-token=9S\!g0U61699r<br/>secret "access-token" created</pre>
<p>Then we can use the <kbd>get secret</kbd> command to check if they're identical to the previous method:</p>
<pre>// check the details of a Secret<br/># kubectl get secret access-token -o yaml<br/>apiVersion: v1<br/>data:<br/>  2-7-1_access-token: OVMhZzBVNjE2OTlyCg==<br/>kind: Secret<br/>metadata:<br/>  creationTimestamp: 2018-01-01T21:44:32Z<br/>  name: access-token<br/>  ...<br/>type: Opaque</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Via configuration file</h1>
                </header>
            
            <article>
                
<p>A Secret can also be created directly through the configuration file; however, you'll have to encode the Secret manually. Just use the kind of Secret:</p>
<pre>// encode Secret manually<br/># echo '9S!g0U61699r' | base64<br/>OVMhZzBVNjE2OTlyCg==<br/><br/>// create a Secret via configuration file, put encoded Secret into the file<br/># cat 2-7-1_secret.yaml<br/>apiVersion: v1<br/>kind: Secret<br/>metadata:<br/>  name: access-token<br/>type: Opaque<br/>data:<br/>  2-7-1_access-token: OVMhZzBVNjE2OTlyCg==<br/><br/>// create the resource<br/># kubectl create -f 2-7-1_secret.yaml<br/>secret "access-token" created<br/><br/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Secrets in Pods</h1>
                </header>
            
            <article>
                
<p>To use Secrets inside Pods, we can choose to expose them in environment variables or mount the Secrets as volumes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">By environment variables</h1>
                </header>
            
            <article>
                
<p>In terms of accessing Secrets inside a Pod, add <kbd>env section</kbd> inside the container spec as follows:</p>
<pre>// using access-token Secret inside a Pod<br/># cat 2-7-2_env.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: secret-example-env<br/>spec:<br/>  containers:<br/>  - name: ubuntu<br/>    image: ubuntu<br/>    command: ["/bin/sh", "-c", "while : ;do echo $ACCESS_TOKEN; sleep 10; done"]<br/>    <strong>env:</strong><br/><strong>        - name: ACCESS_TOKEN</strong><br/><strong>          valueFrom:</strong><br/><strong>            secretKeyRef:</strong><br/><strong>              name: access-token</strong><br/><strong>              key: 2-7-1_access-token</strong><br/><br/>// create a pod<br/># kubectl create -f 2-7-2_env.yaml<br/>pod "secret-example-env" created</pre>
<p>In the preceding example, we expose <kbd>2-7-1_access-token</kbd> key in access-token Secret as <kbd>ACCESS_TOKEN</kbd> environment variable, and print it out through a while infinite loop. Check the <kbd>stdout via kubectl</kbd> log command:</p>
<pre>// check stdout logs<br/># kubectl logs -f secret-example-env<br/>9S!g0U61699r</pre>
<p>Note that the environment variable was exposed during Pod creation. If a new value of Secret is pushed, you'll have to re-launch/rolling-update a Pod or Deployment to reflect that.</p>
<p>If we describe the <kbd>secret-example-env</kbd> Pod, we can see that an environment variable was set to a Secret:</p>
<pre># kubectl describe pods secret-example-env<br/>Name: secret-example-env<br/>...<br/>Environment:<br/>      ACCESS_TOKEN: &lt;set to the key '2-7-1_access-token' in secret 'access-token'&gt; </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">By volumes</h1>
                </header>
            
            <article>
                
<p>A Secret can be also mounted as volume by using the Secret type of the volume. The following is an example of how to use it:</p>
<pre>// example of using Secret volume<br/># cat 2-7-3_volumes.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: secret-example-volume<br/>spec:<br/>  containers:<br/>  - name: ubuntu<br/>    image: ubuntu<br/>    command: ["/bin/sh", "-c", "while : ;do cat /secret/token; sleep 10; done"]<br/>    volumeMounts:<br/>      - name: secret-volume<br/>        mountPath: /secret<br/>        readOnly: true<br/>  volumes:<br/>    - name: secret-volume<br/>    <strong>  secret:</strong><br/><strong>        secretName: access-token</strong><br/><strong>        items:</strong><br/><strong>        - key: 2-7-1_access-token</strong><br/><strong>          path: token</strong><br/><br/>// create the Pod<br/>kubectl create -f 2-7-3_volumes.yaml<br/>pod "secret-example-volume" created</pre>
<p>The preceding example will mount <kbd>secret-volume</kbd> into the <kbd>/secret mount</kbd> point inside the Pod. <kbd>/secret</kbd> will contain a file with the name token, which contains our access token. If we check the Pod details, it'll show that we mounted a read-only Secret volume:</p>
<pre>// check the Pod details<br/># kubectl describe pods secret-example-volume<br/>Name: secret-example-volume<br/>...<br/>Containers:<br/>  ubuntu:<br/>    ...<br/>    Mounts:<br/>      /secret from secret-volume (ro)<br/>      ...<br/>Volumes:<br/>  secret-volume:<br/>    Type: Secret (a volume populated by a Secret)<br/>    SecretName: access-token<br/>    Optional: false<br/>...</pre>
<p>If we check the <kbd>stdout</kbd>, it'll show the Pod can properly retrieve the expected value:</p>
<pre># kubectl logs -f secret-example-volume<br/>9S!g0U61699r</pre>
<p>The same as with the environment variable, the files in the mounted volume are created upon Pod creation time. It won't change dynamically when the Secret value is updated after the Pod creation time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deleting a Secret                                   </h1>
                </header>
            
            <article>
                
<p>To delete a Secret, simply use the <kbd>kubectl delete secret</kbd> command:</p>
<pre># kubectl delete secret access-token<br/>secret "access-token" deleted</pre>
<p>If a Secret is deleted when a Secret volume is attached, it'll show an error message whenever the volume reference disappears:</p>
<pre># kubectl describe pods secret-example-volume<br/>...<br/>Events:<br/>  Warning FailedMount 53s (x8 over 1m) kubelet, minikube MountVolume.SetUp failed for volume "secret-volume" : secrets "access-token" not found</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In order to reduce the risk of leaking the Secrets' content, Secret is not landed to the disk. Instead, kubelet creates a <kbd>tmpfs</kbd> filesystem on the node to store the Secret. The Kubernetes API server pushes the Secret to the node on which the demanded container is running. The data will be flashed when the container is destroyed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Secrets hold small amounts of sensitive data. For application configuration, consider using <kbd>ConfigMaps</kbd> to hold non-sensitive information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using ConfigMaps</h1>
                </header>
            
            <article>
                
<p>Here is an example of using <kbd>ConfigMaps</kbd>:</p>
<pre># cat configmap/2-7-4_configmap.yaml<br/>apiVersion: v1<br/>kind: ConfigMap<br/>metadata:<br/>  name: config-example<br/>data:<br/>  app.properties: |<br/>    name=kubernetes-cookbook<br/>    port=443<br/><br/>// create configmap<br/># kubectl create -f configmap/2-7-4_configmap.yaml<br/>configmap "config-example" created</pre>
<p>Similar to Secret, <kbd>ConfigMaps</kbd> can be retrieved with environment variables or volumes:</p>
<pre># cat configmap/2-7-4_env.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: configmap-env<br/>spec:<br/>  containers:<br/>    - name: configmap<br/>      image: ubuntu<br/>      command: ["/bin/sh", "-c", "while : ;do echo $APP_NAME; sleep 10; done"]<br/>     <strong> env:</strong><br/><strong>        - name: APP_NAME</strong><br/><strong>          valueFrom:</strong><br/><strong>            configMapKeyRef:</strong><br/><strong>              name: config-example</strong><br/><strong>              key: app.properties</strong><br/><br/>// create the pod<br/>#kubectl create -f configmap/2-7-4_env.yaml<br/>pod "configmap-env" created</pre>
<p>Alternatively, you can use <kbd>ConfigMaps</kbd> volume to retrieve the configuration information:</p>
<pre>// using configmap in a pod<br/># cat configmap/2-7-4_volumes.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: configmap-volume<br/>spec:<br/>  containers:<br/>    - name: configmap<br/>      image: ubuntu<br/>      command: ["/bin/sh", "-c", "while : ;do cat /src/app/config/app.properties; sleep 10; done"]<br/>      volumeMounts:<br/>      - name: config-volume<br/>        mountPath: /src/app/config<br/>  volumes:<br/>    - name: config-volume<br/>      <strong>configMap:</strong><br/><strong>        name: config-example</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mounting Secrets and ConfigMap in the same volume</h1>
                </header>
            
            <article>
                
<p>Projected volume is a way to group multiple volume sources into the same mount point. Currently, it supports Secrets, <kbd>ConfigMap</kbd>, and <kbd>downwardAPI</kbd>.</p>
<p>The following is an example of how we group the examples of Secrets and <kbd>ConfigMaps</kbd> that we used in this chapter:</p>
<pre>// using projected volume<br/># cat 2-7-5_projected_volume.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: projected-volume-example<br/>spec:<br/>  containers:<br/>  - name: container-tes<br/>    image: ubuntu<br/>    command: ["/bin/sh", "-c", "while : ;do cat /projected-volume/configmap &amp;&amp; cat /projected-volume/token; sleep 10; done"]<br/>    volumeMounts:<br/>    - name: projected-volume<br/>      mountPath: "/projected-volume"<br/>  volumes:<br/>  - name: projected-volume<br/>  <strong>  projected:</strong><br/><strong>      sources:</strong><br/>      - secret:<br/>          name: access-token<br/>          items:<br/>            - key: 2-7-1_access-token<br/>              path: token<br/>      - configMap:<br/>          name: config-example<br/>          items:<br/>            - key: app.properties<br/>              path: configmap<br/><br/>// create projected volume<br/># kubectl create -f 2-7-5_projected_volume.yaml<br/>pod "projected-volume-example" created</pre>
<p>Let's check <kbd>stdout</kbd> to see if it works properly:</p>
<pre># kubectl logs -f projected-volume-example<br/>name=kubernetes-cookbook<br/>port=443<br/>9S!g0U61699r</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Working with Volumes</em> <span>section</span><span> </span>in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml"><em>Chapter 2</em></a><em>, Walking through Kubernetes Concepts</em></li>
<li>The <em>Working with configuration files </em> <span>section</span> in <em><a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml">Chapter 3</a>, Playing with Containers</em></li>
<li><em>The Moving monolithic to microservices</em> and <em>Working with the private Docker registry </em> <span>sections</span> in <em><a href="669edaf0-c274-48fa-81d8-61150fa36df5.xhtml">Chapter 5</a>, Building Continuous Delivery Pipeline</em></li>
<li>The <em>Advanced settings in kubeconfig </em> <span>section</span> in <em><a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml">Chapter 7</a>, Building Kubernetes on GCP</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with names</h1>
                </header>
            
            <article>
                
<div class="page">
<div class="layoutArea">
<div class="column">
<p><span>When you create any Kubernetes object, such as a Pod, Deployment, and Service, you can assign a name to it. The names in Kubernetes are spatially unique, which means you cannot assign the same name in the Pods.</span></p>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="page">
<p><span>Kubernetes allows us to assign a name with the following restrictions:</span></p>
<ul>
<li><span>Up to 253 characters</span></li>
<li><span>Lowercase of alphabet and numeric characters</span></li>
<li><span>May contain special characters in the middle, but only dashs (-) and dots (.)</span></li>
</ul>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>For assigning a name to the Pod, follow the following steps:</p>
<ol>
<li><span>The following example is the Pod YAML configuration that assigns the Pod name as</span> <kbd>my-pod</kbd><span> to the container name as</span> <kbd>my-container</kbd><span>; you can successfully create it as follows:</span></li>
</ol>
<div class="page">
<div class="layoutArea">
<div class="column">
<pre style="padding-left: 90px"><span># cat my-pod.yaml
apiVersion: v1
kind: Pod
metadata:
</span><span>  name: my-pod
spec:
</span><span>  containers:
  - name: <strong>my-container</strong><br/></span><span>    image: nginx <br/><br/><br/></span><span># kubectl create -f my-pod.yaml<br/></span><span>pod "my-pod" created<br/></span><span><br/><br/># kubectl get pods</span><span><br/></span><span>NAME      READY     STATUS    RESTARTS   AGE<br/></span><span>my-pod    0/1       Running   0          4s</span></pre></div>
</div>
</div>
<ol start="2">
<li>You can use the <kbd>kubectl describe</kbd> command to see the container named <kbd>my-container</kbd> as follows:</li>
</ol>
<pre><span>$ kubectl describe pod my-pod<br/></span><span>Name:         my-pod<br/></span><span>Namespace:    default<br/></span><span>Node:         minikube/192.168.64.12<br/></span><span>Start Time:   Sat, 16 Dec 2017 10:53:38 -0800<br/></span><span>Labels:       &lt;none&gt;<br/></span><span>Annotations:  &lt;none&gt;<br/></span><span>Status:       Running<br/></span><span>IP:           172.17.0.3<br/></span><span>Containers:<br/></span><span><strong>  my-container</strong>:<br/></span><span>    Container ID:   docker://fcf36d0a96a49c5a08eb6de1ef27ca761b4ca1c6b4a3a4312df836cb8e0a5304<br/></span><span>    Image:          nginx<br/></span><span>    Image ID:       docker-pullable://nginx@sha256:2ffc60a51c9d658594b63ef5acfac9d92f4e1550f633a3a16d898925c4e7f5a7<br/></span><span>    Port:           &lt;none&gt;<br/></span><span>    State:          Running<br/></span><span>      Started:      Sat, 16 Dec 2017 10:54:43 -0800<br/></span><span>    Ready:          True<br/></span><span>    Restart Count:  0<br/></span><span>    Environment:    &lt;none&gt;<br/></span><span>    Mounts:<br/></span><span>      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lmd62 (ro)<br/></span><span>Conditions:<br/></span><span>  Type           Status<br/></span><span>  Initialized    True <br/></span><span>  Ready          True <br/></span><span>  PodScheduled   True <br/></span><span>Volumes:<br/></span><span>  default-token-lmd62:<br/></span><span>    Type:        Secret (a volume populated by a Secret)<br/></span><span>    SecretName:  default-token-lmd62<br/></span><span>    Optional:    false<br/></span><span>QoS Class:       BestEffort<br/></span><span>Node-Selectors:  &lt;none&gt;<br/></span><span>Tolerations:     &lt;none&gt;<br/></span><span>Events:<br/></span><span>  Type    Reason                 Age   From               Message<br/></span><span>  ----    ------                 ----  ----               -------<br/></span><span>  Normal  Scheduled              1m    default-scheduler  Successfully assigned my-pod to minikube<br/></span><span>  Normal  SuccessfulMountVolume  1m    kubelet, minikube  MountVolume.SetUp succeeded for volume "default-token-lmd62"<br/></span><span>  Normal  Pulling                1m    kubelet, minikube  pulling image "nginx"<br/></span><span>  Normal  Pulled                 50s   kubelet, minikube  Successfully pulled image "nginx"<br/></span><span>  Normal  Created                50s   kubelet, minikube  Created container<br/></span><span>  Normal  Started                50s   kubelet, minikube  Started container</span></pre>
<div class="page">
<div class="layoutArea">
<ol start="3">
<li>On the other hand, the following example contains two containers, but assigns the same name, <kbd>my-container</kbd>; therefore, the <kbd>kubectl create</kbd> command returns an error and can't create the Pod:</li>
</ol>
<div class="page">
<div class="layoutArea">
<div class="column">
<pre style="padding-left: 90px"><span>//delete previous Pod<br/></span><span>$ kubectl delete pod --all<br/></span><span>pod "my-pod" deleted<br/><br/></span><span><br/>$ cat duplicate.yaml <br/></span><span>apiVersion: v1<br/></span><span>kind: Pod<br/></span><span>metadata:<br/></span><span>  name: my-pod<br/></span><span>spec:<br/></span><span>  containers:<br/></span><span>  - name: <strong>my-container</strong><br/></span><span>    image: nginx<br/></span><span>  - name: <strong>my-container</strong><br/></span><span>    image: centos<br/></span><span>    command: ["/bin/sh", "-c", "while : ;do curl http://localhost:80/; </span><span>sleep 3; done"]<br/><br/><br/></span><span>$ kubectl create -f duplicate.yaml <br/></span><span>The Pod "my-pod" is invalid: spec.containers[1].name: Duplicate value: "my-container"</span></pre></div>
</div>
</div>
<div class="page">
<div class="layoutArea">
<div class="column packt_tip"><span>You can add the</span> <kbd><span>--validate</span></kbd> fl<span>ag.<br/></span> <span>For example, the command</span> <kbd><span>kubectl create -f duplicate.yaml --validate</span></kbd><span> uses a schema to validate the input before sending it.</span></div>
</div>
</div>
</div>
</div>
<div class="page">
<div class="layoutArea">
<div class="column">
<p><span>In another example, the YAML contains a ReplicationController and Service, both of which are using the same name,</span> <kbd>my-nginx</kbd><span>, but it is successfully created because the Deployment and Service are different objects:</span></p>
<pre>$ cat my-nginx.yaml <br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: <strong>my-nginx</strong><br/>spec:<br/>  replicas: 3<br/>  selector:<br/>    matchLabels:<br/>      run: my-label<br/>  template:<br/>    metadata:<br/>      labels:<br/>        run: my-label<br/>    spec:<br/>      containers:<br/>      - name: my-container<br/>        image: nginx<br/>        ports:<br/>        - containerPort: 80<br/>---<br/>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: <strong>my-nginx</strong><br/>spec:<br/>  ports:<br/>    - protocol: TCP<br/>      port: 80<br/>  type: NodePort<br/>  selector:<br/>    run: my-label<br/><br/><br/>//create Deployment and Service<br/>$ kubectl create -f my-nginx.yaml <br/>deployment.apps "my-nginx" created<br/>service "my-nginx" created<br/><br/><br/>//Deployment "my-nginx" is created<br/>$ kubectl get deploy<br/><span>NAME <span class="Apple-converted-space">      </span>DESIRED <span class="Apple-converted-space">  </span>CURRENT <span class="Apple-converted-space">  </span>UP-TO-DATE <span class="Apple-converted-space">  </span>AVAILABLE <span class="Apple-converted-space">  </span>AGE<br/></span><span>my-nginx <span class="Apple-converted-space">  </span>3 <span class="Apple-converted-space">        </span>3 <span class="Apple-converted-space">        </span>3<span class="Apple-converted-space">            </span>3 <span class="Apple-converted-space">          </span>1m</span><br/><br/>//Service "my-nginx" is also created<br/>$ kubectl get svc<br/>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE<br/>kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 13d<br/>my-nginx NodePort 10.0.0.246 &lt;none&gt; 80:31168/TCP 1m</pre></div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>A name is just a unique identifier, and all naming conventions are good; however, it is recommended</span> <span>to look up and identify the container image. For example:</span></p>
<ul>
<li><span><kbd>memcached-pod1</kbd><br/></span></li>
<li><span><kbd>haproxy.us-west</kbd><br/></span></li>
<li><kbd><span>my-project1.mysql</span></kbd></li>
</ul>
<p><span>On the other hand, the following examples do not work because of Kubernetes restrictions:</span></p>
<ul>
<li><span><kbd>Memcache-pod1</kbd> (contains uppercase)</span></li>
<li><span><kbd>haproxy.us_west</kbd> (contains underscore)</span></li>
<li><span><kbd>my-project1.mysql.</kbd> (dot in the last)</span></li>
</ul>
<p><span>Note that</span> <span>Kubernetes supports a label that allows assigning a <kbd>key=value</kbd> style identifier. It also</span> <span>allows duplication. Therefore, if you want to assign something like the following information, use a label instead:</span></p>
<ul>
<li><span>Environment (for example: staging, production)</span></li>
<li><span>Version (for example: v1.2)<br/></span></li>
<li><span>Application role (for example: frontend, worker)</span></li>
</ul>
<p><span>In addition, Kubernetes also supports names that have different Namespaces. This means that you can use the same name in different Namespaces (for example: <kbd>nginx</kbd>). Therefore, if you want to assign just an application name, use Namespaces instead.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p><span>This section from the chapter described how to assign and find the name of objects. This is just a basic</span> <span>methodology, but Kubernetes has more powerful naming tools, such as Namespace and selectors, to manage clusters:</span></p>
<ul>
<li><em><span>Working with Pods<br/></span></em></li>
<li><em>Deployment API</em></li>
<li><em><span>Working with Services<br/></span></em></li>
<li><em><span>Working with Namespaces<br/></span></em></li>
<li><em><span>Working with labels and selectors</span></em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with Namespaces</h1>
                </header>
            
            <article>
                
<p>In a Kubernetes cluster, the name of a resource is a unique identifier within a Namespace. Using a Kubernetes Namespace could separate user spaces for different environments in the same cluster. It gives you the flexibility of creating an isolated environment and partitioning resources to different projects and teams. You may consider Namespace as a virtual cluster. Pods, Services, and Deployments are contained in a certain Namespace. Some low-level resources, such as nodes and <kbd>persistentVolumes</kbd>, do not belong to any Namespace.</p>
<p>Before we dig into the resource Namespace, let's understand <kbd>kubeconfig</kbd> and some keywords first:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-655 image-border" src="assets/b1dc5384-97c8-4a7b-af9b-ddbdc62e60c0.png" style="width:25.42em;height:17.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The relationship of kubeconfig components</div>
<p><kbd>kubeconfig</kbd> is used to call the file which configures the access permission of Kubernetes clusters. As the original configuration of the system, Kubernetes takes <kbd>$HOME/.kube/config</kbd> as a <kbd>kubeconfig</kbd> file. Some concepts that are illustrated by the preceding diagram are as follows:</p>
<ul>
<li><strong>kubeconfig defines user, cluster, and context</strong>: <kbd>kubeconfig</kbd> lists multiple users for defining authentication, and multiple clusters for indicating the Kubernetes API server. Also, the context in <kbd>kubeconfig</kbd> is the combination of a user and a cluster: accessing a certain Kubernetes cluster with what kind of authentication.</li>
<li><strong>Users and clusters are sharable between contexts</strong>: In the previous diagram, both <strong>Context 1</strong> and <strong>Context 3</strong> take <strong>User</strong> <strong>1</strong> as their user content. However, each context can only have a single user and single cluster definition.</li>
<li><strong>Namespace can be attached to context</strong>: Every context can be assigned to an existing Namespace. If there are none, like <strong>Context 3</strong>, it is along with the default Namespace, named <kbd>default</kbd>, as well.</li>
<li><strong>The current context is the default environment for client</strong>: We may have several contexts in <kbd>kubeconfig</kbd>, but only one for the current context. The current context and the Namespace attached on it will construct the default computing environment for users.</li>
</ul>
<p>Now you will get the idea that, as Namespace works with <kbd>kubeconfig</kbd>, users can easily switch default resources for usage by switching the current context in <kbd>kubeconfig</kbd>. Nevertheless, users can still start any resource in a different Namespace with a specified one. In this recipe, you will learn how to create your own Namespace and how to work with it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>By default, Kubernetes has created a Namespace named <kbd>default</kbd>. All the objects created without specifying Namespaces will be put into the <kbd>default</kbd> Namespace. Kubernetes will also create another initial Namespace called <kbd>kube-system</kbd> for locating Kubernetes system objects, such as an add-on or overlay network. Try to list all the Namespaces:</p>
<pre>// check all Namespaces, "ns" is the resource abbreviation of Namespace<br/>$ kubectl get ns<br/>NAME          STATUS    AGE<br/>default       Active    15d<br/>kube-public   Active    15d<br/>kube-system   Active    15d</pre>
<p>You may find an additional Namespace, <kbd>kube-public</kbd>, listed at the initial stage. It is designed for presenting some public configurations for even users without permission to access the Kubernetes system. Both of the provisioning tools, minikube and kubeadm, will create it while booting the system up.</p>
<p>The name of a Namespace must be a DNS label and follow the following rules:</p>
<ul>
<li>At most, 63 characters</li>
<li><span>Matching regex [a-z0-9]([-a-z0-9]*[a-z0-9])</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this section, we will demonstrate how to create a Namespace, change the default Namespace, and delete the Namespace.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Namespace</h1>
                </header>
            
            <article>
                
<p>For creating a Namespace, following are the steps:</p>
<ol>
<li>After deciding on our desired name for Namespace, let's create it with a configuration file:</li>
</ol>
<pre style="padding-left: 90px">$ cat my-first-namespace.yaml<br/>apiVersion: v1<br/>kind: Namespace<br/>metadata:<br/>  name: my-namespace<br/><br/>// create the resource by subcommand "create"<br/>$ kubectl create -f my-first-namespace.yaml<br/>namespace "my-namespace" created<br/>// list the namespaces again<br/>$ kubectl get ns<br/>NAME           STATUS    AGE<br/>default        Active    16d<br/>kube-public    Active    16d<br/>kube-system    Active    16d<br/>my-namespace   Active    6s</pre>
<ol start="2">
<li>You can now see that we have an additional namespace called <kbd>my-namespace</kbd>. Next, let's run a Kubernetes Deployment in this new Namespace:</li>
</ol>
<pre style="padding-left: 90px">// run a Deployment with a flag specifying Namespace<br/>$ kubectl run my-nginx --image=nginx --namespace=my-namespace<br/>deployment.apps "my-nginx" created</pre>
<ol start="3">
<li>While trying to check the newly created resource, we cannot easily find them as usual:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl get deployment<br/>No resources found.</pre>
<ol start="4">
<li>Instead, the Deployment is shown with a flag related to the Namespace:</li>
</ol>
<pre style="padding-left: 90px">// list any Deployment in all Namespaces<br/>$ kubectl get deployment --all-namespaces<br/>NAMESPACE      NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE<br/>kube-system    calico-kube-controllers    1         1         1            1           16d<br/>kube-system    calico-policy-controller   0         0         0            0           16d<br/>kube-system    kube-dns                   1         1         1            1           16d<br/>my-namespace   my-nginx                   1         1         1            1           1m<br/><br/>// get Deployments from my-namespace<br/>$ kubectl get deployment --namespace=my-namespace<br/>NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE<br/>my-nginx   1         1         1            1           1m</pre>
<p>Now you can find the resource that was just created.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Changing the default Namespace</h1>
                </header>
            
            <article>
                
<p>As in the previous introduction, we can change the default Namespace by switching the current context in <kbd>kubeconfig</kbd> to another one:</p>
<ol>
<li>First, we may check the current context with the subcommand <kbd>config</kbd>:</li>
</ol>
<pre style="padding-left: 90px">// check the current context in kubeconfig<br/>$ kubectl config current-context<br/>kubernetes-admin@kubernetes</pre>
<p style="padding-left: 60px">You may feel unfamiliar with the output when checking the current context. The value of the preceding current context is defined and created by <kbd>kubeadm</kbd>. You could get <kbd>minikube</kbd> shown on screen if you leveraged <kbd>minikube</kbd> as your Kubernetes system management tool.</p>
<ol start="2">
<li>No matter what you got from checking the current context in <kbd>kubeconfig</kbd>, use the subcommand <kbd>config set-context</kbd> to create a new context:</li>
</ol>
<pre style="padding-left: 90px">// create a new context called "my-context"<br/>// the new context is going to follow the cluster and the user of current context, but attached with new Namespace<br/>//This is for kubeadm environment<br/>$ kubectl config set-context my-context --namespace=my-namespace --cluster=kubernetes --user=kubernetes-admin<br/>Context "my-context" created.</pre>
<ol start="3">
<li>The preceding command is based on <kbd>kubeadm</kbd> managed Kubernetes; you may fire a similar one for <kbd>minikube</kbd>, with the names of the default cluster and user in <kbd>kubeconfig</kbd>:</li>
</ol>
<pre style="padding-left: 90px">// for minikube environemt<br/>$ kubectl config set-context my-context --namespace=my-namespace --cluster=minikube --user=minikube</pre>
<ol start="4">
<li>Next, check <kbd>kubeconfig</kbd> to verify the changes:</li>
</ol>
<pre style="padding-left: 90px">//check kubectlconfig for the new context<br/>$ kubectl config view<br/>apiVersion: v1<br/>clusters:<br/>- cluster:<br/>    certificate-authority-data: REDACTED<br/>    server: https://192.168.122.101:6443<br/>  name: kubernetes<br/>contexts:<br/>- context:<br/>    cluster: kubernetes<br/>    user: kubernetes-admin<br/>  name: kubernetes-admin@kubernetes<br/><strong>- context:<br/></strong><strong>    cluster: kubernetes<br/></strong><strong>    namespace: my-namespace<br/></strong><strong>    user: kubernetes-admin<br/></strong><strong>  name: my-context<br/></strong>current-context: kubernetes-admin@kubernetes<br/>kind: Config<br/>preferences: {}<br/>users:<br/>- name: kubernetes-admin<br/>  user:<br/>    client-certificate-data: REDACTED<br/>    client-key-data: REDACTED</pre>
<p style="padding-left: 60px">When checking the configuration of <kbd>kubeconfig</kbd>, in the section of contexts, you can find a context named exactly as what we defined and which also takes our newly created Namespace.</p>
<ol start="5">
<li>Fire the following command to switch to using the new context:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl config use-context my-context<br/>Switched to context "my-context".<br/>// check current context<br/>$ kubectl config current-context<br/>my-context</pre>
<p style="padding-left: 60px">Now the current context is our customized one, which is along with the Namespace <kbd>my-namespace</kbd>.</p>
<ol start="6">
<li>Since the default Namespace is changed to <kbd>my-namespace</kbd>, it is possible that we can get the Deployment without specifying the Namespace:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl get deployment<br/>NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE<br/>my-nginx   1         1         1            1           20m<br/><br/>//double check the namespace of resource<br/>$ kubectl describe deployment my-nginx<br/>Name:                   my-nginx<br/>Namespace:              <strong>my-namespace<br/></strong>CreationTimestamp:      Mon, 18 Dec 2017 15:39:46 -0500<br/>Labels:                 run=my-nginx<br/>:<br/>(ignored)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deleting a Namespace</h1>
                </header>
            
            <article>
                
<p>If you followed the previous pages for the Kubernetes resource, you may have gotten the idea that the subcommand <kbd>delete</kbd> is used to remove resources. It is workable in the case of removing a Namespace. At the same time, if we try to delete a Namespace, the resources under it will be removed, as well:</p>
<pre>// first, go ahead to remove the Namespace "my-namespace"<br/>$ kubectl delete ns my-namespace<br/>namespace "my-namespace" deleted<br/>// check the Deployment again, the exited "my-nginx" is terminated<br/>$ kubectl get deployment<br/>No resources found.<br/><br/>// while trying to create anything, the error message showing the default Namespace is not existed<br/>$ kubectl run my-alpine --image=alpine<br/>Error from server (NotFound): namespaces "my-namespace" not found</pre>
<p>To solve this issue, you may attach another Namespace to the current context, or just change your current context to the previous one:</p>
<pre>// first solution: use set-context to update the Namespace<br/>// here we just leave Namespace empty, which means to use default Namespace<br/>$ kubectl config set-context my-context --namespace=""<br/>Context "my-context" modified.<br/><br/>// second solution: switch current context to another context<br/>// in this case, it is kubeadm environment<br/>$ kubectl config use-context kubernetes-admin@kubernetes<br/>Switched to context "kubernetes-admin@kubernetes".</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>Although we discussed the Namespaces and context of <kbd>kubeconfig</kbd> together, they are independent objects in the Kubernetes system. The context of <kbd>kubeconfig</kbd> is a client concept which can only be controlled by certain users, and it makes it easier to work with Namespaces and clusters. On the other hand, Namespace is the concept of the server side, working for resource isolation in clusters, and it is able to be shared between clients.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>We not only leverage Namespace to separate our resources, but also to realize finer computing resource provisioning. By restricting the usage amount of the computing power of a Namespace, the system manager can avoid the client creating too many resources and making servers overload.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a LimitRange</h1>
                </header>
            
            <article>
                
<p>To set the resource limitation of each Namespace, the admission controller <kbd>LimitRanger</kbd> should be added in the Kubernetes API server. Do not worry about this setting if you have <kbd>minikube</kbd> or <kbd>kubeadm</kbd> as your system manager.</p>
<div class="packt_tip"><strong>The admission controller in the Kubernetes API server<br/></strong><span>Admission controller is a setting in the Kubernetes API server which defines more advanced functionality in the API server. There are several functions that can be set in the admission controller. Users can add the functions when starting the API server through the configuration file or using CLI with the flag <kbd>--admission-control</kbd>. Relying on <kbd>minikube</kbd> or <kbd>kubeadm</kbd> for system management, they have their own initial settings in the admission controller:</span>
<ul>
<li><span><strong>Default admission controller in</strong> <strong>kubeadm</strong>: <kbd>Initializers</kbd></span>, <span><kbd>NamespaceLifecycle</kbd></span>, <span><kbd>LimitRanger</kbd></span>, <span><kbd>ServiceAccount</kbd></span>, <span><kbd>PersistentVolumeLabel</kbd></span>, <span><kbd>DefaultStorageClass</kbd></span>, <span><kbd>DefaultTolerationSeconds</kbd></span>, <span><kbd>NodeRestriction</kbd></span>, <span><kbd>ResourceQuota</kbd></span></li>
<li><span><strong>Default admission controller in minikube</strong>: <kbd>NamespaceLifecycle</kbd></span>, <span><kbd>LimitRanger</kbd></span>, <span><kbd>ServiceAccount</kbd></span>, <span><kbd>DefaultStorageClass, ResourceQuota</kbd></span></li>
</ul>
<span>Based on the version of your API server, there is a recommended list in an official document at</span> <a href="https://kubernetes.io/docs/admin/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use">https://kubernetes.io/docs/admin/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use</a><span>. Check for more ideas!</span></div>
<p>A plain new Namespace has no limitation on the resource quota. At the beginning, we start a Namespace and take a look at its initial settings:</p>
<pre>// create a Namespace by YAML file<br/>$ kubectl create -f my-first-namespace.yaml<br/>namespace "my-namespace" created            <br/><br/>$ kubectl describe ns my-namespace<br/>Name:         my-namespace<br/>Labels:       &lt;none&gt;<br/>Annotations:  &lt;none&gt;<br/>Status:       Active<br/><br/>No resource quota.<br/><br/>No resource limits.</pre>
<p>After that, we create a resource called <kbd>LimitRange</kbd> for specifying the resource limitation of a Namespace. The following is a good example of creating a limit in a Namespace:</p>
<pre>$ cat my-first-limitrange.yaml<br/>apiVersion: v1<br/>kind: LimitRange<br/>metadata:<br/>  name: my-limitrange<br/>spec:<br/>  limits:<br/>  - type: Pod<br/>    max:<br/>      cpu: 2<br/>      memory: 1Gi<br/>    min:<br/>      cpu: 200m<br/>      memory: 6Mi<br/>  - type: Container<br/>    default:<br/>      cpu: 300m<br/>      memory: 200Mi<br/>    defaultRequest:<br/>      cpu: 200m<br/>      memory: 100Mi<br/>    max:<br/>      cpu: 2<br/>      memory: 1Gi<br/>    min:<br/>      cpu: 100m<br/>      memory: 3Mi</pre>
<p>We will then limit the resources in a Pod with the values of <kbd>2</kbd> as <kbd>max</kbd> and <kbd>200m</kbd> as a <kbd>min</kbd> for CPU, and <kbd>1Gi</kbd> as max and <kbd>6Mi</kbd> as a min for memory. For the container, the CPU is limited between <kbd>100m - 2</kbd> and the memory is between <kbd>3Mi</kbd> - <kbd>1Gi</kbd>. If the max is set, then you have to specify the limit in the Pod/container spec during the resource creation; if the min is set, then the request has to be specified during the Pod/container creation. The <kbd>default</kbd> and <kbd>defaultRequest</kbd> section in LimitRange is used to specify the default limit and request in the container spec.</p>
<div class="packt_infobox"><strong>The value of CPU limitation in LimitRange<br/></strong>What do the values of <kbd>2</kbd> and <kbd>200m</kbd> mean in the Pod limitation in the file <kbd>my-first-limitrange.yaml</kbd>? The integer value means the number of CPU; the "m" in the value means millicpu, so <kbd>200m</kbd> means 0.2 CPU (200 * 0.001). Similarly, the default CPU limitation of the container is 0.2 to 0.3, and the real limitation is 0.1 to 2.</div>
<p>Afterwards, we create the LimitRange in our plain Namespace and check what will happen:</p>
<pre>// create the limitrange by file with the flag of Namespace<br/>// the flag --namespace can be abbreviated to "n"<br/>$ kubectl create -f my-first-limitrange.yaml -n my-namespace<br/>limitrange "my-limitrange" created<br/><br/>// check the resource by subcommand "get"<br/>$ kubectl get limitrange -n my-namespace<br/>NAME            AGE<br/>my-limitrange   23s<br/><br/>// check the customized Namespace<br/>$ kubectl describe ns my-namespace<br/>Name:         my-namespace<br/>Labels:       &lt;none&gt;<br/>Annotations:  &lt;none&gt;<br/>Status:       Active<br/><br/>No resource quota.<br/><br/>Resource Limits<br/> Type       Resource  Min   Max  Default Request  Default Limit  Max Limit/Request Ratio<br/> ----       --------  ---   ---  ---------------  -------------  -----------------------<br/> Pod        cpu       200m  2    -                -              -<br/> Pod        memory    6Mi   1Gi  -                -              -<br/> Container  memory    3Mi   1Gi  100Mi            200Mi          -<br/> Container  cpu       100m  2    200m             300m           -</pre>
<p>When you query the detail description of <kbd>my-namespace</kbd>, you will see the constraint attached to the Namespace directly. There is not any requirement to add the LimitRange. Now, all the Pods and containers created in this Namespace have to follow the resource limits listed here. If the definitions violate the rule, a validation error will be thrown accordingly:</p>
<pre>// Try to request an overcommitted Pod, check the error message<br/>$ kubectl run my-greedy-nginx --image=nginx --namespace=my-namespace --restart=Never --requests="cpu=4"<br/>The Pod "my-greedy-nginx" is invalid: spec.containers[0].resources.requests: Invalid value: "4": must be less than or equal to cpu limit</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deleting a LimitRange</h1>
                </header>
            
            <article>
                
<p>We can delete the LimitRange resource with the subcommand <kbd>delete</kbd>. Like creating the <kbd>LimitRange</kbd>, deleting a <kbd>LimitRange</kbd> in a Namespace would remove the constraints in the Namespace automatically:</p>
<pre>$ kubectl delete -f my-first-limitrange.yaml -n=my-namespace<br/>limitrange "my-limitrange" deleted<br/>$ kubectl describe ns my-namespace<br/>Name:         my-namespace<br/>Labels:       &lt;none&gt;<br/>Annotations:  &lt;none&gt;<br/>Status:       Active<br/><br/>No resource quota.<br/><br/>No resource limits.</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>Many Kubernetes resources are able to run under a Namespace. To achieve good resource management, check out the following recipes:</p>
<ul>
<li><em>Working with Pods</em></li>
<li><em>Deployment API</em></li>
<li><em>Working with names</em></li>
<li><em>Setting resources in nodes</em> <span>section </span>in <em><a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml">Chapter 7</a>, Building Kubernetes on GCP</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with labels and selectors</h1>
                </header>
            
            <article>
                
<div class="page">
<div class="layoutArea">
<div class="column">
<p><strong>Labels</strong> <span>are a set of key/value pairs, which are attached to object metadata. We could use labels to select, organize, and group objects, such as Pods, ReplicaSets, and Services. Labels are not necessarily unique. Objects could carry the same set of labels.</span></p>
<div class="page">
<div class="layoutArea">
<div class="column">
<p><span>Label selectors are used to query objects with labels of the following types:</span></p>
<ul>
<li>Equality-based:
<ul>
<li>Use equal (<kbd>=</kbd> or <kbd>==</kbd>) or not-equal (<kbd>!=</kbd>) operators</li>
</ul>
</li>
<li>Set-based:
<ul>
<li>Use <kbd>in</kbd> or <kbd>notin</kbd> operators</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="page">
<div class="layoutArea">
<div class="column">
<p><span>Before you get to setting labels in the objects, you should consider the valid naming convention of key and value.</span></p>
<div class="page">
<p><span>A valid key should follow these rules:</span></p>
<ul>
<li><span>A name with an optional prefix, separated by a slash.</span></li>
<li><span>A prefix must be a DNS subdomain, separated by dots, no longer than 253 characters.</span></li>
<li><span>A name must be less than 63 characters with the combination of [a-z0-9A-Z] and dashes, underscores, and dots. Note that symbols are illegal if put at the beginning and the end.</span></li>
</ul>
<p><span>A valid value should follow the following rules:</span></p>
<ul>
<li><span>A name must be less than 63 characters with the combination of [a-z0-9A-Z] and dashes, underscores, and dots. Note that symbols are illegal if put at the beginning and the end.</span></li>
</ul>
<p>You should also consider the purpose, too. For example, there are two projects, <kbd>pilot</kbd> and <kbd>poc</kbd>. Also, those projects are under different environments, such as <kbd>develop</kbd> and <kbd>production</kbd>. In addition, some contain multiple tiers, such as <kbd>frontend</kbd>, <kbd>cache</kbd>, and <kbd>backend</kbd>. We can make our labels key and value pair combination like follows:</p>
<pre>  labels:<br/>    project: pilot<br/>    environment: develop<br/>    tier: frontend</pre></div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<div class="page">
<div class="layoutArea">
<div class="column">
<ol>
<li><span>Let's try to create several Pods with the previous labels to distinguish different projects, environments, and tiers, as follows:</span></li>
</ol>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>YAML Filename</strong></td>
<td><strong>Pod Image</strong></td>
<td><strong>Project</strong></td>
<td><strong>Environment</strong></td>
<td><strong>Tier</strong></td>
</tr>
<tr>
<td><kbd>pilot-dev.yaml</kbd></td>
<td><kbd>nginx</kbd></td>
<td rowspan="4">pilot</td>
<td rowspan="2">develop</td>
<td><kbd>frontend</kbd></td>
</tr>
<tr>
<td><kbd>pilot-dev.yaml</kbd></td>
<td><kbd>memcached</kbd></td>
<td><kbd>cache</kbd></td>
</tr>
<tr>
<td><kbd>pilot-prod.yaml</kbd></td>
<td><kbd>nginx</kbd></td>
<td rowspan="2">production</td>
<td><kbd>frontend</kbd></td>
</tr>
<tr>
<td><kbd>pilot-prod.yaml</kbd></td>
<td><kbd>memcached</kbd></td>
<td><kbd>cache</kbd></td>
</tr>
<tr>
<td><kbd>poc-dev.yaml</kbd></td>
<td><kbd>httpd</kbd></td>
<td rowspan="2">poc</td>
<td rowspan="2">develop</td>
<td><kbd>frontend</kbd></td>
</tr>
<tr>
<td><kbd>poc-dev.yaml</kbd></td>
<td><kbd>memcached</kbd></td>
<td><kbd>cache</kbd></td>
</tr>
</tbody>
</table>
<ol start="2">
<li>For convenience, we will prepare three YAML files that contain two Pods each, with a <kbd>YAML separator ---</kbd> between Pods:</li>
</ol>
<ul>
<li><kbd>pilot-dev.yaml</kbd>:</li>
</ul>
<pre style="padding-left: 90px">apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: pilot.dev.nginx<br/><strong>  labels:</strong><br/><strong>    project: pilot</strong><br/><strong>    environment: develop</strong><br/><strong>    tier: frontend</strong><br/>spec:<br/>  containers:<br/>    - name: nginx<br/>      image: nginx<br/>---<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: pilot.dev.memcached<br/><strong>  labels:</strong><br/><strong>    project: pilot</strong><br/><strong>    environment: develop</strong><br/><strong>    tier: cache</strong><br/>spec:<br/>  containers:<br/>    - name: memcached<br/>      image: memcached</pre>
<ul>
<li><kbd>pilot-prod.yaml</kbd>:</li>
</ul>
<pre style="padding-left: 90px">apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: pilot.prod.nginx<br/><strong>  labels:</strong><br/><strong>    project: pilot</strong><br/><strong>    environment: production</strong><br/><strong>    tier: frontend</strong><br/>spec:<br/>  containers:<br/>    - name : nginx<br/>      image: nginx<br/>---<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: pilot.prod.memcached<br/><strong>  labels:</strong><br/><strong>    project: pilot</strong><br/><strong>    environment: production</strong><br/><strong>    tier: cache</strong><br/>spec:<br/>  containers:<br/>    - name: memcached<br/>      image: memcached</pre>
<ul>
<li><kbd>poc-dev.yaml</kbd>:</li>
</ul>
<pre style="padding-left: 90px">apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: poc.dev.httpd<br/><strong>  labels:</strong><br/><strong>    project: poc</strong><br/><strong>    environment: develop</strong><br/><strong>    tier: frontend</strong><br/>spec:<br/>  containers:<br/>    - name: httpd<br/>      image: httpd<br/>---<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: poc.dev.memcached<br/><strong>  labels:</strong><br/><strong>    project: poc</strong><br/><strong>    environment: develop</strong><br/><strong>    tier: cache</strong><br/>spec:<br/>  containers:<br/>    - name: memcached<br/>      image: memcached</pre></div>
</div>
</div>
<ol start="3">
<li>Create those six Pods with the <kbd>kubectl create</kbd> command, as follows, to see how labels are defined:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl create -f pilot-dev.yaml<br/>pod "pilot.dev.nginx" created<br/>pod "pilot.dev.memcached" created<br/><br/><br/>$ kubectl create -f pilot-prod.yaml<br/>pod "pilot.prod.nginx" created<br/>pod "pilot.prod.memcached" created<br/><br/><br/>$ kubectl create -f poc-dev.yaml<br/>pod "poc.dev.httpd" created<br/>pod "poc.dev.memcached" created</pre>
<ol start="4">
<li>Run <kbd>kubectl describe &lt;Pod name&gt;</kbd> to check labels, as follows. It looks good, so let's use the label selector to query these Pods by different criteria:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl describe pod poc.dev.memcache<br/>Name: poc.dev.memcached<br/>Namespace: default<br/>Node: minikube/192.168.99.100<br/>Start Time: Sun, 17 Dec 2017 17:23:15 -0800<br/><strong>Labels: environment=develop</strong><br/><strong>              project=poc</strong><br/><strong>              tier=cache</strong><br/>Annotations: &lt;none&gt;<br/>Status: Running<br/>...</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>As mentioned earlier in this section, there are two types of label selectors: either equality-based or set-based. Those types have different operators to specify criteria. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Equality-based label selector</h1>
                </header>
            
            <article>
                
<p>The equality-based selector can specify equal or not equal, and also uses commas to add more criteria. Use the <kbd>-l</kbd> or <kbd>--selector</kbd> option to specify these criteria to filter the name of the object; for example:</p>
<ul>
<li> Query Pods which belong to the pilot project:</li>
</ul>
<pre style="padding-left: 90px">$ kubectl get pods -l "project=pilot"<br/>NAME READY STATUS RESTARTS AGE<br/>pilot.dev.memcached 1/1 Running 0 21m<br/>pilot.dev.nginx 1/1 Running 0 21m<br/>pilot.prod.memcached 1/1 Running 0 21m<br/>pilot.prod.nginx 1/1 Running 0 21m</pre>
<ul>
<li>Query Pods which belong to the frontend tier:</li>
</ul>
<pre style="padding-left: 90px">$ kubectl get pods -l "tier=frontend"<br/>NAME READY STATUS RESTARTS AGE<br/>pilot.dev.nginx 1/1 Running 0 21m<br/>pilot.prod.nginx 1/1 Running 0 21m<br/>poc.dev.httpd 1/1 Running 0 21m</pre>
<ul>
<li>Query Pods which belong to the frontend tier AND the under develop environment:</li>
</ul>
<pre style="padding-left: 90px">$ kubectl get pods -l "tier=frontend,environment=develop"<br/>NAME READY STATUS RESTARTS AGE<br/>pilot.dev.nginx 1/1 Running 0 22m<br/>poc.dev.httpd 1/1 Running 0 21m</pre>
<ul>
<li>Query Pods which belong to the frontend tier and NOT the under develop environment:</li>
</ul>
<pre style="padding-left: 90px">$ kubectl get pods -l "tier=frontend,environment!=develop"<br/>NAME READY STATUS RESTARTS AGE<br/>pilot.prod.nginx 1/1 Running 0 29m</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Set-based label selector</h1>
                </header>
            
            <article>
                
<p>With the set-based selector, you can use either the <kbd>in</kbd> or <kbd>notin</kbd> operator, which is similar to the <kbd>SQL IN</kbd> clause that can specify multiple keywords, as in the following examples:</p>
<ul>
<li>Query <kbd>Pods</kbd> which belong to the <kbd>pilot</kbd> project:</li>
</ul>
<pre style="padding-left: 90px">$ kubectl get pods -l "project in (pilot)"<br/>NAME READY STATUS RESTARTS AGE<br/>pilot.dev.memcached 1/1 Running 0 36m<br/>pilot.dev.nginx 1/1 Running 0 36m<br/>pilot.prod.memcached 1/1 Running 0 36m<br/>pilot.prod.nginx 1/1 Running 0 36m<br/><br/></pre>
<ul>
<li>Query <kbd>Pods</kbd> which belong to the pilot project and <kbd>frontend</kbd> tier:</li>
</ul>
<pre style="padding-left: 90px">$ kubectl get pods -l "project in (pilot), tier in (frontend)"<br/>NAME READY STATUS RESTARTS AGE<br/>pilot.dev.nginx 1/1 Running 0 37m<br/>pilot.prod.nginx 1/1 Running 0 37m</pre>
<ul>
<li>Query <kbd>Pods</kbd> which belong to the pilot project and either the <kbd>frontend</kbd> or <kbd>cache</kbd> tier:</li>
</ul>
<pre style="padding-left: 90px"><span>$ kubectl get pods -l "project in (pilot), tier in (frontend,cache)"</span><br/><span>NAME READY STATUS RESTARTS AGE</span><br/><span>pilot.dev.memcached 1/1 Running 0 37m</span><br/><span>pilot.dev.nginx 1/1 Running 0 37m</span><br/><span>pilot.prod.memcached 1/1 Running 0 37m</span><br/><span>pilot.prod.nginx 1/1 Running 0 37m</span></pre>
<ul>
<li>Query <kbd>Pods</kbd> which belong to the pilot project and not the <kbd>frontend</kbd> or <kbd>backend</kbd> tier (note, we didn't create the <kbd>backend</kbd> tier object):</li>
</ul>
<pre style="padding-left: 90px">$ kubectl get pods -l "project in (pilot), tier notin (frontend, backend)"<br/>NAME READY STATUS RESTARTS AGE<br/>pilot.dev.memcached 1/1 Running 0 50m<br/>pilot.prod.memcached 1/1 Running 0 50m</pre>
<p>As you can see in the preceding examples for both the equality-based and set-based label selector, equality-based is simpler and set-based is more expressive. Note that you can mix both operator as follows:</p>
<ul>
<li>Query Pods which do not belong to the pilot project and develop environment:</li>
</ul>
<pre style="padding-left: 90px">$ kubectl get pods -l "project notin (pilot), environment=develop"<br/>NAME READY STATUS RESTARTS AGE<br/>poc.dev.httpd 1/1 Running 0 2m<br/>poc.dev.memcached 1/1 Running 0 2m</pre>
<p>So, you can use the most efficient way to filter out the Kubernetes objects. In addition, you can also use either or both types of selectors to configure the Kubernetes Service, Deployments, and so on. However, some objects support the equality-based selector and some objects support both. So, let's take a look at how to define it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Label selectors are useful to not only list an object, but also to specify the Kubernetes Service and Deployment to bind objects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linking Service to Pods or ReplicaSets using label selectors</h1>
                </header>
            
            <article>
                
<p><span>As of Kubernetes version 1.9, Service only supports the equality-based selector to bind to Pods or</span> ReplicaSet<span>.</span></p>
<p><span>Let's create one Service that binds to <kbd>nginx</kbd>, which belongs to the production environment and the pilot project. Remember that <kbd>nginx</kbd> also belongs to the frontend tier:</span></p>
<pre><span>//check your selector filter is correct or not<br/>$ kubectl get pods -l 'environment=production,project=pilot,tier=frontend'<br/>NAME READY STATUS RESTARTS AGE<br/>pilot.prod.nginx 1/1 Running 0 19m<br/><br/><br/>//create Service yaml that specify selector<br/>$ cat pilot-nginx-svc.yaml <br/></span><span>apiVersion: v1<br/></span><span>kind: Service<br/></span><span>metadata:<br/></span><span>  name: pilot-nginx-svc<br/></span><span>spec:<br/></span><span>  type: NodePort<br/></span><span>  ports:<br/></span><span>    - protocol: TCP<br/></span><span>      port: 80<br/></span><strong>  selector:<br/>    project: pilot<br/>    environment: production<br/></strong><span><strong>    tier: frontend</strong><br/><br/><br/>//create pilot-nginx-svc <br/>$ kubectl create -f pilot-nginx-svc.yaml <br/>service "pilot-nginx-svc" created<br/></span></pre>
<p>Here is the equivalent, where you can use the <kbd>kubectl expose</kbd> command to specify the label selector:</p>
<pre>$ kubectl expose pod pilot.prod.nginx --name=pilot-nginx-svc2 --type=NodePort --port=80 <strong>--selector="project=pilot,environment=develop,tier=frontend"</strong><br/>service "pilot-nginx-svc2" exposed</pre>
<p>Based on your Kubernetes environment, if you are using minikube, it is easier to check your Service with <kbd>minikube service &lt;Service name&gt;</kbd>, as in the following screenshot. If you are not using minikube, access to any Kubernetes node and assigned Service port number. For the following screenshot, it would be <kbd>&lt;node ip&gt;:31981 or &lt;node ip&gt;:31820</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-231 image-border" src="assets/f0be9472-e1c0-4b61-9224-5af359c3be14.png" style="width:166.17em;height:114.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Access to Service which is running on minikube</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linking Deployment to ReplicaSet using the set-based selector</h1>
                </header>
            
            <article>
                
<p>Deployment supports not only the equality-based selector, but also the set-based selector, to specify <kbd>ReplicaSet</kbd>. To do that, you can write <kbd>spec.selector.matchExpressions[]</kbd> to specify the key and <kbd>in</kbd>/<kbd>notin</kbd> operator. For example, if you want to specify <kbd>project in (poc), environment in (staging), tier notn (backend,cache)</kbd>, then <kbd>matchExpressions</kbd> would be as follows:</p>
<pre>$ cat deploy_set_selector.yaml <br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: my-nginx<br/>spec:<br/>  replicas: 3<br/><strong>  selector:</strong><br/><strong>    matchExpressions:</strong><br/><strong>      - {key: project, operator: In, values: [poc]}</strong><br/><strong>      - {key: environment, operator: In, values: [staging]}</strong><br/><strong>      - {key: tier, operator: NotIn, values: [backend,cache]}</strong><br/>  template:<br/>    metadata:<br/>      labels:<br/>        project: poc<br/>        environment: staging<br/>        tier: frontend<br/>    spec:<br/>      containers:<br/>      - name: my-nginx<br/>        image: nginx<br/>        ports:<br/>        - containerPort: 80</pre>
<p>As you can see, the YAML array is represented as <kbd><em>-</em></kbd>, and the map object as <kbd>{}</kbd>, to specify the key, operator, and values. Note that values would also be an array, so use the square bracket <kbd>[]</kbd> to specify one or more values.</p>
<p>One thing you need to aware of is one label, called the <kbd>pod-template-hash</kbd> label, which is created by Deployment. When you create a Deployment, it will also create a <kbd>ReplicaSet</kbd> object. At this time, Deployment will also assign the <kbd>pod-template-hash</kbd> label to the <kbd>ReplicaSet</kbd>. Let's see how it works:</p>
<pre>$ kubectl create -f deploy_set_selector.yaml<br/>deployment.apps "my-nginx" created<br/><br/>$ kubectl get rs<br/><span>NAME<span class="Apple-converted-space">                  </span>DESIRED <span class="Apple-converted-space">  </span>CURRENT <span class="Apple-converted-space">  </span>READY <span class="Apple-converted-space">    </span>AGE<br/></span><span>my-nginx2-764d7cfff <span class="Apple-converted-space">  </span>3 <span class="Apple-converted-space">        </span>3 <span class="Apple-converted-space">        </span>3 <span class="Apple-converted-space">        </span>19s</span><br/><br/>$ kubectl describe rs my-nginx2-764d7cfff<br/><span>Name: <span class="Apple-converted-space">          </span>my-nginx2-764d7cfff<br/></span><span>Namespace:<span class="Apple-converted-space">      </span>default<br/></span><span>Selector: <span class="Apple-converted-space">      </span>environment in (staging),pod-template-hash=320837999,project in (poc),tier notin (backend,cache)<br/></span>...<br/>...<br/><span>Pod Template:<br/></span><span><span class="Apple-converted-space">  </span>Labels:<span class="Apple-converted-space">  </span>environment=staging<br/></span><span><span class="Apple-converted-space">           </span><strong>pod-template-hash=320837999</strong><br/></span><span><span class="Apple-converted-space">           </span>project=poc<br/></span><span><span class="Apple-converted-space">           </span>tier=frontend</span><br/>...<br/>...</pre>
<p>As you can see, the <kbd>ReplicaSet</kbd> <kbd>my-nginx2-764d7cfff</kbd> has an equality-based selector, as <kbd>pod-template-hash=320837999</kbd> is appended to the Selector and Pod template. It will be used to generate a <kbd>ReplicaSet</kbd> and Pod name with a particular hash function (for example, <kbd>my-nginx2-764d7cfff</kbd>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>In this section, you learned how flexible it is to assign a label to your Kubernetes object. In addition, equality-based and set-based selectors allow us to filter out an object by label. Selector is important that loosely couple an object such as Service and ReplicaSet/Pod as well as Deployment and ReplicaSet.</p>
<p>The following sections will also use labels and the concept of selectors to utilize container management:</p>
<ul>
<li class="mce-root"><em>Updating live containers</em> section in <em><a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml">Chapter 3</a>, Playing with Containers</em></li>
<li><em>Managing the Kubernetes</em> <em>cluster on GKE</em> section in <em><a href="dfc46490-f109-4f07-ba76-1a381b006d76.xhtml">Chapter 7</a>, Building Kubernetes on GCP</em></li>
</ul>


            </article>

            
        </section>
    </body></html>