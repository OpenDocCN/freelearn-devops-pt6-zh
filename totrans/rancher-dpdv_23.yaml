- en: '*Chapter 18*: Resource Management'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 18 章*：资源管理'
- en: In this final chapter of our book, we will cover the topic of resource management,
    which includes several items. The first is Pod resource limit and quota, which
    allows you to control your resource spend at a Pod level. We will cover how these
    limits are applied and enforced in a cluster. We will then work our way up the
    chain by covering the topic of namespace limits and quotas and will then move
    up another level by covering how Rancher project limits work. Finally, we will
    cover how to use kubecost to monitor our Kubernetes costs over time.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的最后一章中，我们将讨论资源管理主题，其中包括几个内容。首先是 Pod 资源限制和配额，它允许你在 Pod 层面控制资源的使用。我们将讨论这些限制如何在集群中应用和强制执行。然后，我们将逐步深入，讨论命名空间限制和配额的主题，接着再讨论
    Rancher 项目限制的工作方式。最后，我们将讲解如何使用 kubecost 监控我们的 Kubernetes 成本变化。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: How to apply resource limits and quotas to a Pod
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何为 Pod 应用资源限制和配额
- en: How namespace limits/quotas are calculated
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何计算命名空间的限制/配额
- en: How to use tools such as Kubecost to track usage and cost over time
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用像 Kubecost 这样的工具跟踪使用情况和成本变化
- en: Let's get started!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: How to apply resource limits and quotas to a Pod
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何为 Pod 应用资源限制和配额
- en: The physical resources in a Kubernetes cluster are limited. Resources are measured
    based on the number of **central processing unit** (**CPU**) cores or **random-access
    memory** (**RAM**) allocated per worker node. For example, you might have 10 worker
    nodes with 4 cores per node, meaning this cluster has 40 cores available to use.
    This kind of cluster is acceptable until you start running out of resources. As
    we know, CPU and memory are not free, so we can't just keep throwing resources
    at a cluster.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 集群中的物理资源是有限的。资源的衡量标准是每个工作节点分配的 **中央处理单元** (**CPU**) 核心数或 **随机存取存储器**
    (**RAM**) 的数量。例如，你可能有 10 个工作节点，每个节点 4 个核心，这意味着该集群可以使用 40 个核心。这个集群是可以接受的，直到你开始耗尽资源。正如我们所知，CPU
    和内存是有限的，因此我们不能仅仅通过不断添加资源来扩展集群。
- en: With Kubernetes, by default, all workloads and namespace have unlimited resources,
    meaning nothing stops an application from consuming all the CPU and memory in
    a cluster. For example, an application team could push out a new workload with
    a misconfigured **Horizontal Pod Autoscaler** (**HPA**) with a very high upper
    scale limit. It is important to note that HPAs do not allow unlimited maximum
    replicas, but nothing is stopping you from setting it very high. If the HPA metrics
    were set too low, Kubernetes starts scaling up the workload until it hits the
    max replica count or the cluster runs out of resources. Of course, this can cause
    an outage in Kubernetes clusters that are shared between environments—that is,
    non-production and production—or between application teams. We don't want one
    application/team to cause an outage for another team.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，默认情况下，所有工作负载和命名空间都没有资源限制，意味着没有任何东西可以阻止一个应用程序消耗集群中的所有 CPU 和内存。例如，一个应用程序团队可能会发布一个新工作负载，其中
    **水平 Pod 自动伸缩器** (**HPA**) 被错误配置，设定了非常高的上限。需要注意的是，HPA 不允许设置无限制的最大副本数，但没有任何限制阻止你将其设置得非常高。如果
    HPA 的指标设置得过低，Kubernetes 会开始扩展工作负载，直到达到最大副本数或者集群资源耗尽。当然，这可能会导致 Kubernetes 集群宕机，尤其是在多个环境之间共享集群时——即在非生产环境与生产环境之间，或在应用程序团队之间共享集群时。我们不希望一个应用程序/团队导致另一个团队的宕机。
- en: To protect our clusters, we need to set up resource limits and quotas around
    our workloads and namespaces to prevent this kind of outage. We do this at two
    levels.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保护我们的集群，我们需要为工作负载和命名空间设置资源限制和配额，以防止此类宕机事件的发生。我们在两个层级上进行设置。
- en: The first level is at the workload/Pod level where we want to set resource limits
    and requests. This applies to all workload types (deployments, statefulsets, cronjobs,
    daemonsets, and so on); however, limits are not applied at the workload level
    but at the Pod level. If I set a one core limit for deployment, that limit applies
    to each Pod in that deployment, meaning each Pod can consume one core. You can't
    limit resources at the workload level; for example, you can't—say—only use one
    core across all Pods in a workload.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个层级是在工作负载/Pod 层面，我们希望在此设置资源限制和请求。这适用于所有工作负载类型（部署、状态副本集、定时任务、守护进程集等）；然而，资源限制并不是应用在工作负载层面，而是应用在
    Pod 层面。如果我为部署设置一个核心的限制，那么这个限制适用于该部署中的每个 Pod，意味着每个 Pod 可以使用一个核心。你不能在工作负载层面设置资源限制；例如，你不能限制一个工作负载中的所有
    Pod 总共只使用一个核心。
- en: 'Here is a **YAML Ain''t Markup Language** (**YAML**) output of a Pod with the
    section that we want to focus on being the resources. In that section, you''ll
    see that we are setting both the request and limits. In this case, we are setting
    a CPU request (minimum) of a quarter core with a limit (maximum) of a half core.
    For the memory, we are setting a request of 64 **mebibytes** (**Mi**) and a limit
    of 128 Mi:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个**YAML Ain't Markup Language**（**YAML**）输出的Pod，其中我们要关注的是资源部分。在该部分中，你会看到我们设置了请求和限制。在这个例子中，我们设置了CPU请求（最小值）为四分之一核心，限制（最大值）为半个核心。对于内存，我们设置了请求64
    **Mebibytes**（**Mi**）和限制128 Mi：
- en: '![Figure 18.1 – Pod resource request and limit YAML example'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![图18.1 – Pod资源请求和限制YAML示例'
- en: '](img/B18053_18_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_18_01.jpg)'
- en: Figure 18.1 – Pod resource request and limit YAML example
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.1 – Pod资源请求和限制YAML示例
- en: Looking at the preceding example, we want to focus on the `resources` section.
    In this section, we define requests and limits for this Pod. The `request` values
    are setting the required resources for this Pod. This example tells Kubernetes
    that the Pod needs 64 Mi of memory and a quarter core (1,000 `kube-scheduler`
    uses these values to find a node with the required available resources in the
    Pod scheduling process. For example, if we had a database running inside a Pod,
    we might set the memory request to something such as 16 `kube-scheduler` builds
    its list of candidate nodes, it will filter out nodes without the required resources.
    If a Pod requests more memory than any one node has available, it will get stuck
    in scheduling. It is important to note that Kubernetes will not rebalance the
    cluster to make room for this Pod. An unofficial tool called `descheduler`, found
    at [https://github.com/kubernetes-sigs/descheduler](https://github.com/kubernetes-sigs/descheduler),
    tries to do this. Once the Pod is scheduled on a node, kubelet will reserve the
    requested resources on that node.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下前面的示例，我们要重点关注`resources`部分。在这个部分中，我们为这个Pod定义了请求和限制。`request`值设置了这个Pod所需的资源。这个示例告诉Kubernetes，Pod需要64
    Mi的内存和四分之一核心（1,000 `kube-scheduler`使用这些值来查找Pod调度过程中所需的可用资源节点。例如，如果我们在Pod内运行一个数据库，我们可能会将内存请求设置为16
    `kube-scheduler`构建其候选节点列表时，它会过滤掉没有所需资源的节点。如果一个Pod请求的内存超过了任何节点可用的内存，它将卡在调度中。需要注意的是，Kubernetes不会重新平衡集群以为这个Pod腾出空间。一个名为`descheduler`的非官方工具，位于[https://github.com/kubernetes-sigs/descheduler](https://github.com/kubernetes-sigs/descheduler)，尝试做到这一点。一旦Pod被调度到节点上，kubelet将保留该节点上的请求资源。
- en: The other part of the resource section is `limits`. This is where we are setting
    much of the resources on the node this Pod is allowed to use. For example, we
    might only allow this Pod to use 1 `free -g` command, you'll see that the amount
    of free memory will be available to the node. If the node has 16 GB of free memory,
    the Pod will have 16 GB of free memory. This is why it is crucial to have memory
    limits on your applications and your Pod limits.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 资源部分的另一个内容是`limits`。这里我们设置了这个Pod可以使用的节点资源。例如，我们可能只允许这个Pod使用1个`free -g`命令，你会看到可用的内存数量会显示在节点上。如果节点有16
    GB的空闲内存，那么Pod将拥有16 GB的空闲内存。这就是为什么为你的应用和Pod设置内存限制至关重要的原因。
- en: The classic example is the Java heap size because older versions of Java (before
    8u191) weren't aware of memory and CPU limits for containers. Java would overrun
    its memory and CPU limits simply because Java thought it had more resources available,
    so items such as garbage collection weren't running to reduce memory usage. Red
    Hat published a great blog about this topic and went into great detail about this
    issue and how Java 10 added the **+UseContainerSupport** settings (enabled by
    default) to solve this problem. You can find that blog at [https://developers.redhat.com/blog/2017/03/14/java-inside-docker](https://developers.redhat.com/blog/2017/03/14/java-inside-docker).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经典的例子是Java堆大小，因为旧版本的Java（在8u191之前）并不意识到容器的内存和CPU限制。Java会超出其内存和CPU限制，因为Java认为它有更多的资源可用，因此像垃圾回收这样的操作不会运行以减少内存使用。Red
    Hat发布了一篇关于这个话题的精彩博客，详细探讨了这个问题，以及Java 10是如何通过**+UseContainerSupport**设置（默认启用）解决这个问题的。你可以在[https://developers.redhat.com/blog/2017/03/14/java-inside-docker](https://developers.redhat.com/blog/2017/03/14/java-inside-docker)找到这篇博客。
- en: But to get back to memory limits, kubelet/Docker has no way of reclaiming memory
    —that is, taking it back from the Pod; only the application can release used memory.
    kubelet/Docker can only take `WAIT` time in programs such as `top`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但回到内存限制，kubelet/Docker无法回收内存——即无法从Pod中回收内存；只有应用程序才能释放已使用的内存。kubelet/Docker只能在`top`等程序中记录`WAIT`时间。
- en: Kubernetes expands on limits and requests by adding `BestEffort` class. This
    type of priority is used for Pods that are batch processing or reporting, or for
    other jobs that can be stopped at any time without impacting end users. The next
    class is `Burstable`, and its primary difference is that it allows the Pod to
    consume more resources than the defined limits, but only if a node has the available
    resources. Typical usage of this class is when a Pod is relatively static, such
    as a database, so we want to allow it to use more resources for a short period
    of time. However, at the same time, we don't want to use statefulsets because
    doing so would mean that we can't move around this Pod in the cluster. The other
    main reason is for applications that are using in-memory sessions where a Pod
    going down would cause disruptions. If this Pod gets evicted, the sessions are
    dropped, and users have to log back in. The following section will cover how namespace
    limits and quotas build on top of Pod requests and limits.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes通过添加`BestEffort`类扩展了资源请求和限制。这个优先级类型用于批处理或报告的Pods，或者用于其他可以随时停止而不会影响最终用户的任务。下一个类是`Burstable`，其主要区别在于它允许Pod使用超过定义限制的资源，但前提是节点有可用资源。这个类的典型使用场景是当Pod相对静态时，比如数据库，因此我们希望允许它在短时间内使用更多的资源。然而，同时我们不想使用statefulsets，因为这样做意味着我们无法在集群中移动这个Pod。另一个主要原因是应用程序使用内存会话的情况，如果Pod宕机会导致中断。如果该Pod被驱逐，所有会话会丢失，用户必须重新登录。接下来的部分将介绍命名空间限制和配额如何在Pod的请求和限制之上构建。
- en: How namespace limits/quotas are calculated
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何计算命名空间的限制/配额
- en: One of the nice things about setting CPU and memory requests/limits for all
    Pods is that you can define namespace limits and quotas, which allows you to specify
    the total amount of memory and CPU used by all Pods running in a namespace. This
    can be very helpful when budgeting resources in your cluster; for example, if
    application *A* buys 16 CPUs and 64 GB of RAM for their production environment,
    you can limit their namespace to make sure they can't consume more than what they
    have paid for. This, of course, can be done in two modes, with the first being
    a hard limit that will block all new Pod creation events for that namespace. If
    we go back to our earlier example, the application team has purchased 64 GB of
    RAM for our cluster. Suppose you have four Pods, each with a limit of 16 GB of
    RAM. When they try to start up a fifth Pod, it will be stuck in scheduling until
    the quota increases or another Pod in the namespace releases the space, with CPU
    limits and requests being handled in the same way.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 设置所有Pod的CPU和内存请求/限制的一个好处是，您可以定义命名空间的限制和配额，这允许您指定在命名空间中运行的所有Pod总共使用的内存和CPU数量。这在预算集群资源时非常有用；例如，如果应用程序*A*为其生产环境购买了16个CPU和64GB的RAM，您可以限制他们的命名空间，确保他们不会消耗超过他们所支付的资源。当然，这可以通过两种模式来实现，第一种是硬限制，这将阻止该命名空间中所有新Pod的创建事件。如果我们回到之前的例子，应用程序团队为我们的集群购买了64GB的RAM。假设您有四个Pod，每个Pod的限制是16GB的RAM。当他们尝试启动第五个Pod时，它将卡在调度中，直到配额增加或该命名空间中的其他Pod释放空间，CPU的限制和请求也会以相同的方式处理。
- en: Of course, a namespace can have both limits and requests, just as with a Pod,
    but it's essential to understand how limits and requests are calculated. `kube-scheduler`
    simply adds up all limits and requests for Pods under a namespace, which means
    it does not use current metrics to decide if a Pod should be allowed to be scheduled
    in a namespace or not. It is essential to note that this only applies to hard
    limits. Soft limits use the metrics server to calculate currently used resources
    for each Pod.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，命名空间可以有限制和请求，就像Pod一样，但理解限制和请求是如何计算的非常重要。`kube-scheduler`简单地将命名空间下所有Pod的限制和请求加起来，这意味着它不会使用当前的度量标准来决定Pod是否可以在命名空间中调度。需要注意的是，这仅适用于硬限制。软限制使用度量服务器来计算每个Pod当前使用的资源。
- en: The biggest issue that most people run into is allowing Pods without requests
    and limits into their cluster, as in the case of hard limits. Those Pods need
    not be a part of the calculation as their values will be zero. On the other hand,
    soft limits apply to all Pods as long as the metrics server runs. Because of this,
    it is typically recommended to set both a hard and a soft limit for your namespace.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人遇到的最大问题是允许没有请求和限制的 Pods 进入集群，就像硬限制的情况一样。这些 Pods 不需要参与计算，因为它们的值为零。另一方面，只要指标服务器运行，软限制适用于所有
    Pods。因此，通常建议为你的命名空间设置硬限制和软限制。
- en: 'The other important part is understanding that limits and quotas are not defined
    as part of the namespace definition (that is, the namespace YAML), but are defined
    by the `ResourceQuota` kind, an example of which can be found here:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的部分是理解，限制和配额并不是作为命名空间定义的一部分（即命名空间的 YAML 文件）来定义的，而是通过 `ResourceQuota` 类型来定义的，相关示例可以在这里找到：
- en: '![Figure 18.2 – ResourceQuota YAML example with more options'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 18.2 – 带有更多选项的 ResourceQuota YAML 示例'
- en: '](img/B18053_18_02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_18_02.jpg)'
- en: Figure 18.2 – ResourceQuota YAML example with more options
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.2 – 带有更多选项的 ResourceQuota YAML 示例
- en: However, one of the cool things is that you can set quotas on more than just
    CPU and memory but can also limit things such as **graphics processing units**
    (**GPUs**), Pods, load balancers, and so on. If you look at *Figure 18.2*, we
    specify this namespace to only allow four Pods. This is very low for most clusters/environments,
    but it is important to note that there is no free lunch. For example, a namespace
    with 100 Pods with 64 GB of total memory and another namespace with only four
    Pods using the same amount of memory are two different workloads. The 100 Pods
    put a much larger workload on the node and cluster management services than just
    four Pods. The same is true with a namespace that stores a lot of data in the
    form of secrets versus configmaps as a secret is customarily encrypted and stored
    in memory on the kube-apiservers. Because they are encrypted, they tend to be
    uncompressible, as opposed to configmaps, which are stored in plain text and are
    generally compressible. The other common limit users put on a namespace is a load
    balancer. In many cloud environments, when you deploy a load balancer in Kubernetes,
    you are typically deploying a load balancer in the cloud provider, which costs
    money.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，其中一个很酷的地方是你可以设置的不仅仅是 CPU 和内存的配额，还可以限制诸如 **图形处理单元** (**GPUs**)、Pods、负载均衡器等资源。如果你查看
    *图 18.2*，我们为这个命名空间指定了仅允许四个 Pods。对于大多数集群/环境来说，这个数目非常低，但需要注意的是没有免费的午餐。例如，一个拥有 100
    个 Pods 和 64 GB 总内存的命名空间，与另一个仅有四个 Pods 却使用相同内存的命名空间，它们的工作负载是不同的。100 个 Pods 给节点和集群管理服务带来的负担要远远大于四个
    Pods。同样，存储大量数据的命名空间，比如存储在 secrets 中的数据与存储在 configmaps 中的数据也是不同的，因为 secrets 通常是加密存储的，并存储在
    kube-apiservers 的内存中。由于它们是加密的，因此通常无法压缩，而 configmaps 是以纯文本格式存储的，通常是可以压缩的。用户常常为命名空间设置的另一个限制是负载均衡器。在许多云环境中，当你在
    Kubernetes 中部署负载均衡器时，通常是在云服务提供商中部署负载均衡器，这会产生费用。
- en: Note
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This covers a Layer-4 load balancer and not an ingress, which is usually a shared
    resource.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及的是一个第四层负载均衡器，而不是一个入口，通常它是一个共享资源。
- en: One of the nice things about Rancher is it builds on top of namespace limits
    and quotas by allowing you to define project-level limits and requests. This will
    enable you to define limits for a team that might have multiple namespaces. A
    classic example is a non-production cluster where an application team might buy
    *X* amount of CPU and RAM and then choose how to distribute it across environments.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher 的一个优点是它建立在命名空间限制和配额之上，允许你定义项目级别的限制和请求。这使你能够为一个可能拥有多个命名空间的团队定义限制。一个经典的例子是非生产集群，在这种集群中，应用程序团队可能会购买
    *X* 数量的 CPU 和 RAM，然后选择如何在不同环境中分配这些资源。
- en: For instance, they might assign half of it to DEV and QAS most of the time,
    but during load testing, they might want to spin down their DEV and QAS namespaces
    to shift those resources to their `TEST` namespace and then back, after their
    testing is done. As a cluster administrator, you don't care about nor need to
    make any changes as long as they stand under their project limits. It is important
    to note that projects are Rancher objects because the downstream cluster has no
    idea what a project is. Rancher controls projects and uses namespace labels, annotations,
    and Rancher controllers to synchronize settings between Rancher and the downstream
    cluster.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，他们可能会将一半的资源分配给DEV和QAS，但在负载测试期间，他们可能希望关闭DEV和QAS命名空间，将这些资源转移到`TEST`命名空间，测试完成后再恢复。作为集群管理员，只要他们保持在项目限制范围内，你不需要关心或进行任何更改。值得注意的是，项目是Rancher对象，因为下游集群并不知道什么是项目。Rancher控制项目，并使用命名空间标签、注释和Rancher控制器来同步Rancher和下游集群之间的设置。
- en: In the next section, we will cover using tools such as Kubecost to build on
    top of resource quotas to allow you to do things such as show back and charge
    back to recover costs in your cluster.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍如何使用工具，如Kubecost，基于资源配额来构建，允许你进行例如成本回收和费用追踪等操作，以便为集群中的开销提供支持。
- en: How to use tools such as Kubecost to track usage and cost over time
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用如Kubecost等工具跟踪使用情况和成本随时间变化的情况
- en: One of the essential things about Kubernetes is it enables application teams
    to move fast and consume resources with very few limits by default. This means
    that many environments early on in their Kubernetes journey tend to have a lot
    of spending. The perfect example is what is covered in [*Chapter 13*](B18053_13_Epub.xhtml#_idTextAnchor209),
    *Scaling in Kubernetes*, allowing you to auto-scale both your cluster and workloads.
    This means an application team can make a change that increases your costs by
    a significant number without you knowing until the bill comes, and now you have
    to go back and find out what changed, and—of course—it becomes tough to pull back
    resources after Kubernetes has been used for some time.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的一个关键特点是它使应用团队能够快速移动，并且默认情况下几乎没有限制地消耗资源。这意味着，在Kubernetes的初期阶段，许多环境往往会有大量开支。一个完美的例子是在[*第13章*](B18053_13_Epub.xhtml#_idTextAnchor209)中介绍的内容——*Kubernetes中的扩展*，允许你自动扩展集群和工作负载。这意味着应用团队可能会做出一些改变，导致你的成本显著增加，而你直到账单到来时才知道，而此时你需要回去查找究竟发生了什么变化，当然，随着Kubernetes使用一段时间后，资源的回收将变得非常困难。
- en: We can address this issue by using Kubecost, an open source cost monitoring,
    and reporting tool. Note there is a free community edition and a paid commercial
    product that expands on the open source project. Kubecost connects your cloud
    provider—such as **Amazon Web Services** (**AWS**), Azure, or **Google Cloud Platform**
    (**GCP**)—to get the current cost of your resources, then ties that cost to the
    Pods consuming it in the cluster. For example, you use the latest and greatest
    CPU on some nodes with old CPU (cheaper) versions on other nodes. Kubecost allows
    you to tie the different costs back to the Pod. Because of this, you can choose
    to switch to newer high-performance CPUs as your applications/Pods run better
    on them and use fewer resources than slower CPUs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用Kubecost来解决这个问题，Kubecost是一款开源的成本监控和报告工具。请注意，Kubecost有免费社区版和收费的商业产品，后者在开源项目的基础上进行了扩展。Kubecost连接你的云提供商——如**Amazon
    Web Services**（**AWS**）、Azure或**Google Cloud Platform**（**GCP**）——以获取资源的当前成本，然后将该成本与集群中消耗这些资源的Pods关联。例如，你在一些节点上使用最新的高性能CPU，而其他节点使用较旧的（更便宜的）CPU版本。Kubecost允许你将不同的成本与Pod相关联。因此，你可以选择切换到更新的高性能CPU，因为你的应用程序/Pods在它们上运行得更好，并且比慢速CPU使用更少的资源。
- en: Kubecost is deployed using a Helm chart or a YAML manifest inside the monitoring
    cluster. Currently, Kubecost doesn't support remote monitoring, meaning it must
    be deployed on each cluster in your environment. In addition, Kubecost uses Prometheus
    to collect metrics in your cluster that can be deployed as part of the Helm chart;
    the same applies to Grafana for presenting dashboards. Kubecost has its own network
    metrics collector for collecting traffic costs for different kinds of traffic;
    for example, some cloud providers charge you more for traffic outside your region
    than local traffic. The highest price is for egressing out to the public internet,
    which can be very expensive. The saying is that AWS wants you to bring data into
    their environment at little to no cost but will charge you an arm and leg to get
    it back out.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Kubecost使用Helm图表或YAML清单在监控集群中进行部署。目前，Kubecost不支持远程监控，这意味着它必须在您环境中的每个集群上进行部署。此外，Kubecost使用Prometheus来收集集群中的度量数据，这些数据可以作为Helm图表的一部分进行部署；Grafana也适用于展示仪表盘。Kubecost有自己的网络度量收集器，用于收集不同类型流量的流量费用；例如，某些云提供商对于跨区域流量的收费高于本地流量。最高的费用是从公共互联网出口的流量，费用可能非常昂贵。俗话说，AWS希望您将数据带入他们的环境时几乎不收费，但将数据取回时却会收取高昂费用。
- en: 'The steps can be as simple as executing the `helm install` command with a token
    that ties your install to your Kubecost account for installing Kubecost, an example
    of which you can find in the following screenshot. A complete list of Helm chart
    options can be found at [https://github.com/kubecost/cost-analyzer-helm-chart/blob/master/README.md#config-options](https://github.com/kubecost/cost-analyzer-helm-chart/blob/master/README.md#config-options).
    These options allow you to customize your deployment:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤可以像执行`helm install`命令一样简单，该命令使用一个令牌将您的安装与Kubecost账户关联，您可以在下图中找到一个示例。Helm图表选项的完整列表可以在[https://github.com/kubecost/cost-analyzer-helm-chart/blob/master/README.md#config-options](https://github.com/kubecost/cost-analyzer-helm-chart/blob/master/README.md#config-options)中找到。这些选项允许您自定义部署：
- en: '![Figure 18.3 – ResourceQuota YAML example with more options'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图18.3 – 具有更多选项的ResourceQuota YAML示例'
- en: '](img/B18053_18_03.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_18_03.jpg)'
- en: Figure 18.3 – ResourceQuota YAML example with more options
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.3 – 具有更多选项的ResourceQuota YAML示例
- en: In the preceding example, we install Kubecost with the default settings, which
    will deploy its own instances of Prometheus, Node Exporter, and Grafana. It is
    highly recommended to reuse your current Rancher monitoring `global.prometheus.enabled=false`,
    `prometheus.kube-state-metrics.disabled=true`, and `prometheus.nodeExporter.enabled=true`
    options. I would also recommend reading through the Kubecost documentation at
    [https://guide.kubecost.com](https://guide.kubecost.com), which includes adding
    external resources such as **Simple Storage Service** (**S3**) and **Relational
    Database Service** (**RDS**). In addition, their guide walks you through the steps
    needed to allow Kubecost to query your billing information for any custom pricing;
    for example, larger AWS customers can get a disconnect on their accounts for some
    resource types.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，我们使用默认设置安装Kubecost，这将部署Prometheus、Node Exporter和Grafana的独立实例。强烈建议重用当前Rancher监控的`global.prometheus.enabled=false`、`prometheus.kube-state-metrics.disabled=true`和`prometheus.nodeExporter.enabled=true`选项。我还建议阅读Kubecost的文档，网址为[https://guide.kubecost.com](https://guide.kubecost.com)，该文档包括添加外部资源，如**简单存储服务**（**S3**）和**关系型数据库服务**（**RDS**）。此外，他们的指南会引导您完成允许Kubecost查询账单信息以获取任何自定义定价的步骤；例如，AWS的大型客户可以在某些资源类型上获得账户的断开连接。
- en: Finally, Kubecost has an experimental hosted offering, which can be found at
    [https://guide.kubecost.com/hc/en-us/articles/4425132038167-Installing-Agent-for-Hosted-Kubecost-Alpha-](https://guide.kubecost.com/hc/en-us/articles/4425132038167-Installing-Agent-for-Hosted-Kubecost-Alpha-).
    Also, Kubecost is not the only player in town as products such as Datadog can
    provide cost monitoring and reporting. The essential item is that we want to track
    our costs over time to know when something changes, and—of course—because we understand
    how much each Pod costs, we can create reports for management to show where the
    money is going so that they can turn around and go after application teams to
    pay for their resources. We do this to budget our **information technology** (**IT**)
    spending, allowing us to be proactive instead of reactive.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Kubecost 有一个实验性的托管版本，可以在 [https://guide.kubecost.com/hc/en-us/articles/4425132038167-Installing-Agent-for-Hosted-Kubecost-Alpha-](https://guide.kubecost.com/hc/en-us/articles/4425132038167-Installing-Agent-for-Hosted-Kubecost-Alpha-)
    找到。此外，Kubecost 不是唯一的解决方案，像 Datadog 这样的产品也可以提供成本监控和报告。关键在于，我们希望追踪成本变化，以便知道何时发生变化，当然，由于我们了解每个
    Pod 的成本，我们可以为管理层生成报告，展示资金流向，让他们能够回头追究应用团队的资源费用。我们这么做是为了**信息技术**（**IT**）支出的预算规划，使我们能够主动应对，而不是被动反应。
- en: Summary
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter went over what Pod limits and requests are and how they are enforced
    by Kubernetes, including how they are calculated. We then covered how to use resource
    quotas at the namespace level to limit these to the team/application level, as
    well as limiting other resources such as load balancers, secrets, and so on. We
    then worked our way up the chain by covering the topic of Rancher projects, allowing
    us to set limits across a namespace. Finally, we covered how to use Kubecost to
    monitor our Kubernetes costs over time, including how to install and customize
    it, but more importantly, we covered why we want to monitor and track our costs
    over time. In addition, we covered some additional solutions such as Datadog.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 Pod 限制和请求是什么，以及它们如何通过 Kubernetes 强制执行，包括它们是如何计算的。然后，我们介绍了如何在命名空间级别使用资源配额，限制这些配额到团队/应用级别，同时限制其他资源，如负载均衡器、密钥等。接着，我们通过介绍
    Rancher 项目进一步深入，允许我们跨命名空间设置限制。最后，我们讨论了如何使用 Kubecost 监控 Kubernetes 成本变化，包括如何安装和自定义它，但更重要的是，我们探讨了为什么要追踪和监控成本。此外，我们还介绍了一些其他解决方案，如
    Datadog。
- en: The journey we have undertaken together is coming to a close. I want to congratulate
    you on finishing this book, as well as thank you for taking the time to learn
    about Rancher and Kubernetes. My final note is to remember that Rancher and Kubernetes
    are an ever-evolving ecosystem. It would be best if you always kept learning more,
    with Rancher's official events being an excellent resource for learning about
    new features in both Rancher and Kubernetes. You can find out more about this
    at [https://rancher.com/events](https://rancher.com/events).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们共同走过的旅程即将结束。我想祝贺你完成这本书，同时感谢你花时间了解 Rancher 和 Kubernetes。最后一点要提醒的是，Rancher 和
    Kubernetes 是一个不断发展的生态系统。最好持续不断地学习，Rancher 官方活动是了解 Rancher 和 Kubernetes 新功能的绝佳资源。你可以通过
    [https://rancher.com/events](https://rancher.com/events) 了解更多内容。
