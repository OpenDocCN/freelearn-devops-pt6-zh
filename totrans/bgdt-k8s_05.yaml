- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Big Data Processing with Apache Spark
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Apache Spark 进行大数据处理
- en: As seen in the preceding chapter, Apache Spark has rapidly become one of the
    most widely used distributed data processing engines for big data workloads. In
    this chapter, we will cover the fundamentals of using Spark for large-scale data
    processing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所示，Apache Spark 已迅速成为用于大数据工作负载的最广泛使用的分布式数据处理引擎之一。在本章中，我们将介绍使用 Spark 进行大规模数据处理的基础知识。
- en: We’ll start by discussing how to set up a local Spark environment for development
    and testing. You’ll learn how to launch an interactive PySpark shell and use Spark’s
    built-in DataFrames API to explore and process sample datasets. Through coding
    examples, you’ll gain practical experience with essential PySpark data transformations
    such as filtering, aggregations, and joins.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论如何为开发和测试设置本地 Spark 环境。您将学习如何启动交互式 PySpark Shell，并使用 Spark 内置的 DataFrames
    API 来探索和处理示例数据集。通过编码示例，您将获得有关 PySpark 数据转换的实际经验，例如过滤、聚合和连接操作。
- en: Next, we’ll explore Spark SQL, which allows you to query structured data in
    Spark via SQL. You’ll learn how Spark SQL integrates with other Spark components
    and how to use it to analyze DataFrames. We’ll also cover best practices for optimizing
    Spark workloads. While we won’t dive deep into tuning cluster resources and parameters
    in this chapter, you’ll learn about configurations that can greatly improve Spark
    job performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索 Spark SQL，它允许您通过 SQL 查询 Spark 中的结构化数据。您将学习 Spark SQL 如何与其他 Spark 组件集成，以及如何使用它来分析
    DataFrame。我们还将讨论优化 Spark 工作负载的最佳实践。虽然本章不会深入探讨如何调优集群资源和参数，但您将了解一些配置，这些配置可以显著提高
    Spark 作业的性能。
- en: By the end of this chapter, you’ll understand the Spark architecture and know
    how to set up a local PySpark environment, load data into Spark DataFrames, transform
    and analyze data using PySpark, query data via Spark SQL, and apply some performance
    optimizations. With these fundamental Spark skills, you’ll be prepared to scale
    up to tackling big data processing challenges using Spark’s unified engine for
    large-scale data analytics.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将理解 Spark 架构，并了解如何设置本地 PySpark 环境，加载数据到 Spark DataFrame，使用 PySpark 转换和分析数据，通过
    Spark SQL 查询数据，并应用一些性能优化。掌握了这些 Spark 基础技能，您将为使用 Spark 的统一引擎处理大数据分析挑战做好准备。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Getting started with Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入门 Spark
- en: The DataFrame API and the Spark SQL API
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame API 和 Spark SQL API
- en: Working with real data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用真实数据
- en: By the end of this chapter, you will have hands-on experience with loading,
    transforming, and analyzing large datasets using PySpark, the Python API for Spark.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将获得使用 PySpark（Spark 的 Python API）加载、转换和分析大数据集的实践经验。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To run Spark locally, you will need Java 8 or later and the configuration of
    a `JAVA_HOME` environment variable. To do that, follow the instructions at [https://www.java.com/en/download/help/download_options.html](https://www.java.com/en/download/help/download_options.html).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要在本地运行 Spark，您需要 Java 8 或更高版本，并配置 `JAVA_HOME` 环境变量。为此，请按照[https://www.java.com/en/download/help/download_options.html](https://www.java.com/en/download/help/download_options.html)中的说明操作。
- en: 'To better visualize Spark processes, we will use it interactively with JupyterLab.
    You should also ensure that this feature is available within your Python distribution.
    To install Jupyter, follow the instructions here: [https://jupyter.org/install](https://jupyter.org/install).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了更好地可视化 Spark 过程，我们将通过 JupyterLab 进行交互式使用。您还应该确保您的 Python 发行版中已启用此功能。要安装 Jupyter，请按照此处的说明进行操作：[https://jupyter.org/install](https://jupyter.org/install)。
- en: All the code for this chapter is available in the `Chapter05` folder of this
    book’s GitHub repository at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的所有代码都可以在本书 GitHub 仓库的 `Chapter05` 文件夹中找到，地址为：[https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes)。
- en: Getting started with Spark
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门 Spark
- en: In this first section, we will learn how to get Spark up and running on our
    local machine. We will also get an overview of Spark’s architecture and some of
    its core concepts. This will set the foundation for the more practical data processing
    sections later in the chapter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何在本地计算机上启动并运行 Spark。我们还将概览 Spark 的架构和一些核心概念。这将为本章后续的实际数据处理部分打下基础。
- en: Installing Spark locally
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地安装 Spark
- en: 'Installing Spark nowadays is as easy as a `pip3` `install` command:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在安装 Spark 和执行 `pip3 install` 命令一样简单：
- en: 'After you have installed Java 8, run the following command:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 Java 8 后，运行以下命令：
- en: '[PRE0]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will install PySpark along with its dependencies, such as Spark itself.
    You can test whether the installation was successful by running this command in
    a terminal:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将安装 PySpark 及其依赖项，例如 Spark 本身。你可以通过在终端运行以下命令来测试安装是否成功：
- en: '[PRE1]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You should see a simple output with the Spark logo and Spark version in your
    terminal.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会在终端看到一个简单的输出，显示 Spark 的 logo 和版本信息。
- en: Spark architecture
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 架构
- en: 'Spark follows a distributed/cluster architecture, as you can see in the following
    figure:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 采用分布式/集群架构，正如你在下面的图示中看到的：
- en: '![Figure 5.1 – Spark cluster architecture](img/B21927_05_01.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – Spark 集群架构](img/B21927_05_01.jpg)'
- en: Figure 5.1 – Spark cluster architecture
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – Spark 集群架构
- en: The centerpiece that coordinates the Spark application is called the `SparkSession`
    object that integrates directly with a Spark context. The Spark Context connects
    to a cluster manager that can provision resources across a computing cluster.
    When running locally, an embedded cluster manager runs within the same **Java
    Virtual Machine** (**JVM**) as the driver program. But in production, Spark should
    be configured to use a standalone cluster resource manager such as Yarn or Mesos.
    In our case, we will see later how Spark uses Kubernetes as a cluster manager
    structure.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 协调 Spark 应用程序的核心部分叫做 `SparkSession` 对象，它直接与 Spark 上下文集成。Spark 上下文连接到一个集群管理器，集群管理器可以在计算集群中配置资源。当本地运行时，一个嵌入式集群管理器会与驱动程序程序在同一个**Java
    虚拟机**（**JVM**）内运行。但在生产环境中，Spark 应配置为使用如 Yarn 或 Mesos 这样的独立集群资源管理器。我们稍后将看到 Spark
    如何使用 Kubernetes 作为集群管理器结构。
- en: The cluster manager is responsible for allocating computational resources and
    isolating computations on the cluster. When the driver program requests resources,
    the cluster manager launches Spark executors to perform the required computations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理器负责分配计算资源并隔离集群中的计算。当驱动程序请求资源时，集群管理器会启动 Spark 执行器来执行所需的计算任务。
- en: Spark executors
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 执行器
- en: '**Spark executors** are processes launched on worker nodes in the cluster by
    the cluster manager. They run computations and store data for the Spark application.
    Each application has its own executors that stay up for the duration of the whole
    application and run tasks in multiple threads. Spark executes code snippets called
    **tasks** to perform distributed data processing.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark 执行器**是由集群管理器在集群中的工作节点上启动的进程。它们执行计算任务并为 Spark 应用存储数据。每个应用都有自己的执行器，这些执行器在整个应用程序运行期间保持运行，并在多个线程中执行任务。Spark
    执行名为**任务**的代码片段以执行分布式数据处理。'
- en: Components of execution
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行组件
- en: A **Spark job** triggers the execution of a Spark program. This gets divided
    into smaller sets of tasks called **stages** that depend on each other.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark 作业**触发 Spark 程序的执行。它会被划分为一组个更小的任务集，这些任务集之间相互依赖，称为**阶段**。'
- en: Stages consist of tasks that can be run in parallel. The tasks themselves are
    executed in multiple threads within the executors. The number of tasks that can
    run concurrently within an executor is configured based on the number of **slots**
    (cores) pre-allocated in the cluster.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段由可以并行执行的任务组成。这些任务在执行器内部通过多个线程执行。可以在执行器内并发运行的任务数量基于集群中预先分配的**槽位**（核心）数进行配置。
- en: This whole hierarchy of jobs, stages, tasks, slots, and executors facilitates
    the distributed execution of Spark programs across a cluster. We will go deeper
    into some optimizations around this structure later in the chapter. For now, let’s
    see how we can visualize Spark’s execution components by running a simple interactive
    Spark program.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层次结构包括作业、阶段、任务、槽位和执行器，旨在促进 Spark 程序在集群中的分布式执行。我们将在本章稍后深入探讨与此结构相关的一些优化。现在，让我们通过运行一个简单的交互式
    Spark 程序来可视化 Spark 的执行组件。
- en: Starting a Spark program
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动 Spark 程序
- en: For the next steps, we will use an interactive Python programming environment
    called **Jupyter**. If you don’t have Jupyter installed locally yet, please make
    sure it is installed.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将使用一个名为**Jupyter**的交互式 Python 编程环境。如果你尚未在本地安装 Jupyter，请确保它已安装。
- en: 'You can start a Jupyter environment by typing the following in a terminal:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在终端中键入以下命令来启动 Jupyter 环境：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You will see some output for the Jupyter processes and a new browser window
    should start.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到 Jupyter 进程的输出，并且一个新的浏览器窗口应该会打开。
- en: '![Figure 5.2 – Jupyter interface](img/B21927_05_02.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – Jupyter 界面](img/B21927_05_02.jpg)'
- en: Figure 5.2 – Jupyter interface
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – Jupyter 界面
- en: 'Jupyter will make things easier since we will run an interactive Spark session
    and will be able to monitor Spark through its UI:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter 会使事情变得更加简单，因为我们将运行一个交互式的 Spark 会话，并且能够通过其 UI 监控 Spark：
- en: First, click on the **Python 3** button in the **Notebook** section (*Figure
    5**.2*). This will start a new Jupyter notebook.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，点击 **Python 3** 按钮，位于 **Notebook** 部分（*图 5.2*）。这将启动一个新的 Jupyter 笔记本。
- en: 'Next, we will use some Python code to download the `titanic` dataset from the
    web (available at [https://raw.githubusercontent.com/neylsoncrepalde/titanic_data_with_semicolon/main/titanic.csv](https://raw.githubusercontent.com/neylsoncrepalde/titanic_data_with_semicolon/main/titanic.csv)).
    In the first code chunk, type the following:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用一些 Python 代码从网络下载 `titanic` 数据集（可通过 [https://raw.githubusercontent.com/neylsoncrepalde/titanic_data_with_semicolon/main/titanic.csv](https://raw.githubusercontent.com/neylsoncrepalde/titanic_data_with_semicolon/main/titanic.csv)
    获取）。在第一个代码块中，输入以下内容：
- en: '[PRE3]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we will import the necessary libraries:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将导入必要的库：
- en: '[PRE4]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we will create a dictionary with the name of the file as the key and
    the URL as the value:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个字典，文件名作为键，URL 作为值：
- en: '[PRE5]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we will create a simple Python function to download this dataset and save
    it locally:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个简单的 Python 函数来下载这个数据集并将其保存在本地：
- en: '[PRE6]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we will create a folder named `data` and a subfolder named `titanic`
    to store the dataset. The `exist_ok` parameter lets the code continue and not
    throw an error if these folders already exist. Then, we run our function:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个名为 `data` 的文件夹，以及一个名为 `titanic` 的子文件夹来存储数据集。`exist_ok` 参数允许代码继续运行，如果这些文件夹已经存在则不会抛出错误。然后，我们运行我们的函数：
- en: '[PRE7]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, the `titanic` dataset is available for analysis.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`titanic` 数据集已经可以用于分析。
- en: All of the code presented in this chapter can be found in the [*Chapter 5*](B21927_05.xhtml#_idTextAnchor092)
    folder of this book’s GitHub repository ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter%205](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter%205)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中呈现的所有代码可以在本书 GitHub 仓库的 [*第 5 章*](B21927_05.xhtml#_idTextAnchor092) 文件夹中找到
    ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter%205](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter%205))。
- en: 'Next, we can start configuring our Spark program to analyze this data:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以开始配置 Spark 程序来分析这些数据：
- en: 'To do this, we have to first import the `SparkSession` class and the `functions`
    module. This module will be necessary for most of the data processing we will
    do with Spark:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们必须首先导入 `SparkSession` 类和 `functions` 模块。这个模块将是我们使用 Spark 进行大部分数据处理所必需的：
- en: '[PRE8]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After running the imports, create a Spark session:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行完导入后，创建一个 Spark 会话：
- en: '[PRE9]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This Spark session makes the Spark UI available. We can check it by typing [http://localhost:4040](http://localhost:4040)
    in our browser.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个 Spark 会话使 Spark UI 可用。我们可以通过在浏览器中输入 [http://localhost:4040](http://localhost:4040)
    来查看它。
- en: '![Figure 5.3 – Spark UI](img/B21927_05_03.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – Spark UI](img/B21927_05_03.jpg)'
- en: Figure 5.3 – Spark UI
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – Spark UI
- en: As you can see, there is no data available just yet. Jobs will start showing
    on this monitoring page after we run some things in our Spark program.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，目前还没有数据可用。在我们运行 Spark 程序中的某些操作后，作业将开始显示在这个监控页面上。
- en: Now, let’s get back to our code in Jupyter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到 Jupyter 中的代码。
- en: 'To read the downloaded dataset, run the following:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要读取下载的数据集，运行以下代码：
- en: '[PRE10]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The options in this code state that the first row of the file contains the column
    names (`header = True`), that we want Spark to automatically detect the table
    schema and read it accordingly (`inferSchema = True`), and set the file separator
    or delimiter as `;`.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码的选项说明文件的第一行包含列名（`header = True`），我们希望 Spark 自动检测表格模式并相应地读取它（`inferSchema
    = True`），并设置文件分隔符或定界符为 `;`。
- en: 'To show the first rows of the dataset, run the following:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要显示数据集的前几行，运行以下代码：
- en: '[PRE11]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, if we get back to the Spark UI, we can already see finished jobs.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，如果我们回到 Spark UI，我们已经可以看到完成的作业。
- en: '![Figure 5.4 – The Spark UI with jobs](img/B21927_05_04.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 带有作业的 Spark UI](img/B21927_05_04.jpg)'
- en: Figure 5.4 – The Spark UI with jobs
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 带有作业的 Spark UI
- en: We can check other tabs in the Spark UI for stages and tasks, and visualize
    queries sent to Spark in the **SQL / DataFrame** tab. We will explore those tabs
    later in this chapter for further analysis.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查 Spark UI 中其他选项卡，查看各个阶段和任务，并在 **SQL / DataFrame** 选项卡中可视化发送到 Spark 的查询。我们将在本章稍后对这些选项卡进行进一步分析。
- en: In the next section, we will focus on understanding Spark programming using
    the Python (DataFrame API) and SQL (Spark SQL API) languages and how Spark ensures
    maximum performance regardless of our choice of programming language.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将重点理解如何使用Python（DataFrame API）和SQL（Spark SQL API）语言进行Spark编程，以及Spark如何确保无论我们选择哪种编程语言，都能达到最佳性能。
- en: The DataFrame API and the Spark SQL API
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame API和Spark SQL API
- en: Spark provides different APIs built on top of the core RDD API (the native,
    low-level Spark language) to make it easier to develop distributed data processing
    applications. The two most popular higher-level APIs are the DataFrame API and
    the Spark SQL API.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了不同的API，构建在核心RDD API（原生的低级Spark语言）之上，旨在简化分布式数据处理应用程序的开发。最受欢迎的两个高级API是DataFrame
    API和Spark SQL API。
- en: The DataFrames API provides a domain-specific language to manipulate distributed
    datasets organized into named columns. Conceptually, it is equivalent to a table
    in a relational database or a DataFrame in Python pandas, but with richer optimizations
    under the hood. The DataFrames API enables users to abstract data processing operations
    behind domain-specific terminology such as *grouping* and *joining* instead of
    thinking in `map` and `reduce` operations.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API提供了一种领域特定语言，用于操作组织成命名列的分布式数据集。从概念上讲，它等同于关系数据库中的表或Python pandas中的DataFrame，但在底层有更丰富的优化。DataFrame
    API使用户能够在领域特定的术语（如*分组*和*连接*）后抽象数据处理操作，而不是考虑`map`和`reduce`操作。
- en: The Spark SQL API builds further on top of the DataFrames API by exposing Spark
    SQL, a Spark module for structured data processing. Spark SQL allows users to
    run SQL queries against DataFrames to filter or aggregate data. The SQL queries
    get optimized and translated into native Spark code to be executed. This makes
    it easy for users familiar with SQL to run ad hoc queries against data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL API在DataFrames API的基础上进一步构建，通过暴露Spark SQL，一个用于结构化数据处理的Spark模块。Spark
    SQL允许用户在DataFrame上运行SQL查询，以便对数据进行过滤或聚合。SQL查询会被优化并转换为原生Spark代码执行。这使得熟悉SQL的用户可以轻松地针对数据运行临时查询。
- en: Both APIs rely on the Catalyst optimizer, which leverages advanced programming
    techniques such as predicate pushdown, projection pruning, and a variety of join
    optimizations to build efficient query plans before execution. This differentiates
    Spark from other distributed data processing frameworks by optimizing queries
    based on business logic instead of on hardware considerations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 两个API都依赖Catalyst优化器，它利用先进的编程技术，如谓词下推、投影剪枝和多种连接优化，来构建高效的查询计划。这样，Spark通过根据业务逻辑而非硬件考虑来优化查询，从而与其他分布式数据处理框架区别开来。
- en: When working with Spark SQL and the DataFrames API, it is important to understand
    some key concepts that allow Spark to run fast, optimized data processing. These
    concepts are transformations, actions, lazy evaluation, and data partitioning.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Spark SQL和DataFrames API时，理解一些关键概念非常重要，这些概念使Spark能够快速、高效地进行数据处理。这些概念包括转换、操作、懒评估和数据分区。
- en: Transformations
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换
- en: Transformations define computations that will be done, while actions trigger
    the actual execution of those transformations.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 转换定义了将要执行的计算，而操作触发了这些转换的实际执行。
- en: 'Transformations are operations that produce new DataFrames from existing ones.
    Here are some examples of transformations in Spark:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 转换是从现有的DataFrame中生成新DataFrame的操作。以下是Spark中的一些转换示例：
- en: 'This is the `select` command to select columns in a DataFrame (`df`):'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是`select`命令，用于在DataFrame（`df`）中选择列：
- en: '[PRE12]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is the `filter` command to filter rows based on a given condition:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是`filter`命令，用于根据给定条件过滤行：
- en: '[PRE13]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This is the `orderBy` command to sort the DataFrame based on a given column:'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是`orderBy`命令，用于根据给定列对DataFrame进行排序：
- en: '[PRE14]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Grouped aggregations can be done with the `groupBy` command and aggregation
    functions:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分组聚合可以通过`groupBy`命令和聚合函数来完成：
- en: '[PRE15]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The key thing to understand is that transformations are *lazy*. When you call
    a transformation such as `filter()` or `orderBy()`, no actual computation is performed.
    Instead, Spark just remembers the transformation to apply and waits until an action
    is called to actually execute the computation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 关键要理解的是，转换是*懒惰*的。当你调用`filter()`或`orderBy()`等转换时，并不会执行实际的计算。相反，Spark仅仅记住要应用的转换，并等待直到调用操作时才会执行计算。
- en: This lazy evaluation allows Spark to optimize the full sequence of transformations
    before executing them. This can lead to significant performance improvements compared
    to eager evaluation engines that execute each operation immediately.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种懒惰求值使得 Spark 在执行之前可以优化整个转换序列。与立即执行每个操作的贪婪求值引擎相比，这可能带来显著的性能提升。
- en: Actions
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行动
- en: 'While transformations describe operations on DataFrames, actions actually execute
    the computation and return results. Some common actions in Spark include the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然转换描述了对 DataFrame 的操作，但行动实际上会执行计算并返回结果。在 Spark 中，一些常见的行动包括以下内容：
- en: 'The `count` command to return the number of rows in a DataFrame:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count` 命令用于返回 DataFrame 中的行数：'
- en: '[PRE16]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `first` command to return the first row in a DataFrame:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`first` 命令用于返回 DataFrame 中的第一行：'
- en: '[PRE17]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `show` command to print the content of a DataFrame:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`show` 命令用于打印 DataFrame 的内容：'
- en: '[PRE18]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `collect` command to return an array with all the rows in a DataFrame:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`collect` 命令用于返回包含 DataFrame 所有行的数组：'
- en: '[PRE19]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `write` command to write a DataFrame to a given path:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`write` 命令用于将 DataFrame 写入指定路径：'
- en: '[PRE20]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'When an action is called on a DataFrame, several things happen:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 DataFrame 上调用一个行动时，会发生以下几个事情：
- en: The Spark engine looks at the sequence of transformations that have been applied
    and creates an execution plan to perform them efficiently. This is when optimizations
    happen.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 引擎会查看已应用的转换序列，并创建一个高效的执行计划来执行这些操作。这时就会进行优化。
- en: The execution plan is run across the cluster to perform the actual data manipulation.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行计划在集群中运行以执行实际的数据操作。
- en: The action aggregates and returns the final result to the driver program.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该行动会聚合并将最终结果返回给驱动程序。
- en: So, in summary, transformations describe a computation but do not execute it
    immediately. Actions trigger lazy evaluation and execution of the Spark job, returning
    concrete results.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，转换描述了一个计算过程，但不会立即执行它。行动会触发懒惰求值和 Spark 作业的执行，返回具体的结果。
- en: The process of storing the computation instructions to execute later is called
    **lazy evaluation**. Let’s take a closer look at this concept.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 将计算指令存储起来以便稍后执行的过程称为 **懒惰求值**。让我们更详细地了解这个概念。
- en: Lazy evaluation
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 懒惰求值
- en: Lazy evaluation is a key technique that allows Apache Spark to run efficiently.
    As mentioned previously, when you apply transformations to a DataFrame, no actual
    computation happens at that time.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 懒惰求值是一项关键技术，使得 Apache Spark 能够高效运行。如前所述，当你对 DataFrame 应用转换时，并不会立即进行实际的计算。
- en: Instead, Spark internally records each transformation as an operation to apply
    to the data. The actual execution is deferred until an action is called.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，Spark 会内部记录每个转换作为操作来应用于数据。实际执行被推迟，直到调用行动时才会执行。
- en: 'This delayed computation is very useful for the following reasons:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这种延迟计算非常有用，原因如下：
- en: '**Avoids unnecessary operations**: By looking at the sequence of many transformations
    together, Spark is able to optimize which parts of the computation are actually
    required to return the final result. Some intermediate steps may be eliminated
    if not needed.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免不必要的操作**：通过查看许多转换的顺序，Spark 能够优化哪些计算部分实际上需要返回最终结果。如果某些中间步骤不需要，它们可能会被省略。'
- en: '**Runtime optimizations**: At the moment when an action is triggered, Spark
    formulates an efficient physical execution plan based on partitioning, available
    memory, and parallelism. It makes these optimizations dynamically at runtime.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行时优化**：当行动被触发时，Spark 会根据分区、可用内存和并行性制定一个高效的物理执行计划。它在运行时动态地进行这些优化。'
- en: '**Batches operations together**: Several transformations over multiple DataFrames
    can be batched together into fewer jobs. This amortizes the overhead of job scheduling
    and initialization across many computation steps.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将批处理操作组合在一起**：多个 DataFrame 上的几个转换可以被批量处理为更少的作业。这会把作业调度和初始化的开销分摊到许多计算步骤中。'
- en: As an example, consider a DataFrame with user clickstream data that needs to
    be filtered, aggregated, and sorted before returning the final top 10 rows.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，考虑一个包含用户点击流数据的 DataFrame，在返回最终的前 10 行之前，需要对其进行过滤、聚合和排序。
- en: With lazy evaluation, all these transformations would be recorded when defined,
    and a single optimized job would be executed when the final rows are requested
    via `collect()` or `show()`. Without lazy evaluation, the engine would need to
    execute a separate job for `filter()`, another job for `groupBy()`, another job
    for `orderBy()`, and so on for each step. This would be highly inefficient.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过惰性计算，所有这些转换会在定义时被记录，当通过 `collect()` 或 `show()` 请求最终行时，才会执行一个优化后的任务。如果没有惰性计算，引擎需要为
    `filter()` 执行一个独立的任务，为 `groupBy()` 执行另一个任务，为 `orderBy()` 执行另一个任务，以此类推。这将非常低效。
- en: So, in summary, lazy evaluation separates the definition of the computational
    steps from their execution. This allows Spark to come up with an optimized physical
    plan to perform the full sequence of operations. Next, we will see how Spark can
    distribute computations through data partitioning.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，惰性计算将计算步骤的定义与执行分开。这允许 Spark 提出一个优化的物理计划来执行整个操作序列。接下来，我们将看到 Spark 如何通过数据分区来分配计算任务。
- en: Data partitioning
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分区
- en: Spark’s speed comes from its ability to distribute data processing across a
    cluster. To enable parallel processing, Spark breaks up data into independent
    partitions that can be processed in parallel on different nodes in the cluster.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的速度来源于其能够将数据处理分发到集群中。为了实现并行处理，Spark 将数据划分为独立的分区，可以在集群中的不同节点上并行处理。
- en: When you read data into a Spark DataFrame or RDD, the data is divided into logical
    partitions. On a cluster, Spark will then schedule task execution so that partitions
    run in parallel on different nodes. Each node may process multiple partitions.
    This allows the overall job to process data much faster than if run sequentially
    on a single node.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将数据读入 Spark 的 DataFrame 或 RDD 时，数据会被划分为逻辑分区。在集群上，Spark 会调度任务执行，使得分区能够在不同节点上并行运行。每个节点可以处理多个分区。这使得整个任务的处理速度比在单个节点上按顺序执行要快得多。
- en: Understanding data partitioning in Spark is key to understanding the differences
    between `narrow` and `wide` transformations.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 Spark 中的数据分区对于理解 `狭义` 和 `广义` 转换的区别至关重要。
- en: Narrow versus wide transformations
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 狭义转换与广义转换
- en: Narrow transformations are operations that can be performed on each partition
    independently without any data shuffling across nodes. Examples include `map`,
    `filter`, and other per-record transformations. These allow parallel processing
    without network traffic overhead.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 狭义转换是指可以在每个分区独立执行的操作，而不需要在节点之间进行任何数据洗牌。例子包括 `map`、`filter` 和其他每条记录的转换。这些操作允许并行处理而不会产生网络流量的开销。
- en: Wide transformations require data to be shuffled between partitions and nodes.
    Examples include `groupBy` aggregations, joins, sorts, and window functions. These
    involve either combining data from multiple partitions or repartitioning data
    based on a key.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 广义转换要求在分区和节点之间进行数据洗牌。例子包括 `groupBy` 聚合、连接、排序和窗口函数。这些操作要么涉及将多个分区的数据合并，要么基于某个键重新分区数据。
- en: 'Here is an example to illustrate. We are filtering a DataFrame and keeping
    only rows that have an age value below 20:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个例子来说明。我们正在过滤一个 DataFrame，并且只保留年龄小于 20 的行：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The filtering by age is done independently in each data partition.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 按年龄过滤是在每个数据分区中独立进行的。
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The grouped aggregation requires data exchange between partitions in the cluster.
    This exchange is what we call **shuffle**.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 分组聚合需要在集群中的分区之间交换数据。这个交换就是我们所说的 **shuffle**。
- en: Why does this distinction matter? When possible, it’s best to structure Spark
    workflows with more narrow transformations first before wide ones. This minimizes
    data shuffling across the network, which improves performance.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这个区分很重要？如果可能的话，最好先进行狭义转换，再进行广义转换。这可以最小化数据在网络上的洗牌，从而提高性能。
- en: For example, it is often better to start by filtering data to the subset needed
    and then apply aggregations/windows/joins on the filtered data afterward, rather
    than applying all operations to the entire dataset. Filtering first decreases
    the data volume shuffled across the network.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，通常最好先通过过滤数据得到需要的子集，然后在过滤后的数据上应用聚合/窗口/连接操作，而不是对整个数据集应用所有操作。先过滤数据可以减少在网络上洗牌的数据量。
- en: Understanding narrow versus wide transformations allows optimizing Spark jobs
    for lower latency and higher throughput by minimizing shuffles and partitioning
    data only when needed. It is a key tuning technique for better Spark application
    performance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 理解窄变换和宽变换的区别，可以通过最小化数据洗牌和仅在需要时分区数据，从而优化 Spark 作业，降低延迟并提高吞吐量。这是优化 Spark 应用性能的关键调优技巧。
- en: Now, let’s try putting those concepts to work with our `titanic` dataset.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试将这些概念应用到我们的`titanic`数据集上。
- en: Analyzing the titanic dataset
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析泰坦尼克号数据集
- en: 'Let’s return to the Jupyter notebook we started building earlier. First, we
    start a `SparkSession` and read the `titanic` dataset into Spark:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们之前开始构建的 Jupyter notebook。首先，我们启动一个`SparkSession`，并将`titanic`数据集读取到 Spark
    中：
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will now use the `printSchema()` command to check the table:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用`printSchema()`命令检查表结构：
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we will apply some narrow transformations in the original dataset. We
    will filter only men who are more than 21 years old and save this transformed
    data into an object called `filtered`:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在原始数据集上应用一些窄变换。我们将只筛选出年龄大于 21 岁的男性，并将此变换后的数据保存到名为`filtered`的对象中：
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now, let’s get back to the Spark UI. What happened? *Nothing!* No computation
    was done because (remember) those commands are transformations and do not trigger
    any computations in Spark.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到 Spark UI。发生了什么？*什么都没有发生！* 没有进行任何计算，因为（记住）这些命令是转换操作，不会触发 Spark 中的任何计算。
- en: '![Figure 5.5 – The Spark UI after transformation](img/B21927_05_05.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 变换后的 Spark UI](img/B21927_05_05.jpg)'
- en: Figure 5.5 – The Spark UI after transformation
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 变换后的 Spark UI
- en: 'But now, we run a `show()` command, which is an action:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 但是现在，我们运行了一个`show()`命令，它是一个动作：
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*Et voilà!* Now, we can see that a new job was triggered in Spark.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*瞧！* 现在，我们可以看到 Spark 中触发了一个新的作业。'
- en: '![Figure 5.6 – The Spark UI after action](img/B21927_05_06.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – 执行动作后的 Spark UI](img/B21927_05_06.jpg)'
- en: Figure 5.6 – The Spark UI after action
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – 执行动作后的 Spark UI
- en: We can also check the execution plan in the **SQL / DataFrame** tab. Click this
    tab and then click on the last executed query (the first row in the table). You
    should see the output as shown in *Figure 5**.7*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在**SQL / DataFrame**选项卡中检查执行计划。点击此选项卡，然后点击最后执行的查询（表格中的第一行）。你应该看到如*图 5.7*所示的输出。
- en: '![Figure 5.7 – Execution plan for Spark filters](img/B21927_05_07.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – Spark 过滤器的执行计划](img/B21927_05_07.jpg)'
- en: Figure 5.7 – Execution plan for Spark filters
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – Spark 过滤器的执行计划
- en: The `titanic` dataset is not big enough for Spark to divide it into partitions.
    Later in the chapter, we will see how shuffle (data exchange between partitions)
    happens when we use wide transformations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`titanic`数据集不够大，Spark 无法将其划分为多个分区。在本章的后面部分，我们将看到使用宽变换时如何进行数据洗牌（分区间的数据交换）。'
- en: The last important thing for this section is to see how Spark uses the DataFrame
    and Spark SQL API and transforms all the instructions into RDD for optimized processing.
    Let’s implement a simple query to analyze the `titanic` dataset. We will do that
    in both Python and SQL.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的最后一个重要内容是观察 Spark 如何使用 DataFrame 和 Spark SQL API，并将所有指令转换为 RDD 以便优化处理。让我们实现一个简单的查询来分析`titanic`数据集。我们将在
    Python 和 SQL 中都实现该查询。
- en: 'First, we calculate how many male persons older than 21 survived the Titanic
    in each traveling class. We save the Python query in an object called `queryp`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算在每个旅行舱次中，21岁以上的男性乘客在泰坦尼克号上的幸存情况。我们将 Python 查询保存在一个名为`queryp`的对象中：
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, we’re going to implement the exact same query but with SQL. For that,
    first, we need to create a temporary view and then we use the `spark.sql()` command
    to run SQL code:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用 SQL 实现完全相同的查询。为此，首先，我们需要创建一个临时视图，然后使用`spakr.sql()`命令来运行 SQL 代码：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Both queries are saved in objects that we can use now to inspect the execution
    plan. Let’s do this:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 两个查询都被保存在对象中，我们现在可以使用它们来检查执行计划。让我们来做这件事：
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If you check the output, you will note that both execution plans are exactly
    the same! This is only possible because Spark takes all the instructions given
    in the higher-level APIs and transforms them into RDD code that runs “under the
    hood.” We can execute both queries with a `show()` command and see that the results
    are the same and they are executed with the same performance:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看输出，你会注意到两个执行计划完全相同！ 这是因为 Spark 会将所有在高级 API 中给出的指令转换成运行在底层的 RDD 代码。我们可以通过`show()`命令执行这两个查询，并看到结果是相同的，它们以相同的性能执行：
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output for both commands is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 两个命令的输出如下：
- en: '[PRE31]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We also can check the execution plan visually in the **SQL / DataFrame** tab
    in the Spark UI. Click the two first rows in this tab (the two latest executions)
    and see that the plan is the same.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在 Spark UI 的 **SQL / DataFrame** 标签中直观地查看执行计划。点击该标签中的前两行（最近的两次执行），你会发现执行计划是相同的。
- en: From now on, let’s try to dig deeper into PySpark code while working with a
    more challenging dataset.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，让我们在处理这个更具挑战性的数据集时，尝试深入挖掘 PySpark 代码。
- en: Working with real data
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用真实数据进行工作
- en: We will now work with the IMDb public dataset. This is a more complex dataset
    divided into various tables.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用 IMDb 公共数据集。这个数据集较为复杂，分成了多个表格。
- en: The following code will download five tables from the `imdb` dataset and save
    them into the `./data/imdb/` path (also available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/get_imdb_data.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/get_imdb_data.py)).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将从 `imdb` 数据集中下载五个表格，并将它们保存到 `./data/imdb/` 路径下（也可以在 [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/get_imdb_data.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/get_imdb_data.py)
    找到该代码）。
- en: 'First, we need to download the data locally:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将数据下载到本地：
- en: get_imdb_data.py
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: get_imdb_data.py
- en: '[PRE32]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we will open a Jupyter notebook, start a `SparkSession`, and read the
    tables (you can find this code at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/analyzing_imdb_data.ipynb](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/analyzing_imdb_data.ipynb)):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将打开一个 Jupyter notebook，启动一个 `SparkSession`，并读取表格（你可以在 [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/analyzing_imdb_data.ipynb](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/analyzing_imdb_data.ipynb)
    找到这段代码）：
- en: '[PRE33]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This time, we are not going to use the `inferSchema` parameter to read the tables.
    `inferSchema` is great when we are dealing with small tables. For big data, however,
    this is not recommended as Spark reads all the tables once to define the schema
    and then a second time to read the data correctly, which can result in poor performance.
    Instead, the best practice is to define the schema previously and read it with
    the defined schema. Note that reading a table like this will not trigger an execution
    up to the point we give any *action* instruction. The schemas for the IMDb dataset
    can be found at [https://developer.imdb.com/non-commercial-datasets/](https://developer.imdb.com/non-commercial-datasets/).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将不使用 `inferSchema` 参数来读取表格。`inferSchema` 在处理小型表格时非常有用，但对于大数据来说并不推荐使用，因为
    Spark 会先读取所有表格一次以定义模式，然后再读取一次数据以正确获取数据，这会导致性能下降。最佳做法是事先定义模式，并使用定义好的模式来读取数据。请注意，像这样读取表格，直到我们给出任何
    *动作* 指令之前，是不会触发执行的。IMDb 数据集的模式可以在 [https://developer.imdb.com/non-commercial-datasets/](https://developer.imdb.com/non-commercial-datasets/)
    找到。
- en: 'In order to correctly read IMDb tables, we first define the schemas:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确读取 IMDb 表格，我们首先定义模式（schemas）：
- en: '[PRE34]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, we will read all the tables passing their defined schemas as a parameter:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将读取所有表格，并将其定义好的模式作为参数传递：
- en: '[PRE35]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, we check that the schema was imported correctly by Spark:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们检查 Spark 是否正确导入了模式：
- en: '[PRE36]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If you check the Spark UI, you will note that *no computation was triggered*.
    This will only be done when we call any *action* function. We will proceed to
    analyze this data. Take a look at the `names` table:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检查 Spark UI，你会注意到*没有触发任何计算*。只有当我们调用任何*动作*函数时，才会执行计算。接下来我们将分析这些数据。来看一下 `names`
    表格：
- en: '[PRE37]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The `.show()` command will yield the following output (which is just some selected
    columns to improve visualization):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`.show()` 命令将产生如下输出（这里只选择了部分列以便更好地展示）：'
- en: '[PRE38]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'And this is the exact moment that Spark actually reads the `names` data, as
    soon as we run the `.show()` command. This table contains information about actors,
    producers, directors, writers, and so on. But note how the `knownForTitles` column
    is structured. It contains all the movies that an individual worked in but as
    a string with all the titles separated by a comma. This could make our lives difficult
    in the future when we need to join this table with other information. Let’s **explode**
    this column into multiple rows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Spark 实际读取 `names` 数据的确切时刻，一旦我们运行 `.show()` 命令。这个表格包含关于演员、制作人、导演、编剧等的信息。但注意
    `knownForTitles` 列的结构。它包含了一个人参与过的所有电影，但这些电影的名称以字符串形式存储，所有标题用逗号隔开。这在未来当我们需要将该表格与其他信息进行联接时，可能会给我们带来麻烦。让我们**展开**这一列，将其转换为多行：
- en: '[PRE39]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Note that we did not select the `primaryProfession` column. We won’t need it
    in this analysis. Now, check the `crew` table:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们没有选择`primaryProfession`列。我们在此分析中不需要它。现在，检查`crew`表：
- en: '[PRE40]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Here is the output`:`
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '这是输出`:` '
- en: '[PRE41]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Here, we have the same case: movies that were directed by more than one person.
    This information is stored as a string with multiple values separated by a comma.
    If you cannot visualize this case at first, try filtering the `crew` table for
    values that contain a comma:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有相同的情况：由多个导演执导的电影。这些信息作为一个字符串存储，多个值之间用逗号分隔。如果你一开始不能想象这种情况，试着筛选`crew`表，查找包含逗号的值：
- en: '[PRE42]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We will explode this column into rows as well:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将这个列展开为多行：
- en: '[PRE43]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Then, you can also check (using the `.show()` command) the other tables but
    they do not have this kind of situation.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你还可以检查（使用`.show()`命令）其他表格，但它们没有这种情况。
- en: 'Now, let’s start analyzing this data. We will visualize the most famous Keanu
    Reeves movies. It is not possible to see that with just one table since, in `names`,
    we only have the movie ID (`tconst`). We need to join the `names` and `basics`
    tables. First, we get only the information on Keanu Reeves:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始分析这些数据。我们将可视化最著名的基努·里维斯电影。仅凭一张表格无法查看这一点，因为在`names`表中，我们只有电影ID（`tconst`）。我们需要将`names`和`basics`表连接起来。首先，我们只获取基努·里维斯的信息：
- en: '[PRE44]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we will join this new table with the `basics` table:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将此新表与`basics`表进行连接：
- en: '[PRE45]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'In this block of code, we are selecting only the columns we need from the `basics`
    table and joining them with the `only_keanu` filtered table. The `join` command
    takes three arguments:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们只选择了`basics`表中需要的列，并将它们与过滤后的`only_keanu`表连接。`join`命令有三个参数：
- en: The table that is going to be joined
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将要连接的表
- en: The columns that will be used
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将使用的列
- en: The type of join that Spark is going to perform
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark将执行的连接类型
- en: In this case, we are using `tconst` and the `knownForTitles` columns to join
    and we are performing an inner join, only keeping the records that are found in
    both tables.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用`tconst`和`knownForTitles`列进行连接，并执行内连接，只保留在两个表中都存在的记录。
- en: 'Before we trigger the results of this join with an action, let’s explore the
    execution plan for this join:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们用动作触发这个连接的结果之前，让我们探索一下这个连接的执行计划：
- en: '[PRE46]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Analyzing the output, we notice that Spark will perform a sort-merge join:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 分析输出时，我们注意到Spark将执行排序-合并连接：
- en: '[PRE47]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Joins are a key operation in Spark and they are directly related to Spark’s
    performance. We will get back to the datasets and the joins we are making later
    in the chapter but, before we continue, a quick word about the internals of Spark
    joins.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 连接是Spark中的一个关键操作，并且与Spark的性能直接相关。稍后我们会回到数据集和我们正在进行的连接，但在继续之前，简要说明一下Spark连接的内部原理。
- en: How Spark performs joins
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark如何执行连接
- en: Spark provides several physical join implementations to perform joins efficiently.
    The choice of join implementation depends on the size of the datasets being joined
    and other parameters.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了几种物理连接实现方式，以高效地执行连接。选择哪种连接实现方式取决于所连接数据集的大小和其他参数。
- en: 'There is a variety of ways in which Spark can internally perform a join. We
    will go through the three most common joins: the sort-merge join, the shuffle
    hash join, and the broadcast join.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Spark内部执行连接的方式有很多种。我们将介绍三种最常见的连接：排序-合并连接、洗牌哈希连接和广播连接。
- en: Sort-merge join
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排序-合并连接
- en: 'The **sort-merge join**, as the name suggests, sorts both sides of the join
    on the join key before applying the join. Here are the steps involved:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**排序-合并连接**，顾名思义，在应用连接之前会先对连接键进行排序。以下是涉及的步骤：'
- en: Spark reads the left and right side DataFrames/RDDs and applies any projections
    or filters needed.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark读取左右两边的DataFrame/RDD，并应用任何所需的投影或过滤。
- en: Next, both sides are sorted based on the join keys. This rearrangement of data
    is known as a shuffle, which involves moving data across the cluster.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，两个侧面会根据连接键进行排序。这种数据的重新排列被称为洗牌，它涉及在集群中移动数据。
- en: After the shuffle, rows with the same join key will be co-located on the same
    partition. Spark then merges the sorted partitions by comparing values with the
    same join key on both sides and emitting join output rows.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在洗牌之后，具有相同连接键的行将被定位在同一分区上。然后，Spark通过比较两边具有相同连接键的值来合并排序后的分区，并生成连接输出行。
- en: The sort-merge join works well when the data on both sides can fit into memory
    after the shuffle. The preprocessing step of sorting enables a fast merge. However,
    the shuffle can be expensive for large datasets.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当双方数据在洗牌后都能适配内存时，排序-合并连接效果较好。排序的预处理步骤能够加速合并。然而，洗牌对于大数据集来说可能会非常耗费资源。
- en: Shuffle hash join
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 洗牌哈希连接
- en: 'The **shuffle hash join** optimizes the sort-merge join by avoiding the *sort*
    phase. Here are the main steps:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**洗牌哈希连接**通过避免*排序*阶段来优化排序-合并连接。以下是主要步骤：'
- en: Spark partitions both sides based on the hash of the join key. This partitions
    rows with the same key to the same partition.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 会根据连接键的哈希值对两边数据进行分区。这将相同键的行分配到同一分区。
- en: Since rows with the same key hash to the same partition, Spark can build hash
    tables from one side, probe the hash table for matches from the other side, and
    emit join results within each partition.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于相同键的行被哈希到相同的分区，Spark 可以从一方构建哈希表，从另一方查询哈希表中的匹配项，并在每个分区内输出连接结果。
- en: The shuffle hash join reads each side only once. By avoiding the sort, it saves
    I/O and CPU costs compared to the sort-merge join. But it is less efficient than
    sort-merge when the joined datasets after shuffling can fit into memory.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 洗牌哈希连接每一方仅读取一次。通过避免排序，它比排序-合并连接节省了 I/O 和 CPU 成本。但是当洗牌后的连接数据集可以适配内存时，它的效率不如排序-合并连接。
- en: Broadcast hash join
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 广播哈希连接
- en: 'If one side of the join is small enough to fit into the memory of each executor,
    Spark can broadcast that side using the **broadcast hash join**. Here are the
    steps:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果连接的一方足够小，可以适应每个执行器的内存，Spark 可以使用**广播哈希连接**来广播这一方。以下是步骤：
- en: The smaller DataFrame is hashed and broadcast to all worker nodes. This allows
    the entire dataset to be read into memory.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 较小的 DataFrame 会被哈希并广播到所有工作节点。这使得整个数据集可以加载到内存中。
- en: The larger side is then partitioned by the join key. Each partition probes the
    broadcast in-memory hash table to find matches and emit join results.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 较大的一方随后会根据连接键进行分区。每个分区会查询广播的内存哈希表以寻找匹配项，并输出连接结果。
- en: Since data transfer is minimized, broadcast joins are very fast. Spark automatically
    chooses this if one side is small enough to broadcast. However, the maximum size
    depends on the memory available for broadcasting.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据传输最小化，广播连接非常快速。如果一方足够小以便广播，Spark 会自动选择这一方式。然而，最大大小取决于用于广播的内存。
- en: Now, let’s get back to our datasets and try to force Spark to perform a type
    of join different from the sort-merge join it automatically decided to do.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到数据集，尝试强制 Spark 执行不同于自动选择的排序-合并连接类型的连接。
- en: Joining IMDb tables
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接 IMDb 表
- en: 'The `keanu_movies` query execution plan will perform a sort-merge join, which
    was automatically chosen by Spark because, in this case, it probably brings the
    best performance. Nevertheless, we can force Spark to perform a different kind
    of join. Let’s try a broadcast hash join:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`keanu_movies` 查询执行计划将执行一个排序-合并连接，这是 Spark 自动选择的，因为在这种情况下，它可能带来最佳的性能。不过，我们也可以强制
    Spark 执行不同类型的连接。我们来尝试广播哈希连接：'
- en: '[PRE48]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This query is almost identical to the previous one with one exception: we are
    using the `broadcast` function to force a broadcast join. Let’s check the execution
    plan:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询几乎与之前的查询完全相同，唯一的区别是：我们使用了 `broadcast` 函数来强制执行广播连接。让我们检查一下执行计划：
- en: '[PRE49]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, the execution plan is smaller and it contains a `BroadcastHashJoin` task.
    We can also try to hint at Spark to use the shuffle hash join with the following
    code:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，执行计划变得更小，并且包含一个 `BroadcastHashJoin` 任务。我们也可以尝试通过以下代码提示 Spark 使用洗牌哈希连接：
- en: '[PRE50]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now, let’s take a look at the execution plan:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看执行计划：
- en: '[PRE51]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now, we trigger the execution of all queries with a `show()` command, each
    one in its own code block:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们触发所有查询的执行，通过 `show()` 命令，每个查询都放在自己的代码块中：
- en: '[PRE52]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We can see that the results are exactly the same. However, the way Spark handled
    the joins internally was different and with different performances. Check the
    **SQL / DataFrame** tab in the Spark UI to visualize the executed queries.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到结果完全相同。不过，Spark 在内部处理连接的方式不同，性能也不同。查看 Spark UI 中的**SQL / DataFrame**标签，可以可视化执行的查询。
- en: 'If we wanted to use only SQL to check Keanu Reeves’ movies, we can – by creating
    a temporary view and using the `spark.sql()` command:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仅想使用 SQL 来检查基努·里维斯的电影，我们可以通过创建一个临时视图并使用 `spark.sql()` 命令来实现：
- en: '[PRE53]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Now, let’s try one more query. Let’s see whether we can answer the question,
    *Who were the directors, producers, and writers of the movies in which Tom Hanks
    and Meg Ryan acted together, and which of them has the* *highest rating?*
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们再尝试一个查询。让我们看看是否能够回答这个问题：*汤姆·汉克斯和梅格·瑞恩共同出演的电影的导演、制片人和编剧是谁，哪部电影的评分最高？*
- en: 'First, we have to check `Tom Hanks` and `Meg Ryan` codes in the `names` table:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要检查`Tom Hanks`和`Meg Ryan`在`names`表中的编码：
- en: '[PRE54]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This is the result:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE55]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This query shows us a lot of different Meg Ryan’s codes but the one we want
    is the first one with several movies in the `knownForTitles` column. Then, we
    will find out the movies that they both acted in together. To do that, we will
    filter only movies with their codes in the `principals` table and count the number
    of actors by movie. The ones with two actors in one movie should be the movies
    they acted in together:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询显示了梅格·瑞恩的许多不同编码，但我们想要的是第一项，它在`knownForTitles`列中有几部电影。接着，我们将找出他们两人共同出演的电影。为此，我们将过滤出在`principals`表中有他们编码的电影，并按电影计算演员人数。那些有两位演员的电影应该就是他们共同出演的电影：
- en: '[PRE56]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'And we get this result:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到这个结果：
- en: '[PRE57]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, we can join this information with the other tables to get the answers
    we need. We will create a `subjoin` table joining the information on `principals`,
    `names`, and `basics`. Let’s save `ratings` for later as it consumes more resources
    than we have in our machine:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将这些信息与其他表连接，得到我们需要的答案。我们将创建一个`subjoin`表，连接`principals`、`names`和`basics`中的信息。由于`ratings`表占用的资源较多，我们先将其保留到后面使用：
- en: '[PRE58]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'To speed up further computations, we will cache this table. This will allow
    Spark to save this `subjoin` table in memory so all the previous joins will not
    be triggered again:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速后续的计算，我们将缓存这个表。这将允许Spark将`subjoin`表保存在内存中，从而避免所有之前的连接再次触发：
- en: '[PRE59]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now, let’s find out what movies Tom and Meg did together:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们找出汤姆和梅格一起出演了哪些电影：
- en: '[PRE60]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This is the final output:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的输出如下：
- en: '[PRE61]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Now, we will find out the directors, producers, and writers of those movies:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将找出这些电影的导演、制片人和编剧：
- en: '[PRE62]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now, we can check the ratings of the movies and order them to get the highest
    rated. To do that, we need to join the `subjoin` cached table with the `ratings`
    table. As `subjoin` is already cached, note how fast this join happens:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以查看这些电影的评分并对其进行排序，从而找出评分最高的电影。为此，我们需要将`subjoin`缓存表与`ratings`表进行连接。由于`subjoin`已经被缓存，注意这次连接发生的速度：
- en: '[PRE63]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'This last join yields the following output:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一次连接的结果如下：
- en: '[PRE64]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: And that’s it! Next, you should try, as an exercise, to redo these queries but
    with SQL and the `spark.sql()` command.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！接下来，作为练习，你应该尝试使用SQL和`spak.sql()`命令重新执行这些查询。
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we covered the fundamentals of using Apache Spark for large-scale
    data processing. You learned how to set up a local Spark environment and use the
    PySpark API to load, transform, analyze, and query data in Spark DataFrames.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们介绍了使用Apache Spark进行大规模数据处理的基础知识。你学习了如何设置本地Spark环境，并使用PySpark API加载、转换、分析和查询Spark
    DataFrame中的数据。
- en: We discussed key concepts such as lazy evaluation, narrow versus wide transformations,
    and physical data partitioning that allow Spark to execute computations efficiently
    across a cluster. You gained hands-on experience applying these ideas by filtering,
    aggregating, joining, and analyzing sample datasets with PySpark.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了诸如惰性求值、窄变换与宽变换、以及物理数据分区等关键概念，这些概念使得Spark能够在集群中高效执行计算。你通过使用PySpark进行过滤、聚合、连接和分析示例数据集，获得了实际操作经验。
- en: You also learned how to use Spark SQL to query data, which allows those familiar
    with SQL to analyze DataFrames. We looked at Spark’s query optimization and execution
    components to understand how Spark translates high-level DataFrame and SQL operations
    into efficient distributed data processing plans.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学习了如何使用Spark SQL查询数据，这使得熟悉SQL的人能够分析DataFrame。我们了解了Spark的查询优化和执行组件，以理解Spark如何将高级DataFrame和SQL操作转换为高效的分布式数据处理计划。
- en: While we only scratched the surface of tuning and optimizing Spark workloads,
    you learned about some best practices such as minimizing shuffles and using broadcast
    joins where appropriate to improve performance.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们只触及了Spark工作负载调优和优化的表面，但你学到了一些最佳实践，比如最小化洗牌并在适当时使用广播连接来提高性能。
- en: In the next chapter, we will study one of the most used tools for pipeline orchestration,
    Apache Airflow.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习用于管道编排的最常用工具之一——Apache Airflow。
