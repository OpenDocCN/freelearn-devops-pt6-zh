- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big Data Processing with Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As seen in the preceding chapter, Apache Spark has rapidly become one of the
    most widely used distributed data processing engines for big data workloads. In
    this chapter, we will cover the fundamentals of using Spark for large-scale data
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by discussing how to set up a local Spark environment for development
    and testing. You’ll learn how to launch an interactive PySpark shell and use Spark’s
    built-in DataFrames API to explore and process sample datasets. Through coding
    examples, you’ll gain practical experience with essential PySpark data transformations
    such as filtering, aggregations, and joins.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll explore Spark SQL, which allows you to query structured data in
    Spark via SQL. You’ll learn how Spark SQL integrates with other Spark components
    and how to use it to analyze DataFrames. We’ll also cover best practices for optimizing
    Spark workloads. While we won’t dive deep into tuning cluster resources and parameters
    in this chapter, you’ll learn about configurations that can greatly improve Spark
    job performance.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll understand the Spark architecture and know
    how to set up a local PySpark environment, load data into Spark DataFrames, transform
    and analyze data using PySpark, query data via Spark SQL, and apply some performance
    optimizations. With these fundamental Spark skills, you’ll be prepared to scale
    up to tackling big data processing challenges using Spark’s unified engine for
    large-scale data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DataFrame API and the Spark SQL API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with real data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have hands-on experience with loading,
    transforming, and analyzing large datasets using PySpark, the Python API for Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To run Spark locally, you will need Java 8 or later and the configuration of
    a `JAVA_HOME` environment variable. To do that, follow the instructions at [https://www.java.com/en/download/help/download_options.html](https://www.java.com/en/download/help/download_options.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To better visualize Spark processes, we will use it interactively with JupyterLab.
    You should also ensure that this feature is available within your Python distribution.
    To install Jupyter, follow the instructions here: [https://jupyter.org/install](https://jupyter.org/install).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code for this chapter is available in the `Chapter05` folder of this
    book’s GitHub repository at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this first section, we will learn how to get Spark up and running on our
    local machine. We will also get an overview of Spark’s architecture and some of
    its core concepts. This will set the foundation for the more practical data processing
    sections later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark locally
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Installing Spark nowadays is as easy as a `pip3` `install` command:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After you have installed Java 8, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will install PySpark along with its dependencies, such as Spark itself.
    You can test whether the installation was successful by running this command in
    a terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see a simple output with the Spark logo and Spark version in your
    terminal.
  prefs: []
  type: TYPE_NORMAL
- en: Spark architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark follows a distributed/cluster architecture, as you can see in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Spark cluster architecture](img/B21927_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Spark cluster architecture
  prefs: []
  type: TYPE_NORMAL
- en: The centerpiece that coordinates the Spark application is called the `SparkSession`
    object that integrates directly with a Spark context. The Spark Context connects
    to a cluster manager that can provision resources across a computing cluster.
    When running locally, an embedded cluster manager runs within the same **Java
    Virtual Machine** (**JVM**) as the driver program. But in production, Spark should
    be configured to use a standalone cluster resource manager such as Yarn or Mesos.
    In our case, we will see later how Spark uses Kubernetes as a cluster manager
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: The cluster manager is responsible for allocating computational resources and
    isolating computations on the cluster. When the driver program requests resources,
    the cluster manager launches Spark executors to perform the required computations.
  prefs: []
  type: TYPE_NORMAL
- en: Spark executors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Spark executors** are processes launched on worker nodes in the cluster by
    the cluster manager. They run computations and store data for the Spark application.
    Each application has its own executors that stay up for the duration of the whole
    application and run tasks in multiple threads. Spark executes code snippets called
    **tasks** to perform distributed data processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Components of execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **Spark job** triggers the execution of a Spark program. This gets divided
    into smaller sets of tasks called **stages** that depend on each other.
  prefs: []
  type: TYPE_NORMAL
- en: Stages consist of tasks that can be run in parallel. The tasks themselves are
    executed in multiple threads within the executors. The number of tasks that can
    run concurrently within an executor is configured based on the number of **slots**
    (cores) pre-allocated in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: This whole hierarchy of jobs, stages, tasks, slots, and executors facilitates
    the distributed execution of Spark programs across a cluster. We will go deeper
    into some optimizations around this structure later in the chapter. For now, let’s
    see how we can visualize Spark’s execution components by running a simple interactive
    Spark program.
  prefs: []
  type: TYPE_NORMAL
- en: Starting a Spark program
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the next steps, we will use an interactive Python programming environment
    called **Jupyter**. If you don’t have Jupyter installed locally yet, please make
    sure it is installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start a Jupyter environment by typing the following in a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You will see some output for the Jupyter processes and a new browser window
    should start.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Jupyter interface](img/B21927_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Jupyter interface
  prefs: []
  type: TYPE_NORMAL
- en: 'Jupyter will make things easier since we will run an interactive Spark session
    and will be able to monitor Spark through its UI:'
  prefs: []
  type: TYPE_NORMAL
- en: First, click on the **Python 3** button in the **Notebook** section (*Figure
    5**.2*). This will start a new Jupyter notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will use some Python code to download the `titanic` dataset from the
    web (available at [https://raw.githubusercontent.com/neylsoncrepalde/titanic_data_with_semicolon/main/titanic.csv](https://raw.githubusercontent.com/neylsoncrepalde/titanic_data_with_semicolon/main/titanic.csv)).
    In the first code chunk, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will import the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will create a dictionary with the name of the file as the key and
    the URL as the value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will create a simple Python function to download this dataset and save
    it locally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will create a folder named `data` and a subfolder named `titanic`
    to store the dataset. The `exist_ok` parameter lets the code continue and not
    throw an error if these folders already exist. Then, we run our function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, the `titanic` dataset is available for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: All of the code presented in this chapter can be found in the [*Chapter 5*](B21927_05.xhtml#_idTextAnchor092)
    folder of this book’s GitHub repository ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter%205](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter%205)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can start configuring our Spark program to analyze this data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we have to first import the `SparkSession` class and the `functions`
    module. This module will be necessary for most of the data processing we will
    do with Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After running the imports, create a Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This Spark session makes the Spark UI available. We can check it by typing [http://localhost:4040](http://localhost:4040)
    in our browser.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Spark UI](img/B21927_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Spark UI
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there is no data available just yet. Jobs will start showing
    on this monitoring page after we run some things in our Spark program.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get back to our code in Jupyter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To read the downloaded dataset, run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The options in this code state that the first row of the file contains the column
    names (`header = True`), that we want Spark to automatically detect the table
    schema and read it accordingly (`inferSchema = True`), and set the file separator
    or delimiter as `;`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To show the first rows of the dataset, run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, if we get back to the Spark UI, we can already see finished jobs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.4 – The Spark UI with jobs](img/B21927_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – The Spark UI with jobs
  prefs: []
  type: TYPE_NORMAL
- en: We can check other tabs in the Spark UI for stages and tasks, and visualize
    queries sent to Spark in the **SQL / DataFrame** tab. We will explore those tabs
    later in this chapter for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will focus on understanding Spark programming using
    the Python (DataFrame API) and SQL (Spark SQL API) languages and how Spark ensures
    maximum performance regardless of our choice of programming language.
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame API and the Spark SQL API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark provides different APIs built on top of the core RDD API (the native,
    low-level Spark language) to make it easier to develop distributed data processing
    applications. The two most popular higher-level APIs are the DataFrame API and
    the Spark SQL API.
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrames API provides a domain-specific language to manipulate distributed
    datasets organized into named columns. Conceptually, it is equivalent to a table
    in a relational database or a DataFrame in Python pandas, but with richer optimizations
    under the hood. The DataFrames API enables users to abstract data processing operations
    behind domain-specific terminology such as *grouping* and *joining* instead of
    thinking in `map` and `reduce` operations.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark SQL API builds further on top of the DataFrames API by exposing Spark
    SQL, a Spark module for structured data processing. Spark SQL allows users to
    run SQL queries against DataFrames to filter or aggregate data. The SQL queries
    get optimized and translated into native Spark code to be executed. This makes
    it easy for users familiar with SQL to run ad hoc queries against data.
  prefs: []
  type: TYPE_NORMAL
- en: Both APIs rely on the Catalyst optimizer, which leverages advanced programming
    techniques such as predicate pushdown, projection pruning, and a variety of join
    optimizations to build efficient query plans before execution. This differentiates
    Spark from other distributed data processing frameworks by optimizing queries
    based on business logic instead of on hardware considerations.
  prefs: []
  type: TYPE_NORMAL
- en: When working with Spark SQL and the DataFrames API, it is important to understand
    some key concepts that allow Spark to run fast, optimized data processing. These
    concepts are transformations, actions, lazy evaluation, and data partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: Transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformations define computations that will be done, while actions trigger
    the actual execution of those transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformations are operations that produce new DataFrames from existing ones.
    Here are some examples of transformations in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the `select` command to select columns in a DataFrame (`df`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the `filter` command to filter rows based on a given condition:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the `orderBy` command to sort the DataFrame based on a given column:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Grouped aggregations can be done with the `groupBy` command and aggregation
    functions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The key thing to understand is that transformations are *lazy*. When you call
    a transformation such as `filter()` or `orderBy()`, no actual computation is performed.
    Instead, Spark just remembers the transformation to apply and waits until an action
    is called to actually execute the computation.
  prefs: []
  type: TYPE_NORMAL
- en: This lazy evaluation allows Spark to optimize the full sequence of transformations
    before executing them. This can lead to significant performance improvements compared
    to eager evaluation engines that execute each operation immediately.
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While transformations describe operations on DataFrames, actions actually execute
    the computation and return results. Some common actions in Spark include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `count` command to return the number of rows in a DataFrame:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `first` command to return the first row in a DataFrame:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `show` command to print the content of a DataFrame:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `collect` command to return an array with all the rows in a DataFrame:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `write` command to write a DataFrame to a given path:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When an action is called on a DataFrame, several things happen:'
  prefs: []
  type: TYPE_NORMAL
- en: The Spark engine looks at the sequence of transformations that have been applied
    and creates an execution plan to perform them efficiently. This is when optimizations
    happen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The execution plan is run across the cluster to perform the actual data manipulation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The action aggregates and returns the final result to the driver program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, in summary, transformations describe a computation but do not execute it
    immediately. Actions trigger lazy evaluation and execution of the Spark job, returning
    concrete results.
  prefs: []
  type: TYPE_NORMAL
- en: The process of storing the computation instructions to execute later is called
    **lazy evaluation**. Let’s take a closer look at this concept.
  prefs: []
  type: TYPE_NORMAL
- en: Lazy evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lazy evaluation is a key technique that allows Apache Spark to run efficiently.
    As mentioned previously, when you apply transformations to a DataFrame, no actual
    computation happens at that time.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, Spark internally records each transformation as an operation to apply
    to the data. The actual execution is deferred until an action is called.
  prefs: []
  type: TYPE_NORMAL
- en: 'This delayed computation is very useful for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Avoids unnecessary operations**: By looking at the sequence of many transformations
    together, Spark is able to optimize which parts of the computation are actually
    required to return the final result. Some intermediate steps may be eliminated
    if not needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Runtime optimizations**: At the moment when an action is triggered, Spark
    formulates an efficient physical execution plan based on partitioning, available
    memory, and parallelism. It makes these optimizations dynamically at runtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batches operations together**: Several transformations over multiple DataFrames
    can be batched together into fewer jobs. This amortizes the overhead of job scheduling
    and initialization across many computation steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As an example, consider a DataFrame with user clickstream data that needs to
    be filtered, aggregated, and sorted before returning the final top 10 rows.
  prefs: []
  type: TYPE_NORMAL
- en: With lazy evaluation, all these transformations would be recorded when defined,
    and a single optimized job would be executed when the final rows are requested
    via `collect()` or `show()`. Without lazy evaluation, the engine would need to
    execute a separate job for `filter()`, another job for `groupBy()`, another job
    for `orderBy()`, and so on for each step. This would be highly inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: So, in summary, lazy evaluation separates the definition of the computational
    steps from their execution. This allows Spark to come up with an optimized physical
    plan to perform the full sequence of operations. Next, we will see how Spark can
    distribute computations through data partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: Data partitioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark’s speed comes from its ability to distribute data processing across a
    cluster. To enable parallel processing, Spark breaks up data into independent
    partitions that can be processed in parallel on different nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: When you read data into a Spark DataFrame or RDD, the data is divided into logical
    partitions. On a cluster, Spark will then schedule task execution so that partitions
    run in parallel on different nodes. Each node may process multiple partitions.
    This allows the overall job to process data much faster than if run sequentially
    on a single node.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data partitioning in Spark is key to understanding the differences
    between `narrow` and `wide` transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Narrow versus wide transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Narrow transformations are operations that can be performed on each partition
    independently without any data shuffling across nodes. Examples include `map`,
    `filter`, and other per-record transformations. These allow parallel processing
    without network traffic overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Wide transformations require data to be shuffled between partitions and nodes.
    Examples include `groupBy` aggregations, joins, sorts, and window functions. These
    involve either combining data from multiple partitions or repartitioning data
    based on a key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example to illustrate. We are filtering a DataFrame and keeping
    only rows that have an age value below 20:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The filtering by age is done independently in each data partition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The grouped aggregation requires data exchange between partitions in the cluster.
    This exchange is what we call **shuffle**.
  prefs: []
  type: TYPE_NORMAL
- en: Why does this distinction matter? When possible, it’s best to structure Spark
    workflows with more narrow transformations first before wide ones. This minimizes
    data shuffling across the network, which improves performance.
  prefs: []
  type: TYPE_NORMAL
- en: For example, it is often better to start by filtering data to the subset needed
    and then apply aggregations/windows/joins on the filtered data afterward, rather
    than applying all operations to the entire dataset. Filtering first decreases
    the data volume shuffled across the network.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding narrow versus wide transformations allows optimizing Spark jobs
    for lower latency and higher throughput by minimizing shuffles and partitioning
    data only when needed. It is a key tuning technique for better Spark application
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s try putting those concepts to work with our `titanic` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the titanic dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s return to the Jupyter notebook we started building earlier. First, we
    start a `SparkSession` and read the `titanic` dataset into Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now use the `printSchema()` command to check the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will apply some narrow transformations in the original dataset. We
    will filter only men who are more than 21 years old and save this transformed
    data into an object called `filtered`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s get back to the Spark UI. What happened? *Nothing!* No computation
    was done because (remember) those commands are transformations and do not trigger
    any computations in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – The Spark UI after transformation](img/B21927_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – The Spark UI after transformation
  prefs: []
  type: TYPE_NORMAL
- en: 'But now, we run a `show()` command, which is an action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '*Et voilà!* Now, we can see that a new job was triggered in Spark.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – The Spark UI after action](img/B21927_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – The Spark UI after action
  prefs: []
  type: TYPE_NORMAL
- en: We can also check the execution plan in the **SQL / DataFrame** tab. Click this
    tab and then click on the last executed query (the first row in the table). You
    should see the output as shown in *Figure 5**.7*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Execution plan for Spark filters](img/B21927_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – Execution plan for Spark filters
  prefs: []
  type: TYPE_NORMAL
- en: The `titanic` dataset is not big enough for Spark to divide it into partitions.
    Later in the chapter, we will see how shuffle (data exchange between partitions)
    happens when we use wide transformations.
  prefs: []
  type: TYPE_NORMAL
- en: The last important thing for this section is to see how Spark uses the DataFrame
    and Spark SQL API and transforms all the instructions into RDD for optimized processing.
    Let’s implement a simple query to analyze the `titanic` dataset. We will do that
    in both Python and SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we calculate how many male persons older than 21 survived the Titanic
    in each traveling class. We save the Python query in an object called `queryp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we’re going to implement the exact same query but with SQL. For that,
    first, we need to create a temporary view and then we use the `spark.sql()` command
    to run SQL code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Both queries are saved in objects that we can use now to inspect the execution
    plan. Let’s do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If you check the output, you will note that both execution plans are exactly
    the same! This is only possible because Spark takes all the instructions given
    in the higher-level APIs and transforms them into RDD code that runs “under the
    hood.” We can execute both queries with a `show()` command and see that the results
    are the same and they are executed with the same performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for both commands is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We also can check the execution plan visually in the **SQL / DataFrame** tab
    in the Spark UI. Click the two first rows in this tab (the two latest executions)
    and see that the plan is the same.
  prefs: []
  type: TYPE_NORMAL
- en: From now on, let’s try to dig deeper into PySpark code while working with a
    more challenging dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Working with real data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now work with the IMDb public dataset. This is a more complex dataset
    divided into various tables.
  prefs: []
  type: TYPE_NORMAL
- en: The following code will download five tables from the `imdb` dataset and save
    them into the `./data/imdb/` path (also available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/get_imdb_data.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/get_imdb_data.py)).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to download the data locally:'
  prefs: []
  type: TYPE_NORMAL
- en: get_imdb_data.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will open a Jupyter notebook, start a `SparkSession`, and read the
    tables (you can find this code at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/analyzing_imdb_data.ipynb](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/analyzing_imdb_data.ipynb)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This time, we are not going to use the `inferSchema` parameter to read the tables.
    `inferSchema` is great when we are dealing with small tables. For big data, however,
    this is not recommended as Spark reads all the tables once to define the schema
    and then a second time to read the data correctly, which can result in poor performance.
    Instead, the best practice is to define the schema previously and read it with
    the defined schema. Note that reading a table like this will not trigger an execution
    up to the point we give any *action* instruction. The schemas for the IMDb dataset
    can be found at [https://developer.imdb.com/non-commercial-datasets/](https://developer.imdb.com/non-commercial-datasets/).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to correctly read IMDb tables, we first define the schemas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will read all the tables passing their defined schemas as a parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we check that the schema was imported correctly by Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'If you check the Spark UI, you will note that *no computation was triggered*.
    This will only be done when we call any *action* function. We will proceed to
    analyze this data. Take a look at the `names` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The `.show()` command will yield the following output (which is just some selected
    columns to improve visualization):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'And this is the exact moment that Spark actually reads the `names` data, as
    soon as we run the `.show()` command. This table contains information about actors,
    producers, directors, writers, and so on. But note how the `knownForTitles` column
    is structured. It contains all the movies that an individual worked in but as
    a string with all the titles separated by a comma. This could make our lives difficult
    in the future when we need to join this table with other information. Let’s **explode**
    this column into multiple rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we did not select the `primaryProfession` column. We won’t need it
    in this analysis. Now, check the `crew` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Here is the output`:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have the same case: movies that were directed by more than one person.
    This information is stored as a string with multiple values separated by a comma.
    If you cannot visualize this case at first, try filtering the `crew` table for
    values that contain a comma:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We will explode this column into rows as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Then, you can also check (using the `.show()` command) the other tables but
    they do not have this kind of situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s start analyzing this data. We will visualize the most famous Keanu
    Reeves movies. It is not possible to see that with just one table since, in `names`,
    we only have the movie ID (`tconst`). We need to join the `names` and `basics`
    tables. First, we get only the information on Keanu Reeves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will join this new table with the `basics` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'In this block of code, we are selecting only the columns we need from the `basics`
    table and joining them with the `only_keanu` filtered table. The `join` command
    takes three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: The table that is going to be joined
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The columns that will be used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The type of join that Spark is going to perform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, we are using `tconst` and the `knownForTitles` columns to join
    and we are performing an inner join, only keeping the records that are found in
    both tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we trigger the results of this join with an action, let’s explore the
    execution plan for this join:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Analyzing the output, we notice that Spark will perform a sort-merge join:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Joins are a key operation in Spark and they are directly related to Spark’s
    performance. We will get back to the datasets and the joins we are making later
    in the chapter but, before we continue, a quick word about the internals of Spark
    joins.
  prefs: []
  type: TYPE_NORMAL
- en: How Spark performs joins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark provides several physical join implementations to perform joins efficiently.
    The choice of join implementation depends on the size of the datasets being joined
    and other parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a variety of ways in which Spark can internally perform a join. We
    will go through the three most common joins: the sort-merge join, the shuffle
    hash join, and the broadcast join.'
  prefs: []
  type: TYPE_NORMAL
- en: Sort-merge join
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **sort-merge join**, as the name suggests, sorts both sides of the join
    on the join key before applying the join. Here are the steps involved:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark reads the left and right side DataFrames/RDDs and applies any projections
    or filters needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, both sides are sorted based on the join keys. This rearrangement of data
    is known as a shuffle, which involves moving data across the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the shuffle, rows with the same join key will be co-located on the same
    partition. Spark then merges the sorted partitions by comparing values with the
    same join key on both sides and emitting join output rows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The sort-merge join works well when the data on both sides can fit into memory
    after the shuffle. The preprocessing step of sorting enables a fast merge. However,
    the shuffle can be expensive for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle hash join
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **shuffle hash join** optimizes the sort-merge join by avoiding the *sort*
    phase. Here are the main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark partitions both sides based on the hash of the join key. This partitions
    rows with the same key to the same partition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since rows with the same key hash to the same partition, Spark can build hash
    tables from one side, probe the hash table for matches from the other side, and
    emit join results within each partition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The shuffle hash join reads each side only once. By avoiding the sort, it saves
    I/O and CPU costs compared to the sort-merge join. But it is less efficient than
    sort-merge when the joined datasets after shuffling can fit into memory.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcast hash join
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If one side of the join is small enough to fit into the memory of each executor,
    Spark can broadcast that side using the **broadcast hash join**. Here are the
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The smaller DataFrame is hashed and broadcast to all worker nodes. This allows
    the entire dataset to be read into memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The larger side is then partitioned by the join key. Each partition probes the
    broadcast in-memory hash table to find matches and emit join results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since data transfer is minimized, broadcast joins are very fast. Spark automatically
    chooses this if one side is small enough to broadcast. However, the maximum size
    depends on the memory available for broadcasting.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get back to our datasets and try to force Spark to perform a type
    of join different from the sort-merge join it automatically decided to do.
  prefs: []
  type: TYPE_NORMAL
- en: Joining IMDb tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `keanu_movies` query execution plan will perform a sort-merge join, which
    was automatically chosen by Spark because, in this case, it probably brings the
    best performance. Nevertheless, we can force Spark to perform a different kind
    of join. Let’s try a broadcast hash join:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This query is almost identical to the previous one with one exception: we are
    using the `broadcast` function to force a broadcast join. Let’s check the execution
    plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the execution plan is smaller and it contains a `BroadcastHashJoin` task.
    We can also try to hint at Spark to use the shuffle hash join with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s take a look at the execution plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we trigger the execution of all queries with a `show()` command, each
    one in its own code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the results are exactly the same. However, the way Spark handled
    the joins internally was different and with different performances. Check the
    **SQL / DataFrame** tab in the Spark UI to visualize the executed queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we wanted to use only SQL to check Keanu Reeves’ movies, we can – by creating
    a temporary view and using the `spark.sql()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s try one more query. Let’s see whether we can answer the question,
    *Who were the directors, producers, and writers of the movies in which Tom Hanks
    and Meg Ryan acted together, and which of them has the* *highest rating?*
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to check `Tom Hanks` and `Meg Ryan` codes in the `names` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This query shows us a lot of different Meg Ryan’s codes but the one we want
    is the first one with several movies in the `knownForTitles` column. Then, we
    will find out the movies that they both acted in together. To do that, we will
    filter only movies with their codes in the `principals` table and count the number
    of actors by movie. The ones with two actors in one movie should be the movies
    they acted in together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'And we get this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can join this information with the other tables to get the answers
    we need. We will create a `subjoin` table joining the information on `principals`,
    `names`, and `basics`. Let’s save `ratings` for later as it consumes more resources
    than we have in our machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'To speed up further computations, we will cache this table. This will allow
    Spark to save this `subjoin` table in memory so all the previous joins will not
    be triggered again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s find out what movies Tom and Meg did together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the final output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will find out the directors, producers, and writers of those movies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can check the ratings of the movies and order them to get the highest
    rated. To do that, we need to join the `subjoin` cached table with the `ratings`
    table. As `subjoin` is already cached, note how fast this join happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'This last join yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! Next, you should try, as an exercise, to redo these queries but
    with SQL and the `spark.sql()` command.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the fundamentals of using Apache Spark for large-scale
    data processing. You learned how to set up a local Spark environment and use the
    PySpark API to load, transform, analyze, and query data in Spark DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed key concepts such as lazy evaluation, narrow versus wide transformations,
    and physical data partitioning that allow Spark to execute computations efficiently
    across a cluster. You gained hands-on experience applying these ideas by filtering,
    aggregating, joining, and analyzing sample datasets with PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: You also learned how to use Spark SQL to query data, which allows those familiar
    with SQL to analyze DataFrames. We looked at Spark’s query optimization and execution
    components to understand how Spark translates high-level DataFrame and SQL operations
    into efficient distributed data processing plans.
  prefs: []
  type: TYPE_NORMAL
- en: While we only scratched the surface of tuning and optimizing Spark workloads,
    you learned about some best practices such as minimizing shuffles and using broadcast
    joins where appropriate to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will study one of the most used tools for pipeline orchestration,
    Apache Airflow.
  prefs: []
  type: TYPE_NORMAL
