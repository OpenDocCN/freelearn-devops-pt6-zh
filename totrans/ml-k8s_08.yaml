- en: '*Chapter 8*: Building a Complete ML Project Using the Platform'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, you have seen a few components of the platform and how it works.
    You will start this chapter by understanding the platform at a macro level. The
    holistic view will help you see how the components weave a complete solution for
    your **machine learning** (**ML**) needs.
  prefs: []
  type: TYPE_NORMAL
- en: In the later part of this chapter, you will see how you can start an ML project
    by using a simple example and how the teams and platform will help achieve your
    required goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing the complete picture of the ML platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the business problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data collection, processing, and cleaning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing exploratory data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and evaluating the ML model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproducibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing the complete picture of the ML platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the preceding chapters, you have built a complete ML platform on top of
    Kubernetes. You have installed, configured, and explored the different components
    of the platform. Before you start using the platform, let''s take a step back
    and look at the platform you have built from the tooling perspective. *Figure
    8.1* shows the complete logical architecture of the platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Logical platform architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_08_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Logical platform architecture
  prefs: []
  type: TYPE_NORMAL
- en: The diagram in *Figure 8.1* also shows the interaction of each platform component.
    The entire platform runs inside Kubernetes and is managed entirely by the `Kfdef`
    file. It is also important to note that the ODH operator allows you to add or
    remove tools or swap one tool for another. For example, you could use Argo CD
    for model deployments instead of Airflow. Keycloak is also not part of the ODH
    project. However, the components must be secured by a single sign-on mechanism,
    and Keycloak is one of the best open source tools that can be used to add a single
    sign-on capability to the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Starting at the top of the diagram, you can see that end users interact with
    Jupyter notebooks, and the Spark, Airflow, and MLflow UIs. You have seen and experienced
    these interactions in the preceding chapters. The deployed ML model can then be
    used for inferencing by applications through REST API calls.
  prefs: []
  type: TYPE_NORMAL
- en: In the middle of the diagram, you can see the interactions between the components
    and the kind of interactions they perform with each other. Jupyter servers and
    Airflow jobs can submit Spark applications to the managed Spark clusters. Airflow
    interacts with the MLflow model registry, while Jupyter notebooks can interact
    with MLflow to record experiment runs. Airflow also creates Seldon deployment
    objects that the Seldon controller then converts into running Pods with ML models
    exposed as REST services. There is no limit to how one component can interact
    with other platform components.
  prefs: []
  type: TYPE_NORMAL
- en: At the bottom of the diagram, the ODH operator manages and operates the platform
    components. The ODH operator handles the installation and updates of these components.
    Spark, JupyterHub, and the Seldon controller are also Kubernetes operators that
    manage instances of Spark clusters, Jupyter notebook servers, and Seldon deployments,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the ODH operator also manages the Prometheus and Grafana instances.
    Prometheus is used to collect metrics from each of the components, including the
    statistics of Seldon deployments. Grafana can then visualize those metrics and
    can be configured to raise alerts.
  prefs: []
  type: TYPE_NORMAL
- en: The ODH project is still evolving. There may be changes as to what components
    will be included or excluded in the project in the future. Some of the officially
    supported components may get replaced with another over time. Therefore, it is
    important to understand the architecture and how the ODH operator works so that
    you keep it up to date.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will take a step back and understand ML projects a
    bit more, starting with identifying opportunities where an ML solution fits. You
    will be taken through a scenario that will lead to the creation of a complete
    ML project.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the business problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with any software project, the first thing is to agree on the business problem
    you are trying to solve. We have chosen a fictitious scenario for this book to
    keep it simple while focusing on the process. You can apply the same approach
    to more complex projects.
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume that you work for an airline booking company as a lead data analyst.
    The business team of your company has reported that lots of customers complain
    about flights being delayed. It is causing the company to have bad customer experiences,
    and the phone staff spend lots of time explaining the details to customers. The
    business is looking at you to provide a solution to identify which airlines and
    which flights and times have a lower probability of delays so that the website
    can prioritize those airlines and, therefore, customers end up with fewer delays.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a breather here and analyze how we can solve this problem. Do we
    need ML here? If we take the historical data and place the airlines into two buckets
    of *delayed* and *on time*, with each bucket placing the airlines into the right
    category, this attribute can then be used while the customer searches for airlines
    with better on-time performance. A team of data analysts will analyze the data
    and assign the ratings. Job done!
  prefs: []
  type: TYPE_NORMAL
- en: While exploring this set, the business has mentioned that one bucket per airline
    may not provide the granularity that the solution requires. They would like to
    assess the performance, not at the airline level, but using other factors such
    as origin and destination airport, and time of day. So, airline A, with flights
    from Sydney to Melbourne, may go into the *on time* bucket, while the same airline
    may go into the *delayed* bucket when flying from Tokyo to Osaka. This suddenly
    expands the scope of the problem. If you need to analyze data at this granularity,
    it will take a lot of time to process and assign the correct category, and you
    may need to analyze this data very frequently.
  prefs: []
  type: TYPE_NORMAL
- en: Have you started to think about how you can automate this? The business then
    mentions that the weather plays a vital role in this problem, and the forecast
    data from the weather bureau will need to be fetched and preprocessed to perform
    the analysis. You realize that performing this job with human teams will be slow
    and complicated and does not provide the solution that the business is looking
    for. You then mention to the business that you will need to investigate the existing
    data, which can be used to predict the correct category for a particular flight.
    You and the business agree that the aim is to predict the flight delay 10 days
    before the scheduled time with at least 75% accuracy, to improve the customer
    experience. You will also discuss the response time requirements for the model
    and understand how the model will be used in the overall business process.
  prefs: []
  type: TYPE_NORMAL
- en: You have just defined the success criteria of this project. You have conveyed
    to the business that your team will analyze available data to assess its suitability
    for the project and then plan for the next steps. You have asked the business
    to associate a **subject matter expert** (**SME**) who can assist in data exploration
    at this stage.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, you have outlined the business objectives and the scope of the
    project. You have also defined the evaluation criteria through which the success
    of the project would be measured. It is critical that you keep a note of the business
    value through each stage of the ML life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have defined the criteria, the next step is to start looking at the
    available data. For this use case, the data is available at [https://www.kaggle.com/usdot/flight-delays?select=flights.csv](https://www.kaggle.com/usdot/flight-delays?select=flights.csv).
  prefs: []
  type: TYPE_NORMAL
- en: Data collection, processing, and cleaning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this stage, you will begin with gathering raw data from the identified sources.
    You will write data pipelines to prepare and clean the raw data for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data sources, location, and the format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have started working with the SME to access a subset of the flight data.
    You will understand the data format and the integration process required to access
    this data. The data could be in CSV format, or it may be available in some **relational
    database management system** (**RDBMS**). It is vital to understand how this data
    would be available for your project and how this data is being maintained eventually.
  prefs: []
  type: TYPE_NORMAL
- en: Start this process by identifying what data is easily available. The SME has
    mentioned that the flight records data that covered the flight information, the
    scheduled and actual departure times, and the scheduled and actual arrival times
    is readily available. This information is available in the object store of your
    organization. This could be a good starting point.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data processing and cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data collected from the raw data sources may have many problems. The collected
    data may have duplication, missing values, and/or invalid records. For example,
    you may find that a column of the `string` type may have numerical data in it.
    You will then work with the SME to find out ways to handle the anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: How would you handle the missing data? Choose an estimated value of the missing
    data from the existing set. Or you may decide to drop the column altogether if
    there are many missing values and you can not find any way to impute the missing
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Implement data validation checks to make sure that the cleaned dataset has consistency
    and that the data quality problems described here are properly handled. Imagine
    that the age column has a value of `250`. Although we would all like to live this
    long or beyond, clearly this data is not valid. During this stage, you will find
    the discrepancy in the data and work out how to handle it.
  prefs: []
  type: TYPE_NORMAL
- en: You may find that the flight arrival and departure times are in the local time
    zones, and you may choose to add a new column with the times represented in UTC
    format for easier comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning can happen in both the data engineering stage and the model development
    stage. Data anomalies that are related to the domain or business logic may be
    found and handled in the data engineering stage, while data augmentation and data
    encoding are done at the model development stage. This is because it is the data
    scientist or the ML engineer who knows best what data formats the model training
    requires, while the data engineers work closer to the business domain experts.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to implement such data validation in the data engineering phase is
    through Apache Spark. Spark has a set of built-in functions that you can use for
    data cleaning. The following code shows an example of how to filter out invalid
    rows or rows that contain malformed data while reading from a data source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Another example is the `fillna()` function. It is used to replace null values
    with any other values. The following example shows how to replace all null values
    in the data frame with zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: On the model development side, there are several techniques to perform the same
    operations using pandas to manipulate data frames. You will see this in action
    in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have executed the data cleaning pipeline and created an intermediary
    dataset that can be used for the next stage, the next step is to see whether the
    available data helps you in achieving the business goal.
  prefs: []
  type: TYPE_NORMAL
- en: Performing exploratory data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this stage, you analyze the data to assess its suitability for the given
    problem. Data analysis is essential for building ML models. Before you create
    an ML model, you need to understand the context of the data. Analyzing vast amounts
    of company data and converting it into a useful result is extremely difficult,
    and there is no single answer on how to do it. Figuring out what data is meaningful
    and what data is vital for business is the foundation for your ML model.
  prefs: []
  type: TYPE_NORMAL
- en: This is a preliminary analysis, and it does not guarantee that the model will
    bring the expected results. However, it provides an opportunity to understand
    the data at a higher level and pivot if required.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding sample data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you get a set of data, you first try to understand it by merely looking
    at it. You then go through the business problem and try to determine what set
    of patterns would be helpful for the given situation. A lot of the time, you will
    need to collaborate with SMEs who have relevant domain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, you may choose to convert the data into a tabular form to better
    understand it. Classify the columns according to the data values. Understand each
    variable in the dataset and find out whether the values are continuous, or whether
    it represents a category. You will then summarize the columns using descriptive
    statistics to understand the values your columns contain. These statistics could
    be mean or median or anything that helps you understand the data.
  prefs: []
  type: TYPE_NORMAL
- en: Understand the **data variance**. For example, your data has only 5% records
    of delayed flights and the remaining flights are on time. Would this dataset be
    good for your desired outcomes? You need to get a better dataset that represents
    a more balanced distribution. You may choose to downsample the dataset, if it
    is highly imbalanced, by reducing the examples from the majority class.
  prefs: []
  type: TYPE_NORMAL
- en: Humans are good at visualizing data so, to better understand the data, you will
    need to visualize your columns using charts. There is a series of different charts
    that can help you visualize your data. The platform we present here will assist
    you in writing code to visualize the data using popular libraries such as Matplotlib
    or Seaborn. Before you choose to visualize your data using a chart, think about
    what kind of information you are expected to get from the chart and how it can
    assist you in understanding the data.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we define three basic charts and their characteristics given
    in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Box plots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A box plot ([https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/box-plot-review](https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/box-plot-review))
    is an excellent way to visualize and understand data variance. Box plots show
    results in quartiles, each containing 25% of the values in the dataset; the values
    are plotted to show how the data is distributed. *Figure 8.2* shows a sample box
    plot. Note the black dot is an **outlier**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Box plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_08_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – Box plot
  prefs: []
  type: TYPE_NORMAL
- en: The first component of the box plot is the minimum value of the dataset. Then
    there is the lower quartile, or the minimum 25% values. After that, we have the
    median value at 50% of the dataset. Then, we have the upper quartile, the maximum
    25% value. At the top, we have the maximum value based on the range of the dataset.
    Finally, we have the outliers. Outliers are the extreme data points—on either
    the high or low side—that could potentially impact the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Histograms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **histogram** represents the numerical data distribution. To create a histogram,
    you first split the range of values into intervals called **bins**. Once you have
    defined the number of bins to hold your data, the data is then put into predefined
    ranges in the appropriate bin. The histogram chart shows the distribution of the
    data as per the predefined bins. *Figure 8.3* shows a sample histogram. Note that
    the bins are on the *x* axis of the plot. The following plot shows the distribution
    in just two bins. You can see that the distribution is biased toward the first
    bin.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Histogram'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_08_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Histogram
  prefs: []
  type: TYPE_NORMAL
- en: Density plots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the drawbacks of histograms is that they are sensitive to bin margins
    and the number of bins. The distribution shape is affected by how the bins are
    defined. A histogram may be a better fit if your data contains more discrete values
    (such as gender or postcodes). Otherwise, an alternative is to use a **density
    plot**, which is a smoother version of a histogram. *Figure 8.4* shows a sample
    density plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Density plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_08_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – Density plot
  prefs: []
  type: TYPE_NORMAL
- en: Once you have performed the exploratory data analysis, you may choose to go
    back and collect more data from existing sources or find new sources of data.
    If you are confident during this stage that the data you have captured can help
    you achieve the business goal, then you go to the next stage, feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML is all about data. No matter how advanced our algorithm is, if the data is
    not correct or not enough, our model will not be able to perform as desired. Feature
    engineering transforms input data into features that are closely aligned with
    the model's objectives and converts data into a format that assists in model training.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, there is data that may not be useful for a given training problem.
    How do we make sure that the algorithm is using only the right set of information?
    What about fields that are not individually useful, but when we apply a function
    to a group of fields, the data becomes particularly useful?
  prefs: []
  type: TYPE_NORMAL
- en: The act of making your data useful for the algorithm is called feature engineering.
    Most of the time, a data scientist's job is to find the right set of data for
    a given problem. Feature engineering requires knowledge of domain-specific techniques,
    and you will collaborate with business SMEs to better understand the data.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering is not only about finding the right features from existing
    data, but you may need to create new features from existing data. These features
    are known as **engineered features**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that in your flight dataset, there are fields mentioning `scheduled_departure_time`
    and `departure_time`. Both of these fields will tell you whether the flights are
    late. However, your business is looking to classify whether the flights are late.
    You and the business agree to classify the delay into three categories, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: On time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Short-delayed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long-delayed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A short delay captures the flights that departed with a maximum delay of 30
    minutes. All other delayed flights are classified by the long delay value in the
    delayed column. You will need to add this column or feature to your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: You may end up dropping a column that may not be useful for the given problem.
    Do you think the `Cancellation Reason` column may be useful for predicting the
    flight delay? If not, you may choose to drop this column.
  prefs: []
  type: TYPE_NORMAL
- en: You will also represent your data that can be easily digestible by the ML algorithms.
    A lot of ML algorithms operate on numerical values; however, not all data will
    be in the numerical format. You will apply techniques such as **one-hot encoding**
    to convert the columns into a numerical format.
  prefs: []
  type: TYPE_NORMAL
- en: Often, the ML algorithm works well with the value range between `–1` and `1`
    because it is faster to converge and results in better training time. Even if
    you have numerical data, it could be beneficial to convert it into the range,
    and the process of doing this is called **scaling**. During this stage, you may
    write code to scale the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some cases, you may want to create additional records in your datasets for
    a couple of reasons. One reason is when you do not have enough data to train a
    meaningful model, while another is when you deliberately want to influence the
    behavior of the model to favor one answer over the other, such as correcting **overfitting**.
    This process of creating synthetic data is called **data augmentation**.
  prefs: []
  type: TYPE_NORMAL
- en: All activities related to data collection, processing, cleaning, data analysis,
    feature engineering, and data augmentation can be done in the platform by using
    Jupyter notebooks and, potentially, Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have the datasets cleaned, analyzed, and transformed, the next stage
    is to build and train an ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Building and evaluating the ML model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You are now ready to train your model. You will first evaluate
    what set of algorithms will be a good fit for the given problem. Is it a regression
    or classification problem? How do you evaluate to see whether the model is achieving
    75% correct predictability as described by the business?
  prefs: []
  type: TYPE_NORMAL
- en: Selecting evaluation criteria
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start with accuracy as the model evaluation criteria. This records how
    many times the predicted values are the same as the labels in the test dataset.
    However, if the dataset does not have the right variance, the model may guess
    the majority class for each example, which is effectively not learning anything
    about the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: 'You decided to use the confusion matrix to see the accuracy for each class.
    Let''s say you have 1,000 records in your data, out of which 50 are labeled as
    *delayed*. So, there are 950 examples with the *on time* label. Now, if the model
    correctly predicts `920` out of 950 for *on time* and `12` out of 50 for the *delayed*
    label, the matrix will look like the table in *Figure 8.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_08_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – Confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: For the imbalanced dataset, it is recommended to choose the metrics such as
    **recall** and **precision** or F-score to get a full picture. In this case, the
    precision is 31% (12/38) and the recall is 24% (12/50), compared to the accuracy,
    which is 93.2% (932/1000), and which could be misleading in your scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Building the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will start with splitting your data into training, validation, and test
    sets. Consider a scenario where you split your data into these sets and train
    a model; let's call this *experiment 1*. Now, you want to retrain the model using
    different hyperparameters and you split the data again for this new iteration
    and train the model; let's call it *experiment 2*. Can you compare the results
    of the two experiments if the data splits across the two experiments are not consistent?
    It is critical that your data splits are repeatable to compare different runs
    of your training exercise.
  prefs: []
  type: TYPE_NORMAL
- en: You will try different algorithms or an ensemble of algorithms to assess the
    performance of the data validation set and review the quality of the predictions.
    During this stage, every time you try a new adjustment to the model (for example,
    hyperparameter or different algorithms), you will measure and record the evaluation
    metrics that were set with the SME during the *Understanding the business problem*
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the steps of the modeling stage are iterative. Depending on the result
    of your experiments, you might realize that the model performance is not as expected.
    In this case, you may want to go back to the previous steps of the life cycle,
    such as feature engineering. Or, you may want to redo your data analysis to make
    sure you understand the data correctly. During training, you will revisit the
    business objectives and data to find the right balance. You may decide that additional
    data points from new sources are needed to enhance the training data. It is highly
    recommended that you present the results to the business stakeholders during this
    stage. This communication will share the value of the model to the business in
    the initial stages, collect early feedback, and give the team a chance to course-correct
    if required.
  prefs: []
  type: TYPE_NORMAL
- en: The next stage is to deploy your model for inferencing.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have trained your model, the next stage is to version the model in
    MLflow and deploy it into an environment where the model can be used to make predictions
    for the incoming requests. The versioning of the models will allow you to keep
    track of models and roll back to an older version if the need arises.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will use the on-line model inference approach. The model has
    been containerized using the platform's Seldon component and exposed as a REST
    API. Each call to this REST API will result in one prediction. The stateless container
    running on Kubernetes will scale hundreds of thousands of requests because of
    the inherent ability of containers to scale.
  prefs: []
  type: TYPE_NORMAL
- en: The other way is to serve the incoming requests in batches. Imagine a scenario
    where you have hundreds of thousands of records of labeled data, and you want
    to test that model behavior for all these records. Making individual REST API
    calls may not be the right approach in this scenario. Instead, batch inferencing
    provides an asynchronous approach to making predictions for millions of records.
    Seldon has the capability to infer batches of data, but it is out of scope for
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: The REST API you expose for your flight delay prediction could be utilized by
    the web application to further enhance the customer experience.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, you know what an ML life cycle would look like and how the platform assists
    you in every step of your journey. As an individual, you may be able to write
    every step of the data pipelines and model training and tuning in a single notebook.
    However, this may cause a problem in teams where different people are working
    on different parts of the life cycle. Let's say someone wants to run the model
    training part but the entire process is tied up with one another. Your team may
    not be able to scale with this approach.
  prefs: []
  type: TYPE_NORMAL
- en: A better and more scalable approach is to write different notebooks for various
    stages (such as data processing and model training) in your project life cycle
    and use a workflow engine to tie them up. Using the Kubernetes platform, all the
    stages will be executed using containers and provide a consistent environment
    for your project between different runs. The platform provides Airflow, an engine
    that could be used for creating and executing workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this short chapter, we wanted to step back and show you the big picture of
    the platform and the model life cycle. We encourage you to refer to [*Chapter
    2*](B18332_02_ePub.xhtml#_idTextAnchor027), *Understanding MLOps*, where we presented
    a typical ML life cycle, for a more detailed discussion. Recall the importance
    of collaborations across multiple teams and how investing more time in understanding
    the available data will result in a model that delivers the expected business
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Now you know what the various stages of your project will look like. In the
    next two chapters, you will implement the flight delay prediction service using
    the ML platform that we have presented in this book and you will perform each
    of the stages we have described in this chapter. The idea is to show you how the
    platform caters to every stage of your project and how you can implement this
    platform in your organization.
  prefs: []
  type: TYPE_NORMAL
