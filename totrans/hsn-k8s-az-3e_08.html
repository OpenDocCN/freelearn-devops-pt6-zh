<html><head></head><body>
		<div>
			<div id="_idContainer088" class="Content">
			</div>
		</div>
		<div id="_idContainer089" class="Content">
			<h1 id="_idParaDest-52">4. <a id="_idTextAnchor053"/>Building scalable applications</h1>
		</div>
		<div id="_idContainer126" class="Content">
			<p>When running an application efficiently, the ability to scale and upgrade your application is critical. Scaling allows your application to handle additional load. While upgrading, scaling is needed to keep your application up to date and to introduce new functionality.</p>
			<p>Scaling on demand is one of the key benefits of using cloud-native applications. It also helps optimize resources for your application. If the front end component encounters heavy load, you can scale the front end alone, while keeping the same number of back end instances. You can increase or reduce the number of <strong class="bold">virtual machines</strong> (<strong class="bold">VMs</strong>) required depending on your workload and peak demand hours. This chapter will cover the scale dimensions of the application and its infrastructure in detail.</p>
			<p>In this chapter, you will learn how to scale the sample guestbook application that was introduced in <em class="italics">Chapter 3,</em> <em class="italics">Application deployment on AKS</em>. You will first scale this application using manual commands, and afterward you'll learn how to autoscale it using the <strong class="bold">Horizontal Pod Autoscaler (HPA)</strong>. The goal is to make you comfortable with <strong class="inline">kubectl</strong>, which is an important tool for managing applications running on top of <strong class="bold">Azure Kubernetes</strong> <strong class="bold">Service</strong> (<strong class="bold">AKS</strong>). After scaling the application, you will also scale the cluster. You will first scale the cluster manually, and then use the <strong class="bold">cluster autoscaler</strong> to automatically scale the cluster. In addition, you will get a brief introduction on how you can upgrade applications running on top of AKS.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Scaling your application</li>
				<li>Scaling your cluster</li>
				<li>Upgrading your application</li>
			</ul>
			<p>Let's begin this chapter by discussing the different dimensions of scaling applications on top of AKS.</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor054"/>Scaling your application</h2>
			<p>There are two scale dimensions for applications running on top of AKS. The first scale dimension is the number of pods a deployment has, while the second scale dimension in AKS is the number of nodes in the cluster.</p>
			<p>By adding new pods to a deployment, also known as scaling out, you can add additional compute power to the deployed application. You can either scale out your applications manually or have Kubernetes take care of this automatically via HPA. HPA can monitor metrics such as the CPU to determine whether pods need to be added to your deployment.</p>
			<p>The second scale dimension in AKS is the number of nodes in the cluster. The number of nodes in a cluster defines how much CPU and memory are available for all the applications running on that cluster. You can scale your cluster manually by changing the number of nodes, or you can use the cluster autoscaler to automatically scale out your cluster. The cluster autoscaler watches the cluster for pods that cannot be scheduled due to resource constraints. If pods cannot be scheduled, it will add nodes to the cluster to ensure that your applications can run.</p>
			<p>Both scale dimensions will be covered in this chapter. In this section, you will learn how you can scale your application. First, you will scale your application manually, and then later, you will scale your application automatically.</p>
			<h3 id="_idParaDest-54"><a id="_idTextAnchor055"/>Manually scaling your application</h3>
			<p>To demonstrate manual scaling, let's use the guestbook example that we used in the previous chapter. Follow these steps to learn how to implement manual scaling: </p>
			<h4>Note</h4>
			<p class="callout">In the previous chapter, we cloned the example files in Cloud Shell. If you didn't do this back then, we recommend doing that now:</p>
			<p class="callout"><strong class="inline">git clone https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Azure-third-edition</strong></p>
			<p class="callout">For this chapter, navigate to the <strong class="inline">Chapter04</strong> directory:</p>
			<p class="callout"><strong class="inline">cd Chapter04</strong></p>
			<ol>
				<li>Set up the guestbook by running the <strong class="inline">kubectl create</strong> command in the Azure command line:<p class="snippet">kubectl create -f guestbook-all-in-one.yaml</p></li>
				<li>After you have entered the preceding command, you should see something similar to what is shown in <em class="italics">Figure 4.1</em> in your command-line output:<div id="_idContainer090" class="IMG---Figure"><img src="image/B17338_04_01.jpg" alt="Setting up the guestbook application by running the kubectl create command"/></div><p class="figure">Figure 4.1: Launching the guestbook application</p></li>
				<li>Right now, none of the services are publicly accessible. We can verify this by running the following command:<p class="snippet">kubectl get service</p></li>
				<li>As seen in <em class="italics">Figure 4.2</em>, none of the services have an external IP:<div id="_idContainer091" class="IMG---Figure"><img src="image/B17338_04_02.jpg" alt="Verifying to ensure none of the services are publicly accessible"/></div><p class="figure">Figure 4.2: Output confirming that none of the services have a public IP</p></li>
				<li>To test the application, you will need to expose it publicly. For this, let's introduce a new command that will allow you to edit the service in Kubernetes without having to change the file on your file system. To start the edit, execute the following command:<p class="snippet">kubectl edit service frontend</p></li>
				<li>This will open a <strong class="inline">vi</strong> environment. Use the down arrow key to navigate to the line that says <strong class="inline">type:</strong> <strong class="inline">ClusterIP</strong> and change that to <strong class="inline">type: LoadBalancer</strong>, as shown in <em class="italics">Figure 4.3</em>. To make that change, hit the <em class="italics">I</em> button, change <strong class="inline">type</strong> to <strong class="inline">LoadBalancer</strong>, hit the <em class="italics">Esc</em> button, type <strong class="inline">:wq!</strong>, and then hit <em class="italics">Enter</em> to save the changes:<div id="_idContainer092" class="IMG---Figure"><img src="image/B17338_04_03.jpg" alt="Changing the type from ClusterIP to LoadBalancer"/></div><p class="figure">Figure 4.3: Changing this line to type: LoadBalancer</p></li>
				<li>Once the changes are saved, you can watch the service object until the public IP becomes available. To do this, type the following:<p class="snippet">kubectl get service -w</p></li>
				<li>It will take a couple of minutes to show you the updated IP. Once you see the correct public IP, you can exit the <strong class="inline">watch</strong> command by hitting <em class="italics">Ctrl</em> + <em class="italics">C</em>:<div id="_idContainer093" class="IMG---Figure"><img src="image/B17338_04_04.jpg" alt="Output showing the front-end service getting a public IP"/></div><p class="figure">Figure 4.4: Output showing the front-end service getting a public IP</p></li>
				<li>Type the IP address from the preceding output into your browser navigation bar as follows: <strong class="inline">http://&lt;EXTERNAL-IP&gt;/</strong>. The result of this is shown in <em class="italics">Figure 4.5</em>:</li>
			</ol>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B17338_04_05.jpg" alt="Entering the IP address in the browser to view the guestbook sample"/>
				</div>
			</div>
			<p class="figure">Figure 4.5: Browse to the guestbook application</p>
			<p>The familiar guestbook sample should be visible. This shows that you have successfully publicly accessed the guestbook.</p>
			<p>Now that you have the guestbook application deployed, you can start scaling the different components of the application.</p>
			<h3 id="_idParaDest-55"><a id="_idTextAnchor056"/>Scaling the guestbook front-end component</h3>
			<p>Kubernetes gives us the ability to scale each component of an application dynamically. In this section, we will show you how to scale the front end of the guestbook application. Right now, the front-end deployment is deployed with three replicas. You can confirm by using the following command:</p>
			<p class="snippet">kubectl get pods</p>
			<p>This should return an output as shown in <em class="italics">Figure 4.6</em>:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B17338_04_06.jpg" alt="Output confirming that we have three replicas in the front-end deployment"/>
				</div>
			</div>
			<p class="figure">Figure 4.6: Confirming the three replicas in the front-end deployment</p>
			<p>To scale the front-end deployment, you can execute the following command:</p>
			<p class="snippet">kubectl scale deployment/frontend --replicas=6</p>
			<p>This will cause Kubernetes to add additional pods to the deployment. You can set the number of replicas you want, and Kubernetes takes care of the rest. You can even scale it down to zero (one of the tricks used to reload the configuration when the application doesn't support the dynamic reload of configuration). To verify that the overall scaling worked correctly, you can use the following command:</p>
			<p class="snippet">kubectl get pods</p>
			<p>This should give you the output shown in <em class="italics">Figure 4.7</em>:</p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B17338_04_07.jpg" alt="Output showing 6 pods running in the frontend deployment after scaling out "/>
				</div>
			</div>
			<p class="figure">Figure 4.7: Different pods running in the guestbook application after scaling out</p>
			<p>As you can see, the front-end service scaled to six pods. Kubernetes also spread these pods across multiple nodes in the cluster. You can see the nodes that this is running on with the following command:</p>
			<p class="snippet">kubectl get pods -o wide</p>
			<p>This will generate the following output:</p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B17338_04_08.jpg" alt="Output showing the nodes on which the pods are running"/>
				</div>
			</div>
			<p class="figure">Figure 4.8: Showing which nodes the pods are running on</p>
			<p>In this section, you have seen how easy it is to scale pods with Kubernetes. This capability provides a very powerful tool for you to not only dynamically adjust your application components but also provide resilient applications with failover capabilities enabled by running multiple instances of components at the same time. However, you won't always want to manually scale your application. In the next section, you will learn how you can automatically scale your application in and out by automatically adding and removing pods in a deployment.</p>
			<h3 id="_idParaDest-56"><a id="_idTextAnchor057"/>Using the HPA</h3>
			<p>Scaling manually is useful when you're working on your cluster. For example, if you know your load is going to increase, you can manually scale out your application. In most cases, however, you will want some sort of autoscaling to happen on your application. In Kubernetes, you can configure autoscaling of your deployment using an object called the <strong class="bold">Horizontal Pod Autoscaler</strong> (<strong class="bold">HPA</strong>).</p>
			<p>HPA monitors Kubernetes metrics at regular intervals and, based on the rules you define, it automatically scales your deployment. For example, you can configure the HPA to add additional pods to your deployment once the CPU utilization of your application is above 50%.</p>
			<p>In this section, you will configure the HPA to scale the front-end of the application automatically:</p>
			<ol>
				<li value="1">To start the configuration, let's first manually scale down our deployment to one instance:<p class="snippet">kubectl scale deployment/frontend --replicas=1</p></li>
				<li>Next up, we'll create an HPA. Open up the code editor in Cloud Shell by typing <strong class="inline">code hpa.yaml</strong> and enter the following code:<p class="snippet">1   apiVersion: autoscaling/v1</p><p class="snippet">2   kind: HorizontalPodAutoscaler</p><p class="snippet">3   metadata:</p><p class="snippet">4     name: frontend-scaler</p><p class="snippet">5   spec:</p><p class="snippet">6     scaleTargetRef:</p><p class="snippet">7       apiVersion: apps/v1</p><p class="snippet">8       kind: Deployment</p><p class="snippet">9       name: frontend</p><p class="snippet">10    minReplicas: 1</p><p class="snippet">11    maxReplicas: 10</p><p class="snippet">12    targetCPUUtilizationPercentage: 50</p><p>Let's investigate what is configured in this file:</p><ul><li><strong class="bold">Line 2</strong>: Here, we define that we need <strong class="inline">HorizontalPodAutoscaler</strong>.</li><li><strong class="bold">Lines 6-9</strong>: These lines define the deployment that we want to autoscale.</li><li><strong class="bold">Lines 10-11</strong>: Here, we configure the minimum and maximum pods in our deployment.</li><li><strong class="bold">Lines 12</strong>: Here, we define the target CPU utilization percentage for our deployment.</li></ul></li>
				<li>Save this file, and create the HPA using the following command:<p class="snippet">kubectl create -f hpa.yaml</p><p>This will create our autoscaler. You can see your autoscaler with the following command:</p><p class="snippet">kubectl get hpa</p><p>This will initially output something as shown in <em class="italics">Figure 4.9</em>:</p><div id="_idContainer098" class="IMG---Figure"><img src="image/B17338_04_09.jpg" alt="Output displaying target as unknown, which indicates that the HPA isn't ready yet"/></div><p class="figure">Figure 4.9: The target unknown shows that the HPA isn't ready yet</p><p>It takes a couple of seconds for the HPA to read the metrics. Wait for the return from the HPA to look something similar to the output shown in <em class="italics">Figure 4.10</em>:</p><div id="_idContainer099" class="IMG---Figure"><img src="image/B17338_04_10.jpg" alt="Output showing the target with a percentage indicating that the HPA is ready"/></div><p class="figure">Figure 4.10: Once the target shows a percentage, the HPA is ready</p></li>
				<li>You will now go ahead and do two things: first, you will watch the pods to see whether new pods are created. Then, you will create a new shell, and create some load for the system. Let's start with the first task—watching our pods:<p class="snippet">kubectl get pods -w</p><p>This will continuously monitor the pods that get created or terminated.</p><p>Let's now create some load in a new shell. In Cloud Shell, hit the <strong class="bold">open new session</strong> icon to open a new shell:</p><div id="_idContainer100" class="IMG---Figure"><img src="image/B17338_04_11.jpg" alt="Clicking the open new session icon to open a new Cloud Shell"/></div><p class="figure">Figure 4.11: Use this button to open a new Cloud Shell</p><p>This will open a new tab in your browser with a new session in Cloud Shell. You will generate load for the application from this tab.</p></li>
				<li>Next, you will use a program called <strong class="inline">hey</strong> to generate this load. <strong class="inline">hey</strong> is a tiny program that sends loads to a web application. You can install and run <strong class="inline">hey</strong> using the following commands:<p class="snippet">export GOPATH=~/go</p><p class="snippet">export PATH=$GOPATH/bin:$PATH</p><p class="snippet">go get -u github.com/rakyll/hey</p><p class="snippet">hey -z 20m http://&lt;external-ip&gt;</p><p>The <strong class="inline">hey</strong> program will now try to create up to 20 million connections to the front-end. This will generate CPU loads on the system, which will trigger the HPA to start scaling the deployment. It will take a couple of minutes for this to trigger a scale action, but at a certain point, you should see multiple pods being created to handle the additional load, as shown in <em class="italics">Figure 4.12</em>:</p><div id="_idContainer101" class="IMG---Figure"><img src="image/B17338_04_12.jpg" alt="Output showing new pods being created by the HPA to handle the additional load"/></div><p class="figure">Figure 4.12: New pods get started by the HPA</p><p>At this point, you can go ahead and kill the <strong class="inline">hey</strong> program by hitting <em class="italics">Ctrl</em> + <em class="italics">C</em>.</p></li>
				<li>Let's have a closer look at what the HPA did by running the following command:<p class="snippet">kubectl describe hpa</p><p>We can see a few interesting points in the <strong class="inline">describe</strong> operation, as shown in <em class="italics">Figure 4.13</em>:</p><div id="_idContainer102" class="IMG---Figure"><img src="image/B17338_04_13.jpg" alt="Running the kubectl describe hpa command to get a detailed view of the HPA"/></div><p class="figure">Figure 4.13: Detailed view of the HPA</p><p>The annotations in <em class="italics">Figure 4.13</em> are explained as follows:</p><ul><li>This shows you the current CPU utilization (<strong class="inline">384%</strong>) versus the desired (<strong class="inline">50%</strong>). The current CPU utilization will likely be different in your situation.</li><li>This shows you that the current desired replica count is higher than the actual maximum you had configured. This ensures that a single deployment doesn't consume all resources in the cluster.</li><li>This shows you the scaling actions that the HPA took. It first scaled to 4, then to 8, and then to 10 pods in the deployment.</li></ul></li>
				<li>If you wait for a couple of minutes, the HPA should start to scale down. You can track this scale-down operation using the following command:<p class="snippet">kubectl get hpa -w</p><p>This will track the HPA and show you the gradual scaling down of the deployment, as displayed in <em class="italics">Figure 4.14</em>:</p><div id="_idContainer103" class="IMG---Figure"><img src="image/B17338_04_14.jpg" alt="Tracking the HPA scale down using the kubectl get hpa -w command"/></div><p class="figure">Figure 4.14: Watching the HPA scale down</p></li>
				<li>Before we move on to the next section, let's clean up the resources we created in this section:<p class="snippet">kubectl delete -f hpa.yaml</p><p class="snippet">kubectl delete -f guestbook-all-in-one.yaml</p></li>
			</ol>
			<p>In this section, you first manually and then automatically scaled an application. However, the infrastructure supporting the application was static; you ran this on a two-node cluster. In many cases, you might also run out of resources on the cluster. In the next section, you will deal with this issue and learn how you can scale the AKS cluster yourself.</p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor058"/>Scaling your cluster</h2>
			<p>In the previous section, you dealt with scaling the application running on top of a cluster. In this section, you'll learn how you can scale the actual cluster you are running. First, you will manually scale your cluster to one node. Then, you'll configure the cluster autoscaler. The cluster autoscaler will monitor your cluster and scale out when there are pods that cannot be scheduled on the cluster.</p>
			<h3 id="_idParaDest-58"><a id="_idTextAnchor059"/>Manually scaling your cluster</h3>
			<p>You can manually scale your AKS cluster by setting a static number of nodes for the cluster. The scaling of your cluster can be done either via the Azure portal or the command line.</p>
			<p>In this section, you'll learn how you can manually scale your cluster by scaling it down to one node. This will cause Azure to remove one of the nodes from your cluster. First, the workload on the node that is about to be removed will be rescheduled onto the other node. Once the workload is safely rescheduled, the node will be removed from your cluster, and then the VM will be deleted from Azure.</p>
			<p>To scale your cluster, follow these steps:</p>
			<ol>
				<li value="1">Open the Azure portal and go to your cluster. Once there, go to <strong class="bold">Node pools</strong> and click on the number below <strong class="bold">Node count</strong>, as shown in <em class="italics">Figure 4.15</em>:<div id="_idContainer104" class="IMG---Figure"><img src="image/B17338_04_15.jpg" alt="Manually scaling the cluster using the Azure portal"/></div><p class="figure">Figure 4.15: Manually scaling the cluster</p></li>
				<li>This will open a pop-up window that will give the option to scale your cluster. For our example, we will scale down our cluster to one node, as shown in <em class="italics">Figure 4.16</em>:<div id="_idContainer105" class="IMG---Figure"><img src="image/B17338_04_16.jpg" alt="Pop-up window confirming the new cluster size"/></div><p class="figure">Figure 4.16: Pop-up window confirming the new cluster size</p></li>
				<li>Hit the <strong class="bold">Apply</strong> button at the bottom of the screen to save these settings. This will cause Azure to remove a node from your cluster. This process will take about 5 minutes to complete. You can follow the progress by clicking on the notification icon at the top of the Azure portal as follows:</li>
			</ol>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B17338_04_17.jpg" alt="Clicking the notification icon in the Azure portal to check the progress of the scale-down operation"/>
				</div>
			</div>
			<p class="figure">Figure 4.17: Cluster scaling can be followed using the notifications in the Azure portal</p>
			<p>Once this scale-down operation has completed, relaunch the guestbook application on this small cluster:</p>
			<p class="snippet">kubectl create -f guestbook-all-in-one.yaml</p>
			<p>In the next section, you will scale out the guestbook so that it can no longer run on this small cluster. You will then configure the cluster autoscaler to scale out the cluster.</p>
			<h3 id="_idParaDest-59"><a id="_idTextAnchor060"/>Scaling your cluster using the cluster autoscaler</h3>
			<p>In this section, you will explore the cluster autoscaler. The cluster autoscaler will monitor the deployments in your cluster and scale your cluster to meet your application requirements. The cluster autoscaler watches the number of pods in your cluster that cannot be scheduled due to insufficient resources. You will first force your deployment to have pods that cannot be scheduled, and then configure the cluster autoscaler to automatically scale your cluster.</p>
			<p>To force your cluster to be out of resources, you will—manually—scale out the <strong class="inline">redis-replica</strong>  deployment. To do this, use the following command:</p>
			<p class="snippet">kubectl scale deployment redis-replica --replicas 5</p>
			<p>You can verify that this command was successful by looking at the pods in our cluster:</p>
			<p class="snippet">kubectl get pods</p>
			<p>This should show you something similar to the output shown in <em class="italics">Figure 4.18</em>:</p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B17338_04_18.jpg" alt="Output displaying four out of five pods as pending due to the cluster being out of resources"/>
				</div>
			</div>
			<p class="figure">Figure 4.18: Four out of five pods are pending, meaning they cannot be scheduled</p>
			<p>As you can see, you now have two pods in a <strong class="inline">Pending</strong> state. The <strong class="inline">Pending</strong> state in Kubernetes means that that pod cannot be scheduled onto a node. In this case, this is due to the cluster being out of resources.</p>
			<h4>Note</h4>
			<p class="callout">If your cluster is running on a larger VM size than the DS2v2, you might not notice pods in a <strong class="inline">Pending</strong> state now. In that case, increase the number of replicas to a higher number until you see pods in a pending state.</p>
			<p>You will now configure the cluster autoscaler to automatically scale the cluster. Similar to manual scaling in the previous section, there are two ways you can configure the cluster autoscaler. You can configure it either via the Azure portal—similar to how we did the manual scaling—or you can configure it using the <strong class="bold">command-line interface</strong> <strong class="bold">(CLI)</strong>. In this example, you will use CLI to enable the cluster autoscaler. The following command will configure the cluster autoscaler for your cluster:</p>
			<p class="snippet">az aks nodepool update --enable-cluster-autoscaler \</p>
			<p class="snippet">  -g rg-handsonaks --cluster-name handsonaks \</p>
			<p class="snippet">  --name agentpool --min-count 1 --max-count 2</p>
			<p>This command configures the cluster autoscaler on the node pool you have in the cluster. It configures it to have a minimum of one node and a maximum of two nodes. This will take a couple of minutes to configure.</p>
			<p>Once the cluster autoscaler is configured, you can see it in action by using the following command to watch the number of nodes in the cluster:</p>
			<p class="snippet">kubectl get nodes -w</p>
			<p>It will take about 5 minutes for the new node to show up and become <strong class="inline">Ready</strong> in the cluster. Once the new node is <strong class="inline">Ready</strong>, you can stop watching the nodes by hitting <em class="italics">Ctrl</em> + <em class="italics">C</em>. You should see an output similar to what you see in <em class="italics">Figure 4.19</em>:</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B17338_04_19.jpg" alt="Output showing a new node joining the cluster "/>
				</div>
			</div>
			<p class="figure">Figure 4.19: The new node joins the cluster</p>
			<p>The new node should ensure that your cluster has sufficient resources to schedule the scaled-out <strong class="inline">redis-</strong> <strong class="inline">replica</strong> deployment. To verify this, run the following command to check the status of the pods:</p>
			<p class="snippet">kubectl get pods</p>
			<p>This should show you all the pods in a <strong class="inline">Running</strong> state as follows:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B17338_04_20.jpg" alt="Output displaying all pods in a Running state"/>
				</div>
			</div>
			<p class="figure">Figure 4.20: All pods are now in a Running state</p>
			<p>Now clean up the resources you created, disable the cluster autoscaler, and ensure that your cluster has two nodes for the next example. To do this, use the following commands:</p>
			<p class="snippet">kubectl delete -f guestbook-all-in-one.yaml</p>
			<p class="snippet">az aks nodepool update --disable-cluster-autoscaler \</p>
			<p class="snippet">  -g rg-handsonaks --cluster-name handsonaks --name agentpool</p>
			<p class="snippet">az aks nodepool scale --node-count 2 -g rg-handsonaks \</p>
			<p class="snippet">  --cluster-name handsonaks --name agentpool</p>
			<h4>Note</h4>
			<p class="callout">The last command from the previous example will show you an error message, <strong class="inline">The new node count is the same as the current node count.</strong>, if the cluster already has two nodes. You can safely ignore this error.</p>
			<p>In this section, you first manually scaled down your cluster and then used the cluster autoscaler to scale out your cluster. You used the Azure portal to scale down the cluster manually and then used the Azure CLI to configure the cluster autoscaler. In the next section, you will look into how you can upgrade applications running on AKS.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor061"/>Upgrading your application</h2>
			<p>Using deployments in Kubernetes makes upgrading an application a straightforward operation. As with any upgrade, you should have good failbacks in case something goes wrong. Most of the issues you will run into will happen during upgrades. Cloud-native applications are supposed to make dealing with this relatively easy, which is possible if you have a very strong development team that embraces DevOps principles.</p>
			<p>The State of DevOps report (<a href="https://puppet.com/resources/report/2020-state-of-devops-report/">https://puppet.com/resources/report/2020-state-of-devops-report/</a>) has reported for multiple years that companies that have high software deployment frequency rates have higher availability and stability in their applications as well. This might seem counterintuitive, as doing software deployments heightens the risk of issues. However, by deploying more frequently and deploying using automated DevOps practices, you can limit the impact of software deployment.</p>
			<p>There are multiple ways you can make updates to applications running in a Kubernetes cluster. In this section, you will explore the following ways to update Kubernetes resources:</p>
			<ul>
				<li>Upgrading by changing YAML files: This method is useful when you have access to the full YAML file required to make the update. This can be done either from your command line or from an automated system.</li>
				<li>Upgrading using <strong class="inline">kubectl edit</strong>: This method is mostly used for minor changes on a cluster. It is a quick way to update your configuration live on a cluster.</li>
				<li>Upgrading using <strong class="inline">kubectl patch</strong>: This method is useful when you need to script a particular small update to a Kubernetes but don't have access to the full YAML file. It can be done either from a command line or an automated system. If you have access to the original YAML files, it is typically better to edit the YAML file and use <strong class="inline">kubectl apply</strong> to apply the updates.</li>
				<li>Upgrading using Helm: This method is used when your application is deployed through Helm.</li>
			</ul>
			<p>The methods described in the following sections work great if you have stateless applications. If you have a state stored anywhere, make sure to back up that state before you try upgrading your application.</p>
			<p>Let's start this section by doing the first type of upgrade by changing YAML files.</p>
			<h3 id="_idParaDest-61"><a id="_idTextAnchor062"/>Upgrading by changing YAML files</h3>
			<p>In order to upgrade a Kubernetes service or deployment, you can update the actual YAML definition file and apply that to the currently deployed application. Typically, we use <strong class="inline">kubectl create</strong> to create resources. Similarly, we can use <strong class="inline">kubectl apply</strong> to make changes to the resources.</p>
			<p>The deployment detects the changes (if any) and matches the running state to the desired state. Let's see how this is done:</p>
			<ol>
				<li value="1">Start with our guestbook application to explore this example:<p class="snippet">kubectl apply -f guestbook-all-in-one.yaml</p></li>
				<li>After a few minutes, all the pods should be running. Let's perform the first upgrade by changing the service from <strong class="inline">ClusterIP</strong> to <strong class="inline">LoadBalancer</strong>, as you did earlier in the chapter. However, now you will edit the YAML file rather than using <strong class="inline">kubectl edit</strong>. Edit the YAML file using the following command:<p class="snippet">code guestbook-all-in-one.yaml</p><p>Uncomment line 102 in this file to set the <strong class="inline">type</strong> to <strong class="inline">LoadBalancer</strong>, and save the file, as shown in <em class="italics">Figure 4.21</em>:</p><div id="_idContainer110" class="IMG---Figure"><img src="image/B17338_04_21.jpg" alt="Changing the service type from ClusterIP to LoadBalancer using the YAML file"/></div><p class="figure">Figure 4.21: Setting the type to LoadBalancer in the guestbook-all-in-one YAML file</p></li>
				<li>Apply the change as shown in the following code:<p class="snippet">kubectl apply -f guestbook-all-in-one.yaml</p><p>You should see an output similar to <em class="italics">Figure 4.22</em>:</p><div id="_idContainer111" class="IMG---Figure"><img src="image/B17338_04_22.jpg" alt="Output confirming that the service’s frontend has been updated"/></div><p class="figure">Figure 4.22: The service's front-end is updated</p><p>As you can see in <em class="italics">Figure 4.22</em>, only the object that was updated in the YAML file, which is the service in this case, was updated on Kubernetes, and the other objects remained unchanged.</p></li>
				<li>You can now get the public IP of the service using the following command:<p class="snippet">kubectl get service</p><p>Give it a few minutes, and you should be shown the IP, as displayed in <em class="italics">Figure 4.23</em>:</p><div id="_idContainer112" class="IMG---Figure"><img src="image/B17338_04_23.jpg" alt="Using the kubectl get service command to display the public IP of the service"/></div><p class="figure">Figure 4.23: Output displaying a public IP</p></li>
				<li>You will now make another change. You'll downgrade the front-end image on line 127 from <strong class="inline">image: gcr.io/google-samples/gb-frontend:v4</strong> to the following:<p class="snippet">image: gcr.io/google-samples/gb-frontend:v3</p><p>This change can be made by opening the guestbook application in the editor by using this familiar command:</p><p class="snippet">code guestbook-all-in-one.yaml</p></li>
				<li>Run the following command to perform the update and watch the pods change:<p class="snippet">kubectl apply -f guestbook-all-in-one.yaml &amp;&amp; kubectl get pods -w</p><p>This will generate an output similar to <em class="italics">Figure 4.24</em>:</p><div id="_idContainer113" class="IMG---Figure"><img src="image/B17338_04_24.jpg" alt="Output displaying new pods created from a new ReplicaSet"/></div><p class="figure">Figure 4.24: Pods from a new ReplicaSet are created</p><p>What you can see here is that a new version of the pod gets created (based on a new ReplicaSet). Once the new pod is running and ready, one of the old pods is terminated. This create-terminate loop is repeated until only new pods are running. In <em class="italics">Chapter 5, Handling common failures in AKS,</em> you'll see an example of such an upgrade gone wrong and you'll see that Kubernetes will not continue with the upgrade process until the new pods are healthy.</p></li>
				<li>Running <strong class="inline">kubectl get events | grep ReplicaSet</strong> will show the rolling update strategy that the deployment uses to update the front-end images:<div id="_idContainer114" class="IMG---Figure"><img src="image/B17338_04_25.jpg" alt="Monitoring Kubernetes events and filtering to only see ReplicaSet-related events"/></div><p class="figure">Figure 4.25: Monitoring Kubernetes events and filtering to only see ReplicaSet-related events</p><h4>Note</h4><p class="callout">In the preceding example, you are making use of a pipe—shown by the <strong class="inline">|</strong> sign—and the <strong class="inline">grep</strong> command. A pipe in Linux is used to send the output of one command to the input of another command. In this case, you sent the output of <strong class="inline">kubectl get events</strong><em class="italics"> </em>to the <strong class="inline">grep</strong> command. Linux uses the <strong class="inline">grep</strong> command to filter text. In this case, you used the <strong class="inline">grep</strong> command to only show lines that contain the word ReplicaSet.</p><p>You can see here that the new ReplicaSet gets scaled up, while the old one gets scaled down. You will also see two ReplicaSets for the front-end, the new one replacing the other one pod at a time:</p><p class="snippet">kubectl get replicaset</p><p>This will display the output shown in <em class="italics">Figure 4.26</em>:</p><div id="_idContainer115" class="IMG---Figure"><img src="image/B17338_04_26.jpg" alt="Output showing two ReplicaSets are available for the frontend deployment, one with 0 pods, the other with 3 pods"/></div><p class="figure">Figure 4.26: Two different ReplicaSets</p></li>
				<li>Kubernetes will also keep a history of your rollout. You can see the rollout history using this command:<p class="snippet">kubectl rollout history deployment frontend</p><p>This will generate the output shown in <em class="italics">Figure 4.27</em>:</p><div id="_idContainer116" class="IMG---Figure"><img src="image/B17338_04_27.jpg" alt="Displaying the rollout history of the deployment"/></div><p class="figure">Figure 4.27: Deployment history of the application</p></li>
				<li>Since Kubernetes keeps a history of the rollout, this also enables rollback. Let's do a rollback of your deployment:<p class="snippet">kubectl rollout undo deployment frontend</p><p>This will trigger a rollback. This means that the new ReplicaSet will be scaled down to zero instances, and the old one will be scaled up to three instances again. You can verify this using the following command:</p><p class="snippet">kubectl get replicaset</p><p>The resultant output is as shown in <em class="italics">Figure 4.28</em>:</p><div id="_idContainer117" class="IMG---Figure"><img src="image/B17338_04_28.jpg" alt="Output displaying the old ReplicaSet with three pods and the new ReplicaSet scaled down to zero"/></div><p class="figure">Figure 4.28: The old ReplicaSet now has three pods, and the new one is scaled down to zero</p><p>This shows you, as expected, that the old ReplicaSet is scaled back to three instances and the new one is scaled down to zero instances.</p></li>
				<li>Finally, let's clean up again by running the <strong class="inline">kubectl delete</strong> command:<p class="snippet">kubectl delete -f guestbook-all-in-one.yaml</p><p>Congratulations! You have completed the upgrade of an application and a rollback to a previous version.</p></li>
			</ol>
			<p>In this example, you have used <strong class="inline">kubectl apply</strong> to make changes to your application. You can similarly also use <strong class="inline">kubectl edit</strong> to make changes, which will be explored in the next section.</p>
			<h3 id="_idParaDest-62"><a id="_idTextAnchor063"/>Upgrading an application using kubectl edit</h3>
			<p>You can also make changes to your application running on top of Kubernetes by using <strong class="inline">kubectl edit</strong>. You used this previously in this chapter, in the <em class="italics">Manually scaling your application</em> section. When running <strong class="inline">kubectl edit</strong>, the <strong class="inline">vi</strong> editor will be opened for you, which will allow you to make changes directly against the object in Kubernetes.</p>
			<p>Let's redeploy the guestbook application without a public load balancer and use <strong class="inline">kubectl</strong> to create the load balancer:</p>
			<ol>
				<li value="1">Undo the changes you made in the previous step. You can do this by using the following command:<p class="snippet">git reset --hard</p></li>
				<li>You will then deploy the guestbook application:<p class="snippet">kubectl create -f guestbook-all-in-one.yaml</p></li>
				<li>To start the edit, execute the following command:<p class="snippet">kubectl edit service frontend</p></li>
				<li>This will open a <strong class="inline">vi</strong> environment. Navigate to the line that now says <strong class="inline">type:</strong> <strong class="inline">ClusterIP</strong> (line 27) and change that to <strong class="inline">type: LoadBalancer</strong>, as shown in <em class="italics">Figure 4.29</em>. To make that change, hit the <em class="italics">I</em> button, type your changes, hit the <em class="italics">Esc </em>button, type <strong class="inline">:wq!</strong>, and then hit <em class="italics">Enter</em> to save the changes:<div id="_idContainer118" class="IMG---Figure"><img src="image/B17338_04_29.jpg" alt="Displaying the rollout history of the deployment"/></div><p class="figure">Figure 4.29: Changing this line to type: LoadBalancer</p></li>
				<li>Once the changes are saved, you can watch the service object until the public IP becomes available. To do this, type the following:<p class="snippet">kubectl get svc -w</p></li>
				<li>It will take a couple of minutes to show you the updated IP. Once you see the right public IP, you can exit the <strong class="inline">watch</strong> command by hitting <em class="italics">Ctrl</em> + <em class="italics">C</em>.</li>
			</ol>
			<p>This is an example of using <strong class="inline">kubectl edit</strong> to make changes to a Kubernetes object. This command will open up a text editor to interactively make changes. This means that you need to interact with the text editor to make the changes. This will not work in an automated environment. To make automated changes, you can use the <strong class="inline">kubectl patch</strong> command.</p>
			<h3 id="_idParaDest-63"><a id="_idTextAnchor064"/>Upgrading an application using kubectl patch</h3>
			<p>In the previous example, you used a text editor to make the changes to Kubernetes. In this example, you will use the <strong class="inline">kubectl patch</strong> command to make changes to resources on Kubernetes. The <strong class="inline">patch</strong> command is particularly useful in automated systems when you don't have access to the original YAML file that is deployed on a cluster. It can be used, for example, in a script or in a continuous integration/continuous deployment system.</p>
			<p>There are two main ways in which to use <strong class="inline">kubectl patch</strong>: either by creating a file containing your changes (called a patch file) or by providing the changes inline. Both approaches will be explained here. First, in this example, you'll change the image of the front-end from <strong class="inline">v4</strong> to <strong class="inline">v3</strong> using a patch file:</p>
			<ol>
				<li value="1">Start this example by creating a file called <strong class="inline">frontend-image-patch.yaml</strong>:<p class="snippet">code frontend-image-patch.yaml</p></li>
				<li>Use the following text as a patch in that file:<p class="snippet">spec:</p><p class="snippet">  template:</p><p class="snippet">    spec:</p><p class="snippet">      containers:</p><p class="snippet">      - name: php-redis</p><p class="snippet">        image: gcr.io/google-samples/gb-frontend:v3</p><p>This patch file uses the same YAML layout as a typical YAML file. The main thing about a patch file is that it only has to contain the changes and doesn't have to be capable of deploying the whole resource.</p></li>
				<li>To apply the patch, use the following command:<p class="snippet">kubectl patch deployment frontend \</p><p class="snippet">  --patch "$(cat frontend-image-patch.yaml)"</p><p>This command does two things: first, it reads the <strong class="inline">frontend-image-patch.yaml</strong> file using the <strong class="inline">cat</strong> command, and then it passes that to the <strong class="inline">kubectl patch</strong> command to execute the change.</p></li>
				<li>You can verify the changes by describing the front-end deployment and looking for the <strong class="inline">Image</strong> section:<p class="snippet">kubectl describe deployment frontend</p><p>This will display an output as follows:</p><div id="_idContainer119" class="IMG---Figure"><img src="image/B17338_04_30.jpg" alt="Running the kubectl describe deployment frontend command to confirm if we’re running the old image"/></div><p class="figure">Figure 4.30: After the patch, we are running the old image</p><p>This was an example of using the <strong class="inline">patch</strong> command using a patch file. You can also apply a patch directly on the command line without creating a YAML file. In this case, you would describe the change in JSON rather than in YAML.</p><p>Let's run through an example in which we will revert the image change to <strong class="inline">v4</strong>:</p></li>
				<li>Run the following command to patch the image back to <strong class="inline">v4</strong>:<p class="snippet">kubectl patch deployment frontend \</p><p class="snippet">--patch='</p><p class="snippet">{</p><p class="snippet">    "spec": {</p><p class="snippet">        "template": {</p><p class="snippet">            "spec": {</p><p class="snippet">                "containers": [{</p><p class="snippet">                    "name": "php-redis",</p><p class="snippet">                    "image": "gcr.io/google-samples/gb-frontend:v4"</p><p class="snippet">                }]</p><p class="snippet">            }</p><p class="snippet">        }</p><p class="snippet">    }</p><p class="snippet">}'</p></li>
				<li>You can verify this change by describing the deployment and looking for the <strong class="inline">Image</strong> section:<p class="snippet">kubectl describe deployment frontend</p><p>This will display the output shown in <em class="italics">Figure 4.31</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B17338_04_31.jpg" alt="Running the kubectl describe deployment frontend command to confirm if we’re running the new image"/>
				</div>
			</div>
			<p class="figure">Figure 4.31: After another patch, we are running the new version again</p>
			<p>Before moving on to the next example, let's remove the guestbook application from the cluster:</p>
			<p class="snippet">kubectl delete -f guestbook-all-in-one.yaml</p>
			<p>So far, you have explored three ways of upgrading Kubernetes applications. First, you made changes to the actual YAML file and applied them using <strong class="inline">kubectl apply</strong>. Afterward, you used <strong class="inline">kubectl edit</strong> and <strong class="inline">kubectl patch</strong> to make more changes. In the final section of this chapter, you will use Helm to upgrade an application.</p>
			<h3 id="_idParaDest-64"><a id="_idTextAnchor065"/>Upgrading applications using Helm</h3>
			<p>This section will explain how to perform upgrades using Helm operators:</p>
			<ol>
				<li value="1">Run the following command:<p class="snippet">helm install wp bitnami/wordpress</p><p>You will force an update of the image of the MariaDB container. Let's first check the version of the current image:</p><p class="snippet">kubectl describe statefulset wp-mariadb | grep Image</p><p>At the time of writing, the image version is <strong class="inline">10.5.8-debian-10-r46</strong> as follows:</p><div id="_idContainer121" class="IMG---Figure"><img src="image/B17338_04_32.jpg" alt="Output displaying the current image version of the StatefulSet"/></div><p class="figure">Figure 4.32: Getting the current image of the StatefulSet</p><p>Let's look at the tags from <a href="https://hub.docker.com/r/bitnami/mariadb/tags">https://hub.docker.com/r/bitnami/mariadb/tags</a> and select another tag. For example, you could select the <strong class="inline">10.5.8-debian-10-r44</strong> tag to update your StatefulSet.</p><p>However, in order to update the MariaDB container image, you need to get the root password for the server and the password for the database. This is because the WordPress application is configured to use these passwords to connect to the database. By default, the update using Helm on the WordPress deployment would generate new passwords. In this case, you'll be providing the existing passwords, to ensure the application remains functional.</p><p>The passwords are stored in a Kubernetes Secret object. Secrets will be explained in more depth in <em class="italics">Chapter 10, Storing secrets in AKS</em>. You can get the MariaDB passwords in the following way:</p><p class="snippet">kubectl get secret wp-mariadb -o yaml</p><p>This will generate the output shown in <em class="italics">Figure 4.33</em>:</p><div id="_idContainer122" class="IMG---Figure"><img src="image/B17338_04_33.jpg" alt="Output displaying encrypted secrets, that is, the MariaDB passwords"/></div><p class="figure">Figure 4.33: The encrypted secrets that MariaDB uses</p><p>In order to get the decoded password, use the following command:</p><p class="snippet">echo "&lt;password&gt;" | base64 -d</p><p>This will show us the decoded root password and the decoded database password, as shown in <em class="italics">Figure 4.34</em>:</p><div id="_idContainer123" class="IMG---Figure"><img src="image/B17338_04_34.jpg" alt="Output displaying the unencrypted version of the MariaDB passwords"/></div><p class="figure">Figure 4.34: The decoded root and database passwords</p><p>You also need the WordPress password. You can get that by getting the <strong class="inline">wp-wordpress</strong> secret and using the same decoding process:</p><p class="snippet">kubectl get secret wp-wordpress -o yaml</p><p class="snippet">echo "&lt;WordPress password&gt;" | base64 -d</p></li>
				<li>You can update the image tag with Helm and then watch the pods change using the following command:<p class="snippet">helm upgrade wp bitnami/wordpress \</p><p class="snippet">--set mariadb.image.tag=10.5.8-debian-10-r44\</p><p class="snippet">--set mariadb.auth.password="&lt;decoded password&gt;" \</p><p class="snippet">--set mariadb.auth.rootPassword="&lt;decoded password&gt;" \</p><p class="snippet">--set wordpressPassword="&lt;decoded password&gt;" \</p><p class="snippet">&amp;&amp; kubectl get pods -w</p><p>This will update the image of MariaDB and make a new pod start. You should see an output similar to <em class="italics">Figure 4.35</em>, where you can see the previous version of the database pod being terminated, and a new one start:</p><div id="_idContainer124" class="IMG---Figure"><img src="image/B17338_04_35.jpg" alt="Output displaying the previous MariaDB pod beingterminated and a new one starting"/></div><p class="figure">Figure 4.35: The previous MariaDB pod gets terminated and a new one starts</p><p>Running <strong class="inline">describe</strong> on the new pod and grepping for <strong class="inline">Image</strong> will show us the new image version:</p><p class="snippet">kubectl describe pod wp-mariadb-0 | grep Image</p><p>This will generate an output as shown in <em class="italics">Figure 4.36</em>:</p><div id="_idContainer125" class="IMG---Figure"><img src="image/B17338_04_36.jpg" alt="Output displaying the new image version"/></div><p class="figure">Figure 4.36: Showing the new image</p></li>
				<li>Finally, clean up by running the following command:<p class="snippet">helm delete wp</p><p class="snippet">kubectl delete pvc --all</p><p class="snippet">kubectl delete pv --all</p></li>
			</ol>
			<p>You have now learned how to upgrade an application using Helm. As you have seen in this example, upgrading using Helm can be done by using the <strong class="inline">--set</strong> operator. This makes performing upgrades and multiple deployments using Helm efficient.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor066"/>Summary</h2>
			<p>This a chapter covered a plethora of information on building scalable applications. The goal was to show you how to scale deployments with Kubernetes, which was achieved by creating multiple instances of your application.</p>
			<p>We started the chapter by looking at how to define the use of a load balancer and leverage the deployment scale feature in Kubernetes to achieve scalability. With this type of scalability, you can also achieve failover by using a load balancer and multiple instances of the software for stateless applications. We also looked into using the HPA to automatically scale your deployment based on load.</p>
			<p>After that, we looked at how you can scale the cluster itself. First, we manually scaled the cluster, and afterward we used a cluster autoscaler to scale the cluster based on application demand.</p>
			<p>We finished the chapter by looking into different ways to upgrade a deployed application: first, by exploring updating YAML files manually, and then by learning two additional <strong class="inline">kubectl</strong> commands (<strong class="inline">edit</strong> and <strong class="inline">patch</strong>) that can be used to make changes. Finally, we learned how Helm can be used to perform these upgrades.</p>
			<p>In the next chapter, we will look at a couple of common failures that you may face while deploying applications to AKS and how to fix them.</p>
		</div>
	</body></html>