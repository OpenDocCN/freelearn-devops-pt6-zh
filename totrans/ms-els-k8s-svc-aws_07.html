<html><head></head><body>
		<div id="_idContainer063">
			<h1 id="_idParaDest-107" class="chapter-number"><a id="_idTextAnchor107"/>7</h1>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor108"/>Networking in EKS</h1>
			<p><strong class="bold">Kubernetes</strong> (<strong class="bold">K8s</strong>) isn’t prescriptive about external networking. This means it is possible to use multiple network plugins and configurations in Kubernetes to meet security, latency, and <span class="No-Break">operational requirements.</span></p>
			<p>In this chapter, we will focus on how standard K8s Pod and cluster networking works and then discuss the similarities and differences in an AWS <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>). Specifically, we will cover <span class="No-Break">the following:</span></p>
			<ul>
				<li>Understanding networking <span class="No-Break">in Kubernetes</span></li>
				<li>Getting to grips with basic <span class="No-Break">AWS networking</span></li>
				<li>Understanding <span class="No-Break">EKS networking</span></li>
				<li>Configuring EKS networking using the <span class="No-Break">VPC CNI</span></li>
				<li>Common <span class="No-Break">networking issues</span></li>
			</ul>
			<p>The reader should have a familiarity with TCP/IP networking, how networks work in AWS, and the concepts of NAT. This chapter is intended to give the reader the skills to configure and manage EKS networking for one or <span class="No-Break">more clusters.</span></p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor109"/>Understanding networking in Kubernetes</h1>
			<p>Kubernetes<a id="_idIndexMarker272"/> is designed to be extensible, and as such it supports<a id="_idIndexMarker273"/> multiple network implementations, all of which meet a clearly defined networking model. K8s has some basic networking rules that all network <a id="_idIndexMarker274"/>plugins <span class="No-Break">must follow:</span></p>
			<ul>
				<li>Every Pod<a id="_idIndexMarker275"/> gets its own <span class="No-Break">IP address</span></li>
				<li>Containers within a Pod share the Pod <span class="No-Break">IP address</span></li>
				<li>Pods can communicate with all other Pods in the cluster using Pod IP addresses (<span class="No-Break">without NAT)</span></li>
				<li>Isolation of <a id="_idIndexMarker276"/>Pods at the <a id="_idIndexMarker277"/>network level is performed using <span class="No-Break">network policies</span></li>
			</ul>
			<p>For compliance reasons, any K8s network <a id="_idIndexMarker278"/>implementation must be built to support the <strong class="bold">Container Network Interface</strong> (<strong class="bold">CNI</strong>) specification, which is a <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>) project. The CNI specification <a id="_idIndexMarker279"/>consists of guides and libraries for writing plugins to configure network interfaces in containers. While it is possible to have multiple CNIs in a single cluster, by default, a single K8s cluster will be configured to support only a single CNI. There are many types and <a id="_idIndexMarker280"/>providers of CNI plugins, but they all allow Pods to <a id="_idIndexMarker281"/>connect to an external network and/or the allocation of Pod <span class="No-Break">IP addresses.</span></p>
			<p>Before we dive into networking specifically for EKS, it’s important to understand how networking generally works in K8s as most CNI implementations follow <span class="No-Break">this pattern.</span></p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor110"/>Network implementation in Kubernetes</h2>
			<p>A Pod<a id="_idIndexMarker282"/> is the smallest unit that <a id="_idIndexMarker283"/>can be deployed and managed by Kubernetes. A <a id="_idIndexMarker284"/>Pod can contain more than one container. Containers in a Pod share a network namespace, which means they share the same IP address, network port space, and Ethernet interface. The following diagram illustrates Pod-to-Pod connectivity within a node and across nodes in the <span class="No-Break">same cluster.</span></p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B18129_07_01.jpg" alt="Figure 7.1 – Basic Pod networking"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Basic Pod networking</p>
			<p>K8s network <a id="_idIndexMarker285"/>communication<a id="_idIndexMarker286"/> happens in several ways, depending on the sources <span class="No-Break">and destinations:</span></p>
			<ul>
				<li>As the <a id="_idIndexMarker287"/>containers in a Pod share the same network namespace and port space, they can communicate with each other using a localhost (<span class="No-Break"><strong class="source-inline">127.0.0.1</strong></span><span class="No-Break">) address.</span></li>
				<li>Each Pod has a corresponding interface (veth) in the root network namespace of the host, as well as its own interface in its network namespace. This is known as a veth pair, which acts as a virtual network cable between the Pod network namespace and the host networking, which has the actual Ethernet interface. Pods that want to talk to each other use the cluster DNS to resolve a service name to an IP address, and the ARP protocol to map the IP address to a Pod <span class="No-Break">Ethernet address.</span></li>
				<li>If the Pod is on another node, the cluster DNS resolves the IP address. In cases where the ARP request fails, the packet is routed out of the host to the network where it <a id="_idIndexMarker288"/>hopefully finds a route to the target <span class="No-Break">IP address.</span></li>
			</ul>
			<p>The CNI<a id="_idIndexMarker289"/> integrates<a id="_idIndexMarker290"/> with kubelet, which is the primary K8s agent that runs on all worker nodes. When a new Pod is created, it doesn’t have a network interface. The kubelet will send an <strong class="source-inline">ADD</strong> command to the CNI, which is then responsible for <span class="No-Break">the following:</span></p>
			<ol>
				<li>Inserting a network interface into the container network <span class="No-Break">namespace (</span><span class="No-Break"><strong class="source-inline">eth0</strong></span><span class="No-Break">)</span></li>
				<li>Making any necessary changes on the host such as creating the <strong class="source-inline">veth</strong> interface and attaching it to the <strong class="source-inline">Bridge0</strong> and the <span class="No-Break"><strong class="source-inline">eth0</strong></span><span class="No-Break"> interfaces</span></li>
				<li>Assigning an IP address to the interface and setting up the <span class="No-Break">relevant routes</span></li>
			</ol>
			<p>Kubernetes adds further abstraction on top of the basic Pod networking. A Kubernetes cluster allows multiple replicas of the same Pod to be deployed across multiple hosts and allows ingress traffic to be routed to any one of those hosts. There are different types of service; we will focus on a <strong class="source-inline">NodePort</strong> service for this example. When a service is created, it will select (typically) Pods based on a label. It creates a new DNS name, virtual IP, assigns a dynamic port on each node, and keeps a map of which nodes are hosting which Pods with the label <a id="_idIndexMarker291"/>defined in the service specifcation. This is shown in the <span class="No-Break">following diagram.</span></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B18129_07_02.jpg" alt="Figure 7.2 – Nodeport services"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Nodeport services</p>
			<p>As traffic arrives at the service (using the service DNS name or <em class="italic">host:dynamic port</em> combination), iptables or IP Virtual Server (IPVS) are used to rewrite the request service address to a relevant Pod address (under the control of kube-proxy) and then the basic Pod networking rules are applied as described previously. In the case of service 1 (<span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.2</em>), traffic can be sent to each node and the destination will be rewritten to the Pod running on that node. In the case of service 2, traffic arriving at node 3 has no local Pod, so traffic will be sent to either node 1 or <span class="No-Break">node 2.</span></p>
			<p>By default, traffic<a id="_idIndexMarker292"/> will be source NAT’d from node 3, so traffic always flows in and out of<a id="_idIndexMarker293"/> node 3 irrespective of where the Pods are located. The Kubernetes network proxy (kube-proxy) runs on each node and is responsible for managing services and the requests (including SNAT) and load balancing for <span class="No-Break">the Pods.</span></p>
			<p>SNAT means replacing the source IP address of the IP packet with another address. In most cases, this will be the IP address of the node’s Ethernet address. <strong class="bold">Destination NAT</strong> (<strong class="bold">DNAT</strong>) is where<a id="_idIndexMarker294"/> the destination IP address is replaced with another address, generally the IP address of a Pod. The following diagram illustrates <span class="No-Break">these concepts.</span></p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B18129_07_03.jpg" alt="Figure 7.3 – K8s source/destination NAT"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – K8s source/destination NAT</p>
			<p>In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em>, using a nodePort service as <span class="No-Break">the example:</span></p>
			<ol>
				<li>The traffic is received on node 3 from the client (<strong class="source-inline">10.2.3.4</strong>) for the service exposed on nodeport service <span class="No-Break">port </span><span class="No-Break"><strong class="source-inline">3124</strong></span><span class="No-Break">.</span></li>
				<li>kube-proxy will perform the SNAT, mapping the source IP to the local node’s Ethernet address and using DNAT to map a service address to a Pod IP address (on <span class="No-Break">node 1).</span></li>
				<li>The packet is sent to node 1 (as this will respond to the ARP request for the Pod IP address). The K8s endpoint, which contains the IP addresses of any Pods that match the service selector, is used to send the packet to <span class="No-Break">the Pod.</span></li>
				<li>The Pod response is set back to node 3 (based on the source IP address) and then mapped<a id="_idIndexMarker295"/> back to the client based on the source port mapping<a id="_idIndexMarker296"/> maintained <span class="No-Break">by kube-proxy.</span></li>
			</ol>
			<p>AWS networking<a id="_idIndexMarker297"/> is prescriptive, it configures K8s networking to work in conjunction with AWS VPC networking, and it has a big impact on how EKS networking works by default. The next section will quickly review how AWS VPC networking works and some of the concepts you need to understand as we dive deeper in <span class="No-Break">EKS networking.</span></p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor111"/>Getting to grips with basic AWS networking</h1>
			<p>Before we discuss EKS networking, we <a id="_idIndexMarker298"/>will quickly review basic VPC networking in AWS. When you sign up to AWS, you are provided with an AWS account that can deploy services across multiple Regions, and <a id="_idIndexMarker299"/>multiple <strong class="bold">Availability Zones</strong> (<strong class="bold">AZ</strong>) in each Region. A Region<a id="_idIndexMarker300"/> is a geographic location, such as London, Frankfurt, or Oregon, and consists of multiple AZs, which in turn each consist of two or more AWS data centers connected to each other over high-speed networks. An AZ is the basic unit of network reliability <span class="No-Break">in AWS.</span></p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B18129_07_04.jpg" alt="Figure 7.4 – Basic VPC structure"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Basic VPC structure</p>
			<p>A VPC<a id="_idIndexMarker301"/> is a regional <a id="_idIndexMarker302"/>construct that is defined by an IP <strong class="bold">Classless Inter-Domain Routing</strong> (<strong class="bold">CIDR</strong>) range such as <strong class="source-inline">10.1.0.0/16</strong>. Subnets are assigned from a VPC and map to one AZ. Services that have an IP address, such as EKS, are assigned to a subnet (or group of subnets) and the AWS platform will assign an available IP address from the subnet range and create an <strong class="bold">Elastic Network Interface</strong> (<strong class="bold">ENI</strong>) in that<a id="_idIndexMarker303"/> subnet. In most AWS VPCs, RFC1918, that is, private, addressing is used, which means VPC CIDR ranges are drawn from the <span class="No-Break">following subnets:</span></p>
			<ul>
				<li><strong class="source-inline">10.0.0.0</strong> – <strong class="source-inline">10.255.255.255</strong> (<span class="No-Break"><strong class="source-inline">10/8</strong></span><span class="No-Break"> prefix)</span></li>
				<li><strong class="source-inline">172.16.0.0</strong> – <strong class="source-inline">172.31.255.255</strong> (<span class="No-Break"><strong class="source-inline">172.16/12</strong></span><span class="No-Break"> prefix)</span></li>
				<li><strong class="source-inline">192.168.0.0</strong> – <strong class="source-inline">192.168.255.255</strong> (<span class="No-Break"><strong class="source-inline">192.168/16</strong></span><span class="No-Break"> prefix)</span></li>
			</ul>
			<p>In addition, the VPC can now use non-RFC1918 addresses, those in the <strong class="source-inline">100.64.0.0/10</strong> and <strong class="source-inline">198.19.0.0/16</strong> ranges, which EKS supports. In large enterprises, these ranges are shared across the existing data centers and offices, so a small range of addresses are typically given to the AWS platform, which is then shared across multiple <a id="_idIndexMarker304"/>VPCs and AWS services including EKS. It is possible to add additional IP ranges to a VPC, that is, secondary addressing, but not to change the ranges once they have been set. In the preceding example, an additional range, <strong class="source-inline">100.64.0.0/10</strong>, has been added and three additional subnets created from that range in three separate AZs. Within a VPC, any IP range, primary or secondary, is routable. In the preceding example, a host on <a id="_idIndexMarker305"/>subnet <strong class="source-inline">10.1.1.0/24</strong> can route to any other subnet including <strong class="source-inline">100.64.0.0/16</strong>; however, AWS <strong class="bold">Security Groups</strong> (<strong class="bold">SGs</strong>) and/or <strong class="bold">Network Access Control List</strong> (<strong class="bold">NACLs</strong>) control which systems can communicate with <a id="_idIndexMarker306"/>which <span class="No-Break">other systems.</span></p>
			<p>Three additional services are needed to allow access to and from the internet. An <strong class="bold">Internet Gateway</strong> (<strong class="bold">IGW</strong>) allows<a id="_idIndexMarker307"/> mapping between public IP addresses and the VPC addresses (ingress and <span class="No-Break">egress traffic).</span></p>
			<p>A <strong class="bold">NAT Gateway</strong> (<strong class="bold">NATGW</strong>) can use<a id="_idIndexMarker308"/> an IGW to provide outbound access only and is used when applications/systems need to access public AWS APIs (such as the EKS API) or public services such as Docker Hub to pull container images, but don’t want to be accessed by anything on the internet. Private NATGWs are also possible, which simply involves a NAT of a private subnet to a private address without any relationship to an IGW. This is used to translate between a range that is being reused elsewhere (on-premises or in another part of the AWS cloud) or is not being routed <span class="No-Break">on premises.</span></p>
			<p>A <strong class="bold">Transit Gateway</strong> (<strong class="bold">TGW</strong>) is used to<a id="_idIndexMarker309"/> route between other VPCs (in the same or other AWS accounts) and connects to on-premises workloads and services (through a VPN or a Direct Connect <span class="No-Break">private connection).</span></p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor112"/>Understanding EKS networking</h1>
			<p>Now that we understand the basic K8s<a id="_idIndexMarker310"/> network models, what a CNI is, and how VPC networking works, we can explore how EKS networking works. The VPC CNI has several configuration options; we will not cover all possible configurations in this section, only the most <span class="No-Break">common ones.</span></p>
			<p>EKS is a managed service, and the control plane is managed by AWS in a separate VPC. The two main networking questions you need to ask when configuring your cluster are: how do I access the API endpoint from kubectl (and other) clients? And how are my Pods accessed or access other systems? We covered public and private endpoints in <a href="B18129_06.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, so for the remainder of this chapter, we will focus on Pod networking. Let’s start with a basic EKS deployment, a private cluster with two EC2 instances in a node group. The cluster has been configured to connect to two private VPC subnets; the node group is also deployed to the <a id="_idIndexMarker311"/>same <span class="No-Break">two subnets.</span></p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B18129_07_05.jpg" alt="Figure 7.5 – EKS networking (basic)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – EKS networking (basic)</p>
			<p>If you look at the VPC in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.5</em>, you can see four interfaces (ENIs) – one for each of the worker nodes and two (typically) for the EKS cluster – along with a private hosted zone that maps the server’s name to those two cluster ENIs. There are also two security groups, one for the worker nodes and one for EKS control plane/APIs. Currently, this is all default AWS platform behavior. Each of the ENIs has been assigned an IP address from the subnet it is attached to. The security groups will reference each other and allow access between the worker nodes <span class="No-Break">and API.</span></p>
			<p>EKS is deployed with the AWS VPC CNI as the default CNI for the cluster. Other CNIs can be used, some of which are described in <a href="B18129_09.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Advanced Networking with EKS</em>. The <strong class="bold">vpc-cni </strong>works <a id="_idIndexMarker312"/>in conjunction with the kubelet agent to request and map an IP address from the VPC to the ENI used by the host and then assign it to the Pod. The number of EC2 ENIs and therefore the number of IP addresses that can be assigned to Pods is limited per EC2 <a id="_idIndexMarker313"/>instance type. For example, a <strong class="bold">m4.4xlarge</strong> node can have up to 8 ENIs, and each ENI can have up to 30 IP addresses, which means you can theoretically support up to 120 addresses per worker node (as we’ll see later, there are some limits <span class="No-Break">to this).</span></p>
			<p>The advantage of this approach is <a id="_idIndexMarker314"/>that the Pod is a first-class citizen in an AWS VPC. There is no difference between the Pod and an EC2 instance; Pod networking behaves exactly as described in this chapter. Another benefit is when traffic leaves the node: traffic can be routed to and controlled through the same AWS network gateways and controls, used by all the other services <span class="No-Break">in AWS.</span></p>
			<p>The disadvantage to this approach is that the EKS cluster, given the ephemeral nature of Pods/containers, can quickly <em class="italic">eat</em> all your available subnet addresses, preventing you from deploying new Pods and/or other AWS services such as databases (RDS). This is particularly problematic if you have small VPC or subnet IP (CIDR) ranges. There are several approaches to mitigate <span class="No-Break">this issue.</span></p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor113"/>Non-routable secondary addresses</h2>
			<p>The concept of <em class="italic">non-routable</em> is to use an <a id="_idIndexMarker315"/>existing range used on premises, or ideally one of the new non-RFC1918 ranges that is not routed on premises for AWS for Pod addresses, allowing a large range to be used. This is shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B18129_07_06.jpg" alt="Figure 7.6 – Non-routable Pod networking"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Non-routable Pod networking</p>
			<p>In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.6</em>, two different<a id="_idIndexMarker316"/> IP zones or routing domains are shown. In <strong class="bold">Routing Domain 1</strong>, the VPC itself, all IP ranges are routable, so the primary range, <strong class="source-inline">10.1.0.0/16</strong>, and the secondary range, <strong class="source-inline">100.64.0.0/10</strong>, can both communicate with the enterprise network <span class="No-Break">on </span><span class="No-Break"><strong class="source-inline">10.0.0.0/8</strong></span><span class="No-Break">.</span></p>
			<p>In <strong class="bold">Routing Domain 2</strong>, the secondary ranges in the VPC subnets using the <strong class="source-inline">100.64.0.0/10</strong> range are private and not routable. They use a NATGW, which means that all outbound traffic undergoes NAT based on the source address as it leaves the <strong class="source-inline">100.64.0.0</strong> subnets, so these IP addresses are never seen outside <span class="No-Break">the VPC.</span></p>
			<p>Any Pods assigned an address from the <strong class="source-inline">100.64.x.x</strong> range (<strong class="bold">Routing Domain 2</strong>) are not reachable from the enterprise network and the TGW doesn’t advertise <span class="No-Break">ten routes.</span></p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor114"/>Prefix addressing</h2>
			<p>The default behavior with EC2 worker <a id="_idIndexMarker317"/>nodes involves allocating the number of addresses available to assign to Pods based on the number of IP addresses assigned to ENIs as well as the number of network interfaces attached to your Amazon EC2 node. For example, the <strong class="bold">m5.large</strong> node can have up to 3 ENIs, and each ENI can have up to 10 IP addresses, so with some limits, it can support 29 Pods based on the <span class="No-Break">following calculation:</span></p>
			<p><em class="italic">3 ENIs * (10 IP addresses -1) + 2 (AWS CNI and kube-proxy Pods per node) = 29 Pods </em><span class="No-Break"><em class="italic">per node</em></span></p>
			<p>Version 1.9.0 or later of the Amazon VPC CNI supports <em class="italic">prefix assignment mode</em>, enabling you to run more Pods per node on <strong class="bold">AWS Nitro-based EC2</strong> instance types. This is done by assigning <strong class="source-inline">/28</strong> IPv4 address prefixes to each of the host ENIs as long as you have enough space in your VPC <span class="No-Break">CIDR range:</span></p>
			<p><em class="italic">3 ENIs * (9 prefixes per ENI * 16 IPs per prefix) + 2 = 434 Pods </em><span class="No-Break"><em class="italic">per node</em></span></p>
			<p>However, please note that the Kubernetes scalability guide recommends a maximum number of 110 Pods per node, and in most cases this will be the maximum enforced by the CNI. Prefix addressing can be used in conjunction with non-routable addresses as it will only work if the VPC CIDR is able to allocate contiguous <strong class="source-inline">/28</strong> subnets from the <span class="No-Break">VPC CIDR.</span></p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor115"/>IPv6</h2>
			<p>Another option is to<a id="_idIndexMarker318"/> use IPv6 instead<a id="_idIndexMarker319"/> of IPv4. A full discussion of the differences between IPv6 and IPv4 is out of scope here, but in a VPC, if you enable IPv6, you automatically get a public <strong class="source-inline">/56</strong> IPv6 CIDR block and each subnet is allocated a <strong class="source-inline">/64</strong> range. This provides 2^64 (approximately 18 quintillion) IPv6 addresses per subnet, so you will never exhaust the IP range. If the cluster is configured with IPv6, each Pod is assigned a native IPv6 address, which is used for Pod-to-Pod communication and an IPv6 IGW (egress only) is used for IPv6 <span class="No-Break">internet access.</span></p>
			<p>As most environments will support a mix of IPv6 and IPv4, EKS implements a <strong class="bold">host-local CNI</strong> plugin<a id="_idIndexMarker320"/> that is paired with<a id="_idIndexMarker321"/> the <strong class="bold">VPC CNI</strong>, which supports Pods with only an IPv6 address connecting to IPv4 endpoints outside the cluster (egress only). IPv6 definitively solves IP allocation issues but introduces more complexity as you need to manage IPv4 NAT and needs to be considered carefully. IPv6 is discussed in more detail in described in <a href="B18129_09.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Advanced Networking </em><span class="No-Break"><em class="italic">with EKS</em></span><span class="No-Break">.</span></p>
			<p>In this section, we’ve reviewed at a high level how native K8s networking works and how EKS/VPC networking is different. In the next section, we will review in detail how to configure and manage <span class="No-Break">EKS networking.</span></p>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor116"/>Configuring EKS networking using the VPC CNI</h1>
			<p>As discussed previously, the AWS VPC CNI is<a id="_idIndexMarker322"/> installed by default, but you may need to upgrade the CNI to use prefix assignment mode, for example, or change a configuration parameter. The following sections will take you through configuration steps for <span class="No-Break">common tasks.</span></p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor117"/>Managing the CNI plugin</h2>
			<p>The simplest way to carry out an <a id="_idIndexMarker323"/>upgrade of the CNI for a new cluster is to apply the new Kubernetes manifest. The following code snippet will install version v1.9.1 onto your cluster and change the version as desired. Be aware, however, that downgrading the CNI version can be very tricky and, in some cases, will <span class="No-Break">not work!</span></p>
			<p>In a script or CI/CD pipeline, it’s often a good idea to be able to export the version of the currently running CNI (as long as it is deployed). The following code snippet will allow you to <span class="No-Break">do that:</span></p>
			<pre class="console">
$ export CNI_VER=$(kubectl describe daemonset aws-node --namespace kube-system | grep Image | cut -d "/" -f 2 | sed -e 's/amazon-k8s-cni-init:\(.*\)-eksbuild.1/\1/')
$ echo $CNI_VER
v1.11.3 amazon-k8s-cni:v1.11.3-eksbuild.1</pre>
			<p>We can now deploy the CNI using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/v1.9.1/config/v1.9/aws-k8s-cni.yaml</pre>
			<p>To enable prefix assignment in the CNI configuration, you can use the following command (this will work for any of the CNI <span class="No-Break">configuration parameters):</span></p>
			<pre class="console">
$ kubectl set env daemonset aws-node                            -n kube-system ENABLE_PREFIX_DELEGATION=true</pre>
			<p>The EKS cluster also supports the use of add-ons, which allow you to configure, deploy, and update the operational software, or provide key functionality to support your Kubernetes applications such as the VPC CNI. Add-ons are the preferred way to manage your cluster after the initial build and when you have running workloads. The easiest way to create an add-on is to use the <strong class="source-inline">eksctl</strong> tool, <span class="No-Break">as follows:</span></p>
			<pre class="console">
$ eksctl create addon --name vpc-cni --version $CNI_VER --cluster $CLUSTERNAME –force</pre>
			<p>This will create an<a id="_idIndexMarker324"/> add-on (visible in the AWS console). You can see the managed fields if you run the <strong class="source-inline">kubectl get</strong> command <span class="No-Break">as follows:</span></p>
			<pre class="console">
$ kubectl get daemonset/aws-node --namespace kube-system --show-managed-fields -o yaml</pre>
			<p>You should be able to see the fields managed by the EKS control plane in the YAML, that is, the output under the <strong class="source-inline">managedFields</strong> key, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
..
managedFields:
  - apiVersion: apps/v1
    fieldsType: FieldsV1
    fieldsV1:</pre>
			<p>A simpler way to look at the plugin is to use the <span class="No-Break"><strong class="source-inline">eksctl</strong></span><span class="No-Break"> command:</span></p>
			<pre class="console">
$ eksctl get addons --cluster $CLUSTERNAME --region $AWS_REGION</pre>
			<p>This will output something similar to the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
vpc-cni v1.9.1-eksbuild.1       ACTIVE  0       arn:aws:iam::119991111:role/eksctl-mycluster-addon-vpc-cni-Role1-4454        v1.10.2-eksbuild.1,v1.10.1-eksbuild.1,v1.9.3-eksbuild.1</pre>
			<p>This tells us <a id="_idIndexMarker325"/>there are updates available: v1.10.2, v1.10.1, and v1.9.3. So, if we want to upgrade the CNI, we issue the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ eksctl update addon --name vpc-cni --version 1.9.3 --cluster $CLUSTERNAME --region $AWS_REGION --force</pre>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor118"/>Disabling CNI source NAT</h2>
			<p>When Pod network traffic is<a id="_idIndexMarker326"/> destined for an IPv4 address outside of the VPC, by default, <strong class="bold">vpc-cni</strong> translates the address of each Pod to the IP address of the EC2 node that the Pod is running on based on the Pod’s source address (SNAT). This behavior is controlled by the <strong class="source-inline">AWS_VPC_K8S_CNI_EXTERNALSNAT</strong> variable, which is set to <strong class="bold">false</strong> <span class="No-Break">by default.</span></p>
			<p>If you want to use an external NAT device such as the AWS NATGW, you need to disable this behavior using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl set env daemonset aws-node -n kube-system AWS_VPC_K8S_CNI_EXTERNALSNAT=true</pre>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor119"/>Configuring custom networking</h2>
			<p>When Pods are created, their <a id="_idIndexMarker327"/>ENI will use the security groups and subnet of the node’s primary network interface. Custom networking allows the use of a different security group or subnet within the same VPC, and we’ve already described a use case (non-routable secondary addresses) that requires this configuration. To enable custom networking, you first need to have configured the required security groups and subnets in your VPC. Then you can run the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl set env daemonset aws-node -n kube-system AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true</pre>
			<p>You will need to create an <strong class="source-inline">ENIConfig</strong> file that defines the required subnets and security groups; an example is shown next. Note that the name is set to the AZ the subnet is in; this is a best practice and allows EKS to automatically assign the right subnet based on the node/AZ combination a Pod is <span class="No-Break">deployed to:</span></p>
			<pre class="source-code">
apiVersion: crd.k8s.amazonaws.com/v1alpha1
kind: ENIConfig
metadata:
  name: eu-central-1a
spec:
  securityGroups:
    - sg-67346437643864389
  subnet: subnet-7847489798437</pre>
			<p>This configuration is applied using the <strong class="source-inline">kubectl apply -f eu-central-1a.yaml</strong> command (assuming you have given the file the same name as the resource in the <strong class="source-inline">metadata</strong> section <a id="_idIndexMarker328"/>of the file). You can then apply the following command to automatically map to the right <strong class="bold">ENIConfig</strong> based on the AZ (<span class="No-Break"><strong class="source-inline">topology.kubernetes.io/zone</strong></span><span class="No-Break">) label:</span></p>
			<pre class="console">
$ kubectl set env daemonset aws-node -n kube-system ENI_CONFIG_LABEL_DEF=topology.kubernetes.io/zone</pre>
			<p>Let’s look at some common EKS networking issues and how to <span class="No-Break">troubleshoot them.</span></p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor120"/>Common networking issues</h1>
			<p>Networking is generally <a id="_idIndexMarker329"/>a complex issue, and although K8s defines a standard model, each CNI introduces different issues. We will look at how to solve some of the more common issues associated with the VPC <span class="No-Break">CNI next.</span></p>
			<table id="table001-4" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Issue</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Solution</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>My worker nodes cannot join <span class="No-Break">the cluster.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Check that the worker nodes subnets have IP access to the internet (through an IGW or NATGW) as well as access to the EKS API ENIs. Check the route tables and associated security groups to <span class="No-Break">make sure.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>My Pods cannot be assigned an IP address from <span class="No-Break">the VPC.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Check that the VPC has enough IP addresses free, if not assign a secondary CIDR range. Enable prefix addressing once you have IP addresses or make the EC2 instance size bigger (<span class="No-Break">more ENIs).</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Pods are unable to resolve K8S <span class="No-Break">DNS names.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Ensure all worker node subnets do not have any security groups or network ACLS that block outbound or inbound UDP port <strong class="source-inline">53</strong> and ensure your VPC has <strong class="source-inline">enableDNSHostnames</strong> and <strong class="source-inline">enableDNSSupport</strong> set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">true</strong></span><span class="No-Break">.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>AWS load balancers cannot <span class="No-Break">be deployed.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Ensure the worker node subnets are tagged with either <strong class="source-inline">kubernetes.io/role/elb</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">kubernetes.io/role/internal-elb</strong></span><span class="No-Break">.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>In this section, we have looked <a id="_idIndexMarker330"/>at the detailed commands needed to configure and manage the VPC CNI. We’ll now revisit the key learning points from <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor121"/>Summary</h1>
			<p>In this chapter, we explored the basic concept of networking and the network model in native Kubernetes and how EKS differs. We described how EKS comes configured with the AWS VPC CNI, which integrates with the AWS VPC to assign ENIs and IP addresses to Pods from <span class="No-Break">the VPC.</span></p>
			<p>We also learned that Pods in EKS are native VPC citizens and traffic can use VPC network devices such as Internet Gateway, Transit Gateway, and NAT Gateway, and can be controlled using VPC network controls such as SGs and/or NACLs. However, this can come with some challenges such as VPC IP exhaustion. We discussed a few ways to handle IP exhaustion, including non-routable subnets, prefix addressing, <span class="No-Break">and IPv6.</span></p>
			<p>Finally, we talked about performing common tasks such as managing and upgrading the CNI, disabling CNI source NAT so you can use external NAT devices such as the AWS NATGW, and configuring custom networking so Pods can use other SGs or subnets to the main worker node to help with security or <span class="No-Break">IP exhaustion.</span></p>
			<p>In the next chapter, we will discuss EKS managed node groups, what they are, and how they are configured <span class="No-Break">and managed.</span></p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor122"/>Further reading</h1>
			<ul>
				<li>AWS VPC CNI <span class="No-Break">repository: </span><a href="https://github.com/aws/amazon-vpc-cni-k8s"><span class="No-Break">https://github.com/aws/amazon-vpc-cni-k8s</span></a></li>
				<li>What is an EC2 <span class="No-Break">ENI?: </span><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html"><span class="No-Break">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html</span></a></li>
				<li>Overview of EKS and <span class="No-Break">IPv6: </span><a href="https://aws.amazon.com/blogs/containers/amazon-eks-launches-ipv6-support/"><span class="No-Break">https://aws.amazon.com/blogs/containers/amazon-eks-launches-ipv6-support/</span></a></li>
				<li>Supported CNIs on <span class="No-Break">EKS: </span><a href="https://docs.aws.amazon.com/eks/latest/userguide/alternate-cni-plugins.html"><span class="No-Break">https://docs.aws.amazon.com/eks/latest/userguide/alternate-cni-plugins.html</span></a></li>
				<li>Private NAT <span class="No-Break">Gateways: </span><a href="https://aws.amazon.com/about-aws/whats-new/2021/06/aws-removes-nat-gateways-dependence-on-internet-gateway-for-private-communications/"><span class="No-Break">https://aws.amazon.com/about-aws/whats-new/2021/06/aws-removes-nat-gateways-dependence-on-internet-gateway-for-private-communications/</span></a></li>
				<li>Using Transit <span class="No-Break">Gateway: </span><a href="https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway.html"><span class="No-Break">https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway.html</span></a></li>
				<li>EC2 Max Pods Details by instance <span class="No-Break">type: </span><a href="https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt"><span class="No-Break">https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt</span></a></li>
				<li>Kubernetes scaling <span class="No-Break">limits: </span><a href="https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md"><span class="No-Break">https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md</span></a></li>
				<li>Overview of EKS <span class="No-Break">add-ons: </span><a href="https://aws.amazon.com/blogs/containers/introducing-amazon-eks-add-ons/"><span class="No-Break">https://aws.amazon.com/blogs/containers/introducing-amazon-eks-add-ons/</span></a></li>
			</ul>
		</div>
	</body></html>