<html><head></head><body>
		<div id="_idContainer034">
			<h1 id="_idParaDest-127"><em class="italic"><a id="_idTextAnchor126"/>Chapter 8</em>: Preparing for Ongoing Maintenance of Your Operator</h1>
			<p>In this book, we have shown the steps for creating a new <strong class="bold">Kubernetes</strong> Operator. We've covered the full range of processes from conception to design, to coding, deployment, and finally, release. But, very few software projects end their life cycle after the initial release, Operators included. In fact, for many Operators, the majority of work will eventually be done after release on a long enough timescale. So, it's valuable to prepare for the future maintenance of your Operator by understanding the expectations of your users and the Operator community at large.</p>
			<p>As a Kubernetes-based project, it can be very helpful to rely on the established conventions from Kubernetes and its subprojects for your own ongoing development. While you are free to form your own guidelines for future releases, it is likely that your Operator will depend on at least some library or aspect of the core Kubernetes platform simply by virtue of being built for Kubernetes. For that reason, knowing the policies in place for Kubernetes can help align your own development practices in preparation for changes in the upstream platform, which you will almost certainly inherit and be forced to react to. That's why this chapter will focus on those conventions throughout the following sections:</p>
			<ul>
				<li>Releasing new versions of your Operator</li>
				<li>Planning for deprecation and backward compatibility</li>
				<li>Complying with Kubernetes standards for changes</li>
				<li>Aligning with the Kubernetes release timeline</li>
				<li>Working with the Kubernetes community</li>
			</ul>
			<p>The procedures established by the Kubernetes community provide an excellent template for your own development practices and a familiar set of policies for your users. There is, of course, no requirement for any Operator to strictly follow these guidelines, but it is the goal of this chapter to explain them in a way that provides the relevant precedent for your project.</p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor127"/>Technical requirements</h1>
			<p>For this chapter, the only technical work will be done in the <em class="italic">Releasing new versions of your Operator</em> section, in which we will build on the existing nginx Operator code from earlier chapters to add new code and run that code in a Kubernetes cluster. As such, the requirements for this chapter include the following:</p>
			<ul>
				<li>The <strong class="source-inline">operator-sdk</strong> binary</li>
				<li><strong class="source-inline">Go</strong> 1.16+</li>
				<li>Docker </li>
				<li>Access to a running Kubernetes cluster</li>
			</ul>
			<p>The Code in Action video for this chapter can be viewed at: <a href="https://bit.ly/3aiaokl">https://bit.ly/3aiaokl</a></p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor128"/>Releasing new versions of your Operator</h1>
			<p>Now <a id="_idIndexMarker463"/>that your Operator is published, the fun has <a id="_idIndexMarker464"/>just begun. It's now time to start thinking about your next release! As with any software project, your Operator will evolve over time by introducing new features and adapting to changes in upstream Kubernetes. There are tomes of literature written on releasing software with recommendations on when and how to publish updates to a software product. Much of that<a id="_idIndexMarker465"/> information<a id="_idIndexMarker466"/> is out of the scope of this book. Instead, we will explain the technical steps required to create and publish a new version of your Operator with the <strong class="bold">Operator SDK</strong>, <strong class="bold">Operator Lifecycle Manager</strong> (<strong class="bold">OLM</strong>), and <strong class="bold">OperatorHub</strong> in <a id="_idIndexMarker467"/>mind. From there, methods and timing for your release are entirely up to your organization (though you may want to learn more about how other Kubernetes projects are released later in this chapter in the <em class="italic">Aligning with the Kubernetes release timeline</em> section).</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor129"/>Adding an API version to your Operator</h2>
			<p>While<a id="_idIndexMarker468"/> there are many factors that may influence your decision<a id="_idIndexMarker469"/> to release a new version of your Operator (such as bug fixes or simply following a regular release schedule), one type of change that is common among Operators is updating the Operator's config API. Recall that this is the API translated into the Operator's <strong class="bold">CustomResourceDefinition</strong> (<strong class="bold">CRD</strong>). So, in some cases, it <a id="_idIndexMarker470"/>may be necessary to update the API that is shipped with your Operator to indicate changes to users (see the <em class="italic">Complying with Kubernetes standards for changes</em> section).</p>
			<p>To do this, we need to create a new API version and include that version in the Operator's CRD (for a more in-depth look at what this means from a technical standpoint, see the Kubernetes documentation on CRD versioning<a id="_idIndexMarker471"/> for information on the details of how this works: <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/">https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/</a>).</p>
			<p>In <a href="B18147_04_ePub.xhtml#_idTextAnchor066"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing an Operator with the Operator SDK</em>, we initialized the first version of our Operator's API with the following command:</p>
			<p class="source-code">operator-sdk create api --group operator --version v1alpha1 --kind NginxOperator --resource --controller</p>
			<p>This created the <strong class="source-inline">NginxOperator</strong> API type as the <strong class="source-inline">v1alpha1</strong> version. We then filled out the API in <strong class="source-inline">api/v1alpha1/nginxoperator_types.go</strong> and generated the corresponding CRD, which provides the interface for using the Operator once it was deployed in a cluster.</p>
			<p>If some incompatible changes needed to be made to this API that required a new version, that version could be generated similarly. Say, for example, that we wanted to allow the nginx Deployment managed by our Operator to expose multiple ports, such as one for HTTP and another for HTTPS requests. We could do this by changing the <strong class="source-inline">port</strong> field in the existing nginx Operator's CRD to a <strong class="source-inline">ports</strong> field that defines a list of <strong class="source-inline">v1.ContainerPorts</strong> (this is a native Kubernetes API type that allows for naming multiple ports for a container). This new type exposes additional information, such as <strong class="source-inline">Name</strong> and <strong class="source-inline">HostPort</strong>, but it also includes the same <strong class="source-inline">int32</strong> value for defining a singular <strong class="source-inline">ContainerPort</strong> that the original <strong class="source-inline">port</strong> field used. Let's take the following line from <strong class="source-inline">controllers/nginxoperator_controller.go</strong> as an example:</p>
			<pre class="source-code">func (r *NginxOperatorReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {</pre>
			<pre class="source-code">  if operatorCR.Spec.Port != nil {</pre>
			<pre class="source-code">    deployment.Spec.Template.Spec.Containers[0].Ports[0].ContainerPort = *operatorCR.Spec.Port</pre>
			<pre class="source-code">  }</pre>
			<p>This <a id="_idIndexMarker472"/>could be simplified to just the following:</p>
			<pre class="source-code">func (r *NginxOperatorReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {</pre>
			<pre class="source-code">  if len(operatorCR.Spec.Ports) &gt; 0 {</pre>
			<pre class="source-code">    deployment.Spec.Template.Spec.Containers[0].Ports = operatorCR.Spec.Ports</pre>
			<pre class="source-code">  }</pre>
			<p>To show <a id="_idIndexMarker473"/>what this means for the Operator types, we will take the existing <strong class="source-inline">NginxOperatorSpec</strong> type in <strong class="source-inline">v1alpha1</strong>:</p>
			<pre class="source-code">// NginxOperatorSpec defines the desired state of NginxOperator</pre>
			<pre class="source-code">type NginxOperatorSpec struct {</pre>
			<pre class="source-code">   // Port is the port number to expose on the Nginx Pod</pre>
			<pre class="source-code">   Port *int32 `json:"port,omitempty"`</pre>
			<pre class="source-code">   // Replicas is the number of deployment replicas to scale</pre>
			<pre class="source-code">   Replicas *int32 `json:"replicas,omitempty"`</pre>
			<pre class="source-code">   // ForceRedploy is any string, modifying this field instructs</pre>
			<pre class="source-code">   // the Operator to redeploy the Operand</pre>
			<pre class="source-code">   ForceRedploy string `json:"forceRedploy,omitempty"`</pre>
			<pre class="source-code">}</pre>
			<p>And now, we<a id="_idIndexMarker474"/> will change it to a new <strong class="source-inline">NginxOperatorSpec</strong> type in <strong class="source-inline">v1alpha2</strong> that<a id="_idIndexMarker475"/> looks like the following:</p>
			<pre class="source-code">// NginxOperatorSpec defines the desired state of NginxOperator</pre>
			<pre class="source-code">type NginxOperatorSpec struct {</pre>
			<pre class="source-code">   // Ports defines the ContainerPorts exposed on the Nginx Pod</pre>
			<pre class="source-code"><strong class="bold">   Ports []v1.ContainerPort `json:"ports,omitempty""`</strong></pre>
			<pre class="source-code">   // Replicas is the number of deployment replicas to scale</pre>
			<pre class="source-code">   Replicas *int32 `json:"replicas,omitempty"`</pre>
			<pre class="source-code">   // ForceRedploy is any string, modifying this field instructs</pre>
			<pre class="source-code">   // the Operator to redeploy the Operand</pre>
			<pre class="source-code">   ForceRedploy string `json:"forceRedploy,omitempty"`</pre>
			<pre class="source-code">}</pre>
			<p>In order to preserve functionality for our users, it's important to introduce the new version in a way that ensures the Operator supports both versions for as long as required by the deprecation policy.</p>
			<h3>Generating the new API directory</h3>
			<p>The <a id="_idIndexMarker476"/>first step is to generate the new API files. The new API version is generated as a scaffold with the <strong class="source-inline">operator-sdk</strong> command, just like when we generated <strong class="source-inline">v1alpha1</strong>:</p>
			<p class="source-code">$ operator-sdk create api --group operator --version v1alpha2 --kind NginxOperator --resource</p>
			<p class="source-code">Create Controller [y/n]</p>
			<p class="source-code">n</p>
			<p class="source-code">Writing kustomize manifests for you to edit...</p>
			<p class="source-code">Writing scaffold for you to edit...</p>
			<p class="source-code">api/v1alpha2/nginxoperator_types.go</p>
			<p class="source-code">Update dependencies:</p>
			<p class="source-code">$ go mod tidy</p>
			<p class="source-code">Running make:</p>
			<p class="source-code">$ make generate</p>
			<p class="source-code">/Users/mdame/nginx-operator/bin/controller-genobject:headerFile="hack/boilerplate.go.txt" paths="./..."</p>
			<p class="source-code">Next: implement your new API and generate the manifests (e.g. CRDs,CRs) with:</p>
			<p class="source-code">$ make manifests</p>
			<p>Note that this time, the <strong class="source-inline">--controller</strong> flag was omitted (and we chose <strong class="source-inline">n</strong> for <strong class="source-inline">Create Controller [y/n]</strong>) because the controller for this Operator already exists (<strong class="source-inline">controllers/nginxoperator_controller.go</strong>), so we don't need to generate another one. </p>
			<p>Instead, the existing controller will need to be manually updated to remove references to <strong class="source-inline">v1alpha1</strong> and replace them with <strong class="source-inline">v1alpha2</strong>. This step can also be automated with tools such as <strong class="source-inline">sed</strong>, but be sure to carefully review any changes whenever automating code updates like this.</p>
			<p>When <a id="_idIndexMarker477"/>the version is generated, it will create a new <strong class="source-inline">api/v1alpha2</strong> folder, which also contains an <strong class="source-inline">nginxoperator_types.go</strong> file. Copy the existing type definitions from <strong class="source-inline">api/v1alpha1/nginxoperator_types.go</strong> into this file and change the <strong class="source-inline">port</strong> field into <strong class="source-inline">ports</strong>, as shown in the preceding code. The new file should look as follows (note the highlighted change to <strong class="source-inline">Ports</strong>):</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">api/v1alpha2/nginxoperator_types.go:</p>
			<pre class="source-code">package v1alpha2</pre>
			<pre class="source-code">import (</pre>
			<pre class="source-code">   v1 "k8s.io/api/core/v1"</pre>
			<pre class="source-code">   metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">const (</pre>
			<pre class="source-code">   ReasonCRNotAvailable          = "OperatorResourceNotAvailable"</pre>
			<pre class="source-code">   ReasonDeploymentNotAvailable  = "OperandDeploymentNotAvailable"</pre>
			<pre class="source-code">   ReasonOperandDeploymentFailed = "OperandDeploymentFailed"</pre>
			<pre class="source-code">   ReasonSucceeded               = "OperatorSucceeded"</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">// NginxOperatorSpec defines the desired state of NginxOperator</pre>
			<pre class="source-code">type NginxOperatorSpec struct {</pre>
			<pre class="source-code">   // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster</pre>
			<pre class="source-code">   // Important: Run "make" to regenerate code after modifying this file</pre>
			<pre class="source-code"><strong class="bold">   // Ports defines the ContainerPorts exposed on the Nginx Pod</strong></pre>
			<pre class="source-code"><strong class="bold">   Ports []v1.ContainerPort `json:"ports,omitempty""`</strong></pre>
			<pre class="source-code">   // Replicas is the number of deployment replicas to scale</pre>
			<pre class="source-code">   Replicas *int32 `json:"replicas,omitempty"`</pre>
			<pre class="source-code">   // ForceRedploy is any string, modifying this field instructs</pre>
			<pre class="source-code">   // the Operator to redeploy the Operand</pre>
			<pre class="source-code">   ForceRedploy string `json:"forceRedploy,omitempty"`</pre>
			<pre class="source-code">}</pre>
			<pre class="source-code">// NginxOperatorStatus defines the observed state of NginxOperator</pre>
			<pre class="source-code">type NginxOperatorStatus struct {</pre>
			<pre class="source-code">   // Conditions is the list of the most recent status condition updates</pre>
			<pre class="source-code">   Conditions []metav1.Condition `json:"conditions"`</pre>
			<pre class="source-code">}</pre>
			<pre class="source-code">//+kubebuilder:object:root=true</pre>
			<pre class="source-code">//+kubebuilder:subresource:status</pre>
			<pre class="source-code">//+kubebuilder:storageversion</pre>
			<pre class="source-code">// NginxOperator is the Schema for the nginxoperators API</pre>
			<pre class="source-code">type NginxOperator struct {</pre>
			<pre class="source-code">   metav1.TypeMeta   `json:",inline"`</pre>
			<pre class="source-code">   metav1.ObjectMeta `json:"metadata,omitempty"`</pre>
			<pre class="source-code">   Spec   NginxOperatorSpec   `json:"spec,omitempty"`</pre>
			<pre class="source-code">   Status NginxOperatorStatus `json:"status,omitempty"`</pre>
			<pre class="source-code">}</pre>
			<pre class="source-code">//+kubebuilder:object:root=true</pre>
			<pre class="source-code">// NginxOperatorList contains a list of NginxOperator</pre>
			<pre class="source-code">type NginxOperatorList struct {</pre>
			<pre class="source-code">   metav1.TypeMeta `json:",inline"`</pre>
			<pre class="source-code">   metav1.ListMeta `json:"metadata,omitempty"`</pre>
			<pre class="source-code">   Items           []NginxOperator `json:"items"`</pre>
			<pre class="source-code">}</pre>
			<pre class="source-code">func init() {</pre>
			<pre class="source-code">   SchemeBuilder.Register(&amp;NginxOperator{}, &amp;NginxOperatorList{})</pre>
			<pre class="source-code">}</pre>
			<h3>Updating the Operator's CRD</h3>
			<p>Next, the <a id="_idIndexMarker478"/>Operator's CRD needs to be <a id="_idIndexMarker479"/>updated to include definitions for both <strong class="source-inline">v1alpha1</strong> and <strong class="source-inline">v1alpha2</strong>. First, one version needs to be defined as <a id="_idIndexMarker480"/>the <strong class="bold">storage version</strong>. This is the version representation in which the object is persisted in <strong class="source-inline">etcd</strong>. When the Operator only had one version, it wasn't necessary to specify this (that was the only version available to be stored). Now, however, the API server needs to know how to store the object. This is done by adding another <strong class="source-inline">kubebuilder</strong> marker (<strong class="source-inline">//+kubebuilder:storageversion</strong>) to the <strong class="source-inline">NginxOperator</strong> struct:</p>
			<pre class="source-code">//+kubebuilder:object:root=true</pre>
			<pre class="source-code">//+kubebuilder:subresource:status</pre>
			<pre class="source-code"><strong class="bold">//+kubebuilder:storageversion</strong></pre>
			<pre class="source-code">// NginxOperator is the Schema for the nginxoperators API</pre>
			<pre class="source-code">type NginxOperator struct {</pre>
			<pre class="source-code">   metav1.TypeMeta   `json:",inline"`</pre>
			<pre class="source-code">   metav1.ObjectMeta `json:"metadata,omitempty"`</pre>
			<pre class="source-code">   Spec   NginxOperatorSpec   `json:"spec,omitempty"`</pre>
			<pre class="source-code">   Status NginxOperatorStatus `json:"status,omitempty"`</pre>
			<pre class="source-code">}</pre>
			<p>This instructs the CRD generator to label <strong class="source-inline">v1alpha2</strong> as the storage version. Now, running <strong class="source-inline">make manifests</strong> will generate new changes to the CRD:</p>
			<p class="source-code">$ make manifests</p>
			<p class="source-code">$ git status</p>
			<p class="source-code">On branch master</p>
			<p class="source-code">Changes not staged for commit:</p>
			<p class="source-code">      modified:   config/crd/bases/operator.example.com_nginxoperators.yaml</p>
			<p>Now, the <a id="_idIndexMarker481"/>Operator's CRD should include <a id="_idIndexMarker482"/>a new <strong class="source-inline">v1alpha2</strong> specification definition under <strong class="source-inline">versions</strong>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">config/crd/bases/operator.example.com_nginxoperators.yaml:</p>
			<pre class="source-code">apiVersion: apiextensions.k8s.io/v1</pre>
			<pre class="source-code">kind: CustomResourceDefinition</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  annotations:</pre>
			<pre class="source-code">    controller-gen.kubebuilder.io/version: v0.7.0</pre>
			<pre class="source-code">  creationTimestamp: null</pre>
			<pre class="source-code">  name: nginxoperators.operator.example.com</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  group: operator.example.com</pre>
			<pre class="source-code">  names:</pre>
			<pre class="source-code">    kind: NginxOperator</pre>
			<pre class="source-code">    listKind: NginxOperatorList</pre>
			<pre class="source-code">    plural: nginxoperators</pre>
			<pre class="source-code">    singular: nginxoperator</pre>
			<pre class="source-code">  scope: Namespaced</pre>
			<pre class="source-code"><strong class="bold">  versions:</strong></pre>
			<pre class="source-code"><strong class="bold">  - name: v1alpha1</strong></pre>
			<pre class="source-code">    schema:</pre>
			<pre class="source-code">      openAPIV3Schema:</pre>
			<pre class="source-code">      ...</pre>
			<pre class="source-code">    served: true</pre>
			<pre class="source-code">    storage: false</pre>
			<pre class="source-code">    subresources:</pre>
			<pre class="source-code">      status: {}</pre>
			<pre class="source-code"><strong class="bold">  - name: v1alpha2</strong></pre>
			<pre class="source-code">    schema:</pre>
			<pre class="source-code">      openAPIV3Schema:</pre>
			<pre class="source-code">      ...</pre>
			<pre class="source-code">    served: true</pre>
			<pre class="source-code"><strong class="bold">    storage: true</strong></pre>
			<pre class="source-code">    subresources:</pre>
			<pre class="source-code">      status: {}</pre>
			<h3>Implementing API conversions</h3>
			<p>Finally, the<a id="_idIndexMarker483"/> API server needs to know how to convert between these two incompatible versions. Specifically, the <strong class="source-inline">int32</strong> value for the <strong class="source-inline">v1alpha1</strong> port needs to be converted into a <strong class="source-inline">ContainerPort</strong> value in a list of <strong class="source-inline">ports</strong> for <strong class="source-inline">v1alpha2</strong>. For the sake of this example, we will define the behavior as the following:</p>
			<ul>
				<li><strong class="source-inline">v1alpha1</strong> to <strong class="source-inline">v1alpha2</strong>: <strong class="source-inline">int32(port)</strong> becomes <strong class="source-inline">ports[0].ContainerPort</strong>.</li>
				<li><strong class="source-inline">v1alpha2</strong> to <strong class="source-inline">v1alpha1</strong>: <strong class="source-inline">ports[0].ContainerPort</strong> becomes <strong class="source-inline">int32(port)</strong>.</li>
			</ul>
			<p>In other words, if we are handed a list of ports and need to convert to a single port (<strong class="source-inline">v1alpha2</strong> to <strong class="source-inline">v1alpha1</strong>), we will take the first value in the list and use that. In reverse (<strong class="source-inline">v1alpha1</strong> to <strong class="source-inline">v1alpha2</strong>), we will take the single <strong class="source-inline">port</strong> value and make it the first (and only) value in a new list of ports.</p>
			<p>To define these conversion rules, we will implement the <strong class="source-inline">Convertible</strong> and <strong class="source-inline">Hub</strong> interfaces from <strong class="source-inline">sigs.k8s.io/controller-runtime/pkg/conversion</strong>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">sigs.k8s.io/controller-runtime/pkg/conversion/conversion.go:</p>
			<pre class="source-code">package conversion</pre>
			<pre class="source-code">import "k8s.io/apimachinery/pkg/runtime"</pre>
			<pre class="source-code">// Convertible defines capability of a type to convertible i.e. it can be converted to/from a hub type.</pre>
			<pre class="source-code">type <strong class="bold">Convertible</strong> interface {</pre>
			<pre class="source-code">      runtime.Object</pre>
			<pre class="source-code">      ConvertTo(dst Hub) error</pre>
			<pre class="source-code">      ConvertFrom(src Hub) error</pre>
			<pre class="source-code">}</pre>
			<pre class="source-code">// Hub marks that a given type is the hub type for conversion. This means that</pre>
			<pre class="source-code">// all conversions will first convert to the hub type, then convert from the hub</pre>
			<pre class="source-code">// type to the destination type. All types besides the hub type should implement</pre>
			<pre class="source-code">// Convertible.</pre>
			<pre class="source-code">type <strong class="bold">Hub</strong> interface {</pre>
			<pre class="source-code">      runtime.Object</pre>
			<pre class="source-code">      Hub()</pre>
			<pre class="source-code">}</pre>
			<p>These will then<a id="_idIndexMarker484"/> be exposed to the API server via a <strong class="bold">webhook</strong> that creates an endpoint that can be queried by the API server to instruct it on how to perform conversions between different API versions of an object. The steps for creating a webhook with <strong class="source-inline">operator-sdk</strong> are wrappers for kubebuilder commands, so the steps for implementing the conversion webhook in an Operator are the same as for any other controller, as shown in the kubebuilder documentation). This process defines one version as the <strong class="source-inline">Hub</strong> type through which the <strong class="source-inline">Convertible</strong> spoke types are converted.</p>
			<p>To get started, first create a new file, <strong class="source-inline">api/v1alpha2/nginxoperator_conversion.go</strong>, to define <strong class="source-inline">v1alpha2</strong> as the Hub version:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">api/v1alpha2/nginxoperator_conversion.go:</p>
			<pre class="source-code">package v1alpha2</pre>
			<pre class="source-code">// Hub defines v1alpha2 as the hub version</pre>
			<pre class="source-code">func (*NginxOperator) Hub() {}</pre>
			<p>Next, create <a id="_idIndexMarker485"/>another file, <strong class="source-inline">api/v1alpha1/nginxoperator_conversion.go</strong> (note this is in the <strong class="source-inline">v1alpha1</strong> directory). This file will implement the <strong class="source-inline">ConvertTo()</strong> and <strong class="source-inline">ConvertFrom()</strong> functions to translate <strong class="source-inline">v1alpha1</strong> to and from <strong class="source-inline">v1alpha2</strong>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">api/v1alpha1/nginxoperator_conversion.go:</p>
			<pre class="source-code">package v1alpha1</pre>
			<pre class="source-code">import (</pre>
			<pre class="source-code">   "github.com/sample/nginx-operator/api/v1alpha2"</pre>
			<pre class="source-code">   v1 "k8s.io/api/core/v1"</pre>
			<pre class="source-code">   "k8s.io/utils/pointer"</pre>
			<pre class="source-code">   "sigs.k8s.io/controller-runtime/pkg/conversion"</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">// ConvertTo converts v1alpha1 to v1alpha2</pre>
			<pre class="source-code">func (src *NginxOperator) ConvertTo(dst conversion.Hub) error {</pre>
			<pre class="source-code">   return nil</pre>
			<pre class="source-code">}</pre>
			<pre class="source-code">// ConvertFrom converts v1alpha2 to v1alpha1</pre>
			<pre class="source-code">func (dst *NginxOperator) ConvertFrom(src conversion.Hub) error {</pre>
			<pre class="source-code">   return nil</pre>
			<pre class="source-code">}</pre>
			<p>Then, fill in these <a id="_idIndexMarker486"/>functions to do the actual conversions. For the fields such as <strong class="source-inline">Replicas</strong> and <strong class="source-inline">ForceRedeploy</strong>, the conversion is 1:1 (it's also important to copy the <strong class="source-inline">Metadata</strong> and <strong class="source-inline">Status.Conditions</strong> too). But, for <strong class="source-inline">Port</strong>/<strong class="source-inline">Ports</strong>, we need to add the logic defined previously. That makes <strong class="source-inline">ConvertTo()</strong> look like the following:</p>
			<pre class="source-code">// ConvertTo converts v1alpha1 to v1alpha2</pre>
			<pre class="source-code">func (src *NginxOperator) ConvertTo(dst conversion.Hub) error {</pre>
			<pre class="source-code">   objV1alpha2 := dst.(*v1alpha2.NginxOperator)</pre>
			<pre class="source-code">   objV1alpha2.ObjectMeta = src.ObjectMeta</pre>
			<pre class="source-code">   objV1alpha2.Status.Conditions = src.Status.Conditions</pre>
			<pre class="source-code">   if src.Spec.Replicas != nil {</pre>
			<pre class="source-code">      objV1alpha2.Spec.Replicas = src.Spec.Replicas</pre>
			<pre class="source-code">   }</pre>
			<pre class="source-code">   if len(src.Spec.ForceRedploy) &gt; 0 {</pre>
			<pre class="source-code">      objV1alpha2.Spec.ForceRedploy = src.Spec.ForceRedploy</pre>
			<pre class="source-code">   }</pre>
			<pre class="source-code"><strong class="bold">   if src.Spec.Port != nil {</strong></pre>
			<pre class="source-code"><strong class="bold">      objV1alpha2.Spec.Ports = make([]v1.ContainerPort, 0, 1)</strong></pre>
			<pre class="source-code"><strong class="bold">      objV1alpha2.Spec.Ports = append(objV1alpha2.Spec.Ports,</strong></pre>
			<pre class="source-code"><strong class="bold">        v1.ContainerPort{ContainerPort: *src.Spec.Port})</strong></pre>
			<pre class="source-code"><strong class="bold">   }</strong></pre>
			<pre class="source-code">   return nil</pre>
			<pre class="source-code">}</pre>
			<p>And <strong class="source-inline">ConvertFrom()</strong> is <a id="_idIndexMarker487"/>similar, but in reverse:</p>
			<pre class="source-code">// ConvertFrom converts v1alpha2 to v1alpha1</pre>
			<pre class="source-code">func (dst *NginxOperator) ConvertFrom(src conversion.Hub) error {</pre>
			<pre class="source-code">   objV1alpha2 := src.(*v1alpha2.NginxOperator)</pre>
			<pre class="source-code">   dst.ObjectMeta = objV1alpha2.ObjectMeta</pre>
			<pre class="source-code">   dst.Status.Conditions = objV1alpha2.Status.Conditions</pre>
			<pre class="source-code">   if objV1alpha2.Spec.Replicas != nil {</pre>
			<pre class="source-code">      dst.Spec.Replicas = objV1alpha2.Spec.Replicas</pre>
			<pre class="source-code">   }</pre>
			<pre class="source-code">   if len(objV1alpha2.Spec.ForceRedploy) &gt; 0 {</pre>
			<pre class="source-code">      dst.Spec.ForceRedploy = objV1alpha2.Spec.ForceRedploy</pre>
			<pre class="source-code">   }</pre>
			<pre class="source-code"><strong class="bold">   if len(objV1alpha2.Spec.Ports) &gt; 0 {</strong></pre>
			<pre class="source-code"><strong class="bold">      dst.Spec.Port = pointer.Int32(objV1alpha2.Spec.Ports[0].ContainerPort)</strong></pre>
			<pre class="source-code"><strong class="bold">   }</strong></pre>
			<pre class="source-code">   return nil</pre>
			<pre class="source-code">}</pre>
			<p>Now, we can generate <a id="_idIndexMarker488"/>the webhook logic and endpoints by using <strong class="source-inline">operator-sdk create webhook</strong>, with the following command:</p>
			<p class="source-code">$ operator-sdk create webhook --conversion --version v1alpha2 --kind NginxOperator --group operator --force</p>
			<p class="source-code">Writing kustomize manifests for you to edit...</p>
			<p class="source-code">Writing scaffold for you to edit...</p>
			<p class="source-code">api/v1alpha2/nginxoperator_webhook.go</p>
			<p class="source-code">Webhook server has been set up for you.</p>
			<p class="source-code">You need to implement the conversion.Hub and conversion.Convertible interfaces for your CRD types.</p>
			<p class="source-code">Update dependencies:</p>
			<p class="source-code">$ go mod tidy</p>
			<p class="source-code">Running make:</p>
			<p class="source-code">$ make generate</p>
			<p class="source-code">/Users/mdame/nginx-operator/bin/controller-gen object:headerFile="hack/boilerplate.go.txt" paths="./..."</p>
			<p class="source-code">Next: implement your new Webhook and generate the manifests with:</p>
			<p class="source-code">$ make manifests</p>
			<p>You can ignore the message saying <strong class="source-inline">You need to implement the conversion.Hub and conversion.Convertible interfaces for your CRD types</strong> because we already did <a id="_idIndexMarker489"/>that (the generator simply assumes that it will be run before these are implemented). At this point, everything has been implemented, and the only next step is to ensure that the webhook is properly deployed with the Operator when it is installed in a cluster.</p>
			<h3>Updating project manifests to deploy the webhook</h3>
			<p>Just <a id="_idIndexMarker490"/>like how the manifests to enable metrics <a id="_idIndexMarker491"/>resources needed to be uncommented in order to be deployed with the Operator (<a href="B18147_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>, <em class="italic">Developing an Operator – Advanced Functionality</em>), so too do resources related to the webhook.</p>
			<p>To do this, first, modify <strong class="source-inline">config/crd/kustomization.yaml</strong> to uncomment the <strong class="source-inline">patches/webhook_in_nginxoperators.yaml</strong> and <strong class="source-inline">patches/cainject_in_nginxoperators.yaml</strong> lines to include these two patch files in the deployment:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">config/crd/kustomization.yaml:</p>
			<pre class="source-code">resources:</pre>
			<pre class="source-code">- bases/operator.example.com_nginxoperators.yaml</pre>
			<pre class="source-code">#+kubebuilder:scaffold:crdkustomizeresource</pre>
			<pre class="source-code">patchesStrategicMerge:</pre>
			<pre class="source-code"># [WEBHOOK] To enable webhook, uncomment all the sections with [WEBHOOK] prefix.</pre>
			<pre class="source-code"># patches here are for enabling the conversion webhook for each CRD</pre>
			<pre class="source-code"><strong class="bold">- patches/webhook_in_nginxoperators.yaml</strong></pre>
			<pre class="source-code">#+kubebuilder:scaffold:crdkustomizewebhookpatch</pre>
			<pre class="source-code"># [CERTMANAGER] To enable cert-manager, uncomment all the sections with [CERTMANAGER] prefix.                            # patches here are for enabling the CA injection for each CRD</pre>
			<pre class="source-code"><strong class="bold">- patches/cainjection_in_nginxoperators.yaml</strong></pre>
			<pre class="source-code">#+kubebuilder:scaffold:crdkustomizecainjectionpatch </pre>
			<pre class="source-code"># the following config is for teaching kustomize how to do kustomization for CRDs. </pre>
			<pre class="source-code">configurations:</pre>
			<pre class="source-code">- kustomizeconfig.yaml</pre>
			<p>Now, modify <a id="_idIndexMarker492"/>one of those files, <strong class="source-inline">patches/webhook_in_nginxoperators.yaml</strong>, to add the two CRD versions as <strong class="source-inline">conversionReviewVersions</strong> in the<a id="_idIndexMarker493"/> Operator's CRD:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">config/crd/patches/webhook_in_nginxoperators.yaml:</p>
			<pre class="source-code"># The following patch enables a conversion webhook for the CRD</pre>
			<pre class="source-code">apiVersion: apiextensions.k8s.io/v1</pre>
			<pre class="source-code">kind: CustomResourceDefinition</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: nginxoperators.operator.example.com</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  conversion:</pre>
			<pre class="source-code">    strategy: Webhook</pre>
			<pre class="source-code">    webhook:</pre>
			<pre class="source-code">      clientConfig:</pre>
			<pre class="source-code">        service:</pre>
			<pre class="source-code">          namespace: system</pre>
			<pre class="source-code">          name: webhook-service</pre>
			<pre class="source-code">          path: /convert</pre>
			<pre class="source-code">      conversionReviewVersions:</pre>
			<pre class="source-code">       - v1</pre>
			<pre class="source-code"><strong class="bold">      - v1alpha1</strong></pre>
			<pre class="source-code"><strong class="bold">      - v1alpha2</strong></pre>
			<p>Next, make the following changes to <strong class="source-inline">config/default/kustomization.yaml</strong> to uncomment the following lines:</p>
			<ul>
				<li><strong class="source-inline">- ../webhook</strong></li>
				<li><strong class="source-inline">- ../certmanager</strong></li>
				<li><strong class="source-inline">- manager_webhook_patch.yaml</strong></li>
				<li>All of the variables in the <strong class="source-inline">vars</strong> section with the <strong class="source-inline">[CERTMANAGER]</strong> label.</li>
			</ul>
			<p>The final file<a id="_idIndexMarker494"/> will look as follows (uncommented lines<a id="_idIndexMarker495"/> highlighted and some sections omitted for brevity):</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">config/default/kustomization.yaml:</p>
			<pre class="source-code">...</pre>
			<pre class="source-code">bases:</pre>
			<pre class="source-code">- ../crd</pre>
			<pre class="source-code">- ../rbac</pre>
			<pre class="source-code">- ../manager</pre>
			<pre class="source-code"># [WEBHOOK] To enable webhook, uncomment all the sections with [WEBHOOK] prefix including the one in</pre>
			<pre class="source-code"># crd/kustomization.yaml</pre>
			<pre class="source-code"><strong class="bold">- ../webhook</strong></pre>
			<pre class="source-code"># [CERTMANAGER] To enable cert-manager, uncomment all sections with 'CERTMANAGER'. 'WEBHOOK' components are required.</pre>
			<pre class="source-code"><strong class="bold">- ../certmanager</strong></pre>
			<pre class="source-code"># [PROMETHEUS] To enable prometheus monitor, uncomment all sections with 'PROMETHEUS'.</pre>
			<pre class="source-code">- ../prometheus</pre>
			<pre class="source-code">...</pre>
			<pre class="source-code"># [WEBHOOK] To enable webhook, uncomment all the sections with [WEBHOOK] prefix including the one in</pre>
			<pre class="source-code"># crd/kustomization.yaml</pre>
			<pre class="source-code"><strong class="bold">- manager_webhook_patch.yaml</strong></pre>
			<pre class="source-code"># [CERTMANAGER] To enable cert-manager, uncomment all sections with 'CERTMANAGER'.</pre>
			<pre class="source-code"># Uncomment 'CERTMANAGER' sections in crd/kustomization.yaml to enable the CA injection in the admission webhooks.</pre>
			<pre class="source-code"># 'CERTMANAGER' needs to be enabled to use ca injection</pre>
			<pre class="source-code">#- webhookcainjection_patch.yaml</pre>
			<pre class="source-code"># the following config is for teaching kustomize how to do var substitution</pre>
			<pre class="source-code"><strong class="bold">vars:</strong></pre>
			<pre class="source-code"># [CERTMANAGER] To enable cert-manager, uncomment all sections with 'CERTMANAGER' prefix.</pre>
			<pre class="source-code"><strong class="bold">- name: CERTIFICATE_NAMESPACE # namespace of the certificate CR</strong></pre>
			<pre class="source-code"><strong class="bold">  objref:</strong></pre>
			<pre class="source-code"><strong class="bold">    kind: Certificate</strong></pre>
			<pre class="source-code"><strong class="bold">    group: cert-manager.io</strong></pre>
			<pre class="source-code"><strong class="bold">    version: v1</strong></pre>
			<pre class="source-code"><strong class="bold">    name: serving-cert # this name should match the one in certificate.yaml</strong></pre>
			<pre class="source-code"><strong class="bold">  fieldref:</strong></pre>
			<pre class="source-code"><strong class="bold">    fieldpath: metadata.namespace</strong></pre>
			<pre class="source-code"><strong class="bold">- name: CERTIFICATE_NAME</strong></pre>
			<pre class="source-code"><strong class="bold">  objref:</strong></pre>
			<pre class="source-code"><strong class="bold">    kind: Certificate</strong></pre>
			<pre class="source-code"><strong class="bold">    group: cert-manager.io</strong></pre>
			<pre class="source-code"><strong class="bold">    version: v1</strong></pre>
			<pre class="source-code"><strong class="bold">    name: serving-cert # this name should match the one in certificate.yaml</strong></pre>
			<pre class="source-code"><strong class="bold">- name: SERVICE_NAMESPACE # namespace of the service</strong></pre>
			<pre class="source-code"><strong class="bold">  objref:</strong></pre>
			<pre class="source-code"><strong class="bold">    kind: Service</strong></pre>
			<pre class="source-code"><strong class="bold">    version: v1</strong></pre>
			<pre class="source-code"><strong class="bold">    name: webhook-service</strong></pre>
			<pre class="source-code"><strong class="bold">  fieldref:</strong></pre>
			<pre class="source-code"><strong class="bold">    fieldpath: metadata.namespace</strong></pre>
			<pre class="source-code"><strong class="bold">- name: SERVICE_NAME</strong></pre>
			<pre class="source-code"><strong class="bold">  objref:</strong></pre>
			<pre class="source-code"><strong class="bold">    kind: Service</strong></pre>
			<pre class="source-code"><strong class="bold">    version: v1</strong></pre>
			<pre class="source-code"><strong class="bold">    name: webhook-service</strong></pre>
			<p>And finally, comment <a id="_idIndexMarker496"/>out the <strong class="source-inline">manifests.yaml</strong> line<a id="_idIndexMarker497"/> in <strong class="source-inline">config/webhook/kustomization.yaml</strong> (this file does not exist for our use case, and trying to deploy without uncommenting this line will result in an error). The following snippet shows which line should be commented out with <strong class="source-inline">#</strong>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">config/webhook/kustomization.yaml:</p>
			<pre class="source-code"> resources:</pre>
			<pre class="source-code">#- manifests.yaml</pre>
			<pre class="source-code">                               </pre>
			<pre class="source-code">- service.yaml</pre>
			<p>With these <a id="_idIndexMarker498"/>changes, the Operator can be re-built<a id="_idIndexMarker499"/> and deployed using the <strong class="source-inline">operator-sdk</strong> and <strong class="source-inline">make</strong> commands from earlier chapters.</p>
			<h3>Deploying and testing the new API version</h3>
			<p>To confirm that the <a id="_idIndexMarker500"/>API server now understands and can convert between versions <a id="_idIndexMarker501"/>of the Operator CRD, install it in a cluster. Note that now your Operator will depend <a id="_idIndexMarker502"/>on <strong class="bold">cert-manager</strong> being present in the cluster, so be sure to install that first (instructions are available at <a href="https://cert-manager.io/docs/installation/">https://cert-manager.io/docs/installation/</a>).</p>
			<p>Remember, you need to update <strong class="source-inline">controllers/nginxoperator_controller.go</strong> to replace <strong class="source-inline">v1alpha1</strong> references with <strong class="source-inline">v1alpha2</strong> and change the <strong class="source-inline">Ports</strong> check (in <strong class="source-inline">Reconcile()</strong>) to match the following:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">controllers/nginxoperator_controller.go:</p>
			<pre class="source-code">func (*NginxOperatorReconciler) Reconcile(…) {</pre>
			<pre class="source-code">  if len(operatorCR.Spec.Ports) &gt; 0 {</pre>
			<pre class="source-code">   deployment.Spec.Template.Spec.Containers[0].Ports = operatorCR.Spec.Ports</pre>
			<pre class="source-code">  }</pre>
			<pre class="source-code">}</pre>
			<p>Forgetting to do this will cause an error when creating or retrieving the Operator CRD (which won't show up at compile time). This is because the <strong class="source-inline">v1alpha1</strong> API types are still defined and valid (so the Operator code compiles correctly), but the new client and reconciliation code will expect the object to be retrieved in the <strong class="source-inline">v1alpha2</strong> format.</p>
			<p>To deploy the nginx Operator, build and push the new container image before running <strong class="source-inline">make deploy</strong>:</p>
			<p class="source-code">$ export IMG=docker.io/mdame/nginx-operator:v0.0.2</p>
			<p class="source-code">$ make docker-build docker-push</p>
			<p class="source-code">$ make deploy</p>
			<p>Next, create a<a id="_idIndexMarker503"/> simple <strong class="source-inline">NginxOperator</strong> object. To demonstrate the <a id="_idIndexMarker504"/>API conversion, create it as the <strong class="source-inline">v1alpha1</strong> version and use the old <strong class="source-inline">port</strong> field:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">sample-cr.yaml:</p>
			<pre class="source-code">apiVersion: operator.example.com/v1alpha1</pre>
			<pre class="source-code">kind: NginxOperator</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: cluster</pre>
			<pre class="source-code">  namespace: nginx-operator-system</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  replicas: 1</pre>
			<pre class="source-code">  port: 8080</pre>
			<p>Next, create the custom resource object with kubectl:</p>
			<p class="source-code">$ kubectl apply -f sample-cr.yaml</p>
			<p>Now, using <strong class="source-inline">kubectl get</strong> to view the object will show it as <strong class="source-inline">v1alpha2</strong> because it was automatically converted and stored as this version:</p>
			<p class="source-code">$ kubectl get -o yaml nginxoperators/cluster -n nginx-operator-system</p>
			<p class="source-code">apiVersion: operator.example.com/v1alpha2</p>
			<p class="source-code">kind: NginxOperator</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  ...</p>
			<p class="source-code">  name: cluster</p>
			<p class="source-code">  namespace: nginx-operator-system</p>
			<p class="source-code">  resourceVersion: "9032"</p>
			<p class="source-code">  uid: c22f6e2f-58a5-4b27-be6e-90fd231833e2</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  ports:</p>
			<p class="source-code">  - containerPort: 8080</p>
			<p class="source-code">    protocol: TCP</p>
			<p class="source-code">  replicas: 1</p>
			<p class="source-code">...</p>
			<p>You can choose to<a id="_idIndexMarker505"/> specifically view the object as <strong class="source-inline">v1alpha1</strong> with the<a id="_idIndexMarker506"/> following command, which will instruct the API server to call the Operator's webhook and convert it back using the functions we wrote:</p>
			<p class="source-code">$ kubectl get -o yaml nginxoperators.v1alpha1.operator.example.com/cluster -n nginx-operator-system</p>
			<p class="source-code">apiVersion: operator.example.com/v1alpha1</p>
			<p class="source-code">kind: NginxOperator</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: cluster</p>
			<p class="source-code">  namespace: nginx-operator-system</p>
			<p class="source-code">  resourceVersion: "9032"</p>
			<p class="source-code">  uid: c22f6e2f-58a5-4b27-be6e-90fd231833e2</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  port: 8080</p>
			<p class="source-code">  replicas: 1</p>
			<p>What this means <a id="_idIndexMarker507"/>for your users is that they can continue using the existing <a id="_idIndexMarker508"/>API, which allows valuable transition time while you introduce a new version. Note that if they are already using the Operator, and you introduce a new <a id="_idIndexMarker509"/>storage version, they may need to use <strong class="bold">kube-storage-version-migrator</strong> (<a href="https://github.com/kubernetes-sigs/kube-storage-version-migrator">https://github.com/kubernetes-sigs/kube-storage-version-migrator</a>) to migrate the existing storage version to the new one. You can provide migration files for them (or even automate it into the Operator, as migrations are simply Kubernetes resources) to make this easier.</p>
			<p>With a new API version introduced and conversion handled, your Operator is now ready to be packaged into a new bundle so that deployment can be managed by the OLM. This means updating your Operator's CSV to a new version.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor130"/>Updating the Operator CSV version</h2>
			<p>Updating the version of <a id="_idIndexMarker510"/>the Operator in its CSV provides information to the OLM, OperatorHub, and users about which version of the Operator they are running. It also instructs the OLM on which versions replace other versions for in-cluster upgrades. This allows developers to define specific upgrade <strong class="bold">channels</strong> (such as <strong class="source-inline">alpha</strong> and <strong class="source-inline">beta</strong>), which are similar to the versioning channels in other software projects that allow users to subscribe to a different release cadence. The Operator SDK documentation goes into technical detail about this process on GitHub, but it is not necessary to<a id="_idIndexMarker511"/> understand these details for completing this section (<a href="https://github.com/operator-framework/operator-lifecycle-manager/blob/b43ecc4/doc/design/how-to-update-operators.md">https://github.com/operator-framework/operator-lifecycle-manager/blob/b43ecc4/doc/design/how-to-update-operators.md</a>). In this section, however, we will cover the simple task of updating the CSV version in a single channel.</p>
			<p>The first step toward bumping the Operator's CSV version is updating the version that will be replaced with the current version. In other words, <strong class="source-inline">v0.0.2</strong> will replace <strong class="source-inline">v0.0.1</strong>, so the <strong class="source-inline">v0.0.2</strong> CSV must indicate that it is replacing <strong class="source-inline">v0.0.1</strong>.</p>
			<p>This is done by modifying the base CSV in <strong class="source-inline">config/manifests/bases</strong> to add a <strong class="source-inline">replaces</strong> field under its spec, as shown in the following example:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">config/manifests/bases/nginx-operator.clusterserviceversion.yaml:</p>
			<pre class="source-code">apiVersion: operators.coreos.com/v1alpha1</pre>
			<pre class="source-code">kind: ClusterServiceVersion</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  annotations:</pre>
			<pre class="source-code">    alm-examples: '[]'</pre>
			<pre class="source-code">    capabilities: Basic Install</pre>
			<pre class="source-code">  name: nginx-operator.v0.0.0</pre>
			<pre class="source-code">  namespace: placeholder</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  ...</pre>
			<pre class="source-code">  replaces: nginx-operator.v0.0.1</pre>
			<p>Next, update the <strong class="source-inline">VERSION</strong> variable<a id="_idIndexMarker512"/> in the project's Makefile (you can also export this variable to the new version in your shell similar to the other environment variables we have used from this file, but updating it manually clearly indicates the version and ensures that the right version will be propagated when built on any machine):</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Makefile:</p>
			<pre class="source-code"># VERSION defines the project version for the bundle. </pre>
			<pre class="source-code">       </pre>
			<pre class="source-code"># Update this value when you upgrade the version of your project. </pre>
			<pre class="source-code"># To re-generate a bundle for another specific version without changing the standard setup, you can:  </pre>
			<pre class="source-code"># - use the VERSION as arg of the bundle target (e.g make bundle VERSION=0.0.2) </pre>
			<pre class="source-code"># - use environment variables to overwrite this value (e.g export VERSION=0.0.2) </pre>
			<pre class="source-code">VERSION ?= 0.0.2</pre>
			<p>Now, the new CSV can be built as part of the regular make bundle command from <a href="B18147_07_ePub.xhtml#_idTextAnchor108"><em class="italic">Chapter 7</em></a>, <em class="italic">Installing and Running Operators with the Operator Lifecycle Manager</em>:</p>
			<p class="source-code">$ make bundle IMG=docker.io/sample/nginx-operator:v0.0.2</p>
			<p>This updates the version of the Operator listed in <strong class="source-inline">bundle/manifests/nginx-operator.clusterserviceversion.yaml</strong> (which is the main CSV packaged into the Operator's bundle). If you followed the steps from the previous section to add a new API version, it also adds information about that new version (and the conversion webhook) to the CSV and the sample CRD packaged along with the bundle. In addition, it will generate a new Service manifest in the bundle that exposes the conversion endpoint for the two API versions.</p>
			<p>The bundle image<a id="_idIndexMarker513"/> can then be built, pushed, and run with the OLM just like before:</p>
			<p class="source-code">$ export BUNDLE_IMG=docker.io/sample/nginx-bundle:v0.0.2</p>
			<p class="source-code">$ make bundle-build bundle-push</p>
			<p class="source-code">$ operator-sdk run bundle docker.io/same/nginx-bundle:v0.0.2</p>
			<p>With a new bundle image built and working, the only step remaining in releasing your new version is to publish it on OperatorHub for users to find.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor131"/>Releasing a new version on OperatorHub</h2>
			<p>With a new<a id="_idIndexMarker514"/> version of your Operator, the bundle that is <a id="_idIndexMarker515"/>published to OperatorHub also needs to be updated (if you have chosen to release your Operator on OperatorHub). Thankfully, this process is not too complex. In fact, it is essentially the same as releasing your initial version (as in <a href="B18147_07_ePub.xhtml#_idTextAnchor108"><em class="italic">Chapter 7</em></a>, <em class="italic">Installing and Running Operators with the Operator Lifecycle Manager</em>), wherein you created a folder with your Operator's bundle and submitted that folder as a pull request to the community Operators repository<a id="_idIndexMarker516"/> on GitHub (<a href="https://github.com/k8s-operatorhub/community-operators">https://github.com/k8s-operatorhub/community-operators</a>).</p>
			<p>To release a new version of your Operator, simply create a new folder under your Operator's directory in the community Operators project named after the version number. For example, if the first version was <strong class="source-inline">nginx-operator/0.0.1</strong>, this version would be <strong class="source-inline">nginx-operator/0.0.2</strong>.</p>
			<p>Then, just like with your first version, simply copy the contents of your Operator's <strong class="source-inline">bundle</strong> directory (after generating the new bundle version) into the new version folder. Commit and push the changes to your fork of the GitHub repository and open a new pull request against the main repository with the changes.</p>
			<p>When your pull<a id="_idIndexMarker517"/> request passes the automated checks, it<a id="_idIndexMarker518"/> should merge, and the new version of your Operator should be visible on OperatorHub soon after.</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B18147_08_001.jpg" alt="Figure 8.1 – Screenshot of version and channel listing on OperatorHub for the Grafana Operator"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Screenshot of version and channel listing on OperatorHub for the Grafana Operator</p>
			<p>Now, you have finished releasing a new version of your Operator. By introducing a new API, ensuring that the new API is convertible between existing versions, updating your Operator's bundle, and publishing that updated bundle on OperatorHub, you should now announce to your users that the new version of your Operator is available. In the next sections, we'll discuss ways to ensure that future releases continue to go smoothly by planning ahead <a id="_idIndexMarker519"/>to minimize breaking API changes that require<a id="_idIndexMarker520"/> new versions and following the Kubernetes standards for making new changes.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor132"/>Planning for deprecation and backward compatibility</h1>
			<p>In the previous<a id="_idIndexMarker521"/> section, we discussed the work needed to release a new version of an Operator. While the processes for bundling and publishing a new version are relatively simple in terms of the effort required, implementing a new API version is not an insignificant task. As such, doing so should be done only as necessary in order to minimize the use of engineering resources and disruption to users. </p>
			<p>Of course, it will occasionally be unavoidable that incompatible changes must be introduced, for example, in the case of deprecation. Some of this deprecation might even come from upstream, where it is beyond your direct control (see the <em class="italic">Complying with Kubernetes standards for changes</em> section). However, the frequency of such changes can often be controlled through careful planning. In this section, we'll discuss some ways to plan for deprecation and support backward compatibility without causing undue strain on your engineers or users.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor133"/>Revisiting Operator design</h2>
			<p>In <a href="B18147_02_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, <em class="italic">Understanding How Operators Interact with Kubernetes</em>, the <em class="italic">Planning for changes in your Operator</em> section discussed, in general terms, various design approaches that establish good practices for outlining an Operator's design for future evolution. These suggested guidelines (which could, in reality, be applied to many software projects) were to start small, iterate effectively, and deprecate gracefully.</p>
			<p>Now, having gone through <a id="_idIndexMarker522"/>the steps of building an Operator from scratch in the form of our nginx Operator, it will be helpful to revisit these guidelines and examine how they were specifically applied to our own design.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor134"/>Starting small</h2>
			<p>The<a id="_idIndexMarker523"/> nginx Operator design started out very simple. The Operator was intended to serve a very basic function: manage the deployment of an nginx Pod. For configuration options, it exposed three fields to control the container port on the Pod, the number of replicas to deploy, and an extra field to trigger a forced redeployment. As a <strong class="bold">minimum viable product</strong> (<strong class="bold">MVP</strong>), this served well to get our Operator off the <a id="_idIndexMarker524"/>ground. While this design was intentionally kept minimal for the purpose of a reasonably sized demonstration, it still shows the mindset that keeps Operator CRDs from starting out with too many knobs exposed. Shipping excessive configuration options can be confusing to users, which then poses potential risks to the stability of your product if users cannot fully understand what every option does. This also adds a support burden on your own teams.</p>
			<p>Remember that the first release of an Operator will actually likely make up a minority of its life cycle. There will be plenty of time to add more features in future releases, but this becomes difficult as the CRD grows in its API definition and new features dilute the viability of existing ones. Of course, that's not to say you should never add new features to your Operator or its CRD. But, when that time comes, it's important to do so carefully, which is the focus of iterating effectively.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor135"/>Iterating effectively</h2>
			<p>In this chapter, we introduced an API change to the Operator wherein the type of one field was converted to an entirely different nested struct. This effectively removed the old field, which would be a breaking change for any user who was already relying on that field (in fact, we did technically remove the old field, but more on that in a minute). The benefit of changes such as this needs to be weighed against the negatives, which include disruption for your users (if applicable) and ongoing support for your own team.</p>
			<p>In our case, we chose to move from a single <strong class="source-inline">int32</strong> field (named <strong class="source-inline">port</strong>) to a list of <strong class="source-inline">v1.ContainerPort</strong> objects (named <strong class="source-inline">ports</strong>). This added some complexity to the CRD (because <strong class="source-inline">v1.ContainerPort</strong> contains other nested fields). However, it also shifted our CRD to rely <a id="_idIndexMarker525"/>on a native, stable API type from upstream Kubernetes. This, along with the added functionality of being able to configure multiple ports and added fields, offers usability and stability benefits for users and developers of the Operator (not that <strong class="source-inline">int32</strong> is likely to be unstable, but the general idea remains).</p>
			<p>Still, this change required the removal of the existing <strong class="source-inline">port</strong> field. This is what users must ultimately react to, but that transition can be made smoother through graceful deprecation.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor136"/>Deprecating gracefully</h2>
			<p>When adding the new <strong class="source-inline">ports</strong> field to our Operator's CRD, a conscious decision was made to remove the existing single <strong class="source-inline">port</strong> field, and that's perfectly acceptable; in fact, keeping it would have been unnecessarily redundant. But, the fact remains that users who were <a id="_idIndexMarker526"/>relying on the old field would have to transition to the new one or face data loss when their old configurations were migrated to the new ones. While that may seem small for something such as a single integer value, the scope of degradation can clearly scale with more complex Operators.</p>
			<p>This is why we added a conversion webhook to the Operator, to translate the old field into the new one automatically. But that webhook would not have been as simple to implement if the new <strong class="source-inline">ports</strong> field wasn't effectively a super-set of the old field. Choosing to go with a fairly compatible existing type made this transition much easier for developers to implement and users to understand. Design decisions such as this greatly help to reduce friction in a growing code base.</p>
			<p>However, our conversion was not completely perfect. While the transition from <strong class="source-inline">v1alpha1</strong> to <strong class="source-inline">v1alpha2</strong> carries over just fine, the reverse can only hold one port value (the first from the list). This may be suitable for practical use cases, as most users would be more likely to upgrade to the new version than downgrade to the deprecated one, but from a support standpoint, lossy conversion like this can create headaches down the road. There are ways to address this that are relevant to the next section, which discusses <a id="_idIndexMarker527"/>how Kubernetes prescribes ways to smoothly implement changes.</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor137"/>Complying with Kubernetes standards for changes</h1>
			<p>Kubernetes <a id="_idIndexMarker528"/>defines a standard set of policies for deprecation (and other breaking changes) that all core projects must abide by. This policy is<a id="_idIndexMarker529"/> available at <a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">https://kubernetes.io/docs/reference/using-api/deprecation-policy/</a>. It's not necessary to read and understand the whole policy for the purposes of Operator development, as we will highlight some of the relevant bits here. It primarily defines the standards for deprecating parts of the Kubernetes API, with many of the same (or similar) guidelines being applied to other types of deprecation as well (such as user-facing features that are not directly part of the API). It does this by enumerating a list of explicit rules for deprecating changes, some of which we will cover in this section.</p>
			<p>As a third-party component, your Operator is under no obligation to follow the Kubernetes deprecation policy. But, in a practical sense, there are benefits to working within the constraints of the ecosystem your Operator is built to be a part of. These include a template provided to set expectations for your users and a set of guidelines for planning your own ongoing development. And, even if you choose not to follow these policies for your own Operator, it is still crucial to understand how they are enforced upstream to be prepared for deprecations and changes beyond your control that you will eventually inherit.</p>
			<p>The full <a id="_idIndexMarker530"/>deprecation policy linked at the start of this section goes into specific detail for standards that govern every applicable Kubernetes component. So, some details of the deprecation are not directly relevant to Operator development. However, certain elements, such as those pertaining to the support and removal of API fields, do apply to Operators (should you choose to follow them).</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor138"/>Removing APIs</h2>
			<p>The Kubernetes deprecation policy<a id="_idIndexMarker531"/> is very clear on forbidding the removal of API elements from the current API version. In fact, this is the first rule in the entire policy.</p>
			<p class="callout-heading">Rule #1 </p>
			<p class="callout">API elements may only be removed by incrementing the version of the API group.</p>
			<p>This means that it is forbidden to remove any field or object from an existing API. The removal can only be done by introducing a new API version with that element removed.</p>
			<p>This is relevant to our nginx Operator, where we removed the <strong class="source-inline">port</strong> field that was present in <strong class="source-inline">v1alpha1</strong> as we incremented the API version to <strong class="source-inline">v1alpha2</strong>. Following this rule ensures that current users of an API version will not have their workflows suddenly broken by updating to a new release. The distinction between API versions makes a clear indication of some level of incompatibility.</p>
			<p>Inversely, this rule allows for the addition of API elements without incrementing the version of the existing API. This is because a new element in the current API version will not break any existing use cases because it is as if consumers of the object are simply leaving this field blank (as opposed to the removal of an existing field, which could result in data loss as non-blank entries are dropped). This is directly relevant to our use case because it provides <a id="_idIndexMarker532"/>an ability for seamless conversion.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor139"/>API conversion</h2>
			<p>The second rule of the Kubernetes <a id="_idIndexMarker533"/>deprecation policy is as follows.</p>
			<p class="callout-heading">Rule #2 </p>
			<p class="callout">API objects must be able to round-trip between API versions in a given release without information loss, with the exception of whole REST resources that do not exist in some versions.</p>
			<p>This means that when any two API versions exist within the same release of Kubernetes (or, in this case, your Operator), objects of either API version must be able to convert between the two versions while preserving all data fields. </p>
			<p>In our nginx Operator, we did not follow this rule (because a <strong class="source-inline">v1alpha2</strong> CRD object with multiple ports defined cannot translate them all to a single <strong class="source-inline">port</strong> value). This is OK because, as a third-party project, we are not beholden to the upstream Kubernetes policies. But, from a practical standpoint, it would be useful for us and our users to support such lossless conversions. This could be done by adding the <strong class="source-inline">ports</strong> field to both <strong class="source-inline">v1alpha2</strong> and <strong class="source-inline">v1alpha1</strong>. Then, our conversion could have stored the additional ports in the new field when converting to <strong class="source-inline">v1alpha1</strong>. Existing code that only knows about the <strong class="source-inline">v1alpha1</strong>  single <strong class="source-inline">port</strong> field may not be able to use these additional ports, but the important part is that the data is preserved during conversion. Alternatively, we could have simply stored the additional <strong class="source-inline">ports</strong> values as an annotation in the CRD object's metadata and read from this during conversion.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor140"/>API lifetime</h2>
			<p>How long you choose to support an <a id="_idIndexMarker534"/>API version is entirely up to your organization's agreement with your users. The Kubernetes standards for support timelines vary based on the stability level of that API. The three levels of stability are alpha, beta, and general availability (GA).</p>
			<p>In our nginx Operator, the API is currently in alpha. Technically, this means that there is no support guarantee for any duration. However, when working toward graduating an API to a more stable level, it is good practice to operate as if that API is already at the next level of stability. For beta APIs, this timeline is the longer of 9 months or three releases (see the <em class="italic">Aligning with the Kubernetes release timeline</em> section). APIs that are graduated to GA cannot be removed, but they can be marked as deprecated. The intent is that an API that is GA can be assumed stable for the lifetime of that major version of Kubernetes.</p>
			<p>Removal, conversion, and lifetime are the three main points of the Kubernetes deprecation policy relevant to our Operator development. There are more details in the link provided at the top of this section. You can also follow the planned timeline for upcoming upstream <a id="_idIndexMarker535"/>API deprecation, which is published at <a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/">https://kubernetes.io/docs/reference/using-api/deprecation-guide/</a>, for each release.</p>
			<p>While this section mentioned the Kubernetes release as a unit of time, we did not define exactly how much time that is. In the next section, we will explain exactly how the Kubernetes release breaks down and how it relates to your Operator development. </p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor141"/>Aligning with the Kubernetes release timeline</h1>
			<p>Each new<a id="_idIndexMarker536"/> release of Kubernetes is driven by the hard work and dedication of many people from different companies and time zones. This process for publishing a new version of Kubernetes is, therefore, the product of an orchestration effort among component areas and <strong class="bold">special interest groups</strong> (<strong class="bold">SIGs</strong>) to ensure timely, stable releases. While there are occasional roadblocks and delays, the release timeline is, for the most part, a well-defined collaborative effort that strives to provide predictable updates that the Kubernetes ecosystem of users and downstream products can depend on.</p>
			<p>When developing an Operator, you or your organization will likely have similar organized release efforts. You also want to provide a dependable and timely schedule for shipping updates to your users. And, while your own release schedule may differ from the Kubernetes timeline, there are still benefits to a keen understanding of how the upstream release works. For example, it offers you the ability to plan around specific dates at which a beta API will be promoted to GA, or an entirely new feature will become available that you can leverage in your own product to pass on to your users. On the other hand, it also outlines the remaining time that deprecated APIs have before they are removed entirely. As a product vendor, you can rely on this timeline to guide your release planning.</p>
			<p>For these <a id="_idIndexMarker537"/>reasons, aligning your Operator's release timeline for updates with that of Kubernetes is a valuable effort and it's why we will explain that timeline more in this section.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor142"/>Overview of a Kubernetes release</h2>
			<p>The <a id="_idIndexMarker538"/>Kubernetes release cycle is approximately 15 weeks long. As of Kubernetes 1.22, this has defined a goal of three releases per calendar year. Of course, three 15-week releases do not account for an entire 52-week-long year. This is because the release cycle allows for several breaks in work, including holidays, end-of-year travel, and events or conferences such <a id="_idIndexMarker539"/>as <strong class="bold">KubeCon</strong>. This timeline was decided on by the SIG Release team with input from the community, and you can read more details <a id="_idIndexMarker540"/>about the decision in a blog post at <a href="https://kubernetes.io/blog/2021/07/20/new-kubernetes-release-cadence/">https://kubernetes.io/blog/2021/07/20/new-kubernetes-release-cadence/</a>.</p>
			<p>During the course of a single release, there are several key dates that signal important progress updates in that release. These <a id="_idIndexMarker541"/>include an <strong class="bold">Enhancements Freeze</strong>, <strong class="bold">Code Freeze</strong>, deadlines <a id="_idIndexMarker542"/>for documentation updates and blog posts, and <strong class="bold">release candidate</strong> (<strong class="bold">RC</strong>) version releases. The exact timeline for each release is posted in the SIG <a id="_idIndexMarker543"/>Release GitHub repository at <a href="https://github.com/kubernetes/sig-release">https://github.com/kubernetes/sig-release</a>. As an example, the Kubernetes 1.24 release cycle looked like the following, with some of these key dates highlighted:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B18147_08_002.jpg" alt="Figure 8.2 – Kubernetes 1.24 release cycle calendar"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Kubernetes 1.24 release cycle calendar</p>
			<p>Each of these <a id="_idIndexMarker544"/>dates plays an important role in the progression of an individual release. In the following sections, we'll explain what each of them means in more detail and how they can relate to your own release cycle for your Operator. Much of this information<a id="_idIndexMarker545"/> is also documented in the SIG Release repository at <a href="https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md">https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md</a>.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor143"/>Start of release</h2>
			<p>Fairly <a id="_idIndexMarker546"/>self-explanatory, this is the official date that the release cycle for the next version of Kubernetes begins. This provides a reference date from which all other steps in the cycle can be derived. Note that this is not the same date as the previous release's publication because of the buffer between release cycles, along with post-release processes <a id="_idIndexMarker547"/>such as <strong class="bold">Retrospective</strong>. So, there may already be work-in-progress<a id="_idIndexMarker548"/> for the current release before this date (see the <em class="italic">GA release/Code Thaw</em> subsection). However, just as any race needs a starting line, every release needs a date to denote its official beginning.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor144"/>Enhancements Freeze</h2>
			<p>New features in<a id="_idIndexMarker549"/> Kubernetes take many forms. While all <a id="_idIndexMarker550"/>changes to the platform are valuable regardless of size, certain undertakings involve increased effort and a broadened scope of work. Examples include significant user-facing changes or architectural designs involving collaboration between multiple components and SIGs. Such features require additional management to ensure their successful rollout. At this point, the feature may be <a id="_idIndexMarker551"/>considered as an <strong class="bold">enhancement</strong> or <strong class="bold">Kubernetes Enhancement Proposal</strong> (<strong class="bold">KEP</strong>).</p>
			<p>We have already mentioned several individual KEPs in this book. For example, in <a href="B18147_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>, <em class="italic">Developing an Operator – Advanced Functionality,</em> we referred to <strong class="bold">KEP-1623</strong>, which defined the standard <strong class="source-inline">Condition</strong> type for components to report their status (including our nginx Operator). All KEPs such as this are tracked in the <em class="italic">Kubernetes Enhancements</em> GitHub repository at <a href="https://github.com/kubernetes/enhancements">https://github.com/kubernetes/enhancements</a>.</p>
			<p>The full details of the KEP process and repository are beyond the scope of this book, but the essential knowledge is that KEPs signify wide-scale design changes that can impact users or consumers of APIs. As the developer of an Operator, which is itself a component that may consume one or more upstream APIs (such as the <strong class="source-inline">Condition</strong> field), it is then important to be aware of the status of upcoming changes of this scale, as they may directly affect you. Providing this information is the goal of the KEP process.</p>
			<p>During a release cycle, the Enhancements Freeze stage signifies the point at which all proposed KEPs for that release must be either accepted and committed toward implementing their changes or delayed for a future release. This is the critical <em class="italic">go</em>/<em class="italic">no-go</em> date for moving forward with a KEP for that release. And, while many in-progress KEPs are able to proceed at this point, there may be a reasonable justification why certain changes may not be able to commit to the release by this date. In these cases, the developers of an enhancement may request an <strong class="bold">exception</strong> during the <em class="italic">Call for Exceptions</em> period, described next.</p>
			<p>For Operator developers, the Enhancements Freeze deadline is an important date to keep in mind during your own development cycle because, while KEPs are often used to introduce new features to Kubernetes, part of their definition is to outline the removal of other features (such as any that are being replaced). Being aware of any features you depend on being officially slated for removal helps to gauge the urgency with which you will need to react to that removal. On the other hand, knowing that a KEP missed the deadline for Enhancements Freeze<a id="_idIndexMarker552"/> can provide assurance that any related features planned for removal <a id="_idIndexMarker553"/>will continue to be supported for at least another release unless that plan receives an exception.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor145"/>Call for Exceptions</h2>
			<p>If, during the<a id="_idIndexMarker554"/> course of a release, a certain feature is not yet ready to<a id="_idIndexMarker555"/> commit to Enhancement Freeze (or <em class="italic">Code Freeze</em>), the contributors and participating SIGs who are working on that enhancement may request an exception to the freeze deadlines. If approved, this allows those contributors a reasonable extension to accommodate the additional time needed to prepare the enhancement for release.</p>
			<p>Enhancements seeking an exception need to meet certain criteria to ensure that they do not risk impacting the stability of the platform or delaying the release. Therefore, release managers evaluate each exception request based on its scope, the estimated extension time requested, and the time at which the request was submitted within the release cycle.</p>
			<p>Exception requests and approvals are generally communicated via the participating SIG's mailing lists, Slack channels, and the specific KEP issue discussion page on GitHub. This is why it's important to monitor these communication channels for features your Operator implements or depends on. Just because a new feature missed Enhancements Freeze, that doesn't mean it won't be implemented until an exception has been officially denied. The same goes for planned removals, in the event that an exception is granted. Knowing whether the upstream SIG related to your Operator's function is going to request any exceptions after Enhancements Freeze (or Code Freeze) is a good signal for what to commit to in your Operator's development cycle.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor146"/>Code Freeze</h2>
			<p>Code Freeze<a id="_idIndexMarker556"/> is the <a id="_idIndexMarker557"/>point during the release cycle at which all code changes must be complete and merged into the Kubernetes code base (besides any features that have been granted an exception). This signifies that no more changes will be accepted into the release unless they are critical to the stability of the platform. </p>
			<p>As the developer of a downstream Operator, this is relevant to your own project timeline because it is the point at which you can begin updating your libraries to the latest Kubernetes version with a reasonable expectation of stability. This can be done by updating your dependencies to the latest RC version of the upstream Kubernetes libraries. </p>
			<p>The first RC for a new release is usually published soon after the Code Freeze date. This timing allows developers to update dependencies with additional <em class="italic">soak time</em> to catch any additional updates or reactions that need to be made in order to function with the new version of Kubernetes before it is finally released. It is very beneficial to take advantage of this timing to minimize the delay between publishing your updated Operator after the Kubernetes version release. Due to the size and fluctuating nature of Kubernetes, it's recommended to upgrade any upstream dependencies regularly. Failure to do so is likely to result in snowballing technical debt and eventual lack of compatibility with newer versions of the platform.</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor147"/>Test Freeze</h2>
			<p>While Code Freeze<a id="_idIndexMarker558"/> defines a strict deadline for enhancements to have their implementations completely merged, Test Freeze<a id="_idIndexMarker559"/> allows an additional buffer to expand test coverage. This provides an opportunity to improve test cases once a feature has merged before release. After this date, the only changes allowed to any tests are to fix or remove tests that are consistently failing. This date may not have a consistent impact on your own release cycle, but it is a good signal to be aware of when monitoring the progress of a key enhancement.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor148"/>GA release/Code Thaw</h2>
			<p>At last, the moment <a id="_idIndexMarker560"/>everyone has been waiting for. If there are a few problems discovered during the release that cause a delay, this is the date that Kubernetes releases its newest version. For developers, this means that the code in Kubernetes (at <a href="https://github.com/kubernetes/kubernetes">https://github.com/kubernetes/kubernetes</a>) is updated with a new Git tag, which allows the opportunity to easily <a id="_idIndexMarker561"/>reference the release point at a definitive point in code (for example, when updating your Operator's dependencies in its <strong class="source-inline">go.mod</strong> file). In addition, the client libraries that support Kubernetes and its subprojects are also updated, including the following:</p>
			<ul>
				<li><a href="https://github.com/kubernetes/api">https://github.com/kubernetes/api</a> – Core API types used by the Kubernetes platform (imported as <strong class="source-inline">k8s.io/api</strong>)</li>
				<li><a href="https://github.com/kubernetes/apimachinery">https://github.com/kubernetes/apimachinery</a> – The library used to implement the encoding and decoding of API types in code (<strong class="source-inline">k8s.io/apimachinery</strong>)</li>
				<li><a href="https://github.com/kubernetes/client-go">https://github.com/kubernetes/client-go</a> – The Go client used by Operators and other programs to interact with Kubernetes resources programmatically (<strong class="source-inline">k8s.io/client-go</strong>) </li>
			</ul>
			<p>These are just a few of the additional dependencies that, on GA release<a id="_idIndexMarker562"/> day, will be updated with a new Git tag. These dependencies are actually part of the core Kubernetes repository (<strong class="source-inline">k8s.io/kubernetes</strong>), but they are synced to symbolic canonical locations by an automated bot for better dependency management. This can sometimes cause a slight delay between core Kubernetes releasing its tag and the libraries updating <a id="_idIndexMarker563"/>with their own (which are usually more important to developers).</p>
			<p class="callout-heading">Kubernetes as a Dependency</p>
			<p class="callout">The core Kubernetes repository at <strong class="source-inline">k8s.io/kubernetes</strong> (or <a href="https://github.com/kubernetes/kubernetes">https://github.com/kubernetes/kubernetes</a>) contains all of the code needed by the core platform components. So, it may be tempting to import code from here directly. However, due to its complexity, it is not recommended to import code from <strong class="source-inline">k8s.io/kubernetes</strong> directly into your project as it can cause dependency issues that are difficult to resolve with Go modules and bring excess transitive dependencies into your project. Instead, it is best to rely on the component libraries (such as those listed previously), which are meant to be imported into outside projects.</p>
			<p>After the new release is officially published, the Kubernetes<a id="_idIndexMarker564"/> project enters <strong class="bold">Code Thaw</strong>, which, as its name implies, is the opposite of Code Freeze. This means that code development for the next release can begin merging into the main branch of Kubernetes. So, working with code from the <strong class="source-inline">master</strong> (or <strong class="source-inline">main</strong>) branch at any point after this really means you<a id="_idIndexMarker565"/> are interacting with the <strong class="source-inline">N+1</strong> version code (where <strong class="source-inline">N</strong> is the current version of Kubernetes).</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor149"/>Retrospective</h2>
			<p>When the dust has settled, the<a id="_idIndexMarker566"/> SIG Release team takes time to review the release during <strong class="bold">Retrospective</strong>. The intent of Retrospective<a id="_idIndexMarker567"/> is to meet and discuss any roadblocks or concerns that occurred during that release and propose solutions to address them in the future. In addition, any exceptional successes are identified and praised. It is meant as a blameless process through which the end goal is simply to reduce friction for future releases. While the specific details of the Retrospective may not be directly relevant to the development of your Operator (the focus is generally more on the release infrastructure and processes that drive the release rather than specific feature changes), it can be a very informative way to be aware of future changes to the release cycle.</p>
			<p>All of the above dates form the most important signals in the Kubernetes release cycle. Being aware of them can help inform your team about the status of the current features you depend on, and that information can be passed on in the form of the commitments you make to your users. Staying up to date with the latest Kubernetes releases is critical to avoid tiresome technical debt from piling up, as the Kubernetes project is constantly evolving within the support constraints it offers.</p>
			<p>In addition, knowing where you are in the current release cycle also provides the opportunity to contribute to Kubernetes at meaningful times. In the next section, we will discuss ways you can do this and how it can benefit you.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor150"/>Working with the Kubernetes community</h1>
			<p>This chapter focused<a id="_idIndexMarker568"/> heavily on the standards, policies, and timelines of Kubernetes as they relate to Operator development. While it might seem that these are fixed, prescriptive decrees, the reality is that they are fluid frameworks for development created through an open and fair process. This process is organized by contributors from different companies all over the world, and it is always open to new voices.</p>
			<p>As a developer for the Kubernetes platform, you have inherent stock in the community that organizes Kubernetes upstream. Therefore, improvements or concerns that affect you are likely to affect others as well. This is why it's not only OK but encouraged that you play an active role in upstream development alongside your own Operator. If nothing else, doing so serves to benefit your own development, as the open process allows you to help steer upstream work as you feel it should be done.</p>
			<p>Getting involved is as<a id="_idIndexMarker569"/> simple as sending a message or joining a video call. The various GitHub repositories and Slack channels shown in other chapters for support are a great place to offer support yourself. The Kubernetes Slack server is <a href="http://slack.k8s.io">slack.k8s.io</a>, and it is free to join and contribute. You may also want to follow the various SIG meetings for topics of interest, all of which are listed on the <em class="italic">Kubernetes Community</em> repository<a id="_idIndexMarker570"/> at <a href="https://github.com/kubernetes/community">https://github.com/kubernetes/community</a>. This repository includes links and schedules for all SIG and community meetings.</p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor151"/>Summary</h1>
			<p>Once, in an interview, champion football quarterback Tom Brady was asked which of his championship rings was his favorite, to which he replied, "The next one." Coming from a person who many already considered to be one of the most successful in his field, that response showed powerful commitment to a continuous pursuit of achievement (if not a little tongue-in-cheek hubris). As software developers, that same passion is what drives the never-ending cycle of improvement with every new release.</p>
			<p>This chapter was about the part of the Operator development cycle that is even more important than the first release: the next one. Releasing new software versions is not something that is exclusive to Operators, but the idiomatic processes and upstream Kubernetes standards do create a unique set of requirements for Operator projects. By exploring the technical steps necessary to create and publish a new version, alongside the more abstract policies and timelines that dictate guidelines for doing so, we have ensured that you are aware of a few suggestions for how to keep your Operator running for many releases to come.</p>
			<p>This concludes the technical tutorial section of this book. While there are many topics and details that, unfortunately, did not fit within the scope of these chapters, we have explained all of the foundational concepts to build an Operator following the Operator Framework. In the next chapter, we'll summarize these concepts in an FAQ-style format to quickly refresh what was covered.</p>
		</div>
	</body></html>