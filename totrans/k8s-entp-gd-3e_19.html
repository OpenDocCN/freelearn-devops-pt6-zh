<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer333">
<h1 class="chapterNumber">19</h1>
<h1 class="chapterTitle" id="_idParaDest-611">Building a Developer Portal</h1>
<p class="normal">One of the more popular concepts in recent years of DevOps and automation is to provide an <strong class="keyWord">Internal Developer Portal</strong> (<strong class="keyWord">IDP</strong>). The purpose of this portal is to provide a single point of service for your <a id="_idIndexMarker1750"/>developers and infrastructure team to be able to access architectural services without having to send an email to “your guy” in IT. This is often the promise of cloud-based services, though it requires considerable custom development to achieve. It also provides the foundation for creating the guardrails needed to develop a manageable architecture.</p>
<p class="normal">This chapter is going to combine the theory we walked through in <em class="chapterRef">Chapter 18</em>, <em class="italic">Provisioning a Multitenant Platform</em>, along with most of the concepts and technologies we’ve learned throughout this book to create an IDP. Once you’ve completed this chapter, you’ll have an idea of how to build an IDP for your infrastructure, as well as context for how the various technologies we have built and integrated into Kubernetes through this book should come together.</p>
<p class="normal">This chapter will cover the following topics:</p>
<ul>
<li class="bulletList">Technical requirements</li>
<li class="bulletList">Deploying our IDP</li>
<li class="bulletList">Onboarding a tenant</li>
<li class="bulletList">Deploying an application</li>
<li class="bulletList">Expanding our platform</li>
</ul>
<p class="normal">Finally, before we dive into the chapter, we’d like to say <em class="italic">thank you!</em> This book has been quite the journey for us. It’s amazing to see how much has changed in our industry since we wrote the second edition, and how far we have come. Thank you for joining us on our quest to build out enterprise Kubernetes, and explore the different technologies and how the enterprise world impacts how we create that technology.</p>
<h1 class="heading-1" id="_idParaDest-612">Technical Requirements</h1>
<p class="normal">This chapter has more significant technology requirements than the previous chapters. You’ll need three Kubernetes clusters with the following requirements:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Compute</strong>: 16 GB memory and 8 cores. You’re going to be running GitLab, Vault, OpenUnison, Argo CD, etc. It’s going to require some real horsepower.</li>
<li class="bulletList"><strong class="keyWord">Access</strong>: Make sure you can update and access the local nodes. You’ll need this to add our CA certificate to your nodes so that it can be trusted when pulling container images.</li>
<li class="bulletList"><strong class="keyWord">Networking</strong>:<strong class="keyWord"> </strong>You won’t need public IPs, but you will need to be able to access all three clusters from your workstation. It will make the implementation easier if you use load balancers for each, but it’s certainly not a requirement.</li>
<li class="bulletList"><strong class="keyWord">Pulumi and Python 3</strong>: We’re going to be using Pulumi to deploy our platform, running on Python 3. The workstation you use will need to be able to run these tools. We built and wrote this chapter on macOS, using Homebrew (<a href="https://brew.sh/"><span class="url">https://brew.sh/</span></a>) for Python.</li>
</ul>
<p class="normal">Before we start building, we’re going to spend some time on the technical requirements for this chapter and why they are requirements, relating them back to common enterprise scenarios. First, let’s look at our compute requirements.</p>
<h2 class="heading-2" id="_idParaDest-613">Fulfilling Compute Requirements</h2>
<p class="normal">Throughout the rest of this <a id="_idIndexMarker1751"/>book, our goal was to run all the labs on a single VM. We did this for a few reasons:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Cost</strong>:<strong class="keyWord"> </strong>We know how quickly costs can climb when learning technology and we wanted to make sure we weren’t asking you to spend more money for this.</li>
<li class="bulletList"><strong class="keyWord">Simplicity</strong>: Kubernetes is hard enough without getting into the details of how compute and networking are set up in enterprise environments! We also didn’t want to have to worry about storage, which brings several complications to the table.</li>
<li class="bulletList"><strong class="keyWord">Ease of Implementation and Support</strong>: We wanted to make sure we could help with the labs, so limiting how you deployed them made that much easier for us.</li>
</ul>
<p class="normal">With that all said, this chapter is <a id="_idIndexMarker1752"/>different. You could use three VMs running KinD, but that would probably start causing more problems than it would be worth. There are two primary options we’re going to cover: using the cloud and building a home lab.</p>
<h3 class="heading-3" id="_idParaDest-614">Using Cloud-Managed Kubernetes</h3>
<p class="normal">It’s very popular to use a managed Kubernetes when there’s nothing to deploy. Every major cloud has its own, and so do most smaller clouds. These are great if you are OK with spending some money and are more focused on Kubernetes than the infrastructure that runs it. Make sure, though, that when you set up your clusters, you are able to directly access your <a id="_idIndexMarker1753"/>worker nodes via SSH or some other means.</p>
<p class="normal">Many cloud-based <a id="_idIndexMarker1754"/>managed clusters make it the default that you can’t access your nodes, which, from a security standpoint, is great! You can’t breach something you can’t access! The downside is that you can’t customize it either. We’ll cover this in the next section, but most enterprises require customized nodes at some level, even when using cloud-managed Kubernetes.</p>
<p class="normal">Also, make sure you’re able to handle the costs. A setup with three clusters is not likely to stay within whatever free credits you get for long. You’ll want to make sure you can afford to spend the money. That said, if throwing money at a cloud isn’t for you, then maybe a home lab will be your best bet.</p>
<h3 class="heading-3" id="_idParaDest-615">Building a Home Lab</h3>
<p class="normal">The cloud can get really<a id="_idIndexMarker1755"/> expensive, and that money is just thrown away. You don’t have anything to show for it! The alternative is building a home lab to run your clusters. As of the time of writing this book, it’s never been easier to <a id="_idIndexMarker1756"/>run enterprise-grade infrastructure in your own home or apartment. It doesn’t require a massive investment and can be far cheaper than a single cloud-managed cluster for just a month or two.</p>
<p class="normal">You can start very simply with a single refurbished or even home-built server for under $500 on eBay and other auction sites. Once you have a server, install Linux and a hypervisor and you’re ready to start. This, of course, requires more time being spent on underlying infrastructure than we’ve done so far, but can be very rewarding both from a personal level and an economic one. If you’re going through this book in the hopes of breaking into the enterprise Kubernetes world, knowing about how your infrastructure works can be a real <a id="_idIndexMarker1757"/>differentiator among other candidates.</p>
<p class="normal">If you’re in the position to spend a few more dollars on your home lab, there are projects that make it <a id="_idIndexMarker1758"/>easier to build out your lab. Two such projects are:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Metal as a Service (MaaS)</strong>: This project (<a href="https://maas.io/"><span class="url">https://maas.io/</span></a>) from Canonical makes it easier to <a id="_idIndexMarker1759"/>quickly onboard infrastructure to a lab by providing resource management, DNS, network boot, etc. Canonical is the same company that created the Ubuntu Linux distribution. While it started as a project for onboarding hardware quickly, it also supports KVM via the <code class="inlineCode">virsh</code> protocol, which allows for the management of VMs via SSH. This is what I run my home lab on right now on a few home-built PCs running Ubuntu.</li>
<li class="bulletList"><strong class="keyWord">Container Craft Kargo</strong>: A relatively new platform (<a href="https://github.com/ContainerCraft/Kargo"><span class="url">https://github.com/ContainerCraft/Kargo</span></a>) that combines several “enterprise” quality systems to build a home lab built on Kubernetes. The great thing about<a id="_idIndexMarker1760"/> this project is it starts with Talos, a combination operating system and Kubernetes distribution, and uses KubeVirt to leverage the Kubernetes API for deploying VMs. It’s a great project that I’ve started working on and using and will be moving my home lab, too.</li>
</ul>
<p class="normal">Having worked through what to build a home lab on and where you can deploy your IDP, we’ll next explore why you’ll need to have direct access to your nodes.</p>
<h2 class="heading-2" id="_idParaDest-616">Customizing Nodes</h2>
<p class="normal">When working with a managed Kubernetes such as Amazon or Azure, the nodes are provided for you. There’s <a id="_idIndexMarker1761"/>also an option to disable external access. This is great from a security standpoint because you don’t need to secure what you can’t access!</p>
<div class="note">
<p class="normal">As we’ve said before, there’s a difference between security and compliance. While a fully managed node may be more secure, your compliance rules may say that you, as the cluster manager, must have processes in place to manage node access. Simply removing all access may not cover this compliance issue.</p>
</div>
<p class="normal">There’s a functional drawback to this approach though; you’re now not able to customize the nodes. This drawback can manifest in several ways:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Custom Certificates</strong>: We’ve made the point multiple times throughout this book that enterprises often maintain internal <strong class="keyWord">Certificate Authorities</strong> (<strong class="keyWord">CAs</strong>). We’re mirroring this process by using our own internal CA for issuing certificates used by Ingresses and other components. This includes our Harbor instance, which means for our cluster to be able to pull images, the node it runs on must trust our CA. For this work, the node must be configured to trust our CA. There’s no API for Kubernetes to trust a private CA, unfortunately.</li>
<li class="bulletList"><strong class="keyWord">Drivers</strong>: While<a id="_idIndexMarker1762"/> not as important with cloud-managed Kubernetes, it’s not unusual for enterprises to use specific hardware stacks that are certified to work with specific hardware. For instance, your <strong class="keyWord">Storage Area Network</strong> (<strong class="keyWord">SAN</strong>) may have specific kernel drivers. If you don’t have access to your nodes, you can’t install these drivers.</li>
<li class="bulletList"><strong class="keyWord">Supported Operating Systems</strong>: Many enterprises, especially those in highly regulated industries, want to make sure they’re running a supported operating system and configuration. For instance, if you’re running Azure Kubernetes but your enterprise has standardized on <strong class="keyWord">Red Hat Enterprise Linux</strong> (<strong class="keyWord">RHEL</strong>), you’ll need to create a custom node image.</li>
</ul>
<p class="normal">While requiring access to your nodes complicates your deployment in multiple ways, such as requiring a way to manage and secure that access, it’s often a necessary evil to the deployment and management of Kubernetes.</p>
<p class="normal">While you may be most familiar with building nodes on Ubuntu, RHEL, or RHEL clones, Talos Linux from Sidero (<a href="https://www.talos.dev/"><span class="url">https://www.talos.dev/</span></a>) provides <a id="_idIndexMarker1763"/>a novel approach by stripping down the OS to the bare minimum needed to start Kubernetes. This means that all interaction with your OS happens via an API, either from Kubernetes or Talos. This makes for some very interesting management because you no longer need to worry about patching the OS; the upgrades are all done via APIs. No more securing SSH, but you do still need to lock down the API. Not having access to the OS means you can’t just deploy a driver either. The Kargo project we mentioned earlier uses Talos for its OS. When I wanted to integrate my Synology <strong class="keyWord">Network Attached Storage</strong> (<strong class="keyWord">NAS</strong>), I had to <a id="_idIndexMarker1764"/>create a <code class="inlineCode">DaemonSet</code> for the task to make it work with iSCSI (<a href="https://github.com/ContainerCraft/Kargo/tree/main"><span class="url">https://github.com/ContainerCraft/Kargo/blob/main/ISCSI.md</span></a>).</p>
<p class="normal">Since we’re using our own internal CA, your nodes will need to be customizable at least to the point of being able to include custom certificates.</p>
<div class="note">
<p class="normal">You may think that an easy way to avoid this situation is to <a id="_idIndexMarker1765"/>use Let’s Encrypt (<a href="https://letsencrypt.org/"><span class="url">https://letsencrypt.org/</span></a>) to generate certificates, avoiding the need for custom certificate authorities. The issue with this approach is that it avoids the common need to use custom certificates in enterprises, whereas Let’s Encrypt doesn’t provide a standard way of issuing internal certificates. Its automatic issuance APIs are built on public validation techniques such as having publicly available URLs or via DNS. Neither is acceptable to most enterprises so Let’s Encrypt is generally not allowed for internal systems. Since Let’s Encrypt isn’t generally used for enterprises’ internal systems, we won’t use it here.</p>
</div>
<p class="normal">Now that we know why we need access to our nodes, next, we’ll talk about network management.</p>
<h2 class="heading-2" id="_idParaDest-617">Accessing Services on Your Nodes</h2>
<p class="normal">Throughout this book, we’ve assumed <a id="_idIndexMarker1766"/>everything runs on a single VM. Even when we ran multiple nodes in KinD, we did tricks with port forwarding to get access to containers running on those nodes. Since these clusters are larger, you may need a different approach. We covered MetalLB in <em class="chapterRef">Chapter 4</em>, <em class="italic">Services, Load Balancing, and Network Policies</em>, as a load balancer, which is potentially a great option for multiple node clusters. You can also deploy your <code class="inlineCode">Ingress</code> as a <code class="inlineCode">DaemonSet</code>, with the pods using host ports to listen across all your nodes, then use DNS to resolve all your nodes.</p>
<p class="normal">Regardless of which approach you use, we’re going to assume that all services will be accessed via your <code class="inlineCode">Ingress</code> controller. This includes text or binary protocols:</p>
<ul>
<li class="bulletList">Control Plane:<ul>
<li class="bulletList level-2">80/443: http/https</li>
<li class="bulletList level-2">22: ssh</li>
<li class="bulletList level-2">3306: MySQL</li>
</ul>
</li>
<li class="bulletList">Dev Node:<ul>
<li class="bulletList level-2">80/443: http/https</li>
<li class="bulletList level-2">3306: MySQL</li>
</ul>
</li>
<li class="bulletList">Production Node:<ul>
<li class="bulletList level-2">80/443: http/https</li>
<li class="bulletList level-2">3306: MySQL</li>
</ul>
</li>
</ul>
<p class="normal">When we begin our rollout, we’ll see that NGINX can be used to forward both web protocols and binary protocols, allowing you to use one <code class="inlineCode">LoadBalancer</code> per cluster. It’s important to note that your control plane cluster will need to be able to access HTTPS and MySQL on both your dev <a id="_idIndexMarker1767"/>and production nodes. Also note that port <code class="inlineCode">22</code> will be needed by our control plane cluster, so if you plan on supporting SSH directly for your nodes, you’ll need to configure it for another port if you’re not using an external <code class="inlineCode">LoadBalancer</code> like MetalLB.</p>
<p class="normal">We know how we’re going to run our clusters, how we’ll customize the worker nodes, and how we’ll access their services. Our last step is to get Pulumi ready.</p>
<h2 class="heading-2" id="_idParaDest-618">Deploying Pulumi</h2>
<p class="normal">In the last chapter, we introduced the <a id="_idIndexMarker1768"/>concept of <strong class="keyWord">Infrastructure as Code</strong> (<strong class="keyWord">IaC</strong>) and said that we would be using <a id="_idIndexMarker1769"/>Pulumi’s IaC tooling for deploying our IDP. In order to use Pulumi, you’ll need a workstation to host and run the client.</p>
<p class="normal">Before we dive too deeply into how to deploy Pulumi, it’s important to understand some key concepts relating to IaC that are often glossed over. All IaC tools are made up of at least three components:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Controller</strong>: The controller is <a id="_idIndexMarker1770"/>generally a workstation or service that runs the IaC tooling. For our book, we’re assuming a workstation will run our Pulumi programs. For larger-scale or production implementations, it’s generally better to deploy a controller service that runs the IaC tooling on your behalf. With Pulumi, this could be their own SaaS service or a Kubernetes operator.</li>
<li class="bulletList"><strong class="keyWord">Tooling</strong>: This is the core<a id="_idIndexMarker1771"/> component of IaC. It’s the part that you create for building your infrastructure and is specific to each IaC tool.</li>
<li class="bulletList"><strong class="keyWord">Remote APIs</strong>: Each IaC tool interacts with remote systems via an API. The original IaC tools interacted with Linux servers via SSH, and then with clouds via their own APIs. Today, IaC<a id="_idIndexMarker1772"/> tools interact with individual systems using their own providers that wrap the target’s APIs. One of the more difficult aspects of this is how to secure these APIs. We’ve spent much of this book stressing the importance of short-lived tokens, which can also be applied to our IaC implementation.</li>
</ul>
<p class="normal">In addition to the above three components, many IaC tools include some kind of state management file. In the previous chapter, we described how IaC tools, like Pulumi and Terraform, generate an expected state based on your IaC tooling that then is applied to the downstream systems. This state will contain all the same privileged information as your infrastructure and <a id="_idIndexMarker1773"/>should be treated as a “secret.” For instance, if you were to provision a password for your database via IaC, your state file has a copy of that password.</p>
<p class="normal">For our deployment, we’re going to use a local state file. Pulumi offers options for storing state on remote services like S3 buckets or using its own cloud offering. While any of these options are better for management than a local file, we didn’t want you to have to sign up for anything to read this book and run the exercises, so we’re using a local file for all Pulumi state management.</p>
<div class="note">
<p class="normal">If you’ve been observant of IaC industry news, you may have seen that HashiCorp, the company that created Terraform (and Vault), changed the open source license to a “Business Source License” in the summer of 2023. This was to combat the large number of SaaS providers that were offering “Terraform as a Service” that weren’t paying anything back to HashiCorp. This change in license led to many of these SaaS providers creating OpenTofu, a fork of Terraform under the original Apache 2 license. We’re not making any judgments or recommendations on the situation, only to point out that managed services around state and controllers are where IaC companies make most of their revenue.</p>
</div>
<p class="normal">Since Pulumi is a commercial, albeit open source, package, you’ll want to follow their instructions for getting the command-line Pulumi tools installed onto the workstation you want to run as your<a id="_idIndexMarker1774"/> controller: <a href="https://www.pulumi.com/docs/install/"><span class="url">https://www.pulumi.com/docs/install/</span></a>.</p>
<p class="normal">Finally, you’ll need the chapter’s Git repository from GitHub. You can access the code for this chapter at the following GitHub repository: <a href="https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter19"><span class="url">https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter19</span></a>.</p>
<p class="normal">Now that we’ve covered the environment and requirements for our IDP build, we can dive into the deployment of our IDP.</p>
<h1 class="heading-1" id="_idParaDest-619">Deploying our IDP</h1>
<p class="normal">With our technical requirements out of the way, let’s deploy our portal! First off, I assume that you have three<a id="_idIndexMarker1775"/> running clusters. If you’ve got a <code class="inlineCode">LoadBalancer</code> solution for each, then the next step is to deploy NGINX. We didn’t include NGINX in the Pulumi tooling because, depending on how your clusters are deployed, this can change how you deploy NGINX. For example, I didn’t use typical clusters with <code class="inlineCode">LoadBalancer</code>; I just used single-node clusters and patched NGINX with host ports for <code class="inlineCode">80</code> and <code class="inlineCode">443</code>.</p>
<p class="normal">We’re also assuming you have some kind of native storage attached and have set a default <code class="inlineCode">StorageClass</code>.</p>
<p class="normal">We’re going to run NGINX assuming that it will be the Ingress for HTTP(S), MySQL, and SSH. This is pretty easy to do with the Helm chart for NGINX. On all three clusters, run the following:</p>
<pre class="programlisting con"><code class="hljs-con">helm upgrade --install ingress-nginx ingress-nginx \
  --repo https://kubernetes.github.io/ingress-nginx \
  --namespace ingress-nginx --create-namespace \
  --set tcp.3306=mysql/mysql:3306
</code></pre>
<p class="normal">This will deploy NGINX as an Ingress controller and launch a <code class="inlineCode">LoadBalancer</code>.</p>
<div class="note">
<p class="normal">If you’re using a single-node cluster, now would be the time to patch your <code class="inlineCode">Deployment</code> with something like: <code class="inlineCode">kubectl patch deployments ingress-nginx-controller -n ingress-nginx -p '{"spec":{"template":{"spec":{"containers":[{"name":"controller","ports":[{"containerPort":80,"hostPort":80,"protocol":"TCP"},{"containerPort":443,"hostPort":443,"protocol":"TCP"},{"containerPort":22,"hostPort":22,"protocol":"TCP"},{"containerPort":3306,"hostPort":3306,"protocol":"TCP"}]}]}}}}</code>.</p>
<p class="normal">This will force the ports through the NGINX deployed on your cluster. The command is in <code class="inlineCode">chapter19/scripts/patch-nginx.txt</code>.</p>
</div>
<p class="normal">Once NGINX is deployed, you’ll need DNS wildcards for all three of your <code class="inlineCode">LoadBalancer</code> IPs. It’s tempting to just use IP addresses if you don’t have access to DNS, but don’t! IP addresses can be handled in odd ways with certificate management. If you don’t have a domain name you can use, then use <code class="inlineCode">nip.io</code> the way we have throughout this book. I’m using three domains, which I’ll use in all examples:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Control Plane – </strong>*<code class="inlineCode">.idp-cp.tremolo.dev</code></li>
<li class="bulletList"><strong class="keyWord">Development Cluster –</strong> *<code class="inlineCode">.idp-dev.tremolo.dev</code></li>
<li class="bulletList"><strong class="keyWord">Production Cluster –</strong> *<code class="inlineCode">.idp-prod.tremolo.dev</code></li>
</ul>
<p class="normal">With our environment<a id="_idIndexMarker1776"/> now ready for deployment, let’s begin by creating a Pulumi virtual environment.</p>
<h2 class="heading-2" id="_idParaDest-620">Setting Up Pulumi</h2>
<p class="normal">Before we can begin running our Pulumi program to start our rollout, we first need to create a Python virtual environment. Python dynamically links to libraries in ways that can create problems and <a id="_idIndexMarker1777"/>conflict with other systems built on Python. To avoid these conflicts, which back in our Windows programming days was referred to as “DLL Hell,” you need to create a virtual environment that will be isolated for just your Pulumi program. In a new directory, run the following:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python3 -m venv .
</code></pre>
<p class="normal">This creates a virtual environment that you can now use with Pulumi without interfering with other systems. Next, we’ll need to “source” this environment so that our execution uses it:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>. ./bin/activate
</code></pre>
<p class="normal">The last Python step is to download your dependencies, assuming you’ve checked out the latest Git repository for the book:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>pip3 install -r /path/to/Kubernetes-An-Enterprise-Guide-Third-Edition/chapter19/pulumi/requirements.txt
</code></pre>
<p class="normal">The <code class="inlineCode">pip3</code> command reads all the packages named in <code class="inlineCode">requirements.txt</code> and installs them into our virtual environment. At this point, Python is ready, and we need to initialize our stack.</p>
<p class="normal">The first step to getting Pulumi ready is to “log in” to store your state file. There are multiple options, from using Pulumi’s cloud to S3 buckets to your localhost. You can see the various options on its website: <a href="https://www.pulumi.com/docs/concepts/state/"><span class="url">https://www.pulumi.com/docs/concepts/state/</span></a>. We’re going to use our local directory:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>pulumi login file://.
</code></pre>
<p class="normal">This creates a directory in <a id="_idIndexMarker1778"/>your current directory called <code class="inlineCode">./pulumi</code> that will contain your backend. Next, we need to initialize a Pulumi “stack” to track the state for your deployment:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> chapter19
<span class="hljs-con-meta">$ </span>git archive --format=tar HEAD &gt; /path/to/venv/chapter19/chapter19.tar
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> /path/to/venv/chapter19/
<span class="hljs-con-meta">$ </span>tar -xvf chapter19.tar
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">rm</span> chapter19.tar
</code></pre>
<p class="normal">Next, edit <code class="inlineCode">/path/to/venv/chapter19/pulumi/Pulumi.yaml</code>, changing <code class="inlineCode">runtime.options.virtualenv</code> to point to <code class="inlineCode">/path/to/venv</code>. Finally, we can initialize our stack:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> /path/to/venv/chapter19/pulumi
<span class="hljs-con-meta">$ </span>pulumi stack init bookv3-platform
</code></pre>
<p class="normal">You’ll be asked to provide a password to encrypt your secrets. Make sure to write it down someplace secure! You’ll now have a file called <code class="inlineCode">/path/to/venv/chapter19/pulumi/Pulumi.bookv3-platform.yaml</code> that is used to track your state.</p>
<p class="normal">Our environment is now prepped and ready to go! Next, we’ll configure our variables and start our rollout.</p>
<h2 class="heading-2" id="_idParaDest-621">Initial Deployment</h2>
<blockquote class="packt_quote">
<p class="quote">Eventual Consistency is a Lie – Ancient Cloud-Native Sith Proverb</p>
</blockquote>
<p class="normal">In the Kubernetes world, we <a id="_idIndexMarker1779"/>often assume the idea of “eventual consistency,” where we create a control loop that waits for our expected conditions to become reality. This is generally an overly simplistic outlook on systems, especially when working with enterprise systems. All this is to say that even though almost all of our deployment is managed in a single Pulumi program, it will need to be run multiple times to get the environment fully deployed. As we walk through each step, we’ll explain why it needed to be run on its own.</p>
<p class="normal">With that out of the way, we need to configure our variables. We wanted to minimize the amount of configuration, so you’ll need to set the following:</p>
<table class="table-container" id="table001-11">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Option</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Description</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Example</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">openunison.cp.dns_suffix</code></p>
</td>
<td class="table-cell">
<p class="normal">The DNS domain name of the control plane cluster</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">idp-cp.tremolo.dev</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kube.cp.context</code></p>
</td>
<td class="table-cell">
<p class="normal">The Kubernetes context for the control plane in the control plane’s kubectl configuration</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kubernetes-admin@kubernetes</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">harbor:url</code></p>
</td>
<td class="table-cell">
<p class="normal">The URL for Harbor after deployment</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">https://harbor.idp-cp.tremolo.dev</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kube.cp.path</code></p>
</td>
<td class="table-cell">
<p class="normal">The path to the kubectl configuration file for your control plane cluster</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">/path/to/idp-cp</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">harbor:username</code></p>
</td>
<td class="table-cell">
<p class="normal">The admin username for Harbor</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">Always admin</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">openunison.dev.dns_suffix</code></p>
</td>
<td class="table-cell">
<p class="normal">The DNS suffix for the development cluster</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">idp-dev.tremolo.dev</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">openunison.prod.dns_suffix</code></p>
</td>
<td class="table-cell">
<p class="normal">The DNS suffix for the production cluster</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">idp-prod.tremolo.dev</code></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 19.1: Configuration options in Kubernetes clusters</p>
<p class="normal">To make it easier, you<a id="_idIndexMarker1780"/> can customize <code class="inlineCode">chapter19/scripts/pulumi-initialize.sh</code> and run it. You can set each one of these options manually by running:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>pulumi config <span class="hljs-con-built_in">set</span> option value
</code></pre>
<p class="normal">where <code class="inlineCode">option</code> is the option you want to set and value is its <code class="inlineCode">value</code>. Finally, we can run the deployment:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> /path/to/venv/chapter19/pulumi
<span class="hljs-con-meta">$ </span>pulumi up -y
</code></pre>
<p class="normal">You’ll be asked to provide the password to decrypt your secrets. Once done, this initial deployment will take a while. Depending on the speed of your network connection and how powerful your <a id="_idIndexMarker1781"/>control plane cluster is, it could take 10 to 15 minutes.</p>
<p class="normal">Once everything is deployed, you’ll see a message like this:</p>
<pre class="programlisting con"><code class="hljs-con">.
.
.
Resources:
    + 53 created
Duration: 7m6s
</code></pre>
<p class="normal">If you look at the output of the command, you’ll see all the resources that were created! This lines up with the design we put together in the previous chapter.</p>
<div class="note">
<p class="normal">We’re not going to walk through all of the code. There are over 45,000 lines! We’ll cover the highlights after everything is deployed.</p>
</div>
<p class="normal">At this point, we have some gaps:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Vault</strong>:<strong class="keyWord"> </strong>Vault is deployed but hasn’t been configured. We can’t configure Vault until we’ve unsealed it.</li>
<li class="bulletList"><strong class="keyWord">GitLab</strong>: The baseline of GitLab has been deployed, but we don’t have a way to run workflows. We also need to generate an access token so OpenUnison can interact with it.</li>
<li class="bulletList"><strong class="keyWord">Harbor</strong>: Harbor is running, but we can’t complete SSO integration without having the Harbor admin password. We’ll also need this password for integration with OpenUnison.</li>
<li class="bulletList"><strong class="keyWord">OpenUnison</strong>: The baseline OpenUnison Namespace as a Service portal has been deployed, but we haven’t deployed any of the additional configurations needed to power our IDP.</li>
</ul>
<p class="normal">Next, let’s get Vault unsealed and ready for deployment.</p>
<h2 class="heading-2" id="_idParaDest-622">Unsealing Vault</h2>
<p class="normal">Remember how in <em class="chapterRef">Chapter 8</em>, <em class="italic">Managing Secrets</em>, we had to “unseal” Vault by extracting randomly generated keys and running a script in the pod to unlock the running Vault. There’s no easy way to do this in Pulumi, so we need to use a Bash script. We also want to be able to store the unsealed keys in a safe space because once you’ve retrieved them, you can’t get them a <a id="_idIndexMarker1782"/>second time. Thankfully, Pulumi’s secret management makes it easy to store the keys in the same place as the rest of our configuration. First, let’s unseal our Vault:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> /path/to/venv/chapter19/pulumi
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> KUBECONFIG=/path/to/cp/kubectl.conf
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> PULUMI_CONFIG_PASSPHRASE=mysecretpassword
<span class="hljs-con-meta">$ </span>../vault/unseal.sh
</code></pre>
<p class="normal">It’s important to set either <code class="inlineCode">PULUMI_CONFIG_PASSPHRASE</code> or <code class="inlineCode">PULUMI_CONFIG_PASSPHRASE_FILE</code> before running <code class="inlineCode">unseal.sh</code>. Once done, you’ll see that there are two more secrets if you run <code class="inlineCode">pulumi config</code>:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>pulumi pulumi config --show-secrets
.
.
.
vault.key  hvs.I...
vault.tokens  {
                "unseal_keys_b64": [
.
.
.
</code></pre>
<p class="normal">Now your configuration is stored in your Pulumi configuration. If you’re using a centralized configuration, such as with Pulumi Cloud or S3 buckets, this would probably be much more useful! If you need to restart your pod for whatever reason, you can unseal it again by running:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> /path/to/venv/chapter19/pulumi
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> KUBECONFIG=/path/to/cp/kubectl.conf
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> PULUMI_CONFIG_PASSPHRASE=mysecretpassword
<span class="hljs-con-meta">$ </span>../vault/unseal_after_init.sh
</code></pre>
<p class="normal">With Vault ready to be configured, next, we’ll get Harbor’s configuration ready.</p>
<h2 class="heading-2" id="_idParaDest-623">Completing the Harbor Configuration</h2>
<p class="normal">Configuring Harbor is<a id="_idIndexMarker1783"/> actually very simple:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> /path/to/venv/chapter19/pulumi
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> KUBECONFIG=/path/to/cp/kubectl.conf
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> PULUMI_CONFIG_PASSPHRASE=mysecretpassword
<span class="hljs-con-meta">$ </span>../scripts/harbor-get-root-password.sh
</code></pre>
<p class="normal">This script does two things:</p>
<ul>
<li class="bulletList">Gets the randomly generated password from the <code class="inlineCode">harbor-admin</code> secret and stores it in the Pulumi configuration</li>
<li class="bulletList">Sets a flag so our Pulumi program knows to finish the SSO configuration</li>
</ul>
<p class="normal">We had to go through this step because the Harbor provider uses configuration options specific to the <code class="inlineCode">harbor</code> namespace. This is different from our other configuration options. Let’s consider code that looks like this in your Pulumi program:</p>
<pre class="programlisting con"><code class="hljs-con">my_config = pulumi.config("someconfig")
</code></pre>
<p class="normal">Your code isn’t saying “Get the configuration called <code class="inlineCode">someconfig</code>"; it’s saying “Get the configuration called <code class="inlineCode">someconfig</code> in my stack’s namespace.” The separation between namespaces means that our code can’t get configuration information from another namespace. From a practical standpoint, this means we’re defining the same information multiple times between <code class="inlineCode">harbor:url</code> and <code class="inlineCode">harbor:password</code>, as well as <code class="inlineCode">harbor:username</code>.</p>
<p class="normal">This approach seems inefficient and error-prone in our scenario, but at scale, it makes for a great way to secure separate silos. In many deployments, the people who own Harbor aren’t the same people who might own the automation. By not allowing our code to have access to the <code class="inlineCode">harbor</code> namespace, but being able to call libraries that depend on it, we’re able to use this secret data without ever actually knowing it! Of course, since we’re using a single secret set that we have access to, this security benefit is negated. However, if you’re using a centrally managed service for your Pulumi controller, it allows developers to write code that never knows the secret data it relies upon.</p>
<p class="normal">Now that Harbor is ready for its final configuration, we need to run some manual steps in GitLab. We’ll do that in the next section.</p>
<h2 class="heading-2" id="_idParaDest-624">Completing the GitLab Configuration</h2>
<p class="normal">There are two key components we’re missing from GitLab. First, we need to generate a token for OpenUnison to use when automating GitLab. The other is we need to manually configure a <a id="_idIndexMarker1784"/>runner. Later on, we’re going to use GitLab’s integrated workflows to build a container and push it into Harbor. GitLab does this by launching a pod, which requires some automation. The service that launches these pods needs to be registered with GitLab. This makes sense because you might want to run services on a local Kubernetes cluster or a remote cloud. To handle this scenario, we need to tell GitLab to generate a runner and give us a registration token. First, we’ll generate the runner registration token.</p>
<h3 class="heading-3" id="_idParaDest-625">Generating a GitLab Runner</h3>
<p class="normal">The first step is to log in to<a id="_idIndexMarker1785"/> GitLab. We haven’t configured SSO yet, so you’ll need to log in with the root credentials. These are stored as a Secret in the <code class="inlineCode">GitLab</code> namespace that ends with <code class="inlineCode">gitlab-initial-root-password</code>. Once you have the password, log in to GitLab with the username <code class="inlineCode">root</code>. The URL will be <code class="inlineCode">https://gitlab.controlplane.dns.suffix</code>, where <code class="inlineCode">controlplane.dns.suffix</code> is the DNS suffix for your control plane cluster. For me, the URL is <code class="inlineCode">https://gitlab.idp-cp.tremolo.dev/</code>.</p>
<p class="normal">Once logged in, click on <strong class="screenText">Admin Area</strong> in the lower left-hand corner:</p>
<figure class="mediaobject"><img alt="Graphical user interface, website  Description automatically generated" height="522" src="../Images/B21165_19_01.png" width="877"/></figure>
<p class="packt_figref">Figure 19.1: GitLab main screen</p>
<p class="normal">Next, expand <strong class="screenText">CI/CD</strong> and <a id="_idIndexMarker1786"/>click on <strong class="screenText">Runners</strong>:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application, Teams  Description automatically generated" height="522" src="../Images/B21165_19_02.png" width="876"/></figure>
<p class="packt_figref">Figure 19.2: GitLab Admin Area</p>
<p class="normal">Once the screen loads, click on <strong class="screenText">New instance runner</strong>:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="452" src="../Images/B21165_19_03.png" width="877"/></figure>
<p class="packt_figref">Figure 19.3: GitLab Runners</p>
<p class="normal">When the <strong class="screenText">New instance runner</strong> screen loads, check <strong class="screenText">Run untagged jobs</strong> since we’re only running jobs on<a id="_idIndexMarker1787"/> our own cluster. You can use these tags to manage running jobs across multiple platforms, similar to how you can use node tags to manage where to run workloads in Kubernetes. Next, click <strong class="screenText">Create runner</strong>:</p>
<figure class="mediaobject"><img alt="Graphical user interface, text, application, email  Description automatically generated" height="487" src="../Images/B21165_19_04.png" width="878"/></figure>
<p class="packt_figref">Figure 19.4: GitLab new instance runner</p>
<p class="normal">Finally, you’ll have<a id="_idIndexMarker1788"/> a token that we can configure in our Pulumi configuration:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application, Teams  Description automatically generated" height="497" src="../Images/B21165_19_05.png" width="875"/></figure>
<p class="packt_figref">Figure 19.5: New runner token</p>
<p class="normal">Copy this token and configure it in Pulumi:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> /path/to/venv/chapter19/pulumi
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> PULUMI_CONFIG_PASSPHRASE=mysecretpassword
<span class="hljs-con-meta">$ </span>pulumi config <span class="hljs-con-built_in">set</span> gitlab.runner.token \
  <span class="hljs-con-string">'glrt-Y-fSdvy_6_xgXcyFW_PW'</span> --secret
</code></pre>
<p class="normal">Next, we’ll configure<a id="_idIndexMarker1789"/> a token for automating GitLab.</p>
<h3 class="heading-3" id="_idParaDest-626">Generating a GitLab Personal Access Token</h3>
<p class="normal">In the previous<a id="_idIndexMarker1790"/> section, we configured a runner. Next, we need a token so that OpenUnison can automate provisioning into GitLab. Unfortunately, GitLab doesn’t provide an alternative to tokens. You can make the token expire regularly, but you’ll need to replace it. That said, while logged in to GitLab as root, click on the colorful icon in the upper left-hand corner and click <strong class="screenText">Preferences</strong>:</p>
<figure class="mediaobject"><img alt="Graphical user interface, website  Description automatically generated" height="461" src="../Images/B21165_19_06.png" width="877"/></figure>
<p class="packt_figref">Figure 19.6: GitLab preferences</p>
<p class="normal">Once the <strong class="screenText">Preferences</strong> screen loads, click on <strong class="screenText">Access Tokens</strong> and then <strong class="screenText">Add new token</strong>:</p>
<figure class="mediaobject"><img alt="Graphical user interface, text, application, email  Description automatically generated" height="559" src="../Images/B21165_19_07.png" width="877"/></figure>
<p class="packt_figref">Figure 19.7: GitLab access tokens</p>
<p class="normal">When the new<a id="_idIndexMarker1791"/> token screen loads, you need to give it a name and an expiration and click on the <strong class="screenText">api</strong> option to give it full access. Once that’s done, click on the <strong class="screenText">Create personal access token</strong> button at the bottom of the screen:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="852" src="../Images/B21165_19_08.png" width="876"/></figure>
<p class="packt_figref">Figure 19.8: Create a new GitLab personal access token</p>
<p class="normal">Once the token is <a id="_idIndexMarker1792"/>generated, the last step is to copy it and then configure it as a Pulumi configuration secret:</p>
<figure class="mediaobject"><img alt="Graphical user interface, text, application, email  Description automatically generated" height="514" src="../Images/B21165_19_09.png" width="878"/></figure>
<p class="packt_figref">Figure 19.9: GitLab personal access token</p>
<p class="normal">Copy the token <a id="_idIndexMarker1793"/>and set it in the Pulumi configuration:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> /path/to/venv/chapter19/pulumi
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> PULUMI_CONFIG_PASSPHRASE=mysecretpassword
<span class="hljs-con-meta">$ </span>pulumi config <span class="hljs-con-built_in">set</span> gitlab.root.token \
  <span class="hljs-con-string">'glpat-suHtqupeNetAsYfGwVyz'</span> --secret
</code></pre>
<p class="normal">We’ve now finished the extra steps needed to complete the control plane’s configuration. Next, we’ll complete the control plane rollout in our Pulumi program.</p>
<h2 class="heading-2" id="_idParaDest-627">Finishing the Control Plane Rollout</h2>
<p class="normal">The next step is to <a id="_idIndexMarker1794"/>rerun our Pulumi program to complete the integrations:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> /path/to/venv/chapter19/pulumi
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> PULUMI_CONFIG_PASSPHRASE=mysecretpassword
<span class="hljs-con-meta">$ </span>pulumi up -y
</code></pre>
<p class="normal">This should take less time than the initial run but will still take a few minutes. OpenUnison needs to be rolled out again with the new configuration options, which will take the longest amount of time.</p>
<p class="normal">Once the rollout is done, we have one more task to complete on our control plane. We need to update NGINX to forward SSH on port <code class="inlineCode">22</code> to the GitLab shell <code class="inlineCode">Service</code>. We can do this by getting the name of the shell <code class="inlineCode">Service</code> in the <code class="inlineCode">GitLab</code> namespace and updating our control plane NGINX:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> chapter19/scripts
<span class="hljs-con-meta">$ </span>./patch_nginx_ssh.sh
</code></pre>
<p class="normal">Once NGINX is running<a id="_idIndexMarker1795"/> again, you should be able to ssh into GitLab. At this point, you can log in to OpenUnison by accessing <code class="inlineCode">https://k8sou.idp-cp.tremolo.dev</code> (replace <code class="inlineCode">idp-cp.tremolo.dev</code> with your control plane suffix) and log in with the username <code class="inlineCode">mmosley</code> and the password <code class="inlineCode">start123</code>:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="588" src="../Images/B21165_19_10.png" width="877"/></figure>
<p class="packt_figref">Figure 19.10: OpenUnison Main Page</p>
<p class="normal">We haven’t finished integrating our dev or production systems yet, but you should login to Vault, GitLab, Harbor, ArgoCD, and the control plane Kubernetes using SSO with OpenUnison. Since you’re the first person to log in, you are automatically both the control plane cluster administrator and the top-level approver.</p>
<p class="normal">With the control <a id="_idIndexMarker1796"/>plane configured, the last step in Pulumi is to onboard the development and production clusters, which we’ll cover next.</p>
<h2 class="heading-2" id="_idParaDest-628">Integrating Development and Production</h2>
<p class="normal">So far, we’ve spent <a id="_idIndexMarker1797"/>all our time on the control plane. There won’t be any user workloads here, however. Our tenants will be on the development and production clusters. For our automation plan to work, we’re going to need to integrate these clusters into OpenUnison so that we’re using short-lived tokens for all automation API calls. Thankfully, OpenUnison already has everything we need, and it’s been integrated into the OpenUnison Helm charts.</p>
<p class="normal">The first step is to deploy NGINX to each cluster. We’re going to port-forward port <code class="inlineCode">3306</code> for MySQL as well so that OpenUnison on the control plane can talk to MySQL on each cluster. While we could have used the control plane’s MySQL for vClusters, we don’t want to be in a situation where a problem on the control plane takes down either development or production. By running MySQL on each cluster, an outage on one doesn’t stop the operations for another. Run the following:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> KUBECONFIG=/path/to/idp-dev.conf
<span class="hljs-con-meta">$ </span>helm upgrade --install ingress-nginx ingress-nginx \
  --repo https://kubernetes.github.io/ingress-nginx \
  --namespace ingress-nginx --create-namespace \
  --<span class="hljs-con-built_in">set</span> tcp.3306=mysql/mysql:3306
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> KUBECONFIG=/path/to/idp-prod.conf
<span class="hljs-con-meta">$ </span>helm upgrade --install ingress-nginx ingress-nginx \
  --repo https://kubernetes.github.io/ingress-nginx \
  --namespace ingress-nginx --create-namespace \
  --<span class="hljs-con-built_in">set</span> tcp.3306=mysql/mysql:3306
</code></pre>
<p class="normal">Once running, you’ll be able to update the configuration for Pulumi. </p>
<table class="table-container" id="table002-8">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Option</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Description</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Example</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kube.dev.path</code></p>
</td>
<td class="table-cell">
<p class="normal">The path to the kubectl configuration file for your development cluster</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">/path/to/idp-dev</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kube.dev.context</code></p>
</td>
<td class="table-cell">
<p class="normal">The Kubernetes context for the development cluster in its kubectl configuration</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kubernetes-admin@kubernetes</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kube.prod.path</code></p>
</td>
<td class="table-cell">
<p class="normal">The path to the kubectl configuration file for your production cluster</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">/path/to/idp-prod</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kube.prod.context</code></p>
</td>
<td class="table-cell">
<p class="normal">The Kubernetes context for the prod cluster in its kubectl configuration</p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kubernetes-admin@kubernetes</code></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 19.2: Configuration options for Kubernetes and paths in development and production clusters</p>
<p class="normal">Once your configuration is <a id="_idIndexMarker1798"/>added, we can finish our Pulumi rollout:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> /path/to/venv/chapter19/pulumi
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> PULUMI_CONFIG_PASSPHRASE=mysecretpassword
<span class="hljs-con-meta">$ </span>pulumi up -y
</code></pre>
<p class="normal">This will take a few minutes to run, but once we’re done, we’ll have a final step in OpenUnison to finish the rollout. If all goes well, you should see something like:</p>
<pre class="programlisting con"><code class="hljs-con">Resources:
    + 46 created
    ~ 2 updated
    +-1 replaced
    49 changes. 68 unchanged
Duration: 4m59s
</code></pre>
<p class="normal">Congratulations, our infrastructure is deployed! Three clusters, a dozen systems, all integrated! Next, we’ll use OpenUnison’s workflows to finish the last integration steps.</p>
<h2 class="heading-2" id="_idParaDest-629">Bootstrapping GitOps with OpenUnison</h2>
<p class="normal">We’ve deployed several <a id="_idIndexMarker1799"/>systems to support our GitOps workflows, and we’ve integrated them via SSO so that users can log in, but we haven’t <a id="_idIndexMarker1800"/>started the GitOps bootstrapping <a id="_idIndexMarker1801"/>process. What we mean by “bootstrapping” in this context is to set up some initial repositories in GitLab, integrate them into Argo CD, and make it so they sync to the control plane, development, and production clusters. This way, as we add new tenants, we’ll do so by creating manifests in Git instead of writing directly to the API servers of our clusters. We’ll still write to the API servers for ephemeral objects, like <code class="inlineCode">Jobs</code> we’ll be using to deploy vClusters and integrate them with our control plane, but otherwise, we want to write everything into Git.</p>
<p class="normal">We’re going to do this last step in OpenUnison instead of Pulumi. You may be wondering why we would use OpenUnison for this part when we could have used Pulumi. That was the original plan, but unfortunately, a known bug with Pulumi’s GitLab provider kept us from being able to create groups in the GitLab Community Edition. Since OpenUnison’s workflow <a id="_idIndexMarker1802"/>engine has this capability already, and<a id="_idIndexMarker1803"/> this is a step that would only ever be run once, we decided to just do it in OpenUnison’s workflow engine.</p>
<p class="normal">With that said, log in to <a id="_idIndexMarker1804"/>OpenUnison using the instructions from the last section. Next, click on <strong class="screenText">Request Access</strong> on the left-hand side, choose <strong class="screenText">Kubernetes Administration</strong>, and add the development and production clusters by adding <strong class="screenText">Kubernetes-prod Cluster Administrator </strong>and <strong class="screenText">Kubernetes-dev Cluster Administrator</strong> to your cart:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="392" src="../Images/B21165_19_11.png" width="876"/></figure>
<p class="packt_figref">Figure 19.11: OpenUnison request access to Dev and Prod cluster administration</p>
<p class="normal">Once added to your cart, click on the new <strong class="screenText">Checkout</strong> menu option on the left, add a reason for the request, and<a id="_idIndexMarker1805"/> click on <strong class="screenText">SUBMIT YOUR REQUESTS</strong>.</p>
<figure class="mediaobject"><img alt="Graphical user interface, application, Teams  Description automatically generated" height="539" src="../Images/B21165_19_12.png" width="876"/></figure>
<p class="packt_figref">Figure 19.12: Submit access request for cluster administration</p>
<p class="normal">When you refresh your screen, you’ll now see you have two open requests. Act on them just as you did in <em class="chapterRef">Chapter 9</em>, <em class="italic">Building Multitenant Clusters with vClusters</em>, and log out. When you log back in, you’ll <a id="_idIndexMarker1806"/>have access to both the development and production clusters as well as the<a id="_idIndexMarker1807"/> control plane.</p>
<p class="normal">Once you’re logged back in, go back to <strong class="screenText">Request Access</strong>, click on <strong class="screenText">OpenUnison Internal Management Workflows</strong>, and add <strong class="screenText">Initialize OpenUnison</strong> to your cart. Check it out of your cart with a reason, just as before. This time, there won’t be an approval step. It will take a few minutes, but once it is done, you can log in to Argo CD and you’ll see three new projects:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="541" src="../Images/B21165_19_13.png" width="879"/></figure>
<p class="packt_figref">Figure 19.13: Argo CD after the OpenUnison initialization</p>
<p class="normal">The state is <strong class="screenText">Unknown</strong><strong class="screenText"><a id="_idIndexMarker1808"/></strong> because we haven’t yet trusted<a id="_idIndexMarker1809"/> the keys from GitLab’s ssh service. Download the Argo CD command-line utility using your favorite method and run<a id="_idIndexMarker1810"/> the following:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>argocd login --grpc-web \
  --sso argocd.idp-cp.tremolo.dev
<span class="hljs-con-meta">$ </span>ssh-keyscan gitlab-ssh.idp-cp.tremolo.dev \
  | argocd cert add-ssh --batch
</code></pre>
<p class="normal">This will add the correct keys to Argo CD. After a few minutes, you’ll see that our applications in Argo CD are syncing! Now is a good time to look around both Argo CD and GitLab. You’ll see how the basic scaffolding of our GitOps infrastructure will look. You can also look at the onboarding workflow in <code class="inlineCode">chapter19/pulumi/src/helm/kube-enterprise-guide-openunison-idp/templates/workflows/initialization/init-openunison.yaml</code>. I think the most important thing you’ll see is how we tie everything together via identity. This is something that is too often overlooked in the DevOps world. Especially in Argo CD, we create an <code class="inlineCode">AppProject</code> that constrains which repositories and clusters can be added. We then create a <code class="inlineCode">Secret</code> for each cluster, but the <code class="inlineCode">Secret</code> doesn’t contain any secret data. Finally, we generate <code class="inlineCode">ApplicationSets</code> to generate the <code class="inlineCode">Application</code> objects. We’ll follow this pattern again when we deploy our tenants.</p>
<p class="normal">You now have a working multitenant IDP! It took about 30 pages of explanation, probably a few hours <a id="_idIndexMarker1811"/>of cluster design and<a id="_idIndexMarker1812"/> setup, and several thousand lines of automation, but you have it! Next, it’s time to deploy a <a id="_idIndexMarker1813"/>tenant!</p>
<h1 class="heading-1" id="_idParaDest-630">Onboarding a Tenant</h1>
<p class="normal">So far, we’ve spent <a id="_idIndexMarker1814"/>all our time setting up our infrastructure. We have a Git repository, clusters for our control plane as well as development and production, a GitOps controller, a secrets manager, and an SSO and automation system. Now we can build out our first tenant! The good news is this part is pretty easy and doesn’t require any command-line tools. Like how we onboarded a vCluster in <em class="chapterRef">Chapter 9</em>, <em class="italic">Building Multitenant Clusters with vClusters</em>, we’re going to use OpenUnison as our portal for requesting and approving the new tenant. If you’re already logged in to OpenUnison, log out and log back in, but this time, with the username <code class="inlineCode">jjackson</code> and the password <code class="inlineCode">start123</code>. You’ll notice you have much fewer badges on the main page because you don’t have access to anything yet! We’ll fix that by creating a new tenant. Click on the <strong class="screenText">New Kubernetes Namespace</strong> badge:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="446" src="../Images/B21165_19_14.png" width="879"/></figure>
<p class="packt_figref">Figure 19.14: Log in to OpenUnison with jjackson</p>
<p class="normal">Once it’s open, use the <a id="_idIndexMarker1815"/>name <code class="inlineCode">myapp</code> with the reason <code class="inlineCode">new application</code> and then hit <strong class="screenText">SAVE</strong>:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application, Teams  Description automatically generated" height="258" src="../Images/B21165_19_15.png" width="877"/></figure>
<p class="packt_figref">Figure 19.15: New Project Screen</p>
<p class="normal">Once saved, log out and log back in, but this time, as <code class="inlineCode">mmosley</code> with the password <code class="inlineCode">start123</code>. You’ll see there’s an<a id="_idIndexMarker1816"/> open approval. Click on <strong class="screenText">Open Approvals</strong>, <strong class="screenText">ACT ON REQUEST</strong>, provide a justification, and click <strong class="screenText">APPROVE REQUEST</strong>. When the button turns turquoise, click <strong class="screenText">CONFIRM APPROVAL</strong>. This is going to take a while. It’s not that the steps are very computationally expensive. The time is mostly waiting for systems to sync. That’s because we’re not writing directly to our cluster’s API servers but are instead creating manifests in our GitLab deployment that are then synced into the clusters by Argo CD. We’re also deploying multiple vClusters and OpenUnisons, which also takes time. If you want to watch the progress, you can watch the logs in the <code class="inlineCode">openunison-orchestra</code> pod on the control plane. This can take 10 to 15 minutes to fully roll out. We don’t have a working email server, so you’ll know when the workflow is done by logging in to Argo CD and you’ll see two new applications: one for our development vCluster and one for our production vCluster:</p>
<figure class="mediaobject"><img alt="Graphical user interface, text, application  Description automatically generated" height="491" src="../Images/B21165_19_16.png" width="879"/></figure>
<p class="packt_figref">Figure 19.16: Argo CD after the new tenant is deployed</p>
<p class="normal">If you click on<a id="_idIndexMarker1817"/> the <code class="inlineCode">myapp/k8s-myapp-prod</code> application, you’ll see we don’t have much synchronized in:</p>
<figure class="mediaobject"><img alt="Graphical user interface, text, application, chat or text message  Description automatically generated" height="497" src="../Images/B21165_19_17.png" width="879"/></figure>
<p class="packt_figref">Figure 19.17: Production tenant Argo CD application</p>
<p class="normal">What we’ve created at <a id="_idIndexMarker1818"/>this point is a <code class="inlineCode">ServiceAccount</code> that can communicate with Vault and an <code class="inlineCode">ExternalSecret</code> that syncs the pull secret we generated for Harbor into the default namespace. Next, we’ll want to look at our new tenant! Make sure to log out of everything, or just open an incognito/private window in a different browser and log in as <code class="inlineCode">jjackson</code> again.</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="454" src="../Images/B21165_19_18.png" width="876"/></figure>
<p class="packt_figref">Figure 19.18: OpenUnison as jjackson after tenant deployment</p>
<p class="normal">You’ll see that we now have badges for our cluster management apps. When you log in to them, you’ll see that your view is limited. Argo CD only lets you interact with the <code class="inlineCode">Application</code> objects for your tenant. GitLab only lets you see the projects associated with your tenant. Harbor only lets you see the images for your tenant, and finally, Vault only lets you interact with the secrets for your tenant. If you look in the <code class="inlineCode">myapp-dev</code> and <code class="inlineCode">myapp-prod</code> sections of the tree at the top of the screen, you’ll see you now have tokens and a dashboard for <a id="_idIndexMarker1819"/>your two vClusters too. Isn’t identity amazing?</p>
<p class="normal">So far, we’ve built out a tremendous amount of infrastructure and created a tenant using a common identity, allowing each system’s own policy-based system to determine how to draw boundaries. Next, we’ll deploy an application to our tenant.</p>
<h1 class="heading-1" id="_idParaDest-631">Deploying an Application</h1>
<p class="normal">We’ve built out<a id="_idIndexMarker1820"/> quite a bit of infrastructure to support our multitenant platform and now have a tenant to run our application in place. Let’s go ahead and deploy our application!</p>
<p class="normal">If we log in to GitLab as <code class="inlineCode">jjackson</code>, we’ll see there are three projects:</p>
<ul>
<li class="bulletList"><strong class="keyWord">myapp-prod/myapp-application</strong>:<strong class="keyWord"> </strong>This repository will store the code for our application and the build to generate our container.</li>
<li class="bulletList"><strong class="keyWord">myapp-dev/myapp-ops</strong>: The repository for the manifests for our development cluster.</li>
<li class="bulletList"><strong class="keyWord">myapp-prod/myapp-ops</strong>: Where the production cluster’s manifests are stored.</li>
</ul>
<p class="normal">There’s no direct fork from the development project to the production project. That was our original intent, but that stringent path from development to production doesn’t work well. Development environments and production environments are rarely the same and often have different owners of infrastructure. For example, I maintain a public safety identity provider where our development environment doesn’t have trust established with all the jurisdictions<a id="_idIndexMarker1821"/> that our production environment does. To address this, we set up an additional system to stand in for those identity providers. These variances make it difficult to have a direct line to automatically merge changes from development into production.</p>
<p class="normal">With that said, let’s build out our application. The first step is to generate an SSH key and add it to <code class="inlineCode">jjackson</code>'s profile in GitLab so we can check the code in. Once you have your SSH key updated, clone the <code class="inlineCode">myapp-dev/myapp-ops</code> project. This project has the manifests for your development cluster. You’ll see there are already manifests to support synchronizing the pull secret for Harbor into the default namespace. Create a folder called <code class="inlineCode">yaml/namespaces/default/deployments</code> and add <code class="inlineCode">chapter19/examples/ops/python-hello.yaml</code> to it. Commit and push to your GitLab. Within a few minutes, Argo CD will attempt to sync it, which will fail because the image points to something that doesn’t exist. That’s OK. Next, we’ll take care of that.</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="494" src="../Images/B21165_19_19.png" width="878"/></figure>
<p class="packt_figref">Figure 19.19: Broken Argo CD Rollout</p>
<p class="normal">The image tag in our deployment will be updated upon a successful build of our application. This gives us the automation we’re looking for and the ability to easily roll back if we need to. Our<a id="_idIndexMarker1822"/> application is a simple Python web service. Clone the <code class="inlineCode">myapp-prod/myapp-application</code> project and copy it in the <code class="inlineCode">chapter19/examples/myapp</code> folder. There are two items to make note of:</p>
<ul>
<li class="bulletList"><strong class="keyWord">source</strong>:<strong class="keyWord"> </strong>This folder contains our Python source code. It’s not very important and we won’t spend any time on it.</li>
<li class="bulletList"><strong class="keyWord">.gitlab-ci.yml</strong>:<strong class="keyWord"> </strong>This is the GitLab build script that’s responsible for generating a Docker container, pushing it into Harbor, then patching our Deployment in Git.</li>
</ul>
<p class="normal">If you look at the <code class="inlineCode">.gitlab-ci.yml</code> file, you might notice that it looks similar to the Tekton tasks we built in previous editions. What’s similar about GitLab’s pipelines is that each stage is run as a pod in our cluster. We have two stages. The first builds our container and the second deploys it.</p>
<p class="normal">The build stage uses a tool from Google<a id="_idIndexMarker1823"/> called <strong class="keyWord">Kaniko</strong> (<a href="https://github.com/GoogleContainerTools/kaniko"><span class="url">https://github.com/GoogleContainerTools/kaniko</span></a>) for building and pushing Docker images without needing to interact with a Docker daemon. This means our build container doesn’t require a privileged container and makes it easier to secure our build environment. Kaniko uses a Docker configuration to manage credentials. If you are using AWS or any of the other major clouds, you can integrate directly with their IAM solution. In this instance, we’re using Harbor, so our OpenUnison workflow provisioned a Docker <code class="inlineCode">config.json</code> as a variable into the GitLab project. The build process uses the short version of the Git SHA hash as a tag. This way, we can track each container to the build and the commit that produced it.</p>
<p class="normal">The second stage of the build is<a id="_idIndexMarker1824"/> the deployment. This stage first checks out our <code class="inlineCode">myapp-dev/myapp-ops</code> project, patches our <code class="inlineCode">Deployment</code> with the correct image URL, and then commits and pushes it back to GitLab. Unfortunately, neither GitLab nor GitHub makes it easy to check out code using the identity of workflow. To make this work, our OpenUnison onboarding workflow created a “deployment key” for our Ops project, marking it as writeable, and then added the private key to the app project. This way, the app project can configure SSH to allow for cloning the DevOps repository, generating the patch, and then committing and pushing. Once completed, Argo CD will detect the change within a few minutes and synchronize it into your development vCluster.</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="508" src="../Images/B21165_19_20.png" width="878"/></figure>
<p class="packt_figref">Figure 19.20: Argo CD with a working synced Deployment</p>
<p class="normal">Assuming everything went well, you now have an automatically updated development environment! You’re now ready to run any automated tests before promoting to production. Since we don’t currently <a id="_idIndexMarker1825"/>have an automated process for this, let’s explore how we can approach it.</p>
<h2 class="heading-2" id="_idParaDest-632">Promoting to Production</h2>
<p class="normal">So far, we’ve deployed our<a id="_idIndexMarker1826"/> application into our development vCluster. What about our production vCluster? There’s no direct relationship between the <code class="inlineCode">myapp-dev/myapp-ops</code> project in GitLab and the <code class="inlineCode">myapp-prod/myapp-ops</code> projects. If you read the first two editions of this book, you might remember that the dev project was a fork of the prod project. You would promote a container from development to production by submitting a merge request (GitLab’s version of a pull request) and, once approved, the changes in development would be merged into production allowing Argo CD to synchronize them into our cluster. The problem with this approach is it assumes dev and prod are the exact same, and that’s never true. Something as simple as a different hostname for a database would have broken this approach. We needed to break this pattern to make our cluster usable.</p>
<p class="normal">For our own lab, the easiest way to go is to create the <code class="inlineCode">yaml/namespaces/default/deployments</code> directory in the <code class="inlineCode">myapp-prod/myapp-ops</code> project, add <code class="inlineCode">chapter19/examples/ops/python-hello.yaml</code> to it, and update the image to point to our current image in Harbor. This is not a well-automated process, but it would work. Eventually, Argo CD will finish synchronizing the manifests and our service will be running on our production cluster.</p>
<p class="normal">If you’re looking to automate this more, you have multiple options:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Create GitLab workflows</strong>: A workflow could be used to automate the updates in a similar way as production. You would need a way to trigger it, but this solution works nicely because you can leverage GitOps to track the rollouts.</li>
<li class="bulletList"><strong class="keyWord">Create custom jobs or scripts</strong>: We’re running Kubernetes, so there are multiple ways to create batch jobs to run the upgrade process. Depending on how you choose to run batch jobs in Kubernetes, this can also be done using GitOps.</li>
<li class="bulletList"><strong class="keyWord">Akuity Kargo</strong>:<strong class="keyWord"> </strong>A new project focused on solving this problem in a consistent way.</li>
</ul>
<p class="normal">The first two options are variations on the same theme: a customized rollout script. There’s nothing that says this is an antipattern. It just seems there’s probably a better way, or at least a more consistent way, to do this. Enter the Akuity Kargo project (<a href="https://github.com/akuity/kargo"><span class="url">https://github.com/akuity/kargo</span></a>), not to be confused with the Container Craft Kargo project! Akuity was founded by many of the original developers of Argo CD. The Kargo project is not under the Argo umbrella; it’s completely separate. It takes an interesting approach to automating the process of syncing across repositories to promote systems across <a id="_idIndexMarker1827"/>environments. We originally thought we’d integrate this tool directly into our cluster, but it’s still pre-version-1, so we decided to give it a mention instead. It’s certainly a project we’ll be keeping our eyes on!</p>
<p class="normal">Now that we’ve rolled out our application to production, what can we do next? We can add more users of course! We’ll explore that and how we can expand on our platform next.</p>
<h1 class="heading-1" id="_idParaDest-633">Adding Users to a Tenant</h1>
<p class="normal">Now that we have our<a id="_idIndexMarker1828"/> tenant created and our application deployed, it might be a good time to look at how we add new users. The good news is that we can add more users! OpenUnison makes it easy for team members to request access. Log in to OpenUnison, click on <strong class="screenText">Request Access</strong>, and pick the application and the role you want. The application owner will be responsible for approving that access. The great thing about this approach is that the cluster owners never get involved. It’s entirely up to the application owners who have access to their system. That access is then provisioned, just in time, into all of the components of your platform.</p>
<figure class="mediaobject"><img alt="Graphical user interface, application, Teams  Description automatically generated" height="404" src="../Images/B21165_19_21.png" width="876"/></figure>
<p class="packt_figref">Figure 19.21: Adding users to application roles in OpenUnison</p>
<p class="normal">We’ve deployed our platform, and we know how to deploy a tenant and how to add new members to<a id="_idIndexMarker1829"/> that tenant. What can we do to improve our new platform? Where are the gaps? We’ll walk through those next.</p>
<h1 class="heading-1" id="_idParaDest-634">Expanding Our Platform</h1>
<p class="normal">We’ve covered quite a<a id="_idIndexMarker1830"/> bit in the last two chapters to build out a multitenant platform. We walked through how GitOps works, different strategies, and how IaC tools like Pulumi make automation easier. Find finally, we built out our multi-tenant platform over three clusters. Our platform includes Git and builds using GitLab, secrets management using Vault, GitOps with Argo CD, a Docker registry in Harbor, and finally, it’s all integrated via identity using OpenUnison. That’s it, right? No, unfortunately not. This section will cover some of the gaps or areas where our platform can be built out. First, we’ll start with identity.</p>
<h2 class="heading-2" id="_idParaDest-635">Different Sources of Identity</h2>
<p class="normal">One area we have taken a really focused view on throughout this book is how a user’s identity crosses various boundaries of the systems that make up our clusters. In this platform, we use our Active Directory for user authentication and use OpenUnison’s internal groups for authorization. Similar to <em class="chapterRef">Chapter 9</em>, we could also integrate our enterprise’s groups for <a id="_idIndexMarker1831"/>authorization. We can also expand outside of Active Directory to use Okta, Entra ID (formerly Azure AD), GitHub, etc. A great addition is to integrate multi-factor authentication. We’ve said it multiple times throughout this book, but it bears repeating: multi-factor authentication is one of the easiest ways to help lock down your environment!</p>
<p class="normal">Having looked at how else to identify users, let’s look at monitoring and logging.</p>
<h2 class="heading-2" id="_idParaDest-636">Integrating Monitoring and Logging</h2>
<p class="normal">In <em class="chapterRef">Chapter 15</em>, <em class="italic">Monitoring Clusters and Workloads</em>, we learned how to monitor Kubernetes with Prometheus and <a id="_idIndexMarker1832"/>aggregate logs using OpenSearch. We didn’t integrate either of these systems into our platform. We did this for a few reasons:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Simplicity</strong>:<strong class="keyWord"> </strong>These last two chapters were complex enough; they didn’t need more stuff to integrate!</li>
<li class="bulletList"><strong class="keyWord">Lack of multi-tenancy</strong>:<strong class="keyWord"> </strong>Prometheus has no concept of identity and the Grafana Community edition only allows two roles. It does appear that OpenSearch supports multi-tenancy, but that would have required considerable engineering.</li>
<li class="bulletList"><strong class="keyWord">Complexity with vCluster</strong>: This is similar to the multi-tenancy issue; would we have had a Prometheus for each vCluster? There are ways to do this, but they would likely require their own book.</li>
</ul>
<p class="normal">Given that this solution is designed for a production environment, you would want to integrate some kind of monitoring and logging tied directly into both your host clusters and your vClusters. That can get complex, but would be well worth it. Imagine an application owner being able to view all their logs from a single location without having to log in to a cluster. The tools are all there, it’s just a matter of integration!</p>
<p class="normal">We know it’s important to monitor a cluster, but let’s next talk about policy management.</p>
<h2 class="heading-2" id="_idParaDest-637">Integrating Policy Management</h2>
<p class="normal">This book covered two chapters on policy management and included authorization management in our chapter on Istio. It’s an important aspect of Kubernetes that we didn’t include in our platform. We also spent a chapter on runtime security. We decided not to include policy management <a id="_idIndexMarker1833"/>because while it would be needed for production, it didn’t provide any additional benefit for the lab itself. It should definitely be included for any production deployment, though!</p>
<p class="normal">Now that we’ve covered which technologies we didn’t include in our platform, let’s talk about what you could replace.</p>
<h2 class="heading-2" id="_idParaDest-638">Replacing Components</h2>
<p class="normal">We chose the components we did for our platform because we wanted to show how to build off what we learned<a id="_idIndexMarker1834"/> throughout the book. We made a conscious decision to use only open source projects and to avoid any kind of service that would require a sign-up or trial. With that said, you could replace Vault with a<a id="_idIndexMarker1835"/> service like <strong class="keyWord">AKeyLess</strong>, replace GitLab with GitHub, etc. It’s your environment, so deploy what works best for you!</p>
<p class="normal">The story is far from over on our platform, but this does bring us to the end of our book!</p>
<h1 class="heading-1" id="_idParaDest-639">Summary</h1>
<p class="normal">This chapter covered considerable ground. We started by looking at how to build out three Kubernetes clusters to support our platform. Once we had our clusters, we deployed <code class="inlineCode">cert-manager</code>, Argo CD, MySQL, GitLab, Harbor, Vault, and OpenUnison via Pulumi, integrating them all. With our platform in place, we deployed a tenant to see how to use automation and GitOps to simplify our management, and finally, talked about different ways to update and adjust our multi-tenant platform. That’s quite a lot of ground to cover in just one chapter!</p>
<p class="normal">I want to say thank you to Kat Morgan for writing the Pulumi code I used as the starting point for this chapter and helping me out when I ran into issues.</p>
<p class="normal">Finally, <em class="italic">thank you!</em> Through 19 chapters and dozens of different technologies, labs, and systems, you joined us on our fantastic journey through the enterprise cloud-native landscape. We hope you had as much fun reading and working with this book as we did writing it. Please, if you run into issues or just want to say hi, open an issue on our GitHub repository. If you have thoughts or ideas, that would be great as well! We cannot say this enough, but once again, thank you for joining us!</p>
<h1 class="heading-1" id="_idParaDest-640">Questions</h1>
<ol>
<li class="numberedList" value="1">Kubernetes has an API for trusting certificates:<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">True</li>
<li class="alphabeticList level-2">False</li>
</ol>
</li>
<li class="numberedList">Where can the Pulumi store state?<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">Local file</li>
<li class="alphabeticList level-2">S3 bucket</li>
<li class="alphabeticList level-2">Pulumi’s SaaS service</li>
<li class="alphabeticList level-2">All of the above</li>
</ol>
</li>
<li class="numberedList">What sources of identity can OpenUnison use?<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">Active Directory</li>
<li class="alphabeticList level-2">Entra ID (formerly Azure AD)</li>
<li class="alphabeticList level-2">Okta</li>
<li class="alphabeticList level-2">GitHub</li>
<li class="alphabeticList level-2">All of the above</li>
</ol>
</li>
<li class="numberedList">GitLab lets you check out code from another repository using your workflow’s token:<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">True</li>
<li class="alphabeticList level-2">False</li>
</ol>
</li>
<li class="numberedList">Argo CD can check repositories for changes:<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">True</li>
<li class="alphabeticList level-2">False</li>
</ol>
</li>
</ol>
<h1 class="heading-1" id="_idParaDest-641">Answers</h1>
<ol>
<li class="numberedList" value="1">b: False: The node’s operating system must trust the remote certificate.</li>
<li class="numberedList">d: All of these are valid options.</li>
<li class="numberedList">e: OpenUnison supports all of these options.</li>
<li class="numberedList">b: False: Neither GitLab nor GitHub supports workflow tokens to check out other repositories.</li>
<li class="numberedList">a: True: Letting Argo CD check for updates to a repository instead of using a webhook can simplify deployment.</li>
</ol>
<h1 class="heading-1" id="_idParaDest-642">Join our book’s Discord space</h1>
<p class="normal">Join the book’s Discord workspace for a monthly <em class="italic">Ask Me Anything</em> session with the authors:</p>
<p class="normal"><a href="https://packt.link/K8EntGuide"><span class="url">https://packt.link/K8EntGuide</span></a></p>
<p class="normal"><img alt="" height="176" src="../Images/QR_Code965214276169525265.png" width="176"/></p>
</div>
</div></body></html>