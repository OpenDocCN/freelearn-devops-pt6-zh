<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer128">
    <h1 class="chapterNumber">6</h1>
    <h1 class="chapterTitle" id="_idParaDest-219">Namespaces, Quotas, and Limits for Multi-Tenancy in Kubernetes</h1>
    <p class="normal">So far, we’ve learned about Kubernetes’ key concepts by launching objects into our clusters and observing their behavior. You may have noticed that, in the long run, it would be difficult to maintain a cleanly organized cluster. As your clusters grow, it will become more and more difficult to maintain the ever-increasing number of resources managed in your cluster. That’s when Kubernetes namespaces come into play.</p>
    <p class="normal">In this chapter, we will learn about <strong class="keyWord">namespaces</strong>. They help us keep our clusters well organized by grouping our resources by <a id="_idIndexMarker510"/>application or environment. Kubernetes namespaces are another key aspect of Kubernetes management, and it’s really important to master them!</p>
    <p class="normal">In this chapter, we’re going to cover the following main topics:</p>
    <ul>
      <li class="bulletList">Introduction to Kubernetes namespaces</li>
      <li class="bulletList">How namespaces impact your resources and services</li>
      <li class="bulletList">Configuring ResourceQuota and Limits at the namespace level</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-220">Technical requirements</h1>
    <p class="normal">For this chapter, you will need the following:</p>
    <ul>
      <li class="bulletList">A working Kubernetes cluster (local or cloud-based, but this is not important)</li>
      <li class="bulletList">A working <code class="inlineCode">kubectl</code> CLI configured to communicate with the cluster</li>
    </ul>
    <p class="normal">If you do not have these technical requirements, please read <em class="chapterRef">Chapter 2</em>, <em class="italic">Kubernetes Architecture – from Container Images to Running Pods</em>, and <em class="chapterRef">Chapter 3</em>, <em class="italic">Installing Your First Kubernetes Cluster</em>, to get them.</p>
    <p class="normal">You can download the latest code samples for this chapter from the official GitHub repository at <a href="https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter06"><span class="url">https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter06</span></a>.</p>
    <h1 class="heading-1" id="_idParaDest-221">Introduction to Kubernetes namespaces</h1>
    <p class="normal">The more applications you deploy on your Kubernetes clusters, the greater the need to keep your cluster resources organized. You can use labels and annotations to manage the objects within your cluster, but you can take organization further by using <strong class="keyWord">namespaces</strong>. Namespaces<a id="_idIndexMarker511"/> in Kubernetes allow you to logically isolate parts of your cluster, helping you manage resources more effectively. However, to enforce resource allocation and limits, additional objects<a id="_idIndexMarker512"/> like <code class="inlineCode">ResourceQuotas</code> are required. Once namespaces have been created, you can launch Kubernetes objects such as Pods, which will only exist in that namespace. So all the operations that are run against the cluster with <code class="inlineCode">kubectl</code> will be<a id="_idIndexMarker513"/> scoped to that individual namespace, where you can perform as many operations as possible while eliminating the risk of impacting resources that are in another namespace.</p>
    <p class="normal">Let’s start by finding out what exactly namespaces are and why they were created.</p>
    <div class="note">
      <p class="normal">For advanced multi-cluster and <a id="_idIndexMarker514"/>multi-tenancy scenarios in Kubernetes, projects<a id="_idIndexMarker515"/> like <strong class="keyWord">Capsule</strong> (<a href="https://capsule.clastix.io/"><span class="url">https://capsule.clastix.io/</span></a>) and <strong class="keyWord">HyperShift</strong> (<a href="https://github.com/openshift/hypershift"><span class="url">https://github.com/openshift/hypershift</span></a>) offer robust solutions. Capsule enables secure, multi-tenant Kubernetes clusters by allowing different teams or tenants to manage their own isolated namespaces. HyperShift simplifies the management of multiple clusters by providing a lightweight and scalable approach to securely isolate and manage Kubernetes resources across different environments.</p>
    </div>
    <p class="normal">Now, let’s move on to the next section on the importance of namespaces in Kubernetes.</p>
    <h2 class="heading-2" id="_idParaDest-222">The importance of namespaces in Kubernetes</h2>
    <p class="normal">As we mentioned previously, namespaces<a id="_idIndexMarker516"/> in Kubernetes are a way to help the cluster administrator keep everything clean and organized, while providing resource isolation.</p>
    <p class="normal">The biggest Kubernetes clusters can run hundreds or even thousands of applications. When everything is deployed in the same namespace, it can become very complex to know which particular resource belongs to which application.</p>
    <p class="normal">If, by misfortune, you update or modify the wrong resource, you might end up breaking an app running in your cluster. To resolve that, you can use labels and selectors, but even then, as the number of resources grows, managing the cluster will quickly become chaotic if you don’t start using namespaces.</p>
    <p class="normal">We learned the basics of creating namespaces in <em class="chapterRef">Chapter 8</em>, <em class="italic">Exposing Your Pods with Services</em>, but we didn’t learn that in much detail. Let’s now learn in detail how the namespaces are used to keep everything clean and organized while providing resource isolation.</p>
    <h2 class="heading-2" id="_idParaDest-223">How namespaces are used to split resources into chunks</h2>
    <p class="normal">Right after you’ve installed Kubernetes, when<a id="_idIndexMarker517"/> your cluster is brand new, it is created with a few namespaces for the cluster components. So even if you didn’t notice previously, you are already using namespaces as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get namespaces
NAME              STATUS   AGE
default           Active   8d
kube-node-lease   Active   8d
kube-public       Active   8d
kube-system       Active   8d
</code></pre>
    <p class="normal">The main concept is to deploy your Pods and other objects in Kubernetes while specifying a namespace of your preference. This practice helps to keep your cluster tidy and well structured. It’s worth noting that Kubernetes comes with a default namespace, which is used if you don’t specify one explicitly.</p>
    <p class="normal">The following image illustrates the namespaces and isolation at a high level.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_06_01.png"/></figure>
    <p class="packt_figref">Figure 6.1: Kubernetes namespaces and resource isolation</p>
    <p class="normal">In a broader sense, Kubernetes<a id="_idIndexMarker518"/> namespaces serve several purposes for administrators, including:</p>
    <ul>
      <li class="bulletList">Resource isolation</li>
      <li class="bulletList">Scoping resource names</li>
      <li class="bulletList">Hardware allocation and consumption limitation</li>
      <li class="bulletList">Permissions and access control with Role-Based Access Control</li>
    </ul>
    <p class="normal">We recommend that you create one namespace per microservice or application, and then deploy all the resources that belong to a microservice within its namespace. However, be aware that Kubernetes does not impose any specific rules on you. For example, you could decide to use namespaces in these ways:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Differentiating between environments</strong>: For example, one namespace is for a production environment, and the other one is for a development environment.</li>
      <li class="bulletList"><strong class="keyWord">Differentiating between the tiers</strong>: One namespace is for databases, one is for application Pods, and another is for middleware deployment.</li>
      <li class="bulletList"><strong class="keyWord">Using the default namespace</strong>: For the smallest clusters that only deploy a few resources, you can go for<a id="_idIndexMarker519"/> the simplest setup and just use one big default namespace, deploying everything into it.</li>
    </ul>
    <p class="normal">Either way, keep in mind that even though two Pods are deployed in different namespaces and exposed through services, they can still interact and communicate with each other. Even though Kubernetes services are created in a given namespace, they’ll receive a <strong class="keyWord">fully qualified domain name</strong> (<strong class="keyWord">FQDN</strong>) that will be accessible on the whole cluster. So even if an <a id="_idIndexMarker520"/>application running on namespace A needs to interact with an application in namespace B, it will have to call the service exposing app B by its FQDN. You don’t need to worry about cross-namespace communication, as this is allowed by default and can be controlled via network policies.</p>
    <p class="normal">Now, let’s learn about the default namespaces.</p>
    <h2 class="heading-2" id="_idParaDest-224">Understanding default namespaces</h2>
    <p class="normal">Most Kubernetes clusters<a id="_idIndexMarker521"/> are created with a <a id="_idIndexMarker522"/>few namespaces by default. You can list your namespaces using <code class="inlineCode">kubectl get namespaces</code> (or <code class="inlineCode">kubectl get ns</code>), as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get namespaces
NAME              STATUS   AGE
default           Active   8d
kube-node-lease   Active   8d
kube-public       Active   8d
kube-system       Active   8d
</code></pre>
    <p class="normal">For instance, we are using a minikube cluster. By reading this command’s output, we can see that the cluster we are currently using was set up with the following namespaces from the start:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">default</code>: Kubernetes <a id="_idIndexMarker523"/>automatically provides this namespace, allowing you to<a id="_idIndexMarker524"/> begin using your new cluster without the need to create one manually. This namespace has been the default location to create all your resources thus far and is also utilized as the default namespace when no other is specified.</li>
      <li class="bulletList"><code class="inlineCode">kube-public</code>: This <a id="_idIndexMarker525"/>namespace is accessible to all clients, including those <a id="_idIndexMarker526"/>without authentication. Primarily designated for cluster-wide purposes, it ensures certain resources are publicly visible and readable across an entire cluster. However, it’s important to note that the public aspect of this namespace is more of a convention than a strict requirement. Currently <a id="_idIndexMarker527"/>unused, you can<a id="_idIndexMarker528"/> safely leave it as is.</li>
      <li class="bulletList"><code class="inlineCode">kube-system</code>: This <a id="_idIndexMarker529"/>namespace is reserved for objects created by the Kubernetes system itself. It’s where Kubernetes deploys the necessary <a id="_idIndexMarker530"/>objects for its operation. In typical Kubernetes setups, essential components like <code class="inlineCode">kube-scheduler</code> and <code class="inlineCode">kube-apiserver</code> are deployed as Pods within this<a id="_idIndexMarker531"/> namespace. </li>
    </ul>
    <p class="normal-one">These components are <a id="_idIndexMarker532"/>vital for the proper functioning of the Kubernetes cluster they serve. Therefore, it’s advisable to refrain from making changes to this namespace, as any alterations could potentially disrupt the cluster’s functionality.</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">kube-node-lease</code>: The<a id="_idIndexMarker533"/> purpose of this namespace is to store<a id="_idIndexMarker534"/> Lease objects linked to individual nodes. These node leases enable the kubelet to transmit heartbeats, facilitating the detection of node failures by the control plane.</li>
    </ul>
    <div class="note">
      <p class="normal">Depending on which Kubernetes distribution you use, the pre-existing set of namespaces can change. But most of the time, these namespaces will be created by default.</p>
    </div>
    <p class="normal">Let’s leave this namespace aside for now because we are going to get to the heart of the matter and start creating namespaces. We will look at the impact that these can have on your Pods, particularly at the level of the DNS resolution of your services.</p>
    <h1 class="heading-1" id="_idParaDest-225">How namespaces impact your resources and services</h1>
    <p class="normal">In this <a id="_idIndexMarker535"/>section, we will learn how to create, update, and delete namespaces, as well as the impacts that namespaces have on services and Pods.</p>
    <p class="normal">We will also learn how to create resources by specifying a custom namespace so that we don’t rely on the default one.</p>
    <h2 class="heading-2" id="_idParaDest-226">Listing namespaces inside your cluster</h2>
    <p class="normal">We saw this in<a id="_idIndexMarker536"/> the previous section, <em class="italic">Understanding default namespaces</em>, but in this section, we will learn how to list and explore the namespaces that have been created in your Kubernetes cluster:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get namespaces
NAME              STATUS   AGE
default           Active   8d
kube-node-lease   Active   8d
kube-public       Active   8d
kube-system       Active   8d
</code></pre>
    <p class="normal">Keep in mind that all the commands that make use of the <code class="inlineCode">namespaces</code> resource <code class="inlineCode">kind</code> can also use the <code class="inlineCode">ns</code> alias to benefit from a shorter format.</p>
    <h2 class="heading-2" id="_idParaDest-227">Retrieving the data of a specific namespace</h2>
    <p class="normal">Retrieving the data of a<a id="_idIndexMarker537"/> specific namespace can be achieved using the <code class="inlineCode">kubectl describe</code> command, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl describe namespaces default
Name:         default
Labels:       kubernetes.io/metadata.name=default
Annotations:  &lt;none&gt;
Status:       Active
No resource quota.
No LimitRange resource.
</code></pre>
    <p class="normal">You can also use the <code class="inlineCode">get</code> command and redirect the YAML format to a file to get the data from a specific namespace:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get namespaces default -o yaml &gt; default-ns.yaml
</code></pre>
    <p class="normal">Please note that a namespace can be in one of two states:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Active</strong>: The<a id="_idIndexMarker538"/> namespace is active; it can be used to place new objects into it.</li>
      <li class="bulletList"><strong class="keyWord">Terminating</strong>: The <a id="_idIndexMarker539"/>namespace is being deleted, along with all its objects. It can’t be used to host new objects while in this status.</li>
    </ul>
    <p class="normal">Now, let’s learn how to create a new namespace imperatively.</p>
    <h2 class="heading-2" id="_idParaDest-228">Creating a namespace using imperative syntax</h2>
    <p class="normal">To imperatively create a namespace, you<a id="_idIndexMarker540"/> can use the <code class="inlineCode">kubectl create namespaces</code> command by specifying the name of the namespace to create. Here, we are going to create a new namespace called <code class="inlineCode">custom-ns</code>. Please notice that all the operations related to namespaces in <code class="inlineCode">kubectl</code> can be written with the shorter <code class="inlineCode">ns</code> alias:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create ns custom-ns
namespace/custom-ns created
</code></pre>
    <p class="normal">The new namespace, called <code class="inlineCode">custom-ns</code>, should now be created in your cluster. You can check it by running the <code class="inlineCode">kubectl get</code> command once more:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get ns custom-ns
NAME        STATUS   AGE
custom-ns   Active   35s
</code></pre>
    <p class="normal">As you can see, the namespace has been created and is in the <code class="inlineCode">Active</code> state. We can now place resources in it.</p>
    <div class="note">
      <p class="normal">Please avoid naming your cluster with a name starting with the <code class="inlineCode">kube-</code> prefix, as this is the terminology for Kubernetes’ objects and system namespaces.</p>
    </div>
    <p class="normal">Now, let’s learn how to create another namespace using declarative syntax.</p>
    <h2 class="heading-2" id="_idParaDest-229">Creating a namespace using declarative syntax</h2>
    <p class="normal">Let’s discover how to create a <a id="_idIndexMarker541"/>namespace using declarative syntax. As always, you must use a YAML (or JSON) file. Here is a basic YAML file to create a new namespace in your cluster. Please pay attention to the <code class="inlineCode">kind: Namespace</code> in the file:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># custom-ns-2.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">custom-ns-2</span>
</code></pre>
    <p class="normal">Apply the definition using the <code class="inlineCode">kubectl create</code> command, by defining the YAML file path:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f custom-ns-2.yaml
namespace/custom-ns-2 created
</code></pre>
    <p class="normal">With that, we have created two custom namespaces. The first one, which was created imperatively, is <a id="_idIndexMarker542"/>called <code class="inlineCode">custom-ns</code>, while the second one, which was created declaratively, is called <code class="inlineCode">custom-ns-2</code>.</p>
    <p class="normal">Now, let’s learn how to remove these two namespaces using <code class="inlineCode">kubectl</code>.</p>
    <h2 class="heading-2" id="_idParaDest-230">Deleting a namespace</h2>
    <p class="normal">You can delete a<a id="_idIndexMarker543"/> namespace using <code class="inlineCode">kubectl delete</code>, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete namespaces custom-ns
namespace "custom-ns" deleted
</code></pre>
    <p class="normal">Please note this can also be achieved using declarative syntax. Let’s delete the <code class="inlineCode">custom-ns-2</code> namespace that was created using the previous YAML file:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete -f custom-ns-2.yaml
namespace "custom-ns-2" deleted
</code></pre>
    <p class="normal">Running this command will take the namespace out of the <code class="inlineCode">Active</code> status; it will enter the <code class="inlineCode">Terminating</code> status. Right after the command, the namespace will be unable to host new objects, and after a few moments, it should completely disappear from the cluster.</p>
    <div class="note">
      <p class="normal">We have to warn you about using the <code class="inlineCode">kubectl delete namespace</code> command, as it is extremely dangerous. Deleting a namespace is permanent and definitive. All the resources that were created in the namespace will be destroyed. If you need to use this command, be sure to have YAML files to recreate the destroyed resources and even the destroyed namespace.</p>
    </div>
    <p class="normal">Now, let’s discover how to create resources inside a specific namespace.</p>
    <h2 class="heading-2" id="_idParaDest-231">Creating a resource inside a namespace</h2>
    <p class="normal">The following code<a id="_idIndexMarker544"/> shows how to create an NGINX Pod by specifying a custom namespace. Here, we are going to recreate a new <code class="inlineCode">custom-ns</code> namespace and launch an NGINX Pod in it:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create ns custom-ns
<span class="hljs-con-meta">$ </span>kubectl run nginx --image nginx:latest -n custom-ns
Pod/nginx created
</code></pre>
    <p class="normal">Pay attention to the <code class="inlineCode">-n</code> option, which, in its long form, is the <code class="inlineCode">--namespace</code> option. This is used to enter the name of the namespace where you want to create the resource (or get the details from). This option is supported by all the <code class="inlineCode">kind</code> resources that can be scoped in a namespace.</p>
    <p class="normal">Here is another command to demonstrate this. The following command will create a new <code class="inlineCode">configmap</code> in the <code class="inlineCode">custom-ns</code> namespace:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create configmap configmap-custom-ns --from-literal=Lorem=Ipsum -n custom-ns
configmap/configmap-custom-ns created
</code></pre>
    <p class="normal">You can also specify a namespace when using declarative syntax. Here is how to create a Pod in a specific namespace with declarative syntax:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># pod-in-namespace.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx2</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">custom-ns</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:latest</span>
</code></pre>
    <p class="normal">Please <a id="_idIndexMarker545"/>note the <code class="inlineCode">namespace</code> key under the <code class="inlineCode">metadata</code> section, just under the Pod’s name, which says to create the Pod in the <code class="inlineCode">custom-ns</code> namespace. Now, we can apply this file using <code class="inlineCode">kubectl</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f pod-in-namespace.yaml
pod/nginx2 created
</code></pre>
    <p class="normal">At this point, we should have a namespace called <code class="inlineCode">custom-ns</code> that contains two <code class="inlineCode">nginx</code> Pods, as well as a <code class="inlineCode">configmap</code> called <code class="inlineCode">configmap-custom-ns</code>.</p>
    <div class="note">
      <p class="normal">When you’re using namespaces, you should always specify the <code class="inlineCode">-n</code> flag to target the specific namespace of your choosing. Otherwise, you might end up running operations in the wrong namespace.</p>
    </div>
    <p class="normal">Now, let’s move on to list the resources inside specific namespaces.</p>
    <h2 class="heading-2" id="_idParaDest-232">Listing resources inside a specific namespace</h2>
    <p class="normal">To be able to list the <a id="_idIndexMarker546"/>resources within a namespace, you must add the <code class="inlineCode">-n</code> option, just like when creating a resource. Use the following command to list the Pods in the <code class="inlineCode">custom-ns</code> namespace:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods -n custom-ns 
NAME     READY   STATUS    RESTARTS   AGE 
nginx    1/1     Running   0          9m23s 
nginx2   1/1     Running   0          94s
</code></pre>
    <p class="normal">Here, you can see that the <code class="inlineCode">nginx</code> Pod that we created earlier is present in the namespace. From now on, all the commands that target this particular Pod should contain the <code class="inlineCode">-n custom-ns</code> option.</p>
    <p class="normal">The reason for this is that the Pod does not exist in the default namespace, and if you omit passing the <code class="inlineCode">-n</code> option, then the default namespace will be requested. Let’s try to remove <code class="inlineCode">-n custom-ns</code> from the <code class="inlineCode">get</code> command. We will see that the <code class="inlineCode">nginx</code> Pod is not here anymore:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods
No resources found in default namespace.
</code></pre>
    <p class="normal">Now, we can also run the <code class="inlineCode">get</code> <code class="inlineCode">configmap</code> command to check whether <code class="inlineCode">configmap</code> is listed in the output. As you can see, the behavior is the same as when trying to list Pods. If you omit the <code class="inlineCode">-n</code> option, the list operation will run against the <code class="inlineCode">default</code> namespace:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get cm
NAME               DATA   AGE
kube-root-ca.crt   1      9d
<span class="hljs-con-meta">$ </span>kubectl get cm -n custom-ns
NAME                  DATA   AGE
configmap-custom-ns   1      70m
kube-root-ca.crt      1      76m
</code></pre>
    <p class="normal">The most <a id="_idIndexMarker547"/>important point to remember from all that we have discussed so far in this section is to never forget to add the <code class="inlineCode">-n</code> option when working on a cluster that has multiple namespaces. This little carelessness could waste your time because, if you forget it, everything you do will be done on the <code class="inlineCode">default</code> namespace.</p>
    <p class="normal">Instead of passing the namespace information in the command line every time, it is possible to set it in the kubeconfig context. In the next section, we will learn how to set the working namespace in the current context of <code class="inlineCode">kubeconfig</code>.</p>
    <h2 class="heading-2" id="_idParaDest-233">Setting the current namespace using kubectl config set-context</h2>
    <p class="normal">It is also <a id="_idIndexMarker548"/>possible to set your <a id="_idIndexMarker549"/>current namespace in some situations. For example, if you are working on a specific project and using a specific namespace for your application and other resources, then you can set the namespace context as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config set-context --current --namespace=custom-ns
Context "minikube" modified.
</code></pre>
    <p class="normal">We can also check if any namespace is configured in the context as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config view --minify --output <span class="hljs-con-string">'jsonpath={..namespace}'</span>
custom-ns
</code></pre>
    <p class="normal">Now, we can get the details of the application or apply configuration without mentioning the <code class="inlineCode">-n &lt;namespace&gt;</code> option:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods
NAME     READY   STATUS    RESTARTS   AGE
nginx    1/1     Running   0          79m
nginx2   1/1     Running   0          71m
</code></pre>
    <div class="note">
      <p class="normal">Running the <code class="inlineCode">kubectl config</code> command and sub-commands will only trigger modification or read operations against the <code class="inlineCode">~/.kube/config</code> file, which is the configuration file that <code class="inlineCode">kubectl</code> uses.</p>
      <p class="normal">When you’re using the <code class="inlineCode">kubectl config set-context</code> command, you’re just updating that file to make it point to another namespace.</p>
    </div>
    <p class="normal">Knowing how to switch between namespaces with <code class="inlineCode">kubectl</code> is important, but before you run any write operations such as <code class="inlineCode">kubectl delete</code> or <code class="inlineCode">kubectl create</code>, make sure that you are in the correct namespace. Otherwise, you should continue to use the <code class="inlineCode">-n</code> flag. As this switching operation might be executed a lot of times, Kubernetes users tend to create Linux aliases to make them easier to use. Do not hesitate to define a Linux alias if you think it can be useful to you.</p>
    <p class="normal">For example, you can set an alias in your <code class="inlineCode">~/.bashrc</code> file (assuming that you are using Linux or macOS) as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">alias kubens='kubectl config set-context --current --namespace'
</code></pre>
    <p class="normal">And use this alias next time, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubens custom-ns
Context "minikube" modified.
<span class="hljs-con-meta">$ </span>kubectl config view --minify --output <span class="hljs-con-string">'jsonpath={..namespace}'</span>
custom-n
<span class="hljs-con-meta"># </span>change to another namespace
<span class="hljs-con-meta">$ </span>kubens default
Context "minikube" modified.
<span class="hljs-con-meta">$ </span>kubectl config view --minify --output <span class="hljs-con-string">'jsonpath={..namespace}'</span>
default
</code></pre>
    <p class="normal">But again, it is<a id="_idIndexMarker550"/> highly recommended to use the <code class="inlineCode">-n namespace</code> option to avoid any accidents in Kubernetes <a id="_idIndexMarker551"/>operations.</p>
    <p class="normal">Before we continue our chapter and hands-on tutorials, let’s set the namespace in context back to normal by running the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span> kubectl config set-context --current --namespace=default
Context "minikube" modified.
</code></pre>
    <p class="normal">Now, let’s discover how to list all the resources inside a specific namespace.</p>
    <h2 class="heading-2" id="_idParaDest-234">Listing all the resources inside a specific namespace</h2>
    <p class="normal">If you want to list all the <a id="_idIndexMarker552"/>resources in a specific namespace, there is a very useful command that you can use called <code class="inlineCode">kubectl get all -n custom-ns</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get all -n custom-ns
</code></pre>
    <p class="normal">As you can see, this command can help you retrieve all the resources that are created in the namespace specified in the <code class="inlineCode">-n</code> flag.</p>
    <h2 class="heading-2" id="_idParaDest-235">Recognizing how names are scoped within a namespace</h2>
    <p class="normal">It’s important to <a id="_idIndexMarker553"/>understand that namespaces offer an additional advantage: defining the scope for the names of the resources they contain.</p>
    <p class="normal">Take the example of Pod names. When you work without namespaces, you interact with the default namespace, and when you create two Pods with the same name, you get an error because Kubernetes uses the names of the Pods as their unique identifiers to distinguish them.</p>
    <p class="normal">Let’s try to create two Pods in the default namespace. Both will be called <code class="inlineCode">nginx</code>. Here, we can simply run the same command twice in a row:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl run nginx --image nginx:latest
Pod/nginx created
<span class="hljs-con-meta">$ </span>kubectl run nginx --image nginx:latest
Error from server (AlreadyExists): Pods "nginx" already exists
</code></pre>
    <p class="normal">The second command produces an error, saying that the Pod already exists, which it does. If we run <code class="inlineCode">kubectl get pods</code>, we can see that only one Pod exists:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          64s
</code></pre>
    <p class="normal">Now, let’s try to list the Pods again but, this time, in the <code class="inlineCode">custom-ns</code> namespace:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get Pods --namespace custom-ns
NAME     READY   STATUS    RESTARTS   AGE
nginx    1/1     Running   0          89m
nginx2   1/1     Running   0          23m
</code></pre>
    <p class="normal">As you can see, this namespace also has a Pod called <code class="inlineCode">nginx</code>, and it’s not the same Pod that is contained in the <code class="inlineCode">default</code> namespace. This is one of the major advantages of namespaces. By using them, your Kubernetes cluster can now define multiple resources with the same names, so long as they are in different namespaces. You can easily duplicate microservices or applications by using the namespace element.</p>
    <p class="normal">Also, note that you can override the key to the namespaces of the resources that you create declaratively. By adding the <code class="inlineCode">-n</code> option to the <code class="inlineCode">kubectl create</code> command, you force a namespace as the context for your command; <code class="inlineCode">kubectl</code> will take the namespace that was passed in the command into account, not the one present in the YAML file. By doing this, it becomes very easy to duplicate your resources between different namespaces – for example, a production environment in a <code class="inlineCode">production</code> namespace and a test environment in a <code class="inlineCode">test</code> namespace.</p>
    <h2 class="heading-2" id="_idParaDest-236">Understanding that not all resources are in a namespace</h2>
    <p class="normal">In Kubernetes, not all objects belong to a namespace. This is the case, for example, with nodes, which are represented at the cluster level by an entry of the <code class="inlineCode">Node</code> kind but that does not belong to any particular namespace. You can list resources that do not belong to a namespace using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl api-resources --namespaced=<span class="hljs-con-literal">false</span>
NAME                              SHORTNAMES   APIVERSION                        NAMESPACED   KIND
componentstatuses                 cs           v1                                false        ComponentStatus
namespaces                        ns           v1                                false        Namespace
nodes                             no           v1                                false        Node
persistentvolumes                 pv           v1                                false        PersistentVolume
…&lt;removed for brevity&gt;...
</code></pre>
    <p class="normal">You can also list all the resources that belong to a namespace by passing <code class="inlineCode">--namespaced</code> to <code class="inlineCode">true</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl api-resources --namespaced=<span class="hljs-con-literal">true</span>
</code></pre>
    <p class="normal">Now, let’s learn how namespaces affect a service’s DNS.</p>
    <h2 class="heading-2" id="_idParaDest-237">Resolving a service using namespaces</h2>
    <p class="normal">As we <a id="_idIndexMarker554"/>discovered in <em class="chapterRef">Chapter 8</em>, <em class="italic">Exposing Your Pods with Services</em>, Pods can be exposed through a type of object called Services. When created, services are assigned a DNS record that allows Pods in the cluster to access them.</p>
    <p class="normal">However, when a Pod tries to call a service through DNS, it can only reach it if the service is in the same namespace as the Pod, which is limiting. Namespaces have a solution to this problem. When a service is created in a particular namespace, the name of its service is added to its DNS:</p>
    <pre class="programlisting con"><code class="hljs-con">&lt;service_name&gt;.&lt;namespace_name&gt;.svc.cluster.local
</code></pre>
    <p class="normal">By querying this domain name, you can easily query any service that is in any namespace in your Kubernetes cluster. So you are not limited to that level. Pods are still capable of achieving inter-communication, even if they don’t run in the same namespace.</p>
    <p class="normal">In the following section, we will explore some of the best practices for Kubernetes namespaces.</p>
    <h2 class="heading-2" id="_idParaDest-238">Best practices for Kubernetes namespaces</h2>
    <p class="normal">Even though there are no strict rules on <a id="_idIndexMarker555"/>namespace creation and management, let us learn some of the industry best practices related to namespaces.</p>
    <ul>
      <li class="bulletList">Organization and separation<ul>
          <li class="bulletList level-2"><strong class="keyWord">Logical Grouping</strong>: Put <a id="_idIndexMarker556"/>together apps, services, and resources that go together based on what they do, where they are in development (like dev, test, and prod), or who owns them (e.g., different teams). This keeps things organized and makes managing resources easier.</li>
          <li class="bulletList level-2"><strong class="keyWord">Isolation</strong>: Use namespaces to keep deployments separate. This means apps in one namespace won’t mess with stuff in another, which reduces conflicts. You can also increase the security and isolation by <a id="_idIndexMarker557"/>applying appropriate <strong class="keyWord">Role-Based Access Control </strong>(<strong class="keyWord">RBAC</strong>) network <a id="_idIndexMarker558"/>policies.</li>
        </ul>
      </li>
      <li class="bulletList">Naming rules<ul>
          <li class="bulletList level-2"><strong class="keyWord">Clear and Descriptive</strong>: Give<a id="_idIndexMarker559"/> your namespaces names that <a id="_idIndexMarker560"/>say what they’re for. This makes it easier to keep track of them, especially in big setups. Stick to common naming tricks like <code class="inlineCode">dev-</code>, <code class="inlineCode">test-</code>, or <code class="inlineCode">prod-</code> for different environments.</li>
          <li class="bulletList level-2"><strong class="keyWord">Stay Consistent</strong>: Use the same naming style across your cluster. This makes it easier for your team to talk about and understand what’s going on.</li>
        </ul>
      </li>
      <li class="bulletList">Managing resources<ul>
          <li class="bulletList level-2"><strong class="keyWord">Resource Limits</strong>: Set limits <a id="_idIndexMarker561"/>on how much stuff a namespace can use. This stops one deployment from hogging everything and makes sure everyone gets their fair share. Please remember that it is also possible to set the resource limits at the Pod level for increased control.</li>
          <li class="bulletList level-2"><strong class="keyWord">Limits for All</strong>: Make rules for how much stuff each Pod and container in a namespace can ask for or use. This gives you more control over how resources are used.</li>
        </ul>
      </li>
      <li class="bulletList">Controlling access<ul>
          <li class="bulletList level-2"><strong class="keyWord">Role-Based Access</strong>: Use RBAC to <a id="_idIndexMarker562"/>control who can do what in each namespace. Give people and services the right permissions to manage stuff in their namespace.</li>
        </ul>
      </li>
      <li class="bulletList">Keeping an eye out<ul>
          <li class="bulletList level-2"><strong class="keyWord">Watch Things</strong>: Keep an eye on how much stuff is used and if your apps are healthy in each namespace. This helps you spot problems and use resources better.</li>
          <li class="bulletList level-2"><strong class="keyWord">Lifecycle of a Namespace</strong>: Check up on your namespaces regularly. Get rid of the ones that you don’t use anymore to keep things tidy and avoid security risks. Think about automating how you create and delete namespaces.</li>
        </ul>
      </li>
      <li class="bulletList">Other stuff to think about<ul>
          <li class="bulletList level-2"><strong class="keyWord">Not Perfect Isolation</strong>: Even though namespaces help keep things separate, they’re not foolproof. You might need network rules for extra safety.</li>
          <li class="bulletList level-2"><strong class="keyWord">Clusters versus Namespaces:</strong> If your setup is complicated and needs a lot of separation, think about using<a id="_idIndexMarker563"/> different Kubernetes clusters instead of just namespaces.</li>
        </ul>
      </li>
    </ul>
    <p class="normal">By following these best practices, you can keep your Kubernetes setup organized, safe, and easy to handle using namespaces. Just remember to tweak things to fit your own setup for the best results. With that, we’re done with the basics of namespaces in Kubernetes. We have learned what namespaces are, how to create and delete them, how to use them to keep a cluster clean and organized, and how to update the <code class="inlineCode">kubeconfig</code> context to make <code class="inlineCode">kubectl</code> point to a specific namespace.</p>
    <p class="normal">Now, we’ll look at a few more advanced options related to namespaces. It is a good time to introduce <code class="inlineCode">ResourceQuota</code> and <code class="inlineCode">Limit</code>, which you can use to limit the computing resources that an application deployed on Kubernetes can access!</p>
    <h1 class="heading-1" id="_idParaDest-239">Configuring ResourceQuota and Limit at the namespace level</h1>
    <p class="normal">In this section, we’re going to discover that namespaces can not only be used to sort resources in a cluster but also to limit the computing resources that Pods can access.</p>
    <p class="normal">Using <code class="inlineCode">ResourceQuota</code> and <code class="inlineCode">Limits</code> with namespaces, you can create limits regarding the computing resources your Pods can access. We’re going to learn how to proceed and exactly how to use these new concepts. In general, defining <code class="inlineCode">ResourceQuota</code> and <code class="inlineCode">Limits</code> is considered good practice for production clusters – that’s why you should use them wisely.</p>
    <h2 class="heading-2" id="_idParaDest-240">Understanding the need to set ResourceQuotas</h2>
    <p class="normal">Just like applications or systems, Kubernetes Pods will require a certain amount of computing resources to work properly. In Kubernetes, you can <a id="_idIndexMarker564"/>configure two types of computing resources:</p>
    <ul>
      <li class="bulletList">CPU</li>
      <li class="bulletList">Memory</li>
    </ul>
    <p class="normal">All your nodes (compute and controller) work together to provide CPU and memory, and in Kubernetes, adding more CPU and memory simply consists of adding more compute (or worker) nodes to make room for more Pods. Depending on whether your Kubernetes cluster is based on-premises or in the cloud, adding more compute nodes can be achieved by purchasing the hardware and setup to do so, or by simply calling the cloud API to create additional virtual machines.</p>
    <h2 class="heading-2" id="_idParaDest-241">Understanding how Pods consume these resources</h2>
    <p class="normal">When you launch <a id="_idIndexMarker565"/>a Pod on Kubernetes, a control plane component, known as <code class="inlineCode">kube-scheduler</code>, will elect a compute node and assign the Pods to it. Then, the <code class="inlineCode">kubelet</code> on the elected compute node will attempt to launch the containers defined in the Pod.</p>
    <p class="normal">This process of compute node election is<a id="_idIndexMarker566"/> called <strong class="keyWord">Pod scheduling</strong> in Kubernetes.</p>
    <p class="normal">When a Pod gets scheduled and launched on a compute node, it has, by default, access to all the resources that the compute node has. Nothing prevents it from accessing more and more CPU and memory as the application is used, and ultimately, if the Pods run out of memory or CPU resources to work properly, then the application simply crashes.</p>
    <p class="normal">This can become a real problem because compute nodes can be used to run multiple applications – and, therefore, multiple Pods – at the same time. So if 10 Pods are launched on the same compute node but one of them consumes all the computing resources, then this will have an impact on all 10 Pods running on the compute node.</p>
    <p class="normal">This problem means that you have two things you must consider:</p>
    <ul>
      <li class="bulletList">Each Pod should be able to require some computing resources to work.</li>
      <li class="bulletList">The cluster should be able to restrict the Pod’s consumption so that it doesn’t take all the resources available, sharing them with other Pods too.</li>
    </ul>
    <p class="normal">It is possible to address these two problems in Kubernetes, and we will discover how to use two options that are exposed to the Pod object. The first one is called <strong class="keyWord">resource requests</strong>, which<a id="_idIndexMarker567"/> is the option that’s used to let a Pod indicate what amount of computing resources it needs, while the other one is called <strong class="keyWord">resource limit</strong> and will<a id="_idIndexMarker568"/> be used to indicate the maximum computing resources the Pod will have access to.</p>
    <p class="normal">Let’s explore these options now.</p>
    <h2 class="heading-2" id="_idParaDest-242">Understanding how Pods can require computing resources</h2>
    <p class="normal">The <code class="inlineCode">request</code> and <code class="inlineCode">limit</code> options will be<a id="_idIndexMarker569"/> declared within the YAML definition file of a Pod resource, or you can apply it to the running deployment using the <code class="inlineCode">kubectl set resource</code> command. Here, we’re going to focus on the <code class="inlineCode">request</code> option.</p>
    <p class="normal">The resource request is simply the minimal amount of computing resources a Pod will need to work properly, and it is a good practice to always define a <code class="inlineCode">request</code> option for your Pods, at least for those that are meant to run in production.</p>
    <p class="normal">Let’s say that you want to launch an NGINX Pod on your Kubernetes cluster. By filling in the <code class="inlineCode">request</code> option, you can tell Kubernetes that your NGINX Pod will need, at the bare minimum, 512 MiB of memory and 25% of a CPU core to work properly.</p>
    <p class="normal">Here is the YAML definition file that will create this Pod:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta"># </span>pod-in-namespace-with-request.yaml
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-with-request</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">custom-ns</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:latest</span>
      <span class="hljs-attr">resources:</span>
        <span class="hljs-attr">requests:</span>
          <span class="hljs-attr">memory:</span> <span class="hljs-string">"512Mi"</span>
          <span class="hljs-attr">cpu:</span> <span class="hljs-string">"250m"</span>
</code></pre>
    <p class="normal">As you can see in the above YAML snippet, you can define <code class="inlineCode">request</code> at the container level and set different ones for each container.</p>
    <p class="normal">There are three things to note about this Pod:</p>
    <ul>
      <li class="bulletList">It is created inside the <code class="inlineCode">custom-ns</code> namespace.</li>
      <li class="bulletList">It requires <code class="inlineCode">512Mi</code> of memory.</li>
      <li class="bulletList">It requires <code class="inlineCode">250m</code> of CPU.</li>
    </ul>
    <h2 class="heading-2" id="_idParaDest-243">But what do these metrics mean?</h2>
    <p class="normal">Memory is expressed in<a id="_idIndexMarker570"/> bytes (one MiB is equal to 1,048,576 bytes), whereas CPU is expressed in <strong class="keyWord">millicores</strong> and <a id="_idIndexMarker571"/>allows fractional values. If you want your Pod to consume <a id="_idIndexMarker572"/>one entire CPU core, you can set the <code class="inlineCode">cpu</code> key to <code class="inlineCode">1000m</code>. If you want two cores, you must set it to <code class="inlineCode">2000m</code>; for half of a core, it will be <code class="inlineCode">500m</code> or <code class="inlineCode">0.5</code>; and so on. However, to request a full CPU core, it’s simpler and more common practice to use the whole number (e.g., <code class="inlineCode">2</code>) instead of <code class="inlineCode">2000m</code>. So the preceding YAML definition says that the NGINX Pod we will create will forcibly need 512 MiB of memory, since memory is expressed in bytes, and one-quarter of a CPU core of the underlying compute node. There is nothing related to the CPU or memory frequency here.</p>
    <p class="normal">When you apply this YAML definition file to your cluster, the scheduler will look for a compute node that is capable of launching your Pods. This means that you need a compute node where there is enough room in terms of available CPU and memory to meet your Pods’ requests.</p>
    <p class="normal">But what if no compute node is capable of fulfilling these requirements? Here, the Pod will never be scheduled and never be launched. Unless you remove some running Pods to make room for this one or add a compute node that is capable of launching this Pod, it won’t ever be launched.</p>
    <div class="note">
      <p class="normal">Keep in mind that Pods cannot span multiple nodes. So if you set <code class="inlineCode">8000m</code> (which represents eight CPU cores) but your cluster is made up of two compute nodes with four cores each, then no compute node will be able to fulfill the request, and your Pod won’t be scheduled.</p>
    </div>
    <p class="normal">So use the <code class="inlineCode">request</code> option wisely – consider it as the minimum amount of compute resources the Pod will need to work. You have the risk that your Pod will never be scheduled if you set too high a request, but on the other hand, if your Pod is scheduled and launched successfully, this amount of resources is guaranteed.</p>
    <p class="normal">Now, let’s see how we can limit resource consumption.</p>
    <h2 class="heading-2" id="_idParaDest-244">Understanding how you can limit resource consumption</h2>
    <p class="normal">When you <a id="_idIndexMarker573"/>write a YAML definition file, you can define resource limits regarding what a Pod will be able to consume.</p>
    <p class="normal">Setting a resource request won’t suffice to do things properly. You should set a limit each time you set a resource. Setting a limit will tell Kubernetes to let the Pod consume resources up to that limit, and never above. This way, you ensure that your Pod won’t take all the resources of the compute for itself.</p>
    <p class="normal">However, be careful – Kubernetes won’t behave the same, depending on what kind of limit is reached. If the Pod reaches its CPU limit, it is going to be throttled, and you’ll notice performance degradation. But if your Pod reaches its memory limit, then it might be terminated. The reason for this is that memory is not something that can be throttled, and Kubernetes still needs to ensure that other applications are not impacted and remain stable. So be aware of that.</p>
    <p class="normal">Without a limit, the Pod will be able to consume all the resources of the compute node. Here is an updated YAML file corresponding to the NGINX Pod we saw earlier, but now, it has been updated to define a limit on memory and CPU.</p>
    <p class="normal">Here, the Pod will be able to consume up to 1 GiB of memory and up to 1 entire CPU core of the underlying compute node:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># pod-in-namespace-with-request-and-limit.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-with-request-and-limit</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">quota-ns</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:latest</span>
      <span class="hljs-attr">resources:</span>
        <span class="hljs-attr">requests:</span>
          <span class="hljs-attr">memory:</span> <span class="hljs-string">"512Mi"</span>
          <span class="hljs-attr">cpu:</span> <span class="hljs-string">"250m"</span>
        <span class="hljs-attr">limits:</span>
          <span class="hljs-attr">memory:</span> <span class="hljs-string">"1Gi"</span>
          <span class="hljs-attr">cpu:</span> <span class="hljs-string">"1000m"</span>
</code></pre>
    <p class="normal">So when you set a request, set a limit too. Let us try this request and limit it in our next hands-on lab.</p>
    <p class="normal">For this exercise, let us check current system resource availability. Since we are using the minikube cluster for our demonstration, let us enable metrics for detailed resource usage information. You will use metrics in <em class="chapterRef">Chapter 10</em>, <em class="italic">Running Production-Grade Kubernetes Workloads</em>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube addons <span class="hljs-con-built_in">enable</span> metrics-server
</code></pre>
    <p class="normal">Wait for the metrics server Pods to enter a <code class="inlineCode">Running</code> state before you continue:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get po -n kube-system | grep metrics
metrics-server-7c66d45ddc-82ngt            1/1     Running   0               113m
</code></pre>
    <p class="normal">Let us check the cluster usage using the metrics information:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl top node
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%  
minikube   181m         1%     806Mi           2%  
</code></pre>
    <p class="normal">In this case, we have <a id="_idIndexMarker574"/>about 800Mi memory and 180m CPU available to consume in our Kubernetes cluster.</p>
    <div class="note">
      <p class="normal">If you are using <code class="inlineCode">minikube</code> with Podman or the Docker driver, the <code class="inlineCode">minikube</code> will show the actual host CPU and memory, not the memory of the <code class="inlineCode">minikube</code> Kubernetes cluster node. In such cases, you can try another <code class="inlineCode">minikube</code> cluster using VirtualBox (<code class="inlineCode">minikube start --profile cluster2-vb --driver=virtualbox</code>) so that it will use the <code class="inlineCode">minikube</code> VM CPU and memory resource.</p>
    </div>
    <p class="normal">Let us create a new namespace for the resource request and limit demonstration:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create ns quota-ns
namespace/quota-ns created
</code></pre>
    <p class="normal">Now, let us create a new YAML file where we have non-realistic memory requests such as <code class="inlineCode">100Gi resources.requests.memory</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># pod-with-request-and-limit-1.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-with-request-and-limit-1</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">quota-ns</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:latest</span>
      <span class="hljs-attr">resources:</span>
        <span class="hljs-attr">requests:</span>
          <span class="hljs-attr">memory:</span> <span class="hljs-string">"100Gi"</span>
          <span class="hljs-attr">cpu:</span> <span class="hljs-string">"100m"</span>
        <span class="hljs-attr">limits:</span>
          <span class="hljs-attr">memory:</span> <span class="hljs-string">"100Gi"</span>
          <span class="hljs-attr">cpu:</span> <span class="hljs-string">"500m"</span>
</code></pre>
    <p class="normal">Create the Pod using <code class="inlineCode">kubectl apply</code>, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f pod-with-request-and-limit-1.yaml
pod/nginx-with-request-and-limit-1 created
</code></pre>
    <p class="normal">Check the Pod status to see the Pod creation:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pod -n quota-ns
NAME                             READY   STATUS    RESTARTS   AGE
nginx-with-request-and-limit-1   0/1     Pending   0          45s
</code></pre>
    <p class="normal">This says <code class="inlineCode"><a id="_idIndexMarker575"/></code><code class="inlineCode">Pending</code> and that the Pod is not running yet. Let us check the Pod details using the <code class="inlineCode">kubectl describe</code> command, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl describe po nginx-with-request-and-limit-1 -n quota-ns
Name:             nginx-with-request-and-limit-1
Namespace:        quota-ns
...&lt;removed for brevity&gt;...
Status:           Pending
IP:              
IPs:              &lt;none&gt;
Containers:
  nginx:
...&lt;removed for brevity&gt;...
    Limits:
      cpu:     500m
      memory:  100Gi
    Requests:
      cpu:        100m
      memory:     100Gi
    Environment:  &lt;none&gt;
...&lt;removed for brevity&gt;...
  Warning  FailedScheduling  105s  default-scheduler  0/1 nodes are available: 1 Insufficient memory. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.
</code></pre>
    <p class="normal">You will see an error, as the scheduler cannot find any nodes in the cluster to accommodate your Pod with the memory requests, which means that the Pod will be scheduled until the Kubernetes cluster has a suitable node to place the Pod.</p>
    <p class="normal">Now, we will update the<a id="_idIndexMarker576"/> YAML with reasonable memory, <code class="inlineCode">1Gi resources.requests.memory</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># pod-with-request-and-limit-2.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-with-request-and-limit-2</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">quota-ns</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:latest</span>
      <span class="hljs-attr">resources:</span>
        <span class="hljs-attr">requests:</span>
          <span class="hljs-attr">memory:</span> <span class="hljs-string">"1Gi"</span>
          <span class="hljs-attr">cpu:</span> <span class="hljs-string">"100m"</span>
        <span class="hljs-attr">limits:</span>
          <span class="hljs-attr">memory:</span> <span class="hljs-string">"2Gi"</span>
          <span class="hljs-attr">cpu:</span> <span class="hljs-string">"500m"</span>
</code></pre>
    <p class="normal">Let us create the Pod now:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f pod-with-request-and-limit-2.yaml
pod/nginx-with-request-and-limit-2 created
<span class="hljs-con-meta">$ </span>kubectl get po -n quota-ns
NAME                             READY   STATUS    RESTARTS   AGE
nginx-with-request-and-limit-1   0/1     Pending   0          8m24s
nginx-with-request-and-limit-2   1/1     Running   0          20s
</code></pre>
    <p class="normal">Now, the Kubernetes scheduler can find the suitable node, based on the resource request, and the Pod has already started running as usual.</p>
    <p class="normal">So now that you are aware of this request and limit consideration, don’t forget to add it to your Pods!</p>
    <h2 class="heading-2" id="_idParaDest-245">Understanding why you need ResourceQuota</h2>
    <p class="normal">You can entirely manage your<a id="_idIndexMarker577"/> Pod’s consumptions by relying entirely on its request and limit options. All the applications in Kubernetes are just Pods, so setting these two options provides you with a strong and reliable way to manage resource consumption on your cluster, given that you never forget to set these.</p>
    <p class="normal">It is easy to forget these two options and deploy a Pod on your cluster that won’t define any request or limit. Maybe it will be you, or maybe a member of your team, but the risk of deploying such a Pod is high because everyone can forget about these two options. And if you do so, the risk of application instability is high, as a Pod without a limit can eat all the resources on the compute node it is launched on.</p>
    <p class="normal">Kubernetes provides a way to mitigate this issue, thanks to two objects called <code class="inlineCode">ResourceQuota</code> and <code class="inlineCode">LimitRange</code>. These two objects are extremely useful because they can enforce these constraints at the namespace level.</p>
    <p class="normal"><code class="inlineCode">ResourceQuota</code> is another resource kind, just like a Pod or ConfigMap. The workflow is quite simple and consists of two steps:</p>
    <ol>
      <li class="numberedList" value="1">You must create a new namespace.</li>
      <li class="numberedList">You must create a <code class="inlineCode">ResourceQuota</code> and a <code class="inlineCode">LimitRange</code> object inside that namespace.</li>
    </ol>
    <p class="normal">Then, all the Pods that are launched in that namespace will be constrained by these two objects.</p>
    <p class="normal">These quotas are used, for example, to ensure that all the containers that are accumulated in a namespace do not consume more than 4 GiB of RAM.</p>
    <p class="normal">Therefore, it is possible and even recommended to set restrictions on what can and cannot run within Pods. It is strongly recommended that you always define a <code class="inlineCode">ResourceQuota</code> and a <code class="inlineCode">LimitRange</code> object for each namespace you create in your cluster!</p>
    <p class="normal">Without these quotas, the deployed resources could consume as much CPU or RAM as they want, which would ultimately make your cluster and all the applications running on it unstable, given that the Pods don’t hold requests and limits as part of their respective configurations.</p>
    <p class="normal">In general, <code class="inlineCode">ResourceQuota</code> is used to do the following:</p>
    <ul>
      <li class="bulletList">Limit CPU consumption within a namespace</li>
      <li class="bulletList">Limit memory consumption within a namespace</li>
      <li class="bulletList">Limit the absolute number of objects such as Pods, Services, ReplicationControllers, Replicas, Deployments, etc. operating within a namespace</li>
      <li class="bulletList">Limit<a id="_idIndexMarker578"/> consumption of storage resources based on the associated storage class</li>
    </ul>
    <p class="normal">There are a lot of use cases, and you can discover all of them directly in the Kubernetes documentation. Now, let’s learn how to define <code class="inlineCode">ResourceQuota</code> in a namespace.</p>
    <h2 class="heading-2" id="_idParaDest-246">Creating a ResourceQuota</h2>
    <p class="normal">To demonstrate <a id="_idIndexMarker579"/>the usefulness of <code class="inlineCode">ResourceQuota</code>, we are going to create one <code class="inlineCode">ResourceQuota</code> object for the namespace <code class="inlineCode">quota-ns</code>. This <code class="inlineCode">ResourceQuota</code> will be used to create requests and limits that all the Pods within this namespace combined will be able to use. Here is the YAML file that will create <code class="inlineCode">ResourceQuota</code>; please note the resource kind:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># resourcequota.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ResourceQuota</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">resourcequota</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">quota-ns</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">hard:</span>
    <span class="hljs-attr">requests.cpu:</span> <span class="hljs-string">"1000m"</span>
    <span class="hljs-attr">requests.memory:</span> <span class="hljs-string">"1Gi"</span>
    <span class="hljs-attr">limits.cpu:</span> <span class="hljs-string">"2000m"</span>
    <span class="hljs-attr">limits.memory:</span> <span class="hljs-string">"2Gi"</span>
</code></pre>
    <p class="normal">Keep in mind that the <code class="inlineCode">ResourceQuota</code> object is scoped to one namespace.</p>
    <p class="normal">This one is stating that, in this namespace, the following will occur:</p>
    <ul>
      <li class="bulletList">All the Pods combined won’t be able to request more than one CPU core.</li>
      <li class="bulletList">All the Pods combined won’t be able to request more than 1 GiB of memory.</li>
      <li class="bulletList">All the Pods combined won’t be able to consume more than two CPU cores.</li>
      <li class="bulletList">All the Pods combined won’t be able to consume more than 2 GiB of memory.</li>
    </ul>
    <p class="normal">Create the <code class="inlineCode">ResourceQuota</code> by applying the YAML configuration:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f resourcequota.yaml
resourcequota/resourcequota created
<span class="hljs-con-meta">$ </span>kubectl get quota -n quota-ns
NAME            AGE   REQUEST                                          LIMIT
resourcequota   4s    requests.cpu: 100m/1, requests.memory: 1Gi/1Gi   limits.cpu: 500m/2, limits.memory: 2Gi/2Gi
</code></pre>
    <p class="normal">Let’s check the current resources in the <code class="inlineCode">quota-ns</code> namespace as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get po -n quota-ns
NAME                             READY   STATUS    RESTARTS        AGE
nginx-with-request-and-limit-2   1/1     Running   1 (5h41m ago)   7h18m
<span class="hljs-con-meta">$ </span>kubectl top pod -n quota-ns
NAME                             CPU(cores)   MEMORY
</code></pre>
    <p class="normal">There is an nginx <a id="_idIndexMarker580"/>Pod (if you haven’t deleted the previous demo Pod) and usage is very low. </p>
    <p class="normal">Now, we have a new Pod YAML file but we request <code class="inlineCode">3Gi</code> memory for the Pod as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># pod-with-request-and-limit-3.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-with-request-and-limit-3</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">quota-ns</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:latest</span>
      <span class="hljs-attr">resources:</span>
        <span class="hljs-attr">requests:</span>
          <span class="hljs-attr">memory:</span> <span class="hljs-string">"3Gi"</span>
          <span class="hljs-attr">cpu:</span> <span class="hljs-string">"100m"</span>
        <span class="hljs-attr">limits:</span>
          <span class="hljs-attr">memory:</span> <span class="hljs-string">"4Gi"</span>
          <span class="hljs-attr">cpu:</span> <span class="hljs-string">"500m"</span>
</code></pre>
    <p class="normal">Now, let us try to create this Pod and see what the result will be:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f pod-with-request-and-limit-3.yaml
Error from server (Forbidden): error when creating "pod-with-request-and-limit-3.yaml": pods "nginx-with-request-and-limit-3" is forbidden: exceeded quota: resourcequota, requested: limits.memory=4Gi,requests.memory=3Gi, used: limits.memory=2Gi,requests.memory=1Gi, limited: limits.memory=2Gi,requests.memory=1Gi
</code></pre>
    <p class="normal">Yes, the error is very clear; we are requesting more resources than the quota, and Kubernetes will not allow the creation of a new Pod.</p>
    <p class="normal">You can have<a id="_idIndexMarker581"/> as many Pods and containers in the namespace, so long as they respect these constraints. Most of the time, <code class="inlineCode">ResourceQuotas</code> are used to enforce constraints on requests and limits, but they can also be used to enforce these limits per namespace.</p>
    <div class="packt_tip">
      <p class="normal">While setting up <code class="inlineCode">ResourceQuotas</code> at the namespace level, it’s crucial to prevent any single namespace from consuming all cluster resources; it’s also important to apply resource requests and limits at the Pod or Deployment level. This dual-layered approach ensures that resource-hogging is contained both within individual namespaces and at the application level. By enforcing these limits, you create a more predictable and stable environment, preventing any one component from disrupting the entire system.</p>
    </div>
    <p class="normal">In the following example, the preceding <code class="inlineCode">ResourceQuota</code> has been updated to specify that the namespace where it is created cannot hold more than 10 ConfigMaps and 5 services, which is pointless but a good example to demonstrate the different possibilities with <code class="inlineCode">ResourceQuota</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># resourcequota-with-object-count.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ResourceQuota</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">my-resourcequota</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">quota-ns</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">hard:</span>
    <span class="hljs-attr">requests.cpu:</span> <span class="hljs-string">"1000m"</span>
    <span class="hljs-attr">requests.memory:</span> <span class="hljs-string">"1Gi"</span>
    <span class="hljs-attr">limits.cpu:</span> <span class="hljs-string">"</span><span class="hljs-string">2000m"</span>
    <span class="hljs-attr">limits.memory:</span> <span class="hljs-string">"2Gi"</span>
    <span class="hljs-attr">configmaps:</span> <span class="hljs-string">"10"</span>
    <span class="hljs-attr">services:</span> <span class="hljs-string">"5"</span>
</code></pre>
    <div class="note">
      <p class="normal">When applying a <code class="inlineCode">ResourceQuota</code> YAML definition, ensure that the <code class="inlineCode">ResourceQuota</code> is assigned to the correct namespace. If the namespace isn’t specified within the YAML file, remember to use the <code class="inlineCode">--namespace</code> flag to specify where the <code class="inlineCode">ResourceQuota</code> should be applied.</p>
    </div>
    <p class="normal">Create<a id="_idIndexMarker582"/> the <code class="inlineCode">ResourceQuota</code> as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f resourcequota-with-object-count.yaml
</code></pre>
    <p class="normal">In the following section, we will learn about the storage <code class="inlineCode">ResourceQuota</code> in Kubernetes.</p>
    <h2 class="heading-2" id="_idParaDest-247">Storage resource quotas</h2>
    <p class="normal">In Kubernetes, resource<a id="_idIndexMarker583"/> quotas allow you to control the total storage resources requested within a namespace. You can set limits on both the sum of storage requests across all persistent volume claims and the number of persistent volume claims allowed. Additionally, quotas can be defined based on specific storage classes, enabling separate limits for different storage class types. For example, you can set quotas for gold and bronze storage classes separately. Starting from release 1.8, quotas also support local ephemeral storage, allowing you to limit the sum of local ephemeral storage requests and limits across all Pods within a namespace.</p>
    <p class="normal">Now, let’s learn how to list <code class="inlineCode">ResourceQuotas</code>.</p>
    <h2 class="heading-2" id="_idParaDest-248">Listing ResourceQuota</h2>
    <p class="normal"><code class="inlineCode">ResourceQuota</code> objects<a id="_idIndexMarker584"/> can be accessed through <code class="inlineCode">kubectl</code> using the quota’s resource name option. The <code class="inlineCode">kubectl get</code> command will do this for us:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get resourcequotas -n quota-ns
NAME            AGE   REQUEST                                          LIMIT
resourcequota   15m   requests.cpu: 100m/1, requests.memory: 1Gi/1Gi   limits.cpu: 500m/2, limits.memory: 2Gi/2Gi
</code></pre>
    <p class="normal">Now, let’s learn how to delete <code class="inlineCode">ResourceQuota</code> from a Kubernetes cluster.</p>
    <h2 class="heading-2" id="_idParaDest-249">Deleting ResourceQuota</h2>
    <p class="normal">To remove <a id="_idIndexMarker585"/>a <code class="inlineCode">ResourceQuota</code> object from your cluster, use the <code class="inlineCode">kubectl delete</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete -f resourcequota-with-object-count.yaml
resourcequota "my-resourcequota" deleted
</code></pre>
    <p class="normal">Now, let’s introduce the notion of <code class="inlineCode">LimitRange</code>.</p>
    <h2 class="heading-2" id="_idParaDest-250">Introducing LimitRange</h2>
    <p class="normal"><code class="inlineCode">LimitRange</code> is another object <a id="_idIndexMarker586"/>that is similar to <code class="inlineCode">ResourceQuota</code>, as it is created at the namespace level. The <code class="inlineCode">LimitRange</code> object is used to enforce default requests and limit values to individual containers. Even by using the <code class="inlineCode">ResourceQuota</code> object, you could create one object that consumes all the available resources in the namespace, so the <code class="inlineCode">LimitRange</code> object is here to prevent you from creating too small or too large containers within a namespace.</p>
    <p class="normal">Here is a YAML file that will create <code class="inlineCode">LimitRange</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># limitrange.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">LimitRange</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">my-limitrange</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">quota-ns</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">limits:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">default:</span> <span class="hljs-comment"># this section defines default limits</span>
        <span class="hljs-attr">cpu:</span> <span class="hljs-string">500m</span>
        <span class="hljs-attr">memory:</span> <span class="hljs-string">256Mi</span>
      <span class="hljs-attr">defaultRequest:</span> <span class="hljs-comment"># this section defines default requests</span>
        <span class="hljs-attr">cpu:</span> <span class="hljs-string">500m</span>
        <span class="hljs-attr">memory:</span> <span class="hljs-string">128Mi</span>
      <span class="hljs-attr">max:</span> <span class="hljs-comment"># max and min define the limit range</span>
        <span class="hljs-attr">cpu:</span> <span class="hljs-string">"</span><span class="hljs-string">1"</span>
        <span class="hljs-attr">memory:</span> <span class="hljs-string">1000Mi</span>
      <span class="hljs-attr">min:</span>
        <span class="hljs-attr">cpu:</span> <span class="hljs-string">100m</span>
        <span class="hljs-attr">memory:</span> <span class="hljs-string">128Mi</span>
      <span class="hljs-attr">type:</span> <span class="hljs-string">Container</span>
</code></pre>
    <p class="normal">As you can see, the <code class="inlineCode"><a id="_idIndexMarker587"/></code><code class="inlineCode">LimitRange</code> object consists of four important keys that all contain <code class="inlineCode">memory</code> and <code class="inlineCode">cpu</code> configuration. These keys are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">default</code>: This helps you enforce default values for the <code class="inlineCode">memory</code> and <code class="inlineCode">cpu</code> limits of containers if you forget to apply them at the Pod level. Each container that is set up without limits will inherit these default ones from the <code class="inlineCode">LimitRange</code> object.</li>
      <li class="bulletList"><code class="inlineCode">defaultRequest</code>: This is the same as <code class="inlineCode">default</code> but for the <code class="inlineCode">request</code> option. If you don’t set a <code class="inlineCode">request</code> option to one of your containers in a Pod, the ones from this key in the <code class="inlineCode">LimitRange</code> object will be automatically used by default.</li>
      <li class="bulletList"><code class="inlineCode">max</code>: This value indicates the maximum limit (not request) container that a Pod can set. You cannot configure a Pod with a limit value that is higher than this one. It is the same as the <code class="inlineCode">default</code> value in that it cannot be higher than the one defined here.</li>
      <li class="bulletList"><code class="inlineCode">min</code>: This value works like <code class="inlineCode">max</code> but for requests. It is the minimum amount of computing resources that a Pod can request, and the <code class="inlineCode">defaultRequest</code> option cannot be lower than this one.</li>
    </ul>
    <p class="normal">Finally, note that if you omit the <code class="inlineCode">default</code> and <code class="inlineCode">defaultRequest</code> keys, then the <code class="inlineCode">max</code> key will be used as the <code class="inlineCode">default</code> key, and the <code class="inlineCode">min</code> key will be used as the <code class="inlineCode">default</code> key.</p>
    <p class="normal">Defining <code class="inlineCode">LimitRange</code> is a good idea if you want to protect yourself from forgetting to set requests and limits on your Pods. At least with <code class="inlineCode">LimitRange</code>, these objects will have default limits and requests!</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f limitrange.yaml
</code></pre>
    <p class="normal">Now, let’s learn how to list <code class="inlineCode">LimitRanges</code>.</p>
    <h2 class="heading-2" id="_idParaDest-251">Listing LimitRanges</h2>
    <p class="normal">The <code class="inlineCode">kubectl</code> command<a id="_idIndexMarker588"/> line will help you list your <code class="inlineCode">LimitRanges</code>. Don’t forget to add the <code class="inlineCode">-n</code> flag to scope your request to a specific namespace:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get limitranges -n quota-ns
NAME            CREATED AT
my-limitrange   2024-03-10T16:13:00Z
<span class="hljs-con-meta">$ </span>kubectl describe limitranges my-limitrange -n quota-ns
Name:       my-limitrange
Namespace:  quota-ns
Type        Resource  Min    Max     Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---    ---     ---------------  -------------  -----------------------
Container   memory    128Mi  1000Mi  128Mi            256Mi          -
Container   cpu       100m   1       500m             500m           -
</code></pre>
    <p class="normal">Now, let’s learn how to delete <code class="inlineCode">LimitRange</code> from a namespace.</p>
    <h2 class="heading-2" id="_idParaDest-252">Deleting LimitRange</h2>
    <p class="normal">Deleting <code class="inlineCode">LimitRange</code> can be <a id="_idIndexMarker589"/>achieved using the <code class="inlineCode">kubectl</code> command-line tool. Here is how to proceed:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete limitranges my-limitrange -n quota-ns
limitrange "my-limitrange" deleted
</code></pre>
    <p class="normal">As always, don’t forget to add the <code class="inlineCode">-n</code> flag to scope your request to a specific namespace; otherwise, you may target the wrong one!</p>
    <h1 class="heading-1" id="_idParaDest-253">Summary</h1>
    <p class="normal">This chapter introduced you to namespaces, which are extremely important in Kubernetes. You cannot manage your cluster effectively without using namespaces because they provide logical resource isolation in your cluster. Most people use production and development namespaces, for example, or one namespace for each application. It is generally not rare to see clusters where hundreds of namespaces are created.</p>
    <p class="normal">We discovered that most Kubernetes resources are scoped to a namespace, although some are not. Keep in mind that, by default, Kubernetes is set up with a few preconfigured namespaces, such as <code class="inlineCode">kube-system</code>, and that it is generally a bad idea to change the things that run in these namespaces, especially if you do not know what you are doing.</p>
    <p class="normal">We also discovered that namespaces can be used to set quotas and limit the resources that Pods can consume, and it is a really good practice to set these quotas and limits at the namespace level, using the <code class="inlineCode">ResourceQuota</code> and <code class="inlineCode">LimitRange</code> objects, to prevent your Pods from consuming too many computing resources. By implementing these measures, you’re laying the foundation for effective capacity management, a critical consideration for organizations aiming to maintain the stability and efficiency of all applications running on a cluster.</p>
    <p class="normal">In the next chapter, we will learn how to handle configuration infomation and sensitive data in Kubernetes using ConfigMaps and Secrets.</p>
    <h1 class="heading-1" id="_idParaDest-254">Further reading</h1>
    <ul>
      <li class="bulletList"><em class="italic">Initial namespaces</em>: <span class="url">https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/#initial-namespaces</span></li>
      <li class="bulletList"><em class="italic">Resource Management for Pods and Containers</em>: <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"><span class="url">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</span></a></li>
      <li class="bulletList"><em class="italic">Resource Quotas</em>: <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/"><span class="url">https://kubernetes.io/docs/concepts/policy/resource-quotas/</span></a></li>
      <li class="bulletList"><em class="italic">Limit Ranges</em>: <a href="https://kubernetes.io/docs/concepts/policy/limit-range/"><span class="url">https://kubernetes.io/docs/concepts/policy/limit-range/</span></a></li>
      <li class="bulletList"><em class="italic">Resource metrics pipeline</em>: <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/"><span class="url">https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/</span></a></li>
      <li class="bulletList"><em class="italic">Configure Default Memory Requests and Limits for a Namespace</em>: <a href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/"><span class="url">https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/</span></a></li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-255">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img alt="" src="image/QR_Code119001106479081656.png"/></p>
  </div>
</body></html>