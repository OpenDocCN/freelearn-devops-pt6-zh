<html><head></head><body>
  <div id="_idContainer157" class="Basic-Text-Frame">
    <h1 class="chapterNumber">6</h1>
    <h1 id="_idParaDest-297" class="chapterTitle">Managing Storage</h1>
    <p class="normal">In this chapter, we’ll look at how Kubernetes manages storage. Storage is very different from compute, but at a high level they are both resources. Kubernetes as a generic platform takes the approach of abstracting storage behind a programming model and a set of plugins for storage providers. First, we’ll go into detail about the conceptual storage model and how storage is made available to containers in the cluster. Then, we’ll cover the common<a id="_idIndexMarker633"/> cloud platform <a id="_idIndexMarker634"/>storage providers, such as <strong class="keyWord">Amazon Web Services </strong>(<strong class="keyWord">AWS</strong>), <strong class="keyWord">Google Compute Engine </strong>(<strong class="keyWord">GCE</strong>), and Azure. Then we’ll look at a prominent open source storage provider, GlusterFS from Red Hat, which provides a distributed filesystem. We’ll also look into another solution – Ceph – that manages your data in containers as part of the Kubernetes cluster using the Rook operator. We’ll see how Kubernetes supports the <a id="_idIndexMarker635"/>integration of existing enterprise storage solutions. Finally, we will explore the <strong class="keyWord">Constrainer Storage Interface</strong> (<strong class="keyWord">CSI</strong>) and all the advanced capabilities it brings to the table.</p>
    <p class="normal">This chapter will cover the following main topics:</p>
    <ul>
      <li class="bulletList">Persistent volumes walk-through</li>
      <li class="bulletList">Demonstrating persistent volume storage end to end</li>
      <li class="bulletList">Public cloud storage volume types – GCE, AWS, and Azure</li>
      <li class="bulletList">GlusterFS and Ceph volumes in Kubernetes</li>
      <li class="bulletList">Integrating enterprise storage into Kubernetes</li>
      <li class="bulletList">The Container Storage Interface</li>
    </ul>
    <p class="normal">At the end of this chapter, you’ll have a solid understanding of how storage is represented in Kubernetes, the various storage options in each deployment environment (local testing, public cloud, and enterprise), and how to choose the best option for your use case.</p>
    <p class="normal">You should try the code samples in this chapter on minikube, or another cluster that supports storage adequately. The KinD cluster has some problems related to labeling nodes, which is necessary for some storage solutions.</p>
    <h1 id="_idParaDest-298" class="heading-1">Persistent volumes walk-through</h1>
    <p class="normal">In this section, we will understand the Kubernetes storage conceptual model and see how to map persistent storage into containers, so they can read and write. Let’s start by understanding the problem of storage.</p>
    <p class="normal">Containers and pods<a id="_idIndexMarker636"/> are ephemeral. Anything a container<a id="_idIndexMarker637"/> writes to its own filesystem gets wiped out when the container dies. Containers can also mount directories from their host node and read or write to them. These will survive container restarts, but the nodes themselves are not immortal. Also, if the pod itself is evicted and scheduled to a different node, the pod’s containers will not have access to the old node host’s filesystem.</p>
    <p class="normal">There are other problems, such as ownership of mounted hosted directories when the container dies. Just imagine a bunch of containers writing important data to various data directories on their host and then going away, leaving all that data all over the nodes with no direct way to tell what container wrote what data. You can try to record this information, but where would you record it? It’s pretty clear that for a large-scale system, you need persistent storage accessible from any node to reliably manage the data.</p>
    <h2 id="_idParaDest-299" class="heading-2">Understanding volumes</h2>
    <p class="normal">The basic Kubernetes storage abstraction<a id="_idIndexMarker638"/> is the volume. Containers mount volumes that are bound to their pod, and they access the storage wherever it may be as if it’s in their local filesystem. This is nothing new, and it is great, because as a developer who writes applications that need access to data, you don’t have to worry about where and how the data is stored. Kubernetes supports many types of volumes with their own distinctive features. Let’s review some of the main volume types.</p>
    <h3 id="_idParaDest-300" class="heading-3">Using emptyDir for intra-pod communication</h3>
    <p class="normal">It is very simple to share<a id="_idIndexMarker639"/> data between containers in<a id="_idIndexMarker640"/> the same pod using a shared volume. Container 1 and container 2 simply mount the same volume and can communicate by reading and writing to this shared space. The most basic volume is the <code class="inlineCode">emptyDir</code>. An <code class="inlineCode">emptyDir</code> volume is an empty directory on the host. Note that it is not persistent because when the pod is evicted or deleted, the contents are erased. If a container just crashes, the pod will stick around, and the restarted container can access the data in the volume. Another very interesting option is to use a RAM disk, by specifying the medium as <code class="inlineCode">Memory</code>. Now, your containers communicate through shared memory, which is much faster, but more volatile of course. If the node is restarted, the <code class="inlineCode">emptyDir</code>'s volume contents are lost.</p>
    <p class="normal">Here is a pod configuration file that has two containers that mount the same volume, called <code class="inlineCode">shared-volume</code>. The containers mount it in different paths, but when the <code class="inlineCode">hue-global-listener</code> container is writing a file to <code class="inlineCode">/notifications</code>, the <code class="inlineCode">hue-job-scheduler</code> will see that file under <code class="inlineCode">/incoming</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">hue-scheduler</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/hue-global-listener:1.0</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">hue-global-listener</span>
    <span class="hljs-attr">volumeMounts:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/notifications</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">shared-volume</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/hue-job-scheduler:1.0</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">hue-job-scheduler</span>
    <span class="hljs-attr">volumeMounts:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/incoming</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">shared-volume</span>
  <span class="hljs-attr">volumes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">shared-volume</span>
    <span class="hljs-attr">emptyDir:</span> {}
</code></pre>
    <p class="normal">To use the shared memory option, we just need to add <code class="inlineCode">medium: Memory</code> to the <code class="inlineCode">emptyDir</code> section:</p>
    <pre class="programlisting code"><code class="hljs-code">  <span class="hljs-attr">volumes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">shared-volume</span>
    <span class="hljs-attr">emptyDir:</span>
     <span class="hljs-attr">medium:</span> <span class="hljs-string">Memory</span>
</code></pre>
    <p class="normal">Note<a id="_idIndexMarker641"/> that memory-based <code class="inlineCode">emptyDir</code> counts <a id="_idIndexMarker642"/>toward the container’s memory limit.</p>
    <p class="normal">To verify it worked, let’s create the pod and then write a file using one container and read it using the other container:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f hue-scheduler.yaml
pod/hue-scheduler created
</code></pre>
    <p class="normal">Note that the pod has two containers:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get pod hue-scheduler -o json | jq .spec.containers
[
  {
    "image": "g1g1/hue-global-listener:1.0",
    "name": "hue-global-listener",
    "volumeMounts": [
      {
        "mountPath": "/notifications",
        "name": "shared-volume"
      },
      ...
    ]
    ...
  },
  {
    "image": "g1g1/hue-job-scheduler:1.0",
    "name": "hue-job-scheduler",
    "volumeMounts": [
      {
        "mountPath": "/incoming",
        "name": "shared-volume"
      },
      ...  
    ]
    ...
  }
]
</code></pre>
    <p class="normal">Now, we can create a file in the <code class="inlineCode">/notifications</code> directory of the <code class="inlineCode">hue-global-listener</code> container and list it in the <code class="inlineCode">/incoming</code> directory of the <code class="inlineCode">hue-job-scheduler</code> container:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl exec -it hue-scheduler -c hue-global-listener -- touch /notifications/1.txt
$ kubectl exec -it hue-scheduler -c hue-job-scheduler -- ls /incoming
1.txt
</code></pre>
    <p class="normal">As you can<a id="_idIndexMarker643"/> see, we are able to see a file that was <a id="_idIndexMarker644"/>created in one container in the file system of another container; thereby, the containers can communicate via the shared file system.</p>
    <h3 id="_idParaDest-301" class="heading-3">Using HostPath for intra-node communication</h3>
    <p class="normal">Sometimes, you want <a id="_idIndexMarker645"/>your pods to get access to some host<a id="_idIndexMarker646"/> information (for example, the Docker daemon) or you want pods on the same node to communicate with each other. This is useful if the pods know they are on the same host. Since Kubernetes schedules pods based on available resources, pods usually don’t know what other pods they share the node with. There are several cases where a pod can rely on other pods being scheduled with it on the same node:</p>
    <ul>
      <li class="bulletList">In a single-node cluster, all pods obviously share the same node</li>
      <li class="bulletList"><code class="inlineCode">DaemonSet</code> pods always share a node with any other pod that matches their selector</li>
      <li class="bulletList">Pods with required pod affinity are always scheduled together</li>
    </ul>
    <p class="normal">For example, in <em class="chapterRef">Chapter 5</em>, <em class="italic">Using Kubernetes Resources in Practice</em>, we discussed a <code class="inlineCode">DaemonSet</code> pod that serves as an aggregating proxy to other pods. Another way to implement this behavior is for the pods to simply write their data to a mounted volume that is bound to a host directory, and the <code class="inlineCode">DaemonSet</code> pod can directly read it and act on it.</p>
    <p class="normal">A <code class="inlineCode">HostPath</code> volume is a host file or directory that is mounted into a pod. Before you decide to use the <code class="inlineCode">HostPath</code> volume, make sure you understand the consequences:</p>
    <ul>
      <li class="bulletList">It is a security risk since access to the host filesystem can expose sensitive data (e.g. kubelet keys) </li>
      <li class="bulletList">The behavior of pods with the same configuration might be different if they are data-driven and the files on their host are different</li>
      <li class="bulletList">It can violate resource-based scheduling because Kubernetes can’t monitor <code class="inlineCode">HostPath</code> resources</li>
      <li class="bulletList">The containers that access host directories must have a security context with <code class="inlineCode">privileged</code> set to <code class="inlineCode">true</code> or, on the host side, you need to change the permissions to allow writing</li>
      <li class="bulletList">It’s difficult to coordinate disk usage across multiple pods on the same node.</li>
      <li class="bulletList">You can easily run out of disk space</li>
    </ul>
    <p class="normal">Here is a configuration file that mounts the <code class="inlineCode">/coupons</code> directory into the <code class="inlineCode">hue-coupon-hunter</code> container, which is mapped to the host’s <code class="inlineCode">/etc/hue/data/coupons</code> directory:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">hue-coupon-hunter</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">the_g1g1/hue-coupon-hunter</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">hue-coupon-hunter</span>
    <span class="hljs-attr">volumeMounts:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/coupons</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">coupons-volume</span>
  <span class="hljs-attr">volumes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">coupons-volume</span>
    <span class="hljs-attr">host-path:</span>
        <span class="hljs-attr">path:</span> <span class="hljs-string">/etc/hue/data/coupons</span>
</code></pre>
    <p class="normal">Since the <a id="_idIndexMarker647"/>pod doesn’t have a privileged security <a id="_idIndexMarker648"/>context, it will not be able to write to the host directory. Let’s change the container spec to enable it by adding a security context:</p>
    <pre class="programlisting code"><code class="hljs-code">  <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">the_g1g1/hue-coupon-hunter</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">hue-coupon-hunter</span>
    <span class="hljs-attr">volumeMounts:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/coupons</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">coupons-volume</span>
    <span class="hljs-attr">securityContext:</span>
      <span class="hljs-attr">privileged:</span> <span class="hljs-literal">true</span>
</code></pre>
    <p class="normal">In the following <a id="_idIndexMarker649"/>diagram, you can see that each container<a id="_idIndexMarker650"/> has its own local storage area inaccessible to other containers or pods, and the host’s <code class="inlineCode">/data</code> directory is mounted as a volume into both container 1 and container 2:</p>
    <figure class="mediaobject"><img src="../Images/B18998_06_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.1: Container local storage</p>
    <h3 id="_idParaDest-302" class="heading-3">Using local volumes for durable node storage</h3>
    <p class="normal">Local volumes <a id="_idIndexMarker651"/>are similar to <code class="inlineCode">HostPath</code>, but they persist across pod restarts and node restarts. In that sense they are considered persistent volumes. They were added in Kubernetes 1.7. As of Kubernetes 1.14 they are considered stable. The purpose of local volumes is to support Stateful Sets where specific pods need to be scheduled on nodes that contain specific storage volumes. Local volumes have node affinity annotations that simplify the binding of pods to the storage they need to access.</p>
    <p class="normal">We need to define a storage class for using local volumes. We will cover storage classes in depth later in this chapter. In one sentence, storage classes use a provisioner to allocate storage to pods. Let’s define the storage class in a file called <code class="inlineCode">local-storage-class.yaml</code> and create it:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">storage.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">StorageClass</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">local-storage</span>
<span class="hljs-attr">provisioner:</span> <span class="hljs-string">kubernetes.io/no-provisioner</span>
<span class="hljs-attr">volumeBindingMode:</span> <span class="hljs-string">WaitForFirstConsumer</span>
<span class="hljs-string">$</span> <span class="hljs-string">k</span> <span class="hljs-string">create</span> <span class="hljs-string">-f</span> <span class="hljs-string">local-storage-class.yaml</span>
<span class="hljs-string">storageclass.storage.k8s.io/local-storage</span> <span class="hljs-string">created</span> 
</code></pre>
    <p class="normal">Now, we can <a id="_idIndexMarker652"/>create a persistent volume using the storage class that will persist even after the pod that’s using it is terminated:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolume</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">local-pv</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">release:</span> <span class="hljs-string">stable</span>
    <span class="hljs-attr">capacity:</span> <span class="hljs-string">10Gi</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">capacity:</span>
    <span class="hljs-attr">storage:</span> <span class="hljs-string">10Gi</span>
  <span class="hljs-attr">volumeMode:</span> <span class="hljs-string">Filesystem</span>
  <span class="hljs-attr">accessModes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">persistentVolumeReclaimPolicy:</span> <span class="hljs-string">Delete</span>
  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">local-storage</span>
  <span class="hljs-attr">local:</span>
    <span class="hljs-attr">path:</span> <span class="hljs-string">/mnt/disks/disk-1</span>
  <span class="hljs-attr">nodeAffinity:</span>
    <span class="hljs-attr">required:</span>
      <span class="hljs-attr">nodeSelectorTerms:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">matchExpressions:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">kubernetes.io/hostname</span>
          <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span>
          <span class="hljs-attr">values:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-string">k3d-k3s-default-agent-1</span>
</code></pre>
    <h3 id="_idParaDest-303" class="heading-3">Provisioning persistent volumes</h3>
    <p class="normal">While <code class="inlineCode">emptyDir</code> volumes can <a id="_idIndexMarker653"/>be mounted and used by containers, they are not persistent and don’t require any special provisioning because they use existing storage on the node. <code class="inlineCode">HostPath</code> volumes persist on the original node, but if a pod is restarted on a different node, it can’t access the <code class="inlineCode">HostPath</code> volume from its previous node. Local volumes are real persistent volumes that use storage provisioned ahead of time by administrators or dynamic provisioning via storage classes. They persist on the node and can survive pod restarts and rescheduling and even node restarts. Some persistent volumes use external storage (not a disk physically attached to the node) provisioned ahead of time by administrators. In cloud environments, the provisioning may be very streamlined, but it is still required, and as a Kubernetes cluster administrator you have to at least make sure your storage quota is adequate and monitor usage versus quota diligently.</p>
    <p class="normal">Remember that <a id="_idIndexMarker654"/>persistent volumes are resources that the Kubernetes cluster is using, similar to nodes. As such they are not managed by the Kubernetes API server.</p>
    <p class="normal">You can provision resources statically or dynamically.</p>
    <h4 class="heading-4">Provisioning persistent volumes statically</h4>
    <p class="normal">Static <a id="_idIndexMarker655"/>provisioning is straightforward. The cluster administrator creates persistent volumes backed up by some storage media ahead of time, and these persistent volumes can be claimed by containers.</p>
    <h4 class="heading-4">Provisioning persistent volumes dynamically</h4>
    <p class="normal">Dynamic <a id="_idIndexMarker656"/>provisioning may happen when a persistent volume claim doesn’t match any of the statically provisioned persistent volumes. If the claim specified a storage class and the administrator configured that class for dynamic provisioning, then a persistent volume may be provisioned on the fly. We will see examples later when we discuss persistent volume claims and storage classes.</p>
    <h4 class="heading-4">Provisioning persistent volumes externally</h4>
    <p class="normal">Kubernetes <a id="_idIndexMarker657"/>originally contained a lot of code for storage provisioning “in-tree” as part of the main Kubernetes code base. With the introduction of CSI, storage provisioners started to migrate out of Kubernetes core into volume plugins (AKA out-of-tree). External provisioners work just like in-tree dynamic provisioners but can be deployed and updated independently. Most in-tree storage provisioners have been migrated out-of-tree. Check out this project for a library and guidelines for writing external storage provisioners: <a href="https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner"><span class="url">https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner</span></a>.</p>
    <h2 id="_idParaDest-304" class="heading-2">Creating persistent volumes</h2>
    <p class="normal">Here is the<a id="_idIndexMarker658"/> configuration file for an NFS persistent volume:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolume</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">pv-777</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">capacity:</span>
    <span class="hljs-attr">storage:</span> <span class="hljs-string">10Gi</span>
  <span class="hljs-attr">volumeMode:</span> <span class="hljs-string">Filesystem</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteMany</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadOnlyMany</span>
  <span class="hljs-attr">persistentVolumeReclaimPolicy:</span> <span class="hljs-string">Recycle</span>
  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">slow</span>
  <span class="hljs-attr">mountOptions:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">hard</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">nfsvers=4.2</span>
  <span class="hljs-attr">nfs:</span>
    <span class="hljs-attr">path:</span> <span class="hljs-string">/tmp</span>
    <span class="hljs-attr">server:</span> <span class="hljs-string">nfs-server.default.svc.cluster.local</span>
</code></pre>
    <p class="normal">A persistent volume has a spec and metadata that possibly includes a storage class name. Let’s focus on the spec here. There are six sections: capacity, volume mode, access modes, reclaim policy, storage class, and the volume type (<code class="inlineCode">nfs</code> in the example).</p>
    <h3 id="_idParaDest-305" class="heading-3">Capacity</h3>
    <p class="normal">Each volume <a id="_idIndexMarker659"/>has a designated amount of storage. Storage claims may be satisfied by persistent volumes that have at least that amount of storage. In the example, the persistent volume has a capacity of 10 gibibytes (a single gibibyte is 2 to the power of 30 bytes). </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">capacity:</span>
    <span class="hljs-attr">storage:</span> <span class="hljs-string">10Gi</span>
</code></pre>
    <p class="normal">It is important when allocating static persistent volumes to understand the storage request patterns. For example, if you provision 20 persistent volumes with 100 GiB capacity and a container claims a persistent volume with 150 GiB, then this claim will not be satisfied <a id="_idIndexMarker660"/>even though there is enough capacity overall in the cluster.</p>
    <h3 id="_idParaDest-306" class="heading-3">Volume mode</h3>
    <p class="normal">The optional <a id="_idIndexMarker661"/>volume mode was added in Kubernetes 1.9 as an Alpha feature (moved to Beta in Kubernetes 1.13) for static provisioning. It lets you specify if you want a file system (<code class="inlineCode">Filesystem</code>) or raw storage (<code class="inlineCode">Block</code>). If you don’t specify volume mode, then the default is <code class="inlineCode">Filesystem</code>, just like it was pre-1.9.</p>
    <h3 id="_idParaDest-307" class="heading-3">Access modes</h3>
    <p class="normal">There are three <a id="_idIndexMarker662"/>access modes:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">ReadOnlyMany</code>: Can be mounted read-only by many nodes</li>
      <li class="bulletList"><code class="inlineCode">ReadWriteOnce</code>: Can be mounted as read-write by a single node</li>
      <li class="bulletList"><code class="inlineCode">ReadWriteMany</code>: Can be mounted as read-write by many nodes</li>
    </ul>
    <p class="normal">The storage is mounted to nodes, so even with <code class="inlineCode">ReadWriteOnce</code>, multiple containers on the same node can mount the volume and write to it. If that causes a problem, you need to handle it through some other mechanism (for example, claim the volume only in DaemonSet pods that you know will have just one per node).</p>
    <p class="normal">Different storage providers support some subset of these modes. When you provision a persistent volume, you can specify which modes it will support. For example, NFS supports all modes, but in the example, only these modes were enabled:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteMany</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadOnlyMany</span>
</code></pre>
    <h3 id="_idParaDest-308" class="heading-3">Reclaim policy</h3>
    <p class="normal">The reclaim <a id="_idIndexMarker663"/>policy determines what happens when a persistent volume claim is deleted. There are three different policies:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Retain</code> – the volume will need to be reclaimed manually</li>
      <li class="bulletList"><code class="inlineCode">Delete</code> – the content, the volume, and the backing storage are removed</li>
      <li class="bulletList"><code class="inlineCode">Recycle</code> – delete content only (<code class="inlineCode">rm -rf /volume/*</code>)</li>
    </ul>
    <p class="normal">The <code class="inlineCode">Retain</code> and <code class="inlineCode">Delete</code> policies mean the persistent volume is not available anymore for future claims. The <code class="inlineCode">Recycle</code> policy allows the volume to be claimed again.</p>
    <p class="normal">At the moment, <a id="_idIndexMarker664"/>NFS and <code class="inlineCode">HostPath</code> support the recycle policy, while AWS EBS, GCE PD, Azure disk, and Cinder volumes support the delete policy. Note that dynamically provisioned volumes are always deleted.</p>
    <h3 id="_idParaDest-309" class="heading-3">Storage class</h3>
    <p class="normal">You can <a id="_idIndexMarker665"/>specify a storage class using the optional <code class="inlineCode">storageClassName</code> field of the spec. If you do then only persistent volume claims that specify the same storage class can be bound to the persistent volume. If you don’t specify a storage class, then only PV claims that don’t specify a storage class can be bound to it.</p>
    <pre class="programlisting code"><code class="hljs-code">  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">slow</span>
</code></pre>
    <h3 id="_idParaDest-310" class="heading-3">Volume type</h3>
    <p class="normal">The volume type is<a id="_idIndexMarker666"/> specified by name in the spec. There is no <code class="inlineCode">volumeType</code> stanza in the spec. In the preceding example, <code class="inlineCode">nfs</code> is the volume type:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">nfs:</span>
    <span class="hljs-attr">path:</span> <span class="hljs-string">/tmp</span>
    <span class="hljs-attr">server:</span> <span class="hljs-number">172.17.0.8</span>
</code></pre>
    <p class="normal">Each volume type may have its own set of parameters. In this case, it’s a path and server.</p>
    <p class="normal">We will go over various volume types later.</p>
    <h3 id="_idParaDest-311" class="heading-3">Mount options</h3>
    <p class="normal">Some persistent <a id="_idIndexMarker667"/>volume types have additional mount options you can specify. The mount options are not validated. If you provide an invalid mount option, the volume provisioning will fail. For example, NFS supports additional mount options:</p>
    <pre class="programlisting code"><code class="hljs-code">  <span class="hljs-attr">mountOptions:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">hard</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">nfsvers=4.1</span>
</code></pre>
    <p class="normal">Now that we have looked at provisioning a single persistent volume, let’s look at projected volumes, which add more flexibility and abstraction of storage.</p>
    <h2 id="_idParaDest-312" class="heading-2">Projected volumes</h2>
    <p class="normal">Projected volumes<a id="_idIndexMarker668"/> allow you to mount multiple persistent volumes into the same directory. You need to be careful of naming conflicts of course.</p>
    <p class="normal">The following volume types support projected volumes: </p>
    <ul>
      <li class="bulletList"><code class="inlineCode">ConfigMap</code></li>
      <li class="bulletList"><code class="inlineCode">Secret </code></li>
      <li class="bulletList"><code class="inlineCode">SownwardAPI </code></li>
      <li class="bulletList"><code class="inlineCode">ServiceAccountToken</code></li>
    </ul>
    <p class="normal">The snippet below projects a <code class="inlineCode">ConfigMap</code> and a <code class="inlineCode">Secret</code> into the same directory: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">projected-volumes-demo</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">projected-volumes-demo</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">busybox:1.28</span>
    <span class="hljs-attr">volumeMounts:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">projected-volumes-demo</span>
      <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/projected-volume"</span>
      <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">volumes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">projected-volumes-demo</span>
    <span class="hljs-attr">projected:</span>
      <span class="hljs-attr">sources:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">secret:</span>
          <span class="hljs-attr">name:</span> <span class="hljs-string">the-user</span>
          <span class="hljs-attr">items:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">username</span>
              <span class="hljs-attr">path:</span> <span class="hljs-string">the-group/the-user</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">configMap:</span>
          <span class="hljs-attr">name:</span> <span class="hljs-string">the-config-map</span>
          <span class="hljs-attr">items:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">config</span>
              <span class="hljs-attr">path:</span> <span class="hljs-string">the-group/the-config-map</span>
</code></pre>
    <p class="normal">The parameters for projected volumes are very similar to regular volumes. The exceptions are:</p>
    <ul>
      <li class="bulletList">To maintain consistency with <code class="inlineCode">ConfigMap</code> naming, the field <code class="inlineCode">secretName</code> has been updated to <code class="inlineCode">name</code> for secrets.</li>
      <li class="bulletList">The <code class="inlineCode">defaultMode</code> can only be set at the projected level and cannot be specified individually for each volume source (but you can specify the mode explicitly for each projection).</li>
    </ul>
    <p class="normal">Let’s look at a special kind of projected volume – the <code class="inlineCode">serviceAccountToken</code> exceptions.</p>
    <h3 id="_idParaDest-313" class="heading-3">serviceAccountToken projected volumes</h3>
    <p class="normal">Kubernetes pods can access the<a id="_idIndexMarker669"/> Kubernetes API server using the <a id="_idIndexMarker670"/>permissions of the service account associated with the pod. <code class="inlineCode">serviceAccountToken</code> projected volumes give you more granularity and control from a security standpoint. The token can have an expiration and a specific audience.</p>
    <p class="normal">More details are available here: <a href="https://kubernetes.io/docs/concepts/storage/projected-volumes/#serviceaccounttoken"><span class="url">https://kubernetes.io/docs/concepts/storage/projected-volumes/#serviceaccounttoken</span></a>.</p>
    <h2 id="_idParaDest-314" class="heading-2">Creating a local volume</h2>
    <p class="normal">Local volumes are <a id="_idIndexMarker671"/>static persistent disks that are allocated on a specific node. They are similar to <code class="inlineCode">HostPath</code> volumes, but Kubernetes knows which node a local volume belongs to and will schedule pods that bind to that local volume always to that node. This means the pod will not be evicted and scheduled to another node where the data is not available.</p>
    <p class="normal">Let’s create a<a id="_idIndexMarker672"/> local volume. First, we need to create a backing directory. For KinD and k3d clusters you can access the node through Docker:</p>
    <pre class="programlisting gen"><code class="hljs">$ docker exec -it k3d-k3s-default-agent-1 mkdir -p /mnt/disks/disk-1
$ docker exec -it k3d-k3s-default-agent-1 ls -la /mnt/disks
total 12
drwxr-xr-x 3 0 0 4096 Jun 29 21:40 .
drwxr-xr-x 3 0 0 4096 Jun 29 21:40 ..
drwxr-xr-x 2 0 0 4096 Jun 29 21:40 disk-1
</code></pre>
    <p class="normal">For minikube you need to use <code class="inlineCode">minikube ssh</code>.</p>
    <p class="normal">Now, we can create a local volume backed by the <code class="inlineCode">/mnt/disks/disk1</code> directory:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolume</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">local-pv</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">release:</span> <span class="hljs-string">stable</span>
    <span class="hljs-attr">capacity:</span> <span class="hljs-string">10Gi</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">capacity:</span>
    <span class="hljs-attr">storage:</span> <span class="hljs-string">10Gi</span>
  <span class="hljs-attr">volumeMode:</span> <span class="hljs-string">Filesystem</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">persistentVolumeReclaimPolicy:</span> <span class="hljs-string">Delete</span>
  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">local-storage</span>
  <span class="hljs-attr">local:</span>
    <span class="hljs-attr">path:</span> <span class="hljs-string">/mnt/disks/disk-1</span>
  <span class="hljs-attr">nodeAffinity:</span>
    <span class="hljs-attr">required:</span>
      <span class="hljs-attr">nodeSelectorTerms:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">matchExpressions:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">kubernetes.io/hostname</span>
              <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span>
              <span class="hljs-attr">values:</span>
                <span class="hljs-bullet">-</span> <span class="hljs-string">k3d-k3s-default-agent-1</span>
</code></pre>
    <p class="normal">Here is<a id="_idIndexMarker673"/> the <code class="inlineCode">create</code> command:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f local-volume.yaml
persistentvolume/local-pv created
$ k get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                         STORAGECLASS    REASON   AGE
local-pv   10Gi       RWO            Delete           Bound    default/local-storage-claim   local-storage            6m44s
</code></pre>
    <h2 id="_idParaDest-315" class="heading-2">Making persistent volume claims</h2>
    <p class="normal">When containers<a id="_idIndexMarker674"/> want access to some persistent storage they make a claim (or rather, the developer and cluster administrator coordinate on necessary storage resources to claim). Here is a sample claim that matches the persistent volume from the previous section - <em class="italic">Creating a local volume</em>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">local-storage-claim</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">resources:</span>
    <span class="hljs-attr">requests:</span>
      <span class="hljs-attr">storage:</span> <span class="hljs-string">8Gi</span>
  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">local-storage</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">release:</span> <span class="hljs-string">"stable"</span>
    <span class="hljs-attr">matchExpressions:</span>
      <span class="hljs-bullet">-</span> {<span class="hljs-attr">key:</span> <span class="hljs-string">capacity</span>, <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span>, <span class="hljs-attr">values:</span> [<span class="hljs-string">8Gi</span>, <span class="hljs-string">10Gi</span>]}
</code></pre>
    <p class="normal">Let’s create the claim and then explain what the different pieces do:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f local-persistent-volume-claim.yaml
persistentvolumeclaim/local-storage-claim created
$ k get pvc
NAME                  STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS    AGE
local-storage-claim    WaitForFirstConsumer    local-pv   10Gi       RWO            local-storage   21m
</code></pre>
    <p class="normal">The name <code class="inlineCode">local-storage-claim</code> will be important later when mounting the claim into a container.</p>
    <p class="normal">The access mode in the spec is <code class="inlineCode">ReadWriteOnce</code>, which means if the claim is satisfied no other claim with the <code class="inlineCode">ReadWriteOnce</code> access mode can be satisfied, but claims for <code class="inlineCode">ReadOnlyMany</code> can still be satisfied.</p>
    <p class="normal">The resources section requests 8 GiB. This can be satisfied by our persistent volume, which has a capacity of 10 Gi. But, this is a little wasteful because 2 Gi will not be used by definition.</p>
    <p class="normal">The storage class name is <code class="inlineCode">local-storage</code>. As mentioned earlier it must match the class name of the persistent volume. However, with PVC there is a difference between an empty class name (<code class="inlineCode">""</code>) and no class name at all. The former (an empty class name) matches persistent volumes with no storage class name. The latter (no class name) will be able to bind to persistent volumes only if the <code class="inlineCode">DefaultStorageClass</code> admission plugin is turned on and the default storage class is used.</p>
    <p class="normal">The selector section allows you to filter available volumes further. For example, here the volume must match the label <code class="inlineCode">release:stable</code> and also have a label with either <code class="inlineCode">capacity:8Gi</code> or <code class="inlineCode">capacity:10Gi</code>. Imagine that we have several other volumes provisioned with capacities of 20 Gi and 50 Gi. We don’t want to claim a 50 Gi volume when we only need 8 Gi.</p>
    <div class="note">
      <p class="normal">Kubernetes always tries to match the smallest volume that can satisfy a claim, but if there are no 8 Gi or 10 Gi volumes then the labels will prevent assigning a 20 Gi or 50 Gi volume and use dynamic provisioning instead.</p>
      <p class="normal">It’s important to realize that claims don’t mention volumes by name. You can’t claim a specific volume. The matching is done by Kubernetes based on storage class, capacity, and labels.</p>
    </div>
    <p class="normal">Finally, persistent volume <a id="_idIndexMarker675"/>claims belong to a namespace. Binding a persistent volume to a claim is exclusive. That means that a persistent volume will be bound to a namespace. Even if the access mode is <code class="inlineCode">ReadOnlyMany</code> or <code class="inlineCode">ReadWriteMany</code>, all the pods that mount the persistent volume claim must be from that claim’s namespace.</p>
    <h2 id="_idParaDest-316" class="heading-2">Mounting claims as volumes</h2>
    <p class="normal">OK. We have <a id="_idIndexMarker676"/>provisioned a volume and claimed it. It’s time to use the <a id="_idIndexMarker677"/>claimed storage in a container. This turns out to be pretty simple. First, the persistent volume claim must be used as a volume in the pod and then the containers in the pod can mount it, just like any other volume. Here is a pod manifest that specifies the persistent volume claim we created earlier (bound to the local persistent volume we provisioned):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">the-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">the-container</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/py-kube:0.3</span>
      <span class="hljs-attr">volumeMounts:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/mnt/data"</span>
        <span class="hljs-attr">name:</span> <span class="hljs-string">persistent-volume</span>
  <span class="hljs-attr">volumes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">persistent-volume</span>
      <span class="hljs-attr">persistentVolumeClaim:</span>
        <span class="hljs-attr">claimName:</span> <span class="hljs-string">local-storage-claim</span>
</code></pre>
    <p class="normal">The key is in the <code class="inlineCode">persistentVolumeClaim</code> section under volumes. The claim name (<code class="inlineCode">local-storage-claim</code> here) uniquely identifies within the current namespace the specific claim and makes it available as a volume (named <code class="inlineCode">persistent-volume</code> here). Then, the container can refer to it by its name and mount it to <code class="inlineCode">"/mnt/data"</code>.</p>
    <p class="normal">Before we <a id="_idIndexMarker678"/>create the pod it’s important to note that the <a id="_idIndexMarker679"/>persistent volume claim didn’t actually claim any storage yet and wasn’t bound to our local volume. The claim is pending until some container actually attempts to mount a volume using the claim:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get pvc
NAME                  STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    AGE
local-storage-claim   Pending                                      local-storage   6m14s
</code></pre>
    <p class="normal">Now, the claim will be bound when creating the pod:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f pod-with-local-claim.yaml
pod/the-pod created
$ k get pvc
NAME                  STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS    AGE
local-storage-claim   Bound    local-pv   100Gi      RWO            local-storage   20m
</code></pre>
    <h2 id="_idParaDest-317" class="heading-2">Raw block volumes</h2>
    <p class="normal">Kubernetes 1.9 added this capability as an Alpha feature. Kubernetes 1.13 moved it to Beta. Since Kubernetes 1.18 it is GA.</p>
    <p class="normal">Raw block volumes <a id="_idIndexMarker680"/>provide direct access to the underlying storage, which is not mediated via a file system abstraction. This is very useful for applications that require high-performance storage like databases or when consistent I/O performance and low latency are needed. The following storage providers support raw block volumes:</p>
    <ul>
      <li class="bulletList">AWSElasticBlockStore</li>
      <li class="bulletList">AzureDisk</li>
      <li class="bulletList"><strong class="keyWord">FC</strong> (<strong class="keyWord">Fibre Channel</strong>)</li>
      <li class="bulletList">GCE Persistent Disk</li>
      <li class="bulletList">iSCSI</li>
      <li class="bulletList">Local volume</li>
      <li class="bulletList">OpenStack Cinder</li>
      <li class="bulletList">RBD (Ceph Block Device)</li>
      <li class="bulletList">VsphereVolume</li>
    </ul>
    <p class="normal">In addition many CSI storage providers also offer raw block volume. For the full list check out: <a href="https://kubernetes-csi.github.io/docs/drivers.html"><span class="url">https://kubernetes-csi.github.io/docs/drivers.html</span></a>.</p>
    <p class="normal">Here is how to define a raw block volume using the FireChannel provider:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolume</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">block-pv</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">capacity:</span>
    <span class="hljs-attr">storage:</span> <span class="hljs-string">10Gi</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">volumeMode:</span> <span class="hljs-string">Block</span>
  <span class="hljs-attr">persistentVolumeReclaimPolicy:</span> <span class="hljs-string">Retain</span>
  <span class="hljs-attr">fc:</span>
    <span class="hljs-attr">targetWWNs:</span> [<span class="hljs-string">"</span><span class="hljs-string">50060e801049cfd1"</span>]
    <span class="hljs-attr">lun:</span> <span class="hljs-number">0</span>
    <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">false</span>
</code></pre>
    <p class="normal">A matching <strong class="keyWord">Persistent Volume Claim</strong> (<strong class="keyWord">PVC</strong>) MUST specify <code class="inlineCode">volumeMode: Block</code> as well. Here is what it looks like:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">block-pvc</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">volumeMode:</span> <span class="hljs-string">Block</span>
  <span class="hljs-attr">resources:</span>
    <span class="hljs-attr">requests:</span>
      <span class="hljs-attr">storage:</span> <span class="hljs-string">10Gi</span>
</code></pre>
    <p class="normal">Pods consume<a id="_idIndexMarker681"/> raw block volumes as devices under <code class="inlineCode">/dev</code> and NOT as mounted filesystems. Containers can then access these devices and read/write to them. In practice this means that I/O requests to block storage go directly to the underlying block storage and don’t pass through the file system drivers. This is in theory faster, but in practice it can actually decrease performance if your application benefits from file system buffering.</p>
    <p class="normal">Here is a pod with a container that binds the <code class="inlineCode">block-pvc</code> with the raw block storage as a device named <code class="inlineCode">/dev/xdva</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-with-block-volume</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">fc-container</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">fedora:26</span>
      <span class="hljs-attr">command:</span> [<span class="hljs-string">"/bin/sh"</span>, <span class="hljs-string">"-c"</span>]
      <span class="hljs-attr">args:</span> [<span class="hljs-string">"tail -f /dev/null"</span>]
      <span class="hljs-attr">volumeDevices:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">data</span>
          <span class="hljs-attr">devicePath:</span> <span class="hljs-string">/dev/xvda</span>
  <span class="hljs-attr">volumes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">data</span>
      <span class="hljs-attr">persistentVolumeClaim:</span>
        <span class="hljs-attr">claimName:</span> <span class="hljs-string">block-pvc</span>
</code></pre>
    <h2 id="_idParaDest-318" class="heading-2">CSI ephemeral volumes</h2>
    <p class="normal">We will cover <a id="_idIndexMarker682"/>the <strong class="keyWord">Container Storage Interface</strong> (<strong class="keyWord">CSI</strong>) in detail later in the chapter in the section <em class="italic">The Container Storage Interface</em>. CSI ephemeral volumes are backed by local storage on the node. These<a id="_idIndexMarker683"/> volumes’ lifecycles are tied to the pod’s lifecycle. In addition, they can only be mounted by containers of that pod, which is useful for populating secrets and certificates directly into a pod, without going through a Kubernetes secret object.</p>
    <p class="normal">Here is an example of a pod with a CSI ephemeral volume:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">the-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">the-container</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/py-kube:0.3</span>
      <span class="hljs-attr">volumeMounts:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/data"</span>
          <span class="hljs-attr">name:</span> <span class="hljs-string">the-volume</span>
      <span class="hljs-attr">command:</span> [ <span class="hljs-string">"sleep"</span>, <span class="hljs-string">"1000000"</span> ]
  <span class="hljs-attr">volumes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">the-volume</span>
      <span class="hljs-attr">csi:</span>
        <span class="hljs-attr">driver:</span> <span class="hljs-string">inline.storage.kubernetes.io</span>
        <span class="hljs-attr">volumeAttributes:</span>
          <span class="hljs-attr">key:</span> <span class="hljs-string">value</span>
</code></pre>
    <p class="normal">CSI ephemeral volumes have been GA since Kubernetes 1.25. However, they may not be supported by all CSI drivers. As usual check the list: <a href="https://kubernetes-csi.github.io/docs/drivers.html"><span class="url">https://kubernetes-csi.github.io/docs/drivers.html</span></a>.</p>
    <h2 id="_idParaDest-319" class="heading-2">Generic ephemeral volumes</h2>
    <p class="normal">Generic ephemeral volumes <a id="_idIndexMarker684"/>are yet another volume type that is tied to the pod lifecycle. When the pod is gone the generic ephemeral volume is gone.</p>
    <p class="normal">This volume type actually creates a full-fledged persistent volume claim. This provides several capabilities:</p>
    <ul>
      <li class="bulletList">The storage for the volume can be either local or network-attached.</li>
      <li class="bulletList">The volume has the option to be provisioned with a fixed size.</li>
      <li class="bulletList">Depending on the driver and specified parameters, the volume may contain initial data.</li>
      <li class="bulletList">If supported by the driver, typical operations such as snapshotting, cloning, resizing, and storage capacity tracking can be performed on the volumes.</li>
    </ul>
    <p class="normal">Here is an example of a <a id="_idIndexMarker685"/>pod with a generic ephemeral volume:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">the-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">the-container</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/py-kube:0.3</span>
      <span class="hljs-attr">volumeMounts:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/data"</span>
          <span class="hljs-attr">name:</span> <span class="hljs-string">the-volume</span>
      <span class="hljs-attr">command:</span> [ <span class="hljs-string">"sleep"</span>, <span class="hljs-string">"1000000"</span> ]
  <span class="hljs-attr">volumes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">the-volume</span>
      <span class="hljs-attr">ephemeral:</span>
        <span class="hljs-attr">volumeClaimTemplate:</span>
          <span class="hljs-attr">metadata:</span>
            <span class="hljs-attr">labels:</span>
              <span class="hljs-attr">type:</span> <span class="hljs-string">generic-ephemeral-volume</span>
          <span class="hljs-attr">spec:</span>
            <span class="hljs-attr">accessModes:</span> [ <span class="hljs-string">"ReadWriteOnce"</span> ]
            <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">generic-storage</span>
            <span class="hljs-attr">resources:</span>
              <span class="hljs-attr">requests:</span>
                <span class="hljs-attr">storage:</span> <span class="hljs-string">1Gi</span>
</code></pre>
    <p class="normal">Note that from a security point of view users that have permission to create pods, but not PVCs, can now create PVCs via generic ephemeral volumes. To prevent that it is possible to use admission control.</p>
    <h2 id="_idParaDest-320" class="heading-2">Storage classes</h2>
    <p class="normal">We’ve run into storage classes<a id="_idIndexMarker686"/> already. What are they exactly? Storage classes let an administrator configure a cluster with custom persistent storage (as long as there is a proper plugin to support it). A storage class has a name in the metadata (it must be specified in the <code class="inlineCode">storageClassName</code> file of the claim), a provisioner, a reclaim policy, and parameters.</p>
    <p class="normal">We declared a storage class for local storage earlier. Here is a sample storage class that uses AWS EBS as a provisioner (so, it works only on AWS):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">storage.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">StorageClass</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">standard</span>
<span class="hljs-attr">provisioner:</span> <span class="hljs-string">kubernetes.io/aws-ebs</span>
<span class="hljs-attr">parameters:</span>
  <span class="hljs-attr">type:</span> <span class="hljs-string">gp2</span>
<span class="hljs-attr">reclaimPolicy:</span> <span class="hljs-string">Retain</span>
<span class="hljs-attr">allowVolumeExpansion:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">mountOptions:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">debug</span>
<span class="hljs-attr">volumeBindingMode:</span> <span class="hljs-string">Immediate</span>
</code></pre>
    <p class="normal">You may create multiple storage classes for the same provisioner with different parameters. Each provisioner has its own parameters.</p>
    <p class="normal">The currently supported provisioners are:</p>
    <ul>
      <li class="bulletList">AWSElasticBlockStore</li>
      <li class="bulletList">AzureFile</li>
      <li class="bulletList">AzureDisk</li>
      <li class="bulletList">CephFS</li>
      <li class="bulletList">Cinder</li>
      <li class="bulletList">FC</li>
      <li class="bulletList">FlexVolume</li>
      <li class="bulletList">Flocker</li>
      <li class="bulletList">GCE Persistent Disk</li>
      <li class="bulletList">GlusterFS</li>
      <li class="bulletList">iSCSI</li>
      <li class="bulletList">Quobyte</li>
      <li class="bulletList">NFS</li>
      <li class="bulletList">RBD</li>
      <li class="bulletList">VsphereVolume</li>
      <li class="bulletList">PortworxVolume</li>
      <li class="bulletList">ScaleIO</li>
      <li class="bulletList">StorageOS</li>
      <li class="bulletList">Local</li>
    </ul>
    <p class="normal">This list doesn’t contain<a id="_idIndexMarker687"/> provisioners for other volume types, such as <code class="inlineCode">configMap</code> or <code class="inlineCode">secret</code>, that are not backed by your typical network storage. Those volume types don’t require a storage class. Utilizing volume types intelligently is a major part of architecting and managing your cluster.</p>
    <h3 id="_idParaDest-321" class="heading-3">Default storage class</h3>
    <p class="normal">The cluster administrator can also <a id="_idIndexMarker688"/>assign a default storage class. When a default storage class is assigned and the <code class="inlineCode">DefaultStorageClass</code> admission plugin is turned on, then claims with no storage class will be dynamically provisioned using the default storage class. If the default storage class is not defined or the admission plugin is not turned on, then claims with no storage class can only match volumes with no storage class.</p>
    <p class="normal">We covered a lot of ground and a lot of options for provisioning storage and using it in different ways. Let’s put everything together and show the whole process from start to finish.</p>
    <h1 id="_idParaDest-322" class="heading-1">Demonstrating persistent volume storage end to end</h1>
    <p class="normal">To illustrate all the <a id="_idIndexMarker689"/>concepts, let’s do a mini demonstration where we create a <code class="inlineCode">HostPath</code> volume, claim it, mount it, and have containers write to it. We will use k3d for this part.</p>
    <p class="normal">Let’s start by creating a <code class="inlineCode">hostPath</code> volume using the <code class="inlineCode">dir</code> storage class. Save the following in <code class="inlineCode">dir-persistent-volume.yaml</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolume</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">dir-pv</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">dir</span>
  <span class="hljs-attr">capacity:</span>
    <span class="hljs-attr">storage:</span> <span class="hljs-string">1Gi</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteMany</span>
  <span class="hljs-attr">hostPath:</span>
    <span class="hljs-attr">path:</span> <span class="hljs-string">"/tmp/data"</span>
</code></pre>
    <p class="normal">Then, let’s create it:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f dir-persistent-volume.yaml
persistentvolume/dir-pv created
</code></pre>
    <p class="normal">To check out the available volumes, you can use the resource type <code class="inlineCode">persistentvolumes</code> or <code class="inlineCode">pv</code> for short:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS    REASON   AGE
dir-pv     1Gi        RWX            Retain           Available           dir                      22s
</code></pre>
    <p class="normal">The<a id="_idIndexMarker690"/> capacity is 1 GiB as requested. The reclaim policy is <code class="inlineCode">Retain</code> because host path volumes are retained (not destroyed). The status is <code class="inlineCode">Available</code> because the volume has not been claimed yet. The access mode is specified as <code class="inlineCode">RWX</code>, which means <code class="inlineCode">ReadWriteMany</code>. All of the access modes have a shorthand version:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">RWO</code> – <code class="inlineCode">ReadWriteOnce</code></li>
      <li class="bulletList"><code class="inlineCode">ROX</code> – <code class="inlineCode">ReadOnlyMany</code></li>
      <li class="bulletList"><code class="inlineCode">RWX</code> – <code class="inlineCode">ReadWriteMany</code></li>
    </ul>
    <p class="normal">We have a persistent volume. Let’s create a claim. Save the following to <code class="inlineCode">dir-persistent-volume-claim.yaml</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">dir-pvc</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">resources:</span>
    <span class="hljs-attr">requests:</span>
      <span class="hljs-attr">storage:</span> <span class="hljs-string">1Gi</span>
</code></pre>
    <p class="normal">Then, run the following command:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f  dir-persistent-volume-claim.yaml
persistentvolumeclaim/dir-pvc created
</code></pre>
    <p class="normal">Let’s check the claim and the volume:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get pvc
NAME                  STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS    AGE
dir-pvc               Bound    dir-pv     1Gi        RWX            dir             106s
$ k get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS    REASON   AGE
dir-pv     1Gi        RWX            Retain           Bound    default/dir-pvc   dir                      4m25s
</code></pre>
    <p class="normal">As you can see, the claim and the volume are bound to each other and reference each other. The reason the binding works is that the same storage class is used by the volume and the claim. But, what happens if they don’t match? Let’s remove the storage class from the <a id="_idIndexMarker691"/>persistent volume claim and see what happens. Save the following persistent volume claim to <code class="inlineCode">some-persistent-volume-claim.yaml</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">some-pvc</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">accessModes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteMany</span>
  <span class="hljs-attr">resources:</span>
    <span class="hljs-attr">requests:</span>
      <span class="hljs-attr">storage:</span> <span class="hljs-string">1Gi</span>
</code></pre>
    <p class="normal">Then, create it:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f some-persistent-volume-claim.yaml
persistentvolumeclaim/some-pvc created
</code></pre>
    <p class="normal">Ok. It was created. Let’s check it out:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get pvc some-pvc
NAME       STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
some-pvc   Pending                                      local-path     3m29s
</code></pre>
    <p class="normal">Very interesting. The <code class="inlineCode">some-pvc</code> claim was associated with the <code class="inlineCode">local-path</code> storage class that we never specified, but it is still pending. Let’s understand why.</p>
    <p class="normal">Here is the <code class="inlineCode">local-path</code> storage class:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get storageclass local-path -o yaml
kind: StorageClass
metadata:
  annotations:
    objectset.rio.cattle.io/applied: H4sIAAAAAAAA/4yRT+vUMBCGv4rMua1bu1tKwIO u7EUEQdDzNJlux6aZkkwry7LfXbIqrIffn2PyZN7hfXIFXPg7xcQSwEBSiXimaupSxfJ2q6GAiYMDA9 /+oKPHlKCAmRQdKoK5AoYgisoSUj5K/5OsJtIqslQWVT3lNM4xUDzJ5VegWJ63CQxMTXogW128+czBvf/gnIQXIwLOBAa8WPTl30qvGkoL2jw5rT2V6ZKUZij+SbG5eZVRDKR0F8SpdDTg6rW8YzCgcSW4FeCxJ/+sjxHTCAbqrhmag20Pw9DbZtfu210z7JuhPnQ719m2w3cOe7fPof81W1DHfLlE2Th/IEUwEDHYkWJe8PCs gJgL8PxVPNsLGPhEnjRr2cSvM33k4Dicv4jLC34g60niiWPSo4S0zhTh9jsAAP//ytgh5S0CAAA
    objectset.rio.cattle.io/id: ""
    objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
    objectset.rio.cattle.io/owner-name: local-storage
    objectset.rio.cattle.io/owner-namespace: kube-system
    storageclass.kubernetes.io/is-default-class: "true"
  creationTimestamp: "2022-06-22T18:16:56Z"
  labels:
    objectset.rio.cattle.io/hash: 183f35c65ffbc3064603f43f1580d8c68a2dabd4
  name: local-path
  resourceVersion: "290"
  uid: b51cf456-f87e-48ac-9062-4652bf8f683e
provisioner: rancher.io/local-path
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
</code></pre>
    <p class="normal">It is a storage class that comes with k3d (k3s).</p>
    <p class="normal">Note the<a id="_idIndexMarker692"/> annotation: <code class="inlineCode">storageclass.kubernetes.io/is-default-class: "true"</code>. It tells Kubernetes that this is the default storage class. Since our PVC had no storage class name it was associated with the default storage class. But, why is the claim still pending? The reason is that <code class="inlineCode">volumeBindingMode</code> is <code class="inlineCode">WaitForFirstConsumer</code>. This means that the volume for the claim will be provisioned dynamically only when a container attempts to mount the volume via the claim.</p>
    <p class="normal">Back to our <code class="inlineCode">dir-pvc</code>. The<a id="_idIndexMarker693"/> final step is to create a pod with two containers and assign the claim as a volume to both of them. Save the following to <code class="inlineCode">shell-pod.yaml</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">just-a-shell</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">just-a-shell</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">a-shell</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/py-kube:0.3</span>
      <span class="hljs-attr">command:</span> [<span class="hljs-string">"sleep"</span>, <span class="hljs-string">"10000"</span>]
      <span class="hljs-attr">volumeMounts:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/data"</span>
          <span class="hljs-attr">name:</span> <span class="hljs-string">pv</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">another-shell</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/py-kube:0.3</span>
      <span class="hljs-attr">command:</span> [<span class="hljs-string">"</span><span class="hljs-string">sleep"</span>, <span class="hljs-string">"10000"</span>]
      <span class="hljs-attr">volumeMounts:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/another-data"</span>
          <span class="hljs-attr">name:</span> <span class="hljs-string">pv</span>
  <span class="hljs-attr">volumes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">pv</span>
      <span class="hljs-attr">persistentVolumeClaim:</span>
        <span class="hljs-attr">claimName:</span> <span class="hljs-string">dir-pvc</span>
</code></pre>
    <p class="normal">This pod has two containers that use the <code class="inlineCode">g1g1/py-kube:0.3</code> image and both just sleep for a long time. The idea is that the containers will keep running, so we can connect to them later and check their file system. The pod mounts our persistent volume claim with a volume name of <code class="inlineCode">pv</code>. Note that the volume specification is done at the pod level just once and multiple containers can mount it into different directories.</p>
    <p class="normal">Let’s create the pod and verify that both containers are running:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f shell-pod.yaml
pod/just-a-shell created
$ k get po just-a-shell -o wide
NAME           READY   STATUS    RESTARTS   AGE   IP            NODE                      NOMINATED NODE   READINESS GATES
just-a-shell   2/2     Running   0          74m   10.42.2.104   k3d-k3s-default-agent-1   &lt;none&gt;           &lt;none&gt;
</code></pre>
    <p class="normal">Then, connect to <a id="_idIndexMarker694"/>the node (<code class="inlineCode">k3d-k3s-default-agent-1</code>). This is the host whose <code class="inlineCode">/tmp/data</code> is the pod’s volume that is mounted as <code class="inlineCode">/data</code> and <code class="inlineCode">/another-data</code> into each of the running containers:</p>
    <pre class="programlisting gen"><code class="hljs">$ docker exec -it k3d-k3s-default-agent-1 sh
/ #
</code></pre>
    <p class="normal">Then, let’s create a file in the <code class="inlineCode">/tmp/data</code> directory on the host. It should be visible by both containers via the mounted volume:</p>
    <pre class="programlisting gen"><code class="hljs">/ # echo "yeah, it works" &gt; /tmp/data/cool.txt
</code></pre>
    <p class="normal">Let’s verify from the outside that the file <code class="inlineCode">cool.txt</code> is indeed available:</p>
    <pre class="programlisting gen"><code class="hljs">$ docker exec -it k3d-k3s-default-agent-1 cat /tmp/data/cool.txt
yeah, it works 
</code></pre>
    <p class="normal">Next, let’s verify the file is available in the containers (in their mapped directories):</p>
    <pre class="programlisting gen"><code class="hljs">$ k exec -it just-a-shell -c a-shell -- cat  /data/cool.txt
yeah, it works
$ k exec -it just-a-shell -c another-shell -- cat  /another-data/cool.txt
yeah, it works
</code></pre>
    <p class="normal">We can even create a new file, <code class="inlineCode">yo.txt</code>, in one of the containers and see that it’s available to the other container or to the node itself:</p>
    <pre class="programlisting gen"><code class="hljs">$ k exec -it just-a-shell -c another-shell – bash –c "echo yo  &gt; /another-data/yo.txt"
yo /another-data/yo.txt
$ k exec -it just-a-shell -c a-shell cat /data/yo.txt
yo
$ k exec -it just-a-shell -c another-shell cat /another-data/yo.txt
yo
</code></pre>
    <p class="normal">Yes. Everything <a id="_idIndexMarker695"/>works as expected and both containers share the same storage.</p>
    <h1 id="_idParaDest-323" class="heading-1">Public cloud storage volume types – GCE, AWS, and Azure</h1>
    <p class="normal">In this section, we’ll look at <a id="_idIndexMarker696"/>some of the common volume types available in the leading public cloud platforms. Managing storage at scale is a difficult task that eventually involves physical resources, similar to nodes. If you choose to run your Kubernetes cluster on a public cloud platform, you can let your cloud provider deal with all these challenges and focus on your system. But it’s important to understand the various options, constraints, and limitations of each volume type.</p>
    <div class="note">
      <p class="normal">Many of the volume types we will go over used to be handled by in-tree plugins (part of core Kubernetes), but have now migrated to out-of-tree CSI plugins.</p>
      <p class="normal">The CSI migration feature allows in-tree plugins that have corresponding out-of-tree CSI plugins to direct operations toward the out-of-tree plugins as a transitioning measure.</p>
      <p class="normal">We will cover the CSI itself later.</p>
    </div>
    <h2 id="_idParaDest-324" class="heading-2">AWS Elastic Block Store (EBS)</h2>
    <p class="normal">AWS provides <a id="_idIndexMarker697"/>the <strong class="keyWord">Elastic Block Store</strong> (<strong class="keyWord">EBS</strong>) as <a id="_idIndexMarker698"/>persistent storage for EC2 instances. An AWS Kubernetes cluster can use AWS EBS as persistent storage with the following limitations:</p>
    <ul>
      <li class="bulletList">The pods must run on AWS EC2 instances as nodes</li>
      <li class="bulletList">Pods can only access EBS volumes provisioned in their availability zone</li>
      <li class="bulletList">An EBS volume can be mounted on a single EC2 instance</li>
    </ul>
    <p class="normal">Those are severe limitations. The restriction for a single availability zone, while great for performance, eliminates the ability to share storage at scale or across a geographically distributed system without custom replication and synchronization. The limit of a single EBS volume to a single EC2 instance means even within the same availability zone, pods can’t share storage (even for reading) unless you make sure they run on the same node.</p>
    <p class="normal">This is an example of an in-tree plugin that also has a CSI driver and supports CSIMigration. That means that if the CSI driver for AWS EBS (<code class="inlineCode">ebs.csi.aws.com</code>) is installed, then the in-tree plugin will redirect all plugin operations to the out-of-tree plugin.</p>
    <p class="normal">It is also possible to disable loading the in-tree <code class="inlineCode">awsElasticBlockStore</code> storage plugin from being loaded by setting the <code class="inlineCode">InTreePluginAWSUnregister</code> feature gate to <code class="inlineCode">true</code> (the default is <code class="inlineCode">false</code>).</p>
    <p class="normal">Check out all the feature gates here: <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/"><span class="url">https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/</span></a>.</p>
    <p class="normal">Let’s see how to<a id="_idIndexMarker699"/> define an<a id="_idIndexMarker700"/> AWS EBS persistent volume (static provisioning):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolume</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">test-pv</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">accessModes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">capacity:</span>
    <span class="hljs-attr">storage:</span> <span class="hljs-string">5Gi</span>
  <span class="hljs-attr">csi:</span>
    <span class="hljs-attr">driver:</span> <span class="hljs-string">ebs.csi.aws.com</span>
    <span class="hljs-attr">volumeHandle:</span> {<span class="hljs-string">EBS</span> <span class="hljs-string">volume</span> <span class="hljs-string">ID</span>}
  <span class="hljs-attr">nodeAffinity:</span>
    <span class="hljs-attr">required:</span>
      <span class="hljs-attr">nodeSelectorTerms:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">matchExpressions:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">topology.ebs.csi.aws.com/zone</span>
              <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span>
              <span class="hljs-attr">values:</span>
                <span class="hljs-bullet">-</span> {<span class="hljs-string">availability</span> <span class="hljs-string">zone</span>}
</code></pre>
    <p class="normal">Then you need to define a PVC:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">ebs-claim</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">resources:</span>
    <span class="hljs-attr">requests:</span>
      <span class="hljs-attr">storage:</span> <span class="hljs-string">5Gi</span>
</code></pre>
    <p class="normal">Finally, a <a id="_idIndexMarker701"/>pod can <a id="_idIndexMarker702"/>mount the PVC:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">some-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">some-container</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">some-container</span>
    <span class="hljs-attr">volumeMounts:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">persistent-storage</span>
      <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/data</span>
  <span class="hljs-attr">volumes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">persistent-storage</span>
    <span class="hljs-attr">persistentVolumeClaim:</span>
      <span class="hljs-attr">claimName:</span> <span class="hljs-string">ebs-claim</span>
</code></pre>
    <h2 id="_idParaDest-325" class="heading-2">AWS Elastic File System (EFS)</h2>
    <p class="normal">AWS has a service<a id="_idIndexMarker703"/> called the <strong class="keyWord">Elastic File System</strong> (<strong class="keyWord">EFS</strong>). This is really a managed NFS service. It uses the NFS 4.1 protocol and has<a id="_idIndexMarker704"/> many benefits over EBS:</p>
    <ul>
      <li class="bulletList">Multiple EC2 instances can access the same files across multiple availability zones (but within the same region)</li>
      <li class="bulletList">Capacity is automatically scaled up and down based on actual usage</li>
      <li class="bulletList">You pay only for what you use</li>
      <li class="bulletList">You can connect on-premise servers to EFS over VPN</li>
      <li class="bulletList">EFS runs off SSD drives that are automatically replicated across availability zones</li>
    </ul>
    <p class="normal">That said, EFS is more expansive than EBS even when you consider the automatic replication to multiple AZs (assuming you fully utilize your EBS volumes). The recommended way to use EFS via its dedicated CSI driver: <a href="https://github.com/kubernetes-sigs/aws-efs-csi-driver"><span class="url">https://github.com/kubernetes-sigs/aws-efs-csi-driver</span></a>.</p>
    <p class="normal">Here is an example of static provisioning. First, define the persistent volume:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolume</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">efs-pv</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">capacity:</span>
    <span class="hljs-attr">storage:</span> <span class="hljs-string">1Gi</span>
  <span class="hljs-attr">volumeMode:</span> <span class="hljs-string">Filesystem</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">persistentVolumeReclaimPolicy:</span> <span class="hljs-string">Retain</span>
  <span class="hljs-attr">csi:</span>
    <span class="hljs-attr">driver:</span> <span class="hljs-string">efs.csi.aws.com</span>
    <span class="hljs-attr">volumeHandle:</span> <span class="hljs-string">&lt;Filesystem</span> <span class="hljs-string">Id&gt;</span> 
</code></pre>
    <p class="normal">You can find the <code class="inlineCode">Filesystem Id</code> using the AWS CLI:</p>
    <pre class="programlisting gen"><code class="hljs">aws efs describe-file-systems --query "FileSystems[*].FileSystemId"
</code></pre>
    <p class="normal">Then <a id="_idIndexMarker705"/>define <a id="_idIndexMarker706"/>a PVC:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">efs-claim</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">""</span>
  <span class="hljs-attr">resources:</span>
    <span class="hljs-attr">requests:</span>
      <span class="hljs-attr">storage:</span> <span class="hljs-string">1Gi</span>
</code></pre>
    <p class="normal">Here is a pod that consumes it:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">piVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">efs-app</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">app</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">centos</span>
    <span class="hljs-attr">command:</span> [<span class="hljs-string">"/bin/sh"</span>]
    <span class="hljs-attr">args:</span> [<span class="hljs-string">"-c"</span>, <span class="hljs-string">"while true; do echo $(date -u) &gt;&gt; /data/out.txt; sleep 5; done"</span>]
    <span class="hljs-attr">volumeMounts:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">persistent-storage</span>
      <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/data</span>
  <span class="hljs-attr">volumes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">persistent-storage</span>
    <span class="hljs-attr">persistentVolumeClaim:</span>
      <span class="hljs-attr">claimName:</span> <span class="hljs-string">efs-claim</span>
</code></pre>
    <p class="normal">You can also use dynamic provisioning by defining a proper storage class instead of creating a static volume:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">StorageClass</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">storage.k8s.io/v1</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">efs-sc</span>
<span class="hljs-attr">provisioner:</span> <span class="hljs-string">efs.csi.aws.com</span>
<span class="hljs-attr">parameters:</span>
  <span class="hljs-attr">provisioningMode:</span> <span class="hljs-string">efs-ap</span>
  <span class="hljs-attr">fileSystemId:</span> <span class="hljs-string">&lt;Filesystem</span> <span class="hljs-string">Id&gt;</span>
  <span class="hljs-attr">directoryPerms:</span> <span class="hljs-string">"700"</span>
  <span class="hljs-attr">gidRangeStart:</span> <span class="hljs-string">"1000"</span> <span class="hljs-comment"># optional</span>
  <span class="hljs-attr">gidRangeEnd:</span> <span class="hljs-string">"2000"</span> <span class="hljs-comment"># optional</span>
  <span class="hljs-attr">basePath:</span> <span class="hljs-string">"/dynamic_provisioning"</span> <span class="hljs-comment"># optional</span>
</code></pre>
    <p class="normal">The PVC is <a id="_idIndexMarker707"/>similar, but now uses the storage<a id="_idIndexMarker708"/> class name:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">efs-claim</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteMany</span>
  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">efs-sc</span>
  <span class="hljs-attr">resources:</span>
    <span class="hljs-attr">requests:</span>
      <span class="hljs-attr">storage:</span> <span class="hljs-string">5Gi</span>
</code></pre>
    <p class="normal">The pod consumes the PVC just like before:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">efs-app</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">app</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">centos</span>
      <span class="hljs-attr">command:</span> [<span class="hljs-string">"/bin/sh"</span>]
      <span class="hljs-attr">args:</span> [<span class="hljs-string">"-c"</span>, <span class="hljs-string">"while true; do echo $(date -u) &gt;&gt; /data/out; sleep 5; done"</span>]
      <span class="hljs-attr">volumeMounts:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">persistent-storage</span>
          <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/data</span>
  <span class="hljs-attr">volumes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">persistent-storage</span>
      <span class="hljs-attr">persistentVolumeClaim:</span>
        <span class="hljs-attr">claimName:</span> <span class="hljs-string">efs-claim</span>
</code></pre>
    <h2 id="_idParaDest-326" class="heading-2">GCE persistent disk</h2>
    <p class="normal">The <code class="inlineCode">gcePersistentDisk</code> volume<a id="_idIndexMarker709"/> type is very similar to <code class="inlineCode">awsElasticBlockStore</code>. You must provision the disk ahead of time. It can only be used <a id="_idIndexMarker710"/>by GCE instances in the same project and zone. But the same volume can be used as read-only on multiple instances. This means it supports <code class="inlineCode">ReadWriteOnce</code> and <code class="inlineCode">ReadOnlyMany</code>. You can use a GCE persistent disk to share data as read-only between multiple pods in the same zone.</p>
    <p class="normal">It also has a CSI driver called <code class="inlineCode">pd.csi.storage.gke.io</code> and supports CSIMigration.</p>
    <p class="normal">If the pod that’s using a persistent disk in <code class="inlineCode">ReadWriteOnce</code> mode is controlled by a replication controller, a replica set, or a deployment, the replica count must be 0 or 1. Trying to scale beyond 1 will fail for obvious reasons.</p>
    <p class="normal">Here is a storage class for GCE persistent disk using the CSI driver:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">storage.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">StorageClass</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">csi-gce-pd</span>
<span class="hljs-attr">provisioner:</span> <span class="hljs-string">pd.csi.storage.gke.io</span>
<span class="hljs-attr">parameters:</span>
  <span class="hljs-attr">labels:</span> <span class="hljs-string">key1=value1,key2=value2</span>
<span class="hljs-attr">volumeBindingMode:</span> <span class="hljs-string">WaitForFirstConsumer</span>
</code></pre>
    <p class="normal">Here is the PVC:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">gce-pd-pvc</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">csi-gce-pd</span>
  <span class="hljs-attr">resources:</span>
    <span class="hljs-attr">requests:</span>
      <span class="hljs-attr">storage:</span> <span class="hljs-string">200Gi</span>
</code></pre>
    <p class="normal">A Pod can<a id="_idIndexMarker711"/> consume it<a id="_idIndexMarker712"/> for dynamic provisioning:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">some-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">some-image</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">some-container</span>
    <span class="hljs-attr">volumeMounts:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/pd</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">some-volume</span>
  <span class="hljs-attr">volumes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">some-volume</span>
    <span class="hljs-attr">persistentVolumeClaim:</span>
       <span class="hljs-attr">claimName:</span> <span class="hljs-string">gce-pd-pvc</span>
       <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">false</span>
</code></pre>
    <p class="normal">The GCE persistent disk has supported a regional disk option since Kubernetes 1.10 (in Beta). Regional persistent disks automatically sync between two zones. Here is what the storage class looks like for a regional persistent disk:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">storage.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">StorageClass</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">csi-gce-pd</span>
<span class="hljs-attr">provisioner:</span> <span class="hljs-string">pd.csi.storage.gke.io</span>
<span class="hljs-attr">parameters:</span>
  <span class="hljs-attr">type:</span> <span class="hljs-string">pd-standard</span>
  <span class="hljs-attr">replication-type:</span> <span class="hljs-string">regional-pd</span>
<span class="hljs-attr">volumeBindingMode:</span> <span class="hljs-string">WaitForFirstConsumer</span>
</code></pre>
    <h2 id="_idParaDest-327" class="heading-2">Google Cloud Filestore</h2>
    <p class="normal">Google Cloud Filestore<a id="_idIndexMarker713"/> is the managed NFS file service of GCP. Kubernetes doesn’t<a id="_idIndexMarker714"/> have an in-tree plugin for it and there is no general-purpose supported CSI driver.</p>
    <p class="normal">However, there is a CSI driver used on GKE and if you are adventurous, you may want to try it even if you’re installing Kubernetes yourself on GCP and want to use Google Cloud Storage as a storage option.</p>
    <p class="normal">See: <a href="https://github.com/kubernetes-sigs/gcp-filestore-csi-driver"><span class="url">https://github.com/kubernetes-sigs/gcp-filestore-csi-driver</span></a>.</p>
    <h2 id="_idParaDest-328" class="heading-2">Azure data disk</h2>
    <p class="normal">The Azure data disk is <a id="_idIndexMarker715"/>a virtual hard disk stored in Azure storage. It’s similar in capabilities to <a id="_idIndexMarker716"/>AWS EBS or a GCE persistent disk.</p>
    <p class="normal">It also has a CSI driver called <code class="inlineCode">disk.csi.azure.com</code> and supports CSIMigration. See: <a href="https://github.com/kubernetes-sigs/azuredisk-csi-driver"><span class="url">https://github.com/kubernetes-sigs/azuredisk-csi-driver</span></a>.</p>
    <p class="normal">Here is an example of defining an Azure disk persistent volume:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolume</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">pv-azuredisk</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">capacity:</span>
    <span class="hljs-attr">storage:</span> <span class="hljs-string">10Gi</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">persistentVolumeReclaimPolicy:</span> <span class="hljs-string">Retain</span>
  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">managed-csi</span>
  <span class="hljs-attr">csi:</span>
    <span class="hljs-attr">driver:</span> <span class="hljs-string">disk.csi.azure.com</span>
    <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">false</span>
    <span class="hljs-attr">volumeHandle:</span> <span class="hljs-string">/subscriptions/{sub-id}/resourcegroups/{group-name}/providers/microsoft.compute/disks/{disk-id}</span>
    <span class="hljs-attr">volumeAttributes:</span>
      <span class="hljs-attr">fsType:</span> <span class="hljs-string">ext4</span>
</code></pre>
    <p class="normal">In addition to the mandatory <code class="inlineCode">diskName</code> and <code class="inlineCode">diskURI</code> parameters, it also has a few optional parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">kind</code>: The available options for disk storage configurations are <code class="inlineCode">Shared</code> (allowing multiple disks per storage account), <code class="inlineCode">Dedicated</code> (providing a single blob disk per storage account), or <code class="inlineCode">Managed</code> (offering an Azure-managed data disk). The default is <code class="inlineCode">Shared</code>.</li>
      <li class="bulletList"><code class="inlineCode">cachingMode</code>: The disk caching mode. This must be one of <code class="inlineCode">None</code>, <code class="inlineCode">ReadOnly</code>, or <code class="inlineCode">ReadWrite</code>. The default is <code class="inlineCode">None</code>.</li>
      <li class="bulletList"><code class="inlineCode">fsType</code>: The filesystem type set to mount. The default is <code class="inlineCode">ext4</code>.</li>
      <li class="bulletList"><code class="inlineCode">readOnly</code>: Whether the filesystem is used as <code class="inlineCode">readOnly</code>. The default is <code class="inlineCode">false</code>.</li>
    </ul>
    <p class="normal">Azure data disks <a id="_idIndexMarker717"/>are limited to 32 GiB. Each Azure<a id="_idIndexMarker718"/> VM can have up to 32 data disks. Larger VM sizes can have more disks attached. You can attach an Azure data disk to a single Azure VM.</p>
    <p class="normal">As usual you should create a PVC and consume it in a pod (or a pod controller).</p>
    <h2 id="_idParaDest-329" class="heading-2">Azure file</h2>
    <p class="normal">In addition to the data disk, Azure <a id="_idIndexMarker719"/>has also a shared filesystem <a id="_idIndexMarker720"/>similar to AWS EFS. However, Azure file storage uses the SMB/CIFS protocol (it supports SMB 2.1 and SMB 3.0). It is based on the Azure storage platform and has the same availability, durability, scalability, and geo-redundancy capabilities as Azure Blob, Table, or Queue storage.</p>
    <p class="normal">In order to use Azure file storage, you need to install on each client VM the <code class="inlineCode">cifs-utils</code> package. You also need to create a secret, which is a required parameter:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Secret</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">azure-file-secret</span>
<span class="hljs-attr">type:</span> <span class="hljs-string">Opaque</span>
<span class="hljs-attr">data:</span>
  <span class="hljs-attr">azurestorageaccountname:</span> <span class="hljs-string">&lt;base64</span> <span class="hljs-string">encoded</span> <span class="hljs-string">account</span> <span class="hljs-string">name&gt;</span>
  <span class="hljs-attr">azurestorageaccountkey:</span> <span class="hljs-string">&lt;base64</span> <span class="hljs-string">encoded</span> <span class="hljs-string">account</span> <span class="hljs-string">key&gt;</span>
</code></pre>
    <p class="normal">Here is a pod that uses Azure file storage:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
 <span class="hljs-attr">name:</span> <span class="hljs-string">some-pod</span>
<span class="hljs-attr">spec:</span>
 <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">some-container</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">some-container</span>
    <span class="hljs-attr">volumeMounts:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">some-volume</span>
        <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/azure</span>
 <span class="hljs-attr">volumes:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">some-volume</span>
        <span class="hljs-attr">azureFile:</span>
          <span class="hljs-attr">secretName:</span> <span class="hljs-string">azure-file-secret</span>
         <span class="hljs-attr">shareName:</span> <span class="hljs-string">azure-share</span>
          <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">false</span>
</code></pre>
    <p class="normal">Azure file storage<a id="_idIndexMarker721"/> supports sharing within the same region <a id="_idIndexMarker722"/>as well as connecting on-premise clients.</p>
    <p class="normal">This covers the public cloud storage volume types. Let’s look at some distributed storage volumes you can install on your own in your cluster. </p>
    <h1 id="_idParaDest-330" class="heading-1">GlusterFS and Ceph volumes in Kubernetes</h1>
    <p class="normal">GlusterFS <a id="_idIndexMarker723"/>and Ceph <a id="_idIndexMarker724"/>are two distributed persistent storage systems. GlusterFS is, at its core, a network filesystem. Ceph is, at its core, an object store. Both expose block, object, and filesystem interfaces. Both use the <code class="inlineCode">xfs</code> filesystem under the hood to store the data and metadata as <code class="inlineCode">xattr</code> attributes. There are several reasons why you may want to use GlusterFS or Ceph as persistent volumes in your Kubernetes cluster:</p>
    <ul>
      <li class="bulletList">You run on-premises and cloud storage is not available</li>
      <li class="bulletList">You may have a lot of data and applications that access the data in GlusterFS or Ceph</li>
      <li class="bulletList">You have operational expertise managing GlusterFS or Ceph</li>
      <li class="bulletList">You run in the cloud, but the limitations of the cloud platform persistent storage are a non-starter</li>
    </ul>
    <p class="normal">Let’s take a closer look at GlusterFS.</p>
    <h2 id="_idParaDest-331" class="heading-2">Using GlusterFS</h2>
    <p class="normal">GlusterFS is <a id="_idIndexMarker725"/>intentionally simple, exposing the underlying directories as they are and leaving it to clients (or middleware) to handle high availability, replication, and distribution. GlusterFS organizes the data into logical volumes, which encompass multiple nodes (machines) that contain bricks, which store files. Files are allocated to bricks according to DHT (distributed hash table). If files are renamed or the GlusterFS cluster is expanded or rebalanced, files may be moved between bricks. The following diagram shows the GlusterFS building blocks:</p>
    <figure class="mediaobject"><img src="../Images/B18998_06_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.2: GlusterFS building blocks</p>
    <p class="normal">To use a GlusterFS cluster as persistent storage for Kubernetes (assuming you have an up-and-running GlusterFS cluster), you need to follow several steps. In particular, the GlusterFS nodes are managed by the plugin as a Kubernetes service.</p>
    <h3 id="_idParaDest-332" class="heading-3">Creating endpoints</h3>
    <p class="normal">Here is an<a id="_idIndexMarker726"/> example of an endpoints resource that you can create as a normal Kubernetes resource using <code class="inlineCode">kubectl create</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">Endpoints</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">glusterfs-cluster</span>
<span class="hljs-attr">subsets:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">addresses:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">ip:</span> <span class="hljs-number">10.240.106.152</span>
  <span class="hljs-attr">ports:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">1</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">addresses:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">ip:</span> <span class="hljs-number">10.240.79.157</span>
  <span class="hljs-attr">ports:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">1</span> 
</code></pre>
    <h3 id="_idParaDest-333" class="heading-3">Adding a GlusterFS Kubernetes service</h3>
    <p class="normal">To make <a id="_idIndexMarker727"/>the endpoints persistent, you use a Kubernetes service with no selector to indicate the endpoints are managed manually:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">glusterfs-cluster</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">ports:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">1</span>
</code></pre>
    <h3 id="_idParaDest-334" class="heading-3">Creating pods</h3>
    <p class="normal">Finally, in the <a id="_idIndexMarker728"/>pod spec’s <code class="inlineCode">volumes</code> section, provide the following information:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">volumes:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">glusterfsvol</span>
  <span class="hljs-attr">glusterfs:</span>
    <span class="hljs-attr">endpoints:</span> <span class="hljs-string">glusterfs-cluster</span>
    <span class="hljs-attr">path:</span> <span class="hljs-string">kube_vol</span>
    <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>
</code></pre>
    <p class="normal">The containers can then mount <code class="inlineCode">glusterfsvol</code> by name.</p>
    <p class="normal">The endpoints tell the GlusterFS volume plugin how to find the storage nodes of the GlusterFS cluster.</p>
    <p class="normal">There was an<a id="_idIndexMarker729"/> effort to create a CSI driver for GlusterFS, but it was abandoned: <a href="https://github.com/gluster/gluster-csi-driver"><span class="url">https://github.com/gluster/gluster-csi-driver</span></a>.</p>
    <p class="normal">After covering GlusterFS let’s look at CephFS.</p>
    <h2 id="_idParaDest-335" class="heading-2">Using Ceph</h2>
    <p class="normal">Ceph’s object store<a id="_idIndexMarker730"/> can be accessed using multiple interfaces. Unlike GlusterFS, Ceph does a lot of work automatically. It does distribution, replication, and self-healing all on its own. The following diagram shows how RADOS – the underlying object store – can be accessed in multiple ways.</p>
    <figure class="mediaobject"><img src="../Images/B18998_06_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.3: Accessing RADOS</p>
    <p class="normal">Kubernetes supports Ceph via the <strong class="keyWord">Rados Block Device</strong> (<strong class="keyWord">RBD</strong>) interface.</p>
    <h3 id="_idParaDest-336" class="heading-3">Connecting to Ceph using RBD</h3>
    <p class="normal">You must <a id="_idIndexMarker731"/>install <code class="inlineCode">ceph-common</code> on each node of the Kubernetes<a id="_idIndexMarker732"/> cluster. Once you have your Ceph cluster up and running, you need to provide some information required by the Ceph RBD volume plugin in the pod configuration file:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">monitors</code>: Ceph monitors.</li>
      <li class="bulletList"><code class="inlineCode">pool</code>: The name of the RADOS pool. If not provided, the default RBD pool is used.</li>
      <li class="bulletList"><code class="inlineCode">image</code>: The image name that RBD has created.</li>
      <li class="bulletList"><code class="inlineCode">user</code>: The RADOS username. If not provided, the default admin is used.</li>
      <li class="bulletList"><code class="inlineCode">keyring</code>: The path to the keyring file. If not provided, the default <code class="inlineCode">/etc/ceph/keyring</code> is used.</li>
      <li class="bulletList"><code class="inlineCode">secretName</code>: The name of the authentication secrets. If provided, <code class="inlineCode">secretName</code> overrides <code class="inlineCode">keyring</code>. Note: see the following paragraph about how to create a secret.</li>
      <li class="bulletList"><code class="inlineCode">fsType</code>: The filesystem type (<code class="inlineCode">ext4</code>, <code class="inlineCode">xfs</code>, and so on) that is formatted on the device.</li>
      <li class="bulletList"><code class="inlineCode">readOnly</code>: Whether the filesystem is used as <code class="inlineCode">readOnly</code>.</li>
    </ul>
    <p class="normal">If the Ceph authentication secret is used, you need to create a secret object:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Secret</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">ceph-secret</span>
<span class="hljs-attr">type:</span> <span class="hljs-string">"kubernetes.io/rbd"</span>
<span class="hljs-attr">data:</span>
  <span class="hljs-attr">key:</span> <span class="hljs-string">QVFCMTZWMVZvRjVtRXhBQTVrQ1FzN2JCajhWVUxSdzI2Qzg0SEE9PQ==</span>
</code></pre>
    <p class="normal">The secret type is <code class="inlineCode">kubernetes.io/rbd</code>.</p>
    <p class="normal">Here is a sample pod that uses Ceph through RBD with a secret using the in-tree provider:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">rbd2</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">kubernetes/pause</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">rbd-rw</span>
      <span class="hljs-attr">volumeMounts:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">rbdpd</span>
        <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/mnt/rbd</span>
  <span class="hljs-attr">volumes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">rbdpd</span>
      <span class="hljs-attr">rbd:</span>
        <span class="hljs-attr">monitors:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">'10.16.154.78:6789'</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">'10.16.154.82:6789'</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">'10.16.154.83:6789'</span>
        <span class="hljs-attr">pool:</span> <span class="hljs-string">kube</span>
        <span class="hljs-attr">image:</span> <span class="hljs-string">foo</span>
        <span class="hljs-attr">fsType:</span> <span class="hljs-string">ext4</span>
        <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>
        <span class="hljs-attr">user:</span> <span class="hljs-string">admin</span>
        <span class="hljs-attr">secretRef:</span>
          <span class="hljs-attr">name:</span> <span class="hljs-string">ceph-secret</span>
</code></pre>
    <p class="normal">Ceph RBD supports <code class="inlineCode">ReadWriteOnce</code> and <code class="inlineCode">ReadOnlyMany</code> access modes. But, these days it is best to work with Ceph via Rook.</p>
    <h2 id="_idParaDest-337" class="heading-2">Rook</h2>
    <p class="normal">Rook is an <a id="_idIndexMarker733"/>open source cloud native storage orchestrator. It is currently a graduated CNCF project. It used to provide a consistent experience on top of multiple storage solutions like Ceph, edgeFS, Cassandra, Minio, NFS, CockroachDB, and YugabyteDB. But, eventually it laser-focused on supporting only Ceph. Here are the features<a id="_idIndexMarker734"/> Rook provides:</p>
    <ul>
      <li class="bulletList">Automating deployment</li>
      <li class="bulletList">Bootstrapping</li>
      <li class="bulletList">Configuration</li>
      <li class="bulletList">Provisioning</li>
      <li class="bulletList">Scaling</li>
      <li class="bulletList">Upgrading</li>
      <li class="bulletList">Migration</li>
      <li class="bulletList">Scheduling</li>
      <li class="bulletList">Lifecycle management</li>
      <li class="bulletList">Resource management</li>
      <li class="bulletList">Monitoring</li>
      <li class="bulletList">Disaster <a id="_idIndexMarker735"/>recovery</li>
    </ul>
    <p class="normal">Rook takes advantage of modern Kubernetes best practices like CRDs and operators.</p>
    <p class="normal">Here is the Rook <a id="_idIndexMarker736"/>architecture: </p>
    <figure class="mediaobject"><img src="../Images/B18998_06_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.4: Rook architecture</p>
    <p class="normal">Once you install<a id="_idIndexMarker737"/> the Rook operator you can create a Ceph cluster using a Rook CRD such as: <a href="https://github.com/rook/rook/blob/release-1.10/deploy/examples/cluster.yaml"><span class="url">https://github.com/rook/rook/blob/release-1.10/deploy/examples/cluster.yaml</span></a>.</p>
    <p class="normal">Here is a shortened version (without the comments):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">ceph.rook.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">CephCluster</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">rook-ceph</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">rook-ceph</span> <span class="hljs-comment"># namespace:cluster</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">cephVersion:</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">quay.io/ceph/ceph:v17.2.5</span>
    <span class="hljs-attr">allowUnsupported:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">dataDirHostPath:</span> <span class="hljs-string">/var/lib/rook</span>
  <span class="hljs-attr">skipUpgradeChecks:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">continueUpgradeAfterChecksEvenIfNotHealthy:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">waitTimeoutForHealthyOSDInMinutes:</span> <span class="hljs-number">10</span>
  <span class="hljs-attr">mon:</span>
    <span class="hljs-attr">count:</span> <span class="hljs-number">3</span>
    <span class="hljs-attr">allowMultiplePerNode:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">mgr:</span>
    <span class="hljs-attr">count:</span> <span class="hljs-number">2</span>
    <span class="hljs-attr">allowMultiplePerNode:</span> <span class="hljs-literal">false</span>
    <span class="hljs-attr">modules:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">pg_autoscaler</span>
        <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">dashboard:</span>
    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">ssl:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">monitoring:</span>    
   <span class="hljs-attr">enabled:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">network:</span>
    <span class="hljs-attr">connections:</span>
      <span class="hljs-attr">encryption:</span>
        <span class="hljs-attr">enabled:</span> <span class="hljs-literal">false</span>
      <span class="hljs-attr">compression:</span>
        <span class="hljs-attr">enabled:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">crashCollector:</span>
    <span class="hljs-attr">disable:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">logCollector:</span>
    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">periodicity:</span> <span class="hljs-string">daily</span> <span class="hljs-comment"># one of: hourly, daily, weekly, monthly</span>
    <span class="hljs-attr">maxLogSize:</span> <span class="hljs-string">500M</span> <span class="hljs-comment"># SUFFIX may be 'M' or 'G'. Must be at least 1M.</span>
  <span class="hljs-attr">cleanupPolicy:</span>
    <span class="hljs-attr">confirmation:</span> <span class="hljs-string">""</span>
    <span class="hljs-attr">sanitizeDisks:</span>
      <span class="hljs-attr">method:</span> <span class="hljs-string">quick</span>
      <span class="hljs-attr">dataSource:</span> <span class="hljs-string">zero</span>
      <span class="hljs-attr">iteration:</span> <span class="hljs-number">1</span>
    <span class="hljs-attr">allowUninstallWithVolumes:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">annotations:</span>
  <span class="hljs-attr">labels:</span>
  <span class="hljs-attr">resources:</span>
  <span class="hljs-attr">removeOSDsIfOutAndSafeToRemove:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">priorityClassNames:</span>
    <span class="hljs-attr">mon:</span> <span class="hljs-string">system-node-critical</span>
    <span class="hljs-attr">osd:</span> <span class="hljs-string">system-node-critical</span>
    <span class="hljs-attr">mgr:</span> <span class="hljs-string">system-cluster-critical</span>
  <span class="hljs-attr">storage:</span> <span class="hljs-comment"># cluster level storage configuration and selection</span>
    <span class="hljs-attr">useAllNodes:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">useAllDevices:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">config:</span>
    <span class="hljs-attr">onlyApplyOSDPlacement:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">disruptionManagement:</span>
    <span class="hljs-attr">managePodBudgets:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">osdMaintenanceTimeout:</span> <span class="hljs-number">30</span>
    <span class="hljs-attr">pgHealthCheckTimeout:</span> <span class="hljs-number">0</span>
    <span class="hljs-attr">manageMachineDisruptionBudgets:</span> <span class="hljs-literal">false</span>
    <span class="hljs-attr">machineDisruptionBudgetNamespace:</span> <span class="hljs-string">openshift-machine-api</span>
  <span class="hljs-attr">healthCheck:</span>
    <span class="hljs-attr">daemonHealth:</span>
      <span class="hljs-attr">mon:</span>
        <span class="hljs-attr">disabled:</span> <span class="hljs-literal">false</span>
        <span class="hljs-attr">interval:</span> <span class="hljs-string">45s</span>
      <span class="hljs-attr">osd:</span>
        <span class="hljs-attr">disabled:</span> <span class="hljs-literal">false</span>
        <span class="hljs-attr">interval:</span> <span class="hljs-string">60s</span>
      <span class="hljs-attr">status:</span>
        <span class="hljs-attr">disabled:</span> <span class="hljs-literal">false</span>
        <span class="hljs-attr">interval:</span> <span class="hljs-string">60s</span>
    <span class="hljs-attr">livenessProbe:</span>
      <span class="hljs-attr">mon:</span>
        <span class="hljs-attr">disabled:</span> <span class="hljs-literal">false</span>
      <span class="hljs-attr">mgr:</span>
        <span class="hljs-attr">disabled:</span> <span class="hljs-literal">false</span>
      <span class="hljs-attr">osd:</span>
        <span class="hljs-attr">disabled:</span> <span class="hljs-literal">false</span>
    <span class="hljs-attr">startupProbe:</span>
      <span class="hljs-attr">mon:</span>
        <span class="hljs-attr">disabled:</span> <span class="hljs-literal">false</span>
      <span class="hljs-attr">mgr:</span>
        <span class="hljs-attr">disabled:</span> <span class="hljs-literal">false</span>
      <span class="hljs-attr">osd:</span>
        <span class="hljs-attr">disabled:</span> <span class="hljs-literal">false</span>
</code></pre>
    <p class="normal">Here is a storage class for CephFS:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">storage.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">StorageClass</span>
<span class="hljs-attr">metadata:</span>
   <span class="hljs-attr">name:</span> <span class="hljs-string">rook-ceph-retain-bucket</span>
<span class="hljs-attr">provisioner:</span> <span class="hljs-string">rook-ceph.ceph.rook.io/bucket</span> <span class="hljs-comment"># driver:namespace:cluster</span>
<span class="hljs-comment"># set the reclaim policy to retain the bucket when its OBC is deleted</span>
<span class="hljs-attr">reclaimPolicy:</span> <span class="hljs-string">Retain</span>
<span class="hljs-attr">parameters:</span>
   <span class="hljs-attr">objectStoreName:</span> <span class="hljs-string">my-store</span> <span class="hljs-comment"># port 80 assumed</span>
   <span class="hljs-attr">objectStoreNamespace:</span> <span class="hljs-string">rook-ceph</span> <span class="hljs-comment"># namespace:cluster</span>
</code></pre>
    <p class="normal">The full code is available here: <a href="https://github.com/rook/rook/blob/release-1.10/deploy/examples/storageclass-bucket-retain.yaml"><span class="url">https://github.com/rook/rook/blob/release-1.10/deploy/examples/storageclass-bucket-retain.yaml</span></a>.</p>
    <p class="normal">Now that we’ve covered using distributed storage using GlusterFS, Ceph, and Rook, let’s look at enterprise storage options. </p>
    <h1 id="_idParaDest-338" class="heading-1">Integrating enterprise storage into Kubernetes</h1>
    <p class="normal">If you have an<a id="_idIndexMarker738"/> existing <strong class="keyWord">Storage Area Network</strong> (<strong class="keyWord">SAN</strong>) exposed over the iSCSI<a id="_idIndexMarker739"/> interface, Kubernetes has a volume<a id="_idIndexMarker740"/> plugin for you. It follows the same model as other shared persistent storage plugins we’ve seen earlier. It supports the following features:</p>
    <ul>
      <li class="bulletList">Connecting to one portal</li>
      <li class="bulletList">Mounting a device directly or via <code class="inlineCode">multipathd</code></li>
      <li class="bulletList">Formatting and partitioning any new device</li>
      <li class="bulletList">Authenticating via CHAP</li>
    </ul>
    <p class="normal">You must configure the iSCSI initiator, but you don’t have to provide any initiator information. All you need to provide is the following:</p>
    <ul>
      <li class="bulletList">IP address of the iSCSI target and port (if not the default <code class="inlineCode">3260</code>)</li>
      <li class="bulletList">Target’s <strong class="keyWord">IQN</strong> (<strong class="keyWord">iSCSI Qualified Name</strong>) – typically the<a id="_idIndexMarker741"/> reversed domain<a id="_idIndexMarker742"/> name</li>
      <li class="bulletList"><strong class="keyWord">LUN</strong> (<strong class="keyWord">Logical Unit Number</strong>)</li>
      <li class="bulletList">Filesystem type</li>
      <li class="bulletList"><code class="inlineCode">Readonly</code> Boolean flag</li>
    </ul>
    <p class="normal">The iSCSI plugin supports <code class="inlineCode">ReadWriteOnce</code> and <code class="inlineCode">ReadonlyMany</code>. Note that you can’t partition your device at this time. Here is an example pod with an iSCSI volume spec:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">iscsipd</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">iscsipd-rw</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">kubernetes/pause</span>
    <span class="hljs-attr">volumeMounts:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/mnt/iscsipd"</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">iscsipd-rw</span>
  <span class="hljs-attr">volumes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">iscsipd-rw</span>
    <span class="hljs-attr">iscsi:</span>
      <span class="hljs-attr">targetPortal:</span> <span class="hljs-number">10.0.2.15</span><span class="hljs-string">:3260</span>
      <span class="hljs-attr">portals:</span> [<span class="hljs-string">'10.0.2.16:3260'</span>, <span class="hljs-string">'10.0.2.17:3260'</span>]
      <span class="hljs-attr">iqn:</span> <span class="hljs-string">iqn.2001-04.com.example:storage.kube.sys1.xyz</span>
      <span class="hljs-attr">lun:</span> <span class="hljs-number">0</span>
      <span class="hljs-attr">fsType:</span> <span class="hljs-string">ext4</span>
      <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>
</code></pre>
    <h2 id="_idParaDest-339" class="heading-2">Other storage providers</h2>
    <p class="normal">The Kubernetes storage <a id="_idIndexMarker743"/>scene keeps innovating. A lot of companies adapt their products to Kubernetes and some companies and organizations build Kubernetes-dedicated storage solutions. Here are some of the more popular and mature solutions:</p>
    <ul>
      <li class="bulletList">OpenEBS</li>
      <li class="bulletList">Longhorn</li>
      <li class="bulletList">Portworx</li>
    </ul>
    <h1 id="_idParaDest-340" class="heading-1">The Container Storage Interface</h1>
    <p class="normal">The <strong class="keyWord">Container Storage Interface</strong> (<strong class="keyWord">CSI</strong>) is a<a id="_idIndexMarker744"/> standard interface for the interaction between container orchestrators and storage providers. It was developed by Kubernetes, Docker, Mesos, and Cloud Foundry. The idea is that storage providers implement just one CSI driver and all container orchestrators need to support only the CSI. It is the equivalent of CNI for storage.</p>
    <p class="normal">A CSI volume plugin was added in Kubernetes 1.9 as an Alpha feature and has been generally available since Kubernetes 1.13. The older FlexVolume approach (which you may have come across) is deprecated now.</p>
    <p class="normal">Here is a diagram that demonstrates how CSI works within Kubernetes:</p>
    <figure class="mediaobject"><img src="../Images/B18998_06_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.5: CSI architecture</p>
    <p class="normal">The migration <a id="_idIndexMarker745"/>effort to port all in-tree plugins to out-of-tree CSI drivers is well underway. See <a href="https://kubernetes-csi.github.io"><span class="url">https://kubernetes-csi.github.io</span></a> for more details.</p>
    <h2 id="_idParaDest-341" class="heading-2">Advanced storage features</h2>
    <p class="normal">These features are available <a id="_idIndexMarker746"/>only to CSI drivers. They represent the benefits of a uniform storage model that allows adding optional advanced functionality across all storage providers with a uniform interface.</p>
    <h3 id="_idParaDest-342" class="heading-3">Volume snapshots</h3>
    <p class="normal">Volume snapshots<a id="_idIndexMarker747"/> are <a id="_idIndexMarker748"/>generally available as of Kubernetes 1.20. They are exactly what they sound like – a snapshot of a volume at a certain point in time. You can create and later restore volumes from a snapshot. It’s interesting that the API objects associated with snapshots are CRDs and not part of the core Kubernetes API. The objects are:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">VolumeSnapshotClass</code></li>
      <li class="bulletList"><code class="inlineCode">VolumeSnapshotContents</code></li>
      <li class="bulletList"><code class="inlineCode">VolumeSnapshot</code></li>
    </ul>
    <p class="normal">Volume snapshots work using an <code class="inlineCode">external-prosnapshotter</code> sidecar container that the Kubernetes team developed. It watches for snapshot CRDs to be created and interacts with the snapshot controller, which can invoke the <code class="inlineCode">CreateSnapshot</code> and <code class="inlineCode">DeleteSnapshot</code> operations of CSI drivers that implement snapshot support.</p>
    <p class="normal">Here is how to<a id="_idIndexMarker749"/> declare a volume <a id="_idIndexMarker750"/>snapshot:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">snapshot.storage.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">VolumeSnapshot</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">new-snapshot-test</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">volumeSnapshotClassName:</span> <span class="hljs-string">csi-hostpath-snapclass</span>
  <span class="hljs-attr">source:</span>
    <span class="hljs-attr">persistentVolumeClaimName:</span> <span class="hljs-string">pvc-test</span>
</code></pre>
    <p class="normal">You can also provision volumes from a snapshot.</p>
    <p class="normal">Here is a persistent volume claim bound to a snapshot:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">restore-pvc</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">csi-hostpath-sc</span>
  <span class="hljs-attr">dataSource:</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">new-snapshot-test</span>
    <span class="hljs-attr">kind:</span> <span class="hljs-string">VolumeSnapshot</span>
    <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">snapshot.storage.k8s.io</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">resources:</span>
    <span class="hljs-attr">requests:</span>
      <span class="hljs-attr">storage:</span> <span class="hljs-string">10Gi</span>
</code></pre>
    <p class="normal">See <a href="https://github.com/kubernetes-csi/external-snapshotter#design"><span class="url">https://github.com/kubernetes-csi/external-snapshotter#design</span></a> for more details.</p>
    <h3 id="_idParaDest-343" class="heading-3">CSI volume cloning</h3>
    <p class="normal">Volume cloning <a id="_idIndexMarker751"/>is available in GA as of Kubernetes 1.18. Volume <a id="_idIndexMarker752"/>clones are new volumes that are populated with the content of an existing volume. Once the volume cloning is complete there is no relation between the original and the clone. Their content will diverge over time. You can perform a clone manually by creating a snapshot and then create a new volume from the snapshot. But, volume cloning is more streamlined and efficient.</p>
    <p class="normal">It only works for dynamic provisioning and uses the storage class of the source volume for the clone as well. You initiate a volume clone by specifying an existing persistent volume claim as a data source of a new persistent volume claim. That triggers the dynamic provisioning of a new volume that clones the source claim’s volume:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span>
<span class="hljs-attr">metadata:</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">clone-of-pvc-1</span>
    <span class="hljs-attr">namespace:</span> <span class="hljs-string">myns</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">accessModes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">cloning</span>
  <span class="hljs-attr">resources:</span>
    <span class="hljs-attr">requests:</span>
      <span class="hljs-attr">storage:</span> <span class="hljs-string">5Gi</span>
  <span class="hljs-attr">dataSource:</span>
    <span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">pvc-1</span>
</code></pre>
    <p class="normal">See <a href="https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/"><span class="url">https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/</span></a> for more details.</p>
    <h3 id="_idParaDest-344" class="heading-3">Storage capacity tracking</h3>
    <p class="normal">Storage capacity<a id="_idIndexMarker753"/> tracking (GA as of Kubernetes 1.24) allows the <a id="_idIndexMarker754"/>scheduler to better schedule pods that require storage into nodes that can provide that storage. This requires a CSI driver that supports storage capacity tracking.</p>
    <p class="normal">The CSI driver will create a <code class="inlineCode">CSIStorageCapacity</code> object for each storage class and determine which nodes have access to this storage. In addition the <code class="inlineCode">CSIDriverSpec</code>'s field <code class="inlineCode">StorageCapacity</code> must be set to true.</p>
    <p class="normal">When a pod specifies a storage class name in <code class="inlineCode">WaitForFirstConsumer</code> mode and the CSI driver has <code class="inlineCode">StorageCapacity</code> set to true the Kubernetes scheduler will consider the CSIStorageCapacity object associated with the storage class and schedule the pod only to nodes that <a id="_idIndexMarker755"/>have<a id="_idIndexMarker756"/> sufficient storage.</p>
    <p class="normal">Check out: <a href="https://kubernetes.io/docs/concepts/storage/storage-capacity"><span class="url">https://kubernetes.io/docs/concepts/storage/storage-capacity</span></a> for more details.</p>
    <h3 id="_idParaDest-345" class="heading-3">Volume health monitoring</h3>
    <p class="normal">Volume health monitoring <a id="_idIndexMarker757"/>is a recent addition to the storage APIs. It has been in<a id="_idIndexMarker758"/> Alpha since Kubernetes 1.21. It involves two components:</p>
    <ul>
      <li class="bulletList">An external health monitor </li>
      <li class="bulletList">The kubelet</li>
    </ul>
    <p class="normal">CSI drivers that support volume health monitoring will update PVCs with events on abnormal conditions of associated storage volumes. The external health monitor also watches nodes for failures and will report events on PVCs bound to these nodes.</p>
    <p class="normal">In the case where a CSI driver enables volume health monitoring from the node side, any abnormal condition detected will result in an event being reported for every pod that utilizes a PVC with the corresponding issue.</p>
    <p class="normal">There is also a new metric associated with volume health: <code class="inlineCode">kubelet_volume_stats_health_status_abnormal</code>.</p>
    <p class="normal">It has two labels: <code class="inlineCode">namespace</code> and <code class="inlineCode">persistentvolumeclaim</code>. The values are 0 or 1.</p>
    <p class="normal">More details are available here: <a href="https://kubernetes.io/docs/concepts/storage/volume-health-monitoring/"><span class="url">https://kubernetes.io/docs/concepts/storage/volume-health-monitoring/</span></a>.</p>
    <p class="normal">CSI is an exciting initiative that simplified the Kubernetes code base itself by externalizing storage drivers. It simplified the life of storage solutions that can develop out-of-tree drivers and added a lot of advanced capabilities to the Kubernetes storage story.</p>
    <h1 id="_idParaDest-346" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we took a deep look into storage in Kubernetes. We’ve looked at the generic conceptual model based on volumes, claims, and storage classes, as well as the implementation of volume plugins. Kubernetes eventually maps all storage systems into mounted filesystems in containers or devices of raw block storage. This straightforward model allows administrators to configure and hook up any storage system from local host directories, through cloud-based shared storage, all the way to enterprise storage systems. The transition of storage provisioners from in-tree to CSI-based out-of-tree drivers bodes well for the storage ecosystem. You should now have a clear understanding of how storage is modeled and implemented in Kubernetes and be able to make intelligent choices on how to implement storage in your Kubernetes cluster.</p>
    <p class="normal">In <em class="chapterRef">Chapter 7</em>, <em class="italic">Running Stateful Applications with Kubernetes</em>, we’ll see how Kubernetes can raise the level of abstraction and, on top of storage, help to develop, deploy, and operate stateful applications using concepts such as stateful sets.</p>
    <h1 id="_idParaDest-347" class="heading-1">Join us on Discord!</h1>
    <p class="normal">Read this book alongside other users, cloud experts, authors, and like-minded professionals.</p>
    <p class="normal">Ask questions, provide solutions to other readers, chat with the authors via. Ask Me Anything sessions and much more.</p>
    <p class="normal">Scan the QR code or visit the link to join the community now.</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img src="../Images/QR_Code844810820358034203.png" alt=""/></p>
  </div>
</body></html>