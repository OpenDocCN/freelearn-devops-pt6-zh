<html><head></head><body>
		<div id="_idContainer029">
			<h1 id="_idParaDest-57" class="chapter-number"><a id="_idTextAnchor056"/>3</h1>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor057"/>K3s Advanced Configurations and Management</h1>
			<p>This chapter covers more advanced configurations for your K3s clusters. By default, K3s includes a load balancer called KlipperLB, but it has some limitations. For example, you don't have to repeat a port while creating a service, and it affects the way that you use a regular load balancer and NodePort service. It works well for simple deployments. In case you need another load balancer instead of Klipper, we cover how to install <strong class="bold">MetalLB</strong>, a bare metal load balancer. Then, we cover how to use advanced storage configuration to support read/write access modes for storage volumes with Longhorn, substituting the default local storage class provided by K3s. After this, we will do some common cluster management, including upgrading K3s, backing up, and restoring the cluster.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Bare metal load balancer with MetalLB</li>
				<li>Setting up Longhorn for storage</li>
				<li>Upgrading your cluster</li>
				<li>Backing up and restoring your K3s configurations</li>
			</ul>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor058"/>Technical requirements</h1>
			<p>For this chapter, you need the following:</p>
			<ul>
				<li>Raspberry Pi 4 model B with 4 GB RAM (minimum suggested)</li>
				<li>A cloud server or VM with Ubuntu 20.04 LTS</li>
				<li>Helm v3 installed in your device or client</li>
			</ul>
			<p>With this, we are ready to learn this advanced configuration for K3s. So, let's get started.</p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor059"/>Bare metal load balancer with MetalLB</h1>
			<p>In this <a id="_idIndexMarker158"/>section, you are going to explore MetalLB <a id="_idIndexMarker159"/>as a bare metal load balancer, which can give you powerful features to expose your services at the edge.</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor060"/>Load balancer services in Kubernetes</h2>
			<p>Before <a id="_idIndexMarker160"/>starting with KlipperLB, it's necessary <a id="_idIndexMarker161"/>to give some context about load balancers <a id="_idIndexMarker162"/>in Kubernetes. Kubernetes uses services to communicate or access your application. A ClusterIP service creates a DNS record, so this service could be reachable from within the cluster. A NodePort service exposes the service on each node's IP at a static port. This port is in the range of 30000–32767. And, finally, Kubernetes supports a load balancer service that exposes the service externally using a cloud provider's load balancer. In the case of K3s, it's going to use KlipperLB by default.</p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor061"/>KlipperLB and MetalLB as bare metal load balancers</h2>
			<p>Edge devices and edge computing don't have a lot of resources, so it is common to find clusters <a id="_idIndexMarker163"/>that only have a single node. Generally, a Kubernetes <a id="_idIndexMarker164"/>load balancer service <a id="_idIndexMarker165"/>depends on the implementation of a specific cloud provider. It also works in layer 4 (the transport layer) to transmit <strong class="bold">Transmission Control Protocol</strong> (<strong class="bold">TCP</strong>) and <strong class="bold">User Datagram Protocol</strong> (<strong class="bold">UDP</strong>) protocols, and <a id="_idIndexMarker166"/>this load balancer is connected to a NodePort service too. So, in the case of edge devices, K3s implements KlipperLB.</p>
			<p>KlipperLB works really nicely on low-resource devices or environments as k3s' load balancer. But when you have multi-node clusters, maybe KlipperLB doesn't offer the best features for service availability. That's where MetalLB comes into the game. KlipperLB and MetalLB offer a bare metal load balancer service on Kubernetes. In this case, you can use these implementations on K3s.</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor062"/>KlipperLB and MetalLB – the goods and the bads</h2>
			<p>Now, let's mention the pros and cons of each of those bare metal load balancers in terms of the implementation, dependencies, and use case. So, let's get started with KlipperLB.</p>
			<p>The pros <a id="_idIndexMarker167"/>of KlipperLB are as follows:</p>
			<ul>
				<li>Pretty lightweight</li>
				<li>Simple to use with enough features for single node clusters</li>
			</ul>
			<p>The cons <a id="_idIndexMarker168"/>of KlipperLB are as follows:</p>
			<ul>
				<li>Depends on <strong class="bold">hostPort</strong> or available ports to expose a pod.</li>
				<li>If the port is not available, the load balancer service stays on the pending state.</li>
			</ul>
			<p>Talking about <a id="_idIndexMarker169"/>MetalLB, it uses layer 2 (the data link layer) where the format <a id="_idIndexMarker170"/>of data is defined. In this way, MetalLB uses a node for load balancing and has its own advantages and disadvantages. The next table summarizes this information:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B16945_Table_3.1.jpg" alt=""/>
				</div>
			</div>
			<p>In general, choose KlipperLB if you have a single node cluster and you want to avoid complex installations that use unique ports. Use MetalLB for multi-node clusters or installations where you can reuse ports and a more robust load balancer service.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor063"/>Installing MetalLB</h2>
			<p>You need <a id="_idIndexMarker171"/>a K3s installation with the <strong class="source-inline">--disable servicelb</strong> option; if you have a previous installation, you have to reinstall K3s. To install K3s with this option, follow these steps:</p>
			<ol>
				<li>Log in to your <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) or device using the following command:<p class="source-code"><strong class="bold">$ ssh ubuntu@YOUR_VM_IP</strong></p></li>
				<li>Install K3s using the following line. This applies to a simple ARM device for a basic installation without installing KlipperLB:<p class="source-code"><strong class="bold">$ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--write-kubeconfig-mode 644 --no-deploy traefik --disable traefik --disable servicelb" sh -s -</strong></p></li>
			</ol>
			<p>(<em class="italic">Optional</em>) Install K3s using the following lines. First, set the <strong class="source-inline">PUBLIC_IP</strong> environment variable with the IP of your device or VM: </p>
			<p class="source-code"><strong class="bold">$ PUBLIC_IP=YOUR_PUBLIC_IP|YOUR_PRIVATE_IP</strong></p>
			<p>Then, use the next lines to install K3s in a node that has a public IP:</p>
			<p class="source-code"><strong class="bold">$ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--write-kubeconfig-mode 644 --no-deploy traefik --disable traefik --tls-san "$PUBLIC_IP" --node-external-ip "$PUBLIC_IP" --disable servicelb" sh -s -</strong></p>
			<ol>
				<li value="3">Create a MetalLB namespace (<strong class="source-inline">metallb-system</strong>) with the official manifests, executing the following lines:<p class="source-code"><strong class="bold">$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/namespace.yaml</strong></p></li>
				<li>Before running the command to install MetalLB, you have to create a ConfigMap called <strong class="source-inline">metallb-config</strong> inside the <strong class="source-inline">metallb-system</strong> namespace. Let's call this file <strong class="source-inline">config.yaml</strong> with the following content:<p class="source-code"><strong class="bold">apiVersion: v1 </strong></p><p class="source-code"><strong class="bold">kind: ConfigMap </strong></p><p class="source-code"><strong class="bold">metadata: </strong></p><p class="source-code"><strong class="bold">  namespace: metallb-system </strong></p><p class="source-code"><strong class="bold">  name: config </strong></p><p class="source-code"><strong class="bold">data: </strong></p><p class="source-code"><strong class="bold">  config: | </strong></p><p class="source-code"><strong class="bold">    address-pools: </strong></p><p class="source-code"><strong class="bold">    - name: default </strong></p><p class="source-code"><strong class="bold">      protocol: layer2 </strong></p><p class="source-code"><strong class="bold">      addresses: </strong></p><p class="source-code"><strong class="bold">      - 192.168.0.240-192.168.0.250</strong></p></li>
				<li>Now, create the ConfigMap by executing the following command:<p class="source-code"><strong class="bold">$ kubectl apply -f config.yaml</strong></p></li>
				<li>Install MetalLB with the official manifests by executing the following lines:<p class="source-code"><strong class="bold">$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/metallb.yaml</strong></p></li>
			</ol>
			<p>Now that <a id="_idIndexMarker172"/>MetalLB is installed using YAML files, let's continue with the installation using Helm instead of YAML files.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">If you want to delete this or other installations, use the <strong class="source-inline">delete</strong> option instead of <strong class="source-inline">apply</strong> using the same command – for example, <strong class="source-inline">kubectl delete -f YOUR_YAML_FILE</strong>.</p>
			<p>In case you want to install MetalLB using Helm v3, follow these steps:</p>
			<ol>
				<li value="1">Add the Helm Chart repository of MetalLB using the following commands:<p class="source-code"><strong class="bold">$ helm repo add metallb https://metallb.github.io/metallb</strong></p></li>
				<li>Install MetalLB using Helm by executing the following command:<p class="source-code"><strong class="bold">$ helm install metallb -n metallb-system metallb/metallb</strong></p></li>
				<li>If you want to install MetalLB with the <strong class="source-inline">values.yaml</strong> file, execute the following lines:<p class="source-code"><strong class="bold">$ helm install metallb -n metallb-system metallb/metallb -f values.yaml</strong></p></li>
				<li>You have <a id="_idIndexMarker173"/>to create the <strong class="source-inline">values.yaml</strong> file, with the following example content:<p class="source-code"><strong class="bold">configInline</strong></p><p class="source-code"><strong class="bold">  address-pools:</strong></p><p class="source-code"><strong class="bold">   - name: default</strong></p><p class="source-code"><strong class="bold">     protocol: layer2</strong></p><p class="source-code"><strong class="bold">     addresses:</strong></p><p class="source-code"><strong class="bold">     - 192.168.0.240-192.168.0.250</strong></p></li>
				<li>Now, you have to create the ConfigMap based on the installation using <strong class="source-inline">kubectl</strong> and change the namespace to <strong class="source-inline">metallb-system</strong> and the name to <strong class="source-inline">metallb-config</strong>. Then, apply <strong class="source-inline">YAML</strong>:<p class="source-code"><strong class="bold">$ kubectl apply -f config.yaml</strong></p></li>
			</ol>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The <strong class="source-inline">addresses</strong> field corresponds to the range of IP addresses that MetalLB will use to assign to your services every time that you create a <strong class="source-inline">LoadBalancer</strong> service in Kubernetes.</p>
			<ol>
				<li value="6">Now, MetalLB is installed and ready to use.</li>
			</ol>
			<p>Now you have a fresh installation of MetalLB ready to use. Now you have to learn how to troubleshoot MetalLB in the next section.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor064"/>Troubleshooting MetalLB</h2>
			<p>Sometimes, it's necessary to troubleshoot our installations. If you are having trouble with your <a id="_idIndexMarker174"/>installation, here are some commands that you can use to troubleshoot a new installation of MetalLB. The following are steps and commands that you can use for this:</p>
			<ol>
				<li value="1">Log in to your VM or device:<p class="source-code"><strong class="bold">$ ssh ubuntu@NODE_IP</strong></p></li>
				<li> Create a pod to check whether your cluster can schedule pods:<p class="source-code"><strong class="bold">$ kubectl run nginx --image=nginx --restart=Never</strong></p></li>
				<li>Create a service to expose the pod created previously and test whether the <strong class="source-inline">LoadBalancer</strong> service works:<p class="source-code"><strong class="bold">$ kubectl expose pod/nginx --port=8001 \</strong></p><p class="source-code"><strong class="bold">  --target-port=80 \</strong></p><p class="source-code"><strong class="bold">  --type=LoadBalancer</strong></p></li>
				<li>Execute the following command if you want to check whether the services and port work to expose your services, which can be either <strong class="source-inline">LoadBalancer</strong> or <strong class="source-inline">NodePort</strong>:<p class="source-code"><strong class="bold">$ kubectl get services</strong></p></li>
				<li>Now, perform an access check for the assigned external IP to the NGINX service and execute the following command to check that MetalLB exposed your service:<p class="source-code"><strong class="bold">$ curl http://EXTERNAL_IP:8001</strong></p></li>
			</ol>
			<p>In case you want to check the logs of MetalLB in case of errors, look at the next pods inside the <strong class="source-inline">metallb-system</strong> namespace:</p>
			<ul>
				<li>Controller</li>
				<li>Speaker</li>
			</ul>
			<p>Now you <a id="_idIndexMarker175"/>know how to do basic troubleshooting of MetalLB. Let's move to a more advanced storage configuration using Longhorn in the next section.</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor065"/>Setting up Longhorn for storage</h1>
			<p>In terms of persistent information, you will find two types of containers, stateless and stateful <a id="_idIndexMarker176"/>containers. A stateless or ephemeral container doesn't persist information generated inside a container. A stateful container can persist the information even when this is deleted. K3s includes, by default, a way to persist data <a id="_idIndexMarker177"/>using a storage type (called <strong class="bold">storage class</strong> in Kubernetes) called <strong class="bold">local-path</strong>. This storage <a id="_idIndexMarker178"/>is a basic and pretty lightweight implementation, designed for edge devices. A common feature used on Kubernetes is to have a persistent volume claim that allows your pods to consume (write and read data) from different nodes. And this is a persistence volume configuration <a id="_idIndexMarker179"/>with the access mode key, set as <strong class="bold">ReadWriteMany</strong> (<strong class="bold">RWX</strong>). This feature is often used in production scenarios and it's pretty important because it enables you to share information from your different services. Longhorn provides this feature in a pretty lightweight presentation and it's optimized for edge devices. Let's move to learn what Longhorn is and how you can install it.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor066"/>Why use Longhorn?</h2>
			<p>Longhorn is designed to be a distributed and hyper-converged storage stack. Hyper-converged <a id="_idIndexMarker180"/>storage means that virtualization software abstracts and pools storage. Longhorn doesn't use a lot of resources, which gives you the ability to <a id="_idIndexMarker181"/>use it for advanced storage in edge devices. You can even simplify your workflows of snapshots, backups, and even disaster recovery. So, if you are looking for lightweight and advanced edge solutions for storage, Longhorn can fit your needs. There are other options, such as Rook, but Longhorn is an easy piece of software that can give you extra storage power without having to sacrifice resource consumptions. So, let's move on to learn how to install it and create a simple <strong class="bold">persistent volume claim</strong> for a pod in the next section.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor067"/>Installing Longhorn with ReadWriteMany mode</h2>
			<p>To install <a id="_idIndexMarker182"/>Longhorn, follow these steps:</p>
			<ol>
				<li value="1">Log in <a id="_idIndexMarker183"/>to your VM or device:<p class="source-code"><strong class="bold">$ ssh ubuntu@NODE_IP</strong></p></li>
				<li>If you want to install the ReadWriteMany PVC mode, you have to install <strong class="source-inline">nfs-common</strong> on each VM with Ubuntu installed in your cluster. For this, execute the following command:<p class="source-code"><strong class="bold">$ sudo apt install -y nfs-common</strong></p></li>
				<li>Apply the official Longhorn manifests, as follows:<p class="source-code"><strong class="bold">$ kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.3.1/deploy/longhorn.yaml</strong></p></li>
			</ol>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Longhorn will be installed in the <strong class="source-inline">longhorn-system</strong> namespace.</p>
			<ol>
				<li value="4">Create a <strong class="source-inline">pvc.yaml</strong> file:<p class="source-code"><strong class="bold">apiVersion: v1</strong></p><p class="source-code"><strong class="bold">kind: PersistentVolumeClaim</strong></p><p class="source-code"><strong class="bold">metadata:</strong></p><p class="source-code"><strong class="bold">  name: longhorn-volv-pvc</strong></p><p class="source-code"><strong class="bold">spec:</strong></p><p class="source-code"><strong class="bold">  accessModes:</strong></p><p class="source-code"><strong class="bold">    - ReadWriteMany</strong></p><p class="source-code"><strong class="bold">  storageClassName: longhorn</strong></p><p class="source-code"><strong class="bold">  resources:</strong></p><p class="source-code"><strong class="bold">    requests:</strong></p><p class="source-code"><strong class="bold">      storage: 2Gi</strong></p></li>
				<li>Apply the <strong class="source-inline">pvc.yaml</strong> file:<p class="source-code"><strong class="bold">$ kubectl create -f pvc.yaml</strong></p></li>
			</ol>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can use different PVC modes such as <strong class="source-inline">ReadWriteOnce</strong> or <strong class="source-inline">ReadOnlyMany</strong>. By default, the storage classes at least support <strong class="source-inline">ReadWriteOnce</strong>. So, <strong class="source-inline">ReadWriteMany</strong> is a special feature that uses <strong class="source-inline">NFS</strong> and is included in Longhorn.</p>
			<p>Now, it's <a id="_idIndexMarker184"/>time to create a pod <a id="_idIndexMarker185"/>using this PVC using the Longhorn storage class. To do this, follow these steps:</p>
			<ol>
				<li value="6">Create the <strong class="source-inline">pod.yaml</strong> file to create a pod using the previously created PVC:<p class="source-code"><strong class="bold">echo "</strong></p><p class="source-code"><strong class="bold">apiVersion: v1</strong></p><p class="source-code"><strong class="bold">kind: Pod</strong></p><p class="source-code"><strong class="bold">metadata:</strong></p><p class="source-code"><strong class="bold">  name: volume-test</strong></p><p class="source-code"><strong class="bold">  namespace: default</strong></p><p class="source-code"><strong class="bold">spec:</strong></p><p class="source-code"><strong class="bold">  containers:</strong></p><p class="source-code"><strong class="bold">  - name: volume-test</strong></p><p class="source-code"><strong class="bold">    image: nginx:stable-alpine</strong></p><p class="source-code"><strong class="bold">    imagePullPolicy: IfNotPresent</strong></p><p class="source-code"><strong class="bold">    volumeMounts:</strong></p><p class="source-code"><strong class="bold">    - name: volv</strong></p><p class="source-code"><strong class="bold">      mountPath: /data</strong></p><p class="source-code"><strong class="bold">    ports:</strong></p><p class="source-code"><strong class="bold">    - containerPort: 80</strong></p><p class="source-code"><strong class="bold">  volumes:</strong></p><p class="source-code"><strong class="bold">  - name: volv</strong></p><p class="source-code"><strong class="bold">    persistentVolumeClaim:</strong></p><p class="source-code"><strong class="bold">      claimName: longhorn-volv-pvc" &gt; pod.yaml</strong></p></li>
				<li>Apply the <strong class="source-inline">pod.yaml</strong> file to create the pod:<p class="source-code"><strong class="bold">$ kubectl create -f pod.yaml</strong></p></li>
			</ol>
			<p>Now, you <a id="_idIndexMarker186"/>have Longhorn installed <a id="_idIndexMarker187"/>and running. So, let's move on to learn how to use the Longhorn UI in the next section.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor068"/>Using Longhorn UI</h2>
			<p>If you <a id="_idIndexMarker188"/>want to access the Longhorn UI, you have to check the services created on <strong class="source-inline">longhorn-system</strong> and execute a port-forward; if you installed MetalLB, you can create a <strong class="source-inline">LoadBalancer</strong> service to expose the Longhorn UI. </p>
			<p>To access Longhorn with a port-forward, execute the following steps:</p>
			<ol>
				<li value="1">Run the next port-forward command locally in order to access the UI in your browser:<p class="source-code"><strong class="bold">$ kubectl port-forward svc/longhorn-frontend -n longhorn-system 8080:80</strong></p></li>
				<li>Now, open your browser at <strong class="source-inline">http://localhost:8080</strong>; you will see the following dashboard:</li>
			</ol>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B16945_Figure_3.1.jpg" alt="Figure 3.1 – Longhorn UI&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Longhorn UI</p>
			<p>With this <a id="_idIndexMarker189"/>dashboard, you can manage your <strong class="bold">Persistent Volume Claims</strong> (<strong class="bold">PVCs</strong>) using <a id="_idIndexMarker190"/>the UI; for more references, you can visit the following link: <a href="https://longhorn.io/docs/1.3.1/deploy/accessing-the-ui">https://longhorn.io/docs/1.3.1/deploy/accessing-the-ui</a>.</p>
			<p>Now you know how to install and use Longhorn. Let's go ahead and do some basic troubleshooting.</p>
			<h3>Troubleshooting Longhorn</h3>
			<p>Using the <a id="_idIndexMarker191"/>preceding example as reference, to troubleshoot the PVC creation using Longhorn, you can use the following commands:</p>
			<ol>
				<li value="1">Check whether the Longhorn pods are running successfully with the following command:<p class="source-code"><strong class="bold">$ kubectl get pods –n longhonr-system</strong></p></li>
				<li>Check whether the PV was created:<p class="source-code"><strong class="bold">$ kubectl get pv</strong></p></li>
				<li>Check whether the PVC was created:<p class="source-code"><strong class="bold">$ kubectl get pvc</strong></p></li>
				<li>Check whether the pod from <strong class="source-inline">pod.yaml</strong> using the new Longhorn storage class was created:<p class="source-code"><strong class="bold">$ kubectl get pods</strong></p></li>
			</ol>
			<p>With these <a id="_idIndexMarker192"/>commands, you can find errors that come up when a pod or deployment uses a PVC with the Longhorn storage class.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The previous four commands will return errors in case something goes wrong. For more information about this, you can check <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage">https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage</a> or <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#class">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#class</a>.</p>
			<p>Now, we are ready to learn another advanced topic about upgrading the cluster. So, let's move to the next section.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/>Upgrading your cluster</h1>
			<p>Sometimes, you <a id="_idIndexMarker193"/>want to be up to date with the new versions and features of K3s. The next sections explain how to perform these upgrading processes.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor070"/>Upgrading using K3s Bash scripts</h2>
			<p>To perform <a id="_idIndexMarker194"/>an upgrade in your nodes, you <a id="_idIndexMarker195"/>have to follow these steps:</p>
			<ol>
				<li value="1">First, you have to stop K3s on your device with the following command:<p class="source-code"><strong class="bold">$ /usr/local/bin/k3s-killall.sh</strong></p></li>
				<li>Now, you have to choose the version which you want to upgrade to. In general, there are three options – choose the latest or most stable channel, or pick a specific version. The next command will update your cluster to the latest stable version available:<p class="source-code"><strong class="bold">$ curl -sfL https://get.k3s.io | sh -</strong></p></li>
				<li>Now, if you want to update to the latest version, which is not so stable, you can execute the following command:<p class="source-code"><strong class="bold">$ curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh -</strong></p></li>
				<li>The last <a id="_idIndexMarker196"/>option is to pick a specific <a id="_idIndexMarker197"/>version. For this, you have to execute the following command:<p class="source-code"><strong class="bold">$ curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh -</strong></p></li>
			</ol>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can visit <a href="https://update.k3s.io/v1-release/channels">https://update.k3s.io/v1-release/channels</a> to check the latest, stable, or specific <a id="_idIndexMarker198"/>available version of K3s or the official site of k3s at <a href="https://k3s.io">https://k3s.io</a> in the GitHub section.</p>
			<p>Now you know how to upgrade your cluster using the K3s scripts. Let's move on to learn this manually in the next section.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor071"/>Upgrading K3s manually</h2>
			<p>If you <a id="_idIndexMarker199"/>want to perform a manual upgrading of the K3s version, you can follow the following steps, based on the official K3s website documentation:</p>
			<ol>
				<li value="1">Download your desired version of the K3s binary from releases. To do this, check this link: <a href="https://github.com/k3s-io/k3s/releases">https://github.com/k3s-io/k3s/releases</a>.</li>
				<li>Copy the downloaded binary to the <strong class="source-inline">/usr/local/bin</strong> folder.</li>
				<li>Stop the old k3s binary. For this, you can execute the following command:<p class="source-code"><strong class="bold">$ /usr/local/bin/k3s-killall.sh</strong></p></li>
				<li>Delete the old binary.</li>
				<li>Launch the new K3s binary with the next command:<p class="source-code"><strong class="bold">$ sudo systemctl restart k3s</strong></p></li>
			</ol>
			<p>Now, you know <a id="_idIndexMarker200"/>how to do the K3s manually, but there is something that you have to know, and that is to restart the service to apply the next changes. This is covered in the next section.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor072"/>Restarting K3s</h2>
			<p>When you <a id="_idIndexMarker201"/>perform software or hardware upgrades, or when a restart is needed to fix errors, you can restart K3s services using <strong class="source-inline">systemd</strong> and <strong class="bold">OpenRC</strong>.</p>
			<p>To restart K3s using <strong class="source-inline">systemd</strong>, follow these steps:</p>
			<ol>
				<li value="1">To restart the K3s service in your master node, execute the following command:<p class="source-code"><strong class="bold">$ sudo systemctl restart k3s</strong></p></li>
				<li>To restart the K3s agent service in your agent nodes, execute the following command:<p class="source-code"><strong class="bold">$ sudo systemctl restart k3s-agent</strong></p></li>
			</ol>
			<p>To restart K3s using OpenRC, follow these steps:</p>
			<ol>
				<li value="1">To restart the K3s service in your master node, execute the following command:<p class="source-code"><strong class="bold">$ sudo service k3s restart</strong></p></li>
				<li>To restart the K3s-agent service in your agent nodes, execute the following command:<p class="source-code"><strong class="bold">$ sudo service k3s-agent restart</strong></p></li>
			</ol>
			<p>Now that you know all the necessary steps to upgrade your K3s cluster, it's time to move on to other advanced topics – backups and restorations. Let's move on to the next section to learn about this.</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor073"/>Backing up and restoring your K3s configurations</h1>
			<p>Backups and restoration of your Kubernetes objects are something to consider in production <a id="_idIndexMarker202"/>environments. This section explains how to <a id="_idIndexMarker203"/>perform these kinds of tasks for the default storage, SQLite, how to install and manage <strong class="bold">etcd</strong> on K3s, and basic resources if you are using the SQL backends of K3s.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor074"/>Backups from SQLite</h2>
			<p>If you <a id="_idIndexMarker204"/>are using the default storage, SQLite, follow <a id="_idIndexMarker205"/>these steps:</p>
			<ol>
				<li value="1">Log in to your master node:<p class="source-code"><strong class="bold">$ ssh ubuntu@NODE_IP</strong></p></li>
				<li>Stop the K3s using the following command:<p class="source-code"><strong class="bold">$ k3s-killall.sh</strong></p></li>
				<li>Change to the <strong class="source-inline">/var/lib/rancher/k3s/</strong> directory server:<p class="source-code"><strong class="bold">$ sudo cd /var/lib/rancher/k3s</strong></p></li>
				<li>Copy the folder server inside the <strong class="source-inline">k3s</strong> folder: <p class="source-code"><strong class="bold">$ sudo cp -R /var/lib/rancher/k3s folder_of_destination</strong></p></li>
				<li>Download this folder on another device if necessary.</li>
			</ol>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor075"/>Backups and restoring from the SQL database K3s backend</h2>
			<p>If <a id="_idIndexMarker206"/>you are using external <a id="_idIndexMarker207"/>storage – let's say, for example, MySQL – you have to use a tool or the command to back up your database. </p>
			<h3>Backing up MySQL</h3>
			<p>In the <a id="_idIndexMarker208"/>case of MySQL, you can execute the following steps to back up your K3s configurations:</p>
			<ol>
				<li value="1">Get your database credentials to use the <strong class="source-inline">mysqldump</strong> command.</li>
				<li>Run the following command to back up your database, which in this case is called <strong class="source-inline">k3s</strong>, using the <strong class="source-inline">YOUR_USER</strong> user, the <strong class="source-inline">YOUR_PASSWORD</strong> password, and an output file called <strong class="source-inline">output.sql</strong> from the <strong class="source-inline">YOUR_HOST</strong> host:<p class="source-code"><strong class="bold">$ mysqldump -h YOUR_HOST -u YOUR_USER -pYOUR_PASSWORD k3s &gt; output.sql</strong></p></li>
			</ol>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can modify the <strong class="source-inline">YOUR_HOST</strong>, <strong class="source-inline">YOUR_USER</strong>, and <strong class="source-inline">YOUR_PASSWORD</strong> values, the database name instead of <strong class="source-inline">k3s</strong>, and even the name of the output file to customize your backup command. The <strong class="source-inline">–h</strong> option can be optional if you are connected to the same host where the database is installed. By default, it connects to <strong class="source-inline">localhost</strong>. You can check this link for other examples to back up your MySQL: <a href="https://www.tecmint.com/mysql-backup-and-restore-commands-for-database-administration">https://www.tecmint.com/mysql-backup-and-restore-commands-for-database-administration</a>.</p>
			<p>Now the backup is ready to be used. In the next section, you are going to use the backup to restore your database.</p>
			<h3>Restoring MySQL</h3>
			<p>Now it <a id="_idIndexMarker209"/>is time to restore your database. Follow the next steps for the restoration:</p>
			<ol>
				<li value="1">Get your database credentials to use the database with the <strong class="source-inline">mysql</strong> command.</li>
				<li>Run the following command to restore your database backup. We are using the <strong class="source-inline">k3s</strong> database. Change <strong class="source-inline">YOUR_HOST</strong> and <strong class="source-inline">YOUR_PASSWORD</strong> parameters according to the database used as data storage for your <strong class="source-inline">k3s</strong> cluster. Finally, the <strong class="source-inline">output.sql</strong> file is used to load your backup and restore your database:<p class="source-code"><strong class="bold">$ mysql -h YOUR_HOST -u YOUR_USER -pYOUR_PASSWORD k3s &lt; output.sql</strong></p></li>
			</ol>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can <a id="_idIndexMarker210"/>modify the values from the previous command to perform your restoration with the <strong class="source-inline">output.sql</strong> file.</p>
			<h3>Backing up and restoring other data storages</h3>
			<p>If you <a id="_idIndexMarker211"/>are using other K3s backends, such <a id="_idIndexMarker212"/>as PostgreSQL or <strong class="source-inline">etcd</strong>, you can check the official documentation for each database.</p>
			<p>For PostgreSQL, check <a id="_idIndexMarker213"/>the following link: <a href="https://www.postgresql.org/docs/8.3/backup-dump.html">https://www.postgresql.org/docs/8.3/backup-dump.html</a>.</p>
			<p>For <strong class="source-inline">etcd</strong>, check <a id="_idIndexMarker214"/>the following link: <a href="https://etcd.io">https://etcd.io</a>.</p>
			<p>Now that you have learned how to restore your MySQL data storage for your K3s cluster, let's move on to the next section to understand how to use <strong class="source-inline">etcd</strong> as your data storage.</p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor076"/>Embedded etcd management</h1>
			<p><strong class="source-inline">etcd</strong> is the <a id="_idIndexMarker215"/>default type of storage to store all the Kubernetes objects in your <a id="_idIndexMarker216"/>cluster. <strong class="source-inline">etcd</strong>, by default, was removed from K3s, but you can install it. K3s customized how <strong class="source-inline">etcd</strong> works for your cluster; this includes some custom features that you can't find in a regular Kubernetes cluster that uses <strong class="source-inline">etcd</strong>. So, let's get started with installing <strong class="source-inline">etcd</strong> in K3s.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor077"/>Installing the etcd backend</h2>
			<p>If you <a id="_idIndexMarker217"/>want to install it, follow these steps:</p>
			<ol>
				<li value="1">To install K3s with the <strong class="source-inline">etcd</strong> backend, you have to execute the following command to include <strong class="source-inline">etcd</strong> in the K3s installation. This has to be executed in the master node:<p class="source-code"><strong class="bold">$ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --cluster-init" sh -s -</strong></p></li>
				<li>Set your <strong class="source-inline">TOKEN</strong> variable, with the <strong class="source-inline">YOUR_TOKEN</strong> master token, to join the nodes to the cluster:<p class="source-code"><strong class="bold">$ TOKEN=YOUR_TOKEN</strong></p></li>
				<li>Now, if you need a multi-cluster configuration, execute the following command:<p class="source-code"><strong class="bold">$ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server </strong></p><p class="source-code"><strong class="bold">--server https://MASTER_IP:6443" K3S_TOKEN=$TOKEN sh -s -</strong></p></li>
			</ol>
			<p>Now that you <a id="_idIndexMarker218"/>have learned how to install the <strong class="source-inline">etcd</strong> feature for K3s, let's move on to the next section to learn how to create and restore <strong class="source-inline">etcd</strong> snapshots for your Kubernetes objects configurations.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor078"/>Creating and restoring etcd snapshots</h2>
			<p>K3s includes an experimental feature to back up and restore <strong class="source-inline">etcd</strong>. In this section, you are going to <a id="_idIndexMarker219"/>learn how to perform <strong class="source-inline">etcd</strong> snapshots and restoration for <strong class="source-inline">etcd</strong>. The snapshots are enabled by default with this <a id="_idIndexMarker220"/>backend. These snapshots are stored in <strong class="source-inline">/var/lib/rancher/k3s/server/db/snapshots</strong>. To create a backup, manually execute the following steps:</p>
			<ol>
				<li value="1">Create a backup manually:<p class="source-code"><strong class="bold">$ k3s etcd-snapshot --name=mysnapshot</strong></p></li>
			</ol>
			<p>This will generate a file inside the <strong class="source-inline">snapshots</strong> folder.</p>
			<ol>
				<li value="2">To restore your <strong class="source-inline">etcd</strong> from this backup, execute the following command:<p class="source-code"><strong class="bold">$ k3s server \</strong></p><p class="source-code"><strong class="bold">--cluster-reset \</strong></p><p class="source-code"><strong class="bold">--cluster-reset-restore-path=&lt;PATH-TO-SNAPSHOT&gt;</strong></p></li>
				<li>You <a id="_idIndexMarker221"/>can automate <a id="_idIndexMarker222"/>the snapshot generation with the following option:<p class="source-code"><strong class="bold">--etcd-snapshot-schedule-cron</strong></p></li>
			</ol>
			<p>For more references to configure this, visit this link: <a href="https://rancher.com/docs/k3s/latest/en/backup-restore/#options">https://rancher.com/docs/k3s/latest/en/backup-restore/#options</a>.</p>
			<p>You can even use the official documentation of <strong class="source-inline">etcd</strong>: <a href="https://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md">https://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md</a>.</p>
			<p>That's how you manage your <strong class="source-inline">etcd</strong> snapshots. Now, let's take a recap of what we have covered in this chapter.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor079"/>Summary</h1>
			<p>This chapter covered common advanced configurations for Kubernetes edge clusters using Ubuntu and K3s. One of these common configurations was to install a bare metal load balancer using MetalLB. We also discussed the pros and cons of this as compared to the default K3s load balancer, KlipperLB, followed by the use cases of when to use each one. Then, we jumped to the advanced storage configurations of Longhorn, which is a really lightweight storage solution, and easy to install and configure for ReadWriteMany access modes for storage. Finally, we saw how to upgrade our cluster, and perform backups and restorations when using different data storage such as SQL or <strong class="source-inline">etcd</strong>. With all this knowledge, you are ready to create a production-ready cluster. In the next chapter, we are going to learn how to use k3OS to create your clusters using the K3s ISO image and overlay installation.</p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/>Questions</h1>
			<p>Here are a few questions to validate your new knowledge:</p>
			<ol>
				<li value="1">When do you choose KlipperLB or MetalLB as a bare metal load balancer solution?</li>
				<li>How can I troubleshoot my MetalLB installation?</li>
				<li>How can I install Longhorn to get more robust data storage solutions for my deployments?</li>
				<li>How can I troubleshoot my Longhorn installation?</li>
				<li>What other data storage solutions can I use instead of Longhorn?</li>
				<li>What are the steps to upgrade my K3s clusters?</li>
				<li>What are the steps to back up or restore my Kubernetes object configurations if I use a SQL backend or <strong class="source-inline">etcd</strong>?</li>
			</ol>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor081"/>Further reading</h1>
			<p>You can refer to the following references for more information on the topics covered in this chapter:</p>
			<ul>
				<li>What is the OSI Model?: <a href="https://www.cloudflare.com/en-gb/learning/ddos/glossary/open-systems-interconnection-model-osi/">https://www.cloudflare.com/en-gb/learning/ddos/glossary/open-systems-interconnection-model-osi/</a></li>
				<li>MetalLB official documentation: <a href="https://metallb.universe.tf">https://metallb.universe.tf</a></li>
				<li>MetalLB in layer 2 mode: <a href="https://metallb.universe.tf/concepts/layer2">https://metallb.universe.tf/concepts/layer2</a></li>
				<li>Kubernetes 101: Why You Need To Use MetalLB: <a href="https://www.youtube.com/watch?v=Ytc24Y0YrXE">https://www.youtube.com/watch?v=Ytc24Y0YrXE</a></li>
				<li>MetalLB ConfigMap configuration: <a href="https://metallb.universe.tf/configuration">https://metallb.universe.tf/configuration</a></li>
				<li>Persistent Volumes: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes">https://kubernetes.io/docs/concepts/storage/persistent-volumes</a></li>
				<li>Volumes and Storage: <a href="https://rancher.com/docs/k3s/latest/en/storage">https://rancher.com/docs/k3s/latest/en/storage</a></li>
				<li>Longhorn official page: <a href="https://longhorn.io">https://longhorn.io</a></li>
				<li>Installing OpenEBS with RWM support: <a href="https://docs.openebs.io/docs/next/rwm.html">https://docs.openebs.io/docs/next/rwm.html</a></li>
				<li>Installing Rook with RWM support: <a href="https://rook.io/docs/nfs/v1.7">https://rook.io/docs/nfs/v1.7</a></li>
				<li>Upgrading a K3s cluster: <a href="https://rancher.com/docs/k3s/latest/en/upgrades">https://rancher.com/docs/k3s/latest/en/upgrades</a></li>
				<li>Backing up and restoring a K3s cluster: <a href="https://rancher.com/docs/k3s/latest/en/backup-restore">https://rancher.com/docs/k3s/latest/en/backup-restore</a></li>
				<li>Installation options: <a href="https://rancher.com/docs/k3s/latest/en/installation/install-options/#registration-options-for-the-k3s-server">https://rancher.com/docs/k3s/latest/en/installation/install-options/#registration-options-for-the-k3s-server</a></li>
			</ul>
		</div>
	</body></html>