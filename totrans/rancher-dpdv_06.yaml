- en: '*Chapter 4*: Creating an RKE and RKE2 Cluster'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*：创建RKE和RKE2集群'
- en: The standard way of deploying Rancher is by creating an **Rancher Kubernetes
    Engine (RKE**) cluster then using Helm to install Rancher on the cluster. The
    new way to deploy Kubernetes clusters using RKE2 is built on K3s and the new internal
    cluster management model. By doing so, Rancher can manage the cluster it lives
    on directly without requiring an external tool such as RKE. This chapter will
    cover when using RKE2 over RKE makes sense, and how to bootstrap the first node
    and join additional nodes to the cluster. At this point, we'll install Rancher
    on the cluster using the **Helm** tool, which installs the Rancher server workload
    on the cluster. Finally, we'll cover how to configure a load balancer to support
    the Rancher URL.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 部署Rancher的标准方式是创建一个**Rancher Kubernetes Engine (RKE)**集群，然后使用Helm在集群上安装Rancher。使用RKE2部署Kubernetes集群的新方式建立在K3s和新的内部集群管理模型之上。通过这种方式，Rancher可以直接管理其所在的集群，而无需像RKE这样的外部工具。本章将讨论何时使用RKE2而非RKE是合理的，以及如何引导第一个节点并将其他节点加入集群。此时，我们将使用**Helm**工具在集群上安装Rancher，这将把Rancher服务器工作负载安装到集群中。最后，我们将介绍如何配置负载均衡器以支持Rancher
    URL。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主要主题：
- en: What is an RKE cluster?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是RKE集群？
- en: What is an RKE2 cluster?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是RKE2集群？
- en: What is RancherD?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是RancherD？
- en: Requirements and limitations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求和限制
- en: Rules for architecting a solution
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决方案架构规则
- en: Install steps (RKE)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装步骤（RKE）
- en: Install steps (RKE2)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装步骤（RKE2）
- en: Configuring an external load balancer (HAProxy)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置外部负载均衡器（HAProxy）
- en: Configuring MetalLB
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置MetalLB
- en: Let's get started!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧！
- en: What is an RKE cluster?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是RKE集群？
- en: RKE is Rancher's Kubernetes distribution that runs entirely inside Docker containers.
    Of course, RKE is a CNCF-certified distribution, so all the standard Kubernetes
    components and API resources are available. The easiest way to understand RKE
    clusters is to know where it originated and how it works.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: RKE是Rancher的Kubernetes发行版，完全运行在Docker容器内。当然，RKE是CNCF认证的发行版，因此所有标准的Kubernetes组件和API资源都可以使用。了解RKE集群的最简单方法是知道它的起源以及它是如何工作的。
- en: Where did RKE come from?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE的来源是什么？
- en: Originally, when Rancher first started creating Kubernetes clusters, Rancher
    used its clustering software called **Cattle**, with the idea being Kubernetes
    was just another application in the cluster. This caused several problems ranging
    from kubelet and Cattle fighting to control the containers to even needing a custom
    load balancer solution built on top of HAProxy. But for most components, the most
    significant issue was that the Rancher server managed the Cattle side of the cluster,
    and Kubernetes managed the pod side. This meant that the cluster had a dependency
    on Rancher to get pod IPs and needed Rancher to update the load balancer when
    pod changes happened. This all changed with Rancher v2.x and the creation of RKE,
    the idea being RKE will create and manage the cluster independent of the Rancher
    server. Of course, some of the core ideas of Cattle were brought forward into
    RKE, with the main idea being if everything is just a container, then Rancher
    doesn't need to care about the OS. RKE doesn't need any libraries or packages.
    At its core, RKE manages standalone Docker containers for core Kubernetes services.
    These include etcd, kube-apiserver, kube-scheduler, kube-controller-manager, kubelet,
    and kube-proxy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，当Rancher开始创建Kubernetes集群时，Rancher使用其名为**Cattle**的集群软件，目的是将Kubernetes视为集群中的另一个应用程序。这导致了一些问题，从kubelet与Cattle争夺控制容器，到需要基于HAProxy构建自定义负载均衡器解决方案。对于大多数组件来说，最主要的问题是Rancher服务器管理集群中的Cattle部分，而Kubernetes管理pod部分。这意味着集群依赖于Rancher来获取pod的IP，并且在pod变化时需要Rancher来更新负载均衡器。随着Rancher
    v2.x和RKE的出现，这一切发生了变化，RKE的理念是它将独立于Rancher服务器创建和管理集群。当然，Cattle的一些核心思想被带入了RKE，主要的理念是，如果一切都只是容器，那么Rancher就不需要关心操作系统。RKE不需要任何库或软件包。RKE的核心是管理独立的Docker容器来提供核心的Kubernetes服务。这些服务包括etcd、kube-apiserver、kube-scheduler、kube-controller-manager、kubelet和kube-proxy。
- en: How does RKE work?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE是如何工作的？
- en: One of the core design principles is that RKE has desired state configuration
    in the form of a configuration file called `cluster.yaml`. With this file, RKE
    knows what kind of cluster you want to build, what nodes to use, and how each
    Kubernetes component should be configured. This file is a YAML formatted file,
    so you will need to follow the YAML standard when creating/editing this file with
    a common trap for new players being tabs. YAML uses spaces and not taba, even
    though they look the same. If you start running into syntax errors, it might be
    because of tabs. What follows is an example `cluster.yaml` with the next section.
    We'll break down the different parts of the config file.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: RKE 的核心设计原则之一是具有所需的状态配置，形式是一个名为 `cluster.yaml` 的配置文件。通过这个文件，RKE 知道您希望构建什么样的集群，使用哪些节点，以及每个
    Kubernetes 组件如何配置。这个文件是 YAML 格式的，因此在创建或编辑时，您需要遵循 YAML 标准。一个常见的陷阱是制表符（Tab）。YAML
    使用空格而非制表符，虽然它们看起来一样。如果开始遇到语法错误，可能是因为制表符。接下来是一个 `cluster.yaml` 示例，以及接下来的部分。我们将逐步分析配置文件的不同部分。
- en: '![Figure 4.1 – Example cluster.yml'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.1 – 示例 cluster.yml](img/B18053_04_001.jpg)'
- en: '](img/B18053_04_001.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_04_001.jpg)'
- en: Figure 4.1 – Example cluster.yml
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 示例 cluster.yml
- en: 'Full config: [https://raw.githubusercontent.com/PacktPublishing/Rancher-Deep-Dive/main/ch04/example_configs/simple_3_node_cluster.yml](https://raw.githubusercontent.com/PacktPublishing/Rancher-Deep-Dive/main/ch04/example_configs/simple_3_node_cluster.yml)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 完整配置：[https://raw.githubusercontent.com/PacktPublishing/Rancher-Deep-Dive/main/ch04/example_configs/simple_3_node_cluster.yml](https://raw.githubusercontent.com/PacktPublishing/Rancher-Deep-Dive/main/ch04/example_configs/simple_3_node_cluster.yml)
- en: The first section is `nodes`. In this section, you'll define the nodes used
    to create the cluster. If we break down the node definition, we'll see the first
    line, which is `address`. This is the hostname or IP address that RKE will use
    to connect to the node. It is pretty standard to use a server's **FQDN** (**Fully
    Qualified Domain Name**).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分是 `nodes`。在此部分中，您将定义用于创建集群的节点。如果我们拆解节点定义，会看到第一行是 `address`。这是 RKE 用于连接节点的主机名或
    IP 地址。通常情况下，使用服务器的 **FQDN**（**完全合格的域名**）是标准做法。
- en: Important Note
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The server where RKE is being run from must be able to resolve the hostname.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 RKE 的服务器必须能够解析主机名。
- en: This address should not be changed without removing the node from the cluster
    first then rejoining it as a new node. The `address` field is required when defining
    a node.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有先从集群中移除节点并将其作为新节点重新加入之前，不应更改此地址。定义节点时，`address` 字段是必需的。
- en: The following section is `hostname_override`, which sets the node name in Kubernetes.
    Most people will set this to be the short hostname. This name does not have to
    be in DNS as it is just a label in Kubernetes. For example, AWS uses the naming
    convention of `ip-12-34-56-78.us-west-2.compute.internal`, but you might want
    to override this to be something more helpful such as `etcd01` or `prod-worker01`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分是 `hostname_override`，用于在 Kubernetes 中设置节点名称。大多数人会将其设置为短主机名。此名称不需要在 DNS
    中注册，因为它只是 Kubernetes 中的一个标签。例如，AWS 使用的命名约定是 `ip-12-34-56-78.us-west-2.compute.internal`，但您可能想将其重写为更有帮助的名称，例如
    `etcd01` 或 `prod-worker01`。
- en: Note
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Just like the address field, the hostname field should not be changed after
    a node has been configured.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `address` 字段一样，节点配置后，主机名字段不应更改。
- en: If you would like to change the hostname, IP address, or role, you should remove,
    clean it, and rejoin it. If this field is not set, RKE will default to using the
    `address` field.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想更改主机名、IP 地址或角色，您应当先移除、清理节点，然后重新加入。如果此字段未设置，RKE 将默认使用 `address` 字段。
- en: The following field is `user`. This field is used by RKE when creating its SSH
    tunnel to the nodes. This account should have permission to run the `docker` command
    without `sudo`. This user must be a member of the group Docker or be root. If
    a user is not defined at the node level, RKE will default to the currently running
    user. It is standard for this to be root or a service account. Rancher recommends
    typically not using a personal account as you will need to set up SSH keys.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下字段是 `user`。此字段在 RKE 创建 SSH 隧道到节点时使用。该账户应具有在不使用 `sudo` 的情况下运行 `docker` 命令的权限。此用户必须是
    Docker 组的成员或是 root 用户。如果节点级别未定义用户，RKE 将默认为当前运行的用户。标准做法是使用 root 用户或服务账户。Rancher
    推荐通常不要使用个人账户，因为您需要设置 SSH 密钥。
- en: 'This brings us to the next field: `ssh_key_path`. This should be the path to
    the SSH private key for connecting to a node. RKE does require SSH keys to be
    set up between the server running the RKE binary and all of the nodes. When you
    SSH to the nodes, you get prompted with a password or `ssh_key_path` is not set
    at the node level, it will default to the cluster''s default option as defined
    in the `ssh_key_path` is not set, RKE will default to `~/.ssh/id_rsa`.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了下一个字段：`ssh_key_path`。这个字段应该是用于连接节点的SSH私钥的路径。RKE要求在运行RKE二进制文件的服务器与所有节点之间设置SSH密钥。当你SSH连接到节点时，如果没有设置`ssh_key_path`，或者提示输入密码时，它会默认使用集群的默认选项，默认情况下`ssh_key_path`未设置时，RKE会默认为`~/.ssh/id_rsa`。
- en: This ties into the next section, `port`, which is the port that RKE will use
    for connecting to the SSH server. RKE will default to port `22`. Usually, this
    is not changed, but in rare cases, when using port forwarding, multiple servers
    can share the same public IP address without RKE needing direct IP access to the
    nodes. The following field is `docker_socket`, which is the file path to the Docker
    socket. This file is a Unix domain socket, sometimes called an IPC socket. This
    file provides API access to dockerd.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这与下一个部分`port`相关，`port`是RKE用于连接SSH服务器的端口。RKE默认使用端口`22`。通常情况下，这个端口不会更改，但在少数情况下，当使用端口转发时，多个服务器可以共享同一个公共IP地址，而不需要RKE直接访问节点的IP。接下来的字段是`docker_socket`，这是Docker套接字的文件路径。这个文件是一个Unix域套接字，有时也称为IPC套接字。这个文件提供了访问dockerd的API。
- en: Note that this API does not have authentication or encryption and has complete
    control over Docker and its containers. This file must be protected; hence, by
    default, this file is owned by the root user and the Docker group. RKE uses this
    file to connect to Docker Engine to run commands, create containers, and so on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个API没有认证或加密，并且具有对Docker及其容器的完全控制权。这个文件必须得到保护，因此默认情况下，文件的拥有者是root用户和Docker组。RKE使用这个文件连接到Docker引擎以执行命令、创建容器等操作。
- en: Finally, we get to the `role` fields. These fields define what role (`etcd`,
    `controlplane`, or `worker`) is assigned to a node. You can mix and match these
    role assignments as you want. For example, the standard three-node cluster has
    all three nodes having all three roles. We will go into more detail in the *Rules
    for architecting a solution* section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们进入`role`字段。这些字段定义了分配给节点的角色（`etcd`、`controlplane`或`worker`）。你可以根据需要混合和匹配这些角色分配。例如，标准的三节点集群将所有三个节点分配有这三种角色。我们将在*架构解决方案的规则*部分详细介绍。
- en: The following section is what I call the global settings section. This section
    is where you can define cluster-level settings. We'll be covering the most common
    settings, with the complete list of cluster settings being located at [https://rancher.com/docs/rke/latest/en/config-options/](https://rancher.com/docs/rke/latest/en/config-options/).
    The first field is `cluster_name`, which is used for setting the name of your
    cluster. This setting doesn't affect the cluster, with the only real change being
    the `kubeconfig` file that RKE generates will have the cluster name in it, which
    can make mapping kubeconfig to a cluster much more straightforward. By default,
    RKE will set this to local, and this setting can be changed at any time.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分是我所说的全局设置部分。在这一部分，你可以定义集群级别的设置。我们将涵盖最常见的设置，完整的集群设置列表可以在[https://rancher.com/docs/rke/latest/en/config-options/](https://rancher.com/docs/rke/latest/en/config-options/)找到。第一个字段是`cluster_name`，用于设置集群的名称。此设置不会影响集群，唯一的实际变化是RKE生成的`kubeconfig`文件中会包含集群名称，这样将`kubeconfig`映射到集群时会更加直观。默认情况下，RKE会将此设置为local，并且可以随时更改。
- en: The next most common setting is `ignore_docker_version`. This setting tells
    RKE if it should ignore an unsupported Docker version on a node. RKE has a built-in
    metadata file that maps all the supported versions that have been tested and approved
    by Rancher. It is expected that Docker will be upgraded with the operating system
    as part of standard patching, which can cause RKE not to upgrade a cluster if
    the RKE release is not as up to date. It is pretty common to set this setting
    to `true` so that RKE will still throw a warning message in the logs, but it will
    continue building the cluster.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个最常见的设置是`ignore_docker_version`。此设置告知RKE是否应该忽略节点上不支持的Docker版本。RKE具有一个内置的元数据文件，映射了所有经过Rancher测试并批准的支持版本。通常，Docker会随着操作系统的标准修补程序进行升级，这可能会导致如果RKE发布版本不够新，RKE无法升级集群。将此设置为`true`是很常见的，这样RKE仍然会在日志中抛出警告信息，但会继续构建集群。
- en: The next field is probably the most important setting you can set, which is
    `kubernetes_version`. By default, when RKE is created, it will have a default
    Kubernetes version set. This is usually the highest officially supported version
    at build time. For example, RKE v1.2.3 will default the Kubernetes version `v1.19.4-rancher1-1`,
    which is fine when the cluster is created. But if later, someone upgrades RKE
    to v1.3.1, which has the new default of `v1.21.5-rancher1-1`, suppose you didn't
    set your Kubernetes versions in your `cluster.yaml` file. The next RKE upgrade
    event will cause what I call an accidental upgrade. This can be fine but has been
    known to cause problems. We don't want to upgrade a cluster without testing and
    planning. Hence, Rancher usually recommends setting `kubernetes_version` in your
    `cluster.yaml` as a safety measure.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个字段可能是你可以设置的最重要的设置，那就是 `kubernetes_version`。默认情况下，当创建 RKE 时，它会设置一个默认的 Kubernetes
    版本。这通常是构建时官方支持的最高版本。例如，RKE v1.2.3 会默认使用 Kubernetes 版本 `v1.19.4-rancher1-1`，在创建集群时是可以的。但是，如果后来有人将
    RKE 升级到 v1.3.1，而这个版本的新默认 Kubernetes 版本是 `v1.21.5-rancher1-1`，假设你没有在 `cluster.yaml`
    文件中设置 Kubernetes 版本。下一个 RKE 升级事件将导致我所称的意外升级。这可能没问题，但已知会引发一些问题。我们不想在没有测试和规划的情况下升级集群。因此，Rancher
    通常建议在 `cluster.yaml` 中设置 `kubernetes_version`，作为一种安全措施。
- en: Note
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This setting also sets the image version tags of all the Kubernetes components
    such as etcd, kube-apiserver, canal, ingress-nginx-controller, and so on.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此设置还会设置所有 Kubernetes 组件的镜像版本标签，例如 etcd、kube-apiserver、canal、ingress-nginx-controller
    等等。
- en: 'This brings us to the next field, `system_images`. This section has a list
    of all the Docker images and their tags for all the different components. For
    example, the line `etcd: rancher/coreos-etcd:v3.1.12` sets the Docker image to
    use for etcd. Note that, by default, Docker pulls images without a registry name
    from Docker Hub. You can change the behavior using the `--registry-mirror` flag
    to force Docker to use a private registry instead. This is usually used in air-gapped
    environments where your servers cannot pull images from Docker Hub. If you want
    to learn more about setting this up, please see Rancher''s documentation at [https://rancher.com/docs/rke/latest/en/config-options/system-images/#air-gapped-setups](https://rancher.com/docs/rke/latest/en/config-options/system-images/#air-gapped-setups).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '这引出了下一个字段，`system_images`。该部分列出了所有不同组件的所有 Docker 镜像及其标签。例如，`etcd: rancher/coreos-etcd:v3.1.12`
    设置了用于 etcd 的 Docker 镜像。请注意，默认情况下，Docker 会从 Docker Hub 拉取没有注册表名称的镜像。你可以使用 `--registry-mirror`
    标志来改变这种行为，强制 Docker 使用私有注册表。这通常用于与外网隔离的环境，在这些环境中，服务器无法从 Docker Hub 拉取镜像。如果你想了解更多关于如何设置，请参见
    Rancher 的文档：[https://rancher.com/docs/rke/latest/en/config-options/system-images/#air-gapped-setups](https://rancher.com/docs/rke/latest/en/config-options/system-images/#air-gapped-setups)。'
- en: Finally, we come to the `services` section. In this section, we'll define the
    settings for each of the Kubernetes components, for e.g., if you want to configure
    etcd backups. Note that you should have etcd backups turned on, and newer RKE
    versions turn local backups on by default. You go to services, etcd, and `backup_config`.
    There you can enable recurring etcd snapshots by setting `enabled` to `true`.
    You can also set the backup schedule using `interval_hours`. RKE doesn't use a
    schedule like cron for its backup schedule. The schedule is based on when the
    `etcd-tools` container is started. The basic process is that `etcd-tools` will
    take a backup as soon the container starts, then sleep for X number of hours as
    defined in `interval_hours` until taking another backup and repeating this process.
    Currently, there is no way of telling RKE to take a backup at a scheduled time.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来到了 `services` 部分。在该部分，我们将定义每个 Kubernetes 组件的设置，例如，如果你想配置 etcd 备份。请注意，你应该启用
    etcd 备份，并且较新的 RKE 版本默认开启本地备份。你可以进入服务、etcd 和 `backup_config`，在那里你可以通过将 `enabled`
    设置为 `true` 来启用定期的 etcd 快照。你还可以使用 `interval_hours` 设置备份的时间间隔。RKE 并不像 cron 那样使用计划任务来安排备份时间。其备份计划是基于
    `etcd-tools` 容器启动的时间。基本过程是，`etcd-tools` 会在容器启动后立即进行一次备份，然后按照 `interval_hours`
    中定义的小时数休眠，直到进行下一次备份并重复此过程。目前，RKE 没有办法让你在预定时间进行备份。
- en: The next setting is `retention`, which sets how many hours `etcd-tools` will
    keep a snapshot before purging them. The current default is 6 hours. But it is
    common to increase this setting to something like 72 hours. This is mainly to
    stop backups from rolling off too quickly. For example, if a change was made late
    on a Friday, you might not catch it until Monday. With the default setting, you
    will have lost that recovery point. But if you set it to 72 hours, you still have
    a chance. It is important to note that etcd snapshots are complete copies of the
    database, so if your etcd database is 1 GB in size, your backup will be 1 GB before
    compression, with most backups being in the 100~200 MB range after compression.
    By default, RKE will back up locally on the etcd nodes to the directory `/opt/rke/etcd-snapshots`
    with each etcd node having a full copy of the backups. This is great for ease
    of use but leads to the problem that you are storing your backups on the same
    server you are backing up. You can, of course, set up `rsync` scripts to copy
    this data off another server or use a backup tool such as TSM to back up this
    directory, or even use a tool such as Veeam to take an image backup of the whole
    server. But what I usually recommend is to use the S3 backup option.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个设置是`retention`，它设置了`etcd-tools`在清除快照之前将保留多少小时。当前默认值是6小时。但通常会将此设置增加到72小时左右。这样做的主要目的是防止备份过快地滚动。例如，如果某个更改在周五晚间进行，你可能要等到周一才能发现。使用默认设置时，你会丢失那个恢复点。但如果你将其设置为72小时，你仍然有机会恢复。需要注意的是，etcd快照是数据库的完整副本，因此，如果你的etcd数据库大小为1
    GB，那么备份在压缩之前将是1 GB，压缩后的备份大多数在100~200 MB范围内。默认情况下，RKE会将备份保存在etcd节点的本地目录`/opt/rke/etcd-snapshots`中，每个etcd节点都会有一个完整的备份副本。这对于易用性非常好，但也带来了一个问题：你将备份存储在同一个备份服务器上。当然，你可以设置`rsync`脚本，将这些数据复制到另一台服务器，或者使用备份工具如TSM来备份该目录，甚至可以使用Veeam等工具对整个服务器进行镜像备份。但我通常推荐使用S3备份选项。
- en: This leads us into the next section, which is `s3backupconfig`. These settings
    allow you to configure `etcd-tools` to send its etcd snapshots to an S3 bucket
    instead of local storage. This helps in disaster recovery cases where you lose
    a data center or when someone deleted all your etcd nodes in vCenter by mistake,
    because with `cluster.yaml`, `cluster.rkestate`, and an etcd backup, we can rebuild
    a cluster from nothing. Please have a look at my Kubernetes Master Class on Disaster
    Recovery located at [https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery](https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery)
    for more details on this process. It's also important to note that just because
    this is S3, it doesn't mean you have to use AWS's S3 offering. Any S3 provider
    will work assuming they are following the S3 standard.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这将引导我们进入下一个部分，即`s3backupconfig`。这些设置允许你配置`etcd-tools`将etcd快照发送到S3存储桶，而不是本地存储。这对于灾难恢复情况非常有帮助，例如丢失数据中心或有人误删了vCenter中的所有etcd节点，因为通过`cluster.yaml`、`cluster.rkestate`和etcd备份，我们可以从零重建一个集群。有关此过程的更多详细信息，请查看我的Kubernetes灾难恢复大师课程，位于[https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery](https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery)。同样重要的是要注意，尽管这是S3，但这并不意味着你必须使用AWS的S3服务。任何符合S3标准的S3提供商都可以使用。
- en: What is an RKE2 cluster?
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是RKE2集群？
- en: RKE2, also known as RKE Government, is Rancher's new Kubernetes distribution.
    RKE2 differs from RKE in several ways, the first being its focus on security.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: RKE2，也称为RKE Government，是Rancher的新Kubernetes发行版。RKE2与RKE有几个不同之处，第一个就是它更注重安全性。
- en: Note
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To make things easier in this section, we will call the original RKE distribution
    **RKE1**, with the new distribution being called **RKE2**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化本节内容，我们将原始的RKE发行版称为**RKE1**，新的发行版称为**RKE2**。
- en: 'Originally in RKE1 clusters, the cluster was not entirely secure by default,
    meaning you had to take steps (The Rancher Hardening Guide: https://rancher.com/docs/rancher/v2.6/en/security/#rancher-hardening-guide)
    to pass the CIS Kubernetes Benchmark. Both because of the difficulty of the process
    and the fact that some people didn''t even know about it meant a good number of
    RKE1 clusters were left insecure. RKE2 flips that model around by being secure
    by default and requiring you to go through several complex steps to make it less
    secure. RKE2 also passes the CIS Kubernetes Benchmark v1.5 and v1.6 like RKE1
    does and FIPS 140-2 compliance. Finally, on the security side, the RKE2 process
    has been built from the start with CVE scanning as part of the build pipeline,
    making it very difficult to include known CVE issues in the product.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '在 RKE1 集群中，集群默认并不完全安全，这意味着你需要采取一些步骤（《Rancher 硬化指南》: https://rancher.com/docs/rancher/v2.6/en/security/#rancher-hardening-guide）才能通过
    CIS Kubernetes 基准测试。由于这个过程的复杂性以及一些人甚至不知道这件事，导致许多 RKE1 集群没有得到充分保护。RKE2 改变了这一模型，它默认是安全的，要求你通过多个复杂步骤来降低安全性。RKE2
    还像 RKE1 一样通过了 CIS Kubernetes 基准测试 v1.5 和 v1.6，并符合 FIPS 140-2 标准。最后，在安全方面，RKE2
    过程从一开始就构建了 CVE 扫描，作为构建流水线的一部分，这使得在产品中包含已知的 CVE 问题变得非常困难。'
- en: The second big difference is the conversion from Docker to containerd. With
    RKE1, everything was built around Docker and Docker-based commands/APIs. And with
    the announcement of removing support for the Docker runtime in Kubernetes v1.22,
    this migration is a must in the long-term supportability of Kubernetes. I will
    note that Dockershim, an adapter between the Kubernetes **CRI** (**Container Runtime
    Interface**) and Docker, has allowed people to keep using Docker with Kubernetes
    for the foreseeable future. Rancher is also going to maintain a fork of cri-dockerd.
    Please see [https://github.com/rancher/rancher/issues/30307](https://github.com/rancher/rancher/issues/30307)
    for more details and the official statement. With all that being said, RKE2 and
    K3s moved to containerd because of speed and management overhead. Docker brings
    a lot of tools and libraries into the picture that are not needed by a Kubernetes
    host and wastes resources. This is because Docker is running containerd under
    the hood, so why not remove that layer and just let kubelet directly manage containterd?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个重大区别是从 Docker 转向 containerd。在 RKE1 中，一切都是围绕 Docker 和基于 Docker 的命令/APIs 构建的。而随着
    Kubernetes v1.22 宣布不再支持 Docker 运行时，这一迁移对于 Kubernetes 的长期支持是必须的。我需要指出的是，Dockershim
    是 Kubernetes **CRI**（**容器运行时接口**）和 Docker 之间的适配器，它允许人们在可预见的未来继续在 Kubernetes 中使用
    Docker。Rancher 也将维护 cri-dockerd 的一个分支。有关更多细节和官方声明，请参见 [https://github.com/rancher/rancher/issues/30307](https://github.com/rancher/rancher/issues/30307)。话虽如此，RKE2
    和 K3s 之所以转向 containerd，是因为它在速度和管理开销方面的优势。Docker 带来了许多 Kubernetes 主机并不需要的工具和库，浪费了资源。因为
    Docker 本身就在底层运行 containerd，那么为何不去掉这层，并直接让 kubelet 管理 containerd 呢？
- en: The third significant change was moving from running everything in containers
    to allowing some low-level components such as kubelet to run as a binary directly
    on the host. Because items such as kubelet are outside of containterd, kubelet
    can do tasks such as upgrading containerd without running into the chicken and
    the egg issue as you would in RKE1\. Kubelet can't upgrade Docker because the
    first step is to stop Docker, which stops kubelet, which stops the upgrade. As
    you can see in the following diagram shown, RKE2 has **Managed processes**, which
    run directly on the operating system and not as containers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个重要变化是从将所有内容运行在容器中，转变为允许一些低级组件（如 kubelet）作为二进制文件直接在主机上运行。由于 kubelet 等项目不在
    containerd 中运行，它可以执行如升级 containerd 这样的任务，而不会遇到 RKE1 中的“先有鸡还是先有蛋”的问题。Kubelet 不能升级
    Docker，因为第一步是停止 Docker，这会停止 kubelet，进而停止升级。正如下图所示，RKE2 有**管理的进程**，这些进程直接运行在操作系统上，而不是作为容器运行。
- en: '![Figure 4.2 – RKE2 server and agent high-level diagram'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2 – RKE2 服务器和代理的高级架构图'
- en: '](img/B18053_04_002.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_04_002.jpg)'
- en: Figure 4.2 – RKE2 server and agent high-level diagram
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – RKE2 服务器和代理的高级架构图
- en: The fourth significant change was no more containers that are not pods. In RKE1,
    most of the core components such as the kubelet, etcd, kube-apiserver, kube-controller-manger,
    and kube-scheduler were Docker containers but were not Kubernetes pods. This was
    because these containers were created and managed by the RKE binary and not by
    kubelet. With RKE2, kubelet is a binary on the host operating system, and kubelet
    can use static pods. These pods are unique because they can be created and started
    without etcd, kube-apiserver, and so on. This is done by making the manifest files
    that include all the items usually provided by the cluster ahead of time, such
    as the pod name, IP, MAC address, secrets, volumes, and so on, at which point,
    kubelet can start and manage the pod just like any other pod. Once kubelet can
    connect to the cluster, that pod can be discovered and added into etcd.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个重大变化是不再使用非 Pod 容器。在 RKE1 中，许多核心组件，如 kubelet、etcd、kube-apiserver、kube-controller-manager
    和 kube-scheduler，都是 Docker 容器，但不是 Kubernetes Pod。这是因为这些容器是由 RKE 二进制文件创建和管理的，而不是由
    kubelet 管理的。随着 RKE2 的出现，kubelet 成为主机操作系统上的一个二进制文件，kubelet 可以使用静态 Pod。这些 Pod 很特别，因为它们可以在没有
    etcd、kube-apiserver 等的情况下创建和启动。这个过程是通过预先创建包含通常由集群提供的所有项的清单文件来完成的，例如 Pod 名称、IP、MAC
    地址、机密、卷等，在此时，kubelet 可以像管理任何其他 Pod 一样启动并管理该 Pod。一旦 kubelet 能够连接到集群，该 Pod 就可以被发现并添加到
    etcd 中。
- en: 'This leads us to the most significant change in RKE2 over RKE1: moving from
    centralized control to a distributed control model. With RKE1, all the core cluster
    management tasks were managed by the RKE1 binary itself. Even when Rancher is
    managing the cluster, it creates the `cluster.yml` file and runs the RKE1 binary
    inside the Rancher server. Because of this, with RKE1, you must update `cluster.yaml`
    and run an `rke up` command. This process then causes the RKE1 binary to reconcile
    the cluster. RKE2 takes this process and moves it into the cluster, meaning an
    RKE2 cluster can manage itself. RKE2 uses an approach where you bootstrap the
    first master node in the cluster then join the other master and worker nodes.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了 RKE2 相对于 RKE1 的最重要变化：从集中式控制转向分布式控制模型。在 RKE1 中，所有核心的集群管理任务都是由 RKE1 二进制文件本身管理的。即使
    Rancher 管理集群，它也会在 Rancher 服务器内部创建 `cluster.yml` 文件并运行 RKE1 二进制文件。因此，在 RKE1 中，您必须更新
    `cluster.yaml` 并运行 `rke up` 命令。这个过程会导致 RKE1 二进制文件进行集群的协调。RKE2 将这一过程转移到集群内部，这意味着
    RKE2 集群能够自我管理。RKE2 使用的方法是先引导集群中的第一个主节点，然后将其他主节点和工作节点加入集群。
- en: As part of moving to a distributed model with RKE1, you had the option to deploy
    simple YAML files as an add-on job. RKE2 builds on this to allow you to deploy
    YAML files and Helm charts as part of the deployment process. This allows you
    to move several tasks into the cluster creation process. For example, you are
    deploying Rancher's Longhorn product, which provides a distributed storage provider,
    as part of cluster creation. This is important as a good number of environments
    will need storage to start deploying over services such as Prometheus. RKE2 integrates
    K3s' Helm controller, which allows you to define a Helm chart installation via
    YAML. Then the Helm controller will spin up a pod to handle deploying and upgrading
    that chart, with the end goal being to move most of the cluster tasks, such as
    adding and removing nodes to and from clusters, from a process inside the Rancher
    server to a process on the downstream cluster itself. This helps Rancher support
    managing clusters at a mass scale (1 million+ clusters for a single Rancher deployment),
    including better data handling for edge clusters by lowering traffic between the
    Rancher servers and the edge cluster.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RKE1 迁移到分布式模型的过程中，您可以选择将简单的 YAML 文件部署为附加作业。RKE2 在此基础上进行扩展，允许您在部署过程中部署 YAML
    文件和 Helm 图表。这使得您能够将多个任务迁移到集群创建过程中。例如，您在部署 Rancher 的 Longhorn 产品时，它作为集群创建的一部分提供分布式存储服务。这一点非常重要，因为许多环境在开始部署像
    Prometheus 这样的服务时需要存储。RKE2 集成了 K3s 的 Helm 控制器，允许您通过 YAML 定义 Helm 图表的安装。然后，Helm
    控制器将启动一个 Pod 来处理该图表的部署和升级，最终目标是将大多数集群任务（例如将节点添加到集群或从集群中移除节点）从 Rancher 服务器内的过程转移到下游集群本身的过程。这帮助
    Rancher 支持大规模集群管理（单个 Rancher 部署可支持 100 万+ 集群），并通过降低 Rancher 服务器与边缘集群之间的流量来改善边缘集群的数据处理。
- en: Finally, the process of joining a node to an existing RKE2 cluster is a lot
    different than an RKE1 cluster. With RKE1, the binary handles deploying the containers
    to the new node. With RKE2, there is a simple process where the first node is
    started with a special flag, `cluster-init`. This tells RKE2 to create a new cluster.
    This includes creating a new etcd cluster and creating a fresh root CA (kube-ca),
    and if it is not set, RKE2 will generate a token for the cluster. Once the first
    node has been created, the Kubernetes API should be available and used by the
    rest of the nodes to join the cluster. This is why you need a round-robin DNS
    record or a load balancer for your Kubernetes API endpoint. Moving forward, you
    need to set the RKE2 cluster token on each new node in the cluster along with
    the server endpoint. Both these settings are required for RKE2 even to start.
    These settings are defined in the file `/etc/rancher/rke2/config.yaml`. Note that
    with RKE2, this file configures RKE2 in general. So if you want to define kubelet
    settings, node labels/taints, and so on, this file must be protected as the token
    stored in that file is used for several security-related tasks such as Kube-API
    access and etcd encryption. So, the token should be treated as a password, that
    is, it should be unique, random, and long. The default length is 32 characters.
    You should also avoid using special control characters such as the dollar sign,
    backslash, quotemarks, and so on.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将节点加入现有 RKE2 集群的过程与加入 RKE1 集群的过程有很大不同。在 RKE1 中，二进制文件负责将容器部署到新节点上。而在 RKE2
    中，存在一个简单的过程，首先启动第一个节点时使用特殊标志 `cluster-init`，该标志告诉 RKE2 创建一个新集群。这包括创建一个新的 etcd
    集群并创建一个新的根 CA（kube-ca）。如果没有设置，RKE2 会为集群生成一个令牌。一旦第一个节点创建完成，Kubernetes API 应该就可以使用，其他节点可以用它来加入集群。因此，你需要为
    Kubernetes API 端点设置一个轮询 DNS 记录或负载均衡器。接下来，你需要在集群中的每个新节点上设置 RKE2 集群令牌以及服务器端点。为了使
    RKE2 启动，这两个设置都是必需的。这些设置定义在文件 `/etc/rancher/rke2/config.yaml` 中。请注意，在 RKE2 中，这个文件用于总体配置
    RKE2。所以如果你想定义 kubelet 设置、节点标签/污点等，必须保护该文件，因为文件中存储的令牌用于多个安全相关任务，如 Kube-API 访问和
    etcd 加密。因此，令牌应视为密码，即它应是唯一的、随机的且足够长。默认长度为 32 个字符。你还应避免使用特殊控制字符，如美元符号、反斜杠、引号等。
- en: What is RancherD?
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 RancherD？
- en: RancherD is a special binary designed to bootstrap a Kubernetes cluster (K3s/RKE2)
    and Rancher. The idea is to simplify the creation of the K3s/RKE2 cluster that
    directly supports the Rancher server application and its components. RancherD
    is designed only to be run once on a node, at which point RancherD takes care
    of setting up K3s/RKE2 and the manifest files needed for Rancher. This process
    can be beneficial when you want to deploy large numbers of Rancher deployments.
    For example, if you wish each of your Kubernetes clusters to have its own Rancher
    dashboard, environments do not share a single Rancher deployment. This is seen
    a lot with hosting companies that provide Rancher as a service to their customers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: RancherD 是一个特殊的二进制文件，旨在引导 Kubernetes 集群（K3s/RKE2）和 Rancher。其目的是简化创建直接支持 Rancher
    服务器应用及其组件的 K3s/RKE2 集群的过程。RancherD 设计为仅在节点上运行一次，运行时，它负责设置 K3s/RKE2 以及为 Rancher
    所需的清单文件。当你希望在大规模部署 Rancher 时，这个过程非常有用。例如，如果你希望每个 Kubernetes 集群都有自己的 Rancher 仪表板，而不同的环境不共享同一个
    Rancher 部署。很多提供 Rancher 服务的托管公司都会使用这种方式。
- en: It is important to note that RancherD uses RKE2 behind the scenes, so the same
    requirements and limitations apply. We'll be covering those requirements and limitations
    in the next section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，RancherD 在后台使用 RKE2，因此相同的要求和限制适用。我们将在下一节中介绍这些要求和限制。
- en: Requirements and limitations
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要求和限制
- en: In this section, we'll discuss the basic requirements for RKE and RKE2 clusters
    along with their limitations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将讨论 RKE 和 RKE2 集群的基本要求及其限制。
- en: Basic requirements
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本要求
- en: 'Let''s have a look at the basic requirements for **RKE** first:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来看一下 **RKE** 的基本要求：
- en: RKE requires Docker to be installed on the host before RKE can join it to the
    cluster.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 RKE 将主机加入集群之前，必须先在主机上安装 Docker。
- en: RKE runs on almost any Linux OS, but you should refer to the Rancher Support
    Matrix located at [https://rancher.com/support-maintenance-terms/](https://rancher.com/support-maintenance-terms/).
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE 可以在几乎任何 Linux 操作系统上运行，但你应该参考位于 [https://rancher.com/support-maintenance-terms/](https://rancher.com/support-maintenance-terms/)
    的 Rancher 支持矩阵。
- en: RKE requires SSH access to all nodes in the cluster using an SSH key.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE 需要使用 SSH 密钥访问集群中的所有节点。
- en: RKE requires permissions to run Docker commands without sudo or a password.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE 需要权限以在不使用 sudo 或密码的情况下运行 Docker 命令。
- en: All nodes in a cluster must be routable to all other nodes, meaning you can
    have nodes on direct subnets, but you must be able to connect between nodes directly
    without using an NAT.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中的所有节点必须能够相互路由，这意味着你可以将节点放在直接子网中，但必须能够在节点之间直接连接，而无需使用 NAT。
- en: RKE requires firewall rules to be opened between nodes depending on the role
    of the node. Please see https://rancher.com/docs/rke/latest/en/os/#ports for more
    details.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE 需要根据节点角色在节点之间开放防火墙规则。有关更多详情，请参见 [https://rancher.com/docs/rke/latest/en/os/#ports](https://rancher.com/docs/rke/latest/en/os/#ports)。
- en: An RKE cluster requires at least one node for each role in the cluster, with
    the roles being etcd, controlplane, and worker. The cluster will not come online
    until this requirement is met. Note that a node can have multiple roles, including
    having all three roles.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 RKE 集群至少需要一个节点来承担每个角色，包括 etcd、controlplane 和 worker。集群在满足这一要求之前不会上线。请注意，一个节点可以承担多个角色，包括同时拥有这三个角色。
- en: RKE only requires a minimum of one core, which changes with cluster size and
    node role selection. It's recommended to at least have two cores, with the standard
    size being four cores.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE 只需要至少一个核心，具体取决于集群大小和节点角色选择。建议至少使用两个核心，标准配置为四个核心。
- en: The memory requirement is based on the node's role, with the etcd role needing
    about 4 GB, kube-apiserver needing 4 GB, and the worker role needing about 2 GB.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存要求取决于节点的角色，其中 etcd 角色需要大约 4 GB，kube-apiserver 需要 4 GB，工作节点角色需要大约 2 GB。
- en: For storage, you'll need around 10 GB of tier 1/2 storage (SSD is preferred
    but not required).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于存储，你需要大约 10 GB 的一级/二级存储（推荐 SSD，但不是必需的）。
- en: For the filesystem, RKE relies on Docker storage drivers, so please review Docker's
    documentation at [https://docs.docker.com/storage/storagedriver/](https://docs.docker.com/storage/storagedriver/).
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于文件系统，RKE 依赖 Docker 存储驱动程序，因此请查看 Docker 的文档：[https://docs.docker.com/storage/storagedriver/](https://docs.docker.com/storage/storagedriver/)。
- en: RKE doesn't require its own filesystem, but it's recommended that `/var/lib/docker`
    be on its own filesystem/disk to prevent Docker from filling up the root filesystem.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE 不需要自己的文件系统，但建议将 `/var/lib/docker` 存放在独立的文件系统/磁盘上，以防止 Docker 填满根文件系统。
- en: On etcd nodes, the database is stored in a bind mount located at `/var/lib/etcd`
    and is not required to be on its own dedicated disk. Also, with larger clusters,
    it is recommended for this data to be stored on tier 1 storage as etcd can be
    sensitive to storage latency.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 etcd 节点上，数据库存储在位于 `/var/lib/etcd` 的绑定挂载中，不需要放在专用磁盘上。此外，在更大的集群中，建议将这些数据存储在一级存储上，因为
    etcd 对存储延迟较为敏感。
- en: If you are using RKE's local backup option, the `/opt/rke/etcd-snapshots` directory
    should be its own filesystem or an NFS share for safety reasons.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用 RKE 的本地备份选项，`/opt/rke/etcd-snapshots` 目录应使用独立的文件系统或 NFS 共享，以确保安全。
- en: 'The following are the requirements of **RKE2**:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 **RKE2** 的要求：
- en: RKE2 runs on almost any Linux OS, but you should refer to the Rancher Support
    Matrix located at [https://rancher.com/support-maintenance-terms/](https://rancher.com/support-maintenance-terms/).
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE2 可以运行在几乎所有的 Linux 操作系统上，但你应参考位于 [https://rancher.com/support-maintenance-terms/](https://rancher.com/support-maintenance-terms/)
    的 Rancher 支持矩阵。
- en: Installing and configuring RKE2 requires root-level permissions on the host.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和配置 RKE2 需要主机上的 root 权限。
- en: All nodes in a cluster must be routable to all other nodes, meaning you can
    have nodes on direct subnets, but you must be able to connect between nodes directly
    without using an NAT.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中的所有节点必须能够相互路由，这意味着你可以将节点放在直接子网中，但必须能够在节点之间直接连接，而无需使用 NAT。
- en: RKE2 requires firewall rules to be opened between nodes depending on the role
    of the node. Please see [https://rancher.com/docs/rancher/v2.5/en/installation/requirements/ports/](https://rancher.com/docs/rancher/v2.5/en/installation/requirements/ports/)
    for more details.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE2 需要根据节点角色在节点之间开放防火墙规则。有关更多详情，请参见 [https://rancher.com/docs/rancher/v2.5/en/installation/requirements/ports/](https://rancher.com/docs/rancher/v2.5/en/installation/requirements/ports/)。
- en: RKE2 requires a master and worker node in the cluster. Note that both roles
    can be on the same node.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE2 需要集群中的主节点和工作节点。请注意，这两个角色可以在同一个节点上。
- en: RKE2 only requires a minimum of one core, which changes with cluster size and
    node role selection. It's recommended to have at least have two cores, with the
    standard size being four cores.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE2 只需要至少一个核心，具体取决于集群大小和节点角色选择。建议至少使用两个核心，标准配置为四个核心。
- en: The memory requirement is based on the node's role, with the master role needing
    4 GB and the worker/agent role requiring about 2 GB.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存需求取决于节点的角色，主节点需要 4 GB，而工作节点/代理节点大约需要 2 GB。
- en: RKE2 stores its data under the mount point `/var/lib/rancher`, which includes
    containerd data, images, etcd, and so on.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE2 将其数据存储在挂载点 `/var/lib/rancher` 下，包括 containerd 数据、镜像、etcd 等。
- en: For master nodes, etcd is stored under `/var/lib/rancher/rke2/server/db/etcd`;
    it is recommended that this data be stored on tier 1 storage as etcd can be sensitive
    to storage latency.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于主节点，etcd 存储在 `/var/lib/rancher/rke2/server/db/etcd` 下；建议将此数据存储在一级存储上，因为 etcd
    对存储延迟较为敏感。
- en: RKE2 has etcd backups turned on by default and stores the backups under `/var/lib/rancher/rke2/server/db/snapshots,`
    which should be its own filesystem or can be an NFS share for safety reasons.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE2 默认启用 etcd 备份，并将备份存储在 `/var/lib/rancher/rke2/server/db/snapshots` 下，建议将其存储在独立的文件系统中，或者出于安全原因，使用
    NFS 共享。
- en: Design limitations and considerations
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计限制和考虑事项
- en: Now, let's discuss the design limitations for both clusters.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论一下两个集群的设计限制。
- en: 'The following lists the design considerations for RKE:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列出了 RKE 的设计考虑事项：
- en: After creating a cluster, you cannot change the network provider (CNI), called
    the network plugin in the RKE documentation located at [https://rancher.com/docs/rancher/v2.5/en/faq/networking/cni-providers/](https://rancher.com/docs/rancher/v2.5/en/faq/networking/cni-providers/).
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建集群后，无法更改网络提供者（CNI），在 RKE 文档中称为网络插件，相关文档位于 [https://rancher.com/docs/rancher/v2.5/en/faq/networking/cni-providers/](https://rancher.com/docs/rancher/v2.5/en/faq/networking/cni-providers/)。
- en: By default, RKE will use the subnets `10.42.0.0/16` and `10.43.0.0/16` for the
    pod overlay and service network. This cannot be changed after cluster creation,
    so if these networks overlap with your current network, you'll need to choose
    different subnets for your cluster.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，RKE 将使用子网 `10.42.0.0/16` 和 `10.43.0.0/16` 作为 Pod 覆盖和服务网络。集群创建后无法更改此设置，因此，如果这些网络与您的当前网络重叠，您需要为集群选择不同的子网。
- en: Support for Windows worker nodes is currently limited to Rancher-managed RKE
    clusters. You cannot use RKE directly to join a Windows node.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 Windows 工作节点的支持目前仅限于 Rancher 管理的 RKE 集群。您无法直接使用 RKE 将 Windows 节点加入集群。
- en: RKE currently doesn't provide support for the ARM64 OS.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE 目前不支持 ARM64 操作系统。
- en: RKE supports an air-gapped environment, but you are required to do some additional
    steps, which are located at https://rancher.com/docs/rke/latest/en/config-options/system-images/#air-gapped-setups.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE 支持空网环境，但需要执行一些额外步骤，相关步骤可参见 https://rancher.com/docs/rke/latest/en/config-options/system-images/#air-gapped-setups。
- en: Switching an RKE to an air-gapped environment can be done by reconfiguring the
    cluster, deployments, and applications.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 RKE 切换到空网环境可以通过重新配置集群、部署和应用程序来完成。
- en: RKE clusters can be built across data centers so long as there is an odd number
    of data centers with an etcd and control plane node in each data center.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只要数据中心数量为奇数，并且每个数据中心中都有 etcd 和控制平面节点，RKE 集群可以跨数据中心构建。
- en: Network latency between etcd nodes should be less than 10 ms.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd 节点之间的网络延迟应小于 10 毫秒。
- en: 'The following are the design considerations for RKE2:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 RKE2 的设计考虑事项：
- en: After creating a cluster, you cannot change the network provider (CNI), called
    the network plugin in the RKE2 documentation located at [https://docs.rke2.io/install/network_options/](https://docs.rke2.io/install/network_options/).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建集群后，无法更改网络提供者（CNI），在 RKE2 文档中称为网络插件，相关文档位于 [https://docs.rke2.io/install/network_options/](https://docs.rke2.io/install/network_options/)。
- en: By default, RKE2 will use the subnets `10.42.0.0/16` and `10.43.0.0/16` for
    the pod overlay and service network. This cannot be changed after cluster creation
    so if you are currently using one of these subnets in your current network, you
    should change this setting.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，RKE2 将使用子网 `10.42.0.0/16` 和 `10.43.0.0/16` 作为 Pod 覆盖和服务网络。集群创建后无法更改此设置，因此，如果您当前的网络使用了这些子网之一，您应该更改此设置。
- en: As of writing, RKE2 provides experimental support for Windows nodes, but you
    still need to provide a Linux node in the cluster.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截至目前，RKE2 提供对 Windows 节点的实验性支持，但您仍然需要在集群中提供 Linux 节点。
- en: As of writing, RKE2 doesn't provide support for the ARM64 OS even though K3s
    does.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截至目前，RKE2 不支持 ARM64 操作系统，尽管 K3s 支持。
- en: Note
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: There is an open feature request ([https://github.com/rancher/rke2/issues/1946](https://github.com/rancher/rke2/issues/1946))
    to offer ARM64 support.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有一个公开的功能请求（[https://github.com/rancher/rke2/issues/1946](https://github.com/rancher/rke2/issues/1946)），要求提供对
    ARM64 的支持。
- en: RKE2 supports an air-gapped environment, but you are required to do some additional
    steps, which are located at [https://docs.rke2.io/install/airgap/](https://docs.rke2.io/install/airgap/).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE2支持气隙环境，但您需要执行一些额外的步骤，这些步骤位于[https://docs.rke2.io/install/airgap/](https://docs.rke2.io/install/airgap/)。
- en: RKE2 in an air-gapped environment can work in private registry mode and tarball
    mode with the steps located at [https://docs.rke2.io/install/airgap/](https://docs.rke2.io/install/airgap/).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE2在气隙环境中可以通过私有注册表模式和tarball模式工作，相关步骤位于[https://docs.rke2.io/install/airgap/](https://docs.rke2.io/install/airgap/)。
- en: RKE2 clusters can be built across data centers so long as there is an odd number
    of data centers with a master node in each data center.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只要数据中心数量为奇数，并且每个数据中心都有一个主节点，RKE2集群可以跨数据中心构建。
- en: Network latency between etcd nodes should be less than 10 ms.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd节点之间的网络延迟应该小于10毫秒。
- en: Now that we understand the limitations of RKE1 and RKE2, we'll be using these
    along with a set of rules and examples to help us design a solution using RKE1
    and RKE2.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了RKE1和RKE2的限制，我们将利用这些信息以及一系列规则和示例，帮助我们使用RKE1和RKE2设计解决方案。
- en: Rules for architecting a solution
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决方案架构规则
- en: In this section, we'll cover some standard designs and the pros and cons of
    each. It is important to note that each environment is unique and will require
    tuning for the best performance and experience. It's also important to note that
    all CPU, memory, and storage sizes are recommended starting points and may need
    to be increased or decreased by your workloads and deployment processes.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍一些标准设计及其优缺点。需要注意的是，每个环境都是独特的，需要进行调整，以实现最佳的性能和体验。同时，所有的CPU、内存和存储大小是推荐的起始值，可能需要根据工作负载和部署过程的需求进行增加或减少。
- en: 'Before designing a solution, you should be able to answer the following questions:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计解决方案之前，您应该能够回答以下问题：
- en: Will multiple environments be sharing the same cluster?
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有多个环境共享同一集群？
- en: Will production and non-production workloads be on the same cluster?
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产环境和非生产环境的工作负载是否会共享同一个集群？
- en: What level of availability does this cluster require?
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该集群需要什么级别的可用性？
- en: Will this cluster be spanning multiple data centers in a metro cluster environment?
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该集群是否将在多个数据中心之间跨越多个数据中心的环境中运行？
- en: How much latency will there be between nodes in the cluster?
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中节点之间的延迟是多少？
- en: How many pods will be hosted in the cluster?
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中将托管多少个Pod？
- en: What are the average and maximum size of pods you will be deploying in the cluster?
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将部署到集群中的Pod的平均大小和最大大小是多少？
- en: Will you need GPU support for some of your applications?
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您是否需要为某些应用程序提供GPU支持？
- en: Will you need to provide storage to your applications?
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您是否需要为应用程序提供存储？
- en: If you need storage, do you need only **RWO** (**Read Write Once**), or will
    you need **RWX** (**Read Write Many**)?
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要存储，您只需要**RWO**（**一次读写**），还是需要**RWX**（**多次读写**）？
- en: RKE clusters
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE集群
- en: The following are designs for RKE clusters.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是RKE集群的设计方案。
- en: Single-node clusters
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单节点集群
- en: In this design, we will be deploying an RKE cluster on a single node with all
    roles.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在此设计中，我们将在单个节点上部署一个包含所有角色的RKE集群。
- en: '![Figure 4.3 – RKE single-node cluster'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.3 – RKE单节点集群](img/B18053_04_003.jpg)'
- en: '](img/B18053_04_003.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_04_003.jpg)'
- en: Figure 4.3 – RKE single-node cluster
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – RKE单节点集群
- en: 'Example config: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/00_single_node_cluster/cluster.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/00_single_node_cluster/cluster.yaml)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 示例配置：[https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/00_single_node_cluster/cluster.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/00_single_node_cluster/cluster.yaml)
- en: 'The **pros** of a single-node cluster are listed as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 单节点集群的**优点**如下所示：
- en: Simple to set up.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置简单。
- en: Fast and easy to create.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建速度快且容易。
- en: No external load balancer is needed.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要外部负载均衡器。
- en: A great cluster for CI/CD pipelines that need a Kubernetes cluster for testing
    that can be destroyed later.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个适合CI/CD管道的优秀集群，适用于需要Kubernetes集群进行测试且之后可以销毁的场景。
- en: Useful for sandbox testing where HA and scale is not needed.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于不需要高可用性和扩展的沙箱测试。
- en: Can be installed on a developer's laptop where resources are minimal.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以安装在资源有限的开发者笔记本电脑上。
- en: A single-node RKE cluster can be converted to an HA cluster later.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单节点RKE集群可以后续转换为高可用集群（HA集群）。
- en: 'The **cons** are listed as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下所示：'
- en: No HA.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无高可用性（HA）。
- en: Downtime is required during patching and upgrades.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在打补丁和升级过程中需要停机。
- en: Can encourage bad application behavior by using the server's IP or hostname
    for the application endpoint instead of VIP or CNAME.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用服务器的 IP 地址或主机名作为应用程序端点，而不是 VIP 或 CNAME，可能会导致不良的应用程序行为。
- en: Many Kubernetes components get their HA features from the cluster itself, so
    a lot of the components won't be able to handle failures as cleanly as they would
    in an HA cluster.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多 Kubernetes 组件通过集群本身获得其高可用性功能，因此许多组件无法像在 HA 集群中那样干净地处理故障。
- en: User applications share the same nodes as management services, meaning that
    a runaway application can take down the cluster.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户应用程序与管理服务共享相同的节点，这意味着一个失控的应用程序可能会导致集群崩溃。
- en: 'The following are the **hardware requirements**:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是**硬件要求**：
- en: 'Servers(s): 1 physical/virtual server'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器：1 台物理/虚拟服务器
- en: 'CPU: 4 cores'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：4 核
- en: 'Memory: 4-8GB'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：4-8GB
- en: Small three-node clusters
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小型三节点集群
- en: In this design, we will be deploying the smallest RKE cluster with full HA,
    a three-node cluster with all nodes having all roles.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在此设计中，我们将部署最小的 RKE 集群，具有完整的高可用性（HA），一个三节点集群，所有节点都具有所有角色。
- en: '![Figure 4.4 – Standard three-node RKE cluster'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.4 – 标准三节点 RKE 集群'
- en: '](img/B18053_04_004.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_04_004.jpg)'
- en: Figure 4.4 – Standard three-node RKE cluster
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – 标准三节点 RKE 集群
- en: 'Example config: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/01_small_cluster/cluster.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/01_small_cluster/cluster.yaml)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 示例配置：[https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/01_small_cluster/cluster.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/01_small_cluster/cluster.yaml)
- en: 'The **pros** of a small three-node cluster are listed as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 小型三节点集群的**优点**如下所示：
- en: Full HA – you can lose any node in the cluster and still have full cluster and
    application availability.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的高可用性 – 您可以丢失集群中的任何节点，仍然可以保持集群和应用程序的可用性。
- en: Simple to manage as all nodes have the same roles, so all nodes are the same.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理简单，因为所有节点具有相同的角色，所以所有节点都是一样的。
- en: No required downtime during patching and upgrades. Please see Rancher's zero
    downtime documentation for more details at [https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/](https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在打补丁和升级过程中不需要停机。有关更多详细信息，请参阅 Rancher 的零停机文档：[https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/](https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/).
- en: 'The **cons** are listed as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下所示：'
- en: An external load balancer or a round-robin DNS record is needed for external
    application access.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要外部负载均衡器或轮询 DNS 记录来访问外部应用程序。
- en: User applications share the same nodes as management services, meaning that
    a runaway application can take down the cluster.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户应用程序与管理服务共享相同的节点，这意味着一个失控的应用程序可能会导致集群崩溃。
- en: Only `N+1` of availability, so during maintenance tasks, you cannot suffer a
    failure of a node without loss of service.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有 `N+1` 的可用性，因此在维护任务期间，如果失去一个节点，服务也会中断。
- en: 'The following are the **hardware requirements**:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是**硬件要求**：
- en: 'Servers(s): 3 physical/virtual servers'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器：3 台物理/虚拟服务器
- en: 'CPU: 4 cores per server'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：每台服务器 4 核
- en: 'Memory: 4-8GB per server'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：每台服务器 4-8GB
- en: Medium clusters
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 中型集群
- en: In this design, we will be deploying the standard RKE cluster where we have
    migrated the core management services for Kubernetes to their nodes. This is done
    because as clusters grow in size, protecting the management services for Kubernetes
    becomes even more critical. This design tries to balance HA with cost. This is
    done by having etcd and the control plane share the same nodes but with the change
    of moving the worker roles to their own set nodes. This design works for 2 to
    10 worker node clusters.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设计中，我们将部署标准的 RKE 集群，其中我们已将 Kubernetes 的核心管理服务迁移到它们的节点上。这样做的原因是，随着集群规模的扩大，保护
    Kubernetes 的管理服务变得更加关键。此设计尝试在高可用性（HA）和成本之间取得平衡。通过将 etcd 和控制平面共享相同的节点，同时将工作节点角色移到专用节点上来实现。这种设计适用于
    2 到 10 个工作节点的集群。
- en: '![Figure 4.5 – RKE cluster with separate nodes for management services'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.5 – 具有独立节点的 RKE 集群用于管理服务'
- en: '](img/B18053_04_005.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_04_005.jpg)'
- en: Figure 4.5 – RKE cluster with separate nodes for management services
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – 具有独立节点的 RKE 集群用于管理服务
- en: 'Diagram: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/02_medium_cluster/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/02_medium_cluster/README.md)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图示：[https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/02_medium_cluster/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/02_medium_cluster/README.md)
- en: 'Example config: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/02_medium_cluster/cluster.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/02_medium_cluster/cluster.yaml)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 示例配置：[https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/02_medium_cluster/cluster.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke/02_medium_cluster/cluster.yaml)
- en: 'The **pros** of a medium cluster are listed as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 中型集群的**优点**如下：
- en: Full HA – you can lose any one of the management nodes (etcd and control plane)
    in the cluster and still have complete cluster management.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全高可用 – 你可以丢失集群中的任何一个管理节点（etcd和控制平面），仍然能够保持完整的集群管理。
- en: User workloads and management services run on different nodes, stopping runaway
    applications from taking down the cluster.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户工作负载和管理服务运行在不同的节点上，防止失控的应用程序导致集群崩溃。
- en: Due to the scalable limitations of etcd, having more than five etcd nodes causes
    a decrease in performance. So it is normally recommended to scale the design vertically
    instead of horizontally.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于etcd的可扩展性限制，etcd节点超过五个时，性能会下降。因此，通常建议采用纵向扩展设计，而非横向扩展。
- en: More than one worker node can fail without loss of service, assuming you have
    enough CPU and memory available on the remaining workers.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果剩余的工作节点有足够的 CPU 和内存，多个工作节点可以故障而不丧失服务。
- en: No required downtime during patching and upgrades. Please see Rancher's zero
    downtime documentation for more details at [https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/](https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在修补和升级过程中无需停机。详细信息请参阅 Rancher 的零停机文档：[https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/](https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/)。
- en: 'The **cons** are listed as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: An external load balancer or a round-robin DNS record is needed for external
    application access.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部应用访问需要外部负载均衡器或轮询DNS记录。
- en: Only `N+1` of availability, so during maintenance tasks, you cannot suffer a
    failure of a node without loss of service at the management plane (etcd and control
    plane).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅有`N+1`的可用性，因此在维护任务期间，管理平面（etcd和控制平面）的节点发生故障时，无法保证服务不中断。
- en: Additional complexity when creating nodes as you might need to size management
    nodes differently than worker nodes.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建节点时的额外复杂性，因为你可能需要根据管理节点和工作节点的不同需求来调整节点大小。
- en: 'The following are the **hardware requirements** for etcd and the control plane:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是etcd和控制平面的**硬件要求**：
- en: 'Servers(s): 3 physical/virtual servers'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器：3 台物理/虚拟服务器
- en: 'CPU: 8 cores per server'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：每台服务器 8 核
- en: 'Memory: 8-16 GB'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：8-16 GB
- en: Note
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Worker node sizing should be based on your workload and its requirements.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工作节点的大小应该根据你的工作负载及其要求来确定。
- en: Large clusters
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型集群
- en: In this design, we're expanding on the design for a medium cluster but breaking
    up etcd and the control plane and then changing the node sizing and node count.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在此设计中，我们扩展了中型集群的设计，但将etcd和控制平面分开，并调整了节点的大小和数量。
- en: '![Figure 4.6 – RKE cluster with separate nodes for etcd and control plane'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6 – RKE 集群，etcd 和控制平面使用独立节点'
- en: '](img/B18053_04_006.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_04_006.jpg)'
- en: Figure 4.6 – RKE cluster with separate nodes for etcd and control plane
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – RKE 集群，etcd 和控制平面使用独立节点
- en: 'Diagram: [https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke/03_large_cluster/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke/03_large_cluster/README.md)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图示：[https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke/03_large_cluster/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke/03_large_cluster/README.md)
- en: 'Example config: [https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke/03_large_cluster/cluster.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke/03_large_cluster/cluster.yaml)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 示例配置：[https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke/03_large_cluster/cluster.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke/03_large_cluster/cluster.yaml)
- en: 'The **pros** of large clusters are listed as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 大型集群的**优点**如下：
- en: Full HA – you can lose any two management nodes (etcd and control plane) in
    the cluster and still have complete cluster management.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的高可用性 – 集群中可以丢失任意两个管理节点（etcd 和控制平面），仍然可以完全管理集群。
- en: User workloads and management services run on different nodes, stopping runaway
    applications from taking down the cluster.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户工作负载和管理服务在不同节点上运行，防止失控的应用程序使集群崩溃。
- en: Due to the scalable limitations of etcd because having more than five etcd nodes
    causes slowness. So it is normally recommended to as etcd to design to scale vertically
    instead of horizontally.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 etcd 的扩展性限制，超过五个 etcd 节点会导致性能下降。因此，通常建议将 etcd 设计为垂直扩展，而非横向扩展。
- en: The control plane isn't designed to scale by adding more nodes because kube-apiserver
    is active on all nodes, but each node has a caching layer to increase performance
    so scaling horizontally makes the caching less efficient.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制平面并不设计为通过增加更多节点来扩展，因为 kube-apiserver 在所有节点上都是活动的，但每个节点都有缓存层以提高性能，因此水平扩展会导致缓存效率降低。
- en: '`N+2` of availability, so during maintenance tasks, you can suffer a failure
    of a node without loss of service at the management plane (etcd and control plane).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N+2` 可用性，因此在进行维护任务时，如果一个节点发生故障，管理平面（etcd 和控制平面）不会丢失服务。'
- en: More than one worker node can fail without loss of service, assuming you have
    enough CPU and memory available on the remaining workers.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果剩余工作节点的 CPU 和内存资源足够，多个工作节点可以失败而不丧失服务。
- en: No required downtime during patching and upgrades. Please see Rancher's zero
    downtime documentation for more details at [https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/](https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 补丁和升级期间无需停机。有关更多详细信息，请参见 Rancher 的零停机文档，网址为 [https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/](https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/)。
- en: 'The **cons** are listed as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下所示：'
- en: An external load balancer or a round-robin DNS record is needed for external
    application access.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要外部负载均衡器或轮询 DNS 记录来访问外部应用程序。
- en: The controllers in the control plane are not scalable, with only one controller
    being the leader at a time.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制平面中的控制器不可扩展，每次只有一个控制器是领导者。
- en: Additional complexity when creating nodes as you might need to size management
    nodes differently than worker nodes.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建节点时需要更多的复杂性，因为您可能需要根据工作节点与管理节点的不同需求来调整节点大小。
- en: 'The following are the **hardware requirements**:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 **硬件要求**：
- en: '**etcd plane**:'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**etcd 平面**：'
- en: 'Servers(s): 5 physical/virtual servers.'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器：5 台物理/虚拟服务器。
- en: 'CPU: 8-16 cores per server.'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：每台服务器 8-16 核心。
- en: 'Memory: 32-64 GB per server for the management plane.'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：每台服务器为管理平面配置 32-64 GB 内存。
- en: 'Storage: NVME storage is recommended.'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储：推荐使用 NVME 存储。
- en: '**Control plane**:'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制平面**：'
- en: 'Servers(s): 4 physical/virtual servers.'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器：4 台物理/虚拟服务器。
- en: 'CPU: 8-16 cores per server.'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：每台服务器 8-16 核心。
- en: 'Memory: 32-64 GB per server for the management plane. Note: It is recommended
    for the control plane node to match the size of etcd nodes as a starting point.'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：每台服务器为管理平面配置 32-64 GB 内存。注意：建议控制平面节点的内存配置与 etcd 节点相匹配作为起始点。
- en: Note
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Worker node sizing should be based on your workload and its requirements.
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工作节点的规格应根据您的工作负载及其要求来确定。
- en: RKE2 clusters
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE2 集群
- en: The following are design recommendations for RKE2 clusters.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 RKE2 集群的设计建议。
- en: Single-node clusters
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单节点集群
- en: In this design, we will be deploying an RKE2 cluster on a single node with all
    roles.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设计中，我们将在单个节点上部署一个包含所有角色的 RKE2 集群。
- en: '![Figure 4.7 – Single-node RKE2'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7 – 单节点 RKE2'
- en: '](img/B18053_04_007.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_04_007.jpg)'
- en: Figure 4.7 – Single-node RKE2
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 单节点 RKE2
- en: 'Diagram: [https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke2/00_single_node_cluster/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke2/00_single_node_cluster/README.md)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图示：[https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke2/00_single_node_cluster/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke2/00_single_node_cluster/README.md)
- en: 'The **pros** are as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: Simple to set up.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单易于设置。
- en: Fast and easy to create.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速且容易创建。
- en: No external load balancer needed.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需外部负载均衡器。
- en: Great for CI/CD pipeline jobs that need a Kubernetes cluster to test their deployments
    with, after which the cluster will be destroyed.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常适合 CI/CD 管道任务，需要一个 Kubernetes 集群来测试其部署，测试完成后集群将被销毁。
- en: Great for sandbox testing where HA and scale are not needed.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常适合沙箱测试，在这种测试中不需要高可用性和扩展性。
- en: Can be installed on a developer's laptop environments where resources are minimal.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以安装在资源有限的开发者笔记本环境中。
- en: A single-node RKE2 cluster can be converted to an HA cluster at a later date.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个单节点的 RKE2 集群可以在以后转换为高可用集群。
- en: 'The **cons** are as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: No HA.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无高可用性。
- en: Downtime is required during patching and upgrades.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在修补和升级过程中需要停机。
- en: Can encourage bad application behavior by using the server's IP or hostname
    for the application endpoint instead of VIP or CNAME.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用服务器的 IP 或主机名作为应用程序端点，而不是 VIP 或 CNAME，可能会促使应用程序出现不良行为。
- en: Many Kubernetes components get their HA features from the cluster itself, so
    a lot of the components won't be able to handle failures as cleanly as they would
    in an HA cluster.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多 Kubernetes 组件通过集群本身获得其高可用性特性，因此，许多组件在处理故障时不会像高可用集群那样顺利。
- en: User applications share the same nodes as management services, meaning that
    a runaway application can take down the cluster.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户应用与管理服务共享相同的节点，意味着一个失控的应用可能会导致整个集群崩溃。
- en: 'The following are the **hardware requirements**:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是**硬件要求**：
- en: 'Servers(s): 1 physical/virtual server'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器：1 台物理/虚拟服务器
- en: 'CPU: 2 cores'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：2 核
- en: 'Memory: 4 GB'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：4 GB
- en: Small three-node clusters
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小型三节点集群
- en: In this design, we will be deploying the smallest RKE2 cluster with full HA,
    a three-node cluster with all nodes having all roles.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在此设计中，我们将部署一个最小的 RKE2 集群，具备完全的高可用性，一个三节点集群，所有节点都承担所有角色。
- en: '![Figure 4.8 – Three-node RKE2 cluster with HA'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.8 – 带有高可用性的三节点 RKE2 集群'
- en: '](img/B18053_04_008.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_04_008.jpg)'
- en: Figure 4.8 – Three-node RKE2 cluster with HA
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 带有高可用性的三节点 RKE2 集群
- en: 'Diagram: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/01_small_cluster/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/01_small_cluster/README.md)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图示： [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/01_small_cluster/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/01_small_cluster/README.md)
- en: 'Example commands: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/01_small_cluster/commands.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/01_small_cluster/commands.md)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 示例命令：[https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/01_small_cluster/commands.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/01_small_cluster/commands.md)
- en: 'The **pros** are as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: Full HA – you can lose any node in the cluster and still have full cluster and
    application availability.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全高可用性 – 你可以失去集群中的任何节点，仍然保持集群和应用的完全可用性。
- en: Simple to manage as all nodes have the same roles, so all nodes are the same.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理简单，因为所有节点具有相同的角色，因此所有节点都相同。
- en: No required downtime during patching and upgrades. Please see Rancher's zero
    downtime documentation for more details at [https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/](https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/).
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在修补和升级过程中无需停机。详情请参考 Rancher 的零停机文档：[https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/](https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/).
- en: 'The **cons** are as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: An external load balancer or a round-robin DNS record is needed for external
    application access and RKE2 management.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要外部负载均衡器或轮询 DNS 记录来实现外部应用访问和 RKE2 管理。
- en: User applications share the same nodes as management services, meaning that
    a runaway application can take down the cluster.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户应用与管理服务共享相同的节点，意味着一个失控的应用可能会导致整个集群崩溃。
- en: Only `N+1` of availability, so during maintenance tasks, you cannot suffer a
    failure of a node without loss of service.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅有 `N+1` 的可用性，因此在进行维护任务时，若节点发生故障，服务不会中断。
- en: 'The following are the **hardware requirements**:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是**硬件要求**：
- en: 'Servers(s): 3 physical/virtual servers'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器：3 台物理/虚拟服务器
- en: 'CPU: 2-4 cores per server'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：每台服务器 2-4 核
- en: 'Memory: 4-8 GB per server'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：每台服务器 4-8 GB
- en: Medium clusters
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 中型集群
- en: In this design, we will be deploying the standard RKE2 cluster where we have
    migrated the core management services for Kubernetes to their own nodes. This
    is done because as clusters grow in size, protecting the management services for
    Kubernetes becomes even more critical. This design tries to balance HA with cost.
    This is done by having the master role on its own nodes with the worker roles
    on their own nodes. This design works for 2 to 10 worker node clusters.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在此设计中，我们将部署标准的 RKE2 集群，其中我们已将 Kubernetes 的核心管理服务迁移到专用节点。这是因为随着集群规模的增长，保护 Kubernetes
    的管理服务变得愈加关键。此设计旨在在高可用性（HA）与成本之间取得平衡。通过将主节点和工作节点分别部署在独立的节点上来实现。这种设计适用于 2 到 10 个工作节点的集群。
- en: '![Figure 4.9 – RKE2 cluster with separate nodes for management services'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.9 – RKE2 集群，其中管理服务使用独立节点'
- en: '](img/B18053_04_009.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_04_009.jpg)'
- en: Figure 4.9 – RKE2 cluster with separate nodes for management services
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – RKE2 集群，其中管理服务使用独立节点
- en: 'Diagram: [https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke2/02_medium_cluster/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke2/02_medium_cluster/README.md)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图表：[https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke2/02_medium_cluster/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/raw/main/ch04/standard_designs/rke2/02_medium_cluster/README.md)
- en: 'Example commands: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/02_medium_cluster/commands.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/02_medium_cluster/commands.md)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 示例命令：[https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/02_medium_cluster/commands.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/02_medium_cluster/commands.md)
- en: 'The **pros** are as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: Full HA – you can lose any one of the master nodes in the cluster and still
    have complete cluster management.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全高可用性 – 即使集群中的任何一个主节点发生故障，仍然可以保持完整的集群管理。
- en: User workloads and management services run on different nodes, stopping runaway
    applications from taking down the cluster.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户工作负载和管理服务运行在不同的节点上，防止失控的应用程序使集群崩溃。
- en: Due to the scalable limitations of etcd, having more than five etcd nodes causes
    a decrease in performance. So it is normally recommended to scale the design vertically
    instead of horizontally.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 etcd 的可扩展性限制，超过五个 etcd 节点会导致性能下降。因此，通常建议垂直扩展设计，而不是水平扩展。
- en: More than one worker node can fail without loss of service, assuming you have
    enough CPU and memory available on the remaining workers.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使多个工作节点发生故障，只要剩余工作节点上有足够的 CPU 和内存可用，服务也不会中断。
- en: No required downtime during patching and upgrades. Please see Rancher's zero
    downtime documentation for more details at [https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/](https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/).
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在修补和升级期间无需停机。请参见 Rancher 的零停机文档，了解更多详情：[https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/](https://rancher.com/docs/rke/latest/en/upgrades/maintaining-availability/)。
- en: 'The **cons** are as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: An external load balancer or a round-robin DNS record is needed for external
    application access and RKE2 management.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要外部负载均衡器或轮询 DNS 记录来访问外部应用程序和 RKE2 管理。
- en: Only `N+1` of availability, so during maintenance tasks, you cannot suffer a
    failure of a node without loss of service at the management plane (etcd and control
    plane).
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅具有 `N+1` 的可用性，因此在进行维护任务时，无法在不损失管理平面（etcd 和控制平面）服务的情况下承受节点故障。
- en: Additional complexity when creating nodes as you might need to size management
    nodes differently than worker nodes.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建节点时的额外复杂性，因为您可能需要根据管理节点与工作节点的不同需求进行不同的节点规模配置。
- en: 'The following are the **hardware requirements**:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是**硬件要求**：
- en: 'Master node:'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点：
- en: 'Servers(s): 3 physical/virtual servers'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器：3 台物理/虚拟服务器
- en: 'CPU: 4 cores per server'
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：每台服务器 4 核
- en: 'Memory: 8 GB'
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：8 GB
- en: Note
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注
- en: Worker node sizing should be based on your workload and its requirements.
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工作节点的大小应根据工作负载及其要求进行调整。
- en: Large clusters
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型集群
- en: For a larger cluster with RKE2, you are limited in your design because in an
    RKE2 cluster, etcd and control plane services are tied together and cannot be
    separated into different planes. The only real change that can be made is to increase
    the master node count from 3 to 5 then start increasing the size of the node.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较大的 RKE2 集群，设计上有一定的限制，因为在 RKE2 集群中，etcd 和控制平面服务是绑定在一起的，无法分离到不同的平面。唯一可以做的真正变化是将主节点数量从
    3 增加到 5，然后开始增加节点的规模。
- en: Install steps (RKE)
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装步骤（RKE）
- en: Once you have created `cluster.yaml`, you have RKE create a cluster for you.
    This is done by running the `rke up --config cluster.yaml` command. RKE will look
    for the `cluster.rkestate file`. If it cannot find that file, RKE will assume
    that you are creating a new cluster, which causes RKE to create a new root CA
    certificate called `/etc/kubernetes/ssl directory`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你创建了`cluster.yaml`文件，你可以通过让RKE为你创建一个集群。方法是运行`rke up --config cluster.yaml`命令。RKE会寻找`cluster.rkestate`文件。如果找不到该文件，RKE将假定你正在创建一个新的集群，这会导致RKE创建一个新的根CA证书，存放在`/etc/kubernetes/ssl`目录下。
- en: RKE will then check if any etcd nodes are being added or removed from the cluster.
    Suppose RKE detects that the downtime settings for etcd are currently violated.
    By default, RKE only allows one etcd node to be down. RKE then handles the process
    of removing etcd nodes by stopping the etcd container and removing the etcd member
    from the etcd leader. RKE will then take care of adding any new etcd nodes to
    the cluster. It is important to note that this process is designed to be slow
    and safe – RKE will only do one etcd node at a time and take an etcd snapshot
    before changing each node.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，RKE会检查集群中是否有etcd节点正在添加或移除。如果RKE检测到etcd的停机设置当前被违反，默认情况下，RKE只允许一个etcd节点处于离线状态。RKE将处理移除etcd节点的过程，包括停止etcd容器并将etcd成员从etcd领导者中移除。接下来，RKE将处理将任何新的etcd节点添加到集群的过程。需要注意的是，这个过程被设计得既缓慢又安全——RKE一次只处理一个etcd节点，并在更改每个节点之前进行etcd快照。
- en: Once the etcd plane has been completed successfully, RKE will take care of starting
    the controlplane. This process includes kube-apiserver, kube-controller-manager,
    and kube-scheduler. RKE will start each component on each control plane node one
    at a time. RKE will test that its health check endpoint is available for each
    component, which for most components is `/healthz`. It is vital to note RKE follows
    the same process as the etcd plane of verifying the max unavailable settings are
    not currently violated. Suppose the settings become violated during this process.
    RKE will stop and fail with an error.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦etcd平面成功完成，RKE将负责启动控制平面。此过程包括kube-apiserver、kube-controller-manager和kube-scheduler。RKE将依次在每个控制平面节点上启动各个组件。RKE将检查每个组件的健康检查端点是否可用，对于大多数组件来说，端点是`/healthz`。值得注意的是，RKE遵循与etcd平面相同的过程，验证最大不可用设置当前没有被违反。如果在此过程中设置被违反，RKE将停止并报告错误。
- en: Next, RKE will handle creating the worker plane. This process is different than
    the etcd and control plane because it's designed to be done in parallel. This
    is mainly done for larger clusters where you might have hundreds of worker nodes
    in the cluster. So, by default, RKE will process 10% of the worker nodes at once.
    For the existing node, RKE will cordon the node to prevent changes to the node.
    It is important to note that the application pods continue to run during this
    process, with the only impact being that the CNI provider might need to be restarted.
    The effect is like unplugging the NIC from the node for a few seconds before plugging
    it back in. This can affect applications that use long-lived connections that
    need to be held open. This is typically seen with applications that use database
    connection pooling, where the application will create several database connections
    then keep them open. Depending on the application, these connections might not
    reconnect automatically and may need to be restarted.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，RKE将处理创建工作平面。与etcd平面和控制平面不同，这一过程是并行进行的。这样做主要是为了处理较大的集群，在这些集群中，可能会有数百个工作节点。因此，默认情况下，RKE会一次处理10%的工作节点。对于现有节点，RKE会将该节点标记为不可用，以防止对节点进行更改。需要注意的是，在此过程中，应用程序的Pods会继续运行，唯一的影响是CNI提供程序可能需要重新启动。这个过程的影响类似于暂时拔掉节点的网卡几秒钟后再插回去。这可能会影响使用长期连接的应用程序，这些连接需要保持打开状态。通常，这些应用程序会使用数据库连接池，应用程序会创建多个数据库连接并保持它们处于打开状态。根据应用程序的不同，这些连接可能不会自动重新连接，可能需要重新启动。
- en: Install steps (RKE2)
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装步骤（RKE2）
- en: RKE2 handles the process of cluster creation very differently compared to RKE1\.
    With RKE2, the first master node in the cluster is unique because it handles bootstrapping
    the cluster. The bootstrap process creates a root CA certificate, and if the cluster
    token has not been set, RKE2 will handle creating one. Then RKE2 will initialize
    the etcd cluster. Finally, RKE2 will create the etcd encryption key based on the
    cluster token. RKE2 then stores the cluster state in a unique bootstrap key pair
    in etcd called `bootstrap`. This bootstrap data includes the Kubernetes certificates,
    private keys, and the etcd encryption keys. Once the cluster has been bootstrapped,
    the additional master nodes can join the cluster using the Kubernetes API endpoint
    to connect to the first node in the cluster. The RKE2 will use the cluster token
    to authenticate and decrypt the bootstrap data. Finally, once all the master nodes
    have been created, the same process is done for the worker nodes, with the only
    difference being the `INSTALL_RKE2_TYPE="agent"` install option, which tells RKE2
    to configure this node as a worker node.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 与 RKE1 不同，RKE2 在集群创建过程中采用了非常不同的处理方式。在 RKE2 中，集群中的第一个主节点是独一无二的，因为它负责集群的引导过程。引导过程会创建一个根
    CA 证书，如果集群令牌尚未设置，RKE2 会处理创建一个令牌。接着，RKE2 会初始化 etcd 集群。最后，RKE2 会基于集群令牌创建 etcd 加密密钥。RKE2
    然后会将集群状态存储在 etcd 中一个名为 `bootstrap` 的唯一引导密钥对中。这些引导数据包括 Kubernetes 证书、私钥以及 etcd
    加密密钥。一旦集群完成引导，其他主节点就可以使用 Kubernetes API 端点连接到集群中的第一个节点。RKE2 将使用集群令牌进行身份验证并解密引导数据。最后，一旦所有主节点都创建完成，工作节点的设置过程也会按照相同的步骤进行，唯一的区别是
    `INSTALL_RKE2_TYPE="agent"` 安装选项，这告诉 RKE2 将该节点配置为工作节点。
- en: The following are some example commands for creating a standard three master
    node cluster with worker nodes. More details about these commands can be found
    at [https://docs.rke2.io/install/ha/](https://docs.rke2.io/install/ha/).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例命令，用于创建一个标准的三主节点集群以及工作节点。关于这些命令的更多细节可以参考 [https://docs.rke2.io/install/ha/](https://docs.rke2.io/install/ha/)。
- en: '[PRE0]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This preceding code handles bootstrapping the first node in the cluster along
    with setting up the SAN certificate for the Kubernetes API endpoint and the node
    taint to prevent user workloads from running on this server.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这段前置代码处理了集群中第一个节点的引导工作，并设置了 Kubernetes API 端点的 SAN 证书以及节点污点，以防止用户的工作负载在该服务器上运行。
- en: '[PRE15]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We need this token in the preceding code block in order for the other nodes
    to join the cluster.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在前面的代码块中使用这个令牌，以便其他节点能够加入集群。
- en: '[PRE17]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This preceding code handles joining the additional master nodes to the existing
    cluster.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这段前置代码处理了将额外的主节点加入到现有集群中的过程。
- en: '[PRE34]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This preceding code then handles joining the worker nodes to the cluster we
    just built.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这段前置代码接着处理了将工作节点加入到我们刚刚创建的集群中的过程。
- en: 'Example commands: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/02_medium_cluster/commands.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/02_medium_cluster/commands.md)'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 示例命令：[https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/02_medium_cluster/commands.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/standard_designs/rke2/02_medium_cluster/commands.md)
- en: Now that we have successfully created the cluster, the next step will be to
    prepare an external load balancer to act as a frontend endpoint for the cluster.
    In the next section, we'll be configuring HAProxy in both HTTP and TCP mode. These
    settings are fairly standard, and you should be able to use them as a template
    for other load balancer technologies, for example, F5 or A10\.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功创建了集群，下一步将是准备一个外部负载均衡器，作为集群的前端端点。在接下来的部分中，我们将配置 HAProxy 的 HTTP 和 TCP
    模式。这些设置非常标准，你应该可以将它们作为其他负载均衡技术的模板，例如 F5 或 A10。
- en: Configuring an external load balancer (HAProxy)
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置外部负载均衡器（HAProxy）
- en: With RKE/RKE2 clusters, you will get an ingress-nginx-controller. This is a
    daemonset that, by default, runs on all worker nodes. And by default, nginx will
    listen on ports `80` and `443`. Nginx will then act as a layer 7 (HTTP/HTTPS mode)
    load balancer for applications hosted inside the Kubernetes cluster. This is great
    for load balancing applications inside the cluster, but the issue you run into
    is how you provide redundancy across nodes. The simplest way is to create a **DNS
    A** record with all the worker nodes' IP addresses in the cluster and just use
    round-robin DNS to load balance between the nodes and handle fault-tolerance.
    The downside is round-robin DNS can be very slow to update, and you must rely
    on the clients operating the failover. In the real world, this process can be
    very unreliable. To solve this issue, we're going to place an HAProxy server in
    front of the cluster. This process would be very similar for other load balancers,
    such as A10, F5, nginx, and so on. Next, we're going to cover two different ways
    for configuring HAProxy.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: TCP mode
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This mode is only responsible for transferring data at the transport protocol
    layer, and in this case, we're only looking at TCP/80 and TCP/443\. HAProxy is
    not terminating the connection, so things such as host-based routing and SSL are
    not available. Because of this, TCP mode is sometimes called *Layer 4* load balancing
    because it's just passing traffic. So, in this case, we will have a frontend **Virtual
    IP Address (VIP**) that does a one-to-one mapping for the TCP ports. It is important
    to note that by default, TCP mode doesn't have any session management enabled.
    It is normal to allow sticky sessions in TCP mode using source IP matching. This
    can be needed for applications that use server-based session management.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – HAProxy example design'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_04_010.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.10 – HAProxy example design
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'Diagram: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/haproxy/diagrams/tcp_mode.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/haproxy/diagrams/tcp_mode.md)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a sample configuration:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Example HAProxy config when in TCP mode'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_04_011.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11 – Example HAProxy config when in TCP mode
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'Full config: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/haproxy/config/tcp.cfg](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/haproxy/config/tcp.cfg)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this example config, I have the two additional endpoints exposed
    in HAProxy, the first being the Prometheus metrics endpoint, which allows a Prometheus
    server to scrape HAProxy for metrics. Please see [https://www.haproxy.com/blog/haproxy-exposes-a-prometheus-metrics-endpoint/](https://www.haproxy.com/blog/haproxy-exposes-a-prometheus-metrics-endpoint/)
    for more details.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: The second is the `stats` endpoint, enabling you to view the current status
    of the frontend and backend sections. This can be very helpful when troubleshooting
    an issue. Please see [https://www.haproxy.com/blog/exploring-the-haproxy-stats-page/](https://www.haproxy.com/blog/exploring-the-haproxy-stats-page/)
    for more details. It is important to note that these endpoints should be protected
    by a basic user login page and firewall rules.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: HTTP/HTTPS mode
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This mode is responsible for terminating the HTTP and SSL connection. Because
    of this, HAProxy can modify the request or make routing decisions. For example,
    `dev.example.com` can be routed to the dev RKE cluster, with `prod.example.com`
    being routed to the production RKE cluster even though they share the same VIP.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Example HAProxy config when in HTTP mode'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_04_012.jpg)'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.12 – Example HAProxy config when in HTTP mode
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'Diagram: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/haproxy/diagrams/http_mode.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch04/haproxy/diagrams/http_mode.md)'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: For environments where you don't want an external load balancer, MetalLB is
    an alternative option, and in the next section, we'll cover installing and configuring
    MetalLB in the simplest form, which is in Layer2 mode.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Configuring MetalLB
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MetalLB replaces the need for an external load balancer. It does this by announcing
    external IP addresses using a **VIP** or **Border Gateway Protocol (BGP**), then
    using port mapping to forward the traffic to the Kubernetes service. Each service
    exposed by MetalLB has its own IP address, which is pulled from a pool of IP addresses
    defined in the ConfigMap. MetalLB uses a daemonset called **speaker** to handle
    assigning the IP address on nodes, with the controller handling the orchestration.
    For more details about how this process works, please see the MetalLB documentation
    at [https://metallb.universe.tf/concepts/](https://metallb.universe.tf/concepts/).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps will install MetalLB''s controller and its speaker. More
    details about this process can be found at [https://metallb.universe.tf/installation/](https://metallb.universe.tf/installation/):'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Configuration
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, we'll define the IP address pool. More details about this process
    can be found at [https://metallb.universe.tf/configuration/](https://metallb.universe.tf/configuration/).
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a configmap with the following values:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Finally, to add a MetalLB IP to a cluster service, simply add the following
    annotation to the service definition. More details about this process can be found
    at [https://metallb.universe.tf/usage/](https://metallb.universe.tf/usage/):'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Summary
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about RKE, RKE2, and RancherD, including how each
    of these tools works. We then went over the requirements and limitations of each
    tool. We covered the rules of architecting RKE and RKE2 clusters, including some
    example configs and the pros and cons of each solution. We finally went into detail
    about the steps for creating clusters using the configs we made earlier. We then
    ended the chapter by covering how to install and configure HAProxy and MetalLB
    as a load balancer for both RKE and RKE2 clusters. After completing this chapter,
    you should be able to design a solution that meets your environment needs, then
    deploy the cluster types. Also, by understanding how each of the clusters operate,
    you should be able to troubleshoot most basic issues.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们学习了 RKE、RKE2 和 RancherD，包括这些工具的工作原理。然后，我们讨论了每个工具的要求和限制。我们还讲解了架构 RKE 和
    RKE2 集群的规则，包括一些示例配置以及每种解决方案的优缺点。最后，我们详细介绍了使用我们之前创建的配置文件创建集群的步骤。随后，我们结束本章内容，讲解了如何安装和配置
    HAProxy 和 MetalLB 作为 RKE 和 RKE2 集群的负载均衡器。完成本章后，你应该能够设计一个符合环境需求的解决方案，并部署集群类型。此外，通过了解每个集群的工作方式，你应该能够排除大多数基本问题。
- en: In the next chapter, we will cover how to deploy Rancher on a hosted cluster
    and some of the limitations and rules that need to be followed.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍如何在托管集群上部署 Rancher，以及需要遵循的一些限制和规则。
