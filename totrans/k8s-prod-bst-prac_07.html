<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer037">
			<h1 id="_idParaDest-130"><em class="italic"><a id="_idTextAnchor157"/>Chapter 7</em>: Managing Storage and Stateful Applications</h1>
			<p>In the previous chapters, we learned how to provision and prepare our Kubernetes clusters for production workloads. It is part of the critical production readiness requirement to configure and fine-tune day zero tasks, including networking, security, monitoring, logging, observability, and scaling, before we bring our applications and data to Kubernetes. Kubernetes was originally designed for mainly stateless applications in order to keep containers portable. Therefore, data management and running stateful applications are still among the top challenges in the cloud native space. There are a number of ways and a variety of solutions to address storage needs. New solutions emerge in the Kubernetes and cloud-native ecosystem every day; therefore, we will start with popular in-production solutions and also learn the approach and criteria to look for when evaluating future solutions.</p>
			<p>In this chapter, we will learn the technical challenges associated with stateful applications on Kubernetes. We will follow the cloud-native approach completely to fine-tune Kubernetes clusters for persistent storage. We will learn the different storage solutions and their shortcomings, and how to use and configure them with our Kubernetes cluster. </p>
			<p>In this chapter, we will cover the following main topics:</p>
			<ul>
				<li><a id="_idTextAnchor158"/>Understanding the challenges with stateful applications </li>
				<li>Tuning Kubernetes storage</li>
				<li>Choosing a persistent storage solution</li>
				<li>Deploying stateful applications </li>
			</ul>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor159"/>Technical requirements</h1>
			<p>You should have the following tools installed from the previous chapters:</p>
			<ul>
				<li>AWS CLI V2</li>
				<li>AWS IAM Authenticator</li>
				<li><strong class="source-inline">kubectl</strong></li>
			</ul>
			<p>We will also need to install the following tools:</p>
			<ul>
				<li>Helm</li>
				<li>CSI driver</li>
			</ul>
			<p>You need to have an up and running Kubernetes cluster as per the instructions in <a href="B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 3</em></a>, <em class="italic">Provisioning Kubernetes Clusters Using AWS and Terraform</em>.</p>
			<p>The code for this chapter is located at <a href="https://github.com/PacktPublishing/Kubernetes-Infrastructure-Best-Practices/tree/master/Chapter07">https://github.com/PacktPublishing/Kubernetes-Infrastructure-Best-Practices/tree/master/Chapter07</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/3jemcot">https://bit.ly/3jemcot</a></p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor160"/>Installing the required tools</h2>
			<p>In this section, we will install the tools that we will use to provision applications using Helm charts and provide dynamically provisioned volumes to the stateful applications in your Kubernetes infrastructure during this chapter and the upcoming ones. As a cloud and Kubernetes learner, you may be familiar with these tools from before.</p>
			<h3>Installing Helm </h3>
			<p>Helm<a id="_idIndexMarker446"/> is a package manager for Kubernetes. Helm is also a great way to find and deploy vendor and community published applications on Kubernetes. We will use Helm to deploy applications on our Kubernetes cluster. If you do not have Helm installed in your <a id="_idIndexMarker447"/>cluster, you can follow these instructions to do that.</p>
			<p>Execute the following commands to install Helm 3 in your Kubernetes cluster:</p>
			<p class="source-code">$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3</p>
			<p class="source-code">$ chmod 700 get_helm.sh</p>
			<p class="source-code">$ ./get_helm.sh</p>
			<p>Next, we will install the CSI drivers.</p>
			<h3>Installing CSI drivers</h3>
			<p><strong class="bold">Container Storage Interface</strong> (<strong class="bold">CSI</strong>) is the <a id="_idIndexMarker448"/>standardized APIs to extend Kubernetes with third-party <a id="_idIndexMarker449"/>storage provider solutions. CSI drivers are vendor specific and, of course, you only need an AWS EBS CSI driver if you are running on AWS infrastructure, including EC2 or EKS-based clusters. To install the latest<a id="_idIndexMarker450"/> AWS EBS CSI drivers, refer to the Amazon EKS official documentation at <a href="https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.htm">https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.htm</a>.</p>
			<p>If you are running on a self-managed Kubernetes solution, bare metal/on-premises, or virtualized environment, you may need to<a id="_idIndexMarker451"/> use another vendor's CSI driver or <strong class="bold">Container Attached Storage</strong> (<strong class="bold">CAS</strong>) solutions. To install other CSI vendor drivers, you can<a id="_idIndexMarker452"/> refer the links to specific driver instructions on the official CSI documentation at <a href="https://kubernetes-csi.github.io/docs/drivers.html">https://kubernetes-csi.github.io/docs/drivers.html</a>.</p>
			<p>Now that we have installed the prerequisites required in the chapter to deploy Helm Charts and consume AWS EBS volumes using the CSI driver, let's go ov<a id="_idTextAnchor161"/>er the implementation principles we will be following, making storage provider decisions with a view to solving our stateful application challenges. </p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor162"/>Implementation principles</h1>
			<p>In <a href="B16192_01_Final_PG_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to Kubernetes Infrastructure and Production-Readiness</em>, we learned about<a id="_idIndexMarker453"/> the infrastructure design principles that we will follow during the book. I would like to start this chapter by highlighting the notable principles that influenced the cloud-native data management suggestions and the technical decisions in this chapter:</p>
			<ul>
				<li><strong class="bold">Simplication</strong>: In this chapter, we will retain our commitment to the simplification principle. Unless you are operating in a multi-cloud environment, it is not necessary to introduce new tools and complicate operations. On public clouds, we will use the native storage data management technology stack provided, and which is supported by your managed service vendor. Many stateful applications today are designed to fail and provide built-in, high-availability functionality. We will identify different types of stateful applications and learn how to simply data paths and fine-tune for performance. We will also learn the additional design principles to achieve higher availability across availability zones, as well as unifying data management in on-premises and hybrid cloud environments.</li>
				<li><strong class="bold">Cloud agnostic</strong>: Data has gravity. When running stateless applications, cloud vendor lock-in may not be as important since container images can be brought up almost instantly on any infrastructure, but when dealing with stateful workloads, it is easy to get into this situation. We will use cloud-native solutions to abstract storage layers and eliminate dependencies. The solutions we will implement will work exactly the same way on any cloud provider, managed Kubernetes service, and even on a self-managed on-premise environment. </li>
				<li><strong class="bold">Design for availability</strong>: CSI is great, but, at the same time, it is nothing more than standardized APIs. Your data still needs to be stored on a highly available media somewhere. It <a id="_idIndexMarker454"/>is important to consider the blast radius of your storage solution. It doesn't make sense to store your loosely coupled applications in a single scale-out storage solution, or on a legacy storage appliance. Doing so would create scale bottlenecks and will slow you down along the way. We will learn the benefits of cloud-native storage solutions. We will also learn how to use snapshots, clones, and backups for increased service availability and quick service recovery. </li>
				<li><strong class="bold">Automation</strong>: You can't automate your CI/CD pipelines unless everything can be dynamically provisioned. We will learn about Kubernetes storage primitives and the use of dynamic provisioners. </li>
			</ul>
			<p>In this section, we have covered the implementation principles we will be following when making storage provider decisions. Let's now take a look at some of the common stateful application challenges we will need to address.</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor163"/>Understanding the challenges with stateful applications </h1>
			<p>Kubernetes was initially built <a id="_idIndexMarker455"/>for stateless applications in order to keep containers portable. Even when we run stateful applications, the applications themselves are actually very often stateless containers where the state is stored separately and mounted from a resource called <strong class="bold">Persistent Volume</strong> (<strong class="bold">PV</strong>). We will<a id="_idIndexMarker456"/> learn the different resource types used to maintain state and also keep some form of flexibility later in the <em class="italic">Understanding storage primitives in Kubernetes</em> section.</p>
			<p>I would like to highlight the six notable stateful application challenges that we will try to address in thi<a id="_idTextAnchor164"/>s chapter:</p>
			<ul>
				<li><strong class="bold">Deployment challenges</strong>: Especially<a id="_idIndexMarker457"/> when running a mission-critical service in production, finding the ideal deployment method of a certain stateful application can be challenging to start with. Should we use a YAML file we found in a blog article, open source repository examples, Helm charts, or an operator? Your choice will have an impact on future scalability, manageability, upgrades, and service recoverability. We will learn the best practices to follow for deploying a stateful application later in this chapter in the <em class="italic">Deploying stateful applications</em> section.</li>
				<li><strong class="bold">Persistency challenges</strong>: Storing <a id="_idIndexMarker458"/>the actual persistent data that makes the application stateful needs to be carefully picked. You should never store the state inside the application container itself since the container images and pods can be restarted and updated, which would result in losing the data. Similarly, if you are running your cluster across multiple availability zones on top of EBS volumes when a node is restarted, your application may come up in a node located on a separate availability zone with no access to previous EBS volumes. In that case, you should consider a<a id="_idIndexMarker459"/> container-attached storage solution with across <strong class="bold">availability zone</strong> (<strong class="bold">AZ</strong>) replication functionality.<p>On the other hand, if your application is a distributed database with built-in high availability, adding an additional layer of high availability from a storage provider would have a negative impact on capacity, cost, and performance. Persistency decisions need to carefully consider an application's requirements. </p></li>
				<li><strong class="bold">Scalability challenges</strong>: One of the<a id="_idIndexMarker460"/> main reasons behind the popularity of the Kubernetes orchestration platform is the flexibility of scaling up services. Kubernetes platforms allow you to start on a single worker node and dynamically scale up to thousands of nodes according to demand and increasing loads. Not every storage solution is designed for scale. We will learn the best practices to follow and the differences between the storage options to consider when deploying a scalable stateful application later in this chapter in the <em class="italic">Choosing a persistent storage solution</em> section.</li>
				<li><strong class="bold">Mobility challenges</strong>: Data<a id="_idIndexMarker461"/> mobility means being able to get data where and when you need it. Especially in an infrastructure where hybrid or multi-cloud are requirements, your choice of storage provider becomes a key factor. This requirement is also aligned with the cloud-agnostic design principles that we introduced in <a href="B16192_01_Final_PG_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to Kubernetes Infrastructure and Production-Readiness</em>. If needed, your stateful applications should be able to migrate to different zones and even different storage and cloud vendors.</li>
				<li><strong class="bold">Life cycle manageability challenges</strong>: The real challenge starts after you deploy your stateful applications. Day<a id="_idIndexMarker462"/> two operations need to be planned in advance before you go to production with your services. This sometimes creates a dependency and requirement on your deployment method as well. You need to pick the deployment method that will support rollover upgrades, monitoring, observability, and troubleshooting.</li>
				<li><strong class="bold">Disaster recovery (DR) and backup challenges</strong>: You need to plan for service availability in case of application and or infrastructure failures. Your data needs to be backed up on a regular basis. Some applications may require application-consistent backups, and some might be good with <a id="_idIndexMarker463"/>just crash-consistent backups. CSI-operated snapshots and copying that data to object storage needs to be scheduled. Taking a backup is one side of the problem, but being able to recover from your backup in a timely fashion is another challenge. When there is a service outage, end user service impact is measured using mainly two data points; the <strong class="bold">Recovery Time Objective</strong> (<strong class="bold">RTO</strong>) and the <strong class="bold">Recovery Point Objective</strong> (<strong class="bold">RPO</strong>). RTO measures the time required to<a id="_idIndexMarker464"/> bring a service back, while RPO measures the <a id="_idIndexMarker465"/>backup frequency. Data created by your application may grow quickly when you go to production with your services. Recovering a large amount of data from S3-like object storage will take time. In that case, stream backup solutions need to be considered. This requirement is also aligned with the <em class="italic">design for availability</em> design principles that I introduced in <a href="B16192_01_Final_PG_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to Kubernetes Infrastructure and Production-Readiness</em>. If needed, your application needs to be able to switch to its DR copy as quickly as possible with minimal downtime.</li>
			</ul>
			<p>These six core challenges contribute to the architectural design decisions we need to make in order to run stateful applications in production. We will consider these challenges later in this chapter when we evaluate our storage options and make a relevant technical decision based on it.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor165"/>Tuning Kubernetes storage </h1>
			<p>At some point, we<a id="_idIndexMarker466"/> have all experienced and been frustrated by storage performance and the technical limitations of it. In this chapter, we will learn the fundamentals of Kubernetes storage, including storage primitives, creating static <strong class="bold">persistent volumes</strong> (<strong class="bold">PVs</strong>), and using storage classes to provision dynamic PVs to simplify management.</p>
			<p>Understanding containerized stateful applications requires us to get into the cloud-native mindset. Although referred to as stateful, data used by pods is either accessed remotely or orchestrated and stored in Kubernetes as separate resources. Therefore, some flexibility is maintained to schedule applications across worker nodes and update when needed without losing the data. Before we get into the tuning, let's understand some of the basic storage<a id="_idIndexMarker467"/> primitives in Kubernetes.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor166"/>Understanding storage primitives in Kubernetes</h2>
			<p>The beauty of <a id="_idIndexMarker468"/>Kubernetes is that every part of it is abstracted as an object that can be managed and configured declaratively with YAML or JSON through the <strong class="source-inline">kube-api</strong> server. This makes Kubernetes configuration easier to manage as code. Storage is also handled as abstracted objects. To be able to understand the reasoning behind the best practices, I highly recommend that you learn the separation of the storage object. In this section, we will learn the following core storage primitives to request <a id="_idIndexMarker469"/>persistent storage from Kubernetes and orchestrate the provisioning through storage providers associated with it: </p>
			<ul>
				<li>Volume</li>
				<li><strong class="bold">Persistent Volume</strong> (<strong class="bold">PV</strong>)</li>
				<li><strong class="bold">Persistent Volume Claim</strong> (<strong class="bold">PVC</strong>)</li>
				<li><strong class="bold">Storage Class</strong> (<strong class="bold">SC</strong>)</li>
			</ul>
			<p>Let's discuss each of these in the following sections.</p>
			<h3>Volumes</h3>
			<p>Kubernetes<a id="_idIndexMarker470"/> volumes are basically just a directory accessible to the <a id="_idIndexMarker471"/>applications running in containers in a pod. How this directory is created and protected, and where it is stored really depends on the type of volume used, which makes this a critical decision when running stateful applications in production. Kubernetes supports many types of volumes. For a detailed list of support volume types, refer to the<a id="_idIndexMarker472"/> official Kubernetes documentation at <a href="https://kubernetes.io/docs/concepts/storage/volumes/">https://kubernetes.io/docs/concepts/storage/volumes/</a>. Some of the volume types are ephemeral, in other words, their lifespan is limited to its pod. Therefore, they should only be used for stateless applications where the persistency of data is not necessary across restarts. In the context of stateful applications, our focus is PV types, including remote PVs and local PVs. Let's <a id="_idIndexMarker473"/>now learn about the use of PV <a id="_idIndexMarker474"/>objects. </p>
			<h3>PVs</h3>
			<p>PVs are volumes <a id="_idIndexMarker475"/>that can retain the data during<a id="_idIndexMarker476"/> pod restarts or other resource failures. PVs can be created either statically in advance or dynamically when requested by the user application. I will explain the use of static or dynamic PV objects with a practical example while we deploy a Percona server. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter07/stateful/percona/pv-percona.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter07/stateful/percona/pv-percona.yaml</a>.</p>
			<p>Let's start with a<a id="_idIndexMarker477"/> static volume to understand its limitations, in other words, the value and logic behind the dynamic provisioning:</p>
			<ol>
				<li>Create an AWS Elastic Block Store volume with a size of 100 GB using the volume type <strong class="source-inline">gp2</strong>. Make sure that the EBS volume is in the same availability zone as your Kubernetes worker nodes:<p class="source-code"><strong class="bold">$ aws ec2 create-volume --size=10 --availability-zone=us-east-1a --volume-type=gp2</strong></p></li>
				<li>Repeat the previous step to create one volume per worker node available in your cluster. If you have three nodes available, then create a total of three volumes. Execute the following command to get the list of <strong class="source-inline">InstanceId</strong> strings for the nodes:<p class="source-code">$ aws ec2 describe-instances | grep InstanceId</p></li>
				<li>Execute the following command to attach each volume you have created to one worker node in your cluster at a time using the AWS CLI. Replace <strong class="source-inline">WORKER_NODE_ID</strong> and <strong class="source-inline">VOLUME_ID</strong> from the output of step 1:<p class="source-code"><strong class="bold">$ aws ec2 attach-volume --device /dev/sdf --instance-id &lt;WORKER_NODE_ID&gt; --volume-id &lt;YOUR_VOLUME_ID&gt;</strong></p></li>
				<li>Create a <a id="_idIndexMarker478"/>Kubernetes PV named <strong class="source-inline">percona-pv1</strong> with a size of <strong class="source-inline">5Gi</strong> in the following path – <strong class="source-inline">stateful/percona/pv-percona.yaml</strong>. Make sure to replace <strong class="source-inline">volumeID</strong> with a valid<a id="_idIndexMarker479"/> volume ID of your EBS volume:<p class="source-code">apiVersion: v1</p><p class="source-code">kind: PersistentVolume</p><p class="source-code">metadata:</p><p class="source-code">  name : percona-pv1</p><p class="source-code">spec:</p><p class="source-code">  accessModes:</p><p class="source-code">  - ReadWriteOnce</p><p class="source-code">  capacity:</p><p class="source-code">    storage: 5Gi</p><p class="source-code">  persistentVolumeReclaimPolicy: Retain</p><p class="source-code">  awsElasticBlockStore:</p><p class="source-code">    volumeID: &lt;YOUR EBS VOLUME ID HERE&gt;</p><p class="source-code">    fsType: xfs</p></li>
				<li>Execute the following <strong class="source-inline">kubectl</strong> command to create a static PV in the cluster:<p class="source-code"><strong class="bold">$ kubectl apply -f pv-percona.yaml</strong></p></li>
			</ol>
			<p>Now you have <a id="_idIndexMarker480"/>created a PV that can bind to your stateful application. As you can see, if you have a dynamically scaling environment, creating volumes manually in advance will not provide a scalable option.</p>
			<h3>PV claims</h3>
			<p>A <strong class="bold">PV claim</strong> (<strong class="bold">PVC</strong>) is a request <a id="_idIndexMarker481"/>for storage. PVC <a id="_idIndexMarker482"/>requests can be fulfilled either by static or dynamic PVs. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter07/stateful/percona/pvc-percona.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter07/stateful/percona/pvc-percona.yaml</a>.</p>
			<p>Here, we will create a PVC manifest to request the static PV we created earlier:</p>
			<ol>
				<li value="1">Create a PVC named <strong class="source-inline">percona-pv1</strong> with a size of <strong class="source-inline">5Gi</strong> in the following path – <strong class="source-inline">stateful/percona/pvc-percona.yaml</strong>:<p class="source-code">kind: PersistentVolumeClaim</p><p class="source-code">apiVersion: v1</p><p class="source-code">metadata:</p><p class="source-code">  name: percona-pvc</p><p class="source-code">spec:</p><p class="source-code">  accessModes:</p><p class="source-code">    - ReadWriteOnce</p><p class="source-code">  resources:</p><p class="source-code">    requests:</p><p class="source-code">      storage: 5Gi</p></li>
				<li>In the following part of the template, we will set <strong class="source-inline">storageClassName</strong> to blank. Otherwise, the default storage class will be used and a PV is created dynamically using the default storage provisioner. This time, we are specifically requesting a PV with no storage class specified, so it can only be bound to our existing PV:<p class="source-code">  storageClassName: ""</p></li>
				<li>Execute the<a id="_idIndexMarker483"/> following <strong class="source-inline">kubectl</strong> command to create a PVC object in the cluster:<p class="source-code"><strong class="bold">$ kubectl apply -f pv-percona.yaml</strong></p><p class="callout-heading">Important note</p><p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter07/stateful/percona/deployment-percona.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter07/stateful/percona/deployment-percona.yaml</a>.</p></li>
			</ol>
			<p>In the following code snippet, you create the <strong class="source-inline">percona</strong> deployment that will use the PVC to request the PV we created earlier:</p>
			<ol>
				<li value="1">Create a Kubernetes <a id="_idIndexMarker484"/>secret to keep the Percona root password by executing the following command. This will be used in the deployment later. You can read more about the detailed usage of Kubernetes secrets at <a href="https://kubernetes.io/docs/concepts/configuration/secret/">https://kubernetes.io/docs/concepts/configuration/secret/</a>:<p class="source-code">$ kubectl create secret generic mysql-root \</p><p class="source-code">     --from-literal=mysql-root-passwd=MyP@ssW0rcl \</p><p class="source-code">     --dry-run -o yaml | kubectl apply -f -</p></li>
				<li>Create the template for the <strong class="source-inline">percona</strong> deployment in the following path – <strong class="source-inline">stateful/percona/deployment-percona.yaml</strong>:<p class="source-code">---</p><p class="source-code">apiVersion: apps/v1</p><p class="source-code">kind: Deployment</p><p class="source-code">metadata:</p><p class="source-code">  name: percona</p><p class="source-code">spec:</p><p class="source-code">  selector:</p><p class="source-code">    matchLabels:</p><p class="source-code">      app: percona</p><p class="source-code">  template:</p><p class="source-code">    metadata:</p><p class="source-code">      labels:</p><p class="source-code">        app: percona</p><p class="source-code">    spec:</p><p class="source-code">      containers:</p><p class="source-code">      - image: percona</p><p class="source-code">        name: percona</p><p class="source-code">        env:</p><p class="source-code">        - name: MYSQL_ROOT_PASSWORD</p><p class="source-code">          valueFrom:</p><p class="source-code">            secretKeyRef:</p><p class="source-code">              name: mysql-root</p><p class="source-code">              key: mysql-root-passwd</p><p class="source-code">        ports:</p><p class="source-code">        - conta<a id="_idTextAnchor167"/>inerPort: 3306</p><p class="source-code">          name: percona</p></li>
				<li>In the following part of the template, we will define the <strong class="source-inline">volumeMounts</strong> using the name <strong class="source-inline">percona-volume</strong>, with the <strong class="source-inline">mountPath</strong> parameter configured as the path <strong class="source-inline">/var/lib/mysql</strong>, where your<a id="_idIndexMarker485"/> PV will be mounted inside the container:<p class="source-code">        volumeMounts:</p><p class="source-code">        - name: percona-volume</p><p class="source-code">          mountPath: /var/lib/mysql</p></li>
				<li>Finally, in the <a id="_idIndexMarker486"/>following part of the template, we will define where your request will be directed. In our case, as defined before in the case of <strong class="source-inline">claimName</strong>, this should be <strong class="source-inline">percona-pvc</strong>:<p class="source-code">     volumes:</p><p class="source-code">        - name: percona-volume</p><p class="source-code">          persistentVolumeClaim:</p><p class="source-code">            claimName: percona-pvc</p></li>
				<li>Execute the following <strong class="source-inline">kubectl</strong> command to create <strong class="source-inline">percona</strong> deployment in the cluster:<p class="source-code"><strong class="bold">$ kubectl apply -f deployment-percona.yaml</strong></p></li>
			</ol>
			<p>Now you have created a stateful application deployment with a binding to a static PV. Although it can be useful to know how to clone an existing volume and mount it to a new pod, this is not a scalable solution. Therefore, we will now learn about the dynamic provisioning of PVs using <strong class="source-inline">StorageClass</strong>. </p>
			<h3>Storage class</h3>
			<p>The <strong class="source-inline">StorageClass</strong> object allows <a id="_idIndexMarker487"/>dynamic provisioning requests <a id="_idIndexMarker488"/>through a PVC. You can maintain multiple classes that map to different availability and QoS levels using internal or external third-party provisioners. The <strong class="source-inline">StorageClass</strong> concept is similar to tiers or profiles in traditional storage solutions. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter07/stateful/percona/deployment-percona.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter07/stateful/percona/deployment-percona.yaml</a>.</p>
			<p>Let's review a <strong class="source-inline">StorageClass</strong> template <a id="_idIndexMarker489"/>used for provisioning EBS volumes on AWS:</p>
			<p class="source-code">apiVersion: storage.k8s.io/<a id="_idTextAnchor168"/>v1</p>
			<p class="source-code">kind: StorageClass</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: gp2</p>
			<p>In the following part <a id="_idIndexMarker490"/>of the template, we set <strong class="source-inline">StorageClass</strong> as the default storage class. It is highly recommended good practice to set a default storage class, so PVCs missing the <strong class="source-inline">storageClassName</strong> field are automatically assigned to your default class:</p>
			<p class="source-code">  annotations:</p>
			<p class="source-code">    storage<a id="_idTextAnchor169"/>class.kubernetes.io/is-default-class: "true"</p>
			<p>In the following part of the template, we set the EBS volume type to <strong class="source-inline">gp2</strong>, with AWS EBS volumes of <strong class="source-inline">io1</strong>, <strong class="source-inline">gp2</strong>, <strong class="source-inline">sc1</strong>, or <strong class="source-inline">st1</strong>. You can read about the differences in the official AWS documentation at <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a>. We also set <strong class="source-inline">fsType</strong> to <strong class="source-inline">ext4</strong>:</p>
			<p class="source-code">parameters:</p>
			<p class="source-code">  type: gp2</p>
			<p class="source-code">  fsType: ext4</p>
			<p>In the following part of the template, we set the <strong class="source-inline">provisioner</strong> type to <strong class="source-inline">kubernetes.io/aws-ebs</strong>. This field can be internal or an external provisioner. In our following template, it is set to Kubernetes' internal <strong class="source-inline">aws-ebs</strong> provisioner, <strong class="source-inline">kubernetes.io/aws-ebs</strong>. We will review the available storage options later in this chapter in the <em class="italic">Choosing a persistent storage solution</em> section:</p>
			<p class="source-code">provisioner: kubernetes.io/aws-ebs</p>
			<p class="source-code">reclaimPolicy: Retain</p>
			<p class="source-code">allowVolumeExpansion: true</p>
			<p class="source-code">volumeBindingMode: Immediate</p>
			<p><strong class="source-inline">reclaimPolicy</strong> can be set to <strong class="source-inline">Delete</strong>, <strong class="source-inline">Recycle</strong>, or <strong class="source-inline">Retain</strong> and it defines the action when a corresponding PVC is deleted. When <strong class="source-inline">Retain</strong> is selected, after the PVC is removed, the PV is moved to the <strong class="source-inline">Released</strong> state. Hence, <strong class="source-inline">Retain</strong> is the suggested option to avoid accidents.</p>
			<p>The <strong class="source-inline">allowVolumeExpansion</strong> field is used if you need to request a larger size PVC later and you want the same volume<a id="_idIndexMarker491"/> to be resized instead of getting a new volume. You can only expand a PVC if its <a id="_idIndexMarker492"/>storage class has the <strong class="source-inline">allowVolumeExpansion</strong> parameter set to <strong class="source-inline">true</strong>. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">AWS EBS volume expansions can take time and one modification is allowed every 6 hours.</p>
			<p><strong class="source-inline">volumeBindingMode</strong> can be set to <strong class="source-inline">Immediate</strong> or <strong class="source-inline">WaitForFirstConsumer</strong>. This parameter st<a id="_idTextAnchor170"/>ipulates when the volume binding should occur.</p>
			<p>To learn about the remainder of the <strong class="source-inline">StorageClass</strong> parameters, please check the official Kubernetes documentation here: <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">https://kubernetes.io/docs/concepts/storage/storage-classes/</a>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter07/stateful/percona/deployment-percona-sc.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter07/stateful/percona/deployment-percona-sc.yaml</a>.</p>
			<p>Now, we will modify the <strong class="source-inline">pvc-percona.yaml</strong> and <strong class="source-inline">deployment-percona.yaml</strong> manifest files. We will adjust the <strong class="source-inline">percona</strong> deployment to use a storage class to dynamically request a PV through a PVC:</p>
			<ol>
				<li value="1">Edit the template for the <strong class="source-inline">percona-pvc</strong> PVC in this path, <strong class="source-inline">stateful/percona/pvc-percona.yaml</strong>, using your preferred text editor. Adjust the name and <strong class="source-inline">storageClassName</strong> fields as follows:<p class="source-code">kind: PersistentVolumeClaim</p><p class="source-code">apiVersion: v1</p><p class="source-code">metadata:</p><p class="source-code">  name: percona-pvc-gp2</p><p class="source-code">spec:</p><p class="source-code">  accessModes:</p><p class="source-code">    - ReadWriteOnce</p><p class="source-code">  resources:</p><p class="source-code">    requests:</p><p class="source-code">      storage: 5Gi</p><p class="source-code">  storageClassName: gp2</p></li>
				<li>Edit the template for the <strong class="source-inline">percona</strong> deployment in this path, <strong class="source-inline">stateful/percona/deployment-percona.yaml</strong>, using your preferred<a id="_idIndexMarker493"/> text editor. Adjust the last <a id="_idIndexMarker494"/>line, <strong class="source-inline">claimName</strong>, as follows:<p class="source-code">            claimName: percona-pvc-gp2</p></li>
				<li>Execute the following <strong class="source-inline">kubectl</strong> commands to create a <strong class="source-inline">percona</strong> deployment in the cluster using a dynamically provisioned PV:<p class="source-code"><strong class="bold">$ kubectl apply -f pv-percona.yaml</strong></p><p class="source-code"><strong class="bold">$ kubectl apply -f deployment-percona.yaml</strong></p></li>
			</ol>
			<p>Now you have created a stateful application deployment with a binding to a dynamically provisioned PV using <strong class="source-inline">StorageClass</strong>. This step completely eliminated the need for manual EBS volume creation. Therefore, we will use this method later in this chapter when creating new stateful applications. </p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor171"/>Choosing a persistent storage solution</h1>
			<p>Two of the biggest stateful<a id="_idIndexMarker495"/> application challenges in Kubernetes are storage orchestration and data management. There are an infinite number of solutions out there. First, we will explain the main storage attributes and topologies we need to consider when evaluating storage alternatives. Let's review the topologies used by the most common storage systems:</p>
			<ul>
				<li><strong class="bold">Centralized</strong>: Traditional, or<a id="_idIndexMarker496"/> also referred to as monolithic, storage systems are most often tightly coupled with a proprietary hardware and internal communication protocols. They are usually associated with scale-up models since it is difficult to scale-out tightly coupled components of the storage nodes. </li>
				<li><strong class="bold">Distributed</strong>: Distributed storage systems<a id="_idIndexMarker497"/> are more likely to be a software-defined solution and they may be architected to favor availability, consistency, durability, performance, or scalability. Usually, distributed systems scale out better than others to support many storage server nodes in parallel.</li>
				<li><strong class="bold">Hyperconverged</strong>: Hyperconverged storage solutions<a id="_idIndexMarker498"/> are designed to take advantage of the same network and compute resources where the applications run. They are largely designed to run as software and are orchestrated by the same platform used to manage applications, VMs, or containers, such as a hypervisor or container orchestrators. </li>
				<li><strong class="bold">Sharded</strong>: Sharded storage solutions<a id="_idIndexMarker499"/> partition the data into datasets and store them across multiple nodes. Sharded storage solutions can be complex to manage and rebalance and performance is limited to the performance of a single node where the dataset is located. </li>
			</ul>
			<p>The category of<a id="_idIndexMarker500"/> storage solutions available for the cloud-native application is known as <a id="_idIndexMarker501"/>cloud-native storage by the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>). Currently, there are 17 open source and 32 proprietary solutions, hence a total of 49 solutions, listed in the category. </p>
			<p>For the most up-to-date list of solutions, you can refer to the official CNCF cloud-native interactive landscape documentation<a id="_idIndexMarker502"/> at <a href="https://landscape.cncf.io/">https://landscape.cncf.io/</a>:</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="Images/B16192_07_001.jpg" alt="Figure 7.1 – CNCF cloud-native landscape with cloud-native storage providers&#13;&#10;" width="1650" height="459"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – CNCF cloud-native landscape with cloud-native storage providers</p>
			<p>When the challenges mentioned in the <em class="italic">Understanding the challenges with stateful applications</em> section are considered for the simplicity of the deployment and life cycle management of block storage, <strong class="bold">Container Attached Storage</strong> (<strong class="bold">CAS</strong>) and <strong class="bold">Cloud Storage</strong> are preferred<a id="_idIndexMarker503"/> over the <a id="_idIndexMarker504"/>centralized topology. To satisfy persistence across different infrastructure and data mobility requirements, <strong class="bold">CAS</strong> and <strong class="bold">Distributed</strong> solutions should be preferred over the solutions on the right. When we talk about Kubernetes-grade scalability, again <strong class="bold">Cloud Storage</strong> and <strong class="bold">CAS</strong> solutions offer significant advantages over the centralized topology. Overall, <strong class="bold">CAS</strong> and <strong class="bold">Cloud Storage</strong> providers satisfy all the architectural concerns. That said, on many occasions, we will have to utilize your company's existing investment. Cloud storage is only available on the cloud vendor provided infrastructure, and if you are running on-premises/private clouds, you may need to utilize your existing hardware solutions. In that case, you can still leverage <strong class="bold">CAS</strong> solutions to unify data management, add the advantages of cloud-native storage, including data mobility<a id="_idIndexMarker505"/> and scalability, and simplify the life cycle management of PVs on top of your investment.</p>
			<p>Now that you have learned the storage topologies used by the most common storage solutions, let's focus on how we can use a CAS solution to deploy a stateful application.</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor172"/>Deploying stateful applications </h1>
			<p>Kubernetes provides a <a id="_idIndexMarker506"/>number of controller APIs to manage the deployment of pods within a Kubernetes cluster. Initially designed for stateless applications, these controllers are used to group pods based on need. In this section, we will briefly learn the differences between the following Kubernetes objects – pods, ReplicaSets, deployments, and StatefulSets. In the event of a node failure, individual Pods will not be rescheduled on other nodes. Therefore, they should be avoided when running stateful workloads.</p>
			<p><strong class="bold">Deployments</strong> are <a id="_idIndexMarker507"/>used when managing pods, and <strong class="bold">ReplicaSets</strong> when <a id="_idIndexMarker508"/>we need to roll out changes to replica Pods. Both ReplicaSets and Deployments are used when provisioning stateless applications. To learn about Deployments, please check the official Kubernetes documentation here: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a>.</p>
			<p><strong class="bold">StatefulSets</strong> are <a id="_idIndexMarker509"/>another controller that reached a <strong class="bold">General Availability</strong> (<strong class="bold">GA</strong>) milestone<a id="_idIndexMarker510"/> with the release of Kubernetes 1.9. The real adoption of stateful applications started following the introduction of the StatefulSets object. With StatefulSets, every pod replica has its own state, in other words, its own volume, and therefore retains its state and identity across restarts. When deploying stateful applications, and when we need storage to be stateful, we will use StatefulSets. The following diagram shows the components of an application deployed using StatefulSets:</p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="Images/B16192_07_002.jpg" alt="Figure 7.2 – Kubernetes StatefulSet deployment diagram&#13;&#10;" width="1053" height="628"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – Kubernetes StatefulSet deployment diagram</p>
			<p>StatefulSets require a headless service for handling the network identity of the related pods. When a StatefulSet requests volumes to be created, it uses the StorageClass to call the PV provisioner. Earlier<a id="_idIndexMarker511"/> in this chapter, you learned to use StorageClass to dynamically provision PVs. </p>
			<p>Before we deploy a stateful application, we will learn how to install one of the popular open source storage provisioner options, OpenEBS, which we mentioned in the <em class="italic">Choosing a persistent storage solution</em> section.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor173"/>Installing OpenEBS</h2>
			<p>OpenEBS is an open source<a id="_idIndexMarker512"/> CNCF project for Kubernetes designed to enable stateful applications to easily access dynamic local PVs, or replicated and highly available PVs. OpenEBS is an example of the new category of cloud-native storage solutions known as CAS. CAS solutions are easy to maintain, are portable, can run on any platform, are scalable, and fulfil the infrastructure design principles that I introduced in <a href="B16192_01_Final_PG_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to Kubernetes Infrastructure and Production-Readiness</em>.</p>
			<p>To learn more about its prerequisites and the<a id="_idIndexMarker513"/> detailed usage of OpenEBS, please refer to the following link: <a href="https://docs.openebs.io/">https://docs.openebs.io/</a>.</p>
			<p>Now, let's install OpenEBS on your Kubernetes cluster and prepare your cluster to provide dynamically provisioned PVs:</p>
			<ol>
				<li value="1">Create a namespace called <strong class="source-inline">openebs</strong>:<p class="source-code"><strong class="bold">$ kubectl create ns openebs</strong></p></li>
				<li>Add the OpenEBS Helm chart repository to your local repository list:<p class="source-code"><strong class="bold">$ helm repo add openebs https://openebs.github.io/charts</strong></p></li>
				<li>Update the Helm chart repositories:<p class="source-code"><strong class="bold">$ helm repo update</strong></p></li>
				<li>Install <strong class="source-inline">openebs</strong> from its Helm repository:<p class="source-code"><strong class="bold">$ helm install --namespace openebs openebs openebs/openebs</strong></p></li>
				<li>Verify successful installation by executing the following command: <p class="source-code"><strong class="bold">$ kubectl get pods -n openebs</strong></p></li>
				<li>The output of the <a id="_idIndexMarker514"/>preceding command should look as follows:</li>
			</ol>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="Images/B16192_07_003.jpg" alt="Figure 7.3 – List of the OpenEBS pods running following successful installation&#13;&#10;" width="806" height="206"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – List of the OpenEBS pods running following successful installation</p>
			<p>Now that you can use OpenEBS for dynamically creating PVs, you can either create a new SC or use one of the default storage classes provided by OpenEBS. </p>
			<p>OpenEBS provides various types of block storage options, including storage engines called <strong class="source-inline">Jiva</strong>, <strong class="source-inline">cStor</strong>, and <strong class="source-inline">Mayastor</strong>, for persistent workloads that require highly available volumes during node failures and <strong class="source-inline">Dynamic Local PV</strong> (device, host path, ZFS) alternatives for distributed applications, such as Cassandra, Elastic, Kafka, or MinIO.</p>
			<p>Execute the following command to get the list of default storage classes in your cluster:</p>
			<p class="source-code">$ kubectl get sc</p>
			<p>You will notice the new storage classes, <strong class="source-inline">openebs-device</strong>, <strong class="source-inline">openebs-hostpath</strong>, <strong class="source-inline">openebs-jiva-default</strong>, and <strong class="source-inline">openebs-snapshot-promoter</strong>, added to your list. </p>
			<p>Here is an example of a YAML manifest to create a PVC using the default <strong class="source-inline">openebs-jiva-default</strong> storage class:</p>
			<p class="source-code">---</p>
			<p class="source-code">kind: PersistentVolumeClaim</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: openebs-pvc</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  storageClassName: openebs-jiva-default</p>
			<p class="source-code">  accessModes:</p>
			<p class="source-code">    - ReadWriteOnce</p>
			<p class="source-code">  resources:</p>
			<p class="source-code">    requests:</p>
			<p class="source-code">      storage: 5G</p>
			<p class="source-code">---</p>
			<p>Now you have learned who to create a PV for with your stateful applications using an open source CAS alternative – OpenEBS. </p>
			<p>From now on, if running on an AWS infrastructure, you can continue to consume your existing EBS volumes using the <strong class="source-inline">gp2</strong> storage class or the <strong class="source-inline">ebs-sc</strong> storage class created earlier using <strong class="source-inline">Amazon_EBS_CSI_Driver</strong>, or take advantage of OpenEBS to abstract data management. OpenEBS, in <a id="_idIndexMarker515"/>the same way as CAS solutions, helps to reduce many of the challenges we described in the <em class="italic">Understanding the challenges with stateful applications</em> section earlier in this chapter. </p>
			<p>Now that we have learned how to use storage provisioners to dynamically provision a PV, let's use it, along with a stateful application, to simplify the life cycle of data management.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor174"/>Deploying a stateful application on OpenEBS volumes</h2>
			<p>OpenEBS <a id="_idIndexMarker516"/>provides a flexible data plane with a <a id="_idIndexMarker517"/>few storage engines options that are optimized for different application and performance expectations. You can read about the differences between storage engines on the official OpenEBS documentation site at <a href="https://docs.openebs.io/docs/next/casengines.html">https://docs.openebs.io/docs/next/casengines.html</a>. Here, we will dive into one of the defaults, the low-footprint storage engine option, <strong class="source-inline">Jiva</strong>. </p>
			<p>Now, we will modify the <strong class="source-inline">pvc-percona.yaml</strong> and <strong class="source-inline">deployment-percona.yaml</strong> manifest files. We will adjust the <strong class="source-inline">percona</strong> deployment to use a StorageClass to dynamically request a PV through a PVC:</p>
			<ol>
				<li value="1">Create a <strong class="source-inline">StorageClass</strong> named <strong class="source-inline">openebs-jiva-3r</strong> with a <strong class="source-inline">ReplicaCount</strong> of <strong class="source-inline">3</strong> in the following path – <strong class="source-inline">stateful/percona/sc-openebs-jiva.yaml</strong>. This will create three copies of the volume and make it highly available in the event of node failure:<p class="source-code">apiVersion: storage.k8s.io/v1</p><p class="source-code">kind: StorageClass</p><p class="source-code">metadata:</p><p class="source-code">  name: openebs-jiva-3r</p><p class="source-code">  annotations:</p><p class="source-code">    openebs.io/cas-type: jiva</p><p class="source-code">    cas.openebs.io/config: |</p><p class="source-code">      - name: ReplicaCount</p><p class="source-code">        value: "3"</p><p class="source-code">      - name: StoragePool</p><p class="source-code">        value: default</p><p class="source-code">provisioner: openebs.io/provisioner-iscsi</p></li>
				<li>Execute the <a id="_idIndexMarker518"/>following <strong class="source-inline">kubectl</strong> command <a id="_idIndexMarker519"/>to create the StorageClass:<p class="source-code"><strong class="bold">$ kubectl apply -f sc-openebs-jiva.yaml</strong></p></li>
				<li>Edit the template for the <strong class="source-inline">percona-pvc</strong> PVC in this path, <strong class="source-inline">stateful/percona/pvc-percona.yaml</strong>, using your preferred text editor. Adjust the name and <strong class="source-inline">storageClassName</strong> fields as follows:<p class="source-code">  storageClassName: openebs-jiva-3r</p></li>
				<li>Edit the template for the <strong class="source-inline">percona</strong> deployment in this path, <strong class="source-inline">stateful/percona/deployment-percona.yaml</strong>, using your preferred text editor. Adjust the last line, <strong class="source-inline">claimName</strong>, as follows:<p class="source-code">            claimName: percona-pvc-openebs</p></li>
				<li>Execute<a id="_idIndexMarker520"/> the following <strong class="source-inline">kubectl</strong> commands to create the <strong class="source-inline">percona</strong> deployment in the cluster using a<a id="_idIndexMarker521"/> dynamically provisioned PV:<p class="source-code"><strong class="bold">$ kubectl apply -f pvc-percona.yaml</strong></p><p class="source-code"><strong class="bold">$ kubectl apply -f deployment-percona.yaml</strong></p></li>
			</ol>
			<p>Now you have created a stateful application deployment backed by dynamically created OpenEBS PVs. This step helped us to abstract data management on cloud and bare-metal or VM-based Kubernetes clusters. </p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor175"/>Summary</h1>
			<p>In this chapter, we learned the stateful application challenges and best practices to consider when choosing the best storage management solutions, both open source and commercial, and finally, the stateful application considerations when deploying them in production using Kubernetes' StatefulSet and deployment objects. </p>
			<p>We deployed the AWS EBS CSI driver and OpenEBS. We also created a highly available replicated storage using OpenEBS and deployed our application on OpenEBS volumes.</p>
			<p>We gained a solid understanding of Kubernetes storage in this chapter, but you should perform a detailed evaluation of your cluster storage requirements and take further action to deploy any extra tools and configurations that may be required, including your storage provider's CSI driver.</p>
			<p>In the next chapter, we will learn in detail about seamless and reliable applications. We will also get to grips with containerization best practices to easily scale our applications.</p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor176"/>Further reading</h1>
			<p>You can refer to the following links for more information on the topics covered in this chapter:</p>
			<ul>
				<li><em class="italic">Kubernetes – A Complete DevOps Cookbook</em> (<a href="B16192_05_Final_PG_ePub.xhtml#_idTextAnchor118"><em class="italic">Chapter 5</em></a>, <em class="italic">Preparing for Stateful Workloads</em>): <a href="https://www.packtpub.com/product/kubernetes-a-complete-devops-cookbook/9781838828042">https://www.packtpub.com/product/kubernetes-a-complete-devops-cookbook/9781838828042</a>.</li>
				<li><em class="italic">Kubernetes Container Storage Interface (CSI) Documentation</em>:<a href="https://kubernetes-csi.github.io/docs/introduction.html">https://kubernetes-csi.github.io/docs/introduction.html</a> </li>
				<li><em class="italic">QuickStart Guide to OpenEBS</em>: <a href="https://docs.openebs.io/docs/next/quickstart.html">https://docs.openebs.io/docs/next/quickstart.html</a> </li>
			</ul>
		</div>
	</div></body></html>