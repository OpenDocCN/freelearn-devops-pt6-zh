<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Your Own Kubernetes Cluster</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Exploring the Kubernetes architecture</li>
<li>Setting up a Kubernetes cluster on macOS by minikube</li>
<li><span>Setting up a</span> Kubernetes cluster on Windows by minikube</li>
<li><span>Setting up a</span> Kubernetes cluster on Linux by kubeadm</li>
<li><span>Setting up a</span> Kubernetes cluster on Linux by Ansible (kubespray)</li>
<li>Running your first container in Kubernetes</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">Welcome to your journey into Kubernetes! In this very first section, you will learn how to build your own Kubernetes cluster. Along with understanding each component and connecting them together, you will learn how to run your first container on Kubernetes. Having a Kubernetes cluster will help you continue your studies in the chapters ahead.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the Kubernetes architecture</h1>
                </header>
            
            <article>
                
<p>Kubernetes is an open source container management tool. It is a Go language-based (<a href="https://golang.org">https://golang.org</a>), lightweight and portable application. You can set up a Kubernetes cluster on a Linux-based OS to deploy, manage, and scale Docker container applications on multiple hosts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Kubernetes is <span><span>made up of</span></span> the following components:</p>
<ul>
<li>Kubernetes master</li>
<li>Kubernetes nodes</li>
<li>etcd</li>
<li>Kubernetes network</li>
</ul>
<p class="NormalPACKT">These components are connected via a network, as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e964924c-0254-4850-ae57-8d05133ea0aa.png" style="width:47.17em;height:17.08em;"/></div>
<p>The preceding diagram can be summarized as follows:</p>
<ul>
<li><strong>Kubernetes master</strong>: It connects to etcd via HTTP or HTTPS to store the data</li>
<li><strong>Kubernetes nodes</strong>: It connect to the Kubernetes master via HTTP or HTTPS to get a command and report the status</li>
<li><strong>Kubernetes network</strong>: It L2, L3 or overlay make a connection of their container applications</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">In this section, we are going to explain how to use the Kubernetes master and nodes to realize the main functions of the Kubernetes system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes master</h1>
                </header>
            
            <article>
                
<p>The Kubernetes master is the main component of the Kubernetes cluster. It serves several functionalities, such as the following:</p>
<ul>
<li>Authorization and authentication</li>
<li>RESTful API entry point</li>
<li>Container deployment scheduler to Kubernetes nodes</li>
<li>Scaling and replicating controllers</li>
<li>Reading the configuration to set up a cluster</li>
</ul>
<p>The following diagram shows how master daemons work together to fulfill the aforementioned functionalities:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d05d8b65-158f-4a4b-8a4c-b9cf00ef1133.png" style="width:36.50em;height:26.92em;"/></div>
<p class="NormalPACKT">There are several daemon processes that form the Kubernetes master's functionality, such as <kbd>kube-apiserver</kbd>, <kbd>kube-scheduler</kbd> and <kbd>kube-controller-manager</kbd>. Hypercube, the wrapper binary, can launch all these daemons.</p>
<p class="NormalPACKT">In addition, the Kubernetes command-line interface, <span class="KeyWordPACKT">kubect</span> can control the Kubernetes master functionality.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">API server (kube-apiserver)</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">The API server provides an HTTP- or HTTPS-based RESTful API, which is the hub between Kubernetes components, such as kubectl, the scheduler, the replication controller, the etcd data store, the kubelet and kube-proxy, which runs on Kubernetes nodes, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scheduler (kube-scheduler)</h1>
                </header>
            
            <article>
                
<p>The scheduler helps to choose which container runs on which nodes. It is a simple algorithm that defines the priority for dispatching and binding containers to nodes. For example:</p>
<ul>
<li>CPU</li>
<li>Memory</li>
<li>How many containers are running?</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Controller manager (kube-controller-manager)</h1>
                </header>
            
            <article>
                
<p>The controller manager performs cluster operations. For example:</p>
<ul>
<li>Manages Kubernetes nodes</li>
<li>Creates and updates the Kubernetes internal information</li>
<li>Attempts to change the current status to the desired status</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Command-line interface (kubectl)</h1>
                </header>
            
            <article>
                
<p>After you install the Kubernetes master, you can use the Kubernetes command-line interface, <kbd>kubectl</kbd>, to control the Kubernetes cluster. For example, <kbd>kubectl get cs</kbd> returns the status of each component. Also, <kbd>kubectl get nodes</kbd> returns a list of Kubernetes nodes:</p>
<pre><strong>//see the Component Statuses</strong><br/><strong># kubectl get cs</strong><br/><strong>NAME                 STATUS    MESSAGE              ERROR</strong><br/><strong>controller-manager   Healthy   ok                   nil</strong><br/><strong>scheduler            Healthy   ok                   nil</strong><br/><strong>etcd-0               Healthy   {"health": "true"}   nil</strong><br/><br/><br/><strong>//see the nodes</strong><br/><strong># kubectl get nodes</strong><br/><strong>NAME          LABELS                           STATUS    AGE</strong><br/><strong>kub-node1   kubernetes.io/hostname=kub-node1   Ready     26d</strong><br/><strong>kub-node2   kubernetes.io/hostname=kub-node2   Ready     26d</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes node</h1>
                </header>
            
            <article>
                
<p>The Kubernetes node is a slave node in the Kubernetes cluster. It is controlled by the Kubernetes master to run container applications using Docker (<a href="http://docker.com">http://docker.com</a>) or rkt (<a href="http://coreos.com/rkt/docs/latest/">http://coreos.com/rkt/docs/latest/)</a>. In this book, we will use the Docker container runtime as the default engine.</p>
<div class="packt_infobox">
<p><span class="packt_screen">Node or slave?</span></p>
<p>The term slave is used in the computer industry to represent the cluster worker node; however, it is also associated with discrimination. The Kubernetes project uses minion in the early version and node in the current version.</p>
</div>
<p class="NormalPACKT">The following diagram displays the role and tasks of daemon processes in the node:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bd21b8ae-fe86-4015-8b84-3a5e8aa502a9.png" style="width:27.67em;height:15.83em;"/></div>
<p class="NormalPACKT">The node also has two daemon processes, named kubelet and kube-proxy, to support its functionalities.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">kubelet</h1>
                </header>
            
            <article>
                
<p>kubelet is the main process on the Kubernetes node that communicates with the Kubernetes master to handle the following operations:</p>
<ul>
<li>Periodically accesses the API controller to check and report</li>
<li>Performs container operations</li>
<li>Runs the HTTP server to provide simple APIs</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Proxy (kube-proxy)</h1>
                </header>
            
            <article>
                
<p>The proxy handles the network proxy and load balancer for each container. It changes Linux iptables rules (nat table) to control TCP and UDP packets across the containers.</p>
<p>After starting the kube-proxy daemon, it configures iptables rules; you can use <kbd>iptables -t nat -L</kbd> or <kbd>iptables -t nat -S</kbd> to check the nat table rules, as follows:</p>
<pre><strong>//the result will be vary and dynamically changed by kube-proxy</strong><br/><strong># sudo iptables -t nat -S</strong><br/><strong>-P PREROUTING ACCEPT</strong><br/><strong>-P INPUT ACCEPT</strong><br/><strong>-P OUTPUT ACCEPT</strong><br/><strong>-P POSTROUTING ACCEPT</strong><br/><strong>-N DOCKER</strong><br/><strong>-N FLANNEL</strong><br/><strong>-N KUBE-NODEPORT-CONTAINER</strong><br/><strong>-N KUBE-NODEPORT-HOST</strong><br/><strong>-N KUBE-PORTALS-CONTAINER</strong><br/><strong>-N KUBE-PORTALS-HOST</strong><br/><strong>-A PREROUTING -m comment --comment "handle ClusterIPs; NOTE: this must be before the NodePort rules" -j KUBE-PORTALS-CONTAINER</strong><br/><strong>-A PREROUTING -m addrtype --dst-type LOCAL -m comment --comment "handle service NodePorts; NOTE: this must be the last rule in the chain" -j KUBE-NODEPORT-CONTAINER</strong><br/><strong>-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER</strong><br/><strong>-A OUTPUT -m comment --comment "handle ClusterIPs; NOTE: this must be before the NodePort rules" -j KUBE-PORTALS-HOST</strong><br/><strong>-A OUTPUT -m addrtype --dst-type LOCAL -m comment --comment "handle service NodePorts; NOTE: this must be the last rule in the chain" -j KUBE-NODEPORT-HOST</strong><br/><strong>-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER</strong><br/><strong>-A POSTROUTING -s 192.168.90.0/24 ! -o docker0 -j MASQUERADE</strong><br/><strong>-A POSTROUTING -s 192.168.0.0/16 -j FLANNEL</strong><br/><strong>-A FLANNEL -d 192.168.0.0/16 -j ACCEPT</strong><br/><strong>-A FLANNEL ! -d 224.0.0.0/4 -j MASQUERADE</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">There are two more components to complement Kubernetes node functionalities, the data store etcd and the inter-container network. You can learn how they support the Kubernetes system in the following subsections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">etcd</h1>
                </header>
            
            <article>
                
<p>etcd (<a href="https://coreos.com/etcd/">https://coreos.com/etcd/</a>) is the distributed key-value data store. It can be accessed via the RESTful API to perform CRUD operations over the network. Kubernetes uses etcd as the main data store.</p>
<p>You can explore the Kubernetes configuration and status in etcd (<kbd>/registry</kbd>) using the <kbd>curl</kbd> command, as follows:</p>
<pre><strong>//example: etcd server is localhost and default port is 4001</strong><br/><strong># curl -L http://127.0.0.1:4001/v2/keys/registry</strong><br/><strong>{"action":"get","node":{"key":"/registry","dir":true,"nodes":[{"key":"/registry/namespaces","dir":true,"modifiedIndex":6,"createdIndex":6},{"key":"/registry/pods","dir":true,"modifiedIndex":187,"createdIndex":187},{"key":"/registry/clusterroles","dir":true,"modifiedIndex":196,"createdIndex":196},{"key":"/registry/replicasets","dir":true,"modifiedIndex":178,"createdIndex":178},{"key":"/registry/limitranges","dir":true,"modifiedIndex":202,"createdIndex":202},{"key":"/registry/storageclasses","dir":true,"modifiedIndex":215,"createdIndex":215},{"key":"/registry/apiregistration.k8s.io","dir":true,"modifiedIndex":7,"createdIndex":7},{"key":"/registry/serviceaccounts","dir":true,"modifiedIndex":70,"createdIndex":70},{"key":"/registry/secrets","dir":true,"modifiedIndex":71,"createdIndex":71},{"key":"/registry/deployments","dir":true,"modifiedIndex":177,"createdIndex":177},{"key":"/registry/services","dir":true,"modifiedIndex":13,"createdIndex":13},{"key":"/registry/configmaps","dir":true,"modifiedIndex":52,"createdIndex":52},{"key":"/registry/ranges","dir":true,"modifiedIndex":4,"createdIndex":4},{"key":"/registry/minions","dir":true,"modifiedIndex":58,"createdIndex":58},{"key":"/registry/clusterrolebindings","dir":true,"modifiedIndex":171,"createdIndex":171}],"modifiedIndex":4,"createdIndex":4}}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes network</h1>
                </header>
            
            <article>
                
<p>Network communication between containers is the most difficult part. Because Kubernetes manages multiple nodes (hosts) running several containers, those containers on different nodes may need to communicate with each other.</p>
<p>If the container's network communication is only within a single node, you can use Docker network or Docker compose to discover the peer. However, along with multiple nodes, Kubernetes uses an overlay network or <strong>container network interface</strong> (<strong>CNI</strong>) to achieve multiple container communication.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">This recipe describes the basic architecture and methodology of Kubernetes and the related components. Understanding Kubernetes is not easy, but a step-by-step learning process on how to set up, configure, and manage Kubernetes is really fun.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the Kubernetes cluster on macOS by minikube</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">Kubernetes consists of combination of multiple open source components. These are developed by different parties, making it difficult to find and download all the related packages and install, configure, and make them work from scratch.</p>
<p class="NormalPACKT">Fortunately, there are some different solutions and tools that have been developed to set up Kubernetes clusters effortlessly. Therefore, it is highly recommended you use such a tool to set up Kubernetes on your environment.</p>
<p class="NormalPACKT">The following tools are categorized by different types of solution to build your own Kubernetes:</p>
<ul>
<li>Self-managed solutions that include:
<ul>
<li>minikube</li>
<li>kubeadm</li>
<li>kubespray</li>
<li>kops</li>
</ul>
</li>
<li>Enterprise solutions that include:
<ul>
<li>OpenShift (<a href="https://www.openshift.com">https://www.openshift.com</a>)</li>
<li>Tectonic (<a href="https://coreos.com/tectonic/">https://coreos.com/tectonic/</a>)</li>
</ul>
</li>
<li>Cloud-hosted solutions that include:<br/>
<ul>
<li>Google Kubernetes engine (<a href="https://cloud.google.com/kubernetes-engine/">https://cloud.google.com/kubernetes-engine/</a>)</li>
<li>Amazon elastic container service for Kubernetes <span>(Amazon EKS,</span> <a href="https://aws.amazon.com/eks/">https://aws.amazon.com/eks/</a>)</li>
<li>Azure Container Service (AKS, <a href="https://azure.microsoft.com/en-us/services/container-service/">https://azure.microsoft.com/en-us/services/container-service/</a>)</li>
</ul>
</li>
</ul>
<p>A self-managed solution is suitable if we just want to build a development environment or do a proof of concept quickly.</p>
<p>By using minikube (<a href="https://github.com/kubernetes/minikube">https://github.com/kubernetes/minikube</a>) and <span>kubeadm (</span><a href="https://kubernetes.io/docs/admin/kubeadm/">https://kubernetes.io/docs/admin/kubeadm/</a><span>)</span>, we can easily build the desired environment on our machine locally; however, it is not practical if we want to build a production environment.</p>
<p><span>By using kubespray (</span><a href="https://github.com/kubernetes-incubator/kubespray">https://github.com/kubernetes-incubator/kubespray</a><span>) and kops (<a href="https://github.com/kubernetes/kops">https://github.com/kubernetes/kops</a>), we can also build a production-grade environment quickly from scratch.</span></p>
<p>An enterprise solution or cloud-hosted solution is the easiest starting point if we want to create a production environment. In particular, the <strong>Google Kubernetes Engine</strong> (<strong>GKE</strong>), which has been used by Google for many years, comes with comprehensive management, meaning that users don't need to care much about the installation and settings. Also, Amazon EKS is a new service that was introduced at AWS re: Invent 2017, which is managed by the Kubernetes service on AWS.</p>
<p>Kubernetes can also run on different clouds and on-premise VMs by custom solutions. To get started, we will build Kubernetes using minikube on macOS desktop machines in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>minikube runs Kubernetes on the Linux VM on macOS. It relies on a hypervisor (virtualization technology), such as VirtualBox (<a href="https://www.virtualbox.org">https://www.virtualbox.org</a>), VMWare fusion (<a href="https://www.vmware.com/products/fusion.html">https://www.vmware.com/products/fusion.html</a>), or hyperkit (<a href="https://github.com/moby/hyperkit">https://github.com/moby/hyperkit</a>) In addition, we will need to have the Kubernetes <strong>command-line interface</strong> (<strong>CLI</strong>) <kbd>kubectl</kbd>, which is used to connect through the hypervisor, to control Kubernetes.</p>
<p>With minikube, you can run the entire suite of the Kubernetes stack on your macOS, including the Kubernetes master, node, and CLI. It is recommended that macOS has enough memory to run Kubernetes. By default, minikube uses VirtualBox as the hypervisor.</p>
<p>In this chapter, however, we will demonstrate how to use hyperkit, which is the most lightweight solution. As Linux VM consumes 2 GB of memory, at least 4 GB of memory is recommended. Note that hyperkit is built on the top of the hypervisor framework (<a href="https://developer.apple.com/documentation/hypervisor">https://developer.apple.com/documentation/hypervisor</a>) on macOS; therefore, macOS 10.10 Yosemite or later is required.</p>
<p class="NormalPACKT">The following diagram shows the relationship between kubectl, the hypervisor, minikube, and macOS:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/52edf167-0558-4969-b440-eddb763d898e.png" style="width:25.42em;height:12.83em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>macOS doesn't have an official package management tool, such as yum and apt-get on Linux. But there are some useful tools available for macOS. <kbd>Homebrew</kbd> (<a href="https://brew.sh">https://brew.sh</a>) is the most popular package management tool and manages many open source tools, including minikube.</p>
<p>In order to install <kbd>Homebrew</kbd> on macOS, perform the following steps:</p>
<ol>
<li>Open the Terminal and then type the following command:</li>
</ol>
<pre class="CodePACKT" style="padding-left: 90px"><strong>$ /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"</strong></pre>
<ol start="2">
<li class="NormalPACKT">Once installation is completed, you can type <kbd><span class="ScreenTextPACKT">/usr/local/bin/brew help</span></kbd> to see the available command options.</li>
</ol>
<div class="packt_tip">
<p>If you just install or upgrade Xcode on your macOS, the <kbd>Homebrew</kbd> installation may stop. In that case, open Xcode to accept the license agreement or type <kbd>sudo xcodebuild -license</kbd> beforehand.</p>
</div>
<ol start="3">
<li class="NormalPACKT">Next, install the <kbd>hyperkit driver</kbd> for minikube. At the time of writing (February 2018), HomeBrew does not support hyperkit; therefore type the following command to install it:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px"><strong>$ curl -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-hyperkit \</strong><br/><strong>&amp;&amp; chmod +x docker-machine-driver-hyperkit \</strong><br/><strong>&amp;&amp; sudo mv docker-machine-driver-hyperkit /usr/local/bin/ \</strong><br/><strong>&amp;&amp; sudo chown root:wheel /usr/local/bin/docker-machine-driver-hyperkit \</strong><br/><strong>&amp;&amp; sudo chmod u+s /usr/local/bin/docker-machine-driver-hyperkit</strong></pre>
<ol start="4">
<li><span>Next, let's install the Kubernetes CLI. Use Homebrew with the following comment to install the</span> <kbd>kubectl</kbd> <span>command on your macOS:</span></li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px"><strong>//install kubectl command by "kubernetes-cli" package</strong><br/><strong>$ brew install kubernetes-cli</strong></pre>
<p style="padding-left: 60px">Finally, you can install minikube. It is not managed by Homebrew; however, Homebrew has an extension called <kbd>homebrew-cask</kbd> (<a href="https://github.com/caskroom/homebrew-cask">https://github.com/caskroom/homebrew-cask</a>) that supports minikube.</p>
<ol start="5">
<li>In order to install minikube by <kbd>homebrew-cask</kbd>, just simply type the following command:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px"><strong>//add "cask" option</strong><br/><strong>$ brew cask install minikube</strong></pre>
<ol start="6">
<li>If you have never installed <strong>Docker for Mac</strong> on your machine, you need to install it via <kbd>homebrew-cask</kbd> as well</li>
</ol>
<pre style="padding-left: 90px"><strong>//only if you don't have a Docker for Mac<br/>$ brew cask install docker<br/><br/>//start Docker<br/>$ open -a Docker.app</strong></pre>
<ol start="7">
<li class="NormalPACKT">Now you are all set! The following command shows whether the required packages have been installed on your macOS or not:</li>
</ol>
<pre style="padding-left: 90px"><strong>//check installed package by homebrew</strong><br/><strong>$ brew list</strong><br/><strong>kubernetes-cli</strong><br/><br/><br/><strong>//check installed package by homebrew-cask</strong><br/><strong>$ brew cask list</strong><br/><strong>minikube</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">minikube is suitable for setting up Kubernetes on your macOS with the following command, which downloads and starts a Kubernetes VM stet, and then configures the kubectl configuration (<kbd>~/.kube/config</kbd>):</p>
<pre class="CommandLinePACKT"><strong>//use --vm-driver=hyperkit to specify to use hyperkit</strong><br/><strong>$ /usr/local/bin/minikube start --vm-driver=hyperkit</strong><br/><strong>Starting local Kubernetes v1.10.0 cluster...</strong><br/><strong>Starting VM...</strong><br/><strong>Downloading Minikube ISO</strong><br/><strong> 150.53 MB / 150.53 MB [============================================] 100.00% 0s</strong><br/><strong>Getting VM IP address...</strong><br/><strong>Moving files into cluster...</strong><br/><strong>Downloading kubeadm v1.10.0</strong><br/><strong>Downloading kubelet v1.10.0</strong><br/><strong>Finished Downloading kubelet v1.10.0</strong><br/><strong>Finished Downloading kubeadm v1.10.0</strong><br/><strong>Setting up certs...</strong><br/><strong>Connecting to cluster...</strong><br/><strong>Setting up kubeconfig...</strong><br/><strong>Starting cluster components...</strong><br/><strong>Kubectl is now configured to use the cluster.</strong><br/><strong>Loading cached images from config file.</strong><br/><br/><br/><strong>//check whether .kube/config is configured or not</strong><br/><strong>$ cat ~/.kube/config </strong><br/><strong>apiVersion: v1</strong><br/><strong>clusters:</strong><br/><strong>- cluster:</strong><br/><strong>    certificate-authority: /Users/saito/.minikube/ca.crt</strong><br/><strong>    server: https://192.168.64.26:8443</strong><br/><strong>  name: minikube</strong><br/><strong>contexts:</strong><br/><strong>- context:</strong><br/><strong>    cluster: minikube</strong><br/><strong>    user: minikube</strong><br/><strong>  name: minikube</strong><br/><strong>current-context: minikube</strong><br/><strong>kind: Config</strong><br/><strong>preferences: {}</strong><br/><strong>users:</strong><br/><strong>- name: minikube</strong><br/><strong>  user:</strong><br/><strong>    as-user-extra: {}</strong><br/><strong>    client-certificate: /Users/saito/.minikube/client.crt</strong><br/><strong>    client-key: /Users/saito/.minikube/client.key<br/></strong></pre>
<p>After getting all the necessary packages, perform the following steps:</p>
<ol>
<li>Wait for a few minutes for the Kubernetes cluster setup to complete.</li>
<li class="NormalPACKT">Use <kbd><span class="ScreenTextPACKT">kubectl version</span></kbd> to check the Kubernetes master version and <kbd><span class="ScreenTextPACKT">kubectl get cs</span></kbd> to see the component status.</li>
<li class="NormalPACKT">Also, use the <kbd><span class="ScreenTextPACKT">kubectl get nodes</span></kbd> command to check whether the Kubernetes node is ready or not:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px"><strong>//it shows kubectl (Client) is 1.10.1, and Kubernetes master (Server) is 1.10.0</strong><br/><strong>$ /usr/local/bin/kubectl version --short</strong><br/><strong>Client Version: v1.10.1</strong><br/><strong>Server Version: v1.10.0</strong><br/><br/><br/><strong>//get cs will shows Component Status</strong><br/><strong>$ kubectl get cs</strong><br/><strong>NAME                 STATUS    MESSAGE              ERROR</strong><br/><strong>controller-manager   Healthy   ok                   </strong><br/><strong>scheduler            Healthy   ok                   </strong><br/><strong>etcd-0               Healthy   {"health": "true"}   </strong><br/><br/><br/><strong>//Kubernetes node (minikube) is ready</strong><br/><strong>$ /usr/local/bin/kubectl get nodes</strong><br/><strong>NAME       STATUS    ROLES     AGE       VERSION<br/><span>minikube <span class="Apple-converted-space">  </span>Ready <span class="Apple-converted-space">    </span>master<span class="Apple-converted-space">    </span>2m<span class="Apple-converted-space">        </span>v1.10.0</span></strong></pre>
<ol start="4">
<li class="NormalPACKT">Now you can start to use Kubernetes on your machine. The following sections describe how to use the <kbd>kubectl</kbd> command to manipulate Docker containers.</li>
<li class="NormalPACKT">Note that, in some cases, you may need to maintain the Kubernetes cluster, such as starting/stopping the VM or completely deleting it. The following commands  maintain the minikube environment:</li>
</ol>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Command</strong></p>
</td>
<td>
<p><strong>Purpose</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>minikube start --vm-driver=hyperkit</kbd></p>
</td>
<td>
<p>Starts the Kubernetes VM using the hyperkit driver</p>
</td>
</tr>
<tr>
<td>
<p><kbd>minikube stop</kbd></p>
</td>
<td>
<p>Stops the Kubernetes VM</p>
</td>
</tr>
<tr>
<td>
<p><kbd>minikube delete</kbd></p>
</td>
<td>
<p>Deletes a Kubernetes VM image</p>
</td>
</tr>
<tr>
<td>
<p><kbd>minikube ssh</kbd></p>
</td>
<td>
<p>ssh to the Kubernetes VM guest</p>
</td>
</tr>
<tr>
<td>
<p><kbd>minikube ip</kbd></p>
</td>
<td>
<p>Shows the Kubernetes VM (node) IP address</p>
</td>
</tr>
<tr>
<td>
<p><kbd>minikube update-context</kbd></p>
</td>
<td>
<p>Checks and updates <kbd>~/.kube/config</kbd> if the VM IP address is changed</p>
</td>
</tr>
<tr>
<td>
<p><kbd>minikube dashboard</kbd></p>
</td>
<td>
<p>Opens the web browser to connect the Kubernetes UI</p>
</td>
</tr>
</tbody>
</table>
<p class="NormalPACKT">For example, minikube starts a dashboard (the Kubernetes UI) by the default. If you want to access the dashboard, type <kbd>minikube dashboard</kbd>; it then opens your default browser and connects the Kubernetes UI, as illustrated in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8d9207f5-35c3-4b5a-82cc-0025425f1ff1.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">This recipe describes how to set up a Kubernetes cluster on your macOS using minikube. It is the easiest way to start using Kubernetes. We also learned how to use kubectl, the Kubernetes command-line interface tool, which is the entry point to control our Kubernetes cluster!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the Kubernetes cluster on Windows by minikube</h1>
                </header>
            
            <article>
                
<p>By nature, Docker and Kubernetes are based on a Linux-based OS. Although it is not ideal to use the Windows OS to explore Kubernetes, many people are using the Windows OS as their desktop or laptop machine. Luckily, there are a lot of ways to run the Linux OS on Windows using virtualization technologies, which makes running a Kubernetes cluster on Windows machines possible. Then, we can build a development environment or do a proof of concept on our local Windows machine.</p>
<p>You can run the Linux VM by using any hypervisor on Windows to set up Kubernetes from scratch, but using minikube (<a href="https://github.com/kubernetes/minikube">https://github.com/kubernetes/minikube</a>) is the fastest way to build a Kubernetes cluster on Windows. Note that this recipe is not ideal for a production environment because it will set up a Kubernetes on Linux VM on Windows.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>To set up minikube on Windows requires a hypervisor, either VirtualBox (<a href="https://www.virtualbox.org">https://www.virtualbox.org</a>) or Hyper-V, because, again, minikube uses the Linux VM on Windows. This means that you cannot use the Windows virtual machine (for example, running the Windows VM on macOS by parallels).</p>
<p>However, <kbd>kubectl</kbd> , the Kubernetes CLI, supports a Windows native binary that can connect to Kubernetes over a network. So, you can set up a portable suite of Kubernetes stacks on your Windows machine.</p>
<p>The following diagram shows the relationship between kubectl, Hypervisor, minikube, and Windows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a68272cf-8a6b-4ad9-8b69-bf2e0a3b06b4.png" style="width:26.17em;height:13.83em;"/></div>
<p>Hyper-V is required for Windows 8 Pro or later. While many users still use Windows 7, we will use VirtualBox as the minikube hypervisor in this recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>First of all, VirtualBox for Windows is required:</p>
<ol>
<li>Go to the VirtualBox website (<a href="https://www.virtualbox.org/wiki/Downloads">https://www.virtualbox.org/wiki/Downloads</a>) to download the Windows installer.</li>
<li>Installation is straightforward, so we <span>can </span>just choose the default options and click <span class="packt_screen">Next</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/974c0731-431f-4311-92d5-e90fb7125196.png" style="width:37.33em;height:29.08em;"/></div>
<ol start="3">
<li class="NormalPACKT">Next, create the <kbd>Kubernetes</kbd> folder, which is used to store the minikube and kubectl binaries. Let's create the <kbd><span class="KeyWordPACKT">k8s</span></kbd> folder on top of the <kbd>C:</kbd> drive, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0e05fcfa-9a7e-4433-8092-da2715ed76df.png"/></div>
<ol start="4">
<li class="NormalPACKT">This folder must be in the command search path, so open <span class="packt_screen">System Properties</span>, then move to the <span class="packt_screen">Advanced</span> tab.</li>
<li class="NormalPACKT">Click the <span class="packt_screen">Environment Variables...</span> button, then choose <span class="packt_screen">Path</span> , and then click the <span class="packt_screen">Edit...</span> button, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5aca299b-611c-48ae-973d-e8a219ab1b47.png"/></div>
<ol start="6">
<li>Then, append <kbd>c:\k8s</kbd> , as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/06503e44-8ead-4b89-b980-9317068883c4.png"/></div>
<ol start="7">
<li>After clicking the <span class="packt_screen">OK</span> button, log off and logo on to Windows again (or reboot) to apply this change.</li>
<li>Next, download minikube for Windows. It is a single binary, so use any web browser to download <a href="https://github.com/kubernetes/minikube/releases/download/v0.26.1/minikube-windows-amd64">https://github.com/kubernetes/minikube/releases/download/v0.26.1/minikube-windows-amd64</a> and then copy it to the <kbd>c:\k8s</kbd> folder, but change the filename to <kbd>minikube.exe</kbd>.</li>
</ol>
<p> </p>
<ol start="9">
<li>Next, download kubectl for Windows, which can communicate with Kubernetes. It is also single binary like minikube. So, download <a href="https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/windows/amd64/kubectl.exe">https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/windows/amd64/kubectl.exe</a> and then copy it to the <kbd>c:\k8s</kbd> folder as well.</li>
<li>Eventually, you will see two binaries in the <kbd>c:\k8s</kbd> folder, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d769ed44-88be-4a27-93d7-bdb86f28fdbf.png"/></div>
<div class="packt_tip"><span>If you are running anti-virus software, it may prevent you from running <kbd>kubectl.exe</kbd> and <kbd>minikube.exe</kbd>. If so, please update your anti-virus software setting that allows running these two binaries.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">Let's get started!</p>
<ol>
<li class="NormalPACKT">Open Command Prompt and then type <kbd><span class="KeyWordPACKT">minikube start</span></kbd> , as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cf21415a-f5c0-4528-95d0-74ae67d5f90f.png"/></div>
<ol start="2">
<li class="NormalPACKT">minikube downloads the Linux VM image and then sets up Kubernetes on the Linux VM; now if you open VirtualBox, you can see that the minikube guest has been registered, as illustrated in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/49a5d949-29c4-4762-8ee0-f89969082c19.png"/></div>
<ol start="3">
<li class="NormalPACKT">Wait for a few minutes to complete the setup of the Kubernetes cluster.</li>
<li class="NormalPACKT">As per the following screenshot, type <kbd><span class="ScreenTextPACKT">kubectl version</span></kbd> to check the Kubernetes master version.</li>
<li class="NormalPACKT">Use the <kbd><span class="ScreenTextPACKT">kubectl get nodes</span></kbd> command to check whether the Kubernetes node is ready or not:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d323c09d-4c29-4862-8fdd-9ab4ab63be36.png" style="width:50.25em;height:26.50em;"/></div>
<ol start="6">
<li>Now you can start to use Kubernetes on your machine! Again, Kubernetes is running on the Linux VM, as shown in the next screenshot.</li>
<li>U<span>sing</span> <kbd><span class="KeyWordPACKT">minikube ssh</span></kbd> <span>allows you to access the Linux VM that runs Kubernetes:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9f745739-14ce-487c-9a3b-16ff37edefe0.png" style="width:52.00em;height:27.58em;"/></div>
<p class="NormalPACKT" style="padding-left: 60px"><span>Therefore, any Linux-based Docker image is capable of running on your Windows machine.</span></p>
<ol start="8">
<li class="NormalPACKT">Type <kbd>minikube ip</kbd> to verify which IP address the Linux VM uses and also <kbd>minikube dashboard</kbd><span>, to open your default web browser and navigate to the Kubernetes UI ,as shown in the following screenshot:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4cac90a8-990e-4b86-b195-f660c51d0cb4.png"/></div>
<ol start="9">
<li class="NormalPACKT">If you don't need to use Kubernetes anymore, type <kbd><span class="KeyWordPACKT">minikube stop</span></kbd> or open VirtualBox to stop the Linux guest and release the resource, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/09fe9f63-c0cf-420f-ac7d-a09f00810126.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">This recipe describes how to set up a Kubernetes cluster on your Windows OS using minikube. It is the easiest way to start using Kubernetes. It also describes kubectl, the Kubernetes command-line interface tool, which is the entry point form which to control your Kubernetes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the Kubernetes cluster on Linux via kubeadm</h1>
                </header>
            
            <article>
                
<p>In this recipe, we are going to show how to create a Kubernetes cluster along with kubeadm (<a href="https://github.com/kubernetes/kubeadm">https://github.com/kubernetes/kubeadm</a>) on Linux servers. Kubeadm is a command-line tool that simplifies the procedure of creating and managing a Kubernetes cluster. Kubeadm leverages the fast deployment feature of Docker, running the system services of the Kubernetes master and the etcd server as containers. When triggered by the <kbd>kubeadm</kbd> command, the container services will contact kubelet on the Kubernetes node directly; kubeadm also checks whether every component is healthy. Through the kubeadm setup steps, you can avoid having a bunch of installation and configuration commands when you build everything from scratch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We will provide instructions of two types of OS:</p>
<ul>
<li>Ubuntu Xenial 16.04 (LTS)</li>
<li>CentOS 7.4</li>
</ul>
<p>Make sure the OS version is matched before continuing. Furthermore, the software dependency and network settings should be also verified before you proceed to thecd cd next step. Check the following items to prepare the environment:</p>
<ul>
<li><strong>Every node has a unique MAC address and product UUID</strong>: Some plugins use the MAC address or product UUID as a unique machine ID to identify nodes (for example, <kbd>kube-dns</kbd>). If they are duplicated in the cluster, kubeadm may not work while starting the plugin:</li>
</ul>
<pre style="padding-left: 90px"><strong>// check MAC address of your NIC<br/></strong><strong>$ ifconfig -a</strong><br/><strong>// check the product UUID on your host</strong><br/><strong>$ sudo cat /sys/class/dmi/id/product_uuid</strong></pre>
<ul>
<li><strong>Every node has a different hostname</strong>: If the hostname is duplicated, the Kubernetes system may collect logs or statuses from multiple nodes into the same one.</li>
<li><strong>Docker is installed</strong>: As mentioned previously, the Kubernetes master will run its daemon as a container, and every node in the cluster should get Docker installed. For how to perform the Docker installation, you can follow the steps on the official website: (Ubuntu: <a href="https://docs.docker.com/engine/installation/linux/docker-ce/ubuntu/">https://docs.docker.com/engine/installation/linux/docker-ce/ubuntu/</a>, and CentOS: <a href="https://docs.docker.com/engine/installation/linux/docker-ce/centos/">https://docs.docker.com/engine/installation/linux/docker-ce/centos/</a>) Here we have Docker CE 17.06 installed on our machines; however, only <span>Docker versions 1.11.2 to 1.13.1, and 17.03.x are verified with Kubernetes version 1.10.</span></li>
<li><strong>Network ports are available</strong>: The Kubernetes system services need network ports for communication. The ports in the following table should now be occupied according to the role of the node:</li>
</ul>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>Node role</strong></td>
<td><strong>Ports</strong></td>
<td><strong>System service</strong></td>
</tr>
<tr>
<td rowspan="6">Master</td>
<td><kbd>6443</kbd></td>
<td>Kubernetes API server</td>
</tr>
<tr>
<td><kbd>10248/10250/10255</kbd></td>
<td>kubelet local healthz endpoint/Kubelet API/Heapster (read-only)</td>
</tr>
<tr>
<td><kbd>10251</kbd></td>
<td>kube-scheduler</td>
</tr>
<tr>
<td><kbd>10252</kbd></td>
<td>kube-controller-manager</td>
</tr>
<tr>
<td><kbd>10249/10256</kbd></td>
<td>kube-proxy</td>
</tr>
<tr>
<td><kbd>2379/2380</kbd></td>
<td>etcd client/etcd server communication</td>
</tr>
<tr>
<td rowspan="2">Node</td>
<td><kbd>10250/10255</kbd></td>
<td>Kubelet API/Heapster (read-only)</td>
</tr>
<tr>
<td><kbd>30000~32767</kbd></td>
<td>Port range reserved for exposing container service to outside world</td>
</tr>
</tbody>
</table>
<div class="packt_figref"/>
<ul>
<li>
<p>The Linux command, <kbd>netstat</kbd>, can help to check if the port is in use or not:</p>
</li>
</ul>
<pre style="padding-left: 90px"><strong>// list every listening port</strong><br/><strong>$ sudo netstat -tulpn | grep LISTEN</strong></pre>
<ul>
<li>Network tool packages are installed. <kbd>ethtool</kbd> and <kbd>ebtables</kbd> are two required utilities for kubeadm. They can be download and installed by the<kbd>apt-get</kbd> or <kbd>yum</kbd>package managing tools.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The installation procedures for two Linux OSes, Ubuntu and CentOS, are going to be introduced separately in this recipe as they have different setups.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Package installation</h1>
                </header>
            
            <article>
                
<p>Let's get the Kubernetes packages first! The repository for downloading needs to be set in the source list of the package management system. Then, we are able to get them installed easily through the command-line.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ubuntu</h1>
                </header>
            
            <article>
                
<p>To install Kubernetes packages in Ubuntu perform the following steps:</p>
<ol>
<li>Some repositories are URL with HTTPS. The <kbd>apt-transport-https</kbd> package must be installed to access the HTTPS endpoint:</li>
</ol>
<pre style="padding-left: 90px"><strong>$ sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https</strong></pre>
<ol start="2">
<li>Download the public key for accessing packages on Google Cloud, and add it as follows:</li>
</ol>
<pre style="padding-left: 90px"><strong>$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -<br/>OK</strong></pre>
<ol start="3">
<li>Next, add a new source list for the Kubernetes packages:</li>
</ol>
<pre style="padding-left: 90px"><strong>$ sudo bash -c 'echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" &gt; /etc/apt/sources.list.d/kubernetes.list'</strong></pre>
<ol start="3">
<li>Finally, it is good to install the Kubernetes packages:</li>
</ol>
<pre style="padding-left: 90px"><strong>// on Kubernetes master</strong><br/><strong>$ sudo apt-get update &amp;&amp; sudo apt-get install -y kubelet kubeadm kubectl<br/>// on Kubernetes node</strong><br/><strong>$ sudo apt-get update &amp;&amp; sudo apt-get install -y kubelet</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CentOS</h1>
                </header>
            
            <article>
                
<p>To install Kubernetes packages in CentOS perform the following steps:</p>
<ol>
<li><span>As with</span> Ubuntu, new repository information needs to be added:</li>
</ol>
<pre style="padding-left: 90px"><strong>$ sudo vim /etc/yum.repos.d/kubernetes.repo</strong><br/><strong>[kubernetes]</strong><br/><strong>name=Kubernetes</strong><br/><strong>baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</strong><br/><strong>enabled=1</strong><br/><strong>gpgcheck=1</strong><br/><strong>repo_gpgcheck=1</strong><br/><strong>gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg</strong><br/><strong>       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</strong></pre>
<ol start="2">
<li>Now, we are ready to pull the packages from the Kubernetes source base via the <kbd>yum</kbd> command:</li>
</ol>
<pre style="padding-left: 90px"><strong>// on Kubernetes master</strong><br/><strong>$ sudo yum install -y kubelet kubeadm kubectl</strong><br/><strong>// on Kubernetes node</strong><br/><strong>$ sudo yum install -y kubelet</strong></pre>
<ol start="3">
<li>No matter what OS it is, check the version of the package you get!</li>
</ol>
<pre style="padding-left: 90px"><strong>// take it easy! server connection failed since there is not server running</strong><br/><strong>$ kubectl version</strong><br/><strong>Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.2", GitCommit:"81753b10df112992bf51bbc2c2f85208aad78335", GitTreeState:"clean", BuildDate:"2018-04-27T09:22:21Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}<br/>The connection to the server 192.168.122.101:6443 was refused - did you specify the right host or port?</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">System configuration prerequisites</h1>
                </header>
            
            <article>
                
<p>Before running up the whole system by kubeadm, please check that Docker is running on your machine for Kubernetes. Moreover, in order to avoid critical errors while executing kubeadm, we will show the necessary service configuration on both the system and kubelet. As well as the master, please set the following configurations on the Kubernetes nodes to get kubelet to work fine with kubeadm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CentOS system settings</h1>
                </header>
            
            <article>
                
<p>There are other additional settings in CentOS to make Kubernetes behave correctly. Be aware that, even if we are not using kubeadm to manage the Kubernetes cluster, the following setup should be considered while running kubelet:</p>
<ol>
<li>Disable SELinux, since kubelet does not support SELinux completely:</li>
</ol>
<pre style="padding-left: 90px"><strong>// check the state of SELinux, if it has already been disabled, bypass below commands<br/>$ sestatus</strong></pre>
<p style="padding-left: 60px">We can <kbd>disable SELinux</kbd> through the following command, or by <kbd>modifying the configuration file</kbd>:</p>
<pre style="padding-left: 90px"><strong>// disable SELinux through command<br/>$  sudo setenforce 0<br/>// or modify the configuration file<br/></strong><span><strong>$ sudo sed –I 's/ SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux</strong><br/></span></pre>
<p style="padding-left: 60px">Then we'll need to <kbd>reboot</kbd> the machine:</p>
<pre style="padding-left: 90px"><strong>// reboot is required<br/>$ sudo reboot</strong></pre>
<ol start="2">
<li>Enable the usage of iptables. To prevent some routing errors happening, add runtime parameters:</li>
</ol>
<pre style="padding-left: 90px"><strong>// enable the parameters by setting them to 1</strong><br/><strong>$ sudo bash -c 'echo "net.bridge.bridge-nf-call-ip6tables = 1" &gt; /etc/sysctl.d/k8s.conf'<br/>$ sudo bash -c 'echo "net.bridge.bridge-nf-call-iptables = 1" &gt;&gt; /etc/sysctl.d/k8s.conf'<br/>// reload the configuration<br/>$ sudo sysctl --system</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Booting up the service</h1>
                </header>
            
            <article>
                
<p>Now we can start the service. First enable and then start kubelet on your Kubernetes master machine:</p>
<pre><strong>$ sudo systemctl enable kubelet &amp;&amp; sudo systemctl start kubelet</strong></pre>
<p>While checking the status of kubelet, you may be worried to see the status displaying activating (<kbd>auto-restart</kbd>); and you may get further frustrated to see the detail logs by the <kbd>journalctl</kbd> command, as follows:</p>
<p><kbd>error: unable to load client CA file /etc/kubernetes/pki/ca.crt: open /etc/kubernetes/pki/ca.crt: no such file or directory</kbd></p>
<p>Don't worry. kubeadm takes care of creating the certificate authorities file. It is defined in the service configuration file, <kbd>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf by argument KUBELET_AUTHZ_ARGS</kbd>. The kubelet service won't be a healthy without this file, so keep trying to restart the daemon by itself.</p>
<p>Go ahead and start all the master daemons via kubeadm. It is worth noting that using kubeadm requires the root permission to achieve a service level privilege. For any sudoer, each kubeadm would go after the <kbd>sudo</kbd> command:</p>
<pre><strong>$ sudo kubeadm init</strong></pre>
<div class="packt_infobox">Find preflight checking error while firing command <kbd>kubeadm init</kbd>? Using following one to disable running swap as description.<br/>
<br/>
<kbd>$ sudo kubeadm init --ignore-preflight-errors=Swap</kbd></div>
<p>And you will see the sentence <kbd>Your Kubernetes master has initialized successfully!</kbd> showing on the screen. Congratulations! You are almost done! Just follow the information about the user environment setup below the greeting message:</p>
<pre><strong>$ mkdir -p $HOME/.kube</strong><br/><strong>$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</strong><br/><strong>$ sudo chown $(id -u):$(id -g) $HOME/.kube/config</strong></pre>
<p>The preceding commands ensure every Kubernetes instruction is fired by your account execute with the proper credentials and connects to the correct server portal:</p>
<pre><strong>// Your kubectl command works great now</strong><br/><strong>$ kubectl version</strong><br/><strong>Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.2", GitCommit:"81753b10df112992bf51bbc2c2f85208aad78335", GitTreeState:"clean", BuildDate:"2018-04-27T09:22:21Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}</strong><br/><strong>Server Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.2", GitCommit:"81753b10df112992bf51bbc2c2f85208aad78335", GitTreeState:"clean", BuildDate:"2018-04-27T09:10:24Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}</strong></pre>
<p>More than that, kubelet goes into a healthy state now:</p>
<pre><strong>// check the status of kubelet</strong><br/><strong>$ sudo systemctl status kubelet</strong><br/><strong>...<br/>Active: active (running) Mon 2018-04-30 18:46:58 EDT; </strong><strong>2min 43s </strong><strong>ago</strong><br/>...</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network configurations for containers</h1>
                </header>
            
            <article>
                
<p>After the master of the cluster is ready to handle jobs and the services are running, for the purpose of making containers accessible to each other through networking, we need to set up the network for container communication. It is even more important initially while building up a Kubernetes cluster with kubeadm, since the master daemons are all running as containers. kubeadm supports the CNI (<a href="https://github.com/containernetworking/cni">https://github.com/containernetworking/cni</a>). We are going to attach the CNI via a Kubernetes network add-on.</p>
<p>There are many third-party CNI solutions that supply secured and reliable container network environments. Calico (<a href="https://www.projectcalico.org">https://www.projectcalico.org</a>), one CNI provide stable container networking. Calico is light and simple, but still well implemented by the CNI standard and integrated with Kubernetes:</p>
<pre><strong>$ kubectl apply -f https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml</strong></pre>
<p>Here, whatever your host OS is, the command kubectl can fire any sub command for utilizing resources and managing systems. We use <kbd>kubectl</kbd> to apply the configuration of Calico to our new-born Kubernetes.</p>
<p>More advanced management of networking and Kubernetes add-ons will be discussed in <a href="dfc46490-f109-4f07-ba76-1a381b006d76.xhtml" target="_blank">Chapter 7</a>, <em>Building Kubernetes on GCP</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting a node involved</h1>
                </header>
            
            <article>
                
<p>Let's log in to your Kubernetes node to join the group controlled by kubeadm:</p>
<ol>
<li>First, enable and start the service, <kbd>kubelet</kbd>. Every Kubernetes machine should have <kbd>kubelet</kbd> running on it:</li>
</ol>
<pre style="padding-left: 90px"><strong>$ sudo systemctl enable kubelet &amp;&amp; sudo systemctl start kubelet</strong></pre>
<ol start="2">
<li>After that, fire the <kbd>kubeadm</kbd> join command with an input flag token and the IP address of the master, notifying the master that it is a secured and authorized node. You can get the token on the master node via the <kbd>kubeadm</kbd> command:</li>
</ol>
<pre style="padding-left: 90px"><strong>// on master node, list the token you have in the cluster</strong><br/><strong>$ sudo kubeadm token list</strong><br/><strong>TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS</strong><br/><strong>da3a90.9a119695a933a867   6h       2018-05-01T18:47:10-04:00   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token</strong></pre>
<ol start="3">
<li>In the preceding output, if <kbd>kubeadm init</kbd> succeeds, the default token will be generated. Copy the token and paste it onto the node, and then compose the following command:</li>
</ol>
<pre style="padding-left: 90px"><strong>// The master IP is 192.168.122.101, token is da3a90.9a119695a933a867, 6443 is the port of api server.</strong><br/><strong>$ sudo kubeadm join --token da3a90.9a119695a933a867 192.168.122.101:6443 --discovery-token-unsafe-skip-ca-verification</strong></pre>
<div class="packt_infobox">What if you call <kbd>kubeadm token list</kbd> to list the tokens, and see they are all expired? You can create a new one manually by this command: <kbd>kubeadm token create</kbd> .</div>
<ol start="4">
<li>Please make sure that the master's firewall doesn't block any traffic to port <kbd>6443</kbd>, which is for API server communication. Once you see the words <kbd>Successfully established connection</kbd> showing on the screen, it is time to check with the master if the group got the new member:</li>
</ol>
<pre style="padding-left: 90px"><strong>// fire kubectl subcommand on master</strong><br/><strong>$ kubectl get nodes</strong><br/><strong>NAME       STATUS    ROLES     AGE       VERSION</strong><br/><strong>ubuntu01   Ready     master    11h       v1.10.2</strong><br/><strong>ubuntu02   Ready     &lt;none&gt;    26s       v1.10.2</strong></pre>
<p>Well done! No matter if whether your OS is Ubuntu or CentOS, kubeadm is installed and kubelet is running. You can easily go through the preceding steps to build your Kubernetes cluster.</p>
<p>You may be wondering about the flag <kbd>discovery-token-unsafe-skip-ca-verification</kbd> used while joining the cluster. Remember the kubelet log that says the certificate file is not found? That's it, since our Kubernetes node is brand new and clean, and has never connected with the master before. There is no certificate file to find for verification. But now, because the node has shaken hands with the master, the file exists. We may join in this way (in some situation requiring rejoining the same cluster):</p>
<p><kbd>kubeadm join --token $TOKEN $MASTER_IPADDR:6443 --discovery-token-ca-cert-hash sha256:$HASH</kbd></p>
<p>The hash value can be obtained by the <kbd>openssl</kbd> command:</p>
<pre><strong>// rejoining the same cluster</strong><br/><strong>$ HASH=$(openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //')</strong><br/><strong>$ sudo kubeadm join --token da3a90.9a119695a933a867 192.168.122.101:6443 --discovery-token-ca-cert-hash sha256:$HASH</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>When kubeadm init sets up the master, there are six stages:</p>
<ol>
<li><strong>Generating certificate files and keys for services</strong>: Certificated files and keys are used for security management during cross-node communications. They are located in the <kbd>/etc/kubernetes/pki</kbd> directory. Take kubelet, for example. It cannot access the Kubernetes API server without passing the identity verification.</li>
<li><strong>Writing kubeconfig files</strong>: The <kbd>kubeconfig</kbd> files define permissions, authentication, and configurations for kubectl actions. In this case, the Kubernetes controller manager and scheduler have related <kbd>kubeconfig</kbd> files to fulfill any API requests.</li>
</ol>
<p> </p>
<ol start="3">
<li><strong>Creating service daemon YAML files</strong>: The service daemons under kubeadm's control are just like computing components running on the master. As with setting deployment configurations on disk, kubelet will make sure each daemon is active.</li>
<li><strong>Waiting for kubelet to be alive, running the daemons as pods</strong>: When kubelet is alive, it will boot up the service pods described in the files under the <kbd>/etc/kubernetes/manifests</kbd> directory. Moreover, kubelet guarantees to keep them activated, restarting the pod automatically if it crashes.</li>
<li><strong>Setting post-configuration for the cluster</strong>: Some cluster configurations still need to be set, such as configuring <strong>role-based accessing control</strong> (<strong>RBAC</strong>) rules, creating a namespace, and tagging the resources.</li>
<li><strong>Applying add-ons</strong>: DNS and proxy services can be added along with the kubeadm system.</li>
</ol>
<p>While the user enters kubeadm and joins the Kubernetes node, kubeadm will complete the first two stages like the master.</p>
<p>If you have faced a heavy and complicated set up procedure in earlier versions of Kubernetes, it is quite a relief to set up a Kubernetes cluster with kubeadm. kubeadm reduces the overhead of configuring each daemon and starting them one by one. Users can still do customization on kubelet and master services, by just modifying a familiar file, <kbd>10-kubeadm.conf</kbd> and the YAML files under <kbd>/etc/kubernetes/manifests</kbd>. Kubeadm not only helps to establish the cluster but also enhances security and availability, saving you time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>We talked about how to build a Kubernetes cluster. If you're ready to run your first application on it, check the last recipe in this chapter and run the container! And for advanced management of your cluster, you can also look at <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml" target="_blank">Chapter 8</a><span>,</span> <em>Advanced Cluster Administration</em>, of this book:</p>
<ul>
<li><em>Advanced settings in kubeconfig</em>, in <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml" target="_blank">Chapter 8</a>, <em>Advanced Cluster Administration</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the Kubernetes cluster on Linux via Ansible (kubespray)</h1>
                </header>
            
            <article>
                
<p>If you are familiar with configuration management, such as Puppet, Chef and Ansible, kubespray (<a href="https://github.com/kubernetes-incubator/kubespray">https://github.com/kubernetes-incubator/kubespray</a>) is the best choice to set up a Kubernetes cluster from scratch. It provides the Ansible playbook that supports the majority of Linux distributions and public clouds, such as AWS and GCP.</p>
<p>Ansible (<a href="https://www.ansible.com">https://www.ansible.com</a>) is a Python-based SSH automation tool that can configure Linux as your desired state based on the configuration, which is called playbook. This cookbook describes how to use kubespray to set up Kubernetes on Linux.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">As of May 2018, the latest version of kubespray is 2.5.0, which supports the following operation systems to install Kubernetes:</p>
<ul>
<li>RHEL/CentOS 7</li>
<li>Ubuntu 16.04 LTS</li>
</ul>
<div class="packt_infobox">
<p>According to the kubespray documentation, it also supports CoreOS and debian distributions. However, those distributions may need some additional steps or have technical difficulties. This cookbook uses CentOS 7 and Ubuntu 16.04 LTS.</p>
</div>
<p class="NormalPACKT">In addition, you need to install Ansible on your machine. Ansible works on Python 2.6, 2.7, and 3.5 or higher. macOS and Linux might be the best choice to install Ansible because Python is preinstalled by most of macOS and Linux distributions by default. In order to check which version of Python you have, open a Terminal and type the following command:</p>
<pre class="CommandLinePACKT"><strong>//Use capital V</strong><br/><strong>$ python -V</strong><br/><strong>Python 2.7.5</strong></pre>
<p>Overall, you need at least three machines, as mentioned in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Type of host</strong></p>
</td>
<td>
<p><strong>Recommended OS/Distribution</strong></p>
</td>
</tr>
<tr>
<td>
<p>Ansible</p>
</td>
<td>
<p>macOS or any Linux which has Python 2.6, 2.7, or 3.5</p>
</td>
</tr>
<tr>
<td>
<p>Kubernetes master</p>
</td>
<td>
<p>RHEL/CentOS 7 or Ubuntu 16.04 LTS</p>
</td>
</tr>
<tr>
<td>
<p>Kubernetes node</p>
</td>
<td>
<p>RHEL/CentOS 7 or Ubuntu 16.04 LTS</p>
</td>
</tr>
</tbody>
</table>
<p>There are some network communicating with each other, so you need to at least open a network port (for example, AWS Security Group or GCP Firewall rule) as:</p>
<ul>
<li><strong>TCP/22 (ssh)</strong>: Ansible to Kubernetes master/node host</li>
<li><strong>TCP/6443 (Kubernetes API server)</strong>: Kubernetes node to master</li>
<li><strong>Protocol 4 (IP encapsulated in IP)</strong>: Kubernetes master and node to each other by Calico</li>
</ul>
<div class="packt_tip">In Protocol 4 (<span>IP encapsulated in IP</span>), if you are using AWS, set an ingress rule to specify <kbd>aws ec2 authorize-security-group-ingress --group-id &lt;your SG ID&gt; --cidr &lt;network CIDR&gt; --protocol 4</kbd>. <span>I</span><span>n addition, if you are using GCP, set the firewall rule to specify as <kbd>cloud compute firewall-rules create allow-calico --allow 4 --network &lt;your network name&gt; --source-ranges &lt;network CIDR&gt;</kbd></span>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing pip</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">The easiest way to install Ansible, is to use pip, the Python package manager. Some of newer versions of Python have <kbd>pip</kbd> already (Python 2.7.9 or later and Python 3.4 or later):</p>
<ol>
<li class="NormalPACKT">To confirm whether <kbd>pip</kbd> is installed or not, similar to the Python command, use <kbd>-V</kbd>:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px"><strong>//use capital V</strong><br/><strong>$ pip -V</strong><br/><strong>pip 9.0.1 from /Library/Python/2.7/site-packages (python 2.7)</strong></pre>
<ol start="2">
<li class="NormalPACKT">On the other hand, if you see the following result, you need to install <kbd>pip</kbd>:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px"><strong>//this result shows you don't have pip yet</strong><br/><strong>$ pip -V</strong><br/><strong>-bash: pip: command not found</strong></pre>
<ol start="3">
<li class="NormalPACKT">In order to install pip, download <kbd>get-pip.py</kbd> and install by using the following command:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px"><strong>//download pip install script</strong><br/><strong>$ curl -LO https://bootstrap.pypa.io/get-pip.py</strong><br/><br/><br/><strong>//run get-pip.py by privileged user (sudo)</strong><br/><strong>$ sudo python get-pip.py </strong><br/><strong>Collecting pip</strong><br/><strong>  Downloading pip-9.0.1-py2.py3-none-any.whl (1.3MB)</strong><br/><strong>    100% |################################| 1.3MB 779kB/s </strong><br/><strong>Collecting wheel</strong><br/><strong>  Downloading wheel-0.30.0-py2.py3-none-any.whl (49kB)</strong><br/><strong>    100% |################################| 51kB 1.5MB/s </strong><br/><strong>Installing collected packages: pip, wheel</strong><br/><strong>Successfully installed pip-9.0.1 wheel-0.30.0</strong><br/><br/><br/><strong>//now you have pip command</strong><br/><strong>$ pip -V</strong><br/><strong>pip 9.0.1 from /usr/lib/python2.7/site-packages (python 2.7)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Ansible</h1>
                </header>
            
            <article>
                
<p>Perform the following steps to install Ansible:</p>
<ol>
<li class="NormalPACKT">Once you have installed <kbd>pip</kbd>, you can install Ansible with the following command:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px"><strong>//ran by privileged user (sudo)</strong><br/><strong>$ sudo pip install ansible</strong></pre>
<div class="NormalPACKT packt_infobox" style="padding-left: 90px"><kbd>pip</kbd> scans your Python and installs the necessary libraries for Ansible, so it may take a few minutes to complete.</div>
<ol start="2">
<li>Once you have successfully installed Ansible by <kbd>pip</kbd>, you can verify it with the following command and see output as this:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px"><strong>$ which ansible</strong><br/><strong>/usr/bin/ansible</strong><br/><br/><strong>$ ansible --version</strong><br/><strong>ansible 2.4.1.0</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing python-netaddr</h1>
                </header>
            
            <article>
                
<p>Next, according to kubespray's documentation (<a href="https://github.com/kubernetes-incubator/kubespray#requirements">https://github.com/kubernetes-incubator/kubespray#requirements</a>), it needs the <kbd>python-netaddr</kbd> package. This package can also be installed by pip, as shown in the following code:</p>
<pre class="CommandLinePACKT"><strong>$ sudo pip install netaddr</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up ssh public key authentication</h1>
                </header>
            
            <article>
                
<p>One more thing, as mentioned previously, Ansible is actually the ssh automation tool. If you log on to host via ssh, you have to have an appropriate credential (user/password or ssh public key) to the target machines. In this case, the target machines mean the Kubernetes master and nodes.</p>
<p>Due to security reasons, especially in the public cloud, Kubernetes uses only the ssh public key authentication instead of ID/password authentication.</p>
<p>To follow the best practice, let's copy the ssh public key from your Ansible machine to the Kubernetes master/node machines:</p>
<div class="packt_infobox">
<p>If you've already set up ssh public key authentication between the Ansible machine to Kubernetes candidate machines, you can skip this step.</p>
</div>
<ol>
<li>In order to create an ssh public/private key pair from your Ansible machine, type the following command:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px"><strong>//with –q means, quiet output</strong><br/><strong>$ ssh-keygen -q</strong></pre>
<ol start="2">
<li>It will ask you to set a passphrase. You may set or skip (empty) this, but you have to remember it.</li>
<li>Once you have successfully created a key pair, you can see the private key as <kbd>~/.ssh/id_rsa</kbd> and public key as <kbd>~/.ssh/id_rsa.pub</kbd>. You need to append the public key to the target machine under <kbd>~/.ssh/authorized_keys</kbd>, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/669c5f59-6af8-4865-a8f6-5d4b7dea7d7b.png"/></div>
<ol start="4">
<li>You need to copy and paste your public key to all Kubernetes master and node candidate machines.</li>
<li>To make sure your ssh public key authentication works, just ssh from the Ansible machine to the target host that won't ask for your logon password, as here:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px"><strong>//use ssh-agent to remember your private key and passphrase (if you set)</strong><br/><strong>ansible_machine$ ssh-agent bash</strong><br/><strong>ansible_machine$ ssh-add</strong><br/><strong>Enter passphrase for /home/saito/.ssh/id_rsa: Identity added: /home/saito/.ssh/id_rsa (/home/saito/.ssh/id_rsa)</strong><br/><br/><br/><strong>//logon from ansible machine to k8s machine which you copied public key</strong><br/><strong>ansible_machine$ ssh 10.128.0.2</strong><br/><strong>Last login: Sun Nov  5 17:05:32 2017 from 133.172.188.35.bc.googleusercontent.com</strong><br/><strong>k8s-master-1$</strong></pre>
<p class="NormalPACKT">Now you are all set! Let's set up Kubernetes using kubespray (Ansible) from scratch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>kubespray is provided through the GitHub repository (<a href="https://github.com/kubernetes-incubator/kubespray/tags">https://github.com/kubernetes-incubator/kubespray/tags</a>), as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/59438d0b-62fd-4241-896c-f27f0e78185c.png" style="width:43.92em;height:23.00em;"/></div>
<p>Because kubespray is an Ansible playbook, not a binary, you can download the latest version <span>(as of May 2018, version 2.5.0 is the latest)</span> of the <kbd>zip</kbd> or <kbd>tar.gz</kbd> to your Ansible machine directly and unarchive it with the following command:</p>
<pre><strong>//download tar.gz format</strong><br/><strong>ansible_machine$ curl -LO https://github.com/kubernetes-incubator/kubespray/archive/v2.5.0.tar.gz</strong><br/><br/><br/><strong>//untar</strong><br/><strong>ansible_machine$ tar zxvf v2.5.0.tar.gz </strong><br/><br/><br/><strong>//it unarchives under kubespray-2.</strong><strong>5.0 directory</strong><br/><strong>ansible_machine$ ls -F</strong><br/><strong>get-pip.py  kubespray-2.5.0/  v2.5.0.tar.gz</strong><br/><br/><br/><strong>//change to kubespray-2.5.0 directory</strong><br/><strong>ansible_machine$ cd kubespray-2.5.0/</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Maintaining the Ansible inventory</h1>
                </header>
            
            <article>
                
<p>In order to perform the Ansible playbook, you need to maintain your own inventory file, which contains target machine IP addresses:</p>
<ol>
<li>There is a sample inventory file under the inventory directory, so you can copy it by using the following:</li>
</ol>
<pre style="padding-left: 90px"><strong>//copy sample to mycluster</strong><br/><strong>ansible_machine$ cp -rfp inventory/sample inventory/mycluster<br/></strong><br/><strong>//edit hosts.ini</strong><br/><strong>ansible_machine$ vi inventory/mycluster/hosts.ini </strong></pre>
<ol start="2">
<li>In this cookbook, we are using target machines that have the following IP addresses:
<ul>
<li>Kubernetes master : <kbd>10.128.0.2</kbd></li>
<li>Kubernetes node : <kbd>10.128.0.4</kbd></li>
</ul>
</li>
<li>In this case, <kbd>hosts.ini</kbd> should be in the following format:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ea729763-477f-49f1-b64e-4ef047102ad7.png"/></div>
<ol start="4">
<li>Please change the IP address to match your environment.</li>
</ol>
<p>Note that hostname (<kbd>my-master-1</kbd> and <kbd>my-node-1</kbd>) will be set by the kubespray playbook based on this <kbd>hosts.ini</kbd>, so feel free to assign a meaningful hostname.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the Ansible ad hoc command to test your environment</h1>
                </header>
            
            <article>
                
<p>Before running the kubespray playbook, let's check whether <kbd>hosts.ini</kbd> and Ansible itself work properly or not:</p>
<ol>
<li>To do that, use the Ansible ad hoc command, using the ping module, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f64fdf05-c6ff-4b77-9fc9-23af593c08aa.png"/></div>
<ol start="2">
<li>This result indicates <kbd>SUCCESS</kbd>. But if you see the following error, probably the IP address is wrong or the target machine is down, so please the check target machine first:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/13ae1baa-a1ae-4655-8ea9-426410983ed7.png"/></div>
<ol start="3">
<li>Next, check your authority whether you can escalate a privilege on the target machine or not. In other words, whether you can run <kbd>sudo</kbd> or not<span>. This is because you will need to install Kubernetes, Docker, and some related binaries, and configurations that need a root privilege. To confirm that, add the</span> <kbd>-b</kbd> <span>(become) option, as shown in the following screenshot:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0c2f06a0-2e53-40e7-be10-cb2c28eb05ae.png"/></div>
<ol start="4">
<li>With the <kbd>-b</kbd> option, it actually tries to perform sudo on the target machine. If you see <kbd>SUCCESS</kbd>, you are all set! Go to the <em>How it works…</em> section to run kubespray.</li>
</ol>
<p>If you're unfortunate enough to see some errors, please refer to the following section to solve Ansible issues.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ansible troubleshooting</h1>
                </header>
            
            <article>
                
<p>The ideal situation would be to use the same Linux distribution, version, settings, and logon user. However, the environment will be different based on policy, compatibility, and other reasons. Ansible is flexible and can support many use cases to run <kbd>ssh</kbd> and <kbd>sudo</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Need to specify a sudo password</h1>
                </header>
            
            <article>
                
<p>Based on your Linux machine setting, you may see the following error when adding the <kbd>-b</kbd> option. In this case, you need to type your password while running the <kbd>sudo</kbd> command:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8cb344cb-f98a-4b6d-9526-bac1590dadbe.png"/></div>
<p>In this case, add <kbd>-K</kbd> (ask for the <kbd>sudo</kbd> password) and run again. It will ask for your sudo password when running the Ansible command, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f2efbf16-c053-420d-9fe7-8ee7933675fd.png"/></div>
<div class="packt_tip">
<p>If your Linux uses the <kbd>su</kbd> command instead of <kbd>sudo</kbd>, adding <kbd>--become-method=su</kbd> to run the Ansible command could help. Please read the Ansible documentation for more details : <a href="http://docs.ansible.com/ansible/latest/become.html">http://docs.ansible.com/ansible/latest/become.html</a></p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Need to specify different ssh logon user</h1>
                </header>
            
            <article>
                
<p>Sometimes you may need to ssh to target machines using a different logon user. In this case, you can append the <kbd>ansible_user</kbd> parameter to an individual host in <kbd>hosts.ini</kbd>. For example:</p>
<ul>
<li>Use the username <kbd>kirito</kbd> to <kbd>ssh</kbd> to <kbd>my-master-1</kbd></li>
<li>Use the username <kbd>asuna</kbd> to <kbd>ssh</kbd> to <kbd>my-node-1</kbd></li>
</ul>
<p>In this case, change <kbd>hosts.ini</kbd>, as shown in the following code:</p>
<pre><strong>my-master-1 ansible_ssh_host=10.128.0.2 ansible_user=kirito</strong><br/><strong>my-node-1 ansible_ssh_host=10.128.0.4 ansible_user=asuna</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Need to change ssh port</h1>
                </header>
            
            <article>
                
<p>Another scenario is where you may need to run the ssh daemon on some specific port number rather than the default port number <kbd>22</kbd>. Ansible also supports this scenario and uses the <kbd>ansible_port</kbd> parameter to the individual host in <kbd>hosts.ini</kbd>, as shown in the following code (in the example, the <kbd>ssh</kbd> daemon is running at <kbd>10022</kbd> on <kbd>my-node-1</kbd>):</p>
<pre><strong>my-master-1 ansible_ssh_host=10.128.0.2</strong><br/><strong>my-node-1 ansible_ssh_host=10.128.0.4 ansible_port=10022</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"> Common ansible issue</h1>
                </header>
            
            <article>
                
<p>Ansible is flexible enough to support any other situations. If you need any specific parameters to customize the ssh logon for the target host, read the Ansible inventory documentation to find a specific parameter: <a href="http://docs.ansible.com/ansible/latest/intro_inventory.html">http://docs.ansible.com/ansible/latest/intro_inventory.html</a></p>
<p>In addition, Ansible has a configuration file, <kbd>ansible.cfg</kbd>, on top of the <kbd>kubespray</kbd> directory. It defines common settings for Ansible. For example, if you are using a very long username that usually causes an Ansible error, change <kbd>ansible.cfg</kbd> to set <kbd>control_path</kbd> to solve the issue, as shown in the following code:</p>
<pre><strong>[ssh_connection]</strong><br/><strong>control_path = %(directory)s/%%h-%%r</strong></pre>
<p>If you plan to set up more than <kbd>10</kbd> nodes, you may need to increase ssh simultaneous sessions. In this case, adding the <kbd>forks</kbd> parameter also requires you to increase the ssh timeout from <kbd>10</kbd> seconds to <kbd>30</kbd> seconds by adding the timeout parameter, as shown in the following code:</p>
<pre><strong>[ssh_connection]</strong><br/><strong>forks = 50</strong><br/><strong>timeout = 30</strong></pre>
<p>The following screenshot contains all of the preceding configurations in <kbd>ansible.cfg</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f7643fd9-85af-40ff-804c-f62fadabb2f3.png"/></div>
<p>For more details, please visit the Ansible configuration documentation at <a href="http://docs.ansible.com/ansible/latest/intro_configuration.html">http://docs.ansible.com/ansible/latest/intro_configuration.html</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">Now you can start to run the kubepray playbook:</p>
<ol>
<li class="NormalPACKT">You've already created an inventory file as <kbd>inventory/mycluster/hosts.ini</kbd>. Other than <kbd>hosts.ini</kbd>, you need to check and update global variable configuration files at <kbd>inventory/mycluster/group_vars/all.yml</kbd>.</li>
<li class="NormalPACKT">There are a lot of variables defined, but at least one variable, <kbd><span class="ItalicsPACKT">bootstrap_os</span></kbd> , needs to be changed from <kbd>none</kbd> to your target Linux machine. If you are using RHEL/CentOS7, set <kbd>bootstrap_os</kbd> as <kbd>centos</kbd>. If you are using Ubuntu 16.04 LTS, set <kbd>bootstrap_os</kbd> as <kbd>ubuntu</kbd> as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6d901bec-7cf7-4f38-965b-8699f376d321.png"/></div>
<div class="packt_tip">
<p>You can also update other variables, such as <kbd>kube_version</kbd>, to change or install a Kubernetes version. For more details, read the documentation at <a href="https://github.com/kubernetes-incubator/kubespray/blob/master/docs/vars.md">https://github.com/kubernetes-incubator/kubespray/blob/master/docs/vars.md</a>.<a href="https://github.com/kubernetes-incubator/kubespray/blob/master/docs/vars.md"/></p>
</div>
<ol start="3">
<li class="NormalPACKT">Finally, you can execute the playbook. Use the <kbd><span class="ScreenTextPACKT">ansible-playbook</span></kbd> command instead of the Ansible command. Ansible-playbook runs multiple Ansible modules based on tasks and roles that are defined in the playbook.</li>
<li class="NormalPACKT">To run the kubespray playbook, type the ansible-playbook command with the following parameters:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px"><strong>//use –b (become), -i (inventory) and specify cluster.yml as playbook</strong><br/><strong>$ ansible-playbook -b -i inventory/mycluster/hosts.ini cluster.yml <br/></strong></pre>
<div class="packt_infobox">
<p>The ansible-playbook argument parameter is the same as the Ansible command. So, if you need to use <kbd>-K</kbd> (ask for the <kbd>sudo</kbd> password) or <kbd>--become-method=su</kbd>, you need to specify for ansible-playbook as well.</p>
</div>
<ol start="5">
<li class="NormalPACKT">It takes around 5 to 10 minutes to complete based on the machine spec and network bandwidth. But eventually you can see <kbd>PLAY RECAP</kbd>, as shown in the following screenshot, to see whether it has succeeded or not:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1402d1ac-b161-4138-a5ad-5be4b9470580.png" style="width:58.17em;height:32.17em;"/></div>
<ol start="6">
<li class="NormalPACKT">If you see <kbd>failed=0</kbd> like in the preceding screenshot, you have been successful in setting up a Kubernetes cluster. You can ssh to the Kubernetes master machine and run the <kbd>/usr/local/bin/kubectl</kbd> command to see the status, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8a93387d-c43c-4f24-a7d3-4c2115f4fecd.png" style="width:59.50em;height:24.92em;"/></div>
<ol start="7">
<li class="NormalPACKT">The preceding screenshot shows that you have been successful in setting up the Kubernetes version 1.10.2 master and node. You can continue to use the <kbd>kubectl</kbd> command to configure you Kubernetes cluster in the following chapters.</li>
<li class="NormalPACKT">Unfortunately, if you see a failed count of more than 0, the Kubernetes cluster has probably not been set up correctly. Because failure is caused by many reasons, there is no single solution. It is recommended that you append the verbose option <kbd>-v</kbd> to see more detailed output from Ansible, as shown in the following code:</li>
</ol>
<pre class="CommandLinePACKT" style="padding-left: 90px"><strong>//use –b (become), -i (inventory) and –v (verbose)</strong><br/><strong>$ ansible-playbook -v -b -i inventory/mycluster/hosts.ini cluster.yml</strong></pre>
<ol start="9">
<li>If the failure is timeout, just retrying the ansible-playbook command again may solve it. Because Ansible is designed as an idempotency, if you re-perform the ansible-playbook command twice or more, Ansible still can configure correctly.</li>
</ol>
<p> </p>
<ol start="10">
<li>If the failure is change target IP address after you run ansible-playbook (for example, re-using the Ansible machine to set up another Kubernetes cluster), you need to clean up the fact cache file. It is located under <kbd>/tmp</kbd> directory, so you just delete this file, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6990eb72-ec1c-4471-8af9-6cb8fdc41985.png" style="width:44.00em;height:18.67em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">This section describes how to set up the Kubernetes cluster on the Linux OS using kubespray. It is the Ansible playbook that supports major Linux distribution. Ansible is simple, but due to supporting any situation and environment, you need to care about some different use cases. Especially with ssh and sudo-related configurations, you need to understand Ansible deeper to fit it with your environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running your first container in Kubernetes</h1>
                </header>
            
            <article>
                
<p>Congratulations! You've built your own Kubernetes cluster in the previous recipes. Now, let's get on with running your very first container, nginx (<a href="http://nginx.org/">http://nginx.org/</a>), which is an open source reverse proxy server, load balancer, and web server. Along with this recipe, you will create a simple nginx application and expose it to the outside world.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Before you start to run your first container in Kubernetes, it's better to check if your cluster is in a healthy mode. A checklist showing the following items would make your <kbd>kubectl</kbd> sub commands stable and successful, without unknown errors caused by background services:</p>
<ol>
<li>Checking the master daemons. Check whether the Kubernetes components are running:</li>
</ol>
<pre style="padding-left: 90px"><strong>// get the components status</strong><br/><strong>$ kubectl get cs</strong><br/><strong>NAME                 STATUS    MESSAGE              ERROR</strong><br/><strong>controller-manager   Healthy   ok</strong><br/><strong>scheduler            Healthy   ok</strong><br/><strong>etcd-0               Healthy   {"health": "true"}</strong></pre>
<ol start="2">
<li>Check the status of the Kubernetes master:</li>
</ol>
<pre style="padding-left: 90px"><strong>// check if the master is running</strong><br/><strong>$ kubectl cluster-info</strong><br/><strong>Kubernetes master is running at https://192.168.122.101:6443</strong><br/><strong>KubeDNS is running at https://192.168.122.101:6443/api/v1/namespaces/kube-system/services/kube-dns/proxy<br/><br/>To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.<br/></strong></pre>
<ol start="3">
<li>Check whether all the nodes are ready:</li>
</ol>
<pre style="padding-left: 90px"><strong>$ kubectl get nodes</strong><br/><strong>NAME       STATUS    ROLES     AGE       VERSION</strong><br/><strong>ubuntu01   Ready     master    20m       v1.10.2</strong><br/><strong>ubuntu02   Ready     &lt;none&gt;    2m        v1.10.2</strong></pre>
<div class="packt_infobox">
<p>Ideal results should look like the preceding outputs. You can successfully fire the <kbd>kubectl</kbd> command and get the response without errors. If any one of the checked items failed to meet the expectation, check out the settings in the previous recipes based on the management tool you used.</p>
</div>
<ol start="4">
<li>Check the access permission of the Docker registry, as we will use the official free image as an example. If you want to run your own application, be sure to dockerize it first! What you need to do for your custom application is to write a Dockerfile (<a href="https://docs.docker.com/engine/reference/builder/">https://docs.docker.com/engine/reference/builder/</a>), and build and push it into the public or private Docker registry.</li>
</ol>
<div class="packt_tip">
<div>
<p><span class="packt_screen">Test your node connectivity with the public/private Docker registry</span></p>
<p>On your node, try the Docker pull nginx command to test whether you can pull the image from the Docker Hub. If you're behind a proxy, please add <kbd>HTTP_PROXY</kbd> into your Docker configuration file(<a href="https://docs.docker.com/engine/admin/systemd/#httphttps-proxy">https://docs.docker.com/engine/admin/systemd/#httphttps-proxy</a>). If you want to run the image from the private repository in the Docker Hub, or the image from the private Docker registry, a Kubernetes secret is required. Please check <em>Working with secrets</em>, in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml" target="_blank">Chapter 2</a>, <em>Working through Kubernetes Concepts</em>, <em>for the instructions.</em></p>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>We will use the official Docker image of nginx as an example. The image is provided in the Docker Hub (<a href="https://store.docker.com/images/nginx">https://store.docker.com/images/nginx</a>), and also the Docker Store (<a href="https://hub.docker.com/_/nginx/">https://hub.docker.com/_/nginx/</a>).</p>
<p>Many of the official and public images are available on the Docker Hub or Docker Store so that you do not need to build them from scratch. Just pull them and set up your custom setting on top of them.</p>
<div class="packt_tip">
<p><span class="packt_screen">Docker Store versus Docker Hub</span></p>
<p>As you may be aware, there is a more familiar official repository, Docker Hub, which was launched for the community for sharing the based image. Compared with the Docker Hub, the Docker Store is focused on enterprise applications. It provides a place for enterprise-level Docker images, which could be free or paid for software. You may feel more confident in using a more reliable image on the Docker Store.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running a HTTP server (nginx)</h1>
                </header>
            
            <article>
                
<p>On the Kubernetes master, we <span>can </span>use <kbd>kubectl run</kbd> to create a certain number of containers. The Kubernetes master will then schedule the pods for the nodes to run, with general command formatting, as follows:</p>
<pre><strong>$ kubectl run &lt;replication controller name&gt; --image=&lt;image name&gt; --replicas=&lt;number of replicas&gt; [--port=&lt;exposing port&gt;]</strong></pre>
<p>The following example will create two replicas with the name <kbd>my-first-nginx</kbd> from the nginx image and expose port <kbd>80</kbd>. We <span>can </span>deploy one or more containers in what is referred to as a pod. In this case, we will deploy one container per pod. Just like a normal Docker behavior, if the nginx image doesn't exist locally, it will pull it from the Docker Hub by default:  </p>
<pre><strong>// run a deployment with 2 replicas for the image nginx and expose the container port 80</strong><br/><strong>$ kubectl run my-first-nginx --image=nginx --replicas=2 --port=80</strong><br/><strong>deployment "my-first-nginx" created</strong></pre>
<div class="packt_tip">
<p><span class="packt_screen">The name of deployment &lt;my-first-nginx&gt; cannot be duplicated</span></p>
<p>The resource (pods, services, deployment, and so on) in one Kubernetes namespace cannot be duplicated. If you run the preceding command twice, the following error will pop up:  </p>
<pre><strong>Error from server (AlreadyExists): deployments.extensions "my-first-nginx" already exists</strong></pre></div>
<p>Let's move on and see the current status of all the pods by <kbd>kubectl get pods</kbd>. Normally the status of the pods will hold on Pending for a while, since it takes some time for the nodes to pull the image from the registry:</p>
<pre><strong>// get all pods</strong><br/><strong>$ kubectl get pods</strong><br/><strong>NAME                              READY     STATUS    RESTARTS   AGE</strong><br/><strong>my-first-nginx-7dcd87d4bf-jp572   1/1       Running   0          7m</strong><br/><strong>my-first-nginx-7dcd87d4bf-ns7h4   1/1       Running   0          7m</strong></pre>
<div class="packt_tip">
<p><span class="packt_screen">If the pod status is not running for a long time</span></p>
<p>You could always use kubectl get pods to check the current status of the pods, and kubectl describe pods <kbd>$pod_name</kbd> to check the detailed information in a pod. If you make a typo of the image name, you might get the <kbd>ErrImagePull</kbd> error message, and if you are pulling  the images from a private repository or registry without proper credentials, you might get the <kbd>ImagePullBackOff</kbd> message. If you get the <kbd>Pending</kbd> status for a long time and check out the node capacity, make sure you don't run too many replicas that exceed the node capacity. If there are other unexpected error messages, you could either stop the pods or the entire replication controller to force the master to schedule the tasks again.</p>
</div>
<p>You can also check the details about the deployment to see whether all the pods are ready:</p>
<pre><strong>// check the status of your deployment</strong><br/><strong>$ kubectl get deployment</strong><br/><strong>NAME             DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</strong><br/><strong>my-first-nginx   2         2         2            2           2m</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exposing the port for external access</h1>
                </header>
            
            <article>
                
<p>We might also want to create an external IP address for the nginx deployment. On cloud providers that support an external load balancer (such as Google compute engine), using the <kbd>LoadBalancer</kbd> type will provision a load balancer for external access. On the other hand, you can still expose the port by creating a Kubernetes service as follows, even though you're not running on platforms that support an external load balancer. We'll describe how to access this externally later:</p>
<pre><strong>// expose port 80 for replication controller named my-first-nginx</strong><br/><strong>$ kubectl expose deployment my-first-nginx --port=80 --type=LoadBalancer</strong><br/><strong>service "my-first-nginx" exposed</strong></pre>
<p>We can see the service status we just created:</p>
<pre><strong>// get all services</strong><br/><strong>$ kubectl get service</strong><br/><strong>NAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</strong><br/><strong>kubernetes       ClusterIP      10.96.0.1       &lt;none&gt;        443/TCP        2h</strong><br/><strong>my-first-nginx   LoadBalancer   10.102.141.22   &lt;pending&gt;     80:31620/TCP   3m</strong></pre>
<p>You may find an additional service named <kbd>kubernetes</kbd> if the service daemon run as a container (for example, using kubeadm as a management tool). It is for exposing the REST API of the Kubernetes API server internally. The pending state of <kbd>my-first-nginx</kbd> service's external IP indicates that it is waiting for a specific public IP from cloud provider. Take a look at <a href="b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml" target="_blank">Chapter 6</a>, <span><em>Building Kubernetes on AWS</em>,</span> and <a href="dfc46490-f109-4f07-ba76-1a381b006d76.xhtml" target="_blank">Chapter 7</a>, <span><em>Building Kubernetes on GCP</em>,</span> for more details.</p>
<p>Congratulations! You just ran your first container with a Kubernetes pod and exposed port <kbd>80</kbd> with the Kubernetes service.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stopping the application</h1>
                </header>
            
            <article>
                
<p>We <span>can </span>stop the application using commands such as the delete deployment and service. Before this, we suggest you read through the following code first to understand more about how it works:</p>
<pre><strong>// stop deployment named my-first-nginx</strong><br/><strong>$ kubectl delete deployment my-first-nginx</strong><br/><strong>deployment<span>.extensions </span>"my-first-nginx" deleted</strong><br/><br/><strong>// stop service named my-first-nginx</strong><br/><strong>$ kubectl delete service my-first-nginx</strong><br/><strong>service "my-first-nginx" deleted</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>Let's take a look at the insight of the service using describe in the <kbd>kubectl</kbd> command. We will create one Kubernetes service with the type <kbd>LoadBalancer</kbd>, which will dispatch the traffic into two endpoints, <kbd>192.168.79.9</kbd> and <kbd>192.168.79.10</kbd> with port <kbd>80</kbd>:</p>
<pre><strong>$ kubectl describe service my-first-nginx</strong><br/><strong>Name:                     my-first-nginx</strong><br/><strong>Namespace:                default</strong><br/><strong>Labels:                   run=my-first-nginx</strong><br/><strong>Annotations:              &lt;none&gt;</strong><br/><strong>Selector:                 run=my-first-nginx</strong><br/><strong>Type:                     LoadBalancer</strong><br/><strong>IP:                       10.103.85.175</strong><br/><strong>Port:                     &lt;unset&gt;  80/TCP</strong><br/><strong>TargetPort:               80/TCP</strong><br/><strong>NodePort:                 &lt;unset&gt;  31723/TCP</strong><br/><strong>Endpoints:                192.168.79.10:80,192.168.79.9:80</strong><br/><strong>Session Affinity:         None</strong><br/><strong>External Traffic Policy:  Cluster</strong><br/><strong>Events:                   &lt;none&gt;</strong></pre>
<p>The port here is an abstract service port, which will allow any other resources to access the service within the cluster. The <kbd>nodePort</kbd> will be indicating the external port to allow external access. The <kbd>targetPort</kbd> is the port the container allows traffic into; by default, it will be the same port.</p>
<p>In the following diagram, external access will access the service with <kbd>nodePort</kbd>. The service acts as a load balancer to dispatch the traffic to the pod using port <kbd>80</kbd>. The pod will then pass through the traffic into the corresponding container using <kbd>targetPort 80</kbd>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/5f7b733a-b8aa-47b9-904e-137cc1e958f6.png" style="width:47.08em;height:29.83em;"/></div>
<p>In any nodes or master, once the inter-connection network is set up, you should be able to access the nginx service using <kbd>ClusterIP</kbd> <kbd>192.168.61.150</kbd> with port <kbd>80</kbd>:</p>
<pre><strong>// curl from service IP</strong><br/><strong>$ curl 10.103.85.175:80</strong><br/><strong>&lt;!DOCTYPE html&gt;</strong><br/><strong>&lt;html&gt;</strong><br/><strong>&lt;head&gt;</strong><br/><strong>&lt;title&gt;Welcome to nginx!&lt;/title&gt;</strong><br/><strong>&lt;style&gt;</strong><br/><strong>    body {</strong><br/><strong>        width: 35em;</strong><br/><strong>        margin: 0 auto;</strong><br/><strong>        font-family: Tahoma, Verdana, Arial, sans-serif;</strong><br/><strong>    }</strong><br/><strong>&lt;/style&gt;</strong><br/><strong>&lt;/head&gt;</strong><br/><strong>&lt;body&gt;</strong><br/><strong>&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</strong><br/><strong>&lt;p&gt;If you see this page, the nginx web server is successfully installed and</strong><br/><strong>working. Further configuration is required.&lt;/p&gt;</strong><br/><strong>&lt;p&gt;For online documentation and support please refer to</strong><br/><strong>&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</strong><br/><strong>Commercial support is available at</strong><br/><strong>&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</strong><br/><strong>&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;</strong><br/><strong>&lt;/body&gt;</strong><br/><strong>&lt;/html&gt;</strong></pre>
<p>It will be the same result if we <kbd>curl</kbd> to the target port of the pod directly:</p>
<pre><strong>// curl from endpoint, the content is the same as previous nginx html</strong><br/><strong>$ curl 192.168.79.10:80</strong><br/><strong>&lt;!DOCTYPE html&gt;</strong><br/><strong>&lt;html&gt;</strong><br/>...</pre>
<p>If you'd like to try out external access, use your browser to access the external IP address. Please note that the external IP address depends on which environment you're running in.</p>
<p>In the Google compute engine, you could access it via a <kbd>ClusterIP</kbd> with a proper rewall rules setting:</p>
<pre><strong>$ curl http://&lt;clusterIP&gt;</strong></pre>
<p>In a custom environment, such as on-premise data center, you could go through the IP address of nodes to access :</p>
<pre><strong>$ curl http://&lt;nodeIP&gt;:&lt;nodePort&gt;</strong></pre>
<p>You should be able to see the following page using a web browser:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/a3d0dfde-0b38-463c-809f-65f6cc14f357.png" style="width:41.08em;height:16.50em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>We have run our very first container in this section. Go ahead and read the next chapter to aquire more knowledge about Kubernetes:</p>
<ul>
<li><a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml" target="_blank">Chapter 2</a>, <em>Walking through Kubernetes Concepts</em></li>
</ul>


            </article>

            
        </section>
    </body></html>