["```\napiVersion: v1 \nkind: ReplicationController \nmetadata: \n  name: node-js-scale \n  labels: \n    name: node-js-scale \nspec: \n  replicas: 1 \n  selector: \n    name: node-js-scale \n  template: \n    metadata: \n      labels: \n        name: node-js-scale \n    spec: \n      containers: \n      - name: node-js-scale \n        image: jonbaier/pod-scaling:0.1 \n        ports: \n        - containerPort: 80\n```", "```\napiVersion: v1 \nkind: Service \nmetadata: \n  name: node-js-scale \n  labels: \n    name: node-js-scale \nspec: \n  type: LoadBalancer \n  sessionAffinity: ClientIP \n  ports: \n  - port: 80 \n  selector: \n    name: node-js-scale\n```", "```\n$ kubectl create -f pod-scaling-controller.yaml\n$ kubectl create -f pod-scaling-service.yaml\n```", "```\n$ kubectl get pods -l name=node-js-scale\n```", "```\n$ kubectl scale --replicas=3 rc/node-js-scale\n```", "```\n$ kubectl rolling-update node-js-scale --image=jonbaier/pod-scaling:0.2 --update-period=\"2m\"\n```", "```\n$ kubectl get pods -l name=node-js-scale\n```", "```\n$ kubectl rolling-update node-js-scale node-js-scale-v2.0 --image=jonbaier/pod-scaling:0.2 --update-period=\"2m\"\n```", "```\napiVersion: v1 \nkind: Service \nmetadata: \n  name: node-js-scale-ab \n  labels: \n    service: node-js-scale-ab \nspec: \n  type: LoadBalancer \n  ports: \n  - port: 80 \n  sessionAffinity: ClientIP \n  selector: \n    service: node-js-scale-ab\n```", "```\n$ kubectl create -f pod-AB-service.yaml\n```", "```\napiVersion: v1 \nkind: ReplicationController \nmetadata: \n  name: node-js-scale-a \n  labels: \n    name: node-js-scale-a \n    version: \"0.2\" \n    service: node-js-scale-ab \nspec: \n  replicas: 2 \n  selector: \n    name: node-js-scale-a \n    version: \"0.2\" \n    service: node-js-scale-ab \n  template: \n    metadata: \n      labels: \n        name: node-js-scale-a \n        version: \"0.2\" \n        service: node-js-scale-ab \n    spec: \n      containers: \n      - name: node-js-scale \n        image: jonbaier/pod-scaling:0.2 \n        ports: \n        - containerPort: 80 \n        livenessProbe: \n          # An HTTP health check \n          httpGet: \n            path: / \n            port: 80 \n          initialDelaySeconds: 30 \n          timeoutSeconds: 5 \n        readinessProbe: \n          # An HTTP health check \n          httpGet: \n            path: / \n            port: 80 \n          initialDelaySeconds: 30 \n          timeoutSeconds: 1\napiVersion: v1 \nkind: ReplicationController \nmetadata: \n  name: node-js-scale-b \n  labels: \n    name: node-js-scale-b \n    version: \"0.3\" \n    service: node-js-scale-ab \nspec: \n  replicas: 2 \n  selector: \n    name: node-js-scale-b \n    version: \"0.3\" \n    service: node-js-scale-ab \n  template: \n    metadata: \n      labels: \n        name: node-js-scale-b \n        version: \"0.3\" \n        service: node-js-scale-ab \n    spec: \n      containers: \n      - name: node-js-scale \n        image: jonbaier/pod-scaling:0.3 \n        ports: \n        - containerPort: 80 \n        livenessProbe: \n          # An HTTP health check \n          httpGet: \n            path: / \n            port: 80 \n          initialDelaySeconds: 30 \n          timeoutSeconds: 5 \n        readinessProbe: \n          # An HTTP health check \n          httpGet: \n            path: / \n            port: 80 \n          initialDelaySeconds: 30 \n          timeoutSeconds: 1\n```", "```\n$ kubectl create -f pod-A-controller.yaml\n$ kubectl create -f pod-B-controller.yaml\n```", "```\n$ kubectl scale --replicas=3 rc/node-js-scale-b\n$ kubectl scale --replicas=1 rc/node-js-scale-a\n$ kubectl scale --replicas=4 rc/node-js-scale-b\n$ kubectl scale --replicas=0 rc/node-js-scale-a\n```", "```\n$ kubectl delete rc/node-js-scale-a\n```", "```\n$ kubectl scale --replicas=1 rc/node-js-scale\n```", "```\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: node-js-scale\nspec:\n  minReplicas: 1\n  maxReplicas: 3\n  scaleTargetRef:\n    apiVersion: v1\n    kind: ReplicationController\n    name: node-js-scale\n  targetCPUUtilizationPercentage: 20\n```", "```\n$ kubectl get hpa \n```", "```\n$ kubectl get pods -l name=node-js-scale\n```", "```\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: boomload\nspec:\n  replicas: 1\n  selector:\n    app: loadgenerator\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n    spec:\n      containers:\n      - image: williamyeh/boom\n        name: boom\n        command: [\"/bin/sh\",\"-c\"]\n        args: [\"while true ; do boom http://node-js-scale/ -c 10 -n 100      \n        ; sleep 1 ; done\"]\n```", "```\n$ kubectl get pods -l name=node-js-scale\n```", "```\n$ kubectl delete rc/boomload\n```", "```\n$ export NUM_MINIONS=5\n$ export KUBE_AUTOSCALER_MIN_NODES=2\n$ export KUBE_AUTOSCALER_MAX_NODES=5\n$ export KUBE_ENABLE_CLUSTER_AUTOSCALER=true \n```", "```\n$ kubectl get nodes\n```", "```\n$ kubectl config current-context\nkubernetes-admin@kubernetes\nNext up, let's grab the helm install script and install it locally. Make sure to read the script through first so you're comfortable with that it does!\n```", "```\nmaster $ curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > get_helm.sh\n% Total % Received % Xferd Average Speed Time Time Time Current\nDload Upload Total Spent Left Speed\n100 6740 100 6740 0 0 22217 0 --:--:-- --:--:-- --:--:-- 22244\nmaster $ chmod 700 get_helm.sh\n$ ./get_helm.sh\nmaster $ ./get_helm.sh\nHelm v2.9.1 is available. Changing from version v2.8.2.\nDownloading https://kubernetes-helm.storage.googleapis.com/helm-v2.9.1-linux-amd64.tar.gz\nPreparing to install into /usr/local/bin\nhelm installed into /usr/local/bin/helm\nRun 'helm init' to configure helm\n```", "```\nmaster $ helm init\nCreating /root/.helm\nCreating /root/.helm/repository\nCreating /root/.helm/repository/cache\nCreating /root/.helm/repository/local\nCreating /root/.helm/plugins\nCreating /root/.helm/starters\nCreating /root/.helm/cache/archive\nCreating /root/.helm/repository/repositories.yaml\nAdding stable repo with URL: https://kubernetes-charts.storage.googleapis.com\nmaster $ helm init\nCreating /root/.helm\nCreating /root/.helm/repository\nCreating /root/.helm/repository/cache\nCreating /root/.helm/repository/local\nCreating /root/.helm/plugins\nCreating /root/.helm/starters\nCreating /root/.helm/cache/archive\nCreating /root/.helm/repository/repositories.yaml\nAdding stable repo with URL: https://kubernetes-charts.storage.googleapis.com\nAdding local repo with URL: http://127.0.0.1:8879/charts\n$HELM_HOME has been configured at /root/.helm.\nTiller (the Helm server-side component) has been installed into your Kubernetes Cluster.\nPlease note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.\nFor more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation\nHappy Helming!\n```", "```\n$ helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Skip local chart repository\n...Successfully got an update from the \"stable\" chart repository\nUpdate Complete.  Happy Helming!\n```", "```\n$ helm install stable/mysql\nNAME:   guilded-otter\nLAST DEPLOYED: Mon Jun  4 01:49:46 2018\nNAMESPACE: default\nSTATUS: DEPLOYED\nRESOURCES:\n==> v1beta1/Deployment\nNAME                 DESIRED CURRENT UP-TO-DATE  AVAILABLE AGE\nguilded-otter-mysql  1 1 1         0 0s\n==> v1/Pod(related)\nNAME                                  READY STATUS RESTARTS AGE\nguilded-otter-mysql-5dd65c77c6-46hd4  0/1 Pending 0 0s\n==> v1/Secret\nNAME                 TYPE DATA AGE\nguilded-otter-mysql  Opaque 2 0s\n==> v1/ConfigMap\nNAME                      DATA AGE\nguilded-otter-mysql-test  1 0s\n==> v1/PersistentVolumeClaim\nNAME                 STATUS VOLUME CAPACITY  ACCESS MODES STORAGECLASS AGE\nguilded-otter-mysql  Pending 0s\n==> v1/Service\nNAME                 TYPE CLUSTER-IP    EXTERNAL-IP PORT(S) AGE\nguilded-otter-mysql  ClusterIP 10.105.59.60  <none> 3306/TCP 0s\n```"]