- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Going Serverless with Knative and OpenFaaS Frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we discussed Kubeflow, which provides an easy-to-deploy,
    simple-to-use toolchain for data scientists to integrate the various resources
    they will need to run models on Kubernetes, such as Jupyter notebooks, Kubernetes
    deployment files, and machine learning libraries such as PyTorch and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: By using Kubeflow’s built-in Notebooks services, you can create notebooks and
    share them with your teams. We also went over how to set up a machine learning
    pipeline to develop and deploy an example model using the Kubeflow machine learning
    platform. Additionally, we established that Kubeflow on MicroK8s is simple to
    set up and configure, lightweight, and capable of simulating real-world conditions
    while building, migrating, and deploying pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at the most popular open source serverless frameworks
    that extend Kubernetes with components for deploying, operating, and managing
    serverless, cloud-native apps. These frameworks enable you to create a service
    by encapsulating the code in a container image and delivering the required functionalities.
    Serverless frameworks automatically start and stop instances, so your code only
    runs when it’s needed. Unless your code needs to accomplish something, resources
    aren’t used.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes’ container orchestration capabilities (such as scheduling, load balancing,
    and health monitoring) make container proliferation much easier. However, this
    requires developers to perform or template several repetitive tasks, such as pulling
    application source code from repositories, building and provisioning a container
    image around the code, and configuring network connections outside of Kubernetes
    using various tools. Additionally, integrating Kubernetes-managed containers into
    an automated **continuous integration/continuous delivery** (**CI/CD**) pipeline
    necessitates the use of new tools and scripting.
  prefs: []
  type: TYPE_NORMAL
- en: With serverless frameworks automating the aforementioned activities from within
    Kubernetes, it eliminates complexity. A developer would be able to define the
    contents and configuration of a container in a single YAML manifest file, and
    serverless frameworks would take care of the rest, including building the container
    and conducting network programming to set up a route, should be Ingress, load
    balancing, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless computing is becoming the preferred cloud-native execution approach
    as it makes developing and running applications much easier and more cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: 'The serverless model of computing offers the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning of resources on demand, scaling transparently based on demands,
    and scaling to zero when no more requests are made
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offloading all infrastructure management responsibilities to the infrastructure
    provider, allowing developers to spend their time and effort on creation and innovation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allowing users to pay only for resources that are used, never for idle capacity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes cannot run serverless apps on its own; we would need customized software
    that combines Kubernetes with a specific infrastructure provider’s serverless
    platform. By abstracting away the code and handling network routing, event triggers,
    and autoscaling, the serverless frameworks would enable any container to run as
    a serverless workload on any Kubernetes cluster; it doesn’t matter whether the
    container is built around a serverless function or other application code (for
    example, microservices).
  prefs: []
  type: TYPE_NORMAL
- en: Serverless computing, especially when deployed at the network’s edge, is considered
    a key enabler for the building of increasingly complex **Internet of Things**
    (**IoT**) systems in the future. However, when installing new edge infrastructures
    for serverless workloads, additional attention must be paid to resource usage
    and network connectivity. Studies show that edge-oriented distributions, such
    as MicroK8s, perform better in the majority of tests, including cold start delay,
    serial execution performance, parallel execution with a single replica, and parallel
    execution using various autoscaling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll look at two of the most popular serverless frameworks included with MicroK8s
    in this chapter: Knative and OpenFaaS. Both serverless frameworks are Kubernetes-based
    platforms for building, deploying, and managing modern serverless workloads. In
    this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview – Knative framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling the Knative add-on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying and running a sample service on Knative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview – OpenFaaS framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling the OpenFaaS add-on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying and running a sample function on OpenFaaS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for developing and deploying serverless applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of the Knative framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Knative is a Kubernetes-based platform for deploying, managing, and scaling
    modern serverless workloads. Knative has the following three main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Build**: Provides streamlined source-to-container builds that are easy to
    utilize. By utilizing common constructs, you gain an advantage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serving**: Networking, autoscaling, and revision tracking are all handled
    by Knative. All you have to do now is concentrate on your core logic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eventing**: Handles the subscription, delivery, and management of events.
    By connecting containers to a data stream via declarative event connection and
    developer-friendly object architecture, you can create modern apps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MicroK8s is the optimal solution to getting started with all of the components
    of Knative (Build, Serving, and Eventing) because it provides native support for
    Knative. We’ll go through each component in detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Build components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Knative Build component simplifies the process of building a container
    from source code. This procedure usually consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading source code from a code repository such as GitHub
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing the underlying dependencies that the code requires to run, such as
    environment variables and software libraries
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Container image creation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Placing container images in a registry accessible to the Kubernetes cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For its Build process, Knative makes use of Kubernetes **application programming
    interfaces** (**APIs**) and other technologies. The developer can use a single
    manifest (usually a YAML file) that describes all of the variables’ location of
    the source code, required dependencies, and so on. Knative leverages the manifest
    to automate the container building and image creation process.
  prefs: []
  type: TYPE_NORMAL
- en: Serving components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Containers are deployed and run as scalable Knative services via the Serving
    component. The following are the key capabilities provided by the Serving component:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Configuration**: A service’s state is defined and maintained by configuration.
    It also has version control. Each change to the configuration creates a new version
    of the service, which is saved alongside earlier versions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intelligent service routing**: Developers can use intelligent service routing
    to direct traffic to different versions of the service. Assume you’ve produced
    a new version of a service and want to test it out on a small group of users before
    moving everyone. Intelligent service routing allows you to send a portion of user
    requests to the new service and the rest to an older version. As you gain confidence
    in the new service, you can send more traffic to it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoscaling**: Knative can scale services up to thousands of instances and
    down to zero instances, which is critical for serverless applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Istio** ([https://istio.io/](https://istio.io/)): This is an open source
    Kubernetes service mesh deployed along with Knative. It offers service request
    authentication, automatic traffic encryption for safe communication between services,
    and extensive metrics on microservices and serverless function operations for
    developers and administrators to use to improve infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Knative Serving is defined by a set of objects known as Kubernetes **Custom
    Resource Definitions** (**CRDs**). The following components define and govern
    the behavior of your serverless workload on the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Service**: Controls the entire life cycle of your workload for you. It ensures
    that your app has a route, a configuration, and a new revision for each service
    update by controlling the creation of additional objects. The service can be configured
    to always send traffic to the most recent revision or a pinned revision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Route**: A network endpoint is mapped to one or more revisions. Traffic can
    be managed in a variety of ways, including fractional traffic and named routes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Revision**: This is a snapshot of the code and configuration for each change
    made to the workload at a specific moment in time. Revisions are immutable objects
    that can be kept for as long as they are needed. Knative Serving Revisions can
    be scaled up and down automatically in response to incoming traffic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configuration**: This keeps your deployment in the desired state. It adheres
    to the Twelve-Factor App paradigm and provides a clear separation between code
    and configuration. A new revision is created when you change a configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To summarize, the Serving component is responsible for deploying and running
    containers as scalable Knative services.
  prefs: []
  type: TYPE_NORMAL
- en: Eventing components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Knative’s Eventing component allows various events to trigger container-based
    services and functions. There is no need to develop scripts or implement middleware
    because Knative queues handle the distribution of events to the respective containers.
    A messaging bus that distributes events to containers and channels, which are
    nothing but queues of events (from which developers can choose), is also handled
    by Knative. Developers can also establish feeds that connect an event to a specific
    action that their containers should execute.
  prefs: []
  type: TYPE_NORMAL
- en: Knative event sources make integration with third-party event providers easier
    for developers. The Eventing component will connect to the event producer and
    route the generated events automatically. It also provides tools for routing events
    from event producers to sinks, allowing developers to build applications that
    use an event-driven architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Knative Eventing resources are loosely coupled and can be developed and deployed
    separately. Any producer can generate events and any event consumer can express
    interest in that event or group of events. Knative Eventing also takes care of
    sending and receiving events between event producers and sinks using standard
    HTTP POST requests. The following are the Eventing components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event sources**: These are the primary event producers in a Knative Eventing
    deployment. Events are routed to either a sink or a subscriber.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Brokers and Triggers**: These provide an event mesh model that allows event
    producers to deliver events to a Broker, which then distributes them uniformly
    to consumers via Triggers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Channels and Subscriptions**: These work together to create an event pipe
    model that transforms and routes events between channels via Subscriptions. This
    model is suitable for event pipelines in which events from one system must be
    transformed before being routed to another process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event registry**: Knative Eventing defines an EventType object to help consumers
    discover the types of events available from Brokers. The registry is made up of
    various event types. The event types stored in the registry contain all of the
    information needed for a consumer to create a Trigger without using an out-of-band
    mechanism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following figure, Knative components are represented. Serving and Eventing
    collaborate on tasks and applications to automate and manage them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Knative components ](img/Figure_10.01_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Knative components
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, Knative provides components that allow the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Serverless containers can be deployed quickly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling pods down to zero as well as autoscaling based on demands.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple networking layers are supported for integration including Contour,
    Kourier, and Istio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for point-in-time snapshots of deployed code and configurations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for both HTTP and HTTPS networking protocols.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve covered the basics of Knative, we’ll enable the add-on and deploy
    one of the samples in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the Knative add-on
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since Knative isn’t available for ARM64 architecture, we will be using an Ubuntu
    virtual machine for this section. The instructions for setting up the MicroK8s
    cluster are the same as in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070)*,*
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll enable the Knative add-on that adds Knative middleware to your cluster.
    Use the following command to enable the Knative add-on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: When you enable this add-on, Istio and DNS will be also added to MicroK8s.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command execution output confirms that the Knative add-on is
    being enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Enabling the Knative add-on ](img/Figure_10.02_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Enabling the Knative add-on
  prefs: []
  type: TYPE_NORMAL
- en: 'It will take some time to finish activating the add-on. The following command
    execution output shows that Knative has been successfully enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Knative add-on activated ](img/Figure_10.03_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Knative add-on activated
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to the next step, let’s verify whether the add-on has been
    enabled and that all the required pods are running.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see whether the add-on is activated or not, use the `kubectl get pods -n
    knative-serving` command. The following command execution output indicates that
    Knative Serving components are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Knative Serving component pods are running ](img/Figure_10.04_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Knative Serving component pods are running
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to the next step, let’s make sure that all of the Knative
    Eventing components are up and running using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output indicates that Knative Eventing components
    are also running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Knative Eventing components are running ](img/Figure_10.05_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Knative Eventing components are running
  prefs: []
  type: TYPE_NORMAL
- en: We now have all of the components of Knative up and running.
  prefs: []
  type: TYPE_NORMAL
- en: We will proceed to the next step of installing the Knative `kn`. Without having
    to create or edit YAML files manually, `kn` provides a quick and easy interface
    for building Knative resources, such as services and event sources. It also makes
    it easier to complete tasks such as autoscaling and traffic splitting that might
    otherwise be difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `kn` binary can be downloaded from the release page ([https://github.com/knative/client/releases](https://github.com/knative/client/releases))
    and copied to `/usr/local/bin` using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output confirms that the `kn` CLI has been
    downloaded successfully and is available at `/usr/local/bin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Installing the Knative CLI ](img/Figure_10.06_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – Installing the Knative CLI
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on to the next step, let’s verify whether the `kn` CLI is working
    by running the following `kn version` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output confirms that the `kn` CLI is operational, and its version
    and build date are displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Verifying whether the kn CLI is operational ](img/Figure_10.07_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – Verifying whether the kn CLI is operational
  prefs: []
  type: TYPE_NORMAL
- en: 'For the `kn` CLI to access Kubernetes configuration, copy the MicroK8s configuration
    file to `$HOME/.kube/config` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Copy MicroK8s configuration file to $HOME folder ](img/Figure_10.08_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – Copy MicroK8s configuration file to $HOME folder
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the Knative components, as well as the Knative CLI `kn` setup, are now
    up and running. We’ll now move on to the following step: deploying and running
    the sample service.'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and running a sample service on Knative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will deploy the `Hello world` sample service from the Knative
    samples repo. The sample service prints `Hello` `$TARGET!` after reading the `TARGET`
    environment variable. If `TARGET` is not given, the default value is “`World`”.
  prefs: []
  type: TYPE_NORMAL
- en: Now in the following steps, we’ll deploy the service by specifying the image
    location and the `TARGET` environment variable. We are going to create a Knative
    service (Serving component), which is a time-based representation of a single
    serverless container environment (such as a microservice). It includes both the
    network address for accessing the service and the application code and settings
    required to run the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Knative service lifespan is controlled by the `serving.knative.dev` CRD.
    To create the Knative service, we’ll use the `kn` CLI as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output indicates that the service creation
    is successful and the service can be accessed at the URL `http//kn-serverless.default.example.com`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Creating a new Knative service ](img/Figure_10.09_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – Creating a new Knative service
  prefs: []
  type: TYPE_NORMAL
- en: Congrats! We have successfully created a new Knative service and deployed it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a recap of the Serving components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Service**: Manages the whole life cycle of your workload'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Route**: Takes care of mapping the network endpoint to one or more revisions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configuration**: Maintains the desired state for the deployment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Revision**: A point-in-time snapshot of the code and configuration of the
    workload'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Serving components involved in the definition and control of how serverless
    workloads behave on the cluster are depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Knative Serving components ](img/Figure_10.10_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – Knative Serving components
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now invoke the service that we previously created using the `curl` command
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`$SERVICE_IP` and `$INGRESS_PORT` point to the Knative service and the Ingress
    port that is exposed. The output of the following command confirms that the Knative
    service has been invoked and output has been displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – Invoking the Knative service ](img/Figure_10.11_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – Invoking the Knative service
  prefs: []
  type: TYPE_NORMAL
- en: 'To observe how a pod is created to service the requests, run the `watch kubectl
    get pods` command in a new Terminal tab. If there are no inbound requests for
    60 seconds, Knative will automatically scale this pod down to zero as shown in
    the following command execution output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – Pods are terminated if there are no inbound requests for 60
    seconds ](img/Figure_10.12_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 – Pods are terminated if there are no inbound requests for 60 seconds
  prefs: []
  type: TYPE_NORMAL
- en: 'You may also issue the preceding `curl` command after the pods have scaled
    down to zero to watch the pod spin up and serve the request zero as shown in the
    following command execution output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13 – Pods are spun up to serve the requests ](img/Figure_10.13_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 – Pods are spun up to serve the requests
  prefs: []
  type: TYPE_NORMAL
- en: Knative, in a nutshell, is a Kubernetes-powered platform for developing, deploying,
    and managing modern serverless workloads. We also discovered that MicroK8s has
    native Knative support and is the best way to get started with all of Knative’s
    components (Build, Serving, and Eventing).
  prefs: []
  type: TYPE_NORMAL
- en: We have deployed a sample application and used its endpoints to call it from
    the command line. We will now look at the next choice, OpenFaaS, in the next section
    to run the sample application, and analyze the features it offers.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the OpenFaaS framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenFaaS (**FaaS** standing for **functions as a service**) is a framework for
    creating serverless functions using the Docker and Kubernetes container technologies.
    Any process can be packaged as a function, allowing the consumption of a variety
    of web events without having to write boilerplate code over and over. It’s an
    open source initiative that’s gaining a lot of traction in the community.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the key advantages of the OpenFaaS framework are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Running functions on any infrastructure without concern of lock-in with an open
    source functions framework.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating functions in any programming language and packaging them in Docker/OCI
    containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Built-in UI, robust CLI, and one-click installation make it simple to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale as you go – handle traffic spikes and scale down when not in use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A community edition and a pro edition are available along with production support.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve covered the concepts of OpenFaaS, we’ll enable the add-on and
    deploy one of the samples in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the OpenFaaS add-on
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since OpenFaaS isn’t available for ARM64 architecture, we will be using an Ubuntu
    virtual machine for this section. The instructions for setting up the MicroK8s
    cluster are the same as in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070)*,*
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before enabling the OpenFaaS add-on, enable the DNS and Registry add-ons using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The DNS is used to provide address resolution services to Kubernetes so that
    services can communicate with each other. The following command execution output
    confirms that the DNS add-on is enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Enabling the DNS add-on ](img/Figure_10.14_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 – Enabling the DNS add-on
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the DNS add-on is enabled, we will move on to the next step of enabling
    the Registry add-on using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The Registry add-on creates a private registry in Docker and exposes it at
    `localhost:32000`. As part of this add-on, the storage add-on will also be enabled
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15 – Enabling the Registry add-on ](img/Figure_10.15_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 – Enabling the Registry add-on
  prefs: []
  type: TYPE_NORMAL
- en: We can move on to the next step of enabling the OpenFaaS add-on now that we’ve
    enabled the DNS and Registry add-ons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to enable the OpenFaaS add-on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output confirms that the OpenFaaS add-on is
    being enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16 – Enabling the OpenFaaS add-on ](img/Figure_10.16_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 – Enabling the OpenFaaS add-on
  prefs: []
  type: TYPE_NORMAL
- en: 'It will take some time to finish activating the add-on. The following command
    execution output shows that the OpenFaaS add-on has been successfully enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.17 – The OpenFaaS add-on is enabled ](img/Figure_10.17_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.17 – The OpenFaaS add-on is enabled
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, deployment scripts generate a username (admin) and password
    combination during the installation. Save the credentials so we can use them in
    the following steps.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to the next step, let’s verify whether the add-on has been
    enabled and that all the required pods are running.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see whether the add-on is activated, use the `kubectl get pods` command.
    The following command execution output indicates that OpenFaaS components are
    running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.18 – OpenFaaS pods are running ](img/Figure_10.18_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.18 – OpenFaaS pods are running
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have all of the following components of OpenFaaS up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nats` provides asynchronous execution and queuing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`prometheus` provides metrics and enables auto-scaling through `alertmanager`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`gateway` provides an external route into the functions and also scales functions
    according to demand.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`queue-worker` is in charge of handling asynchronous requests.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will proceed to the next step of installing the OpenFaaS CLI tool. The CLI
    can be used to create and deploy OpenFaaS functions. From a set of supported language
    templates, you can create OpenFaaS functions (such as Node.js, Python, C#, and
    Ruby). Please see the list of supported languages at [https://github.com/openfaas/templates](https://github.com/openfaas/templates)
    for further information.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `curl` command to install the CLI after acquiring the binaries
    from the releases page as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Here we are using the `–insecure` flag to avoid any certificate download issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command execution output confirms that the CLI installation is
    successful. The `faas-cli` command and the `faas` alias are available post-installation
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.19 – Installing the OpenFaaS CLI ](img/Figure_10.19_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.19 – Installing the OpenFaaS CLI
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve installed `faas-cli`, we can use the `faas-cli` command to start
    creating and deploying functions in the next section
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and running a sample function on OpenFaaS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will cover the creation, build, and deployment of a new FaaS Python
    function. We’ll also use OpenFaaS CLI commands to test the deployed function.
    The OpenFaaS CLI has a template engine that can be used to set up new functions
    in any programming language. To create a new function, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here `–prefix` `localhost:32000` refers to the local MicroK8s registry that
    we have enabled in the preceding steps.
  prefs: []
  type: TYPE_NORMAL
- en: This command works by reading a list of templates from the `./template` directory
    in your current working folder.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the `faas-cli template pull` command to pull the templates
    from the official OpenFaaS language templates from GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: To check the list of languages that are supported, use the `faas-cli new –list`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command execution indicates that the new `openfaas-serverless`
    Python function has been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.20 – Creating a new function using the CLI ](img/Figure_10.20_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.20 – Creating a new function using the CLI
  prefs: []
  type: TYPE_NORMAL
- en: 'A stack file and a new folder with the function name are generated in the current
    working folder as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.21 – A stack file and a new folder with the function name ](img/Figure_10.21_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.21 – A stack file and a new folder with the function name
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve created a new function, we’ll need to build it so that a container
    image can be created and used in the following steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to build the new function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `faas-cli build` command creates a Docker image on your local MicroK8s registry,
    which could be used locally or could be uploaded to a remote container registry
    (in case of a multi-node cluster setup). Each change to your function necessitates
    issuing a new `faas-cli build` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command execution indicates that the new `openfaas-serverless`
    Python function has been built successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.22 – Building a new OpenFaaS function ](img/Figure_10.22_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.22 – Building a new OpenFaaS function
  prefs: []
  type: TYPE_NORMAL
- en: 'It may take some time to complete the build process, but once completed, you
    should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.23 – Successful OpenFaaS function build ](img/Figure_10.23_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.23 – Successful OpenFaaS function build
  prefs: []
  type: TYPE_NORMAL
- en: We can move on to the next step of pushing the Docker image to the registry
    now that the images have been built.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to push Docker images to our local registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output indicates that the `openfaas-serverless`
    function has been pushed to the registry successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.24 – OpenFaaS function pushed to the local registry ](img/Figure_10.24_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.24 – OpenFaaS function pushed to the local registry
  prefs: []
  type: TYPE_NORMAL
- en: Let’s set an `OPENFAAS_URL` environment variable and also retrieve the necessary
    admin credentials before moving on to the next step of deploying the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'An `OPENFAAS_URL` environment variable defines the default gateway URL that
    the CLI uses to contact the OpenFaaS server as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.25 – Set the OPENFAAS_URL environment variable ](img/Figure_10.25_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.25 – Set the OPENFAAS_URL environment variable
  prefs: []
  type: TYPE_NORMAL
- en: 'To retrieve the admin credentials, use the following command that was printed
    during the installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output indicates that the command was successfully
    executed and that the password was retrieved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.26 – Retrieving admin credentials ](img/Figure_10.26_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.26 – Retrieving admin credentials
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s log in to the OpenFaaS server with the admin credentials so we can deploy
    the function. The following command execution output indicates that the login
    was successful and the credentials were saved to the local store:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.27 – Using the retrieved password to log in to the OpenFaaS server
    ](img/Figure_10.27_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.27 – Using the retrieved password to log in to the OpenFaaS server
  prefs: []
  type: TYPE_NORMAL
- en: We can proceed to the next step of deploying the function to the OpenFaaS server
    now that the credentials have been saved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to deploy the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output indicates that the deployment is successful
    and we now have the URL for accessing the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.28 – Successful function deployment ](img/Figure_10.28_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.28 – Successful function deployment
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can use the `faas-cli up` command to build, push, and deploy
    the function in a single command.
  prefs: []
  type: TYPE_NORMAL
- en: Congrats! We have successfully created a new function and deployed it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To call the function, we’ll utilize the CLI’s `invoke` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the function accepts an input parameter and outputs the input parameter
    value. To change the logic, the stack file and the handler file need to be modified,
    and then the function needs to be redeployed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.29 – Invoking the function ](img/Figure_10.29_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.29 – Invoking the function
  prefs: []
  type: TYPE_NORMAL
- en: 'You could also use the OpenFaaS UI to invoke the deployed functions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.30 – OpenFaaS UI ](img/Figure_10.30_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.30 – OpenFaaS UI
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, in simple terms, OpenFaaS provides the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A simple approach to package any code or binary, as well as a diverse ecosystem
    of language templates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Built-in autoscaling and a function repository for collaboration and sharing
    metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Kubernetes-native experience and a devoted community
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best practices for developing and deploying serverless apps will be discussed
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for developing and deploying serverless applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We must adhere to best practices in order to safeguard our resources, applications,
    and infrastructure service provider accounts. Here are some guiding principles
    that need to be considered.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless function = specific function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A serverless function must accomplish a certain task. A serverless function
    should execute a logical function, similar to how a function or method in any
    code should accomplish one thing.
  prefs: []
  type: TYPE_NORMAL
- en: Using microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microservices enable us to link together the data storage and functions in a
    manageable manner. The microservice will be bound by a contract that specifies
    what it is allowed and prohibited to do. A payment microservice, for example,
    can be used to create, update, and delete user payments. Outside of the user account
    data storage, this microservice should never modify any data. It will also have
    its own API. Other microservices can now interact with user account serverless
    functions in a consistent manner without modifying any of the user account data
    stores.
  prefs: []
  type: TYPE_NORMAL
- en: Using appropriate stacks for various resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When deploying resources, serverless frameworks allow us to employ several language
    stacks, and each framework configuration deploys the appropriate stack. One stack
    per resource type should be the goal. Our user payment microservice, for example,
    might have a database stack (to store account metadata in MongoDB), an **identity
    provider** (**IdP**) stack (to set up and maintain user sessions with an OAuth2
    provider), a function stack (to deploy functions that provide the user payment
    microservice API), and an object store stack (to capture user account profile
    pictures in S3). This enables us to edit one resource type without affecting another.
    If you make a mistake in the deployment of a function’s stack, for example, your
    other stacks are unaffected.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the principle of least privilege
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The minimal set of IAM permissions should be applied to all of your resources.
    A serverless function that reads a MongoDB table, for example, should only contain
    the read action for that MongoDB table. When defining privileges, you should avoid
    using an asterisk (*) whenever feasible. A hacker can read and delete all database
    data if your function is ever compromised and it employs asterisks to make all
    MongoDB accessible and every operation permissible.
  prefs: []
  type: TYPE_NORMAL
- en: Performing load testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Load testing your serverless functions would help you identify how much memory
    to allocate and what timeout value to use. In a serverless environment, there
    could be complicated apps, and you may not be aware of dependencies inside applications
    that prevent them from performing a function on heavy loads. Load testing allows
    you to identify possible problems that are critical to running a high-availability
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Using a CI/CD pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s fine to deploy using the CLI when you’re starting to build an application.
    Ideally, you should use a CI/CD pipeline to deploy your code before releasing
    it to production. Before enabling a pull request to merge, the CI section of the
    pipeline allows you to perform linting checks, unit tests, and a variety of additional
    automated checks. When a PR is merged or a branch is updated, the CD section of
    the pipeline allows you to deploy your serverless application automatically. Using
    a CI/CD pipeline eliminates human error and ensures that your process is repeatable.
  prefs: []
  type: TYPE_NORMAL
- en: Constant monitoring is required
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We should monitor our serverless resources using services such as Knative monitoring
    and Prometheus. There may be many resources and they may be used so frequently
    that manually checking them for faults would be difficult. Health, longer executions,
    delays, and errors can all be reported by monitoring services. Having a service
    that alerts us (such as Alert Manager) when our serverless application and resources
    are experiencing problems allows us to locate and resolve issues more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Auditing in addition to monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We want to audit in addition to monitoring. When anything stops working or has
    problems, monitoring alerts you. When our resources stray from a known configuration
    or are wrongly configured, auditing alerts us. We may develop rules that audit
    our resources and their configurations using services such as Knative Config or
    an OpenFaaS stack file.
  prefs: []
  type: TYPE_NORMAL
- en: Auditing software dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’d like to audit our software dependencies as well. Just because we don’t
    have a server anymore doesn’t mean we’re immune to “patching.” We want to make
    sure that any software dependencies we specify are current and do not include
    any known vulnerabilities. We can utilize automated tools to keep track of which
    software packages need to be updated.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we examined two of the most popular serverless frameworks included
    with MicroK8s, Knative and OpenFaaS, both of which are Kubernetes-based platforms
    for developing, deploying, and managing modern serverless workloads. We’ve deployed
    a few of the samples and used their endpoints to invoke them via the CLI. We also
    looked at how serverless frameworks scale down pods to zero when there are no
    requests and spin up new pods when there are more requests.
  prefs: []
  type: TYPE_NORMAL
- en: We realized that the ease of deployment of MicroK8s appears to be related to
    the ease with which serverless frameworks can be implemented. We’ve also discussed
    some guiding principles to keep in mind when developing and deploying serverless
    applications. However, deploying serverless resources is pretty simple. We also
    understood that in order to protect our resources, apps, and infrastructure service
    provider accounts, we needed to adhere to best practices.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll look at how to use OpenEBS to implement storage replication
    that synchronizes data across several nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 4: Deploying and Managing Applications on MicroK8s'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part focuses on the deployment and management aspects of typical IoT/Edge
    computing applications, such as setting up storage replication for your stateful
    applications, implementing a service mesh for cross-cutting concerns and a high
    availability cluster to withstand a component failure and continue to serve workloads
    without interruption, configuring containers with workload isolation, and running
    secured containers in isolation from a host system.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part of the book comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B18115_11.xhtml#_idTextAnchor180)*, Managing Storage Replication
    with OpenEBS*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B18115_12.xhtml#_idTextAnchor196)*, Implementing Service Mesh
    for Cross-Cutting Concerns*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B18115_13.xhtml#_idTextAnchor212)*, Resisting Component Failure
    Using HA Cluster*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B18115_14.xhtml#_idTextAnchor223)*, Hardware Virtualization
    for Securing Containers*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 15*](B18115_15.xhtml#_idTextAnchor243)*, Implementing Strict Confinement
    for Isolated Containers*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 16*](B18115_16.xhtml#_idTextAnchor257)*, Diving into the Future*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
