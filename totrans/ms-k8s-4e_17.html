<html><head></head><body>
  <div id="_idContainer326" class="Basic-Text-Frame">
    <h1 class="chapterNumber">17</h1>
    <h1 id="_idParaDest-786" class="chapterTitle">Running Kubernetes in Production</h1>
    <p class="normal">In the previous chapter, we discussed governance and policy engines. This is an important part of managing large-scale Kubernetes-based systems in production. However, it is only one part. In this chapter, we will turn our attention to the overall management of Kubernetes in production. The focus will be on running multiple Managed Kubernetes clusters in the cloud.</p>
    <p class="normal">The topics we will cover are:</p>
    <ul>
      <li class="bulletList">Understanding Managed Kubernetes in the cloud</li>
      <li class="bulletList">Managing multiple clusters</li>
      <li class="bulletList">Building effective processes for large-scale Kubernetes deployments</li>
      <li class="bulletList">Handling infrastructure at scale</li>
      <li class="bulletList">Managing clusters and node pools</li>
      <li class="bulletList">Upgrading Kubernetes</li>
      <li class="bulletList">Troubleshooting</li>
      <li class="bulletList">Cost management</li>
    </ul>
    <h1 id="_idParaDest-787" class="heading-1">Understanding Managed Kubernetes in the cloud</h1>
    <p class="normal"><strong class="keyWord">Managed Kubernetes</strong> is a <a id="_idIndexMarker1843"/>service provided by cloud providers <a id="_idIndexMarker1844"/>such <a id="_idIndexMarker1845"/>as <strong class="keyWord">Amazon Web Services</strong> (<strong class="keyWord">AWS</strong>), <strong class="keyWord">Google Cloud Platform</strong> (<strong class="keyWord">GCP</strong>), and <strong class="keyWord">Microsoft Azure</strong> that <a id="_idIndexMarker1846"/>simplifies the deployment, management, and scaling of containerized applications in the cloud. With Managed Kubernetes, organizations can focus on developing and deploying their applications without worrying too much about the underlying infrastructure.</p>
    <p class="normal">Managed Kubernetes provides a pre-configured and optimized environment for deploying containers, eliminating the need for the manual setup and maintenance of a Kubernetes cluster. This allows organizations to quickly deploy and scale their applications, reducing time to market and freeing up valuable resources.</p>
    <p class="normal">Additionally, Managed Kubernetes integrates with the cloud providers’ other services, such as databases, networking, storage solutions, security, identity, and observability features, making it easier to manage and secure the entire application stack. This also enables organizations to leverage the providers’ expertise in managing large-scale infrastructure, ensuring high availability, and reducing downtime.</p>
    <p class="normal">Overall, Managed Kubernetes<a id="_idIndexMarker1847"/> provides a simplified and efficient way to deploy and manage containerized applications in the cloud, reducing operational overhead and improving time to market. This makes it an attractive option for organizations of all sizes looking to take advantage of the benefits of containers and the cloud.</p>
    <h2 id="_idParaDest-788" class="heading-2">Deep integration</h2>
    <p class="normal">Cloud providers utilize the extensibility of <a id="_idIndexMarker1848"/>Kubernetes to offer deep integration of their Managed Kubernetes solutions with their cloud services via the CNI, the CSI, and authentication/authorization plugins. The cloud providers also implement the <strong class="keyWord">Cloud Controller Interface </strong>(<strong class="keyWord">CCI</strong>) to allow <a id="_idIndexMarker1849"/>their compute infrastructure to serve Kubernetes nodes.</p>
    <p class="normal">However, the integration runs deeper. The cloud providers often configure the kubelet, control the container runtime that runs on every node, and deploy various DaemonSets on every node.</p>
    <p class="normal">For example, AKS leverages many Azure services:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Azure Compute</strong>: AKS leverages Azure Compute<a id="_idIndexMarker1850"/> resources such as <strong class="keyWord">virtual</strong> <strong class="keyWord">machines</strong> (<strong class="keyWord">VMs</strong>), availability sets, and scale sets to provide a managed Kubernetes experience.</li>
      <li class="bulletList"><strong class="keyWord">Azure Virtual Network</strong>: AKS integrates with <a id="_idIndexMarker1851"/>Azure Virtual Network, allowing users to create and manage their own virtual networks and subnets. This provides users with control over their network layout and the ability to tightly control network traffic.</li>
      <li class="bulletList"><strong class="keyWord">Azure Blob Storage</strong>: AKS integrates with <a id="_idIndexMarker1852"/>Azure Blob Storage, allowing users to store and manage their application data in the cloud. This provides users with scalable, secure, and highly available storage for their applications.</li>
      <li class="bulletList"><strong class="keyWord">Azure Key Vault</strong>: AKS integrates with <a id="_idIndexMarker1853"/>Azure Key Vault, allowing users to securely manage and store secrets such as passwords, keys, and certificates. This provides users with secure storage for their application secrets.</li>
      <li class="bulletList"><strong class="keyWord">Azure Monitor</strong>: AKS integrates with <a id="_idIndexMarker1854"/>Azure Monitor, allowing users to collect and analyze metrics, logs, and traces from their applications. This provides users with the ability to monitor and troubleshoot their workloads.</li>
      <li class="bulletList"><strong class="keyWord">Azure Active Directory </strong>(<strong class="keyWord">AAD</strong>): AKS integrates <a id="_idIndexMarker1855"/>with AAD to provide a secure, reliable, and highly available platform for running Kubernetes clusters. AAD provides an efficient and secure way to authenticate and authorize users and applications to access the cluster. AAD can also be integrated with Kubernetes <strong class="keyWord">RBAC</strong> (<strong class="keyWord">role-based access control</strong>) to provide granular control over access to<a id="_idIndexMarker1856"/> cluster resources.</li>
    </ul>
    <p class="normal">Let’s move on and discuss one of the key elements of successfully managing a production Kubernetes-based system in the cloud.</p>
    <h2 id="_idParaDest-789" class="heading-2">Quotas and limits</h2>
    <p class="normal">Cloud infrastructure has revolutionized the way organizations store and manage their data and run their workloads. However, one major issue that requires consideration and attention is the use of quotas and limits by cloud service providers. These quotas and limits, while necessary for ensuring the stability and security of the cloud infrastructure, can be a major source of frustration and even outages for users.</p>
    <p class="normal">Quotas<a id="_idIndexMarker1857"/> and limits<a id="_idIndexMarker1858"/> are restrictions placed on the number of resources that a user can consume. For example, there may be a limit on the number of VMs of a particular type that can be created in each region environment, or a quota on the amount of storage space that can be used. These quotas and limits are put in place to prevent a single user from consuming too many resources and potentially disrupting the overall performance of the cloud infrastructure. It also protects users from inadvertently provisioning a huge quantity of resources that they don’t really need but will have to pay for.</p>
    <p class="normal">The cloud is in theory <a id="_idIndexMarker1859"/>infinitely scalable and elastic. In practice, this is true only within the <a id="_idIndexMarker1860"/>quotas and limits.</p>
    <p class="normal">Let’s look at some real-world examples in the next sections.</p>
    <h3 id="_idParaDest-790" class="heading-3">Real-world examples of quotas and limits</h3>
    <p class="normal">On GCP, quotas <a id="_idIndexMarker1861"/>can generally be increased, while limits are fixed. Also, each service has its own <a id="_idIndexMarker1862"/>page of quotas and limits. The <strong class="keyWord">virtual private cloud </strong>(<strong class="keyWord">VPC</strong>) page<a id="_idIndexMarker1863"/> is found at <a href="https://cloud.google.com/vpc/docs/quota"><span class="url">https://cloud.google.com/vpc/docs/quota</span></a>.</p>
    <p class="normal">You can view the quotas in the GCP console as well as request increases: <a href="https://console.cloud.google.com/iam-admin/quotas"><span class="url">https://console.cloud.google.com/iam-admin/quotas</span></a>.</p>
    <p class="normal">There are currently 9,441 quotas!</p>
    <p class="normal">Here is a screenshot that shows some quotas for the GCP Compute Engine service: </p>
    <figure class="mediaobject"><img src="../Images/B18998_17_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.1: Screenshot of GCP compute engine quotas</p>
    <p class="normal">Now that we understand what quotas and limits are, let’s discuss capacity planning.</p>
    <h3 id="_idParaDest-791" class="heading-3">Capacity planning</h3>
    <p class="normal">In the olden days, capacity planning<a id="_idIndexMarker1864"/> meant thinking about how many servers you needed in your data centers, how big the disks should be, and the bandwidth of your network. This was based on the usage of your workloads as well as keeping healthy headroom for redundancy as well as growth. Then, you had to consider upgrades and how to phase out obsolete hardware. In the cloud, you don’t need to worry about hardware. However, you do need to plan around the quotas and limits. This means you need to monitor the quotas and limits and, whenever you get close to the current quota, request an increase. For quotas such as the number of VM instances of a particular VM family, I recommend staying below 50%-60% if possible. This should give you ample room for disaster recovery and growth, as well blue-green deployments where you run your new version and old version side by side for a while.</p>
    <h2 id="_idParaDest-792" class="heading-2">When should you not use Managed Kubernetes?</h2>
    <p class="normal">Managed <a id="_idIndexMarker1865"/>Kubernetes is great, but it is not a panacea. There are several situations and use cases where you may prefer to manage Kubernetes yourself, such as:</p>
    <ul>
      <li class="bulletList">The obvious use case is if you run Kubernetes on-prem and a managed solution is simply not available. However, you can run a similar stack to cloud-managed Kubernetes via platforms like GKE Anthos, AWS Outposts, and Azure Arc.</li>
      <li class="bulletList">You require extreme control over the control plane, the node components, and the daemonsets that run on each node. </li>
      <li class="bulletList">You already have in-house expertise in running Kubernetes yourself and using Managed Kubernetes will require a steep learning curve and might cost more. </li>
      <li class="bulletList">You manage highly sensitive information that you must fully control and can’t trust the cloud provider with. </li>
      <li class="bulletList">You run Kubernetes on multiple cloud providers and/or hybrid environments and prefer to have a uniform way to manage Kubernetes in all environments. </li>
      <li class="bulletList">You want to make sure you are not locked into a particular cloud provider.</li>
    </ul>
    <p class="normal">In short, there are various situations where you may take on managing Kubernetes yourself. Let’s look at the various ways you may deploy and manage multiple clusters of Kubernetes in different environments.</p>
    <h1 id="_idParaDest-793" class="heading-1">Managing multiple clusters</h1>
    <p class="normal">A Kubernetes cluster is<a id="_idIndexMarker1866"/> powerful and can manage a lot of workloads (thousands of nodes, and hundreds of thousands of pods). As a startup, you may get pretty far with just one cluster. However, at enterprise scale, you’ll need more than one cluster. Let’s consider some use cases.</p>
    <h2 id="_idParaDest-794" class="heading-2">Geo-distributed clusters</h2>
    <p class="normal">Geo-distributed clusters are clusters that run<a id="_idIndexMarker1867"/> in different locations. There are three main reasons for using geo-distributed clusters:</p>
    <ul>
      <li class="bulletList">Keeping your data and workloads close to their consumers.</li>
      <li class="bulletList">Compliance and data privacy laws where data must remain in its country of origin.</li>
      <li class="bulletList">High availability and disaster recovery in case of a regional outage.</li>
    </ul>
    <h2 id="_idParaDest-795" class="heading-2">Multi-cloud</h2>
    <p class="normal">If you run on multiple<a id="_idIndexMarker1868"/> clouds, then naturally you need at least one cluster per cloud provider.</p>
    <p class="normal">Running on multiple clouds can be complicated, but at enterprise scale, it may be unavoidable and sometimes desirable. For example, your company may run Kubernetes on cloud X and acquire a company that runs Kubernetes on cloud Y. Migrating from Y to X might be too risky and expensive.</p>
    <p class="normal">Another valid reason to run on multiple clouds is to have leverage against the cloud providers to secure better discounts and ensure you are not fully locked in. Finally, it may allow you to tolerate a complete cloud provider outage (this is not trivial to pull off).</p>
    <h2 id="_idParaDest-796" class="heading-2">Hybrid</h2>
    <p class="normal">Hybrid Kubernetes means running <a id="_idIndexMarker1869"/>some Kubernetes clusters in the cloud (with a single or multiple cloud providers) and some Kubernetes clusters on-prem.</p>
    <p class="normal">This situation may arise as before because of an acquisition or even if you are in the process of migrating from on-prem Kubernetes to the cloud. Large systems might take years to migrate and during the migration, you’ll have to run a mix of Kubernetes clusters running in a hybrid environment.</p>
    <p class="normal">You may also adopt patterns <a id="_idIndexMarker1870"/>like burst to cloud where most of your Kubernetes clusters run on-prem, but you have the flexibility to deploy workloads to Kubernetes clusters running in the cloud, which can scale quickly if you are hit with unanticipated load or if your on-prem infrastructure is having issues.</p>
    <h2 id="_idParaDest-797" class="heading-2">Kubernetes on the edge</h2>
    <p class="normal">Most enterprise data (around 90%) is<a id="_idIndexMarker1871"/> generated in the cloud and private data centers; however, that number will drop to just 25% by 2025 according to Gartner.</p>
    <p class="normal">This is mind-blowing. Edge computing (AKS IoT) will be responsible for this massive shift.</p>
    <p class="normal">A lot of devices spread all over the place will generate constant streams of data. Some of that data will be sent back to the backend for processing, aggregation, and storage. However, it makes a lot of sense to perform various forms of data processing close to the data instead of sending the raw data as it is. In some cases, you can even run workloads locally close to the data and serve users completely on the edge.</p>
    <p class="normal">This is the promise of <a id="_idIndexMarker1872"/>edge computing. Running Kubernetes at the edge allows organizations to bring the processing of data closer to the source of data generation, reducing the latency and bandwidth requirements of sending data to a centralized data center or cloud. This results in improved response times and real-time processing of data, making it an ideal solution for use cases such as industrial IoT, autonomous vehicles, and other real-time data processing applications.</p>
    <p class="normal">However, running Kubernetes at the edge comes with its own set of challenges. Edge devices are typically resource-constrained, making it necessary to optimize the deployment of Kubernetes for the edge environment. Organizations must also consider the network connectivity and reliability of edge devices, as well as the security and privacy implications of deploying a Kubernetes cluster at the edge.</p>
    <p class="normal">Projects like CNCF KubeEdge (<a href="https://kubeedge.io"><span class="url">https://kubeedge.io</span></a>) can<a id="_idIndexMarker1873"/> get you started.</p>
    <p class="normal">However, we will focus for the rest of this chapter on large-scale Kubernetes-based systems in the cloud.</p>
    <h1 id="_idParaDest-798" class="heading-1">Building effective processes for large-scale Kubernetes deployments</h1>
    <p class="normal">To run multi-cluster Kubernetes systems in production, you must develop a set of effective processes and best practices that encompass every aspect of the operation. Here are some of the critical areas to address.</p>
    <h2 id="_idParaDest-799" class="heading-2">The development lifecycle</h2>
    <p class="normal">The development lifecycle<a id="_idIndexMarker1874"/> of a multi-cluster <a id="_idIndexMarker1875"/>Kubernetes-based system in production can be a complex process, but it is possible to streamline it with the right approach.</p>
    <p class="normal">You should absolutely implement a CI/CD pipeline that automatically builds, tests, and deploys code changes. This pipeline should be integrated with version control systems such as Git, and it should also include automated testing to ensure code quality.</p>
    <p class="normal">It’s important to manage the ownership and approval process for different areas of the code base.</p>
    <p class="normal">Kubernetes namespaces can provide a convenient way to organize workloads and corresponding software assets and associate them with teams and stakeholders.</p>
    <p class="normal">You should have a complete track of changes, ongoing builds, and deployments, and the ability to freeze activity and roll back changes of each workload.</p>
    <p class="normal">It’s also important to control the gradual deployment to different clusters and regions to avoid a situation where a bad change is deployed simultaneously across the board and brings the entire system down.</p>
    <h2 id="_idParaDest-800" class="heading-2">Environments</h2>
    <p class="normal">Code review and careful <a id="_idIndexMarker1876"/>incremental deployment <a id="_idIndexMarker1877"/>while monitoring the outcome are required, but they are insufficient for large enterprise systems with multiple Kubernetes clusters. Some changes might display a negative impact only after running for a while or under certain conditions, which will escape the control mechanisms we mentioned earlier. The best practice is to have multiple runtime environments such as production, staging, and development. The exact division of environments can vary, but you typically need at least a production environment, which is the actual system that manages all the data and your users interact with, and a staging environment, which mimics the production system and where you can test changes and new versions without impacting your users and risking bringing the <a id="_idIndexMarker1878"/>production <a id="_idIndexMarker1879"/>environment down.</p>
    <p class="normal">Let’s consider some aspects of using multiple environments.</p>
    <h3 id="_idParaDest-801" class="heading-3">Separated environments</h3>
    <p class="normal">It is critical that the staging<a id="_idIndexMarker1880"/> environment can’t accidentally contaminate and impact <a id="_idIndexMarker1881"/>the production environment. For example, if you run a stress test in staging and some workloads in the staging environment are misconfigured and hit production endpoints, you will have a very unpleasant time untangling the mess.</p>
    <p class="normal">Rigid network segmentation where the staging environment is unable to reach the production environment is a good first step. You will still need to be mindful of the interaction between staging and production through public endpoints. The staging workloads should not have secrets and identities that allow production access.</p>
    <h3 id="_idParaDest-802" class="heading-3">Staging environment fidelity</h3>
    <p class="normal">The primary motivation for the staging <a id="_idIndexMarker1882"/>environment is to test changes and interactions <a id="_idIndexMarker1883"/>with other sub-systems before deploying a change to production. This means that the staging environment should mimic the production environment as much as possible. However, running an exact replica of the production environment is prohibitively expensive.</p>
    <p class="normal">The staging environment should be configured and set up using the same automated CI/CD pipeline that is able to deploy staging and production.</p>
    <p class="normal">Staging infrastructure and resources should also be provisioned using the same tools as production although there will typically be fewer resources allocated to staging.</p>
    <p class="normal">You may want to be able to temporarily scale down or even completely shut down some parts of the staging environment and be able to bring them back up only when necessary for running large-scale tests in staging.</p>
    <h3 id="_idParaDest-803" class="heading-3">Resource quotas</h3>
    <p class="normal">Resource quotas in staging and production <a id="_idIndexMarker1884"/>can ensure that misconfiguration or even an attack doesn’t cause excessive resource usage.</p>
    <h3 id="_idParaDest-804" class="heading-3">Promotion process</h3>
    <p class="normal">Once a change has been thoroughly tested in staging, there should be a clear promotion process for deploying it to production. The <a id="_idIndexMarker1885"/>process may be different for different components depending on the scope and impact of the change. The promotion may be completely automatic where the CI/CD pipeline detects that staging tests are completed successfully and moves ahead with production deployment or involve extra steps and possibly another explicit deployment to production.</p>
    <h2 id="_idParaDest-805" class="heading-2">Permissions and access control</h2>
    <p class="normal">When you manage <a id="_idIndexMarker1886"/>a constellation of <a id="_idIndexMarker1887"/>Kubernetes clusters running on cloud infrastructure, you need to pay a lot of attention to the permission model and your access control. This builds on the previous best practices of the development lifecycle and environments.</p>
    <h3 id="_idParaDest-806" class="heading-3">The principle of least privilege</h3>
    <p class="normal">The principle of least privilege<a id="_idIndexMarker1888"/> comes from the security field, but it is useful even beyond security for reliability, performance, and cost. Actors – either humans or <a id="_idIndexMarker1889"/>workloads – should not have more permissions than necessary to accomplish their tasks. By reducing access to the bare minimum, you ensure that no accidental or malicious activity occurs for forbidden resources.</p>
    <p class="normal">Also, when investigating an incident, it automatically narrows down the possible culprits to those who had the permissions to act on the misconfigured resource or take the invalid action.</p>
    <p class="normal">If you follow the GitOps model, it is possible to create a workflow where every change to your clusters and your infrastructure is done by CI/CD and dedicated tooling. Human engineers have only read-only access. Some special exceptions can be made (see the <em class="italic">Break glass</em> section in this chapter).</p>
    <h3 id="_idParaDest-807" class="heading-3">Assign permissions to groups</h3>
    <p class="normal">It is highly <a id="_idIndexMarker1890"/>recommended to assign permissions to groups or teams as opposed to individuals. Even if just a single person is currently carrying out a task that requires some set of permissions, you should define a group where this person is the single member. That will make it easier to add other people later or replace the person.</p>
    <h3 id="_idParaDest-808" class="heading-3">Fine-tune your permission model</h3>
    <p class="normal">However, sometimes <a id="_idIndexMarker1891"/>too strict a permission model can be detrimental. You’ll have to maintain a very fine-grained set of permissions to a large set of resources. Whenever the slightest change occurs such that another action is needed on some resource, you’ll have to modify the permissions. </p>
    <p class="normal">Find the golden path between granting super-admin permissions to everyone and painstakingly creating hundreds and thousands of roles for each and every resource.</p>
    <p class="normal">In particular, consider relaxing the permission model in the development environment and potentially in the staging environment too. This is where a lot of experiments take place and where you discover what actions you need to perform and what permissions are actually necessary and then you can tweak your permissions model before deploying to production.</p>
    <h3 id="_idParaDest-809" class="heading-3">Break glass</h3>
    <p class="normal">Sometimes, your CI/CD <a id="_idIndexMarker1892"/>pipeline itself will be broken or, due to incomplete coverage, some resources may have been provisioned manually. In these cases, human engineers must intervene and, so to speak, “<em class="italic">break the glass</em>” and take direct action against Kubernetes or cloud infrastructure.</p>
    <p class="normal">It is recommended to have a formal process of acquiring <strong class="keyWord">break glass</strong> access, who is allowed to have it, how long it lasts, and record who had it.</p>
    <p class="normal">This brings us to the next section about observability.</p>
    <h2 id="_idParaDest-810" class="heading-2">Observability</h2>
    <p class="normal">A <a id="_idIndexMarker1893"/>comprehensive observability stack is an <a id="_idIndexMarker1894"/>absolute must. Complex systems composed of multiple Kubernetes clusters can be reasoned about and understood theoretically. You must have a complete record of events from cloud providers, Kubernetes itself, and workloads. Your CI/CD pipeline and other tools must also be fully observable.</p>
    <p class="normal">Let’s look at some elements of multi-cluster observability.</p>
    <h3 id="_idParaDest-811" class="heading-3">One-stop shop observability</h3>
    <p class="normal">Cloud <a id="_idIndexMarker1895"/>providers and Kubernetes itself provide a lot of observability in the form of logs and metrics out of the box. However, those are typically organized at the cluster level. If you are dealing with some widespread issue across multiple clusters, it is very difficult, and at a certain scale impossible, to go into each individual cluster, extract the observability data, and try to make sense of it. You must ship all observability data into a single centralized system where it can be aggregated, summarized, and be ready for multi-cluster analysis and response.</p>
    <h3 id="_idParaDest-812" class="heading-3">Troubleshooting your observability stack</h3>
    <p class="normal">Your observability <a id="_idIndexMarker1896"/>stack is an indispensable component of your system. If it is down or degraded, you may be flying blind and unable to effectively respond to issues. Moreover, a cross-cluster issue may impact your observability stack as it impacts your entire system. Consider this scenario very carefully and make sure you have plenty of redundancies and observability alternatives if your primary observability stack is not up to the task temporarily. For example, you may rely on in-cluster observability solutions if your centralized observability stack is compromised. If you want complete redundancy, you may have a parallel observability stack on two cloud providers.</p>
    <p class="normal">Consider testing these harsh scenarios of CI/CD pipeline and observability stack outages and see how you operate.</p>
    <p class="normal">Let’s look more specifically into different types of infrastructure and how to handle them at scale.</p>
    <h1 id="_idParaDest-813" class="heading-1">Handling infrastructure at scale</h1>
    <p class="normal">One of the most<a id="_idIndexMarker1897"/> demanding tasks when running large-scale multi-cluster Kubernetes in the cloud is dealing with the cloud infrastructure. In some respects, it is much better than being responsible for low-level compute, network, and storage infrastructure. However, you lose a lot of control, and troubleshooting issues is challenging.</p>
    <p class="normal">Before diving into each category of infrastructure, let’s look at some general cloud-level considerations.</p>
    <h2 id="_idParaDest-814" class="heading-2">Cloud-level considerations</h2>
    <p class="normal">In the cloud, you organize<a id="_idIndexMarker1898"/> your resources in entities such as AWS accounts, GCP projects, and Azure subscriptions. An organization may have multiple such groups, and each one has its own limits and quotas. For the sake of brevity, let’s call them accounts. Enterprise organizations’ infrastructure requirements will exceed the capacity of a single account. It’s critical to decide how to break down your infrastructure into different accounts. One good heuristic is to separate environments – production, staging, and development – into separate accounts. Account-level isolation is beneficial for these environments. However, this may not be sufficient and within a single environment, you might need more resources than can fit in one account.</p>
    <p class="normal">Having a solid account management strategy is key. Accounts can also be a boundary of access control as even account administrators can’t access other accounts.</p>
    <p class="normal">Consult with your security team about security-motivated account breakdowns.</p>
    <p class="normal">Another important aspect is the breakdown of regions. If you manage infrastructure across multiple regions and workloads in these regions communicate with each other, then this has severe latency and cost implications. In particular, cross-region egress is typically not free.</p>
    <p class="normal">Let’s look at each category of infrastructure.</p>
    <h2 id="_idParaDest-815" class="heading-2">Compute</h2>
    <p class="normal">Compute infrastructure<a id="_idIndexMarker1899"/> for Managed Kubernetes includes the <a id="_idIndexMarker1900"/>Kubernetes clusters themselves and their worker nodes. The worker nodes are typically grouped into node pools, which is not a Kubernetes concept. How you break down your system into Kubernetes clusters and what types of node pools exist in each cluster will greatly impact your ability to manage the system at scale.</p>
    <p class="normal">Ideally, you can treat clusters like cattle, provision identical clusters, and easily add or remove clusters in different locations. Each cluster will have the same node pools.</p>
    <p class="normal">This uniform and consistent organization of clusters is not always possible. There are sometimes reasons to have different clusters for particular purposes. You should still strive for a small <a id="_idIndexMarker1901"/>number of cluster types that can be replicated<a id="_idIndexMarker1902"/> easily.</p>
    <h3 id="_idParaDest-816" class="heading-3">Design your cluster breakdown</h3>
    <p class="normal">Clusters in the<a id="_idIndexMarker1903"/> cloud typically allocate private IP<a id="_idIndexMarker1904"/> addresses to nodes and pods from a virtual network that resides in one region. Yes, it’s possible to have wide clusters that cross regions, but this is the exception and not the rule. It is highly recommended to manage clusters as cattle if possible and automatically provision clusters across all regions of operations.</p>
    <h3 id="_idParaDest-817" class="heading-3">Design your node pool breakdown</h3>
    <p class="normal">Node pools are<a id="_idIndexMarker1905"/> groups of nodes that have the<a id="_idIndexMarker1906"/> same instance type. They can typically autoscale to accommodate the needs of the cluster with the help of the cluster autoscaler. How you choose what node pools to provision in your clusters is a fundamental decision that impacts performance, cost, and operational complexity.</p>
    <p class="normal">We will dive into a deeper discussion on this later in the chapter.</p>
    <h2 id="_idParaDest-818" class="heading-2">Networking</h2>
    <p class="normal">Networking is a very dynamic area <a id="_idIndexMarker1907"/>of infrastructure. There are many<a id="_idIndexMarker1908"/> degrees of freedom. The interplay between latency and bandwidth has nuances. Workloads can’t request a certain amount of bandwidth or guaranteed latency. In addition, there are a lot of external factors that impact the connectivity, reachability, and performance of your network. Let’s look at some of the important topics we have to consider and plan for.</p>
    <h3 id="_idParaDest-819" class="heading-3">IP address space management</h3>
    <p class="normal">When you <a id="_idIndexMarker1909"/>run a multi-cluster Kubernetes-based system, every<a id="_idIndexMarker1910"/> pod in a cluster gets a unique private IP address. However, if you want to connect multiple clusters and have workloads in one cluster reach workloads in other clusters via their private IP address, then two conditions must exist:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">The networks of the different clusters must be peered together.</li>
      <li class="numberedList">The private IP address of pods must be unique across all clusters.</li>
    </ol>
    <p class="normal">This requires centralized management of the private IP address space and carefully assigning IP ranges to different clusters.</p>
    <p class="normal">Cloud providers differ a little in their approach to assigning IP address ranges to clusters. AKS requires that each cluster belongs to a VNet with its own IP address range and then subnets are assigned to nodes and pods with sub-ranges from the VNet IP address range. GKE comes with a default network that has no IP address range of its own. Clusters are provisioned with subnets that have their own IP address ranges.</p>
    <p class="normal">In addition, services require their own IP addresses too, and possibly some other components.</p>
    <p class="normal">The entire private IPv4 address space consists of several blocks:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">10.0.0.0/8</code> (class A)</li>
      <li class="bulletList"><code class="inlineCode">172.16.0.0/12</code> (class B)</li>
      <li class="bulletList"><code class="inlineCode">192.168.0.0/8</code> (class C)</li>
    </ul>
    <p class="normal">At scale, the most important one is <code class="inlineCode">10.0.0.0/8</code>, which consists of 2^24 IP addresses, which is more than 16 million addresses. That is a lot of IP addresses, but if you don’t plan carefully, you may cause fragmentation and run out of large blocks even if you have plenty of unused IP addresses.</p>
    <p class="normal">Here are some best practices for managing the IP address space:</p>
    <ul>
      <li class="bulletList">Allocate CIDR blocks to pods, nodes, and services that will be sufficient and utilized without too much space.</li>
      <li class="bulletList">Be aware of Kubernetes and cloud provider limits in terms of the number of supported nodes and pods.</li>
      <li class="bulletList">Consider network peering and service meshes spanning clusters.</li>
      <li class="bulletList">Make sure you use non-overlapping CIDR blocks for connected clusters.</li>
      <li class="bulletList">Use proper tools to manage the address space.</li>
      <li class="bulletList">Consider the impact of pod density on IP address space (e.g., on AKS, IP addresses are pre-allocated to the max number of pods on a node even if not utilized).</li>
      <li class="bulletList">Be aware<a id="_idIndexMarker1911"/> of limits such as <a id="_idIndexMarker1912"/>the maximum number of IP addresses available in a region.</li>
    </ul>
    <h3 id="_idParaDest-820" class="heading-3">Network topology</h3>
    <p class="normal">All <a id="_idIndexMarker1913"/>cloud providers offer a virtual network or VPC concept. All <a id="_idIndexMarker1914"/>cloud providers also have the concept of a region, which is a geographical area where cloud providers host resources. Specifically, virtual networks are always confined to a single region. Since a Kubernetes cluster in the cloud is associated with a virtual network, it follows that a single Kubernetes cluster can’t span more than one region. This has implications for availability and reliability. If you want to survive a regional outage, you need to run each critical workload across multiple clusters in different regions. Moreover, all these clusters typically need to be connected to each other. We will discuss this more in the <em class="italic">Cross-cluster communication</em> section that follows. However, as far as network topology goes, it may be better to have multiple clusters in the same region share the same virtual network.</p>
    <h3 id="_idParaDest-821" class="heading-3">Network segmentation</h3>
    <p class="normal">Network<a id="_idIndexMarker1915"/> segmentation is about dividing a network into smaller subnets. In the context of Kubernetes, the most important subnets are the subnets<a id="_idIndexMarker1916"/> for nodes, pods, and services. In some cases, the nodes and pods share the same subnet and in other cases, there are separate subnets for nodes and pods. Regardless, you need to plan and understand how many nodes and pods your cluster can accommodate and size your cluster subnets accordingly.</p>
    <h3 id="_idParaDest-822" class="heading-3">Cross-cluster communication</h3>
    <p class="normal">When running multiple <a id="_idIndexMarker1917"/>Kubernetes clusters, it is often<a id="_idIndexMarker1918"/> beneficial to consider groups of clusters as a single conceptual cluster. This means that pods in any cluster can directly reach pods in other clusters via their private IP address. This flat IP address model is an extension of the standard Kubernetes networking model within a single cluster to multiple clusters. This requires two elements we discussed earlier:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Non-conflicting IP address ranges for pods across all connected clusters.</li>
      <li class="numberedList">Network peering between clusters.</li>
    </ol>
    <p class="normal">The network peering <a id="_idIndexMarker1919"/>requirement might be <a id="_idIndexMarker1920"/>tedious as clusters come and go. Having a single regional virtual network reduces the overhead and allows peering just the regional virtual networks. However, if you run a lot of clusters in the same region sharing the same virtual network, you might run into cloud provider limits that will stunt your growth. For example, on Azure, a VNet can have at most 64K unique IP addresses.</p>
    <h3 id="_idParaDest-823" class="heading-3">Cross-cloud communication</h3>
    <p class="normal">If your system spans multiple <a id="_idIndexMarker1921"/>cloud providers, you need to consider how to connect your Kubernetes clusters across cloud providers. There are several ways, with <a id="_idIndexMarker1922"/>different pros and cons.</p>
    <p class="normal">First, you may decide to avoid direct communication between clusters on different cloud providers. If Kubernetes clusters deployed on different clouds need to communicate with each other, they can do so through public APIs. This approach is simple but eliminates the idea of a unified conceptual cluster where pods can communicate directly with each other regardless of where they are.</p>
    <p class="normal">A site-to-site VPN is a communication method where different cloud providers can connect systems via BGP and establish a VPN connection to networks managed by another cloud provider via a VPN gateway that sits in front of the virtual networks. This establishes a secure channel; however, a VPN gateway is not trivial to set up and incurs a significant overhead.</p>
    <p class="normal">Direct connect (AKA direct peering) is another option that requires installing a router in a cloud provider point of presence. This method allows connecting Kubernetes clusters running in private data centers to clusters in the cloud. In addition, the performance is much better because there is no VPN gateway in the middle. The downside is that it is quite complicated to set up and you might have to comply with various requirements. It’s a good option for organizations with deep low-level networking expertise.</p>
    <p class="normal">Carrier or <a id="_idIndexMarker1923"/>partner peering is similar to direct <a id="_idIndexMarker1924"/>connect; however, you take advantage of the expertise of an established third party that specializes in providing this service and already has an established relationship and is certified with the cloud provider. You will have to pay for the service, of course.</p>
    <h3 id="_idParaDest-824" class="heading-3">Cross-cluster service meshes</h3>
    <p class="normal">Service meshes bring tremendous value to <a id="_idIndexMarker1925"/>Kubernetes, as we <a id="_idIndexMarker1926"/>discussed in <em class="chapterRef">Chapter 14</em>, <em class="italic">Utilizing Service Meshes</em>. When running multiple Kubernetes clusters in production, it is arguably even more important to connect all the clusters via a service mesh. The advanced capabilities and policies of a service mesh can be applied and manage connectivity and routing across all clusters.</p>
    <p class="normal">There are two approaches to consider here:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">A single fully connected service mesh.</li>
      <li class="numberedList">Divide your clusters into multiple meshes.</li>
    </ol>
    <p class="normal">The single fully connected single mesh aligns conceptually with the single conceptual Kubernetes cluster approach. Everything is straightforward. New clusters just join the mesh, and the mesh is configured to allow every pod to talk to every other pod (as long as the routing policies allow it).</p>
    <p class="normal">However, you might eventually run into scalability barriers as a single mesh means that the mesh control plane needs to handle policies for all workloads in all the clusters, and updating sidecars for all pods can put a lot of burden on your clusters.</p>
    <p class="normal">An alternative approach is to have multiple independent meshes. Pods in clusters that belong to a particular mesh can directly talk to pods in all other clusters in the same service mesh but must go through public endpoints to access clusters in other service meshes.</p>
    <p class="normal">The multi-mesh approach is more scalable but much more complicated. You need to consider how to divide your system into different service meshes and when new clusters join or leave the system, and how it impacts your overall architecture.</p>
    <p class="normal">The private IP address space management in the multi-service mesh case can be more nuanced too where different service meshes can have conflicting IP addresses. This means that you can manage the IP address space for each mesh separately.</p>
    <p class="normal">Service meshes offer another interesting solution to the cross-cluster connectivity story, which is the east-west gateway. With the east-west gateway approach, workloads in different clusters communicate indirectly through dedicated gateways in each cluster. This means that the private IP addresses of each cluster are unknown and there is an extra hop for each<a id="_idIndexMarker1927"/> cross-cluster<a id="_idIndexMarker1928"/> communication.</p>
    <h3 id="_idParaDest-825" class="heading-3">Managing egress at scale</h3>
    <p class="normal">Some systems need<a id="_idIndexMarker1929"/> to access external systems aggressively. Maybe you frequently fetch data from external systems or maybe the purpose of your system is to manage some external systems via APIs.</p>
    <p class="normal">There may be unique issues for egress traffic that require special attention. Some third-party organizations or even countries may have policies that block or throttle traffic coming from certain geographical areas or specific IP CIDR blocks. For example, China and its great firewall are famous for blocking and censoring a vast number of companies, such as Google and Facebook. If you run on GCP and need to access China, it might be a serious issue.</p>
    <p class="normal">Beyond total blocking, there may be limits and throttling in place if you try to access some third-party APIs at scale.</p>
    <p class="normal">If you persist in accessing those third-party APIs, you could even be reported, and your cloud provider could potentially impose various sanctions.</p>
    <p class="normal">Let’s consider some solutions to deal with these real-world problems.</p>
    <p class="normal">If your current cloud provider is not allowed to access your target destination, then you must establish an egress presence outside your cloud provider. This can be on another cloud provider or via an intermediate organization in good standing. This proxy approach can take many shapes, which is beyond the scope of this section.</p>
    <p class="normal">If you are getting throttled, then the issue may be that you send too many requests from the same source IP address. A good solution here is to create a pool of egress nodes with different public IP addresses and distribute your requests through multiple different IP addresses. It can also help if you rotate your public IP addresses periodically, which is pretty easy in the cloud by just re-creating instances, which receive new public IP addresses.</p>
    <p class="normal">The opposite issue is if you have an agreement with some third-party company and they specifically allow traffic from your organization by whitelisting some IP addresses you provide. In this case, you need to manage static public IP addresses that don’t change and ensure that all requests to that third-party organization go out through the whitelisted IP addresses.</p>
    <p class="normal">Finally, to <a id="_idIndexMarker1930"/>address the risk of being reported and flagged by your cloud provider, you may need to isolate your egress access to a separate account. Most cloud provider sanctions are at the account level. If your egress account is disabled by your cloud provider, at least it will not bring down the entire enterprise.</p>
    <h3 id="_idParaDest-826" class="heading-3">Managing the DNS at the cluster level</h3>
    <p class="normal">Large-scale <a id="_idIndexMarker1931"/>clusters with lots of pods and services may put a high load on CoreDNS, which is the internal DNS server of Kubernetes. It’s important to ensure sufficient DNS capacity since most internal communication between workloads in the cluster uses DNS names for addressing and not direct IP addresses.</p>
    <p class="normal">It is recommended to <a id="_idIndexMarker1932"/>use DNS autoscaling, which is often not enabled by default. See <a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/"><span class="url">https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/</span></a> for more details.</p>
    <h2 id="_idParaDest-827" class="heading-2">Storage</h2>
    <p class="normal">Storage is arguably the most <a id="_idIndexMarker1933"/>critical element of your infrastructure. This is where<a id="_idIndexMarker1934"/> your persistent data lives, which is the long-term memory of organizations.</p>
    <h3 id="_idParaDest-828" class="heading-3">Choose the right storage solutions</h3>
    <p class="normal">There are many <a id="_idIndexMarker1935"/>storage solutions available for Kubernetes clusters in the cloud, such as cloud-native blob storage, managed storage services, managed databases, and managed file systems. You should develop a deep understanding of the performance, durability, and cost of each storage solution and match them against your storage use cases.</p>
    <p class="normal">Your baseline should always be cloud-native blob storage (AKA buckets) like AWS S3, GCP Google Cloud Storage, or Azure Blob Storage. It’s hard to imagine a large-scale Managed Kubernetes enterprise that doesn’t use buckets.</p>
    <p class="normal">Then, consider more structured or high-level storage solutions. If you aim to stay cloud-agnostic, you may ignore cloud-based managed storage solutions and deploy your own solutions, as we saw in <em class="chapterRef">Chapter 6</em>, <em class="italic">Managing Storage</em>.</p>
    <p class="normal">At an enterprise <a id="_idIndexMarker1936"/>scale, it may be worthwhile considering different levels of access speed and cost for data at different levels of importance.</p>
    <h3 id="_idParaDest-829" class="heading-3">Data backup and recovery</h3>
    <p class="normal">Plan for data <a id="_idIndexMarker1937"/>backup and recovery. Your data is valuable. Data backup and recovery are crucial for production environments. Consider implementing data backup and recovery processes that are reliable and scalable, and make sure they are regularly tested and updated.</p>
    <p class="normal">You should also consider data retention policies and not automatically assume that all data must be kept forever.</p>
    <p class="normal">Of course, to comply with data privacy laws and regulations like the GDPR, you will need to build the ability to selectively delete data too.</p>
    <h3 id="_idParaDest-830" class="heading-3">Storage monitoring</h3>
    <p class="normal">Set up storage<a id="_idIndexMarker1938"/> monitoring. Period. Monitoring storage performance, usage, and capacity is essential for identifying and resolving issues before they impact the availability or performance of your applications. Set up monitoring and alerting for storage utilization, latency, and throughput. This is important for managed storage, but also for node storage where logs can easily accumulate and render a node un-operational.</p>
    <h3 id="_idParaDest-831" class="heading-3">Data security</h3>
    <p class="normal">Implement data <a id="_idIndexMarker1939"/>security measures. Protecting sensitive data and ensuring compliance with data protection regulations is critical for production environments. Implement access controls, encryption, and data security policies to safeguard your data.</p>
    <h3 id="_idParaDest-832" class="heading-3">Optimize storage usage</h3>
    <p class="normal">Kubernetes <a id="_idIndexMarker1940"/>clusters in the cloud can be expensive, and storage costs can add up quickly. Optimize your storage usage by deleting unused data, using data compression or deduplication, and setting up storage tiering.</p>
    <h3 id="_idParaDest-833" class="heading-3">Test and validate storage performance</h3>
    <p class="normal">Before deploying <a id="_idIndexMarker1941"/>applications in a<a id="_idIndexMarker1942"/> production environment, test and validate the performance of your storage solution to ensure it meets the performance requirements of your workloads.</p>
    <p class="normal">By considering these factors and implementing best practices for managing storage in production for Kubernetes clusters in the cloud, you can ensure reliable and scalable storage performance for your applications.</p>
    <p class="normal">Now that we have covered a lot of guidelines and best practices for managing cloud infrastructure at scale for Kubernetes, let’s shine a spotlight on the management of clusters and node pools, which is at the heart of managing multi-cluster Kubernetes in production.</p>
    <h1 id="_idParaDest-834" class="heading-1">Managing clusters and node pools</h1>
    <p class="normal">Managing your<a id="_idIndexMarker1943"/> clusters and<a id="_idIndexMarker1944"/> node pools is the top infrastructure administration activity for a large-scale Kubernetes-based enterprise. In this section, we will look at several crucial aspects, including provisioning, bin packing and utilization, upgrades, troubleshooting, and cost management.</p>
    <h2 id="_idParaDest-835" class="heading-2">Provisioning managed clusters and node pools</h2>
    <p class="normal">There are different<a id="_idIndexMarker1945"/> methods for provisioning clusters and node pools. You should <a id="_idIndexMarker1946"/>choose the method that is best for your use case wisely because failure here can result in devastating outages. Let’s review some options. All cloud providers offer cluster and node pool provisioning via APIs, CLIs, and UIs. I highly recommend avoiding directly using any of these methods and instead using GitOps-based declarative approaches. Here are some solid options to consider.</p>
    <h3 id="_idParaDest-836" class="heading-3">The Cluster API</h3>
    <p class="normal">The Cluster API is <a id="_idIndexMarker1947"/>an open-source project from the Cluster Lifecycle SIG. Its goal is to make provisioning, upgrading, and operating multiple Kubernetes clusters easier. It is focused on clusters’ and node pools’ lifecycles. However, it started mostly as a way to provision clusters using kubeadm. Managed cluster support on different cloud providers was added later, and it is still young. In particular, GKE is not supported (although you can provision Kubernetes clusters on GCP as an infrastructure layer). AKS and EKS are supported.</p>
    <p class="normal">The Cluster API has a lot of momentum, and if you don’t operate GKE, you should definitely look into it.</p>
    <h3 id="_idParaDest-837" class="heading-3">Terraform/Pulumi</h3>
    <p class="normal">Terraform<a id="_idIndexMarker1948"/> and Pulumi<a id="_idIndexMarker1949"/> are similar in their approach. They can provision clusters and node pools on all cloud providers. However, these tools on their own can’t respond to out-of-band changes and don’t monitor the state of the infrastructure after provisioning. Their internal state can deviate from the real world and that can cause difficult to recover from situations that require careful “surgery.” In particular, node pools often need to be provisioned or updated, and Terraform or Pulumi might not be up to the task. If you have a lot of experience with these tools and are aware of their quirks and special requirements, they may still be a good option for you.</p>
    <h3 id="_idParaDest-838" class="heading-3">Kubernetes operators</h3>
    <p class="normal">Another alternative is to use Kubernetes operators<a id="_idIndexMarker1950"/> that reconcile CRDs with cluster and node pool specs with the cloud provider. Under the covers, the operator will invoke Managed Kubernetes APIs from the cloud provider. This requires non-trivial work and expertise in writing Kubernetes operators but gives you the ultimate control.</p>
    <p class="normal">You may try to use Crossplane instead of writing your own operator; however, Crossplane support seems pretty basic and incomplete at the moment. One option to expand the scope is to use the <a id="_idIndexMarker1951"/>Upjet project (<a href="https://github.com/upbound/upjet"><span class="url">https://github.com/upbound/upjet</span></a>) to generate Crossplane controllers from Terraform providers.</p>
    <h2 id="_idParaDest-839" class="heading-2">Utilizing managed nodes</h2>
    <p class="normal">You can also try to use <a id="_idIndexMarker1952"/>managed nodes, so you never need to deal with provisioning node pools and nodes directly. All cloud providers offer some form of managed nodes such as GKE AutoPilot, EKS + Fargate, and AKS + ACI. For enterprise use cases, I believe you will need more control than fully managed node pools provide. It may be a good option for a subset of your workloads. However, at scale, you will want to optimize your resource usage and performance and the limitations of managed node pools might be too severe.</p>
    <p class="normal">Once you have figured out how to provision and manage your clusters and node pools, you should turn your attention to effectively using the resources you provisioned.</p>
    <h1 id="_idParaDest-840" class="heading-1">Bin packing and utilization</h1>
    <p class="normal">Cloud resources are expensive. Efficient usage of resources on Kubernetes has two parts: efficiently scheduling pods to nodes based on their resource requests, and pods actually using the resources they requested.</p>
    <p class="normal">Bin packing<a id="_idIndexMarker1953"/> means ensuring that the total sum of resource requests is as close as possible to the allocatable resources on the target node. Once a workload is scheduled to a node, it will not be evicted under normal conditions even if the node is highly underutilized, but components like the cluster autoscaler can help here.</p>
    <p class="normal">Resource utilization measures what percentage of the requested resource is actually used. Resource utilization is in general not fixed as the resource usage of workloads may vary widely throughout their lifetimes.</p>
    <p class="normal">There are a lot of nuances to<a id="_idIndexMarker1954"/> bin packing, resource utilization, and the interplay between them. For example, there are different resources such as CPU, memory, disk, and network. A node may have 100% bin packing for CPU, but only 20% bin packing memory. Network and non-ephemeral disks on the node are shared resources that pods can request to ensure they will always have a certain amount. This complicates operation and reliability. Let’s discuss some principles and concepts that can assist in navigating this complicated topic.</p>
    <h2 id="_idParaDest-841" class="heading-2">Understanding workload shape</h2>
    <p class="normal">Workload shape<a id="_idIndexMarker1955"/> is the ratio between the workload CPU requests and its memory requests. In the cloud, there is a standard ratio of 1 CPU to 4 GiB of memory. As a result, most VM types that you can choose offer resource capacities with this ratio. Some workloads need more memory or more CPU than this ratio. All cloud providers also offer high-memory VM types with a ratio of 1 CPU to 8 GiB of memory as well as high-CPU VM types with 1 CPU to 2 GiB of memory.</p>
    <p class="normal">Understanding the resource shape of your workloads is necessary to inform the VM types you choose to optimize your resource usage.</p>
    <p class="normal">For example, if a <a id="_idIndexMarker1956"/>workload requires 1 CPU and 8 GiB of memory and you schedule it on a VM type with a ratio of 1:4, you will need to run it on a node that has 2 CPUs and 8 GiB of memory. No other pod can run on this node since the original workload uses all the 8 GiB of memory. However, 1 CPU out of 2 is not used at all. It would have been much better to schedule the workload on a node with a VM type of 1:8 ratio, which ensures optimal bin packing.</p>
    <h2 id="_idParaDest-842" class="heading-2">Setting requests and limits</h2>
    <p class="normal">Setting requests <a id="_idIndexMarker1957"/>and limits for your workloads is a key for proper resource utilization. As you recall when you set requests for your pod’s containers, it will be scheduled to a node that has at least the requested number of resources available for the total sum of the requests of all containers. The requested resources are allocated for the exclusive use of each container for as long as the pod running on the node. The containers may use more resources than the requests if available. If you specify CPU limits and the container tries to use more CPU than the limit, then the pod may get throttled. If you specify a memory limit and the container tries to use more memory than the limit, then the container will be OOMKilled and restarted.</p>
    <p class="normal">It is best practice to set resource requests for CPU, memory, and even ephemeral storage if the container uses any. How do you know how much to request? You can start with a rough estimate and monitor the actual resource usage over time and fine-tune it later. </p>
    <p class="normal">But even this straightforward method has some subtleties. Suppose a workload uses between 2 CPUs and 4 CPUs with an average of 3 CPUs. Should you request 4 CPUs and know for sure that the workload will never get throttled? But then, you waste a whole CPU because the average usage is just 3 CPUs. If you request 3 CPUs, are you going to get throttled every time the workload needs more than 3 CPUs? That depends on the available CPU on the node the pod is scheduled to. If the overall CPU on the node is saturated because all the pods need a lot of CPU, then it is possible.</p>
    <p class="normal">On top of plain requests, you can also assign priorities to workloads, which allow you to control the destiny of high-priority workloads and ensure they take precedence over non-prioritized or low-priority workloads.</p>
    <p class="normal">Yes, scheduling is far from trivial. If you need a refresher, check out the <em class="italic">Understanding the design of the Kubernetes scheduler</em> section in <em class="chapterRef">Chapter 15</em>, <em class="italic">Extending Kubernetes</em>.</p>
    <p class="normal">Let’s turn our attention to<a id="_idIndexMarker1958"/> limits. A simple approach is to set limits equal to the requests. This ensures in general that containers will not use more resources than they requested, which makes bin packing easy. However, in real-world situations, the resource usage of workloads varies. It is often more economical to request less than the maximal usage or sometimes even less than the average usage. In this case, you may opt not to set limits at all or set the limits higher than the requests. For example, if a workload uses 1 to 4 CPUs, then you may decide to request 2 CPUs and set the limits to 4 CPUs. Requesting just 2 CPUs will allow packing more pods into the same node or schedule the pod into a smaller node. So, why set limits at all? Well, setting some limits ensures the pods don’t get out of control, hog all the CPU, and starve all other pods that may also set lower requests but actually need additional CPU.</p>
    <p class="normal">Setting memory high limits is even more important, especially for workloads that are more sensitive and that shouldn’t be restarted often since any attempt to use more allocated memory than the limit will result in a container restart.</p>
    <h2 id="_idParaDest-843" class="heading-2">Utilizing init containers</h2>
    <p class="normal">Some workloads need to do a lot of <a id="_idIndexMarker1959"/>work when they just start and then their resource requirements are lower. For example, a workload needs 10 GiB of memory and 4 CPUs to fetch some data and process it in memory before it is ready to handle requests. However, once it’s running, it doesn’t need more than 1 CPU and 4 GiB. It would be pretty wasteful to request 4 CPUs and 10 GiB if the pod is a long-running one. This is where init containers are very useful. You can split your workload into two containers. All the initialization work that requires 4 CPUs and 10 GiB can be done by an init container and then the main container can request just 1 CPU and 4 GiB.</p>
    <h2 id="_idParaDest-844" class="heading-2">Shared nodes vs. dedicated nodes</h2>
    <p class="normal">When designing your<a id="_idIndexMarker1960"/> node pools, you have two fundamental choices to <a id="_idIndexMarker1961"/>make. Shared node pools have multiple different workloads scheduled and run side by side on the same node. Dedicated node pools have a single workload taking over a single node (possibly multiple instances of the same workload).</p>
    <p class="normal">Shared node pools are simple. The extreme case is that you have just a single node pool and all pods are scheduled to nodes from this node pool. If you have multiple shared node pools (e.g., one with regular nodes and one with spot instances), then you need to assign taints to node pools and tolerations to workloads as well as dealing with node and pod affinity.</p>
    <p class="normal">Since you don’t know exactly which combination of pods will end up on which node, there might be inefficiencies with bin packing. However, as long as the overall average workload shape matches the resource ratio of your nodes, bin packing at a large scale should be close to optimal.</p>
    <p class="normal">Workloads can request the CPU, memory, and ephemeral storage they need. However, there are some shared resources on the node, like network and disk I/O, that you can’t easily carve out for your workload when other workloads on the same node might try to use the same resource.</p>
    <p class="normal">This is where dedicated node pools come in. Critical workloads like databases or event queues require predictable network and disk I/O. Scheduling such workloads on a dedicated node ensures the workload doesn’t have to worry about other workloads cannibalizing the shared resources.</p>
    <p class="normal">It makes sense in this case for the workload to request more than 50% of the standard resources like CPU or memory to ensure exactly one pod of the critical workload is scheduled on the node.</p>
    <p class="normal">Remember that system daemons will also run on the node and have higher priority. If your dedicated workload requests too many resources, it might become unschedulable.</p>
    <p class="normal">I have run into<a id="_idIndexMarker1962"/> this issue after an upgrade where the daemonsets <a id="_idIndexMarker1963"/>on the node required more resources and caused the dedicated workload to be unschedulable until it reduced its resource requests.</p>
    <h2 id="_idParaDest-845" class="heading-2">Large nodes vs, small nodes</h2>
    <p class="normal">In the cloud, nodes come in a<a id="_idIndexMarker1964"/> variety of sizes, from 1 core to tens or even <a id="_idIndexMarker1965"/>hundreds of cores. Should you have lots of small nodes or fewer large nodes?</p>
    <p class="normal">First and foremost, you must have nodes that your largest workloads fit into. For example, if a workload requests 8 CPUs, then you must have a node with at least 8 CPUs available.</p>
    <p class="normal">But what about much bigger nodes? There are advantages in terms of efficiency for larger nodes. In the cloud, when you provision (for example) a node with 1 CPU core and 4 GIB of memory, you don’t really get all these resources. First, the OS, the container runtime, and kube-proxy take their resources, then the additional processes the cloud provider decides to run on each node, then various sys daemonsets and your own daemonsets. Finally, what’s left is available for your workloads. All these processes and workloads that always run on every node need a lot of resources. However, the resources they require are not proportional to the size of the node. This means that on small nodes, a much smaller percentage of the resources you pay for will be available for your pods. Let’s look at an example. </p>
    <p class="normal">Here is the resource breakdown for a real node running on an AKS production cluster. It has a VM type of <strong class="screenText">Standard_F2s_v2</strong> (<a href="https://learn.microsoft.com/en-us/azure/virtual-machines/fsv2-series"><span class="url">https://learn.microsoft.com/en-us/azure/virtual-machines/fsv2-series</span></a>). It has 2 CPUs and 4 GIB of memory. However, the allocatable CPU and memory is 1.9 CPU and 2.1 GiB. Yes, this is correct. You barely get a little more than 50% of the memory available on the node:</p>
    <pre class="programlisting gen"><code class="hljs">Capacity:
  cpu:                2
  memory:             4019488Ki
Allocatable:
  cpu:                1900m
  memory:             2202912Ki
</code></pre>
    <p class="normal">But the story doesn’t end here. There are system daemonsets running in kube-system. You can find them with the following command:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po --field-selector spec.nodeName=&lt;node-name&gt; -n kube-system
</code></pre>
    <p class="normal">Let’s look at the resources requested by these workloads on our node:</p>
    <pre class="programlisting gen"><code class="hljs">  Namespace     Name                                  CPU Requests  Memory Requests 
  ---------     ----                                  ------------  ---------------    
  kube-system   azure-ip-masq-agent-8pkqx             100m (5%)     50Mi (2%)      
  kube-system   azure-npm-twjlx                       250m (13%)    300Mi (13%)      
  kube-system   cloud-node-manager-fv5gs              50m (2%)      50Mi (2%)        
  kube-system   csi-azuredisk-node-kqnn7              30m (1%)      60Mi (2%)        
  kube-system   csi-azurefile-node-h8zpw              30m (1%)      60Mi (2%)        
  kube-system   kube-proxy-lgzcf                      100m (5%)     0 (0%)           
</code></pre>
    <p class="normal">That’s a total of 0.56 CPU and 520Mi of memory. If we subtract it from the allocatable CPU and memory, we end up with 1.4 CPU and 1.58 GiB of memory.</p>
    <p class="normal">This is quite eye-opening. On a small node with 2 CPUs and 4 GIB of memory, we end up with 70% of the CPU and less than 40% of the memory. Beyond the cost implications, if you miscalculate and assume you can schedule, for example, a pod that requests 2 GIB of memory on a 4 GiB node, you’ll have an unpleasant surprise when your pod remains pending because it doesn’t fit on this node.</p>
    <p class="normal">Let’s look at large nodes. A <a id="_idIndexMarker1966"/>Standard_D64ads_v5 Azure VM has a whopping 64 <a id="_idIndexMarker1967"/>cores and 256 GiB of memory. It is undoubtedly a beast. Let’s look at its capacity and allocatable resources:</p>
    <pre class="programlisting gen"><code class="hljs">Capacity:
  cpu:                64
  memory:             263932684Ki
Allocatable:
  cpu:                63260m
  memory:             250707724Ki
</code></pre>
    <p class="normal">Here, we lost 740 mcpu (as opposed to 100 mcpu on the small node) and 17 GiB of memory. This sounds like a lot, but proportionally, it is much better. Let’s look at system workloads to get the full picture:</p>
    <pre class="programlisting gen"><code class="hljs">Namespace     Name                       CPU Requests  Memory Requests  
---------     ----                       ------------  ---------------
kube-system   azure-ip-masq-agent-lgzxq  50m (0%)      50Mi (0%)
kube-system   azure-npm-s5gsd            100m (0%)     300Mi (0%)
kube-system   cloud-node-manager-jntvt   10m (0%)      50Mi (0%)
kube-system   csi-azuredisk-node-srcfg   30m (0%)      60Mi (0%)
kube-system   csi-azurefile-node-gx247   75m (0%)      60Mi (0%)
kube-system   kube-proxy-xgppg           100m (0%)     0 (0%)
</code></pre>
    <p class="normal">That’s a total of 0.365 CPU and 520Mi of memory. Surprisingly, less CPU is requested than the small node and the memory requests are the same. If we subtract it from the allocatable CPU and memory, we end up with 62.9 CPU and 238.48 GiB of memory.</p>
    <p class="normal">On a large node with 64 cores and 256 GiB of memory, we end up with more than 98% of the CPU and more than 93% of the memory.</p>
    <p class="normal">This is a pretty clear victory for large nodes in terms of resource provisioning efficiency and getting more<a id="_idIndexMarker1968"/> resources<a id="_idIndexMarker1969"/> available for your workloads.</p>
    <p class="normal">However, there are additional nuances and considerations to consider. Let’s consider small and short-lived workloads.</p>
    <h2 id="_idParaDest-846" class="heading-2">Small and short-lived workloads</h2>
    <p class="normal">Suppose <a id="_idIndexMarker1970"/>we use large nodes, and our cluster is bin-packed very efficiently. Some deployment needs to scale up and create a new pod. If there is no room for the new pod in any of the existing nodes, then a new node must be provisioned. However, if the new pod is small, then we actually waste a lot of resources by running just one small pod on a large node. At scale, and when a lot of pods come and go pretty quickly, this may not be a problem. However, consider the following scenario – our cluster is normally running on 100 large nodes. During a temporary spike of activity, our clusters scaled up to 200 large nodes and then the activity went back to normal. Our resource utilization is now 50% (the cluster needs 100 nodes out of 200). In an ideal world, the cluster autoscaler will eventually scale down empty nodes until we have 100 properly bin-packed nodes. But, in the real world, especially in the presence of small short-lived pods, new pods may get scheduled arbitrarily to all 200 nodes and the autoscaler might have a difficult time scaling down. We will see later, in the custom scheduling section, some options.</p>
    <p class="normal">Another issue with short-lived workloads <a id="_idIndexMarker1971"/>is that even if they have room on an existing node, they can still waste resources if they take a while to get ready. Consider a pod that takes 1 minute to get ready and runs, on average, for 1 minute. This pod with optimal utilization of its resources still can do better than 50% because after it gets scheduled, it reserves its resources on the node for 2 minutes, but actually does work for only 1 minute. If the pod needs to pull its image, then it can easily take several minutes to get ready.</p>
    <p class="normal">The Kubernetes scheduler is very sophisticated and can be extended too, as we covered in <em class="chapterRef">Chapter 15</em>, <em class="italic">Extending Kubernetes</em>. The issues with the inefficient scheduling of pods in the different use cases we mentioned could potentially be addressed by choosing a different scoring strategy. The default scoring<a id="_idIndexMarker1972"/> strategy of <strong class="keyWord">RequestedToCapacityRatio</strong> is intended to evenly distribute workloads across all nodes. This is not ideal for tight bin packing. The <strong class="keyWord">MostAllocated</strong> scoring strategy<a id="_idIndexMarker1973"/> may be preferable here.</p>
    <p class="normal">Check out <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/resource-bin-packing"><span class="url">https://kubernetes.io/docs/concepts/scheduling-eviction/resource-bin-packing </span></a>for more details.</p>
    <h2 id="_idParaDest-847" class="heading-2">Pod density</h2>
    <p class="normal">Pod density is the<a id="_idIndexMarker1974"/> maximum number of pods per node (the Kubernetes default is 110). As mentioned earlier, some resources like private IP addresses or system daemon CPU and memory may be correlated with the pod density. If your pod density is too high, then you may waste the resources that were pre-allocated to support many pods on each node. However, if you set the pod density too low, then you may not be able to schedule enough pods to run on the node. Let’s consider a large node with 64 CPU cores and 256 GiB of memory. If the pod density is 100, then at most 100 pods can run on this node. Suppose we have a lot of small pods that use only 10 mcpu and 100 MiB of memory. 100 pods need only 10 CPU cores and 10 GiB of memory combined. If 100 such pods get scheduled to one large node, the node will be highly underutilized. 54 CPU cores and 246 GIB of memory will be wasted.</p>
    <p class="normal">If you go with the shared node pool model, then it’s an arbitrary mix of pods with different workload shapes, and resource requirements can get scheduled to nodes.</p>
    <h2 id="_idParaDest-848" class="heading-2">Fallback node pools</h2>
    <p class="normal">Cloud providers <a id="_idIndexMarker1975"/>suffer from temporary capacity issues from time to time, and as a result, are unable to provision new nodes. In addition, spot instances may disappear at any time if there is a lot of demand for regular nodes. The good news is that these outages are a zonal affair and also are typically limited to a specific instance type or VM family.</p>
    <p class="normal">A good strategy to address this issue is to use fallback node pools.</p>
    <p class="normal">A fallback node pool<a id="_idIndexMarker1976"/> is an empty node pool with autoscaling disabled that has the same labels and taints as another active node pool but with a different VM family or a different node type (e.g., regular vs. spot). If the active node pool is unable to provision more nodes and there are pending pods, then the back node pool can be resized and/or become auto-scaling. This will allow the pending pods to be scheduled to the backup node pool until the situation with the native node pool is resolved.</p>
    <p class="normal">If you choose this path, you need to come up with a proper procedure to activate the backup node pool, which includes detection of issues in the active node pool, a manual or automated process for backup node pool activation, and a scale-back process when the active node pool is back to normal.</p>
    <p class="normal">It is very important to ensure the backup node pool has enough quota to replace its active node pool <a id="_idIndexMarker1977"/>when needed.</p>
    <p class="normal">This was a very thorough treatment of bin packing and resource utilization. Let’s turn our attention to upgrades.</p>
    <h1 id="_idParaDest-849" class="heading-1">Upgrading Kubernetes</h1>
    <p class="normal">Upgrading <a id="_idIndexMarker1978"/>Kubernetes can be a very stressful operation. A hasty upgrade might remove support for resource versions, and if you have unsupported resources deployed, you will encounter catastrophic failures. Using Managed Kubernetes has its pros and cons. When it comes to upgrades, there is, at any point in time, a range of supported versions. </p>
    <p class="normal">You may upgrade to more recent supported versions. However, if you delay and neglect to upgrade, then the cloud provider will upgrade your clusters and node pools automatically once you fall behind the cutting edge of supported versions. Let’s look at the various elements of upgrading Kubernetes you must be on top of.</p>
    <h2 id="_idParaDest-850" class="heading-2">Know the lifecycle of your cloud provider</h2>
    <p class="normal">Cloud providers can’t support just any<a id="_idIndexMarker1979"/> Kubernetes version in existence. It is critical to know when the current version of your clusters and node pools is going to be defunct. All cloud providers have a methodical process and share the information broadly. Here are the locations for each of the three major cloud providers:</p>
    <ul>
      <li class="bulletList">AWS EKS: <a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html"><span class="url">https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html</span></a></li>
      <li class="bulletList">Azure AKS: <a href="https://learn.microsoft.com/en-us/azure/aks/supported-kubernetes-versions"><span class="url">https://learn.microsoft.com/en-us/azure/aks/supported-kubernetes-versions</span></a></li>
      <li class="bulletList">Google GKE: <a href="https://cloud.google.com/kubernetes-engine/docs/release-schedule"><span class="url">https://cloud.google.com/kubernetes-engine/docs/release-schedule</span></a></li>
    </ul>
    <p class="normal">For example, AKS, at the time of writing, supports versions 1.23 through 1.26. In addition, each version has an official end-of-life date. For example, the end-of-life date for 1.23 is April 2023. If your cluster is still on 1.23, then AKS may upgrade your cluster automatically to version 1.24. The process of cloud provider upgrade is gradual, done region by region, and might take several weeks.</p>
    <p class="normal">All cloud providers offer an API and CLI to check the exact list of versions (including patch versions) in every region.</p>
    <p class="normal">For example, at the moment, these are versions supported on AKS for the Central US region:</p>
    <pre class="programlisting gen"><code class="hljs">$ az aks get-versions --location centralus --output table
KubernetesVersion    Upgrades
-------------------  -----------------------
1.26.0(preview)      None available
1.25.5               1.26.0(preview)
1.25.4               1.25.5, 1.26.0(preview)
1.24.9               1.25.4, 1.25.5
1.24.6               1.24.9, 1.25.4, 1.25.5
1.23.15              1.24.6, 1.24.9
1.23.12              1.23.15, 1.24.6, 1.24.9
</code></pre>
    <p class="normal">As you can see, for<a id="_idIndexMarker1980"/> each minor version, there are several patch versions. It is even nice enough to mention which versions you may upgrade to yourself. Due to security concerns, the cloud provider may drop support for patch versions at any time.</p>
    <p class="normal">Let’s talk about the upgrade process of the control plane.</p>
    <h2 id="_idParaDest-851" class="heading-2">Upgrading clusters</h2>
    <p class="normal">When using Managed<a id="_idIndexMarker1981"/> Kubernetes in the cloud, you are not responsible for the operation of the control plane, but you still need to manage the upgrade process. You have two options:</p>
    <ul>
      <li class="bulletList">Auto upgrade</li>
      <li class="bulletList">Manual upgrade</li>
    </ul>
    <p class="normal">In an auto upgrade, the cloud provider will update your cluster according to their schedule, but you still must ensure that the versions of resources in your cluster are compatible with the new version. A manual upgrade requires you to upgrade yourself but gives you more control over timing. For example, you may choose to update earlier to benefit from some new features.</p>
    <p class="normal">Remember that a manual upgrade doesn’t mean you can stay on the same Kubernetes version forever. The cloud provider will forcefully upgrade you if you fall behind the minimal supported version.</p>
    <p class="normal">Kubernetes releases a new version roughly every 3 months. Cloud providers support roughly 4 versions. This means that if you just upgraded to the latest supported version, you may hold off for about a year on upgrades, but then you will be on the minimal supported version, which means you will now have to upgrade every 3 months.</p>
    <p class="normal">Note that you should upgrade the control plane one minor version at a time. If you are on version 1.24, and you want to upgrade to 1.26, you have to upgrade to 1.25 first and then from 1.25 to 1.26.</p>
    <p class="normal">The bottom line is that <a id="_idIndexMarker1982"/>upgrading the Kubernetes control plane is a standard operation that takes place multiple times per year. You should have a streamlined process for it.</p>
    <p class="normal">Let’s look at what is involved.</p>
    <h3 id="_idParaDest-852" class="heading-3">Planning an upgrade</h3>
    <p class="normal">You should <a id="_idIndexMarker1983"/>plan your upgrades and coordinate them with cluster users. Control plane upgrades typically take 20-45 minutes. This is a non-disruptive operation for your workloads. Your workloads will keep running, and new pods will be scheduled to existing nodes. However, node pool operations might be blocked during the control plane upgrade.</p>
    <p class="normal">If you’re running multiple clusters with a redundancy scheme, it is best to perform the upgrades gradually and start with non-critical clusters (e.g., a development or staging environment).</p>
    <p class="normal">I recommend having owners (engineers or teams) for every namespace. Notify all owners about upcoming upgrades so they can reserve time for converting incompatible resources.</p>
    <h3 id="_idParaDest-853" class="heading-3">Detecting incompatible resources</h3>
    <p class="normal">The main <a id="_idIndexMarker1984"/>concern with an upgrade is that the functionality of your system will be compromised or completely broken because it uses resources that are not supported anymore. In most cases, a specific version of a resource will be removed, and a newer version will be available.</p>
    <p class="normal">But you don’t have to wait until the last minute to scramble and replace removed resources or versions. Kubernetes has a deprecation policy and resources will be deprecated for several versions before they are fully removed. I suggest making sure before each upgrade that all deprecated resources are updated or replaced. This will ensure that the upgrade process is not stressful because, even if you didn’t manage to update all resources, the deprecated resources are still going to be supported by the new version and you will have some extra time to update them before they are fully removed.</p>
    <p class="normal">Kubernetes publishes a migration guide with details about deprecated and removed APIs in each version. See <a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide"><span class="url">https://kubernetes.io/docs/reference/using-api/deprecation-guide</span></a>.</p>
    <p class="normal">For example, Kubernetes 1.25 stopped serving the <code class="inlineCode">CronJob</code> resource with the API version of <code class="inlineCode">batch/v1beta1</code>. Instead, the <code class="inlineCode">batch/v1</code> <code class="inlineCode">CronJob</code> resource has been available since Kubernetes 1.21. Ideally, after you upgrade to Kubernetes 1.21, you have updated all your <code class="inlineCode">CronJob</code> resources to use <code class="inlineCode">batch/v1</code>, and by the time you upgrade to Kubernetes 1.25, the fact that <code class="inlineCode">batch/v1beta1</code> is removed is not an issue because you are already on the supported version.</p>
    <p class="normal">There are several ways to make sure you detect all deprecated and/or removed resources that you currently use. You can use the manual method of reading the deprecation guide and just scanning your code and detecting incompatible resources. Most releases don’t have a lot of deprecations or removals. However, some releases may have up to ten different resources that are being deprecated or removed. For example, Kubernetes 1.25 stopped serving seven different resource versions.</p>
    <p class="normal">A more systematic way is to use a tool like <code class="inlineCode">kube-no-trouble</code> (<a href="https://github.com/doitintl/kube-no-trouble"><span class="url">https://github.com/doitintl/kube-no-trouble</span></a>), which scans your clusters and can output a list of deprecated resources.</p>
    <p class="normal">Here is how to install it:</p>
    <pre class="programlisting gen"><code class="hljs">$ sh -c "$(curl -sSL 'https://git.io/install-kubent')"
&gt;&gt;&gt; kubent installation script &lt;&lt;&lt;
&gt; Detecting latest version
&gt; Downloading version 0.7.0
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 11.7M  100 11.7M    0     0  4154k      0  0:00:02  0:00:02 --:--:-- 8738k
&gt; Done. kubent was installed to /usr/local/bin/.
</code></pre>
    <p class="normal">I have a 1.25 cluster <a id="_idIndexMarker1985"/>that doesn’t contain any deprecated resources at the moment. However, in Kubernetes 1.26, the <strong class="keyWord">HorizontalPodAutoscaler</strong> of <a id="_idIndexMarker1986"/>version autoscaling/v2beta2 will be removed as it has been deprecated since Kubernetes 1.23. Let’s create such a resource. There is a <code class="inlineCode">kyverno</code> deployment in the cluster:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get deploy -n kyverno
NAME      READY   UP-TO-DATE   AVAILABLE   AGE
kyverno   1/1     1            1           64d
</code></pre>
    <p class="normal">Here is an HPA that sets the min replicas to 1 and the max replicas to 3:</p>
    <pre class="programlisting gen"><code class="hljs">$ cat &lt;&lt;EOF | k apply -f -
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: kyverno
  namespace: kyverno
spec:
  maxReplicas: 3
  metrics:
  - resource:
      name: cpu
      target:
        averageUtilization: 80
        type: Utilization
    type: Resource
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kyverno
EOF
Warning: autoscaling/v2beta2 HorizontalPodAutoscaler is deprecated in v1.23+, unavailable in v1.26+; use autoscaling/v2 HorizontalPodAutoscaler
horizontalpodautoscaler.autoscaling/kyverno created
</code></pre>
    <p class="normal">As you can see, kubectl gives a very nice warning when you create a deprecated resource version that tells when the resource was deprecated (1.23), when it will be removed (1.26), and which version to replace it with (autoscaling/v2).</p>
    <p class="normal">This is nice, but it is not sufficient. You probably create your resources through CI/CD, which might not receive the same warning, and even if it does, might not surface it, because it is not an error. However, if you created the HPA when your cluster was on an earlier version of Kubernetes than 1.23, then you wouldn’t get any warning because at the time it wasn’t deprecated.</p>
    <p class="normal">Let’s see if kubent<a id="_idIndexMarker1987"/> can detect the deprecated HPA:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubent
6:27AM INF &gt;&gt;&gt; Kube No Trouble `kubent` &lt;&lt;&lt;
6:27AM INF version 0.7.0 (git sha d1bb4e5fd6550b533b2013671aa8419d923ee042)
6:27AM INF Initializing collectors and retrieving data
6:27AM INF Target K8s version is 1.25.3
6:28AM INF Retrieved 7 resources from collector name=Cluster
6:28AM INF Retrieved 109 resources from collector name="Helm v3"
6:28AM INF Loaded ruleset name=custom.rego.tmpl
6:28AM INF Loaded ruleset name=deprecated-1-16.rego
6:28AM INF Loaded ruleset name=deprecated-1-22.rego
6:28AM INF Loaded ruleset name=deprecated-1-25.rego
6:28AM INF Loaded ruleset name=deprecated-1-26.rego
6:28AM INF Loaded ruleset name=deprecated-future.rego
__________________________________________________________________________________________
&gt;&gt;&gt; Deprecated APIs removed in 1.26 &lt;&lt;&lt;
------------------------------------------------------------------------------------------
KIND                      NAMESPACE   NAME      API_VERSION           REPLACE_WITH (SINCE)
HorizontalPodAutoscaler   kyverno     kyverno   autoscaling/v2beta2   autoscaling/v2 (1.23.0)
</code></pre>
    <p class="normal">Yes, it does. You get the same information: when it will be removed, when it was deprecated, and what to replace it with.</p>
    <h3 id="_idParaDest-854" class="heading-3">Updating incompatible resources</h3>
    <p class="normal">Updating<a id="_idIndexMarker1988"/> an incompatible resource may require some changes to your manifests. If the API change just adds new fields, then you may just change the API version and be done with it. However, sometimes it may require additional changes.</p>
    <p class="normal">OK. We’re about to upgrade our cluster, and we detected some incompatible resources. Kubectl and the kubectl-convert plugin can help here. Follow the instructions here to install the plugin: <a href="https://kubernetes.io/docs/tasks/tools/#kubectl"><span class="url">https://kubernetes.io/docs/tasks/tools/#kubectl</span></a>. Let’s convert our HPA manifest and see what it looks like:</p>
    <pre class="programlisting gen"><code class="hljs">$ k convert -f hpa.yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: kyverno
  namespace: kyverno
spec:
  maxReplicas: 3
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kyverno
  targetCPUUtilizationPercentage: 80
status:
  currentReplicas: 0
  desiredReplicas: 0
</code></pre>
    <p class="normal">The conversion succeeded but created a few unnecessary fields. The <code class="inlineCode">creationTimestamp: null</code> is useless as it will be updated on a live resource. Also, the status is useless as this is just a manifest file, and the status will be updated at runtime.</p>
    <p class="normal">However, the main differences are that <code class="inlineCode">apiVersion</code> was changed to <code class="inlineCode">apiVersion: autoscaling/v1</code> and that the target CPU percentage is now specified as a single field:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">targetCPUUtilizationPercentage:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">in autoscaling/v1beta1 it was specified as:</span>
  <span class="hljs-attr">metrics:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">resource:</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">cpu</span>
      <span class="hljs-attr">target:</span>
        <span class="hljs-attr">averageUtilization:</span> <span class="hljs-number">80</span>
        <span class="hljs-attr">type:</span> <span class="hljs-string">Utilization</span>
</code></pre>
    <p class="normal">Using kubectl-<a id="_idIndexMarker1989"/>convert saves time, and it is a well-tested tool.</p>
    <h3 id="_idParaDest-855" class="heading-3">Dealing with removed features</h3>
    <p class="normal">There is one other<a id="_idIndexMarker1990"/> situation we need to address, which is the complete removal of a feature without an upgrade path. Kubernetes 1.25 completely removed support for <strong class="keyWord">Pod Security Policies</strong> (<strong class="keyWord">PSPS</strong>). The application of PSPs to pods has caused confusion for many users who have attempted to utilize them. Check this link for more details: <a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/"><span class="url">https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/</span></a>.</p>
    <p class="normal">If you used PSPs, then when the time comes to upgrade to Kubernetes 1.25, your PSPs will no longer work. The Kubernetes developers didn’t just remove the feature with no alternative. There are two alternatives to PSPs:</p>
    <ul>
      <li class="bulletList">Pod security admission</li>
      <li class="bulletList">A third-party admission controller</li>
    </ul>
    <p class="normal">The pod security admin is a simplified solution that may or may not be a complete replacement for PSPs. The Kubernetes developers published a detailed guide for migration. Check it out: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/"><span class="url">https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/</span></a>.</p>
    <p class="normal">If you choose a third party (e.g., Kyverno), then you should check its documentation. Kyverno comes with a lot of sample policies for pod security and the transition is pretty straightforward.</p>
    <h2 id="_idParaDest-856" class="heading-2">Upgrading node pools</h2>
    <p class="normal">Upgrading the node pools of multiple <a id="_idIndexMarker1991"/>clusters can be a major undertaking. If you have tens to hundreds of clusters and in each cluster, you have multiple node pools (5-20 is not uncommon), then be ready for a serious adventure. Control plane upgrades (once you have ensured your workloads are compatible with the new version) are pretty quick and painless. Node pool upgrades are very difficult. Realistically, it can take several weeks to upgrade all the node pools in a large Kubernetes-based system with tens to hundreds of node pools with thousands of nodes.</p>
    <h3 id="_idParaDest-857" class="heading-3">Syncing with control plane upgrades</h3>
    <p class="normal">Kubernetes imposes<a id="_idIndexMarker1992"/> constraints on the versions of the control plane and the worker nodes. The Kubernetes node components may be two minor versions behind the control plane. If the control plane is on version N, then the node pools may be on version N-2. Since node pool upgrades are much more disruptive and labor-intensive than control plane upgrades, I recommend upgrading node pools only to every other version of Kubernetes. For example, suppose we start with a Kubernetes cluster where both the control plane and the nodes are on version 1.24. When we upgrade to version 1.25, we upgrade only the control plane to 1.25 and keep the node pools on 1.24, which is compatible. Then, when it’s time to upgrade to 1.26, we upgrade the control plane first from 1.25 to 1.26, and then we start upgrading all the node pools from 1.24 directly to 1.26. Let’s see how to go about upgrading node pools.</p>
    <h3 id="_idParaDest-858" class="heading-3">How to perform node pool upgrades</h3>
    <p class="normal">Node pool upgrades require<a id="_idIndexMarker1993"/> a new node pool. It is not possible to upgrade nodes within the node pool. It is not even possible to add new nodes with a new version to an existing node pool. The node version is one of the essential properties of a node pool. What it means is that you actually don’t upgrade an existing node pool. You replace your node pool. First, you create a new node pool, scale it up, and start draining nodes from the old node pool until all the pods run on the new node pool and then you can delete the old (now empty) node pool.</p>
    <p class="normal">If your original node pool was an autoscaling node pool, then before starting the upgrade process, you must turn off autoscaling; otherwise, the pods you evicted from a node in the old node pool might get scheduled right back to the old node pool. Let’s list the exact steps you need to take to upgrade a node pool:</p>
    <ul>
      <li class="bulletList">Create a new node pool with the exact same specifications (instance type, labels, tolerations) as the existing node with the new version.</li>
      <li class="bulletList">You may pre-allocate some instances to the new pool, so they are ready to schedule pods from the old node pool.</li>
      <li class="bulletList">Turn autoscaling off in the old node pool.</li>
      <li class="bulletList">Cordon all the nodes in the old node pool to prevent the scheduling of new pods to the old pool.</li>
      <li class="bulletList">Drain the nodes of the old node pool.</li>
      <li class="bulletList">Observe and deal with problems.</li>
      <li class="bulletList">Wait for the cluster autoscaler to delete empty old pool nodes or delete them <a id="_idIndexMarker1994"/>yourself to expedite the process.</li>
    </ul>
    <p class="normal">Let’s look at some problems that can delay or even hold up the upgrade process.</p>
    <h3 id="_idParaDest-859" class="heading-3">Concurrent draining</h3>
    <p class="normal">If you need to upgrade many node <a id="_idIndexMarker1995"/>pools with lots of nodes in each, you may decide to provision new node pools and start draining all your node pools at once or in large batches. This can cause you to exceed your quota or hit cloud provider capacity issues.</p>
    <p class="normal">You should pay attention to your quota and ensure you have a sufficient quota regardless of upgrades. If you’re getting close to your quota ceiling, I suggest bumping it before engaging in a complex operation like a node pool upgrade. The last thing you want is to be in the middle of an upgrade when you need to scale your capacity due to business needs and realize you maxed out your quota.</p>
    <p class="normal">A good strategy for handling capacity issues and ramping up speed (how fast the cloud provider provision can instances for your new nodes) is to pre-allocate those instances. Again, this requires that you have a sufficient quota for the old node pool and the new node pool at the same time.</p>
    <p class="normal">Let’s understand what happens if you don’t pre-allocate nodes in the new node pool. When you drain a node from the old node pool, all the pods are evicted from the node and become pending pods. Kubernetes will try to schedule these pods to existing nodes if any are available. The old node pool is cordoned, so either Kubernetes can find suitable nodes on other existing node pools (it’s a good thing that improves bin packing) or the cluster autoscaler will need to provision a new node. That takes several minutes. If you drain multiple nodes at the same time, then all the pods from these nodes will be pending for a few minutes until new nodes can be provisioned and your system’s capacity is degraded. In addition, if the cloud provider has capacity issues, maybe it can’t provision new nodes and your pods will remain pending until then.</p>
    <p class="normal">Pre-allocating <a id="_idIndexMarker1996"/>nodes means that the new node pool will have nodes ready to go. The moment a pod is evicted from the old node pool, it will immediately be scheduled to an available node in the new node pool.</p>
    <h3 id="_idParaDest-860" class="heading-3">Dealing with workloads with no PDB</h3>
    <p class="normal">When draining a <a id="_idIndexMarker1997"/>node, Kubernetes is mindful of <strong class="keyWord">pod disruption budgets</strong> (<strong class="keyWord">PDBs</strong>). If a deployment has a PDB that says only one pod can be unavailable and there are two pods of this deployment on the drained node, then Kubernetes will evict just one pod and wait until it is eventually scheduled before evicting the other pod. However, if you have workloads without PDBs, then that means Kubernetes is allowed to evict all the pods of those workloads at the same time. For most workloads, this is unacceptable. You should identify these workloads and work with their owner to add a PDB. Note that in the scenario of draining all nodes at once, workloads with no PDB are vulnerable even if they have many pods running on different nodes.</p>
    <h3 id="_idParaDest-861" class="heading-3">Dealing with workloads with PDB zero</h3>
    <p class="normal">However, the <a id="_idIndexMarker1998"/>opposite problem of unevictable pods is the bane of node pool upgrades. If a node contains an unevictable pod, then it can’t be drained, and Kubernetes will wait forever (or until the pod is manually deleted) before it fully drains the node. This can halt the upgrade process indefinitely and typically requires coordinating with the workload owner to resolve it. </p>
    <p class="normal">If a workload has a PDB with <code class="inlineCode">minUnavailable: 0</code>, it means that Kubernetes is not allowed to evict even a single pod from the workload regardless of how many replicas the workload has.</p>
    <p class="normal">Some workloads (usually stateful) are more sensitive than others and prefer not to be disrupted at all. This is, of course, an unrealistic expectation because the node itself might go bad or the underlying VM might disappear due to cloud provider issues, and then the pods scheduled on it will have to be evicted. It’s best to work with workload owners and come up with a solution that minimizes disruption, but still allows upgrades to progress. This has to be worked out before the upgrade process starts. You don’t want to be in a situation where a single workload holds a node pool upgrade process hostage, and you have to beg the workload owner to allow you to evict a pod.</p>
    <p class="normal">But, in addition to strict PDB-zero workloads, you might run into effective PDB-zero situations. Consider a workload with a PDB of <code class="inlineCode">minUnavailable: 1</code>. This is pretty common and means the workload allows one pod at a time to be unavailable. When draining the pods of this workload, Kubernetes is allowed to evict one pod as long as all the other pods are running. However, if even one of the pods of this workload is pending or unable to be ready due to any reason, then effectively the workload already has one pod unavailable, and the upgrade process will be halted again.</p>
    <p class="normal">The best practice here is to identify these workloads before the upgrade process starts and ensure that all workloads are healthy and can participate in the node pool upgrade process.</p>
    <p class="normal">However, even if you did all the preparation work ahead of time, some workloads might get into an unhealthy state during the upgrade process (remember we’re talking about a process that can take weeks).</p>
    <p class="normal">I recommend having strong monitoring of the progress of the upgrade process, detecting stuck pods, and working with owners to resolve issues. In the case of pods that are scheduled on the old node pool and can’t be ready, it is a simple solution to just delete the pod, see it is scheduled to the new node pool, and let the workload owner resolve the problem on the new node pool.</p>
    <p class="normal">Other cases might<a id="_idIndexMarker1999"/> require more creative solutions.</p>
    <p class="normal">Let’s turn our attention to various problems that can occur in a cluster and how to handle them, especially at scale.</p>
    <h1 id="_idParaDest-862" class="heading-1">Troubleshooting</h1>
    <p class="normal">In this section, we will cover the troubleshooting process in a production cluster and the logical procession of actions to take. The pod lifecycle involves multiple phases and failures can occur at each phase. In addition, pod containers go through their own mini lifecycle where init containers are running to completion and then the main containers start running. Let’s see what can go wrong along the way and how to handle it.</p>
    <p class="normal">First, let’s look at pending pods.</p>
    <h2 id="_idParaDest-863" class="heading-2">Handling pending pods</h2>
    <p class="normal">When a new pod was<a id="_idIndexMarker2000"/> created, Kubernetes used to place it in the Pending state and try to find a node to schedule it on. However, since Kubernetes 1.26, there is an even earlier state where a pod can’t be scheduled. </p>
    <p class="normal">Let’s create a new 1.26 kind cluster called “<code class="inlineCode">trouble</code>" and enable the pod scheduling readiness feature. Here is the configuration file (<code class="inlineCode">cluster-config.yaml</code>):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kind.x-k8s.io/v1alpha4</span>
<span class="hljs-attr">name:</span> <span class="hljs-string">trouble</span>
<span class="hljs-attr">nodes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span>
<span class="hljs-attr">featureGates:</span>
  <span class="hljs-attr">"PodSchedulingReadiness":</span> <span class="hljs-literal">true</span>
</code></pre>
    <p class="normal">And here is <a id="_idIndexMarker2001"/>how to create the kind cluster:</p>
    <pre class="programlisting gen"><code class="hljs">$ kind create cluster -n trouble --config cluster-config.yaml --image kindest/node:v1.26.0
Creating cluster "trouble" ...
 <img src="../Images/B18998_17_001.png" alt=""/> Ensuring node image (kindest/node:v1.26.0) <img src="../Images/B18998_04_002.png" alt=""/>
 <img src="../Images/B18998_04_001.png" alt=""/> Preparing nodes <img src="../Images/B18998_04_004.png" alt=""/>
 <img src="../Images/B18998_17_001.png" alt=""/> Writing configuration <img src="../Images/B18998_17_006.png" alt=""/>
 <img src="../Images/B18998_04_001.png" alt=""/> Starting control-plane <img src="../Images/B18998_17_008.png" alt=""/>
 <img src="../Images/B18998_17_001.png" alt=""/> Installing CNI <img src="../Images/B18998_04_010.png" alt=""/>
 <img src="../Images/B18998_04_001.png" alt=""/> Installing StorageClass <img src="../Images/B18998_04_012.png" alt=""/>
Set kubectl context to "kind-trouble"
You can now use your cluster with:
kubectl cluster-info --context kind-trouble
</code></pre>
    <p class="normal">Next, we’ll create a new namespace called <code class="inlineCode">trouble</code> and take it from there.</p>
    <pre class="programlisting gen"><code class="hljs">$ k create ns trouble
namespace/trouble created
</code></pre>
    <p class="normal">Let’s create a pod with a scheduling gate called <code class="inlineCode">no-scheduling-yet</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ cat &lt;&lt;EOF | k apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: some-pod
  namespace: trouble
  labels:
    app: the-app
spec:
  schedulingGates:
    - name: no-schedule-yet
  containers:
    - name: pause
      image: registry.k8s.io/pause:3.8
EOF
pod/some-pod created
$ k get po -n trouble
NAME       READY   STATUS            RESTARTS   AGE
some-pod   0/1     SchedulingGated   0          10s
</code></pre>
    <p class="normal">As you can see, the pod has a status of <code class="inlineCode">SchedulingGated</code>.</p>
    <p class="normal">The benefit of the scheduling gate is that if the pod can’t be scheduled yet due to issues like the quota, which need to be resolved externally, then the pod in this state will not cause a lot of churns to the Kubernetes scheduler, which will ignore it. After the external issue is resolved, you (or more likely an operator) can remove the feature gate and the pod will become a pending pod ready to be scheduled.</p>
    <p class="normal">Now, let’s turn our attention to pending pods. It’s okay for a pod to be pending; however, if a pod is pending for more than a few minutes, something is wrong and we need to investigate it. I suggest having an alert set up for pods pending for more than X minutes (reasonable values for X can be between 10 and 60 minutes).</p>
    <p class="normal">There are<a id="_idIndexMarker2002"/> two types of pending pods: temporarily pending pods and <a id="_idIndexMarker2003"/>permanent pending pods. Temporarily pending pods <a id="_idIndexMarker2004"/>may be scheduled to one of the <a id="_idIndexMarker2005"/>existing node pools; however, there is currently no room on any of the nodes. If the node <a id="_idIndexMarker2006"/>pool has autoscaling enabled, the cluster autoscaler will try to provision a new node. If the node pool has autoscaling disabled, then the pod will remain pending until some other pods complete or are evicted from a node to make room. Another category of temporarily unschedulable pods is if the target namespace has a resource quota that is maxed out at the moment. Here is an example, where the namespace has a resource quota of 1 CPU and a deployment with 3 replicas is created where each pod requests 0.5 CPU. Only 2 pods can fit with the namespace quota. The third pod will be pending:</p>
    <pre class="programlisting gen"><code class="hljs">cat &lt;&lt;EOF | k apply -f -
apiVersion: v1
kind: ResourceQuota
metadata:
  name: cpu-requests
  namespace: trouble
spec:
  hard:
    requests.cpu: "1"
EOF
resourcequota/cpu-requests created
</code></pre>
    <p class="normal">Now, let’s create the<a id="_idIndexMarker2007"/> deployment and see what happens:</p>
    <pre class="programlisting gen"><code class="hljs">cat &lt;&lt;EOF | k apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: some-deployment
  namespace: trouble
  labels:
    app: the-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: the-app
  template:
    metadata:
      namespace: trouble
      labels:
        app: the-app
    spec:
      containers:
        - name: pause
          image: registry.k8s.io/pause:3.8
          resources:
            requests:
              cpu: "0.5"
EOF
deployment.apps/some-deployment created
</code></pre>
    <p class="normal">We end up with just two running pods as expected:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get deploy -n trouble
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
some-deployment   2/3     2            2           5m3s
$ k get po -n trouble
NAME                               READY   STATUS    RESTARTS   AGE
some-deployment-7f876df998-fc7kq   1/1     Running   0          4m34s
some-deployment-7f876df998-htdsn   1/1     Running   0          4m34s
</code></pre>
    <p class="normal">Note, that there is no third pending pod in this case. Kubernetes is smart enough to create only two pods in this case.</p>
    <p class="normal">The reason is the namespace quota:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get deploy some-deployment -o yaml  -n trouble | grep forbidden -A 2
    message: 'pods "some-deployment-7f876df998-84z9s" is forbidden: exceeded quota:
      cpu-requests, requested: requests.cpu=500m, used: requests.cpu=1, limited: requests.cpu=1'
    reason: FailedCreate
</code></pre>
    <p class="normal">Permanent pending<a id="_idIndexMarker2008"/> pods are pods that can’t be scheduled on any of the available node pools, so provisioning a new node will not help. There are several categories of such permanently unschedulable pods:</p>
    <ul>
      <li class="bulletList">All the node pools have taints, and the pod doesn’t have the proper tolerations.</li>
      <li class="bulletList">The pod requests more resources than are available on any of the node pools.</li>
      <li class="bulletList">The pod is waiting for a persistent volume.</li>
      <li class="bulletList">The pod has incorrect <code class="inlineCode">nodeSelector</code> or <code class="inlineCode">nodeAffinity</code>.</li>
    </ul>
    <p class="normal">Let’s delete the previous deployment and resource quota and look at an example of a pod that just requests way too many resources (666 CPU cores):</p>
    <pre class="programlisting gen"><code class="hljs">$ k delete resourcequota cpu-requests -n trouble
resourcequota "cpu-requests" deleted
$ k delete deploy some-deployment –n troublet
deployment.apps "some-deployment" deleted
$ cat &lt;&lt;EOF | k apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: some-deployment
  namespace: trouble
  labels:
    app: the-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: the-app
  template:
    metadata:
      labels:
        app: the-app
    spec:
      containers:
        - name: pause
          image: registry.k8s.io/pause:3.8
          resources:
            requests:
              cpu: "666"
              memory: 1Gi
EOF
deployment.apps/some-deployment created
</code></pre>
    <p class="normal">If we look at the pod that was created now, we can see it is indeed pending:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po -n trouble
NAME                              READY   STATUS    RESTARTS   AGE
some-deployment-bbf88d559-pdf8p   0/1     Pending   0          2m37s
</code></pre>
    <p class="normal">To understand why it is pending, we can look at the status of the pod:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po some-deployment-bbf88d559-pdf8p -o yaml -n trouble | yq .status
conditions:
  - lastProbeTime: null
    lastTransitionTime: "2023-03-26T05:35:30Z"
    message: '0/1 nodes are available: 1 Insufficient cpu. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod..'
    reason: Unschedulable
    status: "False"
    type: PodScheduled
phase: Pending
qosClass: Burstable
</code></pre>
    <p class="normal">The message is pretty clear<a id="_idIndexMarker2009"/> and explains that 0 out of 1 node is available to schedule. It even says that 1 node has insufficient CPU. If there were other nodes in the cluster with other reasons, it would list them too.</p>
    <p class="normal">Pending pods don’t use resources and don’t take up space in nodes; however, they put some pressure on the API server and also it means that some workloads don’t get to do their work and they are waiting for a node to be scheduled on, which might be very serious in production. Mind <a id="_idIndexMarker2010"/>your pending pods and make sure to resolve any issues quickly.</p>
    <p class="normal">The next category of problems is about pods that are scheduled to a node but are unable to run.</p>
    <h2 id="_idParaDest-864" class="heading-2">Handling scheduled pods that are not running</h2>
    <p class="normal">There may be <a id="_idIndexMarker2011"/>several reasons why a scheduled pod is not running. One of the most common ones is failure to pull an image required by one of the pod’s containers. The kubelet will just keep trying and the pod’s status will show as <code class="inlineCode">ErrImagePull</code>:</p>
    <pre class="programlisting gen"><code class="hljs">cat &lt;&lt;EOF | k apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: no-such-image
  namespace: trouble
  labels:
    app: no-such-image
spec:
  replicas: 1
  selector:
    matchLabels:
      app: no-such-image
  template:
    metadata:
      labels:
        app: no-such-image
    spec:
      containers:
        - name: no-such-image
          image: no-such-image:6.6.6
EOF
deployment.apps/no-such-image created
</code></pre>
    <p class="normal">Let’s check the pods:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po -l app=no-such-image -n trouble
NAME                             READY   STATUS         RESTARTS   AGE
no-such-image-77585fd5b4-tqpv2   0/1     ErrImagePull   0          27s
</code></pre>
    <p class="normal">To see a more elaborate message, we can check the <code class="inlineCode">containerStatuses</code> field of the status:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po no-such-image-77585fd5b4-tqpv2 -n trouble -o yaml | yq '.status.containerStatuses[0].state'
waiting:
  message: Back-off pulling image "no-such-image:6.6.6"
  reason: ImagePullBackOff
</code></pre>
    <p class="normal">Image pull errors could relate to a misconfigured image. The image name or the image tag might be wrong. However, the image may be correct but might have been deleted accidentally from the registry. If you try to pull from a private registry, then possibly you don’t have the correct permissions. Finally, the image registry may be unavailable. For example, Docker Hub often has rate limits.</p>
    <p class="normal">You may prefer to pull all your images from a single source you control, where you can scan and curate the images and ensure that images don’t disappear from you. If you’re on the cloud, then every cloud provider offers their image registry. This should be the preferred option in most cases. You may save some money by using a private registry, and you may prefer a different solution in hybrid cloud scenarios.</p>
    <p class="normal">Another reason that a <a id="_idIndexMarker2012"/>pod doesn’t start running is an init container that doesn’t complete:</p>
    <pre class="programlisting gen"><code class="hljs">cat &lt;&lt;EOF | k apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: infinite-init
  namespace: trouble
  labels:
    app: infinite-init
spec:
  replicas: 1
  selector:
    matchLabels:
      app: infinite-init
  template:
    metadata:
      name: infinite-init
      labels:
        app: infinite-init
    spec:
      initContainers:
        - name: init-pause
          image: registry.k8s.io/pause:3.8      
      containers:
        - name: main-pause
          image: registry.k8s.io/pause:3.8
EOF
deployment.apps/infinite-init created
</code></pre>
    <p class="normal">Checking the pod status, we can see that it is stuck in the init phase because our init container is in the pause container, which never completes:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po -l app=infinite-init -n trouble
NAME                            READY   STATUS     RESTARTS   AGE
infinite-init-d555945fb-dccbk   0/1     Init:0/1   0          46s
</code></pre>
    <p class="normal">Sometimes the pod starts running, but the container keeps failing. In this case, you need to check your pod’s logs or your Dockerfile for the reason. Here is a pod that keeps crashing because its <a id="_idIndexMarker2013"/>Dockerfile command just exists with exit code 1:</p>
    <pre class="programlisting gen"><code class="hljs">cat &lt;&lt;EOF | k apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: run-container-error
  namespace: trouble
spec:
  containers:
    - name: run-container-error
      image: bash
      command:
        - exit
        - "1"
EOF
pod/run-container-error created
</code></pre>
    <p class="normal">The result is that the pod will have a status of <code class="inlineCode">RunContainerError</code>. Kubernetes will keep restarting the pod (assuming the default restart policy of always):</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po run-container-error -n trouble
NAME                  READY   STATUS              RESTARTS     AGE
run-container-error   0/1     RunContainerError   4 (8s ago)   100s
</code></pre>
    <p class="normal">Getting your pods and containers to a running state is a good start, but it is not enough. In order for Kubernetes to send requests to your pods, all the containers must be ready. Let’s see what <a id="_idIndexMarker2014"/>problems you might run into.</p>
    <h2 id="_idParaDest-865" class="heading-2">Handling running pods that are not ready</h2>
    <p class="normal">If all your init <a id="_idIndexMarker2015"/>containers are completed and all your main containers are running with no errors, then probes come into play. Kubernetes considers a pod with running containers ready to receive requests if no probes are defined or if all probes for all containers succeed. The startup probe is checked initially until it succeeds. If it fails, the pod is not considered ready. If your container has a hung startup pod, it will never be ready. </p>
    <p class="normal">Kubernetes will eventually kill and restart your container and the startup probe will have another chance.</p>
    <p class="normal">Here is a deployment where the main container has a startup probe that will always fail (the pause container doesn’t even listen on port 80). The pod will never get to the ready state. After some retries and delays defined by the startup probe, Kubernetes will restart the container and the cycle will repeat:</p>
    <pre class="programlisting gen"><code class="hljs">Cat &lt;&lt;EOF | k apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bad-startup-probe
  namespace: trouble
  labels:
    app: bad-startup-probe
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bad-startup-probe
  template:
    metadata:
      name: bad-startup-probe
      labels:
        app: bad-startup-probe
    spec:
      containers:
        - name: pause
          image: registry.k8s.io/pause:3.8
          startupProbe:
            httpGet:
              path: /
              port: 80
            failureThreshold: 3
            periodSeconds: 10
            initialDelaySeconds: 5
            timeoutSeconds: 2
EOF
deployment/bad-startup-probe created
</code></pre>
    <p class="normal">Checking the pod shows that<a id="_idIndexMarker2016"/> the pod is in <code class="inlineCode">CrashloopBackoff</code> and Kubernetes keeps restarting the container:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po -l app=bad-startup-probe –n trouble
NAME                                 READY   STATUS             RESTARTS          AGE
bad-startup-probe-66fb7cf4fd-qw8kp   0/1     CrashLoopBackOff   160 (3m56s ago)   8h
</code></pre>
    <p class="normal">Note that the delay between restarts grows exponentially to avoid a bad container putting a lot of stress on the API server and the kubelet having to keep restarting often.</p>
    <p class="normal">If there is no startup probe or it succeeds, the hurdle is the readiness probe. It works very similar to a startup probe, except Kubernetes will not restart the container. It will just keep checking the readiness probe. When a readiness probe fails, the pod will be removed from the endpoints list of any service that matches its labels, so it doesn’t get to handle any requests. However, the pod remains alive and consumes resources on the node.</p>
    <p class="normal">Let’s see it in action:</p>
    <pre class="programlisting gen"><code class="hljs">cat &lt;&lt;EOF | k apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bad-readiness-probe
  namespace: trouble
  labels:
    app: bad-readiness-probe
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bad-readiness-probe
  template:
    metadata:
      name: bad-readiness-probe
      labels:
        app: bad-readiness-probe
    spec:
      containers:
        - name: pause
          image: registry.k8s.io/pause:3.8
          readinessProbe:
            httpGet:
              path: /
              port: 80
            failureThreshold: 3
            periodSeconds: 10
            initialDelaySeconds: 5
            timeoutSeconds: 2
EOF
deployment.apps/bad-readiness-probe created
</code></pre>
    <p class="normal">As you can see, the pod is running for an hour, it never gets ready, and it is not restarted:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po -l app=bad-readiness-probe –n trouble
NAME                                   READY   STATUS    RESTARTS   AGE
bad-readiness-probe-7458b65d98-5jrjq   0/1     Running   0          60m
</code></pre>
    <p class="normal">The last type of probe is the liveness probe. It works just like the startup probe (the container will get restarted and the pod will be in <code class="inlineCode">CrashloopBackoff</code>), except it is checked periodically, even if it succeeds while the startup probe is a one-time deal. Once it succeeds, it is never checked again.</p>
    <p class="normal">The reason both startup and liveness <a id="_idIndexMarker2017"/>probes are needed is that some containers need a longer startup period, but once they are initialized, then periodic liveness checks should be shorter.</p>
    <p class="normal">That covers troubleshooting the pod lifecycle. When pods are scheduled but are not running or are not ready, it has cost implications. Let’s move on and consider cost management.</p>
    <h1 id="_idParaDest-866" class="heading-1">Cost management</h1>
    <p class="normal">When running Kubernetes at a large<a id="_idIndexMarker2018"/> scale in the cloud, one of the major concerns is the cost of the infrastructure. Cloud providers offer a variety of infrastructure options and services for your Kubernetes clusters. These are expensive. To harness your costs, make sure you follow best practices such as:</p>
    <ul>
      <li class="bulletList">Having a cost mindset</li>
      <li class="bulletList">Cost observability</li>
      <li class="bulletList">The smart selection of resources</li>
      <li class="bulletList">Efficient usage of resources</li>
      <li class="bulletList">Discounts, reserved instances, and spot instances</li>
      <li class="bulletList">Invest in<a id="_idIndexMarker2019"/> local environments</li>
    </ul>
    <p class="normal">Let’s review them one by one.</p>
    <h2 id="_idParaDest-867" class="heading-2">Cost mindset</h2>
    <p class="normal">Engineers often neglect <a id="_idIndexMarker2020"/>cost or put it way down the priority list. I often think in this order: make it<a id="_idIndexMarker2021"/> work, make it fast, make it last, make it secure, and only then make it cheap. This is not necessarily a bad thing, especially for startup companies or new projects. Growth and velocity are often the top priorities. After all, if you don’t have a good product, and you don’t have customers, then the business will fail even if your costs are zero. </p>
    <p class="normal">In addition, when the system is small, the absolute cost might be relatively small even if there is a lot of waste. Add to that the fact that cloud providers lure companies in with generous credits.</p>
    <p class="normal">However, if you are part of a large enterprise or your startup succeeds and grows, at some point, cost will become a significant concern. At this point, you need to shift your thinking and have cost as the primary concern for everything you do. Cost may or may not be aligned with other initiatives. For example, everybody loves Pareto improvements. If you just manage to use 20% fewer VMs to accomplish the same task, then you will automatically save a lot of money without negatively impacting any other aspect.</p>
    <p class="normal">But those easy wins and low-hanging fruit will eventually dry up. Then, you get to harder decisions. For example, caching the last week of data in memory will give you excellent response times, but at a large cost. What if you just cache one day?</p>
    <p class="normal">Availability and redundancy are often at odds with cost as well. Do you really need a full-fledged zero-downtime active-active setup across multiple availability zones, regions, and cloud providers or can you get by with some downtime and recovery from backups in the event of catastrophic failure?</p>
    <p class="normal">You may end up choosing the more expensive option, but you should do it with an explicit understanding of how much you pay for it and ensure the value you receive is greater.</p>
    <p class="normal">That takes us to the next topic of cost observability.</p>
    <h2 id="_idParaDest-868" class="heading-2">Cost observability</h2>
    <p class="normal">To manage your<a id="_idIndexMarker2022"/> infrastructure cost on Kubernetes and in the cloud in general, you must <a id="_idIndexMarker2023"/>have strong cost-oriented observability. Let’s look at some of the ways to accomplish it.</p>
    <h3 id="_idParaDest-869" class="heading-3">Tagging</h3>
    <p class="normal">Tagging<a id="_idIndexMarker2024"/> is associating every resource <a id="_idIndexMarker2025"/>with a set of tags or labels. From a cost perspective, tags should enable you to attribute the cost of any infrastructure resource to the relevant stakeholders. For example, you may have a team tag or an owner tag. If the resources provisioned by a specific team suddenly grow rapidly, you can narrow down the issue more quickly. The specific tags are up to you. Common tags may include environment (production, staging, and development), release, and git sha. Many resources in the cloud come with cloud-provider tags that you can take advantage of.</p>
    <h3 id="_idParaDest-870" class="heading-3">Policies and budgets</h3>
    <p class="normal">Policies and<a id="_idIndexMarker2026"/> budgets let you rein in wild spending on infrastructure. Some policies are implicit, such as a namespace resource quota that will block the provisioning of excess resources. However, other policies may be more cost-specific and be informed by cost tracking. Budgets let you set a hard limit on spending at different scopes. All cloud providers offer budgets<a id="_idIndexMarker2027"/> as part of their cost management solutions:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Tutorial: Create and manage Azure budgets: <a href="https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/tutorial-acm-create-budgets"><span class="url">https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/tutorial-acm-create-budgets</span></a></li>
      <li class="numberedList">Managing your costs with AWS Budgets: <a href="https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html"><span class="url">https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html</span></a></li>
      <li class="numberedList">Create, edit, or delete budgets and budget alerts in GCP: <a href="https://cloud.google.com/billing/docs/how-to/budgets"><span class="url">https://cloud.google.com/billing/docs/how-to/budgets</span></a></li>
    </ol>
    <p class="normal">Policies and budgets are great, but sometimes they are not sufficient, or you don’t have the cycles to specify and update them. This is where alerting comes into play.</p>
    <h3 id="_idParaDest-871" class="heading-3">Alerting</h3>
    <p class="normal">Budgets are <a id="_idIndexMarker2028"/>often the <a id="_idIndexMarker2029"/>last resort to mitigate against rogue infrastructure provisioning or accidental runaway provisioning. For example, you may set broad budgets for several overall categories, like no more than $500,000 of compute spending. These budgets of course need to align with business growth and make sure that they don’t cause an incident if you temporarily need to provision more infrastructure to handle a temporary spike in demand. Budgets are often set or modified only with top management approval.</p>
    <p class="normal">Fine-grained and day-to-day cost management alerts are much more nimble and practical. If you cross or come close to some cost limit, you can set alerts that will let you know and escalate as necessary. The alerts rely on proper tagging, so you can have meaningful information to evaluate the cause of the cost increase and the responsible party.</p>
    <p class="normal">As you can see, managing costs is a dynamic and complex activity. You need good tools to help you.</p>
    <h3 id="_idParaDest-872" class="heading-3">Tools</h3>
    <p class="normal">All the cloud providers have strong cost management<a id="_idIndexMarker2030"/> tools. Check out:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">AWS Cost <a id="_idIndexMarker2031"/>Explorer: <a href="https://aws.amazon.com/aws-cost-management/aws-cost-explorer/"><span class="url">https://aws.amazon.com/aws-cost-management/aws-cost-explorer/</span></a></li>
      <li class="numberedList">Azure <a id="_idIndexMarker2032"/>Cost Analyzer: <a href="https://www.serverless360.com/azure-cost-analysis"><span class="url">https://www.serverless360.com/azure-cost-analysis</span></a></li>
      <li class="numberedList">GCP Cost<a id="_idIndexMarker2033"/> Management: <a href="https://cloud.google.com/cost-management/"><span class="url">https://cloud.google.com/cost-management/</span></a></li>
    </ol>
    <p class="normal">In addition, you may opt to use a multi-cloud <a id="_idIndexMarker2034"/>open-source tool like kubecost (<a href="https://www.kubecost.com"><span class="url">https://www.kubecost.com</span></a>) or a paid product<a id="_idIndexMarker2035"/> like cast.ai (<a href="https://cast.ai"><span class="url">https://cast.ai</span></a>). Of course, you can do everything yourself, ingest cost metrics from the cloud provider into Prometheus, and build your Grafana dashboards and alerts on top of them.</p>
    <p class="normal">Remember that picking the right set of tools can literally pay for itself very quickly.</p>
    <h2 id="_idParaDest-873" class="heading-2">The smart selection of resources</h2>
    <p class="normal">The cloud offers a<a id="_idIndexMarker2036"/> plethora of choices and combinations for resources like VMs, networking, and disks. We covered all the considerations in the <em class="italic">Bin packing and utilization</em> section earlier in this chapter. However, with a focus on cost, you should ensure that you understand that you may be able to get the job done with a cheaper alternative and reduce your costs significantly. Familiarize yourself with the inventory of resources and stay up to date as cloud providers update their offerings and may change prices too.</p>
    <h2 id="_idParaDest-874" class="heading-2">Efficient usage of resources</h2>
    <p class="normal">When you’re cutting costs, any <a id="_idIndexMarker2037"/>unused resources are a red flag. You leave money on the table. It is often necessary to build in flexibility. For example, the industry average CPU utilization is in the range of 40%-60%. This might seem low, but it is not easy to improve on that in a very dynamic environment with constraints like high availability, quick recovery, and the ability to scale up quickly.</p>
    <h2 id="_idParaDest-875" class="heading-2">Discounts, reserved instances, and spot instances</h2>
    <p class="normal">One of the best ways to reduce costs is to just pay less money to the cloud providers for the same resources. The common ways to accomplish this are discounts, reserved instances, and spot instances.</p>
    <h3 id="_idParaDest-876" class="heading-3">Discounts and credits</h3>
    <p class="normal">Discounts are the best. They only <a id="_idIndexMarker2038"/>have an upside. You simply pay less than the sticker price. That’s it. Well, it’s not that easy. You will need to negotiate to get the best prices and often show promise for growth and commitment to stay longer on the platform.</p>
    <p class="normal">Credits are another great way to offset initial cloud spending. All the major cloud providers offer various credit programs, and you may be able to negotiate more credits too.</p>
    <p class="normal">AWS has the <a id="_idIndexMarker2039"/>Activate program, targeted mostly at startups, where you can get up to $100,000 of AWS credit. See <a href="https://aws.amazon.com/activate/"><span class="url">https://aws.amazon.com/activate/</span></a>.</p>
    <p class="normal">Azure has the <a id="_idIndexMarker2040"/>Microsoft for Startups program, which offers up to $150,000 of Azure credit. See <a href="https://www.microsoft.com/en-us/startups"><span class="url">https://www.microsoft.com/en-us/startups</span></a>.</p>
    <p class="normal">GCP has the <a id="_idIndexMarker2041"/>Google for Startups cloud program, which offers (like AWS) up to $100,000 of GCP credit. See <a href="https://cloud.google.com/startup"><span class="url">https://cloud.google.com/startup</span></a>.</p>
    <p class="normal">These programs are <a id="_idIndexMarker2042"/>designed to boost young startups without a lot of revenue. Let’s move on to options for established enterprise organizations that still want to reduce their cloud spending.</p>
    <h3 id="_idParaDest-877" class="heading-3">Reserved instances</h3>
    <p class="normal">Reserved instances <a id="_idIndexMarker2043"/>are a very<a id="_idIndexMarker2044"/> good way to reduce your costs. They require that you purchase capacity in bulk and commit for a long period (one year or three years). The longer period carries better discounts. Overall, the discounts are significant and can vary from 30% to 75% compared to on-demand prices.</p>
    <p class="normal">Beyond the price, reserved instances also ensure capacity compared to on-demand instances, which may temporarily be unavailable for particular instance types in a particular availability zone.</p>
    <p class="normal">The downsides of reserved instances are that you typically have to prepay, and the commitment is often tied to specific instance types and regions. You may be able to exchange equivalent reservations, but you’ll have to check the terms and conditions of the cloud provider. Also, if you are unable to use all your reserved capacity, you still pay for it (although at a very discounted price).</p>
    <p class="normal">If you consider reserved instances (RIs), you may opt for a limited capacity of reserved instances, which you know you can always utilize, and then use on-demand and spot instances to handle spikes and take advantage of the elasticity of the cloud. If you discover later that you spend too much on on-demand, you can always reserve more instances and come up with a mix of three-year reserved instances, one-year reserved instances, on-demand, and spot instances. This is a great segue to the next item on the list, which is spot instances.</p>
    <h3 id="_idParaDest-878" class="heading-3">Spot instances</h3>
    <p class="normal">Cloud providers love <a id="_idIndexMarker2045"/>reserved instances. They sell them, provision them, make<a id="_idIndexMarker2046"/> their profit, and can forget about them (except for making sure they’re up and running). The on-demand side is very different. Cloud providers have to make sure they can reasonably provision more capacity when their customers demand it. In particular, cloud providers should roughly have enough capacity to handle the quota of each customer even if the customer significantly underutilizes their quota. That means that, in practice, under normal conditions, cloud providers have a lot of idle or underutilized capacity. This is where spot instances come in. Cloud providers can sell this excess capacity, which is in theory allocated for on-demand use. If it is needed, then the cloud provider just takes away the spot instances and provides them to the on-demand customers.</p>
    <p class="normal">Why should you use spot instances? Well, because they are much cheaper. Due to their potentially ephemeral nature, they carry significant discounts of up to 90%. Remember, from the cloud provider’s point of view, this is free money. Those instances are already accounted for and paid for by the markup of on-demand instances in use and the quota ratio of each customer.</p>
    <p class="normal">In practice, spot instances don’t disappear from under you very quickly. The Kubernetes way advocates that workloads shouldn’t care too much about specific nodes. If you run on a spot instance and it goes away, all your pods will be evicted and scheduled to other nodes. If you have sensitive workloads that don’t handle eviction from their node well, then these workloads are not good Kubernetes citizens in the first place. Nodes become unhealthy all the time, regardless, and your workloads should be able to handle eviction.</p>
    <p class="normal">However, there<a id="_idIndexMarker2047"/> is one situation that needs to be addressed, specifically if you run critical workloads on spot instances. In case of a zonal outage of a specific instance type, it is possible that many spot instances will be taken by the cloud provider. I suggest having empty fallback on-demand node pools with the same or<a id="_idIndexMarker2048"/> similar instance type and the same labels and taints. If a node pool using spot instances suddenly loses a lot of nodes and is unable to scale up (because the spot instances are unavailable), then you can scale up your empty on-demand node pool and your pods will be scheduled there until spot instances become available again.</p>
    <p class="normal">Make sure you have enough quota for the fallback node pools to pick up the slack if the equivalent spot instances are unavailable.</p>
    <p class="normal">Next, let’s talk about local environments and how they can help us save money.</p>
    <h2 id="_idParaDest-879" class="heading-2">Invest in local environments</h2>
    <p class="normal">Organizations<a id="_idIndexMarker2049"/> practice different protocols of development and testing. Some organizations do a lot of testing in the cloud in staging and development environments. Sometimes, engineers provision infrastructure for various experiments and tests. Such development and test environments can be difficult to manage effectively as infrastructure is often provisioned in an ad hoc manner. There are solutions like disposable Kubernetes clusters and virtual clusters. Another direction is to invest in local development environments that engineers can run on their local machines. The advantages are that these environments are often very quick to set up and discard, and they don’t incur any expensive cloud costs. The downside is that such environments might not be fully representative of the staging or production environments. I suggest looking into local environments and finding use cases that will save cloud costs without compromising other critical aspects of the system.</p>
    <h1 id="_idParaDest-880" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we covered in depth what it takes to run large-scale Managed Kubernetes systems in production. We looked at managing multiple clusters, building effective processes, handling infrastructure at scale, managing clusters and node pools, bin packing and utilization, upgrading Kubernetes, and troubleshooting and cost management. That’s a lot, but even that is just the tip of the iceberg. There is no substitute for in-depth familiarity with your use cases and special concerns. The bottom line is that large-scale enterprise systems are complex to manage, but Kubernetes gives you a lot of industrial-strength tools to accomplish it.</p>
    <p class="normal">The next chapter will conclude the book. We will look at the future of Kubernetes and the road ahead. Spoiler alert: the future is very bright. Kubernetes has established itself as the gold standard for cloud-native computing. It is being used across the board, and it keeps evolving responsibly. An entire support system has developed around Kubernetes, including training, open-source projects, tools, and products. The community is amazing, and the momentum is very strong.</p>
    <h1 id="_idParaDest-881" class="heading-1">Join us on Discord!</h1>
    <p class="normal">Read this book alongside other users, cloud experts, authors, and like-minded professionals.</p>
    <p class="normal">Ask questions, provide solutions to other readers, chat with the authors via. Ask Me Anything sessions and much more.</p>
    <p class="normal">Scan the QR code or visit the link to join the community now.</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img src="../Images/QR_Code844810820358034203.png" alt=""/></p>
  </div>
</body></html>