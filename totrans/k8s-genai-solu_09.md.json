["```\n\n    resource \"aws_launch_template\" \"example\" {\n      name = \"example\"\n      ...\n        metadata_options {\n        http_endpoint               = \"enabled\"\n        http_tokens                 = \"required\"\n        http_put_response_hop_limit = 1\n        instance_metadata_tags      = \"enabled\"\n      }\n      ...\n    ```", "```\n\n    securityContext:\n      runAsUser: 1000\n      runAsGroup: 1000\n      securityContext, as shown here:\n\n    ```", "```\n\n    ```", "```\n\n    $ kubectl label namespace test-ns \\\n    /etc/passwd or /etc/shadow, and network connections to untrusted IP addresses. It can also detect when a Pod tries to access a K8s Service account token improperly, which may indicate a compromise or misconfiguration.When such behavior violates its defined security rules, Falco triggers alerts instantly. For example, alerts may look like this: **Terminal shell detected in nginx container** or **Suspicious file access by unknown process**. To make these alerts actionable, teams should implement clear response strategies, such as isolating the Pod, initiating incident response procedures, or alerting security teams. Combining these technical capabilities with practical workflows allows teams to confidently enforce runtime security and respond to threats in production-grade K8s environments.Some other third-party tools also offer advanced runtime protection, integration, and reporting capabilities. Use agents provided by solutions such as **Prisma Cloud’s Container Security** ([https://www.paloaltonetworks.com/prisma/cloud/container-security](https://www.paloaltonetworks.com/prisma/cloud/container-security)), **Aqua Security** ([https://www.aquasec.com/products/kubernetes-security/](https://www.aquasec.com/products/kubernetes-security/)), or **Wiz container security** ([https://www.wiz.io/solutions/container-and-kubernetes-security](https://www.wiz.io/solutions/container-and-kubernetes-security)) to monitor and protect containers.\n    ```", "```\n\n    ...\n          \"Effect\": \"Allow\",\n          \"Principal\": {\n            \"Federated\": \"arn:aws:iam::<account_id>:oidc-provider/<oidc_provider>\"\n          },\n          \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n          \"Condition\": {\n            \"StringEquals\": {\n              \"<oidc_provider>:aud\": \"sts.amazonaws.com\",\n              \"<oidc_provider>:sub\": \"system:serviceaccount:<namespace>:<service_account>\"\n    ...\n    ```", "```\n\n    apiVersion: v1\n    kind: ServiceAccount\n    metadata:\n      name: example-sa\n      namespace: example-ns\n      annotations:\n        eks.amazonaws.com/role-arn: arn:aws:iam::<account_id>:role/<iam_role_name>\n    ```", "```\n\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: example-app\n      ...\n        spec:\n          serviceAccount: example-sa\n          containers:\n            - name: example-container\n    ...\n    ```", "```\n\n    ...\n                \"Effect\": \"Allow\",\n                \"Principal\": {\n                    \"Service\": \"pods.eks.amazonaws.com\"\n                },\n                \"Action\": [\n                    \"sts:AssumeRole\",\n                    \"sts:TagSession\"\n    ...\n    ```", "```\n\n    aws eks create-pod-identity-association --cluster-name my-cluster --role-arn arn:aws:iam::<account_id>:role/my-role --namespace <namespace> --service-account <service_account>\n    ```", "```\n\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: example-app\n      ...\n        spec:\n          serviceAccount: example-sa\n          containers:\n            - name: example-container\n    ...\n    ```", "```\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: example-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: \"external\"\n    # Reference the ACM certificate ARN for TLS termination at the load balancer\n    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"arn:aws:acm:us-east-1:<account_id>:certificate/<certificate_id>\"\n    # The port(s) that should use SSL/TLS\n    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \"443\"\nspec:\n  type: LoadBalancer\n...\n```", "```\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    # Define which ports the ALB should listen on; here we set HTTPS on port 443\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n    # The ARN of the ACM certificate for TLS termination\n    alb.ingress.kubernetes.io/certificate-arn: \"arn:aws:acm:us-east-1:<account_id>:certificate/<certificate_id>\"\n...\n```", "```\n\nalb.ingress.kubernetes.io/wafv2-acl-arn: arn:aws:wafv2:us-west-2:xxxxx:regional/webacl/xxxxxxx/yyyyyyyyy\n```", "```\n\n    $ aws ecr describe-repositories --repository-names my-llama-finetuned --query 'repositories[0].encryptionConfiguration.encryptionType' --output text\n    ecr-kms-key and configures the sample-app-repoECR repository to use it for encryption:\n\n    ```", "```\n\n    resource \"aws_ecr_repository\" \"my-llama-finetuned\" {\n      name = \"my-llama-finetuned\"\n      image_tag_mutability = \"IMMUTABLE\"\n    }\n    ```", "```\n\n    ```", "```\n\n    resource \"aws_ecr_registry_scanning_configuration\" \"ecr_scanning_configuration\" {\n      scan_type = \"ENHANCED\"\n      rule {\n        scan_frequency = \"CONTINUOUS_SCAN\"\n        repository_filter {\n          filter      = \"my-llama-finetuned\"\n          filter_type = \"WILDCARD\"\n        }\n      }\n      rule {\n        scan_frequency = \"SCAN_ON_PUSH\"\n        repository_filter {\n          filter      = \"my-llama-finetuned\"\n          filter_type = \"WILDCARD\"\n        }\n      }\n    }\n    ```", "```\n    $ terraform plan\n    my-llama-finetuned repository and display the vulnerabilities in the AWS console.\n    ```", "```\n\n    module \"eks\" {\n      source = \"terraform-aws-modules/eks/aws\"\n      ...\n      eks_managed_node_groups = {\n        eks-mng = {\n          ami_type = \"BOTTLEROCKET_x86_64\"\n          metadata_options {\n            http_endpoint = \"enabled\"\n            http_tokens = \"required\"\n            http_put_response_hop_limit = 1\n          }\n        ...\n        eks-gpu-mng = {\n          ami_type = \"BOTTLEROCKET_x86_64_NVIDIA\"\n    ...\n    ```", "```\n\n    $ kubectl label namespace default pod-security.kubernetes.io/enforce=baseline pod-security.kubernetes.io/warn=restricted pod-security.kubernetes.io/audit=restricted\n    ```", "```\n    $ kubectl run pss-demo --image=nginx --privileged\n    my-llama-finetuned inference endpoint. In this walkthrough, we’ll enhance security by storing the Hugging Face token in AWS Secrets Manager. We’ll then use secrets-store-csi-driver to dynamically retrieve and inject the secret into the K8s Pod during creation. Let’s get started:1.  Store the Hugging Face access token in AWS Secrets Manager and run the following command to create a secret in your AWS account. Replace the value with your access token created in the Hugging Face portal:\n\n        ```", "```\n        $ terraform init\n        $ terraform plan\n        kube-system namespace using the following command; you will notice secrets-store-csi-driver and secrets-store-csi-driver-provider-aws in the output:\n\n        ```", "```\n        $ terraform init\n        $ terraform plan\n        secrets-store-csi-driver, we need to configure SecretProviderClass, a custom Kubernetes resource that defines how secrets-store-csi-driver should connect to and retrieve secrets from external providers such as AWS Secrets Manager. Let’s create one for our setup to retrieve hugging-face-secret from AWS Secrets Manager. To create this, download the secret-provider-class.yaml file from the GitHub repository at https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch9/inference/secret-provider-class.yaml and run the following command:\n\n        ```", "```\n        env command inside the Pod using the following commands:\n        ```", "```\n        $ kubectl get pods -l app.kubernetes.io/name=my-llama-finetuned\n        $ kubectl exec -it $(kubectl get pods -l app.kubernetes.io/name=my-llama-finetuned -o jsonpath=\"{.items[0].metadata.name}\") -- env | grep HUGGING_FACE\n        ```", "```\n\n        ```", "```\n\n        ```", "```\n\n    ```"]