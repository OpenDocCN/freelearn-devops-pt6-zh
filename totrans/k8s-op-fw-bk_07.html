<html><head></head><body>
		<div id="_idContainer023">
			<h1 id="_idParaDest-79"><em class="italic"><a id="_idTextAnchor078"/>Chapter 5</em>: Developing an Operator – Advanced Functionality</h1>
			<p>While a cluster with Operators that are capable of basic installation and upgrade functionality is a considerable improvement over non-Operator-based Kubernetes clusters, there is still more that can be done to improve cluster administration and user experience. Advanced features can help users to achieve more sophisticated automation, guide failure recovery, and inform data-driven deployment decisions with features such as metrics and status updates.</p>
			<p>These are some of the fundamental features for higher-level Operators within the <strong class="bold">Capability Model</strong> (as described in <a href="B18147_01_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to the Operator Framework</em>). As such, this chapter will first explain the cost and benefits of implementing advanced functionality (in relation to the effort necessary to do so) before demonstrating ways to add these features in the following sections:</p>
			<ul>
				<li>Understanding the need for advanced functionality</li>
				<li>Reporting status conditions</li>
				<li>Implementing metrics reporting</li>
				<li>Implementing leader election</li>
				<li>Adding health checks</li>
			</ul>
			<p>Conveniently, the code required to implement these features does not require any significant refactoring of the existing Operator code. In fact, the hierarchical nature of the Capability Model and the development patterns provided by the Operator SDK encourage this iterative construction. It is, therefore, the goal of this chapter to build upon the basic Operator code from <a href="B18147_04_ePub.xhtml#_idTextAnchor066"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing an Operator with the Operator SDK</em>, to create a more complex Operator capable of providing the features we just listed.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor079"/>Technical requirements</h1>
			<p>The examples shown throughout this chapter build upon the project code that was started in <a href="B18147_04_ePub.xhtml#_idTextAnchor066"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing an Operator with the Operator SDK</em>. Therefore, it is recommended to start with that chapter (and the prerequisites for it), which covers project initialization and basic Operator functionality. This is not required, however, and the sections in this chapter can be generally applied to any Operator SDK project. That is, any project initialized by the Operator SDK will work with the following steps and you do not need to specifically implement all of the code from previous chapters. </p>
			<p>With that in mind, the requirements for this chapter are as follows:</p>
			<ul>
				<li>Any existing Operator SDK project</li>
				<li>Go 1.16+</li>
			</ul>
			<p>The Code in Action video for this chapter can be viewed at: <a href="https://bit.ly/3zbsvD0">https://bit.ly/3zbsvD0</a></p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/>Understanding the need for advanced functionality</h1>
			<p>With a basic, functional <a id="_idIndexMarker251"/>Operator already built and ready for deployment, you may be asking, What else do I really need? Indeed, now that your operand is installable and its health is managed by your Operator, there may be nothing more to do. This is a perfectly acceptable level of functionality for an Operator to have. In fact, it may be preferable to start with a simple Operator and iterate as your development resources allow (recall discussing this in <a href="B18147_03_ePub.xhtml#_idTextAnchor050"><em class="italic">Chapter 3</em></a>, <em class="italic">Designing an Operator – CRD, API, and Target Reconciliation</em>).</p>
			<p>The point is that there is no shame during the development of your Operator in stopping here. The Capability Model defines lower-level Operators for a reason (in other words, if it was unacceptable to have an Operator that can only install an operand, then why would Level I be defined at all?).</p>
			<p>However, the Capability Model does define higher-level Operators for a reason too. It is not difficult to imagine, for example, that during the course of a user's interaction with your Operator, they may wish to see more detailed insights into how it performs within a production environment. This is a good use case for adding custom metrics to an Operator. Or, there may be a common failure state that is difficult to debug (wherein status conditions would help expose more information about the failure in an efficient way). </p>
			<p>The following sections are just a few of the most common additional features that help elevate an Operator to a higher level of functionality. Some of these are also covered in additional detail by the Operator SDK documentation on <em class="italic">Advanced Topics</em> (<a href="https://sdk.operatorframework.io/docs/building-operators/golang/advanced-topics/">https://sdk.operatorframework.io/docs/building-operators/golang/advanced-topics/</a>). It is, of course, not<a id="_idIndexMarker252"/> feasible to list every possible feature that you could add to your Operator. But hopefully, these examples serve as a good starting point for your own development.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor081"/>Reporting status conditions</h1>
			<p>Status conditions<a id="_idIndexMarker253"/> were discussed in <a href="B18147_03_ePub.xhtml#_idTextAnchor050"><em class="italic">Chapter 3</em></a>, <em class="italic">Designing an Operator – CRD, API, and Target Reconciliation</em>, as a way to efficiently communicate human-readable information about the Operator's health to administrators. By presenting directly in an Operator's <strong class="bold">CustomResourceDefinition</strong> (<strong class="bold">CRD</strong>), the<a id="_idIndexMarker254"/> information they provide is more easily highlighted and viewable in a more centralized starting point for debugging issues. In this way, they provide an advantage over error logs, which can contain lots of unrelated information and often lack direct context, making them hard to trace to a root cause.</p>
			<p>Implementing conditions in an Operator is<a id="_idIndexMarker255"/> made easier by the <strong class="bold">Kubernetes</strong> API's standardization of the concept. The standard <strong class="source-inline">Condition</strong> type was implemented in <em class="italic">KEP-1623</em> (<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1623-standardize-conditions">https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1623-standardize-conditions</a>) around Kubernetes 1.19. That type is now part of the Kubernetes API in the <strong class="source-inline">k8s.io/apimachinery/pkg/api/meta</strong> module. This allows developers to work with a consistent understanding of how conditions should be reported in Kubernetes with all of the compatibility assurances of Kubernetes API support.</p>
			<p>The Operator Framework has also implemented conditions based on the Kubernetes type, both in the<a id="_idIndexMarker256"/> Operator SDK and the <strong class="bold">Operator Lifecycle Manager</strong> (<strong class="bold">OLM</strong>). Accordingly, conditions can be set either on the Operator's custom resource or on an additional <strong class="source-inline">OperatorCondition</strong> resource that the OLM creates. This section will cover both approaches.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor082"/>Operator CRD conditions</h2>
			<p>As part of the Kubernetes<a id="_idIndexMarker257"/> API conventions covered in <a href="B18147_03_ePub.xhtml#_idTextAnchor050"><em class="italic">Chapter 3</em></a><em class="italic">,</em> <em class="italic">Designing an Operator – CRD, API, and Target Reconciliation</em>, objects (including custom resources) should include both a <strong class="source-inline">spec</strong> and <strong class="source-inline">status</strong> field. In the case of Operators, we are using <strong class="source-inline">spec</strong> as an input for configuring the Operator's parameters already. However, we have not yet modified the <strong class="source-inline">status</strong> field. We will now change that by adding a list of conditions as a new field in <strong class="source-inline">api/v1alpha1/nginxoperator_types.go</strong>:</p>
			<pre class="source-code">// NginxOperatorStatus defines the observed state of NginxOperator</pre>
			<pre class="source-code">type NginxOperatorStatus struct {</pre>
			<pre class="source-code">   // Conditions is the list of status condition updates</pre>
			<pre class="source-code">   Conditions []metav1.Condition `json:"conditions"`</pre>
			<pre class="source-code">}</pre>
			<p>Then, we will run <strong class="source-inline">make generate</strong> (to update the generated client code) and <strong class="source-inline">make manifests</strong> (to update the<a id="_idIndexMarker258"/> Operator's CRD with the new field), or simply <strong class="source-inline">make</strong> (which runs all generators, though we don't need some of them right now). The new field is now reflected in the CRD:</p>
			<pre class="source-code">properties:</pre>
			<pre class="source-code">  <strong class="bold">conditions</strong>:</pre>
			<pre class="source-code">    <strong class="bold">description: Conditions is the list of the most recent status condition</strong></pre>
			<pre class="source-code">      updates</pre>
			<pre class="source-code">    items:</pre>
			<pre class="source-code">      description: "Condition contains details for one aspect </pre>
			<pre class="source-code">                   of the current state of this API Resource. </pre>
			<pre class="source-code">                   --- This struct is intended for direct use </pre>
			<pre class="source-code">                   as an array at the field path </pre>
			<pre class="source-code">                   .status.conditions."</pre>
			<pre class="source-code">      properties:</pre>
			<pre class="source-code">        lastTransitionTime:</pre>
			<pre class="source-code">          description: lastTransitionTime is the last time the </pre>
			<pre class="source-code">                       condition transitioned from one status </pre>
			<pre class="source-code">                       to another. This should be when the </pre>
			<pre class="source-code">                       underlying condition changed.  If that </pre>
			<pre class="source-code">                       is not known, then using the time when </pre>
			<pre class="source-code">                       the API field changed is acceptable.</pre>
			<pre class="source-code">          format: date-time</pre>
			<pre class="source-code">          type: string</pre>
			<pre class="source-code">        message:</pre>
			<pre class="source-code">          description: message is a human readable message </pre>
			<pre class="source-code">                       indicating details about the </pre>
			<pre class="source-code">                       transition. This may be an empty </pre>
			<pre class="source-code">                       string.</pre>
			<pre class="source-code">          maxLength: 32768</pre>
			<pre class="source-code">          type: string</pre>
			<pre class="source-code">...</pre>
			<pre class="source-code">        status:</pre>
			<pre class="source-code">          description: status of the condition, one of True, </pre>
			<pre class="source-code">                       False, Unknown.</pre>
			<pre class="source-code">          enum:</pre>
			<pre class="source-code">          - "True"</pre>
			<pre class="source-code">          - "False"</pre>
			<pre class="source-code">          - Unknown</pre>
			<pre class="source-code">          type: string</pre>
			<pre class="source-code">        type:</pre>
			<pre class="source-code">          description: type of condition in CamelCase or in </pre>
			<pre class="source-code">                       foo.example.com/CamelCase. --- Many </pre>
			<pre class="source-code">                       .condition.type values are consistent </pre>
			<pre class="source-code">                       across resources like Available, but </pre>
			<pre class="source-code">                       because arbitrary conditions can be </pre>
			<pre class="source-code">                       useful (see .node.status.conditions), </pre>
			<pre class="source-code">                       the ability to deconflict is important. </pre>
			<pre class="source-code">                       The regex it matches is </pre>
			<pre class="source-code">                     (dns1123SubdomainFmt/)?(qualifiedNameFmt)</pre>
			<pre class="source-code">          maxLength: 316</pre>
			<pre class="source-code">          pattern: ^([a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*/)?(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])$</pre>
			<pre class="source-code">          type: string</pre>
			<pre class="source-code">      required:</pre>
			<pre class="source-code">      - lastTransitionTime</pre>
			<pre class="source-code">      - message</pre>
			<pre class="source-code">      - reason</pre>
			<pre class="source-code">      - status</pre>
			<pre class="source-code">      - type</pre>
			<pre class="source-code">      type: object</pre>
			<pre class="source-code">    type: array</pre>
			<pre class="source-code">required:</pre>
			<pre class="source-code">- conditions</pre>
			<pre class="source-code">type: object</pre>
			<p>Note that this also imports all of the embedded validation requirements for the <strong class="source-inline">Condition</strong> type from the Kubernetes API. </p>
			<p>Now that the Operator's CRD has a field to report the latest status conditions, the code can be updated to implement them. For this, we can use the <strong class="source-inline">SetStatusCondition()</strong> helper function, which is available in the <strong class="source-inline">k8s.io/apimachinery/pkg/api/meta</strong> module. For this example, we will start with a single condition called <strong class="source-inline">OperatorDegraded</strong>, which will default to <strong class="source-inline">False</strong> to indicate that the Operator is successfully reconciling changes in the cluster. If the Operator does encounter an<a id="_idIndexMarker259"/> error, however, we will update this condition to <strong class="source-inline">True</strong> with a message indicating the error. This will involve some refactoring of the <strong class="source-inline">Reconcile()</strong> function in <strong class="source-inline">controllers/nginxoperator_controller.go</strong> to match the following:</p>
			<pre class="source-code">func (r *NginxOperatorReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {</pre>
			<pre class="source-code">   logger := log.FromContext(ctx)</pre>
			<pre class="source-code">   operatorCR := &amp;operatorv1alpha1.NginxOperator{}</pre>
			<pre class="source-code">   err := r.Get(ctx, req.NamespacedName, operatorCR)</pre>
			<pre class="source-code">   if err != nil &amp;&amp; errors.IsNotFound(err) {</pre>
			<pre class="source-code">      logger.Info("Operator resource object not found.")</pre>
			<pre class="source-code">      return ctrl.Result{}, nil</pre>
			<pre class="source-code">   } else if err != nil {</pre>
			<pre class="source-code">      logger.Error(err, "Error getting operator resource object")</pre>
			<pre class="source-code">      <strong class="bold">meta.SetStatusCondition(&amp;operatorCR.Status.Conditions, metav1.Condition{</strong></pre>
			<pre class="source-code"><strong class="bold">         Type:               "OperatorDegraded",</strong></pre>
			<pre class="source-code"><strong class="bold">         Status:             metav1.ConditionTrue,</strong></pre>
			<pre class="source-code"><strong class="bold">         Reason:             "OperatorResourceNotAvailable",</strong></pre>
			<pre class="source-code"><strong class="bold">         LastTransitionTime: metav1.NewTime(time.Now()),</strong></pre>
			<pre class="source-code"><strong class="bold">         Message:            fmt.Sprintf("unable to get operator custom resource: %s", err.Error()),</strong></pre>
			<pre class="source-code"><strong class="bold">      })</strong></pre>
			<pre class="source-code"><strong class="bold">      return ctrl.Result{}, utilerrors.NewAggregate([]error{err, r.Status().Update(ctx, operatorCR)})</strong></pre>
			<pre class="source-code">   }</pre>
			<p>The preceding code will now attempt to report a degraded status condition if the controller is initially unable to<a id="_idIndexMarker260"/> access the Operator's CRD. The code continues as follows:</p>
			<pre class="source-code">   deployment := &amp;appsv1.Deployment{}</pre>
			<pre class="source-code">   create := false</pre>
			<pre class="source-code">   err = r.Get(ctx, req.NamespacedName, deployment)</pre>
			<pre class="source-code">   if err != nil &amp;&amp; errors.IsNotFound(err) {</pre>
			<pre class="source-code">      create = true</pre>
			<pre class="source-code">      deployment = assets.GetDeploymentFromFile("assets/nginx_deployment.yaml")</pre>
			<pre class="source-code">   } else if err != nil {</pre>
			<pre class="source-code">      logger.Error(err, "Error getting existing Nginx deployment.")</pre>
			<pre class="source-code">      <strong class="bold">meta.SetStatusCondition(&amp;operatorCR.Status.Conditions, metav1.Condition{</strong></pre>
			<pre class="source-code"><strong class="bold">         Type:               "OperatorDegraded",</strong></pre>
			<pre class="source-code"><strong class="bold">         Status:             metav1.ConditionTrue,</strong></pre>
			<pre class="source-code"><strong class="bold">         Reason:             "OperandDeploymentNotAvailable",</strong></pre>
			<pre class="source-code"><strong class="bold">         LastTransitionTime: metav1.NewTime(time.Now()),</strong></pre>
			<pre class="source-code"><strong class="bold">         Message:            fmt.Sprintf("unable to get operand deployment: %s", err.Error()),</strong></pre>
			<pre class="source-code"><strong class="bold">      })</strong></pre>
			<pre class="source-code"><strong class="bold">      return ctrl.Result{}, utilerrors.NewAggregate([]error{err, r.Status().Update(ctx, operatorCR)})</strong></pre>
			<pre class="source-code">   }</pre>
			<p>This section of code works <a id="_idIndexMarker261"/>similarly to the previous, but now reporting a degraded condition if the Deployment manifest for the nginx operand is unavailable:</p>
			<pre class="source-code">   deployment.Namespace = req.Namespace</pre>
			<pre class="source-code">   deployment.Name = req.Name</pre>
			<pre class="source-code">   if operatorCR.Spec.Replicas != nil {</pre>
			<pre class="source-code">      deployment.Spec.Replicas = operatorCR.Spec.Replicas</pre>
			<pre class="source-code">   }</pre>
			<pre class="source-code">   if operatorCR.Spec.Port != nil {</pre>
			<pre class="source-code">      deployment.Spec.Template.Spec.Containers[0].Ports[0].ContainerPort = *operatorCR.Spec.Port</pre>
			<pre class="source-code">   }</pre>
			<pre class="source-code">   ctrl.SetControllerReference(operatorCR, deployment, r.Scheme)</pre>
			<pre class="source-code">   if create {</pre>
			<pre class="source-code">      err = r.Create(ctx, deployment)</pre>
			<pre class="source-code">   } else {</pre>
			<pre class="source-code">      err = r.Update(ctx, deployment)</pre>
			<pre class="source-code">   }</pre>
			<pre class="source-code">if err != nil {</pre>
			<pre class="source-code">   <strong class="bold">meta.SetStatusCondition(&amp;operatorCR.Status.Conditions, metav1.Condition{</strong></pre>
			<pre class="source-code"><strong class="bold">      Type:               "OperatorDegraded",</strong></pre>
			<pre class="source-code"><strong class="bold">      Status:             metav1.ConditionTrue,</strong></pre>
			<pre class="source-code"><strong class="bold">      Reason:             "OperandDeploymentFailed",</strong></pre>
			<pre class="source-code"><strong class="bold">      LastTransitionTime: metav1.NewTime(time.Now()),</strong></pre>
			<pre class="source-code"><strong class="bold">      Message:            fmt.Sprintf("unable to update operand deployment: %s", err.Error()),</strong></pre>
			<pre class="source-code"><strong class="bold">   })</strong></pre>
			<pre class="source-code"><strong class="bold">   return ctrl.Result{}, utilerrors.NewAggregate([]error{err, r.Status().Update(ctx, operatorCR)})</strong></pre>
			<pre class="source-code">}</pre>
			<p>If the attempt to update the operand Deployment fails, this block will report that the Operator is degraded as well. If the controller is able to succeed past this point, then there is no degradation to <a id="_idIndexMarker262"/>report. For this reason, the next block of code will finish the function by updating the Operator's CRD to show that there is no degraded condition:</p>
			<pre class="source-code">   <strong class="bold">meta.SetStatusCondition(&amp;operatorCR.Status.Conditions, metav1.Condition{</strong></pre>
			<pre class="source-code"><strong class="bold">      Type:               "OperatorDegraded",</strong></pre>
			<pre class="source-code"><strong class="bold">      Status:             metav1.ConditionFalse,</strong></pre>
			<pre class="source-code"><strong class="bold">      Reason:             "OperatorSucceeded",</strong></pre>
			<pre class="source-code"><strong class="bold">      LastTransitionTime: metav1.NewTime(time.Now()),</strong></pre>
			<pre class="source-code"><strong class="bold">      Message:            "operator successfully reconciling",</strong></pre>
			<pre class="source-code"><strong class="bold">   })</strong></pre>
			<pre class="source-code"><strong class="bold">   return ctrl.Result{}, utilerrors.NewAggregate([]error{err, r.Status().Update(ctx, operatorCR)})</strong></pre>
			<pre class="source-code">}</pre>
			<p>In this code, we have added four calls to <strong class="source-inline">SetStatusCondition()</strong>, in which the condition with the type of <strong class="source-inline">"OperatorDegraded"</strong> is updated with the current time and a brief reason:</p>
			<ol>
				<li>If the Operator is unable to access its custom resource (for any reason besides a simple <strong class="source-inline">IsNotFound</strong> error), set the condition to <strong class="source-inline">True</strong> with the reason, <strong class="source-inline">OperatorResourceNotAvailable</strong>.</li>
				<li>If we are unable to get the nginx Deployment manifest from the embedded YAML file, update the condition to <strong class="source-inline">True</strong> with the reason, <strong class="source-inline">OperandDeploymentNotAvailable</strong>.</li>
				<li>If the Deployment manifest is successfully found but a call to create or update it fails, set the condition to <strong class="source-inline">True</strong> with the reason, <strong class="source-inline">OperandDeploymentFailed</strong>.</li>
				<li>Finally, if the <strong class="source-inline">Reconcile()</strong> function has completed with no critical errors, set the <strong class="source-inline">OperatorDegraded</strong> condition to <strong class="source-inline">False</strong>. <p class="callout-heading">True or False as a Success Indicator</p><p class="callout">Note that, as discussed in the status conventions in <a href="B18147_03_ePub.xhtml#_idTextAnchor050"><em class="italic">Chapter 3</em></a>, <em class="italic">Designing an Operator – CRD, API, and Target Reconciliation</em>, the <strong class="source-inline">False</strong> condition indicates a successful run. We could just as easily have inverted this logic, naming the condition something like <strong class="source-inline">OperatorSucceeded</strong>, where the default case is <strong class="source-inline">True</strong> and any failures change the condition to <strong class="source-inline">False</strong>. Doing so would still be consistent with Kubernetes conventions, so the decision is ultimately up to the developer based on the intent they wish to convey.</p></li>
			</ol>
			<p>In this example, we have <a id="_idIndexMarker263"/>used string literals for each <strong class="source-inline">Reason</strong> update. In a practical application, it is common to define various <strong class="source-inline">Reason</strong> constants in the Operator's API, which allows for consistent reusability. For example, we could define the following in <strong class="source-inline">api/v1alpha1/nginxoperator_types.go</strong> and use them through their constant names:</p>
			<pre class="source-code">const (</pre>
			<pre class="source-code"> ReasonCRNotAvailable  = "OperatorResourceNotAvailable"</pre>
			<pre class="source-code"> ReasonDeploymentNotAvailable = "OperandDeploymentNotAvailable"</pre>
			<pre class="source-code">   ReasonOperandDeploymentFailed = "OperandDeploymentFailed"</pre>
			<pre class="source-code">   ReasonSucceeded               = "OperatorSucceeded"</pre>
			<pre class="source-code">)</pre>
			<p>The exact naming scheme for a condition reason is up to the preference of the developer, with the only Kubernetes condition being that it must be CamelCase. For that reason, different condition types and reasons can be named whatever is relevant and preferred for that specific Operator. The only standard condition name that currently exists is <strong class="source-inline">Upgradeable</strong>, which is consumed by the OLM. We will show how to use that condition in the next section.</p>
			<p>With these conditions implemented, we will be able to see the following output when interacting with a custom<a id="_idIndexMarker264"/> resource for our Operator:</p>
			<pre class="source-code">$ kubectl describe nginxoperators/cluster</pre>
			<pre class="source-code">Name:         cluster</pre>
			<pre class="source-code">Namespace:    nginx-operator-system</pre>
			<pre class="source-code">API Version:  operator.example.com/v1alpha1</pre>
			<pre class="source-code">Kind:         NginxOperator</pre>
			<pre class="source-code">Metadata:</pre>
			<pre class="source-code">  Creation Timestamp:  2022-01-20T21:47:32Z</pre>
			<pre class="source-code">  Generation:          1</pre>
			<pre class="source-code">...</pre>
			<pre class="source-code">Spec:</pre>
			<pre class="source-code">  Replicas:  1</pre>
			<pre class="source-code">Status:</pre>
			<pre class="source-code">  Conditions:</pre>
			<pre class="source-code">    Last Transition Time:  2022-01-20T21:47:32Z</pre>
			<pre class="source-code">    Message:               operator successfully reconciling</pre>
			<pre class="source-code">    Reason:                OperatorSucceeded</pre>
			<pre class="source-code">    Status:                False</pre>
			<pre class="source-code">    Type:                  OperatorDegraded</pre>
			<p>In the next section, we will show how to report Operator conditions directly to the OLM.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor083"/>Using the OLM OperatorCondition</h2>
			<p>We have already discussed<a id="_idIndexMarker265"/> how the OLM is capable of managing the currently installed list of Operators, including upgrading and downgrading. In <a href="B18147_07_ePub.xhtml#_idTextAnchor108"><em class="italic">Chapter 7</em></a>, <em class="italic">Installing and Running Operators with the Operator Lifecycle Manager</em>, we will show the OLM in action for some of these features. But, for the moment, we can implement condition reporting in our Operator so that the OLM is aware of certain states that could prevent the Operator from upgrading. Combined with status conditions that are unique to the Operator (as reported in its CRD), this can help inform the OLM of critical information regarding the Operator's current state.</p>
			<p>To read Operator conditions, the OLM creates a custom resource called <strong class="source-inline">OperatorCondition</strong>. The OLM will automatically create an instance of this object for every Operator that it manages from a CRD. A sample <strong class="source-inline">OperatorCondition</strong> object looks like the following:</p>
			<pre class="source-code">apiVersion: operators.coreos.com/v1</pre>
			<pre class="source-code">kind: OperatorCondition</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: sample-operator</pre>
			<pre class="source-code">  namespace: operator-ns</pre>
			<pre class="source-code">status:</pre>
			<pre class="source-code">  conditions:</pre>
			<pre class="source-code">  - type: Upgradeable</pre>
			<pre class="source-code">    status: False</pre>
			<pre class="source-code">    reason: "OperatorBusy"</pre>
			<pre class="source-code">    message: "Operator is currently busy with a critical task"</pre>
			<pre class="source-code">    lastTransitionTime: "2022-01-19T12:00:00Z"</pre>
			<p>This object uses the same <strong class="source-inline">Condition</strong> type from the Kubernetes API as shown earlier (when implementing status conditions in the Operator CRD). This means it also includes all of the same fields, such as <strong class="source-inline">type</strong>, <strong class="source-inline">status</strong>, <strong class="source-inline">reason</strong>, and <strong class="source-inline">message</strong>, which can be updated the same way we did before. The difference is that now, setting <strong class="source-inline">type</strong> to <strong class="source-inline">Upgradeable</strong> will instruct the OLM to block any attempts to upgrade the Operator.</p>
			<p>The other difference is that the Operator needs to report this status change to a different CRD (rather than its own). To do that, there is a library available at <a href="https://github.com/operator-framework/operator-lib">https://github.com/operator-framework/operator-lib</a>, which includes helper functions to update the <strong class="source-inline">OperatorCondition</strong> CRD. Details on using this library are available in the Operator Framework enhancements repository at <a href="https://github.com/operator-framework/enhancements/blob/master/enhancements/operator-conditions-lib.md">https://github.com/operator-framework/enhancements/blob/master/enhancements/operator-conditions-lib.md</a>. One way this can be done is in our <a id="_idIndexMarker266"/>nginx Operator by modifying the <strong class="source-inline">Reconcile()</strong> function in <strong class="source-inline">controllers/nginxoperator_controller.go</strong> like so:</p>
			<pre class="source-code">import (</pre>
			<pre class="source-code">...</pre>
			<pre class="source-code">   <strong class="bold">apiv2 "github.com/operator-framework/api/pkg/operators/v2"</strong></pre>
			<pre class="source-code">   <strong class="bold">"github.com/operator-framework/operator-lib/conditions"</strong></pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">func (r *NginxOperatorReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {</pre>
			<pre class="source-code">...</pre>
			<pre class="source-code">  condition, err := <strong class="bold">conditions.InClusterFactory{r.Client}</strong>.</pre>
			<pre class="source-code">   <strong class="bold">NewCondition(apiv2.ConditionType(apiv2.Upgradeable))</strong></pre>
			<pre class="source-code">  if err != nil {</pre>
			<pre class="source-code">   return ctrl.Result{}, err</pre>
			<pre class="source-code">  }</pre>
			<pre class="source-code">  err = <strong class="bold">condition.Set</strong>(ctx, <strong class="bold">metav1.ConditionTrue</strong>,</pre>
			<pre class="source-code">   <strong class="bold">conditions.WithReason("OperatorUpgradeable")</strong>,</pre>
			<pre class="source-code">   <strong class="bold">conditions.WithMessage("The operator is upgradeable"))</strong></pre>
			<pre class="source-code">  if err != nil {</pre>
			<pre class="source-code">   return ctrl.Result{}, err</pre>
			<pre class="source-code">  }</pre>
			<pre class="source-code">...</pre>
			<pre class="source-code">}</pre>
			<p>This code imports two new modules: the Operator Framework V2 API and the <strong class="source-inline">operator-lib/conditions</strong> library. It then instantiates a new <strong class="source-inline">Factory</strong> object, which uses the same Kubernetes client that is already available to the Operator. That factory can then create new <strong class="source-inline">Condition</strong> objects with <strong class="source-inline">NewCondition()</strong>, which accepts <strong class="source-inline">ConditionType</strong> (which is really just a string) and creates a condition with that type. </p>
			<p>In this example, <strong class="source-inline">Condition</strong> is<a id="_idIndexMarker267"/> created with the <strong class="source-inline">apiv2.Upgradeable</strong> type, which is a constant defined by the Operator Framework for the <strong class="source-inline">Upgradeable</strong> condition that is understood by the OLM.</p>
			<p>Next, the <strong class="source-inline">condition.Set()</strong> function updates the <strong class="source-inline">OperatorCondition</strong> CRD object that the OLM created for our Operator. Specifically, this adds (or updates) the list of conditions with the new condition we just created and the status that is passed to it (in this case, <strong class="source-inline">True</strong>). There are also two functions available that can be optionally passed to <strong class="source-inline">Set()</strong> (<strong class="source-inline">WithReason()</strong> and <strong class="source-inline">WithMessage()</strong>), which set the reason and message for the condition.</p>
			<p>Using these helpers greatly simplifies the work necessary to retrieve and update the <strong class="source-inline">OperatorCondition</strong> CRD object created for our Operator by the OLM. In addition, the OLM takes steps to make sure that Operators cannot delete the CRD or modify anything outside of the object's status. However, there are some other fields in the <strong class="source-inline">OperatorCondition</strong> CRD <strong class="source-inline">spec</strong> that can be modified by administrators. Specifically, the <strong class="source-inline">overrides</strong> field allows users to manually bypass automatically reported condition updates that would otherwise block upgrading. A sample usage of this field looks as follows:</p>
			<pre class="source-code">apiVersion: operators.coreos.com/v1</pre>
			<pre class="source-code">kind: OperatorCondition</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: sample-operator</pre>
			<pre class="source-code">  namespace: operator-ns</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  overrides:</pre>
			<pre class="source-code">  - type: Upgradeable</pre>
			<pre class="source-code">    status: True</pre>
			<pre class="source-code">    reason: "OperatorIsStable"</pre>
			<pre class="source-code">    message: "Forcing an upgrade to bypass bug state"</pre>
			<pre class="source-code">status:</pre>
			<pre class="source-code">  conditions:</pre>
			<pre class="source-code">  - type: Upgradeable</pre>
			<pre class="source-code">    status: False</pre>
			<pre class="source-code">    reason: "OperatorBusy"</pre>
			<pre class="source-code">    message: "Operator is currently busy with a critical task"</pre>
			<pre class="source-code">    lastTransitionTime: "2022-01-19T12:00:00Z"</pre>
			<p>Using <strong class="source-inline">overrides</strong> like this can <a id="_idIndexMarker268"/>be useful in the case of known issues or bugs that should not prevent the Operator from upgrading.</p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor084"/>Implementing metrics reporting</h1>
			<p>Metrics are a crucial aspect of <a id="_idIndexMarker269"/>any Kubernetes cluster. Metrics tools can provide detailed insights into almost any measurable data in the cluster. This is why metrics are a key part of graduating an Operator to Level IV in the Capability Model. In fact, most native Kubernetes controllers already report metrics about themselves, for <a id="_idIndexMarker270"/>example, <strong class="bold">kube-scheduler</strong> and <strong class="bold">kube-controller-manager</strong>. These components export <a id="_idIndexMarker271"/>data in metrics such as <strong class="source-inline">schedule_attempts_total</strong>, which reports the number of attempts the scheduler has made to schedule Pods onto Nodes.</p>
			<p>The original design for the Kubernetes monitoring architecture (<a href="https://github.com/kubernetes/design-proposals-archive/blob/main/instrumentation/monitoring_architecture.md">https://github.com/kubernetes/design-proposals-archive/blob/main/instrumentation/monitoring_architecture.md</a>) defines metrics such as <strong class="source-inline">schedule_attempts_total</strong> as <strong class="bold">service metrics</strong>. The alternative to service metrics is <strong class="bold">core metrics</strong>, which are<a id="_idIndexMarker272"/> metrics that are generally available from all <a id="_idIndexMarker273"/>components. Core metrics currently include information <a id="_idIndexMarker274"/>about CPU and memory usage and are scraped by the Kubernetes <strong class="bold">metrics-server</strong> application (<a href="https://github.com/kubernetes-sigs/metrics-server">https://github.com/kubernetes-sigs/metrics-server</a>). </p>
			<p>On the other hand, service metrics expose application-specific data that is defined in the code of individual <a id="_idIndexMarker275"/>components. All of this data can be scraped and<a id="_idIndexMarker276"/> aggregated by tools such as <strong class="bold">Prometheus</strong> (<a href="https://prometheus.io">https://prometheus.io</a>) or <strong class="bold">OpenTelemetry</strong> (<a href="https://opentelemetry.io">https://opentelemetry.io</a>), and presented with <a id="_idIndexMarker277"/>frontend visualization applications such as <strong class="bold">Grafana</strong> (<a href="https://grafana.com">https://grafana.com</a>).</p>
			<p>The entire concept of metrics in<a id="_idIndexMarker278"/> Kubernetes extends far beyond just implementation with regard to Operators. While that, unfortunately, means there is a lot of information that is outside the scope of this chapter, there is a great deal of community and documentation resources available that cover the fundamentals of this topic. Instead, this section will focus on the relevant Operator implementation steps for a new service metric with an assumed prior understanding of the basics related to metrics in general.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor085"/>Adding a custom service metric</h2>
			<p>The boilerplate code <a id="_idIndexMarker279"/>that is scaffolded by <strong class="source-inline">operator-sdk</strong> when the project is initialized already includes the code and dependencies necessary to <a id="_idIndexMarker280"/>expose a metrics endpoint in the Operator Pod. By default, this is the <strong class="source-inline">/metrics</strong> path on port <strong class="source-inline">8080</strong>. This eliminates the need for meticulous HTTP handler code and allows us to focus instead on simply implementing the metrics themselves, as described in the Prometheus documentation (https://prometheus.io/docs/guides/go-application/#adding-your-own-metrics).</p>
			<p class="callout-heading">Kubebuilder Metrics</p>
			<p class="callout">The built-in metrics handler code is another aspect of the Operator SDK that is actually provided by Kubebuilder under the hood. This implementation relies on the metrics library imported from <strong class="source-inline">sigs.k8s.io/controller-runtime</strong>. That library includes features such as a global registry of metrics that is already available to the core Operator code. This library is easy to hook into in order to register new metrics and update them from anywhere in the <a id="_idIndexMarker281"/>Operator's code base. More information on metrics and their use is available in the kubebuilder book at <a href="https://book.kubebuilder.io/reference/metrics.html">https://book.kubebuilder.io/reference/metrics.html</a>.</p>
			<p>The <strong class="source-inline">controller-runtime</strong> library that this functionality is built with includes several metrics about the Operator's controller <a id="_idIndexMarker282"/>code already, each indicated by the prefix <strong class="source-inline">controller_runtime_</strong>. These include the following:</p>
			<ul>
				<li><strong class="source-inline">controller_runtime_reconcile_errors_total</strong> – A counter metric that shows the cumulative number of times the <strong class="source-inline">Reconcile()</strong> function returned a non-nil error</li>
				<li><strong class="source-inline">controller_runtime_reconcile_time_seconds_bucket</strong> – A histogram showing the<a id="_idIndexMarker283"/> latency for individual reconciliation attempts</li>
				<li><strong class="source-inline">controller_runtime_reconcile_total</strong> – A counter that increases with every attempt to call <strong class="source-inline">Reconcile()</strong></li>
			</ul>
			<p>For this example, we will be recreating the last metric, <strong class="source-inline">controller_runtime_reconcile_total</strong>, as a way to report the number of attempts our Operator has made to reconcile its state in the cluster.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor086"/>RED metrics</h2>
			<p>When it comes to what type of <a id="_idIndexMarker284"/>metrics to define in any application, the <a id="_idIndexMarker285"/>possibilities are practically infinite. This can cause overwhelming decision fatigue for developers, as it may seem there is no good place to start. So, what are the most important metrics for an Operator to expose? The Operator SDK documentation recommends following the <strong class="bold">RED method</strong>, which outlines three key types of metrics that every service should expose in a Kubernetes cluster:</p>
			<ul>
				<li><strong class="bold">Rate</strong> – A metric showing<a id="_idIndexMarker286"/> the number of requests or attempts per second. This can provide insight into how much work an Operator is doing, highlighting frequent re-queues, which could indicate <strong class="source-inline">hotloop</strong> conditions or suboptimal request pipelines.</li>
				<li><strong class="bold">Errors</strong> – Metrics showing the <a id="_idIndexMarker287"/>number of failed attempts by a service (for example, <strong class="source-inline">controller_runtime_reconcile_errors_total</strong>). When correlated with the Rate metric, this can help debug common failures that are degrading an Operator's performance.</li>
				<li><strong class="bold">Duration</strong> – Duration (or latency) metrics<a id="_idIndexMarker288"/> show the length of time it takes for an Operator to complete its work (<strong class="source-inline">controller_runtime_reconcile_time_seconds</strong>). This information can indicate poor performance or other conditions that degrade the cluster's health.</li>
			</ul>
			<p>These basic metrics can provide the foundation for defining <strong class="bold">service-level objectives</strong> (<strong class="bold">SLOs</strong>) to ensure your Operator<a id="_idIndexMarker289"/> functions within the expected standards of your application.</p>
			<p>To begin adding a custom<a id="_idIndexMarker290"/> metric to our Operator, it is a good practice to organize metrics definitions into their own package by creating a new file under <strong class="source-inline">controllers/metrics/metrics.go</strong>. This new module will hold the declaration of our new metric and <a id="_idIndexMarker291"/>automatically register it with the global registry available from <strong class="source-inline">sigs.k8s.io/controller-runtime/pkg/metrics</strong>. In the following code, we define this file and instantiate a new custom metric:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">controllers/metrics/metrics.go:</p>
			<pre class="source-code">package metrics</pre>
			<pre class="source-code">import (</pre>
			<pre class="source-code">   "github.com/prometheus/client_golang/prometheus"</pre>
			<pre class="source-code">   "sigs.k8s.io/controller-runtime/pkg/metrics"</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">var (</pre>
			<pre class="source-code">   <strong class="bold">ReconcilesTotal</strong> = prometheus.NewCounter(</pre>
			<pre class="source-code">      prometheus.CounterOpts{</pre>
			<pre class="source-code">         <strong class="bold">Name: "reconciles_total",</strong></pre>
			<pre class="source-code">         Help: "Number of total reconciliation attempts",</pre>
			<pre class="source-code">      },</pre>
			<pre class="source-code">   )</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">func init() {</pre>
			<pre class="source-code">   <strong class="bold">metrics.Registry.MustRegister(ReconcilesTotal)</strong></pre>
			<pre class="source-code">}</pre>
			<p>This file depends on the <a id="_idIndexMarker292"/>Prometheus client library to define a new counter (a simple, increasing metric) that we store in the <strong class="source-inline">ReconcilesTotal</strong> public variable. The<a id="_idIndexMarker293"/> actual name of the metric is <strong class="source-inline">reconciles_total</strong>, which is the name that will be exposed to Prometheus.</p>
			<p class="callout-heading">Metrics Naming Best Practices</p>
			<p class="callout">In a real environment, it is best to include a prefix to a metric's name that is specific to the application exporting that metric. This is one of the best practices recommended by <strong class="bold">Prometheus</strong> (<a href="https://prometheus.io/docs/practices/naming/">https://prometheus.io/docs/practices/naming/</a>). Although we haven't done that here (for simplicity), we have<a id="_idIndexMarker294"/> followed another best practice of appending <strong class="source-inline">_total</strong> to this cumulative metric. It is helpful to be familiar with these practices, not just for developing your own metrics, but also to know what to expect when interacting with other metrics.</p>
			<p>With this file's <strong class="source-inline">init()</strong> function automatically registering the new metric with the global registry, we can now update this metric from anywhere in the Operator's code. Since this is a measure of the total number of reconciliation attempts by the controller, it makes sense to update it at the start of the <strong class="source-inline">Reconcile()</strong> function declaration in <strong class="source-inline">controllers/nginxoperator_controller.go</strong>. This can be done with two new lines of code, shown<a id="_idIndexMarker295"/> in the following code:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">controllers/nginxoperator_controller.go:</p>
			<pre class="source-code">import (</pre>
			<pre class="source-code">...</pre>
			<pre class="source-code">  <strong class="bold">"github.com/sample/nginx-operator/controllers/metrics"</strong></pre>
			<pre class="source-code">...</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">func (r *NginxOperatorReconciler) Reconcile(...) (...) {</pre>
			<pre class="source-code">  <strong class="bold">metrics.ReconcilesTotal.Inc()</strong></pre>
			<pre class="source-code">  ...</pre>
			<pre class="source-code">}</pre>
			<p>All that is necessary is to import the new <strong class="source-inline">metrics</strong> package just created and call <strong class="source-inline">metrics.ReconcilesTotal.Inc()</strong>. This function doesn't return anything, so there is no need to add any error <a id="_idIndexMarker296"/>handling or status updates. We also want to make this the first line of the function since the goal is to increment for every call to <strong class="source-inline">Reconcile()</strong> (regardless of whether the call succeeds or not). </p>
			<p>The updated metric value is automatically reported by the metrics endpoint initialized by kubebuilder, so it is available to view through a properly configured Prometheus instance, as shown here:</p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B18147_05_001.jpg" alt="Figure 5.1 – Screenshot of reconciles_total metric graph in the Prometheus UI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Screenshot of reconciles_total metric graph in the Prometheus UI</p>
			<p>When <a id="_idIndexMarker297"/>compared to the<a id="_idIndexMarker298"/> built-in <strong class="source-inline">controller_runtime_reconcile_total</strong> metric, we see that the values are the same:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B18147_05_002.jpg" alt="Figure 5.2 – Screenshot comparing custom reconciles_total metric to built-in controller_runtime_reconcile_total"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Screenshot comparing custom reconciles_total metric to built-in controller_runtime_reconcile_total</p>
			<p>We will cover<a id="_idIndexMarker299"/> more about how to install and configure Prometheus to capture this<a id="_idIndexMarker300"/> custom metric in <a href="B18147_06_ePub.xhtml#_idTextAnchor090"><em class="italic">Chapter 6</em></a>, <em class="italic">Building and Deploying Your Operator</em>.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor087"/>Implementing leader election</h1>
			<p>Leader election is an<a id="_idIndexMarker301"/> important concept in any distributed computing system, not just Kubernetes (and not just for Operators, either). High-availability applications will often deploy multiple replicas of their workload Pods to support the uptime guarantees their users expect. In situations where only one workload Pod can do work in a cluster at a time, that<a id="_idIndexMarker302"/> replica is known as the <strong class="bold">leader</strong>. The remaining replicas will wait, running but not doing anything significant, until the current leader becomes unavailable or gives up its status as the leader. Those Pods will then determine among themselves who should be the new leader.</p>
			<p>Enabling proper leader election can greatly benefit the application's uptime. This can include graceful failover handling if one replica fails, or help to maintain application accessibility during rolling upgrades.</p>
			<p>The Operator SDK makes leader <a id="_idIndexMarker303"/>election for Operators simple to implement. The boilerplate code scaffolded by <strong class="source-inline">operator-sdk</strong> creates a flag in our Operator, <strong class="source-inline">--leader-elect</strong>, which defaults to <strong class="source-inline">false</strong> and disables leader election. That flag is passed to the <strong class="source-inline">LeaderElection</strong> option value in the controller initialization function <strong class="source-inline">ctrl.NewManager()</strong>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">main.go:</p>
			<pre class="source-code">func main() {</pre>
			<pre class="source-code">...</pre>
			<pre class="source-code">   var enableLeaderElection bool</pre>
			<pre class="source-code">   flag.BoolVar(&amp;enableLeaderElection, "<strong class="bold">leader-elect</strong>", false,</pre>
			<pre class="source-code">      "Enable leader election for controller manager. "+</pre>
			<pre class="source-code">         "Enabling this will ensure there is only one active controller manager.")</pre>
			<pre class="source-code">...</pre>
			<pre class="source-code">   mgr, err := <strong class="bold">ctrl.NewManager</strong>(ctrl.GetConfigOrDie(), ctrl.Options{</pre>
			<pre class="source-code">...</pre>
			<pre class="source-code">      HealthProbeBindAddress: probeAddr,</pre>
			<pre class="source-code">      <strong class="bold">LeaderElection:         enableLeaderElection,</strong></pre>
			<pre class="source-code">      LeaderElectionID:       "df4c7b26.example.com",</pre>
			<pre class="source-code">   })</pre>
			<pre class="source-code">   if err != nil {</pre>
			<pre class="source-code">      setupLog.Error(err, "unable to start manager")</pre>
			<pre class="source-code">      os.Exit(1)</pre>
			<pre class="source-code">   }</pre>
			<p>The <strong class="source-inline">LeaderElectionID</strong> field represents the name of the resource that the Operator will use in order to hold the leader lock.</p>
			<p>This setup (using the <strong class="source-inline">LeaderElection</strong> option in <strong class="source-inline">ctrl.NewManager()</strong>) sets up the Operator to use the first of two possible leader election strategies, known as <strong class="bold">leader-with-lease</strong>. The other possible strategy for leader election is <strong class="bold">leader-for-life</strong>. There are some benefits and trade-offs to each strategy, as described in the Operator <a id="_idIndexMarker304"/>SDK documentation (<a href="https://sdk.operatorframework.io/docs/building-operators/golang/advanced-topics/#leader-election">https://sdk.operatorframework.io/docs/building-operators/golang/advanced-topics/#leader-election</a>):</p>
			<ul>
				<li><strong class="bold">Leader-with-lease</strong> – The default leader <a id="_idIndexMarker305"/>election strategy wherein the current leader periodically attempts to renew its status as the leader (known as its <em class="italic">lease</em>). If it is<a id="_idIndexMarker306"/> unable to do so, the leader relinquishes its position to a new leader. This strategy improves availability by enabling fast transitions between leaders when necessary. However, this strategy makes it <a id="_idIndexMarker307"/>possible to end up with a so-called <strong class="bold">split-brain</strong> scenario, in which multiple replicas believe they are the leader.</li>
				<li><strong class="bold">Leader-for-life</strong> – The leader <a id="_idIndexMarker308"/>only gives up its position when it is deleted (and by extension, its lease lock resource is deleted via garbage collection). Doing so eliminates the possibility of multiple replicas competing for leader status, but it means that recovery from failure scenarios may be bound by the kube controller manager's Pod eviction timeout (which defaults to 5 minutes).<p class="callout-heading">Solving Split-Brain in Leader-with-Lease</p><p class="callout">Leader-with-lease is vulnerable to split-brain situations where two or more replicas determine inconsistent results for the current leader state. This is because the Kubernetes client that handles <a id="_idIndexMarker309"/>leader election determines the current state of the system's leader election by observing timestamps. Internally, the client looks at the elapsed time to decide whether it is time to renew the leader lease. Therefore, the clock skew rate can become an issue in some clusters. </p><p class="callout">The client library recommends a method for clock synchronization within your cluster to address this. You can also modify the <strong class="source-inline">RenewDeadline</strong> and <strong class="source-inline">LeaseDuration</strong> fields when calling <strong class="source-inline">ctrl.NewManager()</strong> to add approximate toleration for the ratio of clock skew rate between the fastest and slowest nodes. This is because <strong class="source-inline">LeaseDuration</strong> is the amount of time that current non-leaders will wait before attempting to acquire leadership, and <strong class="source-inline">RenewDeadline</strong> is the time that the control plane will wait to attempt refreshing the current leadership. Therefore, as an example, setting <strong class="source-inline">LeaseDuration</strong> to some multiple of <strong class="source-inline">RenewDeadline</strong> will add a tolerance for that same ratio of clock skew rate within the cluster.</p><p class="callout">For example, assume the current leader runs on Node A, which keeps time accurately. If the fail over backup replica is waiting on Node B, which is slow and keeps time at half the speed of Node A, then there is a clock skew rate between Nodes A and B with a ratio of 2:1. </p><p class="callout">In this scenario, if the leader has <strong class="source-inline">LeaseDuration</strong> of 1 hour and fails for some reason, then it may take 2 hours for Node B to notice that the lease has expired and attempt to acquire a new lease. </p><p class="callout">However, with <strong class="source-inline">RenewDeadline</strong> of 30 minutes, the original leader will fail to renew its lease within that time frame. This allows the replica on the slower node to act at what it believes is only 30 minutes, but the actual time has surpassed <strong class="source-inline">LeaseDuration</strong> of 1 hour. This is an obscure detail in the leader election library, but one worth noting for Operators in clusters that may be affected by this skew. There is more discussion on this topic in the original GitHub pull request to add leader election to the Kubernetes clients at <a href="https://github.com/kubernetes/kubernetes/pull/16830">https://github.com/kubernetes/kubernetes/pull/16830</a>.</p></li>
			</ul>
			<p>To make our Operator <a id="_idIndexMarker310"/>configurable so that users can decide between leader-with-lease and leader-for-life election strategies, we can leverage the existing <strong class="source-inline">--leader-elect</strong> flag to enable or disable a call to the <strong class="source-inline">leader.Become()</strong> function:</p>
			<pre class="source-code">import (</pre>
			<pre class="source-code">   "github.com/operator-framework/operator-lib/leader"</pre>
			<pre class="source-code">   ...</pre>
			<pre class="source-code">   )</pre>
			<pre class="source-code">var (</pre>
			<pre class="source-code">   ...</pre>
			<pre class="source-code">   setupLog = ctrl.Log.WithName("setup")</pre>
			<pre class="source-code">   )</pre>
			<pre class="source-code">func main() {</pre>
			<pre class="source-code">   ...</pre>
			<pre class="source-code">   var enableLeaderElection bool</pre>
			<pre class="source-code">   flag.BoolVar(&amp;enableLeaderElection, "leader-elect", false,</pre>
			<pre class="source-code">      "Enable leader election for controller manager. "+</pre>
			<pre class="source-code">         "Enabling this will ensure there is only one active controller manager.")</pre>
			<pre class="source-code">   ...</pre>
			<pre class="source-code">   if <strong class="bold">!enableLeaderElection</strong> {</pre>
			<pre class="source-code">      err := <strong class="bold">leader.Become(context.TODO(), "nginx-lock")</strong></pre>
			<pre class="source-code">      if err != nil {</pre>
			<pre class="source-code">         setupLog.Error(err, "unable to acquire leader lock")</pre>
			<pre class="source-code">         os.Exit(1)</pre>
			<pre class="source-code">      }</pre>
			<pre class="source-code">   }</pre>
			<pre class="source-code">   mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{</pre>
			<pre class="source-code">      ...</pre>
			<pre class="source-code">      LeaderElection:         enableLeaderElection,</pre>
			<pre class="source-code">      LeaderElectionID:       "df4c7b26.example.com",</pre>
			<pre class="source-code">   })</pre>
			<p>Now, our Operator will always use some method of leader election. The <strong class="source-inline">leader.Become()</strong> function is a blocking call that will prevent the Operator from running if it cannot acquire the<a id="_idIndexMarker311"/> leader lock. It will attempt to do so every second. If it succeeds, it will create the lock as a ConfigMap (in our case, that ConfigMap will be named nginx-lock).</p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor088"/>Adding health checks</h1>
			<p>Health checks (also known <a id="_idIndexMarker312"/>as liveness and readiness probes) are a way for any Pod to make its current functional<a id="_idIndexMarker313"/> state discoverable by other components in the cluster. This is usually done by way of exposing an endpoint in the container (traditionally <strong class="source-inline">/healthz</strong> for liveness checks and <strong class="source-inline">/readyz</strong> for readiness). That endpoint can be reached by other components (such as the kubelet) to determine whether the Pod is healthy and ready to serve requests. The topic is covered in detail in the Kubernetes documentation at <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/</a>.</p>
			<p>The code initialized by the Operator SDK in <strong class="source-inline">main.go</strong> contains the <strong class="source-inline">healthz</strong> and <strong class="source-inline">readyz</strong> check setup by default:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">main.go</p>
			<pre class="source-code">import (</pre>
			<pre class="source-code">  ...</pre>
			<pre class="source-code">  "sigs.k8s.io/controller-runtime/pkg/healthz"</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">func main() {</pre>
			<pre class="source-code">...</pre>
			<pre class="source-code">   mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(),   ctrl.Options{</pre>
			<pre class="source-code">   Scheme:                 scheme,</pre>
			<pre class="source-code">   MetricsBindAddress:     metricsAddr,</pre>
			<pre class="source-code">   Port:                   9443,</pre>
			<pre class="source-code">   HealthProbeBindAddress: probeAddr,</pre>
			<pre class="source-code">   LeaderElection:         enableLeaderElection,</pre>
			<pre class="source-code">   LeaderElectionID:       "df4c7b26.example.com",</pre>
			<pre class="source-code">   })</pre>
			<pre class="source-code">...</pre>
			<pre class="source-code">    if err := <strong class="bold">mgr.AddHealthzCheck("healthz", healthz.Ping)</strong>; err != nil {</pre>
			<pre class="source-code">      setupLog.Error(err, "unable to set up health check")</pre>
			<pre class="source-code">      os.Exit(1)</pre>
			<pre class="source-code">   }</pre>
			<pre class="source-code">   if err := <strong class="bold">mgr.AddReadyzCheck("readyz", healthz.Ping)</strong>; err != nil {</pre>
			<pre class="source-code">      setupLog.Error(err, "unable to set up ready check")</pre>
			<pre class="source-code">      os.Exit(1)</pre>
			<pre class="source-code">   }</pre>
			<pre class="source-code">   setupLog.Info("starting manager")</pre>
			<pre class="source-code">   if err := <strong class="bold">mgr.Start</strong>(ctrl.SetupSignalHandler()); err != nil {</pre>
			<pre class="source-code">      setupLog.Error(err, "problem running manager")</pre>
			<pre class="source-code">      os.Exit(1)</pre>
			<pre class="source-code">   }</pre>
			<pre class="source-code">}</pre>
			<p>This code sets up the two endpoints to begin serving immediately before the main controller is started. This makes sense because, at this point, all other startup code has already run. It wouldn't make <a id="_idIndexMarker314"/>sense to start advertising your Operator as healthy and ready to serve requests before it has even attempted to create a client connection.</p>
			<p>The two functions, <strong class="source-inline">mgr.AddHealthzCheck</strong> and <strong class="source-inline">mgr.AddReadyzCheck</strong>, each take two parameters: a string (for the name of the check) and a function that returns the status of the check. That function must have the following signature:</p>
			<pre class="source-code">func(*http.Request) error</pre>
			<p>This signature shows that the function accepts an HTTP request (because the check is serving an endpoint that will be queried by components such as the kubelet) and returns an error (if the check fails). The <strong class="source-inline">healthz.Ping</strong> function that the Operator is already populated with in<a id="_idIndexMarker315"/> this code is a simple <strong class="bold">no-operation</strong> (<strong class="bold">no-op</strong>) function that always returns a <strong class="source-inline">nil</strong> error (indicating success). This is not very insightful, but its location in the code does provide the minimum of reporting that the Operator has successfully passed most of the initialization process.</p>
			<p>However, following the preceding function signature, it is possible to implement custom health checks. These additional checks can be added by simply calling <strong class="source-inline">mgr.AddHealthzCheck</strong> or <strong class="source-inline">mgr.AddReadyzCheck</strong> again (depending on the type of check) and passing the new function to each call. The checks are run sequentially (in no guaranteed order) when the <strong class="source-inline">/healthz</strong> or <strong class="source-inline">/readyz</strong> endpoints are queried, and if any checks fail, then an <strong class="source-inline">HTTP 500</strong> status is returned. With this, it is possible to develop your own liveness and readiness checks with more sophisticated logic that is unique to your Operator (for example, relying on dependent components to be accessible before reporting readiness).</p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor089"/>Summary</h1>
			<p>This chapter highlighted some additional options for functionality beyond the bare minimum that was established in <a href="B18147_04_ePub.xhtml#_idTextAnchor066"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing an Operator with the Operator SDK.</em> This list is obviously not exhaustive of every possibility for advanced features, but it is intended to showcase some of the most common additional features added to Operators. At this point, some of the patterns for feature development should start to become clear (for example, startup and initialization code usually goes in <strong class="source-inline">main.go</strong>, while features related to core logic can fit nicely with the controller code in <strong class="source-inline">nginxoperator_controller.go</strong>, or its own package).</p>
			<p>The work done in this chapter shows some of the steps necessary in order to graduate an Operator from lower-level functionality to higher levels in the Capability Model. For example, metrics are a key aspect of a Level IV (<em class="italic">Deep Insights</em>) Operator and, therefore, something that is expected of the highest-function Operators available to users. In addition, leader election can help establish fail over processes (helpful in reaching Level III – <em class="italic">Full lifecycle</em>). Adding functionality like this helps build a useful and feature-rich Operator that improves application performance and, by extension, user experience.</p>
			<p>In the next chapter, we will begin compiling the code that has been built throughout <a href="B18147_04_ePub.xhtml#_idTextAnchor066"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing an Operator with the Operator SDK</em>, and <a href="B18147_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>, <em class="italic">Developing an Operator – Advanced Functionality</em>. We will then demonstrate how to deploy this code and run our nginx Operator in a local Kubernetes cluster (without the use of the OLM). This will be a useful development process, as bypassing the need for the OLM in a testing environment can simplify and expedite our ability to iterate and test new changes.</p>
		</div>
	</body></html>