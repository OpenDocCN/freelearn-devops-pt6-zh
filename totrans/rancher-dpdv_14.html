<html><head></head><body>
		<div id="_idContainer064">
			<h1 id="_idParaDest-173"><em class="italic"><a id="_idTextAnchor172"/>Chapter 10</em>: Monitoring and Logging</h1>
			<p>The previous chapters covered cluster configuration, backup, and recovery. This chapter will cover Rancher monitoring and how Rancher uses Prometheus and Grafana to collect metrics for a cluster and then appoint them. Then, we will cover Rancher logging and how Rancher uses the Banzai Cloud Logging operator and Fluent Bit to collect the logs from the Kubernetes components and collect application logs, including filtering logs.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>What is Prometheus and Grafana?</li>
				<li>Deploying Rancher's monitoring stack </li>
				<li>Adding custom application metrics to Prometheus</li>
				<li>Creating alert rules in Prometheus</li>
				<li>Creating a Grafana dashboard</li>
				<li>What is the Banzai Cloud Logging operator?</li>
				<li>What is Fluent Bit?</li>
				<li>Deploying Rancher logging</li>
				<li>Filtering application logs</li>
				<li>Writing logs to multiple log servers</li>
			</ul>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor173"/>What is Prometheus and Grafana?</h1>
			<p>In this section, we'll be covering the most popular monitoring solution for Kubernetes clusters.</p>
			<p><strong class="bold">Prometheus</strong> is an open source monitoring<a id="_idIndexMarker713"/> and alerting framework that the Kubernetes community has widely adopted. Prometheus was initially created by SoundCloud back in 2012 before it was accepted by the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>) as its second incubated project<a id="_idIndexMarker714"/> after Kubernetes. Prometheus was built from the ground up to work with Kubernetes, the core idea being that everything<a id="_idIndexMarker715"/> should be discoverable via the Kubernetes API. At this point, Prometheus<a id="_idIndexMarker716"/> will pull the metrics and store them as time-series key-value pairs.</p>
			<p>Of course, the first question that always comes up is, <em class="italic">what are metrics?</em> In the simplest terms, it's a numerical measurement of a resource. For example, it can be the current memory usage of a pod or the current number of connections to a database server. It is important to note that Prometheus doesn't support anything but an integer or floating-point number for the values. You can't set a value to something such as the words <em class="italic">up</em> or <em class="italic">down</em> for the value of a metric. For example, if you want to check whether a job was successful or failed, you might output that value as <strong class="source-inline">0</strong> for a successful status and <strong class="source-inline">1</strong> for a failed status. </p>
			<p>The other central point with metrics is they should be a point-in-time value. For example, you might want the average number of connections at a given time. So, you would define a metrics endpoint at the pod level. One of the traps for new players is to add the metric endpoint to a service record. This might be easier but is not recommended for the long term because you might want a different rule in the future. For example, you start with just finding an average number of connections over the last 5 minutes, and then you want to change that to 15 minutes. Do you change the value of the current metric, which might affect historical reporting, or do you add another metric, which then means you are collecting duplicate data? The best approach is to output the raw data as a metric and then process it inside Prometheus and Grafana.</p>
			<p>The next question that comes up is, <em class="italic">how does Prometheus get its data?</em> Because Prometheus uses a <strong class="source-inline">pull</strong> instead of a <strong class="source-inline">push</strong> model, this is done by running a web server that exports the metrics as a simple text output of key-value pairs. This is commonly called an exporter<a id="_idIndexMarker717"/> in Prometheus. These exporters can be built directly into your application, as in the case of most of the core components of Kubernetes. For example, etcd has a built-in metrics exporter that runs on a different port, <strong class="source-inline">2379</strong>. It is common to run metrics on a different port than the main application because Prometheus, by default, will try making a <strong class="source-inline">GET</strong> request without authentication. Prometheus can query an endpoint that requires authentication, but setting up the tokens or credentials requires additional work and maintenance. So, most users will avoid it and use the fact that metrics are only exposed internally to the cluster and not to the public as <em class="italic">good enough</em> security.</p>
			<p>Of course, Prometheus uses exporters to collect metrics, so the question of what exporters are available comes up. And luckily, because of the open source community, there are a significant number<a id="_idIndexMarker718"/> of third-party exporters for most standard applications. For example, almost<a id="_idIndexMarker719"/> all major open source databases have an exporter<a id="_idIndexMarker720"/> such as MySQL, CouchDB, MongoDB, MSSQL, Oracle DB, and PostgreSQL. You can find the official list at https://prometheus.io/docs/instrumenting/exporters/#databases. It's the same with standard web servers such as Apache and NGINX, with the complete list available at https://prometheus.io/docs/instrumenting/exporters/#http. And, of course, almost all Kubernetes native applications such as CoreDNS, Longhorn, Linkerd, and OPA Gatekeeper have Prometheus exporters built right into the application. </p>
			<p>For application developers, there are several libraries available for Go, Java/JVM, Python, and Node.js that allow even custom applications to add built-in support for Prometheus. Of course, if you can't find a premade exporter for your application, upstream Prometheus provides excellent resources for writing your exporter, including naming standards, example code, and different<a id="_idIndexMarker721"/> technical aspects for handling use cases. All this can be found at <a href="https://prometheus.io/docs/instrumenting/writing_exporters/">https://prometheus.io/docs/instrumenting/writing_exporters/</a>.</p>
			<p>Finally, one of the newer features added to Prometheus is alerting. Because Prometheus is already collecting your environment and applications data, it makes sense to add alerting into Prometheus using AlertManager. The core concept is that you define a set of queries that will run inside the Prometheus server that, if violated, will trigger an alert, which will be sent to Alertmanager, which will forward that alert to several external services such as email, Slack, and PagerDuty. Later in this chapter, we'll cover creating alerts in Prometheus along with some examples.</p>
			<p>The one main feature that Prometheus is missing is a way to visualize your data. This is where Grafana comes into the picture. Grafana allows you to visualize data stored in Prometheus and other data sources such as MySQL, Loki, and InfluxDB. The main idea behind Grafana is that you create a dashboard that will query a data source (Prometheus, in this case) and then use this data to develop a range of graphs, charts, gauges, and so on. It is important to note that Gradana doesn't store any data outside of caching query results. Grafana also supports exploring logs from sources such as Loki and Elasticsearch. It also has a notification system that can trigger alerts based on queries, as Prometheus does. This can be helpful for application teams to create custom alerts.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor174"/>Deploying Rancher's monitoring stack </h1>
			<p>With Rancher, there are two main versions of monitoring, v1 and v2. The original v1 monitoring that came with Rancher 2.0 to 2.4 is based on Prometheus and Grafana. But with the Rancher<a id="_idIndexMarker722"/> server and UI managing the deployment and configuration of the monitoring stack, the basic idea is to deploy Prometheus at the cluster level and additional Prometheus servers for each Rancher project. This approach was fine if you had a small number of projects that didn't need to be controlled via automation. This was mainly done because, initially, all the configurations of the Prometheus server were done by changing configmap. This required a great deal of work to manage the monitoring settings as clusters and applications grew in size and complexity. </p>
			<p>With the creation of the Prometheus operator, it all changed. The core idea is that the Prometheus<a id="_idIndexMarker723"/> operator monitors a set of <strong class="bold">Custom Resource Definitions</strong> (<strong class="bold">CRDs</strong>). This includes the description of the Prometheus server and its related services such as node-exporter and Alertmanager. It is important to note that monitoring v1 and v2 has built-in rules and dashboards but, most importantly, the configuration of probes, alerts, and other related Prometheus settings, with Prometheus operator handling, creating, and updating the configuration files used by Prometheus. </p>
			<p>In October 2020, Rancher 2.5 migrated from monitoring v1 to v2, with v2 being built on the operator model. It is important to note that both Prometheus and Grafana moved to this new model. This also included Rancher using the standard upstream Prometheus and Grafana image instead of the Rancher customized images.</p>
			<p>If you are currently<a id="_idIndexMarker724"/> running the old v1 monitoring, migrating to the new v2 monitoring is recommended. The official process can be found at <a href="https://rancher.com/docs/rancher/v2.5/en/monitoring-alerting/guides/migrating/">https://rancher.com/docs/rancher/v2.5/en/monitoring-alerting/guides/migrating/</a>, but the process can be summarized as follows:</p>
			<ol>
				<li>You need to delete all the current settings and configurations. </li>
				<li>Then, uninstall the old Prometheus server and its components. </li>
				<li>At this point, you can install v2 monitoring and reconfigure all the settings. </li>
			</ol>
			<p>You must make sure that nothing is left behind from v1 monitoring before installing v2. Luckily, one of the engineers at Rancher named Bastian Hofmann created a script that handles the process of collecting all the alerts and dashboards and migrating them over to v2 (<a href="https://github.com/bashofmann/rancher-monitoring-v1-to-v2">https://github.com/bashofmann/rancher-monitoring-v1-to-v2</a>). It is important to note that it is not an official script, and you should take an etcd backup before starting this process.</p>
			<p>For deploying monitoring v1, log into the Rancher UI, go to <strong class="bold">Tools</strong> | <strong class="bold">Monitoring</strong>, and click the <strong class="bold">Enable</strong> button. At this point, the Rancher server will take over deploying the Prometheus server<a id="_idIndexMarker725"/> and node exporters. Then, all the monitoring configuration will be done via the Rancher UI. For example, if you wanted to view the CPU usage of a pod, you would browse to the pod in the Rancher UI, and Grafana graphs will be displayed right inside the UI. For additional details about the workload metrics<a id="_idIndexMarker726"/> that can be collected, please see the official Rancher documentation at <a href="https://rancher.com/docs/rancher/v2.0-v2.4/en/cluster-admin/tools/cluster-monitoring/cluster-metrics/">https://rancher.com/docs/rancher/v2.0-v2.4/en/cluster-admin/tools/cluster-monitoring/cluster-metrics/</a>. </p>
			<p>It is important to note that cluster monitoring is only designed to be used by users that have full view access to the cluster. If you want to scope monitoring to a single project, you'll need to enable project monitoring by going to the project and selecting <strong class="bold">Monitoring</strong> from the <strong class="bold">Tools</strong> menu. This will cause the Rancher server to deploy an additional Prometheus server with its own namespace inside the project. This Prometheus server is scoped to the project and its namespace:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B18053_10_01.jpg" alt="Figure 10.1 – Rancher monitoring v1&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Rancher monitoring v1</p>
			<p>For deploying monitoring v2, you have a couple of different options. The first one is to go to <strong class="bold">Cluster explorer</strong> | <strong class="bold">Cluster Tools</strong> and click <strong class="bold">Install</strong> next to <strong class="bold">Monitoring</strong>: </p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B18053_10_02.jpg" alt="Figure 10.2 – Rancher monitoring v2&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Rancher monitoring v2</p>
			<p>This will deploy Rancher's monitoring<a id="_idIndexMarker727"/> chart via the app catalog. This chart is just a repackage of upstream images with no code changes. The only real difference is to use Rancher's Docker Hub repositories in place of upstreams. Also, the default namespace is set to <strong class="source-inline">cattle-monitoring-system</strong>, but this can be customized if you so choose. Because monitoring v2 is a Helm chart, you can choose to deploy it directly via the <strong class="source-inline">helm</strong> command, which can be very helpful when managing clusters at scale, using tools such as Rancher's fleet. The following is an example command:</p>
			<pre class="source-code">helm repo add rancher-charts https://git.rancher.io/charts</pre>
			<pre class="source-code">helm repo update</pre>
			<pre class="source-code">helm upgrade –install -n cattle-monitoring-system rancher-monitoring rancher-charts/rancher-monitoring</pre>
			<p>You can find the complete command and <strong class="source-inline">values.yaml</strong> for installing Rancher monitoring via the <strong class="source-inline">helm</strong> command at <a href="https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-monitoring-v2">https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-monitoring-v2</a>.</p>
			<p>The second option is to deploy<a id="_idIndexMarker728"/> the upstream Helm chart, commonly called <strong class="source-inline">kube-prometheus-stack</strong>: </p>
			<pre class="source-code">helm repo add prometheus-community https://prometheus-community.github.io/helm-charts</pre>
			<pre class="source-code">helm repo update</pre>
			<pre class="source-code">helm upgrade –install -n monitoring monitoring prometheus-community/kube-prometheus-stack</pre>
			<p>It is important to note that, at the time of writing, this chart is still in beta and is subject to change, and Rancher<a id="_idIndexMarker729"/> doesn't support all the versions available in the upstream<a id="_idIndexMarker730"/> charts. So, it recommends reviewing Rancher's support matrix at <a href="https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/">https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/</a>. You can also find a complete list of the configuration options by viewing the chart value at https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#configuration. </p>
			<p>At this point, you should have Prometheus and Grafana installed on your cluster. It is important to note that it can take approximately 5–10 minutes for all the pods and services that Prometheus needs to start entirely. It is also important to note that, at the time of writing, Rancher does not fully support Prometheus federation – the idea being that you can have a central Prometheus server that scans all other Prometheus servers across your other clusters. If you would like to learn<a id="_idIndexMarker731"/> more about this, I recommend looking at the official documentation at <a href="https://prometheus.io/docs/prometheus/latest/federation/">https://prometheus.io/docs/prometheus/latest/federation/</a>, but I would note that this is still a new feature and still evolving.</p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor175"/>Adding custom application metrics to Prometheus</h1>
			<p>Of course, now that you have Prometheus<a id="_idIndexMarker732"/> and Grafana all installed<a id="_idIndexMarker733"/> and working, the question becomes, <em class="italic">how do you get metrics from our applications into Prometheus?</em> In this section, we will cover two main ways of doing this.</p>
			<p>The easiest way is to use a community-created chart such as Bitnami's MariaDB chart, including the <strong class="source-inline">metrics.enabled=true</strong> option. This option enables a sidecar that adds <strong class="source-inline">mysqld-exporter</strong> to the deployment, with many community-created charts using this model of having the exporter be a sidecar container to the main container. It is important to note that you should read the documentation for the Helm chart to see if any additional steps need to be taken when enabling metrics in your chart, as some applications will require a service account or permissions to be set for the exporter to work correctly. </p>
			<p>Besides enabling the metrics, you'll also see an annotation section with the <strong class="source-inline">prometheus.io/scrape=true</strong> and <strong class="source-inline">prometheus.io/port=9000</strong> key pair. The port might be different, but it is a standard practice to set it to something in this range. These two annotations<a id="_idIndexMarker734"/> are significant, as they are what Prometheus<a id="_idIndexMarker735"/> uses when discovering all the different pods that should be scraped.</p>
			<p>But let's assume that you are using a custom-made application and want to capture metrics from this application. The following are a couple of examples of different applications where the metrics exporter is installed.</p>
			<p>With GoLang, Prometheus provides an official library located at <a href="https://github.com/prometheus/client_golang/">https://github.com/prometheus/client_golang/</a>. This library handles most of the heavy lifting when generating the metrics output. You can find an example Go application and deployment at <a href="https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/go">https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/go</a>. You need to run the <strong class="source-inline">kubectl apply -f deploy.yaml</strong> command in order to deploy the example application. If you curl the pod IP address with the path/metrics, you'll find that the application returns a list of different metrics (for example, curl <strong class="source-inline">10.42.7.23:8080/metrics</strong>). Once the application is up and running, you can send a <strong class="source-inline">GET</strong> request to <strong class="source-inline">/ping</strong>, which will return the word <strong class="source-inline">pong</strong>. Then, inside the application, it will increase a counter called <strong class="source-inline">ping_request_count</strong>, which is a custom metric that is being exposed.</p>
			<p>Here is an example of the metrics output:</p>
			<pre class="source-code"># HELP ping_request_count No of request handled by Ping handler</pre>
			<pre class="source-code"># TYPE ping_request_count counter</pre>
			<pre class="source-code">ping_request_count 4</pre>
			<pre class="source-code">….</pre>
			<pre class="source-code"># HELP promhttp_metric_handler_requests_in_flight Current number of scrapes being served.</pre>
			<pre class="source-code"># TYPE promhttp_metric_handler_requests_in_flight gauge</pre>
			<pre class="source-code">promhttp_metric_handler_requests_in_flight 1</pre>
			<pre class="source-code"># HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code.</pre>
			<pre class="source-code"># TYPE promhttp_metric_handler_requests_total counter</pre>
			<pre class="source-code">promhttp_metric_handler_requests_total{code="200"} 2</pre>
			<pre class="source-code">promhttp_metric_handler_requests_total{code="500"} 0</pre>
			<pre class="source-code">promhttp_metric_handler_requests_total{code="503"} 0</pre>
			<p>The full output can be found at <a href="https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/go/output.txt.">https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/go/output.txt.</a></p>
			<p>With Apache2, we need to take<a id="_idIndexMarker736"/> the sidecar option to add the exporter<a id="_idIndexMarker737"/> to the deployment. In our example, we are going to use a popular third-party exporter called <strong class="source-inline">apache_exporter</strong>. You can read more about this project at <a href="https://github.com/Lusitaniae/apache_exporter">https://github.com/Lusitaniae/apache_exporter</a>. The basic idea behind this project is to act as a translation layer between the Apache <strong class="source-inline">mod_status</strong> module and Prometheus. We need to install/enable the <strong class="source-inline">mod_status</strong> module to the primary web server container in the example deployment. Then, we need to expose the <strong class="source-inline">server-status</strong> page to the sidecar container that hosts the exporter. You can find the example and deployment file at <a href="https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/apache">https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/apache</a>. You need to run the <strong class="source-inline">kubectl apply -f deploy.yaml</strong> command to deploy the example application. </p>
			<p>Here is an example of the metrics output:</p>
			<pre class="source-code"># HELP apache_accesses_total Current total apache accesses (*)</pre>
			<pre class="source-code"># TYPE apache_accesses_total counter</pre>
			<pre class="source-code">apache_accesses_total 6</pre>
			<pre class="source-code"># HELP apache_connections Apache connection statuses</pre>
			<pre class="source-code"># TYPE apache_connections gauge</pre>
			<pre class="source-code">apache_connections{state="closing"} 0</pre>
			<pre class="source-code">apache_connections{state="keepalive"} 0</pre>
			<pre class="source-code">apache_connections{state="total"} 1</pre>
			<pre class="source-code">apache_connections{state="writing"} 0</pre>
			<pre class="source-code"># HELP apache_cpu_time_ms_total Apache CPU time</pre>
			<pre class="source-code"># TYPE apache_cpu_time_ms_total counter</pre>
			<pre class="source-code">apache_cpu_time_ms_total{type="system"} 30</pre>
			<pre class="source-code">apache_cpu_time_ms_total{type="user"} 0</pre>
			<pre class="source-code"># HELP apache_cpuload The current percentage CPU used by each worker and in total by all workers combined (*)</pre>
			<pre class="source-code"># TYPE apache_cpuload gauge</pre>
			<pre class="source-code">apache_cpuload</pre>
			<p>The full output can be found at <a href="https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/apache/output.txt">https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/apache/output.txt</a>.</p>
			<p>With NGINX, we will use a similar<a id="_idIndexMarker738"/> process as we did with<a id="_idIndexMarker739"/> Apache, but this time, we will use an exporter provided by NGINX. You can find the example and deployment file at <a href="https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/nginx">https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/nginx</a>. You simply need to run the <strong class="source-inline">kubectl apply -f deploy.yaml</strong> command. </p>
			<p>Here is an example of the metrics output:</p>
			<pre class="source-code"># HELP nginx_connections_accepted Accepted client connections</pre>
			<pre class="source-code"># TYPE nginx_connections_accepted counter</pre>
			<pre class="source-code">nginx_connections_accepted 1</pre>
			<pre class="source-code"># HELP nginx_connections_active Active client connections</pre>
			<pre class="source-code"># TYPE nginx_connections_active gauge</pre>
			<pre class="source-code">nginx_connections_active 1</pre>
			<pre class="source-code"># HELP nginx_connections_handled Handled client connections</pre>
			<pre class="source-code"># TYPE nginx_connections_handled counter</pre>
			<pre class="source-code">nginx_connections_handled 1</pre>
			<pre class="source-code"># HELP nginx_connections_reading Connections where NGINX is reading the request header</pre>
			<pre class="source-code"># TYPE nginx_connections_reading gauge</pre>
			<pre class="source-code">nginx_connections_reading 0</pre>
			<pre class="source-code"># HELP nginx_connections_waiting Idle client connections</pre>
			<pre class="source-code"># TYPE nginx_connections_waiting gauge</pre>
			<pre class="source-code">nginx_connections_waiting 0</pre>
			<pre class="source-code"># HELP nginx_connections_writing Connections where NGINX is writing the response back to the client</pre>
			<pre class="source-code"># TYPE nginx_connections_writing gauge</pre>
			<pre class="source-code">nginx_connections_writing 1</pre>
			<pre class="source-code"># HELP nginx_http_requests_total Total http requests</pre>
			<pre class="source-code"># TYPE nginx_http_requests_total counter</pre>
			<pre class="source-code">nginx_http_requests_total 9</pre>
			<pre class="source-code"># HELP nginx_up Status of the last metric scrape</pre>
			<pre class="source-code"># TYPE nginx_up gauge</pre>
			<pre class="source-code">nginx_up 1</pre>
			<p>The full output can be found at <a href="https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/nginx/output.txt.">https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/nginx/output.txt.</a>.</p>
			<p>At this point, we can monitor<a id="_idIndexMarker740"/> the metrics of our different<a id="_idIndexMarker741"/> applications, but we're missing the ability to create alerts based on these metrics. In the next section, we'll dive into alert rules for Prometheus. </p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor176"/>Creating alert rules in Prometheus</h1>
			<p>The Prometheus operator defines<a id="_idIndexMarker742"/> alert rules via the CRD PrometheusRule. At its core, all an alert<a id="_idIndexMarker743"/> is is an expression with a trigger. Let's look at the following example alert. This alert is from Longhorn, which we'll cover in the next chapter. As you can see, the expression is denoted by the <strong class="source-inline">expr</strong> field, which has a formula to take the actual size of the volume, divided by the capacity, and convert it to a percentage. Then, if that value is greater than 90%, the expression is <strong class="source-inline">true</strong>, which will trigger an alert. The <strong class="source-inline">description</strong> section is mainly for the end user. Still, it's important to note that you can have variables inside the description because the alert will contain the same explanation as the summary, typically used for the subject line. For example, when sending an email alert, the email's subject will be set to the subject of the alert, with the body of the email being the description.</p>
			<p>Here is an example of an alert:</p>
			<pre class="source-code">apiVersion: monitoring.coreos.com/v1</pre>
			<pre class="source-code">kind: PrometheusRule</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  labels:</pre>
			<pre class="source-code">    prometheus: longhorn</pre>
			<pre class="source-code">    role: alert-rules</pre>
			<pre class="source-code">  name: prometheus-longhorn-rules</pre>
			<pre class="source-code">  namespace: monitoring</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  groups:</pre>
			<pre class="source-code">  - name: longhorn.rules</pre>
			<pre class="source-code">    rules:</pre>
			<pre class="source-code">    - alert: LonghornVolumeActualSpaceUsedWarning</pre>
			<pre class="source-code">      annotations:</pre>
			<pre class="source-code">        description: The actual space used by Longhorn volume {{$labels.volume}} on {{$labels.node}} is at {{$value}}% capacity for</pre>
			<pre class="source-code">          more than 5 minutes.</pre>
			<pre class="source-code">        summary: The actual used space of Longhorn volume is over 90% of the capacity.</pre>
			<pre class="source-code">      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 &gt; 90</pre>
			<pre class="source-code">      for: 5m</pre>
			<pre class="source-code">      labels:</pre>
			<pre class="source-code">        issue: The actual used space of Longhorn volume {{$labels.volume}} on {{$labels.node}} is high.</pre>
			<pre class="source-code">        severity: warning</pre>
			<p>It is important to note that Prometheus will only find rules located in the same namespace as the server<a id="_idIndexMarker744"/> by default. This can cause issues, as application teams<a id="_idIndexMarker745"/> might need access to the namespace to add/edit/remove their alerts. To work around this issue, you'll need to add the following settings in your <strong class="source-inline">values.yaml</strong> file.</p>
			<p>Here is an example of <strong class="source-inline">values.yaml</strong>:</p>
			<pre class="source-code">  prometheusSpec:</pre>
			<pre class="source-code">    podMonitorNamespaceSelector: {}</pre>
			<pre class="source-code">    serviceMonitorNamespaceSelector: {}</pre>
			<pre class="source-code">    ruleNamespaceSelector: {}</pre>
			<p>At this point, we have Prometheus up and running. It's collecting all the data about our cluster, but the built-in UI doesn't give you a way to visualize this data in a useful way. In the next section, we'll be diving into Grafana to bring dashboards to our data. </p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor177"/>Creating a Grafana dashboard</h1>
			<p>At this point, we should have Prometheus and Grafana up and running, with the server collecting all the data<a id="_idIndexMarker746"/> about our cluster. Still, you can't see most of the data unless you use Rancher's monitoring charts, including some prebuilt dashboards that are mostly related to the cluster and its core services such as etcd, kube-apiserver, and CoreDNS. But, of course, a question comes up: <em class="italic">How do I create my own?</em></p>
			<p>The most straightforward answer is to find a premade dashboard and let someone else do all the hard work for you. The Grafana Labs dashboard repository <a id="_idIndexMarker747"/>is the most extensive resource, located at <a href="https://grafana.com/grafana/dashboards/">https://grafana.com/grafana/dashboards/</a>. Their search tool lets you filter results by applications, data sources, and so on. But the coolest part is their dashboard ID system. All dashboards on the official site have an ID number – for example, the NGINX Ingress controller dashboard has an ID of <strong class="source-inline">9614</strong>, and all you need to do to use this dashboard is copy that ID number and go to the Grafana UI. Browse to <strong class="bold">Dashboards</strong> | <strong class="bold">Manage</strong> | <strong class="bold">Import</strong>. Note that you might need to log into Grafana using the default login, which is <strong class="source-inline">admin/prom-operator</strong>. Then, paste the ID number in, and you're done.</p>
			<p>Of course, Rancher monitoring provides some example dashboards bundled into the rancher-monitoring chart. You can find the raw JSON files at <a href="https://github.com/rancher/system-charts/tree/dev-v2.6/charts/rancher-monitoring/v0.3.1/charts/grafana/dashboards">https://github.com/rancher/system-charts/tree/dev-v2.6/charts/rancher-monitoring/v0.3.1/charts/grafana/dashboards</a>. Additionally, you can add baseboards by ID too, with some of the most important ones being the component and etcd dashboards, which can be used to provide a great deal of insight into cluster performance issues.</p>
			<p>But let's say the application you are deploying is a community-created application that doesn't have a dashboard on the official site. Most repositories will have the dashboard defined as a JSON file. You'll copy and paste it into the UI and import it with this file. But let's say you are deploying a custom in-house application and want to start from zero. I recommend watching the <em class="italic">Getting started with Grafana dashboard design</em> video at <a href="https://grafana.com/go/webinar/guide-to-dashboard-design/">https://grafana.com/go/webinar/guide-to-dashboard-design/</a>. I use a lot of community-created dashboards<a id="_idIndexMarker748"/> and tune them to my needs. You can click the share button at the top and export a dashboard as a JSON file, at which point you can copy and paste the parts you like. It is also imperative that you save your work when making changes to your dashboard by clicking the save icon at the top-right corner. If you close the page without clicking that icon, all your changes will be lost.</p>
			<p>So far in this chapter, we have<a id="_idIndexMarker749"/> been talking about monitoring and alerting. In the next section, we will shift gears and focus on the other half of the equation, logging.</p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor178"/>What is the Banzai Cloud Logging operator?</h1>
			<p>Along with migrating to monitoring v2 in Rancher 2.5, Rancher also migrated to logging v2 for mostly the same reasons. In v1, logging is built on Fluentd and uses plugins to ship logs to different logging services<a id="_idIndexMarker750"/> such as Elasticsearch, Splunk, Kafka, and Syslog. With v1, the Rancher server was in complete control of the logging deployments, which made customizing and tuning the logging solution complicated. Most of the settings were hard coded inside Rancher. This is where Banzai's Logging operator comes into the picture. </p>
			<p>The Logging operator uses the CRD model, just like the Prometheus operator, wherein you'll define your Fluent Bit deployment and its setting via a CRD. The operator takes over pushing out your changes. Because everything is a CRD, including your settings, you can let your application teams define their logging settings. For example, one team might want their logs sent to a cloud log service such as Splunk, while another team might have the legal requirement<a id="_idIndexMarker751"/> for everything to stay running on RKE or another K8s cluster hosted on-premises, and you can do this because of the Logging operator. The idea is that you have logging flows that are a set of application pods going to an output, which can be any number of logging servers/services.</p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor179"/>What is Fluent Bit and Fluentd?</h1>
			<p>When talking about logging, two of the questions are, <em class="italic">what is Fluent Bit?</em> and <em class="italic">what is Fluentd?</em></p>
			<p>Before diving into Fluent Bit, let's talk about Fluentd, which came first. Fluentd is an open source project<a id="_idIndexMarker752"/> written in Ruby by the Treasure<a id="_idIndexMarker753"/> Data team back in 2011. Its core idea was that all logs should be JSON objects. With Fluentd and Docker, the basic process used to collect logs from the containers is to use the default Docker log driver that writes the container logs to a file on the disk. Then, Fluentd will read the whole log file and bring forward the events onto the server, at which point Fluentd will open a tail file handler that will hold the log file open and read all writes to it. </p>
			<p>It is important to note that Fluentd<a id="_idIndexMarker754"/> has a process for handling log rotation, so it is recommended to<a id="_idIndexMarker755"/> enable log rotation in Docker Engine. The following is an example<a id="_idIndexMarker756"/> configuration. You can find the complete documentation at <a href="https://docs.docker.com/config/containers/logging/configure/">https://docs.docker.com/config/containers/logging/configure/</a>. Docker will wait until the <strong class="source-inline">logs</strong> file is 100 MB before rotating the file in the following example. This is done to prevent the loss of events for applications that create large amounts of events. Fluentd needs to read all the events and forward them to the log server before the rotation.</p>
			<p>In the following example, we are defining the log options for the <strong class="source-inline">json-file</strong> log driver, which is built into Docker Engine by default. </p>
			<p>Here is an example at <strong class="source-inline">/etc/docker/daemon.json</strong>:</p>
			<pre class="source-code">{</pre>
			<pre class="source-code">  "log-driver": "json-file",</pre>
			<pre class="source-code">  "log-opts": {</pre>
			<pre class="source-code">    "max-size": "100m",</pre>
			<pre class="source-code">    "max-file": "3"</pre>
			<pre class="source-code">  }</pre>
			<pre class="source-code">}</pre>
			<p>In this case, we are allowing the log file for all containers to reach a maximum size of 100 MB before rotating the file. Then, we'll only be keeping the last three rotated files.</p>
			<p>Now, let's go over Fluent Bit. Basically, Fluentd was designed to be a simple tool that is fast and lightweight. Fluent Bit is built on top of Fluentd and is intended to provide the additional filtering<a id="_idIndexMarker757"/> and routing that Banzai<a id="_idIndexMarker758"/> logging needs. You can find more about the differences at <a href="https://docs.fluentbit.io/manual/about/fluentd-and-fluent-bit">https://docs.fluentbit.io/manual/about/fluentd-and-fluent-bit</a>. </p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor180"/>Deploying Rancher logging</h1>
			<p>With Rancher logging, it is recommended to deploy<a id="_idIndexMarker759"/> via the Apps and Marketplace in the Rancher UI by going to <strong class="bold">Cluster Tools</strong> and clicking <strong class="bold">Logging app</strong>. This will deploy the operator in the <strong class="source-inline">cattle-logging-system</strong> namespace. It is important to note that you'll see two applications, <strong class="source-inline">rancher-logging</strong> and <strong class="source-inline">rancher-logging-crd</strong>, in the <strong class="bold">Installed</strong> section after the installation is complete. Also, depending on the size of the cluster, it might take 5 to 15 minutes for all the pods to start up and go into the <em class="italic">ready</em> state. Once Rancher logging is installed on the cluster, we will be able to configure filtering and log flows, which we'll cover in the next two sections. </p>
			<p>Because logging v2 is a Helm chart, you can choose to deploy it directly via the <strong class="source-inline">helm</strong> command, which can be very helpful when managing clusters at scale, using tools such as Rancher's Fleet. The following is an example command:</p>
			<pre class="source-code">helm repo add rancher-charts https://git.rancher.io/charts</pre>
			<pre class="source-code">helm repo update</pre>
			<pre class="source-code">helm upgrade –install -n cattle-logging-system rancher-logging rancher-charts/rancher-logging</pre>
			<p>You can find the full commands and <strong class="source-inline">values.yaml</strong> to install Rancher<a id="_idIndexMarker760"/> monitoring via the <strong class="source-inline">helm</strong> command at <a href="https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-logging-v2">https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-logging-v2</a>.</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor181"/>Filtering application logs</h1>
			<p>The first setting that most people configure<a id="_idIndexMarker761"/> with Rancher logging is <strong class="bold">ClusterFlows</strong> and <strong class="bold">ClusterOutput</strong>. ClusterFlow is designed<a id="_idIndexMarker762"/> to be scoped<a id="_idIndexMarker763"/> to all namespaces and can set the default logging policy for the cluster as a whole. To configure this setting, you'll go to the Rancher UI and browse to <strong class="bold">Logging</strong> and then <strong class="bold">ClusterFlows</strong>. From there, you'll fill out the form. Then, once that is done, you'll want to define ClusterOutput, where you define the target location for your logs, ElasticSearch, Splunk, Syslog, and so on. For an example of each of the different<a id="_idIndexMarker764"/> logging providers, please see Rancher's official documentation at <a href="https://rancher.com/docs/rancher/v2.5/en/logging/custom-resource-config/outputs/">https://rancher.com/docs/rancher/v2.5/en/logging/custom-resource-config/outputs/</a>.</p>
			<p>Once you have ClusterFlows and ClusterOutput configured, you can call it done. But if you want to customize the logging<a id="_idIndexMarker765"/> for an application, you need to repeat the process. Still, this time, you'll be configuring the Flows and Outputs, with the main difference being setting the selector rules<a id="_idIndexMarker766"/> or what the documentation calls <strong class="bold">matches</strong>, which are a set of <strong class="source-inline">include</strong> or <strong class="source-inline">exclude</strong> labels that you can use to limit the scope of the Flows and Outputs. The following is an example YAML for a NGINX application in the default namespace. It is important to note that Flows are namespace-scoped:</p>
			<p>Here is a Flow example:</p>
			<pre class="source-code">apiVersion: logging.banzaicloud.io/v1beta1</pre>
			<pre class="source-code">kind: Flow</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: flow-sample</pre>
			<pre class="source-code">  namespace: default</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  filters:</pre>
			<pre class="source-code">    - parser:</pre>
			<pre class="source-code">        remove_key_name_field: true</pre>
			<pre class="source-code">        parse:</pre>
			<pre class="source-code">          type: nginx</pre>
			<pre class="source-code">    - tag_normaliser:</pre>
			<pre class="source-code">        format: ${namespace_name}.${pod_name}.${container_name}</pre>
			<pre class="source-code">  localOutputRefs:</pre>
			<pre class="source-code">    - s3-output</pre>
			<pre class="source-code">  match:</pre>
			<pre class="source-code">    - select:</pre>
			<pre class="source-code">        labels:</pre>
			<pre class="source-code">          app: nginx</pre>
			<p>At this point, we should<a id="_idIndexMarker767"/> have logging configured on our cluster and be forwarding the logs to a log server. In the next section, we'll cover a more advanced setup that a number of users use in their environments to log to multiple servers. </p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor182"/>Writing logs to multiple log servers</h1>
			<p>Because you can define<a id="_idIndexMarker768"/> as many outputs as you would like, suppose you wanted<a id="_idIndexMarker769"/> to send logs to multiple log servers, such as sending to a local Syslog and Splunk server. It is imperative to note that this will duplicate your logs, so this is not recommended<a id="_idIndexMarker770"/> as an <strong class="bold">High Availability</strong> (<strong class="bold">HA</strong>) solution – that is, sending to a production log server or a <strong class="bold">Disaster Recovery</strong> (<strong class="bold">DR</strong>) log server. It's also vital that you do not dispatch<a id="_idIndexMarker771"/> logs to an offline server. Fluent Bit does cache <strong class="source-inline">failed to send</strong> logs, so you can run into memory pressure issues if you have a misconfigured log server or the log server is offline for long periods.</p>
			<p>You can find YAML examples in Rancher's official documentation, located at <a href="https://rancher.com/docs/rancher/v2.6/en/logging/custom-resource-config/outputs/">https://rancher.com/docs/rancher/v2.6/en/logging/custom-resource-config/outputs/</a>. It's important<a id="_idIndexMarker772"/> to note that, as of Rancher 2.6.3, the logging settings in the Rancher UI are still buggy (for example, <a href="https://github.com/rancher/rancher/issues/36516">https://github.com/rancher/rancher/issues/36516</a>, where the <strong class="bold">ClusterOutput</strong> field is failing to update in the UI), so it's recommended to use YAML files as much as possible.</p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor183"/>Summary</h1>
			<p>In this chapter, we learned about Rancher monitoring and logging. This includes how Prometheus, Grafana, Fluentd, and Fluent Bit work. We learned how to install Rancher monitoring and logging. We finally went into detail about some example dashboards. We ended the chapter by talking about customizing application logging and its flows. </p>
			<p>The next chapter will cover Rancher's storage project to provide storage to Kubernetes clusters.</p>
		</div>
	</body></html>