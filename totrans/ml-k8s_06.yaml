- en: '*Chapter 7*: Model Deployment and Automation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you saw how the platform enables you to build and register
    the model in an autonomous fashion. In this chapter, we will extend the **machine
    learning** (**ML**) engineering domain to model deployment, monitoring, and automation
    of deployment activities.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn how the platform provides the model packaging and deployment
    capabilities and how you can automate them. You will take the model from the registry,
    package it as a container, and deploy the model onto the platform to be consumed
    as an API. You will then automate all these steps using the workflow engine provided
    by the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Once your model is deployed, it works well for the data it was trained upon.
    The real world, however, changes. You will see how the platform allows you to
    observe your model's performance. This chapter discusses the tools and techniques
    to monitor your model performance. The performance data could be used to decide
    whether the model needs retraining on the new dataset, or whether it is time to
    build a new model for the given problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model inferencing with Seldon Core
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packaging, running, and monitoring a model using Seldon Core
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Apache Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating ML model deployments in Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter includes some hands-on setup and exercises. You will need a running
    Kubernetes cluster configured with **Operator Lifecycle Manager**. Building such
    a Kubernetes environment is covered in [*Chapter 3*](B18332_03_ePub.xhtml#_idTextAnchor040),
    *Exploring Kubernetes*. Before attempting the technical exercises in this chapter,
    please make sure that you have a working Kubernetes cluster and **Open Data Hub**
    (**ODH**) is installed on your Kubernetes cluster. Installing ODH is covered in
    [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The Anatomy of a Machine
    Learning Platform*.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model inferencing with Seldon Core
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you built the model. These models are built by data
    science teams to be used in production and serve the prediction requests. There
    are many ways to use a model in production, such as embedding the model with your
    customer-facing program, but the most common way is to expose the model as a REST
    API. The REST API can then be used by any application. In general, running and
    serving a model in production is called **model serving**.
  prefs: []
  type: TYPE_NORMAL
- en: However, once the model is in production, it needs to be monitored for performance
    and needs updating to meet the expected criteria. A hosted model solution enables
    you to not only serve the model but monitor its performance and generate alerts
    that can be used to trigger retraining of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Seldon is a UK-based firm that created a set of tools to manage the model''s
    life cycle. Seldon Core is an open source framework that helps expose ML models
    to be consumed as REST APIs. Seldon Core automatically exposes the monitoring
    statistics for the REST API, which can be consumed by **Prometheus**, the platform''s
    monitoring component. To expose your model as a REST API in the platform, the
    following steps are required:'
  prefs: []
  type: TYPE_NORMAL
- en: Write a language-specific wrapper for your model to expose as a service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Containerize your model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define and deploy the model using the inference graph of your model using Seldon
    Deployment **custom resource** (**CR**) in Kubernetes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will see these three steps in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping the model using Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's see how you can apply the preceding steps. In [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086),
    *Machine Learning Engineering*, you registered your experiment details and a model
    with the MLflow server. Recall that the model file was stored in the artifacts
    of MLflow and named `model.pkl`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the model file and write a simple Python wrapper around it. The
    job of the wrapper is to use Seldon libraries to conveniently expose the model
    as a REST service. You can find the example of the wrapper in the code at `chapter7/model_deploy_pipeline/model_build_push/Predictor.py`.
    The key component of this wrapper is a function named `predict` that will be invoked
    from an HTTP endpoint created by the Seldon framework. *Figure 7.1* shows a simple
    Python wrapper using a `joblib` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – A Python language wrapper for the model prediction'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – A Python language wrapper for the model prediction
  prefs: []
  type: TYPE_NORMAL
- en: The `predict` function receives a `numpy` array (`data_array`) and a set of
    column names (`column_names`), serialized from the HTTP request. The method returns
    the result of the prediction as either a `numpy` array or a list of values or
    bytes. There are many more methods available for the language wrapper and a full
    list is available at [https://docs.seldon.io/projects/seldon-core/en/v1.12.0/python/python_component.html#low-level-methods](https://docs.seldon.io/projects/seldon-core/en/v1.12.0/python/python_component.html#low-level-methods).
    Note that in later chapters of this book, you will see a more thorough inferencing
    example that will have additional wrappers for data transformation before prediction.
    But, for this chapter, we keep it as simple as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The language wrapper is ready, and the next stage is to containerize the model
    and the language wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: Containerizing the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What would you put in the container? Let's start with a list. You will need
    the model and the wrapper files. You will need the Seldon Python packages available
    in the container. Once you have all these packages, then you will use the Seldon
    services to expose the model. *Figure 7.2* shows a `Docker` file that is building
    one such container. This file is available in `Chapter 7/model_deployment_pipeline/model_build_push/Dockerfile.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Docker file to package the model as a container'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – Docker file to package the model as a container
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s understand the content of the Docker file:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Line 1* indicates the base container image for your model service. We have
    chosen the freely available image from Red Hat, but you can choose as per your
    convenience. This image could be your organization''s base image with the standard
    version of Python and related software.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Line 3*, we have created a `microservice` directory to place all the related
    artifacts in our container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Line 4*, the first file we need to build the container is `base_requirements.txt`.
    This file contains the packages and dependencies for the Seldon Core system. You
    can find this file at `chapter7/model_deployment_pipeline/model_build_push/base_requirements.txt`.
    In this file, you will see that Seldon Core packages and `joblib` packages have
    been added.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 7.3* shows the `base_requirements.txt` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – File adding Seldon and Joblib to the container'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – File adding Seldon and Joblib to the container
  prefs: []
  type: TYPE_NORMAL
- en: '*Line 5* is using the `base_requirements.txt` file to install the Python packages
    onto the container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Lines 7* and *8*, when you are training the model, you may use different
    packages. During inferencing, some of the packages may be needed; for example,
    if you have done input data scaling before model training using a library, you
    may need the same library to apply the scaling at inference time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086), *Machine Learning Engineering*,
    you registered your experiment details and a model with the MLflow server. Recall
    that the model file was stored in the artifacts along with a file containing packages
    used to train the model named `requirements.txt`. Using the `requirements.txt`
    file generated by MLflow, you can install the packages required to run your model,
    or you may choose to add these dependencies on your own to a custom file. *Figure
    7.4* shows the MLflow snapshot referred to in [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086),
    *Machine Learning Engineering*. You can see the `requirements.txt` file here next
    to the `model.pkl` file.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – MLflow run artifacts'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.4 – MLflow run artifacts
  prefs: []
  type: TYPE_NORMAL
- en: '*Line 10*: You add the language wrapper files and the model files to the container.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Line 11*: Here, you are using the `seldon-core-microservice` server to start
    the inferencing server. Notice that the parameters have been passed here, and
    in the next section, you will see how we can pass these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MODEL_NAME**: This is the name of the Python class in the language wrapper
    containing the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MODEL`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GRPC_PORT**: The port at which the **Google Remote Procedure Call** (**gRPC**)
    endpoint will listen for model inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**METRICS_PORT**: The port at which the service performance data will be exposed.
    Note that this is the performance data for the service and not the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HTTP_NAME**: The HTTP port where will you serve the model over HTTP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we have a container specification in the form of the Docker file. Next,
    we will see how to deploy the container on the Kubernetes platform using the Seldon
    controller.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the model using the Seldon controller
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our ML platform provides a Seldon controller, a piece of software that runs
    as a pod and assists in deploying the containers you built in the preceding section.
    Note that the controller in our platform is the extension of the existing Seldon
    operator. At the time of writing, the Seldon operator was not compatible with
    Kubernetes version 1.22, so we have extended the existing operator to work with
    the latest and future versions of the Kubernetes platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The Anatomy
    of a Machine Learning Platform*, where you learned about installing ODH and how
    it works on the Kubernetes cluster. In an equivalent manner, the Seldon controller
    is also installed by the ODH operator. The `manifests/ml-platform.yaml` file has
    the configuration for installing the Seldon controller. *Figure 7.5* shows the
    settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – MLFlow section of the manifest file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.5 – MLFlow section of the manifest file
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s verify whether the Seldon controller is running correctly in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Seldon controller pod'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 – Seldon controller pod
  prefs: []
  type: TYPE_NORMAL
- en: 'The Seldon controller pod is installed by the ODH operators, which watch for
    the Seldon Deployment CR. This schema for this resource is defined by the Seldon
    Deployment `manifests/odhseldon/cluster/base/seldon-operator-crd-seldondeployments.yaml`.
    Once you create the Seldon Deployment CR, the controller deploys the pods associated
    with the CR. *Figure 7.7* shows this relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Components of the platform for deploying Seldon services'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.7 – Components of the platform for deploying Seldon services
  prefs: []
  type: TYPE_NORMAL
- en: Let's see the different components of the Seldon Deployment CR. You can find
    one simple example in `chapter7/manual_model_deployment/SeldonDeploy.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Seldon Deployment CR contains all the information that is required by the
    Seldon controller to deploy your model on the Kubernetes cluster. There are three
    main sections in the Seldon Deployment CR:'
  prefs: []
  type: TYPE_NORMAL
- en: '`apiVersion`, `kind`, and other Kubernetes-related information. You will define
    the labels and name of the Seldon Deployment as any other Kubernetes object. You
    can see in the following screenshot that it contains the labels and annotations
    for the object:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Seldon Deployment – Kubernetes-related information'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.8 – Seldon Deployment – Kubernetes-related information
  prefs: []
  type: TYPE_NORMAL
- en: '`chapter7/manual_model_deployment/SeldonDeploy.yaml` file that has this information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that `containers` take an array for the `image` object, so you can add
    more images to it. The `image` key will have the location of your container. The
    `env` array defines the environment variables that will be available for the pod.
    Recall that, in our Docker file in the previous section, these variables have
    been used. `MODEL_NAME` has a value of `Predictor`, which is the name of the class
    you have used as a wrapper. `SERVICE_TYPE` has a value of `MODEL`, which mentions
    the type of service this container provides.
  prefs: []
  type: TYPE_NORMAL
- en: The last part has `hpaSpec`, which the Seldon controller will translate onto
    the `maxReplicas` is set to `1`, so there will not be any new pods, but you can
    control this value for each deployment. The scalability will kick in if the CPU
    utilization goes beyond 80% for the pods in the following example; however, because
    `maxReplica` is `1`, there will not be any new pods created.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Seldon Deployment – Seldon service containers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.9 – Seldon Deployment – Seldon service containers
  prefs: []
  type: TYPE_NORMAL
- en: '`graph` key builds the inference graph for your service. An inference graph
    will have different nodes and you will define what container will be used at each
    node. You will see there is a `children` key that takes an array of objects through
    which you define your inference graph. For this example, `graph` has only one
    node and the `children` key has no information associated with it; however, in
    the later chapters, you will see how to build the inference graph with more nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining fields under the graph define the first node of your inference
    graph. The `name` field has the value that corresponds to the name you have given
    in the `containers` section. Note that this is the key through which Seldon knows
    what container would be serving at this node of your inference graph.
  prefs: []
  type: TYPE_NORMAL
- en: The other important part is the `logger` section. Seldon can automatically forward
    the request and response to the URL mentioned under the `logger` section. The
    capability of forwarding the request and response can be used for a variety of
    scenarios, such as storing the payload for audit/legal reasons or applying data
    drift algorithms to trigger retraining or anything else. Note that Seldon can
    also forward to Kafka if needed, but this is outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Seldon Deployment – inference graph'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.10 – Seldon Deployment – inference graph
  prefs: []
  type: TYPE_NORMAL
- en: Once you create the Seldon Deployment CR using the routine `kubectl` command,
    the Seldon controller will deploy the pods, and the model will be available for
    consumption as a service.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll move on to packaging and deploying the basic model that you built
    in [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086), *Machine Learning Engineering*.
  prefs: []
  type: TYPE_NORMAL
- en: Packaging, running, and monitoring a model using Seldon Core
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will package and build the container from the model file
    you built in [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086), *Machine Learning
    Engineering*. You will then use the Seldon Deployment to deploy and access the
    model. Later in this book, you will automate the process, but to do it manually,
    as you'll do in this section, we will further strengthen your understanding of
    the components and how they work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you start this exercise, please make sure that you have created an account
    with a public Docker registry. We will use the free `quay.io` as our registry,
    but you are free to use your preferred one:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first verify that MLflow and Minio (our S3 server) are running in our
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – MLflow and Minio are running on the platform'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.11 – MLflow and Minio are running on the platform
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the ingress list for MLflow, and log in to MLflow using the `mlflow` URL
    available from the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – ingress in your Kubernetes cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.12 – ingress in your Kubernetes cluster
  prefs: []
  type: TYPE_NORMAL
- en: Once you are in the MLflow UI, navigate to the experiment that you recorded
    in [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086), *Machine Learning Engineering*.
    The name of the experiment is **HelloMIFlow**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.13 – MlFlow Experiment Tracking'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.13 – MlFlow Experiment Tracking
  prefs: []
  type: TYPE_NORMAL
- en: Select the first run from the right-hand panel to get to the detail page of
    the run. From the `model.pkl` and you will see a little download arrow icon to
    the right. Use the icon to download the `requirements.txt` files from this screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.14 – MLflow experiment tracking – run details'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.14 – MLflow experiment tracking – run details
  prefs: []
  type: TYPE_NORMAL
- en: Go to the folder where you have cloned the code repository that comes with this
    book. If you have not done so, please clone the [https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git](https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git)
    repository on your local machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, go to the `chapter7/model_deploy_pipeline/model_build_push` folder and
    copy the two files downloaded in the previous step to this folder. In the end,
    this folder will have the following files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Sample files to package the model as a container'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.15 – Sample files to package the model as a container
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The last two files are the ones that you have just copied. All other files are
    coming from the code repository that you have cloned.
  prefs: []
  type: TYPE_NORMAL
- en: Curious people will note that the `requirements.txt` file that you have downloaded
    from the MLFlow server contains the packages required while you run the notebook
    for model training. Not all of these packages (`mlflow`, for example) will be
    needed to execute the saved model. To keep things simple, we will add all of them
    to our container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s build the container on the local machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Packaging the model as a container'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.16 – Packaging the model as a container
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to tag the container and push it to the repository of your
    choice. Before you push your image to a repository, you will need to have an account
    with an image registry. If you do not have one, you can create one at [https://hub.docker.com](https://hub.docker.com)
    or [https://quay.io](https://quay.io). Once you have created your registry, you
    can run the following commands to tag and push the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response. You will notice that, in the following
    screenshot, we refer to `quay.io/ml-on-k8s` as our registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Pushing the model to a public repository'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_017.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.17 – Pushing the model to a public repository
  prefs: []
  type: TYPE_NORMAL
- en: Now that your container is available in a registry, you will need to use the
    Seldon Deployment CR to deploy it as a service. Open the `chapter7/manual_model_deployment/SeldonDeploy.yaml`
    file and adjust the location of the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can see the file after I have modified *line 16* (as per my image location)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Seldon Deployment CR with the image location'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_018.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.18 – Seldon Deployment CR with the image location
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s deploy the model as a service by deploying the `chapter7/manual_model_deployment/SeldonDeploy.yaml`
    file. Run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Creating the Seldon Deployment CR'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_019.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.19 – Creating the Seldon Deployment CR
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that the container is in a running state. Run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will note that the name that you have put in the `graph` section of the
    `SeldonDeploy.yaml` file (`model-test-predictor`) is part of the container name.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – Validating the pod after the Seldon Deployment CR'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_020.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.20 – Validating the pod after the Seldon Deployment CR
  prefs: []
  type: TYPE_NORMAL
- en: 'Great! You have a model running as a service. Now, let''s see what is in the
    pod created for us by the Seldon controller. Run the following command to get
    a list of containers inside our pod:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Containers inside the Seldon pod'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_021.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.21 – Containers inside the Seldon pod
  prefs: []
  type: TYPE_NORMAL
- en: You will see that there are two containers. One is `model-test-predictor`, which
    is the image that we have built, and the second container is `seldon-container-engine`,
    which is the Seldon server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `model-test-predictor` container has the model and is using the language
    wrapper to expose the model over HTTP and gRPC. You can use the following command
    to see the logs and what ports have been exposed from `model-test-predictor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response (among other logs):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.22 – Containers log showing the ports'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_022.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.22 – Containers log showing the ports
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that the servers are ready to take the calls on `9000` for HTTP
    and on `6005` for the metrics server. This metrics server will have the Prometheus-based
    monitoring data exposed on the `/prometheus` endpoint. You can see this in the
    following portion of the log:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.23 – Containers log showing the Prometheus endpoint'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_023.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.23 – Containers log showing the Prometheus endpoint
  prefs: []
  type: TYPE_NORMAL
- en: The second container is `seldon-container-engine`, which does the orchestration
    for the inference graph and forwards the payloads to the service configured by
    you in the `logger` section of the Seldon Deployment CR.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this step, you will find out what Kubernetes objects your Seldon Deployment
    CR has created for you. A simple way to find out is by running the command as
    follows. This command depends on the Seldon controller labeling the objects it
    creates with the label key as `seldon-deployment-id`, and the value is the name
    of your Seldon Deployment CR, which is `model-test`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.24 – Kubernetes objects created by the Seldon controller'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_024.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.24 – Kubernetes objects created by the Seldon controller
  prefs: []
  type: TYPE_NORMAL
- en: You can see that there are Deployment objects, services, and **Horizontal Pod
    Autoscalers** (**HPA**) objects created for you for the Seldon controller using
    the configuration that you have provided in the Seldon Deployment CR. The deployment
    ends up creating pods and a replica set for your pods. The Seldon controller made
    it easy to deploy our model on the Kubernetes platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have noticed that there is no ingress object created by the Seldon
    Deployment CR. Let''s create the ingress object so that we can call our model
    from outside the cluster by running the command as follows. The ingress object
    is created by the file in `chapter7/manual_model_deployment/Ingress.yaml`. Make
    sure to adjust the `host` value as per your configuration, as you have done in
    earlier chapters. You will also notice that the ingress is forwarding traffic
    to port `8000`. Seldon provides the listener to this port, which orchestrates
    the inference call. This service is available in the container named `seldon-container-engine`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.25 – Creating ingress objects for our service'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_025.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.25 – Creating ingress objects for our service
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that the ingress has been created by issuing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.26 – Validating the ingress for our service'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_026.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.26 – Validating the ingress for our service
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our Seldon Deployment CR has referenced a logger URL, you will deploy
    a simple HTTP echo server that will just print the calls it received. This will
    assist us in validating whether the payloads have been forwarded to the configured
    URL in the `logger` section of the Seldon Deployment CR. A very simple echo server
    can be created via the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.27 – Creating a simple HTTP echo server to validate payload logging'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_027.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.27 – Creating a simple HTTP echo server to validate payload logging
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that the pod has been created by issuing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.28 – Validating a simple HTTP echo server'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_028.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.28 – Validating a simple HTTP echo server
  prefs: []
  type: TYPE_NORMAL
- en: Let's make a call for our model to predict something. The model we developed
    in the previous chapter is not very useful, but it will help us understand and
    validate the overall process of packaging and deploying the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recall from [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086), *Machine Learning
    Engineering*, that the `hellomlflow` notebook has the input for the model with
    shape `(4,2)`, and the output shape is `(4,)`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.29 – Input and output for the model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_029.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.29 – Input and output for the model
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if we want to send data to our model, it would be an array of integer pairs
    such as [`2,1`]. When you make a call to your model, the input data is required
    within an `ndarray` field under a key named `data`. The input would look as follows.
    This is the format the Seldon service expects for the data to be sent to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.30 – Input for the model as an HTTP payload'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_030.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.30 – Input for the model as an HTTP payload
  prefs: []
  type: TYPE_NORMAL
- en: 'Next is the REST endpoint for the model. It will be the ingress that you created
    in *Step 13* and the standard Seldon URL. The final form would be as follows:
    http://<INGRESS_LOCATION>/api/v1.0/predictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This would translate, in my case, to [http://model-test.192.168.61.72.nip.io/api/v1.0/predictions](http://model-test.192.168.61.72.nip.io/api/v1.0/predictions).
  prefs: []
  type: TYPE_NORMAL
- en: Now, you have the payload and the URL to send this request to.
  prefs: []
  type: TYPE_NORMAL
- en: In this step, you will make a call to your model. We are using a commonly used
    command-line option to make this call; however, you may choose to use other software,
    such as Postman, to make this HTTP call.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will use the `POST` HTTP verb in the call and then provide the location
    of the service. You will have to pass the `Content-Type` header to mention JSON
    content and the body is passed using the `data-raw` flag of the curl program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The final request should look as follows. Before making this call, make sure
    to change the URL as per your ingress location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response. Note that the output of the command
    shows the array of the same shape as per our model, which is `(4,)`, and it is
    under the `ndarray` key in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.31 – Output payload for the model inference call'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_031.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.31 – Output payload for the model inference call
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s verify that the model payload has been logged onto our echo server.
    You are validating the capability of Seldon to capture input and output and send
    it to the desired location for further processing, such as drift detection or
    audit logging:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see there is a separate record for the input and the output payload.
    You can use the `ce-requestid` key to correlate the two records in the logs. The
    following screenshot displays the main fields of the captured input payload of
    the inference call:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.32 – Captured input payload forwarded to the echo pod'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_032.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.32 – Captured input payload forwarded to the echo pod
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot displays the main fields of the output payload of
    the inference call:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.33 – Captured output payload forwarded to the echo pod'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_033.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.33 – Captured output payload forwarded to the echo pod
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's verify that service monitoring data is captured by the Seldon engine
    and is available for us to use and record. Note that the way Prometheus works
    is by scraping repetitively, so this data is in the current state and the Prometheus
    server is responsible for calling this URL and record in its database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The URL format for this information is as follows. The ingress is the same
    as you created in *Step 13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This would translate to the following for my ingress:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Open a browser and access the URL in it. You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.34 – Accessing monitoring data in Prometheus format'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_034.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.34 – Accessing monitoring data in Prometheus format
  prefs: []
  type: TYPE_NORMAL
- en: You will find that a lot of information is captured, including response times,
    the number of HTTP responses per status code (`200`, `400`, `500`, and so on),
    data capture, server performance, and exposing the Go runtime metrics. We encourage
    you to go through these parameters to develop an understanding of the data available.
    In the later chapters, you will see how to harvest and plot this data to visualize
    the performance of the model inferencing server.
  prefs: []
  type: TYPE_NORMAL
- en: You have done a great deal in this exercise. The aim of this section was to
    showcase the steps and components involved to deploy a model using Seldon Core.
    In the next section, you will be introduced to the workflow component of the platform,
    Airflow, and in the next couple of chapters, all of these steps will be automated
    using the components in the ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Apache Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Airflow is an open source software designed for programmatically authoring,
    executing, scheduling, and monitoring workflows. A workflow is a sequence of tasks
    that can include data pipelines, ML workflows, deployment pipelines, and even
    infrastructure tasks. It was developed by Airbnb as a workflow management system
    and was later open sourced as a project in Apache Software Foundation's incubation
    program.
  prefs: []
  type: TYPE_NORMAL
- en: While most workflow engines use XML to define workflows, Airflow uses Python
    as the core language for defining workflows. The tasks within the workflow are
    also written in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow has many features, but we will cover only the fundamental bits of Airflow
    in this book. This section is by no means a detailed guide for Airflow. Our focus
    is to introduce you to the software components for the ML platform. Let's start
    with DAG.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding DAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A workflow can be simply defined as a sequence of **tasks**. In Airflow, the
    sequence of tasks follows a data structure called a **directed acyclic graph**
    (**DAG**). If you remember your computer science data structures, a DAG is composed
    of nodes and one-way vertices organized in a way to ensure that there are no cycles
    or loops. Hence, a workflow in Airflow is called a DAG.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.35* shows a typical example of a data pipeline workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.35 – Typical data pipeline workflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_035.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.35 – Typical data pipeline workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'The example workflow in *Figure 7.36* is composed of tasks represented by boxes.
    The order of execution of these tasks is determined by the direction of the arrows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.36 – Example workflow with parallel execution'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_036.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.36 – Example workflow with parallel execution
  prefs: []
  type: TYPE_NORMAL
- en: Another example of a workflow is shown in *Figure 7.36*. In this example, there
    are tasks that are executed in parallel. The **Generate Report** tasks will wait
    for both **Transform Data** tasks to complete. This is called **execution dependency**
    and it is one of the problems Airflow is solving. Tasks can only execute if the
    upstream tasks are completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can configure the workflow however you want as long as there are no cycles
    in the graph, as shown in *Figure 7.37*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.37 – Example workflow with cycle'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_037.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.37 – Example workflow with cycle
  prefs: []
  type: TYPE_NORMAL
- en: In the example in *Figure 7.37*, the **Clean Data** task will never be executed
    because it is dependent on the **Store Data** task, which will also not be executed.
    Airflow only allows acyclic graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated, a DAG is a series of tasks, and there are three common types
    of tasks in Airflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Operators**: Predefined tasks that you can use to execute something, They
    can be strung together to form a pipeline or a workflow. Your DAG is composed
    mostly, if not entirely, of operators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensors**: Subtypes of operators that are used for a series of other operators
    based on an external event.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@task`. This allows you to run regular Python functions as tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Airflow operators are extendable, which means there are quite a lot of predefined
    operators created by the community that you can simply use. One of the operators
    that you will mostly use in the following exercises is the **Notebook Operator**.
    This operator allows you to run any Jupyter notebook as tasks in the DAG.
  prefs: []
  type: TYPE_NORMAL
- en: So, what are the advantages of using DAGs to execute a sequence of tasks? Isn't
    it enough to just write a script that can execute other scripts sequentially?
    Well, the answer lies in the features that Airflow offers, which we will explore
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Airflow features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The advantages that Airflow brings when compared with `cron` jobs and scripts
    can be detailed by its features. Let''s start by looking at some of those features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Failure and error management**: In the event of a task failure, Airflow handles
    errors and failures gracefully. Tasks can be configured to automatically retry
    when they fail. You can also configure how many times it retries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of execution sequence, there are two types of task dependencies in
    a typical workflow that can be managed in Airflow much easier than writing a script.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data dependencies**: Some tasks may require that the other tasks be processed
    first because they require data that is generated by other tasks. This can be
    managed in Airflow. Moreover, Airflow allows the passing of small amounts of metadata
    from the output of one task as an input to another task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Execution dependencies**: You may be able to script execution dependencies
    in a small workflow. However, imagine scripting a workflow in Bash with a hundred
    tasks, where some tasks can run concurrently while others can only run sequentially.
    I imagine this to be a pretty daunting task. Airflow helps simplify this by creating
    DAGs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Airflow can horizontally scale to multiple machines or containers.
    The tasks in the workflow may be executed on different nodes while being orchestrated
    centrally by a common scheduler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`git` repository containing your DAGs. This allows you to implement the continuous
    integration of DAGs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step is to understand the different components of Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Airflow components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Airflow comprises multiple components running as independent services. *Figure
    7.38* shows the components of Airflow and their interactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.38 – Airflow components'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_038.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.38 – Airflow components
  prefs: []
  type: TYPE_NORMAL
- en: There are three core services in Airflow. The **Airflow Web** serves the user
    interface where users can visually monitor and interact with DAGs and tasks. The
    **Airflow Scheduler** is a service responsible for scheduling tasks for the Airflow
    Worker. Scheduling does not only mean executing tasks according to their scheduled
    time. It's also about executing the tasks in a particular sequence, taking into
    account the execution dependencies and failure management. **Airflow Worker**
    is the service that executes the tasks. This is also the main scalability point
    of Airflow. The more Airflow Worker is running, the more tasks can be executed
    concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: The DAG repository is a directory in the filesystem where DAG files written
    in Python are stored and retrieved by the scheduler. The Airflow instance configured
    in our platform includes a sidecar container that synchronizes the DAG repository
    with a remote `git` repository. This simplifies the deployment of DAGs by simply
    pushing a Python file to Git.
  prefs: []
  type: TYPE_NORMAL
- en: We will not dig too deep into Airflow in this book. The objective is for you
    to learn enough to a point where you are able to create pipelines in Airflow with
    minimal Python coding. You will use the Elyra notebooks pipeline builder feature
    to build Airflow pipelines graphically. If you want to learn more about Airflow
    and how to build pipelines programmatically in Python, we recommend that you start
    with Apache Airflow's very rich documentation at [https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html](https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html).
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a basic understanding of Airflow, it's time to take a look
    at it in action. In [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The
    Anatomy of a Machine Learning Platform*, you installed a fresh instance of ODH.
    This process also installed the Airflow services for you. Now, let's validate
    this installation.
  prefs: []
  type: TYPE_NORMAL
- en: Validating the Airflow installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To validate that Airflow is running correctly in your cluster, you need to
    perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check whether all the Airflow pods are running by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the three Airflow services pods in running status, as shown
    in the following screenshot in *Figure 7.39*. Verify that all pods are in the
    `Running` state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.39 – Airflow pods in the Running state'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_039.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.39 – Airflow pods in the Running state
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the URL of Airflow Web by looking at the ingress host of `ap-airflow2`.
    You can do this by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see results similar to *Figure 7.39*. Take note of the host value
    of the `ap-airflow2` ingress. The IP address may be different in your environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.40 – Airflow ingress in the ml-workshop namespace'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_040.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.40 – Airflow ingress in the ml-workshop namespace
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to [https://airflow.192.168.49.2.nip.io](https://airflow.192.168.49.2.nip.io).
    Note that the domain name is the host value of the `ap-airflow2` ingress. You
    should see the Airflow Web UI, as shown in *Figure 7.41*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.41 – Home screen of Apache Airflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_041.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.41 – Home screen of Apache Airflow
  prefs: []
  type: TYPE_NORMAL
- en: If you are able to load the Airflow landing page, it means that the Airflow
    installation is valid. You must have also noticed that in the table listing the
    DAGs, there are already existing DAGs currently in failing status. These are existing
    DAG files that are in [https://github.com/airflow-dags/dags/](https://github.com/airflow-dags/dags/),
    the default configured DAG repository. You will need to create your own DAG repository
    for your experiments. The next section will provide the details on how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the Airflow DAG repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A DAG repository is a Git repository where Airflow picks up the DAG files that
    represent your pipelines or workflows. To configure Airflow to point to your own
    DAG repository, you need to create a Git repository and point the Airflow Scheduler
    and Airflow Web to this Git repository. You will use **GitHub** to create this
    repository. The following steps will guide you through the process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a GitHub repository by going to [https://github.com](https://github.com).
    This requires that you have an existing account with GitHub. For the purpose of
    this exercise, let''s call this repository `airflow-dags`. Take note of the URL
    of your new Git repository. It should look like this: [https://github.com/your-user-name/airflow-dags.git](https://github.com/your-user-name/airflow-dags.git).
    We assume that you already know how to create a new repository on GitHub.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Edit your instance of ODH by editing the `kfdef` (**Kubeflow definition**)
    object. You can do this by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should be presented with a `vim` editor showing the `kfdef` manifest file
    as shown in *Figure 7.42*. Press *i* to start editing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.42 – vim editor showing the section defining the Airflow instance'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_042.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.42 – vim editor showing the section defining the Airflow instance
  prefs: []
  type: TYPE_NORMAL
- en: Replace the value of the `DAG_REPO` parameter with the URL of the Git repository
    you created in *Step 1*. The edited file should look like the screenshot in *Figure
    7.43*. Press *Esc*, then *:*, and type `wq` and press *Enter* to save the changes
    you made to the `kfdef` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.43 – Value of the DAG_REPO parameter after editing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_043.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.43 – Value of the DAG_REPO parameter after editing
  prefs: []
  type: TYPE_NORMAL
- en: The changes will be picked up by the ODH operator and will be applied to the
    affected Kubernetes deployment objects, in this case, Airflow Web and Airflow
    Scheduler deployments. This process will take a couple of minutes to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate the changes by inspecting the Airflow deployments. You can do this
    by running the following command to look into the applied manifest of the deployment
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should return a line containing the URL of your GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because this repository is new and is empty, you should not see any DAG files
    when you open the Airflow Web UI. To validate the Airflow web application, navigate
    to your Airflow URL, or refresh your existing browser tab, and you should see
    an empty Airflow DAG list similar to the screenshot in *Figure 7.44*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.44 – Empty Airflow DAG list'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_044.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.44 – Empty Airflow DAG list
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have validated your Airflow installation and updated the DAG repository
    to your own `git` repository, it's time to put Airflow to good use.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Airflow runtime images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Airflow pipelines, or DAGs, can be authored by writing Python files using the
    Airflow libraries. However, it is also possible to create DAGs graphically from
    an Elyra notebook. In this section, you will create an Airflow DAG from Elyra,
    push it to the DAG repository, and execute it in Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'To further validate the Airflow setup and test the configuration, you will
    need to run a simple `Hello world` pipeline. Follow the steps to create a two-task
    pipeline. You will create Python files, a pipeline, and configure runtime images
    to be used throughout the process:'
  prefs: []
  type: TYPE_NORMAL
- en: If you do not have a running notebook environment, start a notebook environment
    by navigating to JupyterHub, clicking **Start My Server**, and selecting a notebook
    image to run, as shown in *Figure 7.45*. Let's use **Base Elyra Notebook Image**
    this time as we do not require any special libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.45 – JupyterHub landing page showing Base Elyra Notebook Image selected'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_045.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.45 – JupyterHub landing page showing Base Elyra Notebook Image selected
  prefs: []
  type: TYPE_NORMAL
- en: In your Elyra browser, navigate to the `Machine-Learning-on-Kubernetes/chapter7/model_deploy_pipeline/`
    directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open a new pipeline editor. You can do this by selecting the menu item `untitled.pipeline`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.46 – Elyra notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_046.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.46 – Elyra notebook
  prefs: []
  type: TYPE_NORMAL
- en: Right-click on the `untitled.pipeline` file and rename it to `hello_world.pipeline`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create two Python files with the same contents containing the following line:
    `print(''Hello airflow!'')`. You can do this by selecting the menu items `hello.py`
    and `world.py`. Your directory structure should look like the screenshot in *Figure
    7.47*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.47 – Elyra directory structure showing the hello.pipeline file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_047.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.47 – Elyra directory structure showing the hello.pipeline file
  prefs: []
  type: TYPE_NORMAL
- en: Create a pipeline with two tasks by dragging the `hello.py` file into the pipeline
    editor window. Do the same for `world.py`. Connect the tasks by dragging the tiny
    circle on the right of the task box to another box. The resulting pipeline topology
    should look like the illustration in *Figure 7.48*. Save the pipeline by clicking
    the **Save** icon in the top toolbar.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.48 – Task topology'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_048.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.48 – Task topology
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can run this pipeline, we need to configure each of the tasks. Because
    each task will run as a container in Kubernetes, we need to tell which container
    image that task will use. Select the **Runtime Images** icon on the toolbar on
    the left. Then, click the **+** button to add a new runtime image, as shown in
    *Figure 7.49*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.49 – Adding a new runtime image in Elyra'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_049.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.49 – Adding a new runtime image in Elyra
  prefs: []
  type: TYPE_NORMAL
- en: In the **Add new Runtime Image** dialog, add the details of the **Kaniko Container
    Builder** image, as shown in *Figure 7.50*, and hit the **SAVE & CLOSE** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This container image ([https://quay.io/repository/ml-on-k8s/kaniko-container-builder](https://quay.io/repository/ml-on-k8s/kaniko-container-builder))
    contains the tools required to build Docker files and push images to an image
    registry from within Kubernetes. This image can also pull ML models and metadata
    from the MLflow model registry. You will use this image to build containers that
    host your ML model in the next section. This container image was created for the
    purpose of this book. You can use any container image as a runtime image for your
    pipeline tasks as long as the image can run on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.50 – Add new Runtime Image dialog for Kaniko builder'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_050.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.50 – Add new Runtime Image dialog for Kaniko builder
  prefs: []
  type: TYPE_NORMAL
- en: 'Add another runtime image called **Airflow Python Runner**. The container image
    is located at [https://quay.io/repository/ml-on-k8s/airflow-python-runner](https://quay.io/repository/ml-on-k8s/airflow-python-runner).
    This image can run any Python 3.8 scripts, and interact with Kubernetes and Spark
    operators. You will use this image to deploy container images to Kubernetes in
    the next section. Refer to *Figure 7.51* for the **Add new Runtime Image** dialog
    field values, and then hit the **SAVE & CLOSE** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.51 – Add new Runtime Image dialog for Airflow Python Runner'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_051.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.51 – Add new Runtime Image dialog for Airflow Python Runner
  prefs: []
  type: TYPE_NORMAL
- en: Pull the images from the remote repository to the local Docker daemon of your
    Kubernetes cluster. This will help speed up the start up times of tasks in Airflow
    by using a runtime image that is already pulled into the local Docker instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can do this by running the following command on the same machine where
    your Minikube is running. This command allows you to connect your Docker client
    to the Docker daemon inside your Minikube **virtual machine** (**VM**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Pull the **Kaniko Container Builder** image by running the following command
    in the same machine where your Minikube is running. This will pull the image from
    [quay.io](http://quay.io) to the Docker daemon inside your Minikube:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pull the **Airflow Python Runner** image by running the following command in
    the same machine where your Minikube is running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Assign `hello.py` task. You can do this by right-clicking the task box and selecting
    the **Properties** context menu item. The properties of the task will be displayed
    in the right pane of the pipeline editor, as shown in *Figure 7.52*. Using the
    **Runtime Image** drop-down box, select **Kaniko Container Builder**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.52 – Setting the runtime image of a task in the pipeline editor'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_052.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.52 – Setting the runtime image of a task in the pipeline editor
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you do not see the newly added runtime images in the drop-down list, you
    need to close and reopen the pipeline editor. This will refresh the list of runtime
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assign the `world.py` task. This is similar to *Step 10*, but for the `world.py`
    task. Refer to *Figure 7.53* for the **Runtime Image** value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.53 – Setting the runtime image of a task in the pipeline editor'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_053.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.53 – Setting the runtime image of a task in the pipeline editor
  prefs: []
  type: TYPE_NORMAL
- en: 'You have just created an Airflow pipeline that has two tasks, where each task
    uses a different runtime. But, before we can run this pipeline in Airflow, we
    need to tell Elyra where Airflow is. To do this, select the **Runtimes** icon
    on the left toolbar of Elyra, as shown in *Figure 7.54*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.54 – Runtimes toolbar'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_054.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.54 – Runtimes toolbar
  prefs: []
  type: TYPE_NORMAL
- en: Hit the `ml-workshop`. This is the namespace of all your ML platform workloads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`github-username/airflow-dags` format. Replace `github-username` with your
    GitHub username.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`minio`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`minio123`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once all the fields are filled correctly, hit the **SAVE & CLOSE** button.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.55 – Adding a new Apache Airflow runtime configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_055.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.55 – Adding a new Apache Airflow runtime configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the pipeline in Airflow by clicking the **Play** button in the top toolbar
    of the pipeline editor. This will bring up a **Run pipeline** dialog. Select **Apache
    Airflow runtime** as the runtime platform and **MyAirflow** as the runtime configuration,
    and then hit **OK**. Refer to *Figure 7.56*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.56 – Run pipeline dialog'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_056.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.56 – Run pipeline dialog
  prefs: []
  type: TYPE_NORMAL
- en: This action generates an Airflow DAG file and pushes the file to the GitHub
    repository configured as a DAG repository. You can verify this by checking your
    GitHub repository for newly pushed files.
  prefs: []
  type: TYPE_NORMAL
- en: Open the Airflow website. You should see the newly create DAG, as shown in *Figure
    7.57*. If you do not see it, refresh the Airflow page a few times. Sometimes,
    it takes a few seconds before the DAGs appear in the UI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.57 – Airflow showing a running DAG'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_057.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.57 – Airflow showing a running DAG
  prefs: []
  type: TYPE_NORMAL
- en: The DAG should succeed in a few minutes. If it does fail, you need to review
    the steps to make sure you set the correct values and that you did not miss any
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: You have just created a basic Airflow DAG using Elyra's graphical pipeline editor.
    The generated DAG is, by default, configured to only run once, indicated by the
    `@once` annotation. In the real world, you may not want to run your DAGs directly
    from Elyra. You may want to add additional customizations to the DAG file. In
    this case, instead of running the DAG by clicking the play button, use the export
    feature. This will export the pipeline into a DAG file that you can further customize,
    such as setting the schedule. You can then push the customized DAG file to the
    DAG repository to submit it to Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: You have just validated your Airflow setup, added Airflow runtime configuration,
    and integrated Elyra with Airflow. Now it is time to build a real deployment pipeline!
  prefs: []
  type: TYPE_NORMAL
- en: Automating ML model deployments in Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have seen in the preceding sections how to manually package an ML model
    into a running HTTP service on Kubernetes. You have also seen how to create and
    run basic pipelines in Airflow. In this section, you will put this new knowledge
    together by creating an Airflow DAG to automate the model deployment process.
    You will create a simple Airflow pipeline for packaging and deploying an ML model
    from the MLflow model registry to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the pipeline by using the pipeline editor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to the previous section, you will use Elyra''s pipeline editor to create
    the model build and deployment DAG:'
  prefs: []
  type: TYPE_NORMAL
- en: If you do not have a running Elyra environment, start a notebook environment
    by navigating to JupyterHub, clicking **Start My Server**, and selecting a notebook
    image to run, as shown in *Figure 7.45*. Let's use **Base Elyra Notebook Image**
    because this time, we do not require any special libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In your Elyra browser, navigate to the `Machine-Learning-on-Kubernetes/chapter7/model_deploy_pipeline/`
    directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open a new pipeline editor. You can do this by selecting the menu item `untitled.pipeline`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Right-click on the `untitled.pipeline` file and rename it `model_deploy.pipeline`.
    Your directory structure should look like the screenshot in *Figure 7.58*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.58 – Elyra showing empty pipeline editor'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_058.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.58 – Elyra showing empty pipeline editor
  prefs: []
  type: TYPE_NORMAL
- en: 'You will build a pipeline with two tasks in it. The first task will pull the
    model artifacts from the MLflow model registry, package the model as a container
    using Seldon core, and then push the container image to an image repository. To
    create the first task, drag and drop the `build_push_image.py` file from the `model_build_push`
    directory to the pipeline editor''s workspace. This action will create a new task
    in the pipeline editor window, as shown in *Figure 7.59*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.59 – Elyra pipeline editor showing the build_push_image task'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_059.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.59 – Elyra pipeline editor showing the build_push_image task
  prefs: []
  type: TYPE_NORMAL
- en: 'The second task will pull the container image from the image repository and
    deploy it to Kubernetes. Create the second task by dragging the `deploy_model.py`
    file from `model_deploy directory` and dropping it into the pipeline editor workspace.
    This action will create a second task in the pipeline editor, as shown in *Figure
    7.60*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.60 – Elyra pipeline editor showing the deploy_model task'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_060.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.60 – Elyra pipeline editor showing the deploy_model task
  prefs: []
  type: TYPE_NORMAL
- en: Connect the two tasks by dragging the tiny circle at the right-hand side of
    the `build_push_image.py` task to the `deploy_model.py` task box. The task topology
    should look like the illustration in *Figure 7.61*. Take note of the direction
    of the arrow highlighted in the red box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.61 – Task topology of the DAG'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_061.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.61 – Task topology of the DAG
  prefs: []
  type: TYPE_NORMAL
- en: Configure the `build_push_image.py` task by right-clicking the box and selecting
    **Properties**. A property panel will appear on the right side of the editor,
    as shown in *Figure 7.62*. Select **Kaniko Container Builder** as the runtime
    image for this task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.62 – Pipeline editor with the property panel displayed showing the
    Kaniko Builder runtime'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_062.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.62 – Pipeline editor with the property panel displayed showing the
    Kaniko Builder runtime
  prefs: []
  type: TYPE_NORMAL
- en: Add file dependencies to `build_push_image.py` by clicking the `Dockerfile`
    – This is the Docker file that will be built to produce the container image that
    contains the ML model and the Predictor Python file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Predictor.py` – This is the Python file used by Seldon to define the inference
    graph. You have seen this file in the preceding section.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Base_requirements.txt` – This is a regular text file that contains a list
    of Python packages required to run this model. This is used by the `pip install`
    command inside the Docker file.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, you should have an idea of what the entire pipeline does. Because
    the pipeline needs to push a container image to a registry, you will need a container
    registry to hold your ML model containers. Create a new repository in a container
    registry of your choice. For the exercises in this book, we will use `mlflowdemo`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once you have the image repository created, set the `build_push_image.py` task,
    as shown in *Figure 7.63*. The following are the six variables you need to set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`MODEL_NAME` is the name of the ML model registered in MLflow. You used the
    name `mlflowdemo` in the previous sections. Set the value of this variable to
    `mlflowdemo`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MODEL_VERSION` is the version number of the ML model registered in MLflow.
    Set the value of this variable to `1`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTAINER_REGISTRY` is the container registry API endpoint. For Docker Hub,
    this is available at [https://index.docker.io/v1](https://index.docker.io/v1).
    Set the value of this variable to `https://index.docker.io/v1/`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTAINER_REGISTRY_USER` is the username of the user who will push images
    to the image registry. Set this to your Docker Hub username.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTAINER_REGISTRY_PASSWORD` is the password of your Docker Hub user. In production,
    you do not want to do this. You may use secret management tools to serve your
    Docker Hub password. However, to keep things simple for this exercise, you will
    put your Docker Hub password as an environment variable.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTAINER_DETAILS` is the name of the repository where the image will be pushed,
    along with the name and tag of the image. This includes the Docker Hub username
    in the `your-username/mlflowdemo:latestv` format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Save the changes by clicking the **Save** icon from the top toolbar of the
    pipeline editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.63 – Example environment variables of the build_push_image.py task'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_063.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.63 – Example environment variables of the build_push_image.py task
  prefs: []
  type: TYPE_NORMAL
- en: 'Configure the `deploy_model.py` task by setting the runtime image, the file
    dependencies, and the environment variables, as shown in *Figure 7.64*. There
    are four environment variables you need to set, as detailed in the following list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`MODEL_NAME` is the name of the ML model registered in MLflow. You used the
    name `mlflowdemo` in the previous sections. Set the value of this variable to
    `mlflowdemo`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`MODEL_VERSION` is the version number of the ML model registered in MLflow.
    Set the value of this variable to `1`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`CONTAINER_DETAILS` is the name of the repository to where the image will be
    pushed and the image name and tag. This includes the Docker Hub username in the
    `your-username/mlflowdemo:latest` format.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`CLUSTER_DOMAIN_NAME` is the DNS name of your Kubernetes cluster, in this case,
    the IP address of Minikube, which is `<Minikube IP>.nip.io`. For example, if the
    response of the `minikube ip` command is `192.168.49.2`, then the cluster domain
    name is `192.168.49.2.nip.io`. This is used to configure the ingress of the ML
    model HTTP service so that it is accessible outside the Kubernetes cluster.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the changes by clicking the **Save** icon from the top toolbar of the pipeline
    editor.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.64 – Properties of the deploy_model.py task'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_064.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.64 – Properties of the deploy_model.py task
  prefs: []
  type: TYPE_NORMAL
- en: You are now ready to run the pipeline. Hit the **Play** button from the top
    toolbar of the pipeline editor. This will bring up the **Run** pipeline dialog,
    as shown in *Figure 7.65*. Select **Apache Airflow runtime** under **Runtime Platform**,
    and **MyAirflow** under **Runtime Configuration**. Click the **OK** button. This
    will generate the Airflow DAG Python file and push it to the Git repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.65 – Run pipeline dialog'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_065.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.65 – Run pipeline dialog
  prefs: []
  type: TYPE_NORMAL
- en: Once the DAG is successfully generated and pushed to the `git` repository, you
    should see a dialog as shown in *Figure 7.66*. Click **OK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.66 – DAG submission confirmation dialog'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_066.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.66 – DAG submission confirmation dialog
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to Airflow's GUI. You should see a new DAG, labeled **model_deploy-some-number**,
    appear in the DAGs table, and it should start running shortly, as shown in *Figure
    7.67*. The mint green color of the job indicates that it is currently running.
    Dark green indicates that it is successful.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you do not see the new DAG, refresh the page until you see it. It may take
    a few seconds for the Airflow to sync with the Git repository.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.67 – Airflow GUI showing the model_deploy DAG'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_067.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.67 – Airflow GUI showing the model_deploy DAG
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, you can explore the DAG by clicking the DAG name and selecting the
    **Graph View** tab. It should display the topology of tasks as you designed it
    in Elyra's pipeline editor, as shown in *Figure 7.68*. You may explore the DAG
    further by selecting the **<> Code** tab. This will display the generated source
    code of the DAG.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.68 – Graph view of the model_deploy DAG in Airflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_068.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.68 – Graph view of the model_deploy DAG in Airflow
  prefs: []
  type: TYPE_NORMAL
- en: 'After a few minutes, the job should succeed and you should see the outline
    of all the tasks in **Graph View** turn to dark green. You can also explore the
    tasks by looking at the pods in Kubernetes. Run the following command and you
    should see two pods with the **Completed** status, as shown in *Figure 7.69*.
    These pods are the two tasks in the pipeline that have been executed successfully:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.69 – Kubernetes pods with a Completed status'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_07_069.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.69 – Kubernetes pods with a Completed status
  prefs: []
  type: TYPE_NORMAL
- en: You have just created a complete ML model build and deployment pipeline using
    Seldon Core, Elyra's pipeline editor, orchestrated by Airflow, and deployed to
    Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Seldon Core and Airflow are big tools that have a lot more features that we
    have not covered and will not be entirely covered in this book. We have given
    you the essential knowledge and skills to start exploring these tools further
    as part of your ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You made it this far!
  prefs: []
  type: TYPE_NORMAL
- en: As of this point, you have already seen and used JupyterHub, Elyra, Apache Spark,
    MLflow, Apache Airflow, Seldon Core, and Kubernetes. You have learned how these
    tools can solve the problems that MLOps is trying to solve. And, you have seen
    all these tools running well on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot more things that we want to show you on the platform. However,
    we can only write so much, as the features of each of those tools that you have
    seen are enough to fill an entire book.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a step back to look at the big picture of
    what has been built so far. Then, you will start using the platform end-to-end
    on an example use case. You will be wearing different hats, such as data scientist,
    ML engineer, data engineer, and a DevOps person in the succeeding chapters.
  prefs: []
  type: TYPE_NORMAL
