- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: External DNS and Global Load Balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will build on what you learned in *Chapter 4*. We will discuss
    some of the limitations of certain load balancer features and how we can configure
    a cluster to resolve those limitations.
  prefs: []
  type: TYPE_NORMAL
- en: We know that Kubernetes has a built-in DNS server that dynamically allocates
    names to resources. These are used for applications to communicate intra-cluster,
    or within the cluster. While this feature is beneficial for internal cluster communication,
    it doesn’t provide DNS resolution for external workloads. Since it does provide
    DNS resolution, why do we say it has limitations?
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we used a dynamically assigned IP address to test our
    `LoadBalancer` service workloads. While our examples have been good for learning,
    in an enterprise, nobody wants to access a workload running on a cluster using
    an IP address. To address this limitation, the Kubernetes SIG has developed a
    project called **ExternalDNS**, which provides the ability to dynamically create
    DNS entries for our `LoadBalancer` services.
  prefs: []
  type: TYPE_NORMAL
- en: Also, in an enterprise, you will commonly run services that run on multiple
    clusters to provide failover for your applications. So far, the options we have
    discussed can’t address failover scenarios. In this chapter, we will explain how
    to implement a solution to provide an automated failover for workloads, making
    them highly available across multiple clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to external DNS resolution and global load balancing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring and deploying ExternalDNS in a Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating DNS name registration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating ExternalDNS with an enterprise DNS server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using GSLB to offer global load balancing across multiple clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s jump into the chapter!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter has the following technical requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: An Ubuntu 22.04+ server running Docker with a minimum of 4 GB of RAM, though
    8 GB is recommended
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A KinD cluster running **MetalLB** – if you completed *Chapter 4*, you should
    already have a cluster running MetalLB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The scripts from the `chapter5` folder from the repository, which you can access
    by going to this book’s GitHub repository: [https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making service names available externally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned in the introduction, you may remember that we used IP addresses
    to test the `LoadBalancer` services we created, whereas for our `Ingress` examples,
    we used domain names. Why did we have to use IP addresses instead of a hostname
    for our `LoadBalancer` services?
  prefs: []
  type: TYPE_NORMAL
- en: Although a Kubernetes load balancer assigns a standard IP address to a service,
    it doesn’t automatically generate a DNS name for workloads to access the service.
    Instead, you must rely on IP addresses to connect to applications within the cluster,
    which becomes confusing and inefficient. Furthermore, manually registering DNS
    names for each IP assigned by **MetalLB** presents a maintenance challenge. To
    deliver a more cloud-like experience and streamline name resolution for `LoadBalancer`
    services, we need an add-on that can address these limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the team that maintains KinD, there is a Kubernetes SIG that is working
    on this feature for Kubernetes called **ExternalDNS**. The main project page can
    be found on the SIG’s GitHub at [https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns).
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, the `ExternalDNS` project supports 34 compatible DNS
    services, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS Cloud Map
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon’s Route 53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure DNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloudflare
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreDNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud DNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pi-hole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RFC2136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are multiple options on how to extend CoreDNS to resolve external names,
    depending on what main DNS server you may be running. Many of the supported DNS
    servers will simply register any services dynamically. ExternalDNS will see the
    created resource and use native calls to register services automatically, like
    **Amazon’s Route 53**. Not all DNS servers natively allow for this type of dynamic
    registration by default.
  prefs: []
  type: TYPE_NORMAL
- en: In these instances, you need to manually configure your main DNS server to forward
    the desired domain requests to a CoreDNS instance running in your cluster. This
    is what we will use for the examples in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Our Kubernetes cluster currently utilizes CoreDNS to handle cluster DNS name
    resolution. However, what might be lesser known is that CoreDNS offers more than
    just internal cluster DNS resolution. It can extend its capabilities to perform
    external name resolution, effectively resolving names for any DNS zone managed
    by a CoreDNS deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move on to how ExternalDNS installs.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up ExternalDNS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Right now, our CoreDNS is only resolving names for internal cluster names, so
    we need to set up a zone for our new `LoadBalancer` DNS entries.
  prefs: []
  type: TYPE_NORMAL
- en: For our example, a company, **FooWidgets**, wants all Kubernetes services to
    go into the `foowidgets.k8s` domain, so we will use that as our new zone.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating ExternalDNS and CoreDNS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**ExternalDNS** is not an actual DNS server; instead, it is a controller that
    will watch for objects that request a new DNS entry. Once a request is seen by
    the controller, it will send the information to an actual DNS server, like CoreDNS,
    for registration.'
  prefs: []
  type: TYPE_NORMAL
- en: The process of how a service is registered is shown in the diagram below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: ExternalDNS registration flow'
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we are using CoreDNS as our DNS server; however, as we mentioned
    previously, ExternalDNS has support for 34 different DNS services and the list
    of supported services is constantly growing. Since we will be using CoreDNS as
    our DNS server, we will need to add a component that will store the DNS records.
    To accomplish this, we need to deploy an ETCD server in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: For our example deployment, we will use the ETCD Helm chart.
  prefs: []
  type: TYPE_NORMAL
- en: Helm is a tool for Kubernetes that makes it easier to deploy and manage applications.
    It uses Helm charts, which are templates that contain the necessary configuration
    and resource values for the application. It automates the setup of complex applications,
    ensuring they are consistently and reliably configured. It’s a powerful tool,
    and you will find that many projects and vendors offer their applications, by
    default, using Helm charts. You can read more about Helm on their main home page
    at [https://v3.helm.sh/](https://v3.helm.sh/).
  prefs: []
  type: TYPE_NORMAL
- en: One reason Helm is such a powerful tool is its ability to use custom options
    that can be declared when you run the `helm instal`l command. The same options
    can also be declared in a file that is passed to the installation using the `-f`
    option. These options make deploying complex systems easier and reproducible since
    the same values file can be used on any deployment.
  prefs: []
  type: TYPE_NORMAL
- en: For our deployment example, we have included a `values.yaml` file, located in
    the `chapter5/etcd` directory, that we will use to configure our ETCD deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Now, finally, let’s deploy ETCD! We have included a script called `deploy-etcd.sh`
    in the `chapter5/etcd` directory that will deploy ETCD with a single replica in
    a new namespace called `etcd-dns`. Execute the script while in the `chapter5/etcd`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to Helm, the script only has two commands– it will create a new namespace,
    and then execute a `Helm install` command to deploy our ETCD instance. In the
    real world, you would want to change the replica count to at least 3 to have a
    highly available ETCD deployment, but we wanted to limit resource requirements
    for our KinD server.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our ETCD for DNS, we can move on to integrating our CoreDNS
    service with our new ETCD deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Adding an ETCD zone to CoreDNS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we showed in the diagram in the last section, CoreDNS will store the DNS
    records in an ETCD instance. This requires us to configure a CoreDNS server with
    the DNS zone(s) we want to register names in and the ETCD server that will store
    the records.
  prefs: []
  type: TYPE_NORMAL
- en: To keep resource requirements lower, we will use the included CoreDNS server
    that most Kubernetes installations include as part of their base cluster creation
    for our new domain. In the real world, you should deploy a dedicated CoreDNS server
    to handle just ExternalDNS registrations.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this section, you will execute a script to deploy a fully configured
    ExternalDNS service that has all of the options and configurations discussed in
    this section. The commands used in this section are only for reference; you do
    not need to execute them on your cluster, since the script will do that for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can integrate CoreDNS, we need to know the IP address of our new
    ETCD service. You can retrieve the address by listing the services in the `etcd-dns`
    namespace using `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will show our ETCD service, along with the IP address of the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `Cluster-IP` you see in the service list will be used to configure the new
    DNS zone as a location to store the DNS records.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you deploy ExternalDNS, you can configure CoreDNS in one of two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Add a zone to the Kubernetes-integrated CoreDNS service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy a new CoreDNS service that will be used for ExternalDNS registrations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For ease of testing, we will add a zone to the Kubernetes CoreDNS service. This
    requires us to edit the CoreDNS `ConfigMap` found in the `kube-sytem` namespace.
    When you execute the script at the end of this section, the modification will
    be done for you. It will add the section shown below in **bold** to the `ConfigMap`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The added lines configure a zone called `foowidgets.k8s` that is ETCD integrated.
    The first line that we added tells CoreDNS that the zone name, `foowidgets.com`,
    is integrated with an ETCD service.
  prefs: []
  type: TYPE_NORMAL
- en: The next line, `stubzone`, tells CoreDNS to allow you to set up a DNS server
    as a “stub resolver” for a particular zone. As a stub resolver, this DNS server
    directly queries specific name servers for a zone’s information without the need
    for recursive resolution throughout the entire DNS hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: The third addition is the `path /skydns` option, which may look confusing since
    it doesn’t mention CoreDNS. Even though the value is `skydns`, it is also the
    default path for CoreDNS integration as well.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the last line tells CoreDNS where to store the records. In our example,
    we have an ETCD service running, using IP address `10.96.149.223` on the default
    ETCD port of `2379`.
  prefs: []
  type: TYPE_NORMAL
- en: You could use the Service’s host name instead of the IP here. We used the IP
    to show the relationships between a pod and a Service, but the name `etcd-dns.etcd-dns.svc`
    would work as well. Which method you choose will depend on your situation. In
    our KinD cluster, we don’t really need to worry about losing the IP because the
    cluster is disposable. In the real world, you would want to use the host name
    to protect against the IP address changing.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand how to add an ETCD integrated zone in CoreDNS, the next
    step is to update the deployment options that ExternalDNS requires to integrate
    with CoreDNS.
  prefs: []
  type: TYPE_NORMAL
- en: ExternalDNS configuration options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ExternalDNS can be configured to register ingress or service objects. This is
    configured in the deployment file of ExternalDNS using the source field. The example
    below shows the options portion of the deployment that we will use in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to configure the provider that ExternalDNS will use, and since
    we are using CoreDNS, we set the provider to `coredns`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last option is the log level we want to set, which we set to `info` to
    keep our log files smaller and easier to read. The arguments that we will use
    are shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have gone over the ETCD options and deployment, how to configure
    a new zone to use ETCD, and the options to configure ExternalDNS to use CoreDNS
    as a provider, we can deploy ExternalDNS in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We have included a script called `deploy-externaldns.sh` in the `chapter5/externaldns`
    folder. Execute the script in the directory to deploy ExternalDNS into your KinD
    cluster. When you execute the script, it will fully configure and deploy an integrated
    ExternalDNS with ETCD.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you see a warning when the script updates the `ConfigMap`, you can safely
    ignore it. Since our `kubectl` command is using `apply` to update the object,
    Kubernetes will look for a last-applied-configuration annotation, if there is
    one set. Since you likely do not have that in the existing object, you will see
    the warning that it’s missing. This is just a warning and will not stop the `ConfigMap`
    update, as you can confirm by looking at the last line of the `kubectl` update
    command, where it shows the `ConfigMap` was updated: `configmap/coredns configured`'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have added the ability for developers to create dynamically registered
    DNS names for their services. Next, let’s see it in action by creating a new service
    that will register itself in our CoreDNS server.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a LoadBalancer service with ExternalDNS integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our ExternalDNS will watch all services for an annotation that contains the
    desired DNS name. This is just a single annotation using the format `annotation
    external-dns.alpha.kubernetes.io/hostname` with the value of the DNS name you
    want to register. For our example, we want to register a name of `nginx.foowidgets.k8s`,
    so we would add an annotation to our NGINX service: `external-dns.alpha.kubernetes.io/hostname:
    nginx.foowidgets.k8s`.'
  prefs: []
  type: TYPE_NORMAL
- en: In the `chapter5/externaldns` directory, we have included a manifest that will
    deploy an NGINX web server using a `LoadBalancer` service that contains the annotation
    to register the DNS name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy the manifest using `kubectl create -f nginx-lb.yaml`, which will deploy
    the resources in the default namespace. The deployment is a standard NGINX deployment,
    but the service has the required annotation to tell the ExternalDNS service that
    you want to register a new DNS name. The manifest for the service is shown below,
    with the annotation in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When ExternalDNS sees the annotation, it will register the requested name in
    the zone. The hostname from the annotation will log an entry in the ExternalDNS
    pod – the registration for our new entry, `nginx.foowidgets.k8s`, is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the last line of the log, the record was added as an A-record
    in the DNS server, pointing to the IP address `172.18.200.101`.
  prefs: []
  type: TYPE_NORMAL
- en: The last step for confirming that ExternalDNS is fully working is to test a
    connection to the application. Since we are using a KinD cluster, we must test
    this from a pod in the cluster. To provide the new names to external resources,
    we would need to configure our main DNS server(s) to forward requests for the
    `foowidgets.k8s` domain to our CoreDNS server. At the end of this section, we
    will show the steps to integrate a Windows DNS server, which could be any main
    DNS server on your network, with our Kubernetes CoreDNS server.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can test the NGINX deployment using the DNS name from our annotation.
    Since you aren’t using the CoreDNS server as your main DNS provider, we need to
    use a container in the cluster to test name resolution. There is a great utility
    called **Netshoot** that contains a number of useful troubleshooting tools; it’s
    a great tool to have in your toolbox to test and troubleshoot clusters and pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run a Netshoot container, we can use the `kubectl run` command. We only
    need the pod to run when we are using it to run tests in the cluster, so we will
    tell the `kubectl run` command to run an interactive shell and to remove the pod
    after we exit. To run Netshoot, execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The pod may take a minute or two to become available, but once it starts up,
    you will see a `tmp-shell` prompt. At this prompt, we can use `nslookup` to verify
    that the DNS entry was successfully added. If you attempt to look up `nginx.foowidgets.k8s`,
    you should receive a reply with the IP address of the service.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Your reply should look similar to the example below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This confirms that our annotation was successful and ExternalDNS registered
    our hostname in the CoreDNS server.
  prefs: []
  type: TYPE_NORMAL
- en: The `nslookup` only proves that there is an entry for `nginx.foowidgets.k8s`;
    it doesn’t test the application. To prove that we have a successful deployment
    that will work when someone enters the name in a browser, we can use the `curl`
    utility that is included with Netshoot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: curl test using the ExternalDNS name'
  prefs: []
  type: TYPE_NORMAL
- en: The curl output confirms that we can use the dynamically created service name
    to access the NGINX web server.
  prefs: []
  type: TYPE_NORMAL
- en: We realize that some of these tests aren’t very exciting since you can’t test
    them using a standard browser. To allow CoreDNS to be used outside of the cluster,
    we need to integrate CoreDNS with your main DNS server, which needs to delegate
    ownership for the zone(s) that are in CoreDNS. When you delegate a zone, any request
    to your main DNS server for a host in the delegated zone will forward the request
    to the DNS server that contains the requested zone.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will integrate the CoreDNS running in our cluster with
    a Windows DNS server. While we are using Windows as our DNS server, the concepts
    for delegating a zone are similar between operating systems and DNS servers.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating CoreDNS with an enterprise DNS server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section will show you how to use a main DNS server to forward the name
    resolution of the `foowidgets.k8s` zone to a CoreDNS server running on a Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The steps provided here are an example of integrating an enterprise DNS server
    with a Kubernetes DNS service. Because of the external DNS requirement and additional
    setup, the steps are for reference and **should not be executed** on your KinD
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: To find a record in a delegated zone, the main DNS server uses a process known
    as a recursive query. A recursive query refers to a DNS inquiry initiated by a
    DNS resolver, acting on behalf of a user. Through the recursive query process,
    the DNS resolver assumes the task of reaching out to multiple DNS servers in a
    hierarchical pattern. Its objective is to find the authoritative DNS server for
    the requested domain and initiate the retrieval of the requested DNS record.
  prefs: []
  type: TYPE_NORMAL
- en: The diagram below illustrates the flow of how DNS resolution is provided by
    delegating a zone to a CoreDNS server in an enterprise environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: DNS delegation flow'
  prefs: []
  type: TYPE_NORMAL
- en: The local client will look at its DNS cache for the name being requested.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the name is not in the local cache, a request is made to the main DNS server
    for `nginx.foowidgets.k8s`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The DNS server receives the query and looks at the zones it knows about. It
    finds the `foowidgets.k8s` zone and sees that the zone has been delegated to the
    CoreDNS running on `192.168.1.200`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The main DNS server sends the query to the delegated CoreDNS server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CoreDNS looks for the name, `nginx`, in the `foowidgets.k8s` zone.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CoreDNS sends the IP address for `foowidgets.k8s` back to the main DNS server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The main DNS server sends the reply containing the address for `nginx.foowidgets.k8s`
    to the client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The client connects to the NGINX server using the IP address returned from CoreDNS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s move on to a real-world example. For our scenario, the main DNS server
    is running on a Windows 2019 server and we will delegate a zone to a CoreDNS server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The components deployed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Our network subnet is `10.2.1.0/24`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Windows 2019 or higher server running DNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A MetalLB address pool with a range of `10.2.1.70`-`10.2.1.75`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A CoreDNS instance deployed in a cluster using a `LoadBalancer` service assigned
    the IP address `10.2.1.74` from our IP pool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployed add-ons, using the configuration from this chapter including ExternalDNS,
    an ETCD deployment for CoreDNS, and a new CoreDNS ETCD integrated zone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bitnami NGINX deployment to test the delegation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s go through the configuration steps to integrate our DNS servers.
  prefs: []
  type: TYPE_NORMAL
- en: Exposing CoreDNS to external requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already covered how to deploy most of the resources that you need to
    integrate – ETCD, ExternalDNS, and configuring CoreDNS with a new zone that is
    ETCD-integrated. To provide external access to CoreDNS, we need to create a new
    service that exposes CoreDNS on TCP and UDP port `53`. A complete service manifest
    is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There is one new option in the service that we haven’t discussed yet – we have
    added the `spec.loadBalancerIP` to our deployment. This option allows you to assign
    an IP address to the service so it will have a stable IP address, even if the
    service is recreated. We need a static IP since we need to enable forwarding from
    our main DNS server to the CoreDNS server in the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Once CoreDNS is exposed using a `LoadBalancer` on port `53`, we can configure
    the main DNS server to forward requests for hosts in the `foowidgets.k8s` domain
    to our CoreDNS server.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the primary DNS server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing to do on our main DNS server is to create a conditional forwarder
    to the node running the CoreDNS pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the Windows DNS host, we need to create a new conditional forwarder for
    `foowidgets.k8s` pointing to the IP address that we assigned to the new CoreDNS
    service. In our example, the CoreDNS service has been assigned to the host `10.2.1.74`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Windows conditional forwarder setup'
  prefs: []
  type: TYPE_NORMAL
- en: This configures the Windows DNS server to forward any request for a host in
    the `foowidgets.k8s` domain to the CoreDNS service running on IP address `10.2.1.74`.
  prefs: []
  type: TYPE_NORMAL
- en: Testing DNS forwarding to CoreDNS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To test the configuration, we will use a workstation on the main network that
    has been configured to use the Windows DNS server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first test we will run is an `nslookup` of the NGINX record that was created
    by the MetalLB annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From Command Prompt, we execute an `nslookup nginx.foowidgets.k8s` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Since the query returned the IP address we expected for the record, we can confirm
    that the Windows DNS server is forwarding requests to CoreDNS correctly.
  prefs: []
  type: TYPE_NORMAL
- en: We can do one more additional NGINX test from the laptop’s browser. In Chrome,
    we can use the URL registered in CoreDNS, `nginx.foowidgets.k8s`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Success browsing from an external workstation using CoreDNS'
  prefs: []
  type: TYPE_NORMAL
- en: One test confirms that the forwarding works, but we want to create an additional
    deployment to verify the system is fully working.
  prefs: []
  type: TYPE_NORMAL
- en: To test a new service, we deploy a different NGINX server called microbot, with
    a service that has an annotation assigning the name `microbot.foowidgets.k8s`.
    MetalLB has assigned the service the IP address of `10.2.1.65`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like our previous test, we test the name resolution using `nslookup`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm that the web server is running correctly, we browse to the URL from
    a workstation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Successful browsing from an external workstation using CoreDNS
    ](img/B21165_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Successful browsing from an external workstation using CoreDNS'
  prefs: []
  type: TYPE_NORMAL
- en: Success! We have now integrated an enterprise DNS server with a CoreDNS server
    running on a Kubernetes cluster. This integration provides users with the ability
    to register service names dynamically by simply adding an annotation to the service.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing between multiple clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are various ways to configure running services in multiple clusters, often
    involving complex and costly add-ons like global load balancers. A global load
    balancer can be thought of as a traffic cop – it knows how to direct incoming
    traffic between multiple endpoints. At a high level, you can create a new DNS
    entry that the global load balancer will control. This new entry will have backend
    systems added to the endpoint list and based on factors like health, connections,
    or bandwidth, it will direct the traffic to the endpoint nodes. If an endpoint
    is unavailable for any reason, the load balancer will remove it from the endpoint
    list. By removing it from the list, traffic will only be sent to healthy nodes,
    providing a smooth end user experience. There’s nothing worse for a customer than
    getting a website not found error when they attempt to access a site.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Global load balancing traffic flow'
  prefs: []
  type: TYPE_NORMAL
- en: The figure above shows a healthy workflow where both clusters are running an
    application that we are load balancing between the clusters. When the request
    hits the load balancer, it will send the traffic in a round-robin fashion between
    the two clusters. The `nginx.foowidgets.k8s` request will ultimately send the
    traffic to either `nginx.clustera.k8s` or `nginx.clusterb.k8s`.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5.8*, we have a failure of our NGINX workload in cluster B. Since
    the global load balancer has a health check on the running workloads, it will
    remove the endpoint in cluster B from the `nginx.foowidgets.k8s` entry.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: Global load balancing traffic flow with a site failure'
  prefs: []
  type: TYPE_NORMAL
- en: Now, for any traffic that comes into the load balancer requesting `nginx.foowidgets.k8s`,
    the only endpoint that will be used for traffic is running on cluster A. Once
    the issue has been resolved on cluster B, the load balancer will automatically
    add the cluster B endpoint back into the `nginx.foowidgets.k8s` record.
  prefs: []
  type: TYPE_NORMAL
- en: Such solutions are widespread in enterprises, with many organizations utilizing
    products from companies like **F5**, **Citrix**, **Kemp**, and **A10**, as well
    as CSP-native solutions like **Route 53** and **Traffic Director**, to manage
    workloads across multiple clusters. However, there are projects with similar functionality
    that integrate with Kubernetes, and they come at little to no cost. While these
    projects may not offer all the features of some vendor solutions, they often meet
    the needs of most use cases without requiring the full spectrum of expensive features.
  prefs: []
  type: TYPE_NORMAL
- en: One such project is **K8GB**, an innovative open-source project that brings
    **Global Server Load Balancing** (**GSLB**) to Kubernetes. With K8GB, organizations
    can easily distribute incoming network traffic across multiple Kubernetes clusters
    located in different geographical locations. By intelligently routing requests,
    K8GB guarantees low latency, optimal response times, and redundancy, providing
    an exceptional solution to any enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: This section will introduce you to K8GB, but if you want to learn more about
    the project, browse to the project’s main page at [https://www.k8gb.io](https://www.k8gb.io).
  prefs: []
  type: TYPE_NORMAL
- en: Since we are using KinD and a single host for our cluster, this section of the
    book is meant to introduce you to the project and the benefits it provides. This
    section is for reference only, since it is a complex topic that requires multiple
    components, some of which are outside of Kubernetes. If you decide you want to
    implement a solution for yourself, we have included example documentation and
    scripts in the book’s repo under the `chapter5/k8gs-example` directory.
  prefs: []
  type: TYPE_NORMAL
- en: K8GB is a CNCF sandbox project, which means it is in its early stages and any
    newer version after the writing of this chapter may have changes to objects and
    configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Kubernetes Global Balancer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why should you care about a project like K8GB?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider an internal enterprise cloud as an example, operating a Kubernetes
    cluster at a production site alongside another cluster at a disaster recovery
    site. To ensure a smooth user experience, it is important to enable applications
    to transition seamlessly between these data centers, without requiring any manual
    intervention during disaster recovery events. The challenge lies in fulfilling
    the enterprise’s demand for high availability of microservices when multiple clusters
    are simultaneously serving these applications. We need to effectively address
    the need for continuous and uninterrupted service across geographically dispersed
    Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: This is where **K8GB** comes in.
  prefs: []
  type: TYPE_NORMAL
- en: 'What makes K8GB an ideal solution for addressing our high availability requirements?
    As documented on their site, the key features include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing is provided using a timeproof DNS protocol that is extremely
    reliable and works well for global deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no requirement for a management cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no single point of failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses native Kubernetes health checks for load balancing decisions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring is as simple as a single Kubernetes CRD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It works with any Kubernetes cluster – on-prem or off-prem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s free!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you will see in this section, K8GB provides an easy and intuitive configuration
    that makes providing global load balancing to your organization easy. This may
    make K8GB look like it doesn’t do very much, but behind the scenes, it provides
    a number of advanced features, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Global Load Balancing**: Facilitates the distribution of incoming network
    traffic among multiple Kubernetes clusters located in different geographic regions.
    As a result, it enables optimized application delivery, ensuring reduced latency
    and improved user experience for users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intelligent Traffic Routing**: Utilizing sophisticated routing algorithms
    to intelligently steer client requests towards the closest or most appropriate
    Kubernetes cluster, considering factors like proximity, server health, and application-specific
    rules. This approach ensures efficient and highly responsive traffic management
    for optimal application performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High Availability and Redundancy**: Guarantees high availability and fault
    tolerance for applications by automatically redirecting traffic in the event of
    cluster, application, or data center failure. This failover mechanism minimizes
    downtime during disaster recovery scenarios, ensuring uninterrupted service delivery
    to users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated Failover**: Simplifies operations by enabling automatic failover
    between data centers without the need for manual intervention. This eliminates
    the requirement for human-triggered **Disaster Recovery** (**DR**) events or tasks,
    ensuring quick and uninterrupted service delivery and streamlined operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with Kubernetes**: Offers seamless integration with Kubernetes,
    simplifying the setup and configuration of GSLB for applications deployed on clusters.
    Leveraging Kubernetes’ native capabilities, K8GB delivers a scalable solution,
    enhancing the overall management and efficiency of global load balancing operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**On-Prem and Cloud Provider Support**: Provides enterprises a way to efficiently
    manage GSLB for multiple Kubernetes clusters, enabling seamless handling of complex
    multi-region deployments and hybrid cloud scenarios. This ensures optimized application
    delivery across different environments, enhancing the overall performance and
    resilience of the infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customization and Flexibility**: Provides users the freedom to define personalized
    rules and policies for traffic routing, providing organizations with the flexibility
    to customize GSLB configurations to meet their unique requirements precisely.
    This empowers enterprises to optimize traffic management based on their specific
    needs and ensures seamless adaptation to ever-changing application demands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring, Metrics, and Tracing**: Includes monitoring, metrics, and tracing
    capabilities, enabling administrators to access insights into traffic patterns,
    health, and performance metrics spanning across multiple clusters. This provides
    enhanced visibility, empowering administrators to make informed decisions and
    optimize the overall performance and reliability of the GSLB setup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have discussed the key features of K8GB, let’s get into the details.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements for K8GB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For a product that provides a complex function like global load balancing,
    K8GB doesn’t require a lot of infrastructure or resources to provide load balancing
    to your clusters. The latest release, which, as of this chapter’s creation, is
    `0.12.2` – has only a handful of requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: The CoreDNS servers Load Balancer IP address in the main DNS zone using the
    naming standard `gslb-ns-<k8gb-name>-gb.foowidgets.k8s` – for example, `gslb-ns-us-nyc-gb.foowidgets.k8s
    and gslb-ns-us-buf-gb.foowidgets.k8s`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are using K8GB with a service like **Route 53**, **Infoblox**, or **NS1**,
    the CoreDNS servers will be added to the domain automatically. Since our example
    is using an on-premises DNS server running on a Windows 2019 server, we need to
    create the records manually.
  prefs: []
  type: TYPE_NORMAL
- en: An Ingress controller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A K8GB controller deployed in the cluster, which will deploy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The K8GB controller
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A CoreDNS server with the CoreDNS CRD plugin configured – this is included in
    the deployment on K8GB
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since we have already explored NGINX ingress controllers in previous chapters,
    we now turn our attention to the additional requirements: deploying and configuring
    the K8GB controller within a cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss the steps to implement K8GB.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying K8GB to a cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have included example files in the GitHub repo under the `chapter5/k8gb-example`
    directory. The scripts are based on the example we will use for the remainder
    of the chapter. If you decide to use the files in a development cluster, you will
    need to meet the requirements below:'
  prefs: []
  type: TYPE_NORMAL
- en: Two Kubernetes clusters (a single-node `kubeadm` cluster for each cluster will
    work)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreDNS deployed in each cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K8GB deployed in each cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An edge DNS server that you can use to delegate the domain for K8GB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing K8GB has been made very simple – you only need to deploy a single
    Helm chart using a `values.yaml` file that has been configured for your infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install K8GB, you will need to add the K8GB repository to your Helm repo
    list and then update the charts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Before we execute a `helm install` command, we need to customize the Helm `values.yaml`
    file for each cluster deployment. We have included a values file for both of the
    clusters used in our example, `k8gb-buff-values.yaml` and `k8gb-nyc-values.yaml`,
    located in the `chapter5/k8gb-example` directory. The options in the values file
    will be discussed in the *Customizing the Helm chart values* section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding K8GB load balancing options
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our example, we will configure K8GB as a failover load balancer between
    two on-premises clusters; however, K8GB is not limited to just failover. Like
    most load balancers, K8GB offers a variety of solutions that can be configured
    differently for each load-balanced URL. It offers the most commonly required strategies,
    including round robin, weighted round robin, failover, and GeoIP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the strategies is described below:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Round Robin**: If you do not specify a strategy, it will default to a simple
    round-robin load balancing configuration. Using round robin means that requests
    will be split between the configured clusters – request 1 will go to cluster 1,
    request 2 will go to cluster 2, request 3 will go to cluster 1, request 4 will
    go to cluster 2, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted Round Robin**: Similar to round robin, this strategy provides the
    ability to specify the percentage of traffic to send to a cluster; for example,
    75% of traffic will go to cluster 1 and 15% will go to cluster 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failover**: All traffic will go to the primary cluster unless all pods for
    a deployment become unavailable. If all pods are down in cluster 1, cluster 2
    will take over the workload until the pods in cluster 1 become available again,
    which will then become the primary cluster again.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GeoIP:** Directs requests to the closest cluster to the client connection.
    If the closest host is down, it will use a different cluster similar to how the
    failover strategy works. To use this strategy, you will need to create a GeoIP
    database (an example can be found here: [https://github.com/k8gb-io/coredns-crd-plugin/tree/main/terratest/geogen](https://github.com/k8gb-io/coredns-crd-plugin/tree/main/terratest/geogen)),
    and your DNS server needs to support the **EDNS0** extension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EDNS0** is based on RFC 2671, which outlines how EDNS0 works and its various
    components, including the format of EDNS0-enabled DNS messages, the structure
    of EDNS0 options, and guidelines for its implementation. The goal of RFC 2671
    is to provide a standardized approach for extending the capabilities of the DNS
    protocol beyond its original limitations, allowing for the incorporation of new
    features, options, and enhancements'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you know the available strategies, let’s go over our example infrastructure
    for our clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Cluster/Server Details** | **Details** |'
  prefs: []
  type: TYPE_TB
- en: '| Corporate DNS Server – New York CityIP: `10.2.1.14` | Main corporate zone`foowidgets.k8s`Host
    records for the CoreDNS servers`gslb-ns-us-nyc-gb.foowidgets.k8s``gslb-ns-us-buf-gb.foowidgets.k8s`Global
    domain configured delegating to the CoreDNS servers in the clusters`gb.foowidgets.k8s`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `New York City`, New York – Cluster 1Primary SiteCoreDNS LB IP: `10.2.1.221`Ingress
    IP: `10.2.1.98` | NGINX Ingress Controller exposed using HostPortCoreDNS deployment
    exposed using MetalLB |'
  prefs: []
  type: TYPE_TB
- en: '| `Buffalo`, New York – Cluster 2Secondary SiteCoreDNS LB IP: `10.2.1.224`Ingress
    IP: `10.2.1.167` | NGINX Ingress Controller exposed using HostPortCoreDNS deployment
    exposed using MetalLB |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5.1: Cluster details'
  prefs: []
  type: TYPE_NORMAL
- en: We will use the details from the above table to explain how we would deploy
    K8GB in our example infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: With the details of the infrastructure, we can now create our Helm `values.yaml`
    files for each deployment. In the next section, we will show the values we need
    to configure using the example infrastructure, explaining each value.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the Helm chart values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Each cluster will have a similar values file; the main changes will be the
    tag values we use. The values file below is for the New York City cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The same file contents will be used for the NYC cluster, with the exception
    of the `clusterGeoTag` and `extGslbClustersGeoTags` values, for the NYC cluster
    they need to be set to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the configuration isn’t very lengthy, requiring only a handful
    of options to configure a usually complex global load balancing configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s go over some of the main details of the values we are using.
  prefs: []
  type: TYPE_NORMAL
- en: The main details that we will explain are the values in the K8GB section, which
    configures all of the options K8GB will use for load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Chart Value** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `dnsZone` | This is the DNS zone that you will use for K8GB – basically,
    this is the zone that will be used for the DNS records that will be used to store
    our global load balanced DNS records. |'
  prefs: []
  type: TYPE_TB
- en: '| `edgeDNSZone` | The main DNS zone that contains the DNS records for the CoreDNS
    servers that are used by the previous option (`dnsZone`). |'
  prefs: []
  type: TYPE_TB
- en: '| `edgeDNSServers` | The edge DNS server – usually the main DNS server used
    for name resolution. |'
  prefs: []
  type: TYPE_TB
- en: '| `clusterGeoTag` | If you have multiple K8GB controllers, this tag is used
    to specify instances between each other. In our example, we set these to `us-buf`
    and `us-nyc` for our clusters. |'
  prefs: []
  type: TYPE_TB
- en: '| `extGslbClusterGeoTags` | Specifies the other K8GB controllers to pair with.
    In our example, each cluster adds the other cluster `clusterGeoTags` – the `Buffalo`
    cluster adds the `us-nyc` tag and the NYC cluster adds the `us-buf` tag. |'
  prefs: []
  type: TYPE_TB
- en: '| `isClusterService` | Set to `true` or `false`. Used for service upgrades;
    you can read more at [https://www.k8gb.io/docs/service_upgrade.xhtml](https://www.k8gb.io/docs/service_upgrade.xhtml).
    |'
  prefs: []
  type: TYPE_TB
- en: '| `exposeCoreDNS` | If set to `true`, a `LoadBalancer` service will be created,
    exposing the CoreDNS deployed in the `k8gb` namespace on port `53`/UDP for external
    access. |'
  prefs: []
  type: TYPE_TB
- en: '| `deployment.skipConfig` | Set to true or false. Setting it to false tells
    the deployment to use the CoreDNS shipped with K8GB. |'
  prefs: []
  type: TYPE_TB
- en: '| `image.repository` | Configures the repository to use for the CoreDNS image.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `image.tag` | Configures the tag to use when pulling the image. |'
  prefs: []
  type: TYPE_TB
- en: '| `serviceAccount.create` | Set to `true` or `false`. When set to true, a service
    account will be created. |'
  prefs: []
  type: TYPE_TB
- en: '| `serviceAccount.name` | Sets the name of the service account from the previous
    option. |'
  prefs: []
  type: TYPE_TB
- en: '| `serviceType` | Configures the service type for CoreDNS. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5.2: K8GB options'
  prefs: []
  type: TYPE_NORMAL
- en: Using Helm to install K8GB
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the overview of K8GB and the Helm values file complete, we can move on
    to installing K8GB in the clusters. We have included scripts to deploy K8GB to
    the `Buffalo` and `NYC` clusters. In `chapter5/k8gb-example/k8gb`, you will see
    two scripts, deploy-`k8gb-buf.sh` and `deploy-k8gb-nyc.sh` – these should be run
    in their corresponding clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script will execute the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Add the K8GB Helm repo to the server’s repo list
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the repos
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy K8GB using the appropriate Helm values file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a **Gslb record** (covered in the next section)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a deployment to use for testing in a namespace called `demo`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once deployed, you will see two pods running in the `k8gb` namespace, one for
    the `k8gb` controller and the other for the CoreDNS server that will be used to
    resolve load balancing names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also verify that the services were created to handle the incoming DNS
    requests. Since we exposed it using a `LoadBalancer` type, we will see the `LoadBalancer`
    service on port `53` using the UDP protocol:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: With the deployment of K8GB complete and verified in both clusters, let’s move
    on to the next section where we will explain how to create our edge delegation
    for our load balanced zone.
  prefs: []
  type: TYPE_NORMAL
- en: Delegating our load balancing zone
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our example, we are using a Windows server as our edge DNS server, which
    is where our K8s DNS names will be registered. On the DNS side, we need to add
    two DNS records for our CoreDNS servers, and then we need to delegate the load
    balancing zone to these servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two A records need to be in the main edge DNS zone. In our example, that
    is the `foowidgets.k8s` zone. In this zone, we need to add two entries for the
    CoreDNS servers that are exposed using a `LoadBalancer` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Adding our CoreDNS servers to the edge zone'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to create a new delegated zone that will be used for our load
    balanced service names. In Windows, this is done by *right-clicking* the zone
    and selecting **New Delegation**; in the delegation wizard, you will be asked
    for the **Delegated domain**. In our example, we are going to delegate the **gb**
    domain as our domain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Creating a new delegated zone'
  prefs: []
  type: TYPE_NORMAL
- en: After you enter the zone name and click **Next**, you will see a new screen
    to add DNS servers for the delegated domain; when you click **Add**, you will
    enter the DNS names for your CoreDNS servers. Remember that we created two A records
    in the main domain, `foowidgets.com`. As you add entries, Windows will verify
    that the entered name resolves correctly and that DNS queries work. Once you add
    both CoreDNS servers, the summary screen will show both with their IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: Adding the DNS names for the CoreDNS servers'
  prefs: []
  type: TYPE_NORMAL
- en: That completes the edge server configuration. For certain edge servers, K8GB
    will create the delegation records, but there are only a handful of servers that
    are compatible with that feature. For any edge server that doesn’t auto create
    the delegated servers, you need to create them manually like we did in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have CoreDNS in our clusters and we have delegated the load balanced
    zone, we will deploy an application that has global load balancing to test our
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a highly available application using K8GB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two methods to enable global load balancing for an application. You
    can create a new record using a custom resource provided by K8GB, or you can annotate
    an Ingress rule. For our demonstration of K8GB, we will deploy a simple NGINX
    web server in a cluster and add it to K8GB using the natively supplied custom
    resource.
  prefs: []
  type: TYPE_NORMAL
- en: Adding an application to K8GB using custom resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we deployed K8GB, a new **Custom Resource Definition** (**CRD**) named
    `Gslb` was added to the cluster. This CRD assumes the role of managing applications
    marked for global load balancing. Within the `Gslb` object, we define a specification
    for the Ingress name, mirroring the format of a regular Ingress object. The sole
    distinction between a standard Ingress and a `Gslb` object lies in the last portion
    of the manifest, the strategy.
  prefs: []
  type: TYPE_NORMAL
- en: The strategy defines the type of load balancing we want to use, which is failover
    for our example, and the primary GeoTag to use for the object. In our example,
    the NYC cluster is our primary cluster, so our `Gslb` object will be set to `us-buf`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy an application that will leverage load balancing, we need to create
    the following in both clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: A standard deployment and service for the application. We will call the deployment
    `nginx`, using the standard NGINX image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A `Gslb` object in each cluster. For our example, we will use the manifest
    below, which will declare the Ingress rule and set the strategy to failover using
    `us-buf` as the primary K8GB. Since the `Gslb` object has the information for
    the Ingress rule, you do not need to create an Ingress rule; `Gslb` will create
    the Ingress object for us. Let’s look at an example below:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When you deploy the manifest for the `Gslb` object, it will create two Kubernetes
    objects, the `Gslb` object and an Ingress object.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we looked at the `demo` namespace for the `Gslb` objects in the `Buffalo`
    cluster, we would see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And if we looked at the Ingress objects in the NYC cluster, we would see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We would also have similar objects in the NYC cluster, which we will explain
    in the *Understanding how K8GB provides global load balancing* section.
  prefs: []
  type: TYPE_NORMAL
- en: Adding an application to K8GB using Ingress annotations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second method for adding an application to K8GB is to add two annotations
    to a standard Ingress rule, which was primarily added to allow developers to add
    an existing Ingress rule to K8GB.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add an Ingress object to the global load balancing list, you only need to
    add two annotations to the Ingress object, `strategy` and `primary-geotag`. An
    example of the annotations is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This would add the Ingress to K8GB using the failover strategy using the `us-buf`
    GeoTag as the primary tag.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have deployed all of the required infrastructure components and
    all of the required objects to enable global load balancing for an application,
    let’s see it in action.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how K8GB provides global load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The design of K8GB is complex, but once you deploy an application and understand
    how K8GB maintains zone files, it will become easier. This is a fairly complex
    topic, and it does assume some previous knowledge of how DNS works, but by the
    end of this section, you should be able to explain how K8GB works.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping the K8GB CoreDNS servers in sync
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first topic to discuss is how K8GB manages to keep two, or more, zone files
    in sync to provide seamless failover for our deployments. Seamless failover is
    a process that ensures an application continues to run smoothly even during system
    issues or failures. It automatically transitions to the backup system or resource,
    maintaining an uninterrupted user experience.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, each K8GB CoreDNS server in the clusters must have
    an entry in the main DNS server.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the DNS server and zone that we configured for the edge values in the
    `values.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'So, in the edge DNS server (`10.2.1.14`), we have a host record for each CoreDNS
    server using the required K8GB naming convention:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: K8GB will communicate between all of the CoreDNS servers and update any records
    that need to be updated due to being added, deleted, or updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'This becomes a little easier to understand with an example. Using our cluster
    example, we have deployed an NGINX web server and created all of the required
    objects in both clusters. After deploying, we would have a `Gslb` and Ingress
    object in each cluster, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Cluster: NYCDeployment: `nginx``Gslb: gslb-failover-nyc`Ingress: `fe.gb.foowidgets.k8s`NGINX
    Ingress IP: `10.2.1.98` | Cluster: `Buffalo` (Primary)Deployment: `nginx``Gslb:
    gslb-failover-buf`Ingress: `fe.gb.foowidgets.k8s`NGINX Ingress IP: `10.2.1.167`
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5.3: Objects in each cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the deployment is healthy in both clusters, the CoreDNS servers will
    have a record for `fe.gb.foowidgets.k8s` with an IP address of `10.2.1.167`, the
    primary deployment. We can verify this by running a `dig` command on any client
    machine that uses the edge DNS server (`10.2.1.14`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the output from `dig`, the host resolved to `10.2.1.167`
    since the application is healthy in the primary cluster. If we curl the DNS name,
    we will see that the NGINX server in `Buffalo` replies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We will simulate a failure by scaling the replicas for the deployment in the
    `Buffalo` cluster to `0`, which will look like a failed application to K8GB. When
    the K8GB controller in the NYC cluster sees that the application no longer has
    any healthy endpoints, it will update the CoreDNS record in all servers with the
    secondary IP address to fail the service over to the secondary cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once scaled down, we can use `dig` to verify what host is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We will `curl` again to verify that the workload has been moved to the NYC
    cluster. When we execute curl, we will see that the NGINX server is now located
    in the NYC cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note that the IP address returned is now the IP address for the deployment in
    the `Buffalo` cluster, the secondary cluster, `10.2.1.98`. This proves that K8GB
    is working correctly and providing us with a Kubernetes-controlled global load
    balancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the application becomes healthy in the primary cluster, K8GB will update
    CoreDNS and any requests will resolve to the main cluster again. To test this,
    we scaled the deployment in `Buffalo` back up to `1` and ran another dig test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the IP has been updated to reflect the NYC Ingress controller
    on address `10.2.1.167`, the primary location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a last curl to verify that the workload is being serviced out of the
    `Buffalo` cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: K8GB is a unique, and impressive, project from the CNCF that offers global load
    balancing similar to what other, more expensive, products offer today.
  prefs: []
  type: TYPE_NORMAL
- en: It’s a project that we are watching carefully, and if you need to deploy applications
    across multiple clusters, you should consider looking into the K8GB project as
    it matures.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to provide automatic DNS registration to any
    service that uses a `LoadBalancer` service. You also learned how to deploy a highly
    available service using the CNCF project, K8GB, which provides global load balancing
    to a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: These projects have become integral to numerous enterprises, offering users
    capabilities that previously required the efforts of multiple teams and, often,
    extensive paperwork, to deliver applications to customers. Now, your teams can
    swiftly deploy and update applications using standard agile practices, providing
    your organization with a competitive advantage.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *Integrating Authentication into Your Cluster*, we will
    explore the best methods and practices for implementing secure authentication
    in Kubernetes. You will learn how to integrate enterprise authentication using
    the OpenID Connect protocol and how to use Kubernetes impersonation. We will also
    discuss the challenges of managing credentials in a cluster and offer practical
    solutions for authenticating users and pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes does not support using both TCP and UDP with services.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: b'
  prefs: []
  type: TYPE_NORMAL
- en: ExternalDNS only integrates with CoreDNS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: b'
  prefs: []
  type: TYPE_NORMAL
- en: What do you need to configure on your edge DNS server for K8GB to provide load
    balancing to a domain?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nothing, it works without additional configuration
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It must point to a cloud-provided DNS server
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: You must delegate a zone that points to your cluster IP
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a delegation to your CoreDNS instances
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: d'
  prefs: []
  type: TYPE_NORMAL
- en: What strategy is not supported by K8GB?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Failover
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Round robin
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Random distribution
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: GeoIP
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: c'
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask Me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/K8EntGuide](https://packt.link/K8EntGuide)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code965214276169525265.png)'
  prefs: []
  type: TYPE_IMG
