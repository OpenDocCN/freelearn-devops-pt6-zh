- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: External DNS and Global Load Balancing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部DNS和全局负载均衡
- en: In this chapter, we will build on what you learned in *Chapter 4*. We will discuss
    some of the limitations of certain load balancer features and how we can configure
    a cluster to resolve those limitations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在*第4章*的基础上进行讲解。我们将讨论一些负载均衡器功能的局限性，以及如何配置集群来解决这些局限性。
- en: We know that Kubernetes has a built-in DNS server that dynamically allocates
    names to resources. These are used for applications to communicate intra-cluster,
    or within the cluster. While this feature is beneficial for internal cluster communication,
    it doesn’t provide DNS resolution for external workloads. Since it does provide
    DNS resolution, why do we say it has limitations?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道Kubernetes有一个内置的DNS服务器，它动态地为资源分配名称。这些名称用于应用程序在集群内部进行通信。虽然这个功能对集群内部通信有利，但它并不为外部工作负载提供DNS解析。既然它提供DNS解析，为什么我们还说它有局限性呢？
- en: In the previous chapter, we used a dynamically assigned IP address to test our
    `LoadBalancer` service workloads. While our examples have been good for learning,
    in an enterprise, nobody wants to access a workload running on a cluster using
    an IP address. To address this limitation, the Kubernetes SIG has developed a
    project called **ExternalDNS**, which provides the ability to dynamically create
    DNS entries for our `LoadBalancer` services.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用动态分配的IP地址来测试我们的`LoadBalancer`服务工作负载。虽然我们的示例对于学习很有帮助，但在企业环境中，没人想通过IP地址访问运行在集群上的工作负载。为了解决这一限制，Kubernetes
    SIG开发了一个名为**ExternalDNS**的项目，它提供了动态为我们的`LoadBalancer`服务创建DNS条目的功能。
- en: Also, in an enterprise, you will commonly run services that run on multiple
    clusters to provide failover for your applications. So far, the options we have
    discussed can’t address failover scenarios. In this chapter, we will explain how
    to implement a solution to provide an automated failover for workloads, making
    them highly available across multiple clusters.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在企业环境中，您通常会运行多个集群上的服务，以为您的应用提供故障转移。到目前为止，我们讨论的选项无法处理故障转移场景。在本章中，我们将解释如何实现一个解决方案，为工作负载提供自动故障转移，使其在多个集群之间高度可用。
- en: 'In this chapter, you will learn the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章您将学习以下内容：
- en: An introduction to external DNS resolution and global load balancing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部DNS解析和全局负载均衡简介
- en: Configuring and deploying ExternalDNS in a Kubernetes cluster
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes集群中配置和部署ExternalDNS
- en: Automating DNS name registration
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化DNS名称注册
- en: Integrating ExternalDNS with an enterprise DNS server
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将ExternalDNS与企业DNS服务器集成
- en: Using GSLB to offer global load balancing across multiple clusters
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GSLB提供跨多个集群的全局负载均衡
- en: Now, let’s jump into the chapter!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入这一章的内容！
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has the following technical requirements:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章具有以下技术要求：
- en: An Ubuntu 22.04+ server running Docker with a minimum of 4 GB of RAM, though
    8 GB is recommended
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行Docker的Ubuntu 22.04+服务器，至少需要4GB的内存，推荐8GB
- en: A KinD cluster running **MetalLB** – if you completed *Chapter 4*, you should
    already have a cluster running MetalLB
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行**MetalLB**的KinD集群——如果您已完成*第4章*，那么您应该已经拥有一个运行MetalLB的集群
- en: 'The scripts from the `chapter5` folder from the repository, which you can access
    by going to this book’s GitHub repository: [https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仓库中`chapter5`文件夹中的脚本，您可以通过访问本书的GitHub仓库来获取：[https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)
- en: Making service names available externally
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使服务名称对外可用
- en: As we mentioned in the introduction, you may remember that we used IP addresses
    to test the `LoadBalancer` services we created, whereas for our `Ingress` examples,
    we used domain names. Why did we have to use IP addresses instead of a hostname
    for our `LoadBalancer` services?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在简介中提到的，您可能还记得我们曾使用IP地址来测试我们创建的`LoadBalancer`服务，而对于我们的`Ingress`示例，我们使用了域名。为什么我们必须使用IP地址而不是主机名来访问我们的`LoadBalancer`服务呢？
- en: Although a Kubernetes load balancer assigns a standard IP address to a service,
    it doesn’t automatically generate a DNS name for workloads to access the service.
    Instead, you must rely on IP addresses to connect to applications within the cluster,
    which becomes confusing and inefficient. Furthermore, manually registering DNS
    names for each IP assigned by **MetalLB** presents a maintenance challenge. To
    deliver a more cloud-like experience and streamline name resolution for `LoadBalancer`
    services, we need an add-on that can address these limitations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Kubernetes 负载均衡器为服务分配了一个标准的 IP 地址，但它不会自动为工作负载生成一个 DNS 名称来访问服务。相反，你必须依赖 IP
    地址来连接集群中的应用程序，这变得混乱且低效。此外，手动为每个由 **MetalLB** 分配的 IP 注册 DNS 名称会带来维护挑战。为了提供更接近云的体验并简化
    `LoadBalancer` 服务的名称解析，我们需要一个能够解决这些限制的附加组件。
- en: Similar to the team that maintains KinD, there is a Kubernetes SIG that is working
    on this feature for Kubernetes called **ExternalDNS**. The main project page can
    be found on the SIG’s GitHub at [https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于维护 KinD 的团队，有一个 Kubernetes SIG 正在为 Kubernetes 开发这个功能，叫做 **ExternalDNS**。该项目的主页面可以在
    SIG 的 GitHub 上找到：[https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns)。
- en: 'At the time of writing, the `ExternalDNS` project supports 34 compatible DNS
    services, including the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写时，`ExternalDNS` 项目支持 34 种兼容的 DNS 服务，包括以下内容：
- en: AWS Cloud Map
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Cloud Map
- en: Amazon’s Route 53
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊的 Route 53
- en: Azure DNS
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure DNS
- en: Cloudflare
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloudflare
- en: CoreDNS
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreDNS
- en: Google Cloud DNS
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud DNS
- en: Pi-hole
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pi-hole
- en: RFC2136
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RFC2136
- en: There are multiple options on how to extend CoreDNS to resolve external names,
    depending on what main DNS server you may be running. Many of the supported DNS
    servers will simply register any services dynamically. ExternalDNS will see the
    created resource and use native calls to register services automatically, like
    **Amazon’s Route 53**. Not all DNS servers natively allow for this type of dynamic
    registration by default.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你所运行的主 DNS 服务器类型，有多种方法可以扩展 CoreDNS 来解析外部名称。许多支持的 DNS 服务器会动态注册任何服务。ExternalDNS
    会看到创建的资源并使用本机调用自动注册服务，像 **Amazon’s Route 53**。并非所有 DNS 服务器默认都允许这种类型的动态注册。
- en: In these instances, you need to manually configure your main DNS server to forward
    the desired domain requests to a CoreDNS instance running in your cluster. This
    is what we will use for the examples in this chapter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，你需要手动配置主 DNS 服务器，将所需的域请求转发到运行在集群中的 CoreDNS 实例。这就是我们将在本章示例中使用的内容。
- en: Our Kubernetes cluster currently utilizes CoreDNS to handle cluster DNS name
    resolution. However, what might be lesser known is that CoreDNS offers more than
    just internal cluster DNS resolution. It can extend its capabilities to perform
    external name resolution, effectively resolving names for any DNS zone managed
    by a CoreDNS deployment.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Kubernetes 集群目前使用 CoreDNS 来处理集群的 DNS 名称解析。然而，可能不太为人所知的是，CoreDNS 提供的不仅仅是内部集群的
    DNS 解析。它可以扩展其功能，执行外部名称解析，实际上可以解析由 CoreDNS 部署管理的任何 DNS 区域的名称。
- en: Now, let’s move on to how ExternalDNS installs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续介绍 ExternalDNS 的安装过程。
- en: Setting up ExternalDNS
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 ExternalDNS
- en: Right now, our CoreDNS is only resolving names for internal cluster names, so
    we need to set up a zone for our new `LoadBalancer` DNS entries.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的 CoreDNS 只为内部集群名称解析名称，因此我们需要为新的 `LoadBalancer` DNS 条目设置一个区域。
- en: For our example, a company, **FooWidgets**, wants all Kubernetes services to
    go into the `foowidgets.k8s` domain, so we will use that as our new zone.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，一个公司 **FooWidgets** 想让所有 Kubernetes 服务都进入 `foowidgets.k8s` 域，因此我们将使用该域作为新的区域。
- en: Integrating ExternalDNS and CoreDNS
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成 ExternalDNS 和 CoreDNS
- en: '**ExternalDNS** is not an actual DNS server; instead, it is a controller that
    will watch for objects that request a new DNS entry. Once a request is seen by
    the controller, it will send the information to an actual DNS server, like CoreDNS,
    for registration.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**ExternalDNS** 不是一个实际的 DNS 服务器；它是一个控制器，负责监控请求新 DNS 条目的对象。一旦控制器看到请求，它会将信息发送到实际的
    DNS 服务器（如 CoreDNS）进行注册。'
- en: The process of how a service is registered is shown in the diagram below.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 服务注册的过程如下面的图所示。
- en: '![](img/B21165_05_01.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_01.png)'
- en: 'Figure 5.1: ExternalDNS registration flow'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：ExternalDNS 注册流程
- en: In our example, we are using CoreDNS as our DNS server; however, as we mentioned
    previously, ExternalDNS has support for 34 different DNS services and the list
    of supported services is constantly growing. Since we will be using CoreDNS as
    our DNS server, we will need to add a component that will store the DNS records.
    To accomplish this, we need to deploy an ETCD server in the cluster.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用 CoreDNS 作为我们的 DNS 服务器；然而，正如我们之前提到的，ExternalDNS 支持 34 种不同的 DNS 服务，并且支持的服务列表还在不断增加中。因为我们将使用
    CoreDNS 作为我们的 DNS 服务器，我们需要添加一个组件来存储 DNS 记录。为了实现这一点，我们需要在集群中部署一个 ETCD 服务器。
- en: For our example deployment, we will use the ETCD Helm chart.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例部署，我们将使用 ETCD Helm chart。
- en: Helm is a tool for Kubernetes that makes it easier to deploy and manage applications.
    It uses Helm charts, which are templates that contain the necessary configuration
    and resource values for the application. It automates the setup of complex applications,
    ensuring they are consistently and reliably configured. It’s a powerful tool,
    and you will find that many projects and vendors offer their applications, by
    default, using Helm charts. You can read more about Helm on their main home page
    at [https://v3.helm.sh/](https://v3.helm.sh/).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Helm 是一个用于 Kubernetes 的工具，它使得部署和管理应用程序变得更加容易。它使用 Helm charts，这些模板包含了应用程序所需的配置和资源值。它自动设置复杂应用程序的环境，确保它们在配置上保持一致和可靠。这是一个强大的工具，您会发现许多项目和供应商默认使用
    Helm charts 提供他们的应用程序。您可以在他们的主页 [https://v3.helm.sh/](https://v3.helm.sh/) 上了解更多关于
    Helm 的信息。
- en: One reason Helm is such a powerful tool is its ability to use custom options
    that can be declared when you run the `helm instal`l command. The same options
    can also be declared in a file that is passed to the installation using the `-f`
    option. These options make deploying complex systems easier and reproducible since
    the same values file can be used on any deployment.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Helm 如此强大的一个原因是它能够使用自定义选项，在运行 `helm install` 命令时声明这些选项。同样的选项也可以在通过 `-f` 选项传递给安装的文件中声明。这些选项使得部署复杂系统变得更加简单和可重现，因为可以在任何部署中使用相同的值文件。
- en: For our deployment example, we have included a `values.yaml` file, located in
    the `chapter5/etcd` directory, that we will use to configure our ETCD deployment.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的部署示例，我们已经包含了一个位于 `chapter5/etcd` 目录中的 `values.yaml` 文件，我们将使用它来配置我们的 ETCD
    部署。
- en: Now, finally, let’s deploy ETCD! We have included a script called `deploy-etcd.sh`
    in the `chapter5/etcd` directory that will deploy ETCD with a single replica in
    a new namespace called `etcd-dns`. Execute the script while in the `chapter5/etcd`
    directory.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最后，让我们部署 ETCD 吧！我们在 `chapter5/etcd` 目录中包含了一个名为 `deploy-etcd.sh` 的脚本，它将在名为
    `etcd-dns` 的新命名空间中部署一个单副本的 ETCD。在 `chapter5/etcd` 目录中执行该脚本。
- en: Thanks to Helm, the script only has two commands– it will create a new namespace,
    and then execute a `Helm install` command to deploy our ETCD instance. In the
    real world, you would want to change the replica count to at least 3 to have a
    highly available ETCD deployment, but we wanted to limit resource requirements
    for our KinD server.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Helm 的帮助，脚本只需两个命令 – 它将创建一个新的命名空间，然后执行一个 `Helm install` 命令来部署我们的 ETCD 实例。在实际环境中，您可能希望将副本数量更改至至少
    3 以实现高可用的 ETCD 部署，但我们希望限制我们的 KinD 服务器的资源需求。
- en: Now that we have our ETCD for DNS, we can move on to integrating our CoreDNS
    service with our new ETCD deployment.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为我们的 DNS 配置了 ETCD，我们可以继续将我们的 CoreDNS 服务与我们的新 ETCD 部署集成。
- en: Adding an ETCD zone to CoreDNS
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加一个 ETCD 区域到 CoreDNS
- en: As we showed in the diagram in the last section, CoreDNS will store the DNS
    records in an ETCD instance. This requires us to configure a CoreDNS server with
    the DNS zone(s) we want to register names in and the ETCD server that will store
    the records.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节的图表中展示的那样，CoreDNS 将在一个 ETCD 实例中存储 DNS 记录。这要求我们配置一个 CoreDNS 服务器，其中包括我们想要在其中注册名称的
    DNS 区域以及将存储记录的 ETCD 服务器。
- en: To keep resource requirements lower, we will use the included CoreDNS server
    that most Kubernetes installations include as part of their base cluster creation
    for our new domain. In the real world, you should deploy a dedicated CoreDNS server
    to handle just ExternalDNS registrations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持资源需求较低，我们将使用大多数 Kubernetes 安装中包含的 CoreDNS 服务器作为我们新域的基本集群创建的一部分。在实际环境中，您应该部署一个专用的
    CoreDNS 服务器来处理仅限于 ExternalDNS 注册的任务。
- en: At the end of this section, you will execute a script to deploy a fully configured
    ExternalDNS service that has all of the options and configurations discussed in
    this section. The commands used in this section are only for reference; you do
    not need to execute them on your cluster, since the script will do that for you.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节结束时，你将执行一个脚本来部署一个完全配置的ExternalDNS服务，其中包含本节中讨论的所有选项和配置。本节中使用的命令仅供参考；你不需要在集群中执行这些命令，因为脚本将为你完成这些操作。
- en: 'Before we can integrate CoreDNS, we need to know the IP address of our new
    ETCD service. You can retrieve the address by listing the services in the `etcd-dns`
    namespace using `kubectl`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以集成CoreDNS之前，我们需要知道新ETCD服务的IP地址。你可以通过使用`kubectl`列出`etcd-dns`命名空间中的服务来获取地址：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will show our ETCD service, along with the IP address of the service:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示我们的ETCD服务以及该服务的IP地址：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `Cluster-IP` you see in the service list will be used to configure the new
    DNS zone as a location to store the DNS records.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你在服务列表中看到的`Cluster-IP`将用于配置新的DNS区域，作为存储DNS记录的位置。
- en: 'When you deploy ExternalDNS, you can configure CoreDNS in one of two ways:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 部署ExternalDNS时，你可以通过两种方式配置CoreDNS：
- en: Add a zone to the Kubernetes-integrated CoreDNS service
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向Kubernetes集成的CoreDNS服务添加区域。
- en: Deploy a new CoreDNS service that will be used for ExternalDNS registrations
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署一个新的CoreDNS服务，该服务将用于ExternalDNS注册。
- en: For ease of testing, we will add a zone to the Kubernetes CoreDNS service. This
    requires us to edit the CoreDNS `ConfigMap` found in the `kube-sytem` namespace.
    When you execute the script at the end of this section, the modification will
    be done for you. It will add the section shown below in **bold** to the `ConfigMap`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于测试，我们将向Kubernetes CoreDNS服务添加一个区域。这需要我们编辑位于`kube-system`命名空间中的CoreDNS `ConfigMap`。当你执行本节末尾的脚本时，修改将自动完成，它会在`ConfigMap`中添加下方**加粗**的部分。
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The added lines configure a zone called `foowidgets.k8s` that is ETCD integrated.
    The first line that we added tells CoreDNS that the zone name, `foowidgets.com`,
    is integrated with an ETCD service.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 添加的行配置了一个名为`foowidgets.k8s`的区域，该区域集成了ETCD。我们添加的第一行告诉CoreDNS，该区域名称`foowidgets.com`已与ETCD服务集成。
- en: The next line, `stubzone`, tells CoreDNS to allow you to set up a DNS server
    as a “stub resolver” for a particular zone. As a stub resolver, this DNS server
    directly queries specific name servers for a zone’s information without the need
    for recursive resolution throughout the entire DNS hierarchy.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行`stubzone`告诉CoreDNS允许你将DNS服务器设置为某个区域的“存根解析器”。作为存根解析器，这个DNS服务器直接查询特定名称服务器的区域信息，而不需要在整个DNS层次结构中进行递归解析。
- en: The third addition is the `path /skydns` option, which may look confusing since
    it doesn’t mention CoreDNS. Even though the value is `skydns`, it is also the
    default path for CoreDNS integration as well.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第三项添加是`path /skydns`选项，这可能看起来有些混淆，因为它没有提到CoreDNS。尽管该值是`skydns`，但它也是CoreDNS集成的默认路径。
- en: Finally, the last line tells CoreDNS where to store the records. In our example,
    we have an ETCD service running, using IP address `10.96.149.223` on the default
    ETCD port of `2379`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行告诉CoreDNS将记录存储在哪里。在我们的示例中，我们运行了一个ETCD服务，使用IP地址`10.96.149.223`，并在默认的ETCD端口`2379`上运行。
- en: You could use the Service’s host name instead of the IP here. We used the IP
    to show the relationships between a pod and a Service, but the name `etcd-dns.etcd-dns.svc`
    would work as well. Which method you choose will depend on your situation. In
    our KinD cluster, we don’t really need to worry about losing the IP because the
    cluster is disposable. In the real world, you would want to use the host name
    to protect against the IP address changing.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里使用服务的主机名而不是IP。我们使用IP来展示Pod和服务之间的关系，但`etcd-dns.etcd-dns.svc`这个名称也可以使用。你选择哪种方式取决于你的具体情况。在我们的KinD集群中，我们不需要担心丢失IP，因为集群是可丢弃的。在实际应用中，你应该使用主机名来避免IP地址变化带来的问题。
- en: Now that you understand how to add an ETCD integrated zone in CoreDNS, the next
    step is to update the deployment options that ExternalDNS requires to integrate
    with CoreDNS.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你理解了如何在CoreDNS中添加ETCD集成的区域，下一步是更新ExternalDNS所需的部署选项，以便与CoreDNS集成。
- en: ExternalDNS configuration options
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ExternalDNS配置选项。
- en: ExternalDNS can be configured to register ingress or service objects. This is
    configured in the deployment file of ExternalDNS using the source field. The example
    below shows the options portion of the deployment that we will use in this chapter.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ExternalDNS可以配置为注册入口或服务对象。这在ExternalDNS的部署文件中使用source字段进行配置。以下示例显示了我们将在本章中使用的部署选项部分。
- en: We also need to configure the provider that ExternalDNS will use, and since
    we are using CoreDNS, we set the provider to `coredns`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要配置ExternalDNS将使用的提供程序，由于我们使用的是CoreDNS，因此我们将提供程序设置为`coredns`。
- en: 'The last option is the log level we want to set, which we set to `info` to
    keep our log files smaller and easier to read. The arguments that we will use
    are shown below:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的选项是我们想要设置的日志级别，我们将其设置为`info`，以使日志文件更小且更易于阅读。我们将使用的参数如下所示：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have gone over the ETCD options and deployment, how to configure
    a new zone to use ETCD, and the options to configure ExternalDNS to use CoreDNS
    as a provider, we can deploy ExternalDNS in our cluster.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经讲解了ETCD选项和部署，如何配置一个新的区域以使用ETCD，以及如何配置ExternalDNS使用CoreDNS作为提供程序，我们可以在集群中部署ExternalDNS了。
- en: We have included a script called `deploy-externaldns.sh` in the `chapter5/externaldns`
    folder. Execute the script in the directory to deploy ExternalDNS into your KinD
    cluster. When you execute the script, it will fully configure and deploy an integrated
    ExternalDNS with ETCD.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`chapter5/externaldns`文件夹中包含了一个名为`deploy-externaldns.sh`的脚本。在该目录下执行脚本，以将ExternalDNS部署到您的KinD集群中。当您执行该脚本时，它将完全配置并部署一个集成了ETCD的ExternalDNS。
- en: '**NOTE**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: 'If you see a warning when the script updates the `ConfigMap`, you can safely
    ignore it. Since our `kubectl` command is using `apply` to update the object,
    Kubernetes will look for a last-applied-configuration annotation, if there is
    one set. Since you likely do not have that in the existing object, you will see
    the warning that it’s missing. This is just a warning and will not stop the `ConfigMap`
    update, as you can confirm by looking at the last line of the `kubectl` update
    command, where it shows the `ConfigMap` was updated: `configmap/coredns configured`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在脚本更新`ConfigMap`时看到警告，您可以安全地忽略它。由于我们的`kubectl`命令使用`apply`来更新对象，Kubernetes会查找最后应用的配置注释（last-applied-configuration），如果有设置的话。由于您可能在现有对象中没有该注释，因此会看到缺失的警告。这只是一个警告，不会阻止`ConfigMap`的更新，您可以通过查看`kubectl`更新命令的最后一行来确认，那里显示`ConfigMap`已被更新：`configmap/coredns
    configured`
- en: Now, we have added the ability for developers to create dynamically registered
    DNS names for their services. Next, let’s see it in action by creating a new service
    that will register itself in our CoreDNS server.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经为开发人员添加了创建动态注册DNS名称的能力，接下来，让我们通过创建一个新的服务来看它的实际效果，该服务将自己注册到我们的CoreDNS服务器中。
- en: Creating a LoadBalancer service with ExternalDNS integration
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个与ExternalDNS集成的LoadBalancer服务
- en: 'Our ExternalDNS will watch all services for an annotation that contains the
    desired DNS name. This is just a single annotation using the format `annotation
    external-dns.alpha.kubernetes.io/hostname` with the value of the DNS name you
    want to register. For our example, we want to register a name of `nginx.foowidgets.k8s`,
    so we would add an annotation to our NGINX service: `external-dns.alpha.kubernetes.io/hostname:
    nginx.foowidgets.k8s`.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的ExternalDNS将监视所有服务，查找包含所需DNS名称的注释。这只是一个单一的注释，格式为`annotation external-dns.alpha.kubernetes.io/hostname`，值为您想要注册的DNS名称。以我们的示例为例，我们想要注册名称`nginx.foowidgets.k8s`，因此我们会在NGINX服务中添加一个注释：`external-dns.alpha.kubernetes.io/hostname:
    nginx.foowidgets.k8s`。'
- en: In the `chapter5/externaldns` directory, we have included a manifest that will
    deploy an NGINX web server using a `LoadBalancer` service that contains the annotation
    to register the DNS name.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在`chapter5/externaldns`目录下，我们包含了一个清单文件，该文件将通过一个包含注册DNS名称注释的`LoadBalancer`服务来部署NGINX
    Web服务器。
- en: 'Deploy the manifest using `kubectl create -f nginx-lb.yaml`, which will deploy
    the resources in the default namespace. The deployment is a standard NGINX deployment,
    but the service has the required annotation to tell the ExternalDNS service that
    you want to register a new DNS name. The manifest for the service is shown below,
    with the annotation in bold:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl create -f nginx-lb.yaml`部署清单，这将把资源部署到默认命名空间。该部署是一个标准的NGINX部署，但服务具有必要的注释，告诉ExternalDNS服务您希望注册一个新的DNS名称。服务的清单如下所示，注释部分为**粗体**：
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When ExternalDNS sees the annotation, it will register the requested name in
    the zone. The hostname from the annotation will log an entry in the ExternalDNS
    pod – the registration for our new entry, `nginx.foowidgets.k8s`, is shown below:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当ExternalDNS看到该注释时，它会在区域中注册请求的名称。来自注释的主机名将在ExternalDNS Pod中记录一条条目 – 我们的新条目`nginx.foowidgets.k8s`的注册信息如下所示：
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see in the last line of the log, the record was added as an A-record
    in the DNS server, pointing to the IP address `172.18.200.101`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在日志的最后一行看到的，记录已作为A记录添加到DNS服务器中，指向IP地址`172.18.200.101`。
- en: The last step for confirming that ExternalDNS is fully working is to test a
    connection to the application. Since we are using a KinD cluster, we must test
    this from a pod in the cluster. To provide the new names to external resources,
    we would need to configure our main DNS server(s) to forward requests for the
    `foowidgets.k8s` domain to our CoreDNS server. At the end of this section, we
    will show the steps to integrate a Windows DNS server, which could be any main
    DNS server on your network, with our Kubernetes CoreDNS server.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 确认ExternalDNS完全正常工作的最后一步是测试与应用程序的连接。由于我们使用的是KinD集群，因此必须从集群中的Pod进行测试。为了将新名称提供给外部资源，我们需要配置主DNS服务器，将对`foowidgets.k8s`域的请求转发到CoreDNS服务器。在本节末尾，我们将展示如何将Windows
    DNS服务器（这可以是您网络中的任何主DNS服务器）与Kubernetes CoreDNS服务器集成的步骤。
- en: Now we can test the NGINX deployment using the DNS name from our annotation.
    Since you aren’t using the CoreDNS server as your main DNS provider, we need to
    use a container in the cluster to test name resolution. There is a great utility
    called **Netshoot** that contains a number of useful troubleshooting tools; it’s
    a great tool to have in your toolbox to test and troubleshoot clusters and pods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用从注释中获得的DNS名称测试NGINX部署。由于您没有将CoreDNS服务器作为主DNS提供商，我们需要使用集群中的容器来测试名称解析。有一个很棒的工具叫做**Netshoot**，它包含许多有用的故障排除工具；这是一个很好的工具，可以用来测试和排查集群及Pod的问题。
- en: 'To run a Netshoot container, we can use the `kubectl run` command. We only
    need the pod to run when we are using it to run tests in the cluster, so we will
    tell the `kubectl run` command to run an interactive shell and to remove the pod
    after we exit. To run Netshoot, execute:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行Netshoot容器，我们可以使用`kubectl run`命令。我们只需要在使用它进行集群测试时运行Pod，因此我们将告诉`kubectl run`命令运行一个交互式Shell，并在退出后删除Pod。要运行Netshoot，请执行：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The pod may take a minute or two to become available, but once it starts up,
    you will see a `tmp-shell` prompt. At this prompt, we can use `nslookup` to verify
    that the DNS entry was successfully added. If you attempt to look up `nginx.foowidgets.k8s`,
    you should receive a reply with the IP address of the service.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Pod可能需要一两分钟才能变得可用，但一旦启动，您将看到`tmp-shell`提示符。在此提示符下，我们可以使用`nslookup`来验证DNS条目是否已成功添加。如果您尝试查找`nginx.foowidgets.k8s`，应该会收到该服务的IP地址作为回应。
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Your reply should look similar to the example below:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您的回复应类似于以下示例：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This confirms that our annotation was successful and ExternalDNS registered
    our hostname in the CoreDNS server.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这确认了我们的注释是成功的，并且ExternalDNS已在CoreDNS服务器中注册了我们的主机名。
- en: The `nslookup` only proves that there is an entry for `nginx.foowidgets.k8s`;
    it doesn’t test the application. To prove that we have a successful deployment
    that will work when someone enters the name in a browser, we can use the `curl`
    utility that is included with Netshoot.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`nslookup`仅能证明`nginx.foowidgets.k8s`有条目；它并没有测试应用程序。为了证明我们有一个成功的部署，并且当有人在浏览器中输入该名称时它能够工作，我们可以使用Netshoot中包含的`curl`工具。'
- en: '![](img/B21165_05_02.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_02.png)'
- en: 'Figure 5.2: curl test using the ExternalDNS name'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：使用ExternalDNS名称的curl测试
- en: The curl output confirms that we can use the dynamically created service name
    to access the NGINX web server.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: curl输出确认我们可以使用动态创建的服务名称访问NGINX Web服务器。
- en: We realize that some of these tests aren’t very exciting since you can’t test
    them using a standard browser. To allow CoreDNS to be used outside of the cluster,
    we need to integrate CoreDNS with your main DNS server, which needs to delegate
    ownership for the zone(s) that are in CoreDNS. When you delegate a zone, any request
    to your main DNS server for a host in the delegated zone will forward the request
    to the DNS server that contains the requested zone.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们意识到这些测试可能不太令人兴奋，因为无法通过标准浏览器进行测试。为了让CoreDNS能够在集群外部使用，我们需要将CoreDNS与您的主DNS服务器集成，这需要将CoreDNS中区域的所有权委托给主DNS服务器。当您委托一个区域时，任何针对主DNS服务器的请求，如果请求的主机位于该委托的区域中，都会将请求转发到包含该区域的DNS服务器。
- en: In the next section, we will integrate the CoreDNS running in our cluster with
    a Windows DNS server. While we are using Windows as our DNS server, the concepts
    for delegating a zone are similar between operating systems and DNS servers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将集成在集群中运行的 CoreDNS 与 Windows DNS 服务器。虽然我们使用 Windows 作为 DNS 服务器，但不同操作系统和
    DNS 服务器之间的区域委托概念是相似的。
- en: Integrating CoreDNS with an enterprise DNS server
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 CoreDNS 与企业 DNS 服务器集成
- en: This section will show you how to use a main DNS server to forward the name
    resolution of the `foowidgets.k8s` zone to a CoreDNS server running on a Kubernetes
    cluster.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将展示如何使用主 DNS 服务器将 `foowidgets.k8s` 区域的名称解析转发到运行在 Kubernetes 集群中的 CoreDNS 服务器。
- en: The steps provided here are an example of integrating an enterprise DNS server
    with a Kubernetes DNS service. Because of the external DNS requirement and additional
    setup, the steps are for reference and **should not be executed** on your KinD
    cluster.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供的步骤是将企业 DNS 服务器与 Kubernetes DNS 服务集成的示例。由于外部 DNS 的要求和额外的设置，这些步骤仅供参考，**不应在您的
    KinD 集群上执行**。
- en: To find a record in a delegated zone, the main DNS server uses a process known
    as a recursive query. A recursive query refers to a DNS inquiry initiated by a
    DNS resolver, acting on behalf of a user. Through the recursive query process,
    the DNS resolver assumes the task of reaching out to multiple DNS servers in a
    hierarchical pattern. Its objective is to find the authoritative DNS server for
    the requested domain and initiate the retrieval of the requested DNS record.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要在委托的区域中查找记录，主 DNS 服务器使用一种称为递归查询的过程。递归查询是由 DNS 解析器发起的 DNS 查询，代表用户进行操作。在递归查询过程中，DNS
    解析器承担了通过层级模式联系多个 DNS 服务器的任务。其目标是找到请求域的权威 DNS 服务器，并启动请求的 DNS 记录检索。
- en: The diagram below illustrates the flow of how DNS resolution is provided by
    delegating a zone to a CoreDNS server in an enterprise environment.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了通过将区域委托给 CoreDNS 服务器来提供 DNS 解析的流程，在企业环境中尤为适用。
- en: '![](img/B21165_05_03.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_03.png)'
- en: 'Figure 5.3: DNS delegation flow'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：DNS 委托流程
- en: The local client will look at its DNS cache for the name being requested.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本地客户端将查看其 DNS 缓存中的请求名称。
- en: If the name is not in the local cache, a request is made to the main DNS server
    for `nginx.foowidgets.k8s`.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果名称不在本地缓存中，将向主 DNS 服务器请求 `nginx.foowidgets.k8s`。
- en: The DNS server receives the query and looks at the zones it knows about. It
    finds the `foowidgets.k8s` zone and sees that the zone has been delegated to the
    CoreDNS running on `192.168.1.200`.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DNS 服务器接收到查询后，会查看它知道的区域。它找到 `foowidgets.k8s` 区域，并发现该区域已被委托给运行在 `192.168.1.200`
    上的 CoreDNS。
- en: The main DNS server sends the query to the delegated CoreDNS server.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主 DNS 服务器将查询发送到委托的 CoreDNS 服务器。
- en: CoreDNS looks for the name, `nginx`, in the `foowidgets.k8s` zone.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CoreDNS 在 `foowidgets.k8s` 区域中查找名称 `nginx`。
- en: CoreDNS sends the IP address for `foowidgets.k8s` back to the main DNS server.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CoreDNS 将 `foowidgets.k8s` 的 IP 地址返回给主 DNS 服务器。
- en: The main DNS server sends the reply containing the address for `nginx.foowidgets.k8s`
    to the client.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主 DNS 服务器将包含 `nginx.foowidgets.k8s` 地址的回复发送给客户端。
- en: The client connects to the NGINX server using the IP address returned from CoreDNS.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端通过从 CoreDNS 返回的 IP 地址连接到 NGINX 服务器。
- en: Let’s move on to a real-world example. For our scenario, the main DNS server
    is running on a Windows 2019 server and we will delegate a zone to a CoreDNS server.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续来看一个现实世界的例子。在我们的场景中，主 DNS 服务器运行在 Windows 2019 服务器上，我们将把一个区域委托给 CoreDNS
    服务器。
- en: 'The components deployed are as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的组件如下：
- en: Our network subnet is `10.2.1.0/24`
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的网络子网是 `10.2.1.0/24`
- en: Windows 2019 or higher server running DNS
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行 DNS 的 Windows 2019 或更高版本服务器
- en: A Kubernetes cluster
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Kubernetes 集群
- en: A MetalLB address pool with a range of `10.2.1.70`-`10.2.1.75`
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 MetalLB 地址池，范围为 `10.2.1.70`-`10.2.1.75`
- en: A CoreDNS instance deployed in a cluster using a `LoadBalancer` service assigned
    the IP address `10.2.1.74` from our IP pool
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `LoadBalancer` 服务部署的 CoreDNS 实例，分配了来自我们的 IP 池的 IP 地址 `10.2.1.74`
- en: Deployed add-ons, using the configuration from this chapter including ExternalDNS,
    an ETCD deployment for CoreDNS, and a new CoreDNS ETCD integrated zone
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署的附加组件，使用本章中的配置，包括 ExternalDNS、CoreDNS 的 ETCD 部署，以及一个新的 CoreDNS ETCD 集成区域
- en: Bitnami NGINX deployment to test the delegation
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bitnami NGINX 部署用于测试委托
- en: Now, let’s go through the configuration steps to integrate our DNS servers.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过配置步骤来集成我们的 DNS 服务器。
- en: Exposing CoreDNS to external requests
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 CoreDNS 暴露给外部请求
- en: We have already covered how to deploy most of the resources that you need to
    integrate – ETCD, ExternalDNS, and configuring CoreDNS with a new zone that is
    ETCD-integrated. To provide external access to CoreDNS, we need to create a new
    service that exposes CoreDNS on TCP and UDP port `53`. A complete service manifest
    is shown below.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There is one new option in the service that we haven’t discussed yet – we have
    added the `spec.loadBalancerIP` to our deployment. This option allows you to assign
    an IP address to the service so it will have a stable IP address, even if the
    service is recreated. We need a static IP since we need to enable forwarding from
    our main DNS server to the CoreDNS server in the Kubernetes cluster.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Once CoreDNS is exposed using a `LoadBalancer` on port `53`, we can configure
    the main DNS server to forward requests for hosts in the `foowidgets.k8s` domain
    to our CoreDNS server.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the primary DNS server
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing to do on our main DNS server is to create a conditional forwarder
    to the node running the CoreDNS pod.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'On the Windows DNS host, we need to create a new conditional forwarder for
    `foowidgets.k8s` pointing to the IP address that we assigned to the new CoreDNS
    service. In our example, the CoreDNS service has been assigned to the host `10.2.1.74`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_04.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Windows conditional forwarder setup'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: This configures the Windows DNS server to forward any request for a host in
    the `foowidgets.k8s` domain to the CoreDNS service running on IP address `10.2.1.74`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Testing DNS forwarding to CoreDNS
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To test the configuration, we will use a workstation on the main network that
    has been configured to use the Windows DNS server.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'The first test we will run is an `nslookup` of the NGINX record that was created
    by the MetalLB annotation:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'From Command Prompt, we execute an `nslookup nginx.foowidgets.k8s` command:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Since the query returned the IP address we expected for the record, we can confirm
    that the Windows DNS server is forwarding requests to CoreDNS correctly.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: We can do one more additional NGINX test from the laptop’s browser. In Chrome,
    we can use the URL registered in CoreDNS, `nginx.foowidgets.k8s`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_05.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Success browsing from an external workstation using CoreDNS'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: One test confirms that the forwarding works, but we want to create an additional
    deployment to verify the system is fully working.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: To test a new service, we deploy a different NGINX server called microbot, with
    a service that has an annotation assigning the name `microbot.foowidgets.k8s`.
    MetalLB has assigned the service the IP address of `10.2.1.65`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'Like our previous test, we test the name resolution using `nslookup`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To confirm that the web server is running correctly, we browse to the URL from
    a workstation:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Successful browsing from an external workstation using CoreDNS
    ](img/B21165_05_06.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Successful browsing from an external workstation using CoreDNS'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Success! We have now integrated an enterprise DNS server with a CoreDNS server
    running on a Kubernetes cluster. This integration provides users with the ability
    to register service names dynamically by simply adding an annotation to the service.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们现在已将企业DNS服务器与运行在Kubernetes集群上的CoreDNS服务器集成。这一集成使用户能够通过简单地向服务添加注释来动态注册服务名称。
- en: Load balancing between multiple clusters
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨多个集群的负载均衡
- en: There are various ways to configure running services in multiple clusters, often
    involving complex and costly add-ons like global load balancers. A global load
    balancer can be thought of as a traffic cop – it knows how to direct incoming
    traffic between multiple endpoints. At a high level, you can create a new DNS
    entry that the global load balancer will control. This new entry will have backend
    systems added to the endpoint list and based on factors like health, connections,
    or bandwidth, it will direct the traffic to the endpoint nodes. If an endpoint
    is unavailable for any reason, the load balancer will remove it from the endpoint
    list. By removing it from the list, traffic will only be sent to healthy nodes,
    providing a smooth end user experience. There’s nothing worse for a customer than
    getting a website not found error when they attempt to access a site.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 配置多个集群中运行的服务有多种方法，通常涉及复杂且昂贵的附加组件，如全局负载均衡器。全局负载均衡器可以看作是一个交通警察——它知道如何在多个端点之间指引传入流量。从高层次来看，你可以创建一个新的DNS条目，供全局负载均衡器控制。这个新的条目将有后端系统被添加到端点列表中，并根据健康状况、连接数或带宽等因素，它将把流量指向这些端点节点。如果某个端点因任何原因不可用，负载均衡器将把它从端点列表中移除。通过将其从列表中移除，流量只会发送到健康的节点，从而提供流畅的终端用户体验。对客户来说，最糟糕的情况就是当他们尝试访问网站时，遇到“网站未找到”的错误。
- en: '![](img/B21165_05_07.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_07.png)'
- en: 'Figure 5.7: Global load balancing traffic flow'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：全局负载均衡流量流程
- en: The figure above shows a healthy workflow where both clusters are running an
    application that we are load balancing between the clusters. When the request
    hits the load balancer, it will send the traffic in a round-robin fashion between
    the two clusters. The `nginx.foowidgets.k8s` request will ultimately send the
    traffic to either `nginx.clustera.k8s` or `nginx.clusterb.k8s`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了一个健康的工作流，其中两个集群都在运行我们进行负载均衡的应用程序。当请求到达负载均衡器时，它将以轮询方式在两个集群之间分配流量。`nginx.foowidgets.k8s`请求最终将流量发送到`nginx.clustera.k8s`或`nginx.clusterb.k8s`。
- en: In *Figure 5.8*, we have a failure of our NGINX workload in cluster B. Since
    the global load balancer has a health check on the running workloads, it will
    remove the endpoint in cluster B from the `nginx.foowidgets.k8s` entry.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.8*中，我们看到集群B中的NGINX工作负载发生故障。由于全局负载均衡器对正在运行的工作负载进行了健康检查，它将从`nginx.foowidgets.k8s`条目中移除集群B的端点。
- en: '![](img/B21165_05_08.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_08.png)'
- en: 'Figure 5.8: Global load balancing traffic flow with a site failure'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8：全局负载均衡流量流程（带站点故障）
- en: Now, for any traffic that comes into the load balancer requesting `nginx.foowidgets.k8s`,
    the only endpoint that will be used for traffic is running on cluster A. Once
    the issue has been resolved on cluster B, the load balancer will automatically
    add the cluster B endpoint back into the `nginx.foowidgets.k8s` record.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于任何请求`nginx.foowidgets.k8s`进入负载均衡器的流量，唯一会被用于流量的端点是在集群A上运行的。一旦集群B的问题得到解决，负载均衡器将自动将集群B的端点重新添加到`nginx.foowidgets.k8s`记录中。
- en: Such solutions are widespread in enterprises, with many organizations utilizing
    products from companies like **F5**, **Citrix**, **Kemp**, and **A10**, as well
    as CSP-native solutions like **Route 53** and **Traffic Director**, to manage
    workloads across multiple clusters. However, there are projects with similar functionality
    that integrate with Kubernetes, and they come at little to no cost. While these
    projects may not offer all the features of some vendor solutions, they often meet
    the needs of most use cases without requiring the full spectrum of expensive features.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的解决方案在企业中广泛应用，许多组织利用**F5**、**Citrix**、**Kemp**和**A10**等公司提供的产品，以及像**Route
    53**和**Traffic Director**这样的CSP本地解决方案来管理跨多个集群的工作负载。然而，也有一些与Kubernetes集成的具有类似功能的项目，而且这些项目几乎不需要成本。虽然这些项目可能没有一些供应商解决方案的所有功能，但它们通常能够满足大多数用例的需求，而无需完整的昂贵功能。
- en: One such project is **K8GB**, an innovative open-source project that brings
    **Global Server Load Balancing** (**GSLB**) to Kubernetes. With K8GB, organizations
    can easily distribute incoming network traffic across multiple Kubernetes clusters
    located in different geographical locations. By intelligently routing requests,
    K8GB guarantees low latency, optimal response times, and redundancy, providing
    an exceptional solution to any enterprise.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: This section will introduce you to K8GB, but if you want to learn more about
    the project, browse to the project’s main page at [https://www.k8gb.io](https://www.k8gb.io).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Since we are using KinD and a single host for our cluster, this section of the
    book is meant to introduce you to the project and the benefits it provides. This
    section is for reference only, since it is a complex topic that requires multiple
    components, some of which are outside of Kubernetes. If you decide you want to
    implement a solution for yourself, we have included example documentation and
    scripts in the book’s repo under the `chapter5/k8gs-example` directory.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: K8GB is a CNCF sandbox project, which means it is in its early stages and any
    newer version after the writing of this chapter may have changes to objects and
    configurations.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Kubernetes Global Balancer
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why should you care about a project like K8GB?
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider an internal enterprise cloud as an example, operating a Kubernetes
    cluster at a production site alongside another cluster at a disaster recovery
    site. To ensure a smooth user experience, it is important to enable applications
    to transition seamlessly between these data centers, without requiring any manual
    intervention during disaster recovery events. The challenge lies in fulfilling
    the enterprise’s demand for high availability of microservices when multiple clusters
    are simultaneously serving these applications. We need to effectively address
    the need for continuous and uninterrupted service across geographically dispersed
    Kubernetes clusters.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: This is where **K8GB** comes in.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'What makes K8GB an ideal solution for addressing our high availability requirements?
    As documented on their site, the key features include the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing is provided using a timeproof DNS protocol that is extremely
    reliable and works well for global deployments
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no requirement for a management cluster
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no single point of failure
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses native Kubernetes health checks for load balancing decisions
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring is as simple as a single Kubernetes CRD
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It works with any Kubernetes cluster – on-prem or off-prem
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s free!
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you will see in this section, K8GB provides an easy and intuitive configuration
    that makes providing global load balancing to your organization easy. This may
    make K8GB look like it doesn’t do very much, but behind the scenes, it provides
    a number of advanced features, including:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '**Global Load Balancing**: Facilitates the distribution of incoming network
    traffic among multiple Kubernetes clusters located in different geographic regions.
    As a result, it enables optimized application delivery, ensuring reduced latency
    and improved user experience for users.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intelligent Traffic Routing**: Utilizing sophisticated routing algorithms
    to intelligently steer client requests towards the closest or most appropriate
    Kubernetes cluster, considering factors like proximity, server health, and application-specific
    rules. This approach ensures efficient and highly responsive traffic management
    for optimal application performance.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High Availability and Redundancy**: Guarantees high availability and fault
    tolerance for applications by automatically redirecting traffic in the event of
    cluster, application, or data center failure. This failover mechanism minimizes
    downtime during disaster recovery scenarios, ensuring uninterrupted service delivery
    to users.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated Failover**: Simplifies operations by enabling automatic failover
    between data centers without the need for manual intervention. This eliminates
    the requirement for human-triggered **Disaster Recovery** (**DR**) events or tasks,
    ensuring quick and uninterrupted service delivery and streamlined operations.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with Kubernetes**: Offers seamless integration with Kubernetes,
    simplifying the setup and configuration of GSLB for applications deployed on clusters.
    Leveraging Kubernetes’ native capabilities, K8GB delivers a scalable solution,
    enhancing the overall management and efficiency of global load balancing operations.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**On-Prem and Cloud Provider Support**: Provides enterprises a way to efficiently
    manage GSLB for multiple Kubernetes clusters, enabling seamless handling of complex
    multi-region deployments and hybrid cloud scenarios. This ensures optimized application
    delivery across different environments, enhancing the overall performance and
    resilience of the infrastructure.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customization and Flexibility**: Provides users the freedom to define personalized
    rules and policies for traffic routing, providing organizations with the flexibility
    to customize GSLB configurations to meet their unique requirements precisely.
    This empowers enterprises to optimize traffic management based on their specific
    needs and ensures seamless adaptation to ever-changing application demands.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring, Metrics, and Tracing**: Includes monitoring, metrics, and tracing
    capabilities, enabling administrators to access insights into traffic patterns,
    health, and performance metrics spanning across multiple clusters. This provides
    enhanced visibility, empowering administrators to make informed decisions and
    optimize the overall performance and reliability of the GSLB setup.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have discussed the key features of K8GB, let’s get into the details.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Requirements for K8GB
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For a product that provides a complex function like global load balancing,
    K8GB doesn’t require a lot of infrastructure or resources to provide load balancing
    to your clusters. The latest release, which, as of this chapter’s creation, is
    `0.12.2` – has only a handful of requirements:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: The CoreDNS servers Load Balancer IP address in the main DNS zone using the
    naming standard `gslb-ns-<k8gb-name>-gb.foowidgets.k8s` – for example, `gslb-ns-us-nyc-gb.foowidgets.k8s
    and gslb-ns-us-buf-gb.foowidgets.k8s`
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are using K8GB with a service like **Route 53**, **Infoblox**, or **NS1**,
    the CoreDNS servers will be added to the domain automatically. Since our example
    is using an on-premises DNS server running on a Windows 2019 server, we need to
    create the records manually.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: An Ingress controller
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A K8GB controller deployed in the cluster, which will deploy:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The K8GB controller
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A CoreDNS server with the CoreDNS CRD plugin configured – this is included in
    the deployment on K8GB
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since we have already explored NGINX ingress controllers in previous chapters,
    we now turn our attention to the additional requirements: deploying and configuring
    the K8GB controller within a cluster.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss the steps to implement K8GB.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Deploying K8GB to a cluster
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have included example files in the GitHub repo under the `chapter5/k8gb-example`
    directory. The scripts are based on the example we will use for the remainder
    of the chapter. If you decide to use the files in a development cluster, you will
    need to meet the requirements below:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Two Kubernetes clusters (a single-node `kubeadm` cluster for each cluster will
    work)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreDNS deployed in each cluster
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K8GB deployed in each cluster
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An edge DNS server that you can use to delegate the domain for K8GB
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing K8GB has been made very simple – you only need to deploy a single
    Helm chart using a `values.yaml` file that has been configured for your infrastructure.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'To install K8GB, you will need to add the K8GB repository to your Helm repo
    list and then update the charts:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Before we execute a `helm install` command, we need to customize the Helm `values.yaml`
    file for each cluster deployment. We have included a values file for both of the
    clusters used in our example, `k8gb-buff-values.yaml` and `k8gb-nyc-values.yaml`,
    located in the `chapter5/k8gb-example` directory. The options in the values file
    will be discussed in the *Customizing the Helm chart values* section.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Understanding K8GB load balancing options
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our example, we will configure K8GB as a failover load balancer between
    two on-premises clusters; however, K8GB is not limited to just failover. Like
    most load balancers, K8GB offers a variety of solutions that can be configured
    differently for each load-balanced URL. It offers the most commonly required strategies,
    including round robin, weighted round robin, failover, and GeoIP.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the strategies is described below:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '**Round Robin**: If you do not specify a strategy, it will default to a simple
    round-robin load balancing configuration. Using round robin means that requests
    will be split between the configured clusters – request 1 will go to cluster 1,
    request 2 will go to cluster 2, request 3 will go to cluster 1, request 4 will
    go to cluster 2, and so on.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted Round Robin**: Similar to round robin, this strategy provides the
    ability to specify the percentage of traffic to send to a cluster; for example,
    75% of traffic will go to cluster 1 and 15% will go to cluster 2.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failover**: All traffic will go to the primary cluster unless all pods for
    a deployment become unavailable. If all pods are down in cluster 1, cluster 2
    will take over the workload until the pods in cluster 1 become available again,
    which will then become the primary cluster again.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GeoIP:** Directs requests to the closest cluster to the client connection.
    If the closest host is down, it will use a different cluster similar to how the
    failover strategy works. To use this strategy, you will need to create a GeoIP
    database (an example can be found here: [https://github.com/k8gb-io/coredns-crd-plugin/tree/main/terratest/geogen](https://github.com/k8gb-io/coredns-crd-plugin/tree/main/terratest/geogen)),
    and your DNS server needs to support the **EDNS0** extension.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EDNS0** is based on RFC 2671, which outlines how EDNS0 works and its various
    components, including the format of EDNS0-enabled DNS messages, the structure
    of EDNS0 options, and guidelines for its implementation. The goal of RFC 2671
    is to provide a standardized approach for extending the capabilities of the DNS
    protocol beyond its original limitations, allowing for the incorporation of new
    features, options, and enhancements'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you know the available strategies, let’s go over our example infrastructure
    for our clusters:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '| **Cluster/Server Details** | **Details** |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| Corporate DNS Server – New York CityIP: `10.2.1.14` | Main corporate zone`foowidgets.k8s`Host
    records for the CoreDNS servers`gslb-ns-us-nyc-gb.foowidgets.k8s``gslb-ns-us-buf-gb.foowidgets.k8s`Global
    domain configured delegating to the CoreDNS servers in the clusters`gb.foowidgets.k8s`
    |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| `New York City`, New York – Cluster 1Primary SiteCoreDNS LB IP: `10.2.1.221`Ingress
    IP: `10.2.1.98` | NGINX Ingress Controller exposed using HostPortCoreDNS deployment
    exposed using MetalLB |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| `Buffalo`, New York – Cluster 2Secondary SiteCoreDNS LB IP: `10.2.1.224`Ingress
    IP: `10.2.1.167` | NGINX Ingress Controller exposed using HostPortCoreDNS deployment
    exposed using MetalLB |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: 'Table 5.1: Cluster details'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: We will use the details from the above table to explain how we would deploy
    K8GB in our example infrastructure.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: With the details of the infrastructure, we can now create our Helm `values.yaml`
    files for each deployment. In the next section, we will show the values we need
    to configure using the example infrastructure, explaining each value.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the Helm chart values
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Each cluster will have a similar values file; the main changes will be the
    tag values we use. The values file below is for the New York City cluster:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The same file contents will be used for the NYC cluster, with the exception
    of the `clusterGeoTag` and `extGslbClustersGeoTags` values, for the NYC cluster
    they need to be set to:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, the configuration isn’t very lengthy, requiring only a handful
    of options to configure a usually complex global load balancing configuration.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s go over some of the main details of the values we are using.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The main details that we will explain are the values in the K8GB section, which
    configures all of the options K8GB will use for load balancing.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '| **Chart Value** | **Description** |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| `dnsZone` | This is the DNS zone that you will use for K8GB – basically,
    this is the zone that will be used for the DNS records that will be used to store
    our global load balanced DNS records. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| `edgeDNSZone` | The main DNS zone that contains the DNS records for the CoreDNS
    servers that are used by the previous option (`dnsZone`). |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| `edgeDNSServers` | The edge DNS server – usually the main DNS server used
    for name resolution. |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| `clusterGeoTag` | If you have multiple K8GB controllers, this tag is used
    to specify instances between each other. In our example, we set these to `us-buf`
    and `us-nyc` for our clusters. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| `extGslbClusterGeoTags` | Specifies the other K8GB controllers to pair with.
    In our example, each cluster adds the other cluster `clusterGeoTags` – the `Buffalo`
    cluster adds the `us-nyc` tag and the NYC cluster adds the `us-buf` tag. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| `isClusterService` | Set to `true` or `false`. Used for service upgrades;
    you can read more at [https://www.k8gb.io/docs/service_upgrade.xhtml](https://www.k8gb.io/docs/service_upgrade.xhtml).
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| `exposeCoreDNS` | If set to `true`, a `LoadBalancer` service will be created,
    exposing the CoreDNS deployed in the `k8gb` namespace on port `53`/UDP for external
    access. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| `deployment.skipConfig` | Set to true or false. Setting it to false tells
    the deployment to use the CoreDNS shipped with K8GB. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| `image.repository` | Configures the repository to use for the CoreDNS image.
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| `image.tag` | Configures the tag to use when pulling the image. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| `serviceAccount.create` | Set to `true` or `false`. When set to true, a service
    account will be created. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| `serviceAccount.name` | Sets the name of the service account from the previous
    option. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| `serviceType` | Configures the service type for CoreDNS. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: 'Table 5.2: K8GB options'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Using Helm to install K8GB
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the overview of K8GB and the Helm values file complete, we can move on
    to installing K8GB in the clusters. We have included scripts to deploy K8GB to
    the `Buffalo` and `NYC` clusters. In `chapter5/k8gb-example/k8gb`, you will see
    two scripts, deploy-`k8gb-buf.sh` and `deploy-k8gb-nyc.sh` – these should be run
    in their corresponding clusters.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'The script will execute the following steps:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Add the K8GB Helm repo to the server’s repo list
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the repos
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy K8GB using the appropriate Helm values file
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a **Gslb record** (covered in the next section)
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a deployment to use for testing in a namespace called `demo`
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once deployed, you will see two pods running in the `k8gb` namespace, one for
    the `k8gb` controller and the other for the CoreDNS server that will be used to
    resolve load balancing names:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can also verify that the services were created to handle the incoming DNS
    requests. Since we exposed it using a `LoadBalancer` type, we will see the `LoadBalancer`
    service on port `53` using the UDP protocol:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With the deployment of K8GB complete and verified in both clusters, let’s move
    on to the next section where we will explain how to create our edge delegation
    for our load balanced zone.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Delegating our load balancing zone
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our example, we are using a Windows server as our edge DNS server, which
    is where our K8s DNS names will be registered. On the DNS side, we need to add
    two DNS records for our CoreDNS servers, and then we need to delegate the load
    balancing zone to these servers.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'The two A records need to be in the main edge DNS zone. In our example, that
    is the `foowidgets.k8s` zone. In this zone, we need to add two entries for the
    CoreDNS servers that are exposed using a `LoadBalancer` service:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_09.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Adding our CoreDNS servers to the edge zone'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to create a new delegated zone that will be used for our load
    balanced service names. In Windows, this is done by *right-clicking* the zone
    and selecting **New Delegation**; in the delegation wizard, you will be asked
    for the **Delegated domain**. In our example, we are going to delegate the **gb**
    domain as our domain.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_10.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Creating a new delegated zone'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: After you enter the zone name and click **Next**, you will see a new screen
    to add DNS servers for the delegated domain; when you click **Add**, you will
    enter the DNS names for your CoreDNS servers. Remember that we created two A records
    in the main domain, `foowidgets.com`. As you add entries, Windows will verify
    that the entered name resolves correctly and that DNS queries work. Once you add
    both CoreDNS servers, the summary screen will show both with their IP addresses.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_05_11.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: Adding the DNS names for the CoreDNS servers'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: That completes the edge server configuration. For certain edge servers, K8GB
    will create the delegation records, but there are only a handful of servers that
    are compatible with that feature. For any edge server that doesn’t auto create
    the delegated servers, you need to create them manually like we did in this section.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have CoreDNS in our clusters and we have delegated the load balanced
    zone, we will deploy an application that has global load balancing to test our
    configuration.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a highly available application using K8GB
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two methods to enable global load balancing for an application. You
    can create a new record using a custom resource provided by K8GB, or you can annotate
    an Ingress rule. For our demonstration of K8GB, we will deploy a simple NGINX
    web server in a cluster and add it to K8GB using the natively supplied custom
    resource.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Adding an application to K8GB using custom resources
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we deployed K8GB, a new **Custom Resource Definition** (**CRD**) named
    `Gslb` was added to the cluster. This CRD assumes the role of managing applications
    marked for global load balancing. Within the `Gslb` object, we define a specification
    for the Ingress name, mirroring the format of a regular Ingress object. The sole
    distinction between a standard Ingress and a `Gslb` object lies in the last portion
    of the manifest, the strategy.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: The strategy defines the type of load balancing we want to use, which is failover
    for our example, and the primary GeoTag to use for the object. In our example,
    the NYC cluster is our primary cluster, so our `Gslb` object will be set to `us-buf`.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy an application that will leverage load balancing, we need to create
    the following in both clusters:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: A standard deployment and service for the application. We will call the deployment
    `nginx`, using the standard NGINX image.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A `Gslb` object in each cluster. For our example, we will use the manifest
    below, which will declare the Ingress rule and set the strategy to failover using
    `us-buf` as the primary K8GB. Since the `Gslb` object has the information for
    the Ingress rule, you do not need to create an Ingress rule; `Gslb` will create
    the Ingress object for us. Let’s look at an example below:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: When you deploy the manifest for the `Gslb` object, it will create two Kubernetes
    objects, the `Gslb` object and an Ingress object.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'If we looked at the `demo` namespace for the `Gslb` objects in the `Buffalo`
    cluster, we would see the following:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And if we looked at the Ingress objects in the NYC cluster, we would see:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We would also have similar objects in the NYC cluster, which we will explain
    in the *Understanding how K8GB provides global load balancing* section.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Adding an application to K8GB using Ingress annotations
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second method for adding an application to K8GB is to add two annotations
    to a standard Ingress rule, which was primarily added to allow developers to add
    an existing Ingress rule to K8GB.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'To add an Ingress object to the global load balancing list, you only need to
    add two annotations to the Ingress object, `strategy` and `primary-geotag`. An
    example of the annotations is shown below:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This would add the Ingress to K8GB using the failover strategy using the `us-buf`
    GeoTag as the primary tag.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have deployed all of the required infrastructure components and
    all of the required objects to enable global load balancing for an application,
    let’s see it in action.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how K8GB provides global load balancing
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The design of K8GB is complex, but once you deploy an application and understand
    how K8GB maintains zone files, it will become easier. This is a fairly complex
    topic, and it does assume some previous knowledge of how DNS works, but by the
    end of this section, you should be able to explain how K8GB works.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Keeping the K8GB CoreDNS servers in sync
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first topic to discuss is how K8GB manages to keep two, or more, zone files
    in sync to provide seamless failover for our deployments. Seamless failover is
    a process that ensures an application continues to run smoothly even during system
    issues or failures. It automatically transitions to the backup system or resource,
    maintaining an uninterrupted user experience.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, each K8GB CoreDNS server in the clusters must have
    an entry in the main DNS server.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the DNS server and zone that we configured for the edge values in the
    `values.yaml` file:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'So, in the edge DNS server (`10.2.1.14`), we have a host record for each CoreDNS
    server using the required K8GB naming convention:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: K8GB will communicate between all of the CoreDNS servers and update any records
    that need to be updated due to being added, deleted, or updated.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'This becomes a little easier to understand with an example. Using our cluster
    example, we have deployed an NGINX web server and created all of the required
    objects in both clusters. After deploying, we would have a `Gslb` and Ingress
    object in each cluster, as shown below:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '| Cluster: NYCDeployment: `nginx``Gslb: gslb-failover-nyc`Ingress: `fe.gb.foowidgets.k8s`NGINX
    Ingress IP: `10.2.1.98` | Cluster: `Buffalo` (Primary)Deployment: `nginx``Gslb:
    gslb-failover-buf`Ingress: `fe.gb.foowidgets.k8s`NGINX Ingress IP: `10.2.1.167`
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: 'Table 5.3: Objects in each cluster'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the deployment is healthy in both clusters, the CoreDNS servers will
    have a record for `fe.gb.foowidgets.k8s` with an IP address of `10.2.1.167`, the
    primary deployment. We can verify this by running a `dig` command on any client
    machine that uses the edge DNS server (`10.2.1.14`):'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As you can see in the output from `dig`, the host resolved to `10.2.1.167`
    since the application is healthy in the primary cluster. If we curl the DNS name,
    we will see that the NGINX server in `Buffalo` replies:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We will simulate a failure by scaling the replicas for the deployment in the
    `Buffalo` cluster to `0`, which will look like a failed application to K8GB. When
    the K8GB controller in the NYC cluster sees that the application no longer has
    any healthy endpoints, it will update the CoreDNS record in all servers with the
    secondary IP address to fail the service over to the secondary cluster.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'Once scaled down, we can use `dig` to verify what host is returned:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will `curl` again to verify that the workload has been moved to the NYC
    cluster. When we execute curl, we will see that the NGINX server is now located
    in the NYC cluster:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that the IP address returned is now the IP address for the deployment in
    the `Buffalo` cluster, the secondary cluster, `10.2.1.98`. This proves that K8GB
    is working correctly and providing us with a Kubernetes-controlled global load
    balancer.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the application becomes healthy in the primary cluster, K8GB will update
    CoreDNS and any requests will resolve to the main cluster again. To test this,
    we scaled the deployment in `Buffalo` back up to `1` and ran another dig test:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We can see that the IP has been updated to reflect the NYC Ingress controller
    on address `10.2.1.167`, the primary location.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a last curl to verify that the workload is being serviced out of the
    `Buffalo` cluster:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: K8GB is a unique, and impressive, project from the CNCF that offers global load
    balancing similar to what other, more expensive, products offer today.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: It’s a project that we are watching carefully, and if you need to deploy applications
    across multiple clusters, you should consider looking into the K8GB project as
    it matures.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to provide automatic DNS registration to any
    service that uses a `LoadBalancer` service. You also learned how to deploy a highly
    available service using the CNCF project, K8GB, which provides global load balancing
    to a Kubernetes cluster.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: These projects have become integral to numerous enterprises, offering users
    capabilities that previously required the efforts of multiple teams and, often,
    extensive paperwork, to deliver applications to customers. Now, your teams can
    swiftly deploy and update applications using standard agile practices, providing
    your organization with a competitive advantage.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *Integrating Authentication into Your Cluster*, we will
    explore the best methods and practices for implementing secure authentication
    in Kubernetes. You will learn how to integrate enterprise authentication using
    the OpenID Connect protocol and how to use Kubernetes impersonation. We will also
    discuss the challenges of managing credentials in a cluster and offer practical
    solutions for authenticating users and pipelines.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes does not support using both TCP and UDP with services.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: b'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: ExternalDNS only integrates with CoreDNS.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: b'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: What do you need to configure on your edge DNS server for K8GB to provide load
    balancing to a domain?
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nothing, it works without additional configuration
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It must point to a cloud-provided DNS server
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: You must delegate a zone that points to your cluster IP
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a delegation to your CoreDNS instances
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: d'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: What strategy is not supported by K8GB?
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Failover
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Round robin
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Random distribution
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: GeoIP
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: c'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask Me Anything* session with
    the authors:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/K8EntGuide](https://packt.link/K8EntGuide)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code965214276169525265.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
