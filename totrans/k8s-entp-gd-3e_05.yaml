- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: External DNS and Global Load Balancing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部DNS和全局负载均衡
- en: In this chapter, we will build on what you learned in *Chapter 4*. We will discuss
    some of the limitations of certain load balancer features and how we can configure
    a cluster to resolve those limitations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在*第4章*的基础上进行讲解。我们将讨论一些负载均衡器功能的局限性，以及如何配置集群来解决这些局限性。
- en: We know that Kubernetes has a built-in DNS server that dynamically allocates
    names to resources. These are used for applications to communicate intra-cluster,
    or within the cluster. While this feature is beneficial for internal cluster communication,
    it doesn’t provide DNS resolution for external workloads. Since it does provide
    DNS resolution, why do we say it has limitations?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道Kubernetes有一个内置的DNS服务器，它动态地为资源分配名称。这些名称用于应用程序在集群内部进行通信。虽然这个功能对集群内部通信有利，但它并不为外部工作负载提供DNS解析。既然它提供DNS解析，为什么我们还说它有局限性呢？
- en: In the previous chapter, we used a dynamically assigned IP address to test our
    `LoadBalancer` service workloads. While our examples have been good for learning,
    in an enterprise, nobody wants to access a workload running on a cluster using
    an IP address. To address this limitation, the Kubernetes SIG has developed a
    project called **ExternalDNS**, which provides the ability to dynamically create
    DNS entries for our `LoadBalancer` services.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用动态分配的IP地址来测试我们的`LoadBalancer`服务工作负载。虽然我们的示例对于学习很有帮助，但在企业环境中，没人想通过IP地址访问运行在集群上的工作负载。为了解决这一限制，Kubernetes
    SIG开发了一个名为**ExternalDNS**的项目，它提供了动态为我们的`LoadBalancer`服务创建DNS条目的功能。
- en: Also, in an enterprise, you will commonly run services that run on multiple
    clusters to provide failover for your applications. So far, the options we have
    discussed can’t address failover scenarios. In this chapter, we will explain how
    to implement a solution to provide an automated failover for workloads, making
    them highly available across multiple clusters.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在企业环境中，您通常会运行多个集群上的服务，以为您的应用提供故障转移。到目前为止，我们讨论的选项无法处理故障转移场景。在本章中，我们将解释如何实现一个解决方案，为工作负载提供自动故障转移，使其在多个集群之间高度可用。
- en: 'In this chapter, you will learn the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章您将学习以下内容：
- en: An introduction to external DNS resolution and global load balancing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部DNS解析和全局负载均衡简介
- en: Configuring and deploying ExternalDNS in a Kubernetes cluster
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes集群中配置和部署ExternalDNS
- en: Automating DNS name registration
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化DNS名称注册
- en: Integrating ExternalDNS with an enterprise DNS server
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将ExternalDNS与企业DNS服务器集成
- en: Using GSLB to offer global load balancing across multiple clusters
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GSLB提供跨多个集群的全局负载均衡
- en: Now, let’s jump into the chapter!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入这一章的内容！
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has the following technical requirements:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章具有以下技术要求：
- en: An Ubuntu 22.04+ server running Docker with a minimum of 4 GB of RAM, though
    8 GB is recommended
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行Docker的Ubuntu 22.04+服务器，至少需要4GB的内存，推荐8GB
- en: A KinD cluster running **MetalLB** – if you completed *Chapter 4*, you should
    already have a cluster running MetalLB
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行**MetalLB**的KinD集群——如果您已完成*第4章*，那么您应该已经拥有一个运行MetalLB的集群
- en: 'The scripts from the `chapter5` folder from the repository, which you can access
    by going to this book’s GitHub repository: [https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仓库中`chapter5`文件夹中的脚本，您可以通过访问本书的GitHub仓库来获取：[https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)
- en: Making service names available externally
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使服务名称对外可用
- en: As we mentioned in the introduction, you may remember that we used IP addresses
    to test the `LoadBalancer` services we created, whereas for our `Ingress` examples,
    we used domain names. Why did we have to use IP addresses instead of a hostname
    for our `LoadBalancer` services?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在简介中提到的，您可能还记得我们曾使用IP地址来测试我们创建的`LoadBalancer`服务，而对于我们的`Ingress`示例，我们使用了域名。为什么我们必须使用IP地址而不是主机名来访问我们的`LoadBalancer`服务呢？
- en: Although a Kubernetes load balancer assigns a standard IP address to a service,
    it doesn’t automatically generate a DNS name for workloads to access the service.
    Instead, you must rely on IP addresses to connect to applications within the cluster,
    which becomes confusing and inefficient. Furthermore, manually registering DNS
    names for each IP assigned by **MetalLB** presents a maintenance challenge. To
    deliver a more cloud-like experience and streamline name resolution for `LoadBalancer`
    services, we need an add-on that can address these limitations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Kubernetes 负载均衡器为服务分配了一个标准的 IP 地址，但它不会自动为工作负载生成一个 DNS 名称来访问服务。相反，你必须依赖 IP
    地址来连接集群中的应用程序，这变得混乱且低效。此外，手动为每个由 **MetalLB** 分配的 IP 注册 DNS 名称会带来维护挑战。为了提供更接近云的体验并简化
    `LoadBalancer` 服务的名称解析，我们需要一个能够解决这些限制的附加组件。
- en: Similar to the team that maintains KinD, there is a Kubernetes SIG that is working
    on this feature for Kubernetes called **ExternalDNS**. The main project page can
    be found on the SIG’s GitHub at [https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于维护 KinD 的团队，有一个 Kubernetes SIG 正在为 Kubernetes 开发这个功能，叫做 **ExternalDNS**。该项目的主页面可以在
    SIG 的 GitHub 上找到：[https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns)。
- en: 'At the time of writing, the `ExternalDNS` project supports 34 compatible DNS
    services, including the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写时，`ExternalDNS` 项目支持 34 种兼容的 DNS 服务，包括以下内容：
- en: AWS Cloud Map
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Cloud Map
- en: Amazon’s Route 53
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊的 Route 53
- en: Azure DNS
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure DNS
- en: Cloudflare
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloudflare
- en: CoreDNS
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreDNS
- en: Google Cloud DNS
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud DNS
- en: Pi-hole
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pi-hole
- en: RFC2136
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RFC2136
- en: There are multiple options on how to extend CoreDNS to resolve external names,
    depending on what main DNS server you may be running. Many of the supported DNS
    servers will simply register any services dynamically. ExternalDNS will see the
    created resource and use native calls to register services automatically, like
    **Amazon’s Route 53**. Not all DNS servers natively allow for this type of dynamic
    registration by default.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你所运行的主 DNS 服务器类型，有多种方法可以扩展 CoreDNS 来解析外部名称。许多支持的 DNS 服务器会动态注册任何服务。ExternalDNS
    会看到创建的资源并使用本机调用自动注册服务，像 **Amazon’s Route 53**。并非所有 DNS 服务器默认都允许这种类型的动态注册。
- en: In these instances, you need to manually configure your main DNS server to forward
    the desired domain requests to a CoreDNS instance running in your cluster. This
    is what we will use for the examples in this chapter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，你需要手动配置主 DNS 服务器，将所需的域请求转发到运行在集群中的 CoreDNS 实例。这就是我们将在本章示例中使用的内容。
- en: Our Kubernetes cluster currently utilizes CoreDNS to handle cluster DNS name
    resolution. However, what might be lesser known is that CoreDNS offers more than
    just internal cluster DNS resolution. It can extend its capabilities to perform
    external name resolution, effectively resolving names for any DNS zone managed
    by a CoreDNS deployment.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Kubernetes 集群目前使用 CoreDNS 来处理集群的 DNS 名称解析。然而，可能不太为人所知的是，CoreDNS 提供的不仅仅是内部集群的
    DNS 解析。它可以扩展其功能，执行外部名称解析，实际上可以解析由 CoreDNS 部署管理的任何 DNS 区域的名称。
- en: Now, let’s move on to how ExternalDNS installs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续介绍 ExternalDNS 的安装过程。
- en: Setting up ExternalDNS
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 ExternalDNS
- en: Right now, our CoreDNS is only resolving names for internal cluster names, so
    we need to set up a zone for our new `LoadBalancer` DNS entries.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的 CoreDNS 只为内部集群名称解析名称，因此我们需要为新的 `LoadBalancer` DNS 条目设置一个区域。
- en: For our example, a company, **FooWidgets**, wants all Kubernetes services to
    go into the `foowidgets.k8s` domain, so we will use that as our new zone.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，一个公司 **FooWidgets** 想让所有 Kubernetes 服务都进入 `foowidgets.k8s` 域，因此我们将使用该域作为新的区域。
- en: Integrating ExternalDNS and CoreDNS
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成 ExternalDNS 和 CoreDNS
- en: '**ExternalDNS** is not an actual DNS server; instead, it is a controller that
    will watch for objects that request a new DNS entry. Once a request is seen by
    the controller, it will send the information to an actual DNS server, like CoreDNS,
    for registration.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**ExternalDNS** 不是一个实际的 DNS 服务器；它是一个控制器，负责监控请求新 DNS 条目的对象。一旦控制器看到请求，它会将信息发送到实际的
    DNS 服务器（如 CoreDNS）进行注册。'
- en: The process of how a service is registered is shown in the diagram below.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 服务注册的过程如下面的图所示。
- en: '![](img/B21165_05_01.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_01.png)'
- en: 'Figure 5.1: ExternalDNS registration flow'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：ExternalDNS 注册流程
- en: In our example, we are using CoreDNS as our DNS server; however, as we mentioned
    previously, ExternalDNS has support for 34 different DNS services and the list
    of supported services is constantly growing. Since we will be using CoreDNS as
    our DNS server, we will need to add a component that will store the DNS records.
    To accomplish this, we need to deploy an ETCD server in the cluster.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用 CoreDNS 作为我们的 DNS 服务器；然而，正如我们之前提到的，ExternalDNS 支持 34 种不同的 DNS 服务，并且支持的服务列表还在不断增加中。因为我们将使用
    CoreDNS 作为我们的 DNS 服务器，我们需要添加一个组件来存储 DNS 记录。为了实现这一点，我们需要在集群中部署一个 ETCD 服务器。
- en: For our example deployment, we will use the ETCD Helm chart.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例部署，我们将使用 ETCD Helm chart。
- en: Helm is a tool for Kubernetes that makes it easier to deploy and manage applications.
    It uses Helm charts, which are templates that contain the necessary configuration
    and resource values for the application. It automates the setup of complex applications,
    ensuring they are consistently and reliably configured. It’s a powerful tool,
    and you will find that many projects and vendors offer their applications, by
    default, using Helm charts. You can read more about Helm on their main home page
    at [https://v3.helm.sh/](https://v3.helm.sh/).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Helm 是一个用于 Kubernetes 的工具，它使得部署和管理应用程序变得更加容易。它使用 Helm charts，这些模板包含了应用程序所需的配置和资源值。它自动设置复杂应用程序的环境，确保它们在配置上保持一致和可靠。这是一个强大的工具，您会发现许多项目和供应商默认使用
    Helm charts 提供他们的应用程序。您可以在他们的主页 [https://v3.helm.sh/](https://v3.helm.sh/) 上了解更多关于
    Helm 的信息。
- en: One reason Helm is such a powerful tool is its ability to use custom options
    that can be declared when you run the `helm instal`l command. The same options
    can also be declared in a file that is passed to the installation using the `-f`
    option. These options make deploying complex systems easier and reproducible since
    the same values file can be used on any deployment.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Helm 如此强大的一个原因是它能够使用自定义选项，在运行 `helm install` 命令时声明这些选项。同样的选项也可以在通过 `-f` 选项传递给安装的文件中声明。这些选项使得部署复杂系统变得更加简单和可重现，因为可以在任何部署中使用相同的值文件。
- en: For our deployment example, we have included a `values.yaml` file, located in
    the `chapter5/etcd` directory, that we will use to configure our ETCD deployment.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的部署示例，我们已经包含了一个位于 `chapter5/etcd` 目录中的 `values.yaml` 文件，我们将使用它来配置我们的 ETCD
    部署。
- en: Now, finally, let’s deploy ETCD! We have included a script called `deploy-etcd.sh`
    in the `chapter5/etcd` directory that will deploy ETCD with a single replica in
    a new namespace called `etcd-dns`. Execute the script while in the `chapter5/etcd`
    directory.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最后，让我们部署 ETCD 吧！我们在 `chapter5/etcd` 目录中包含了一个名为 `deploy-etcd.sh` 的脚本，它将在名为
    `etcd-dns` 的新命名空间中部署一个单副本的 ETCD。在 `chapter5/etcd` 目录中执行该脚本。
- en: Thanks to Helm, the script only has two commands– it will create a new namespace,
    and then execute a `Helm install` command to deploy our ETCD instance. In the
    real world, you would want to change the replica count to at least 3 to have a
    highly available ETCD deployment, but we wanted to limit resource requirements
    for our KinD server.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Helm 的帮助，脚本只需两个命令 – 它将创建一个新的命名空间，然后执行一个 `Helm install` 命令来部署我们的 ETCD 实例。在实际环境中，您可能希望将副本数量更改至至少
    3 以实现高可用的 ETCD 部署，但我们希望限制我们的 KinD 服务器的资源需求。
- en: Now that we have our ETCD for DNS, we can move on to integrating our CoreDNS
    service with our new ETCD deployment.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为我们的 DNS 配置了 ETCD，我们可以继续将我们的 CoreDNS 服务与我们的新 ETCD 部署集成。
- en: Adding an ETCD zone to CoreDNS
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加一个 ETCD 区域到 CoreDNS
- en: As we showed in the diagram in the last section, CoreDNS will store the DNS
    records in an ETCD instance. This requires us to configure a CoreDNS server with
    the DNS zone(s) we want to register names in and the ETCD server that will store
    the records.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节的图表中展示的那样，CoreDNS 将在一个 ETCD 实例中存储 DNS 记录。这要求我们配置一个 CoreDNS 服务器，其中包括我们想要在其中注册名称的
    DNS 区域以及将存储记录的 ETCD 服务器。
- en: To keep resource requirements lower, we will use the included CoreDNS server
    that most Kubernetes installations include as part of their base cluster creation
    for our new domain. In the real world, you should deploy a dedicated CoreDNS server
    to handle just ExternalDNS registrations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持资源需求较低，我们将使用大多数 Kubernetes 安装中包含的 CoreDNS 服务器作为我们新域的基本集群创建的一部分。在实际环境中，您应该部署一个专用的
    CoreDNS 服务器来处理仅限于 ExternalDNS 注册的任务。
- en: At the end of this section, you will execute a script to deploy a fully configured
    ExternalDNS service that has all of the options and configurations discussed in
    this section. The commands used in this section are only for reference; you do
    not need to execute them on your cluster, since the script will do that for you.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节结束时，你将执行一个脚本来部署一个完全配置的ExternalDNS服务，其中包含本节中讨论的所有选项和配置。本节中使用的命令仅供参考；你不需要在集群中执行这些命令，因为脚本将为你完成这些操作。
- en: 'Before we can integrate CoreDNS, we need to know the IP address of our new
    ETCD service. You can retrieve the address by listing the services in the `etcd-dns`
    namespace using `kubectl`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以集成CoreDNS之前，我们需要知道新ETCD服务的IP地址。你可以通过使用`kubectl`列出`etcd-dns`命名空间中的服务来获取地址：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will show our ETCD service, along with the IP address of the service:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示我们的ETCD服务以及该服务的IP地址：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `Cluster-IP` you see in the service list will be used to configure the new
    DNS zone as a location to store the DNS records.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你在服务列表中看到的`Cluster-IP`将用于配置新的DNS区域，作为存储DNS记录的位置。
- en: 'When you deploy ExternalDNS, you can configure CoreDNS in one of two ways:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 部署ExternalDNS时，你可以通过两种方式配置CoreDNS：
- en: Add a zone to the Kubernetes-integrated CoreDNS service
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向Kubernetes集成的CoreDNS服务添加区域。
- en: Deploy a new CoreDNS service that will be used for ExternalDNS registrations
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署一个新的CoreDNS服务，该服务将用于ExternalDNS注册。
- en: For ease of testing, we will add a zone to the Kubernetes CoreDNS service. This
    requires us to edit the CoreDNS `ConfigMap` found in the `kube-sytem` namespace.
    When you execute the script at the end of this section, the modification will
    be done for you. It will add the section shown below in **bold** to the `ConfigMap`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于测试，我们将向Kubernetes CoreDNS服务添加一个区域。这需要我们编辑位于`kube-system`命名空间中的CoreDNS `ConfigMap`。当你执行本节末尾的脚本时，修改将自动完成，它会在`ConfigMap`中添加下方**加粗**的部分。
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The added lines configure a zone called `foowidgets.k8s` that is ETCD integrated.
    The first line that we added tells CoreDNS that the zone name, `foowidgets.com`,
    is integrated with an ETCD service.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 添加的行配置了一个名为`foowidgets.k8s`的区域，该区域集成了ETCD。我们添加的第一行告诉CoreDNS，该区域名称`foowidgets.com`已与ETCD服务集成。
- en: The next line, `stubzone`, tells CoreDNS to allow you to set up a DNS server
    as a “stub resolver” for a particular zone. As a stub resolver, this DNS server
    directly queries specific name servers for a zone’s information without the need
    for recursive resolution throughout the entire DNS hierarchy.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行`stubzone`告诉CoreDNS允许你将DNS服务器设置为某个区域的“存根解析器”。作为存根解析器，这个DNS服务器直接查询特定名称服务器的区域信息，而不需要在整个DNS层次结构中进行递归解析。
- en: The third addition is the `path /skydns` option, which may look confusing since
    it doesn’t mention CoreDNS. Even though the value is `skydns`, it is also the
    default path for CoreDNS integration as well.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第三项添加是`path /skydns`选项，这可能看起来有些混淆，因为它没有提到CoreDNS。尽管该值是`skydns`，但它也是CoreDNS集成的默认路径。
- en: Finally, the last line tells CoreDNS where to store the records. In our example,
    we have an ETCD service running, using IP address `10.96.149.223` on the default
    ETCD port of `2379`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行告诉CoreDNS将记录存储在哪里。在我们的示例中，我们运行了一个ETCD服务，使用IP地址`10.96.149.223`，并在默认的ETCD端口`2379`上运行。
- en: You could use the Service’s host name instead of the IP here. We used the IP
    to show the relationships between a pod and a Service, but the name `etcd-dns.etcd-dns.svc`
    would work as well. Which method you choose will depend on your situation. In
    our KinD cluster, we don’t really need to worry about losing the IP because the
    cluster is disposable. In the real world, you would want to use the host name
    to protect against the IP address changing.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里使用服务的主机名而不是IP。我们使用IP来展示Pod和服务之间的关系，但`etcd-dns.etcd-dns.svc`这个名称也可以使用。你选择哪种方式取决于你的具体情况。在我们的KinD集群中，我们不需要担心丢失IP，因为集群是可丢弃的。在实际应用中，你应该使用主机名来避免IP地址变化带来的问题。
- en: Now that you understand how to add an ETCD integrated zone in CoreDNS, the next
    step is to update the deployment options that ExternalDNS requires to integrate
    with CoreDNS.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你理解了如何在CoreDNS中添加ETCD集成的区域，下一步是更新ExternalDNS所需的部署选项，以便与CoreDNS集成。
- en: ExternalDNS configuration options
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ExternalDNS配置选项。
- en: ExternalDNS can be configured to register ingress or service objects. This is
    configured in the deployment file of ExternalDNS using the source field. The example
    below shows the options portion of the deployment that we will use in this chapter.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ExternalDNS可以配置为注册入口或服务对象。这在ExternalDNS的部署文件中使用source字段进行配置。以下示例显示了我们将在本章中使用的部署选项部分。
- en: We also need to configure the provider that ExternalDNS will use, and since
    we are using CoreDNS, we set the provider to `coredns`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要配置ExternalDNS将使用的提供程序，由于我们使用的是CoreDNS，因此我们将提供程序设置为`coredns`。
- en: 'The last option is the log level we want to set, which we set to `info` to
    keep our log files smaller and easier to read. The arguments that we will use
    are shown below:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的选项是我们想要设置的日志级别，我们将其设置为`info`，以使日志文件更小且更易于阅读。我们将使用的参数如下所示：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have gone over the ETCD options and deployment, how to configure
    a new zone to use ETCD, and the options to configure ExternalDNS to use CoreDNS
    as a provider, we can deploy ExternalDNS in our cluster.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经讲解了ETCD选项和部署，如何配置一个新的区域以使用ETCD，以及如何配置ExternalDNS使用CoreDNS作为提供程序，我们可以在集群中部署ExternalDNS了。
- en: We have included a script called `deploy-externaldns.sh` in the `chapter5/externaldns`
    folder. Execute the script in the directory to deploy ExternalDNS into your KinD
    cluster. When you execute the script, it will fully configure and deploy an integrated
    ExternalDNS with ETCD.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`chapter5/externaldns`文件夹中包含了一个名为`deploy-externaldns.sh`的脚本。在该目录下执行脚本，以将ExternalDNS部署到您的KinD集群中。当您执行该脚本时，它将完全配置并部署一个集成了ETCD的ExternalDNS。
- en: '**NOTE**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: 'If you see a warning when the script updates the `ConfigMap`, you can safely
    ignore it. Since our `kubectl` command is using `apply` to update the object,
    Kubernetes will look for a last-applied-configuration annotation, if there is
    one set. Since you likely do not have that in the existing object, you will see
    the warning that it’s missing. This is just a warning and will not stop the `ConfigMap`
    update, as you can confirm by looking at the last line of the `kubectl` update
    command, where it shows the `ConfigMap` was updated: `configmap/coredns configured`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在脚本更新`ConfigMap`时看到警告，您可以安全地忽略它。由于我们的`kubectl`命令使用`apply`来更新对象，Kubernetes会查找最后应用的配置注释（last-applied-configuration），如果有设置的话。由于您可能在现有对象中没有该注释，因此会看到缺失的警告。这只是一个警告，不会阻止`ConfigMap`的更新，您可以通过查看`kubectl`更新命令的最后一行来确认，那里显示`ConfigMap`已被更新：`configmap/coredns
    configured`
- en: Now, we have added the ability for developers to create dynamically registered
    DNS names for their services. Next, let’s see it in action by creating a new service
    that will register itself in our CoreDNS server.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经为开发人员添加了创建动态注册DNS名称的能力，接下来，让我们通过创建一个新的服务来看它的实际效果，该服务将自己注册到我们的CoreDNS服务器中。
- en: Creating a LoadBalancer service with ExternalDNS integration
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个与ExternalDNS集成的LoadBalancer服务
- en: 'Our ExternalDNS will watch all services for an annotation that contains the
    desired DNS name. This is just a single annotation using the format `annotation
    external-dns.alpha.kubernetes.io/hostname` with the value of the DNS name you
    want to register. For our example, we want to register a name of `nginx.foowidgets.k8s`,
    so we would add an annotation to our NGINX service: `external-dns.alpha.kubernetes.io/hostname:
    nginx.foowidgets.k8s`.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的ExternalDNS将监视所有服务，查找包含所需DNS名称的注释。这只是一个单一的注释，格式为`annotation external-dns.alpha.kubernetes.io/hostname`，值为您想要注册的DNS名称。以我们的示例为例，我们想要注册名称`nginx.foowidgets.k8s`，因此我们会在NGINX服务中添加一个注释：`external-dns.alpha.kubernetes.io/hostname:
    nginx.foowidgets.k8s`。'
- en: In the `chapter5/externaldns` directory, we have included a manifest that will
    deploy an NGINX web server using a `LoadBalancer` service that contains the annotation
    to register the DNS name.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在`chapter5/externaldns`目录下，我们包含了一个清单文件，该文件将通过一个包含注册DNS名称注释的`LoadBalancer`服务来部署NGINX
    Web服务器。
- en: 'Deploy the manifest using `kubectl create -f nginx-lb.yaml`, which will deploy
    the resources in the default namespace. The deployment is a standard NGINX deployment,
    but the service has the required annotation to tell the ExternalDNS service that
    you want to register a new DNS name. The manifest for the service is shown below,
    with the annotation in bold:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl create -f nginx-lb.yaml`部署清单，这将把资源部署到默认命名空间。该部署是一个标准的NGINX部署，但服务具有必要的注释，告诉ExternalDNS服务您希望注册一个新的DNS名称。服务的清单如下所示，注释部分为**粗体**：
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When ExternalDNS sees the annotation, it will register the requested name in
    the zone. The hostname from the annotation will log an entry in the ExternalDNS
    pod – the registration for our new entry, `nginx.foowidgets.k8s`, is shown below:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当ExternalDNS看到该注释时，它会在区域中注册请求的名称。来自注释的主机名将在ExternalDNS Pod中记录一条条目 – 我们的新条目`nginx.foowidgets.k8s`的注册信息如下所示：
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see in the last line of the log, the record was added as an A-record
    in the DNS server, pointing to the IP address `172.18.200.101`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在日志的最后一行看到的，记录已作为A记录添加到DNS服务器中，指向IP地址`172.18.200.101`。
- en: The last step for confirming that ExternalDNS is fully working is to test a
    connection to the application. Since we are using a KinD cluster, we must test
    this from a pod in the cluster. To provide the new names to external resources,
    we would need to configure our main DNS server(s) to forward requests for the
    `foowidgets.k8s` domain to our CoreDNS server. At the end of this section, we
    will show the steps to integrate a Windows DNS server, which could be any main
    DNS server on your network, with our Kubernetes CoreDNS server.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 确认ExternalDNS完全正常工作的最后一步是测试与应用程序的连接。由于我们使用的是KinD集群，因此必须从集群中的Pod进行测试。为了将新名称提供给外部资源，我们需要配置主DNS服务器，将对`foowidgets.k8s`域的请求转发到CoreDNS服务器。在本节末尾，我们将展示如何将Windows
    DNS服务器（这可以是您网络中的任何主DNS服务器）与Kubernetes CoreDNS服务器集成的步骤。
- en: Now we can test the NGINX deployment using the DNS name from our annotation.
    Since you aren’t using the CoreDNS server as your main DNS provider, we need to
    use a container in the cluster to test name resolution. There is a great utility
    called **Netshoot** that contains a number of useful troubleshooting tools; it’s
    a great tool to have in your toolbox to test and troubleshoot clusters and pods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用从注释中获得的DNS名称测试NGINX部署。由于您没有将CoreDNS服务器作为主DNS提供商，我们需要使用集群中的容器来测试名称解析。有一个很棒的工具叫做**Netshoot**，它包含许多有用的故障排除工具；这是一个很好的工具，可以用来测试和排查集群及Pod的问题。
- en: 'To run a Netshoot container, we can use the `kubectl run` command. We only
    need the pod to run when we are using it to run tests in the cluster, so we will
    tell the `kubectl run` command to run an interactive shell and to remove the pod
    after we exit. To run Netshoot, execute:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行Netshoot容器，我们可以使用`kubectl run`命令。我们只需要在使用它进行集群测试时运行Pod，因此我们将告诉`kubectl run`命令运行一个交互式Shell，并在退出后删除Pod。要运行Netshoot，请执行：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The pod may take a minute or two to become available, but once it starts up,
    you will see a `tmp-shell` prompt. At this prompt, we can use `nslookup` to verify
    that the DNS entry was successfully added. If you attempt to look up `nginx.foowidgets.k8s`,
    you should receive a reply with the IP address of the service.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Pod可能需要一两分钟才能变得可用，但一旦启动，您将看到`tmp-shell`提示符。在此提示符下，我们可以使用`nslookup`来验证DNS条目是否已成功添加。如果您尝试查找`nginx.foowidgets.k8s`，应该会收到该服务的IP地址作为回应。
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Your reply should look similar to the example below:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您的回复应类似于以下示例：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This confirms that our annotation was successful and ExternalDNS registered
    our hostname in the CoreDNS server.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这确认了我们的注释是成功的，并且ExternalDNS已在CoreDNS服务器中注册了我们的主机名。
- en: The `nslookup` only proves that there is an entry for `nginx.foowidgets.k8s`;
    it doesn’t test the application. To prove that we have a successful deployment
    that will work when someone enters the name in a browser, we can use the `curl`
    utility that is included with Netshoot.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`nslookup`仅能证明`nginx.foowidgets.k8s`有条目；它并没有测试应用程序。为了证明我们有一个成功的部署，并且当有人在浏览器中输入该名称时它能够工作，我们可以使用Netshoot中包含的`curl`工具。'
- en: '![](img/B21165_05_02.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_02.png)'
- en: 'Figure 5.2: curl test using the ExternalDNS name'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：使用ExternalDNS名称的curl测试
- en: The curl output confirms that we can use the dynamically created service name
    to access the NGINX web server.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: curl输出确认我们可以使用动态创建的服务名称访问NGINX Web服务器。
- en: We realize that some of these tests aren’t very exciting since you can’t test
    them using a standard browser. To allow CoreDNS to be used outside of the cluster,
    we need to integrate CoreDNS with your main DNS server, which needs to delegate
    ownership for the zone(s) that are in CoreDNS. When you delegate a zone, any request
    to your main DNS server for a host in the delegated zone will forward the request
    to the DNS server that contains the requested zone.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们意识到这些测试可能不太令人兴奋，因为无法通过标准浏览器进行测试。为了让CoreDNS能够在集群外部使用，我们需要将CoreDNS与您的主DNS服务器集成，这需要将CoreDNS中区域的所有权委托给主DNS服务器。当您委托一个区域时，任何针对主DNS服务器的请求，如果请求的主机位于该委托的区域中，都会将请求转发到包含该区域的DNS服务器。
- en: In the next section, we will integrate the CoreDNS running in our cluster with
    a Windows DNS server. While we are using Windows as our DNS server, the concepts
    for delegating a zone are similar between operating systems and DNS servers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将集成在集群中运行的 CoreDNS 与 Windows DNS 服务器。虽然我们使用 Windows 作为 DNS 服务器，但不同操作系统和
    DNS 服务器之间的区域委托概念是相似的。
- en: Integrating CoreDNS with an enterprise DNS server
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 CoreDNS 与企业 DNS 服务器集成
- en: This section will show you how to use a main DNS server to forward the name
    resolution of the `foowidgets.k8s` zone to a CoreDNS server running on a Kubernetes
    cluster.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将展示如何使用主 DNS 服务器将 `foowidgets.k8s` 区域的名称解析转发到运行在 Kubernetes 集群中的 CoreDNS 服务器。
- en: The steps provided here are an example of integrating an enterprise DNS server
    with a Kubernetes DNS service. Because of the external DNS requirement and additional
    setup, the steps are for reference and **should not be executed** on your KinD
    cluster.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供的步骤是将企业 DNS 服务器与 Kubernetes DNS 服务集成的示例。由于外部 DNS 的要求和额外的设置，这些步骤仅供参考，**不应在您的
    KinD 集群上执行**。
- en: To find a record in a delegated zone, the main DNS server uses a process known
    as a recursive query. A recursive query refers to a DNS inquiry initiated by a
    DNS resolver, acting on behalf of a user. Through the recursive query process,
    the DNS resolver assumes the task of reaching out to multiple DNS servers in a
    hierarchical pattern. Its objective is to find the authoritative DNS server for
    the requested domain and initiate the retrieval of the requested DNS record.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要在委托的区域中查找记录，主 DNS 服务器使用一种称为递归查询的过程。递归查询是由 DNS 解析器发起的 DNS 查询，代表用户进行操作。在递归查询过程中，DNS
    解析器承担了通过层级模式联系多个 DNS 服务器的任务。其目标是找到请求域的权威 DNS 服务器，并启动请求的 DNS 记录检索。
- en: The diagram below illustrates the flow of how DNS resolution is provided by
    delegating a zone to a CoreDNS server in an enterprise environment.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了通过将区域委托给 CoreDNS 服务器来提供 DNS 解析的流程，在企业环境中尤为适用。
- en: '![](img/B21165_05_03.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_03.png)'
- en: 'Figure 5.3: DNS delegation flow'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：DNS 委托流程
- en: The local client will look at its DNS cache for the name being requested.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本地客户端将查看其 DNS 缓存中的请求名称。
- en: If the name is not in the local cache, a request is made to the main DNS server
    for `nginx.foowidgets.k8s`.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果名称不在本地缓存中，将向主 DNS 服务器请求 `nginx.foowidgets.k8s`。
- en: The DNS server receives the query and looks at the zones it knows about. It
    finds the `foowidgets.k8s` zone and sees that the zone has been delegated to the
    CoreDNS running on `192.168.1.200`.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DNS 服务器接收到查询后，会查看它知道的区域。它找到 `foowidgets.k8s` 区域，并发现该区域已被委托给运行在 `192.168.1.200`
    上的 CoreDNS。
- en: The main DNS server sends the query to the delegated CoreDNS server.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主 DNS 服务器将查询发送到委托的 CoreDNS 服务器。
- en: CoreDNS looks for the name, `nginx`, in the `foowidgets.k8s` zone.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CoreDNS 在 `foowidgets.k8s` 区域中查找名称 `nginx`。
- en: CoreDNS sends the IP address for `foowidgets.k8s` back to the main DNS server.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CoreDNS 将 `foowidgets.k8s` 的 IP 地址返回给主 DNS 服务器。
- en: The main DNS server sends the reply containing the address for `nginx.foowidgets.k8s`
    to the client.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主 DNS 服务器将包含 `nginx.foowidgets.k8s` 地址的回复发送给客户端。
- en: The client connects to the NGINX server using the IP address returned from CoreDNS.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端通过从 CoreDNS 返回的 IP 地址连接到 NGINX 服务器。
- en: Let’s move on to a real-world example. For our scenario, the main DNS server
    is running on a Windows 2019 server and we will delegate a zone to a CoreDNS server.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续来看一个现实世界的例子。在我们的场景中，主 DNS 服务器运行在 Windows 2019 服务器上，我们将把一个区域委托给 CoreDNS
    服务器。
- en: 'The components deployed are as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的组件如下：
- en: Our network subnet is `10.2.1.0/24`
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的网络子网是 `10.2.1.0/24`
- en: Windows 2019 or higher server running DNS
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行 DNS 的 Windows 2019 或更高版本服务器
- en: A Kubernetes cluster
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Kubernetes 集群
- en: A MetalLB address pool with a range of `10.2.1.70`-`10.2.1.75`
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 MetalLB 地址池，范围为 `10.2.1.70`-`10.2.1.75`
- en: A CoreDNS instance deployed in a cluster using a `LoadBalancer` service assigned
    the IP address `10.2.1.74` from our IP pool
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `LoadBalancer` 服务部署的 CoreDNS 实例，分配了来自我们的 IP 池的 IP 地址 `10.2.1.74`
- en: Deployed add-ons, using the configuration from this chapter including ExternalDNS,
    an ETCD deployment for CoreDNS, and a new CoreDNS ETCD integrated zone
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署的附加组件，使用本章中的配置，包括 ExternalDNS、CoreDNS 的 ETCD 部署，以及一个新的 CoreDNS ETCD 集成区域
- en: Bitnami NGINX deployment to test the delegation
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bitnami NGINX 部署用于测试委托
- en: Now, let’s go through the configuration steps to integrate our DNS servers.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过配置步骤来集成我们的 DNS 服务器。
- en: Exposing CoreDNS to external requests
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 CoreDNS 暴露给外部请求
- en: We have already covered how to deploy most of the resources that you need to
    integrate – ETCD, ExternalDNS, and configuring CoreDNS with a new zone that is
    ETCD-integrated. To provide external access to CoreDNS, we need to create a new
    service that exposes CoreDNS on TCP and UDP port `53`. A complete service manifest
    is shown below.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经覆盖了如何部署大多数你需要集成的资源——ETCD、ExternalDNS，并通过一个新的 ETCD 集成区域配置 CoreDNS。为了提供对 CoreDNS
    的外部访问，我们需要创建一个新的服务，将 CoreDNS 显示在 TCP 和 UDP 端口 `53` 上。完整的服务清单如下所示。
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There is one new option in the service that we haven’t discussed yet – we have
    added the `spec.loadBalancerIP` to our deployment. This option allows you to assign
    an IP address to the service so it will have a stable IP address, even if the
    service is recreated. We need a static IP since we need to enable forwarding from
    our main DNS server to the CoreDNS server in the Kubernetes cluster.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 服务中有一个新的选项我们尚未讨论——我们在部署中添加了`spec.loadBalancerIP`。这个选项允许你为服务分配一个 IP 地址，即使服务被重新创建，它仍然会保持一个稳定的
    IP 地址。我们需要一个静态 IP，因为我们需要启用从主 DNS 服务器到 Kubernetes 集群中的 CoreDNS 服务器的转发。
- en: Once CoreDNS is exposed using a `LoadBalancer` on port `53`, we can configure
    the main DNS server to forward requests for hosts in the `foowidgets.k8s` domain
    to our CoreDNS server.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 CoreDNS 通过端口 `53` 使用 `LoadBalancer` 公开，我们可以配置主 DNS 服务器将 `foowidgets.k8s`
    域中的主机请求转发到我们的 CoreDNS 服务器。
- en: Configuring the primary DNS server
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置主 DNS 服务器
- en: The first thing to do on our main DNS server is to create a conditional forwarder
    to the node running the CoreDNS pod.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的主 DNS 服务器上，首先要做的就是创建一个条件转发器，指向运行 CoreDNS Pod 的节点。
- en: 'On the Windows DNS host, we need to create a new conditional forwarder for
    `foowidgets.k8s` pointing to the IP address that we assigned to the new CoreDNS
    service. In our example, the CoreDNS service has been assigned to the host `10.2.1.74`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Windows DNS 主机上，我们需要为 `foowidgets.k8s` 创建一个新的条件转发器，指向我们分配给新 CoreDNS 服务的 IP
    地址。在我们的示例中，CoreDNS 服务已分配给主机 `10.2.1.74`：
- en: '![](img/B21165_05_04.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_04.png)'
- en: 'Figure 5.4: Windows conditional forwarder setup'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：Windows 条件转发器设置
- en: This configures the Windows DNS server to forward any request for a host in
    the `foowidgets.k8s` domain to the CoreDNS service running on IP address `10.2.1.74`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这配置了 Windows DNS 服务器，将任何对 `foowidgets.k8s` 域中主机的请求转发到运行在 IP 地址 `10.2.1.74` 上的
    CoreDNS 服务。
- en: Testing DNS forwarding to CoreDNS
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试 DNS 转发到 CoreDNS
- en: To test the configuration, we will use a workstation on the main network that
    has been configured to use the Windows DNS server.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试配置，我们将使用一台已配置为使用 Windows DNS 服务器的主网络工作站。
- en: 'The first test we will run is an `nslookup` of the NGINX record that was created
    by the MetalLB annotation:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行的第一个测试是对 MetalLB 注解创建的 NGINX 记录进行 `nslookup`：
- en: 'From Command Prompt, we execute an `nslookup nginx.foowidgets.k8s` command:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从命令提示符中，我们执行 `nslookup nginx.foowidgets.k8s` 命令：
- en: '[PRE10]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Since the query returned the IP address we expected for the record, we can confirm
    that the Windows DNS server is forwarding requests to CoreDNS correctly.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于查询返回了我们期望的记录 IP 地址，我们可以确认 Windows DNS 服务器正在正确地将请求转发到 CoreDNS。
- en: We can do one more additional NGINX test from the laptop’s browser. In Chrome,
    we can use the URL registered in CoreDNS, `nginx.foowidgets.k8s`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从笔记本浏览器上再做一个额外的 NGINX 测试。在 Chrome 中，我们可以使用在 CoreDNS 中注册的 URL `nginx.foowidgets.k8s`。
- en: '![](img/B21165_05_05.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_05.png)'
- en: 'Figure 5.5: Success browsing from an external workstation using CoreDNS'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5：使用 CoreDNS 从外部工作站成功浏览
- en: One test confirms that the forwarding works, but we want to create an additional
    deployment to verify the system is fully working.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一个测试确认转发工作正常，但我们还想创建一个额外的部署来验证系统完全正常运行。
- en: To test a new service, we deploy a different NGINX server called microbot, with
    a service that has an annotation assigning the name `microbot.foowidgets.k8s`.
    MetalLB has assigned the service the IP address of `10.2.1.65`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试一个新服务，我们部署了一个不同的 NGINX 服务器，名为 microbot，并为其服务添加了一个注解，指定了名称`microbot.foowidgets.k8s`。MetalLB
    为该服务分配了 IP 地址 `10.2.1.65`。
- en: 'Like our previous test, we test the name resolution using `nslookup`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如同我们之前的测试一样，我们使用`nslookup`测试名称解析：
- en: '[PRE11]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To confirm that the web server is running correctly, we browse to the URL from
    a workstation:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认 Web 服务器是否正确运行，我们从工作站浏览该 URL：
- en: '![Figure 6.22 – Successful browsing from an external workstation using CoreDNS
    ](img/B21165_05_06.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.22 – 使用 CoreDNS 从外部工作站成功浏览](img/B21165_05_06.png)'
- en: 'Figure 5.6: Successful browsing from an external workstation using CoreDNS'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：使用 CoreDNS 从外部工作站成功浏览
- en: Success! We have now integrated an enterprise DNS server with a CoreDNS server
    running on a Kubernetes cluster. This integration provides users with the ability
    to register service names dynamically by simply adding an annotation to the service.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们现在已将企业DNS服务器与运行在Kubernetes集群上的CoreDNS服务器集成。这一集成使用户能够通过简单地向服务添加注释来动态注册服务名称。
- en: Load balancing between multiple clusters
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨多个集群的负载均衡
- en: There are various ways to configure running services in multiple clusters, often
    involving complex and costly add-ons like global load balancers. A global load
    balancer can be thought of as a traffic cop – it knows how to direct incoming
    traffic between multiple endpoints. At a high level, you can create a new DNS
    entry that the global load balancer will control. This new entry will have backend
    systems added to the endpoint list and based on factors like health, connections,
    or bandwidth, it will direct the traffic to the endpoint nodes. If an endpoint
    is unavailable for any reason, the load balancer will remove it from the endpoint
    list. By removing it from the list, traffic will only be sent to healthy nodes,
    providing a smooth end user experience. There’s nothing worse for a customer than
    getting a website not found error when they attempt to access a site.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 配置多个集群中运行的服务有多种方法，通常涉及复杂且昂贵的附加组件，如全局负载均衡器。全局负载均衡器可以看作是一个交通警察——它知道如何在多个端点之间指引传入流量。从高层次来看，你可以创建一个新的DNS条目，供全局负载均衡器控制。这个新的条目将有后端系统被添加到端点列表中，并根据健康状况、连接数或带宽等因素，它将把流量指向这些端点节点。如果某个端点因任何原因不可用，负载均衡器将把它从端点列表中移除。通过将其从列表中移除，流量只会发送到健康的节点，从而提供流畅的终端用户体验。对客户来说，最糟糕的情况就是当他们尝试访问网站时，遇到“网站未找到”的错误。
- en: '![](img/B21165_05_07.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_07.png)'
- en: 'Figure 5.7: Global load balancing traffic flow'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：全局负载均衡流量流程
- en: The figure above shows a healthy workflow where both clusters are running an
    application that we are load balancing between the clusters. When the request
    hits the load balancer, it will send the traffic in a round-robin fashion between
    the two clusters. The `nginx.foowidgets.k8s` request will ultimately send the
    traffic to either `nginx.clustera.k8s` or `nginx.clusterb.k8s`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了一个健康的工作流，其中两个集群都在运行我们进行负载均衡的应用程序。当请求到达负载均衡器时，它将以轮询方式在两个集群之间分配流量。`nginx.foowidgets.k8s`请求最终将流量发送到`nginx.clustera.k8s`或`nginx.clusterb.k8s`。
- en: In *Figure 5.8*, we have a failure of our NGINX workload in cluster B. Since
    the global load balancer has a health check on the running workloads, it will
    remove the endpoint in cluster B from the `nginx.foowidgets.k8s` entry.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.8*中，我们看到集群B中的NGINX工作负载发生故障。由于全局负载均衡器对正在运行的工作负载进行了健康检查，它将从`nginx.foowidgets.k8s`条目中移除集群B的端点。
- en: '![](img/B21165_05_08.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_08.png)'
- en: 'Figure 5.8: Global load balancing traffic flow with a site failure'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8：全局负载均衡流量流程（带站点故障）
- en: Now, for any traffic that comes into the load balancer requesting `nginx.foowidgets.k8s`,
    the only endpoint that will be used for traffic is running on cluster A. Once
    the issue has been resolved on cluster B, the load balancer will automatically
    add the cluster B endpoint back into the `nginx.foowidgets.k8s` record.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于任何请求`nginx.foowidgets.k8s`进入负载均衡器的流量，唯一会被用于流量的端点是在集群A上运行的。一旦集群B的问题得到解决，负载均衡器将自动将集群B的端点重新添加到`nginx.foowidgets.k8s`记录中。
- en: Such solutions are widespread in enterprises, with many organizations utilizing
    products from companies like **F5**, **Citrix**, **Kemp**, and **A10**, as well
    as CSP-native solutions like **Route 53** and **Traffic Director**, to manage
    workloads across multiple clusters. However, there are projects with similar functionality
    that integrate with Kubernetes, and they come at little to no cost. While these
    projects may not offer all the features of some vendor solutions, they often meet
    the needs of most use cases without requiring the full spectrum of expensive features.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的解决方案在企业中广泛应用，许多组织利用**F5**、**Citrix**、**Kemp**和**A10**等公司提供的产品，以及像**Route
    53**和**Traffic Director**这样的CSP本地解决方案来管理跨多个集群的工作负载。然而，也有一些与Kubernetes集成的具有类似功能的项目，而且这些项目几乎不需要成本。虽然这些项目可能没有一些供应商解决方案的所有功能，但它们通常能够满足大多数用例的需求，而无需完整的昂贵功能。
- en: One such project is **K8GB**, an innovative open-source project that brings
    **Global Server Load Balancing** (**GSLB**) to Kubernetes. With K8GB, organizations
    can easily distribute incoming network traffic across multiple Kubernetes clusters
    located in different geographical locations. By intelligently routing requests,
    K8GB guarantees low latency, optimal response times, and redundancy, providing
    an exceptional solution to any enterprise.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个项目是 **K8GB**，一个创新的开源项目，将 **全球服务器负载均衡** (**GSLB**) 引入 Kubernetes。通过 K8GB，组织可以轻松地将传入的网络流量分配到位于不同地理位置的多个
    Kubernetes 集群。通过智能地路由请求，K8GB 保证了低延迟、最佳响应时间和冗余，为任何企业提供了卓越的解决方案。
- en: This section will introduce you to K8GB, but if you want to learn more about
    the project, browse to the project’s main page at [https://www.k8gb.io](https://www.k8gb.io).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍 K8GB，但如果你想了解更多关于该项目的内容，请访问项目的主页 [https://www.k8gb.io](https://www.k8gb.io)。
- en: Since we are using KinD and a single host for our cluster, this section of the
    book is meant to introduce you to the project and the benefits it provides. This
    section is for reference only, since it is a complex topic that requires multiple
    components, some of which are outside of Kubernetes. If you decide you want to
    implement a solution for yourself, we have included example documentation and
    scripts in the book’s repo under the `chapter5/k8gs-example` directory.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用 KinD 和单一主机来搭建集群，本书的这一部分旨在向你介绍该项目及其带来的好处。本节内容仅供参考，因为这是一个复杂的话题，涉及多个组件，其中一些组件超出了
    Kubernetes 的范畴。如果你决定自己实现解决方案，我们已经在本书的代码库中包含了示例文档和脚本，位于 `chapter5/k8gs-example`
    目录下。
- en: K8GB is a CNCF sandbox project, which means it is in its early stages and any
    newer version after the writing of this chapter may have changes to objects and
    configurations.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: K8GB 是一个 CNCF 沙箱项目，这意味着它处于早期阶段，在本章写作后，任何更新的版本都可能对对象和配置做出更改。
- en: Introducing the Kubernetes Global Balancer
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 Kubernetes 全球负载均衡器
- en: Why should you care about a project like K8GB?
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么你应该关心像 K8GB 这样的项目？
- en: Let’s consider an internal enterprise cloud as an example, operating a Kubernetes
    cluster at a production site alongside another cluster at a disaster recovery
    site. To ensure a smooth user experience, it is important to enable applications
    to transition seamlessly between these data centers, without requiring any manual
    intervention during disaster recovery events. The challenge lies in fulfilling
    the enterprise’s demand for high availability of microservices when multiple clusters
    are simultaneously serving these applications. We need to effectively address
    the need for continuous and uninterrupted service across geographically dispersed
    Kubernetes clusters.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个内部企业云为例，企业在生产站点运营一个 Kubernetes 集群，并在灾难恢复站点运营另一个集群。为了确保顺畅的用户体验，重要的是让应用程序能够在这些数据中心之间无缝过渡，且在灾难恢复事件期间不需要任何人工干预。挑战在于，当多个集群同时为这些应用程序提供服务时，如何满足企业对微服务高可用性的需求。我们需要有效地解决跨地理分布的
    Kubernetes 集群中持续和不中断服务的需求。
- en: This is where **K8GB** comes in.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 **K8GB** 的作用。
- en: 'What makes K8GB an ideal solution for addressing our high availability requirements?
    As documented on their site, the key features include the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 是什么使得 K8GB 成为解决我们高可用性需求的理想方案？正如其网站所记录的，关键特性包括以下内容：
- en: Load balancing is provided using a timeproof DNS protocol that is extremely
    reliable and works well for global deployments
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡是通过一个非常可靠、适用于全球部署的抗时间考验的 DNS 协议提供的
- en: There is no requirement for a management cluster
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要管理集群
- en: There is no single point of failure
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有单点故障
- en: It uses native Kubernetes health checks for load balancing decisions
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用原生 Kubernetes 健康检查来做负载均衡决策
- en: Configuring is as simple as a single Kubernetes CRD
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置和创建一个 Kubernetes CRD 一样简单
- en: It works with any Kubernetes cluster – on-prem or off-prem
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它适用于任何 Kubernetes 集群 —— 无论是本地集群还是云端集群
- en: It’s free!
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是免费的！
- en: 'As you will see in this section, K8GB provides an easy and intuitive configuration
    that makes providing global load balancing to your organization easy. This may
    make K8GB look like it doesn’t do very much, but behind the scenes, it provides
    a number of advanced features, including:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在本节中看到的，K8GB 提供了一种简单直观的配置方式，使得为你的组织提供全球负载均衡变得容易。这可能让 K8GB 看起来没有做太多事情，但在幕后，它提供了许多先进的功能，包括：
- en: '**Global Load Balancing**: Facilitates the distribution of incoming network
    traffic among multiple Kubernetes clusters located in different geographic regions.
    As a result, it enables optimized application delivery, ensuring reduced latency
    and improved user experience for users.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全球负载均衡**：促进将传入的网络流量分配到位于不同地理区域的多个Kubernetes集群。由此，它能够优化应用交付，减少延迟并改善用户体验。'
- en: '**Intelligent Traffic Routing**: Utilizing sophisticated routing algorithms
    to intelligently steer client requests towards the closest or most appropriate
    Kubernetes cluster, considering factors like proximity, server health, and application-specific
    rules. This approach ensures efficient and highly responsive traffic management
    for optimal application performance.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**智能流量路由**：利用复杂的路由算法，智能地将客户端请求引导至最近或最合适的Kubernetes集群，考虑诸如接近度、服务器健康状况和特定应用规则等因素。这种方法确保了高效且响应迅速的流量管理，优化了应用的性能。'
- en: '**High Availability and Redundancy**: Guarantees high availability and fault
    tolerance for applications by automatically redirecting traffic in the event of
    cluster, application, or data center failure. This failover mechanism minimizes
    downtime during disaster recovery scenarios, ensuring uninterrupted service delivery
    to users.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高可用性和冗余性**：通过在集群、应用或数据中心发生故障时自动重定向流量，确保应用的高可用性和容错性。这种故障转移机制最大限度减少了灾难恢复场景中的停机时间，确保了对用户的不间断服务交付。'
- en: '**Automated Failover**: Simplifies operations by enabling automatic failover
    between data centers without the need for manual intervention. This eliminates
    the requirement for human-triggered **Disaster Recovery** (**DR**) events or tasks,
    ensuring quick and uninterrupted service delivery and streamlined operations.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动故障切换**：通过启用数据中心之间的自动故障切换，简化了操作，免去了人工干预的需求。这消除了对人为触发的**灾难恢复**（**DR**）事件或任务的要求，确保了快速和不间断的服务交付以及精简的运营。'
- en: '**Integration with Kubernetes**: Offers seamless integration with Kubernetes,
    simplifying the setup and configuration of GSLB for applications deployed on clusters.
    Leveraging Kubernetes’ native capabilities, K8GB delivers a scalable solution,
    enhancing the overall management and efficiency of global load balancing operations.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与Kubernetes的集成**：提供与Kubernetes的无缝集成，简化了为部署在集群中的应用配置GSLB的设置过程。通过利用Kubernetes的原生能力，K8GB提供了一个可扩展的解决方案，提升了全球负载均衡操作的整体管理和效率。'
- en: '**On-Prem and Cloud Provider Support**: Provides enterprises a way to efficiently
    manage GSLB for multiple Kubernetes clusters, enabling seamless handling of complex
    multi-region deployments and hybrid cloud scenarios. This ensures optimized application
    delivery across different environments, enhancing the overall performance and
    resilience of the infrastructure.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地部署和云服务商支持**：为企业提供了一种有效管理多个Kubernetes集群的GSLB方式，使复杂的多区域部署和混合云场景的处理变得无缝。这确保了在不同环境中优化应用交付，提升了基础设施的整体性能和韧性。'
- en: '**Customization and Flexibility**: Provides users the freedom to define personalized
    rules and policies for traffic routing, providing organizations with the flexibility
    to customize GSLB configurations to meet their unique requirements precisely.
    This empowers enterprises to optimize traffic management based on their specific
    needs and ensures seamless adaptation to ever-changing application demands.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定制化和灵活性**：为用户提供定义个性化流量路由规则和策略的自由，使组织能够灵活定制GSLB配置，精确满足其独特需求。这使企业能够根据具体需求优化流量管理，并确保能无缝适应不断变化的应用需求。'
- en: '**Monitoring, Metrics, and Tracing**: Includes monitoring, metrics, and tracing
    capabilities, enabling administrators to access insights into traffic patterns,
    health, and performance metrics spanning across multiple clusters. This provides
    enhanced visibility, empowering administrators to make informed decisions and
    optimize the overall performance and reliability of the GSLB setup.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控、度量和追踪**：包括监控、度量和追踪功能，使管理员能够访问跨多个集群的流量模式、健康状况和性能指标的洞察。这提供了增强的可视性，帮助管理员做出明智的决策，并优化GSLB设置的整体性能和可靠性。'
- en: Now that we have discussed the key features of K8GB, let’s get into the details.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了K8GB的主要特性，接下来我们将深入了解细节。
- en: Requirements for K8GB
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K8GB的要求
- en: 'For a product that provides a complex function like global load balancing,
    K8GB doesn’t require a lot of infrastructure or resources to provide load balancing
    to your clusters. The latest release, which, as of this chapter’s creation, is
    `0.12.2` – has only a handful of requirements:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像全球负载均衡这样提供复杂功能的产品，K8GB 不需要大量的基础设施或资源就可以为您的集群提供负载均衡。最新版本（截至本章节编写时）是`0.12.2`——它只有少数几个要求：
- en: The CoreDNS servers Load Balancer IP address in the main DNS zone using the
    naming standard `gslb-ns-<k8gb-name>-gb.foowidgets.k8s` – for example, `gslb-ns-us-nyc-gb.foowidgets.k8s
    and gslb-ns-us-buf-gb.foowidgets.k8s`
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreDNS 服务器的负载均衡器 IP 地址，使用命名标准 `gslb-ns-<k8gb-name>-gb.foowidgets.k8s`——例如，`gslb-ns-us-nyc-gb.foowidgets.k8s`
    和 `gslb-ns-us-buf-gb.foowidgets.k8s`
- en: If you are using K8GB with a service like **Route 53**, **Infoblox**, or **NS1**,
    the CoreDNS servers will be added to the domain automatically. Since our example
    is using an on-premises DNS server running on a Windows 2019 server, we need to
    create the records manually.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是像 **Route 53**、**Infoblox** 或 **NS1** 这样的服务，CoreDNS 服务器将自动添加到域中。由于我们的示例使用的是运行在
    Windows 2019 服务器上的本地 DNS 服务器，我们需要手动创建记录。
- en: An Ingress controller
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Ingress 控制器
- en: 'A K8GB controller deployed in the cluster, which will deploy:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群中部署的 K8GB 控制器，它将部署：
- en: The K8GB controller
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: K8GB 控制器
- en: A CoreDNS server with the CoreDNS CRD plugin configured – this is included in
    the deployment on K8GB
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置了 CoreDNS CRD 插件的 CoreDNS 服务器——这已经包含在 K8GB 的部署中
- en: 'Since we have already explored NGINX ingress controllers in previous chapters,
    we now turn our attention to the additional requirements: deploying and configuring
    the K8GB controller within a cluster.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在前面的章节中已经探讨了 NGINX Ingress 控制器，现在我们将重点关注额外的要求：在集群内部署和配置 K8GB 控制器。
- en: In the next section, we will discuss the steps to implement K8GB.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论实现 K8GB 的步骤。
- en: Deploying K8GB to a cluster
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 K8GB 部署到集群
- en: 'We have included example files in the GitHub repo under the `chapter5/k8gb-example`
    directory. The scripts are based on the example we will use for the remainder
    of the chapter. If you decide to use the files in a development cluster, you will
    need to meet the requirements below:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已在 GitHub 仓库的 `chapter5/k8gb-example` 目录下包含了示例文件。脚本基于我们将在本章余下部分中使用的示例。如果您决定在开发集群中使用这些文件，您需要满足以下要求：
- en: Two Kubernetes clusters (a single-node `kubeadm` cluster for each cluster will
    work)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个 Kubernetes 集群（每个集群可以使用一个单节点的 `kubeadm` 集群）
- en: CoreDNS deployed in each cluster
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个集群中部署的 CoreDNS
- en: K8GB deployed in each cluster
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个集群中部署 K8GB
- en: An edge DNS server that you can use to delegate the domain for K8GB
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个边缘 DNS 服务器，您可以用来委派 K8GB 的域
- en: Installing K8GB has been made very simple – you only need to deploy a single
    Helm chart using a `values.yaml` file that has been configured for your infrastructure.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 K8GB 已经变得非常简单——您只需要使用已为您的基础设施配置好的 `values.yaml` 文件部署一个单一的 Helm 图表。
- en: 'To install K8GB, you will need to add the K8GB repository to your Helm repo
    list and then update the charts:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 K8GB，您需要将 K8GB 仓库添加到您的 Helm 仓库列表中，然后更新图表：
- en: '[PRE12]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Before we execute a `helm install` command, we need to customize the Helm `values.yaml`
    file for each cluster deployment. We have included a values file for both of the
    clusters used in our example, `k8gb-buff-values.yaml` and `k8gb-nyc-values.yaml`,
    located in the `chapter5/k8gb-example` directory. The options in the values file
    will be discussed in the *Customizing the Helm chart values* section.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行 `helm install` 命令之前，我们需要为每个集群部署自定义 Helm `values.yaml` 文件。我们已经在 `chapter5/k8gb-example`
    目录中包含了我们示例中使用的两个集群的 values 文件，分别是 `k8gb-buff-values.yaml` 和 `k8gb-nyc-values.yaml`。这些选项将在
    *自定义 Helm 图表值* 一节中讨论。
- en: Understanding K8GB load balancing options
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解 K8GB 的负载均衡选项
- en: For our example, we will configure K8GB as a failover load balancer between
    two on-premises clusters; however, K8GB is not limited to just failover. Like
    most load balancers, K8GB offers a variety of solutions that can be configured
    differently for each load-balanced URL. It offers the most commonly required strategies,
    including round robin, weighted round robin, failover, and GeoIP.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将配置 K8GB 作为两个本地集群之间的故障切换负载均衡器；然而，K8GB 不仅仅局限于故障切换。像大多数负载均衡器一样，K8GB
    提供多种解决方案，可以根据每个负载均衡的 URL 配置不同的策略。它提供了最常用的策略，包括轮询、加权轮询、故障切换和 GeoIP。
- en: 'Each of the strategies is described below:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 每个策略的描述如下：
- en: '**Round Robin**: If you do not specify a strategy, it will default to a simple
    round-robin load balancing configuration. Using round robin means that requests
    will be split between the configured clusters – request 1 will go to cluster 1,
    request 2 will go to cluster 2, request 3 will go to cluster 1, request 4 will
    go to cluster 2, and so on.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轮询（Round Robin）**：如果未指定策略，它将默认为简单的轮询负载均衡配置。使用轮询意味着请求将在配置的集群之间进行分配——请求1将发送到集群1，请求2将发送到集群2，请求3将发送到集群1，请求4将发送到集群2，以此类推。'
- en: '**Weighted Round Robin**: Similar to round robin, this strategy provides the
    ability to specify the percentage of traffic to send to a cluster; for example,
    75% of traffic will go to cluster 1 and 15% will go to cluster 2.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权轮询（Weighted Round Robin）**：类似于轮询，该策略提供了指定流量百分比发送到某个集群的能力；例如，75%的流量将发送到集群1，15%的流量将发送到集群2。'
- en: '**Failover**: All traffic will go to the primary cluster unless all pods for
    a deployment become unavailable. If all pods are down in cluster 1, cluster 2
    will take over the workload until the pods in cluster 1 become available again,
    which will then become the primary cluster again.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障转移（Failover）**：除非某个部署的所有Pod不可用，否则所有流量将发送到主集群。如果集群1中的所有Pod都宕机，集群2将接管工作负载，直到集群1中的Pod恢复可用，届时集群1将重新成为主集群。'
- en: '**GeoIP:** Directs requests to the closest cluster to the client connection.
    If the closest host is down, it will use a different cluster similar to how the
    failover strategy works. To use this strategy, you will need to create a GeoIP
    database (an example can be found here: [https://github.com/k8gb-io/coredns-crd-plugin/tree/main/terratest/geogen](https://github.com/k8gb-io/coredns-crd-plugin/tree/main/terratest/geogen)),
    and your DNS server needs to support the **EDNS0** extension.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GeoIP：** 将请求定向到与客户端连接最近的集群。如果最近的主机宕机，它将使用另一个集群，类似于故障转移策略的工作方式。要使用此策略，你需要创建一个GeoIP数据库（示例可参考此处：[https://github.com/k8gb-io/coredns-crd-plugin/tree/main/terratest/geogen](https://github.com/k8gb-io/coredns-crd-plugin/tree/main/terratest/geogen)），并且你的DNS服务器需要支持**EDNS0**扩展。'
- en: '**EDNS0** is based on RFC 2671, which outlines how EDNS0 works and its various
    components, including the format of EDNS0-enabled DNS messages, the structure
    of EDNS0 options, and guidelines for its implementation. The goal of RFC 2671
    is to provide a standardized approach for extending the capabilities of the DNS
    protocol beyond its original limitations, allowing for the incorporation of new
    features, options, and enhancements'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**EDNS0**基于RFC 2671，该规范概述了EDNS0的工作原理及其各个组成部分，包括启用EDNS0的DNS消息格式、EDNS0选项的结构以及其实现指南。RFC
    2671的目标是提供一种标准化的方法，用于扩展DNS协议的功能，突破其原有的局限性，允许加入新功能、选项和增强特性。'
- en: 'Now that you know the available strategies, let’s go over our example infrastructure
    for our clusters:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了可用的策略，我们来回顾一下我们集群的示例基础设施：
- en: '| **Cluster/Server Details** | **Details** |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| **集群/服务器详情** | **详情** |'
- en: '| Corporate DNS Server – New York CityIP: `10.2.1.14` | Main corporate zone`foowidgets.k8s`Host
    records for the CoreDNS servers`gslb-ns-us-nyc-gb.foowidgets.k8s``gslb-ns-us-buf-gb.foowidgets.k8s`Global
    domain configured delegating to the CoreDNS servers in the clusters`gb.foowidgets.k8s`
    |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 企业DNS服务器 – 纽约市IP：`10.2.1.14` | 主要企业区域`foowidgets.k8s`CoreDNS服务器的主机记录`gslb-ns-us-nyc-gb.foowidgets.k8s``gslb-ns-us-buf-gb.foowidgets.k8s`全球域名配置，委派给集群中的CoreDNS服务器`gb.foowidgets.k8s`
    |'
- en: '| `New York City`, New York – Cluster 1Primary SiteCoreDNS LB IP: `10.2.1.221`Ingress
    IP: `10.2.1.98` | NGINX Ingress Controller exposed using HostPortCoreDNS deployment
    exposed using MetalLB |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| `纽约市`，纽约 – 集群1主站CoreDNS负载均衡IP：`10.2.1.221`Ingress IP：`10.2.1.98` | 使用HostPort公开的NGINX
    Ingress Controller，使用MetalLB公开的CoreDNS部署 |'
- en: '| `Buffalo`, New York – Cluster 2Secondary SiteCoreDNS LB IP: `10.2.1.224`Ingress
    IP: `10.2.1.167` | NGINX Ingress Controller exposed using HostPortCoreDNS deployment
    exposed using MetalLB |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| `布法罗`，纽约 – 集群2次要站CoreDNS负载均衡IP：`10.2.1.224`Ingress IP：`10.2.1.167` | 使用HostPort公开的NGINX
    Ingress Controller，使用MetalLB公开的CoreDNS部署 |'
- en: 'Table 5.1: Cluster details'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1：集群详情
- en: We will use the details from the above table to explain how we would deploy
    K8GB in our example infrastructure.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用上表中的详情来解释如何在我们的示例基础设施中部署K8GB。
- en: With the details of the infrastructure, we can now create our Helm `values.yaml`
    files for each deployment. In the next section, we will show the values we need
    to configure using the example infrastructure, explaining each value.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 通过基础设施的详细信息，我们现在可以为每个部署创建我们的Helm `values.yaml`文件。在接下来的部分，我们将展示我们需要使用示例基础设施配置的值，并解释每个值的含义。
- en: Customizing the Helm chart values
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义Helm图表值
- en: 'Each cluster will have a similar values file; the main changes will be the
    tag values we use. The values file below is for the New York City cluster:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 每个集群将有一个类似的值文件；主要的变化是我们使用的标签值。下面的值文件是纽约市集群的配置：
- en: '[PRE13]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The same file contents will be used for the NYC cluster, with the exception
    of the `clusterGeoTag` and `extGslbClustersGeoTags` values, for the NYC cluster
    they need to be set to:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 NYC 集群，除了 `clusterGeoTag` 和 `extGslbClustersGeoTags` 值外，其他文件内容相同，对于 NYC 集群，这些值需要设置为：
- en: '[PRE14]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, the configuration isn’t very lengthy, requiring only a handful
    of options to configure a usually complex global load balancing configuration.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个配置并不冗长，仅需要少数选项来配置通常复杂的全局负载均衡配置。
- en: Now, let’s go over some of the main details of the values we are using.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来了解一下我们使用的部分主要配置项。
- en: The main details that we will explain are the values in the K8GB section, which
    configures all of the options K8GB will use for load balancing.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将解释的主要内容是 K8GB 部分的配置值，这些值配置了 K8GB 用于负载均衡的所有选项。
- en: '| **Chart Value** | **Description** |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| **Chart 值** | **描述** |'
- en: '| `dnsZone` | This is the DNS zone that you will use for K8GB – basically,
    this is the zone that will be used for the DNS records that will be used to store
    our global load balanced DNS records. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `dnsZone` | 这是你将用于 K8GB 的 DNS 区域——基本上，这是用于存储我们全局负载均衡的 DNS 记录的 DNS 区域。 |'
- en: '| `edgeDNSZone` | The main DNS zone that contains the DNS records for the CoreDNS
    servers that are used by the previous option (`dnsZone`). |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| `edgeDNSZone` | 包含用于前一个选项（`dnsZone`）的 CoreDNS 服务器的 DNS 记录的主要 DNS 区域。 |'
- en: '| `edgeDNSServers` | The edge DNS server – usually the main DNS server used
    for name resolution. |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `edgeDNSServers` | 边缘 DNS 服务器——通常是用于名称解析的主要 DNS 服务器。 |'
- en: '| `clusterGeoTag` | If you have multiple K8GB controllers, this tag is used
    to specify instances between each other. In our example, we set these to `us-buf`
    and `us-nyc` for our clusters. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| `clusterGeoTag` | 如果你有多个 K8GB 控制器，使用此标签来指定彼此之间的实例。在我们的示例中，我们将其设置为 `us-buf`
    和 `us-nyc` 来表示我们的集群。 |'
- en: '| `extGslbClusterGeoTags` | Specifies the other K8GB controllers to pair with.
    In our example, each cluster adds the other cluster `clusterGeoTags` – the `Buffalo`
    cluster adds the `us-nyc` tag and the NYC cluster adds the `us-buf` tag. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `extGslbClusterGeoTags` | 指定要与之配对的其他 K8GB 控制器。在我们的示例中，每个集群会添加另一个集群的 `clusterGeoTags`
    —— `Buffalo` 集群添加 `us-nyc` 标签，NYC 集群添加 `us-buf` 标签。 |'
- en: '| `isClusterService` | Set to `true` or `false`. Used for service upgrades;
    you can read more at [https://www.k8gb.io/docs/service_upgrade.xhtml](https://www.k8gb.io/docs/service_upgrade.xhtml).
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `isClusterService` | 设置为 `true` 或 `false`。用于服务升级；你可以在 [https://www.k8gb.io/docs/service_upgrade.xhtml](https://www.k8gb.io/docs/service_upgrade.xhtml)
    阅读更多信息。 |'
- en: '| `exposeCoreDNS` | If set to `true`, a `LoadBalancer` service will be created,
    exposing the CoreDNS deployed in the `k8gb` namespace on port `53`/UDP for external
    access. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `exposeCoreDNS` | 如果设置为 `true`，将创建一个 `LoadBalancer` 服务，公开在 `k8gb` 命名空间中部署的
    CoreDNS，使用端口 `53`/UDP 供外部访问。 |'
- en: '| `deployment.skipConfig` | Set to true or false. Setting it to false tells
    the deployment to use the CoreDNS shipped with K8GB. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| `deployment.skipConfig` | 设置为 true 或 false。设置为 false 时，告诉部署使用 K8GB 附带的 CoreDNS。
    |'
- en: '| `image.repository` | Configures the repository to use for the CoreDNS image.
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| `image.repository` | 配置用于 CoreDNS 镜像的仓库。 |'
- en: '| `image.tag` | Configures the tag to use when pulling the image. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| `image.tag` | 配置拉取镜像时使用的标签。 |'
- en: '| `serviceAccount.create` | Set to `true` or `false`. When set to true, a service
    account will be created. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| `serviceAccount.create` | 设置为 `true` 或 `false`。设置为 true 时，将创建一个服务帐户。 |'
- en: '| `serviceAccount.name` | Sets the name of the service account from the previous
    option. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| `serviceAccount.name` | 设置来自前一个选项的服务帐户名称。 |'
- en: '| `serviceType` | Configures the service type for CoreDNS. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| `serviceType` | 配置 CoreDNS 的服务类型。 |'
- en: 'Table 5.2: K8GB options'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.2：K8GB 配置选项
- en: Using Helm to install K8GB
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Helm 安装 K8GB
- en: With the overview of K8GB and the Helm values file complete, we can move on
    to installing K8GB in the clusters. We have included scripts to deploy K8GB to
    the `Buffalo` and `NYC` clusters. In `chapter5/k8gb-example/k8gb`, you will see
    two scripts, deploy-`k8gb-buf.sh` and `deploy-k8gb-nyc.sh` – these should be run
    in their corresponding clusters.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 完成 K8GB 概述和 Helm 值文件配置后，我们可以开始在集群中安装 K8GB。我们已经包含了将 K8GB 部署到 `Buffalo` 和 `NYC`
    集群的脚本。在 `chapter5/k8gb-example/k8gb` 目录下，你会看到两个脚本，分别是 `deploy-k8gb-buf.sh` 和 `deploy-k8gb-nyc.sh`
    —— 这些脚本应在各自的集群中运行。
- en: 'The script will execute the following steps:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本将执行以下步骤：
- en: Add the K8GB Helm repo to the server’s repo list
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 K8GB Helm 仓库添加到服务器的仓库列表中
- en: Update the repos
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新仓库
- en: Deploy K8GB using the appropriate Helm values file
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用适当的 Helm 值文件部署 K8GB
- en: Create a **Gslb record** (covered in the next section)
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 **Gslb 记录**（在下一部分中讲解）
- en: Create a deployment to use for testing in a namespace called `demo`
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `demo` 的命名空间中的部署，用于测试
- en: 'Once deployed, you will see two pods running in the `k8gb` namespace, one for
    the `k8gb` controller and the other for the CoreDNS server that will be used to
    resolve load balancing names:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 部署完成后，您将看到在 `k8gb` 命名空间中运行的两个 pod，一个是 `k8gb` 控制器，另一个是用于解析负载均衡名称的 CoreDNS 服务器：
- en: '[PRE15]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can also verify that the services were created to handle the incoming DNS
    requests. Since we exposed it using a `LoadBalancer` type, we will see the `LoadBalancer`
    service on port `53` using the UDP protocol:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以验证已经创建了服务来处理传入的 DNS 请求。由于我们使用 `LoadBalancer` 类型进行了暴露，我们将看到在端口 `53` 上使用
    UDP 协议的 `LoadBalancer` 服务：
- en: '[PRE16]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With the deployment of K8GB complete and verified in both clusters, let’s move
    on to the next section where we will explain how to create our edge delegation
    for our load balanced zone.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在 K8GB 部署完成并在两个集群中验证后，让我们继续下一部分，解释如何为我们的负载均衡区创建边缘委派。
- en: Delegating our load balancing zone
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 委派我们的负载均衡区
- en: For our example, we are using a Windows server as our edge DNS server, which
    is where our K8s DNS names will be registered. On the DNS side, we need to add
    two DNS records for our CoreDNS servers, and then we need to delegate the load
    balancing zone to these servers.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用 Windows 服务器作为边缘 DNS 服务器，在这里我们的 K8s DNS 名称将被注册。在 DNS 方面，我们需要为 CoreDNS
    服务器添加两个 DNS 记录，然后我们需要将负载均衡区委派给这些服务器。
- en: 'The two A records need to be in the main edge DNS zone. In our example, that
    is the `foowidgets.k8s` zone. In this zone, we need to add two entries for the
    CoreDNS servers that are exposed using a `LoadBalancer` service:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个 A 记录需要位于主边缘 DNS 区域中。在我们的示例中，这是 `foowidgets.k8s` 区域。在此区域中，我们需要为通过 `LoadBalancer`
    服务暴露的 CoreDNS 服务器添加两个条目：
- en: '![](img/B21165_05_09.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_09.png)'
- en: 'Figure 5.9: Adding our CoreDNS servers to the edge zone'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9：将我们的 CoreDNS 服务器添加到边缘区域
- en: Next, we need to create a new delegated zone that will be used for our load
    balanced service names. In Windows, this is done by *right-clicking* the zone
    and selecting **New Delegation**; in the delegation wizard, you will be asked
    for the **Delegated domain**. In our example, we are going to delegate the **gb**
    domain as our domain.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建一个新的委派区，用于我们的负载均衡服务名称。在 Windows 中，这是通过 *右键单击* 区域并选择 **新建委派** 来完成的；在委派向导中，系统会要求您提供
    **委派域**。在我们的示例中，我们将委派 **gb** 域作为我们的域。
- en: '![](img/B21165_05_10.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_10.png)'
- en: 'Figure 5.10: Creating a new delegated zone'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10：创建一个新的委派区
- en: After you enter the zone name and click **Next**, you will see a new screen
    to add DNS servers for the delegated domain; when you click **Add**, you will
    enter the DNS names for your CoreDNS servers. Remember that we created two A records
    in the main domain, `foowidgets.com`. As you add entries, Windows will verify
    that the entered name resolves correctly and that DNS queries work. Once you add
    both CoreDNS servers, the summary screen will show both with their IP addresses.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入区域名称并点击 **下一步** 后，您将看到一个新的屏幕，要求您为委派域添加 DNS 服务器；点击 **添加** 后，您将输入 CoreDNS 服务器的
    DNS 名称。记住我们在主域 `foowidgets.com` 中创建了两个 A 记录。当您添加条目时，Windows 会验证输入的名称是否正确解析，DNS
    查询是否正常工作。添加了两个 CoreDNS 服务器后，摘要屏幕将显示它们的 IP 地址。
- en: '![](img/B21165_05_11.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_05_11.png)'
- en: 'Figure 5.11: Adding the DNS names for the CoreDNS servers'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11：为 CoreDNS 服务器添加 DNS 名称
- en: That completes the edge server configuration. For certain edge servers, K8GB
    will create the delegation records, but there are only a handful of servers that
    are compatible with that feature. For any edge server that doesn’t auto create
    the delegated servers, you need to create them manually like we did in this section.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了边缘服务器的配置。对于某些边缘服务器，K8GB 会自动创建委派记录，但只有少数几台服务器支持此功能。对于那些不能自动创建委派服务器的边缘服务器，您需要像本节中那样手动创建它们。
- en: Now that we have CoreDNS in our clusters and we have delegated the load balanced
    zone, we will deploy an application that has global load balancing to test our
    configuration.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在集群中部署了 CoreDNS，并且委派了负载均衡区，接下来我们将部署一个具有全球负载均衡的应用程序来测试我们的配置。
- en: Deploying a highly available application using K8GB
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 K8GB 部署一个高可用应用程序
- en: There are two methods to enable global load balancing for an application. You
    can create a new record using a custom resource provided by K8GB, or you can annotate
    an Ingress rule. For our demonstration of K8GB, we will deploy a simple NGINX
    web server in a cluster and add it to K8GB using the natively supplied custom
    resource.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 启用应用程序的全局负载均衡有两种方法。你可以使用 K8GB 提供的自定义资源创建一个新记录，或者你也可以为 Ingress 规则添加注释。为了演示 K8GB，我们将在集群中部署一个简单的
    NGINX Web 服务器，并使用原生提供的自定义资源将其添加到 K8GB。
- en: Adding an application to K8GB using custom resources
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用自定义资源将应用程序添加到 K8GB
- en: When we deployed K8GB, a new **Custom Resource Definition** (**CRD**) named
    `Gslb` was added to the cluster. This CRD assumes the role of managing applications
    marked for global load balancing. Within the `Gslb` object, we define a specification
    for the Ingress name, mirroring the format of a regular Ingress object. The sole
    distinction between a standard Ingress and a `Gslb` object lies in the last portion
    of the manifest, the strategy.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们部署 K8GB 时，一个新的 **自定义资源定义** (**CRD**) 名为 `Gslb` 被添加到了集群中。这个 CRD 扮演着管理标记为全局负载均衡的应用程序的角色。在
    `Gslb` 对象中，我们定义了 Ingress 名称的规范，格式与常规的 Ingress 对象相同。标准 Ingress 和 `Gslb` 对象之间唯一的区别在于清单的最后部分，即策略。
- en: The strategy defines the type of load balancing we want to use, which is failover
    for our example, and the primary GeoTag to use for the object. In our example,
    the NYC cluster is our primary cluster, so our `Gslb` object will be set to `us-buf`.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 策略定义了我们想要使用的负载均衡类型，在我们的示例中是故障转移策略，以及该对象要使用的主 GeoTag。在我们的示例中，NYC 集群是我们的主集群，因此我们的
    `Gslb` 对象将设置为 `us-buf`。
- en: 'To deploy an application that will leverage load balancing, we need to create
    the following in both clusters:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署一个将利用负载均衡的应用程序，我们需要在两个集群中创建以下内容：
- en: A standard deployment and service for the application. We will call the deployment
    `nginx`, using the standard NGINX image.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用程序的标准部署和服务。我们将把部署命名为 `nginx`，使用标准的 NGINX 镜像。
- en: 'A `Gslb` object in each cluster. For our example, we will use the manifest
    below, which will declare the Ingress rule and set the strategy to failover using
    `us-buf` as the primary K8GB. Since the `Gslb` object has the information for
    the Ingress rule, you do not need to create an Ingress rule; `Gslb` will create
    the Ingress object for us. Let’s look at an example below:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个集群中的一个 `Gslb` 对象。对于我们的示例，我们将使用以下清单，该清单将声明 Ingress 规则，并将策略设置为故障转移，使用 `us-buf`
    作为主要的 K8GB。由于 `Gslb` 对象包含 Ingress 规则的信息，你无需创建 Ingress 规则；`Gslb` 会为我们创建 Ingress
    对象。以下是一个示例：
- en: '[PRE17]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: When you deploy the manifest for the `Gslb` object, it will create two Kubernetes
    objects, the `Gslb` object and an Ingress object.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 当你部署 `Gslb` 对象的清单时，它将创建两个 Kubernetes 对象，分别是 `Gslb` 对象和 Ingress 对象。
- en: 'If we looked at the `demo` namespace for the `Gslb` objects in the `Buffalo`
    cluster, we would see the following:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看 `Buffalo` 集群中的 `demo` 命名空间下的 `Gslb` 对象，我们将看到以下内容：
- en: '[PRE18]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And if we looked at the Ingress objects in the NYC cluster, we would see:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看 NYC 集群中的 Ingress 对象，我们会看到：
- en: '[PRE19]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We would also have similar objects in the NYC cluster, which we will explain
    in the *Understanding how K8GB provides global load balancing* section.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将在 NYC 集群中有类似的对象，具体内容将在 *理解 K8GB 如何提供全局负载均衡* 部分进行解释。
- en: Adding an application to K8GB using Ingress annotations
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Ingress 注释将应用程序添加到 K8GB
- en: The second method for adding an application to K8GB is to add two annotations
    to a standard Ingress rule, which was primarily added to allow developers to add
    an existing Ingress rule to K8GB.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 将应用程序添加到 K8GB 的第二种方法是向标准的 Ingress 规则添加两个注释，这主要是为了允许开发人员将现有的 Ingress 规则添加到 K8GB。
- en: 'To add an Ingress object to the global load balancing list, you only need to
    add two annotations to the Ingress object, `strategy` and `primary-geotag`. An
    example of the annotations is shown below:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 Ingress 对象添加到全局负载均衡列表中，你只需要向 Ingress 对象添加两个注释：`strategy` 和 `primary-geotag`。以下是注释的示例：
- en: '[PRE20]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This would add the Ingress to K8GB using the failover strategy using the `us-buf`
    GeoTag as the primary tag.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用 `us-buf` GeoTag 作为主标签，通过故障转移策略将 Ingress 添加到 K8GB。
- en: Now that we have deployed all of the required infrastructure components and
    all of the required objects to enable global load balancing for an application,
    let’s see it in action.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经部署了所有必需的基础设施组件和所有必需的对象，以便为应用程序启用全局负载均衡，让我们看看它的实际运行情况。
- en: Understanding how K8GB provides global load balancing
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 K8GB 如何提供全局负载均衡
- en: The design of K8GB is complex, but once you deploy an application and understand
    how K8GB maintains zone files, it will become easier. This is a fairly complex
    topic, and it does assume some previous knowledge of how DNS works, but by the
    end of this section, you should be able to explain how K8GB works.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: K8GB 的设计很复杂，但一旦你部署了应用程序并了解了 K8GB 如何维护区域文件，理解起来就会变得容易。这个话题相当复杂，它假设你对 DNS 如何工作有一些先验知识，但在本节结束时，你应该能够解释
    K8GB 是如何工作的。
- en: Keeping the K8GB CoreDNS servers in sync
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保持 K8GB CoreDNS 服务器同步
- en: The first topic to discuss is how K8GB manages to keep two, or more, zone files
    in sync to provide seamless failover for our deployments. Seamless failover is
    a process that ensures an application continues to run smoothly even during system
    issues or failures. It automatically transitions to the backup system or resource,
    maintaining an uninterrupted user experience.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个要讨论的话题是 K8GB 如何保持两个或更多区域文件同步，以提供我们部署的无缝故障切换。无缝故障切换是一个确保即使在系统出现问题或故障时，应用程序仍能平稳运行的过程。它会自动切换到备份系统或资源，保持用户体验不中断。
- en: As we mentioned earlier, each K8GB CoreDNS server in the clusters must have
    an entry in the main DNS server.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，每个 K8GB CoreDNS 服务器必须在主 DNS 服务器中有一条记录。
- en: 'This is the DNS server and zone that we configured for the edge values in the
    `values.yaml` file:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们为边缘值在 `values.yaml` 文件中配置的 DNS 服务器和区域：
- en: '[PRE21]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'So, in the edge DNS server (`10.2.1.14`), we have a host record for each CoreDNS
    server using the required K8GB naming convention:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在边缘 DNS 服务器（`10.2.1.14`）中，我们为每个 CoreDNS 服务器使用所需的 K8GB 命名约定配置了主机记录：
- en: '[PRE22]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: K8GB will communicate between all of the CoreDNS servers and update any records
    that need to be updated due to being added, deleted, or updated.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: K8GB 会在所有 CoreDNS 服务器之间进行通信，并更新任何因新增、删除或更新而需要更新的记录。
- en: 'This becomes a little easier to understand with an example. Using our cluster
    example, we have deployed an NGINX web server and created all of the required
    objects in both clusters. After deploying, we would have a `Gslb` and Ingress
    object in each cluster, as shown below:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个例子，这个过程变得更容易理解。使用我们的集群示例，我们已部署了一个 NGINX Web 服务器，并在两个集群中创建了所有必需的对象。部署后，我们将在每个集群中拥有一个
    `Gslb` 和一个 Ingress 对象，如下所示：
- en: '| Cluster: NYCDeployment: `nginx``Gslb: gslb-failover-nyc`Ingress: `fe.gb.foowidgets.k8s`NGINX
    Ingress IP: `10.2.1.98` | Cluster: `Buffalo` (Primary)Deployment: `nginx``Gslb:
    gslb-failover-buf`Ingress: `fe.gb.foowidgets.k8s`NGINX Ingress IP: `10.2.1.167`
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 集群：NYC部署：`nginx` `Gslb: gslb-failover-nyc` Ingress：`fe.gb.foowidgets.k8s`
    NGINX Ingress IP：`10.2.1.98` | 集群：`Buffalo`（主集群）部署：`nginx` `Gslb: gslb-failover-buf`
    Ingress：`fe.gb.foowidgets.k8s` NGINX Ingress IP：`10.2.1.167` |'
- en: 'Table 5.3: Objects in each cluster'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.3：每个集群中的对象
- en: 'Since the deployment is healthy in both clusters, the CoreDNS servers will
    have a record for `fe.gb.foowidgets.k8s` with an IP address of `10.2.1.167`, the
    primary deployment. We can verify this by running a `dig` command on any client
    machine that uses the edge DNS server (`10.2.1.14`):'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 由于部署在两个集群中都处于健康状态，CoreDNS 服务器将为 `fe.gb.foowidgets.k8s` 配置一个 IP 地址 `10.2.1.167`，这是主部署。我们可以通过在任何使用边缘
    DNS 服务器（`10.2.1.14`）的客户端机器上运行 `dig` 命令来验证这一点：
- en: '[PRE23]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As you can see in the output from `dig`, the host resolved to `10.2.1.167`
    since the application is healthy in the primary cluster. If we curl the DNS name,
    we will see that the NGINX server in `Buffalo` replies:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在 `dig` 输出中看到的，由于应用程序在主集群中处于健康状态，主机解析为 `10.2.1.167`。如果我们使用 curl 查询该 DNS 名称，将看到
    `Buffalo` 中的 NGINX 服务器回应：
- en: '[PRE24]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We will simulate a failure by scaling the replicas for the deployment in the
    `Buffalo` cluster to `0`, which will look like a failed application to K8GB. When
    the K8GB controller in the NYC cluster sees that the application no longer has
    any healthy endpoints, it will update the CoreDNS record in all servers with the
    secondary IP address to fail the service over to the secondary cluster.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过将 `Buffalo` 集群中部署的副本数缩放为 `0` 来模拟故障，这将使 K8GB 看起来像是应用程序故障。当 NYC 集群中的 K8GB
    控制器发现应用程序不再有任何健康的端点时，它将更新所有服务器中的 CoreDNS 记录，使用备用 IP 地址将服务切换到辅助集群。
- en: 'Once scaled down, we can use `dig` to verify what host is returned:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦缩小规模，我们可以使用 `dig` 来验证返回了哪个主机：
- en: '[PRE25]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will `curl` again to verify that the workload has been moved to the NYC
    cluster. When we execute curl, we will see that the NGINX server is now located
    in the NYC cluster:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用 `curl` 来验证工作负载是否已移动到 NYC 集群。当我们执行 curl 时，将看到 NGINX 服务器现在位于 NYC 集群中：
- en: '[PRE26]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that the IP address returned is now the IP address for the deployment in
    the `Buffalo` cluster, the secondary cluster, `10.2.1.98`. This proves that K8GB
    is working correctly and providing us with a Kubernetes-controlled global load
    balancer.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，返回的IP地址现在是`Buffalo`集群中的部署IP地址，即次要集群的`10.2.1.98`。这证明了K8GB正常工作并为我们提供了一个由Kubernetes控制的全球负载均衡器。
- en: 'Once the application becomes healthy in the primary cluster, K8GB will update
    CoreDNS and any requests will resolve to the main cluster again. To test this,
    we scaled the deployment in `Buffalo` back up to `1` and ran another dig test:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦应用程序在主集群中变得健康，K8GB将更新CoreDNS，任何请求将再次解析到主集群。为了测试这一点，我们将`Buffalo`集群中的部署规模重新扩展到`1`，并运行了另一个dig测试：
- en: '[PRE27]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We can see that the IP has been updated to reflect the NYC Ingress controller
    on address `10.2.1.167`, the primary location.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到IP已经更新，反映了位于`10.2.1.167`的纽约Ingress控制器，这是主位置。
- en: 'Finally, a last curl to verify that the workload is being serviced out of the
    `Buffalo` cluster:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，再次使用curl验证工作负载是否来自`Buffalo`集群：
- en: '[PRE28]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: K8GB is a unique, and impressive, project from the CNCF that offers global load
    balancing similar to what other, more expensive, products offer today.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: K8GB是一个独特且令人印象深刻的CNCF项目，它提供类似于其他更昂贵产品的全球负载均衡。
- en: It’s a project that we are watching carefully, and if you need to deploy applications
    across multiple clusters, you should consider looking into the K8GB project as
    it matures.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个我们在密切关注的项目，如果你需要在多个集群中部署应用程序，你应该考虑随着K8GB项目的成熟而进行深入了解。
- en: Summary
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned how to provide automatic DNS registration to any
    service that uses a `LoadBalancer` service. You also learned how to deploy a highly
    available service using the CNCF project, K8GB, which provides global load balancing
    to a Kubernetes cluster.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何为任何使用`LoadBalancer`服务的服务提供自动DNS注册。你还学习了如何使用CNCF项目K8GB来部署一个高可用性服务，K8GB为Kubernetes集群提供全球负载均衡。
- en: These projects have become integral to numerous enterprises, offering users
    capabilities that previously required the efforts of multiple teams and, often,
    extensive paperwork, to deliver applications to customers. Now, your teams can
    swiftly deploy and update applications using standard agile practices, providing
    your organization with a competitive advantage.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这些项目已成为许多企业的核心，提供了之前需要多个团队努力且常常需要大量文书工作的功能，帮助将应用程序交付给客户。现在，你的团队可以使用标准的敏捷实践迅速部署和更新应用程序，为你的组织提供竞争优势。
- en: In the next chapter, *Integrating Authentication into Your Cluster*, we will
    explore the best methods and practices for implementing secure authentication
    in Kubernetes. You will learn how to integrate enterprise authentication using
    the OpenID Connect protocol and how to use Kubernetes impersonation. We will also
    discuss the challenges of managing credentials in a cluster and offer practical
    solutions for authenticating users and pipelines.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章《将身份验证集成到你的集群》中，我们将探讨实现Kubernetes中安全身份验证的最佳方法和实践。你将学习如何使用OpenID Connect协议集成企业身份验证，以及如何使用Kubernetes假冒身份。我们还将讨论在集群中管理凭据的挑战，并提供实际解决方案，以便对用户和流水线进行身份验证。
- en: Questions
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Kubernetes does not support using both TCP and UDP with services.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes不支持同时在服务中使用TCP和UDP。
- en: 'True'
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确
- en: 'False'
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误
- en: 'Answer: b'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：b
- en: ExternalDNS only integrates with CoreDNS.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ExternalDNS只与CoreDNS集成。
- en: 'True'
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确
- en: 'False'
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误
- en: 'Answer: b'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：b
- en: What do you need to configure on your edge DNS server for K8GB to provide load
    balancing to a domain?
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要在边缘DNS服务器上配置什么，以便K8GB为一个域提供负载均衡？
- en: Nothing, it works without additional configuration
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么都不需要，它在没有额外配置的情况下就能正常工作
- en: It must point to a cloud-provided DNS server
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它必须指向一个由云提供的DNS服务器
- en: You must delegate a zone that points to your cluster IP
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你必须委托一个指向你集群IP的区域
- en: Create a delegation to your CoreDNS instances
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个委托指向你的CoreDNS实例
- en: 'Answer: d'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：d
- en: What strategy is not supported by K8GB?
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K8GB不支持什么策略？
- en: Failover
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 故障转移
- en: Round robin
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 轮询
- en: Random distribution
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机分布
- en: GeoIP
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: GeoIP
- en: 'Answer: c'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：c
- en: Join our book’s Discord space
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask Me Anything* session with
    the authors:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 加入本书的Discord工作区，参加每月一次的*问我任何问题*环节，与作者互动：
- en: '[https://packt.link/K8EntGuide](https://packt.link/K8EntGuide)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/K8EntGuide](https://packt.link/K8EntGuide)'
- en: '![](img/QR_Code965214276169525265.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code965214276169525265.png)'
