<html><head></head><body>
		<div id="_idContainer036">
			<h1 id="_idParaDest-69" class="chapter-number"><a id="_idTextAnchor068"/>6</h1>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/>Deploying and Scaling Applications with Kubernetes</h1>
			<p>In this chapter, we’ll continue exploring Kubernetes with its rich functionality and ecosystem. We’ll see which other Kubernetes resources exist and what their purpose is, how to implement the self-healing and scaling of applications with Kubernetes, how to use Kubernetes service discovery, and how to run stateful workloads with Kubernetes. We will also perform several exercises with the minikube Kubernetes we’ve installed in the previous chapter (in case you’ve skipped it – check the last section of <a href="B18970_05.xhtml#_idTextAnchor059"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">).</span></p>
			<p>This is going to be one of the densest and most important chapters, so make sure to answer all the questions at the end and complete all practical assignments firsthand before moving further on. If you find it hard to understand some parts, read them twice and refer to the <em class="italic">Further </em><span class="No-Break"><em class="italic">reading</em></span><span class="No-Break"> section.</span></p>
			<p>We’re about to cover the following <span class="No-Break">exciting topics:</span></p>
			<ul>
				<li>Deployments, ReplicaSets, <span class="No-Break">and DaemonSets</span></li>
				<li>Running <span class="No-Break">stateful workloads</span></li>
				<li>Application configuration and <span class="No-Break">service discovery</span></li>
				<li>Ensuring applications are alive <span class="No-Break">and healthy</span></li>
			</ul>
			<p>So, let’s jump right <span class="No-Break">into it!</span></p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor070"/>Deployments, ReplicaSets, and DaemonSets</h1>
			<p>As we saw in the previous chapter, there are more resources in Kubernetes than just <em class="italic">Pods</em> and <em class="italic">namespaces</em>. Let’s learn about <em class="italic">Deployments</em>, <span class="No-Break">for starters.</span></p>
			<p class="callout-heading">Deployment</p>
			<p class="callout">It is a wrapper for declarative updates for Pods and ReplicaSets. After you describe the desired state in the Deployment resource spec, the Kubernetes Deployment controller changes the current state to the desired state at a <span class="No-Break">configured rate.</span></p>
			<p>It sounds complicated, but essentially Deployment is<a id="_idIndexMarker335"/> for controlling Pods and managing the application life cycle of those Pods. Pods are the smallest deployment units that wrap around containers, but they don’t provide any advanced Kubernetes features, such as <em class="italic">self-healing</em>, <em class="italic">rolling updates,</em> or <em class="italic">autoscaling</em>. However, <span class="No-Break">Deployments do.</span></p>
			<p>Because Pods are not resilient, an application container that fails in a pod <em class="italic">takes</em> the pod <em class="italic">down</em> with it. That is why in practice you’ll often use one of the advanced Kubernetes resources such as Deployment to automatically recreate Pods in case of failure. The deployment controller constantly watches the current state of the Pods it is managing and ensures that the desired number of Pods is running. We will shortly have a demonstration to see how <span class="No-Break">this works.</span></p>
			<p class="callout-heading">ReplicaSet</p>
			<p class="callout">A ReplicaSet is<a id="_idIndexMarker336"/> used to maintain the given number of replica Pods running at any given time. ReplicaSets are also used by Deployments to ensure the desired number of Pods (even if only one pod should be running at <span class="No-Break">a time).</span></p>
			<p>Compared to ReplicaSet, Deployment is a higher-level wrapper resource that manages ReplicaSets itself and provides other useful features. ReplicaSet does not allow you to implement custom update orchestration and therefore it is recommended that you use Deployments instead of directly <span class="No-Break">using ReplicaSets.</span></p>
			<p>Let’s get back to our minikube Kubernetes setup from the previous chapter for a quick demo. If you’ve stopped <a id="_idIndexMarker337"/>the cluster before, start it first with the <strong class="source-inline">minikube </strong><span class="No-Break"><strong class="source-inline">start</strong></span><span class="No-Break"> command:</span></p>
			<pre class="source-code">
$ minikube start
😄  minikube v1.25.2 on Darwin 12.4
✨  Using the docker driver based on existing profile
👍  Starting control plane node minikube in cluster minikube
🚜  Pulling base image ...
🔄  Restarting existing docker container for "minikube" ...
🐳  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    ▪ kubelet.housekeeping-interval=5m
🔎  Verifying Kubernetes components...
    ▪ Using image kubernetesui/dashboard:v2.3.1
    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
    ▪ Using image kubernetesui/metrics-scraper:v1.0.7
🌟  Enabled addons: storage-provisioner, default-storageclass, dashboard
🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default</pre>
			<p>If you are <a id="_idIndexMarker338"/>not sure about the state of your minikube Kubernetes, you can also use the <strong class="source-inline">minikube status</strong> command. Make sure that you have <strong class="source-inline">host</strong>, <strong class="source-inline">kubelet</strong>, and <strong class="source-inline">apiserver</strong> in a <span class="No-Break"><strong class="source-inline">Running</strong></span><span class="No-Break"> state:</span></p>
			<pre class="source-code">
$ minikube status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured</pre>
			<p>Previously, we created a simple pod running the <strong class="source-inline">Nginx</strong> web server in the <strong class="source-inline">kcna</strong> Kubernetes namespace. Let’s create a Deployment of the same nginx web server, but with three replicas (<span class="No-Break">three Pods):</span></p>
			<pre class="source-code">
$ minikube kubectl -- create -f https://k8s.io/examples/controllers/nginx-deployment.yaml --namespace kcna
deployment.apps/nginx-deployment created</pre>
			<p>The complete<a id="_idIndexMarker339"/> Deployment specification (found in <strong class="source-inline">nginx-deployment.yaml</strong> in the GitHub repository accompanying this book) looks like <span class="No-Break">the following:</span></p>
			<pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80</pre>
			<p>It is somewhat similar<a id="_idIndexMarker340"/> to the specification of the pod we used previously, but with a number of differences, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">kind: Deployment</strong></span></li>
				<li><strong class="source-inline">apps/v1</strong> indicating the <span class="No-Break">API version</span></li>
				<li>An additional <strong class="source-inline">app: nginx</strong> label in <span class="No-Break">the metadata</span></li>
				<li>The number of Pods is defined by <span class="No-Break"><strong class="source-inline">replicas: 3</strong></span></li>
				<li>There is a selector for matching Pods by the <strong class="source-inline">app: </strong><span class="No-Break"><strong class="source-inline">nginx</strong></span><span class="No-Break"> label</span></li>
				<li>It labels templates with an <strong class="source-inline">app: </strong><span class="No-Break"><strong class="source-inline">nginx</strong></span><span class="No-Break"> label</span></li>
			</ul>
			<p>The <strong class="source-inline">selector:</strong> under the <strong class="source-inline">spec:</strong> field defines how this Deployment finds the pods that it manages. In this example, it picks the Pods that have an <strong class="source-inline">app: </strong><span class="No-Break"><strong class="source-inline">nginx</strong></span><span class="No-Break"> label.</span></p>
			<p>The <strong class="source-inline">template</strong> block has the same pod <strong class="source-inline">containers:</strong> specification with the <strong class="source-inline">image:</strong> and <strong class="source-inline">ports:</strong> fields like we used in the previous chapter for the standalone pod scenario. Additionally, it has metadata with an <strong class="source-inline">app: nginx</strong> label that will be added to each pod created by this specification. Once again, the label is needed for the Deployment to be able to find <span class="No-Break">its Pods.</span></p>
			<p>Let’s check what happened in the <strong class="source-inline">kcna</strong> namespace after we applied our <span class="No-Break">Deployment specification:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get pods -n kcna
NAME                               READY    STATUS    RESTARTS   AGE
nginx-deployment-9456bbbf9-cl95h   1/1      Running   0          10m
nginx-deployment-9456bbbf9-ghxb2   1/1      Running   0          10m
nginx-deployment-9456bbbf9-nvl7r   1/1      Running   0          10m</pre>
			<p>We can see the<a id="_idIndexMarker341"/> three <strong class="source-inline">nginx</strong> Pods, each with its own unique name. Those names will look slightly different for you because the second part of the string is randomized. Now let’s query ReplicaSets in the <strong class="source-inline">kcna</strong> namespace with the <strong class="source-inline">kubectl get </strong><span class="No-Break"><strong class="source-inline">replicasets</strong></span><span class="No-Break"> command:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get replicasets -n kcna
NAME                         DESIRED   CURRENT   READY   AGE
nginx-deployment-9456bbbf9   3         3         3       12m</pre>
			<p>OK, we can see one ReplicaSet; however, we did not define it! It was the nginx Deployment that automatically created a ReplicaSet in order to keep the desired number of Pods. So, it is the Deployment that works on top of ReplicaSets; ReplicaSets that works on top of Pods; and Pods are wrappers on top of containers, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.1</em>. You can also see that ReplicaSet gets a unique ID, and the final Pods inherit this ID in <span class="No-Break">their names.</span></p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B18970_06_01.jpg" alt="Figure 6.1 – The hierarchy of Deployment, ReplicaSet, and Pod"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – The hierarchy of Deployment, ReplicaSet, and Pod</p>
			<p>Let’s carry out a quick experiment, let’s delete one of the three <strong class="source-inline">nginx</strong> pods created by our Deployment<a id="_idIndexMarker342"/> and see what happens (<em class="italic">you’ll have to specify the name of one pod that you have as pod names </em><span class="No-Break"><em class="italic">are unique</em></span><span class="No-Break">):</span></p>
			<pre class="source-code">
$ minikube kubectl -- delete pods nginx-deployment-9456bbbf9-cl95h -n kcna
pod "nginx-deployment-9456bbbf9-cl95h" deleted</pre>
			<p>Now, even<a id="_idIndexMarker343"/> if you are really fast at typing, you probably won’t notice how the deleted pod terminated and a new one was created. Next, get the list of Pods in the <span class="No-Break"><strong class="source-inline">kcna</strong></span><span class="No-Break"> namespace:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get pods -n kcna
NAME                               READY    STATUS    RESTARTS   AGE
nginx-deployment-9456bbbf9-9zv5c   1/1      Running   0          3s
nginx-deployment-9456bbbf9-ghxb2   1/1      Running   0          42m
nginx-deployment-9456bbbf9-nvl7r   1/1      Running   0          42m</pre>
			<p>And there we go – we have a new pod with an <strong class="source-inline">AGE</strong> of <strong class="source-inline">3</strong> seconds with a status of <strong class="source-inline">Running</strong>, and the old, deleted pod (<strong class="source-inline">nginx-deployment-9456bbbf9-cl95h</strong>) is completely gone. Now that’s the <em class="italic">Kubernetes self-healing magic</em> we’ve talked about so much! In just a couple of seconds, Kubernetes detected that the current state of the Nginx deployment was different because only two replicas (Pods) were running when the desired state is three replicas. The Kubernetes <em class="italic">reconciliation loop</em> kicked in and spawned a new, third replica of the <span class="No-Break"><strong class="source-inline">nginx</strong></span><span class="No-Break"> pod.</span></p>
			<p><em class="italic">Self-healing</em> is great and helps to keep our applications running in situations such as node hardware failure (of course, assuming that you run multiple Kubernetes nodes <strong class="bold">as you should do in production</strong>); when an application has a bug and crashes on a certain request type; and in the case of planned or unplanned maintenance when we have to migrate the payloads to <span class="No-Break">another node.</span></p>
			<p>But that’s only the beginning. Let’s imagine for a second that we are anticipating a high number of requests for an application we run on Kubernetes, so we have to get ready and add additional replicas in our application. With Kubernetes, it is as easy as executing a single <strong class="source-inline">kubectl scale </strong><span class="No-Break"><strong class="source-inline">deployment</strong></span><span class="No-Break"> command:</span></p>
			<pre class="source-code">
$ minikube kubectl -- scale deployment nginx-deployment --replicas 5 -n kcna
deployment.apps/nginx-deployment scaled</pre>
			<p>If you check the <a id="_idIndexMarker344"/>state of the respective ReplicaSet fast enough, you might see that new Pods <span class="No-Break">are spawning:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get replicaset -n kcna
NAME                         DESIRED   CURRENT   READY   AGE
nginx-deployment-9456bbbf9   5         5         4       52m</pre>
			<p>And voilà! Just a moment later, both new Pods are already up <span class="No-Break">and running:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get pods -n kcna
NAME                               READY    STATUS    RESTARTS   AGE
nginx-deployment-9456bbbf9-9zv5c   1/1      Running   0          30m
nginx-deployment-9456bbbf9-ghxb2   1/1      Running   0          52m
nginx-deployment-9456bbbf9-hgjnq   1/1      Running   0          23s
nginx-deployment-9456bbbf9-nvl7r   1/1      Running   0          52m
nginx-deployment-9456bbbf9-pzm8q   1/1     Running   0          23s</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Obviously, adding more application replicas on a single node K8s cluster does not bring a lot of practicality for performance or service availability. In production, you should always run multi-node Kubernetes clusters and spread the replicas of your applications across multiple nodes. We are doing these exercises on a single node Kubernetes instance, only for demonstration and <span class="No-Break">educational purposes.</span></p>
			<p>Next, let’s see how we can <a id="_idIndexMarker345"/>perform <em class="italic">rolling updates</em> with Deployments. Rolling updates play an important role because they help to ensure rapid software development cycles with frequent releases and allow us to make updates <a id="_idIndexMarker346"/>with <em class="italic">zero downtime</em> <span class="No-Break">for customers.</span></p>
			<p class="callout-heading">Zero downtime</p>
			<p class="callout">Zero downtime is a deployment method where the updated application is able to serve requests as usual, with no interruptions <span class="No-Break">or errors.</span></p>
			<p>With rolling updates, we can do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Promote application changes from one environment to another (for example, a new image version, configuration, <span class="No-Break">or labels)</span></li>
				<li>Rollback to the previous version in case of <span class="No-Break">any issues</span></li>
				<li>Define how many application replicas can be replaced at <span class="No-Break">a time</span></li>
			</ul>
			<p>Let’s see this in action with our Nginx deployment. We will update the <strong class="source-inline">nginx</strong> container image version tag to <strong class="source-inline">1.20</strong> using <strong class="source-inline">kubectl</strong>. First, check that our deployment <span class="No-Break">is intact:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get deployment -n kcna
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   5/5     5            5           9h</pre>
			<p>Now, change the image <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">nginx:1.20</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
$ minikube kubectl -- set image deployment/nginx-deployment nginx=nginx:1.20 -n kcna
deployment.apps/nginx-deployment image updated</pre>
			<p>Then observe what is happening to the Nginx Pods right after you have changed the image (<em class="italic">you have to be quick to witness </em><span class="No-Break"><em class="italic">the process!</em></span><span class="No-Break">):</span></p>
			<pre class="source-code">
$ minikube kubectl -- get pods -n kcna
NAME                                READY    STATUS              RESTARTS   AGE
nginx-deployment-7b96fbf5d8-dwskw   0/1      ContainerCreating   0          2s
nginx-deployment-7b96fbf5d8-grkv6   0/1      ContainerCreating   0          2s
nginx-deployment-7b96fbf5d8-jcb4p   0/1      ContainerCreating   0          2s
nginx-deployment-9456bbbf9-9zv5c    1/1      Running             0          6h
nginx-deployment-9456bbbf9-ghxb2    1/1      Running             0          9h
nginx-deployment-9456bbbf9-hgjnq    1/1      Running             0          2h
nginx-deployment-9456bbbf9-nvl7r    1/1      Running             0          9h
nginx-deployment-9456bbbf9-pzm8q    1/1      Terminating         0          2h</pre>
			<p>From five<a id="_idIndexMarker347"/> replicas of our Nginx deployment, we see that one has a <strong class="source-inline">Terminating</strong> status, four have a <strong class="source-inline">Running</strong> status, and three new have appeared and are in the <strong class="source-inline">ContainerCreating</strong> status. Just a moment later, we may see that the last few Pods with an old Nginx image are have a <strong class="source-inline">Terminating</strong> status, four new ones are in the <strong class="source-inline">Running</strong> state and one more is in the <span class="No-Break"><strong class="source-inline">ContainerCreating</strong></span><span class="No-Break"> state:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get pods -n kcna
NAME                                READY    STATUS              RESTARTS   AGE
nginx-deployment-7b96fbf5d8-6dh9q   0/1      ContainerCreating   0          2s
nginx-deployment-7b96fbf5d8-dwskw   1/1      Running             0          25s
nginx-deployment-7b96fbf5d8-grkv6   1/1      Running             0          25s
nginx-deployment-7b96fbf5d8-jcb4p   1/1      Running             0          25s
nginx-deployment-7b96fbf5d8-zt7bj   1/1     Running             0          4s
nginx-deployment-9456bbbf9-ghxb2    1/1     Terminating         0          9h
nginx-deployment-9456bbbf9-nvl7r    1/1     Terminating         0          9h</pre>
			<p>It won’t take<a id="_idIndexMarker348"/> long before all old Pods are gone and the last new ones enter a <strong class="source-inline">Running</strong> state. We can also verify that the new image is used, by performing <strong class="source-inline">kubectl describe pod</strong> on any new pod (on Windows, use <strong class="source-inline">findstr</strong> instead of <span class="No-Break"><strong class="source-inline">grep</strong></span><span class="No-Break"> command):</span></p>
			<pre class="source-code">
$ minikube kubectl -- describe pod nginx-deployment-7b96fbf5d8-dwskw -n kcna | grep Image
    Image:        nginx:1.20</pre>
			<p>Now, what do we do if a new image of the deployed application is not the right one or if it has a bug that may cause Pods to crash? Just as easy as updating a Kubernetes Deployment, we can roll back to the previous revision of our Deployment. Each change will be tracked by Kubernetes and gets its own revision version that we can see with the <strong class="source-inline">kubectl rollout </strong><span class="No-Break"><strong class="source-inline">history</strong></span><span class="No-Break"> command:</span></p>
			<pre class="source-code">
$ minikube kubectl -- rollout history deployment -n kcna
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         &lt;none&gt;</pre>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="source-inline">CHANGE-CAUSE</strong> is an optional description that can be set by adding an annotation to the Deployment. For example, we can do the following: <strong class="source-inline">kubectl -n kcna annotate deployment/nginx-deployment kubernetes.io/change-cause="image updated </strong><span class="No-Break"><strong class="source-inline">to 1.20"</strong></span><span class="No-Break">.</span></p>
			<p>If we realized <a id="_idIndexMarker349"/>that we need to get our deployment back to a previous revision, we can simply call <strong class="source-inline">kubectl rollout undo</strong> and optionally specify the exact, possibly older deployment revision. Let’s try to roll back to the previous, first revision of the <strong class="source-inline">nginx</strong> deployment (current revision <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">2</strong></span><span class="No-Break">):</span></p>
			<pre class="source-code">
$ minikube kubectl -- rollout undo deployment/nginx-deployment -n kcna
deployment.apps/nginx-deployment rolled back</pre>
			<p>A moment later, all Pods are recreated in the same rolling update fashion. And we can verify that the image version tag is back to <strong class="source-inline">1.14.2</strong> using the <strong class="source-inline">kubectl get</strong> <strong class="source-inline">pods</strong> command with an extra <strong class="source-inline">-o yaml</strong> option that will show us complete, detailed information about the pod (<em class="italic">the naming will be different in your case, pick any pod from your </em><span class="No-Break"><em class="italic">output list</em></span><span class="No-Break">):</span></p>
			<pre class="source-code">
$ minikube kubectl -- get pods -n kcna
NAME                               READY    STATUS    RESTARTS   AGE
nginx-deployment-9456bbbf9-6xpq2   1/1      Running   0          22s
nginx-deployment-9456bbbf9-75m7d   1/1      Running   0          22s
nginx-deployment-9456bbbf9-hbglw   1/1      Running   0          22s
nginx-deployment-9456bbbf9-hxdjd   1/1      Running   0          16s
nginx-deployment-9456bbbf9-mtxzm   1/1      Running   0          17s
$ minikube kubectl -- get pod nginx-deployment-9456bbbf9-6xpq2 -n kcna -o yaml
apiVersion: v1
kind: Pod
… LONG OUTPUT OMITTED …
spec:
  containers:
  - image: nginx:1.14.2
    imagePullPolicy: IfNotPresent
    name: nginx
… LONG OUTPUT OMITTED …</pre>
			<p>You’ll see a <a id="_idIndexMarker350"/>long, long output with all the details about this particular pod. You can also use <strong class="source-inline">kubectl get</strong> in combination with <strong class="source-inline">-o yaml</strong> for any other Kubernetes resources (<em class="italic">namespaces</em>, <em class="italic">Deployments</em>, and others we’re about to learn) to get full information about the object. You don’t need to understand each and every line of the output at this stage, but it is very helpful to know about <strong class="source-inline">imagePullPolicy</strong>, which defines the rules for how container images should be pulled from the registry. The policy can be one of <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">IfNotPresent</strong> – this is the default setting. The image will be downloaded only if the requested <strong class="source-inline">name:tag</strong> combination is not already present locally (cached) on the node where the pod <span class="No-Break">was scheduled.</span></li>
				<li><strong class="source-inline">Always</strong> – this means that every time a pod with the respective container is started, the image registry will be asked for an image digest (resolved from the image tag). If an image with this <em class="italic">exact digest</em> is already cached locally on the node, it will be used; otherwise, a Kubernetes kubelet will pull the image with the digest resolved by the registry on the <span class="No-Break">target node.</span></li>
				<li><strong class="source-inline">Never</strong> – this means the kubelet won’t attempt to fetch the image from the registry. The image should be delivered to the node somehow in advance; otherwise, the container will fail <span class="No-Break">to spawn.</span></li>
			</ul>
			<p>Additionally, we can control the rolling update process with a number of optional settings and timeouts. The two most important ones are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">maxUnavailable</strong> – this defines the maximum number of unavailable pods during a rolling update. It can be specified as a percentage (for example, <strong class="source-inline">25%</strong>) or as an absolute number (for <span class="No-Break">example, </span><span class="No-Break"><strong class="source-inline">3</strong></span><span class="No-Break">).</span></li>
				<li><strong class="source-inline">maxSurge</strong> – this defines the maximum number of pods that can be created over the desired number of replicas. It can also be specified as a percentage or an absolute number. If set, for example to <strong class="source-inline">25%</strong>, then the total number of <em class="italic">old</em> and <em class="italic">new</em> Pods won’t exceed <strong class="source-inline">125%</strong> of the desired number <span class="No-Break">of replicas.</span></li>
			</ul>
			<p>Finally, if we don’t <a id="_idIndexMarker351"/>want to do rolling updates, we can instead choose the <strong class="source-inline">Recreate</strong> strategy, which means all existing Pods are killed at once and new ones are only created after all old Pods have been terminated. Obviously, this strategy doesn’t allow you to perform zero-downtime updates, as all Pods of an application will be down for at least a few seconds. The strategy can be configured by defining the <strong class="source-inline">.spec.strategy.type</strong> setting in the YAML spec file of the <span class="No-Break">respective deployment.</span></p>
			<p>Now that we know about deployments, let’s <a id="_idIndexMarker352"/>move on to <strong class="bold">DaemonSets</strong>. As you probably know, <strong class="bold">Daemon</strong> in<a id="_idIndexMarker353"/> Unix/Linux world is a background process or service that provides additional functionality or supervises the system. A common example is <strong class="source-inline">sshd,</strong> a service that allows us to log in to remote systems over <a id="_idIndexMarker354"/>the <strong class="bold">Secure </strong><span class="No-Break"><strong class="bold">Shell</strong></span><span class="No-Break"> protocol.</span></p>
			<p class="callout-heading">DaemonSet</p>
			<p class="callout">DaemonSet is a wrapper for pods that ensures that all or certain nodes in the Kubernetes cluster each run a single replica of the target pod. If more nodes are added to the cluster, DaemonSet will ensure that a pod is automatically spawned on a new node as soon as it joins <span class="No-Break">the cluster.</span></p>
			<p>Where Deployment is considered a universal resource in Kubernetes for all kinds of user workloads, DaemonSet’s<a id="_idIndexMarker355"/> typical use cases are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>To run a log collection service on every Kubernetes node (for example, software such <a id="_idIndexMarker356"/>as <span class="No-Break"><strong class="bold">Fluent Bit</strong></span><span class="No-Break">)</span></li>
				<li>To run a<a id="_idIndexMarker357"/> node-monitoring daemon on every node (for example, a node exporter <span class="No-Break">for </span><span class="No-Break"><strong class="bold">Prometheus</strong></span><span class="No-Break">)</span></li>
				<li>To run a cluster storage daemon on <span class="No-Break">every node</span></li>
			</ul>
			<p>Similar to<a id="_idIndexMarker358"/> ReplicaSet, DaemonSet will ensure that the desired state is met, meaning that in the case of a pod failure, it will automatically spawn a new one. By default, DaemonSet will create Pods on all worker nodes in the cluster, but it is also possible to select specific nodes in the cluster or control plane nodes (how to do this will be covered in the next chapter, <a href="B18970_07.xhtml#_idTextAnchor077"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>). What cannot be done with DaemonSet is setting the number of replicas per node, because DaemonSet will always run only one pod per node. The spec file of DaemonSet is very similar to that of a Deployment, with a few differences, such as <strong class="source-inline">kind: DaemonSet</strong> or a lack of the <span class="No-Break"><strong class="source-inline">replicas:</strong></span><span class="No-Break"> setting.</span></p>
			<p>Moving on, we will not create a DaemonSet now, because a proper demonstration requires a multi-node Kubernetes cluster. Feel free to check out the <em class="italic">Further reading</em> section at the end of the chapter and try it out yourself if you’d like. In the following section, we’ll see how to run applications that need to persist information on the disk <span class="No-Break">with Kubernetes.</span></p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor071"/>Running stateful workloads</h1>
			<p>Everything we’ve tried <a id="_idIndexMarker359"/>so far with Kubernetes has not answered one important question – what do we do if we need to persist the application state between pod restarts? Data written on a container filesystem is not persisted by default. If you just take a deployment spec from the recent examples with Nginx and replace the image <a id="_idIndexMarker360"/>with <strong class="bold">PostgreSQL</strong>, that won’t be enough. Technically, your pod with PostgreSQL will come up, and the database will run, but any data written to that database instance won’t survive a pod restart. But, of course, Kubernetes has something to offer for stateful <span class="No-Break">applications too.</span></p>
			<p>As you hopefully remember from <a href="B18970_04.xhtml#_idTextAnchor048"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Exploring Container Runtimes, Interfaces, and Service Meshes</em>, Kubernetes has a <strong class="bold">Container Storage Interface</strong> or <strong class="bold">CSI</strong> that<a id="_idIndexMarker361"/> allows you to integrate various storage solutions into a K8s cluster. In order to augment Pods with external storage, we need <em class="italic">volumes</em> that can be dynamically provisioned via the Kubernetes API. Let’s begin with two new <span class="No-Break">resource definitions:</span></p>
			<ul>
				<li><strong class="bold">PersistentVolume</strong> (<strong class="bold">PV</strong>): This is<a id="_idIndexMarker362"/> a piece of storage in the cluster that can be provisioned either dynamically (by K8s when requested) or statically (for example, provisioned in some way by the cluster administrator and exposed for use <span class="No-Break">in K8s).</span></li>
				<li><strong class="bold">PersistentVolumeClaim</strong> (<strong class="bold">PVC</strong>): This<a id="_idIndexMarker363"/> is a request for storage by the user that <span class="No-Break">consumes </span><span class="No-Break"><em class="italic">PVs</em></span><span class="No-Break">.</span></li>
			</ul>
			<p>When we <a id="_idIndexMarker364"/>want to use persistent storage for our containerized application, we need to define a PVC spec in YAML format that can look like <span class="No-Break">the following:</span></p>
			<pre class="source-code">
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kcna-pv-claim
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi</pre>
			<p>This PVC can then be referenced in deployments and Pods as a volume. Claim allows you  to request a specific size (<strong class="source-inline">3Gi</strong> in the previous example) and <a id="_idIndexMarker365"/>one of the following <span class="No-Break">four </span><span class="No-Break"><strong class="source-inline">accessModes</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="source-inline">ReadWriteOnce</strong> – this <a id="_idIndexMarker366"/>allows the volume to be mounted as a read-write by a single node. This mode can allow multiple Pods on this node to access <span class="No-Break">the volume.</span></li>
				<li><strong class="source-inline">ReadOnlyMany</strong> – this <a id="_idIndexMarker367"/>allows the volume to be mounted as read-only by one or <span class="No-Break">multiple nodes.</span></li>
				<li><strong class="source-inline">ReadWriteMany</strong> – this <a id="_idIndexMarker368"/>allows the volume to be mounted as read-write by many nodes. This should be supported by the storage solution and protocol (for <span class="No-Break">example, </span><span class="No-Break"><strong class="bold">NFS</strong></span><span class="No-Break">).</span></li>
				<li><strong class="source-inline">ReadWriteOncePod</strong> – This<a id="_idIndexMarker369"/> is the same as <strong class="source-inline">ReadWriteOnce</strong>, but with a hard limit of only one<a id="_idIndexMarker370"/> pod in the whole cluster being able to write to <span class="No-Break">this volume.</span></li>
			</ul>
			<p>Since PVs <a id="_idIndexMarker371"/>are the actual storage resources in the Kubernetes cluster, we might have a situation when there is no suitable PV for the PVC request. In that case, Kubernetes can dynamically provision a PV based on the storage class specified in the PVC spec (<strong class="source-inline">storageClassName: standard</strong> in the <span class="No-Break">previous example).</span></p>
			<p class="callout-heading">Storage classes</p>
			<p class="callout">Storage classes<a id="_idIndexMarker372"/> provide a way to classify different storage options available in the cluster. Those might differ by performance, supported access modes and protocols, backup policies, <span class="No-Break">and more.</span></p>
			<p>It is also possible to instruct Kubernetes to only use already provisioned (possibly statically) and available PVs by setting <strong class="source-inline">storageClassName: ""</strong> (empty string) in the PVC spec. In the case of dynamic PV provisioning, the volume will always be of the exact size requested in the PVC spec. However, where we ask to only use already available PVs, we might get a larger volume than specified in PVC resource requests (for example, <strong class="source-inline">3Gi</strong> is requested, but if the closest available PV in the cluster is <strong class="source-inline">5Gi</strong>, it will be taken and all <strong class="source-inline">5Gi</strong> will be usable by the container that <span class="No-Break">mounts it).</span></p>
			<p>Let’s get back to the minikube setup to see this in action. First, create <strong class="source-inline">kcna-pv-claim</strong> with the previous specification (the file can be downloaded from the book’s <span class="No-Break">GitHub repository):</span></p>
			<pre class="source-code">
$ minikube kubectl -- create -f kcna-pv-claim.yaml -n kcna
persistentvolumeclaim/kcna-pv-claim created</pre>
			<p>Now, get the list of PVs in the cluster (the name will be unique in <span class="No-Break">this case):</span></p>
			<pre class="source-code">
$ minikube kubectl -- get pv -n kcna
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   REASON   AGE
pvc-6b56c062-a36b-4bd5-9d92-f344d02aaf5c   3Gi        RWO             Delete           Bound     kcna/kcna-pv-claim   standard                74s</pre>
			<p>A PV was<a id="_idIndexMarker373"/> automatically provisioned by Kubernetes in seconds! At this point, we can start using <strong class="source-inline">kcna-pv-claim</strong> as a volume in our deployment or pod specifications. Let’s delete the old <strong class="source-inline">nginx-deployment</strong> that we created at the beginning of <span class="No-Break">this chapter:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get deployment -n kcna
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   5/5     5            5           2d
$ minikube kubectl -- delete deployment nginx-deployment -n kcna
deployment.apps "nginx-deployment" deleted</pre>
			<p>And create another one, with our new volume attached. For that, we’ll need to make a few changes to the old <strong class="source-inline">nginx-deployment.yaml</strong> spec file (the modified version is available <span class="No-Break">on GitHub):</span></p>
			<pre class="source-code">
$ cat nginx-deployment-with-volume.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-with-volume
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        volumeMounts:
        - name: kcna-volume
          mountPath: "/usr/share/nginx/html"
      volumes:
      - name: kcna-volume
        persistentVolumeClaim:
          claimName: kcna-pv-claim</pre>
			<p>Besides a new <a id="_idIndexMarker374"/>name for the deployment (<strong class="source-inline">nginx-deployment-with-volume</strong>) and the number of replicas being set to <strong class="source-inline">1</strong>, the changes are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>We have added a <strong class="source-inline">volumeMounts:</strong> block under the respective <strong class="source-inline">nginx</strong> container stating which volume (<strong class="source-inline">kcna-volume</strong>) should be mounted at which path (<strong class="source-inline">"/usr/share/nginx/html"</strong> – this is a location for static <span class="No-Break">HTML content).</span></li>
				<li>Additionally, we have defined the <strong class="source-inline">volumes:</strong> block that maps <strong class="source-inline">kcna-volume</strong> to our PVC named <strong class="source-inline">kcna-pv-claim</strong> that we created in the <span class="No-Break">previous step.</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">volumeMounts</strong> is<a id="_idIndexMarker375"/> located within the individual container section because different containers in one pod can mount different (or the same) volumes. The <strong class="source-inline">volumes</strong> block is located at the same level as <strong class="source-inline">containers</strong>, and it should list all volumes that will be used within <span class="No-Break">the Pods.</span></p>
			<p>Now, let’s create a <a id="_idIndexMarker376"/>modified<a id="_idIndexMarker377"/> nginx deployment and see <span class="No-Break">what happens:</span></p>
			<pre class="source-code">
$ minikube kubectl -- create -f nginx-deployment-with-volume.yaml -n kcna
deployment.apps/nginx-deployment-with-volume created
$ minikube kubectl -- get pod -n kcna
NAME                                            READY   STATUS    RESTARTS   AGE
nginx-deployment-with-volume-6775557df5-bjmr6    1/1     Running   0          39s</pre>
			<p>At this stage, nothing looks different, but we can use the <strong class="source-inline">kubectl exec -it</strong> command to get inside our container by starting a new shell process. You might remember that we did something similar in <a href="B18970_03.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, when we used <strong class="source-inline">docker run -it</strong>. You’ll need to specify the name of your unique <span class="No-Break">pod here:</span></p>
			<pre class="source-code">
$ minikube kubectl -- -n kcna exec -it nginx-deployment-with-volume-6775557df5-bjmr6 -- bash
root@nginx-deployment-with-volume-6775557df5-bjmr6:/#</pre>
			<p>Let’s see whether there is a volume mount at <strong class="source-inline">/usr/share/nginx/html</strong> as <span class="No-Break">we’ve requested:</span></p>
			<pre class="source-code">
root@nginx-deployment-with-volume-6775557df5-bjmr6:/# mount | grep nginx
/dev/vda1 on /usr/share/nginx/html type ext4 (rw,relatime)</pre>
			<p>There it is! Our dynamically provisioned PV was automatically mounted to the node where our pod runs. If the pod dies, the data on the volume is preserved, and if the new pod starts on another node, Kubernetes will take care of unmounting and remounting the volume to the right node reaching the desired state we described in the spec file. To make sure that the data really is persisted, we can do a small exercise inside the container. Let’s <a id="_idIndexMarker378"/>install the <strong class="source-inline">curl</strong> utility and try to run it <span class="No-Break">against </span><span class="No-Break"><strong class="source-inline">localhost</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
root@nginx-deployment-with-volume-6775557df5-bjmr6:/# apt update
… LONG OUTPUT OMITTED …
root@nginx-deployment-with-volume-6775557df5-bjmr6:/# apt -y install curl
Reading package lists... Done
Building dependency tree
Reading state information... Done
… LONG OUTPUT OMITTED …
root@nginx-deployment-with-volume-6775557df5-bjmr6:/ # curl localhost
&lt;html&gt;
&lt;head&gt;&lt;title&gt;403 Forbidden&lt;/title&gt;&lt;/head&gt;
&lt;body bgcolor="white"&gt;
&lt;center&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx/1.14.2&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</pre>
			<p>Next, let’s create a simple one-liner <strong class="source-inline">index.html</strong> file in the <strong class="source-inline">/usr/share/nginx/html</strong> path and try running <span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break"> again:</span></p>
			<pre class="source-code">
root@nginx-deployment-with-volume-6775557df5-bjmr6:/# echo "Kubernetes Rocks!" &gt; /usr/share/nginx/html/index.html
root@nginx-deployment-with-volume-6775557df5-bjmr6:/# curl localhost
Kubernetes Rocks!</pre>
			<p>The last part of this exercise is on you. Log out of the container (by either entering the <strong class="source-inline">exit</strong> command or by pressing <em class="italic">Ctrl</em> + <em class="italic">D</em>) and delete the pod with the <strong class="source-inline">kubectl delete pods</strong> command and log in to the new pod when it is spawned. Check whether the <strong class="source-inline">index.html</strong> file that we created is still present at the mount point and has the correct <strong class="source-inline">Kubernetes Rocks!</strong> <span class="No-Break">string inside.</span></p>
			<p>While it is normal practice to use PVs with Kubernetes deployments,  another workload resource was specifically made to manage <span class="No-Break">stateful applications.</span></p>
			<p class="callout-heading">StatefulSet</p>
			<p class="callout"><strong class="source-inline">StatefulSet</strong> is a <a id="_idIndexMarker379"/>resource to manage the deployment and scaling of Pods that guarantees the ordering and uniqueness of <span class="No-Break">these Pods.</span></p>
			<p>What that <a id="_idIndexMarker380"/>means is that Pods created by StatefulSets have stable naming (without randomly generated UUIDs) and allow ordered, graceful deployment as well as ordered rolling updates. In addition to that, StatefulSets can provision a PV per pod replica. That means you won’t need to define and apply a new PVC every time you want to scale your application by adding a new replica. Let’s have a quick look at a StatefulSet <span class="No-Break">example spec:</span></p>
			<pre class="source-code">
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx-statefulset
spec:
  selector:
    matchLabels:
      app: nginx
  serviceName: "nginx"
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nginx-html
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: nginx-html
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "standard"
      resources:
        requests:
          storage: 1Gi</pre>
			<p>As you can <a id="_idIndexMarker381"/>see, the PVC spec is essentially a part of the StatefulSet spec located under the <strong class="source-inline">volumeClaimTemplates</strong> block at the end. Feel free to apply this StatefulSet spec yourself and see what happens. You should get three new PVCs and three new Pods spawned with PVs automatically provisioned <span class="No-Break">and attached.</span></p>
			<p>While this might seem complicated at first, think about how many <em class="italic">manual</em> steps you’d have to do to achieve the same result <em class="italic">without</em> Kubernetes. How much time would it take to create multiple volumes, download container images, and configure and start containers? Kubernetes makes many operational tasks trivial, and in the upcoming section, we will learn more about how Kubernetes allows you to configure applications running in containers and how service <span class="No-Break">discovery works.</span></p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor072"/>Application configuration and service discovery</h1>
			<p>So far, we have explored quite a few of K8s features and resources, but how do we do application configuration? We could add configuration files or environment variables to the container images during the build, but <em class="italic">this is wrong</em>. If you do so, for even the smallest<a id="_idIndexMarker382"/> configuration change, you’ll have to rebuild container images. Also, where you need to have different settings for different environments, you’ll need to maintain multiple images of the same application. Things get messy, complicated, and error-prone, so don’t <span class="No-Break">do this.</span></p>
			<p>Instead, the better approach in Kubernetes<a id="_idIndexMarker383"/> is to use <strong class="bold">ConfigMaps</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="bold">Secrets</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">ConfigMap</p>
			<p class="callout">A ConfigMap is a resource to store non-confidential data and configuration settings in key-value pairs that can be consumed inside Pods as environment variables, command-line arguments, or configuration files. ConfigMaps do not provide secrecy or encryption, so they are not suitable for keeping confidential information, such as passwords or <span class="No-Break">access tokens.</span></p>
			<p class="callout-heading">Secret</p>
			<p class="callout">A Secret is<a id="_idIndexMarker384"/> a resource to store sensitive data such as passwords, tokens, and access keys. Similar to ConfigMaps, Secrets can be consumed inside Pods as environment variables or <span class="No-Break">configuration files.</span></p>
			<p>Both ConfigMaps<a id="_idIndexMarker385"/> and Secrets allow us to decouple configuration from container images, enabling better application portability and reuse of the same container images for <span class="No-Break">different environments.</span></p>
			<p>Let’s explore a quick <a id="_idIndexMarker386"/>example. Imagine you are developing a web application that requires access to the database. The application is written in a way that it looks for the <strong class="source-inline">DATABASE_HOST</strong>, <strong class="source-inline">DATABASE_USERNAME</strong>, and <strong class="source-inline">DATABASE_PASSWORD</strong> environment variables. In this case, you can use a ConfigMap to set <strong class="source-inline">DATABASE_HOST</strong> and a Secret to keep information about the username and the password. This configuration would be consumed in the container with the application and would allow us to use different settings for different environments (for example, different databases and passwords for development, testing, <span class="No-Break">and production).</span></p>
			<p>Besides mapping ConfigMaps and Secrets to environment variables inside containers, we can also mount them inside as if they were regular files. This is done with the <em class="italic">volume</em> concept that we have just covered in the preceding section when learning about PVs <span class="No-Break">and PVCs.</span></p>
			<p>Let’s get back to the keyboard and create a simple Secret using the <strong class="source-inline">kubectl create </strong><span class="No-Break"><strong class="source-inline">secret</strong></span><span class="No-Break"> command:</span></p>
			<pre class="source-code">
$ minikube kubectl -- create secret generic kcna-secret --from-literal="username=kcnauser" --from-literal="password=topsecret" -n kcna
secret/kcna-secret created</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Needless to say, it is also possible to create Secrets by defining a YAML spec file with <strong class="source-inline">kind: Secret</strong> and calling <strong class="source-inline">kubectl create -f</strong> like we previously did for <span class="No-Break">other resources.</span></p>
			<p>Next, find the <strong class="source-inline">nginx-statefulset</strong> spec file that we used in the last section and modify it to mount our new <strong class="source-inline">kcna-secret</strong> as an additional volume at <strong class="source-inline">/etc/nginx/kcna.secret</strong>. Try to do this on your own, but if you experience any difficulties, the<a id="_idIndexMarker387"/> following are the relevant changes to the spec file (a complete modified spec file is also available <span class="No-Break">on GitHub):</span></p>
			<pre class="source-code">
… BEGINNING OF THE SPEC OMITTED …
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nginx-html
          mountPath: /usr/share/nginx/html
        - name: nginx-kcna-secret
          mountPath: /etc/nginx/kcna/
      volumes:
      - name: nginx-kcna-secret
        secret:
          secretName: kcna-secret
  volumeClaimTemplates:
  - metadata:
      name: nginx-html
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "standard"
      resources:
        requests:
          storage: 1Gi</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">It is possible to modify resources already created in Kubernetes instead of deleting them and creating them again from the scratch. However, some fields and resources are immutable and cannot be modified <em class="italic">on </em><span class="No-Break"><em class="italic">the fly</em></span><span class="No-Break">.</span></p>
			<p>Now, let’s <a id="_idIndexMarker388"/>apply the modified spec file using the <strong class="source-inline">kubectl apply -f</strong> command (the spec filename is <strong class="source-inline">statefulset_with_secret.yaml</strong>, <span class="No-Break">as follows):</span></p>
			<pre class="source-code">
$ minikube kubectl -- apply -f statefulset_with_secret.yaml -n kcna
statefulset.apps/nginx-statefulset configured</pre>
			<p>Because we’ve added a new volume, Pods will be recreated <span class="No-Break">straight after:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get pods -n kcna
NAME                                            READY   STATUS    RESTARTS   AGE
nginx-statefulset-0                             1/1     Running   0          12s
nginx-statefulset-1                             1/1     Running   0          15s
nginx-statefulset-2                             1/1     Running   0          18s</pre>
			<p>Let’s execute into one of the pods to see whether our Secret was correctly <span class="No-Break">mounted inside:</span></p>
			<pre class="source-code">
$ minikube kubectl -- -n kcna exec -it nginx-statefulset-0 -- bash
root@nginx-statefulset-0:/# cat /etc/nginx/kcna/username
kcnauser
root@nginx-statefulset-0:/# cat /etc/nginx/kcna/password
topsecret</pre>
			<p>There you go, the<a id="_idIndexMarker389"/> Secret<a id="_idIndexMarker390"/> has been mounted inside our nginx containers. It is worth mentioning that Kubernetes makes it possible to perform all sorts of combinations: Secrets (and individual keys) can be used as environment variables; Secrets can be created from existing files; Secrets can be used to store and mount SSL certificates or SSH keys; individual keys from K8s Secrets can be mounted into different paths <span class="No-Break">and more.</span></p>
			<p>ConfigMaps<a id="_idIndexMarker391"/> are very similar in terms of their capabilities, but their purpose is to store generic configuration. For example, we can create a new <strong class="source-inline">ConfigMap</strong> with nginx configuration and mount it over the <strong class="source-inline">/etc/nginx/nginx.conf</strong> in container overriding the default <span class="No-Break">config file.</span></p>
			<p>In terms of the scope of the KCNA exam, you are not expected to know all details, but as you get to work with Kubernetes, you’ll encounter the need to do one or another, therefore, feel free to check out the links in the <em class="italic">Further reading</em> section at the end of the chapter if you <span class="No-Break">have time.</span></p>
			<p>Coming next, we will talk about <a id="_idIndexMarker392"/>service discovery <span class="No-Break">in Kubernetes.</span></p>
			<p class="callout-heading">Service discovery</p>
			<p class="callout">Service discovery provides the automatic detection of devices and the services offered by these devices on <span class="No-Break">a network.</span></p>
			<p>As you may remember, in the case of microservice architectures, we have a lot of small services that need to talk to each other over the network. That means service discovery plays a huge role because it helps services to find their counterparts, for example, a backend service that has to discover the database it shall connect to. Luckily, Kubernetes solves that problem, too, with its service <a id="_idIndexMarker393"/>discovery mechanism based on <strong class="bold">Domain Name </strong><span class="No-Break"><strong class="bold">System</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">DNS</strong></span><span class="No-Break">).</span></p>
			<p>Kubernetes implements an internal DNS system that keeps track of applications with their names and respective pod IPs (each pod gets its own unique cluster-wide IP address on start). This allows different applications to easily find the endpoints of each other by resolving application names to pod IPs. Kubernetes <strong class="bold">Service</strong> resource comes into <span class="No-Break">play here.</span></p>
			<p class="callout-heading">Service</p>
			<p class="callout">Service is<a id="_idIndexMarker394"/> an abstraction layer that enables loose coupling between dependent pods. It is a resource that allows you to publish application names <em class="italic">inside</em> the cluster and expose applications to be reachable from <em class="italic">outside</em> <span class="No-Break">the cluster.</span></p>
			<p>Kubernetes Pods <a id="_idIndexMarker395"/>can have a relatively short life cycle. If we add a new volume or update the deployment image, or if the node dies, in all cases, Pods are recreated with a new name and a new IP address. That means we cannot rely on pod names, and we should use a Service that will target one or multiple Pods by matching Kubernetes <strong class="bold">labels</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="bold">selectors</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Labels and selectors</p>
			<p class="callout">Labels are<a id="_idIndexMarker396"/> simple key/value metadata pairs that can be attached to any Kubernetes objects during or after creation. Labels can contain the name of the application, version tags, or any other <span class="No-Break">object classification.</span></p>
			<p class="callout">Selectors <a id="_idIndexMarker397"/>allow the identification of a set of Kubernetes objects. For example, a label selector can be used to find a group of objects that have the same <strong class="source-inline">app</strong> label, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B18970_06_02.jpg" alt="Figure 6.2 – Service abstraction in Kubernetes"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Service abstraction in Kubernetes</p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.2</em> demonstrates <a id="_idIndexMarker398"/>how a Service selects all pods that have an <strong class="source-inline">app: nginx</strong> label <a id="_idIndexMarker399"/>assigned. Those can be pods created by a Deployment as well as any other pods that have the selected label assigned. You can list the labels of objects by adding the <strong class="source-inline">--show-labels</strong> parameter to <strong class="source-inline">kubectl get</strong> commands, <span class="No-Break">for example:</span></p>
			<pre class="source-code">
NAME                                            READY   STATUS    RESTARTS   AGE     LABELS
nginx-deployment-with-volume-6775557df5-f6ll7   1/1      Running   0          23h     <strong class="source-inline">app=nginx</strong>,pod-template-hash=6775557df5
nginx-statefulset-0                             1/1     Running   0          46m     <strong class="source-inline">app=nginx</strong>,controller-revision-hash=nginx-statefulset-6fbdf55d78,statefulset.kubernetes.io/pod-name=nginx-statefulset-0
nginx-statefulset-1                             1/1     Running   0          46m     <strong class="source-inline">app=nginx</strong>,controller-revision-hash=nginx-statefulset-6fbdf55d78,statefulset.kubernetes.io/pod-name=nginx-statefulset-1
nginx-statefulset-2                             1/1      Running   0          46m     <strong class="source-inline">app=nginx</strong>,controller-revision-hash=nginx-statefulset-6fbdf55d78,statefulset .kubernetes.io/pod-name=nginx-statefulset-2</pre>
			<p>See, our <a id="_idIndexMarker400"/>nginx deployment pod as well as pods from the <strong class="source-inline">nginx-statefulset</strong> all have the same <strong class="source-inline">app=nginx</strong> label because both the Deployment and StatefulSet have it defined in their <span class="No-Break">spec templates:</span></p>
			<pre class="source-code">
  template:
    metadata:
      labels:
        app: nginx</pre>
			<p>Now, let’s <a id="_idIndexMarker401"/>create a Service that will target all pods with this label. The following is what a simple spec targeting port <strong class="source-inline">80</strong> of selected pods might <span class="No-Break">look like:</span></p>
			<pre class="source-code">
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
    <strong class="source-inline">app: nginx</strong>
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80</pre>
			<p>Go on and<a id="_idIndexMarker402"/> create <span class="No-Break">the Service:</span></p>
			<pre class="source-code">
$ minikube kubectl -- create -f nginx-service.yaml -n kcna
service/nginx created
$ minikube kubectl -- get service -n kcna
NAME    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
nginx   ClusterIP   10.105.246.191   &lt;none&gt;        80/TCP    36s</pre>
			<p>After <a id="_idIndexMarker403"/>creation, you should be able to see the endpoints behind the Service that are, in fact, the IPs of running pods with an <strong class="source-inline">app=nginx</strong> label. Listing endpoints can be done with the <strong class="source-inline">kubectl get endpoints</strong> command, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get endpoints -n kcna
NAME    ENDPOINTS                                               AGE
nginx   172.17.0.2:80,172.17.0.6:80,172.17.0.7:80 + 1 more...   4m</pre>
			<p>If we’re now execute inside to one of the Pods again and run <strong class="source-inline">curl nginx</strong> (the name of the service we created) we should get a reply. Run it a few times (5-10 times) after installing curl into <span class="No-Break">the container:</span></p>
			<pre class="source-code">
$ minikube kubectl -- -n kcna exec -it nginx-statefulset-0 -- bash
root@nginx-statefulset-0:/# apt update &amp;&amp; apt -y install curl
… LONG OUTPUT OMITTED …
root@nginx-statefulset-0:/# curl nginx
Kubernetes Rocks!
root@nginx-statefulset-0:/# curl nginx
Kubernetes Rocks!
root@nginx-statefulset-0:/# curl nginx
&lt;html&gt;
&lt;head&gt;&lt;title&gt;403 Forbidden&lt;/title&gt;&lt;/head&gt;
&lt;body bgcolor="white"&gt;
&lt;center&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx/1.14.2&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</pre>
			<p>And <a id="_idIndexMarker404"/>we get different replies! One of the four pods that we’re currently running has a custom <strong class="source-inline">index.html</strong> file that we created earlier in this chapter, while the three <span class="No-Break">others don’t.</span></p>
			<p>What happens is the service we created load balances the requests between all available <strong class="source-inline">nginx</strong> pod IPs. The Service will also automatically update the list of endpoints if we scale out the number of replicas or if we do <span class="No-Break">the opposite.</span></p>
			<p>Now, let’s see which Service types exist and what they allow you <span class="No-Break">to accomplish:</span></p>
			<ul>
				<li><strong class="bold">ClusterIP</strong>: This<a id="_idIndexMarker405"/> type exposes an application on an internal cluster IP. Only Pods running in the same cluster can reach such a service. This is the default type that gets created unless overridden in <span class="No-Break">the spec.</span></li>
				<li><strong class="bold">NodePort</strong>: This <a id="_idIndexMarker406"/>type exposes the application on the same static port of each node in the cluster. Users will be able to reach the application from outside the cluster by requesting the IP of any node and <span class="No-Break">configured port.</span></li>
				<li><strong class="bold">LoadBalancer</strong>: This <a id="_idIndexMarker407"/>type exposes the application outside of cluster using a cloud provider’s <span class="No-Break">load balancer.</span></li>
				<li><strong class="bold">ExternalName</strong>: This<a id="_idIndexMarker408"/> type maps the service to an external DNS name (for example, <strong class="source-inline">mybestservice.app.com</strong>) by<a id="_idIndexMarker409"/> returning a <strong class="bold">CNAME</strong> record (to map one domain to another) with a configured value. <strong class="source-inline">ExternalName</strong> is not acting as a proxy for application requests like other service <span class="No-Break">types do.</span></li>
			</ul>
			<p>What that <a id="_idIndexMarker410"/>means is that in practice you’ll use the <strong class="source-inline">LoadBalancer</strong> type in most cases when you need to expose an application running in Kubernetes outside of the cluster (assuming your cloud provider or on-premises infrastructure offers load balancers). And in case of multiple applications that need to communicate with each other within the cluster, you’ll use the default <strong class="source-inline">ClusterIP</strong> type. For example, when your backend deployment needs to talk with the database running as a StatefulSet and the database should not be exposed to <span class="No-Break">the internet.</span></p>
			<p>Coming next is the final section of the chapter. As you were doing all of the exercises, you might have wondered how Kubernetes knows that the application is actually running when a pod is running. What happens if an application needs time before it can serve the requests? How do we know that the application is not stuck in a deadlock? Let’s figure <span class="No-Break">that out!</span></p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor073"/>Ensuring applications are alive and healthy</h1>
			<p>By default, Kubernetes<a id="_idIndexMarker411"/> ensures that the desired state of applications in a cluster is reached. It will restart and recreate failed containers when a process exits or a node fails. However, that might not be enough to tell if the application running inside the pod is healthy. In order to ensure that the workloads are alive and healthy, Kubernetes implements the concept <span class="No-Break">of </span><span class="No-Break"><strong class="bold">probes</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Probe</p>
			<p class="callout">A probe is <a id="_idIndexMarker412"/>a diagnostic that is performed by a Kubernetes kubelet on a container. A diagnostic can be an arbitrary command executed inside a container or TCP probe, or an <span class="No-Break">HTTP request.</span></p>
			<p>Kubernetes offers three types of probes, as shown in the <span class="No-Break">following list:</span></p>
			<ul>
				<li><strong class="bold">Liveness</strong>: Ensures<a id="_idIndexMarker413"/> that a process in a container is alive and, if not, restarts the container. For the case when the application catches a deadlock, restarting the container usually helps to make the application more available <span class="No-Break">despite bugs.</span></li>
				<li><strong class="bold">Readiness</strong>: Ensures<a id="_idIndexMarker414"/> that the application is ready to accept traffic. A pod with multiple containers is considered ready when all its containers are ready and all readiness <span class="No-Break">probes succeed.</span></li>
				<li><strong class="bold">Startup</strong>: Allows <a id="_idIndexMarker415"/>you to know when an application in a container has started. If a startup probe is configured, it disables liveness and readiness probes until it succeeds. This might be needed for slow-starting applications to avoid them being killed due to a failed liveness probe before they <span class="No-Break">are up.</span></li>
			</ul>
			<p>All those probes <a id="_idIndexMarker416"/>serve the purpose of increasing the availability of containerized applications, but they cover different scenarios. For example, a liveness probe will cause a container to restart if a probe fails. Complex applications running for a long time might eventually transition to a broken state, and this is where the Kubernetes<a id="_idIndexMarker417"/> liveness <span class="No-Break">probe helps.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">The whole pod is not recreated when the liveness probe of a single container has failed. Only a certain container within the pod is restarted. This is different from the case when the application in the container exits and the pod gets recreated by a controller such as a Deployment, ReplicaSet, <span class="No-Break">or StatefulSet.</span></p>
			<p>A readiness probe<a id="_idIndexMarker418"/> is needed when an application in a container is unable to serve the traffic. Some applications might take a long time to start because of the large datasets they are loading into memory or because they need to perform an initial configuration that takes time. An application might also depend on an external service. In all those situations, we don’t want to kill and restart the container; rather, we don’t want to send any traffic <span class="No-Break">to it.</span></p>
			<p>Readiness probes help to determine which Pods behind a Service are ready to accept connections and serve traffic. If a container fails the readiness probe, its pod IP is automatically taken out from the list of endpoints of the Service. This helps prevent situations when a user request is routed to a <em class="italic">not-yet-working</em> <span class="No-Break">application replica.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">If both liveness and readiness probes are defined, the first does not wait for the second to succeed. It is possible to set initial delays for probes (via the <strong class="source-inline">initialDelaySeconds</strong> setting) or <a id="_idIndexMarker419"/>use a <strong class="source-inline">startupProbe</strong> that temporarily disables the liveness and <span class="No-Break">readiness checks.</span></p>
			<p>Each probe can <a id="_idIndexMarker420"/>execute a custom command, perform an HTTP request, or a TCP probe. In addition to that, liveness and readiness probes are tunable with several parameters: how often the check should be performed (configurable via <strong class="source-inline">periodSeconds</strong>), how long to wait for a probe to finish (configurable via <strong class="source-inline">timeoutSeconds</strong>) or the thresholds for how many times the probe should be retried before giving up and either restarting the container or stopping the traffic depending on the <span class="No-Break">probe type.</span></p>
			<p>Now, let’s examine<a id="_idIndexMarker421"/> the following pod with a simple liveness <span class="No-Break">probe defined:</span></p>
			<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5</pre>
			<p>When<a id="_idIndexMarker422"/> the<a id="_idIndexMarker423"/> pod container is started it creates an empty file at the <strong class="source-inline">/tmp/healthy</strong> path, waits for <strong class="source-inline">30</strong> seconds, and deletes that file. After that, the container does nothing for another <strong class="source-inline">600</strong> seconds before exiting. The liveness probe executes the <strong class="source-inline">cat /tmp/healthy</strong> command every <strong class="source-inline">5</strong> seconds after an initial check delay of <span class="No-Break"><strong class="source-inline">5</strong></span><span class="No-Break"> seconds.</span></p>
			<p>Let’s create the spec and see it <span class="No-Break">in action:</span></p>
			<pre class="source-code">
$ minikube kubectl -- create -f https://k8s.io/examples/pods/probe/exec-liveness.yaml -n kcna
pod/liveness-exec created</pre>
			<p>At first, the pod runs fine, its liveness probe succeeds, and its restart counter shows <span class="No-Break"><strong class="source-inline">0</strong></span><span class="No-Break"> restarts:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get pod -n kcna
NAME                                            READY   STATUS    RESTARTS   AGE
liveness-exec                                   1/1     Running   <strong class="source-inline">0</strong>          59s</pre>
			<p>Sometime later, we can see that there was <span class="No-Break">a restart:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get pod -n kcna
NAME                                            READY   STATUS    RESTARTS      AGE
liveness-exec                                   1/1     Running   <strong class="source-inline">1 (20s ago)</strong>   95s</pre>
			<p>If we <a id="_idIndexMarker424"/>describe the pod, we can see the timeline <span class="No-Break">of events:</span></p>
			<pre class="source-code">
$ minikube kubectl -- describe pod liveness-exec -n kcna
… LONG OUTPUT OMITTED …
Events:
  Type     Reason     Age                From                Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  85s                default-scheduler  Successfully assigned kcna/liveness-exec to minikube
  Normal   Pulled     81s                kubelet             Successfully pulled image "k8s.gcr.io/busybox" in 3.4078911s
  <strong class="bold">Warning  Unhealthy  41s (x3 over 51s)  kubelet            Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory</strong>
<strong class="bold">  Normal   Killing    41s                kubelet             Container liveness failed liveness probe, will be restarted</strong>
  Normal   Pulling    11s (x2 over 85s)  kubelet             Pulling image "k8s.gcr.io/busybox"
  Normal   Created    10s (x2 over 81s)  kubelet             Created container liveness
  Normal   Pulled     10s                kubelet             Successfully pulled image "k8s.gcr.io/busybox" in 1.2501457s
  Normal   Started    9s (x2 over 81s)   kubelet             Started container liveness</pre>
			<p>We are now approaching the end of this long and intense chapter. By now, you’ve learned a lot about Kubernetes features and some of its advanced resources. Many of the in-depth <a id="_idIndexMarker425"/>details explained here are not required to pass the KCNA exam, but they will be required to start working with Kubernetes and will undoubtedly help you in the future if you decide to become CKA or CKAD certified. If you have not been able to grasp 100% of this chapter’s content, that is unlikely to stop you from passing the KCNA exam, but try to get into it as much as you can now. Check out the <em class="italic">Further reading</em> section resources and do additional research <span class="No-Break">if needed.</span></p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor074"/>Summary</h1>
			<p>In this chapter, we’ve seen Kubernetes’ self-healing capabilities in action and how K8s reconciliation loops allow it to reach the desired state of resources in a very short time. Since Pods themselves do not have any means to recover from a failure, we commonly use Kubernetes <em class="italic">Deployments</em> to ensure that the requested number of application replicas are running. <em class="italic">Deployments</em> also allow us to perform controllable <em class="italic">rolling updates</em>, <em class="italic">rollbacks,</em> and <em class="italic">zero-downtime</em> deployments to enable rapid software development cycles that require the frequent release <span class="No-Break">of versions.</span></p>
			<p><em class="italic">DaemonSet</em> is another resource for the scenario when we need to run one replica of the application on each or a particular set of nodes. <em class="italic">DaemonSets</em> are often used for running logging or monitoring agents across <span class="No-Break">the cluster.</span></p>
			<p><em class="italic">StatefulSet</em> is a resource for managing stateful workloads with Kubernetes. It allows us to easily integrate volumes to Pods to keep persistent data between container restarts and automate the dynamic provisioning <span class="No-Break">of PVs.</span></p>
			<p>Next, we have explored ways to provide configuration and sensitive information to applications running in Kubernetes. <em class="italic">ConfigMaps</em> are suitable for generic non-confidential data, and <em class="italic">Secrets</em> are intended to be used for passwords, tokens, and so on. Both ConfigMaps and Secrets are essentially volumes that can be mounted into specific file paths <span class="No-Break">inside containers.</span></p>
			<p>We have also learned that service discovery plays an important role and allows applications to find and communicate with each other within the Kubernetes cluster. The <em class="italic">Service</em> resource allows for the exposure of the application with its Pods both inside and outside of the cluster using distinct Service types such as <strong class="source-inline">LoadBalancer</strong>, <strong class="source-inline">NodePort</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">ClusterIP</strong></span><span class="No-Break">.</span></p>
			<p>Last but not least, we’ve explored the options for ensuring that applications running in Kubernetes are alive and healthy. Kubernetes offers three types of probes (<em class="italic">liveness</em>, <em class="italic">readiness</em>, and <em class="italic">startup</em>) that serve the purpose of verifying the state of the application on startup or periodically at regular intervals. If the application fails the liveness probes, its container is restarted and in case it fails the Readiness probes, it just won’t receive any traffic, and the pod IP will be excluded from the list of Service endpoints. The startup probes are intended for slow-starting applications that need extra time before they can handle other probes or <span class="No-Break">real traffic.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Feel free to delete any Kubernetes resources created in this chapter unless you’re planning to come back to <span class="No-Break">them later.</span></p>
			<p>In the upcoming chapter, we will continue exploring Kubernetes and its features. We will learn about placement controls, resource requests, and ways to debug applications running on Kubernetes. Make sure to answer all recap questions and check out the <em class="italic">Further reading</em> section if you’d like to learn more about the topics in <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor075"/>Questions</h1>
			<p>As we conclude, here is a list of questions for you to test your knowledge regarding this chapter’s material. You will find the answers in the <em class="italic">Assessments</em> section of <span class="No-Break">the </span><span class="No-Break"><em class="italic">Appendix</em></span><span class="No-Break">:</span></p>
			<ol>
				<li>Which of the following Kubernetes resources allows you to recover an application if the node it was running on has failed (<span class="No-Break">select multiple)?</span><ol><li><span class="No-Break">Pod</span></li><li><span class="No-Break">Service</span></li><li><span class="No-Break">StatefulSet</span></li><li><span class="No-Break">Deployment</span></li></ol></li>
				<li>Which of the following Kubernetes resources ensures that the defined number of replicas are always running (<span class="No-Break">select multiple)?</span><ol><li><span class="No-Break">Pod</span></li><li><span class="No-Break">ReplicaSet</span></li><li><span class="No-Break">Deployment</span></li><li><span class="No-Break">DaemonSet</span></li></ol></li>
				<li>Which of the following Kubernetes resources allows us to perform rolling updates and <span class="No-Break">zero-downtime deployments?</span><ol><li><span class="No-Break">Service</span></li><li><span class="No-Break">Deployment</span></li><li><span class="No-Break">ReplicaSet</span></li><li><span class="No-Break">DeploySet</span></li></ol></li>
				<li>Which statement best describes the relationship between Pods and various Kubernetes <span class="No-Break">controllers (resources)?</span><ol><li>Pods are managing <span class="No-Break">the resources</span></li><li>Pods are managed by the <span class="No-Break">container runtime</span></li><li>Pods are always managed by one of the <span class="No-Break">Kubernetes controllers</span></li><li>Pods can be managed by one of the <span class="No-Break">Kubernetes controllers</span></li></ol></li>
				<li>What is the purpose of <span class="No-Break">label selectors?</span><ol><li>They help to determine the purpose of each pod in <span class="No-Break">the cluster</span></li><li>They help to distinguish more important Pods from less <span class="No-Break">important ones</span></li><li>They are simply <span class="No-Break">auxiliary metadata</span></li><li>They allow us to group and select resources <span class="No-Break">by labels</span></li></ol></li>
				<li>Which of the following image pull policies will cause a download from the registry only when the image is not already cached on <span class="No-Break">the node?</span><ol><li><span class="No-Break"><strong class="source-inline">IfNotCached</strong></span></li><li><span class="No-Break"><strong class="source-inline">IfNotPresent</strong></span></li><li><span class="No-Break"><strong class="source-inline">IfNotAvailable</strong></span></li><li><span class="No-Break"><strong class="source-inline">Always</strong></span></li></ol></li>
				<li>How does a Service determine the Pods that are ready to <span class="No-Break">accept traffic?</span><ol><li>Pods that are ready will have the <strong class="source-inline">ready: true</strong> label <span class="No-Break">on them</span></li><li>Only Pods managed by Deployment can accept traffic from <span class="No-Break">a Service</span></li><li>A pod’s readiness probe has <span class="No-Break">to succeed</span></li><li>A pod’s startup probe has <span class="No-Break">to succeed</span></li></ol></li>
				<li>Which type of probe delays the execution of <span class="No-Break">other probes?</span><ol><li><span class="No-Break">Delayed</span></li><li><span class="No-Break">Liveness</span></li><li><span class="No-Break">Startup</span></li><li><span class="No-Break">Readiness</span></li></ol></li>
				<li>Which spec setting controls the number of Pods managed by <span class="No-Break">a Deployment?</span><ol><li><span class="No-Break">podnum</span></li><li><span class="No-Break">Replicas</span></li><li><span class="No-Break">Containers</span></li><li><span class="No-Break">Instances</span></li></ol></li>
				<li>Which Kubernetes controller is best suited for applications that need to save data <span class="No-Break">to disk?</span><ol><li><span class="No-Break"><strong class="source-inline">Deployment</strong></span></li><li><span class="No-Break"><strong class="source-inline">DaemonSet</strong></span></li><li><span class="No-Break"><strong class="source-inline">ReplicaSet</strong></span></li><li><span class="No-Break"><strong class="source-inline">StatefulSet</strong></span></li></ol></li>
				<li>Which of the following allows Kubernetes controllers to detect drift from the <span class="No-Break">desired state?</span><ol><li><span class="No-Break">Replica controller</span></li><li><span class="No-Break">Kubelet</span></li><li><span class="No-Break">Reconciliation loop</span></li><li><span class="No-Break">Liveness probes</span></li></ol></li>
				<li>Which type of service allows the exposure of applications inside <span class="No-Break">the cluster?</span><ol><li><span class="No-Break"><strong class="source-inline">LoadBalancer</strong></span></li><li><span class="No-Break"><strong class="source-inline">ClusterIP</strong></span></li><li><span class="No-Break"><strong class="source-inline">InternalIP</strong></span></li><li><span class="No-Break"><strong class="source-inline">NodePort</strong></span></li></ol></li>
				<li>Which technology is used behind service discovery <span class="No-Break">in Kubernetes?</span><ol><li><span class="No-Break">Avahi</span></li><li><span class="No-Break">Iptables</span></li><li><span class="No-Break">NTP</span></li><li><span class="No-Break">DNS</span></li></ol></li>
				<li>Which of the following service types are suitable for exposing applications outside of the Kubernetes cluster (<span class="No-Break">select multiple)?</span><ol><li><span class="No-Break"><strong class="source-inline">ClusterIP</strong></span></li><li><span class="No-Break"><strong class="source-inline">NodePort</strong></span></li><li><span class="No-Break"><strong class="source-inline">LoadBalancer</strong></span></li><li><span class="No-Break"><strong class="source-inline">ExternalIP</strong></span></li></ol></li>
				<li>Which of the following resources is suitable for storing and injecting generic configuration <span class="No-Break">into containers?</span><ol><li><span class="No-Break">ConfigMap</span></li><li><span class="No-Break">Secret</span></li><li><span class="No-Break">SettingMap</span></li><li><span class="No-Break">PV</span></li></ol></li>
				<li>Which object in Kubernetes represents an actual <span class="No-Break">storage volume?</span><ol><li><span class="No-Break">StatefulSet</span></li><li><span class="No-Break">PVC</span></li><li><span class="No-Break">PV</span></li><li><span class="No-Break">SV</span></li></ol></li>
				<li>Which resource is suitable for representing sensitive information to applications <span class="No-Break">in containers?</span><ol><li><span class="No-Break">ConfigMap</span></li><li><span class="No-Break">Secret</span></li><li><span class="No-Break">Volume</span></li><li><span class="No-Break">PVC</span></li></ol></li>
				<li>Which probe will restart the container <span class="No-Break">if failed?</span><ol><li><span class="No-Break">Aliveness</span></li><li><span class="No-Break">Readiness</span></li><li><span class="No-Break">Startup</span></li><li><span class="No-Break">Liveness</span></li></ol></li>
			</ol>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor076"/>Further reading</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
			<ul>
				<li>Dynamic Volume <span class="No-Break">provisioning: </span><a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/"><span class="No-Break">https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/</span></a></li>
				<li>Managing Kubernetes <span class="No-Break">Secrets: </span><a href="https://kubernetes.io/docs/tasks/configmap-secret/"><span class="No-Break">https://kubernetes.io/docs/tasks/configmap-secret/</span></a></li>
				<li>Creating and using <span class="No-Break">ConfigMaps:</span><span class="No-Break"><span class="hidden"> </span></span><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/"><span class="No-Break">https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/</span></a></li>
				<li>Kubernetes probes <span class="No-Break">configuration: </span><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/"><span class="No-Break">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/</span></a></li>
			</ul>
		</div>
	</body></html>