- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Navigating Cloud-native Operations with GitOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B22100_01.xhtml#_idTextAnchor013), we delved into the foundational
    concepts of GitOps, contrasting its approach with traditional CI/CD and DevOps
    methodologies. We explored its historical evolution, key principles such as Git
    centralization and automated synchronization, and its integration with Kubernetes.
    This chapter will emphasize GitOps’ role in enhancing scalability and security
    in modern cloud environments. By the end of this chapter, you will have a comprehensive
    understanding of GitOps’ transformative impact on software deployment and operations,
    setting the stage for its application in cloud-native operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll focus on the following key areas:'
  prefs: []
  type: TYPE_NORMAL
- en: GitOps and cloud-native tech
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring K3s as a lightweight Kubernetes distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample workflow – effortless CD with Docker and K3s
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To engage with the examples in this chapter, you’ll need a Kubernetes cluster.
    While we’ll guide you through how to install K3s in a way that’s suitable for
    these examples, any Kubernetes setup will suffice.
  prefs: []
  type: TYPE_NORMAL
- en: K3s is optimized for Linux systems, so ensure you have access to a Linux environment.
    If you’re using a non-Linux system, consider alternatives such as **Windows Subsystem
    for Linux** (**WSL**) or **Virtual Box** (see [*1*] and [*2*] in the *Further
    reading* section at the end of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter is available in the `Chapter02` folder in this book’s
    GitHub repository: [https://github.com/PacktPublishing/Implementing-GitOps-with-Kubernetes](https://github.com/PacktPublishing/Implementing-GitOps-with-Kubernetes).'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the integration of GitOps and cloud-native technology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B22100_01.xhtml#_idTextAnchor013), we explored GitOps, a fusion
    of DevOps and Git, emphasizing its ability to enhance operational efficiency and
    system stability by applying software development techniques to infrastructure
    management. Moving forward, [*Chapter 2*](B22100_02.xhtml#_idTextAnchor027) expands
    on this foundation, examining how GitOps integrates with cloud-native technology.
    This technology signifies a significant shift in application development, characterized
    by containerization, microservices, and dynamic orchestration, enhancing scalability
    and resilience.
  prefs: []
  type: TYPE_NORMAL
- en: GitOps complements this by enabling systematic, version-controlled management
    of complex systems. The synergy between GitOps and cloud-native technologies,
    particularly Kubernetes, leads to a more dynamic, agile, and reliable approach
    to system management. This chapter aims to show how GitOps simplifies and elevates
    the capabilities of cloud-native environments.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, GitOps, emerging from the confluence of DevOps and version control,
    leverages Git’s power for managing and automating software system deployments
    and operations. By treating infrastructure as code, GitOps facilitates reviewing,
    versioning, and deploying changes using Git’s familiar pull requests and merges.
    This approach ensures consistency, traceability, and ease of rolling back, proving
    especially potent in cloud-native settings.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-native technology, in contrast, represents a paradigm shift in how applications
    are constructed and deployed. It involves using containers, microservices, and
    dynamic orchestration to create robust, scalable, and independently deployable
    applications. This technology maximizes cloud flexibility, enabling swift scaling
    and resilience. When integrated with GitOps, cloud-native technology becomes more
    robust, allowing teams to manage complex systems more effectively with increased
    confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Before delving into the practical applications of GitOps, it is essential to
    introduce Kubernetes, the orchestration platform that’s central to cloud-native
    technology. Additionally, we will discuss **K3s**, a lightweight variant of Kubernetes.
    K3s is particularly suited for personal development environments as it allows
    Kubernetes clusters to be deployed on individual laptops. This setup allows for
    hands-on experimentation and learning, providing a practical foundation for understanding
    and applying GitOps techniques in a Kubernetes context. This knowledge will be
    crucial as we progress to more advanced topics and practical demonstrations of
    GitOps in action.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the upcoming sections, we will introduce Kubernetes, including a brief historical
    overview of the original project and the core concepts of Kubernetes architecture.
    In the second part, we will delve deeper into K3s and explore how you can use
    it to run a local Kubernetes cluster on your laptop.
  prefs: []
  type: TYPE_NORMAL
- en: What is Kubernetes?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes is a robust and open source platform that was crafted to streamline
    the automation of deploying, scaling, and managing application containers. It
    plays a central role in the kingdom of container orchestration, offering a solid
    framework for the effective management of containerized applications across multiple
    settings, including physical data centers and both public and private cloud environments.
  prefs: []
  type: TYPE_NORMAL
- en: Originally, the Kubernetes project at Google, codenamed *Project 7* as a nod
    to *Star Trek’s* Seven of Nine, symbolized a more approachable version of Google’s
    Borg system. Owing to licensing constraints, the term Kubernetes, Greek for helmsman,
    was adopted and reflected in its seven-spoked wheel logo, subtly honoring its
    *Star Trek*-inspired origins. Following its 2014 announcement, Joe Beda, Brendan
    Burns, and Craig McLuckie, among other Google engineers, spearheaded its development.
    Distinct from Borg’s C++ coding, Kubernetes utilized **Go**. Its first version,
    Kubernetes 1.0, was released in 2015\. Through collaboration with the Linux Foundation,
    Kubernetes became a cornerstone of the **Cloud Native Computing Foundation** (**CNCF**),
    rapidly garnering integration into services offered by major tech entities such
    as Red Hat, VMware, Mesosphere, Docker, Microsoft Azure, and AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes plays a critical role in cloud computing, facilitating both declarative
    configuration and automation. It supports a range of container tools, including
    Docker, and its ability to manage complex container architectures across multiple
    hosts makes it highly valuable.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes simplifies the deployment and scaling of applications, and its automated
    rollouts and rollbacks for containerized applications enhance reliability and
    efficiency. It allows containers to be orchestrated across multiple hosts, handles
    how applications are deployed and scaled, and covers their networking and storage
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: The platform’s self-healing feature automatically restarts, replaces, and reschedules
    containers if they fail. It also scales containers in response to varying loads
    and updates them without downtime using a variety of deployment patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes supports a range of workloads, including **stateless**, **stateful**,
    and **data-processing** workloads. It’s flexible enough to deliver complex applications,
    offering scalability and reliability while managing workloads effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, Kubernetes has revolutionized the way containerized applications are
    deployed and managed, making it a key tool in the world of modern software development
    and operations.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes architecture is built to manage and orchestrate containerized applications.
    It consists of several components that work together.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Kubernetes architecture, the cluster is divided into two primary components:
    the **control plane** and the **worker nodes** (or **data plane**). The control
    plane is responsible for global decision-making and managing the cluster’s state.
    It includes essential elements such as the **API server**, **etcd**, **scheduler**,
    **controller manager**, and **cloud** **controller manager**.'
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, node components are responsible for running the actual workloads.
    Each node contains vital services such as **Kubelet**, a **container runtime**,
    and **kube-proxy**, which ensure that containers run as expected and handle network
    communication within and outside the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: This architecture allows for a robust and scalable system where the control
    plane maintains control and nodes efficiently manage the workload.
  prefs: []
  type: TYPE_NORMAL
- en: 'The control plane includes the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**API server (kube-apiserver)**: This central management entity processes REST
    requests, validates them, and updates the corresponding objects in etcd. It’s
    the main interface of the Kubernetes control plane.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**etcd**: This is a consistent and highly available **key-value store** that
    acts as the primary storage for all cluster data. It’s crucial for the cluster’s
    state management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scheduler (kube-scheduler)**: The scheduler is responsible for assigning
    Pods to nodes based on resource availability, user-defined constraints, taints,
    and selectors. This ensures each Pod is placed on the optimal node that satisfies
    not only resource needs but also respects scheduling policies such as taints and
    affinity/anti-affinity selectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller manager (kube-controller-manager)**: This component runs various
    controller processes in the background. It observes the state of the cluster,
    manages the life cycle of workloads, and handles operations on nodes to ensure
    the desired state of the Kubernetes cluster is maintained.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud controller manager**: An architectural component that embeds cloud-specific
    control logic, allowing cloud vendors to link their platforms with Kubernetes.
    It abstracts away the cloud-specific code from core Kubernetes logic, enabling
    each cloud service to develop its plugins independently. Each node component,
    which hosts the pods, consists of essential components for maintaining and managing
    the containers and network communication:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubelet**: This agent ensures that containers are running in a **Pod**, as
    per the specifications defined in the Pod’s configuration. It manages the state
    of each Pod on the node, communicating with the control plane of the master node
    (or master nodes in the case of highly available Kubernetes clusters).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container runtime**: This is the underlying software that is responsible
    for running containers. Kubernetes supports several container runtimes, such as
    Docker, containerd, and CRI-O, enabling it to run containerized applications.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-proxy**: This component oversees network interactions to and from the
    Pods. It routes TCP and UDP packets and facilitates connection forwarding, adding
    a Kubernetes service abstraction that acts as a proxy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 2**.1* illustrates the Kubernetes architecture described here, with
    components for the control plane and each component node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – The Kubernetes cluster architecture](img/B22100_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – The Kubernetes cluster architecture
  prefs: []
  type: TYPE_NORMAL
- en: For a more in-depth understanding of each component in the Kubernetes architecture,
    please refer to the official Kubernetes documentation ([https://kubernetes.io/](https://kubernetes.io/)).
    This resource provides comprehensive information and detailed explanations of
    various aspects of the Kubernetes system, including its master and node components.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of what Kubernetes is and the main components
    that run in a Kubernetes cluster, it’s time to learn how to set up a local cluster
    on your laptop using K3s, a lightweight Kubernetes distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring K3s as a lightweight Kubernetes distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, throughout this book, and specifically in this chapter,
    we will utilize K3s, a **lightweight** Kubernetes distribution ([https://k3s.io/](https://k3s.io/)),
    to run our examples.
  prefs: []
  type: TYPE_NORMAL
- en: K3s is particularly well-suited for scenarios where the **full-scale** implementation
    of Kubernetes may be too resource-intensive or complex.
  prefs: []
  type: TYPE_NORMAL
- en: 'Its lightweight nature makes it ideal for edge computing and IoT scenarios,
    where resources are often limited, and efficiency is paramount. In these environments,
    K3s provides the necessary Kubernetes features without the overhead. Additionally,
    solutions such as vCluster from Loft have leveraged K3s to run Kubernetes within
    Kubernetes, facilitating multi-tenancy on a host cluster. This approach allows
    for isolated Kubernetes environments within a single cluster, optimizing resource
    usage and offering scalability in multi-tenant setups. These use cases highlight
    K3s’s versatility and efficiency in diverse computing environments. More information
    about K3s can be found in the official documentation: [https://docs.k3s.io/](https://docs.k3s.io/).'
  prefs: []
  type: TYPE_NORMAL
- en: Origin of the K3s name
  prefs: []
  type: TYPE_NORMAL
- en: The name K3s, as explained in the official documentation (https://docs.k3s.io/),
    is derived from the intent to create a Kubernetes installation that’s significantly
    smaller in memory size. The naming convention follows that of Kubernetes, often
    abbreviated as K8s, which consists of 10 letters. Halving this led to K3s, which
    was stylized to represent a more compact version of Kubernetes. Unlike Kubernetes,
    K3s does not have an expanded form, and its pronunciation is not officially defined.
    This naming reflects the goal of a lighter, more efficient version of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: K3s simplifies the process of deploying a Kubernetes cluster, making it accessible
    even for small-scale operations or development purposes. By removing non-essential
    components and using lighter-weight alternatives, K3s significantly reduces the
    size and complexity of Kubernetes while maintaining its core functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: K3s maintain compatibility with the larger Kubernetes ecosystem, ensuring that
    tools and applications designed for Kubernetes can generally be used with K3s
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key features of K3s is its single binary installation, which includes
    both the Kubernetes server and agent, simplifying the setup process. This makes
    it an ideal choice for developers who want to quickly set up a Kubernetes environment
    for testing or development without the overhead of a full Kubernetes installation.
  prefs: []
  type: TYPE_NORMAL
- en: K3s also offers flexible networking and storage options, catering to a wide
    range of use cases – from small local clusters to larger, more complex environments.
    Its versatility and ease of use make it a popular choice for those looking to
    explore Kubernetes without the need for extensive infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, K3s’s lightweight nature and efficiency make it a suitable choice for
    **continuous integration**/**continuous deployment** (**CI/CD**) pipelines, allowing
    for faster build and test cycles in environments where resources are a consideration.
    In [*Chapter 5*](B22100_05.xhtml#_idTextAnchor081), we’ll learn how to use K3s
    to run Kubernetes on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Local cluster setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before diving into our first deployment example, it’s essential to set up the
    environment and understand how Kubernetes, particularly K3s, facilitates our deployments.
    K3s is primarily designed for Linux environments, so make sure you have a modern
    Linux system such as Red Hat Enterprise Linux, CentOS, Fedora, Ubuntu/Debian,
    or even Raspberry Pi. If you’re a Windows user, you can still engage with K3s
    by setting up **WSL** or running a Linux **virtual machine** (**VM**) through
    **VirtualBox**. These setups will prepare you to harness the power of Kubernetes
    for your deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing your local Kubernetes environment – K3s, Minikube, and alternatives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we have chosen to use K3s due to its lightweight nature and
    ease of setup, which makes it particularly suitable for developing and testing
    Kubernetes environments. However, there are several other alternatives for setting
    up local Kubernetes clusters that cater to different needs and platforms. For
    instance, Colima ([https://github.com/abiosoft/colima](https://github.com/abiosoft/colima))
    is an excellent choice for macOS users, offering a Docker and Kubernetes environment
    directly on macOS with minimal configuration. **Minikube** ([https://minikube.sigs.k8s.io](https://minikube.sigs.k8s.io))
    is another popular option that runs on Windows, macOS, and Linux and is ideal
    for those looking to simulate a Kubernetes cluster in a single node where they
    can experiment and test Kubernetes applications.
  prefs: []
  type: TYPE_NORMAL
- en: While K3s is our choice for this chapter, you are encouraged to use the local
    cluster setup that best fits your platform or preferences. In subsequent chapters,
    we will primarily focus on using K3s or Minikube. These platforms provide a convenient
    and consistent environment for learning and deploying applications using Kubernetes,
    ensuring that the concepts and procedures we’ll explore are accessible regardless
    of the specific local cluster technology used.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up WSL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All details regarding the nature of WSL and the procedures for installing it
    on Windows are beyond the scope of this book. However, comprehensive guidance
    on setup steps and in-depth information about WSL can be accessed through the
    official Microsoft documentation (see [*1*] in the *Further reading* section at
    the end of this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – A conceptual illustration representing WSL on a Windows operating
    system](img/B22100_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – A conceptual illustration representing WSL on a Windows operating
    system
  prefs: []
  type: TYPE_NORMAL
- en: Remember, staying updated with the latest WSL versions and features through
    the official site will enhance your experience and ensure compatibility with the
    most recent Windows updates.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up VirtualBox
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**VirtualBox** is an open source **virtualization software** developed by Oracle.
    It allows users to run multiple operating systems on a single physical computer,
    creating VMs that can operate independently. This makes it an invaluable tool
    for software testing, development, and educational purposes as it provides a flexible
    and isolated environment for running and experimenting with different operating
    systems without risk to the host system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – The VirtualBox home page at https://www.virtualbox.org/.](img/B22100_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – The VirtualBox home page at https://www.virtualbox.org/.
  prefs: []
  type: TYPE_NORMAL
- en: The detailed steps for installing VirtualBox are beyond the scope of this book.
    However, comprehensive installation instructions and additional information can
    be found in the official documentation [*2*].
  prefs: []
  type: TYPE_NORMAL
- en: For the most current information and tips, visiting the official VirtualBox
    documentation is highly recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise specified, for this chapter and the subsequent ones, we will
    assume the use of an **Ubuntu-22.04 LTS** installation within WSL. This setup
    provides a consistent and controlled environment for our examples and demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: By focusing on a specific version of Ubuntu, we ensure that the instructions
    and scenarios presented are as relevant and applicable as possible, aligning closely
    with the most common and stable Linux distribution used in WSL.
  prefs: []
  type: TYPE_NORMAL
- en: K3s setup and installation verification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll cover the basic steps that are necessary to establish
    a Kubernetes cluster using K3s in its default configuration, assuming that WSL
    is already installed and functioning correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and installing K3s
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Follow these steps to download and install K3s:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by opening a new Terminal window and typing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.4 – Successfully installing an instance of Ubuntu 22.04.3 LTS on
    WSL](img/B22100_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Successfully installing an instance of Ubuntu 22.04.3 LTS on WSL
  prefs: []
  type: TYPE_NORMAL
- en: 'Before proceeding with the K3s setup, it is always better to execute commands
    to update the operating system with the latest patches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This ensures that you are working with the most recent and secure versions of
    the software.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The apt update and apt upgrade commands
  prefs: []
  type: TYPE_NORMAL
- en: The `apt update` and `apt upgrade` commands are fundamental in maintaining the
    software on systems using the APT package manager, commonly found in Debian-based
    Linux distributions such as Ubuntu. The `apt update` command refreshes the local
    package index by retrieving the latest information about available packages and
    their versions from configured sources. This doesn’t install or upgrade any packages
    and instead updates the package lists to inform the system of new, removed, or
    updated software. Once the package index has been updated, the `apt upgrade` command
    is used to upgrade installed packages to their latest versions. It downloads and
    installs the updates for any packages where newer versions are available, ensuring
    the system is up-to-date and potentially more secure.
  prefs: []
  type: TYPE_NORMAL
- en: 'If required, enter the password you set up while installing Ubuntu. After executing
    these commands, the Terminal should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Terminal window after executing the apt update and apt upgrade
    commands](img/B22100_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Terminal window after executing the apt update and apt upgrade
    commands
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to install K3s using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding command will download and set up the necessary tools, followed
    by launching the K3s server. The successful setup of a K3s instance is depicted
    in *Figure 2**.6*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Successfully setting up K3s](img/B22100_02_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 2.6 – Successfully setting up K3s
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the K3s installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is necessary to use two commands to check the correctness of the K3s setup
    and configuration. The first one is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command is used to check which version of K3s we are running.
    If the K3s server is running correctly, we should be able to see a message similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – The result of executing the k3s –version command](img/B22100_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – The result of executing the k3s –version command
  prefs: []
  type: TYPE_NORMAL
- en: 'The second command that checks the correctness of the K3s setup is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `k3s check-config` command performs a diagnostic check on the system’s
    configuration to ensure it is suitable for running a K3s cluster. It verifies
    critical aspects such as kernel compatibility, required system dependencies, and
    the presence of necessary features and modules. This command helps in identifying
    potential issues or missing configurations before proceeding with the K3s installation,
    ensuring a smoother setup process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Successfully configuring the k3s check-config command](img/B22100_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – Successfully configuring the k3s check-config command
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have confirmed that the K3s server has been installed in
    your local development environment. Now, it’s time to verify the Kubernetes cluster
    and deploy a test application.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the Kubernetes cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To confirm that our K3s node is up and running, let’s type the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If the Kubernetes cluster is working correctly, the preceding command will
    produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Example output after running the kubectl get nodes command](img/B22100_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Example output after running the kubectl get nodes command
  prefs: []
  type: TYPE_NORMAL
- en: 'After confirming that the node is up and running correctly, we can run the
    following command to obtain more information about the running cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `kubectl cluster-info` command is a useful tool in Kubernetes for obtaining
    essential information about a cluster. When executed, it displays key details
    such as the Kubernetes master and services endpoint addresses. This command helps
    users quickly understand the state and connectivity of their cluster’s control
    plane and core services such as KubeDNS and, when applicable, the dashboard. It
    is particularly valuable for troubleshooting and ensuring that the Kubernetes
    cluster is configured correctly and operational. Easy to use, `kubectl cluster-info`
    is often one of the first commands you should run to verify the health and status
    of a Kubernetes environment, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Information provided after executing the kubectl cluster-info
    command](img/B22100_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Information provided after executing the kubectl cluster-info
    command
  prefs: []
  type: TYPE_NORMAL
- en: kubectl
  prefs: []
  type: TYPE_NORMAL
- en: '**kubectl** is a command-line tool that serves as the primary interface for
    interacting with Kubernetes. It allows users to deploy applications, inspect and
    manage cluster resources, and view logs. Essentially, kubectl provides the necessary
    commands to control Kubernetes clusters effectively. Users can create, delete,
    and update parts of their Kubernetes applications and infrastructure using this
    versatile tool. It is designed to be user-friendly, offering comprehensive help
    commands and output formatting options, making it easier to understand and manage
    complex Kubernetes environments. kubectl is an indispensable tool for developers
    and system administrators working with Kubernetes, offering a robust and flexible
    way to handle containerized applications and services in various environments.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes manifest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Kubernetes manifest is a configuration file, typically written in YAML or
    JSON, that defines resources that should be deployed to a Kubernetes cluster.
    It specifies the desired state of objects, such as Pods, Services, or Deployments,
    that Kubernetes needs to create and manage. This manifest enables users to declare
    their applications’ requirements, networking, and storage configurations, among
    other settings, in a structured and versionable format.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, a basic Kubernetes manifest for deploying a simple application
    might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this manifest, a Pod named `hw-gitops-folks` is defined. It contains one
    container named `hw-gitops-container`, which uses the `echoserver:1.4` image from
    Kubernetes’ container registry. The container exposes port `8080`. This manifest,
    when applied to a Kubernetes cluster, will create a Pod running a simple echo
    server that can be used for basic testing.
  prefs: []
  type: TYPE_NORMAL
- en: Our first deployment with K3s
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have successfully set up, configured, and verified our K3s cluster,
    we are poised to embark on an exciting phase: preparing for our first deployment.
    This step marks a significant milestone in our journey as we transition from the
    foundational aspects of K3s to actively utilizing the cluster for practical applications.
    The upcoming deployment process will not only reinforce our understanding of Kubernetes
    concepts but also demonstrate the real-world utility of our K3s environment. It’s
    a moment where theory meets practice, allowing us to see firsthand how our configured
    cluster can host and manage applications. Let’s proceed with an eagerness to explore
    the capabilities of our Kubernetes setup while keeping the practices we’ve learned
    and the robust infrastructure we’ve established in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by typing the following command, which should list all the running
    Pods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result of its execution should look something like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding output is normal since no deployments have been performed so
    far. Let’s try another command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.11 – Example of running Pods in the kube-system namespace](img/B22100_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – Example of running Pods in the kube-system namespace
  prefs: []
  type: TYPE_NORMAL
- en: What is a namespace in Kubernetes?
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes, a namespace is a fundamental concept that’s used to organize
    clusters into logically isolated sub-groups. It provides a way to divide cluster
    resources between multiple users and applications. Essentially, namespaces are
    like virtual clusters within a physical Kubernetes cluster. They allow for resource
    management, access control, and quota management, enabling efficient and secure
    multi-tenancy environments. For instance, different development teams or projects
    can operate in separate namespaces, without interference. Namespaces also facilitate
    resource naming, ensuring that resources with the same name can coexist in different
    namespaces. They play a crucial role in Kubernetes for scalability and maintaining
    order, especially in larger systems with numerous applications and teams.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating different namespaces in Kubernetes is widely regarded as a best practice
    for several compelling reasons. Namespaces provide a logical partitioning of the
    cluster, allowing for more organized and efficient resource management. This separation
    is particularly beneficial in environments with multiple teams or projects as
    it ensures a clear distinction between resources, reduces naming conflicts, and
    enhances security by isolating workloads. Additionally, namespaces facilitate
    fine-grained access control as administrators can assign specific permissions
    and resource limits to different namespaces, preventing accidental or unauthorized
    interactions between distinct parts of the cluster. By using namespaces, teams
    can also streamline deployment processes and monitor resource usage more effectively,
    leading to a more robust and scalable Kubernetes environment. In essence, namespaces
    are crucial in maintaining order, security, and efficiency in complex Kubernetes
    clusters. So, let’s get started by creating one:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s continue by creating a new namespace before continuing with our first
    deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The response to this command should look something like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The command to delete a namespace is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'apiVersion: apps/v1'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'kind: Deployment'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'name: hello-world-deployment'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'namespace: gitops-kubernetes'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '...'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'spec:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '...'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'spec:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'containers:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- name: hello-world'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'image: nginxdemos/hello'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'ports:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- containerPort: 80'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '---'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'kind: Service'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'name: hello-world-service'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'namespace: gitops-kubernetes'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'spec:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'type: NodePort'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '...'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'ports:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- protocol: TCP'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'port: 80'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'nodePort: 30007'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To edit the file, we can use an editor such as `nano` by running the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`hello-world-service` of the `NodePort` type to expose the deployment. This
    service makes the hello-world application accessible on a port on the nodes in
    the cluster (in this example, `port 30007`). In the metadata section, we have
    specified to run the service in the namespace we created previously – that is,
    `namespace: gitops-kubernetes`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NodePort
  prefs: []
  type: TYPE_NORMAL
- en: In this hello-world service example, the `NodePort` service type was chosen
    to demonstrate a simple way of exposing a service to external traffic in Kubernetes.
    `NodePort` opens a specific port on all the nodes; any traffic sent to this port
    is forwarded to the service. While this is useful for development and testing,
    it may not be ideal in a real-world cloud scenario, especially when running on
    a VM in the cloud. This is because `NodePort` exposes a port on the host VM/node,
    potentially posing a security risk by making the service accessible externally.
    In production environments, more secure and controlled methods of exposing services
    are typically preferred.
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply this manifest, use the `kubectl apply -f <``filename>.yaml` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The response to this command should look something like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can list the Pods and services that are running in the `gitpos-kubernetes`
    namespace using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result of this command is shown in *Figure 2**.12*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Results of applying the deployment file, where we can see useful
    information such as the Cluster-IP and the assigned ports](img/B22100_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – Results of applying the deployment file, where we can see useful
    information such as the Cluster-IP and the assigned ports
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have deployed our application in the Kubernetes cluster, the next
    crucial step is to test its functionality. This is where **port forwarding** plays
    a key role.
  prefs: []
  type: TYPE_NORMAL
- en: Port forwarding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Port forwarding with kubectl allows us to temporarily route traffic from our
    local machine to a pod in the Kubernetes cluster. This method is especially useful
    for testing purposes as it enables us to interact with the application as if it
    were running locally, without the need to expose it publicly. By forwarding a
    local port to a port on the pod, we can verify the deployment’s operational aspects,
    ensuring that our application behaves as expected in a controlled environment
    before making it accessible to external traffic. The following steps outline the
    process for executing port forwarding on the running pod and testing its functionality
    using **curl**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl` command to start port forwarding from a local port to a port on the
    Pod:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, we are using the Pod’s name, `hello-world-deployment-6b7f766747-nxj44`.
    So, if we want to forward traffic from local port `9000` to the Pod’s port, `80`,
    the command would be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding output indicates that port forwarding is set up on your machine
    to redirect traffic from a local port to a port on a Kubernetes Pod or another
    network service. Keep this command running as it maintains the port forwarding
    session.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open a new Terminal or Command Prompt and type the following command to open
    a new WSL shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '9000, which kubectl then forwards to the Pod’s port (80). You should see the
    output of the request in your Terminal. Typically, this is the content that’s
    served by your application running in the Kubernetes Pod, as shown in *Figure
    2**.13*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.13 – Example of content served by our application running in the
    Kubernetes Pod](img/B22100_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – Example of content served by our application running in the Kubernetes
    Pod
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations on achieving this remarkable result! You’ve successfully deployed
    your first application in Kubernetes, and the content is being correctly served,
    as evidenced by the successful `curl` call. This is a significant milestone in
    your journey with Kubernetes, showcasing your ability to not only deploy an application
    but also ensure its proper functioning within the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming section, we will delve deeper into Docker, closely examining
    its essential components, functionalities, and practical applications. We’ll build
    our first Docker image and demonstrate how to run it as a container locally.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containers have become a cornerstone in cloud-native application development
    due to their ability to package and isolate applications with all their dependencies.
    This isolation ensures consistency across various environments, making them highly
    efficient for both development and deployment. **Container images**, which are
    static files containing executable code and dependencies, follow a **layered**
    structure for efficient modification and storage, with each layer representing
    changes or additions.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the versatility of containers, Kubernetes does not provide a native
    mechanism for building these images, necessitating external tools such as **Docker**.
  prefs: []
  type: TYPE_NORMAL
- en: Docker, an open source platform, has transformed the world of containerization
    by simplifying the creation, deployment, and execution of applications in containers.
    It enables developers to encapsulate applications with their dependencies in a
    unified format, facilitating software development. Docker’s containers offer a
    **semi-isolated** environment, balancing isolation with efficiency, allowing multiple
    containers to run concurrently on a single host. These containers are both lightweight
    and portable, ensuring uniform functionality across diverse platforms, from local
    laptops to cloud infrastructures.
  prefs: []
  type: TYPE_NORMAL
- en: Docker files are instrumental in creating these images, specifying the steps
    and components to be included.
  prefs: []
  type: TYPE_NORMAL
- en: The **Open Container Initiative** (**OCI**) standardizes container image formats
    and runtimes, further enhancing interoperability and portability across different
    containerization technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Docker setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Up until now, we have focused on using Ubuntu 22.04 as an instance within WSL.
    While a step-by-step setup of Docker falls outside the scope of this book, you
    can find comprehensive installation guides and troubleshooting tips in the official
    Docker documentation: [https://docs.docker.com/engine/install/ubuntu/](https://docs.docker.com/engine/install/ubuntu/).
    After successfully installing Docker, you can verify its installation and check
    that Docker is running correctly on your system by typing the following command
    in your Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sudo docker run hello-world` command quickly verifies the installation
    and setup of Docker by running a very simple container. When executed, it does
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo`: Ensures the command is run with superuser privileges, which are often
    required for Docker commands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker run`: Tells Docker to run a container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hello-world`: Specifies the image to use. In this case, it’s the `hello-world`
    image, a minimal Docker image created by Docker, Inc. It’s commonly used as a
    test image to validate that Docker is installed and running correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If Docker has been correctly installed and configured, this command will pull
    the `hello-world` image from Docker Hub (if it’s not already downloaded), create
    a new container from that image, and run it. The container simply displays a message
    confirming that Docker is installed correctly and then exits, as shown in *Figure
    2**.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 – Result of executing the docker run hello-world command](img/B22100_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – Result of executing the docker run hello-world command
  prefs: []
  type: TYPE_NORMAL
- en: Docker alternatives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although Docker is one of the most popular tools for building container images,
    there are several alternative tools available:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Podman**: An open source, daemonless container engine that can run on Linux
    systems. It is compatible with Docker but does not require a running daemon. Podman
    is known for enabling easier management of containers and pods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rancherdesktop**: An open source application that provides all the essentials
    to work with containers and Kubernetes on desktop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**containerd**: A core container runtime that adheres to industry standards,
    available as a service for both Linux and Windows. It is capable of managing the
    entire life cycle of containers on its host system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CRI-O**: This is a realization of the Kubernetes Container Runtime Interface,
    facilitating the use of runtimes compatible with the OCI. It serves as a bridge,
    connecting OCI-compliant runtimes with kubelet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rkt (pronounced ‘rocket’)**: Developed by CoreOS, it’s a Pod-native container
    engine for Linux. It’s designed for security, simplicity, and composability within
    modern cluster environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LXD**: A cutting-edge manager for system containers and VMs that provides
    a user experience akin to VMs but through the use of Linux containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenVZ**: This is a virtualization solution built on container technology
    for Linux systems that’s capable of generating several secure and isolated Linux
    containers on a singular physical server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To assist in choosing the most suitable containerization tool for your specific
    needs, the following table provides a comparison of various Docker alternatives.
    It highlights their key features and ideal use cases, offering insights into which
    tool may best align with your project’s requirements or preferences:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Alternative** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Podman | Best for environments that prioritize security and for users who
    prefer a solution without a daemon. It’s fully compatible with Docker’s CLI, making
    it a seamless replacement. |'
  prefs: []
  type: TYPE_TB
- en: '| Rancherdesktop | A user-friendly, GUI-based tool tailored for developers
    who want an easier way to manage containers and Kubernetes, especially on desktop
    environments for development and testing purposes. |'
  prefs: []
  type: TYPE_TB
- en: '| Containerd | Chosen for its performance and reliability as a container runtime
    in production environments. Lacks Docker’s image-building features but excels
    in running containers efficiently. |'
  prefs: []
  type: TYPE_TB
- en: '| Rkt | Previously a viable alternative, but its development has ceased, potentially
    limiting its suitability for long-term projects. |'
  prefs: []
  type: TYPE_TB
- en: '| OpenVZ | Ideal for hosting solutions or for scenarios requiring multiple,
    isolated Linux environments on a single host, with a focus on resource efficiency
    and scalability. |'
  prefs: []
  type: TYPE_TB
- en: Table 2.1 – Comparative overview of containerization tools – evaluating alternatives
    to Docker for diverse development needs
  prefs: []
  type: TYPE_NORMAL
- en: Dockerfile
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step in creating a container image involves defining a **Dockerfile**,
    which is essentially a **blueprint** for the image. This file contains a set of
    instructions and commands that tell Docker how to build the image. It starts with
    specifying a base image to build upon, often a minimal version of an operating
    system, such as Ubuntu or Alpine Linux. Then, additional layers are added by specifying
    dependencies, copying application files, and setting environment variables. Each
    command in a Dockerfile creates a new layer in the image, building up the environment
    that’s needed to run the application. The following is an example of a Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a closer look at this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FROM python:3.8-slim`: This line indicates the base image from which you are
    building. The Dockerfile starts with the Python 3.8 image, specifically the slim
    variant, which is a smaller, more compact version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WORKDIR /usr/src/app`: This line sets the working directory inside the container
    to `/usr/src/app`. Future commands will run in this directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`COPY . .`: This line copies files from the Dockerfile’s current directory
    to the working directory in the container (`/usr/src/app`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RUN pip install --no-cache-dir -r requirements.txt`: This line executes a
    command inside the container, which in this case is installing Python dependencies
    listed in `requirements.txt`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EXPOSE 80`: The line informs Docker that the container listens on port `80`
    at runtime. Note that this does not publish the port.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ENV NAME World`: This line sets the `NAME` environment variable to `World`.
    This can be used by the application running in the container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CMD ["python", "app.py"]`: The default command to run when a container starts.
    This line runs the Python application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This Dockerfile provides a simple example of building an image of a simple
    `requirements.txt` file and you want to build a Docker image from this Dockerfile.
    The command you would use is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'At this stage, as shown in *Figure 2**.15*, your container image is in the
    process of being built. During this build, Docker retrieves any existing layers
    from **public container registries** such as DockerHub, Quay, or Red Hat Registry.
    The topic of container registries will be introduced in the upcoming pages. It
    then adds a new layer based on the instructions in your Dockerfile. If some layers
    are already present locally, Docker will use these from the container cache or
    Docker cache, speeding up the build process by avoiding redundant downloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15 – Result of the docker build command](img/B22100_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – Result of the docker build command
  prefs: []
  type: TYPE_NORMAL
- en: 'The container image is now available in the local Docker cache and ready to
    be used. Its presence can be verified with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the image has been created, it can be used locally or uploaded to a public
    container registry for external use, such as within a CI/CD pipeline. For our
    purposes, we’ll run the container image locally. To do this, use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command includes several options:'
  prefs: []
  type: TYPE_NORMAL
- en: The `-p` option binds a port on the host to a port on the container, allowing
    external access to the container’s services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `-t` option allocates a pseudo-TTY, which provides a Terminal within the
    container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `-i` option enables interactive mode, allowing interaction with the container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `-d` option runs the container in the background and outputs a hash, which
    can be used for asynchronous interaction with the container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Public container registry
  prefs: []
  type: TYPE_NORMAL
- en: A public container registry is an online service where users can store and share
    container images. It serves as a centralized repository, facilitating the distribution
    of containerized applications. To upload and manage images, users typically need
    to create an account with the registry provider. This account allows them to publish,
    update, and maintain their images, making them accessible to others. Public registries
    such as Docker Hub, Google Container Registry, and Amazon Elastic Container Registry
    are popular choices, offering easy access over the internet. These platforms not
    only provide storage for container images but often come with additional features
    such as version control, cataloging, and security scanning. An account with these
    services enables developers to deploy applications consistently across different
    environments, streamline software development, and collaborate more effectively
    with others in the community.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the process of creating an account with a public container
    registry, although a crucial step for managing and distributing container images,
    falls outside the scope of this chapter and book. Each registry, such as Docker
    Hub or Google Container Registry, has its own set of guidelines and procedures
    for account creation and management. You are encouraged to refer to the specific
    documentation provided by these services for detailed instructions on setting
    up an account.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding command will launch the application within the Docker network
    and bind it to port `8080` on our local machine. It will then wait for incoming
    requests, as illustrated in *Figure 2**.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Result of the docker run command](img/B22100_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 – Result of the docker run command
  prefs: []
  type: TYPE_NORMAL
- en: 'From a new Terminal, we can try to access the running container using `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll receive a response similar to the one shown in *Figure 2**.17*, where
    I used my name to obtain the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17 – Example of responses received from our Python Flask application
    running as a containerized image](img/B22100_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.17 – Example of responses received from our Python Flask application
    running as a containerized image
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are equipped with the necessary tools and understanding of Docker
    and containers, in the next section, will integrate these elements so that we
    can construct our first CD pipeline using Docker and K3s.
  prefs: []
  type: TYPE_NORMAL
- en: Sample workflow – effortless CD with Docker and K3s
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we are ready to create a very simple CD pipeline using the tools
    we’ve explored so far. The basic idea is to simulate the operations performed
    by a developer who needs to update the Flask app we’ve used so far to add a new
    feature that allows the current date and time to be retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our example will consist of performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Local development**: We will edit the previous Python Flask app to expose
    a new service that returns the current date and time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dockerizing the application and running it locally**: We will build the new
    version of the Docker image locally using the docker build command, as we did
    previously. Use the *Dockerfile* section as a reference.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After building the image, we will run it locally using Docker to ensure the
    containerized application works as expected. Use the *Dockerfile* section as a
    reference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Publishing the image to a public container registry**: We will publish the
    build image to a public registry repository.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deploying to K3s**: We will write the Kubernetes manifest file to specify
    how our application should be deployed on K3s, including which Docker image to
    use and the desired number of replicas.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will apply this configuration to your K3s cluster using the commands you
    learned about in the *Exploring K3s as a lightweight Kubernetes distribution*
    section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Local development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Edit the `app.py` file present in this book’s GitHub repository by adding the
    following Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: You’re free to use whatever code editor you like to edit this file – it doesn’t
    matter.
  prefs: []
  type: TYPE_NORMAL
- en: Dockerizing the application and running it locally
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to Dockerize the application and run It locally:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *Dockerfile* section, we created the first version of our Docker image,
    tagged as `hello-world-py-app:1.0`. Now that we have added a new feature, it’s
    time to create a new version of that image. We will use a tag of `2.0` using the
    following `docker` `build` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upon typing the following command, you should be able to see both images listed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result of this command should look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can run the Docker image locally with the following `docker` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will produce a result similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From a new Terminal, we can try to access the running container using `curl`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We’ll obtain the current response:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Well done – as a developer, you have tested that the new feature is working
    as expected! Now, we can publish our image to a public repository.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing the image to a container registry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Publishing our `hello-world-py-app:2.0` Docker image to a public repository
    involves several steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming that you have already an account on a public container registry such
    as Docker Hub, the first step is to open a new Terminal and log in to the registry
    using the Docker CLI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: $ sudo docker tag hello-world-py-app:2.0 [yourusername]/hello-world-py-app:2.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we need to push the tagged image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Deploying to K3s
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The time to deploy our image to our local Kubernetes cluster has finally arrived!
    We can reuse the same Kubernetes manifest file that we used in the *Our first
    deployment with K3s* section, but we are going to apply a couple of edits, with
    the most important one being to update the manifest file so that it indicates
    where the Kubernetes cluster has to download the image, using our container repository.
    So, let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `deployment` section of the manifest file, we have to change the image
    value from `nginxdemos/hello` to `[yourusername]/hello-world-py-app:2.0`. Then,
    we have to change the name (where specified in the file) from `hello-world` to
    `first-cd-pipeline`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we have also changed the name of the deployment to `first-cd-pipeline-deployment`.
    The deployment file can be found in the `Chapter02` folder in this book’s GitHub
    repository.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Save the new file, naming it as `first-cd-pipeline-deployment.yaml`, and apply
    the deployment with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The response should look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before establishing port forwarding, as described at the end of the *Our first
    deployment with K3s* section, we need to get some useful information by running
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will produce an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, we have all the information we need to perform port forwarding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: $ curl http://localhost:8080/datetime
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '2024-01-13 17:59:39'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To delete the deployment, type the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations on reaching this milestone with a manual CD deployment!
  prefs: []
  type: TYPE_NORMAL
- en: The steps outlined here for publishing a Docker image to a public container
    registry should be viewed as a manual example that illustrates the basic principles
    of CD. In practice, however, this process is typically automated using tools such
    as Git Actions, which streamline and optimize the deployment cycle. While these
    manual steps provide a foundational understanding, real-world applications often
    rely on more sophisticated automation for efficiency and consistency. In the next
    chapter, we’ll delve into how such tools can be integrated into your workflow,
    thereby enhancing the CD process and reducing the need for manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we navigated the practical aspects of deploying cloud-native
    applications using Kubernetes and K3s, highlighting key techniques for efficient
    container management and orchestration. This chapter focused on building foundational
    skills that are crucial for managing cloud-native environments, including understanding
    Kubernetes resources and deployment methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: As we move to the next chapter, the emphasis will shift to introducing Git tools.
    We’ll explore how these tools can be leveraged to create an automated CI/CD pipeline,
    an essential component for seamlessly deploying and managing cloud-native applications,
    as well as enhancing development and operational workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*1*] [https://learn.microsoft.com/en-us/windows/wsl/about](https://learn.microsoft.com/en-us/windows/wsl/about)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*2*] [https://www.virtualbox.org/](https://www.virtualbox.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
