<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer378">
    <h1 class="chapterNumber">18</h1>
    <h1 class="chapterTitle" id="_idParaDest-591">Security in Kubernetes</h1>
    <p class="normal">Authentication and authorization are the cornerstones of modern software systems in terms of providing the necessary identity management and access management, respectively. Many people confuse these two terms, despite the fact that they are quite different processes. Authentication has to do with the verification of the identity of a user, normally through some kind of mechanism like usernames and passwords, while authorization is all about what an authenticated user can access or do within a system. Authentication always comes first, after which authorization would take place in order for the system to be interacted with by verified users. Kubernetes extends this further with another model called <strong class="keyWord">Role-Based Access Control</strong> (<strong class="keyWord">RBAC</strong>), which allows an administrator to define roles with certain privileges and then assign those roles to users, hence effectively implementing the principle of least privilege and allowing fine-grained access control.</p>
    <p class="normal">Apart from Identity and Access Management, Kubernetes has a number of other security mechanisms to harden the rest of the components further. Being the most mature and widely adopted container orchestration platform, the design of Kubernetes places a lot of emphasis on the security of a wide range of components within clusters, nodes, containers, networks, and applications through the mitigation of risks at many layers.</p>
    <p class="normal">Next, this chapter goes into some of the basic Kubernetes security concepts, from the different ways in which the system can flexibly authenticate-in X509 client certificates or tokens from OpenID Connect. In specialized cases, for example, the integration with LDAP, Kubernetes provides additional options. For example, the possibility of using an authenticating proxy or webhooks is also recommended. Then we will review the <strong class="keyWord">RBAC</strong> model from the platform that gives administrators control over access to resources in the cluster and allows them to manage users and groups along with ServiceAccounts.</p>
    <p class="normal">We will also introduce one advanced feature in Kubernetes: Admission Controllers. An Admission Controller enforces security policies at the point of resource admission to validate and control resources before they enter the cluster. Admission controllers provide an additional layer of defense by governing resource requests through the enforcement of policies on the creation and modification of resources.</p>
    <p class="normal">Pods and containers themselves need to be secured, as these are the runtimes of the workloads or applications that could interact with sensitive information. Kubernetes provides a set of <code class="inlineCode">securityContext</code> options that enable administrators to declare particular security settings for containers; this includes forcing containers to run as non-root. Equally important will be network security, and we’ll discuss how NetworkPolicies provide a mechanism to segregate and secure pod communication inside the cluster by controlling traffic flow at a granular level.</p>
    <p class="normal">We’ll then move to container runtime security. We will look at gVisor and Kata Containers as options for runtime, which introduce more security boundaries between either a user-space kernel to intercept system calls or a lightweight VM environment per container, respectively, which provides the speed of containers but the security of a VM.</p>
    <p class="normal">Lastly, and most importantly, private registry credentials hold the key to guaranteeing security around container images inside the cluster. We will go through how Kubernetes handles these credentials safely – ensuring that only authorized components have access to them. By the end of this chapter, you will have a deeper understanding of these advanced security concepts and tools in Kubernetes. You will see precisely how to enhance your cluster’s security posture, reduce risks, and have the best defense against possible vulnerabilities. With such measures, you will be able to secure your Kubernetes deployments at every layer, from identity management right through to runtime isolation, and reinforce the robustness of your containerized applications.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Authentication and Authorization – User Access Control</li>
      <li class="bulletList">Admission Control – Security Policies and Checks</li>
      <li class="bulletList">Securing Pods and Containers</li>
      <li class="bulletList">Managing Secrets and Registry Credentials</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-592">Technical Requirements</h1>
    <p class="normal">For this chapter, you will need the following:</p>
    <ul>
      <li class="bulletList">A Kubernetes cluster to be deployed. We recommend using a multi-node o<em class="italic">r</em> cloud-based Kubernetes cluster.</li>
      <li class="bulletList">The Kubernetes CLI (<code class="inlineCode">kubectl</code>) installed on your local machine and configured to manage your Kubernetes cluster.</li>
    </ul>
    <p class="normal">Basic Kubernetes cluster deployment (local and cloud-based) and <code class="inlineCode">kubectl</code> installation were covered in <em class="chapterRef">Chapter 3</em>, <em class="italic">Installing Your First Kubernetes Cluster</em>.</p>
    <p class="normal">The previous chapters (<em class="chapterRef">15</em>, <em class="chapterRef">16</em>, and <em class="chapterRef">17</em>) of this book have provided you with an overview of how to deploy a fully functional Kubernetes cluster on different cloud platforms and install the requisite CLIs to manage them.</p>
    <p class="normal">You can download the latest code samples for this chapter from the official GitHub repository: <a href="https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter18"><span class="url">https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter18</span></a>.</p>
    <h1 class="heading-1" id="_idParaDest-593">Authentication and Authorization – User Access Control</h1>
    <p class="normal">It gives grounds for access control in ensuring that only authenticated and authorized users can use Kubernetes resources. Authentication<a id="_idIndexMarker1520"/> verifies the identity of a user, while authorization<a id="_idIndexMarker1521"/> decides what an authenticated user is allowed to do within the cluster. Kubernetes offers flexibility in authenticating via X509 certificates, OpenID Connect, token-based, and other approaches. Coupled with the verification process, RBAC does offer fine granular control over what users can do, thus helping administrators manage a wide range of permissions efficiently – a subject that will be dealt with in further detail in the following sections.</p>
    <p class="normal">Let us start with authentication in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-594">Authentication and User Management</h2>
    <p class="normal">The Kubernetes API server provides RESTful endpoints for managing the Kubernetes cluster and acts as the frontend to the shared state of the cluster. All interactions with the cluster, from users to internal components, are channeled through the Kubernetes API server, which acts as a frontend to the cluster’s shared state.</p>
    <p class="normal">Let us see how the authentication mechanism works in Kubernetes in the next sections.</p>
    <h3 class="heading-3" id="_idParaDest-595">The authentication workflow in Kubernetes</h3>
    <p class="normal">Just like a<a id="_idIndexMarker1522"/> high-security facility, your Kubernetes cluster needs robust security measures to protect its resources. This involves a layered approach with several key components working together, as shown in the following figure:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_18_01.png"/></figure>
    <p class="packt_figref">Figure 18.1: Request to Kubernetes API goes through several stages (source: https://kubernetes.io/docs/concepts/security/controlling-access/)</p>
    <ul>
      <li class="bulletList"><strong class="screenText">Authentication</strong>: This <a id="_idIndexMarker1523"/>acts as the first line of defense, verifying the identity of anyone trying to access the Kubernetes API server. Imagine it like a security guard checking IDs at the entrance. Users might use passwords, tokens, or special certificates to prove they’re authorized.</li>
      <li class="bulletList"><strong class="screenText">Authorization</strong>: Once <a id="_idIndexMarker1524"/>someone’s identity is confirmed, authorization determines what they can actually do within the cluster. Think of it as granting specific access levels. Users might have permission to view resources, but not modify them, or they might be authorized to create new resources but only in specific areas.</li>
      <li class="bulletList"><strong class="screenText">Admission Control</strong>: This stage adds an extra layer of scrutiny. Imagine it like a security scanner at the entrance. Admission control<a id="_idIndexMarker1525"/> modules can inspect incoming requests, ensuring they comply with predefined security policies. They can even modify requests to enforce specific rules or reject them entirely if they pose a threat.</li>
      <li class="bulletList"><strong class="screenText">Auditing</strong>: Just like keeping a log of who enters and exits a secure facility, auditing<a id="_idIndexMarker1526"/> in Kubernetes keeps a record of all activity within the cluster. This includes actions taken by users, applications, and even the control plane itself. These logs are invaluable for monitoring suspicious activity and maintaining a secure environment.</li>
    </ul>
    <p class="normal">By working together, these security measures create a layered defense system, ensuring that only authorized users can access your Kubernetes cluster and that their actions comply with established security policies.</p>
    <p class="normal">We will learn some more details about the authentication mechanism in the next section.</p>
    <h3 class="heading-3" id="_idParaDest-596">Authentication to the Kubernetes API</h3>
    <p class="normal">Kubernetes API authentication<a id="_idIndexMarker1527"/> ensures that only authorized users or services are <a id="_idIndexMarker1528"/>allowed to talk to the resources running in a cluster. Each incoming request goes through an authentication setup, which is done in a chain of authenticator modules that are configured.</p>
    <p class="normal">Requests to the API are always one of the following:</p>
    <ul>
      <li class="bulletList">Associated with an external, normal user or a <strong class="keyWord">ServiceAccount</strong> defined <a id="_idIndexMarker1529"/>in the Kubernetes cluster.</li>
      <li class="bulletList">Treated as <em class="italic">anonymous</em> requests if the cluster has been configured to allow anonymous requests.</li>
    </ul>
    <p class="normal">This is determined in the <em class="italic">authentication</em> process – the entire HTTP request is used as input to the process, but usually only request headers or the client certificate is analyzed. Authentication is carried out by authentication modules that depend on the cluster configuration. Your cluster may have multiple authenticator modules enabled, and then each of them is executed in sequence until one succeeds. If the request fails to authenticate, the API server will either respond with an HTTP status code of <code class="inlineCode">401 (unauthorized)</code> or, if anonymous requests are enabled, treat it as anonymous.</p>
    <div class="packt_tip">
      <p class="normal">Anonymous requests are essentially mapped to a special username called <code class="inlineCode">system:anonymous</code> and a group called <code class="inlineCode">system:unauthenticated</code>. This means that you can organize your authorization to resources for such requests, just as you can for other users or ServiceAccounts.</p>
    </div>
    <p class="normal">Since all operations <a id="_idIndexMarker1530"/>inside and outside the cluster must go through the <a id="_idIndexMarker1531"/>Kubernetes API server, this means that all of them must go through the authentication process. This includes the operations of internal cluster components and Pods, which query the API server. For you, as an external user of the cluster, any requests that you make using <code class="inlineCode">kubectl</code> commands or directly to the Kubernetes API server will also go through the authentication process:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Normal users</strong>: Such users are managed <em class="italic">externally</em>, independent from the Kubernetes cluster. Currently, Kubernetes does not provide any objects to represent such users. The external management of users may be as simple (but <em class="italic">not</em> recommended) as static user-password files passed to the API server using the <code class="inlineCode">token-auth-file</code> argument in the static Pod definition file <code class="inlineCode">/etc/kubernetes/manifests/kube-apiserver.yaml</code> inside your control plane <a id="_idIndexMarker1532"/>nodes (AKA master nodes) during <a id="_idIndexMarker1533"/>startup. For production environments, leverage <a id="_idIndexMarker1534"/>existing <strong class="keyWord">identity providers </strong>(<strong class="keyWord">IdPs</strong>) like <strong class="keyWord">Google</strong>, <strong class="keyWord">GitHub</strong>, <strong class="keyWord">Azure Active Directory</strong> (<strong class="keyWord">AAD</strong>), or <strong class="keyWord">AWS IAM</strong> to manage users. Integrate<a id="_idIndexMarker1535"/> your Kubernetes cluster with these IdPs using <strong class="keyWord">OpenID Connect</strong> <strong class="keyWord">(OIDC</strong> – <a href="https://openid.net/connect/"><span class="url">https://openid.net/connect/</span></a>) tokens for a seamless authentication<a id="_idIndexMarker1536"/> experience. Remember, regular user accounts in Kubernetes are global and don’t have namespace restrictions.</li>
      <li class="bulletList"><strong class="keyWord">Service accounts</strong>: These are managed by the Kubernetes cluster and modeled as ServiceAccount objects. You can create and manage service accounts just like any other resource in Kubernetes, for example, using <code class="inlineCode">kubectl</code> and YAML manifest files. This type of account is intended for processes in cluster components or running in Pods. The credentials for ServiceAccounts will be created as Secrets (manually or via TokenRequest API) that are mounted into Pods so that the container process can use them to talk to the Kubernetes API server. When a process authenticates using a <code class="inlineCode">ServiceAccount</code> token, it is seen as a user called <code class="inlineCode">system:serviceaccount:&lt;namespace&gt;:&lt;serviceAccountName&gt;</code>. Note that ServiceAccounts are namespaced.</li>
    </ul>
    <p class="normal">As you can see, user management in Kubernetes is a mixture of different approaches that should fit all the needs of different organizations. The key takeaway here is that after the authentication process, the request will be either rejected (optionally treated as anonymous) or treated as coming from a particular user. The <code class="inlineCode">username</code> attribute may be provided by the external user management system, as in the case of normal users, or it will be <code class="inlineCode">system:serviceaccount:&lt;namespace&gt;:&lt;serviceAccountName&gt;</code> for ServiceAccounts. Additionally, the request will have more attributes associated with it, such as <strong class="keyWord">User ID</strong> (<strong class="keyWord">UID</strong>), <strong class="keyWord">groups</strong>, and <strong class="keyWord">extra fields</strong>. This information is used for authorization processes based on RBAC, which we will explain in the next sections.</p>
    <p class="normal">Now, let’s look at the authentication methods that you can use with Kubernetes.</p>
    <h3 class="heading-3" id="_idParaDest-597">Authentication Methods in Kubernetes</h3>
    <p class="normal">The various authentications, in general, help<a id="_idIndexMarker1537"/> in securely controlling access in Kubernetes to the API server. To validate users and services, a variety of authentication strategies can be enabled. Each is suited to different use cases and levels of security. These include tokens and certificates that verify the identities of both human users and applications interacting with the cluster. The good thing about the Kubernetes API server is that it provides support for multiple authentication mechanisms, so clusters can be configured using a combination of the previously-mentioned methods. In the following section, we will present some common authentication methods such as Static Token files, ServiceAccount tokens, X.509 client certificates, and OpenID Connect tokens.</p>
    <h4 class="heading-4">Static token files</h4>
    <p class="normal">This <a id="_idIndexMarker1538"/>method is the most basic one <a id="_idIndexMarker1539"/>that Kubernetes offers for managing normal users. The approach somewhat resembles the <code class="inlineCode">/etc/shadow</code> and <code class="inlineCode">/etc/passwd</code> files in Unix/Linux systems. Note, however, that it is <em class="italic">not</em> recommended and is considered <em class="italic">unsecure</em> for production clusters.</p>
    <p class="normal">In this method, you define a <code class="inlineCode">.csv</code> file where each line has the following format:</p>
    <pre class="programlisting con"><code class="hljs-con">token,user,uid,"group1,group2,group3"
</code></pre>
    <p class="normal">Then, you <a id="_idIndexMarker1540"/>pass the file when starting the Kubernetes API server <a id="_idIndexMarker1541"/>process using the <code class="inlineCode">token-auth-file</code> parameter in the static Pod definition file <code class="inlineCode">/etc/kubernetes/manifests/kube-apiserver.yaml</code> inside your control plane nodes (AKA master nodes):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># /etc/kubernetes/manifests/kube-apiserver.yaml</span>
<span class="hljs-string">...&lt;removed</span> <span class="hljs-string">for</span> <span class="hljs-string">brevity&gt;...</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">command:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">kube-apiserver</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">--advertise-address=192.168.59.154</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">--allow-privileged=true</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">--authorization-mode=Node,RBAC</span>
    <span class="code-highlight"><strong class="hljs-bullet-slc">-</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">--token-auth-file=/etc/kubernetes/user-tokens.csv</strong></span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">--client-ca-file=/var/lib/minikube/certs/ca.crt</span>
<span class="hljs-string">...&lt;removed</span> <span class="hljs-string">for</span> <span class="hljs-string">brevity&gt;...</span>   
</code></pre>
    <p class="normal">To authenticate against the API server, you need to use a standard HTTP <strong class="keyWord">bearer authentication scheme</strong> for<a id="_idIndexMarker1542"/> your requests. This means that your requests will need to use an additional header that’s in the following form:</p>
    <pre class="programlisting con"><code class="hljs-con">Authorization: Bearer &lt;token&gt;
</code></pre>
    <p class="normal">Based on this request information, the Kubernetes API server will match the token against the static token file and assign user attributes based on the matched record.</p>
    <p class="normal">When using <code class="inlineCode">kubectl</code>, you must modify your <code class="inlineCode">kubeconfig</code>. You can do this using the <code class="inlineCode">kubectl</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config set-credentials &lt;contextUser&gt; --token=&lt;token&gt;
</code></pre>
    <p class="normal">After that, you need to create and use context with this user for your requests using the <code class="inlineCode">kubectl config use-context</code> command.</p>
    <div class="note">
      <p class="normal">In Kubernetes versions prior to 1.19, there was a similar authentication method that allowed us to use<a id="_idIndexMarker1543"/> an HTTP <strong class="keyWord">basic authentication scheme</strong> and a file passed by the <code class="inlineCode">basic-auth-file</code> parameter to the API server. This method is no longer supported due to security reasons.</p>
    </div>
    <p class="normal">The following <a id="_idIndexMarker1544"/>diagram visualizes the principles behind this <a id="_idIndexMarker1545"/>method of authentication:</p>
    <figure class="mediaobject"><img alt="Figure 18.1 – Static token file authentication in Kubernetes&#10;" src="image/B22019_18_02.png"/></figure>
    <p class="packt_figref">Figure 18.2: Static token file authentication in Kubernetes</p>
    <p class="normal">We can now summarize the advantages and disadvantages of using the static token file method for authentication.</p>
    <p class="normal">The advantages of the static token file method<a id="_idIndexMarker1546"/> are as follows:</p>
    <ul>
      <li class="bulletList">It is easy to configure.</li>
      <li class="bulletList">It is easy to understand.</li>
    </ul>
    <p class="normal">The disadvantages<a id="_idIndexMarker1547"/> of the static token file method are as follows:</p>
    <ul>
      <li class="bulletList">It is unsecure; exposing a token file compromises all cluster users.</li>
      <li class="bulletList">It requires that we manually manage users.</li>
      <li class="bulletList">Adding new users or removing existing ones requires that we restart the Kubernetes API server.</li>
      <li class="bulletList">Rotating any tokens requires that we restart the Kubernetes API server.</li>
      <li class="bulletList">It takes extra effort to replicate the Token file content to every control plane node when you have a high availability control plane with multiple control plane nodes.</li>
    </ul>
    <p class="normal">In short, this method is good for development environments and learning the principles behind authentication in Kubernetes, but it is not recommended for production use cases. Next, we will take a look at authenticating users using ServiceAccount tokens.</p>
    <h4 class="heading-4">ServiceAccount tokens</h4>
    <p class="normal">As we <a id="_idIndexMarker1548"/>mentioned in the introduction to this section, ServiceAccounts are meant for in-cluster identities for processes running in Pod containers or for cluster components. However, they can be used for authenticating external requests as well.</p>
    <p class="normal">ServiceAccounts are <a id="_idIndexMarker1549"/>Kubernetes objects and can be managed like any other resource in the cluster; that is, by using <code class="inlineCode">kubectl</code> or raw HTTP requests to the API server. The tokens for ServiceAccounts<a id="_idIndexMarker1550"/> are <strong class="keyWord">JSON Web Tokens</strong> (<strong class="keyWord">JWTs</strong>) and will be generated on-demand or using the <code class="inlineCode">kubectl create token</code> command.</p>
    <div class="note">
      <p class="normal">Every Kubernetes namespace has a pre-created ServiceAccount named <code class="inlineCode">default</code>. Pods without a specified ServiceAccount automatically inherit this default account for authorization within the cluster. You can verify a Pod’s ServiceAccount using <code class="inlineCode">kubectl get pods/&lt;podname&gt; -o yaml</code> and checking the <code class="inlineCode">spec.serviceAccountName</code> field.</p>
    </div>
    <p class="normal">Usually, when defining a Pod, you can specify what ServiceAccount should be used for processes running in the containers. You can do this using <code class="inlineCode">.spec.serviceAccountName</code> in the Pod specification. The JWT token will be injected into the container; then, the process inside can use it in the HTTP bearer authentication scheme to authenticate to the Kubernetes API server. This is only necessary if it interacts with the API server in any way, for example, if it needs to discover other Pods in the cluster. We have summarized this authentication method in the following diagram:</p>
    <figure class="mediaobject"><img alt="Figure 18.2 – ServiceAccount authentication in Kubernetes&#10;" src="image/B22019_18_03.png"/></figure>
    <p class="packt_figref">Figure 18.3: ServiceAccount authentication in Kubernetes</p>
    <p class="normal">This also shows why <a id="_idIndexMarker1551"/>ServiceAccount tokens can be used for external requests – the API server does not care about the origin of the request; all it is interested in is the bearer token that comes with the request header. Again, you can use this token in <code class="inlineCode">kubectl</code> or in raw HTTP requests to the API server. Please note that this is generally not a recommended way to use ServiceAccounts, but it can be used in some scenarios, especially when you are unable to use an external authentication provider for normal users.</p>
    <p class="normal">Prior to version 1.22, Kubernetes<a id="_idIndexMarker1552"/> automatically generated API credentials for ServiceAccounts using Secrets. These Secrets contained tokens that Pods could mount for access. This approach had limitations:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Static Tokens</strong>: Secrets stored tokens in plain text, posing a security risk if compromised.</li>
      <li class="bulletList"><strong class="keyWord">Limited Control</strong>: Token lifespans and permissions were not easily managed.</li>
    </ul>
    <p class="normal">Starting from<a id="_idIndexMarker1553"/> version 1.22, Kubernetes switched to a more secure approach. Pods now obtain<a id="_idIndexMarker1554"/> tokens directly using the <strong class="keyWord">TokenRequest</strong> API. These tokens are as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Short-lived</strong>: Tokens have limited lifespans, reducing the impact of potential compromise.</li>
      <li class="bulletList"><strong class="keyWord">Mounted into Pods</strong>: Tokens are automatically mounted as volumes, eliminating the need for pre-stored Secrets.</li>
    </ul>
    <p class="normal">While automatic mounting is preferred, you can still manually create Secrets for service account tokens. This might be useful for tokens requiring longer lifespans, but it’s important to prioritize automatic token mounting for enhanced security in most scenarios.</p>
    <div class="note">
      <p class="normal">As we learned, Kubernetes automatically mounts Service Account API credentials within Pods for streamlined access. To disable this behavior and manage tokens differently, set <code class="inlineCode">automountServiceAccountToken: false</code> either in the ServiceAccount manifest or within the Pod specification. This setting applies to all Pods referencing the ServiceAccount unless overridden by the specific Pod configuration. If both are defined, the Pod’s setting takes precedence. Refer to the documentation for more details (<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#opt-out-of-api-credential-automounting"><span class="url">https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#opt-out-of-api-credential-automounting</span></a>).</p>
    </div>
    <p class="normal">We will now demonstrate how you can create and manage ServiceAccounts and how you can use JWT tokens to authenticate when using <code class="inlineCode">kubectl</code>. This will also give a sneak peek into RBAC, which we are going to look at in more detail in the next section. Please follow these steps:</p>
    <ol>
      <li class="numberedList" value="1">Create a YAML manifest for a new Namespace and a ServiceAccount as follows. We will configure RBAC for this account so that it can only read Pods in that namespace:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># 01_serviceaccount/example-sa-ns.yaml</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">example-ns</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">example-sa</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">example-ns</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Note that you can also use the <em class="italic">imperative </em>command <code class="inlineCode">kubectl create serviceaccount example-sa</code> to create the resources.</p>
    <ol>
      <li class="numberedList" value="2">Create a <a id="_idIndexMarker1555"/>YAML manifest for a <code class="inlineCode">Role</code> object named <code class="inlineCode">pod-reader</code> in <a id="_idIndexMarker1556"/>the <code class="inlineCode">example-ns</code> namespace. This role will allow you to get, watch, and list Pods in this namespace. The <code class="inlineCode">01_serviceaccount/pod-reader-role.yaml</code> YAML manifest file has the following contents:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># 01_serviceaccount/pod-reader-role.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">example-ns</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-reader</span>
<span class="hljs-attr">rules:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">""</span>]
    <span class="hljs-attr">resources:</span> [<span class="hljs-string">"pods"</span>]
    <span class="hljs-attr">verbs:</span> [<span class="hljs-string">"get"</span>, <span class="hljs-string">"watch"</span>, <span class="hljs-string">"list"</span>]
</code></pre>
      </li>
      <li class="numberedList">Create a YAML manifest for <code class="inlineCode">RoleBinding</code> named <code class="inlineCode">reads-pods</code>. This is what <em class="italic">associates</em> the role that we created with our <code class="inlineCode">example-sa</code> ServiceAccount – the account will now have the privilege of read-only access to Pods, and nothing more. The <code class="inlineCode">01_serviceaccount/read-pods-rolebinding.yaml</code> YAML manifest file has the following contents:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># 01_serviceaccount/read-pods-rolebinding.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">RoleBinding</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">read-pods</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">example-ns</span>
<span class="hljs-attr">subjects:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">example-sa</span>
    <span class="hljs-attr">namespace:</span> <span class="hljs-string">example-ns</span>
<span class="hljs-attr">roleRef:</span>
  <span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-reader</span>
  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>
</code></pre>
      </li>
      <li class="numberedList">Now, we can apply all the manifest files to the cluster at once using the <code class="inlineCode">kubectl apply</code> command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f 01_serviceaccount/
namespace/example-ns created
serviceaccount/example-sa created
role.rbac.authorization.k8s.io/pod-reader created
rolebinding.rbac.authorization.k8s.io/read-pods created
</code></pre>
      </li>
      <li class="numberedList">Now, we <a id="_idIndexMarker1557"/>will create a Token for the ServiceAccount as <a id="_idIndexMarker1558"/>follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create token example-sa -n example-ns
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Collect the JWT token from the command output, which you can use to authenticate as that ServiceAccount. If you are interested, you can inspect the contents of the JWT<a id="_idIndexMarker1559"/> using <a href="https://jwt.io/"><span class="url">https://jwt.io/</span></a> as shown in the following figure:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_18_04.png"/></figure>
    <p class="packt_figref">Figure 18.4: Inspecting a JWT for ServiceAccount</p>
    <p class="normal-one">As you can<a id="_idIndexMarker1560"/> see, the JWT maps to the <code class="inlineCode">example-sa</code> ServiceAccount in <a id="_idIndexMarker1561"/>the <code class="inlineCode">example-ns</code> namespace. Additionally, you can identify that the actual username (marked as a <code class="inlineCode">subject</code> in the payload) that will be mapped to in Kubernetes is <code class="inlineCode">system:serviceaccount:example-ns:example-sa</code>, as we explained previously.</p>
    <ol>
      <li class="numberedList" value="6">With this token, we can set up kubeconfig to test it. First, you need to create a user in your <code class="inlineCode">kubeconfig</code> using the following command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config set-credentials example-sa --token=&lt;your-token&gt;
User "example-sa" set.
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Where the <code class="inlineCode">example-sa</code> is the new ServiceAccount you have created and also replace <code class="inlineCode">&lt;your-token&gt;</code> with the token string you collected earlier.</p>
    <ol>
      <li class="numberedList" value="7">Create a<a id="_idIndexMarker1562"/> new context that uses this user in the <code class="inlineCode">kubeconfig</code>. You also need to know the cluster name that you are connecting to right now – you can check it using the <code class="inlineCode">kubectl config view</code> command. Use <a id="_idIndexMarker1563"/>the <code class="inlineCode">kubectl config set-context</code> command to create a new context:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config set-context &lt;new-context-name&gt; --user=&lt;new-user-created --cluster=&lt;clusterName&gt;
</code></pre>
      </li>
    </ol>
    <p class="normal-one">For example, use the following command to create a new context named <code class="inlineCode">example-sa-context</code> with minikube as the target cluster and <code class="inlineCode">example-sa</code> as the user:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config set-context example-sa-context --user=example-sa --cluster=minikube
Context "example-sa-context" created.
</code></pre>
    <ol>
      <li class="numberedList" value="8">Before we switch to the newly created context, let us create a simple nginx Pod in the <code class="inlineCode">example-ns</code> namespace. Copy the sample YAML <code class="inlineCode">Chapter18/references/sa-demo-nginx-pod.yaml</code> to <code class="inlineCode">Chapter18/01_serviceaccount/nginx-pod.yaml</code> and apply the configuration:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cp</span> references/sa-demo-nginx-pod.yaml 01_serviceaccount/nginx-pod.yaml
<span class="hljs-con-meta">$ </span>kubectl apply -f 01_serviceaccount/nginx-pod.yaml
pod/nginx-pod created
<span class="hljs-con-meta">$ </span> kubectl get po -n example-ns
NAME        READY   STATUS    RESTARTS   AGE
nginx-pod   1/1     Running   0          12m
</code></pre>
      </li>
      <li class="numberedList">Also, before you switch to the new context, you may want to check the name of the context that you are currently using by utilizing the <code class="inlineCode">kubectl config current-context</code> command. This will make it easier to go back to your old cluster admin context:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config current-context
minikube
</code></pre>
      </li>
      <li class="numberedList">Now, switch to the new context using the following command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config use-context example-sa-context
Switched to context "example-sa-context".
</code></pre>
      </li>
      <li class="numberedList">You can also <a id="_idIndexMarker1564"/>verify the identity of the credential you are currently using as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl auth <span class="hljs-con-built_in">whoami</span>
ATTRIBUTE                                           VALUE
Username                                            system:serviceaccount:example-ns:example-sa
UID                                                 ebc5554b-306f-48fe-b9d7-3e5777fabf06
Groups                                              [system:serviceaccounts system:serviceaccounts:example-ns system:authenticated]
Extra: authentication.kubernetes.io/credential-id   [JTI=45dc861c-1024-4857-a694-00a5d2eeba5f]
</code></pre>
      </li>
      <li class="numberedList">We are now<a id="_idIndexMarker1565"/> ready to verify that our authentication works and that the RBAC roles allow read-only access to Pods in the <code class="inlineCode">example-ns</code> namespace. First, try getting Pods:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get po -n example-ns
NAME        READY   STATUS    RESTARTS   AGE
nginx-pod   1/1     Running   0          18m
</code></pre>
      </li>
      <li class="numberedList">This worked as expected! Now, try getting Pods from the <code class="inlineCode">kube-system</code> namespace:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods -n kube-system
Error from server (Forbidden): pods is forbidden: User "system:serviceaccount:example-ns:example-sa" cannot list resource "pods" in API group "" in the namespace "kube-system"
</code></pre>
      </li>
      <li class="numberedList">We have authenticated correctly, but the action was forbidden by RBAC authorization, which is what we expected. Lastly, let’s try getting Service objects:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get svc -n example-ns
Error from server (Forbidden): services is forbidden: User "system:serviceaccount:example-ns:example-sa" cannot list resource "services" in API group "" in the namespace "example-ns"
</code></pre>
      </li>
    </ol>
    <p class="normal">This is also <a id="_idIndexMarker1566"/>expected as the RBAC is not configured for the ServiceAccount to view or <a id="_idIndexMarker1567"/>list the Service resources in the <code class="inlineCode">example-ns</code> namespace.</p>
    <p class="normal">As you can see, we have successfully used our ServiceAccount token for authentication and we have verified that our privileges work correctly. You can now switch back to your old <code class="inlineCode">kubectl</code> context using the <code class="inlineCode">kubectl config use-context &lt;context-name&gt;</code> command.</p>
    <div class="packt_tip">
      <p class="normal">The preceding procedure of configuring the <code class="inlineCode">kubectl</code> context with a bearer token can be used for the static token file authentication method as well.</p>
    </div>
    <p class="normal">Let’s summarize what the advantages and disadvantages of using ServiceAccount tokens for authentication are.</p>
    <p class="normal">The advantages<a id="_idIndexMarker1568"/> of using ServiceAccount tokens are as follows:</p>
    <ul>
      <li class="bulletList">Easy to configure and use, similar to static token files.</li>
      <li class="bulletList">Entirely managed by the Kubernetes cluster, so there’s no need for external authentication providers.</li>
      <li class="bulletList">ServiceAccounts are namespaced.</li>
    </ul>
    <p class="normal">The disadvantages<a id="_idIndexMarker1569"/> of using ServiceAccount tokens are as follows:</p>
    <ul>
      <li class="bulletList">ServiceAccounts are intended for processes running in Pod containers to give them identity and let them use Kubernetes RBAC. <em class="italic">It is not a best practice for a user to use the ServiceAccount token</em>.</li>
    </ul>
    <p class="normal">In general, using ServiceAccount tokens for external authentication is only good for development and test scenarios when you cannot integrate with external authentication providers. However, for production clusters, it is not the best option, mainly due to security concerns. Now, let’s take a look at using X.509 client certificates for Kubernetes API authentication.</p>
    <h4 class="heading-4">X.509 client certificates</h4>
    <p class="normal">Using X.509 client certificates<a id="_idIndexMarker1570"/> is one of the industry standards for <a id="_idIndexMarker1571"/>authentication processes. There is one important catch, however – you need to have good means of managing certificate signing, revoking, and rotation. Otherwise, you may hit very similar security issues as with using ServiceAccount tokens. You can learn more about X.509 certificates and the processes <a id="_idIndexMarker1572"/>around them at <a href="https://www.ssl.com/faqs/what-is-an-x-509-certificate/"><span class="url">https://www.ssl.com/faqs/what-is-an-x-509-certificate/</span></a>.</p>
    <p class="normal">This method works in Kubernetes as follows:</p>
    <ol>
      <li class="numberedList" value="1">The Kubernetes API server starts with the <code class="inlineCode">client-ca-file</code> argument. This provides <strong class="keyWord">certificate authority</strong> (<strong class="keyWord">CA</strong>) information to <a id="_idIndexMarker1573"/>be used to validate client certificates presented to the API server. You can configure a custom CA certificate here or use the default CA created as part of the cluster deployment. For example, if you are using minikube, you can see a default CA file already configured in <code class="inlineCode">kube-apiserver</code> as follows:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># /etc/kubernetes/manifests/kube-apiserver.yaml</span>
<span class="hljs-string">...&lt;removed</span> <span class="hljs-string">for</span> <span class="hljs-string">brevity&gt;...</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">command:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">kube-apiserver</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">--advertise-address=192.168.59.154</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">--allow-privileged=true</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">--authorization-mode=Node,RBAC</span>
    <span class="code-highlight"><strong class="hljs-bullet-slc">-</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">--client-ca-file=/var/lib/minikube/certs/ca.crt</strong></span>
<span class="hljs-string">...&lt;removed</span> <span class="hljs-string">for</span> <span class="hljs-string">brevity&gt;...</span>
</code></pre>
      </li>
      <li class="numberedList">Users who want to authenticate against the API server need to request an X.509 client certificate from the CA. This should be a secure and audited process. The subject common name (the <code class="inlineCode">CN</code> attribute in the subject) of the certificate is used as the <code class="inlineCode">username</code> attribute when authentication is successful. Note that as of Kubernetes 1.19, you can use the Certificates API to manage signing requests. More information is available in the official documentation: <a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/"><span class="url">https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/</span></a>.</li>
      <li class="numberedList">The user must present the client certificate during authentication to the API server, which validates the certificate against the CA. Based on that, the request goes through the authentication process successfully or is rejected. Again, if you are using a minikube cluster, you are already utilizing the certificate-based authentication, as shown in the following example:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config view -o json | jq <span class="hljs-con-string">'.users[]'</span>
{
  "name": "example-sa",
  "user": {
    "token": "REDACTED"
  }
}
{
  "name": "minikube",
  "user": {
    "client-certificate": "/home/iamgini/.minikube/profiles/minikube/client.crt",
    "client-key": "/home/iamgini/.minikube/profiles/minikube/client.key"
  }
}
</code></pre>
      </li>
    </ol>
    <p class="normal">While using the <code class="inlineCode">kubectl</code> commands, users<a id="_idIndexMarker1574"/> can configure this method of authentication in kubeconfig using the <code class="inlineCode">kubectl config set-credentials</code> command, as we <a id="_idIndexMarker1575"/>learned earlier. We have summarized this process in the following diagram:</p>
    <figure class="mediaobject"><img alt="Figure 18.4 – X.509 client certificate authentication in Kubernetes&#10;" src="image/B22019_18_05.png"/></figure>
    <p class="packt_figref">Figure 18.5: X.509 client certificate authentication in Kubernetes</p>
    <p class="normal">Please note that this visualizes the case when initial CSR by the user is handled by the Certificate API in a Kubernetes cluster. This does not need to be the case as CA may be external to the cluster, and the Kubernetes API server can rely on a copy of the CA <code class="inlineCode">.pem</code> file.</p>
    <p class="normal">In the following<a id="_idIndexMarker1576"/> hands-on exercise, we will generate and configure certificate-based authentication<a id="_idIndexMarker1577"/> in Kubernetes:</p>
    <ol>
      <li class="numberedList" value="1">Start with creating a private key using the <code class="inlineCode">openssl</code> command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>openssl genrsa -out iamgini.key 2048
</code></pre>
      </li>
      <li class="numberedList">Generate<a id="_idIndexMarker1578"/> a <strong class="keyWord">CertificateSigningRequest</strong> (<strong class="keyWord">CSR</strong>):
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>openssl req -new -key iamgini.key -out iamgini.csr -subj <span class="hljs-con-string">"/CN=iamgini/O=web1/O=frontend"</span>
</code></pre>
      </li>
      <li class="numberedList">Gather the CSR data and encode it using base64:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cat</span> iamgini.csr | <span class="hljs-con-built_in">base64</span> -w 0
</code></pre>
      </li>
      <li class="numberedList">Now, we need to create a <code class="inlineCode">CertificateSigningRequest</code> resource with <strong class="keyWord">Certificates API</strong>; let us use the<a id="_idIndexMarker1579"/> <code class="inlineCode">csr.yaml</code> file as follows:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># csr.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">certificates.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">CertificateSigningRequest</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">iamgini</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">request:</span> <span class="hljs-string">&lt;</span><span class="code-highlight"><strong class="hljs-string-slc">your</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">encoded</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">CSR</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">content</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">here</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">from</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">Step.3&gt;</strong></span>
  <span class="hljs-attr">signerName:</span> <span class="hljs-string">kubernetes.io/kube-apiserver-client</span>
  <span class="hljs-attr">usages:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">client</span> <span class="hljs-string">auth</span>
</code></pre>
      </li>
      <li class="numberedList">Create the <code class="inlineCode">CertificateSigningRequest</code>:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f csr.yaml
certificatesigningrequest.certificates.k8s.io/iamgini created
</code></pre>
      </li>
      <li class="numberedList">Now the administrators (or the users with the <code class="inlineCode">certificatesigningrequests</code> privilege) can see the CSR resources:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get csr
NAME      AGE   SIGNERNAME                            REQUESTOR       REQUESTEDDURATION   CONDITION
iamgini   25s   kubernetes.io/kube-apiserver-client   minikube-user   &lt;none&gt;              Pending
</code></pre>
      </li>
      <li class="numberedList">Check and approve the CSR as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl certificate approve iamgini
certificatesigningrequest.certificates.k8s.io/iamgini approved
</code></pre>
      </li>
      <li class="numberedList">Once the <a id="_idIndexMarker1580"/>CSR is approved, gather the certificate data<a id="_idIndexMarker1581"/> from the approved CSR resource as follows; the following command will extract the data to <code class="inlineCode">iamgini.crt</code> file:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get csr iamgini -o json | jq -r <span class="hljs-con-string">'.status.certificate'</span> | <span class="hljs-con-built_in">base64</span> --decode &gt; iamgini.crt
</code></pre>
      </li>
      <li class="numberedList">Now, we have the private key and certificate as follows (you can delete the <code class="inlineCode">.csr</code> file as it is not required anymore):
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">ls</span> iamgini.*
iamgini.crt  iamgini.csr  iamgini.key
</code></pre>
      </li>
      <li class="numberedList">Now, we will configure the <code class="inlineCode">kubeconfig</code> with our new user and context; create a new user entry in the <code class="inlineCode">kubeconfig</code> as follows (remember to use the full path of the key and certificate file):
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config set-credentials iamgini --client-key=/full-path/iamgini.key --client-certificate=/full-path/iamgini.crt
User "iamgini" set.
</code></pre>
      </li>
      <li class="numberedList">Create a new context with the new user:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config set-context iamgini --cluster=minikube --user=iamgini
Context "iamgini" created.
</code></pre>
      </li>
      <li class="numberedList">Now, the kubeconfig is updated with the new user and context. Let us test the access. Change the kubeconfig context as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config use-context iamgini
Switched to context "iamgini".
</code></pre>
      </li>
      <li class="numberedList">Verify the context and connection:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl auth <span class="hljs-con-built_in">whoami</span>
ATTRIBUTE   VALUE
Username    iamgini
Groups      [web1 frontend system:authenticated]
</code></pre>
      </li>
    </ol>
    <p class="normal">Congratulations; you have <a id="_idIndexMarker1582"/>configured a new user with X509 certificate-based <a id="_idIndexMarker1583"/>authentication. But remember, the user will not be able to do any kind of operation until you configure the appropriate RBAC resources.</p>
    <p class="normal">Based on what we have learned, we can<a id="_idIndexMarker1584"/> summarize the advantages of this method as follows:</p>
    <ul>
      <li class="bulletList">It’s a much more secure process than using ServiceAccount tokens or static token files.</li>
      <li class="bulletList">Being unable to store certificates in the cluster means that it is not possible to compromise all certificates. X.509 client certificates can be used for high-privileged user accounts.</li>
      <li class="bulletList">X.509 client certificates can be revoked on demand. This is very important in case of security incidents.</li>
    </ul>
    <p class="normal">The disadvantages <a id="_idIndexMarker1585"/>of X.509 client certificate authentication are as follows:</p>
    <ul>
      <li class="bulletList">Certificates have an expiry date, which means they cannot be valid indefinitely. For simple use cases in development, this is a disadvantage. From a security perspective, in production clusters, this is a huge <em class="italic">advantage. But remember to ensure the certificate is stored safely as the file-based authentication mechanism is a security risk; the file could be stolen and used for unauthorized access</em>.</li>
      <li class="bulletList">Monitoring certificate expiration, revocation, and rotation must be handled. This should be an automated process so that we can quickly react in the case of security incidents.</li>
      <li class="bulletList">Using client <a id="_idIndexMarker1586"/>certificates in the browser for authentication is troublesome, for example, when you would like to authenticate to Kubernetes Dashboard.</li>
    </ul>
    <p class="normal">The key takeaway is that using X.509 client certificates is secure but requires sophisticated certificate management so that we have all the benefits. Now, we will take a look at OpenID Connect tokens, which is the recommended method for cloud environments.</p>
    <h4 class="heading-4">OpenID Connect tokens</h4>
    <p class="normal">Using <strong class="keyWord">OpenID Connect</strong> (<strong class="keyWord">OIDC</strong>), you<a id="_idIndexMarker1587"/> can achieve a <strong class="keyWord">single sign-on</strong> (<strong class="keyWord">SSO</strong>) experience<a id="_idIndexMarker1588"/> for your Kubernetes cluster (and possibly other resources in your organization). OIDC is an authentication layer that’s created on top of OAuth 2.0, which <a id="_idIndexMarker1589"/>allows third-party applications to verify the<a id="_idIndexMarker1590"/> identity of the end-user and obtain basic user profile information. OIDC uses JWTs, which you can obtain using flows that conform to the OAuth 2.0 specifications. The most significant issue with using OIDC for authenticating in Kubernetes is the limited availability of OpenID providers. But if you are deploying in a cloud environment, all tier 1 cloud service providers such as Microsoft Azure, Amazon Web Services, and Google Cloud Platform have their versions of OpenID providers. The beauty of <em class="italic">managed</em> Kubernetes cluster deployments in the cloud, such as AKS Amazon EKS, and Google Kubernetes Engine, is that they provide <em class="italic">integration</em> with their native OpenID provider out of the box or by a simple flip of a configuration switch. In other words, you do not need to worry about reconfiguring the Kubernetes API server and making it work with your chosen OpenID provider – you get it alongside the managed solution. If you are interested in learning more about the OIDC protocol, you can refer to the official web <a id="_idIndexMarker1591"/>page at <a href="https://openid.net"><span class="url">https://openid.net</span></a>.</p>
    <p class="normal">For more details and more specific flows, such as in the context of AAD please take a look at <a href="https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-protocols-oidc"><span class="url">https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-protocols-oidc</span></a>.</p>
    <p class="normal">In the following diagram, you can see the basics of the OIDC authentication flow on Kubernetes:</p>
    <figure class="mediaobject"><img alt="Figure 18.5 – OpenID Connect authentication in Kubernetes&#10;" src="image/B22019_18_06.png"/></figure>
    <p class="packt_figref">Figure 18.6: OIDC authentication in Kubernetes</p>
    <p class="normal">The most important thing is that the OpenID provider is responsible for the SSO experience and managing the bearer tokens. Additionally, the Kubernetes API server must validate the bearer token that’s received against the OpenID provider.</p>
    <p class="normal">Using OIDC has the <a id="_idIndexMarker1592"/>following advantages:</p>
    <ul>
      <li class="bulletList">You get SSO experience, which you can use with other services in your organization.</li>
      <li class="bulletList">Most of the cloud service providers have their own OpenID providers that easily integrate with their managed Kubernetes offerings.</li>
      <li class="bulletList">It can be also used with other OpenID providers and non-cloud deployments – this requires a bit more configuration though.</li>
      <li class="bulletList">It’s a secure and scalable solution.</li>
    </ul>
    <p class="normal">The disadvantages of the <a id="_idIndexMarker1593"/>OIDC approach can be summarized as follows:</p>
    <ul>
      <li class="bulletList">Kubernetes has no web interface where you can trigger the authentication process. This means that you need to get the credentials by manually requesting them from the IdP. In managed cloud Kubernetes offerings, this is often solved by additional simple tooling to generate <code class="inlineCode">kubeconfig</code> with credentials.</li>
      <li class="bulletList">OIDC tokens can be revoked by the IdP if it supports the token endpoint revocation feature. This allows you to invalidate tokens before their expiration time, for example, if a user’s account is compromised. However, not all IdPs support this feature, and Kubernetes doesn’t handle <a id="_idIndexMarker1594"/>token revocation itself.</li>
    </ul>
    <h5 class="heading-5">Using OIDC in Kubernetes</h5>
    <p class="normal">Kubernetes does not provide an<a id="_idIndexMarker1595"/> integrated OpenID Connect Identity Provider. Thus, it relies on the external ones provided either by cloud providers or stand-alone tools. As we mentioned earlier in this section, the most popular cloud environments – like AWS, GCP, and Azure – natively provide OIDC integration in their managed Kubernetes offerings, which makes it pretty straightforward to enable SSO. Alternatively, the identity providers can also be set up independently for every organization using tools such as Dex, Keycloak, UAA, or OpenUnison for the non-cloud or self-managed Kubernetes clusters.</p>
    <h5 class="heading-5">Identity Provider Requirements in Kubernetes</h5>
    <p class="normal">For an OIDC identity provider<a id="_idIndexMarker1596"/> to work with Kubernetes, it has to satisfy a number of important prerequisites:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Support OIDC Discovery</strong>: OIDC discovery simplifies configuration efforts as through it all information about IdP endpoints and public keys are made available. Kubernetes reads the IdP’s public keys from the discovery endpoint to validate OIDC tokens.</li>
      <li class="bulletList"><strong class="keyWord">Transport Layer Security</strong> (<strong class="keyWord">TLS</strong>) <strong class="keyWord">Compliance</strong>: The identity provider shall handle TLS to handle non-obsolete ciphers as sensitive authentication data handling is at stake.</li>
      <li class="bulletList"><strong class="keyWord">CA-Signed Certificate</strong>: Whether by using a commercial CA or a self-signed certificate, the certificate of the identity provider must have the <code class="inlineCode">CA</code> flag set to <code class="inlineCode">TRUE</code>. This is because Kubernetes uses Go’s TLS client which strictly enforces this requirement so that Kubernetes can safely trust the identity provider’s certificates during user token verification.</li>
    </ul>
    <div class="note">
      <p class="normal">For the self-deployer of an identity provider without a commercial CA, such tools as the Dex gencert script may be used to create a compliant CA certificate along with the signing key.</p>
    </div>
    <p class="normal">The following list contains some of the popular OIDC Identity Providers for Kubernetes:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Dex</strong>: One of the <a id="_idIndexMarker1597"/>more lightweight, open-source popular IdPs intended for use in a Kubernetes environment. it supports OIDC and works well with the authentication workflow that Kubernetes expects. Dex<a id="_idIndexMarker1598"/> works by hooking into other external IdPs such as LDAP, GitHub, and Google, which would make it a good choice for organizations with more complicated identity scenarios.</li>
      <li class="bulletList"><strong class="keyWord">Keycloak</strong>: This is an <a id="_idIndexMarker1599"/>open-source IdP that offers a more powerful feature set with extensive support for OIDC and SAML. Besides core functionality, Keycloak<a id="_idIndexMarker1600"/> supports enterprise-grade features such as user federation and RBAC. Keycloak would be a good fit if you want to have more control or customization in your authentication setup.</li>
      <li class="bulletList"><strong class="keyWord">OpenUnison</strong>: Another IdP <a id="_idIndexMarker1601"/>that is optimized for Kubernetes is OpenUnison<a id="_idIndexMarker1602"/>, with features like natively integrating the Kubernetes RBAC and identity federation. It should be popular with enterprises that are ready to engage a prebuilt solution optimized to their own needs for securing Kubernetes.</li>
      <li class="bulletList"><strong class="keyWord">Cloud Foundry User Account and Authentication</strong> (<strong class="keyWord">UAA</strong>): This is an open-source multi-purpose IdP originating from<a id="_idIndexMarker1603"/> Cloud Foundry. It supports OIDC and does an extremely strong job with cloud platform and enterprise <a id="_idIndexMarker1604"/>authentication system integrations, making it suitable for more complex Kubernetes deployments in hybrid cloud environments.</li>
    </ul>
    <h5 class="heading-5">Configuring OIDC with Kubernetes API Server</h5>
    <p class="normal">Enabling OIDC in <a id="_idIndexMarker1605"/>Kubernetes will involve <a id="_idIndexMarker1606"/>some configuration of the Kubernetes API server with certain OIDC-related flags. The major configurations include the following:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">oidc-issuer-url</code>: The URL of the OIDC provider. It is used by Kubernetes for verification of token authenticity.</li>
      <li class="bulletList"><code class="inlineCode">oidc-client-id string</code>: The client ID to use when authenticating with the IdP when Kubernetes is the client.</li>
      <li class="bulletList"><code class="inlineCode">oidc-username-claim</code>: Specifies which claim in the token should map to the Kubernetes username.</li>
      <li class="bulletList"><code class="inlineCode">oidc-groups-claim</code>: Maps the groups in the IdP to Kubernetes groups, in order to manage RBAC roles.</li>
    </ul>
    <p class="normal">For further details on <a id="_idIndexMarker1607"/>configuring specific<a id="_idIndexMarker1608"/> OIDC identity providers, you can refer to the official resources such as <a id="_idIndexMarker1609"/>Dex for Kubernetes Guide (<a href="https://dexidp.io/docs/guides/kubernetes/"><span class="url">https://dexidp.io/docs/guides/kubernetes/</span></a>) or OpenID Connect Authentication<a id="_idIndexMarker1610"/> in Kubernetes (<a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/"><span class="url">https://kubernetes.io/docs/reference/access-authn-authz/authentication/</span></a>).</p>
    <p class="normal">The key takeaway about OIDC is that this is your best bet when configuring authentication for Kubernetes, especially if you are deploying production clusters in the cloud.</p>
    <h4 class="heading-4">Other methods</h4>
    <p class="normal">Kubernetes offers a few other authentication methods that you can use. They are mainly intended for advanced use cases, such as integrating with LDAP or Kerberos. The first one is <a id="_idIndexMarker1611"/>an <strong class="keyWord">authenticating proxy</strong>.</p>
    <p class="normal">When you use an authenticating proxy in front of the Kubernetes API server, you can configure the API server to use certain HTTP headers to extract authentication user information from them. In other words, your authenticating proxy is doing the job of authenticating the user and passing down this information alongside the request in the form of additional headers.</p>
    <p class="normal">You can find more<a id="_idIndexMarker1612"/> information in the official documentation (<a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#authenticating-proxy"><span class="url">https://kubernetes.io/docs/reference/access-authn-authz/authentication/#authenticating-proxy</span></a>).</p>
    <p class="normal">Another approach is<a id="_idIndexMarker1613"/> known as <strong class="keyWord">webhook token authentication</strong>, whereby the <a id="_idIndexMarker1614"/>Kubernetes API server uses an external service to verify the bearer tokens. The external service receives the information in the form of a TokenReview object from the API server via an HTTP POST request, performs verification, and sends back a TokenReview object with additional information about the result.</p>
    <p class="normal">Find more information<a id="_idIndexMarker1615"/> from the <a id="_idIndexMarker1616"/>official documentation (<span class="url">https://kuberntes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication</span>).</p>
    <p class="normal">Kubernetes also uses another common authentication method <a id="_idIndexMarker1617"/>called <strong class="keyWord">bootstrap tokens</strong>. But bootstrap tokens <a id="_idIndexMarker1618"/>are not used for general authentication but for the cluster node. Bootstrap tokens are a special type of secret in Kubernetes that simplify adding new nodes to a cluster. Stored in the <code class="inlineCode">kube-system</code> namespace, these short-lived tokens allow the API server to authenticate kubelets (programs running on nodes) during the initial connection. This streamlines the bootstrapping process, making it easier to join new nodes or create new clusters from scratch. They can be used with or without the kubeadm tool and work seamlessly with Kubelet TLS Bootstrapping for secure communication. Refer to <a id="_idIndexMarker1619"/>the documentation (<a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens"><span class="url">https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens</span></a>) to learn about authentication with bootstrap tokens and TLS bootstrapping.</p>
    <p class="normal">In general, you need the authenticating proxy and webhook token authentication methods in special cases where you want to integrate with existing identity providers in your organization that are not supported by Kubernetes out of the box.</p>
    <p class="normal">In the next section, we will look at authorization and RBAC in Kubernetes.</p>
    <h2 class="heading-2" id="_idParaDest-598">Authorization and introduction to RBAC</h2>
    <p class="normal">Security in Kubernetes relies on two crucial processes: <strong class="keyWord">authentication</strong> and <strong class="keyWord">authorization</strong>. Authentication <a id="_idIndexMarker1620"/>verifies the identity of a user attempting to access the <a id="_idIndexMarker1621"/>system, ensuring they are who they claim to be. This initial step typically involves checking credentials like usernames and passwords or tokens.</p>
    <p class="normal">Following successful authentication, authorization comes into play. This process determines what actions a user can perform within the system. In Kubernetes, the API server evaluates a user’s identity (derived from authentication) along with other request attributes, such as the specific API endpoint or action being requested. Based on pre-defined policies or external services, authorization modules decide whether to allow or deny the request.</p>
    <p class="normal">Authentication is <a id="_idIndexMarker1622"/>the first step in determining the identity of the user, whereas<a id="_idIndexMarker1623"/> authorization is the next step when verifying if the user can perform the action they want to.</p>
    <div class="note">
      <p class="normal">Access controls based on specific object fields are handled by admission controllers, which occur after authorization and only if authorization allows the request. We will learn about admission controllers in the later sections of this chapter.</p>
    </div>
    <p class="normal">In the Kubernetes API server, authenticating a request results in a set of additional request attributes such as <strong class="keyWord">user</strong>, <strong class="keyWord">group</strong>, <strong class="keyWord">API request verb</strong>, <strong class="keyWord">HTTP request verb</strong>, and so on. These are then passed further to authorization modules that, based on these attributes, answer whether the user is allowed to do the action or not. If the request is denied by any of the modules, the user will be presented with an HTTP status code of <code class="inlineCode">403 (Forbidden)</code>.</p>
    <div class="packt_tip">
      <p class="normal">This is an important difference between HTTP status codes. If you receive <code class="inlineCode">401 (Unauthorized)</code>, this means that you have been not recognized by the system; for example, you have provided incorrect credentials or the user does not exist. If you receive <code class="inlineCode">403 (Forbidden)</code>, this means that authentication has been successful and you have been recognized, but you are not <em class="italic">allowed</em> to do the action you requested. This is useful when debugging issues regarding access to a Kubernetes cluster.</p>
    </div>
    <p class="normal">Kubernetes has a few authorization modes available that can be enabled by using the <code class="inlineCode">authorization-mode</code> argument when starting the Kubernetes API server, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># /etc/kubernetes/manifests/kube-apiserver.yaml</span>
<span class="hljs-string">...&lt;removed</span> <span class="hljs-string">for</span> <span class="hljs-string">brevity&gt;...</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">command:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">kube-apiserver</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">--advertise-address=192.168.59.154</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">--allow-privileged=true</span>
    <span class="code-highlight"><strong class="hljs-bullet-slc">-</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">--authorization-mode=Node,RBAC</strong></span>
<span class="hljs-string">...&lt;removed</span> <span class="hljs-string">for</span> <span class="hljs-string">brevity&gt;...</span>
</code></pre>
    <p class="normal">The following are the authorization modes available in Kubernetes:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">RBAC</strong>: This allows you to organize access control and management with roles and privileges. RBAC<a id="_idIndexMarker1624"/> is one of the industry standards for access management, also outside of Kubernetes. Roles can be assigned to<a id="_idIndexMarker1625"/> users in the system, which gives them certain privileges and access. In this way, you can achieve very fine-grained access management that can be used to enforce <a id="_idIndexMarker1626"/>the <strong class="keyWord">principle of least privilege</strong>. For example, you can define a role in the system that allows you to access certain files on a network share. Then, you can assign such roles to individual users in groups in the system to allow them to access these files. This can be done by associating the user with a role – in Kubernetes, you model this using<a id="_idIndexMarker1627"/> the <strong class="keyWord">RoleBinding</strong> and <strong class="keyWord">ClusterRoleBinding</strong> objects. In this way, multiple users can be assigned a role, and a<a id="_idIndexMarker1628"/> single user can have multiple roles assigned. Please note that in Kubernetes, RBAC is <em class="italic">permissive</em>, which means that there are no <em class="italic">deny</em> rules. Everything is denied by default, and you have to define <em class="italic">allow</em> rules instead.</li>
      <li class="bulletList"><strong class="keyWord">Attribute-Based Access Control (ABAC)</strong>: This is part of the access control paradigm and is not only<a id="_idIndexMarker1629"/> used in Kubernetes, which uses<a id="_idIndexMarker1630"/> policies based on the attributes of the user, resource, and environment. This is a very fine-grained access control approach – you can, for example, define that the user can access a given file, but only if the user has clearance to access confidential data (user attribute), the owner of the file is Mike (resource attribute), and the user tries to access the file from an internal network (environment attribute). So, policies are sets of attributes that must be present together for the action to be performed. In Kubernetes, this is modeled using Policy objects. For example, you can define that the authenticated user, <code class="inlineCode">mike</code>, can read any Pods in the <code class="inlineCode">default</code> namespace. If you want to give the same access to user <code class="inlineCode">bob</code>, then you need to create a new Policy for user <code class="inlineCode">bob</code>.</li>
      <li class="bulletList"><strong class="keyWord">Node</strong>: This is a <a id="_idIndexMarker1631"/>special-purpose authorization mode used for authorizing API requests <a id="_idIndexMarker1632"/>made by <code class="inlineCode">kubelet</code> in the cluster.</li>
      <li class="bulletList"><strong class="keyWord">Webhook</strong>: This <a id="_idIndexMarker1633"/>mode is similar to webhooks for authentication. You<a id="_idIndexMarker1634"/> can define an external service that needs to handle HTTP POST requests with a <strong class="keyWord">SubjectAccessReview </strong>object that’s sent by the Kubernetes API server. This service must process the request and determine if the request should be allowed or denied. The response from the service should contain <code class="inlineCode">SubjectAccessReview</code>, along with information, on whether the subject is allowed access. Based on that, the Kubernetes API server will either proceed with the request or reject it with an HTTP status code of <code class="inlineCode">403</code>. This approach is useful when you are integrating with existing access control solutions in the organization.</li>
      <li class="bulletList"><strong class="keyWord">AlwaysAllow</strong>: This<a id="_idIndexMarker1635"/> grants unrestricted access to all requests, and is only suitable for testing environments due to security concerns.</li>
      <li class="bulletList"><strong class="keyWord">AlwaysDeny</strong>: This <a id="_idIndexMarker1636"/>blocks all requests, and is useful solely for testing purposes to establish a baseline for authorization.</li>
    </ul>
    <p class="normal">Currently, RBAC is considered an industry standard in Kubernetes due to its flexibility and ease of management. For this reason, RBAC is the only authentication mode we are going to describe in more detail.</p>
    <h3 class="heading-3" id="_idParaDest-599">RBAC mode in Kubernetes</h3>
    <p class="normal">Using RBAC in Kubernetes involves the following types of API resources that belong to the <code class="inlineCode">rbac.authorization.k8s.io</code> API group:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Role</strong> and <strong class="keyWord">ClusterRole</strong>: They define a set of permissions. Each <code class="inlineCode">rule</code> in Role says which verb(s) are allowed for which API resource(s). The only difference between Role<a id="_idIndexMarker1637"/> and ClusterRole<a id="_idIndexMarker1638"/> is that Role is namespace-scoped, whereas ClusterRole is global.</li>
      <li class="bulletList"><strong class="keyWord">RoleBinding</strong> and <strong class="keyWord">ClusterRoleBinding</strong>: They associate users or a set of users (alternatively, groups or ServiceAccounts) with a given Role. Similarly, RoleBinding<a id="_idIndexMarker1639"/> is namespace-scoped, while ClusterRoleBinding is <a id="_idIndexMarker1640"/>cluster-wide. Please note that ClusterRoleBinding works with ClusterRole, but RoleBinding works with both ClusterRole and Role.</li>
    </ul>
    <p class="normal">All these Kubernetes objects can be managed using <code class="inlineCode">kubectl</code> and YAML manifests, just as you do with Pods, Services, and so on.</p>
    <p class="normal">We will now demonstrate this in <a id="_idIndexMarker1641"/>practice. In the previous section, we showed a basic RBAC configuration for a service account that was being used for authentication using <code class="inlineCode">kubectl</code>. The example that we are going to use here will be a bit different and will involve creating a Pod that runs under a <em class="italic">dedicated</em> service account and periodically queries the Kubernetes API server for a list of Pods. In general, having dedicated service accounts for running your Pods is a good practice and makes it possible to ensure the principle of least privilege. For example, if your Pod needs to get the list of Pods in the cluster but does not need to create a new Pod, the ServiceAccount for this Pod should have a role assigned that allows you to list Pods, nothing more. Follow these steps to configure this example:</p>
    <ol>
      <li class="numberedList" value="1">Begin by creating a dedicated namespace for the objects with the following YAML file:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># 02_rbac/rbac-demo-ns.yaml</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">rbac-demo-ns</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Create namespace by applying the YAML</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f 02_rbac/rbac-demo-ns.yaml
namespace/rbac-demo-ns created
</code></pre>
    <ol>
      <li class="numberedList" value="2">To demonstrate, let us create a sample nginx Pod in the same namespace using the <code class="inlineCode">02_rbac/nginx-pod.yaml</code> definition:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span> kubectl apply -f 02_rbac/nginx-pod.yaml
pod/nginx-pod created
</code></pre>
      </li>
    </ol>
    <div class="note-one">
      <p class="normal">Please note, that the <code class="inlineCode">nginx</code> Pod is not doing anything here; we need the Pod <code class="inlineCode">pod-logger-app</code> to fetch the <code class="inlineCode">nginx</code> Pod details in the <code class="inlineCode">rbac-demo-ns</code> namespace later.</p>
    </div>
    <ol>
      <li class="numberedList" value="3">Now, create a ServiceAccount named <code class="inlineCode">pod-logger</code>. Create a YAML manifest named <code class="inlineCode">pod-logger-serviceaccount.yaml</code>:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># 02_rbac/pod-logger-serviceaccount.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-logger</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">rbac-demo-ns</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Apply the<a id="_idIndexMarker1642"/> manifest to the cluster using the following command:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f 02_rbac/pod-logger-serviceaccount.yaml
serviceaccount/pod-logger created
</code></pre>
    <ol>
      <li class="numberedList" value="4">Create a role named <code class="inlineCode">pod-reader</code>. This role will only allow the <code class="inlineCode">get</code>, <code class="inlineCode">watch</code>, and <code class="inlineCode">list</code> verbs on <code class="inlineCode">pods</code> resources in the Kubernetes RESTful API. In other words, this translates into an <code class="inlineCode">/api/v1/namespaces/rbac-demo-ns/pods</code> endpoint in the API. Note that <code class="inlineCode">apiGroups</code> specified as <code class="inlineCode">""</code> mean the <code class="inlineCode">core</code> API group. The structure of the <code class="inlineCode">pod-reader-role.yaml</code> manifest file is as follows:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># 02_rbac/pod-reader-role.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">rbac-demo-ns</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-reader</span>
<span class="hljs-attr">rules:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">""</span>]
    <span class="hljs-attr">resources:</span> [<span class="hljs-string">"pods"</span>]
    <span class="hljs-attr">verbs:</span> [<span class="hljs-string">"get"</span>, <span class="hljs-string">"watch"</span>, <span class="hljs-string">"list"</span>]
</code></pre>
      </li>
      <li class="numberedList">Apply the manifest to the cluster using the following command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f 02_rbac/pod-reader-role.yaml
role.rbac.authorization.k8s.io/pod-reader created
</code></pre>
      </li>
      <li class="numberedList">Now, we would normally create a RoleBinding object to associate the service account with the role. But to make this demonstration more interesting, we will create a Pod that’s running under the <code class="inlineCode">pod-logger</code> service account. This will essentially make the Pod unable to query the API for Pods because it will be <em class="italic">unauthorized</em> (remember that everything is denied by default in RBAC). Create a YAML manifest named <code class="inlineCode">pod-logger-app.yaml</code> for a <a id="_idIndexMarker1643"/>Pod called <code class="inlineCode">pod-logger-app</code>, running without any additional controllers:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># 02_rbac/pod-logger-app.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-logger-app</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">rbac-demo-ns</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">pod-logger</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">logger</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">quay.io/iamgini/k8sutils:debian12</span>
      <span class="hljs-attr">command:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">/bin/sh</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">-c</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">|</span>
          <span class="hljs-string">SERVICEACCOUNT=/var/run/secrets/kubernetes.io/serviceaccount</span>
          <span class="hljs-string">TOKEN=$(cat</span> <span class="hljs-string">${SERVICEACCOUNT}/token)</span>
          <span class="hljs-string">while</span> <span class="hljs-literal">true</span>
          <span class="hljs-string">do</span>
            <span class="hljs-string">echo</span> <span class="hljs-string">"Querying Kubernetes API Server for Pods in default namespace..."</span>
            <span class="hljs-string">curl</span> <span class="hljs-string">--cacert</span> <span class="hljs-string">$SERVICEACCOUNT/ca.crt</span> <span class="hljs-string">--header</span> <span class="hljs-string">"Authorization: Bearer $TOKEN"</span> <span class="hljs-string">-X</span> <span class="hljs-string">GET</span> <span class="hljs-string">https://kubernetes.default.svc.cluster.local/api/v1/namespaces/</span><span class="code-highlight"><strong class="hljs-string-slc">rbac-demo-ns</strong></span><span class="hljs-string">/pods</span>
            <span class="hljs-string">sleep</span> <span class="hljs-number">10</span>
          <span class="hljs-string">done</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Here, the most important fields are <code class="inlineCode">.spec.serviceAccountName</code>, which specifies the service account that the Pod should run under, and the <code class="inlineCode">command</code> in the container definition, which we have overridden to periodically query the Kubernetes API.</p>
    <ol>
      <li class="numberedList" value="7">Let us apply the <code class="inlineCode">02_rbac/pod-logger-app.yaml</code> to create the Pod as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f 02_rbac/pod-logger-app.yaml
pod/pod-logger-app created
<span class="hljs-con-meta">$ </span>kubectl get po -n rbac-demo-ns
NAME             READY   STATUS    RESTARTS   AGE
nginx-pod        1/1     Running   0          15m
pod-logger-app   1/1     Running   0          9s
</code></pre>
      </li>
      <li class="numberedList">Assigning<a id="_idIndexMarker1644"/> the <code class="inlineCode">pod-logger</code> service account, as explained in the previous section, will result in a Secret with a bearer JWT for this account to be mounted in the container filesystem under <code class="inlineCode">/var/run/secrets/kubernetes.io/serviceaccount/token</code>. Let us verify this using <code class="inlineCode">kubectl exec</code>, as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -it -n rbac-demo-ns pod-logger-app -- bash
root@pod-logger-app:/# ls -l /var/run/secrets/kubernetes.io/serviceaccount/
total 0
lrwxrwxrwx 1 root root 13 Jul 14 03:33 ca.crt -&gt; ..data/ca.crt
lrwxrwxrwx 1 root root 16 Jul 14 03:33 namespace -&gt; ..data/namespace
lrwxrwxrwx 1 root root 12 Jul 14 03:33 token -&gt; ..data/token
</code></pre>
      </li>
      <li class="numberedList">The overridden commands run an infinite loop in a Linux shell (e.g., bash) in 10-second intervals. In each iteration, we query the Kubernetes API endpoint (<code class="inlineCode">https://kubernetes/api/v1/namespaces/rbac-demo-ns/pods</code>) for Pods in the <code class="inlineCode">rbac-demo-ns</code> namespace with the HTTP <code class="inlineCode">GET</code> method using the <code class="inlineCode">curl</code> command. To properly authenticate, we pass the contents of <code class="inlineCode">/var/run/secrets/kubernetes.io/serviceaccount/token</code> as a <strong class="keyWord">bearer</strong> token in the <code class="inlineCode">Authorization</code> header for the request. Additionally, we pass a CA certificate path to verify the remote server using the <code class="inlineCode">cacert</code> argument. The certificate is injected into <code class="inlineCode">/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</code> by the Kubernetes runtime. When you inspect its logs, you should expect to see a bunch of messages with an HTTP status code of <code class="inlineCode">403 (Forbidden).</code> This is because the ServiceAccount does not have a RoleBinding type that associates it with the <code class="inlineCode">pod-reader</code> Role yet.</li>
      <li class="numberedList">Start following the logs of the <code class="inlineCode">pod-logger-app</code> Pod using the following command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl logs -n rbac-demo-ns pod-logger-app -f
Querying Kubernetes API Server for Pods in rbac-demo-ns namespace...
<span class="hljs-con-meta">  % </span>Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   336  100   336    0     0  25611      0 --:--:-- --:--:-- --:--:-- 25846
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "pods is forbidden: User \"system:serviceaccount:rbac-demo-ns:pod-logger\" cannot list resource \"pods\" in API group \"\" in the namespace \"rbac-demo-ns\"",
  "reason": "Forbidden",
  "details": {
    "kind": "pods"
  },
  "code": 403
}
</code></pre>
      </li>
      <li class="numberedList">In a new console <a id="_idIndexMarker1645"/>window (or by ending the logs with the <em class="keystroke">Ctrl + F</em> command), we will create and apply a RoleBinding that <em class="italic">associates</em> the <code class="inlineCode">pod-logger</code> ServiceAccount with the <code class="inlineCode">pod-reader</code> Role. Create a YAML manifest named <code class="inlineCode">read-pods-rolebinding.yaml</code> that contains the following contents:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># 02_rbac/read-pods-rolebinding.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">RoleBinding</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">read-pods</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">rbac-demo-ns</span>
<span class="hljs-attr">subjects:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-logger</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">rbac-demo-ns</span>
<span class="hljs-attr">roleRef:</span>
  <span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-reader</span>
  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">There are three key components in the RoleBinding manifest: <code class="inlineCode">name</code>, which is used to identify the user; <code class="inlineCode">subjects</code>, which reference the users, groups, or service accounts; and <code class="inlineCode">roleRef</code>, which references the role.</p>
    <ol>
      <li class="numberedList" value="12">Apply the <a id="_idIndexMarker1646"/>RoleBinding manifest file using the following command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f 02_rbac/read-pods-rolebinding.yaml
rolebinding.rbac.authorization.k8s.io/read-pods created
</code></pre>
      </li>
      <li class="numberedList">Now check the <code class="inlineCode">pod-logger-app</code> logs again; you will see that the Pod was able to successfully retrieve the list of Pods in the <code class="inlineCode">rbac-demo-ns</code> namespace. In other words, the request was successfully authorized:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl logs -n rbac-demo-ns pod-logger-app -f
...&lt;removed for brevity&gt;...
Querying Kubernetes API Server for Pods in default namespace...
<span class="hljs-con-meta">  % </span>Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
{
  "kind": "PodList",
  "apiVersion": "v1",
  "metadata": {
    "resourceVersion": "4889"
  },
  "items": [
    {
      "metadata": {
        "name": "nginx-pod",
        "namespace": "rbac-demo-ns",
        "uid": "b62b2bdb-2677-4809-a134-9d6cfa07ecad",
...&lt;removed for brevity&gt;...
</code></pre>
      </li>
      <li class="numberedList">Lastly, you can delete the RoleBinding type using the following command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete rolebinding read-pods -n rbac-demo-ns
</code></pre>
      </li>
      <li class="numberedList">Now, if you inspect the logs of the <code class="inlineCode">pod-logger-app</code> Pod again, you will see that the requests are denied with an HTTP status code of <code class="inlineCode">403</code> again.</li>
    </ol>
    <p class="normal">Congratulations! You<a id="_idIndexMarker1647"/> have successfully used RBAC in Kubernetes to be able to read the Pods in the cluster for a Pod running under ServiceAccount. To clean up the Kubernetes environment, you can delete the <code class="inlineCode">rbac-demo-ns</code> namespace so that the resources you created will be removed as part of the namespace removal.</p>
    <p class="normal">As we have explored authentication and authorization, in the next section, let us learn about another security feature in Kubernetes called admission controllers.</p>
    <h1 class="heading-1" id="_idParaDest-600">Admission Control – Security Policies and Checks</h1>
    <p class="normal">Imagine a security checkpoint at a critical facility. Admission controllers<a id="_idIndexMarker1648"/> in Kubernetes function similarly for your cluster. They act as gatekeepers, intercepting requests to the Kubernetes API server before resources are created, deleted, or modified. These controllers can validate or modify the requests based on predefined rules, ensuring that only authorized and properly configured resources enter the system. Also note that admission controllers do not (and cannot) block requests to read (get, watch, or list) objects.</p>
    <div class="note">
      <p class="normal">Several key features of Kubernetes rely on specific admission controllers to function correctly. Therefore, a Kubernetes API server without the appropriate admission controllers is incomplete and will not support all expected features.</p>
    </div>
    <p class="normal">There are two types of admission controllers:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Validation controllers</strong>: These <a id="_idIndexMarker1649"/>controllers meticulously<a id="_idIndexMarker1650"/> examine incoming requests. If they find anything suspicious or non-compliant with set policies, they reject the request entirely.</li>
      <li class="bulletList"><strong class="keyWord">Mutation controllers</strong>: These <a id="_idIndexMarker1651"/>controllers have the <a id="_idIndexMarker1652"/>power to modify requests before they are stored permanently. They can, for instance, add missing security annotations or adjust resource limits.</li>
    </ul>
    <p class="normal">Now, let’s get introduced to the two-phase admission process.</p>
    <h2 class="heading-2" id="_idParaDest-601">The Two-Phase Admission Process</h2>
    <p class="normal">Admission control in <a id="_idIndexMarker1653"/>Kubernetes operates in a two-step process, ensuring only compliant resources enter your cluster.</p>
    <p class="normal">The high-level flow taken by the mutation and validation phases of Kubernetes admission control is represented by the following figure. This flow takes incoming requests into Kubernetes to process in a manner that first does the appropriate mutations for modifying or enriching requests before doing any actual validation to see if the request meets all required security and policy validations that are required. </p>
    <p class="normal">This sequence is exposed to the flow, showing the way, Kubernetes enforces consistency, security, and policy compliance before allowing any changes to the cluster state:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_18_07.png"/></figure>
    <p class="packt_figref">Figure 18.7: Admission controllers in the API request processing flow (image source: https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/)</p>
    <p class="normal">Here’s a breakdown of each phase.</p>
    <h3 class="heading-3" id="_idParaDest-602">Mutation Phase</h3>
    <p class="normal">The Mutation phase<a id="_idIndexMarker1654"/> is a step in admission control in Kubernetes, where controllers running in the <a id="_idIndexMarker1655"/>role of mutation controllers will change the incoming API requests to make them compliant with cluster policies before further processing. Such controllers basically act like “molders,” which not only see to it that the requests are consistent with established settings but can also automatically add or adjust settings, for example, defaults or security labels. This makes the system maintain the policy compliance and alignment of configurations without manual input.</p>
    <p class="normal">There are a few <a id="_idIndexMarker1656"/>examples presented here for<a id="_idIndexMarker1657"/> this phase:</p>
    <ul>
      <li class="bulletList">Adding missing security annotations to pods.</li>
      <li class="bulletList">Adjusting resource requests and limits for pods based on pre-defined rules.</li>
      <li class="bulletList">Injecting sidecar containers for specific functionalities.</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-603">Validation Phase</h3>
    <p class="normal">It is during the <a id="_idIndexMarker1658"/>Validation phase, in this sequence, that Kubernetes admission control completes a<a id="_idIndexMarker1659"/> controller’s doing in the preceding Mutation phase. Controllers then closely scrutinize incoming requests that might have been modified by some controller. Often referred to as the “guardians,” these controllers check requests for adherence to cluster policies and security standards. It is an important phase in the prevention of misconfiguration and unauthorized changes that maintain cluster integrity and security by rejecting requests not meeting set criteria.</p>
    <p class="normal">Some of the example actions are listed here:</p>
    <ul>
      <li class="bulletList">Approve the request if it adheres to set criteria (e.g., resource quotas, security standards).</li>
      <li class="bulletList">Reject the request if it violates any policies, providing informative error messages.</li>
    </ul>
    <p class="normal">In the next section, we will learn how to turn off and turn on admission controllers in Kubernetes.</p>
    <h2 class="heading-2" id="_idParaDest-604">Enabling and disabling Admission controllers</h2>
    <p class="normal">To check which Admission controllers are enabled, you typically need to inspect the configuration of <a id="_idIndexMarker1660"/>the <strong class="keyWord">Kubernetes API server</strong>. This is often done by accessing the <a id="_idIndexMarker1661"/>configuration file where the API server is defined, usually located in the system’s configuration directories or managed through a configuration management tool. Look for the <code class="inlineCode">--enable-admission-plugins</code> flag, which specifies the list of admission controllers currently active.</p>
    <p class="normal">For example, in a minikube environment, you can SSH into the minikube VM using the <code class="inlineCode">minikube ssh</code> command. Once inside, you can locate and inspect the <code class="inlineCode">kube-apiserver.yaml</code> file, typically found in <code class="inlineCode">/etc/kubernetes/manifests/</code>. Use <code class="inlineCode">sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml</code> to view its contents and look for the <code class="inlineCode">--enable-admission-plugins</code> flag:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube ssh <span class="hljs-con-string">'sudo grep -- '</span>--enable-admission-plugins<span class="hljs-con-string">' /etc/kubernetes/manifests/kube-apiserver.yaml'</span>
    - --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota
</code></pre>
    <p class="normal">To modify the list of<a id="_idIndexMarker1662"/> enabled plugins, edit this file with a text editor like nano or vi, adjust the plugins as needed, and then save your changes. The kubelet watches the manifest files and will automatically restart the API server (recreate the Pod) if it detects any changes to the manifest file.</p>
    <p class="normal">It is also possible to turn<a id="_idIndexMarker1663"/> off the default admissions controllers as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">kube-apiserver --disable-admission-plugins=PodNodeSelector,AlwaysDeny ...
</code></pre>
    <p class="normal">In the following section, we will learn the list of admission controllers available in Kubernetes.</p>
    <h2 class="heading-2" id="_idParaDest-605">Common Admission Controllers</h2>
    <p class="normal">In Kubernetes, the admission controllers<a id="_idIndexMarker1664"/> are built into the <code class="inlineCode">kube-apiserver</code> and should only be configured by the cluster administrator. Among these controllers, two are particularly <a id="_idIndexMarker1665"/>notable: <strong class="keyWord">MutatingAdmissionWebhook</strong> and <strong class="keyWord">ValidatingAdmissionWebhook</strong>. These controllers execute the respective mutating and validating<a id="_idIndexMarker1666"/> admission control webhooks that are configured through the API:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Basic Controls</strong>: <code class="inlineCode">AlwaysAdmit</code> (deprecated), <code class="inlineCode">AlwaysDeny</code> (deprecated), <code class="inlineCode">AlwaysPullImages</code></li>
      <li class="bulletList"><strong class="keyWord">Defaults</strong>: <code class="inlineCode">DefaultStorageClass</code>, <code class="inlineCode">DefaultTolerationSecond</code>s</li>
      <li class="bulletList"><strong class="keyWord">Security</strong>: <code class="inlineCode">DenyEscalatingExec</code>, <code class="inlineCode">DenyServiceExternalIPs</code>, <code class="inlineCode">PodSecurityPolicy</code>, <code class="inlineCode">SecurityContextDeny</code></li>
      <li class="bulletList"><strong class="keyWord">Resource Management</strong>: <code class="inlineCode">LimitRanger</code>, <code class="inlineCode">ResourceQuota</code>, <code class="inlineCode">RuntimeClass</code></li>
      <li class="bulletList"><strong class="keyWord">Object Lifecycle</strong>: <code class="inlineCode">NamespaceAutoProvision</code>, <code class="inlineCode">NamespaceExists</code>, <a id="_idIndexMarker1667"/><code class="inlineCode">NamespaceLifecycle</code>, P<code class="inlineCode">ersistentVolumeClaimResize</code>, <code class="inlineCode">StorageObjectInUseProtection</code></li>
      <li class="bulletList"><strong class="keyWord">Node Management</strong>: <code class="inlineCode">NodeRestriction</code>, <code class="inlineCode">TaintNodesByCondition</code></li>
      <li class="bulletList"><strong class="keyWord">Webhooks</strong>: <code class="inlineCode">MutatingAdmissionWebhook</code>, <code class="inlineCode">ValidatingAdmissionWebhook</code></li>
      <li class="bulletList"><strong class="keyWord">Others</strong>: <code class="inlineCode">EventRateLimit</code>, <code class="inlineCode">LimitPodHardAntiAffinityTopology</code>, <code class="inlineCode">OwnerReferencesPermissionEnforcement</code>, <code class="inlineCode">PodNodeSelector</code> (deprecated), <code class="inlineCode">Priority</code>, <code class="inlineCode">ServiceAccount</code></li>
    </ul>
    <p class="normal">There are numerous advantages to using admission controllers in a Kubernetes cluster. Let us learn about a few in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-606">Benefits of Admission Controllers</h2>
    <p class="normal">There are multiple advantages<a id="_idIndexMarker1668"/> of using admission controllers in your Kubernetes clusters, including the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Enhanced Security</strong>: By enforcing security policies like pod security standards, admission controllers help keep your cluster safe from unauthorized or vulnerable deployments.</li>
      <li class="bulletList"><strong class="keyWord">Policy Enforcement</strong>: You can define rules for resource usage, image pulling, and more, which admission controllers will automatically enforce.</li>
      <li class="bulletList"><strong class="keyWord">Consistency and Standardization</strong>: Admission controllers ensure that resources across your cluster adhere to established best practices and configurations.</li>
    </ul>
    <p class="normal">To summarize, the admission controllers section has emphasized that admission controllers play a very important role in ensuring the security of Kubernetes, and this is through the Mutation and Validation phases. We learned how the mutation controllers run modifications on the requests to make sure they comply with cluster policies, while validation controllers ensure none other than those meeting the security standards are processed. In all, the foregoing processes improve the overall state of the security of the Kubernetes clusters by assuring compliance and prohibiting unauthorized changes.</p>
    <p class="normal">In the next sections, let us learn about how to secure workloads in Kubernetes using Security Context and NetworkPolicies.</p>
    <h1 class="heading-1" id="_idParaDest-607">Securing Pods and Containers</h1>
    <p class="normal">Securing Pods<a id="_idIndexMarker1669"/> and containers<a id="_idIndexMarker1670"/> is essential to keeping your Kubernetes environment in a healthy state, since these directly interact with workloads and sensitive data. In the next sections, we are going to talk about how the securityContext settings and NetworkPolicies can enforce strict access controls and isolation in place to strengthen the security of Pods and containers in your cluster.</p>
    <h2 class="heading-2" id="_idParaDest-608">Securing Pods and Containers in Kubernetes Using Security Context</h2>
    <p class="normal">In Kubernetes, a <strong class="keyWord">securityContext</strong> defines <a id="_idIndexMarker1671"/>a set of security settings that determine how a Pod or container operates within the cluster. This allows you to enforce security best practices and minimize the attack surface by restricting privileges and controlling access.</p>
    <p class="normal">The primary purpose of securityContext is to enhance the security of your Kubernetes clusters by defining how a pod or container should run within the cluster. By specifying security settings, you can ensure that your applications adhere to the principle of least privilege, reducing the potential for malicious activities and accidental misconfigurations.</p>
    <p class="normal">A typical use case for securityContext is to run containers as non-root users. This prevents containers from having unnecessary permissions, thereby limiting the potential damage if a container is compromised. Additionally, you can configure other security settings such as read-only filesystems and fine-grained capabilities to further strengthen your cluster’s security posture.</p>
    <h3 class="heading-3" id="_idParaDest-609">Key Components of SecurityContext</h3>
    <p class="normal">Here’s a breakdown of the key components of a securityContext along with illustrative examples.</p>
    <h4 class="heading-4">User and Group</h4>
    <p class="normal">This security context specifies the user<a id="_idIndexMarker1672"/> and group<a id="_idIndexMarker1673"/> ID, under which processes inside the container will run. By enforcing the principle of least privilege, it grants containers only the minimum permissions necessary to function. The following code snippet shows a typical example of a Pod definition with the securityContext configured: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">my-app</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">app-container</span>
    <span class="hljs-attr">securityContext:</span>
      <span class="hljs-attr">runAsUser:</span> <span class="hljs-number">1000</span>  <span class="hljs-comment"># Run container processes as user ID 1000</span>
      <span class="hljs-attr">runAsGroup:</span> <span class="hljs-number">1000</span> <span class="hljs-comment"># Run container processes as group ID 1000</span>
</code></pre>
    <h4 class="heading-4">Linux Capabilities</h4>
    <p class="normal">Capabilities are special privileges <a id="_idIndexMarker1674"/>that can be granted to containers beyond the limitations of a user. securityContext allows you to define which capabilities a container should have, enabling specific functionalities without providing full root access, as shown in the following example:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">privileged-container</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">app-container</span>
    <span class="hljs-attr">securityContext:</span>
      <span class="hljs-attr">capabilities:</span>
        <span class="hljs-attr">add:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-string">CAP_NET_ADMIN</span>  <span class="hljs-comment"># Grant network management capabilities</span>
</code></pre>
    <p class="normal">Refer to the Linux capabilities documentation to learn more (<a href="https://linux-audit.com/kernel/capabilities/linux-capabilities-hardening-linux-binaries-by-removing-setuid/"><span class="url">https://linux-audit.com/kernel/capabilities/linux-capabilities-hardening-linux-binaries-by-removing-setuid/</span></a>).</p>
    <h4 class="heading-4">Privileged Mode</h4>
    <p class="normal">By default, containers run in an unprivileged mode. securityContext offers the option to run a container in privileged mode, granting<a id="_idIndexMarker1675"/> it access to capabilities usually reserved for the root user. However, due to the increased security risk, this should be used with extreme caution. Refer to the following code snippet for en example Pod YAML with <a id="_idIndexMarker1676"/>privileged mode securityContext:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">privileged-container</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">app-container</span>
    <span class="hljs-attr">securityContext:</span>
      <span class="hljs-attr">privileged:</span> <span class="hljs-literal">true</span>  <span class="hljs-comment"># Run container in privileged mode (use cautiously)</span>
</code></pre>
    <h4 class="heading-4">Read-Only Root Filesystem</h4>
    <p class="normal">This securityContext allows you to configure<a id="_idIndexMarker1677"/> the container to have a read-only root filesystem. This enhances security by preventing accidental or malicious modifications to the base system:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">read-only-container</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">app-container</span>
    <span class="hljs-attr">securityContext:</span>
      <span class="code-highlight"><strong class="hljs-attr-slc">readOnlyRootFilesystem:</strong><strong class="hljs-slc"> </strong><strong class="hljs-literal-slc">true</strong></span>  <span class="hljs-comment"># Mount root filesystem as read-only</span>
</code></pre>
    <p class="normal">There are<a id="_idIndexMarker1678"/> a few more Security<a id="_idIndexMarker1679"/>Context settings such as <strong class="keyWord">Security Enhanced Linux</strong> (<strong class="keyWord">SELinux</strong>), <strong class="keyWord">AppArmor</strong> (<a href="https://kubernetes.io/docs/tutorials/security/apparmor/"><span class="url">https://kubernetes.io/docs/tutorials/security/apparmor/</span></a>), <strong class="keyWord">Seccomp</strong>, and so on. Refer to <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/"><span class="url">https://kubernetes.io/docs/tasks/configure-pod-container/security-context/</span></a> to learn <a id="_idIndexMarker1680"/>more.</p>
    <p class="normal">You also need to know where is the best place in the configuration you can apply SecurityContext; let us learn that in the next section.</p>
    <h3 class="heading-3" id="_idParaDest-610">Applying SecurityContext at Pod and Container Levels</h3>
    <p class="normal">In Kubernetes, the securityContext can be applied at both the pod level and the container level, offering flexibility in defining security settings for your applications.</p>
    <h4 class="heading-4">Pod-Level SecurityContext</h4>
    <p class="normal">When applied at the <a id="_idIndexMarker1681"/>pod level, the securityContext settings are inherited by all<a id="_idIndexMarker1682"/> containers within the pod. This is useful for setting default security configurations that should apply uniformly across all containers in the pod:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">my-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">securityContext:</span>
    <span class="hljs-attr">runAsUser:</span> <span class="hljs-number">1000</span>  <span class="hljs-comment"># All containers in the pod run as user ID 1000</span>
    <span class="hljs-attr">runAsGroup:</span> <span class="hljs-number">1000</span> <span class="hljs-comment"># All containers in the pod run as group ID 1000</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">app-container</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">my-app-image</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">sidecar-container</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">my-sidecar-image</span>
</code></pre>
    <h4 class="heading-4">Container-Level SecurityContext</h4>
    <p class="normal">When applied at <a id="_idIndexMarker1683"/>the container level, the securityContext settings <a id="_idIndexMarker1684"/>only affect the specific container. This allows for more granular control, where different containers within the same pod can have different security configurations.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">my-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">app-container</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">my-app-image</span>
    <span class="hljs-attr">securityContext:</span>
      <span class="hljs-attr">runAsUser:</span> <span class="hljs-number">1000</span>  <span class="hljs-comment"># This container runs as user ID 1000</span>
      <span class="hljs-attr">capabilities:</span>
        <span class="hljs-attr">add:</span> [<span class="hljs-string">"CAP_NET_ADMIN"</span>]  <span class="hljs-comment"># Grant specific capabilities</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">sidecar-container</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">my-sidecar-image</span>
    <span class="hljs-attr">securityContext:</span>
      <span class="hljs-attr">runAsUser:</span> <span class="hljs-number">2000</span>  <span class="hljs-comment"># This container runs as user ID 2000</span>
      <span class="hljs-attr">readOnlyRootFilesystem:</span> <span class="hljs-literal">true</span>  <span class="hljs-comment"># This container has a read-only root filesystem</span>
</code></pre>
    <p class="normal">In the following section, let us demonstrate the Security Context with an example Pod.</p>
    <h3 class="heading-3" id="_idParaDest-611">Applying Security Context to a Pod</h3>
    <p class="normal">The following <a id="_idIndexMarker1685"/>example creates a Pod with a container that runs with a <code class="inlineCode">read-only</code> root filesystem and specifies non-root user and group IDs:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># pod-with-security-context.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">security-context-demo</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">app-container</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:latest</span>
      <span class="hljs-attr">securityContext:</span>
        <span class="hljs-attr">runAsUser:</span> <span class="hljs-number">1000</span>                <span class="hljs-comment"># Run container processes as user ID 1000</span>
        <span class="hljs-attr">runAsGroup:</span> <span class="hljs-number">1000</span>               <span class="hljs-comment"># Run container processes as group ID 1000</span>
        <span class="hljs-attr">readOnlyRootFilesystem:</span> <span class="hljs-literal">true</span>   <span class="hljs-comment"># Mount the root filesystem as read-only</span>
      <span class="hljs-attr">volumeMounts:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">html-volume</span>
          <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/usr/share/nginx/html</span>
  <span class="hljs-attr">volumes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">html-volume</span>
      <span class="hljs-attr">emptyDir:</span> {}                     <span class="hljs-comment"># Volume to provide writable space</span>
</code></pre>
    <p class="normal">In the preceding YAML, note the following:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">runAsUser</code><strong class="keyWord"> </strong>and <code class="inlineCode">runAsGroup</code>: These settings ensure that the container runs with a specific non-root user ID and group ID, following the principle of least privilege.</li>
      <li class="bulletList"><code class="inlineCode">readOnlyRootFilesystem</code>: This setting mounts the container’s root filesystem as read-only, preventing any accidental or malicious modifications to the base system.</li>
    </ul>
    <p class="normal">Create the Pod using the <a id="_idIndexMarker1686"/>YAML as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f pod-with-security-context.yaml
pod/security-context-demo created
</code></pre>
    <p class="normal">Once the Pod is created, let us test a few commands inside the container to verify the securityContext we applied:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -it security-context-demo -- /bin/sh
~ $ id
uid=1000 gid=1000 groups=1000
~ $ touch /testfile
touch: /testfile: Read-only file system
</code></pre>
    <p class="normal">You can see the Read-only filesystem error; this is expected.</p>
    <p class="normal">Refer to <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/"><span class="url">https://kubernetes.io/docs/tasks/configure-pod-container/security-context/</span></a> to learn more about the security context in Kubernetes.</p>
    <p class="normal">The next section introduces the control of network flow in Kubernetes using the <strong class="keyWord">NetworkPolicy</strong> object. You will see that you can build a kind of network firewall directly in Kubernetes so that you can prevent Pods from being able to reach one another.</p>
    <h2 class="heading-2" id="_idParaDest-612">Securing Pods using the NetworkPolicy object</h2>
    <p class="normal">The <strong class="keyWord">NetworkPolicy</strong> object<a id="_idIndexMarker1687"/> is the last resource kind we need to discover as part of this chapter to have an overview of services in this chapter. NetworkPolicy will allow you to define network firewalls directly implemented in your cluster.</p>
    <h3 class="heading-3" id="_idParaDest-613">Why do you need NetworkPolicy?</h3>
    <p class="normal">When you have to manage <a id="_idIndexMarker1688"/>a real Kubernetes workload in production, you’ll have to deploy more and more applications onto it, and it is possible that these applications will have to communicate with each other.</p>
    <p class="normal">Achieving communication between applications is really one of the fundamental objectives of a microservice architecture. Most of this communication will be done through the network, and the network is forcibly something that you want to secure by using firewalls.</p>
    <p class="normal">Kubernetes has its own <a id="_idIndexMarker1689"/>implementation of network firewalls called NetworkPolicy. Say that you want one nginx resource to be accessible on port <code class="inlineCode">80</code> from a particular IP address and to block any other traffic that doesn’t match these requirements. To do that, you’ll need to use NetworkPolicy and attach it to that Pod.</p>
    <p class="normal">NetworkPolicy brings three<a id="_idIndexMarker1690"/> benefits, as follows:</p>
    <ul>
      <li class="bulletList">You can build egress/ingress rules <a id="_idIndexMarker1691"/>based on <strong class="keyWord">Classless Inter-Domain Routing</strong> (<strong class="keyWord">CIDR</strong>) blocks.</li>
      <li class="bulletList">You can build egress/ingress rules based on Pods labels and selectors (just as we’ve seen before with services’ and Pods’ association).</li>
      <li class="bulletList">You can build egress/ingress rules based on namespaces (a notion we will discover in the next chapter).</li>
    </ul>
    <p class="normal">Lastly, keep in mind that for NetworkPolicy to work, you’ll need to have a Kubernetes cluster with a CNI plugin installed. CNI plugins are generally not installed by default on Kubernetes. If you’re using minikube for learning purposes, the good news is that it has an integration with Calico, which is a CNI plugin with NetworkPolicy support implemented out of the box. You just need to recreate the <code class="inlineCode">minikube</code> cluster this way:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube start --network-plugin=cni --cni=calico --container-runtime=containerd
</code></pre>
    <p class="normal">If you’re using Kubernetes on top of a cloud platform, we suggest you read the documentation of your cloud provider in order to verify which CNI options your cloud platform offers and whether it implements NetworkPolicy support.</p>
    <h3 class="heading-3" id="_idParaDest-614">Understanding Pods are not isolated by default</h3>
    <p class="normal">By default, in Kubernetes, Pods are not isolated and any Pod can be reached by any other Pod without any constraint.</p>
    <p class="normal">If you don’t use NetworkPolicy, Pods will remain just like that: accessible by everything without any constraint. Once you attach the NetworkPolicy to a Pod, the rules described on the NetworkPolicy will be applied to the Pod.</p>
    <p class="normal">To establish communication between two Pods associated with network policies, both sides must be open. It means Pod <em class="italic">A</em> must have an egress rule to Pod <em class="italic">B</em>, and Pod <em class="italic">B</em> must have an ingress rule from Pod <em class="italic">A</em>; otherwise, the traffic will be denied. The following figure illustrates this:</p>
    <figure class="mediaobject"><img alt="Figure 7.7 – One of the Pods is broken but the service will still forward traffic to it &#10;" src="image/B22019_18_08.png"/></figure>
    <p class="packt_figref">Figure 18.8: One of the Pods is broken but the service will still forward traffic to it</p>
    <p class="normal">Keep in mind that you’ll have to troubleshoot NetworkPolicy because it can be the root cause of a lot of issues. Let’s now configure a NetworkPolicy between two Pods by using labels and selectors.</p>
    <h3 class="heading-3" id="_idParaDest-615">Configuring NetworkPolicy with labels and selectors</h3>
    <p class="normal">First, let’s <a id="_idIndexMarker1692"/>create two nginx Pods to demonstrate our example. To demonstrate the isolation, we will use two separate namespaces in this example. You will learn more about Kubernetes namespace a in <em class="chapterRef">Chapter 6</em>, <em class="italic">Namespaces, Quotas, and Limits for Multi-Tenancy in Kubernetes</em>.</p>
    <div class="note">
      <p class="normal">Implementing complete communication isolation within a namespace can be complex and have unintended consequences. Carefully evaluate your needs and potential impacts before applying any restrictions.</p>
    </div>
    <p class="normal">Let’s create the namespaces and two Pods with two distinct labels so that they become easier to target with the NetworkPolicy.</p>
    <p class="normal">Our <code class="inlineCode">web1</code> namespace with <code class="inlineCode">nginx1</code> pod will be created as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># web1-app.yaml</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">project:</span> <span class="hljs-string">web1</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">web1</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx1</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">web1</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx1</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx1</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span>
</code></pre>
    <p class="normal">Also, the <code class="inlineCode">web2</code> namespace <a id="_idIndexMarker1693"/>with a <code class="inlineCode">nginx2</code> pod will be created as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># web2-app.yaml</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">project:</span> <span class="hljs-string">web2</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">web2</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx2</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">web2</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx2</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx2</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span>
</code></pre>
    <p class="normal">In the previous <a id="_idIndexMarker1694"/>code snippets, we used namespaces (<code class="inlineCode">web1</code> and <code class="inlineCode">web2</code>) instead of deploying to the <code class="inlineCode">default</code> namespace.</p>
    <p class="normal">Let’s create the resources and verify the Pods as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f web-app1.yaml
namespace/web1 created
pod/nginx1 created
<span class="hljs-con-meta">$ </span>kubectl apply -f web-app2.yaml
namespace/web2 created
pod/nginx2 created
<span class="hljs-con-meta">$ </span>kubectl get po -o wide -n web1
NAME     READY   STATUS    RESTARTS   AGE   IP              NODE       NOMINATED NODE   READINESS GATES
nginx1   1/1     Running   0          3m    10.244.120.71   minikube   &lt;none&gt;           &lt;none&gt;
<span class="hljs-con-meta">$ </span>kubectl get po -o wide -n web2
NAME     READY   STATUS    RESTARTS   AGE     IP              NODE       NOMINATED NODE   READINESS GATES
nginx2   1/1     Running   0          2m53s   10.244.120.72   minikube   &lt;none&gt;           &lt;none&gt;
</code></pre>
    <p class="normal">Now that the two Pods are created with distinct labels inside different namespaces, we use the <code class="inlineCode">-o wide</code> flag to get the IP address of both Pods. Run a <code class="inlineCode">curl</code> command from the <code class="inlineCode">nginx1</code> Pod to reach the <code class="inlineCode">nginx2</code> Pod, to confirm that by default, network traffic is allowed because no NetworkPolicy is created at this point. The code is illustrated here; <code class="inlineCode">10.244.120.72</code> is the IP address of the <code class="inlineCode">nginx2</code> Pod in the <code class="inlineCode">web2</code> namespace:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl -n web1 <span class="hljs-con-built_in">exec</span> nginx1 -- curl 10.244.120.72
<span class="hljs-con-meta">  % </span>Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   615  100   615    0     0   698k      0 --:--:-- --:--:-- --:--:--  600k
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...&lt;removed for brevity&gt;...
</code></pre>
    <p class="normal">As you can see, we correctly received the nginx home page from the <code class="inlineCode">nginx2</code> Pod.</p>
    <p class="normal">Now, let’s block all the ingress traffic to the <code class="inlineCode">web2</code> namespace explicitly. To do that, we can create a default policy as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># default-deny-ingress.yaml</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">default-deny-ingress</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">podSelector:</span> {}
  <span class="hljs-attr">policyTypes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">Ingress</span>
  <span class="hljs-attr">ingress:</span> []
</code></pre>
    <p class="normal">In the preceding <a id="_idIndexMarker1695"/>YAML snippet, note the following:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">podSelector: {}</code>: Selects pods to which the NetworkPolicy applies. In this case, {} selects all pods in the namespace. This means that the rules defined in the NetworkPolicy <em class="italic">will apply to all pods in the namespace</em>, regardless of their labels.</li>
      <li class="bulletList"><code class="inlineCode">policyTypes: - Ingress</code>: Specifies the type of policy being applied, which is “Ingress” in this case. This means that the NetworkPolicy will control incoming (ingress) traffic to the selected pods.</li>
      <li class="bulletList"><code class="inlineCode">ingress: []</code>: Defines the list of ingress rules for the NetworkPolicy. In this case, the list is empty (<code class="inlineCode">[]</code>), indicating that there are no specific ingress rules defined. Therefore, all incoming traffic to the selected pods will be denied by default.</li>
    </ul>
    <p class="normal">Let’s apply this deny policy to our <code class="inlineCode">web2</code> namespace to block all incoming (ingress) traffic as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f default-deny-ingress.yaml -n web2
networkpolicy.networking.k8s.io/default-deny-ingress created
</code></pre>
    <p class="normal">We will try to access the <code class="inlineCode">nginx2</code> pod from <code class="inlineCode">nginx1</code> pod now and see the output:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl -n web1 <span class="hljs-con-built_in">exec</span> nginx1 -- curl 10.244.120.72
<span class="hljs-con-meta">  % </span>Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:--  0:02:15 --:--:--     0
curl: (28) Failed to connect to 10.244.120.72 port 80 after 135435 ms: Couldn't connect to server
command terminated with exit code 28
</code></pre>
    <p class="normal">It is clear from the previous output that the traffic to <code class="inlineCode">web2</code> namespace and Pods are denied with the <code class="inlineCode">default-deny-ingress</code> NetworkPolicy resource.</p>
    <p class="normal">Now, we will add <a id="_idIndexMarker1696"/>NetworkPolicy to <code class="inlineCode">nginx2</code> in the <code class="inlineCode">web2</code> namespace to explicitly allow traffic coming from the Pod <code class="inlineCode">nginx1</code> in the <code class="inlineCode">web1</code> namespace. Here is how to proceed with the YAML code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># allow-from-web1-netpol.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">allow-from-web1-netpol</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">web2</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">podSelector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">app:</span> <span class="hljs-string">nginx2</span>
  <span class="hljs-attr">policyTypes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">Ingress</span>
  <span class="hljs-attr">ingress:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">from:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">namespaceSelector:</span>
            <span class="hljs-attr">matchLabels:</span>
              <span class="hljs-attr">project:</span> <span class="hljs-string">web1</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">podSelector:</span>
            <span class="hljs-attr">matchLabels:</span>
              <span class="hljs-attr">app:</span> <span class="hljs-string">nginx1</span>
      <span class="hljs-attr">ports:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
          <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>
</code></pre>
    <p class="normal">Please note the <code class="inlineCode">namespaceSelector.matchLabels</code> here with the <code class="inlineCode">project: web1</code> label, which we used for <code class="inlineCode">web1</code> namespace explicitly for this purpose. Let’s apply this NetworkPolicy, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f nginx2-networkpolicy.yaml
networkpolicy.networking.k8s.io/nginx2-networkpolicy created
</code></pre>
    <p class="normal">Now, let’s run the same <code class="inlineCode">curl</code> command we did before, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl -n web1 <span class="hljs-con-built_in">exec</span> nginx1 -- curl 10.244.120.72
<span class="hljs-con-meta">  % </span>Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   615  100   615    0     0  1280k      0 --:--:-- --:--:-- --:--:--  600k
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...&lt;removed for brevity&gt;...
</code></pre>
    <p class="normal">As you can see, it <a id="_idIndexMarker1697"/>works just like it did before. Why? For the following two reasons:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">nginx2</code> now explicitly allows ingress traffic on port <code class="inlineCode">80</code> from <code class="inlineCode">nginx1</code> in the <code class="inlineCode">web1</code> namepsace; everything else is denied.</li>
      <li class="bulletList"><code class="inlineCode">nginx1</code> has no NetworkPolicy, and thus, egress traffic to everything is allowed for it.</li>
    </ul>
    <p class="normal">Keep in mind that if no NetworkPolicy is set on the Pod, the default behaviour applies—everything is allowed for the Pod.</p>
    <p class="normal">We strongly encourage you to make a habit of using NetworkPolicy along with your Pod. Lastly, please be aware that NetworkPolicy can also be used to build firewalls based on CIDR blocks. It might be useful, especially if your Pods are called from outside the cluster. Otherwise, when you need to configure firewalls between Pods, it is recommended to proceed with labels and selectors as you already did with the services’ configuration.</p>
    <p class="normal">Next, we will focus on yet another important aspect of securing Kubernetes, namely Securing Communication via TLS Certificates between Kubernetes components. In this section, we will talk about how the TLS certificate helps in securing the data in transit and ensures secure interactions among various components that make up the ecosystem of Kubernetes.</p>
    <h2 class="heading-2" id="_idParaDest-616">Securing Communication – TLS Certificates Between Kubernetes Components</h2>
    <p class="normal">In Kubernetes, secure communication<a id="_idIndexMarker1698"/> between various components is critical. <strong class="keyWord">Transport Layer Security</strong> (<strong class="keyWord">TLS</strong>), and <strong class="keyWord">Secure Sockets Layer</strong> (<strong class="keyWord">SSL</strong>), play a crucial role in encrypting data transmissions and establishing trust between services.</p>
    <p class="normal">By<a id="_idIndexMarker1699"/> implementing <strong class="keyWord">TLS with mutual authentication</strong> (<strong class="keyWord">mTLS</strong>), both the client and server involved in communication can verify each other’s identities using digital certificates issued by a trusted CA. This adds a layer <a id="_idIndexMarker1700"/>of security by preventing unauthorized access and ensuring data integrity.</p>
    <p class="normal">Here are some examples of how <a id="_idIndexMarker1701"/>TLS certificates are used in Kubernetes:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">API Server to etcd</strong>: The API server, the central control plane component, communicates with etcd, the distributed key-value store, to manage cluster state. Utilizing mTLS between these components safeguards sensitive cluster data from interception or tampering.</li>
      <li class="bulletList"><strong class="keyWord">Ingress Controller to Services</strong>: The ingress controller, acting as a single entry point for external traffic, routes requests to backend services. Implementing mTLS between the ingress controller and services ensures that only authorized services receive traffic, mitigating potential security breaches.</li>
      <li class="bulletList"><strong class="keyWord">Internal Service Communication</strong>: Services within the cluster can also leverage mTLS for secure communication. This is particularly important for services that handle sensitive data or require strong authentication.</li>
      <li class="bulletList"><strong class="keyWord">Service Meshes – for instance, Istio:</strong> These types of service mesh have a variety of advanced traffic management and security capabilities, such as automatic mTLS between microservices. This makes the process of securing service-to-service communication easier without having to embed these communications with TLS configuration in the code that developers manage.</li>
      <li class="bulletList"><strong class="keyWord">Load Balancers:</strong> Applications deployed behind a load balancer can also be used to secure communication between the load balancer and backend services with the use of TLS. In a configuration like this, the data will remain encrypted along the entire path.</li>
      <li class="bulletList">Another<a id="_idIndexMarker1702"/> security mechanism would be enabling IPSec within a Kubernetes cluster to encrypt network traffic between nodes. This may be useful in the protection of the traffic in cloud environments or between various data centers.</li>
    </ul>
    <p class="normal">Through the deployment of TLS certificates with mTLS, Kubernetes administrators significantly bolster the security of their clusters. This approach encrypts communication paths, verifies the identities of communicating components, and mitigates risks associated with unauthorized data access or tampering.</p>
    <p class="normal">In the next section, we will learn how to enable container security using special containers such as gVisor and Kata Containers.</p>
    <h2 class="heading-2" id="_idParaDest-617">Container Security – gVisor and Kata Containers</h2>
    <p class="normal">Traditional containers share the host operating system kernel with other applications running on the machine, which can pose security risks if a container vulnerability allows access to the underlying <a id="_idIndexMarker1703"/>system. <strong class="keyWord">gVisor</strong> and <strong class="keyWord">Kata Containers</strong> emerge as alternative container runtime technologies <a id="_idIndexMarker1704"/>that prioritize security. Let us learn about them in the next sections.</p>
    <h3 class="heading-3" id="_idParaDest-618">gVisor (Guest Virtual Machine Supervisor)</h3>
    <p class="normal">gVisor<a id="_idIndexMarker1705"/> is a lightweight virtual machine implemented in user space. It acts as a sandbox for each container, isolating it from the host kernel and other containers.</p>
    <p class="normal">The following figure shows the high-level architecture<a id="_idIndexMarker1706"/> of gVisor.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_18_09.png"/></figure>
    <p class="packt_figref">Figure 18.9: gVisor architecture</p>
    <p class="normal">By virtualizing kernel<a id="_idIndexMarker1707"/> functionalities for each container, gVisor ensures that container vulnerabilities cannot directly compromise the host system. It establishes a robust isolation boundary, even in compromised container scenarios. gVisor is best suited for environments requiring the highest level of security isolation, despite potentially higher resource overhead.</p>
    <h3 class="heading-3" id="_idParaDest-619">Kata Containers</h3>
    <p class="normal">Kata Containers<a id="_idIndexMarker1708"/> utilize lightweight VMs that are similar to traditional VMs but optimized for container workloads. Kata Containers offers a secure execution environment by isolating containers within lightweight VMs. This enhanced isolation strengthens security compared to standard containers while maintaining performance efficiency.</p>
    <p class="normal">The following figure demonstrates how Kata Containers are different from traditional container technologies.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_18_10.png"/></figure>
    <p class="packt_figref">Figure 18.10: Kata Containers versus traditional containers (source: https://katacontainers.io/learn/)</p>
    <p class="normal">Kata Containers <a id="_idIndexMarker1709"/>are recommended when balancing strong security with optimal performance, particularly for resource-intensive workloads.</p>
    <h3 class="heading-3" id="_idParaDest-620">Using RuntimeClass for Security Profiles</h3>
    <p class="normal">In Kubernetes, utilize the <code class="inlineCode">runtimeClassName</code> field in your pod spec to specify the container runtime <a id="_idIndexMarker1710"/>environment. Here is an example configuration:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">secure-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">runtimeClassName:</span> <span class="hljs-string">kata-containers</span>  <span class="hljs-comment"># Specifies Kata Containers runtime</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">my-app</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">my-secure-image</span>
</code></pre>
    <p class="normal">This setup directs Kubernetes<a id="_idIndexMarker1711"/> to use the Kata Containers runtime for enhanced security isolation.</p>
    <p class="normal">We learned several important things about Kubernetes security in this chapter. Before we conclude the chapter, let us learn about another security topic in the next section, which is about accessing private registries and container images.</p>
    <h1 class="heading-1" id="_idParaDest-621">Managing Secrets and Registry Credentials</h1>
    <p class="normal">In Kubernetes, registry credentials<a id="_idIndexMarker1712"/> are necessary for securely pulling container images from private registries that require authentication. Without these credentials, Kubernetes pods cannot access images stored in private repositories. Managing these credentials securely is crucial to ensure that only authorized pods can retrieve and use specific container images.</p>
    <p class="normal">Using <code class="inlineCode">kubectl create secret docker-registry</code> simplifies the management of container registry credentials in Kubernetes. It ensures security by encrypting secrets at rest, making them accessible only to authorized nodes. This approach reduces complexity compared to manual methods, minimizing errors and improving operational efficiency. Moreover, it seamlessly integrates with Kubernetes pod specifications, allowing straightforward configuration of <code class="inlineCode">imagePullSecrets</code> to authenticate pod access to private container registries.</p>
    <h2 class="heading-2" id="_idParaDest-622">Using kubectl to create a Docker registry secret</h2>
    <p class="normal">To illustrate, here’s how you can <a id="_idIndexMarker1713"/>create a Docker registry<a id="_idIndexMarker1714"/> secret and integrate it into a Kubernetes pod configuration:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create secret docker-registry my-registry-secret \
  --docker-server=your-registry.com \
  --docker-username=your_username \
  --docker-password=your_password \
  --docker-email=your-email@example.com
</code></pre>
    <p class="normal">Replace <code class="inlineCode">your-registry.com</code>, <code class="inlineCode">your_username</code>, <code class="inlineCode">your_password</code>, and <code class="inlineCode">your-email@example.com</code> with your actual registry details.</p>
    <p class="normal">Update your Pod YAML to use the newly created secret for pulling images from the private registry:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">my-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">my-container</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">your-registry.com/your-image:tag</span>
  <span class="hljs-attr">imagePullSecrets:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">my-registry-secret</span>
</code></pre>
    <p class="normal">Ensure <code class="inlineCode">my-registry-secret</code> matches the name used when creating the Docker registry secret.</p>
    <p class="normal">When Kubernetes creates<a id="_idIndexMarker1715"/> the Pod, the image will be pulled<a id="_idIndexMarker1716"/> from the private registry using the imagePullSecrets as the authentication credentials.</p>
    <p class="normal">Congratulations, you have reached the end of this long chapter about Kubernetes security.</p>
    <h1 class="heading-1" id="_idParaDest-623">Summary</h1>
    <p class="normal">This chapter covered <em class="italic">authentication</em> and <em class="italic">authorization</em> in Kubernetes. First, we provided an overview of the available authentication methods in Kubernetes and explained how you can use ServiceAccount tokens for external user authentication. Next, we focused on RBAC in Kubernetes. You learned how to use Roles, ClusterRoles, RoleBindings, and ClusterRoleBindings to manage authorization in your cluster. We demonstrated a practical use case of RBAC for ServiceAccounts by creating a Pod that can list Pods in the cluster using the Kubernetes API (respecting the principle of least privilege).</p>
    <p class="normal">After that, we learned about Admission Controllers in Kubernetes and what controllers are available to secure your Kubernetes cluster. We also learned about SecurityContext and different samples for securityContext configurations. We also discovered how to control traffic flow between Pods by using an object called NetworkPolicy that behaves like a networking firewall within the cluster. As part of the container security, we explored the alternative container runtimes such as Kata Containers and gVisor options. Finally, we learned how to configure the credentials for the private container registries. In the next chapter, we are going to dive deep into advanced techniques for scheduling Pods.</p>
    <h1 class="heading-1" id="_idParaDest-624">Further reading</h1>
    <ul>
      <li class="bulletList"><strong class="keyWord">Controlling Access to the Kubernetes API</strong>: <a href="https://kubernetes.io/docs/concepts/security/controlling-access"><span class="url">https://kubernetes.io/docs/concepts/security/controlling-access</span></a></li>
      <li class="bulletList"><strong class="keyWord">Managing Service Accounts</strong>: <a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/"><span class="url">https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/</span></a></li>
      <li class="bulletList"><strong class="keyWord">Configure Service Accounts for Pods</strong>: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/"><span class="url">https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/</span></a></li>
      <li class="bulletList"><strong class="keyWord">Certificates and Certificate Signing Requests</strong>: <a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/"><span class="url">https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/</span></a></li>
      <li class="bulletList"><strong class="keyWord">Authorization</strong>: <a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/"><span class="url">https://kubernetes.io/docs/reference/access-authn-authz/authorization/</span></a></li>
      <li class="bulletList"><strong class="keyWord">What is OpenID Connect</strong>: <a href="https://openid.net/developers/how-connect-works/ "><span class="url">https://openid.net/developers/how-connect-works/</span></a></li>
      <li class="bulletList"><strong class="keyWord">Admission controller</strong>: <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers"><span class="url">https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers</span></a></li>
      <li class="bulletList"><strong class="keyWord">Security Context</strong>: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context"><span class="url">https://kubernetes.io/docs/tasks/configure-pod-container/security-context</span></a>/</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-625">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img alt="" src="image/QR_Code119001106479081656.png"/></p>
  </div>
</body></html>