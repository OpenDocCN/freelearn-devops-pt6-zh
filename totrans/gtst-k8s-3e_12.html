<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Cluster Federation and Multi-Tenancy</h1>
                </header>
            
            <article>
                
<p>This chapter will discuss the new federation capabilities and how to use them to manage multiple clusters across cloud providers. We will also cover the federated version of the core constructs. We will walk you through federated Deployments, ReplicaSets, ConfigMaps, and Events.</p>
<p>This chapter will discuss the following topics:</p>
<ul>
<li>Federating clusters</li>
<li>Federating multiple clusters</li>
<li>Inspecting and controlling resources across multiple clusters</li>
<li>Launching resources across multiple clusters</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You'll need to have your Google Cloud Platform account enabled and logged in, or you can use a local Minikube instance of Kubernetes. You can also use Play with Kubernetes over the web: <a href="https://labs.play-with-k8s.com/">https://labs.play-with-k8s.com/</a>. There's also the Katacoda playground at <a href="https://www.katacoda.com/courses/kubernetes/playground">https://www.katacoda.com/courses/kubernetes/playground</a>.</p>
<p>You'll also need GitHub credentials, the setting up of which we'll go over later in this chapter. Here's the GitHub repository for this chapter:<span> <a href="https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter12">https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter12</a></span>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to federation</h1>
                </header>
            
            <article>
                
<p>While federation is still very new in Kubernetes, it lays the groundwork for a highly sought after cross-cloud provider solution. Using federation, we can run multiple Kubernetes clusters on-premises and in one or more public cloud providers and manage applications utilizing the entire set of all our organizational resources.</p>
<p>This begins to create a path for avoiding cloud provider lock-in and highly available deployment that can place application servers in multiple clusters and allow for communication to other services located in single points among our federated clusters. We can improve isolation on outages at a particular provider or geographic location while providing greater flexibility for scaling and utilizing total infrastructure.</p>
<p class="mce-root"><span>Currently, the federation plane supports these resources: ConfigMap, DaemonSets</span><span>,</span> <span>Deployment</span><span>,</span> <span>Events</span><span>,</span> <span>Ingress</span><span>,</span> <span>Namespaces</span><span>,</span> <span>ReplicaSets</span><span>,</span> <span>Secrets</span><span>, and </span><span>Services. Note that federation and its components are in alpha and beta phases of release, so functionality may still be a bit temperamental.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Why federation?</h1>
                </header>
            
            <article>
                
<p>There are several major advantages to taking on Kubernetes cluster federation. As mentioned previously, federation allows you increase the availability and tenancy capabilities of your Kubernetes clusters. By scaling across availability zones or regions of a single <strong>cloud service provider</strong> (<strong>CSP</strong>), or by scaling across multiple CSPs, federation takes the concept of high availability to the next level. Some term this global scheduling, which will could enable you to direct traffic in order to maximize an inexpensive CSP resource that becomes available in the spot market. You could also use global scheduling to relocate workloads cluster to end use populations, improving the performance of your applications.</p>
<p>There is also the opportunity to treat entire clusters as if they were Kubernetes objects, and deal with failure on a per-cluster basis instead of per machine. Cluster federation could allow operators to automatically recover from entire clusters failing by routing traffic to redundant, available clusters.</p>
<p>It should be noted that, while federation increases the potential for high availability on your cluster, it's clear that the significant increase in complexity also lowers your potential reliability if your clusters aren't managed well. You can manage some of this complexity by using a hosted PaaS version of Kubernetes such as GKE, where leaving the cluster management to GCP will drastically lower the operational load on your teams.</p>
<p class="mce-root"/>
<p>Federation can also enable your team to support a hybrid environment, with on-premises clusters pairing with your resources in the cloud. Depending on your traffic routing requirements, this may require additional engineering in the form of a service mesh.</p>
<p>There's a number of technical features that federation supplies, which enable higher potential availability.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The building blocks of federation</h1>
                </header>
            
            <article>
                
<p>Federation makes it easy to manage resources across clusters by providing two distinct types of building blocks. The first is resources and the second is service discovery:</p>
<ul>
<li><strong>Resource synchronization across clusters</strong>: Federation is the glue that allows you to keep track of the many resources needed to run sets of applications. When you're running a lot of applications, with many resources and object types, across many clusters, federation is key to keeping your clusters organized and managed well. You may find yourself needing to keep an application deployment running in multiple clusters with a single pane of glass view.</li>
<li><strong>Multi-cluster service discovery</strong>: There are a number of resources that share well between clusters such as DNS, load balancers, object storage, and ingress. Federation gives you the ability to automatically configure those services with multi-cluster awareness, so you can route application traffic and manage the control plane across several clusters.</li>
</ul>
<p>As we'll learn next, Kubernetes federation is managed by a tool named <kbd>kubefed</kbd>, which has a number of command-line flags that allow you to manage many clusters and the building blocks we discussed previously. The major building blocks of <kbd>kubefed</kbd> that we'll use are as follows:</p>
<ul>
<li><kbd>kubefed init</kbd>: Initialize a federation control plane</li>
<li><kbd>kubefed join</kbd>: Join a cluster to a federation</li>
<li><kbd>kubefed options</kbd>: Print the list of flags inherited by all commands</li>
<li><kbd>kubefed unjoin</kbd>: Unjoin a cluster from a federation</li>
<li><kbd>kubefed version</kbd>: Print the client and server version information<span><br/></span></li>
</ul>
<p>Here's a handy list of the options that can be used:</p>
<pre>      --alsologtostderr                              log to standard error as well as files
      --as string                                    Username to impersonate for the operation
      --as-group stringArray                         Group to impersonate for the operation, this flag can be repeated to specify multiple groups.
      --cache-dir string                             Default HTTP cache directory (default "/Users/jrondeau/.kube/http-cache")
      --certificate-authority string                 Path to a cert file for the certificate authority
      --client-certificate string                    Path to a client certificate file for TLS
      --client-key string                            Path to a client key file for TLS
      --cloud-provider-gce-lb-src-cidrs cidrs        CIDRs opened in GCE firewall for LB traffic proxy &amp; health checks (default 130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16)
      --cluster string                               The name of the kubeconfig cluster to use
      --context string                               The name of the kubeconfig context to use
      --default-not-ready-toleration-seconds int     Indicates the tolerationSeconds of the toleration for notReady:NoExecute that is added by default to every pod that does not already have such a toleration. (default 300)
      --default-unreachable-toleration-seconds int   Indicates the tolerationSeconds of the toleration for unreachable:NoExecute that is added by default to every pod that does not already have such a toleration. (default 300)
  -h, --help                                         help for kubefed
      --insecure-skip-tls-verify                     If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure
      --ir-data-source string                        Data source used by InitialResources. Supported options: influxdb, gcm. (default "influxdb")
      --ir-dbname string                             InfluxDB database name which contains metrics required by InitialResources (default "k8s")
      --ir-hawkular string                           Hawkular configuration URL
      --ir-influxdb-host string                      Address of InfluxDB which contains metrics required by InitialResources (default "localhost:8080/api/v1/namespaces/kube-system/services/monitoring-influxdb:api/proxy")
      --ir-namespace-only                            Whether the estimation should be made only based on data from the same namespace.
      --ir-password string                           Password used for connecting to InfluxDB (default "root")
      --ir-percentile int                            Which percentile of samples should InitialResources use when estimating resources. For experiment purposes. (default 90)
      --ir-user string                               User used for connecting to InfluxDB (default "root")
      --kubeconfig string                            Path to the kubeconfig file to use for CLI requests.
      --log-backtrace-at traceLocation               when logging hits line file:N, emit a stack trace (default :0)
      --log-dir string                               If non-empty, write log files in this directory
      --log-flush-frequency duration                 Maximum number of seconds between log flushes (default 5s)
      --logtostderr                                  log to standard error instead of files (default true)
      --match-server-version                         Require server version to match client version
  -n, --namespace string                             If present, the namespace scope for this CLI request
      --password string                              Password for basic authentication to the API server
      --request-timeout string                       The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default "0")
  -s, --server string                                The address and port of the Kubernetes API server
      --stderrthreshold severity                     logs at or above this threshold go to stderr (default 2)
      --token string                                 Bearer token for authentication to the API server
      --user string                                  The name of the kubeconfig user to use
      --username string                              Username for basic authentication to the API server
  -v, --v Level                                      log level for V logs
      --vmodule moduleSpec                           comma-separated list of pattern=N settings for file-filtered logging</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Here's a high-level diagram that shows what all of these pieces look like when strung together:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/43b385da-a4e9-4371-8d7c-b73bb50833a8.png" width="1950" height="1346"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Key components</h1>
                </header>
            
            <article>
                
<p>There are two key components to the federation capability within Kubernetes. These components make up the federation control plane.</p>
<p>The first is <kbd>federation-controller-manager</kbd>, which embeds the core control loops required to operate federation. <kbd>federation-controller-manager</kbd> watches the state of your clusters via <kbd>apiserver</kbd> and makes changes in order to reach a desired state.</p>
<p>The second is <kbd>federation-apiserver</kbd>, which validates and configures Kubernetes objects such as pods, services, and controllers. <kbd>federation-apiserver</kbd> is the frontend for the cluster through which all other components interact.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Federated services</h1>
                </header>
            
            <article>
                
<p>Now that we have the building blocks of federation conceptualized in our mind, let's review one more facet of this before setting up federation. How exactly does a common service, deployed across multiple clusters, work?</p>
<p>Federated services are created in a very similar fashion to regular services: first, by sending the desired state and properties of the service to an API endpoint, which is then brought to bear by the Kubernetes architecture. There are two main differences:</p>
<ul>
<li style="font-weight: 400">A non-federated service will make an API call directly to a cluster API endpoint</li>
<li style="font-weight: 400">A federated service will make the call to the Federated API endpoint at <kbd>federation/v1beta1</kbd>, which will then redirect the API call to all of the individual clusters within the federation control plane</li>
</ul>
<p>This second type of service allows us to extend such things as DNS service discovery across cluster boundaries. The DNS <kbd>resolv</kbd> chain is able to leverage service federation and public DNS records to resolve names across multiple clusters.</p>
<div class="packt_infobox">The API for a federated service is 100% compatible with regular services.</div>
<p>When a service is created, federation takes care of several things. First, it creates matching services in all clusters where <kbd>kubefed</kbd> specifies they reside. The health of those services is monitored so that traffic can be routed or re-routed to them. Lastly, federation ensure that there's a definitive set<span> </span><span>of public DNS records available through providers such as Route 53 or Google Cloud DNS.</span></p>
<p>Microservices residing on different pods within your Kubernetes clusters will use all of this machinery in order to locate the federated service either within their own cluster or navigate to the nearest healthy example within your federation map.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Setting up federation</h1>
                </header>
            
            <article>
                
<p>While we can use the cluster we had running for the rest of the examples, I would highly recommend that you start fresh. The default naming of the clusters and contexts can be problematic for the federation system. Note that the <kbd>--cluster-context</kbd> and <kbd>--secret-name</kbd> flags are there to help you work around the default naming, but for first-time federation, it can still be confusing and less than straightforward.</p>
<p class="mce-root"/>
<p>Hence, starting fresh is how we will walk through the examples in this chapter. Either use new and separate cloud provider (AWS and/or GCE) accounts or tear down the current cluster and reset your Kubernetes control environment by running the following commands:</p>
<pre><strong>$ kubectl config unset contexts<br/></strong><strong>$ kubectl config unset clusters</strong></pre>
<p>Double-check that nothing is listed using the following commands:</p>
<pre><strong>$ kubectl config get-contexts<br/></strong><strong>$ kubectl config get-clusters</strong></pre>
<p>Next, we will want to get t<span>he</span> <kbd>kubefed</kbd><span> command on our path and make it executable. Navigate back to the folder where you have the Kubernetes download extracted. </span><span>The</span> <kbd>kubefed</kbd><span> command is located in the</span> <kbd>/kubernetes/client/bin</kbd> <span>folder. Run the following commands to get in the <kbd>bin</kbd> folder and change the execution permissions:</span></p>
<pre><strong>$ sudo cp kubernetes/client/bin/kubefed /usr/local/bin</strong><br/><strong>$ sudo chmod +x /usr/local/bin/kubefed</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Contexts</h1>
                </header>
            
            <article>
                
<p>Contexts are used by the Kubernetes control plane to keep authentication and cluster configuration stored for multiple clusters. This allows us to access and manage multiple clusters accessible from the same <kbd>kubectl</kbd>. You can always see the contexts available with the <kbd>get-contexts</kbd> command that we used earlier.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">New clusters for federation</h1>
                </header>
            
            <article>
                
<p>Again, make sure you navigate to wherever Kubernetes was downloaded and move into the <kbd>cluster</kbd> sub-folder:</p>
<pre><strong>$ cd kubernetes/cluster/</strong></pre>
<div class="packt_infobox">Before we proceed, make sure you have the GCE command line and the AWS command line installed, authenticated, and configured. Refer to <a href="446f901d-70fa-4ebe-be8a-0de14248f99c.xhtml">Chapter 1</a>, <em>Introduction to Kubernetes</em>, if you need assistance doing so on a new box.</div>
<p>First, we will create the AWS cluster. Note that we are adding an environment variable named <kbd>OVERRIDE_CONTEXT</kbd>, which will allow us to set the context name to something that complies with the DNS naming standards. DNS is a critical component for federation as it allows us to do cross-cluster discovery and service communication. This is important in a federated world where clusters may be in different data centers and even providers.</p>
<p>Run these commands to create your AWS cluster:</p>
<pre><strong>$ export KUBERNETES_PROVIDER=aws</strong><br/><strong>$ export OVERRIDE_CONTEXT=awsk8s</strong><br/><strong>$ ./kube-up.sh</strong></pre>
<p>Next, we will create a GCE cluster, once again using the<span> </span><kbd>OVERRIDE_CONTEXT</kbd><span> environment variable:</span></p>
<pre><strong>$ export KUBERNETES_PROVIDER=gce<br/>$ export OVERRIDE_CONTEXT=gcek8s</strong><br/><strong>$ ./kube-up.sh</strong></pre>
<p class="CDPAlignLeft CDPAlign">If we take a look at our contexts now, we will notice both <kbd>awsk8s</kbd> and <kbd>gcek8s</kbd>, which we just created. The star in front of <kbd>gcek8s</kbd> denotes that it's where <kbd>kubectl</kbd> is currently pointing and executing against:</p>
<div class="CDPAlignLeft CDPAlign">
<pre><strong>$ kubectl config get-contexts<br/></strong></pre></div>
<p class="CDPAlignLeft CDPAlign">The preceding command should produce something like the following:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="Images/0b5b6aae-59f7-4e76-8f74-5520fcb1570a.png" style="width:44.17em;height:5.50em;" width="586" height="73"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Initializing the federation control plane</h1>
                </header>
            
            <article>
                
<p>Now that we have two clusters, let's set up the federation control plane in the GCE cluster. First, we'll need to make sure that we are in the GCE context, and then we will initialize the federation control plane:</p>
<pre><strong>$ kubectl config use-context gcek8s</strong><br/><strong>$ kubefed init master-control --host-cluster-context=gcek8s --dns-zone-name="mydomain.com" </strong></pre>
<p>The preceding command creates a new context just for federation called <kbd>master-control</kbd>. It uses the <kbd>gcek8s</kbd> cluster/context to host the federation components (such as API server and controller). It assumes GCE DNS as the federation's DNS service. You'll need to update <kbd>dns-zone-name</kbd> with a domain suffix you manage.</p>
<div class="packt_tip">By default, the DNS provider is GCE. You can use <kbd>--dns-provider="aws-route53"</kbd> to set it to AWS <kbd>route53</kbd>; however, out of the box implementation still has issues for many users.</div>
<p class="CDPAlignLeft CDPAlign">If we check our contexts once again, we will now see three contexts:</p>
<div class="CDPAlignLeft CDPAlign">
<pre><strong>$ kubectl config get-contexts<br/></strong></pre></div>
<p class="CDPAlignLeft CDPAlign">The preceding command should produce something like the following:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="Images/32642ab3-67a0-4488-b698-d7283f1a474d.png" width="600" height="86"/></div>
<p class="CDPAlignLeft CDPAlign">Let's make sure we have all of the federation components running before we proceed. The federation control plane uses the <kbd>federation-system</kbd> namespace. Use the <kbd>kubectl get pods</kbd> command with the namespace specified to monitor the progress. Once you see two API server pods and one controller pod, you should be set:</p>
<pre><strong>$ kubectl get pods --namespace=federation-system<br/></strong></pre>
<div class="CDPAlignLeft CDPAlign">
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="Images/07a6f78a-e107-4ca2-95e8-a3a0fc7327a7.png" style="width:43.08em;height:8.67em;" width="589" height="118"/></div>
</div>
<p class="CDPAlignLeft CDPAlign">Now that we have the federation components set up and running, let's switch to that context for the next steps:</p>
<pre><strong>$ kubectl config use-context master-control</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Adding clusters to the federation system</h1>
                </header>
            
            <article>
                
<p>Now that we have our federation control plane, we can add the clusters to the federation system. First, we will join the GCE cluster and then the AWS cluster:</p>
<pre><strong>$ kubefed join gcek8s --host-cluster-context=gcek8s --secret-name=fed-secret-gce</strong><br/><strong>$ kubefed join awsk8s --host-cluster-context=gcek8s --secret-name=fed-secret-aws</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Federated resources</h1>
                </header>
            
            <article>
                
<p>Federated resources allow us to deploy across multiple clusters and/or regions. Currently, version 1.5 of Kubernetes support a number of core resource types in the federation API, including ConfigMap, DaemonSets, Deployment, Events, Ingress, Namespaces, ReplicaSets, Secrets, and Services.</p>
<p>Let's take a look at a federated deployment that will allow us to schedule pods across both AWS and GCE. Save the following file as <kbd>node-js-deploy-fed.yaml</kbd>:</p>
<pre>apiVersion: extensions/v1beta1<br/>kind: Deployment<br/>metadata:<br/>  name: node-js-deploy<br/>  labels:<br/>    name: node-js-deploy<br/>spec:<br/>  replicas: 3<br/>  template:<br/>    metadata:<br/>      labels:<br/>        name: node-js-deploy<br/>    spec: <br/>      containers: <br/>      - name: node-js-deploy <br/>        image: jonbaier/pod-scaling:latest <br/>        ports: <br/>        - containerPort: 80</pre>
<p class="mce-root"/>
<p>Create this deployment with the following command:</p>
<pre><strong>$ kubectl create -f node-js-deploy-fed.yaml</strong></pre>
<p>Now, let's try listing the pods from this deployment:</p>
<pre><strong>$ kubectl get pods</strong></pre>
<div class="CDPAlignCenter"><img src="Images/ac71ec43-457f-4b34-9e3e-6ed440646a9b.png" width="386" height="28"/></div>
<p> </p>
<p>We should see a message like the preceding one depicted. This is because we are still using <kbd>master-control</kbd> or federation context, which does not itself run pods. We will, however, see the deployment in the federation plane and, if we inspect the events, we will see that the deployment was in fact created on both of our federated clusters:</p>
<pre><strong>$ kubectl get deployments<br/>$ kubectl describe deployments node-js-deploy<br/></strong></pre>
<p>We should see something like the following. Notice that the <kbd>Events:</kbd> section shows deployments in both our GCE and AWS contexts:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="Images/0fab6b6c-e740-47d0-929e-a70d4f55332e.png" width="632" height="308"/></div>
<p>We can also see the federated events using the following command:</p>
<pre><strong>$ kubectl get events</strong></pre>
<div>
<div class="CDPAlignCenter CDPAlign"><img src="Images/483ab423-3cac-42ae-8b8b-67473995cca6.png" width="671" height="150"/></div>
<p>It may take a moment for all three pods to run. Once that happens, we can switch to each cluster context and see some of the pods on each. Note that we can now use <kbd>get pods</kbd> since we are on the individual clusters and not on the control plane:</p>
<pre><strong>$ kubectl config use-context awsk8s</strong><br/><strong>$ kubectl get pods</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img src="Images/cf4f6def-7210-4221-adc9-85f5163b6ddd.png" style="width:43.17em;height:4.17em;" width="570" height="55"/></div>
<pre><strong>$ kubectl config use-context gcek8s</strong><br/><strong>$ kubectl get pods</strong></pre></div>
<div class="CDPAlignCenter CDPAlign"><img src="Images/52d8853d-7773-4e58-af50-a1dd633d8666.png" style="width:42.83em;height:5.17em;" width="572" height="69"/></div>
<p>We should see the three pods spread across the clusters with two on one and a third on the other. Kubernetes has spread them across the cluster without any manual intervention. Any pods that fail will be restarted, but now we have the added redundancy of two cloud providers.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Federated configurations</h1>
                </header>
            
            <article>
                
<p>In modern software development, it is common to separate configuration variables from the application code itself. In this way, it is easier to make updates to service URLs, credentials, common paths, and so on. Having these values in external configuration files means we can easily update configuration without rebuilding the entire application.</p>
<p>This separation solves the initial problem, but true portability comes when you can remove the dependency from the application completely. Kubernetes offers a configuration store for exactly this purpose. ConfigMaps are simple constructs that store key-value pairs. </p>
<div class="packt_infobox">Kubernetes also supports Secrets for more sensitive configuration data. This will be covered in more detail in <a href="a26334cc-d455-4eaf-9e0e-6d216848a690.xhtml">Chapter 10</a>, <em>Cluster Authentication, Authorization, and Container Security</em>. You can use the example there in both single clusters or on the federation control plane as we are demonstrating with ConfigMaps here.</div>
<p>Let's take a look at an example that will allow us to store some configuration and then consume it in various pods. The following listings will work for both federated and single clusters, but we will continue using a federated setup for this example.</p>
<p>The <kbd>ConfigMap</kbd> kind can be created using literal values, flat files and directories, and finally YAML definition files. The following listing is a YAML definition of the <kbd>configmap-fed.yaml</kbd> file:</p>
<pre>apiVersion: v1<br/>kind: ConfigMap<br/>metadata:<br/>  name: my-application-config<br/>  namespace: default<br/>data:<br/>  backend-service.url: my-backend-service</pre>
<p>Let's first switch back to our federation plane:</p>
<pre><strong>$ kubectl config use-context master-control</strong></pre>
<p>Now, create this listing with the following command:</p>
<pre><strong>$ kubectl create -f configmap-fed.yaml</strong></pre>
<p>Let's display the <kbd>configmap</kbd> object that we just created. The <kbd>-o yaml</kbd> flag helps us to display the full information: </p>
<pre><strong>$ kubectl get configmap my-application-config -o yaml</strong></pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="Images/83de3392-cff3-4276-8c11-9e8b7de0f327.png" style="width:46.75em;height:15.25em;" width="598" height="195"/></div>
<p>Now that we have a <kbd>ConfigMap</kbd> object, let's start up a federated <kbd>ReplicaSet</kbd> that can use the <kbd>ConfigMap</kbd> object. This will create replicas of pods across our cluster that can access the <kbd>ConfigMap</kbd> object. <kbd>ConfigMaps</kbd><strong> </strong>can be accessed via environment variables or mount volumes. This example will use a mount volume that provides a folder hierarchy and the files for each key with the contents representing the values. Save the following file as <kbd>configmap-rs-fed.yaml</kbd>:</p>
<pre>apiVersion: extensions/v1beta1<br/>kind: ReplicaSet<br/>metadata:<br/>  name: node-js-rs<br/>spec:<br/>  replicas: 3<br/>  selector:<br/>    matchLabels:<br/>      name: node-js-configmap-rs<br/>  template:<br/>    metadata:<br/>      labels:<br/>        name: node-js-configmap-rs<br/>    spec:<br/>      containers:<br/>      - name: configmap-pod<br/>        image: jonbaier/node-express-info:latest<br/>        ports:<br/>        - containerPort: 80<br/>          name: web<br/>        volumeMounts:<br/>        - name: configmap-volume<br/>          mountPath: /etc/config</pre>
<pre>volumes:<br/>      - name: configmap-volume<br/>        configMap:<br/>          name: my-application-config</pre>
<p>Create this pod with <kbd>kubectl create -f configmap-rs-fed.yaml</kbd>. After creation, we will need to switch contexts to one of the clusters where the pods are running. You can choose either, but we will use the GCE context here:</p>
<pre><strong>$ kubectl config use-context gcek8s</strong></pre>
<p>Now that we are on the GCE cluster specifically, let's check <kbd>configmaps</kbd> here:</p>
<pre><strong>$ kubectl get configmaps</strong></pre>
<p>As you can see, the <kbd>ConfigMap</kbd> is propagated locally to each cluster. Next, let's find a pod from our federated <kbd>ReplicaSet</kbd>:</p>
<pre><strong>$ kubectl get pods</strong></pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="Images/3dd6ab65-c77d-4d67-b50b-28dc5a464726.png" width="582" height="104"/></div>
<p>Let's take one of the <kbd>node-js-rs</kbd> pod names from the listing and run a bash shell with <kbd>kubectl exec</kbd>:</p>
<pre><strong>$ kubectl exec -it node-js-rs-6g7nj bash</strong></pre>
<p>Then, let's change directories to the <kbd>/etc/config</kbd> folder that we set up in the pod definition. Listing this directory reveals a single file with the name of the <kbd>ConfigMap</kbd> we defined earlier:</p>
<pre><strong>$ cd /etc/config</strong><br/><strong>$ ls</strong></pre>
<p>If we then display the contents of the files with the following command, we should see the value we entered earlier, <kbd>my-backend-service</kbd>:</p>
<pre><strong>$ echo $(cat backend-service.url)</strong></pre>
<p>If we were to look in any of the pods across our federated cluster, we would see the same values. This is a great way to decouple configuration from an application and distribute it across our fleet of clusters.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Federated horizontal pod autoscalers</h1>
                </header>
            
            <article>
                
<p>Let's look at another example of a newer resource that you can use with the federated model: <strong>horizontal pod autoscalers</strong> (<strong>HPAs</strong>).  </p>
<p>Here's what the architecture of these looks like in a single cluster:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/79f2b93f-cefb-44aa-abb8-b1f93733bd4a.png" style="width:24.75em;height:22.75em;" width="395" height="363"/></p>
<div class="packt_infobox"><span class="packt_screen">Credit</span>: <strong><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#how-does-the-horizontal-pod-autoscaler-work">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#how-does-the-horizontal-pod-autoscaler-work</a></strong>.</div>
<p>These HPAs will act in a similar fashion to normal HPAs, with the same functionality and same API-based compatibility—only, with federation, the management will traverse your clusters. This is an alpha feature, so it is not enabled by default on your cluster. In order to enable it, you'll need to run <kbd>federation-apiserver</kbd> with the <kbd>--runtime-config=api/all=true</kbd> option. Currently, the only metrics that work to manage HPAs are CPU utilization metrics.</p>
<p class="mce-root"/>
<p>First, let's create a file that contains the HPA configuration, called <kbd>node-hpa-fed.yaml</kbd>:</p>
<pre>apiVersion: autoscaling/v1<br/>kind: HorizontalPodAutoscaler<br/>metadata:<br/> name: nodejs<br/> namespace: default<br/>spec:<br/> scaleTargetRef:<br/>   apiVersion: apps/v1beta1   kind: Deployment<br/>name: nodejs<br/> minReplicas: 5<br/> maxReplicas: 20<br/> targetCPUUtilizationPercentage: 70</pre>
<p>We can add this to our cluster with the following command:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>kubectl --context=federation-cluster create -f node-hpa-fed.yaml</strong></pre>
<p>In this case, <kbd>--<span>context=federation-cluster</span></kbd> is telling <kbd>kubectl</kbd> to send the request to <kbd>federation-apiserver</kbd> instead of <kbd>kube-apiserver</kbd>.</p>
<p>If, for example, you wanted to restrict this HPA to a subset of your Kubernetes clusters, you can use cluster selectors to restrict the federated object by using the <kbd>federation.alpha.kubernetes.io/cluster-selector</kbd> annotation. It's similar in function to nodeSelector, but acts upon full Kubernetes clusters. Cool! You'll need to create an annotation in JSON format. Here's a specific example of a ClusterSelector annotation:</p>
<pre>metadata:<br/>  annotations:<br/>     federation.alpha.kubernetes.io/cluster-selector: '[{"key": "hipaa", "operator":<br/>       "In", "values": ["true"]}, {"key": "environment", "operator": "NotIn", "values": ["nonprod"]}]'</pre>
<p>This example is going to keep workloads with the <kbd>hipaa</kbd> label out of environments with the <kbd>nonprod</kbd> label.</p>
<div class="packt_tip">For a full list of Top Level Federation API objects, see the following: <a href="https://kubernetes.io/docs/reference/federation/">https://kubernetes.io/docs/reference/federation/</a></div>
<p>You can check your clusters to see whether the HPA was created in an individual location by specifying the context:</p>
<pre><strong>kubectl --context=gce-cluster-01 get HPA nodejs</strong></pre>
<p>Once you're finished with the HPA, it can be deleted with the following <kbd>kubectl</kbd> command:</p>
<pre><strong>kubectl --context=federation-cluster delete HPA nodejs</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to use federated HPAs</h1>
                </header>
            
            <article>
                
<p>HPAs used in the previous manner are an essential tool for ensuring that your clusters scale up as their workloads increase. The default behavior for HPA spreading in clusters ensure that maximum replicas are spread evenly first in all clusters. Let's say that you have 10 registered Kubernetes clusters in your federation control plane. If you have <kbd>spec.maxReplicas = 30</kbd>, each of the clusters will receive the following HPA <kbd>spec</kbd>:</p>
<pre class="mce-root">spec.maxReplicas = 10</pre>
<p class="mce-root">If you were to then set <kbd>spec.minReplicas = 5</kbd>, then some of the clusters will receive the following:</p>
<pre>spec.minReplicas = 1</pre>
<p>This is due to being unable to have a replica sum of 0. It's important to note that federation manipulates the minx/mix replicas it creates on the federated clusters, not by directly monitoring the target object metrics (in our case, CPU). The federated HPA controller is relying on HPAs within the federated cluster to monitor CPU utilization, which then makes changes to specs such as current and desired replicas.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Other federated resources</h1>
                </header>
            
            <article>
                
<p><span>So far, we have seen federated Deployments, ReplicaSets, Events, and ConfigMaps in action. </span><span>DaemonSets, Ingress, Namespaces, Secrets, and Services are also supported. </span>Your specific setup will vary and you may have a set of clusters that differ from our example here. As mentioned earlier, these resources are still in beta, so it's worth spending some time to experiment with the various resource types and understand how well the federation constructs are supported for your particular mix of infrastructure.</p>
<p>Let's look at some examples that we can use to leverage other common Kubernetes API objects from a federated perspective.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Events</h1>
                </header>
            
            <article>
                
<p>If you want to see what events are only stored in the federation control plane, you can use the following command:</p>
<pre><strong>kubectl --context=federation-cluster get events</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Jobs</h1>
                </header>
            
            <article>
                
<p>When you go to create a job, you'll use similar concepts as before. Here's what that looks like when you create a job within the federation context:</p>
<pre><strong>kubectl --context=federation-cluster create -f fedjob.yaml</strong></pre>
<p>You can get the list of these jobs within the federated context with the following:</p>
<pre><strong>kubectl --context=gce-cluster-01 get job fedjob</strong></pre>
<p>As with HPAs, you can spread your jobs across multiple underlying clusters with the appropriate specs. The relevant definitions are <kbd>spec.parallelism</kbd> and <kbd>spec.completions</kbd>, and they can be modified by specifying the correct <kbd>ReplicaAllocationPreferences</kbd> with the <kbd>federation.kubernetes.io/job-preferences</kbd> key.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">True multi-cloud</h1>
                </header>
            
            <article>
                
<p>This is an exciting space to watch. As it grows, it gives us a really good start to doing multi-cloud implementations and providing redundancy across regions, data centers, and even cloud providers. </p>
<p>While Kubernetes does provide an easy and exciting path to multi-cloud infrastructure, it's important to note that production multi-cloud requires much more than distributed deployments. A full set of capabilities from logging and monitoring to compliance and host-hardening, there is much to manage in a multi-provider setup. </p>
<p>True multi-cloud adoption will require a well-planned architecture, and Kubernetes takes a big step forward in pursuing this goal.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting to multi-cloud</h1>
                </header>
            
            <article>
                
<p>In this exercise, we're going to unite two clusters using Istio's multi-cloud feature. Normally, we'd create two clusters from scratch, across two CSPs, but for the purposes of exploring one single isolated concept at a time, we're going to use the GKE to spin up our clusters, so we can focus on the inner workings of Istio's multi-cloud functionality.</p>
<p>Let's get started by logging in to your Google Cloud Project! First, you'll want to create a project in the GUI called <kbd>gsw-k8s-3</kbd>, if you haven't already, and get your Google Cloud Shell to point to it. If you're already pointed at your GCP account, you can disregard that.</p>
<p>Click this button for an easy way to get access to the CLI tools:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/f5d89e61-5ecf-4189-9d5c-9dbf599023e6.png" width="987" height="102"/></p>
<p>Once you've launched the shell, you can point it to your project:</p>
<pre><strong>anonymuse@cloudshell:~$ gcloud config set project gsw-k8s-3</strong><br/><strong><span>Updated property [core/project].<br/></span><span>anonymuse@cloudshell:~ (gsw-k8s-3)$</span></strong></pre>
<p>Next, we'll set up an environment variable for the project ID, which can echo back to see:</p>
<pre><strong>anonymuse@cloudshell:~ (gsw-k8s-3)$ proj=$(gcloud config list --format='value(core.project)')</strong><br/><strong>anonymuse@cloudshell:~ (gsw-k8s-3)$ echo $proj</strong><br/><strong>Gsw-k8s-3</strong></pre>
<p>Now, let's create some clusters. Set some variables for the zone and cluster name:</p>
<pre>zone="us-east1-b"<br/>cluster="cluster-1"</pre>
<p>First, create cluster one:</p>
<pre><strong>gcloud container clusters create $cluster --zone $zone --username "</strong><br/><strong> --cluster-version "1.10.6-gke.2" --machine-type "n1-standard-2" --image-type "COS" --disk-size "100" \</strong><br/><strong> --scopes gke-default \</strong><br/><strong> --num-nodes "4" --network "default" --enable-cloud-logging --enable-cloud-monitoring --enable-ip-alias --async</strong><br/><br/><strong>WARNING: Starting in 1.12, new clusters will not have a client certificate issued. You can manually enable (or disable) the issuance of the client certificate using the `--[no-]issue-client-certificate` flag. This will enable the autorepair feature for nodes. Please see https://cloud.google.com/kubernetes-engine/docs/node-auto-repair for more information on node autorepairs.</strong><br/><br/><strong>WARNING: Starting in Kubernetes v1.10, new clusters will no longer get compute-rw and storage-ro scopes added to what is specified in --scopes (though the latter will remain included in the default --scopes). To use these scopes, add them explicitly to --scopes. To use the new behavior, set container/new_scopes_behavior property (gcloud config set container/new_scopes_behavior true).</strong><br/><br/><strong>NAME       TYPE LOCATION    TARGET STATUS_MESSAGE  STATUS START_TIME  END_TIME</strong><br/><strong>cluster-1        us-east1-b                   PROVISIONING</strong></pre>
<div class="packt_infobox">You may need to change the cluster version to a newer GKE version as updates are made. Older versions become unsupported over time. For example, you might see a message such as this:<br/>
<br/>
<span><kbd>ERROR: (gcloud.container.clusters.create) ResponseError: code=400, message=EXTERNAL: Master version "1.9.6-gke.1" is unsupported.<br/>
<br/></kbd></span> You can check this web page to find out the currently supported version of GKE: <a href="https://cloud.google.com/kubernetes-engine/release-notes">https://cloud.google.com/kubernetes-engine/release-notes</a>.</div>
<p>Next, specify <kbd>cluster-2</kbd>:</p>
<pre>cluster="cluster-2"</pre>
<p>Now, create it, where you'll see messages above. We'll omit them this time around:</p>
<pre><strong>gcloud container clusters create $cluster --zone $zone --username "admin" \</strong><br/><strong>--cluster-version "1.10.6-gke.2" --machine-type "n1-standard-2" --image-type "COS" --disk-size "100" \</strong><br/><strong> --scopes gke-default \</strong><br/><strong> --num-nodes "4" --network "default" --enable-cloud-logging --enable-cloud-monitoring --enable-ip-alias --async</strong></pre>
<p>You'll see the same messaging above. You can create another Google Cloud Shell window by clicking on the <strong>+</strong> icon in order to create some <kbd>watch</kbd> commands to see the clusters created. Take a minute to do this while the instances are created:</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img src="Images/4f244410-3414-4381-8ba5-8ae26af26fd9.png" width="815" height="257"/></p>
<p>In that window, launch this command: <kbd>gcloud container clusters list</kbd>. You should see the following:</p>
<pre><strong>gcloud container clusters list</strong><br/><strong>&lt;snip&gt;</strong><br/><strong>Every 1.0s: gcloud container clusters list                                     cs-6000-devshell-vm-375db789-dcd6-42c6-b1a6-041afea68875: Mon Sep 3 12:26:41 2018</strong><br/><br/><strong>NAME       LOCATION MASTER_VERSION  MASTER_IP MACHINE_TYPE  NODE_VERSION NUM_NODES STATUS</strong><br/><strong>cluster-1  us-east1-b 1.10.6-gke.2    35.237.54.93 n1-standard-2  1.10.6-gke.2 4 RUNNING</strong><br/><strong>cluster-2  us-east1-b 1.10.6-gke.2    35.237.47.212 n1-standard-2  1.10.6-gke.2 4 RUNNING</strong></pre>
<p>On the dashboard, it'll look like so:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/1b410eee-aa4e-44ce-93f4-6250185c2c0d.png" width="1177" height="335"/></p>
<p>Next up, we'll grab the cluster credentials. This command will allow us to set a <kbd>kubeconfig</kbd> context for each specific cluster:</p>
<pre><strong>for clusterid in cluster-1 cluster-2; do gcloud container clusters get-credentials $clusterid --zone $zone; done</strong><br/><strong>Fetching cluster endpoint and auth data.</strong><br/><strong>kubeconfig entry generated for cluster-1.</strong><br/><strong>Fetching cluster endpoint and auth data.</strong><br/><strong>kubeconfig entry generated for cluster-2.</strong></pre>
<p>Let's ensure that we can use <kbd>kubectl</kbd> to get the context for each cluster:</p>
<pre><strong>anonymuse@cloudshell:~ (gsw-k8s-3)$ kubectl config use-context "gke_${proj}_${zone}_cluster-1"</strong><br/><strong>Switched to context "gke_gsw-k8s-3_us-east1-b_cluster-1".</strong></pre>
<p>If you then run <kbd>kubectl get pods --all-namespaces</kbd> after executing each of the cluster context switches, you should see something similar to this for each cluster:</p>
<pre><strong>anonymuse@cloudshell:~ (gsw-k8s-3)$ kubectl get pods --all-namespaces</strong><br/><strong>NAMESPACE NAME READY STATUS RESTARTS AGE</strong><br/><strong>kube-system event-exporter-v0.2.1-5f5b89fcc8-2qj5c 2/2 Running 0 14m</strong><br/><strong>kube-system fluentd-gcp-scaler-7c5db745fc-qxqd4 1/1 Running 0 13m</strong><br/><strong>kube-system fluentd-gcp-v3.1.0-g5v24 2/2 Running 0 13m</strong><br/><strong>kube-system fluentd-gcp-v3.1.0-qft92 2/2 Running 0 13m</strong><br/><strong>kube-system fluentd-gcp-v3.1.0-v572p 2/2 Running 0 13m</strong><br/><strong>kube-system fluentd-gcp-v3.1.0-z5wjs 2/2 Running 0 13m</strong><br/><strong>kube-system heapster-v1.5.3-5c47587d4-4fsg6 3/3 Running 0 12m</strong><br/><strong>kube-system kube-dns-788979dc8f-k5n8c 4/4 Running 0 13m</strong><br/><strong>kube-system kube-dns-788979dc8f-ldxsw 4/4 Running 0 14m</strong><br/><strong>kube-system kube-dns-autoscaler-79b4b844b9-rhxdt 1/1 Running 0 13m</strong><br/><strong>kube-system kube-proxy-gke-cluster-1-default-pool-e320df41-4mnm 1/1 Running 0 13m</strong><br/><strong>kube-system kube-proxy-gke-cluster-1-default-pool-e320df41-536s 1/1 Running 0 13m</strong><br/><strong>kube-system kube-proxy-gke-cluster-1-default-pool-e320df41-9gqj 1/1 Running 0 13m</strong><br/><strong>kube-system kube-proxy-gke-cluster-1-default-pool-e320df41-t4pg 1/1 Running 0 13m</strong><br/><strong>kube-system l7-default-backend-5d5b9874d5-n44q7 1/1 Running 0 14m</strong><br/><strong>kube-system metrics-server-v0.2.1-7486f5bd67-h9fq6 2/2 Running 0 13m</strong></pre>
<p>Next up, we're going to need to create a Google Cloud firewall rule so each cluster can talk to the other. We're going to need to gather all cluster networking data (tags and CIDR), and then create firewall rules with <kbd>gcloud</kbd>. The CIDR ranges will look something like this:</p>
<pre><strong>anonymuse@cloudshell:~ (gsw-k8s-3)$ gcloud container clusters list --format='value(clusterIpv4Cidr)'</strong><br/><strong>10.8.0.0/14</strong><br/><strong>10.40.0.0/14</strong></pre>
<p>The tags will be per-node, resulting in eight total tags:</p>
<pre><strong>anonymuse@cloudshell:~ (gsw-k8s-3)$ gcloud compute instances list --format='value(tags.items.[0])'</strong><br/><strong>gke-cluster-1-37037bd0-node</strong><br/><strong>gke-cluster-1-37037bd0-node</strong><br/><strong>gke-cluster-1-37037bd0-node</strong><br/><strong>gke-cluster-1-37037bd0-node</strong><br/><strong>gke-cluster-2-909a776f-node</strong><br/><strong>gke-cluster-2-909a776f-node</strong><br/><strong>gke-cluster-2-909a776f-node</strong><br/><strong>gke-cluster-2-909a776f-node</strong></pre>
<p>Let's run the full command now to create the firewall rules. Note the <kbd>join_by</kbd> function is a neat hack that allows us to join multiple elements of an array in Bash:</p>
<pre><strong>function join_by { local IFS="$1"; shift; echo "$*"; }</strong><br/><strong>ALL_CLUSTER_CIDRS=$(gcloud container clusters list --format='value(clusterIpv4Cidr)' | sort | uniq)</strong><br/><strong>echo $ALL_CLUSTER_CDIRS</strong><br/><strong>ALL_CLUSTER_CIDRS=$(join_by , $(echo "${ALL_CLUSTER_CIDRS}"))</strong><br/><strong>echo $ALL_CLUSTER_CDIRS</strong><br/><strong>ALL_CLUSTER_NETTAGS=$(gcloud compute instances list --format='value(tags.items.[0])' | sort | uniq)</strong><br/><strong>echo $ALL_CLUSTER_NETTAGS</strong><br/><strong>ALL_CLUSTER_NETTAGS=$(join_by , $(echo "${ALL_CLUSTER_NETTAGS}"))</strong><br/><strong>echo $ALL_CLUSTER_NETTAGS</strong><br/><strong>gcloud compute firewall-rules create istio-multicluster-test-pods \</strong><br/><strong> --allow=tcp,udp,icmp,esp,ah,sctp \</strong><br/><strong> --direction=INGRESS \</strong><br/><strong> --priority=900 \</strong><br/><strong> --source-ranges="${ALL_CLUSTER_CIDRS}" \</strong><br/><strong> --target-tags="${ALL_CLUSTER_NETTAGS}"</strong> </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>That will set up our security firewall rules, which should look similar to this in the GUI when complete:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/275978f9-b5df-4c14-8847-4a8b943bceac.png" width="1222" height="814"/></p>
<p>Let's create an admin role that we can use in future steps. First, set <kbd>KUBE_USER</kbd> to the email address associated with your GCP account with <kbd>KUBE_USER="&lt;YOUR_EMAIL&gt;"</kbd>. Next, we'll create a <kbd>clusterrolebinding</kbd>:</p>
<pre class="mce-root"><strong>kubectl create clusterrolebinding gke-cluster-admin-binding \</strong><br/><strong> --clusterrole=cluster-admin \</strong><br/><strong> --user="${KUBE_USER}"</strong><br/><strong>clusterrolebinding "gke-cluster-admin-binding" created</strong></pre>
<p class="mce-root">Next up, we'll install the Istio control plane with Helm, create a namespace, and deploy Istio using a chart.</p>
<p class="mce-root"/>
<p class="mce-root">Check to make sure you're using <kbd>cluster-1</kbd> as your context with <kbd>kubectl config current-context</kbd>. Next, we'll install Helm with these commands:</p>
<pre class="mce-root"><strong>curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get &gt; get_helm.sh</strong><br/><strong> chmod 700 get_helm.sh</strong><br/><strong>./get_helm.sh</strong><br/><strong>Create a role for tiller to use. Youll need to clone the Istio repo first:</strong><br/><strong>git clone https://github.com/istio/istio.git &amp;&amp; cd istio</strong><br/><strong>Now, create a service account for tiller.</strong><br/><strong>kubectl apply -f install/kubernetes/helm/helm-service-account.yaml</strong><br/><strong>And then we can intialize Tiller on the cluster.</strong><br/><strong>/home/anonymuse/.helm</strong><br/><strong>Creating /home/anonymuse/.helm/repository</strong><br/><strong>...</strong><br/><strong>To prevent this, run `helm init` with the --tiller-tls-verify flag.</strong><br/><strong>For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation</strong><br/><strong>Happy Helming!</strong><br/><strong>anonymuse@cloudshell:~/istio (gsw-k8s-3)$</strong></pre>
<p class="mce-root">Now, switch to another, Istio-specific context where we'll install Istio in its own namespace:</p>
<pre class="mce-root"><strong>kubectl config use-context "gke_${proj}_${zone}_cluster-1"</strong></pre>
<p class="mce-root">Copy over the installation chart for Istio into our home directory:</p>
<pre class="mce-root"><strong>helm template install/kubernetes/helm/istio --name istio --namespace istio-system &gt; $HOME/istio_master.yaml</strong></pre>
<p class="mce-root">Create a namespace for it to be used in, install it, and enable injection:</p>
<pre class="mce-root"><strong>kubectl create ns istio-system \</strong><br/><strong> &amp;&amp; kubectl apply -f $HOME/istio_master.yaml \</strong><br/><strong> &amp;&amp; kubectl label namespace default istio-injection=enabled</strong></pre>
<p class="mce-root">We'll now set some more environment variables to collect the IPs of our pilot, statsD, policy, and telemetry pods:</p>
<pre class="mce-root"><strong>export PILOT_POD_IP=$(kubectl -n istio-system get pod -l istio=pilot -o jsonpath='{.items[0].status.podIP}')</strong><br/><strong>export POLICY_POD_IP=$(kubectl -n istio-system get pod -l istio=mixer -o jsonpath='{.items[0].status.podIP}')</strong><br/><strong>export STATSD_POD_IP=$(kubectl -n istio-system get pod -l istio=statsd-prom-bridge -o jsonpath='{.items[0].status.podIP}')</strong><br/><strong>export TELEMETRY_POD_IP=$(kubectl -n istio-system get pod -l istio-mixer-type=telemetry -o jsonpath='{.items[0].status.podIP}')</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We can now generate a manifest for our remote cluster, <kbd>cluster-2</kbd>:</p>
<pre class="mce-root"><strong>helm template install/kubernetes/helm/istio-remote --namespace istio-system \</strong><br/><strong> --name istio-remote \</strong><br/><strong> --set global.remotePilotAddress=${PILOT_POD_IP} \</strong><br/><strong> --set global.remotePolicyAddress=${POLICY_POD_IP} \</strong><br/><strong> --set global.remoteTelemetryAddress=${TELEMETRY_POD_IP} \</strong><br/><strong> --set global.proxy.envoyStatsd.enabled=true \</strong><br/><strong> --set global.proxy.envoyStatsd.host=${STATSD_POD_IP} &gt; $HOME/istio-remote.yaml</strong></pre>
<p class="mce-root">Now, we'll instill the minimal Istio components and sidecar inject in our target, <kbd>cluster-2</kbd>. Run the following commands in order:</p>
<pre class="mce-root"><strong>kubectl config use-context "gke_${proj}_${zone}_cluster-2"</strong><br/><strong>kubectl create ns istio-system</strong><br/><strong>kubectl apply -f $HOME/istio-remote.yaml</strong><br/><strong>kubectl label namespace default istio-injection=enabled</strong></pre>
<p class="mce-root">Now, we'll create more scaffolding to take advantage of the features of Istio. We'll need to create a file in which we can configure <kbd>kubeconfig</kbd> to work with Istio. First, change back into your home directory with <kbd>cd</kbd>. The <kbd>--minify</kbd> flag will ensure that you only see output associated with your current context. Now, enter the following groups of commands:</p>
<pre class="mce-root"><strong>export WORK_DIR=$(pwd)</strong><br/><strong>CLUSTER_NAME=$(kubectl config view --minify=true -o "jsonpath={.clusters[].name}")</strong><br/><strong>CLUSTER_NAME="${CLUSTER_NAME##*_}"</strong><br/><strong>export KUBECFG_FILE=${WORK_DIR}/${CLUSTER_NAME}</strong><br/><strong>SERVER=$(kubectl config view --minify=true -o "jsonpath={.clusters[].cluster.server}")</strong><br/><strong>NAMESPACE=istio-system</strong><br/><strong>SERVICE_ACCOUNT=istio-multi</strong><br/><strong>SECRET_NAME=$(kubectl get sa ${SERVICE_ACCOUNT} -n ${NAMESPACE} -o jsonpath='{.secrets[].name}')</strong><br/><strong>CA_DATA=$(kubectl get secret ${SECRET_NAME} -n ${NAMESPACE} -o "jsonpath={.data['ca\.crt']}")</strong><br/><strong>TOKEN=$(kubectl get secret ${SECRET_NAME} -n ${NAMESPACE} -o "jsonpath={.data['token']}" | base64 --decode)</strong></pre>
<p>Create a file with the following <kbd>cat</kbd> command. This will inject the contents here into a file that's going to be located in <kbd>~/${WORK_DIR}/{CLUSTER_NAME}</kbd>:</p>
<pre class="mce-root"><strong>cat &lt;&lt;EOF &gt; ${KUBECFG_FILE}</strong><br/><strong>apiVersion: v1</strong><br/><strong>clusters:</strong><br/><strong> - cluster:</strong><br/><strong>     certificate-authority-data: ${CA_DATA}</strong><br/><strong>     server: ${SERVER}</strong><br/><strong>   name: ${CLUSTER_NAME}</strong><br/><strong>contexts:</strong><br/><strong>   - context:</strong><br/><strong>       cluster: ${CLUSTER_NAME}</strong><br/><strong>       user: ${CLUSTER_NAME}</strong><br/><strong>   name: ${CLUSTER_NAME}</strong><br/><strong>current-context: ${CLUSTER_NAME}</strong><br/><strong>kind: Config</strong><br/><strong>preferences: {}</strong><br/><strong>users:</strong><br/><strong>   - name: ${CLUSTER_NAME}</strong><br/><strong>     user:</strong><br/><strong>       token: ${TOKEN}</strong><br/><strong>EOF</strong></pre>
<p>Next up, we'll create a secret so that the control plane for Istio that exists on <kbd>cluster-1</kbd> can access <kbd>istio-pilot</kbd> on <kbd>cluster-2</kbd>. Switch back to the first cluster, create a Secret, and label it:</p>
<pre><strong>anonymuse@cloudshell:~ (gsw-k8s-3)$ kubectl config use-context gke_gsw-k8s-3_us-east1-b_cluster-1</strong><br/><strong>Switched to context "gke_gsw-k8s-3_us-east1-b_cluster-1".</strong><br/><strong>kubectl create secret generic ${CLUSTER_NAME} --from-file ${KUBECFG_FILE} -n ${NAMESPACE}</strong><br/><strong>kubectl label secret ${CLUSTER_NAME} istio/multiCluster=true -n ${NAMESPACE}</strong></pre>
<p>Once we've completed these tasks, let's use all of this machinery to deploy one of Google's code examples, <kbd>bookinfo</kbd>, across both clusters. Run this on the first:</p>
<pre><strong>kubectl config use-context "gke_${proj}_${zone}_cluster-1"</strong><br/><strong><span>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml<br/></span><span>kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml<br/></span><span>kubectl delete deployment reviews-v3</span></strong></pre>
<p><span>Now, create a file called <kbd>reviews-v3.yaml</kbd> for deploying <kbd>bookinfo</kbd> to the remote cluster. The file contents can be found in the repository directory of this chapter:</span></p>
<pre>##################################################################################################<br/># Ratings service<br/>##################################################################################################<br/>apiVersion: v1<br/>kind: Service<br/>metadata:<br/> name: ratings<br/> labels:<br/> app: ratings<br/>spec:<br/> ports:<br/> - port: 9080<br/> name: http<br/>---<br/>##################################################################################################<br/># Reviews service<br/>##################################################################################################<br/>apiVersion: v1<br/>kind: Service<br/>metadata:<br/> name: reviews<br/> labels:<br/> app: reviews<br/>spec:<br/> ports:<br/> - port: 9080<br/> name: http<br/> selector:<br/> app: reviews<br/>---<br/>apiVersion: extensions/v1beta1<br/>kind: Deployment<br/>metadata:<br/> name: reviews-v3<br/>spec:<br/> replicas: 1<br/> template:<br/> metadata:<br/> labels:<br/> app: reviews<br/> version: v3<br/> spec:<br/> containers:<br/> - name: reviews<br/> image: istio/examples-bookinfo-reviews-v3:1.5.0<br/> imagePullPolicy: IfNotPresent<br/> ports:<br/> - containerPort: 9080</pre>
<p>Let's install this deployment on the remote cluster, <kbd>cluster-2</kbd>:</p>
<pre><strong>kubectl config use-context "gke_${proj}_${zone}_cluster-2"</strong><br/><strong>kubectl apply -f $HOME/reviews-v3.yaml</strong></pre>
<p>Once this is complete, you'll need to get access to the <span>external IP of </span>Istio's <kbd>isto-ingressgateway</kbd> service, in order to view the data in the <kbd>bookinfo</kbd> homepage. You can run this command to open that up. You'll need to reload that page dozens of times in order to see Istio's load balancing take place. You can hold down <em>F5</em> in order to reload the page many times.</p>
<p>You can access <kbd>http://&lt;GATEWAY_IP&gt;/productpage</kbd> in order to see the reviews.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deleting the cluster</h1>
                </header>
            
            <article>
                
<p>In order to clean up the control panel once you're finished, you can run the following commands.</p>
<p>First, delete the firewall rules:</p>
<pre><span><strong>gcloud compute firewall-rules delete istio-multicluster-test-pods</strong><br/><strong>The following firewalls will be deleted:</strong><br/><strong> - [istio-multicluster-test-pods]</strong><br/><strong>Do you want to continue (Y/n)? y</strong><br/><strong>Deleted [https://www.googleapis.com/compute/v1/projects/gsw-k8s-3/global/firewalls/istio-multicluster-test-pods].</strong><br/><strong>anonymuse@cloudshell:~ (gsw-k8s-3)$</strong><br/></span></pre>
<p>Next up, we'll delete our cluster-admin-role<strong> </strong>binding:</p>
<pre class="mce-root"><strong>anonymuse@cloudshell:<span>~ </span><span>(gsw-k8s-3)</span>$ kubectl delete clusterrolebinding gke-cluster-admin-bindingclusterrolebinding "gke-cluster-admin-binding" deleted</strong><br/><strong>anonymuse@cloudshell:<span>~ </span><span>(gsw-k8s-3)</span><span>$</span></strong></pre>
<p>Lastly, let's delete our GKE clusters:</p>
<pre><strong>anonymuse@cloudshell:<span>~ </span><span>(gsw-k8s-3)</span>$ gcloud container clusters delete cluster-1 --zone $zone</strong><br/><strong>The following clusters will be deleted. - [cluster-1] in [us-east1-b]</strong><br/><strong>Do you want to continue (Y/n)? y</strong><br/><strong>Deleting cluster cluster-1...done.</strong><br/><strong>Deleted [https://container.googleapis.com/v1/projects/gsw-k8s-3/zones/us-east1-b/clusters/cluster-1].</strong><br/><strong>anonymuse@cloudshell:~ (gsw-k8s-3)</strong></pre>
<p class="mce-root"/>
<p>In the GUI, you can see the cluster being deleted:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/3c3b43ff-8a9a-4df5-8b73-258bfb7efc2f.png" style="width:30.58em;height:52.58em;" width="582" height="1000"/></p>
<p>You can also see it on the command line from your <kbd>watch</kbd> command:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/9386cfa0-813a-48cb-9388-3ca421e49b4c.png" width="1274" height="172"/></p>
<p>Run the same command with your other cluster. You can double-check the <span class="packt_screen">Compute Engine</span> dashboard to ensure that your instances are being deleted:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/69c71f3d-0ec2-4566-b846-3b237ae1f902.png" width="1120" height="690"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at the new federation capabilities in Kubernetes. We saw how we can deploy clusters to multiple cloud providers and manage them from a single control plane. We also deployed an application across clusters in both AWS and GCE. While these features are new and still mainly in alpha and beta, we should now have the skills to utilize them as they evolve and become part of the standard Kubernetes operating model. </p>
<p>In the next chapter, we will take a look at another advanced topic: security. We will cover the basics for secure containers and also how to secure your Kubernetes cluster. We will also look at the Secrets construct, which gives us the capability to store sensitive configuration data similar to our preceding <kbd>ConfigMap</kbd> example.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li style="font-weight: 400">What is the main goal of federation?</li>
<li style="font-weight: 400">What is the main advantage of using federation?</li>
<li style="font-weight: 400">What are the building blocks of federation?</li>
<li style="font-weight: 400">What is the Kubernetes CLI command that controls federation?</li>
<li style="font-weight: 400">What are the two software components of Kubernetes federation?</li>
<li style="font-weight: 400">What is the main difference between HPAs and federated HPAs?</li>
<li style="font-weight: 400">What types of federated resources are available?</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p class="mce-root">If you'd like more information on mastering Kubernetes, check out another excellent Packt resource called <em>Mastering Kubernetes</em> (<a href="https://www.packtpub.com/application-development/mastering-kubernetes-second-edition">https://www.packtpub.com/application-development/mastering-kubernetes-second-edition</a>).</p>


            </article>

            
        </section>
    </div>



  </body></html>