<html><head></head><body>
		<div id="_idContainer174">
			<h1 id="_idParaDest-261" class="chapter-number"><a id="_idTextAnchor264"/>18</h1>
			<h1 id="_idParaDest-262"><a id="_idTextAnchor265"/>Scaling Your EKS Cluster</h1>
			<p>Capacity planning on EKS (and K8s generally) can be hard! If you under- or overestimate your cluster resources, you may not meet your application’s demand or end up paying more than you need. One of the reasons it’s hard is that it can be difficult to know what the expected load will be for your application. With a web application, for example, the load is normally non-deterministic and a successful marketing campaign or an event similar to Amazon Prime Day may see your load triple or quadruple. Some form of cluster/pod scaling strategy is needed to cope with the peaks and troughs of load placed on a <span class="No-Break">modern application.</span></p>
			<p>In this chapter, we will walk through several common strategies and tools that can be used with Amazon EKS and help you understand how to optimize your EKS cluster for load and cost. Specifically, this chapter covers the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Understanding scaling <span class="No-Break">in EKS</span></li>
				<li>Scaling EC2 ASGs with <span class="No-Break">Cluster Autoscaler</span></li>
				<li>Scaling worker nodes <span class="No-Break">with Karpenter</span></li>
				<li>Scaling <a id="_idIndexMarker974"/>applications with Horizontal Pod Autoscaler </li>
				<li>Scaling applications with <span class="No-Break">custom metrics</span></li>
				<li>Scaling <span class="No-Break">with KEDA</span></li>
			</ul>
			<h1 id="_idParaDest-263"><a id="_idTextAnchor266"/>Technical requirements</h1>
			<p>The reader should have familiarity with YAML, AWS IAM, and EKS architecture. Before getting started with this chapter, please ensure <span class="No-Break">the following:</span></p>
			<ul>
				<li>You have network connectivity to your EKS cluster <span class="No-Break">API endpoint</span></li>
				<li>The AWS CLI, Docker, and the <strong class="source-inline">kubectl</strong> binary are installed on your workstation with <span class="No-Break">administrator access</span></li>
			</ul>
			<h1 id="_idParaDest-264"><a id="_idTextAnchor267"/>Understanding scaling in EKS</h1>
			<p>When we consider <a id="_idIndexMarker975"/>scaling in any system or cluster, we tend to think in terms of <span class="No-Break">two dimensions:</span></p>
			<ul>
				<li>Increasing the size of a system or instance, known as <span class="No-Break">vertical scaling</span></li>
				<li>Increasing the number of systems or instances, known as <span class="No-Break">horizontal scaling</span></li>
			</ul>
			<p>The following diagram illustrates <span class="No-Break">these options.</span></p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/Figure_18.01_B18129.jpg" alt="Figure 18.1 – General scaling strategies"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.1 – General scaling strategies</p>
			<p>The scaling strategy is closely linked with the resilience model, where you have a traditional master/standby or N+1 resilience architecture, such as a relational database. Then, when you increase capacity, you normally need to scale <em class="italic">up</em> (i.e., vertically) by increasing the size of your database instances. This is due to the limitations of the <span class="No-Break">system architecture.</span></p>
			<p>In K8s, the resilience model is based on multiple worker nodes hosting multiple pods with an ingress/load balancer providing a consistent entry point. This means that a node failure should have little impact. The scaling strategy is therefore predominantly to scale <em class="italic">out</em> (horizontally) and while you can do things such as vertically scale pods to cope with increases in demand, we will focus on <span class="No-Break">horizontal scaling.</span></p>
			<p>As AWS manages <a id="_idIndexMarker976"/>the EKS control plane (scaling and resilience), we will focus mainly on the data plane (worker nodes) and application resources (pods/containers). Let’s begin with a high-level examination of the technology that supports scaling the data plane <span class="No-Break">on EKS.</span></p>
			<h2 id="_idParaDest-265"><a id="_idTextAnchor268"/>EKS scaling technology</h2>
			<p>There are <a id="_idIndexMarker977"/>three layers of technology involved in supporting <span class="No-Break">EKS scaling:</span></p>
			<ol>
				<li>AWS technology that supports <a id="_idIndexMarker978"/>scaling the data plane, such as EC2 <strong class="bold">Autoscaling Groups</strong> (<strong class="bold">ASGs</strong>) and the AWS APIs that allow systems to interact <span class="No-Break">these ASGs.</span></li>
				<li>The K8s objects (Kinds) that support scaling and deploying pods, such <span class="No-Break">as Deployments.</span></li>
				<li>The K8s scheduler and controllers that provide the link between the K8s objects (2) and the AWS technology (1) to support horizontal scaling at the pod and <span class="No-Break">cluster levels.</span></li>
			</ol>
			<p>The following diagram illustrates these <span class="No-Break">three layers.</span></p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="image/Figure_18.02_B18129.jpg" alt="Figure 18.2 – EKS scaling technology"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.2 – EKS scaling technology</p>
			<p>Let’s look <a id="_idIndexMarker979"/>at these layers <span class="No-Break">in detail.</span></p>
			<h3>AWS technology</h3>
			<p>In <a href="B18129_08.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><em class="italic">, Managing Worker Nodes on EKS</em>, we discussed the use of EC2 ASGs for resilience. When you create an ASG, you specify the minimum number of EC2 instances <a id="_idIndexMarker980"/>in the group, along with the maximum number and your desired number, and the group will scale within those limits. Calls to <a id="_idIndexMarker981"/>the EC2 API will allow the group to scale up and down (within a set of cool-down limits) based on these calls. The entity calling the EC2 ASG API must make the decisions to scale in <span class="No-Break">and out.</span></p>
			<p>In <a href="B18129_15.xhtml#_idTextAnchor220"><span class="No-Break"><em class="italic">Chapter 15</em></span></a><em class="italic">, Working with AWS Fargate</em>, we discussed how pods could be created on a Fargate instance using the Fargate profile. In this case, AWS handles the deployment and placement of pods on the Fargate fleet and the Fargate service handles scaling decisions, while the calling entity just requests the deployment of one or <span class="No-Break">more pods.</span></p>
			<h3>K8s objects</h3>
			<p>Throughout <a id="_idIndexMarker982"/>the book, we’ve used K8s deployments to <a id="_idIndexMarker983"/>deploy and scale our pods. Under the covers, this uses <strong class="bold">deployments</strong>, which in turn use <strong class="bold">ReplicaSets</strong> to add/remove new pods as needed and also to support deployment strategies such as rolling updates. K8s also supports StatefulSets and DaemonSets for <span class="No-Break">pod deployments.</span></p>
			<p>A <strong class="bold">StatefulSet</strong> is a K8s <a id="_idIndexMarker984"/>controller that deploys pods, but will guarantee a specific order or deployment and that Pod names are unique, as well as providing storage. A <strong class="bold">DaemonSet</strong> is also <a id="_idIndexMarker985"/>a controller that ensures that the pod runs on all the nodes of <span class="No-Break">the cluster.</span></p>
			<p>Both ReplicaSets and StatefulSets create pods with the expectation that the K8s scheduler will be able to deploy them. If the scheduler determines that it doesn’t have enough resources – typically, worker <a id="_idIndexMarker986"/>node CPU/RAM or network ports (<strong class="bold">NodePort</strong> services) – then the pod stays in the <span class="No-Break"><em class="italic">Pending</em></span><span class="No-Break"> state.</span></p>
			<h3>K8s scheduler and controller</h3>
			<p>K8s was designed <a id="_idIndexMarker987"/>to be extensible and <a id="_idIndexMarker988"/>can be extended using controllers. With autoscaling, we can extend EKS using the controllers in the following list, which we will examine in more detail in the <span class="No-Break">following sections:</span></p>
			<ul>
				<li><strong class="bold">K8s Cluster Autoscaler </strong>(<strong class="bold">CA</strong>) can <a id="_idIndexMarker989"/>scale AWS ASGs based on K8s <span class="No-Break">scheduler requirements</span></li>
				<li><strong class="bold">Karpenter</strong> can scale <a id="_idIndexMarker990"/>EC2 instances based on K8s <span class="No-Break">scheduler requirements</span></li>
				<li><strong class="bold">K8s Horizontal Pod Autoscaler </strong>(<strong class="bold">HPA</strong>) can scale deployments based on custom <a id="_idIndexMarker991"/>metrics or CPU utilization across EC2 or <span class="No-Break">Fargate instances</span></li>
				<li>Use KEDA to integrate different event sources to trigger scaling actions controlled <span class="No-Break">through HPA</span></li>
			</ul>
			<p>Now that we’ve looked at the three technology layers, let’s deep dive into how we install and configure the different controllers and services to scale our <span class="No-Break">EKS clusters.</span></p>
			<h1 id="_idParaDest-266"><a id="_idTextAnchor269"/>Scaling EC2 ASGs with Cluster Autoscaler</h1>
			<p>Kubernetes CA is <a id="_idIndexMarker992"/>a core part of the K8s ecosystem <a id="_idIndexMarker993"/>and is used to scale worker nodes in or out based on two <span class="No-Break">main conditions:</span></p>
			<ul>
				<li>If there is a Pod in the Kubernetes cluster in the <strong class="source-inline">Pending</strong> state due to an insufficient <span class="No-Break">resources error</span></li>
				<li>If there is a worker node in the Kubernetes cluster that is identified as underutilized by <span class="No-Break">Kubernetes CA</span></li>
			</ul>
			<p>The following diagram illustrates the basic flow of a scale-out operation to support a single pod being placed in the <strong class="source-inline">Pending</strong> state and not <span class="No-Break">being scheduled.</span></p>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="image/Figure_18.03_B18129.jpg" alt="Figure 18.3 – High-level Cluster AutoScaler flow"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.3 – High-level Cluster AutoScaler flow</p>
			<p>In the preceding diagram, we can see <span class="No-Break">the following:</span></p>
			<ol>
				<li>The CA is actively looking for pods that cannot be scheduled for resource reasons and are in the <span class="No-Break"><strong class="source-inline">Pending</strong></span><span class="No-Break"> state.</span></li>
				<li>The CA makes calls <a id="_idIndexMarker994"/>to the EC2 ASG API to increase the desired capacity, which in turn will add a new node to the ASG. A key aspect to note is that the nodes need tagging with <strong class="source-inline">k8s.io/cluster-autoscaler/</strong> so that the CA can discover them and their <span class="No-Break">instance types.</span></li>
				<li>Once the <a id="_idIndexMarker995"/>node has registered with <a id="_idIndexMarker996"/>the cluster, the scheduler will schedule the pod on <span class="No-Break">that node.</span></li>
				<li>Once the pod is deployed and assuming there are no issues with the pod itself, the state is changed <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">Running</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>Now we’ve looked at the concepts behind the CA, let’s <span class="No-Break">install it.</span></p>
			<h2 id="_idParaDest-267"><a id="_idTextAnchor270"/>Installing the CA in your EKS cluster</h2>
			<p>As the CA will interact with the AWS EC2 API, the first thing we need to do is make sure that the subnets <a id="_idIndexMarker997"/>used by the autoscaler <a id="_idIndexMarker998"/>have been tagged correctly. This should be done automatically if you’re using <strong class="bold">eksctl</strong> to provision node groups. We can check whether the two tags, <strong class="source-inline">k8s.io/cluster-autoscaler/enabled</strong> and <strong class="source-inline">k8s.io/cluster-autoscaler/myipv4cluster</strong>, have been applied to <span class="No-Break">the subnets:</span></p>
			<pre class="source-code">
$ aws ec2 describe-subnets --filters "Name=tag:k8s.io/cluster-autoscaler/enabled,Values=true" | jq -r '.Subnets[].SubnetId'
subnet-05d5323d274c6d67e
subnet-087e0a21855f08fd3
subnet-0dbed7d2f514d8897
$ aws ec2 describe-subnets --filters "Name=tag:k8s.io/cluster-autoscaler/myipv4cluster,Values=owned" | jq -r '.Subnets[].SubnetId'
subnet-05d5323d274c6d67e
subnet-087e0a21855f08fd3
subnet-0dbed7d2f514d8897</pre>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="source-inline">myipv4cluster</strong> is the name of the cluster in my example, but yours <span class="No-Break">may vary.</span></p>
			<p>Now we have <a id="_idIndexMarker999"/>confirmed that the subnets have <a id="_idIndexMarker1000"/>the correct tags applied, we can set up the AWS IAM policy to be associated with the K8s service account that the CA pods <span class="No-Break">will use.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">It is a best practice to add a condition to limit the policy to those resources owned by the cluster it is deployed on. In our case, we will use the tags we created/verified in the <span class="No-Break">previous step.</span></p>
			<p>The following is the autoscaling policy that you are <a id="_idTextAnchor271"/>recommended <span class="No-Break">to create:</span></p>
			<pre class="source-code">
{"<a id="_idTextAnchor272"/>Version": "2012-10<a id="_idTextAnchor273"/>-17<a id="_idTextAnchor274"/>",
    "Statement": [ {
            "Effect": "Allow",
<a id="_idTextAnchor275"/>            "Action": [
                "autoscali<a id="_idTextAnchor276"/>ng:SetDesiredCapacity",
                "autoscaling:TerminateInst<a id="_idTextAnchor277"/>anceInAutoScali<a id="_idTextAnchor278"/>ngGroup"
            ],
            "Resource": "*",
            "Condition": {
          <a id="_idTextAnchor279"/>      "StringEquals": {
                    "autoscaling:ResourceTag/k8s.io/cluster-autos<a id="_idTextAnchor280"/>caler/enabled": "true",
                    "aws:ResourceTag/k8s.io/cluster-autoscaler/<a id="_idTextAnchor281"/><strong class="bold">myipv4cluster":</strong> "<a id="_idTextAnchor282"/>o<a id="_idTextAnchor283"/>wne<a id="_idTextAnchor284"/>d"
      <a id="_idTextAnchor285"/>          }}},
        { "Effect": "Allow",
<a id="_idTextAnchor286"/>            "Action": [
                "autoscaling:Describ<a id="_idTextAnchor287"/>eAutoScalingInstances",
                "autoscaling:Desc<a id="_idTextAnchor288"/>ribeAutoScalingGroups",
                "autoscaling:Desc<a id="_idTextAnchor289"/>ribeScalingActivities",
                "ec2:DescribeL<a id="_idTextAnchor290"/>aunchTemplateVersions",
                "aut<a id="_idTextAnchor291"/>oscaling:DescribeTags",
                "autoscaling:Describ<a id="_idTextAnchor292"/>eLaunchConfigurations",
                "ec2<a id="_idTextAnchor293"/>:DescribeInstan<a id="_idTextAnchor294"/>ceTypes"
            ],
    <a id="_idTextAnchor295"/>        <a id="_idTextAnchor296"/>"R<a id="_idTextAnchor297"/>esource": "*"
        }]}</pre>
			<p>We can <a id="_idIndexMarker1001"/>create the policy and associate it <a id="_idIndexMarker1002"/>with an EKS service account using the following commands. Also, take note of the current EKS <span class="No-Break">cluster version:</span></p>
			<pre class="source-code">
$ aws iam create-policy --policy-name AmazonEKSClusterAutoscalerPolicy \
--policy-document file://autoscaler_policy.json
{
    "Policy": {
        "…….}}
$ eksctl create iamserviceaccount  --cluster=myipv4cluster --namespace=kube-system --name=cluster-autoscaler  --attach-policy-arn=arn:aws:iam::112233:policy/AmazonEKSClusterAutoscalerPolicy --override-existing-serviceaccounts --approve
……
2023-05-02 19:31:55 []  created serviceaccount "kube-system/cluster-autoscaler"
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"22+", GitVersion:"v1.22.6-eks-7d68063", GitCommit:"f24e667e49fb137336f7b064dba897beed639bad", GitTreeState:"clean", BuildDate:"2022-02-23T19:32:14Z", GoVersion:"go1.16.12", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"22+", GitVersion:"v1.22.17-eks-ec5523e", GitCommit:"49675beb7b1c90389418d067d37024616a313555", GitTreeState:"clean", BuildDate:"2023-03-20T18:44:58Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}</pre>
			<p>You can <a id="_idIndexMarker1003"/>now download the <a href="https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml">https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml</a> installation <a id="_idIndexMarker1004"/>manifest and modify the following lines (line numbers <span class="No-Break">may vary).</span></p>
			<h3>Modify the autodiscovery tag (line 165)</h3>
			<p>This will <a id="_idIndexMarker1005"/>configure the autoscaler to use the specific cluster tag we (or <span class="No-Break"><strong class="source-inline">eksctl</strong></span><span class="No-Break">) created:</span></p>
			<pre class="source-code">
node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/myipv4cluster</pre>
			<h3>Add command switches under the autodiscovery tag line (line 165)</h3>
			<p>These switches allow the autoscaler to balance more effectively across availability zones and scale autoscaling node groups <span class="No-Break">to zero:</span></p>
			<pre class="source-code">
- --balance-similar-node-groups
- --skip-nodes-with-system-pods=false</pre>
			<h3>Modify the container image to align with your K8s server version (line 149)</h3>
			<p>This <a id="_idIndexMarker1006"/>will use the same autoscaler version as the cluster itself. Although newer versions exist, it’s best practice to use the same version as <span class="No-Break">your cluster:</span></p>
			<pre class="source-code">
image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.22.2</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">The page at <a href="https://github.com/kubernetes/autoscaler/releases">https://github.com/kubernetes/autoscaler/releases</a> can be used to look up the <span class="No-Break">required release.</span></p>
			<p>Now we have a modified manifest, we can deploy the autoscaler, patch the service account, and verify it is running using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
$ kubectl create -f cluster-autoscaler-autodiscover.yaml
….
deployment.apps/cluster-autoscaler created
Error from server (AlreadyExists): error when creating "cluster-autoscaler-autodiscover.yaml": serviceaccounts "cluster-autoscaler" already exists
$ kubectl patch deployment cluster-autoscaler -n kube-system -p '{"spec":{"template":{"metadata":{"annotations":{"cluster-autoscaler.kubernetes.io/safe-to-evict": "false"}}}}}'
$ kubectl get po -n kube-system
NAME                  READY   STATUS    RESTARTS   AGE
aws-node-2wrq6         1/1     Running   0          3d7h
aws-node-blfl2         1/1     Running   0          3d7h
cluster-autoscaler-577 1/1     Running   0          12s
……</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">The deployment returns an error as the service account already exists. This will not affect the deployment or running of the autoscaler pod, but does mean that if you delete the manifest, the SA will also <span class="No-Break">be deleted.</span></p>
			<p>Now <a id="_idIndexMarker1007"/>we have CA installed, let’s test whether it <span class="No-Break">is working.</span></p>
			<h2 id="_idParaDest-268"><a id="_idTextAnchor298"/>Testing Cluster Autoscaler</h2>
			<p>If we look at the ASG created by <strong class="source-inline">eksctl</strong> for the node group created with the cluster, we can see <a id="_idIndexMarker1008"/>the desired, minimum, and maximum capacities are set to <strong class="bold">2</strong>. As there are two nodes already provisioned and the maximum is <strong class="bold">2</strong>, autoscaling will never happen. So, we need to edit the ASG using the <strong class="bold">Edit</strong> button, and increase the maximum capacity <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">5</strong></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="image/Figure_18.04_B18129.jpg" alt="Figure 18.4 – eksctl node group ASG capacity"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.4 – eksctl node group ASG capacity</p>
			<p>If we look at the cluster, we can see we have two nodes deployed and registered. Changing the ASG’s maximum capacity has no effect on the current nodes as nothing has yet triggered <span class="No-Break">any rescaling:</span></p>
			<pre class="source-code">
$ kubectl get node
NAME            STATUS   ROLES    AGE     VERSION
ip-192-168-18-136.eu-central-1.compute.internal   Ready    &lt;none&gt;   3d8h    v1.22.17-eks-a59e1f0
ip-192-168-43-219.eu-central-1.compute.internal   Ready    &lt;none&gt;   3d8h    v1.22.17-eks-a59e1f0</pre>
			<p>If we now deploy a manifest with a lot of replicas, we will see the autoscaler scale out the ASG to <a id="_idIndexMarker1009"/>support the pods. The following manifest will cause more nodes to be provisioned as it requests <span class="No-Break">150 replicas/pods:</span></p>
			<pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 150
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80</pre>
			<p>If we deploy this manifest and monitor the nodes and autoscaler using the following command, we can see <a id="_idIndexMarker1010"/>the pods transition from <strong class="source-inline">Pending</strong> to the <strong class="source-inline">Running</strong> state as the <span class="No-Break">ASG scales:</span></p>
			<pre class="source-code">
$ kubectl create -f tester.yaml
$ kubectl get po
NAME       READY   STATUS    RESTARTS   AGE
nginx-deployment-12   0/1     Pending   0          18m
nginx-deployment-13   0/1     Pending   0          18m
….
WAIT 10 MINUTES
$ kubectl get node
NAME          STATUS   ROLES    AGE    VERSION
ip-192-168-18-136.eu-central-1.compute.internal   Ready    &lt;none&gt;   3d8h   v1.22.17-eks-a59e1f0
ip-192-168-34-4.eu-central-1.compute.internal     Ready    &lt;none&gt;   18m    v1.22.17-eks-a59e1f0
ip-192-168-43-219.eu-central-1.compute.internal   Ready    &lt;none&gt;   3d8h   v1.22.17-eks-a59e1f0
ip-192-168-77-75.eu-central-1.compute.internal    Ready    &lt;none&gt;   18m    v1.22.17-eks-a59e1f0
ip-192-168-85-131.eu-central-1.compute.internal   Ready    &lt;none&gt;   18m    v1.22.17-eks-a59e1f0</pre>
			<p>We can see the nodes have scaled up to <strong class="source-inline">5</strong> (the maximum set in the ASG configuration) to support the increased number of pods, but some pods will remain <strong class="source-inline">Pending</strong> as there are not enough nodes to support them and the autoscaler won’t violate the ASG capacity limits to create more nodes. If we now delete the manifest using the following commands, we can see the ASG scale back to 2 (desired capacity) after nodes have not been needed for at least 10 minutes (this is the default – you can adjust the scan time for CA, but this will introduce more load onto your cluster, and as we will see later, Karpenter is <a id="_idIndexMarker1011"/>much more suited for <span class="No-Break">faster scaling):</span></p>
			<pre class="source-code">
$ kubectl delete -f tester.yaml
deployment.apps "nginx-deployment" deleted
WAIT AT LEAST 10 MINUTES
$ kubectl get node
NAME             STATUS   ROLES    AGE    VERSION
ip-192-168-34-4.eu-central-1.compute.internal    Ready    &lt;none&gt;   47m   v1.22.17-eks-a59e1f0
ip-192-168-77-75.eu-central-1.compute.internal   Ready    &lt;none&gt;   47m   v1.22.17-eks-a59e1f0</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">The original, older nodes were removed and two of the newer <span class="No-Break">nodes remain.</span></p>
			<p>If you want to monitor the autoscaler’s actions, you can use the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
$ kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler
I0502 21:22:59.599092       1 scale_down.go:829] ip-192-168-18-136.eu-central-1.compute.internal was unneeded for 7m2.220392583s</pre>
			<p>Using ASGs has its advantages but when you want to use lots of different instance types or scale up/down quickly, then Karpenter may provide more flexibility. Let’s look at how we can deploy and <span class="No-Break">use Karpenter.</span></p>
			<h1 id="_idParaDest-269"><a id="_idTextAnchor299"/>Scaling worker nodes with Karpenter</h1>
			<p>Karpenter (<a href="https://karpenter.sh">https://karpenter.sh</a>) is an <a id="_idIndexMarker1012"/>open source autoscaling <a id="_idIndexMarker1013"/>solution built for Kubernetes <a id="_idIndexMarker1014"/>and is designed to build and remove capacity to cope with fluctuating demand without the need for node or ASGs. The following diagram illustrates the <span class="No-Break">general flow:</span></p>
			<div>
				<div id="_idContainer167" class="IMG---Figure">
					<img src="image/Figure_18.05_B18129.jpg" alt="Figure 18.5 – High-level Karpenter flow"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.5 – High-level Karpenter flow</p>
			<p>In the preceding diagram, we can see <span class="No-Break">the following:</span></p>
			<ol>
				<li>Karpenter is actively looking for pods in the <strong class="source-inline">Pending</strong> state that cannot be scheduled due to <span class="No-Break">insufficient resources.</span></li>
				<li>The Karpenter controller will review <strong class="bold">resource</strong> <strong class="bold">requests</strong>, <strong class="bold">node selectors</strong>, <strong class="bold">affinities</strong>, and <strong class="bold">tolerations</strong> for these pending pods, and provision nodes that meet the requirements of <span class="No-Break">the pods.</span></li>
				<li>Once the node has registered with the cluster, the scheduler will schedule the pod on <span class="No-Break">that node.</span></li>
				<li>Once the <a id="_idIndexMarker1015"/>pod has been deployed <a id="_idIndexMarker1016"/>and assuming there are no issues with the pod itself, the state is changed <span class="No-Break">to </span><span class="No-Break"><strong class="bold">Running</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>Now that we’ve looked at the concepts behind Karpenter, let’s install and <span class="No-Break">test it.</span></p>
			<h2 id="_idParaDest-270"><a id="_idTextAnchor300"/>Installing Karpenter in your EKS cluster</h2>
			<p>We will <a id="_idIndexMarker1017"/>create the Karpenter controller on the existing managed <a id="_idIndexMarker1018"/>node group (ASG) but use it to schedule workloads on additional <span class="No-Break">worker nodes:</span></p>
			<ol>
				<li>In order for our installation to be successful, we need to first set up a few environment variables. We can do this <span class="No-Break">as follows:</span><pre class="source-code">
$ export CLUSTER_NAME=myipv4cluster
 export AWS_PARTITION="aws"
 export AWS_REGION="$(aws configure list | grep region | tr -s " " | cut -d" " -f3)"
 export OIDC_ENDPOINT="$(aws eks describe-cluster --name ${CLUSTER_NAME} --query "cluster.identity.oidc.issuer" --output text)"
 export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)
$ printenv
…..
AWS_PARTITION=aws
CLUSTER_NAME=myipv4cluster
AWS_REGION=eu-central-1
AWS_ACCOUNT_ID=111222333
OIDC_ENDPOINT=<a href="https://oidc.eks.eu-central-1.amazonaws.com/id/123455">https://oidc.eks.eu-central-1.amazonaws.com/id/123455</a></pre></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Modify the cluster name to align with <span class="No-Break">your cluster.</span></p>
			<ol>
				<li value="2">Next, we need <a id="_idIndexMarker1019"/>to create the AWS IAM policy that <a id="_idIndexMarker1020"/>will be associated with the nodes created by Karpenter. This will ensure they have the right EKS, ECR, and SSM permissions. We start with a trust policy that allows the EC2 instances to assume a role, <span class="No-Break">as follows:</span><pre class="source-code">
{ "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "",
      "Effect": "Allow",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }]}</pre></li>
				<li>We can then create a role, <strong class="source-inline">KarpenterInstanceNodeRole</strong>, which is used by the nodes created by Karpenter and references this trust policy as <span class="No-Break">shown here:</span><pre class="source-code">
$ aws iam create-role --role-name <strong class="bold">KarpenterInstanceNodeRole</strong>     --assume-role-policy-document file://"trust_policy.json"</pre></li>
				<li>We can then <a id="_idIndexMarker1021"/>attach the four required managed <a id="_idIndexMarker1022"/>polices to this role using the <span class="No-Break">following commands:</span><pre class="source-code">
$ aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/<strong class="bold">AmazonEKSWorkerNodePolicy</strong> --role-name <strong class="bold">KarpenterInstanceNodeRole</strong>
$ aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/<strong class="bold">AmazonEKS_CNI_Policy</strong> --role-name <strong class="bold">KarpenterInstanceNodeRole</strong>
$ aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/<strong class="bold">AmazonEC2ContainerRegistryReadOnly</strong> --role-name <strong class="bold">KarpenterInstanceNodeRole</strong>
$ aws iam attach-role-policy --policy-arn  arn:aws:iam::aws:policy/<strong class="bold">AmazonSSMManagedInstanceCore</strong> --role-name <strong class="bold">KarpenterInstanceNodeRole</strong></pre></li>
				<li>We now create the instance profile used for <span class="No-Break">the nodes:</span><pre class="source-code">
$ aws iam create-instance-profile --instance-profile-name "KarpenterNodeInstanceProfile-${CLUSTER_NAME}"
{
    "InstanceProfile": {
        "Path": "/",
        "InstanceProfileName": "KarpenterNodeInstanceProfile-myipv4cluster",
        "InstanceProfileId": "AIPARDV7UN62ZBGEB7AV4",
        "Arn": "arn:aws:iam::111222333:instance-profile/KarpenterNodeInstanceProfile-myipv4cluster",
        "CreateDate": "2023-05-03T06:55:12+00:00",
        "Roles": []
    }
}</pre></li>
				<li>Then we assign the role created previously to the <span class="No-Break">instance profile:</span><pre class="source-code">
$ aws iam add-role-to-instance-profile --instance-profile-name "KarpenterNodeInstanceProfile-${CLUSTER_NAME}" --role-name "KarpenterInstanceNodeRole"</pre></li>
				<li>Now we have <a id="_idIndexMarker1023"/>the instance profile for the EC2 nodes to use. The next task is to create the AWS IAM policy to be associated <a id="_idIndexMarker1024"/>with the Karpenter pods. We will go through a similar process as we did for the instance profile. First, we create the trust policy, which allows the cluster’s OIDC endpoint to assume a role. This uses the <strong class="source-inline">OIDC_ENDPOINT</strong> and <strong class="source-inline">AWS_ACCOUNT</strong> environment variables we set <span class="No-Break">up previously:</span><pre class="source-code">
$ cat &lt;&lt; EOF &gt; controller-trust-policy.json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Federated": "arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_ENDPOINT#*//}"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "${OIDC_ENDPOINT#*//}:aud": "sts.amazonaws.com",
                    "${OIDC_ENDPOINT#*//}:sub": "system:serviceaccount:karpenter:karpenter"
                }
            }
        }
    ]
}
EOF</pre></li>
				<li>Then we <a id="_idIndexMarker1025"/>create the role for the Karpenter <a id="_idIndexMarker1026"/>controller using the trust policy with the <span class="No-Break">following command:</span><pre class="source-code">
$ aws iam create-role --role-name KarpenterControllerRole-${CLUSTER_NAME} --assume-role-policy-document file://controller-trust-policy.json
{"Role": {
        "Path": "/",
        "RoleName": "KarpenterControllerRole-myipv4cluster",
…..</pre></li>
				<li>The following command is used to create the policy needed for the controller and again relies on the environment variables we <span class="No-Break">set earlier:</span><pre class="source-code">
cat &lt;&lt; EOF &gt; controller-policy.json
{
    "Statement": [
        {
            "Action": [
                "ssm:GetParameter",
                "ec2:DescribeImages",
                "ec2:RunInstances",
                "ec2:DescribeSubnets",
                "ec2:DescribeSecurityGroups",
                "ec2:DescribeLaunchTemplates",
                "ec2:DescribeInstances",
                "ec2:DescribeInstanceTypes",
                "ec2:DescribeInstanceTypeOfferings",
                "ec2:DescribeAvailabilityZones",
                "ec2:DeleteLaunchTemplate",
                "ec2:CreateTags",
                "ec2:CreateLaunchTemplate",
                "ec2:CreateFleet",
                "ec2:DescribeSpotPriceHistory",
                "pricing:GetProducts"
            ],
            "Effect": "Allow",
            "Resource": "*",
            "Sid": "Karpenter"
        },
        {
            "Action": "ec2:TerminateInstances",
            "Condition": {
                "StringLike": {
                    "ec2:ResourceTag/karpenter.sh/provisioner-name": "*"
                }
            },
            "Effect": "Allow",
            "Resource": "*",
            "Sid": "ConditionalEC2Termination"
        },
        {
            "Effect": "Allow",
            "Action": "iam:PassRole",
            "Resource": "arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:role/KarpenterInstanceNodeRole",
            "Sid": "PassNodeIAMRole"
        },
        {
            "Effect": "Allow",
            "Action": "eks:DescribeCluster",
            "Resource": "arn:${AWS_PARTITION}:eks:${AWS_REGION}:${AWS_ACCOUNT_ID}:cluster/${CLUSTER_NAME}",
            "Sid": "EKSClusterEndpointLookup"
        }
    ],
    "Version": "2012-10-17"
}
EOF</pre></li>
				<li>We create <a id="_idIndexMarker1027"/>the IAM role that will be used by <a id="_idIndexMarker1028"/>the Karpenter controller using the policy created in the <span class="No-Break">previous step:</span><pre class="source-code">
$ aws iam put-role-policy --role-name KarpenterControllerRole-${CLUSTER_NAME} --policy-name KarpenterControllerPolicy-${CLUSTER_NAME} --policy-document file://controller-policy.json</pre></li>
				<li>We now need to tag the subnets we want Karpenter to use when it adds nodes by adding the <strong class="source-inline">karpenter.sh/discovery=myipv4cluster</strong> tag. We can use the <strong class="source-inline">describe-subnets</strong> command to identify which subnets have been tagged. The following is an example of <span class="No-Break">this usage:</span><pre class="source-code">
$ aws ec2 describe-subnets --filters "Name=tag:karpenter.sh
/discovery,Values=myipv4cluster" | jq -r '.Subnets[].SubnetId'
subnet-05d5323d274c6d67e
subnet-087e0a21855f08fd3
subnet-0dbed7d2f514d8897</pre></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">We’ve used the same subnets as the autoscaler, but you could use <span class="No-Break">different ones.</span></p>
			<ol>
				<li value="12">We also need to tag the security group with the same discovery tag, <strong class="source-inline">karpenter.sh/discovery=myipv4cluster</strong>, so that Karpenter nodes will be able to communicate with the control plan and other nodes. We will use the cluster security group, which can be located in the <span class="No-Break"><strong class="bold">Networking</strong></span><span class="No-Break"> section:</span></li>
			</ol>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<img src="image/Figure_18.06_B18129.jpg" alt="Figure 18.6 – Locating the cluster security group"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.6 – Locating the cluster security group</p>
			<p>We can tag <a id="_idIndexMarker1029"/>the security group with the <a id="_idIndexMarker1030"/><span class="No-Break">following command:</span></p>
			<pre class="source-code">
$ aws ec2 create-tags --tags "Key=karpenter.sh/discovery,Value=${CLUSTER_NAME}" --resources sg-0222247264816d807
$ aws ec2 describe-security-groups --filters "Name=tag:karpenter.sh/discovery,Values=myipv4cluster" | jq -r '.SecurityGroups[].GroupId'
sg-0222247264816d807</pre>
			<ol>
				<li value="13">Finally, we need to edit the <strong class="source-inline">aws-auth</strong> ConfigMap to allow the Karpenter node role to authenticate with the API servers. We need to add a new group with the following configuration to <span class="No-Break">the ConfigMap:</span><pre class="source-code">
mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::123:role/eksctl-myipv4cluster-node
      username: system:node:{{EC2PrivateDNSName}}
<strong class="bold">    - groups:</strong>
<strong class="bold">      - system:bootstrappers</strong>
<strong class="bold">      - system:nodes</strong>
<strong class="bold">      rolearn: arn:aws:iam::123:role/KarpenterInstanceNodeRole</strong>
<strong class="bold">      username: system:node:{{EC2PrivateDNSName}}</strong></pre></li>
			</ol>
			<p>You can use the <strong class="source-inline">$ kubectl edit configmap aws-auth -n kube-system</strong> command to make <span class="No-Break">the changes.</span></p>
			<p>All this preparation <a id="_idIndexMarker1031"/>work now means we can download <a id="_idIndexMarker1032"/>and customize the Karpenter Helm chart. We will use version <em class="italic">0.27.3</em>, which is the latest at the time of writing. The following command can be used set the Karpenter version and customize the Helm chart based on the environment variables and IAM policies created in the previous steps, saving it to the <span class="No-Break"><strong class="source-inline">karpenter.yaml</strong></span><span class="No-Break"> manifest:</span></p>
			<pre class="source-code">
$ export KARPENTER_VERSION=v0.27.3
$ helm template karpenter oci://public.ecr.aws/karpenter/karpenter --version ${KARPENTER_VERSION} --namespace karpenter     --set settings.aws.defaultInstanceProfile=KarpenterInstanceNodeRole  --set settings.aws.clusterName=${CLUSTER_NAME}     --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:role/KarpenterControllerRole-${CLUSTER_NAME}"     --set controller.resources.requests.cpu=1     --set controller.resources.requests.memory=1Gi     --set controller.resources.limits.cpu=1     --set controller.resources.limits.memory=1Gi &gt; karpenter.yaml</pre>
			<p>We now need to add some additional configuration to the <span class="No-Break"><strong class="source-inline">karpenter.yaml</strong></span><span class="No-Break"> file.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">The line numbers may be different <span class="No-Break">for you.</span></p>
			<h3>Modify the affinity rules (line 482)</h3>
			<p>This will deploy Karpenter on the existing <span class="No-Break">node group:</span></p>
			<pre class="source-code">
- matchExpressions:
              - key: eks.amazonaws.com/nodegroup
                operator: In
                values:
                - ipv4mng</pre>
			<p>We then create <a id="_idIndexMarker1033"/>the namespace and add the Karpenter <a id="_idIndexMarker1034"/>custom resources using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
$ kubectl create namespace karpenter
namespace/karpenter created
$ kubectl create -f https://raw.githubusercontent.com/aws/karpenter/${KARPENTER_VERSION}/pkg/apis/crds/karpenter.sh_provisioners.yaml
customresourcedefinition.apiextensions.k8s.io/provisioners.karpenter.sh created
$ kubectl create -f https://raw.githubusercontent.com/aws/karpenter/${KARPENTER_VERSION}/pkg/apis/crds/karpenter.k8s.aws_awsnodetemplates.yaml
customresourcedefinition.apiextensions.k8s.io/awsnodetemplates.karpenter.k8s.aws created</pre>
			<p>And finally, we can deploy the Helm chart and verify the deployment using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
$ kubectl apply -f karpenter.yaml
poddisruptionbudget.policy/karpenter created
serviceaccount/karpenter created
secret/karpenter-cert created
configmap/config-logging created
……
validatingwebhookconfiguration.admissionregistration.k8s.io/validation.webhook.karpenter.k8s.aws created
$ kubectl get all  -n karpenter
NAME        READY   STATUS    RESTARTS   AGE
pod/karpenter-fcd8f5df6-l2h6k   1/1     Running   0   43s
pod/karpenter-fcd8f5df6-r2vzw   1/1     Running       43s
NAME   TYPE   CLUSTER-IP  ..
service/karpenter   ClusterIP   10.100.76.202   ..
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/karpenter   2/2     2         2           43s
NAME          DESIRED   CURRENT   READY   AGE
replicaset.apps/karpenter-fcd8f5df6   2  2    2       43s</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">If you’ve followed this entire chapter from the start, you will have both CA and Karpenter controllers deployed. You can disable CA by scaling down to zero using the following command: <strong class="source-inline">kubectl scale deploy/cluster-autoscaler -n </strong><span class="No-Break"><strong class="source-inline">kube-system --replicas=0</strong></span><span class="No-Break">.</span></p>
			<p>Now we have Karpenter installed, let’s test whether it <span class="No-Break">is working.</span></p>
			<h2 id="_idParaDest-271"><a id="_idTextAnchor301"/>Testing Karpenter autoscaling</h2>
			<p>The first thing <a id="_idIndexMarker1035"/>we need to do is create a <strong class="bold">Provisioner</strong> and its <a id="_idIndexMarker1036"/>associated <strong class="bold">AWSNodeTemplate</strong>. A <strong class="bold">Provisioner</strong> creates the <a id="_idIndexMarker1037"/>rules used by Karpenter to create nodes and defines the pod selection rules. At least one provisioner must exist for Karpenter to work. In our next example, we will allow Karpenter to handle <span class="No-Break">the following:</span></p>
			<ul>
				<li>Creating on-demand <span class="No-Break">EC2 instances</span></li>
				<li>Choosing an instance type of either <strong class="source-inline">c5.large</strong>, <strong class="source-inline">m5.large</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">m5.xlarge</strong></span></li>
				<li>Labeling any new nodes with the <span class="No-Break"><strong class="source-inline">type=karpenter</strong></span><span class="No-Break"> label</span></li>
				<li>De-provisioning the node once it is empty of pods for <span class="No-Break">30 seconds</span></li>
			</ul>
			<p>Karpenter will also be limited from creating additional nodes once the CPU and memory limits have <span class="No-Break">been reached.</span></p>
			<p><strong class="bold">AWSNodeTemplate</strong> is used to describe elements of the AWS-specific configuration, such as the subnet and security groups. You can manually configure these details, but we will use the discovery tags we created in the <span class="No-Break">previous sections.</span></p>
			<p>The following example manifest can be deployed using the <strong class="source-inline">$ kubectl create -f </strong><span class="No-Break"><strong class="source-inline">provisioner.yaml</strong></span><span class="No-Break"> command:</span></p>
			<pre class="source-code">
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: default
spec:
  labels:
    type: karpenter
  requirements:
    - key: karpenter.sh/capacity-type
      operator: In
      values: ["on-demand"]
    - key: "node.kubernetes.io/instance-type"
      operator: In
      values: ["c5.large", "m5.large", "m5.xlarge"]
  limits:
    resources:
      cpu: 1000
      memory: 1000Gi
  providerRef:
    name: default
  ttlSecondsAfterEmpty: 30
---
apiVersion: karpenter.k8s.aws/v1alpha1
kind: AWSNodeTemplate
metadata:
  name: default
spec:
  subnetSelector:
    karpenter.sh/discovery: myipv4cluster
  securityGroupSelector:
    karpenter.sh/discovery: myipv4cluster</pre>
			<p>Once we have successfully deployed the <strong class="bold">Provisioner</strong> and <strong class="bold">AWSNodeTemplate</strong>, we can move on to deploy the following manifest, which will create a namespace and a deployment with <a id="_idIndexMarker1038"/>zero replicas to be scheduled on a node with a <span class="No-Break"><strong class="source-inline">type=karpenter</strong></span><span class="No-Break"> label:</span></p>
			<pre class="source-code">
apiVersion: v1
kind: Namespace
metadata:
  name: other
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inflate
  namespace: other
spec:
  replicas: 0
  selector:
    matchLabels:
      app: inflate
  template:
    metadata:
      labels:
        app: inflate
    spec:
      nodeSelector:
        type: karpenter
      terminationGracePeriodSeconds: 0
      containers:
        - name: inflate
          image: public.ecr.aws/eks-distro/kubernetes/pause:3.2
          resources:
            requests:
              memory: 1Gi</pre>
			<p>Using the <a id="_idIndexMarker1039"/>following commands, we can deploy the previous manifest, validate that no replicas exist, and also check that no nodes exist with the <span class="No-Break"><strong class="source-inline">type=karpenter</strong></span><span class="No-Break"> label:</span></p>
			<pre class="source-code">
$ kubectl create -f deployment.yaml
namespace/other created
deployment.apps/inflate created
$ kubectl get all -n other
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/inflate   0/0     0       0       11s
NAME        DESIRED   CURRENT   READY   AGE
replicaset.apps/inflate-11   0         0         0       11s
$ kubectl get node -l type=karpenter
No resources found</pre>
			<p>Now if we scale the deployment, we will see pods get created in the <strong class="source-inline">Pending</strong> state as there is no EKS node <a id="_idIndexMarker1040"/>satisfying <strong class="bold">NodeSelector</strong>. Karpenter will then create the node and the pods will be deployed and move to the <strong class="source-inline">Running</strong> state. The following commands illustrate <span class="No-Break">this flow:</span></p>
			<pre class="source-code">
$ kubectl scale -n other deployment/inflate --replicas 5
deployment.apps/inflate scaled
$ kubectl get po -n other
NAME                       READY   STATUS    RESTARTS   AGE
inflate-6cc55bfc74-4rfts   0/1     Pending   0          10s
inflate-6cc55bfc74-9kl5n   0/1     Pending   0          10s
inflate-6cc55bfc74-nspbf   0/1     Pending   0          10s
inflate-6cc55bfc74-rq4cn   0/1     Pending   0          10s
inflate-6cc55bfc74-swtmv   0/1     Pending   0          10s
$ kubectl get node -l type=karpenter
NAME     STATUS   ROLES    AGE   VERSION
ip-192-168-55-119.eu-central-1.compute.internal   Ready    &lt;none&gt;   99s   v1.22.17-eks-a59e1f0
$ kubectl get po -n other
NAME                       READY   STATUS    RESTARTS   AGE
inflate-6cc55bfc74-4rfts   1/1     Running   0          2m36s
inflate-6cc55bfc74-9kl5n   1/1     Running   0          2m36s
inflate-6cc55bfc74-nspbf   1/1     Running   0          2m36s
inflate-6cc55bfc74-rq4cn   1/1     Running   0          2m36s
inflate-6cc55bfc74-swtmv   1/1     Running   0          2m36s</pre>
			<p>If we now <a id="_idIndexMarker1041"/>delete the deployment , we will see the node <span class="No-Break">is de-provisioned:</span></p>
			<pre class="source-code">
$ kubectl delete -f deployment.yaml
namespace "other" deleted
deployment.apps "inflate" deleted
$ kubectl get node -l type=karpenter
NAME           STATUS   ROLES    AGE     VERSION
ip-192-168-55-119.eu-central-1.compute.internal   Ready    &lt;none&gt;   5m57s   v1.22.17-eks-a59e1f0
$ kubectl get node -l type=karpenter
No resources found</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">The scale-in/out process is much faster than with CA, which is one of the key advantages <span class="No-Break">of Karpenter.</span></p>
			<p>We have focused <a id="_idIndexMarker1042"/>on scaling the underlying EKS compute node using CA or Karpenter. Both of these controllers look for pods in the <strong class="source-inline">Pending</strong> state due to resource issues, and up to now we have been creating and scaling pods manually. We will now look at how we can scale pods automatically <span class="No-Break">using HPA.</span></p>
			<h1 id="_idParaDest-272"><a id="_idTextAnchor302"/>Scaling applications with Horizontal Pod Autoscaler</h1>
			<p>HPA is <a id="_idIndexMarker1043"/>a component <a id="_idIndexMarker1044"/>of Kubernetes <a id="_idIndexMarker1045"/>that allows you to scale pods (through a <strong class="bold">Deployment</strong>/<strong class="bold">ReplicaSet</strong>) based on metrics rather than manual scaling commands. The metrics are collected by the K8s metrics server, so you will need to have this deployed in your cluster. The following diagram illustrates the <span class="No-Break">general flow.</span></p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="image/Figure_18.07_B18129.jpg" alt="Figure 18.7 – High-level HPA flow"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.7 – High-level HPA flow</p>
			<p>In the <a id="_idIndexMarker1046"/>preceding diagram, we <a id="_idIndexMarker1047"/>can see <span class="No-Break">the following:</span></p>
			<ol>
				<li>HPA reads metrics from the <span class="No-Break">metrics server.</span></li>
				<li>There is a control loop that triggers HPA to read the metrics every <span class="No-Break">15 seconds.</span></li>
				<li>HPA assesses these metrics against the desired state of the autoscaling configuration and will scale the deployment <span class="No-Break">if needed.</span></li>
			</ol>
			<p>Now we’ve looked at the concepts behind the HPA, let’s configure and <span class="No-Break">test it.</span></p>
			<h2 id="_idParaDest-273"><a id="_idTextAnchor303"/>Installing HPA in your EKS cluster</h2>
			<p>As we’ve <a id="_idIndexMarker1048"/>discussed, HPA is <a id="_idIndexMarker1049"/>a feature of K8s, so no installation is necessary; however, it does depend on K8s Metrics Server. To check whether Metrics Server is installed and providing data, the following commands can <span class="No-Break">be used:</span></p>
			<pre class="source-code">
$ kubectl -n kube-system get deployment/metrics-server
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
metrics-server   1/1     1            1           34h
$ kubectl top pod -n kube-system
NAME                              CPU(cores)   MEMORY(bytes)
aws-node-2tx2g                    3m           38Mi
aws-node-9rhjs                    3m           38Mi
coredns-7b6fd76bcb-6h9b6          1m           12Mi
coredns-7b6fd76bcb-vsvv8          1m           12Mi
kube-proxy-rn96m                  1m           11Mi
kube-proxy-tf8d9                  1m           10Mi
metrics-server-68c56b9d8c-24mng   3m           17Mi</pre>
			<p>if you <a id="_idIndexMarker1050"/>don’t have Metrics <a id="_idIndexMarker1051"/>Server installed, you can use the following command to install the <span class="No-Break">latest version:</span></p>
			<pre class="source-code">
<strong class="bold">$ kubectl apply -f </strong><a href="https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml">https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml</a></pre>
			<p>Now we have HPA’s prerequisites installed, let’s test whether it <span class="No-Break">is working.</span></p>
			<h2 id="_idParaDest-274"><a id="_idTextAnchor304"/>Testing HPA autoscaling</h2>
			<p>To test this, we will <a id="_idIndexMarker1052"/>just use the K8s example to deploy a standard web server based on the <strong class="source-inline">php-apache</strong> container image. We then add the HPA autoscaling configuration and then use a load generator to generate load and push the metrics higher to trigger HPA to scale out the deployment. The K8s <a id="_idIndexMarker1053"/>manifest file used is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  selector:
    matchLabels:
      run: php-apache
  template:
    metadata:
      labels:
        run: php-apache
    spec:
      containers:
      - name: php-apache
        image: registry.k8s.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m
---
apiVersion: v1
kind: Service
metadata:
  name: php-apache
  labels:
    run: php-apache
spec:
  ports:
  - port: 80
  selector:
    run: php-apache</pre>
			<p>We can <a id="_idIndexMarker1054"/>now deploy this manifest and add the autoscaling configuration to maintain CPU utilization at around 50% across 1-10 pods using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
$ kubectl create -f hpa-deployment.yaml
deployment.apps/php-apache created
service/php-apache created
$ kubectl get all
NAME      READY   STATUS    RESTARTS   AGE
pod/php-apache-11-22   1/1     Running   0          57s
NAME     TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
service/php-apache   ClusterIP   10.1.2.1 &lt;none&gt;  80/TCP   57s
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/php-apache   1/1     1            1    58s
NAME           DESIRED   CURRENT   READY   AGE
replicaset.apps/php-apache-1122   1         1         1   58s
$ kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
horizontalpodautoscaler.autoscaling/php-apache autoscaled
$ kubectl get hpa
NAME    REFERENCE  TARGETS   MINPODS   MAXPODS REPLICAS   AGE
php-apache Deployment/php-apache   0%/50% 1  10   1       19s</pre>
			<p>Now that <a id="_idIndexMarker1055"/>we’ve added the autoscaling configuration, we can generate some load. As the CPU utilization of the pods will increase under the load, HPA will modify the deployment, scaling in and out in an effort to maintain the CPU utilization <span class="No-Break">at 50%.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">You will need several terminal sessions for <span class="No-Break">this exercise.</span></p>
			<p>In the first terminal session, run the following command to generate the <span class="No-Break">additional load:</span></p>
			<pre class="source-code">
<strong class="bold">$ kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- </strong>http://php-apache;<strong class="bold"> done"</strong>
<strong class="bold">If you don't see a command prompt, try pressing enter.</strong>
<strong class="bold">OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!.</strong></pre>
			<p>In the second terminal, we can look at the HPA stats and will see the number of replicas gradually increase as HPA scales the deployment to cope with the <span class="No-Break">increased load.</span></p>
			<p>You can see that <strong class="source-inline">TARGETS</strong> (the third column in the following output) is initially higher than the 50% target as the load increases, and then as HPA adds more replicas the values come down, until it reaches under the 50% target with 5 replicas. This means HPA should not add any <span class="No-Break">further replicas:</span></p>
			<pre class="source-code">
$ kubectl get hpa php-apache --watch
NAME  REFERENCE   TARGETS  MINPODS   MAXPODS   REPLICAS   AGE
php-apache Deployment/php-apache   119%/50%   1   10  1   10m
php-apache Deployment/php-apache   119%/50%   1   10  3   10m
php-apache Deployment/php-apache   188%/50%   1   10  3   10m
php-apache Deployment/php-apache   88%/50%    1   10  4   11m
php-apache Deployment/php-apache   77%/50%    1   10  4   11m
php-apache Deployment/php-apache   48%/50%    1   10  5   11m</pre>
			<p>If you <a id="_idIndexMarker1056"/>now terminate the container running in the first session, you will see HPA scale back the deployment. The output of the <strong class="source-inline">kubectl get hpa php-apache --watch</strong> command is shown next, demonstrating the current load value dropping to 0 and HPA scaling back to <span class="No-Break">1 replica:</span></p>
			<pre class="source-code">
NAME  REFERENCE  TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
php-apache Deployment/php-apache 119%/50%   1  10  1      10m
php-apache Deployment/php-apache 119%/50%   1  10  3      10m
………………………………………
php-apache Deployment/php-apache 0%/50%  1    10  5     16m
php-apache Deployment/php-apache 0%/50%  1    10  1     16m</pre>
			<p>HPA can query core metrics through the <strong class="source-inline">metrics.k8s.io</strong> K8s API endpoint, and can also <a id="_idIndexMarker1057"/>query custom metrics using the <strong class="source-inline">external.metrics.k8s.io</strong> or <strong class="source-inline">custom.metrics.k8s.io</strong> API endpoints. With more complex applications, you need to monitor more than just <strong class="bold">CPU</strong> and <strong class="bold">memory</strong>, so let’s look at how we can use custom metrics to scale <span class="No-Break">our application.</span></p>
			<h1 id="_idParaDest-275"><a id="_idTextAnchor305"/>Autoscaling applications with custom metrics</h1>
			<p>In order <a id="_idIndexMarker1058"/>for you to use custom metrics, the <a id="_idIndexMarker1059"/>following must <span class="No-Break">be fulfilled:</span></p>
			<ol>
				<li>Your application needs to be instrumented to <span class="No-Break">produce metrics.</span></li>
				<li>These metrics need to be exposed through the <span class="No-Break"><strong class="source-inline">custom.metrics.k8s.io</strong></span><span class="No-Break"> endpoint.</span></li>
			</ol>
			<p>The application developer or dev team is responsible for point 1, and we will install and use Prometheus and the Prometheus adapter to satisfy point 2. The following diagram illustrates the high-level flow of <span class="No-Break">this solution.</span></p>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="image/Figure_18.08_B18129.jpg" alt="Figure 18.8 – HPA custom metrics high-level flow"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.8 – HPA custom metrics high-level flow</p>
			<p>Let’s look at the flow <span class="No-Break">in brief.</span></p>
			<ol>
				<li>The Prometheus <a id="_idIndexMarker1060"/>server installed in <a id="_idIndexMarker1061"/>your cluster will “scrape” custom metrics from <span class="No-Break">your pods.</span></li>
				<li>HPA will do <span class="No-Break">the following:</span><ol><li>Read metrics from the <strong class="source-inline">custom.metrics.k8s.io</strong> custom endpoint, hosted on the <span class="No-Break">Prometheus adapter.</span></li><li>The Prometheus adapter will pull data from the <span class="No-Break">Prometheus server.</span></li></ol></li>
				<li>HPA assesses these metrics against the desired state of the autoscaling configuration. This assessment will reference custom metrics such as the average number of HTTP requests/second, and HPA will scale the deployment <span class="No-Break">if needed.</span></li>
			</ol>
			<p>Now we’ve looked at the concepts behind HPA custom metrics, let’s install the prerequisites and <span class="No-Break">test it.</span></p>
			<h2 id="_idParaDest-276"><a id="_idTextAnchor306"/>Installing the Prometheus components in your EKS cluster</h2>
			<p>We will <a id="_idIndexMarker1062"/>first install <a id="_idIndexMarker1063"/>a local Prometheus server in <a id="_idIndexMarker1064"/>the cluster using Helm. The first set of commands shown in the following code are used to get the latest Prometheus <span class="No-Break">server chart:</span></p>
			<pre class="source-code">
$ kubectl create namespace prometheus
namespace/prometheus created
$ helm repo add prometheus-community <a href="https://prometheus-community.github.io/helm-charts">https://prometheus-community.github.io/helm-charts</a>
"prometheus-community" has been added to your repositories
$ helm repo update
Hang tight while we grab the latest from your chart repositories…
….....
…Successfully got an update from the "prometheus-community" chart repository
…Successfully got an update from the "stable" chart repository
Update Complete. Happy Helming</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">You can use AWS Managed Service for Prometheus instead if you <span class="No-Break">so desire.</span></p>
			<p>We can now install the chart and verify the pods all start using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
$ helm upgrade -i prometheus prometheus-community/prometheus     --namespace prometheus     --set alertmanager.persistentVolume.storageClass="gp2",server.persistentVolume.storageClass="gp2"
Release "prometheus" does not exist. Installing it now.
NAME: prometheus
LAST DEPLOYED: Sun May  7 13:16:28 2023
NAMESPACE: prometheus
STATUS: deployed
REVISION: 1
TEST SUITE: None
……….
For more information on running Prometheus, visit:
<a href="https://prometheus.io/">https://prometheus.io/</a>
$ kubectl get po -n prometheus
NAME          READY   STATUS    RESTARTS   AGE
prometheus-alertmanager-0  1/1     Running   0       19m
prometheus-kube-state-metrics-12  1/1  Running   0 19m
prometheus-prometheus-node-exporter-12 1/1 Running   0    19m
prometheus-prometheus-node-exporter-12  1/1 Running   0   19m
prometheus-prometheus-pushgateway-13   1/1  Running  0    19m
prometheus-server-677fbf6f-14       2/2     Running   0   19m</pre>
			<p>To <a id="_idIndexMarker1065"/>verify <a id="_idIndexMarker1066"/>that <a id="_idIndexMarker1067"/>Prometheus is working, we can use the port forward command shown next and then use a local browser to navigate <span class="No-Break">to </span><span class="No-Break">http://localhost:8080</span><span class="No-Break">:</span></p>
			<pre class="source-code">
$ kubectl --namespace=prometheus port-forward deploy/prometheus-server 8080:9090
Forwarding from 127.0.0.1:8080 -&gt; 9090
Forwarding from [::1]:8080 -&gt; 9090</pre>
			<p>We can <a id="_idIndexMarker1068"/>then use the metrics explorer (the icon is highlighted in the following screenshot) to get data in table or graph formats. The <a id="_idIndexMarker1069"/>example in the following screenshot <a id="_idIndexMarker1070"/>is for the <span class="No-Break"><strong class="source-inline">container_memory_usage_bytes</strong></span><span class="No-Break"> metric.</span></p>
			<div>
				<div id="_idContainer171" class="IMG---Figure">
					<img src="image/Figure_18.09_B18129.jpg" alt="Figure 18.9 – Prometheus server displaying metric data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.9 – Prometheus server displaying metric data</p>
			<p>Now we have <a id="_idIndexMarker1071"/>a Prometheus server instance installed and working, we can install the <strong class="bold">podinfo</strong> application. This is a small microservice often used for testing and exposes a number of metrics and <span class="No-Break">health APIs.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="source-inline">podinfo</strong> requires at least Kubernetes 1.23, so please make sure your cluster is running the correct version. We will also install an HPA configuration that we will replace <span class="No-Break">later on.</span></p>
			<p>Using the <a id="_idIndexMarker1072"/>following command we can <a id="_idIndexMarker1073"/>deploy the pod, its service, and <a id="_idIndexMarker1074"/>the <span class="No-Break">HPA configuration:</span></p>
			<pre class="source-code">
$ kubectl version help
…
Server Version: version.Info{<strong class="bold">Major:"1", Minor:"23+",</strong> GitVersion:"v1.23.17-eks-0a21954", GitCommit:"cd5c12c51b0899612375453f7a7c2e7b6563f5e9", GitTreeState:"clean", BuildDate:"2023-04-15T00:32:27Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/amd64"}
$ kubectl create ns podinfo
namespace/podinfo created
$ kubectl apply -k github.com/stefanprodan/podinfo/kustomize -n podinfo
service/podinfo created
deployment.apps/podinfo created
horizontalpodautoscaler.autoscaling/podinfo created
$ kubectl get hpa -n podinfo
NAME  REFERENCE  TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
podinfo   Deployment/podinfo   2%/99%    2   4     2  13m
$ kubectl get po -n podinfo
NAME                       READY   STATUS    RESTARTS   AGE
podinfo-78989955bc-12   1/1     Running   0          35m
podinfo-78989955bc-13   1/1     Running   0          13m</pre>
			<p>if we look at the <strong class="source-inline">deployment.yaml</strong> file in the <strong class="source-inline">podinfo</strong> GitHub repository, we can see the following <span class="No-Break">two annotations:</span></p>
			<pre class="source-code">
        prometheus.io/scrape: "true"
        prometheus.io/port: "9797"</pre>
			<p>This <a id="_idIndexMarker1075"/>means that Prometheus can automatically scrape the <strong class="source-inline">/metrics</strong> endpoint on port <strong class="source-inline">9797</strong>, so if we <a id="_idIndexMarker1076"/>look in the Prometheus server (using port forwarding) we can see that one of the metrics being collected from the <strong class="source-inline">podinfo</strong> <a id="_idIndexMarker1077"/>pods is http_requests_total, which we will use as our custom metric. This is illustrated in the <span class="No-Break">following screenshot.</span></p>
			<div>
				<div id="_idContainer172" class="IMG---Figure">
					<img src="image/Figure_18.10_B18129.jpg" alt="Figure 18.10 – Reviewing podinfo custom metric data in Prometheus"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.10 – Reviewing podinfo custom metric data in Prometheus</p>
			<p>As we now have a working Prometheus server collecting custom metrics from our application (podinfo deployment), we next have to connect these metrics to the <strong class="source-inline">custom.metrics.k8s.io</strong> endpoint by installing the Prometheus adapter using the <span class="No-Break">following commands.</span></p>
			<p>The adapter will be installed in the Prometheus namespace and will point to the local Prometheus server and port. The adapter will have the default set of metrics configured to start, which we can see by querying the <strong class="source-inline">custom.metrics.k8s.io</strong> endpoint using the <strong class="source-inline">$ kubectl get --raw</strong> command <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
$ helm install prometheus-adapter prometheus-community/prometheus-adapter  --set prometheus.url="http://prometheus-server.prometheus.svc",prometheus.port="80" --set image.tag="v0.10.0" --set rbac.create="true" --namespace prometheusNAME: prometheus-adapter
…..
$ kubectl get po -n prometheus
NAME        READY   STATUS    RESTARTS   AGE
prometheus-adapter-123-chsmv  1/1     Running   0    92s
….
$ kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1 | jq .
{
  "kind": "APIResourceList",
  "apiVersion": "v1",
  "groupVersion": "custom.metrics.k8s.io/v1beta1",
  "resources": [
    {
      "name": "nodes/node_vmstat_pgpgin",
…..</pre>
			<p>The easiest <a id="_idIndexMarker1078"/>way to configure our metrics <a id="_idIndexMarker1079"/>is to replace the default <strong class="source-inline">prometheus-adapter</strong> ConfigMap with <a id="_idIndexMarker1080"/>the configuration <a id="_idIndexMarker1081"/>shown next, which only contains the rules for the <strong class="source-inline">http_requests_total</strong><em class="italic"> </em>metric exported from the <strong class="source-inline">podinfo</strong> application using the <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">metrics</strong></span><span class="No-Break"> endpoint:</span></p>
			<pre class="source-code">
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-adapter
  namespace: prometheus
data:
  config.yaml: |
    rules:
    - seriesQuery: 'http_requests_total'
      resources:
        overrides:
          namespace:
            resource: "namespace"
          pod:
            resource: "pod"
      name:
        matches: "^(.*)_total"
        as: "${1}_per_second"
      metricsQuery: 'rate(http_requests_total{namespace="podinfo",app="podinfo"}[2m])'</pre>
			<p>We can <a id="_idIndexMarker1082"/>now replace the ConfigMap with <a id="_idIndexMarker1083"/>the configuration <a id="_idIndexMarker1084"/>shown previously, restart the <strong class="bold">Deployment</strong> to re-read the new ConfigMap, and query the metrics through the custom endpoint using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
$ kubectl replace cm prometheus-adapter -n prometheus -f cm-adapter.yaml
configmap/prometheus-adapter replaced
$ kubectl rollout restart deployment prometheus-adapter -n prometheus
deployment.apps/prometheus-adapter restarted
$ kubectl get po -n prometheus
NAME     READY   STATUS    RESTARTS   AGE
prometheus-adapter-123-h7ztg  1/1     Running   0    60s
……
$ kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1 | jq .
{
  "kind": "APIResourceList",
  "apiVersion": "v1",
  "groupVersion": "custom.metrics.k8s.io/v1beta1",
  "resources": [
    {
      "name": "pods/http_requests_per_second",
      "singularName": "",
      "namespaced": true,
      "kind": "MetricValueList",
      "verbs": [
        "get"
      ]
    },
    {
      "name": "namespaces/http_requests_per_second",
      "singularName": "",
      "namespaced": false,
      "kind": "MetricValueList",
      "verbs": [
        "get"
      ]}]}
$ kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/podinfo/pods/*/http_requests_per_second" | jq .
{
  "kind": "MetricValueList",
  "apiVersion": "custom.metrics.k8s.io/v1beta1",
  "metadata": {},
  "items": [
    {
      "describedObject": {
        "kind": "Pod",
        "namespace": "podinfo",
        "name": "podinfo-78989955bc-89n8m",
        "apiVersion": "/v1"
      },
      "metricName": "http_requests_per_second",
      "timestamp": "2023-05-08T22:42:11Z",
      "value": "200m",
      "selector": null
    },
    {
      "describedObject": {
        "kind": "Pod",
        "namespace": "podinfo",
        "name": "podinfo-78989955bc-rnr9c",
        "apiVersion": "/v1"
      },
      "metricName": "http_requests_per_second",
      "timestamp": "2023-05-08T22:42:11Z",
      "value": "200m",
      "selector": null
    }]}</pre>
			<p>Now we <a id="_idIndexMarker1085"/>have the <strong class="source-inline">podinfo</strong> metrics being exposed through the custom metric endpoint, we can replace <a id="_idIndexMarker1086"/>the HPA configuration with one <a id="_idIndexMarker1087"/>that uses the custom metrics rather than the standard CPU one it was deployed with. To do this, we use the <span class="No-Break">following configuration:</span></p>
			<pre class="source-code">
---
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: podinfo
  namespace: podinfo
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: podinfo
  minReplicas: 1
  maxReplicas: 10
  metrics:
    - type: Pods
      pods:
        metricName: http_requests_per_second
        targetAverageValue: 10</pre>
			<p>We <a id="_idIndexMarker1088"/>can use the following <a id="_idIndexMarker1089"/>commands to replace the HPA <a id="_idIndexMarker1090"/>configuration and <span class="No-Break">validate it:</span></p>
			<pre class="source-code">
$ kubectl replace -f hpa-requests.yaml
Warning: autoscaling/v2beta1 HorizontalPodAutoscaler is deprecated in v1.22+, unavailable in v1.25+; use autoscaling/v2 HorizontalPodAutoscaler
horizontalpodautoscaler.autoscaling/podinfo replaced
$ kubectl describe hpa podinfo -n podinfo
Warning: autoscaling/v2beta2 HorizontalPodAutoscaler is deprecated in v1.23+, unavailable in v1.26+; use autoscaling/v2 HorizontalPodAutoscaler
Name:                                  podinfo
Namespace:                             podinfo
Labels:                                &lt;none&gt;
Annotations:                           &lt;none&gt;
CreationTimestamp:                     Sun, 07 May 2023 15:30:19 +0000
Reference:                             Deployment/podinfo
Metrics:                               ( current / target )
  "http_requests_per_second" on pods:  200m / 10
Min replicas:                          1
Max replicas:                          10
Deployment pods:                       1 current / 1 desired
…….</pre>
			<p>Now <a id="_idIndexMarker1091"/>we have HPA’s <a id="_idIndexMarker1092"/>prerequisites installed, let’s <a id="_idIndexMarker1093"/>test whether it <span class="No-Break">is working.</span></p>
			<h2 id="_idParaDest-277"><a id="_idTextAnchor307"/>Testing HPA autoscaling with custom metrics</h2>
			<p>To test this, we will <a id="_idIndexMarker1094"/>just use a simple image <a id="_idIndexMarker1095"/>with <strong class="source-inline">curl</strong> installed and call the <strong class="source-inline">podinfo</strong> API repeatedly, increasing the <span class="No-Break">request count.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">You will need several terminal sessions for <span class="No-Break">this exercise.</span></p>
			<p>In the first terminal session, run the following command to <span class="No-Break">generate load:</span></p>
			<pre class="source-code">
$ kubectl run -it load-test --rm --image=nginx -n prometheus – bash
root@load-test:/# curl http://podinfo.podinfo.svc.cluster.local:9898
{
"hostname": "podinfo-78989955bc-zfwdj",
  "version": "6.3.6",
  "revision": "073f1ec5aff930bd3411d33534e91cbe23302324",
  "color": "#34577c",
  "logo": "https://raw.githubusercontent.com/stefanprodan/podinfo/gh-pages/cuddle_clap.gif",
  "message": "greetings from podinfo v6.3.6",
  "goos": "linux",
  "goarch": "amd64",
  "runtime": "go1.20.4",
  "num_goroutine": "9",
  "num_cpu": "2"
}
root@load-test:/# while sleep 0.01; do curl http://podinfo.podinfo.svc.cluster.local:9898; done
……..</pre>
			<p>In the second terminal session, we can look at the HPA stats and see the number of replicas gradually increase as HPA scales the deployment to cope with the <span class="No-Break">increased load.</span></p>
			<p>The third <a id="_idIndexMarker1096"/>column in the following output, <strong class="source-inline">TARGETS</strong>, is initially low but gradually increases as more requests are responded to. Once <a id="_idIndexMarker1097"/>the threshold has been exceeded, then more replicas <span class="No-Break">are added:</span></p>
			<pre class="source-code">
$ kubectl get hpa -n podinfo --watch
NAME   REFERENCE TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
podinfo Deployment/podinfo   200m/10       1 10  1   31h
podinfo   Deployment/podinfo   216m/10     1 10  1   31h
podinfo   Deployment/podinfo   19200m/10   1 10  1   31h
podinfo   Deployment/podinfo   19200m/10   1 10  2   31h
podinfo   Deployment/podinfo   20483m/10   1 10  2   31h
podinfo   Deployment/podinfo   14868m/10   1 10  2   31h
podinfo   Deployment/podinfo   15744m/10   1 10  3   31h</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">m</strong> character shown in the output represents milli-units, which means <span class="No-Break">0.1 req/sec.</span></p>
			<p>If you exit <a id="_idIndexMarker1098"/>the loop and container in the <a id="_idIndexMarker1099"/>first terminal session, HPA will gradually scale down back to a single replica. While this solution works, it is not designed for large production environments. In the final section of this chapter, we will look at <a id="_idIndexMarker1100"/>using <strong class="bold">Kubernetes Event-Driven Autoscaling</strong> (<strong class="bold">KEDA</strong>) for pod autoscaling, which supports large environments and can also use events or metrics from <span class="No-Break">external sources.</span></p>
			<h1 id="_idParaDest-278"><a id="_idTextAnchor308"/>Scaling with Kubernetes Event-Driven Autoscaling</h1>
			<p>KEDA is an open source framework that allows you to scale K8s workloads based on metrics <a id="_idIndexMarker1101"/>or events. We do this by deploying the KEDA operator, which manages all the required components, broadly consisting of <span class="No-Break">the following:</span></p>
			<ul>
				<li>An agent, responsible for scaling the deployment up or down depending <span class="No-Break">on events.</span></li>
				<li>A metrics server that exposes metrics from applications or <span class="No-Break">external sources.</span></li>
				<li>A <strong class="bold">ScaledObject</strong> custom <a id="_idIndexMarker1102"/>resource that maintains the mapping between the external source or metric and the K8s deployment, as well as the scaling rules. This effectively creates a corresponding <span class="No-Break">HPA </span><span class="No-Break"><em class="italic">Kind</em></span><span class="No-Break">.</span></li>
				<li>Internal and external event sources that are used to trigger a <span class="No-Break">KEDA action.</span></li>
			</ul>
			<p>The following diagram illustrates the main <span class="No-Break">KEDA components.</span></p>
			<div>
				<div id="_idContainer173" class="IMG---Figure">
					<img src="image/Figure_18.11_B18129.jpg" alt="Figure 18.11 – Main KEDA components"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.11 – Main KEDA components</p>
			<p>Now we’ve <a id="_idIndexMarker1103"/>looked at the concepts behind the KEDA custom metrics, let’s install and <span class="No-Break">test it.</span></p>
			<h2 id="_idParaDest-279"><a id="_idTextAnchor309"/>Installing the KEDA components in your EKS cluster</h2>
			<p>We will use the existing Prometheus and podinfo deployments we created in the previous <a id="_idIndexMarker1104"/>exercise, but I do suggest first removing the Prometheus adapter using the following command so there are no conflcts <span class="No-Break">to scheduling:</span></p>
			<pre class="source-code">
$ helm delete prometheus-adapter  --namespace prometheus
release "prometheus-adapter" uninstalled</pre>
			<p>Now we can deploy the KEDA operator using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
helm repo add kedacore https://kedacore.github.io/charts
helm repo update
kubectl create namespace keda
helm install keda kedacore/keda --namespace keda –version 2.9.4
$ kubectl get po -n keda
NAME                READY   STATUS    RESTARTS   AGE
keda-operator-123-5cv    1/1     Running   0          7m49s
keda-operator-metrics-apiserver-123   1/1     Running     7m49s</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">As we are using K8s version 1.23, we need to install a 2.9 release. At the time of writing, 2.9.4 is the latest of these. You can use the <strong class="source-inline">helm search repo kedacore -l</strong> command to get the latest <span class="No-Break">chart versions.</span></p>
			<p>We now <a id="_idIndexMarker1105"/>need to deploy <strong class="source-inline">ScaledObject</strong> to tell KEDA what to monitor (metric name), what the external source is (Prometheus), what to scale (the podinfo deployment), and what the threshold is (10). Please note that in the example configuration shown here, we have set <strong class="source-inline">minReplicaCount</strong> to <strong class="source-inline">2</strong>, but the default <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">0</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: prometheus-scaledobject
  namespace: podinfo
spec:
  scaleTargetRef:
    name: podinfo
  minReplicaCount:  2
  maxReplicaCount:  10
  pollingInterval: 10
  cooldownPeriod:  30
  fallback:
    failureThreshold: 3
    replicas: 2
  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-server.prometheus.svc.cluster.local:80
      metricName: http_requests_total
      threshold: '10'
      query: sum(rate(http_requests_total{namespace="podinfo",app="podinfo"}[2m]))</pre>
			<p>We can <a id="_idIndexMarker1106"/>use the following commands to deploy and verify this configuration. Looking at the HPA configuration, we can see two configurations – the one managed by KEDA, <strong class="source-inline">keda-hpa-prometheus-scaledobject</strong>, and the one deployed with the original <span class="No-Break">HPA-based scheduling:</span></p>
			<pre class="source-code">
$ kubectl create -f scaledobject.yaml
scaledobject.keda.sh/prometheus-scaledobject created
$ kubectl get ScaledObject -n podinfo
AME                      SCALETARGETKIND      SCALETARGETNAME    MIN   MAX   TRIGGERS     AUTHENTICATION   READY   ACTIVE   FALLBACK   AGE
prometheus-scaledobject   apps/v1.Deployment   podinfo           2     10    prometheus                    True    True     True       4m25s                    False   False    False
$ kubectl get hpa -n podinfo
NAME    REFERENCE  TARGETS  MINPODS   MAXPODS   REPLICAS   AGE
keda-hpa-prometheus-scaledobject   Deployment/podinfo   200m/10 (avg)   2         10        2          4m42s
podinfo                            Deployment/podinfo   2%/99%  2         4         2          84m</pre>
			<p>Now <a id="_idIndexMarker1107"/>we have KEDA installed and an <strong class="source-inline">ACTIVE</strong> scaling configuration deployed, so let’s test that it <span class="No-Break">is working.</span></p>
			<h2 id="_idParaDest-280"><a id="_idTextAnchor310"/>Testing KEDA autoscaling</h2>
			<p>To test <a id="_idIndexMarker1108"/>this, we will just use a simple image with curl installed and will call the podinfo API repeatedly, increasing the <span class="No-Break">request count.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">You will need several terminal sessions for <span class="No-Break">this exercise.</span></p>
			<p>In the first terminal session, run the following command to <span class="No-Break">generate load:</span></p>
			<pre class="source-code">
$ kubectl run -it load-test --rm --image=nginx -n prometheus -- bash
root@load-test:/# while sleep 0.01; do curl http://podinfo.podinfo.svc.cluster.local:9898; done
……..</pre>
			<p>In the second terminal session, you can look at the HPA stats and see the number of replicas gradually increase as HPA scales the deployment to cope with the <span class="No-Break">increased load.</span></p>
			<p>The third column in the following output, <strong class="source-inline">TARGETS</strong>, is initially low and increases as more requests are responded to. Once the threshold has been exceeded then more replicas are added. You will notice that the number of replicas fluctuates, which is because, unlike native HPA, KEDA dynamically adjusts the replica count to meet the <span class="No-Break">incoming demand.</span></p>
			<p>If we had left <strong class="source-inline">minReplicaCount</strong> at <strong class="source-inline">0</strong>, we would have seen <span class="No-Break">greater fluctuation.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">This highlights a key consideration for using KEDA: if you need to cope with fluctuating, non-deterministic demand, then KEDA is an <span class="No-Break">ideal choice.</span></p>
			<p>The <a id="_idIndexMarker1109"/>following commands will show HPA scaling in and out <span class="No-Break">the pods:</span></p>
			<pre class="source-code">
$ kubectl get hpa keda-hpa-prometheus-scaledobject -n podinfo --watch
NAME   REFERENCE  TARGETS  MINPODS   MAXPODS   REPLICAS   AGE
keda-hpa-prometheus-scaledobject   Deployment/podinfo   16742m/10 (avg)   2         10        2          16m
keda-hpa-prometheus-scaledobject   Deployment/podinfo   8371m/10 (avg)   2         10        4          16m
keda-hpa-prometheus-scaledobject   Deployment/podinfo   16742m/10 (avg)   2         10        2          16m
keda-hpa-prometheus-scaledobject   Deployment/podinfo   8371m/10 (avg)    2         10        4          16m
keda-hpa-prometheus-scaledobject   Deployment/podinfo   4961m/10 (avg)    2         10        3          17m</pre>
			<p>If you exit the loop and container in the first terminal session, KEDA will quickly scale back down to the value set <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">minReplicaCount</strong></span><span class="No-Break">.</span></p>
			<p>In this section, we have looked at different ways to scale sour clusters and their workloads. We’ll now revisit the key learning points from <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-281"><a id="_idTextAnchor311"/>Summary</h1>
			<p>In this chapter, we looked at the different ways to scale EKS compute nodes (EC2) to increase resilience and/or performance. We reviewed the different scaling dimensions for our clusters and then set up node group/ASG scaling using the standard K8s CA. We then discussed how CA can take some time to operate and is restricted to ASGs, and how Karpenter can be used to scale much more quickly without the need for node groups, which means you can configure lots of different instance types. We deployed Karpenter and then showed how it can be used to scale EC2-based worker nodes up and down more quickly than CA using different instance types to the existing <span class="No-Break">node groups.</span></p>
			<p>Once we reviewed how to scale worker nodes, we discussed how we can use HPA to scale pods across our worker nodes. We first looked at basic HPA functionality, which uses K8s Metrics Server to monitor pod CPU and memory statistics to add or remove pods from a deployment as required. We then considered that complex applications usually need to scale based on different, specific metrics and examined how we could deploy a local Prometheus server and the Prometheus adapter to take custom application metrics and expose them through the K8s custom metrics endpoint and scale our deployment based on these <span class="No-Break">custom metrics.</span></p>
			<p>Finally, we reviewed how we can use KEDA to employ custom metrics or external data sources to cope with fluctuating demand and scale pods up and down very quickly based on <span class="No-Break">these events.</span></p>
			<p>In the next chapter, we will look at how we can develop on EKS, covering AWS tools such as Cloud9 and CI/CD tools such as <span class="No-Break">Argo CD.</span></p>
			<h1 id="_idParaDest-282"><a id="_idTextAnchor312"/>Further reading</h1>
			<ul>
				<li>EKS <span class="No-Break">observability tools:</span></li>
			</ul>
			<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html"><span class="No-Break">https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html</span></a></p>
			<ul>
				<li>Getting to <span class="No-Break">know podinfo:</span></li>
			</ul>
			<p><a href="https://github.com/stefanprodan/podinfo"><span class="No-Break">https://github.com/stefanprodan/podinfo</span></a></p>
			<ul>
				<li>Using Managed Service for Prometheus with a <span class="No-Break">Prometheus adapter:</span></li>
			</ul>
			<p><a href="https://aws.amazon.com/blogs/mt/automated-scaling-of-applications-running-on-eks-using-custom-metric-collected-by-amazon-prometheus-using-prometheus-adapter/"><span class="No-Break">https://aws.amazon.com/blogs/mt/automated-scaling-of-applications-running-on-eks-using-custom-metric-collected-by-amazon-prometheus-using-prometheus-adapter/</span></a></p>
			<ul>
				<li>Getting to <span class="No-Break">know KEDA:</span></li>
			</ul>
			<p><a href="https://keda.sh/docs/2.10/concepts/"><span class="No-Break">https://keda.sh/docs/2.10/concepts/</span></a></p>
			<ul>
				<li>Which KEDA version supports which <span class="No-Break">K8s versions?:</span></li>
			</ul>
			<p><a href="https://keda.sh/docs/2.10/operate/cluster/"><span class="No-Break">https://keda.sh/docs/2.10/operate/cluster/</span></a></p>
		</div>
	</body></html>