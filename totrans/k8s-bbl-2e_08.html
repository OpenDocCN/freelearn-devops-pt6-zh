<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer161">
    <h1 class="chapterNumber">8</h1>
    <h1 class="chapterTitle" id="_idParaDest-287">Exposing Your Pods with Services</h1>
    <p class="normal">After reading the previous chapters, you now know how to deploy applications on Kubernetes by building Pods, which can contain one container or multiple containers in the case of more complex applications. You also know that it is possible to decouple applications from their configuration by using Pods and <strong class="keyWord">ConfigMaps</strong> together and that Kubernetes is also capable of storing your sensitive configurations, thanks to <strong class="keyWord">Secret</strong> objects.</p>
    <p class="normal">The good news is that with these three resources, you can start deploying applications on Kubernetes properly and get your first app running. However, you are still missing something important: you need to be able to expose Pods to end users or even to other Pods within the Kubernetes cluster. This is where Kubernetes <strong class="keyWord">Services</strong> comes in, and that’s the concept we’re going to discover now!</p>
    <p class="normal">In this chapter, we’ll learn about a new Kubernetes resource type called the Service. Since Kubernetes Services is a big topic with many things to cover, this chapter will be quite big with a lot of information. But after you master these Services, you’re going to be able to expose your Pods and get your end users to your apps!</p>
    <p class="normal">Services are also a key concept to master <strong class="keyWord">high availability</strong> (<strong class="keyWord">HA</strong>) and redundancy in your Kubernetes setup. Put simply, it is crucial to master them to be effective with Kubernetes!</p>
    <p class="normal">In this chapter, we’re going to cover the following main topics:</p>
    <ul>
      <li class="bulletList">Why would you want to expose your Pods?</li>
      <li class="bulletList">The <code class="inlineCode">NodePort</code> Service</li>
      <li class="bulletList">The <code class="inlineCode">ClusterIP</code> Service</li>
      <li class="bulletList">The <code class="inlineCode">LoadBalancer</code> Service</li>
      <li class="bulletList">The <code class="inlineCode">ExternalName</code> Service type</li>
      <li class="bulletList">Implementing Service readiness using probes</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-288">Technical requirements</h1>
    <p class="normal">To follow along with the examples in this chapter, make sure you have the following:</p>
    <ul>
      <li class="bulletList">A working Kubernetes cluster (whether this is local or cloud-based is of no importance)</li>
      <li class="bulletList">A working <code class="inlineCode">kubectl</code> <strong class="keyWord">command-line interface</strong> (<strong class="keyWord">CLI</strong>) configured to communicate with the Kubernetes cluster</li>
    </ul>
    <p class="normal">You can download the latest code samples for this chapter from the official GitHub repository at<span class="url"> </span><a href="https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter08"><span class="url">https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter08</span></a>.</p>
    <h1 class="heading-1" id="_idParaDest-289">Why would you want to expose your Pods?</h1>
    <p class="normal">In the previous chapters, we discussed the microservice architecture, which exposes your functionality <a id="_idIndexMarker666"/>through <strong class="keyWord">Representational State Transfer</strong> (<strong class="keyWord">REST</strong>) <strong class="keyWord">application programming interfaces</strong> (<strong class="keyWord">APIs</strong>). These APIs rely completely on <strong class="keyWord">HyperText Transfer Protocol</strong> (<strong class="keyWord">HTTP</strong>), which means that your microservices<a id="_idIndexMarker667"/> must be accessible via the web, and thus via an <strong class="keyWord">Internet Protocol</strong> (<strong class="keyWord">IP</strong>) address<a id="_idIndexMarker668"/> on the network.</p>
    <div class="note">
      <p class="normal">While REST APIs<a id="_idIndexMarker669"/> are commonly used for communication in microservice architectures, it’s important to evaluate other communication protocols such as <strong class="keyWord">gRPC</strong>, particularly <a id="_idIndexMarker670"/>in scenarios where performance and efficiency between services are critical. gRPC, built on <strong class="keyWord">HTTP/2</strong> and using binary serialization (Protocol Buffers), can offer significant <a id="_idIndexMarker671"/>advantages in distributed systems, such as faster communication, lower latency, and support for streaming. Before defaulting to REST, consider whether gRPC might be a better fit for your system’s needs.</p>
    </div>
    <p class="normal">In the following section, we will learn about cluster networking in Kubernetes.</p>
    <h2 class="heading-2" id="_idParaDest-290">Cluster networking in Kubernetes</h2>
    <p class="normal">Networking<a id="_idIndexMarker672"/> is a fundamental aspect of Kubernetes, enabling communication between containers, Pods, Services, and external clients. Understanding how Kubernetes manages networking helps ensure seamless operation in distributed environments. There are four key networking <a id="_idIndexMarker673"/>challenges that Kubernetes addresses:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Container-to-container communication</strong>: This is solved using Pods, allowing containers within the same <a id="_idIndexMarker674"/>Pod to communicate through localhost (i.e., internal communication).</li>
      <li class="bulletList"><strong class="keyWord">Pod-to-pod communication:</strong> Kubernetes enables communication between Pods across nodes through its <a id="_idIndexMarker675"/>networking model.</li>
      <li class="bulletList"><strong class="keyWord">Pod-to-service communication</strong>: Services abstract a set of Pods and provide stable <a id="_idIndexMarker676"/>endpoints for communication.</li>
      <li class="bulletList"><strong class="keyWord">External-to-service communication</strong>: This allows traffic from outside the cluster to access <a id="_idIndexMarker677"/>Services, such as web applications or APIs.</li>
    </ul>
    <p class="normal">In a Kubernetes cluster, multiple applications run on the same set of machines. This creates challenges, such as preventing conflicts when different applications use the same network ports.</p>
    <h3 class="heading-3" id="_idParaDest-291">IP address management in Kubernetes</h3>
    <p class="normal">Kubernetes clusters use <a id="_idIndexMarker678"/>non-overlapping IP address ranges for Pods, Services, and Nodes. The network model is implemented through the following configurations:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pods</strong>: A network<a id="_idIndexMarker679"/> plugin, often a <strong class="keyWord">Container Network Interface </strong>(<strong class="keyWord">CNI</strong>) plugin, assigns IP addresses to Pods.</li>
      <li class="bulletList"><strong class="keyWord">Services</strong>: The <a id="_idIndexMarker680"/>kube-apiserver handles assigning IP addresses to Services.</li>
      <li class="bulletList"><strong class="keyWord">Nodes</strong>: IP addresses for Nodes<a id="_idIndexMarker681"/> are managed by either the kubelet or the cloud-controller-manager.</li>
    </ul>
    <p class="normal">You may refer to the<a id="_idIndexMarker682"/> following website, <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/"><span class="url">https://kubernetes.io/docs/concepts/cluster-administration/networking/</span></a>, to learn more about networking in Kubernetes.</p>
    <p class="normal">Now, before exploring the Pod networking and Services, let us understand some of the basic technologies in the Kubernetes networking space.</p>
    <h2 class="heading-2" id="_idParaDest-292">Learning about network plugins</h2>
    <p class="normal"><strong class="keyWord">CNI</strong> is a <a id="_idIndexMarker683"/>specification and set of libraries developed by the <strong class="keyWord">Cloud Native Computing Foundation</strong> (<strong class="keyWord">CNCF</strong>). Its primary purpose is to standardize the configuration of network<a id="_idIndexMarker684"/> interfaces on Linux containers, enabling seamless communication between containers and the external environment.</p>
    <p class="normal">These plugins are essential for implementing the Kubernetes network model, ensuring connectivity and communication within the cluster. It’s crucial to choose a CNI plugin that aligns with the needs and compatibility requirements of your Kubernetes cluster. With various plugins available in the Kubernetes ecosystem, both open-source and closed-source, selecting the appropriate plugin is vital for smooth cluster operations.</p>
    <p class="normal">Here’s a concise list of CNI plugins with a brief description of each:</p>
    <p class="normal"><strong class="keyWord">Open-source CNI plugins:</strong></p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Calico</strong>: Provides <a id="_idIndexMarker685"/>networking<a id="_idIndexMarker686"/> and network security with a focus on Layer 3 connectivity and fine-grained policy controls.</li>
      <li class="bulletList"><strong class="keyWord">Flannel</strong>: A simple <a id="_idIndexMarker687"/>CNI that offers basic networking for Kubernetes clusters, ideal <a id="_idIndexMarker688"/>for overlay networks.</li>
      <li class="bulletList"><strong class="keyWord">Weave Net</strong>: Focuses on <a id="_idIndexMarker689"/>easy setup and encryption, suitable for both cloud and <a id="_idIndexMarker690"/>on-premises environments.</li>
      <li class="bulletList"><strong class="keyWord">Cilium</strong>: Utilizes <strong class="keyWord">eBPF</strong> for <a id="_idIndexMarker691"/>advanced security features and network <a id="_idIndexMarker692"/>observability, perfect for <a id="_idIndexMarker693"/>microservice architectures.</li>
    </ul>
    <p class="normal"><strong class="keyWord">Closed-source CNI plugins:</strong></p>
    <ul>
      <li class="bulletList"><strong class="keyWord">AWS VPC CNI</strong>: Integrates<a id="_idIndexMarker694"/> Kubernetes Pods directly with AWS VPC for seamless IP management <a id="_idIndexMarker695"/>and connectivity.</li>
      <li class="bulletList"><strong class="keyWord">Azure CNI</strong>: Allows Pods <a id="_idIndexMarker696"/>to use IP addresses from the Azure VNet, ensuring integration with <a id="_idIndexMarker697"/>Azure’s networking infrastructure.</li>
      <li class="bulletList"><strong class="keyWord">VMware NSX-T</strong>: Provides <a id="_idIndexMarker698"/>advanced networking capabilities like<a id="_idIndexMarker699"/> micro-segmentation and security for Kubernetes in VMware environments.</li>
    </ul>
    <p class="normal">In some of the exercises in this book, we will use Calico. Now, let’s find out what a service mesh is.</p>
    <h2 class="heading-2" id="_idParaDest-293">What is a service mesh?</h2>
    <p class="normal">A <strong class="keyWord">service mesh</strong> is an <a id="_idIndexMarker700"/>essential infrastructure layer integrated into microservices architecture, facilitating inter-service communication. It encompasses functionalities like service discovery, load balancing, encryption, authentication, and monitoring, all within the network infrastructure. Typically, service meshes are realized through lightweight proxies deployed alongside individual services, enabling precise control over traffic routing and enforcing policies such as rate limiting and circuit breaking. By abstracting complex networking tasks from developers, service meshes streamline the development, deployment, and management of microservice applications, while <a id="_idIndexMarker701"/>also enhancing reliability, security, and observability. Prominent <a id="_idIndexMarker702"/>service mesh implementations <a id="_idIndexMarker703"/>include <strong class="keyWord">Istio</strong>, <strong class="keyWord">Linkerd</strong>, and <strong class="keyWord">Consul Connect</strong>. However, it’s worth noting that this topic is beyond the scope of this book.</p>
    <p class="normal">Next, we’ll explain what they’re used for and how they can help you expose your Pod-launched microservices.</p>
    <h2 class="heading-2" id="_idParaDest-294">Understanding Pod IP assignment</h2>
    <p class="normal">To understand what Services are, we<a id="_idIndexMarker704"/> need to talk about Pods for a moment once again. On Kubernetes, everything is Pod management: Pods host your applications, and they have a special property. Kubernetes assigns them a private IP address as soon as they are created on your cluster. Keep that in mind because it is super important: each Pod created in your cluster has its unique IP address assigned by Kubernetes.</p>
    <p class="normal">To illustrate this, we’ll <a id="_idIndexMarker705"/>start by creating a nginx Pod. We are using an nginx container image to create a Pod here, but in fact, it would be the same outcome for any container image being used to create a Pod.</p>
    <p class="normal">Let’s do this using the declarative way with the following YAML definition:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># new-nginx-pod.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">new-nginx-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.17</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">new-nginx-container</span>
    <span class="hljs-attr">ports:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
    <p class="normal">As you can see from the previous YAML, this Pod called <code class="inlineCode">new-nginx-pod</code> has nothing special and will just launch a container named <code class="inlineCode">new-nginx-container</code> based on the <code class="inlineCode">nginx:1.17</code> container image.</p>
    <p class="normal">Once we have this YAML file, we can apply it using the following command to get the Pod running on our cluster:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f new-nginx-pod.yaml
pod/new-nginx-pod.yaml created
</code></pre>
    <p class="normal">As soon as this command is called, the Pod gets created on the cluster, and as soon as the Pod is created on the cluster, Kubernetes will assign it an IP address that will be unique.</p>
    <p class="normal">Let’s now retrieve the IP address Kubernetes assigned to our Pod. To do that, we can use the <code class="inlineCode">kubectl get pods</code> command to list the Pods. In the following code snippet, please note the usage of the <code class="inlineCode">-o wide</code> option, which will display the IP address as part of the output:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span> kubectl get po -o wide
NAME            READY   STATUS    RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
new-nginx-pod   1/1     Running   0          99s   10.244.0.109   minikube   &lt;none&gt;           &lt;none&gt;  
</code></pre>
    <p class="normal">In our case, the IP address is <code class="inlineCode">10.244.0.109</code>. This IP address will be unique on my Kubernetes cluster and is assigned to this unique Pod.</p>
    <p class="normal">Of course, if you’re following along on your cluster, you will have a different IP address. This IP address is a private IP version 4 (v4) address and only exists in the Kubernetes cluster. If you try to type this IP address into your web browser, you won’t get anything because this address does not exist on the outside network or public internet; it only exists within your Kubernetes cluster.</p>
    <p class="normal">Regardless<a id="_idIndexMarker706"/> of the cloud platform you<a id="_idIndexMarker707"/> use—whether it’s <strong class="keyWord">Amazon Web Services</strong> (<strong class="keyWord">AWS</strong>), <strong class="keyWord">Google Cloud Platform</strong> (<strong class="keyWord">GCP</strong>), or Azure—the <a id="_idIndexMarker708"/>Kubernetes cluster leverages a network segment provided by the cloud provider. This segment, usually referred to as a <strong class="keyWord">virtual private cloud</strong> (<strong class="keyWord">VPC</strong>), defines a<a id="_idIndexMarker709"/> private and isolated network, similar to the private IP range from your on-premise LAN. In all cases, the CNI plugin used by Kubernetes ensures that each Pod is assigned a unique IP address, providing granular isolation at the Pod level. This rule applies across all cloud and on-premises environments.</p>
    <p class="normal">We will now discover that this IP address assignment is dynamic, as well as finding out about the issues it can cause at scale.</p>
    <h2 class="heading-2" id="_idParaDest-295">Understanding the dynamics of Pod IP assignment in Kubernetes</h2>
    <p class="normal">The IP <a id="_idIndexMarker710"/>addresses assigned to the Pods are not static, and if you delete and recreate a Pod, you’re going to see that the Pod will get a new IP address that’s totally different from the one used before, even if it’s recreated with the exact same YAML configuration. To demonstrate that, let’s delete the Pod and recreate it using the same YAML file, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete -f new-nginx-pod.yaml
pod/new-nginx-pod.yaml deleted
<span class="hljs-con-meta">$ </span>kubectl apply -f new-nginx-pod.yaml
pod/new-nginx-pod.yaml created
</code></pre>
    <p class="normal">We can now run the <code class="inlineCode">kubectl get pods -o wide</code> command once more to figure out that the new IP address is not the same as before, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods -o wide
NAME            READY   STATUS    RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
new-nginx-pod   1/1     Running   0          97s   10.244.0.110   minikube   &lt;none&gt;           &lt;none&gt;
</code></pre>
    <p class="normal">Now, the IP address is <code class="inlineCode">10.244.0.110</code>. This IP address is different from the one we had before, <code class="inlineCode">10.244.0.109</code>.</p>
    <p class="normal">As you can see, when a Pod is destroyed and then recreated, even if you recreate it with the same name and the same configuration, it’s going to have a different IP address.</p>
    <p class="normal">The reason is that technically, it is not the same Pod but two different Pods; that is why Kubernetes assigns two completely different IP addresses.</p>
    <p class="normal">Now, imagine you have <a id="_idIndexMarker711"/>an application accessing that nginx Pod that’s using its IP address to communicate with it. If the nginx Pod gets deleted and recreated for some reason, then your application will be broken because the IP address will not be valid anymore.</p>
    <p class="normal">In the next section, we’ll discuss why hardcoding Pod IP addresses in your application code is not recommended and explore the challenges it presents in a production environment. We’ll also look at more reliable methods for ensuring stable communication between microservices in Kubernetes.</p>
    <h2 class="heading-2" id="_idParaDest-296">Not hardcoding the Pod’s IP address in application development</h2>
    <p class="normal">In production environments, relying on Pod IP addresses for application communication poses a significant challenge. Microservices, designed <a id="_idIndexMarker712"/>to interact through HTTP and relying on TCP/IP, require a reliable method to identify and connect to each other.</p>
    <p class="normal">Therefore, establishing a robust mechanism for retrieving Pod information, not just IP addresses, is crucial. This ensures consistent communication even when Pods are recreated or rescheduled across worker nodes.</p>
    <p class="normal">Crucially, avoid hardcoding Pod IP addresses directly in applications because the Pod IP addresses are dynamic. The ephemeral nature of Pods, which means they can be deleted, recreated, or moved, renders the practice of hardcoding Pod IP addresses in an application’s YAML unreliable. If a Pod with a hardcoded IP is recreated, applications dependent on it will lose connectivity due to the IP resolving to nothing.</p>
    <p class="normal">There are very concrete<a id="_idIndexMarker713"/> cases that we can give where this problem can arise, as follows:</p>
    <ul>
      <li class="bulletList">A Pod running a microservice <em class="italic">A</em> has a dependency and calls a microservice <em class="italic">B</em> that is running as another Pod on the same Kubernetes cluster.</li>
      <li class="bulletList">An application running as a Pod needs to retrieve some data from a <strong class="keyWord">MySQL</strong> server<a id="_idIndexMarker714"/> also running as a Pod on the same Kubernetes cluster.</li>
      <li class="bulletList">An application uses a <strong class="keyWord">Redis cluster</strong> as a<a id="_idIndexMarker715"/> caching engine deployed in multiple Pods on the same cluster.</li>
      <li class="bulletList">Your end users access an application by calling an IP address, and that IP address changes because of a Pod failure.</li>
    </ul>
    <p class="normal">Any time you have an interconnection between services or, indeed, any network communication, this problem will arise.</p>
    <p class="normal">The solution to this problem is the usage of the Kubernetes <strong class="keyWord">Service</strong> resource.</p>
    <p class="normal">The Service object will act as an intermediate object that will remain on your cluster. The Service is not meant to be destroyed, but even if it is destroyed, it can be recreated without any impact, as it is the Service name that it used, not the IP address. In fact, they can remain on your cluster in the long term without causing any issues. Service objects provide a layer of abstraction to expose your application running in Pod(s) at the network level without any code or configuration changes through its life cycle.</p>
    <h2 class="heading-2" id="_idParaDest-297">Understanding how Services route traffic to Pods</h2>
    <p class="normal">Kubernetes <strong class="keyWord">Services</strong> exist as <a id="_idIndexMarker716"/>resources within your cluster and act as an abstraction layer for network traffic management. Services utilize CNI plugins to facilitate communication between clients and the Pods behind the Services. Services achieve this by creating service endpoints, which represent groups of Pods, enabling load balancing and ensuring that traffic reaches healthy instances.</p>
    <p class="normal">Kubernetes Services offer a static and reliable way to access Pods within a cluster. They provide a DNS name that remains constant even if the underlying Pods change due to deployments, scaling, or restarts. Services leverage <strong class="keyWord">service discovery</strong> mechanisms <a id="_idIndexMarker717"/>and internal <strong class="keyWord">load balancing</strong> to <a id="_idIndexMarker718"/>efficiently route traffic to healthy Pods behind the scenes.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_08_01.png"/></figure>
    <p class="packt_figref">Figure 8.1: service-webapp is exposing webapp Pods 1, 2, and 3, whereas service-backendapp is exposing backendapp Pods 1 and 2</p>
    <p class="normal">In fact, Services<a id="_idIndexMarker719"/> are deployed to Kubernetes as a resource type, and just as with most Kubernetes objects, you can deploy them to your cluster using interactive commands or declarative YAML files.</p>
    <p class="normal">Like any other resources in Kubernetes, when you create a Service, you’ll have to give it a name. This name will be used by Kubernetes to build a DNS name that all Pods on your cluster will be able to call. This DNS entry will resolve to your Service, which is supposed to remain on your cluster. The only part that is quite tricky at your end will be to give a list of Pods to expose to your Services: we will discover how to do<a id="_idIndexMarker720"/> that in this chapter. Don’t worry—it’s just a configuration based<a id="_idIndexMarker721"/> on <strong class="keyWord">labels</strong> and <strong class="keyWord">selectors</strong>.</p>
    <p class="normal">Once everything is set up, you can just reach the Pods by calling the Service. This Service will receive the requests and forward them to the Pods. And that’s pretty much it!</p>
    <h2 class="heading-2" id="_idParaDest-298">Understanding round-robin load balancing in Kubernetes</h2>
    <p class="normal">Kubernetes Services, once configured <a id="_idIndexMarker722"/>properly, can expose one or several Pods. When multiple Pods are exposed by the same Pod, the requests are evenly load-balanced to the Pods behind the Service using the round-robin algorithm, as illustrated in the following screenshot:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_08_02.png"/></figure>
    <p class="packt_figref">Figure 8.2: Service A proxies three requests to the three Pods behind it. At scale, each Service will receive 33% of the requests received by the Service</p>
    <p class="normal">Scaling <a id="_idIndexMarker723"/>applications becomes easy. Adding more Pods behind the Service will be enough using Pod replicas. You will learn about Deployments and replicas in <em class="chapterRef">Chapter 11</em>, <em class="italic">Using Kubernetes Deployments for Stateless Workloads</em>. As the Kubernetes <a id="_idIndexMarker724"/>Service has round-robin logic implemented, it can proxy requests evenly to the Pods behind it.</p>
    <div class="note">
      <p class="normal">Kubernetes Services offer more than just round-robin load balancing. While round-robin is commonly used in setups leveraging the IPVS mode of kube-proxy, it’s important to note that iptables, the default mode in many distributions, often distributes traffic using random or hash-based methods instead.</p>
      <p class="normal">Kubernetes also supports additional load-balancing algorithms to meet various needs: the fewest connections for balanced workloads, source IP for consistent routing, and even custom logic for more intricate scenarios. Users should also be aware that IPVS offers more advanced traffic management features like session affinity and traffic shaping, which might not be available in iptables mode.</p>
      <p class="normal">Understanding the mode your cluster is using (either iptables or IPVS) can help in fine-tuning service behavior based on your scaling and traffic distribution requirements. Refer to the documentation to learn more (<a href="https://kubernetes.io/docs/reference/networking/virtual-ips/"><span class="url">https://kubernetes.io/docs/reference/networking/virtual-ips/</span></a>).</p>
    </div>
    <p class="normal">If the preceding Pod had four replicas, then each of them would receive roughly 25% of all the requests the service received. If 50 Pods were behind the Service, each of them would receive roughly 2% of all the requests received by the Service. All you need to understand is that Services behave like load balancers by following a specific load-balancing algorithm.</p>
    <p class="normal">Let’s now discover how you can call a Service in Kubernetes from another Pod.</p>
    <h2 class="heading-2" id="_idParaDest-299">Understanding how to call a Service in Kubernetes</h2>
    <p class="normal">When you create a Service in <a id="_idIndexMarker725"/>Kubernetes, it will be attached to two very important things, as follows:</p>
    <ul>
      <li class="bulletList">An IP address that will be unique and specific to it (just as Pods get their own IP)</li>
      <li class="bulletList">An automatically generated internal DNS name that won’t change and is static</li>
    </ul>
    <p class="normal">You’ll be able to use any of the two in order to reach the Service, which will then forward your request to the Pod it is configured in the backend. Most of the time, though, you’ll call the Service by its generated DNS name, which is easy to determine and predictable. Let’s discover how Kubernetes assigns DNS names to services.</p>
    <h2 class="heading-2" id="_idParaDest-300">Understanding how DNS names are generated for Services</h2>
    <p class="normal">The <a id="_idIndexMarker726"/>DNS name generated for a Service is derived<a id="_idIndexMarker727"/> from its name. For example, if you create a Service named <code class="inlineCode">my-app-service</code>, its DNS name will be <code class="inlineCode">my-app-service.default.svc.cluster.local</code>.</p>
    <p class="normal">This one is quite complicated, so let’s break it into smaller parts, as follows:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_08_03.png"/></figure>
    <p class="packt_figref">Figure 8.3: Service FQDN</p>
    <p class="normal">The two moving parts are the first two, which are basically the Service name and the namespace where it lives. The DNS name will always end with the <code class="inlineCode">.svc.cluster.local</code> string.</p>
    <p class="normal">So, at any moment, from anywhere on your cluster, if you try to use <code class="inlineCode">curl</code> or <code class="inlineCode">wget</code> to call the <code class="inlineCode">my-app-service.default.svc.cluster.local</code> address, you know that you’ll reach your Service.</p>
    <p class="normal">That name will resolve to the Service as soon as it’s executed from a Pod within your cluster. But by default, Services won’t proxy to anything if they are not configured to retrieve a list of the Pods to proxy. We will now discover how to do that!</p>
    <h2 class="heading-2" id="_idParaDest-301">How Services discover and route traffic to Pods in Kubernetes</h2>
    <p class="normal">When working with Services in <a id="_idIndexMarker728"/>Kubernetes, you will often come across the<a id="_idIndexMarker729"/> idea of <em class="italic">exposing</em> your Pods. Indeed, this is the terminology Kubernetes uses to tell that a Service is proxying network traffic to Pods. That terminology is everywhere: your colleagues might ask you one day, “<em class="italic">Which Service is exposing that Pod?</em>” The following screenshot shows Pods being exposed:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_08_04.png"/></figure>
    <p class="packt_figref">Figure 8.4: Webapp Pods 1, 2, and 3 are exposed by service-webapp, whereas backendapp Pods 1 and 2 are exposed by service-backendapp</p>
    <p class="normal">You can successfully create a Pod and a Service to expose it using <code class="inlineCode">kubectl</code> in literally one command, using the <code class="inlineCode">--expose</code> parameter. For the sake of this example, let us create a nginx Pod with Service as follows.</p>
    <p class="normal">We will also need to provide a port to the command to tell on which port the Service will be accessible:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl run nginx --image nginx --expose=<span class="hljs-con-literal">true</span> --port=80
service/nginx created
pod/nginx created
</code></pre>
    <p class="normal">Let’s now list the Pods and the Service using <code class="inlineCode">kubectl</code> to demonstrate that the following command created both objects:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get po,svc nginx
NAME        READY   STATUS    RESTARTS   AGE
pod/nginx   1/1     Running   0          24s
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/nginx   ClusterIP   10.111.12.100   &lt;none&gt;        80/TCP    24s
</code></pre>
    <p class="normal">As you can see based on the output of the command, both objects were created. We said earlier that Services can find the Pods they have to expose based on the Pods’ labels. The <code class="inlineCode">nginx</code> Pod we just created surely has some labels. To show them, let’s run the <code class="inlineCode">kubectl get pods nginx --show-labels</code> command. </p>
    <p class="normal">In the following snippet, pay attention to the <code class="inlineCode">--show-labels</code> parameter, which will display the labels as part of the output:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span> kubectl get po nginx --show-labels
NAME    READY   STATUS    RESTARTS   AGE   LABELS
nginx   1/1     Running   0          51s   run=nginx
</code></pre>
    <p class="normal">As you can see, an <code class="inlineCode">nginx</code> Pod was created with a label called <code class="inlineCode">run</code> and a value of <code class="inlineCode">nginx</code>. Let’s now describe <a id="_idIndexMarker730"/>the <code class="inlineCode">nginx</code> Service. It should have a selector that matches <a id="_idIndexMarker731"/>this label. The code is illustrated here:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl describe svc nginx
Name:              nginx
Namespace:         default
Labels:            &lt;none&gt;
Annotations:       &lt;none&gt;
Selector:          run=nginx
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.111.12.100
IPs:               10.111.12.100
Port:              &lt;unset&gt;  80/TCP
TargetPort:        80/TCP
Endpoints:         10.244.0.9:80
Session Affinity:  None
Events:            &lt;none&gt;
</code></pre>
    <p class="normal">You can clearly see that the Service has a line called <code class="inlineCode">Selector</code> that matches the label assigned to the <code class="inlineCode">nginx</code> Pod. This way, the link between the two objects is made. We’re now 100% sure that the Service can reach the <code class="inlineCode">nginx</code> Pod and that everything should work normally.</p>
    <p class="normal">Please note that if you<a id="_idIndexMarker732"/> are using the minikube in the lab, you will not be able to <a id="_idIndexMarker733"/>access the Service outside of your cluster as ClusterIP Services are only accessible from inside the cluster. You need to use debugging methods such as <code class="inlineCode">kubectl port-forward</code> or <code class="inlineCode">kubectl proxy</code> for such scenarios. You will learn how to test ClusterIP-type Services in the next section.</p>
    <p class="normal">Also, to test the Service access, let us create a temporary port-forwarding, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl port-forward pod/nginx 8080:80
Forwarding from 127.0.0.1:8080 -&gt; 80
Forwarding from [::1]:8080 -&gt; 80
</code></pre>
    <p class="normal">Now, open another console and access URL as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>curl 127.0.0.1:8080
&lt;!DOCTYPE html&gt;
&lt;html&gt;
...&lt;removed for brevity&gt;...
</code></pre>
    <p class="normal">In the preceding snippets, port <code class="inlineCode">8080</code> is the localhost port we used for port-forwarding and 80 is the nginx port where the web Service is exposed.</p>
    <p class="normal">Please note that the <code class="inlineCode">kubectl port-forward</code> command will run until you break it using <em class="keystroke">Ctrl+C</em>.</p>
    <p class="normal">Though it works, we strongly advise you to never do that in production. Services are very customizable objects, and the <code class="inlineCode">--expose</code> parameter hides a lot of their features. Instead, you should really use declarative syntax and tweak the YAML to fit your exact needs.</p>
    <p class="normal">Let’s demonstrate that by using the <code class="inlineCode">dnsutils</code> container image.</p>
    <h2 class="heading-2" id="_idParaDest-302">Using a utility Pod for debugging your Services</h2>
    <p class="normal">As your <a id="_idIndexMarker734"/>Services are <a id="_idIndexMarker735"/>created within your cluster, it is often hard to access them as we mentioned earlier, especially if our Pod is meant to remain accessible only within your cluster, or if your cluster has no internet connectivity, and so on.</p>
    <p class="normal">In this case, it is good to deploy a debugging Pod in your cluster with just some binaries installed into it to run basic networking commands such as <code class="inlineCode">wget</code>, <code class="inlineCode">nslookup</code>, and so on. Let us use our custom utility container image <code class="inlineCode">quay.io/iamgini/k8sutils:debian12</code> for this purpose.</p>
    <div class="note">
      <p class="normal">You can add more tools or utilities inside the utility container image if required; refer to the <code class="inlineCode">Chapter-070/Containerfile</code> for the source.</p>
    </div>
    <p class="normal">Here, we’re going to curl the <code class="inlineCode">nginx</code> Pod home page by calling the Service that is exposing the Pod. That Service’s name is just <code class="inlineCode">nginx</code>. Hence, we can forget the DNS name Kubernetes assigned to it: <code class="inlineCode">nginx.default.svc.cluster.local</code>.</p>
    <p class="normal">If you try to reach this <strong class="keyWord">uniform resource locator</strong> (<strong class="keyWord">URL</strong>) from a Pod within your cluster, you should successfully reach the <code class="inlineCode">nginx</code> home page.</p>
    <p class="normal">The following<a id="_idIndexMarker736"/> Pod definition will help us to create a<a id="_idIndexMarker737"/> debugging pod with the <code class="inlineCode">k8sutils</code> image.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># k8sutils.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">k8sutils</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">k8sutils</span>
      <span class="hljs-attr">image:</span> <span class="code-highlight"><strong class="hljs-string-slc">quay.io/iamgini/k8sutils:debian12</strong></span>
      <span class="hljs-attr">command:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">sleep</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">"infinity"</span>
      <span class="hljs-comment"># imagePullPolicy: IfNotPresent</span>
  <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">Always</span>
</code></pre>
    <p class="normal">Let’s run the following command to launch the <code class="inlineCode">k8sutils</code> Pod on our cluster:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f k8sutils.yaml
pod/k8sutils created
</code></pre>
    <p class="normal">Now run the <code class="inlineCode">kubectl get pods</code> command in order to verify that the Pod was launched successfully, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get po k8sutils
NAME       READY   STATUS    RESTARTS   AGE
k8sutils   1/1     Running   0          13m
</code></pre>
    <p class="normal">That’s perfect! Let’s now run the <code class="inlineCode">nslookup</code> command from the <code class="inlineCode">k8sutils</code> Pod against the Service DNS name, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -it k8sutils -- nslookup nginx.default.svc.cluster.local
Server:         10.96.0.10
Address:        10.96.0.10#53
Name:   nginx.default.svc.cluster.local
Address: 10.106.124.200
</code></pre>
    <p class="normal">In the previous snippet,</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Server: 10.96.0.10</code> - is the <code class="inlineCode">kube-dns</code> Service IP (<code class="inlineCode">kubectl get svc kube-dns -n kube-system -o wide</code>). If you are using a different DNS Service, check the Service details accordingly.</li>
      <li class="bulletList"><code class="inlineCode">nginx.default.svc.cluster.local</code> is resolving to the IP address of <code class="inlineCode">nginx</code> Service (<code class="inlineCode">kubectl get svc nginx -o wide</code>), which is <code class="inlineCode">10.106.124</code>.</li>
    </ul>
    <p class="normal">Everything<a id="_idIndexMarker738"/> looks good. Let’s now run a <code class="inlineCode">curl</code> command to <a id="_idIndexMarker739"/>check whether we can retrieve the <code class="inlineCode">nginx</code> home page, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -it k8sutils -- curl nginx.default.svc.cluster.local
...&lt;removed for brevity&gt;....
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;
...&lt;removed for brevity&gt;...
</code></pre>
    <p class="normal">Everything is perfect here! We successfully called the <code class="inlineCode">nginx</code> Service by using the <code class="inlineCode">k8sutils</code> debug Pod, as illustrated in the following screenshot:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_08_05.png"/></figure>
    <p class="packt_figref">Figure 8.5: The k8sutils Pod is used to run curl against the nginx Service to communicate with the nginx Pod behind the Service</p>
    <p class="normal">Keep in mind <a id="_idIndexMarker740"/>that you need to deploy a <code class="inlineCode">k8sutils</code> Pod inside <a id="_idIndexMarker741"/>the cluster to be able to debug the Service. Indeed, the <code class="inlineCode">nginx.default.svc.cluster.local</code> DNS name is not a public one and can only be accessible from within the cluster.</p>
    <p class="normal">Let’s explain why you should not use <code class="inlineCode">expose</code> imperatively to expose your Pods in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-303">Understanding the drawbacks of direct kubectl expose in Kubernetes</h2>
    <p class="normal">It is not recommended to <a id="_idIndexMarker742"/>use <code class="inlineCode">kubectl expose</code> to create Services because you won’t get much control over how the Service gets created. By default, <code class="inlineCode">kubectl expose</code> will create a <code class="inlineCode">ClusterIP</code> Service, but you might want to create a <code class="inlineCode">NodePort</code> Service.</p>
    <p class="normal">Defining the Service type is also possible using the imperative syntax, but in the end, the command you’ll have to issue is going to be very long and complex to understand. That’s why we encourage you to not use the <code class="inlineCode">expose</code> option and stick with declarative syntax for complex objects such as Services.</p>
    <p class="normal">Let’s now discuss how DNS names are generated in Kubernetes when using Services.</p>
    <h2 class="heading-2" id="_idParaDest-304">Understanding how DNS names are generated for Services</h2>
    <p class="normal">You now know that <a id="_idIndexMarker743"/>Kubernetes Service-to-Pod communication <a id="_idIndexMarker744"/>relies entirely on labels on the Pod side and selectors on the Service side.</p>
    <p class="normal">If you don’t use both correctly, communication cannot be established between the two.</p>
    <p class="normal">The workflow goes like this:</p>
    <ol>
      <li class="numberedList" value="1">You create some Pods, and you set the labels arbitrarily.</li>
      <li class="numberedList">You create a Service and configure its selector to match the Pods’ labels.</li>
      <li class="numberedList">The Service starts and looks for Pods that match its selector.</li>
      <li class="numberedList">You call the Service through its DNS or its IP (DNS is way easier).</li>
      <li class="numberedList">The Service forwards the traffic to one of the Pods that matches its labels.</li>
    </ol>
    <p class="normal">If you look at the previous example achieved using the imperative style with the <code class="inlineCode">kubectl expose</code> parameter, you’ll notice that the Pod and the Services were respectively configured with proper labels (on the Pod side) and selector (on the Service side), which is why the Pod is successfully exposed. Please note that in your real-life cases, you will need to use the appropriate labels for your Pods instead of default labels.</p>
    <p class="normal">Besides that, you must understand now that there are not one but several types of Services in Kubernetes—let us learn more about that.</p>
    <h2 class="heading-2" id="_idParaDest-305">Understanding the different types of Services</h2>
    <p class="normal">There are several types of Services in <a id="_idIndexMarker745"/>Kubernetes. Although there is only one kind called Service in Kubernetes, that kind can be configured differently to achieve different results.</p>
    <p class="normal">Fortunately for us, no matter which type of Service you choose, the goal remains the same: to expose your Pods using a single static interface.</p>
    <p class="normal">Each type of Service has its own function and its own use, so basically, there’s one Service for each use case. A Service cannot be of multiple types at once, but you can still expose the same Pods using two Services’ objects with different types as long as the Services’ objects are named differently so that Kubernetes can assign different DNS names.</p>
    <p class="normal">In this chapter, we will discover the three main types of Services, as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">NodePort</code>: This type <a id="_idIndexMarker746"/>binds a port from an ephemeral port range<a id="_idIndexMarker747"/> of the host machine (the worker node) to a port on the Pod, making it available publicly. By calling the port of the host machine, you’ll reach the associated Kubernetes Pod. That’s the way to reach your Pods for traffic coming from outside your cluster.</li>
      <li class="bulletList"><code class="inlineCode">ClusterIP</code>: The <code class="inlineCode">ClusterIP</code> Service<a id="_idIndexMarker748"/> is the one that should be used for private communication between Pods within the Kubernetes cluster. This is <a id="_idIndexMarker749"/>the one we experimented with in this chapter and is the one created by <code class="inlineCode">kubectl expose</code> by default. This is certainly the most commonly used of them all because it allows inter-communication between Pods: as its name suggests, it has a static IP that is set cluster-wide. By reaching its IP address, you’ll be redirected to the Pod behind it. If more than one Pod is behind it, the <code class="inlineCode">ClusterIP</code> Service will provide a load-balancing mechanism following the round-robin or other algorithms.</li>
      <li class="bulletList"><code class="inlineCode">LoadBalancer</code>: The <code class="inlineCode">LoadBalancer</code><code class="inlineCode"><a id="_idIndexMarker750"/></code> Service<a id="_idIndexMarker751"/> in Kubernetes streamlines the process of exposing your Pods to external traffic. It achieves this by automatically provisioning a cloud-specific load balancer, such as AWS ELB, when operating on supported cloud platforms. This removes the necessity for the manual setup of external load balancers within cloud environments. However, it’s crucial to recognize that this Service is not compatible with bare-metal or non-cloud-managed clusters unless configured manually. While alternative solutions like Terraform exist for managing cloud infrastructure, the <code class="inlineCode">LoadBalancer</code> Service provides a convenient option for seamlessly integrating cloud-native load balancers into your Kubernetes deployments, particularly in cloud-centric scenarios. Keep in mind that its suitability hinges on your specific requirements and infrastructure configuration.</li>
    </ul>
    <p class="normal">Now, let’s immediately dive into the first type of Service—the <code class="inlineCode">NodePort</code> one.</p>
    <p class="normal">As mentioned earlier, this one is going to be very useful to access our Pods from outside the cluster in our development environments, by attaching Pods to the Kubernetes node’s port.</p>
    <h1 class="heading-1" id="_idParaDest-306">The NodePort Service</h1>
    <p class="normal"><code class="inlineCode">NodePort</code> is a <a id="_idIndexMarker752"/>Kubernetes Service<a id="_idIndexMarker753"/> type designed to make Pods reachable from a port available on the host machine, the worker node. In this section, we’re going to discover this type of port and be fully focused on <code class="inlineCode">NodePort</code> Services!</p>
    <h2 class="heading-2" id="_idParaDest-307">Why do you need NodePort Services?</h2>
    <p class="normal">The first thing to <a id="_idIndexMarker754"/>understand is that <code class="inlineCode">NodePort</code> Services allow us to access a Pod running on a Kubernetes node, on a port of the node itself. After you expose Pods using the <code class="inlineCode">NodePort</code> type Service, you’ll be able to reach the Pods by getting the IP address of the node and the port of the <code class="inlineCode">NodePort</code> Service, such as <code class="inlineCode">&lt;node_ip_address&gt;:&lt;node port&gt;</code>.</p>
    <p class="normal">The port can be declared in your YAML declaration or can be randomly assigned by Kubernetes. Let’s illustrate all of this by declaring some Kubernetes objects.</p>
    <p class="normal">Most of the time, the <code class="inlineCode">NodePort</code> Service is used as an entry point to your Kubernetes cluster. In the following example, we will create two Pods based on the <code class="inlineCode">containous/whoami</code> container image available on Docker Hub, which is a very nice container image that will simply print the container hostname.</p>
    <p class="normal">We will create two Pods so that we get two containers with different hostnames, and we will expose them using a <code class="inlineCode">NodePort</code> Service.</p>
    <h2 class="heading-2" id="_idParaDest-308">Creating two containous/whoami Pods</h2>
    <p class="normal">Let’s start by creating <a id="_idIndexMarker755"/>two Pods, without forgetting about adding one or more labels, because we will need labels to tell the Service which Pods it’s going to expose.</p>
    <p class="normal">We are also going to open the port on the Pod side. That won’t make it exposed on its own, but it will open a port the Service will be able to reach. The code is illustrated here:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl run whoami1 --image=containous/whoami --port 80 --labels=<span class="hljs-con-string">"app=whoami"</span>
pod/whoami1 created
<span class="hljs-con-meta">$ </span>kubectl run whoami2 --image=containous/whoami --port 80 --labels=<span class="hljs-con-string">"app=whoami"</span>
pod/whoami2 created
</code></pre>
    <p class="normal">Now, we can run a <code class="inlineCode">kubectl get pods</code> command in order to verify that our two Pods are running correctly. We can also add the <code class="inlineCode">--show-labels</code> parameter in order to display the labels as part of the command output, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods --show-labels
NAME      READY   STATUS    RESTARTS   AGE    LABELS
whoami1   1/1     Running   0          3m5s   app=whoami
whoami2   1/1     Running   0          3m     app=whoami
</code></pre>
    <p class="normal">Everything <a id="_idIndexMarker756"/>seems to be okay! Now that we have two Pods created with a label set for each of them, we will be able to expose them using a Service. We’re now going to discover the YAML manifest file that will create the <code class="inlineCode">NodePort</code> Service to expose these two Pods.</p>
    <h2 class="heading-2" id="_idParaDest-309">Understanding NodePort YAML definition</h2>
    <p class="normal">Since Services are<a id="_idIndexMarker757"/> quite complex <a id="_idIndexMarker758"/>resources, it is better to create Services using a YAML file rather than direct command input.</p>
    <p class="normal">Here is the YAML file that will expose the <code class="inlineCode">whoami1</code> and <code class="inlineCode">whoamo2</code> Pods using a <code class="inlineCode">NodePort</code> Service:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># ~/nodeport-whoami.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nodeport-whoami</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">whoami</span>
  <span class="hljs-attr">ports:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">nodePort:</span> <span class="hljs-number">30001</span>
    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>
    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span>
</code></pre>
    <p class="normal">This YAML can be difficult to understand because it refers to three different ports as well as a <code class="inlineCode">selector</code> block.</p>
    <p class="normal">Before explaining the <a id="_idIndexMarker759"/>YAML file, let’s apply it and check if the Service was correctly created afterwards, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f nodeport-whoami.yaml
service/nodeport-whoami created
<span class="hljs-con-meta">$ </span>kubectl get service nodeport-whoami
NAME              TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
nodeport-whoami   NodePort   10.98.160.98   &lt;none&gt;        80:30001/TCP   14s
</code></pre>
    <p class="normal">The previous <code class="inlineCode">kubectl get services</code> command indicated that the Service was properly created!</p>
    <p class="normal">The selector block is crucial for NodePort Services, acting as a label filter to determine which pods the Service exposes. Essentially, it tells the Service what pods to route traffic to. Without a selector, the Service remains inactive. In this example, the selector targets pods with the label key “app” and the value “<code class="inlineCode">whoami</code>". This effectively exposes both the “<code class="inlineCode">whoami1</code>" and “<code class="inlineCode">whoami2</code>" pods through the Service.</p>
    <p class="normal">Next, we have <code class="inlineCode">type</code> as a child key under <code class="inlineCode">spec, </code>where we specify the type of our Service. When we create <code class="inlineCode">ClusterIP</code> or <code class="inlineCode">LoadBalancer</code> Services, we will have to update this line. Here, we’re creating a <code class="inlineCode">NodePort</code> Service, so that’s fine for us.</p>
    <p class="normal">The last thing that is<a id="_idIndexMarker760"/> quite hard to understand is that <code class="inlineCode">ports</code> block. Here, we define a map of multiple port combinations. We indicated three ports, as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">nodePort</code>: The port on the host machine/worker node you want this <code class="inlineCode">NodePort</code> service to be accessible from. Here, we’re specifying port <code class="inlineCode">30001</code>, which makes this NodePort Service accessible from port<code class="inlineCode"> 30001</code> on the IP address of the worker node. You’ll be reaching this<code class="inlineCode"> NodePort</code> Service and the Pods it exposes by calling the following address: <code class="inlineCode">&lt;WORKER_NODE_IP_ADDRESS&gt;:30001</code>. This <code class="inlineCode">NodePort</code> setting cannot be set arbitrarily. Indeed, on a default Kubernetes installation, it can be a port from the <code class="inlineCode">30000</code> - <code class="inlineCode">32767</code> range.</li>
      <li class="bulletList"><code class="inlineCode">port</code>: This setting indicates the port of the <code class="inlineCode">NodePort</code> Service itself. It can be hard to understand, but <code class="inlineCode">NodePort</code> Services do have a port of their own too, and this is where you specify it. You can put whatever you want here if it is a valid port.</li>
      <li class="bulletList"><code class="inlineCode">targetPort</code>: As you might expect, <code class="inlineCode">targetPort</code> is the port of the targeted Pods. It is where the application runs: the port where the NodePort will forward traffic to the Pod found by the selector mentioned previously.</li>
    </ul>
    <p class="normal">Here is a quick diagram to<a id="_idIndexMarker761"/> sum all of this up:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_08_06.png"/></figure>
    <p class="packt_figref">Figure 8.6: NodePort Service. There are three ports involved in the NodePort setup—nodePort is on the worker n006Fde, the port is on the Service itself, and targetPort is on the top</p>
    <p class="normal">In this case, TCP <a id="_idIndexMarker762"/>port <code class="inlineCode">31001</code> is used as the external port on each Node. If you do not specify <code class="inlineCode">nodePort</code>, it will be allocated <code class="inlineCode">dynamically</code> using the range. For internal communication, this Service still behaves like a simple <code class="inlineCode">ClusterIP</code> Service, and you can use its <code class="inlineCode">ClusterIP</code> address.</p>
    <p class="normal">For convenience and to reduce complexity, the <code class="inlineCode">NodePort</code> Service port and target port (the Pods’ port) are often defined to the same value.</p>
    <h2 class="heading-2" id="_idParaDest-310">Making sure NodePort works as expected</h2>
    <p class="normal">To try out your <code class="inlineCode">NodePort</code> setup, the<a id="_idIndexMarker763"/> first thing to do is to retrieve the public IP of your machine running it. In our example, we are running a single-machine Kubernetes setup with <code class="inlineCode">minikube</code> locally. On AWS, GCP, or Azure, your node might have a public IP address or a private one if you access your node <a id="_idIndexMarker764"/>with a <strong class="keyWord">virtual private network</strong> (<strong class="keyWord">VPN</strong>).</p>
    <p class="normal">On <code class="inlineCode">minikube</code>, the easiest way to retrieve the IP address is to issue the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube ip
192.168.64.2
Or you can access the full URL as follows.
<span class="hljs-con-meta">$ </span>minikube service --url nodeport-whoami
http://192.168.49.2:30001
</code></pre>
    <p class="normal">Now that we have all the information, we can open a web browser and enter the URL to access the <code class="inlineCode">NodePort</code> Service and the Pods running. You should see the round-robin algorithm in place and reaching <code class="inlineCode">whoami1</code> and then <code class="inlineCode">whoami2</code>, and so on. The <code class="inlineCode">NodePort</code> Service is doing its job!</p>
    <h2 class="heading-2" id="_idParaDest-311">Is this setup production-ready?</h2>
    <p class="normal">This question<a id="_idIndexMarker765"/> might not <a id="_idIndexMarker766"/>have a definitive answer as it depends on your configuration.</p>
    <p class="normal"><code class="inlineCode">NodePort</code> provides a way to expose Pods to the outside world by exposing them on a Node port. With the current setup, you have no HA: if your two Pods were to fail, you have no way to relaunch them automatically, so your Service wouldn’t be able to forward traffic to anything, resulting in a poor experience for your end user.</p>
    <div class="note">
      <p class="normal">Please note that when we create the Pods using Deployments and replicasets, Kubernetes will create the new Pods in other available nodes. We will learn about Deployments and replicasets in <em class="chapterRef">Chapter 11</em>, <em class="chapterRef">Using Kubernetes Deployments for Stateless Workloads</em>.</p>
    </div>
    <p class="normal">Another problem is the fact that the choice of port is limited. Indeed, by default, you are just forced to use a port in the <code class="inlineCode">30000-32767</code> range, and as it’s forced, it will be inconvenient for a lot of people. Indeed, if you want to expose an HTTP application, you’ll want to use port <code class="inlineCode">80</code> or <code class="inlineCode">443</code> of your frontal machine rather than a port in the <code class="inlineCode">30000</code>-<code class="inlineCode">32767</code> range, because all<a id="_idIndexMarker767"/> web browsers are configured with ports <code class="inlineCode">80</code> and <code class="inlineCode">443</code> as standard HTTP and <strong class="keyWord">HTTP Secure</strong> (<strong class="keyWord">HTTPS</strong>) ports.</p>
    <p class="normal">The solution to this consists of using a tiered architecture. Indeed, a lot of Kubernetes architects tend to not expose a <code class="inlineCode">NodePort</code> Service as the first layer in architecture but to put the Kubernetes cluster behind a reverse proxy, such as the AWS Application Load Balancer, and so on. Two other concepts of Kubernetes are the <code class="inlineCode">Ingress</code> and <code class="inlineCode">IngressController</code> objects: these two objects allow you to configure a reverse proxy such as <code class="inlineCode">nginx</code> or <a id="_idIndexMarker768"/>HAProxy directly from Kubernetes objects and help you to make your application publicly accessible as the first layer of entry to Kubernetes. But this is way beyond the scope of Kubernetes Services.</p>
    <p class="normal">Let us explore some more information about <code class="inlineCode">NodePort</code> in the next sections, including how to list the Services and how to add Pods to the <code class="inlineCode">NodePort</code> Services.</p>
    <h2 class="heading-2" id="_idParaDest-312">Listing NodePort Services</h2>
    <p class="normal">Listing <code class="inlineCode">NodePort</code> Services is <a id="_idIndexMarker769"/>achieved through the usage of the <code class="inlineCode">kubectl</code> command-line tool. You must simply issue a <code class="inlineCode">kubectl get services</code> command to <a id="_idIndexMarker770"/>fetch the Services created within your cluster.</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get service
NAME              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
example-service   ClusterIP   10.106.224.122   &lt;none&gt;        80/TCP         26d
kubernetes        ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP        26d
nodeport-whoami   NodePort    10.100.85.171    &lt;none&gt;        80:30001/TCP   21s
</code></pre>
    <p class="normal">That being said, let’s now discover how we can update <code class="inlineCode">NodePort</code> Services to have them do what we want.</p>
    <h2 class="heading-2" id="_idParaDest-313">Adding more Pods to NodePort Services</h2>
    <p class="normal">If you want to add a <a id="_idIndexMarker771"/>Pod to the pool served by your Services, it’s very easy. In fact, you just need to add a new Pod that matches the label selector defined on the Service—Kubernetes will take care of the rest. The Pod will be part of the pool served by the Service. If you delete a Pod, it will be deleted from the pool of Services as soon as it enters the <code class="inlineCode">Terminating</code> state.</p>
    <p class="normal">Kubernetes handles Service traffic based on Pod availability—for example, if you have three replicas of a web server and one goes down, creating an additional replica that matches the label selector <a id="_idIndexMarker772"/>on the Service will be enough. You’ll discover later, in <em class="chapterRef">Chapter 11</em>, <em class="italic">Using Kubernetes Deployments for Stateless Workloads</em>, that this behavior can be entirely automated.</p>
    <h2 class="heading-2" id="_idParaDest-314">Describing NodePort Services</h2>
    <p class="normal">Describing <code class="inlineCode">NodePort</code> Services is <a id="_idIndexMarker773"/>super easy and is achieved with the help of the <code class="inlineCode">kubectl describe</code> command, just as with any other Kubernetes object. Let us explore the details of the <code class="inlineCode">nodeport-whoami</code> Service in the following<a id="_idIndexMarker774"/> command’s output:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl describe Service nodeport-whoami:
Name:                     nodeport-whoami
...&lt;removed for brevity&gt;...
<span class="code-highlight"><strong class="hljs-slc">Selector:                 app=whoami</strong></span>
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
<span class="code-highlight"><strong class="hljs-slc">IP:                       10.98.160.98</strong></span>
IPs:                      10.98.160.98
Port:                     &lt;unset&gt;  80/TCP
TargetPort:               80/TCP
NodePort:                 &lt;unset&gt;  30001/TCP
<span class="code-highlight"><strong class="hljs-slc">Endpoints:                10.244.0.16:80,10.244.0.17:80</strong></span>
...&lt;removed for brevity&gt;...
</code></pre>
    <p class="normal">In the preceding output, we can see several details, including the following items:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">IP:10.98.160.98</code>: The <code class="inlineCode">ClusterIP</code> assigned to the Service. It’s the internal IP address that other Services in the cluster can use to access this Service.</li>
      <li class="bulletList"><code class="inlineCode">Port: &lt;unset&gt; 80/TCP</code>: The Service listens on port <code class="inlineCode">80</code> with the TCP protocol. The <code class="inlineCode">&lt;unset&gt;</code> means that no name was given for the port.</li>
      <li class="bulletList"><code class="inlineCode">NodePort: &lt;unset&gt; 30001/TCP</code>: The <code class="inlineCode">NodePort</code> is the port on each node in the cluster through which external traffic can access the Service. Here, it’s set to port <code class="inlineCode">30001</code>, allowing external access to the Service via any node’s IP at port <code class="inlineCode">30001</code>.</li>
      <li class="bulletList"><code class="inlineCode">Endpoints: 10.244.0.16:80, 10.244.0.17:80</code>: The actual IP addresses and ports of the Pods <a id="_idIndexMarker775"/>behind the Service. In this case, two Pods are backing the Service, reachable at <code class="inlineCode">10.244.0.16:80</code> and <code class="inlineCode">10.244.0.17:80</code>.</li>
    </ul>
    <p class="normal">In the next section, we will learn how to delete a Service using the <code class="inlineCode">kubectl delete svc</code> command.</p>
    <h2 class="heading-2" id="_idParaDest-315">Deleting Services</h2>
    <p class="normal">Deleting a<a id="_idIndexMarker776"/> Service, whether it is a <code class="inlineCode">NodePort</code> Service or not, should not be done often. Indeed, whereas Pods are supposed to be easy to delete and recreate, Services are supposed to be for the long term. They provide a consistent way to expose your Pod, and deleting them will impact how your applications can be reached.</p>
    <p class="normal">Therefore, you should be careful when deleting Services: it won’t delete the Pods behind it, but they won’t be accessible anymore from outside of your cluster!</p>
    <p class="normal">Here is the command to delete the Service created to expose the <code class="inlineCode">whoami1</code> and <code class="inlineCode">whoami2</code> Pods:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete svc/nodeport-whoami
service "nodeport-whoami" deleted
</code></pre>
    <p class="normal">You can run a <code class="inlineCode">kubectl get svc</code> command now to check that the Service was properly destroyed, and then access it once more through the web browser by refreshing it. You’ll notice that the application is not reachable anymore, but the Pods will remain on the cluster. Pods and Services have completely independent life cycles. If you want to delete Pods, then you’ll need to delete them separately.</p>
    <p class="normal">You probably<a id="_idIndexMarker777"/> remember the <code class="inlineCode">kubectl port-forward</code> command we used when we created an nginx Pod and tested it to display the home page. You might think <code class="inlineCode">NodePort</code> and <code class="inlineCode">kubectl port-forward</code> are the same thing, but they are not. Let’s explain quickly the difference between the two in the upcoming section.</p>
    <h2 class="heading-2" id="_idParaDest-316">NodePort or kubectl port-forward?</h2>
    <p class="normal">It <a id="_idIndexMarker778"/>might be tempting to compare <code class="inlineCode">NodePort</code> Services<a id="_idIndexMarker779"/> with the <code class="inlineCode">kubectl port-forward</code> command because, so far, we have used these two methods to access a running Pod in our cluster using a web browser.</p>
    <p class="normal">The <code class="inlineCode">kubectl port-forward</code> command is a testing tool, whereas <code class="inlineCode">NodePort</code> Services are for real use cases and are a production-ready feature.</p>
    <p class="normal">Keep in mind that <code class="inlineCode">kubectl port-forward</code> must be kept open in your terminal session for it to work. As soon as the command is killed, the port forwarding is stopped too, and your application will become inaccessible from outside the cluster once more. It is only a testing tool meant to be used by the <code class="inlineCode">kubectl</code> user and is just one of the useful tools bundled into the <code class="inlineCode">kubectl</code> CLI.</p>
    <p class="normal"><code class="inlineCode">NodePort</code>, on the other hand, is really meant for production use and is a long-term production-ready solution. It doesn’t require <code class="inlineCode">kubectl</code> to work and makes your application accessible to anyone calling the Service, provided the Service is properly configured and the Pods are correctly labeled.</p>
    <p class="normal">Simply put, if you<a id="_idIndexMarker780"/> just need to test your app, go for <code class="inlineCode">kubectl port-forward</code>. If you need to expose your Pod to the outside world for real, go for <code class="inlineCode">NodePort</code>. Don’t create <code class="inlineCode">NodePort</code> for testing, and don’t try to use <code class="inlineCode">kubectl port-forward</code> for production! Stick with one tool for each use case!</p>
    <p class="normal">Now, we will discover another type of Kubernetes Service called <code class="inlineCode">ClusterIP</code>. This one is probably the most widely used of them all, even more than the <code class="inlineCode">NodePort</code> type!</p>
    <h1 class="heading-1" id="_idParaDest-317">The ClusterIP Service</h1>
    <p class="normal">We’re now going to<a id="_idIndexMarker781"/> discover <a id="_idIndexMarker782"/>another type of Service called <code class="inlineCode">ClusterIP</code>. Now, <code class="inlineCode">ClusterIP</code> is, in fact, the simplest type of Service Kubernetes provides. With a <code class="inlineCode">ClusterIP</code> Service, you can expose your Pod so that other Pods in Kubernetes can communicate with it via its IP address or DNS name.</p>
    <h2 class="heading-2" id="_idParaDest-318">Why do you need ClusterIP Services?</h2>
    <p class="normal">The <code class="inlineCode">ClusterIP</code> Service<a id="_idIndexMarker783"/> type greatly resembles the <code class="inlineCode">NodePort</code> Service type, but <a id="_idIndexMarker784"/>they have one big difference: <code class="inlineCode">NodePort</code> Services are meant to expose Pods to the outside world, whereas <code class="inlineCode">ClusterIP</code> Services are meant to expose Pods to other Pods inside the Kubernetes cluster.</p>
    <p class="normal">Indeed, <code class="inlineCode">ClusterIP</code> Services are the Services that allow different Pods in the same cluster to communicate with each other through a static interface: the <code class="inlineCode">ClusterIP</code> <code class="inlineCode">Service</code> object itself.</p>
    <p class="normal"><code class="inlineCode">ClusterIP</code> answers exactly the same need for a static DNS name or IP address we had with the <code class="inlineCode">NodePort</code> Service: if a Pod fails, is recreated, deleted, relaunched, and so on, then Kubernetes will assign it another IP address. <code class="inlineCode">ClusterIP</code> Services are here to remediate this issue, by providing an internal DNS name only accessible from within your cluster that will resolve to the Pods defined by the label selector.</p>
    <p class="normal">As the name <code class="inlineCode">ClusterIP</code> suggests, this Service grants a static IP within the cluster! Let’s now discover how to expose our Pods using <code class="inlineCode">ClusterIP</code>! Keep in mind that <code class="inlineCode">ClusterIP</code> Services are not accessible from outside the cluster—they are only meant for inter-Pod communication.</p>
    <h2 class="heading-2" id="_idParaDest-319">How do I know if I need NodePort or ClusterIP Services to expose my Pods?</h2>
    <p class="normal">Choosing<a id="_idIndexMarker785"/> between the two types of <a id="_idIndexMarker786"/>Services is extremely simple, basically because they are not meant for the same thing.</p>
    <p class="normal">If you need your app to be accessible from outside the cluster, then you’ll need a <code class="inlineCode">NodePort</code> Service (or other Services we will be exploring later in this chapter), but if your need is for the app to be accessible from inside the cluster, then you’ll need a <code class="inlineCode">ClusterIP</code> Service. <code class="inlineCode">ClusterIP</code> Services are also good for stateless applications that can be scaled, destroyed, recreated, and so on. The reason is that the <code class="inlineCode">ClusterIP</code> Service will maintain a static entry point to a whole pool of Pods without being constrained by a port on the worker node, unlike the <code class="inlineCode">NodePort</code> Service.</p>
    <p class="normal">The <code class="inlineCode">ClusterIP</code> Service exposes Pods using internally visible virtual IP addresses managed by kube-proxy on each Node. This means that the Service will be reachable from within the cluster only. We have visualized the ClusterIP Service principles in the following diagram:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_08_07.png"/></figure>
    <p class="packt_figref">Figure 8.7: ClusterIP Service</p>
    <p class="normal">In the <a id="_idIndexMarker787"/>preceding image, the <code class="inlineCode">ClusterIP</code> Service is configured in such a way that it will map requests coming from<a id="_idIndexMarker788"/> its IP and TCP port <code class="inlineCode">8080</code> to the container’s TCP port <code class="inlineCode">80</code>. The actual <code class="inlineCode">ClusterIP</code> address is assigned dynamically unless you specify one explicitly in the specifications. The internal DNS Service in a Kubernetes cluster is responsible for resolving the <code class="inlineCode">nginx-deployment-example</code> name to the actual <code class="inlineCode">ClusterIP</code> address as a part of Service discovery.</p>
    <div class="note">
      <p class="normal">kube-proxy handles the management of virtual IP addresses on the nodes and adjusts the forwarding rules accordingly. Services in Kubernetes are essentially logical constructs within the cluster. There isn’t a separate physical process running inside the cluster for each Service to handle proxying. Instead, kube-proxy performs the necessary proxying and routing based on these logical Services.</p>
    </div>
    <p class="normal">Contrary to <code class="inlineCode">NodePort</code> Services, <code class="inlineCode">ClusterIP</code> Services will not take one port of the worker node, and thus it is impossible to reach it from outside the Kubernetes cluster.</p>
    <p class="normal">Keep in mind that nothing prevents you from using both types of Services for the same pool of Pods. Indeed, if you have an app that should be publicly accessible but also privately exposed to other Pods, then you can simply create two Services, one <code class="inlineCode">NodePort</code> Service and one <code class="inlineCode">ClusterIP</code> Service.</p>
    <p class="normal">In this specific use case, you’ll simply have to name the two Services differently so that they won’t conflict when creating them against <code class="inlineCode">kube-apiserver</code>. Nothing else prevents you from doing so!</p>
    <h2 class="heading-2" id="_idParaDest-320">Listing ClusterIP Services</h2>
    <p class="normal">Listing <code class="inlineCode">ClusterIP</code> Services is<a id="_idIndexMarker789"/> easy. It’s basically the same command as the <a id="_idIndexMarker790"/>one used for <code class="inlineCode">NodePort</code> Services. Here is the command to run:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get svc 
</code></pre>
    <p class="normal">As always, this command lists the Services with their type added to the output.</p>
    <h2 class="heading-2" id="_idParaDest-321">Creating ClusterIP Services using the imperative way</h2>
    <p class="normal">Creating <code class="inlineCode">ClusterIP</code> Services can be <a id="_idIndexMarker791"/>achieved with a lot of different methods. Since it is an extremely used feature, there are lots of ways to create these, as follows:</p>
    <ul>
      <li class="bulletList">Using the <code class="inlineCode">--expose</code> parameter or <code class="inlineCode">kubectl expose</code> method (the imperative way)</li>
      <li class="bulletList">Using a YAML manifest file (the declarative way)</li>
    </ul>
    <p class="normal">The imperative way consists of using the --<code class="inlineCode">expose</code> method. This will create a <code class="inlineCode">ClusterIP</code> Service directly from a <code class="inlineCode">kubectl run</code> command. In the following example, we will create an <code class="inlineCode">nginx-clusterip</code> Pod as well as a <code class="inlineCode">ClusterIP</code> Service to expose them both at the same time. Using the <code class="inlineCode">--expose</code> parameter will also require defining a <code class="inlineCode">ClusterIP</code> port. <code class="inlineCode">ClusterIP</code> will listen to make the Pod reachable. The code is illustrated here:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl run nginx-clusterip --image nginx --expose=<span class="hljs-con-literal">true</span> --port=80
service/nginx-clusterip created
pod/nginx-clusterip created
</code></pre>
    <p class="normal">As you can see, we get both a Pod and a Service to expose it. Let’s describe the Service.</p>
    <h2 class="heading-2" id="_idParaDest-322">Describing ClusterIP Services</h2>
    <p class="normal">Describing <code class="inlineCode">ClusterIP</code> Services <a id="_idIndexMarker792"/>is the same process as describing any type of object in Kubernetes and is achieved using the <code class="inlineCode">kubectl describe</code> command. You just need to know the name of the Service in order to describe to achieve that.</p>
    <p class="normal">Here, I’m going <a id="_idIndexMarker793"/>to the <code class="inlineCode">ClusterIP</code> Service created previously:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl describe svc/nginx-clusterip
Name:              nginx-clusterip
Namespace:         default
Labels:            &lt;none&gt;
Annotations:       &lt;none&gt;
<span class="code-highlight"><strong class="hljs-slc">Selector:          run=nginx-clusterip</strong></span>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.101.229.225
IPs:               10.101.229.225
Port:              &lt;unset&gt;  80/TCP
TargetPort:        80/TCP
Endpoints:         10.244.0.10:80
Session Affinity:  None
Events:            &lt;none&gt;
</code></pre>
    <p class="normal">The output of this command shows us the <code class="inlineCode">Selector</code> block, which shows that the <code class="inlineCode">ClusterIP</code> Service was created by the <code class="inlineCode">--expose</code> parameter with the proper label configured. This label matches the <code class="inlineCode">nginx-clusterip</code> Pod we created at the same time. To be sure about <a id="_idIndexMarker794"/>that, let’s display the labels of the said Pod, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods/nginx-clusterip --show-labels
NAME              READY   STATUS    RESTARTS   AGE   LABELS
nginx-clusterip   1/1     Running   0          76s   <span class="code-highlight"><strong class="hljs-slc">run=nginx-clusterip</strong></span>
</code></pre>
    <p class="normal">As you can see, the selector on the Service matches the labels defined on the Pod. Communication is thus established between the two. We will now call the <code class="inlineCode">ClusterIP</code> Service directly from another Pod on the cluster.</p>
    <p class="normal">Since the <code class="inlineCode">ClusterIP</code> Service is named <code class="inlineCode">nginx-clusterip</code>, we know that it is reachable at this address: <code class="inlineCode">nginx-clusterip.default.svc.cluster.local</code>.</p>
    <p class="normal">Let’s reuse the <code class="inlineCode">k8sutils</code> container, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> k8sutils -- curl nginx-clusterip.default.svc.cluster.local
<span class="hljs-con-meta">  % </span>Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0&lt;!DOCTYPE ...&lt;removed for brevity&gt;...
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;
 ...&lt;removed for brevity&gt;...
</code></pre>
    <p class="normal">The <code class="inlineCode">ClusterIP</code> Service correctly forwarded the request to the nginx Pod, and we do have the nginx default home page. The Service is working!</p>
    <p class="normal">We did not use <code class="inlineCode">containous/whoami</code> as a web Service this time, but keep in mind that the <code class="inlineCode">ClusterIP</code> Service is also doing load balancing internally following the round-robin algorithm. If you<a id="_idIndexMarker795"/> have 10 Pods behind a <code class="inlineCode">ClusterIP</code> Service and your Service received 1,000 requests, then <a id="_idIndexMarker796"/>each Pod is going to receive 100 requests.</p>
    <p class="normal">Let’s now discover how to create a <code class="inlineCode">ClusterIP</code> Service using YAML.</p>
    <h2 class="heading-2" id="_idParaDest-323">Creating ClusterIP Services using the declarative way</h2>
    <p class="normal"><code class="inlineCode">ClusterIP</code> Services <a id="_idIndexMarker797"/>can also be created the declarative way by applying YAML configuration files against <code class="inlineCode">kube-apiserver</code>.</p>
    <p class="normal">Here’s a YAML manifest file we can use to create the exact same <code class="inlineCode">ClusterIP</code> Service we created before the imperative way:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># clusterip-service.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-clusterip</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">type:</span> <span class="hljs-string">ClusterIP</span> <span class="hljs-comment"># Indicates that the service is a ClusterIP</span>
  <span class="hljs-attr">ports:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># The port exposed by the service</span>
      <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># The destination port on the pods</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">run:</span> <span class="hljs-string">nginx-clusterip</span>
</code></pre>
    <p class="normal">Take some time to read the comments in the YAML, especially the <code class="inlineCode">port</code> and <code class="inlineCode">targetPort</code> ones.</p>
    <p class="normal">Indeed, <code class="inlineCode">ClusterIP</code> Services <a id="_idIndexMarker798"/>have their own port independent of the one exposed on the Pod side. You reach the <code class="inlineCode">ClusterIP</code> Service by calling its DNS name and its port, and the traffic is going to be forwarded to the destination port on the Pods matching the labels and selectors.</p>
    <p class="normal">Keep in mind that no worker <a id="_idIndexMarker799"/>node port is involved here. The ports we are mentioning when it comes to <code class="inlineCode">ClusterIP</code> scenarios have absolutely nothing to do with the host machine!</p>
    <p class="normal">Before we continue with the next section, you can clean up the environment by deleting the <code class="inlineCode">nginx-clusterip</code> Service as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete  svc nginx-clusterip
service "nginx-clusterip" deleted
</code></pre>
    <p class="normal">Keep in mind that deleting the cluster won’t delete the Pods exposed by it. It is a different process; you’ll need to delete Pods separately. We will now discover one additional resource related to <code class="inlineCode">ClusterIP</code> Services, which are headless Services.</p>
    <h2 class="heading-2" id="_idParaDest-324">Understanding headless Services</h2>
    <p class="normal">Headless Services are derived <a id="_idIndexMarker800"/>from the <code class="inlineCode">ClusterIP</code> Service. They are not technically a dedicated type of Service (such as <code class="inlineCode">NodePort</code> or <code class="inlineCode">ClusterIP</code>), but they are an option from <code class="inlineCode">ClusterIP</code>.</p>
    <p class="normal">Headless Services can be configured by setting the <code class="inlineCode">.spec.clusterIP</code> option to <code class="inlineCode">None</code> in a YAML configuration file for the <code class="inlineCode">ClusterIP</code> Service. Here is an example derived from our previous YAML file:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># clusterip-headless.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-clusterip-headless</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">clusterIP:</span> <span class="hljs-string">None</span>
  <span class="hljs-attr">type:</span> <span class="hljs-string">ClusterIP</span> <span class="hljs-comment"># Indicates that the service is a ClusterIP</span>
  <span class="hljs-attr">ports:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># The port exposed by the service</span>
    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># The destination port on the pods</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">run:</span> <span class="hljs-string">nginx-clusterip</span>   
</code></pre>
    <p class="normal">A headless Service roughly consists of a <code class="inlineCode">ClusterIP</code> Service without load balancing and without a pre-allocated <code class="inlineCode">ClusterIP</code> address. Thus, the load-balancing logic and the interfacing with the Pod are not defined by Kubernetes.</p>
    <p class="normal">Since a headless Service has no IP address, you are going to reach the Pod behind it directly, without the proxying and the load-balancing logic. What the headless Service does is return you the DNS names of the Pods behind it so that you can reach them directly. There is still a little load-balancing logic here, but it is implemented at the DNS level, not as Kubernetes logic.</p>
    <p class="normal">When you use<a id="_idIndexMarker801"/> a normal <code class="inlineCode">ClusterIP</code> Service, you’ll always reach one static IP address allocated to the Service and <a id="_idIndexMarker802"/>this is going to be your proxy to communicate with the Pod behind it. With a headless Service, the <code class="inlineCode">ClusterIP</code> Service will just return the DNS names of the Pods behind it and the client will have the responsibility to establish a connection with the DNS name of its choosing.</p>
    <p class="normal">Headless Services in Kubernetes are primarily used in scenarios where direct communication with individual Pods is required, rather than with a single endpoint or load-balanced set of Pods.</p>
    <p class="normal">They are helpful when you want to build connectivity with clustered stateful Services such as <strong class="keyWord">Lightweight Directory Access Protocol</strong> (<strong class="keyWord">LDAP</strong>). In that case, you may want to use an LDAP client that will have access to the different DNS names of the Pods hosting the LDAP server, and this can’t be done with a normal <code class="inlineCode">ClusterIP</code> Service since it will bring both a static IP and Kubernetes’ implementation of load balancing. Let’s now briefly introduce another type of Service called <code class="inlineCode">LoadBalancer</code>.</p>
    <h1 class="heading-1" id="_idParaDest-325">The LoadBalancer Service</h1>
    <p class="normal"><code class="inlineCode">LoadBalancer</code> Services <a id="_idIndexMarker803"/>are very<a id="_idIndexMarker804"/> interesting to explain because this Service relies on the cloud platform where the Kubernetes cluster is provisioned. For it to work, it is thus required to use Kubernetes on a cloud platform that supports the <code class="inlineCode">LoadBalancer</code> Service type.</p>
    <p class="normal">For cloud providers that offer external load balancers, specifying the <code class="inlineCode">type</code> field as <code class="inlineCode">LoadBalancer</code> configures a load balancer for your Service. The creation of the load balancer occurs asynchronously, and details about the provisioned balancer are made available in the <code class="inlineCode">.status.loadBalancer</code> field of the Service.</p>
    <p class="normal">Certain cloud providers offer the option to define the <code class="inlineCode">loadBalancerIP</code>. In such instances, the load balancer is generated with the specified <code class="inlineCode">loadBalancerIP</code>. If the <code class="inlineCode">loadBalancerIP</code> is not provided, the load balancer is configured with an ephemeral IP address. However, if you specify a <code class="inlineCode">loadBalancerIP</code> on a cloud provider that does not support this feature, the provided <code class="inlineCode">loadBalancerIP</code> is disregarded.</p>
    <p class="normal">For a Service with its type set to LoadBalancer, the <code class="inlineCode">.spec.loadBalancerClass</code> field allows you to utilize a load balancer implementation other than the default provided by the cloud provider. When <code class="inlineCode">.spec.loadBalancerClass</code> is not specified, the <code class="inlineCode">LoadBalancer</code> type of Service will automatically use the default load balancer implementation provided by the cloud provider, assuming the cluster is configured with a cloud provider using the <code class="inlineCode">--cloud-provider</code> component flag. However, if you specify <code class="inlineCode">.spec.loadBalancerClass</code>, it indicates that a load balancer implementation matching the specified class is <a id="_idIndexMarker805"/>actively monitoring for Services. In such cases, any default load<a id="_idIndexMarker806"/> balancer implementation, such as the one provided by the cloud provider, will disregard Services with this field set. It’s important to note that <code class="inlineCode">spec.loadBalancerClass</code> can only be set on a Service of type <code class="inlineCode">LoadBalancer</code> and, once set, it cannot be altered. Additionally, the value of <code class="inlineCode">spec.loadBalancerClass</code> must adhere to a label-style identifier format, optionally with a prefix like <code class="inlineCode">internal-vip</code> or <code class="inlineCode">example.com/internal-vip</code>, with unprefixed names being reserved for end users.</p>
    <p class="normal">The Kubernetes <code class="inlineCode">Loadbalancer</code> type Service principles have been visualized in the following diagram:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_08_08.png"/></figure>
    <p class="packt_figref">Figure 8.8: LoadBalancer Service</p>
    <p class="normal">In the next section, we will learn about the supported cloud providers for <code class="inlineCode">LoadBalancer</code> Service types.</p>
    <h2 class="heading-2" id="_idParaDest-326">Supported cloud providers for the LoadBalancer Service type</h2>
    <p class="normal">Not all cloud providers <a id="_idIndexMarker807"/>support the <code class="inlineCode">LoadBalancer</code> Service type, but we can name a <a id="_idIndexMarker808"/>few that do support it. These are as follows:</p>
    <ul>
      <li class="bulletList">AWS</li>
      <li class="bulletList">GCP</li>
      <li class="bulletList">Azure</li>
      <li class="bulletList">OpenStack</li>
    </ul>
    <p class="normal">The list is not exhaustive, but it’s good to know that all three major public cloud providers are supported.</p>
    <p class="normal">If your cloud provider is supported, keep in mind that the load-balancing logic will be the one implemented by the cloud provider: you have less control over how the traffic will be routed to your Pods from Kubernetes, and you will have to know how the load-balancer component of your cloud provider works. Consider it as a third-party component implemented as a Kubernetes resource.</p>
    <h2 class="heading-2" id="_idParaDest-327">Should the LoadBalancer Service type be used?</h2>
    <p class="normal">This question is difficult to <a id="_idIndexMarker809"/>answer but a lot of people tend to not use a <code class="inlineCode">LoadBalancer</code> Service type for a few reasons.</p>
    <p class="normal">The main reason is that <code class="inlineCode">LoadBalancer</code> Services <a id="_idIndexMarker810"/>are nearly impossible to configure from Kubernetes. Indeed, if you must use a cloud provider, it is better to configure it from the tooling provided by the provider rather than from Kubernetes. The <code class="inlineCode">LoadBalancer</code> Service type is a generic way to provision a <code class="inlineCode">LoadBalancer</code> Service but does not expose all the advanced features that the cloud provider may provide.</p>
    <p class="normal">Also, load balancers provided by cloud providers often come with additional costs, which can vary depending on the provider and the amount of traffic being handled.</p>
    <p class="normal">In the next section, we will learn about another Service type called <code class="inlineCode">ExternalName</code> Services.</p>
    <h1 class="heading-1" id="_idParaDest-328">The ExternalName Service type</h1>
    <p class="normal"><code class="inlineCode">ExternalName</code> Services<a id="_idIndexMarker811"/> are a powerful way to connect your Kubernetes cluster to<a id="_idIndexMarker812"/> external resources like databases, APIs, or Services hosted outside the cluster. They work by mapping a Service in your cluster to a DNS name instead of Pods within the cluster. This allows your applications inside the cluster to seamlessly access the external resource without needing to know its IP address or internal details.</p>
    <p class="normal">The following snippet shows a sample <code class="inlineCode">ExternalName</code> type Service YAML definition:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># externalname-service.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">mysql-db</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">prod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">type:</span> <span class="hljs-string">ExternalName</span>
  <span class="hljs-attr">externalName:</span> <span class="hljs-string">app-db.database.example.com</span>
</code></pre>
    <p class="normal">Here’s how it works: instead of <a id="_idIndexMarker813"/>linking the external names or IP address to internal Pods, you <a id="_idIndexMarker814"/>simply define a DNS name like <code class="inlineCode">app-db.database.example.com</code> in the Service configuration. Now, when your applications within the cluster try to access <code class="inlineCode">mysql-db</code>, the magic happens—the cluster’s DNS Service points them to your external database! They interact with it seamlessly, just like any other Service, but the redirection occurs at the DNS level, keeping things clean and transparent.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_08_09.png"/></figure>
    <p class="packt_figref">Figure 8.9: ExternalName Service type</p>
    <p class="normal">The <code class="inlineCode">ExternalName</code> Service can be a Service hosted in another Kubernetes namespace, a Service outside of the Kubernetes cluster, a Service hosted in another Kubernetes cluster, and so on.</p>
    <p class="normal">This approach offers several benefits:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Simplified configuration</strong>: Applications only need to know the Service name, not the external resource details, making things much easier.</li>
      <li class="bulletList"><strong class="keyWord">Flexible resource management</strong>: If you later move the database into your cluster, you can simply update the Service and manage it internally without affecting your applications.</li>
      <li class="bulletList"><strong class="keyWord">Enhanced security</strong>: Sensitive information like IP addresses stays hidden within the cluster, improving overall security.</li>
    </ul>
    <p class="normal">Remember, <code class="inlineCode">ExternalName</code> Services <a id="_idIndexMarker815"/>are all about connecting to external resources. For<a id="_idIndexMarker816"/> internal resource access within your cluster, stick to regular or headless Services.</p>
    <p class="normal">Now that we have learned the different Service types in Kubernetes, let us explore how to use probes to ensure Service availability in the following sections.</p>
    <h1 class="heading-1" id="_idParaDest-329">Implementing Service readiness using probes</h1>
    <p class="normal">When you create a Service to expose an application running inside Pods, Kubernetes doesn’t automatically verify the health of that application. The Pods may be up and running, but the application itself could still have issues, and the Service will continue to route traffic to it. This could result in users or other applications receiving errors or no response at all. To prevent this, Kubernetes provides health check mechanisms called probes. In the following section, we’ll explore the different types of probes—liveness, readiness, and startup probes—and how they help ensure your application is functioning properly.</p>
    <h2 class="heading-2" id="_idParaDest-330">What is ReadinessProbe and why do you need it?</h2>
    <p class="normal"><code class="inlineCode">ReadinessProbe</code>, along with <code class="inlineCode">LivenessProbe</code>, is an<a id="_idIndexMarker817"/> important aspect to master if you want to provide the<a id="_idIndexMarker818"/> best possible experience to your end user. We will first discover how to implement <code class="inlineCode">ReadinessProbe</code> and how it can help you ensure your containers are fully ready to serve traffic.</p>
    <p class="normal">Readiness probes are technically not part of Services, but it is important to discover this feature alongside Kubernetes Services.</p>
    <p class="normal">Just as with everything in Kubernetes, <code class="inlineCode">ReadinessProbe</code> was implemented to bring a solution to a problem. This problem is this: how to ensure a Pod is fully ready before it can receive traffic, possibly from a Service.</p>
    <p class="normal">Services obey a simple rule: they serve traffic to every Pod that matches their label selector. As soon as a Pod gets provisioned, if this Pod’s labels match the selector of Service in your cluster, then this Service will immediately start forwarding traffic to it. This can lead to a simple problem: if the app is not fully launched, because it has a slow launch process or requires some configuration from a remote API, and so on, then it might receive traffic from Services before being ready for it. The result would be a poor <strong class="keyWord">user experience</strong> (<strong class="keyWord">UX</strong>).</p>
    <p class="normal">To make sure this scenario never happens, we can use the feature called <code class="inlineCode">ReadinessProbe</code>, which is an<a id="_idIndexMarker819"/> additional configuration to add to a Pod’s configuration.</p>
    <p class="normal">When a Pod is configured with a readiness probe, it can send a signal to the control plane that it is not ready to receive traffic, and when a Pod is not ready, Services won’t forward any traffic to it. Let’s see how we can implement a readiness probe.</p>
    <h2 class="heading-2" id="_idParaDest-331">Implementing ReadinessProbe</h2>
    <p class="normal"><code class="inlineCode">ReadinessProbe</code> implementation is <a id="_idIndexMarker820"/>achieved by adding some configuration data to a Pod YAML manifest. Please note that it has nothing to do with the <code class="inlineCode">Service</code> object itself. By adding some configuration to the container <code class="inlineCode">spec</code> in the Pod object, you can basically tell Kubernetes to wait for the Pod to be fully ready before it can receive traffic from Services.</p>
    <p class="normal"><code class="inlineCode">ReadinessProbe</code> can be of three different types, as outlined here:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Command</code>: Issue a command inside the pod that should complete with exit code <code class="inlineCode">0</code>, indicating the Pod is ready.</li>
      <li class="bulletList"><code class="inlineCode">HTTP</code>: An HTTP request that should complete with a response code &gt;= <code class="inlineCode">200</code> and &lt; <code class="inlineCode">400</code>, which indicates the Pod is ready.</li>
      <li class="bulletList"><code class="inlineCode">TCP</code>: Issue a TCP connection attempt. If the connection is established, the Pod is ready.</li>
    </ul>
    <p class="normal">Here is a YAML file configuring an nginx Pod with a readiness probe of type HTTP:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># nginx-pod-with-readiness-http.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-pod-with-readiness-http</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-pod-with-readiness-http</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">readinessProbe:</span>
        <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">5</span>
        <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">5</span>
        <span class="hljs-attr">httpGet:</span>
          <span class="hljs-attr">path:</span> <span class="hljs-string">/ready</span>
          <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>
</code></pre>
    <p class="normal">As you can see, we have two important inputs under the <code class="inlineCode">readinessProbe</code> key, as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">initialDelaySeconds</code>, which indicates the number of seconds the probe will wait before running the first health check</li>
      <li class="bulletList"><code class="inlineCode">periodSeconds</code>, which indicates the number of seconds the probe will wait between two consecutive health checks</li>
    </ul>
    <p class="normal">The readiness probe will be replayed regularly, and the interval between two checks will be defined by the <code class="inlineCode">periodSeconds</code> parameter.</p>
    <p class="normal">In our case, our <code class="inlineCode">ReadinessProbe</code> will run an HTTP call against the <code class="inlineCode">/ready</code> path. If this request receives an HTTP response code &gt;= <code class="inlineCode">200</code> and &lt; <code class="inlineCode">400</code>, then the probe will be a success, and the Pod will be considered healthy.</p>
    <p class="normal"><code class="inlineCode">ReadinessProbe</code> is important. In our<a id="_idIndexMarker821"/> example, the endpoint being called should test that the application is really in such a state that it can receive traffic. So, try to call an endpoint that is relevant to the state of the actual application. For example, you can try to call a page that will open a MySQL connection internally to make sure the application is capable of communicating with its database if it is using one, and so on. If you’re a developer, do not hesitate to create a dedicated endpoint that will just open connections to the different backends to be fully sure that the application is definitely ready.</p>
    <p class="normal">The Pod will then join the pool being served by the Service and will start receiving traffic. <code class="inlineCode">ReadinessProbe</code> can also be configured as TCP and commands, but we will keep these examples for <code class="inlineCode">LivenessProbe</code>. Let’s discover them now!</p>
    <h2 class="heading-2" id="_idParaDest-332">What is LivenessProbe and why do you need it?</h2>
    <p class="normal"><code class="inlineCode">LivenessProbe</code> resembles <code class="inlineCode">ReadinessProbe</code> a lot. In fact, if you have used any cloud providers before, you <a id="_idIndexMarker822"/>might already have heard about something called health checks. <code class="inlineCode">LivenessProbe</code> is basically a health check.</p>
    <p class="normal">Liveness probes are used to <a id="_idIndexMarker823"/>determine whether a Pod is in a broken state or not, and the usage of <code class="inlineCode">LivenessProbe</code> is especially suited for long-running processes such as web services. Imagine a situation where you have a Service forwarding traffic to three Pods and one of them is broken. Services cannot detect that on their own, and they will just continue to serve traffic to the three Pods, including the broken one. In such situations, 33% of your requests will inevitably lead to an error response, resulting in a poor UX, as illustrated in the following screenshot:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_08_10.png"/></figure>
    <p class="packt_figref">Figure 8.10: One of the Pods is broken but the Service will still forward traffic to it</p>
    <p class="normal">You want to avoid such <a id="_idIndexMarker824"/>situations, and to do so, you need a way to detect situations where Pods are broken, plus a way to kill such a container so that it goes out of the pool of Pods being targeted by the Service.</p>
    <p class="normal"><code class="inlineCode">LivenessProbe</code> is the solution to this problem and is implemented at the Pod level. Be careful because <code class="inlineCode">LivenessProbe</code> cannot repair a Pod: it can only detect that a Pod is not healthy and command its termination. Let’s see how we can implement a Pod with <code class="inlineCode">LivenessProbe</code>.</p>
    <h2 class="heading-2" id="_idParaDest-333">Implementing LivenessProbe</h2>
    <p class="normal"><code class="inlineCode">LivenessProbe</code> is a<a id="_idIndexMarker825"/> health check that will be executed on a regular schedule to keep track of the application state in the long run. These health checks are executed by the <code class="inlineCode">kubelet</code> component and can be of different types, as outlined here:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Command</strong>, where you issue a command in the container and its result will tell whether the Pod is healthy or not (exit code = <code class="inlineCode">0</code> means healthy)</li>
      <li class="bulletList"><strong class="keyWord">HTTP</strong>, where you run an HTTP request against the Pod, and its result tells whether the Pod is healthy or not (HTTP response code &gt;= <code class="inlineCode">200</code> and &lt; <code class="inlineCode">400</code> means the Pod is healthy)</li>
      <li class="bulletList"><strong class="keyWord">TCP</strong>, where you define a TCP call (a successful connection means the Pod is healthy)</li>
      <li class="bulletList"><strong class="keyWord">GRPC</strong>, if the application supports and implements the gRPC Health Checking Protocol</li>
    </ul>
    <p class="normal">Each of these liveness probes will require you to input a parameter called <code class="inlineCode">periodSeconds</code>, which must be an integer. This will tell the <code class="inlineCode">kubelet</code> component the number of seconds to wait before performing a new health check. You can also use another parameter called <code class="inlineCode">initialDelaySeconds</code>, which will indicate the number of seconds to wait before performing the very first health check. Indeed, in some common situations, a health check might lead to flagging an application as unhealthy just because it was performed too early. That’s why it might be a good idea to wait a little bit before performing the first health check, and that parameter is here to help.</p>
    <p class="normal"><code class="inlineCode">LivenessProbe</code> configuration is achieved at the Pod YAML configuration manifest, not at the Service one. Each container in the Pod can have its own <code class="inlineCode">livenessProbe</code>.</p>
    <h3 class="heading-3" id="_idParaDest-334">HTTP livenessProbe</h3>
    <p class="normal">HTTP probes in Kubernetes <a id="_idIndexMarker826"/>offer <a id="_idIndexMarker827"/>additional customizable fields, such as host, scheme, path, headers, and port, to fine-tune how the health check requests are made to the application. Here is a configuration file that checks if a Pod is healthy by running an HTTP call against a <code class="inlineCode">/healthcheck</code> endpoint in a nginx container:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># nginx-pod-with-liveness-http.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-pod-with-liveness-http</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-pod-with-liveness-http</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">livenessProbe:</span>
        <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">5</span>
        <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">5</span>
        <span class="hljs-attr">httpGet:</span>
          <span class="hljs-attr">path:</span> <span class="hljs-string">/healthcheck</span>
          <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>
          <span class="hljs-attr">httpHeaders:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">My-Custom-Header</span>
              <span class="hljs-attr">value:</span> <span class="hljs-string">My-Custom-Header-Value</span>
</code></pre>
    <p class="normal">Please pay attention to all sections after the <code class="inlineCode">livenessProbe</code> blocks. If you understand this well, you can see that we will wait 5 seconds before performing the first health check, and then, we will run one HTTP call against the <code class="inlineCode">/healthcheck</code> path on port <code class="inlineCode">80</code> every 5 seconds. One custom HTTP header was added. Adding such a header will be useful to identify our health checks in the access logs. Be careful because the <code class="inlineCode">/healthcheck</code> path probably won’t exist in our nginx container, and so this container will never be considered healthy because the liveness probe will result in a <code class="inlineCode">404</code> HTTP response. Keep in mind that for <a id="_idIndexMarker828"/>an HTTP health check to succeed, it must answer <a id="_idIndexMarker829"/>an HTTP &gt;= <code class="inlineCode">200</code> and &lt; <code class="inlineCode">400</code>. With <code class="inlineCode">404</code> being out of this range, the answer Pod won’t be healthy.</p>
    <h3 class="heading-3" id="_idParaDest-335">Command livenessProbe</h3>
    <p class="normal">You can also use <a id="_idIndexMarker830"/>a command to check if a Pod is healthy or not. Let’s grab the same <a id="_idIndexMarker831"/>YAML configuration, but now, we will use a command instead of an HTTP call in the liveness probe, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># nginx-pod-with-liveness-command.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-pod-with-liveness-command</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-pod-with-liveness-command</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">livenessProbe:</span>
        <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">5</span>
        <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">5</span>
        <span class="hljs-attr">exec:</span>
          <span class="hljs-attr">command:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">cat</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">/hello/world</span> 
</code></pre>
    <p class="normal">If you check this example, you can see that it is much simpler than the HTTP one. Here, we are basically running a <code class="inlineCode">cat /hello/world</code> command every 5 seconds. If the file exists and the <code class="inlineCode">cat</code> command completes with an exit code equal to <code class="inlineCode">0</code>, then the health check will <a id="_idIndexMarker832"/>succeed. Otherwise, if the file is not present, the health check <a id="_idIndexMarker833"/>will fail, and the Pod will never be considered healthy and will be terminated.</p>
    <h3 class="heading-3" id="_idParaDest-336">TCP livenessProbe</h3>
    <p class="normal">In this situation, we will attempt <a id="_idIndexMarker834"/>a connection to a TCP socket on port <code class="inlineCode">80</code>. If the connection is <a id="_idIndexMarker835"/>successfully established, then the health check will pass, and the container will be considered ready. Otherwise, the health check will fail, and the Pod will be terminated eventually. The code is illustrated in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># nginx-pod-with-liveness-tcp.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-pod-with-liveness-tcp</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-pod-with-liveness-tcp</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">livenessProbe:</span>
        <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">5</span>
        <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">5</span>
        <span class="hljs-attr">tcpSocket:</span>
          <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>
</code></pre>
    <p class="normal">Using TCP<a id="_idIndexMarker836"/> health checks greatly resembles using HTTP ones since HTTP is based on <a id="_idIndexMarker837"/>TCP. But having TCP as a liveness probe is especially nice if you want to keep track of an application that is not based on using HTTP as protocol and if using that command is irrelevant to you, as when health-checking an LDAP connection, for example.</p>
    <h3 class="heading-3" id="_idParaDest-337">Using named Port with TCP and HTTP livenessProbe</h3>
    <p class="normal">You can <a id="_idIndexMarker838"/>use the <a id="_idIndexMarker839"/>named port to<a id="_idIndexMarker840"/> configure the <code class="inlineCode">livenessProbe</code> for HTTP and TCP types (but not with gRPC) as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># nginx-pod-with-liveness-http-named-port.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-pod-with-liveness-http-named-port</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-pod-with-liveness-http-named-port</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">ports:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">liveness-port</span>
          <span class="hljs-attr">containerPort:</span> <span class="hljs-number">8080</span>
          <span class="hljs-attr">hostPort:</span> <span class="hljs-number">8080</span>
      <span class="hljs-attr">livenessProbe:</span>
        <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">5</span>
        <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">5</span>
        <span class="hljs-attr">httpGet:</span>
          <span class="hljs-attr">path:</span> <span class="hljs-string">/healthcheck</span>
          <span class="hljs-attr">port:</span> <span class="hljs-string">liveness-port</span>
</code></pre>
    <p class="normal">In the preceding example, the <code class="inlineCode">liveness-port</code> has been defined under the <code class="inlineCode">ports</code> section and used under the <code class="inlineCode">httpGet</code> for <code class="inlineCode">livenessProbe</code>.</p>
    <p class="normal">As we have explored multiple liveness probes, let us learn about startupProbe in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-338">Using startupProbe</h2>
    <p class="normal">Legacy applications sometimes<a id="_idIndexMarker841"/> demand extra time on their first startup. This can create a dilemma when setting up liveness probes, as fast response times are crucial for detecting deadlocks.</p>
    <p class="normal">The answer lies in using either initialDelaySeconds or a dedicated <code class="inlineCode">startupProbe</code>. The <code class="inlineCode">initialDelaySeconds</code> parameter allows you to postpone the first readiness probe, giving the application breathing room to initialize.</p>
    <p class="normal">However, for more granular control, consider using a <code class="inlineCode">startupProbe</code>. This probe mirrors your liveness probe (command, HTTP, or TCP check) but with a longer <code class="inlineCode">failureThreshold * periodSeconds</code> duration. This extended waiting period ensures the application has ample time to initialize before being deemed ready for traffic, while still enabling the liveness probe to swiftly detect issues afterwards, as explained in the following YAML snippet:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># nginx-pod-with-startupprobe.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-pod-with-startupprobe</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-pod-with-startupprobe</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">ports:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">liveness-port</span>
          <span class="hljs-attr">containerPort:</span> <span class="hljs-number">8080</span>
          <span class="hljs-attr">hostPort:</span> <span class="hljs-number">8080</span>
      <span class="hljs-attr">livenessProbe:</span>
        <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">5</span>
        <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">5</span>
        <span class="hljs-attr">httpGet:</span>
          <span class="hljs-attr">path:</span> <span class="hljs-string">/healthcheck</span>
          <span class="hljs-attr">port:</span> <span class="hljs-string">liveness-port</span>
      <span class="hljs-attr">startupProbe:</span>
        <span class="hljs-attr">httpGet:</span>
          <span class="hljs-attr">path:</span> <span class="hljs-string">/healthz</span>
          <span class="hljs-attr">port:</span> <span class="hljs-string">liveness-port</span>
        <span class="hljs-attr">failureThreshold:</span> <span class="hljs-number">30</span>
        <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">10</span>
</code></pre>
    <p class="normal">As you can see in the example code above, it is possible to combine multiple probes to ensure the application is ready to serve. In the following section, we will also learn how to use <code class="inlineCode">ReadinessProbe</code> and <code class="inlineCode">LivenessProbe</code> together.</p>
    <h2 class="heading-2" id="_idParaDest-339">Using ReadinessProbe and LivenessProbe together</h2>
    <p class="normal">You can use <code class="inlineCode">ReadinessProbe</code> and <code class="inlineCode">LivenessProbe</code> together in the same Pod.</p>
    <p class="normal">They are configured <a id="_idIndexMarker842"/>almost the same way—they don’t have exactly the same purpose, and it is fine to use them together. Please note that both the probes share these parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">initialDelaySeconds</code>: The number of seconds to wait before the first probe execution.</li>
      <li class="bulletList"><code class="inlineCode">periodSeocnds</code>: The number of seconds between two probes.</li>
      <li class="bulletList"><code class="inlineCode">timeoutSeconds</code>: The number of seconds to wait before timeout.</li>
      <li class="bulletList"><code class="inlineCode">successThreshold</code>: The number of successful attempts to consider a Pod is ready (for <code class="inlineCode">ReadinessProbe</code>) or healthy (for <code class="inlineCode">LivenessProbe</code>).</li>
      <li class="bulletList"><code class="inlineCode">failureThreshold</code>: The number of failed attempts to consider a Pod is not ready (for <code class="inlineCode">ReadinessProbe</code>) or ready to be killed (for <code class="inlineCode">LivenessProbe</code>).</li>
      <li class="bulletList"><code class="inlineCode">TerminationGracePeriodSeconds</code>: Give containers a grace period to shut down gracefully before being forcefully stopped (default inherits Pod-level value).</li>
    </ul>
    <p class="normal">We now have discovered <code class="inlineCode">ReadinessProbe</code> and <code class="inlineCode">LivenessProbe</code>, and we have reached the end of this chapter about Kubernetes Services and implementation methods.</p>
    <h1 class="heading-1" id="_idParaDest-340">Summary</h1>
    <p class="normal">This chapter was dense and contained a huge amount of information on networking in general when applied to Kubernetes. Services are just like Pods: they are the foundation of Kubernetes and mastering them is crucial to being successful with the orchestrator.</p>
    <p class="normal">Overall, in this chapter, we discovered that Pods have dynamic IP assignments, and they get a unique IP address when they’re created. To establish a reliable way to connect to your Pods, you need a proxy called a <code class="inlineCode">Service</code> in Kubernetes. We’ve also discovered that Kubernetes Services can be of multiple types and that each type of Service is designed to address a specific need. We’ve also discovered what <code class="inlineCode">ReadinessProbe</code> and <code class="inlineCode">LivenessProbe</code> are and how they can help you in designing health checks to ensure your pods get traffic when they are ready and live.</p>
    <p class="normal">In the next chapter, we’ll continue to discover the basics of Kubernetes by discovering the concepts of <code class="inlineCode">PersistentVolume</code> and <code class="inlineCode">PersistentVolumeClaims</code>, which are the methods Kubernetes uses to deal with persistent data. It is going to be a very interesting chapter if you want to build and provision stateful applications on your Kubernetes clusters, such as database or file storage solutions.</p>
    <h1 class="heading-1" id="_idParaDest-341">Further reading</h1>
    <ul>
      <li class="bulletList">Services, Load Balancing, and Networking: <a href="https://kubernetes.io/docs/concepts/services-networking/"><span class="url">https://kubernetes.io/docs/concepts/services-networking/</span></a></li>
      <li class="bulletList">Kubernetes Headless Services: <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services"><span class="url">https://kubernetes.io/docs/concepts/services-networking/service/#headless-services</span></a></li>
      <li class="bulletList">Configure Liveness, Readiness and Startup Probes: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/"><span class="url">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/</span></a></li>
      <li class="bulletList">Network Policies: <span class="url">https://kubernetes.io/docs/concepts/services-networking/network-policies/</span></li>
      <li class="bulletList">Declare Network Policy: <span class="url">https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/</span></li>
      <li class="bulletList">Default NetworkPolicy: <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies"><span class="url">https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies</span></a></li>
      <li class="bulletList">EndpointSlices: <a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/"><span class="url">https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/</span></a></li>
      <li class="bulletList">Network Plugins: <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/"><span class="url">https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/</span></a></li>
      <li class="bulletList">Cluster Networking: <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/"><span class="url">https://kubernetes.io/docs/concepts/cluster-administration/networking/</span></a></li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-342">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img alt="" src="image/QR_Code1190011064790816562.png"/></p>
  </div>
</body></html>