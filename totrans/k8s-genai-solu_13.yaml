- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: High Availability and Disaster Recovery for GenAI Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explore the concepts of **high availability** (**HA**) and
    **disaster recovery** (**DR**) tailored for GenAI applications deployed on **Kubernetes**
    (**K8s**) clusters. Given the dynamic and resource-intensive nature of GenAI applications,
    achieving seamless scalability and robust resiliency is essential for high-quality
    production deployments. We will discuss various architectural patterns and configurations
    that empower GenAI workloads to automatically scale based on usage demand while
    ensuring continuous service, even in the event of a disaster such as a regional
    outage.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing for HA and DR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resiliency in K8s
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DR strategies in K8s
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing for HA and DR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'HA ([https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/high-availability-is-not-disaster-recovery.html](https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/high-availability-is-not-disaster-recovery.html))
    ensures that a system remains operational with minimal downtime by eliminating
    single points of failure. It relies on *redundancy* across nodes, regions, or
    clusters and aims to maintain continuous service. HA is measured by uptime percentage,
    failover time, and system redundancy. For example, a system with 99.99% uptime
    allows only ~53 minutes of downtime per year. In the context of GenAI, where foundational
    models often drive critical business operations such as customer support, real-time
    text and image analysis, and so on, downtime can be expensive. HA ensures the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Inference endpoints remain consistently responsive, meeting the business availability
    requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training jobs can handle node or service failures without crashing mid-way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DR ([https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html](https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html))
    is focused on restoring services after catastrophic failures such as hardware
    malfunctions, cyberattacks, or natural disasters. It ensures that data is backed
    up and can be restored quickly to resume operations. DR strategies involve regular
    data backups, redundancy, and automated recovery workflows. Unlike HA, which prevents
    downtime, DR accepts some level of downtime and data loss but ensures that systems
    can be restored efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three key metrics that define HA and DR are the **recovery point objective**
    (**RPO**), **recovery time objective** (**RTO**), and **maximum tolerable** **downtime**
    (**MTD**):'
  prefs: []
  type: TYPE_NORMAL
- en: RPO represents the maximum allowable data loss before recovery. A system with
    an RPO of 0 requires real-time data replication to ensure no data is lost, whereas
    an RPO of several hours may use periodic backups instead. The lower the RPO, the
    more advanced the backup mechanisms need to be.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RTO determines the acceptable downtime before services must be restored. A low
    RTO of seconds or minutes requires active-active failover with redundant systems
    always on standby, while a higher RTO allows for manual intervention and restoration
    from backups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MTD is the longest period a service can be unavailable before causing unacceptable
    consequences to an organization. It defines the threshold for downtime beyond
    which service can suffer operational or financial challenges. MTD is a key component
    of **business continuity planning** (**BCP**) and DR strategies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 13**.1* illustrates these key metrics – RTO, RPO, and MTD in the context
    of data loss and system downtime following a failure event.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Different recovery objectives](img/B31108_13_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – Different recovery objectives
  prefs: []
  type: TYPE_NORMAL
- en: A highly available application should be able to withstand failures and maintain
    continuous operation despite partial network outages or hardware failures. It
    requires that the application has no single point of failure and workloads are
    distributed across multiple isolated failure domains, such as nodes, **Availability
    Zones** (**AZs**), and clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Redundancy at various levels helps to handle potential failures. Key tenets
    of K8s that help to achieve HA include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Redundancy**: Avoid single points of failure in both application components
    and infrastructure. Deploying multiple replicas of the applications using K8s
    Deployment or ReplicaSet objects can ensure redundancy in case of failures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoscaling**: K8s **Horizontal Pod Autoscaling** (**HPA**) can help adjust
    the number of Pod replicas based on demand, ensuring that the application can
    handle varying loads efficiently. Additionally, Cluster Autoscaler and Karpenter
    can help manage the scaling of worker nodes in response to the scheduling needs
    of Pods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self-healing**: Deploying applications using K8s Deployment allows K8s to
    automatically replace failed Pods, maintaining the desired state of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safer upgrades and rollbacks**: By adopting application deployment strategies
    such as blue/green and canary deployments, you can ensure that new versions of
    applications are introduced safely. These strategies enable the testing of new
    versions with a subset of users before a full rollout, reducing the risk of widespread
    issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chaos engineering**: Periodically simulate failures in your applications
    to validate the HA setup. Review and improve runbooks and operational guidelines
    based on simulated incidents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observability**: Collect the logs, metrics, and traces for real-time visibility
    into the infrastructure and the application’s health and performance. Configure
    alerts to detect early signs of failures such as latency, error rate, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we discussed the importance of HA and DR for GenAI applications,
    which are uniquely sensitive to downtime and performance degradation. We also
    highlighted the key metrics that define HA and DR, such as RTO, RPO, and MTD,
    alongside the key K8s tenets that help achieve HA.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will delve deeper into these concepts by focusing on
    resiliency in K8s.
  prefs: []
  type: TYPE_NORMAL
- en: Resiliency in K8s
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GenAI applications are resource-intensive, requiring fault tolerance and scalability
    to handle model training, large-scale inference, and real-time AI workloads. GenAI
    models usually require GPUs for accelerated inference and training, making GPU
    dependency and availability a critical factor in deployment. These workloads often
    experience unpredictable resource spikes, leading to scalability challenges that
    require dynamic provisioning. Additionally, data availability and consistency
    are essential, as large AI models rely on distributed storage and caching to maintain
    performance across multiple nodes. Long-running processes further complicate resilience,
    as model training can take hours or even days.
  prefs: []
  type: TYPE_NORMAL
- en: K8s provides a robust foundation for managing GenAI workloads, but ensuring
    resiliency requires specialized configurations and best practices at every layer
    of K8s, as shown in *Figure 13**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – K8s resiliency across different layers](img/B31108_13_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.2 – K8s resiliency across different layers
  prefs: []
  type: TYPE_NORMAL
- en: 'These layers help ensure that applications remain highly available and can
    recover from failures. Let’s cover each layer, starting from the innermost layer
    at the Pod level:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/healthz` endpoint every 10 seconds on port `80`; similarly, the readiness
    probe is configured to check the `/``readyz` endpoint:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Replica level**: A replica is an identical copy of a Pod managed by a K8s
    controller, such as a **ReplicaSet** or **Deployment**, as covered in previous
    chapters. Having multiple replicas ensures that even if one Pod fails, other instances
    remain available to handle requests. It is especially important for AI model servers,
    such as **TensorFlow Serving** or **Triton Inference Server**, to ensure that
    inference requests can meet the SLA as the demand increases. Deployments should
    define a suitable number of replicas based on workload needs and traffic demands.
    HPA can dynamically adjust the number of replicas based on CPU, memory, and GPU
    usage, providing flexibility during high-load scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For inference workloads, it is a good idea to have a minimum number of GenAI
    inference/model-serving Pods remain available during updates or disruptions by
    using **PodDisruptionBudget** ([https://kubernetes.io/docs/tasks/run-application/configure-pdb/](https://kubernetes.io/docs/tasks/run-application/configure-pdb/)),
    as shown in the following K8s manifest:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`topologySpreadConstraints` to spread Pod replicas across multiple nodes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: At the node level, K8s ensures basic resilience through health checks and eviction
    policies. However, for production-grade GenAI workloads, you often need additional
    safeguards and automatic recovery. You can leverage the K8s `node-problem-detector`
    ([https://github.com/kubernetes/node-problem-detector](https://github.com/kubernetes/node-problem-detector))
    add-on, which makes various node problems visible to the upstream layers in the
    cluster management stack. It runs as a **DaemonSet** Pod on every worker node
    to scan for failures and reports them to *apiserver*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Amazon EKS introduced a **Node monitoring agent** ([https://docs.aws.amazon.com/eks/latest/userguide/node-health.html](https://docs.aws.amazon.com/eks/latest/userguide/node-health.html))
    add-on that automatically reads node logs to detect certain health issues and
    adds *NodeCondition* accordingly. This can be combined with **Node auto repair**
    ([https://docs.aws.amazon.com/eks/latest/userguide/node-health.html#node-auto-repair](https://docs.aws.amazon.com/eks/latest/userguide/node-health.html#node-auto-repair)),
    which monitors the health of nodes, automatically reacting to detected problems
    and replacing nodes when possible. For example, when **Xid errors** ([https://docs.nvidia.com/deploy/xid-errors/index.html#topic_5_1](https://docs.nvidia.com/deploy/xid-errors/index.html#topic_5_1))
    are detected on GPU nodes, it automatically replaces them after 10 minutes and
    evicts the Pods to get them scheduled on healthy nodes. Xid errors are error codes
    generated by NVIDIA GPU drivers indicating that the GPU has encountered an issue,
    such as a hang, reset, or memory fault.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**AZ level**: AZs are isolated data centers within a cloud provider’s region.
    Running workloads across multiple AZs provides higher fault tolerance, protecting
    against failures at the data center level. K8s clusters deployed in a multi-AZ
    configuration ensure that even if an entire AZ experiences an outage, applications
    continue running in another AZ. You can leverage K8s *topologySpreadConstraints*
    scheduling constraints to distribute the Pods managed by a ReplicaSet or StatefulSet
    across different failure domains, such as AZs, to ensure protection against AZ
    issues. Combine it with nodes for an additional layer of resiliency:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Additionally, Amazon EKS supports Amazon **Application Recovery Controller**
    (**ARC**) zonal shift and zonal autoshift ([https://aws.amazon.com/application-recovery-controller/](https://aws.amazon.com/application-recovery-controller/)).
    ARC helps you to manage and coordinate the recovery of applications across AZs
    and AWS Regions. With zonal shift, you can temporarily mitigate issues and incidents
    by triggering a shift and redirecting in-cluster network traffic to a healthy
    AZ. For a fully automated experience, you can authorize AWS to manage this shift
    on your behalf using zonal autoshift. With zonal autoshift, you can configure
    practice runs to test that your cluster environment functions as expected with
    one less AZ. Refer to the AWS documentation at [https://docs.aws.amazon.com/eks/latest/userguide/zone-shift.html](https://docs.aws.amazon.com/eks/latest/userguide/zone-shift.html)
    to learn more about this feature and find instructions to enable it on your EKS
    cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Multi-cluster deployment**: A multi-cluster architecture involves running
    workloads across multiple independent K8s clusters. This approach is useful for
    mitigating failures at the cluster level, ensuring that if one cluster fails due
    to a control plane issue or networking disruption, another cluster can take over
    the workload. Multi-cluster deployments are often used for active-active, DR,
    and geo-distributed applications. You can leverage services such as **Amazon Route
    53** ([https://aws.amazon.com/route53/](https://aws.amazon.com/route53/)) and
    **AWS Global Accelerator** ([https://aws.amazon.com/global-accelerator/](https://aws.amazon.com/global-accelerator/))
    to perform health checks and route the traffic in a multi-cluster setup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global deployment**: At the highest level, deploying workloads across multiple
    geographic regions ensures that applications remain available even if an entire
    AWS Region experiences an outage. This approach not only enhances DR capabilities
    but also provides low-latency access to users in different locations. However,
    multi-region architectures require careful management of data consistency, replication,
    and failover processes to guarantee seamless recovery when regional failures occur.
    Because Amazon EKS is a regional service, you must provision a separate EKS cluster
    in each AWS Region to achieve a truly global deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these layers contributes to overall system resilience in K8s. By implementing
    redundancy at different levels, organizations can build highly available, fault-tolerant
    applications that withstand various types of failures, from individual Pod crashes
    to full-scale regional outages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other K8s options for resiliency and HA are load balancing and service discovery:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load balancing**: K8s services provide built-in load balancing to distribute
    network traffic across multiple Pod instances. By defining a service, you can
    expose an application running on a set of Pods as a network service, with K8s
    handling the distribution of traffic to ensure no single Pod becomes a bottleneck.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service discovery**: K8s offers service discovery mechanisms that allow applications
    and services to locate and communicate with each other efficiently, even as instances
    are created or terminated. This dynamic discovery is facilitated through environment
    variables or DNS, enabling seamless interaction between services within the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we discussed how resiliency can be implemented at various layers
    in K8s environments, from the individual Pods to multi-AZ, multi-cluster, and
    multi-region architectures. In the next section, we will explore various DR strategies
    and how they can be applied to K8s workloads.
  prefs: []
  type: TYPE_NORMAL
- en: DR strategies in K8s
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DR focuses on restoring services and data after catastrophic events such as
    natural disasters, security breaches, and significant system failures. An effective
    DR plan for K8s should aim to minimize data loss (RPO) and reduce downtime (RTO).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.3* highlights four different DR strategies in the cloud, as highlighted
    in the AWS white paper for DR: [https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html](https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html).'
  prefs: []
  type: TYPE_NORMAL
- en: As we move from backup and restore to multi-site active/active, the RPO and
    RTO time shrinks from hours to minutes. However, complexity, orchestration, and
    cloud spend increase.
  prefs: []
  type: TYPE_NORMAL
- en: Choose a DR strategy based on the business application’s uptime requirements
    and use case.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – Disaster recovery strategies](img/B31108_13_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 – Disaster recovery strategies
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore a high-level perspective on architecting these DR strategies
    in K8s environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Backup and restore** (**RPO/RTO time in hours**): In K8s, backup and restore
    strategies are essential for lower-priority workloads where some downtime is acceptable.
    This approach involves periodically backing up data stored in **PersistentVolumes**
    (**PVs**) and other cluster resources such as **ConfigMaps**, **Secrets**, and
    **role-based access control** (**RBAC**) policies. During a disaster, all K8s
    resources must be provisioned again, and the backed-up data is restored. This
    method is cost-effective but results in longer recovery times, as restoring backups
    and re-provisioning the cluster can take hours. While this approach is viable
    for non-mission-critical applications, it does not meet the HA needs of production
    workloads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source tools such as **Velero** ([https://velero.io/](https://velero.io/))
    and commercial solutions such as **Trilio for Kubernetes** ([https://trilio.io/products/kubernetes-backup-and-recovery/](https://trilio.io/products/kubernetes-backup-and-recovery/))
    and **Portworx Backup** ([https://portworx.com/kubernetes-backup/](https://portworx.com/kubernetes-backup/))
    provide automated backup and restore capabilities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Velero is an open source backup and restore solution designed for K8s workloads.
    It supports cloud-native environments, including AWS, Azure, and Google Cloud.
    Velero allows on-demand and scheduled backups of K8s clusters, covering Pods,
    deployments, and persistent volumes. It allows namespace-level and full-cluster
    backups, providing fine-grained control over data protection. One of Velero’s
    strengths is its DR and cluster migration capabilities. Its scheduling features
    allow users to define periodic backups using cron-based scheduling, ensuring compliance
    with recovery and data retention policies. The tool is designed for multi-cloud
    environments, making it easier to implement hybrid cloud strategies. Additionally,
    Velero supports encryption for secure backup storage and uses RBAC to enforce
    security best practice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Besides data, it’s also essential to restore the cluster configuration, Secrets,
    and RBAC policies. These configurations can either be backed up using the same
    tooling or deployed using **infrastructure as code** (**IaC**) or **GitOps** ([https://about.gitlab.com/topics/gitops/](https://about.gitlab.com/topics/gitops/))
    tools. This enables the quick restoration of a K8s environment in case of failure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Pilot light (RPO/RTO: 10s of minutes)**: The pilot light strategy keeps essential
    data and minimal K8s infrastructure live while leaving most services idle until
    a disaster occurs. This allows for quicker recovery compared to backup and restore,
    as some resources are already running and do not need to be provisioned from scratch.
    Persistent storage remains active, ensuring that stateful applications retain
    their critical data. However, the remaining workloads, such as application services
    and networking configurations, only become active when a failure is detected.
    This approach strikes a balance between cost and recovery speed by requiring only
    a fraction of the resources to be continuously available. Tools such as Velero,
    which support namespace-level and cluster-scoped backups, enable this setup by
    ensuring that key K8s objects and data are readily available for rapid scaling
    when needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Warm standby (RPO/RTO: minutes)**: A warm standby configuration ensures that
    a smaller-scale version of the production environment is always running, reducing
    recovery time to minutes. This approach is best suited for business-critical applications
    where downtime must be minimal, but maintaining a full-scale duplicate environment
    would be cost-prohibitive. The warm standby cluster continuously runs with scaled-down
    replicas of workloads, allowing immediate failover and rapid horizontal scaling
    when a disaster occurs. Additionally, real-time data replication solutions such
    as Portworx and Trilio for Kubernetes keep persistent storage synchronized across
    clusters, ensuring data consistency. This approach significantly reduces downtime
    while maintaining cost efficiency compared to a fully active environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-site active/active (RPO/RTO: near real time)**: The multi-site active/active
    strategy offers the highest level of resilience by running multiple K8s clusters
    in different regions or cloud providers in real time. This setup ensures zero
    downtime and near-zero data loss, making it ideal for mission-critical services
    that demand continuous availability. Unlike other approaches, this strategy requires
    full redundancy, meaning that all workloads and data are replicated and running
    across multiple clusters simultaneously. Cross-region cluster deployment and cloud
    load balancers dynamically distribute traffic, ensuring seamless operation even
    if one cluster experiences an outage. Service mesh solutions such as **Istio**
    facilitate secure communication between clusters, while database replication strategies
    keep persistent data synchronized. Though this strategy incurs significant infrastructure
    costs, it provides the most reliable DR solution for organizations that cannot
    afford any service disruptions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s consider a scenario where you have a GenAI application running in the
    AWS US-EAST-1 Region, which is your primary region. To ensure HA, you maintain
    a warm standby cluster in the US-WEST-2 region with a minimal compute footprint.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the event of a regional outage in US-EAST-1, the following steps detail
    how the failover process would occur:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud monitoring, such as Amazon Route 53 health checks and CloudWatch alarms,
    detects that services in US-EAST-1 are unavailable. Application-level readiness
    and liveness probes start failing, indicating service degradation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNS failover mechanisms, such as the **Amazon Route 53 failover routing** ([https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-failover.html](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-failover.html))
    policy, automatically redirect traffic to the US-WEST-2 standby cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HPA/Cluster Autoscaler in the standby cluster triggers scale-up events. GenAI
    application endpoints and underlying worker nodes scale out to handle the production
    load.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standby cluster switches from passive to active mode, serving production
    traffic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once US-EAST-1 is available again, evaluate data integrity and sync any missed
    transactions or logs. Once resynced, demote US-WEST-2 back to standby mode and
    resume normal operations in the primary region.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional K8s DR considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we explore the importance of automating DR using chaos engineering
    to validate the system’s resilience and implementing proactive monitoring to detect
    outages early:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DR automation and testing**: Automating DR processes significantly reduces
    human error and accelerates recovery times. Using IaC tools such as Terraform
    ensures that K8s clusters can be redeployed quickly and consistently in the event
    of an outage. Automated failover solutions, such as Amazon Route 53 health checks,
    detect failures and reroute traffic to healthy instances automatically. To validate
    DR readiness, organizations should regularly conduct DR testing and drills.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaos engineering tools include **Chaos Mesh** ([https://chaos-mesh.org/](https://chaos-mesh.org/)),
    a cloud-native, open source K8s chaos engineering platform that allows users to
    simulate various failure scenarios within K8s clusters. It supports fine-grained
    chaos experiments at multiple levels, including the Pod, network, and storage
    levels. It can inject Pod failures, network disruptions, and node crashes in K8s
    Deployment. It also supports **CustomResourceDefinition** (**CRDs**) to define
    chaos experiments declaratively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Monitoring and observability**: Proactive monitoring and observability help
    detect issues before they escalate into major outages. K8s provides built-in health
    checks through liveness and readiness probes, which restart unhealthy Pods to
    prevent failures from impacting the entire system. Logging and metrics collection
    tools such as Prometheus, Grafana, Fluentd, and Elasticsearch enable real-time
    visibility into cluster performance and system health. Implementing an alerting
    system integrated with PagerDuty or Slack ensures that incidents trigger immediate
    notifications, allowing response teams to act quickly and mitigate potential disruptions.
    A well-configured observability stack is crucial for diagnosing issues and optimizing
    DR strategies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the key concepts for HA and DR for GenAI applications
    deployed on K8s. Given the resource-intensive nature of GenAI workloads, it is
    critical to have scalability and resilience against hardware failures and regional
    outages.
  prefs: []
  type: TYPE_NORMAL
- en: HA minimizes downtime by eliminating single points of failure through redundancy
    across nodes, clusters, and regions. Key HA strategies in K8s include auto-scaling,
    self-healing, multi-cluster deployments, and load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: DR focuses on restoring services after failures such as hardware malfunctions,
    cyberattacks, and natural disasters. Key DR metrics include RPO, RTO, and MTD.
    Various DR strategies include backup and restore (slow recovery but cost-effective),
    pilot light (minimal infrastructure remaining active for quicker recovery), warm
    standby (scaled-down live environment that quickly scales up), and multi-site
    active/active deployment (fully redundant clusters ensuring near-zero downtime).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, chaos engineering, automation, monitoring, and observability are
    crucial for enhancing HA and DR.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover a few other advanced GenAI topics related
    to K8s.
  prefs: []
  type: TYPE_NORMAL
