- en: '*Chapter 10*: Operating and Maintaining Efficient Kubernetes Clusters'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 10 章*: 高效操作与维护 Kubernetes 集群'
- en: In previous chapters, we learned about production best practices for automating
    Kubernetes and its infrastructure components. We discussed challenges with provisioning
    stateless workloads in our clusters, including getting persistent storage up and
    running, choosing container images, and deployment strategies. We also learned
    about important observability tools in the ecosystem and building monitoring and
    logging stacks in our cluster to provide a solid base for our troubleshooting
    needs. Once we have a production-ready cluster and have started to serve workloads,
    it is vital to have efficient operations to oversee the cluster maintenance, availability,
    and other **service-level objectives** (**SLOs**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了自动化 Kubernetes 及其基础设施组件的生产最佳实践。我们讨论了在集群中配置无状态工作负载的挑战，包括如何让持久化存储正常运行、选择容器镜像和部署策略。我们还了解了生态系统中重要的可观察性工具，并在集群中构建监控和日志堆栈，为故障排除需求提供坚实基础。一旦我们拥有一个生产就绪的集群并开始提供工作负载，维持集群的高效运作以保证集群的维护、可用性以及其他**服务级目标**（**SLOs**）就变得至关重要。
- en: In this chapter, we will focus on Kubernetes operation best practices and cover
    topics related to cluster maintenance, such as upgrades and rotation, backups,
    disaster recovery and avoidance, cluster and troubleshooting failures of the cluster
    control plane, workers, and applications. Finally, we will learn about the solutions
    available to validate and improve our cluster's quality.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点介绍 Kubernetes 操作最佳实践，并涵盖与集群维护相关的主题，如升级和轮换、备份、灾难恢复与避免、集群以及控制平面、工作节点和应用程序故障的故障排除。最后，我们将学习用于验证和改善集群质量的解决方案。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论以下主要主题：
- en: Learning about cluster maintenance and upgrades
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习集群维护和升级
- en: Preparing for backups and disaster recovery
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备备份和灾难恢复
- en: Validating cluster quality
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集群质量
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You should have the following tools installed from previous chapters:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经安装了以下工具（来自前几章）：
- en: AWS CLI v2
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS CLI v2
- en: AWS IAM authenticator
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS IAM 认证器
- en: '`kubectl`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl`'
- en: Terraform
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Terraform
- en: Helm 3
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Helm 3
- en: '`metrics-server`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics-server`'
- en: MinIO instance (optional as an S3 target for backups)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MinIO 实例（作为备份的 S3 目标， 可选）
- en: You need to have an up-and-running Kubernetes cluster as per the instructions
    in [*Chapter 3*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073), *Provisioning
    Kubernetes Clusters Using AWS and Terraform*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要根据 [*第 3 章*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073) 中的指示，拥有一个正常运行的
    Kubernetes 集群，*使用 AWS 和 Terraform 配置 Kubernetes 集群*。
- en: The code for this chapter is located at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于 [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10)。
- en: 'Check out the following link to see the Code in Action video:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接，观看实际操作视频：
- en: '[https://bit.ly/3aAdPzl](https://bit.ly/3aAdPzl)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/3aAdPzl](https://bit.ly/3aAdPzl)'
- en: Learning about cluster maintenance and upgrades
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习集群维护和升级
- en: In this section, we will learn about upgrading our Kubernetes clusters in production.
    Generally, a new major Kubernetes version is announced quarterly, and every minor
    version is supported around 12 months after its initial release date. Following
    the rule of thumb for software upgrades, it is not common to upgrade to a new
    version immediately after its release unless it is a severe time-sensitive security
    patch. Cloud providers also follow the same practice and run their conformance
    tests before releasing a new image to the public. Therefore, cloud providers'
    Kubernetes releases usually follow a couple of versions behind the upstream release
    of Kubernetes. If you'd like to read about the latest releases, you can find the
    Kubernetes release notes on the official Kubernetes documentation site at https://kubernetes.io/docs/setup/release/notes/.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何升级生产环境中的 Kubernetes 集群。通常，新的 Kubernetes 主版本会每季度发布一次，每个次版本在发布后的大约
    12 个月内提供支持。按照软件升级的一般规则，除非是紧急的、时间敏感的安全补丁，否则不常在新版本发布后立即进行升级。云服务提供商也遵循相同的做法，并在向公众发布新镜像之前进行合规性测试。因此，云服务提供商的
    Kubernetes 版本通常会滞后于 Kubernetes 上游发布的几个版本。如果你想了解最新的版本发布，可以在 Kubernetes 官方文档站点找到相关的发布说明，网址为
    [https://kubernetes.io/docs/setup/release/notes/](https://kubernetes.io/docs/setup/release/notes/)。
- en: In [*Chapter 3*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073), *Provisioning
    Kubernetes Clusters Using AWS and Terraform*, we learned about cluster deployment
    and rollout strategies. We also learned that cluster deployment is not a one-time
    task. It is a continuous process that affects the cluster's quality, stability,
    and operations, as well as the products and services on top of it. In previous
    chapters, we established a solid infrastructure deployment strategy, and now we
    will follow it with production-grade upgrade best practices in this chapter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 3 章*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073)，*使用 AWS 和 Terraform
    部署 Kubernetes 集群* 中，我们了解了集群部署和发布策略。我们还了解到，集群部署不是一次性任务，而是一个持续的过程，它影响集群的质量、稳定性和运营，甚至影响基于集群运行的产品和服务。在之前的章节中，我们建立了坚实的基础设施部署策略，现在我们将在本章中继续遵循它，实施生产级的升级最佳实践。
- en: In [*Chapter 3*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073), *Provisioning
    Kubernetes Clusters Using AWS and Terraform*, we automated our cluster deployment
    using Terraform. Let's use the same cluster and upgrade it to a newer Kubernetes
    release.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 3 章*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073)，*使用 AWS 和 Terraform
    部署 Kubernetes 集群* 中，我们通过 Terraform 自动化了集群部署。现在让我们使用相同的集群，将其升级到一个更新的 Kubernetes
    版本。
- en: Upgrading kubectl
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 升级 kubectl
- en: 'First, we will upgrade `kubectl` to the latest version. Your `kubectl` version
    should be at least equal to or greater than the Kubernetes version you are planning
    to upgrade to:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将升级 `kubectl` 到最新版本。你的 `kubectl` 版本应该至少等于或大于你计划升级到的 Kubernetes 版本：
- en: 'Download the latest `kubectl` binary and copy it to the `bin` directory:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载最新的 `kubectl` 二进制文件并将其复制到 `bin` 目录：
- en: '[PRE0]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Confirm that the `kubectl` binary is updated to the newer version by executing
    the following command:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令确认 `kubectl` 二进制文件已更新为较新版本：
- en: '[PRE1]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, check your node status and version by executing the following command:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过执行以下命令检查你的节点状态和版本：
- en: '[PRE2]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output of the preceding command should look as follows:'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述命令的输出应如下所示：
- en: '![Figure 10.1 – The kubectl command showing the node status and its version](img/B16192_10_001.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – kubectl 命令显示节点状态及其版本](img/B16192_10_001.jpg)'
- en: Figure 10.1 – The kubectl command showing the node status and its version
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – kubectl 命令显示节点状态及其版本
- en: Here, we have updated `kubectl` to the latest version. Let's move on to the
    next step and upgrade our cluster version.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经将 `kubectl` 升级到最新版本。接下来，继续进行下一步，升级我们的集群版本。
- en: Upgrading the Kubernetes control plane
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 升级 Kubernetes 控制平面
- en: AWS EKS clusters can be upgraded one version at a time. This means that if we
    are on version 1.15, we can upgrade to 1.16, then to 1.17, and so on.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: AWS EKS 集群每次只能升级一个版本。这意味着，如果我们当前的版本是 1.15，我们可以先升级到 1.16，然后再升级到 1.17，以此类推。
- en: Important note
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: You can find the complete source code at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/terraform](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/terraform).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/terraform](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/terraform)
    查找到完整的源代码。
- en: 'Let''s upgrade our controller nodes using the Terraform scripts we also used
    in [*Chapter 3*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073), *Provisioning
    Kubernetes Clusters Using AWS and Terraform*, to deploy our clusters:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们在[*第 3 章*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073)中也使用过的 Terraform
    脚本来升级我们的控制节点，*通过 AWS 和 Terraform 提供 Kubernetes 集群*：
- en: 'Edit the `terraform.tfvars` file under the `Chapter10/terraform/packtclusters`
    directory and increase the `cluster_version` value to the next release version
    number. In our example, we have increased the version from `1.15` to `1.16`:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑 `terraform.tfvars` 文件，该文件位于 `Chapter10/terraform/packtclusters` 目录下，并将 `cluster_version`
    值增加到下一个发布版本号。在我们的示例中，我们已将版本从 `1.15` 升级到 `1.16`：
- en: '[PRE3]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Run the `terraform plan` command to validate the planned changes before applying
    them:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `terraform plan` 命令以验证计划的更改，确保应用这些更改之前没有问题：
- en: '[PRE4]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Execute the `terraform apply` command. Enter `yes` when you get a prompt to
    approve the in-place update:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行 `terraform apply` 命令。当提示您确认本地更新时，输入 `yes`：
- en: '[PRE5]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'While an upgrade is in progress, we can track the progress both from the command
    line or in the AWS console. The cluster status in the AWS console will look similar
    to the following screenshot:'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在升级过程中，我们可以通过命令行或 AWS 控制台跟踪进度。AWS 控制台中的集群状态将类似于以下截图：
- en: '![Figure 10.3 – AWS console output showing the cluster status as Updating'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.3 – AWS 控制台输出显示集群状态为“正在更新”'
- en: '](img/B16192_10_003.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16192_10_003.jpg)'
- en: Figure 10.3 – AWS console output showing the cluster status as Updating
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – AWS 控制台输出显示集群状态为“正在更新”
- en: 'You will get the following result after the `terraform apply` command completes
    successfully. By then, Terraform has successfully changed one AWS resource:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`terraform apply` 命令成功完成后，您将获得以下结果。此时，Terraform 已成功更改了一个 AWS 资源：'
- en: '![Figure 10.4 – The terraform apply command output](img/B16192_10_004.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.4 – terraform apply 命令输出](img/B16192_10_004.jpg)'
- en: Figure 10.4 – The terraform apply command output
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – terraform apply 命令输出
- en: Here, we have updated our Kubernetes control plane to the next version available.
    Let's move on to the next step and upgrade our node groups.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经将 Kubernetes 控制平面更新到下一个可用版本。接下来，继续进行下一步，升级我们的节点组。
- en: Upgrading Kubernetes components
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 升级 Kubernetes 组件
- en: 'Upgrading the Kubernetes control plane doesn''t upgrade the worker nodes or
    our Kubernetes add-ons, such as `kube-proxy`, CoreDNS, and the Amazon VPC CNI
    plugin. Therefore, after upgrading the control plane, we need to carefully upgrade
    each and every component to a supported version if needed. You can read more about
    the supported component versions and Kubernetes upgrade prerequisites on the Amazon
    EKS documentation site at [https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html](https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html).
    The following figure shows an example support matrix table for the upgrade path
    we will follow in our example:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 升级 Kubernetes 控制平面并不会升级工作节点或我们的 Kubernetes 附加组件，如 `kube-proxy`、CoreDNS 和 Amazon
    VPC CNI 插件。因此，在升级控制平面后，如果需要，我们需要仔细升级每个组件到支持的版本。您可以在 Amazon EKS 文档网站上了解更多有关支持的组件版本和
    Kubernetes 升级先决条件的内容：[https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html](https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html)。以下图示例显示了我们将在示例中遵循的升级路径支持矩阵表：
- en: '![Figure 10.5 – An example of a Kubernetes component support matrix'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.5 – Kubernetes 组件支持矩阵示例'
- en: '](img/B16192_10_005.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16192_10_005.jpg)'
- en: Figure 10.5 – An example of a Kubernetes component support matrix
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – Kubernetes 组件支持矩阵示例
- en: Some version upgrades may also require changes in your application's YAML manifest
    to reference the new APIs. It is highly recommended to test your application behavior
    using a continuous integration workflow.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 某些版本升级可能还需要在应用程序的 YAML 清单中进行更改，以引用新的 API。强烈建议使用持续集成工作流测试应用程序的行为。
- en: 'Now that our EKS control plane is upgraded, let''s upgrade `kube-proxy`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的 EKS 控制平面已经升级，让我们升级 `kube-proxy`：
- en: 'Get the current version of the `kube-proxy` component by executing the following
    command:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令以获取当前版本的 `kube-proxy` 组件：
- en: '[PRE6]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of the preceding command should look as follows. Note that your
    account ID and region will be different:'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述命令的输出应如下所示。请注意，您的账户 ID 和区域将不同：
- en: '[PRE7]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, upgrade the `kube-proxy` image to the supported version from *Figure 10.5*
    by using the output of the previous command:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过使用之前命令的输出，将 `kube-proxy` 镜像升级到支持的版本，该版本见 *图 10.5*：
- en: '[PRE8]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Run the command from *step 1* to confirm the version change. This time, the
    output of the preceding command should look as follows:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行*步骤 1*中的命令以确认版本更改。这时，前一个命令的输出应该如下所示：
- en: '[PRE9]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s learn how we can upgrade `coredns` when needed. Note that only an upgrade
    from 1.17 to 1.18 requires the `coredns` version to be at 1.7.0\. Confirm that
    your cluster uses `coredns` as the DNS provider by executing the following command:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们了解如何在需要时升级 `coredns`。请注意，只有从 1.17 升级到 1.18 时，`coredns` 版本需要为 1.7.0。通过执行以下命令确认您的集群使用
    `coredns` 作为 DNS 提供程序：
- en: '[PRE10]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output of the preceding command should look as follows:'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前一个命令的输出应该如下所示：
- en: '![Figure 10.6 – CoreDNS pods running on the Kubernetes cluster](img/B16192_10_006.jpg)'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 10.6 – 在 Kubernetes 集群中运行的 CoreDNS pod](img/B16192_10_006.jpg)'
- en: Figure 10.6 – CoreDNS pods running on the Kubernetes cluster
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.6 – 在 Kubernetes 集群中运行的 CoreDNS pod
- en: 'Get the current version of the `coredns` component by executing the following
    command:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令获取当前 `coredns` 组件的版本：
- en: '[PRE11]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output of the preceding command should look as follows. Note that your
    account ID and region will be different:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前一个命令的输出应该如下所示。请注意，您的账户 ID 和区域会有所不同：
- en: '[PRE12]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, upgrade the `coredns` image to the supported version from *Figure 10.5*
    by using the output of the previous command:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用前一个命令的输出，将 `coredns` 镜像升级到支持的版本（参考*图 10.5*）：
- en: '[PRE13]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Run the command from *step 1* to confirm the version change. This time, the
    output of the preceding command should look as follows:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行*步骤 1*中的命令以确认版本更改。这时，前一个命令的输出应该如下所示：
- en: '[PRE14]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, we have updated our Kubernetes components to the next version available.
    Let's move on to the next step and upgrade our worker nodes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已将 Kubernetes 组件更新到下一个可用版本。接下来，我们将继续升级工作节点。
- en: Upgrading Kubernetes worker nodes
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 升级 Kubernetes 工作节点
- en: After upgrading AWS EKS controllers, we will follow with adding new worker nodes
    using updated AMI images. We will drain the old nodes and help Kubernetes to migrate
    workloads to the newly created nodes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 升级 AWS EKS 控制器后，我们将继续添加使用更新的 AMI 镜像的新工作节点。我们将排空旧节点，并帮助 Kubernetes 将工作负载迁移到新创建的节点。
- en: 'Let''s upgrade our worker nodes:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们升级我们的工作节点：
- en: 'Edit the `config.tf` file under the `Chapter03/terraform/packtclusters` directory
    and change the name of the workers AMI ID increased version from `1.15` to `1.16`:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑 `Chapter03/terraform/packtclusters` 目录下的 `config.tf` 文件，并将工作节点的 AMI ID 的版本从
    `1.15` 更改为 `1.16`：
- en: '[PRE15]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Edit the `terraform.tfvars` file under the `Chapter03/terraform/packtclusters`
    directory and increase `workers_number_min` if you like:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑 `Chapter03/terraform/packtclusters` 目录下的 `terraform.tfvars` 文件，如果需要，增加 `workers_number_min`：
- en: '[PRE16]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Run the `terraform plan` command to validate the planned changes before applying
    them:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `terraform plan` 命令来验证计划中的更改，确认无误后再应用：
- en: '[PRE17]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Execute the `terraform apply` command. Enter `yes` when you get a prompt to
    approve the in-place update:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行 `terraform apply` 命令。当提示确认进行就地更新时，输入 `yes`：
- en: '[PRE18]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Execute the `kubectl get nodes` command to get the name of your old nodes. You
    will get the following output and as we can see, two out of three nodes in our
    cluster are still on v1.15.12:![Figure 10.9 – The kubectl output showing node
    names and version
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行 `kubectl get nodes` 命令以获取旧节点的名称。您将看到以下输出，正如我们所见，集群中三个节点中的两个仍在使用 v1.15.12：![图
    10.9 – 显示节点名称和版本的 kubectl 输出](img/B16192_10_009.jpg)
- en: '](img/B16192_10_009.jpg)'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16192_10_009.jpg)'
- en: Figure 10.9 – The kubectl output showing node names and version
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.9 – 显示节点名称和版本的 kubectl 输出
- en: 'Now that we''ve confirmed one new node is added to our cluster, we need to
    move our pods from the old nodes to the new nodes. First, one by one, taint the
    old nodes and drain them:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经确认新节点已添加到集群中，接下来需要将 pod 从旧节点迁移到新节点。首先，一个一个地对旧节点进行标记，并将其排空：
- en: '[PRE19]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, remove the old nodes from your cluster. New nodes will be automatically
    created and added to our cluster. Let''s confirm all nodes are upgraded by executing
    the `kubectl get nodes` command. The output of the command should look as follows:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从集群中移除旧节点。新的节点将自动创建并添加到集群中。让我们通过执行 `kubectl get nodes` 命令来确认所有节点已升级。该命令的输出应该如下所示：
- en: '![Figure 10.10 – The kubectl output showing updated node version](img/B16192_10_010.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.10 – 显示更新后的节点版本的 kubectl 输出](img/B16192_10_010.jpg)'
- en: Figure 10.10 – The kubectl output showing updated node version
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 – 显示更新后的节点版本的 kubectl 输出
- en: We have now learned how to upgrade the Kubernetes control plane and workers
    using Terraform. It is a production best practice to have a regular backup of
    persistent data and applications from our clusters. In the next section, we will
    focus on taking a backup of applications and preparing our clusters for disaster
    recovery.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经学会了如何使用Terraform升级Kubernetes控制平面和工作节点。定期备份我们集群中的持久数据和应用程序是一种生产最佳实践。在接下来的部分，我们将专注于备份应用程序，并为我们的集群做好灾难恢复的准备。
- en: Preparing for backups and disaster recovery
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为备份和灾难恢复做准备
- en: In this section, we will be taking a complete, instant, or scheduled backup
    of the applications running in our cluster. Not every application requires or
    can even take advantage of regular backups. Stateless application configuration
    is usually stored in a Git repository and can be easily deployed as part of the
    **Continuous Integration and Continuous Delivery** (**CI/CD**) pipelines when
    needed. Of course, this is not the case for stateful applications such as databases,
    user data, and content. Our business running online services can be challenged
    to meet legal requirements and industry-specific regulations and retain copies
    of data for a certain time.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将对运行在集群中的应用程序进行完整、即时或定期备份。并非每个应用程序都需要或能够利用常规备份。无状态应用程序的配置通常存储在Git仓库中，并且可以在需要时作为**持续集成和持续交付**（**CI/CD**）流水线的一部分轻松部署。当然，对于有状态应用程序，如数据库、用户数据和内容，情况并非如此。我们的在线业务服务可能面临法律要求和行业特定法规的挑战，需要保留一定时间的数据副本。
- en: For reasons external or internal to our clusters, we can lose applications or
    the whole cluster and may need to recover services as quickly as possible. In
    that case, for disaster recovery use cases, we will learn how to use our backup
    data stored in an S3 target location to restore services.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于集群外部或内部的原因，我们可能会丢失应用程序或整个集群，并且可能需要尽快恢复服务。在这种情况下，对于灾难恢复的使用场景，我们将学习如何使用存储在S3目标位置的备份数据来恢复服务。
- en: In this section, we will use the open source Velero project as our backup solution.
    We will learn how to install Velero to take a scheduled backup of our data and
    restore it.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用开源的Velero项目作为我们的备份解决方案。我们将学习如何安装Velero，定期备份我们的数据并恢复它。
- en: Installing Velero on Kubernetes
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Kubernetes上安装Velero
- en: Traditional backup solutions and similar services offered by cloud vendors focus
    on protecting node resources. In Kubernetes, an application running on nodes can
    dynamically move across nodes, therefore taking a backup of node resources does
    not fulfill the requirements of a container orchestration platform. Cloud-native
    applications require a granular, application-aware backup solution. This is exactly
    the kind of solution cloud-native backup solutions such as Velero focus on. Velero
    is an open source project to back up and restore Kubernetes resources and their
    persistent volumes. Velero can be used to perform migration operations and disaster
    recovery on Kubernetes resources. You can read more about Velero and its concepts
    on the official Velero documentation site at [https://velero.io/docs/main/](https://velero.io/docs/main/).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 传统备份解决方案以及云服务提供商提供的类似服务，主要关注保护节点资源。在Kubernetes中，运行在节点上的应用程序可以动态地在节点之间移动，因此备份节点资源无法满足容器编排平台的要求。云原生应用程序需要细粒度、了解应用程序的备份解决方案。这正是云原生备份解决方案（如Velero）关注的方向。Velero是一个开源项目，用于备份和恢复Kubernetes资源及其持久化卷。Velero可以用于在Kubernetes资源上执行迁移操作和灾难恢复。你可以在Velero官方文档网站[https://velero.io/docs/main/](https://velero.io/docs/main/)上阅读更多关于Velero及其概念的内容。
- en: Information
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: You can find the complete source code at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero)找到完整的源代码。
- en: 'Now, let''s install Velero using its latest version and prepare our cluster
    to start taking a backup of our resources:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用最新版本安装Velero，并准备我们的集群开始备份我们的资源：
- en: 'Let''s get the latest release version tag of `velero` and keep it in a variable
    called `VELEROVERSION`:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们获取`velero`的最新发布版本标签，并将其保存在名为`VELEROVERSION`的变量中：
- en: '[PRE20]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, download the latest `velero` release binary and install by executing the
    following command:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，下载最新的`velero`发布二进制文件，并通过执行以下命令进行安装：
- en: '[PRE21]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Confirm that the `velero` command can execute:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认 `velero` 命令可以执行：
- en: '[PRE22]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Create the credentials file for Velero to access your S3 target in this `chapter10/velero/credentials-velero`
    path. Replace `aws_access_key_id` and `aws_secret_access_key` with your AWS ID
    and access key and save the file:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此 `chapter10/velero/credentials-velero` 路径中创建 Velero 用于访问您的 S3 目标的凭证文件。用您的
    AWS ID 和访问密钥替换 `aws_access_key_id` 和 `aws_secret_access_key`，然后保存该文件：
- en: '[PRE23]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Before you run the following command, update `s3Url` with your AWS S3 bucket
    address or S3-compatible object storage, such as a MinIO object storage server
    address. Install the Velero server components by executing the following command:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行以下命令之前，请使用您的 AWS S3 存储桶地址或兼容 S3 的对象存储（如 MinIO 对象存储服务器地址）更新 `s3Url`。通过执行以下命令安装
    Velero 服务器组件：
- en: '[PRE24]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output of the preceding command should look as follows:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面命令的输出应如下所示：
- en: '![Figure 10.11 – Velero installer output showing successful installation'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 10.11 – Velero 安装程序输出显示安装成功](img/B16192_10_011.jpg)'
- en: '](img/B16192_10_011.jpg)'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16192_10_011.jpg)'
- en: Figure 10.11 – Velero installer output showing successful installation
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.11 – Velero 安装程序输出显示安装成功
- en: 'Confirm that the Velero server components are successfully installed by executing
    the following command:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令确认 Velero 服务器组件已成功安装：
- en: '[PRE25]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output of the preceding command should look as follows:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面命令的输出应如下所示：
- en: '![Figure 10.12 – Velero deployments status showing ready](img/B16192_10_012.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.12 – Velero 部署状态显示已准备就绪](img/B16192_10_012.jpg)'
- en: Figure 10.12 – Velero deployments status showing ready
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12 – Velero 部署状态显示已准备就绪
- en: Now we have Velero installed and configured to take a backup of resources to
    an S3 target. Next, we will learn how to take a bundled backup of Kubernetes resources.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装并配置了 Velero，以便将资源备份到 S3 目标。接下来，我们将学习如何对 Kubernetes 资源进行打包备份。
- en: Taking a backup of specific resources using Velero
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Velero 备份特定资源
- en: 'Let''s follow the steps here to get a backup of Kubernetes resources we would
    like to protect. For this example, we will need a stateful application. We will
    deploy a MinIO object storage workload, upload some files on it, and take a backup
    of all resources to demonstrate the backup and restoration capabilities. You can
    apply the same steps to any application you wish:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照此处的步骤获取我们想要保护的 Kubernetes 资源的备份。对于此示例，我们将需要一个有状态应用程序。我们将部署一个 MinIO 对象存储工作负载，上传一些文件，并对所有资源进行备份，以演示备份和恢复功能。您可以对任何想要的应用程序应用相同的步骤：
- en: Information
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: You can find the complete source code at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero/backup](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero/backup).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero/backup](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero/backup)
    查找完整的源代码。
- en: 'If you already have a stateful application with persistent volumes to protect,
    you can skip to *step 4*. Otherwise, execute the following command to deploy a
    MinIO instance to continue with the scenario:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您已经有一个带有持久卷的有状态应用程序需要保护，您可以跳过到 *步骤 4*。否则，执行以下命令部署一个 MinIO 实例以继续此场景：
- en: '[PRE26]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Verify that MinIO pods, service, and persistent volumes are created by executing
    the following command:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令验证 MinIO pod、服务和持久卷是否已创建：
- en: '[PRE27]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output of the preceding command should look as follows:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面命令的输出应如下所示：
- en: '![Figure 10.13 – Status of the MinIO pods, service, and persistent volume](img/B16192_10_013.jpg)'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 10.13 – MinIO pod、服务和持久卷的状态](img/B16192_10_013.jpg)'
- en: Figure 10.13 – Status of the MinIO pods, service, and persistent volume
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.13 – MinIO pod、服务和持久卷的状态
- en: 'Now, we will create a backup for all resources that have the label `app=minio`.
    Make sure to match the selector if you are using different labels. Execute the
    following command to create a backup:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将为所有具有 `app=minio` 标签的资源创建备份。如果您使用不同的标签，请确保匹配选择器。执行以下命令以创建备份：
- en: '[PRE28]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Run the following command to verify that the backup job is completed:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令以验证备份作业是否完成：
- en: '[PRE29]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As an alternative, we can back up resources in an entire namespace. Let''s
    make another backup, this time using a namespace, by executing the following command:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为替代方案，我们可以备份整个命名空间中的资源。让我们再做一次备份，这次使用命名空间，通过执行以下命令：
- en: '[PRE30]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Now, we have learned how to create a backup of our first resource group and
    namespace in Kubernetes. Let's simulate a disaster scenario and test recovering
    our application.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经学习了如何在 Kubernetes 中创建第一个资源组和命名空间的备份。接下来，让我们模拟一个灾难场景，并测试恢复我们的应用程序。
- en: Restoring an application resource from its backup using Velero
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Velero 从备份恢复应用程序资源
- en: 'Let''s follow these steps to completely remove resources in a namespace and
    restore the previous backup to recover them. You can apply the same steps on any
    application to migrate from one cluster to another. This method can also serve
    as a cluster upgrade strategy to reduce upgrade time:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下步骤完全删除命名空间中的资源，并恢复之前的备份以恢复它们。你可以对任何应用程序应用相同的步骤，以便从一个集群迁移到另一个集群。这种方法还可以作为集群升级策略，以减少升级时间：
- en: 'Delete all resources in a namespace of your application by executing the following
    command:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令删除应用程序命名空间中的所有资源：
- en: '[PRE31]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Create a new namespace and execute the following command to restore the application
    and its resources from its backup:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的命名空间，并执行以下命令，从备份中恢复应用程序及其资源：
- en: '[PRE32]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Wait for a couple of second for resources to be restored and verify that your
    MinIO pods, service, and persistent volume are restored by executing the following
    command:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待几秒钟让资源恢复，并通过执行以下命令验证你的 MinIO pods、服务和持久卷是否已恢复：
- en: '[PRE33]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output of the preceding command should look as follows:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的命令输出应如下所示：
- en: '![Figure 10.14 – Status of the MinIO pods, service, and persistent volume](img/B16192_10_014.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.14 – MinIO pods、服务和持久卷的状态](img/B16192_10_014.jpg)'
- en: Figure 10.14 – Status of the MinIO pods, service, and persistent volume
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.14 – MinIO pods、服务和持久卷的状态
- en: Now we have learned how to restore a resource group backup of the service in
    our production Kubernetes clusters. Let's take a look at how we can improve by
    continuously validating the quality of our clusters and troubleshooting issues.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经学习了如何恢复生产 Kubernetes 集群中服务的资源组备份。接下来，让我们看看如何通过持续验证集群质量和排查问题来进行改进。
- en: Validating cluster quality
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证集群质量
- en: 'In this section, we will learn about some of the best practices and tools in
    the ecosystem to improve different aspects of our Kubernetes clusters. Continuous
    improvement is a wide-ranging concept that encompasses everything from providing
    a smooth platform to services on Kubernetes and setting a particular **Quality
    of Service** (**QoS**) for resources, to making sure resources are evenly distributed
    and unused resources are released to reduce the pressure on cluster resources
    and the overall cost of providing services. The definition of improvement itself
    is gradually getting more granular, and it is not limited to the practices that
    we will discuss here. Before we learn about the conformance and cost management
    tools, let''s learn about a few common-sense quality best practices we should
    consider:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习生态系统中一些最佳实践和工具，以提高 Kubernetes 集群的各个方面的质量。持续改进是一个广泛的概念，涵盖了从提供流畅平台到在
    Kubernetes 上提供服务、为资源设置特定的**服务质量**（**QoS**），以及确保资源均匀分配和释放未使用资源，以减少集群资源的压力和提供服务的整体成本。改进的定义本身正逐渐变得更加细化，它并不限于我们将在这里讨论的实践。在了解一致性和成本管理工具之前，先让我们了解一些常识性的质量最佳实践，供我们参考：
- en: '**Generate state-of-cluster reports**: Although it is expected that Kubernetes
    clusters should behave the same whether it''s a managed Kubernetes service provided
    by a public cloud provider, a distribution provided by a specific vendor, or a
    self-managed cluster based on upstream Kubernetes, the reality is there may be
    limitations and configuration differences that we should validate. Conformance
    testing is a great way to ensure that the clusters we support are properly configured
    and conform to official Kubernetes specifications.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成集群状态报告**：尽管预计无论是由公有云提供商提供的托管 Kubernetes 服务、特定供应商提供的分发版本，还是基于上游 Kubernetes
    的自管集群，Kubernetes 集群应表现一致，但实际上，可能会有一些限制和配置差异，我们应该进行验证。合规性测试是确保我们支持的集群被正确配置并符合官方
    Kubernetes 规范的一个很好的方式。'
- en: '`Guaranteed`, `Burstable`, or `BestEffort`.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Guaranteed`、`Burstable` 或 `BestEffort`。'
- en: '**Reduce latency to closer to users'' location**: There is a reason why cloud
    providers offer clusters in different geographic locations. It is common to start
    locally and observe end user latencies and traffic before spinning clusters in
    different geographies. Observe issues and bottlenecks, and expand to additional
    regions closer to users when needed.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少延迟，接近用户的位置**：云服务提供商在不同地理位置提供集群是有原因的。通常，我们会从本地开始，观察最终用户的延迟和流量情况，然后再在不同的地理位置启动集群。观察问题和瓶颈，并在需要时扩展到更接近用户的其他区域。'
- en: '**Define storage classes with different QoSes**: In a Kubernetes cluster, the
    CPU, memory, and also to a degree, the network, QoS can be managed by Kubernetes.
    Storage QoS is expected to be handled by storage providers. Storage can be provided
    by the external storage of the cluster or hyper-converged **Container Attached
    Storage** (**CAS**) outside. A best practice is to abstract data management from
    specific storage vendors to provide vendor-agnostic service flexibility with storage
    classes. Different storage classes can be used to provide cold storage, SSD, or
    NVMe-backed storage depending on the application''s needs. We learned about tuning
    Kubernetes storage and choosing the storage solution in [*Chapter 7*](B16192_07_Final_PG_ePub.xhtml#_idTextAnchor157),
    *Managing Storage and Stateful Applications*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义具有不同 QoS 的存储类**：在 Kubernetes 集群中，CPU、内存以及一定程度上的网络 QoS 可以由 Kubernetes 管理。存储
    QoS 预计由存储提供商处理。存储可以来自集群的外部存储或外部的超融合 **容器附加存储** (**CAS**) 。最佳实践是将数据管理从特定存储供应商中抽象出来，以便使用存储类提供供应商无关的服务灵活性。根据应用的需求，可以使用不同的存储类来提供冷存储、SSD
    或 NVMe 支持的存储。我们在[*第7章*](B16192_07_Final_PG_ePub.xhtml#_idTextAnchor157)《管理存储和有状态应用程序》中了解了如何调整
    Kubernetes 存储并选择存储解决方案。'
- en: '**Optimize container images**: It is recommended to continuously monitor your
    cluster resources'' top consumers, improve their consumption, and look for ways
    to optimize their consumption. Optimizing container images can have a significant
    impact on resource utilization and performance. You can read more about the challenges
    and best practices of improving container images in [*Chapter 8*](B16192_08_Final_PG_ePub.xhtml#_idTextAnchor177),
    *Deploying Seamless and Reliable Applications*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化容器镜像**：建议持续监控集群资源的主要消耗者，改善其消耗情况，并寻找优化消耗的方式。优化容器镜像对资源利用率和性能有显著影响。你可以在[*第8章*](B16192_08_Final_PG_ePub.xhtml#_idTextAnchor177)《部署无缝且可靠的应用程序》中阅读有关改善容器镜像的挑战和最佳实践。'
- en: '**Optimize cluster resource spend**: In theory, the only limit on the cloud
    provider''s resources is your budget. It is recommended to monitor the cost of
    resources and project allocation to get the full cost of running a product.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化集群资源支出**：理论上，云服务提供商资源的唯一限制是你的预算。建议监控资源和项目分配的成本，以获取运行产品的全部成本。'
- en: Now we have learned the best practices for improving the quality of our cluster;
    we have touched on some of the topics in previous chapters. Let's look into the
    remaining areas that we haven't covered yet, including how we can validate cluster
    resources in a non-destructive manner and monitoring the cost of resources.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了改善集群质量的最佳实践，并且已经涉及了前几章的一些内容。接下来，让我们深入了解尚未覆盖的其他领域，包括如何以非破坏性的方式验证集群资源，以及如何监控资源成本。
- en: Generating compliance reports
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成合规报告
- en: 'There are many ways and tools to get a Kubernetes cluster up and running. It
    is an administrative challenge to maintain a proper configuration. Fortunately,
    there are tools to validate reports and detect configuration problems. Sonobuoy
    is one of the popular open source tools available to run Kubernetes conformance
    tests and validate our cluster''s health. Sonobuoy is cluster-agnostic and can
    generate reports of our cluster''s characteristics. These reports are used to
    ensure the best practices applied by eliminating distribution-specific issues
    and conforming clusters can be ported into our clusters. You can read more about
    custom data collection capabilities using plugins and integrated **end-to-end**
    (**e2e**) testing at Sonobuoy''s official documentation site, https://sonobuoy.io/docs/v0.20.0/.
    Now, let''s install the latest version of Sonobuoy and validate our cluster by
    running a Kubernetes conformance test:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法和工具可以让 Kubernetes 集群顺利运行。维护一个合适的配置是一个管理挑战。幸运的是，有一些工具可以验证报告并检测配置问题。Sonobuoy
    是一个流行的开源工具，能够运行 Kubernetes 合规性测试并验证我们的集群健康状态。Sonobuoy 是集群无关的，可以生成我们集群特征的报告。这些报告用于确保最佳实践的应用，通过消除特定于发行版的问题，使得符合规范的集群能够迁移到我们的集群中。你可以在
    Sonobuoy 的官方网站文档中了解更多关于使用插件和集成 **端到端**（**e2e**）测试的自定义数据收集功能，网址：[https://sonobuoy.io/docs/v0.20.0/](https://sonobuoy.io/docs/v0.20.0/)。现在，让我们安装
    Sonobuoy 的最新版本并通过运行 Kubernetes 合规性测试来验证集群：
- en: 'Let''s get the latest release version tag of Sonobuoy and keep it in a variable
    called `SONOBUOYVERSION`:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取 Sonobuoy 的最新发布版本标签，并将其保存在一个名为`SONOBUOYVERSION`的变量中：
- en: '[PRE34]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, download the latest `sonobuoy` release binary and install by executing
    the following command ([https://github.com/vmware-tanzu/sonobuoy/releases/download/v0.20.0/sonobuoy_0.20.0_linux_amd64.tar.gz](https://github.com/vmware-tanzu/sonobuoy/releases/download/v0.20.0/sonobuoy_0.20.0_linux_amd64.tar.gz)):'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，下载最新的 `sonobuoy` 发布二进制文件，并通过执行以下命令进行安装（[https://github.com/vmware-tanzu/sonobuoy/releases/download/v0.20.0/sonobuoy_0.20.0_linux_amd64.tar.gz](https://github.com/vmware-tanzu/sonobuoy/releases/download/v0.20.0/sonobuoy_0.20.0_linux_amd64.tar.gz)）：
- en: '[PRE35]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Confirm that Sonobuoy is installed, and the command can execute:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认 Sonobuoy 已安装，并且可以执行命令：
- en: '[PRE36]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Make sure that the cluster has enough resources to execute all the tests. You
    can find a specific suggestion for every provider on Sonobuoy''s source repository
    at [https://github.com/cncf/k8s-conformance/tree/master/v1.16](https://github.com/cncf/k8s-conformance/tree/master/v1.16).
    For EKS, the suggested cluster size is 10 `c5.xlarge` worker instances. Start
    the conformance tests on your EKS cluster by executing the following command:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保集群有足够的资源来执行所有测试。你可以在 Sonobuoy 的源代码库中找到每个提供商的具体建议，网址：[https://github.com/cncf/k8s-conformance/tree/master/v1.16](https://github.com/cncf/k8s-conformance/tree/master/v1.16)。对于
    EKS，建议的集群规模是 10 个 `c5.xlarge` 工作节点实例。在 EKS 集群上执行以下命令以启动合规性测试：
- en: '[PRE37]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To shorten testing and validate the configuration rather than full certified
    conformance, we can run the test with the `--mode quick` option:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了缩短测试时间并验证配置，而不是进行完整的认证合规性测试，我们可以使用 `--mode quick` 选项运行测试：
- en: '[PRE38]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Validation will take up to an hour to complete depending on the tests executed
    on the cluster. Once finished, execute the following command to get the plugins''
    results and inspect the results for failures. For a detailed list of options to
    inspect results, see the documentation at [https://sonobuoy.io/docs/v0.20.0/results/](https://sonobuoy.io/docs/v0.20.0/results/):'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证过程可能需要最多一小时的时间，具体取决于执行的测试。完成后，执行以下命令获取插件的结果并检查失败的结果。有关检查结果的详细选项列表，请参阅文档：[https://sonobuoy.io/docs/v0.20.0/results/](https://sonobuoy.io/docs/v0.20.0/results/)：
- en: '[PRE39]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output of the preceding command should look as follows:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述命令的输出应如下所示：
- en: '![Figure 10.15 – Sonobuoy validation results](img/B16192_10_015.jpg)'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 10.15 – Sonobuoy 验证结果](img/B16192_10_015.jpg)'
- en: Figure 10.15 – Sonobuoy validation results
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.15 – Sonobuoy 验证结果
- en: 'Delete the Sonobuoy components from the cluster and clean up the resources:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除集群中的 Sonobuoy 组件并清理资源：
- en: '[PRE40]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now we have learned how to validate our Kubernetes cluster configuration. Let's
    look into how we can detect overprovisioned, idle resources and optimize our cluster's
    total cost.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何验证 Kubernetes 集群配置。接下来，让我们看看如何检测过度配置的空闲资源，并优化集群的总成本。
- en: Managing and improving the cost of cluster resources
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理和优化集群资源成本
- en: 'Monitoring project cost and team chargeback and managing total cluster spending
    are some of the big challenges of managing Kubernetes on public cloud providers.
    Since we have a theoretically unlimited scale available through cloud vendors,
    utilization fees can quickly go up and become a problem if not managed. Kubecost
    helps you monitor and continuously improve the cost of Kubernetes clusters. You
    can read more about the cost and capacity management capabilities of Kubecost
    at Kubecost''s official documentation site: https://docs.kubecost.com/.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 监控项目成本和团队费用分摊，以及管理集群总支出，是在公共云提供商上管理 Kubernetes 的一些重大挑战。由于我们可以通过云供应商获得理论上无限的扩展能力，如果不加以管理，利用费用很容易迅速上涨并成为问题。Kubecost
    帮助您监控并持续改进 Kubernetes 集群的成本。您可以在 Kubecost 的官方文档网站上了解更多关于成本和容量管理的功能：[https://docs.kubecost.com/](https://docs.kubecost.com/)。
- en: 'Now, let''s install Kubecost using Helm and start analyzing cost allocation
    in our cluster:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 Helm 安装 Kubecost，并开始分析集群中的成本分配：
- en: 'Create a namespace called `kubecost`:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `kubecost` 的命名空间：
- en: '[PRE41]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Add the `cost-analyzer` Helm Chart repository to your local repository list:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `cost-analyzer` Helm Chart 仓库添加到本地仓库列表：
- en: '[PRE42]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Update the Helm Chart repositories:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 Helm Chart 仓库：
- en: '[PRE43]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Install `cost-analyzer` from its Helm repository:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从其 Helm 仓库安装 `cost-analyzer`：
- en: '[PRE44]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Verify successful installation by executing the following command:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令以验证安装是否成功：
- en: '[PRE45]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now we have `cost-analyzer` installed. Let''s access the Kubecost dashboard.
    Create port forwarding to access the Kubecost interface locally:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经安装了 `cost-analyzer`。让我们访问 Kubecost 仪表盘。创建端口转发以便在本地访问 Kubecost 界面：
- en: '[PRE46]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Open a browser window and visit `http://localhost:9090`, which is forwarded
    to the `kubecost-cost-analyzer` service running in the cluster. The dashboard
    will immediately show the running monthly cost of your cluster, similar to the
    following:![Figure 10.17 – Kubecost Available Clusters screen
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开浏览器窗口并访问 `http://localhost:9090`，该地址会转发到运行在集群中的 `kubecost-cost-analyzer` 服务。仪表盘将立即显示集群的月度运行成本，类似于以下内容：![图
    10.17 – Kubecost 可用集群屏幕
- en: '](img/B16192_10_017.jpg)'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16192_10_017.jpg)'
- en: Figure 10.17 – Kubecost Available Clusters screen
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.17 – Kubecost 可用集群屏幕
- en: Click on your cluster from the list and access the Kubecost dashboard. The top
    part of the dashboard will show a summary of the total cost and any potential
    identified savings, similar to that in the following screenshot:![Figure 10.18
    – Kubecost dashboard
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从列表中点击您的集群，进入 Kubecost 仪表盘。仪表盘的顶部将显示总成本摘要和任何可能识别出的节省，类似于以下截图：![图 10.18 – Kubecost
    仪表盘
- en: '](img/B16192_10_018.jpg)'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16192_10_018.jpg)'
- en: Figure 10.18 – Kubecost dashboard
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.18 – Kubecost 仪表盘
- en: Let's scroll down the dashboard screen to find a summary of the controller component
    and service allocation. At the bottom of the dashboard, we will see the health
    scores. A health score is an assessment of infrastructure reliability and performance
    risks:![Figure 10.19 – Kubecost dashboard showing the cluster health assessment
    score](img/B16192_10_019.jpg)
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们向下滚动仪表盘屏幕，找到控制器组件和服务分配的摘要。在仪表盘的底部，我们将看到健康评分。健康评分是对基础设施可靠性和性能风险的评估：![图 10.19
    – Kubecost 仪表盘显示集群健康评估评分](img/B16192_10_019.jpg)
- en: Figure 10.19 – Kubecost dashboard showing the cluster health assessment score
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.19 – Kubecost 仪表盘显示集群健康评估评分
- en: The most important quick summary pages on the dashboard are the health assessment
    and estimated saving detail pages. Let's click on each to get to the areas where
    you can improve your cluster's cost and performance. In the following screenshot,
    we can see an example of a significant saving suggestion from Kubecost after analyzing
    our cluster:![Figure 10.20 – Kubecost estimated savings dashboard](img/B16192_10_020.jpg)
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仪表盘上最重要的快速摘要页面是健康评估和估算节省详情页面。让我们点击每个页面，查看您可以改进集群成本和性能的区域。在以下截图中，我们可以看到 Kubecost
    在分析集群后提供的一个显著节省建议：![图 10.20 – Kubecost 估算节省仪表盘](img/B16192_10_020.jpg)
- en: Figure 10.20 – Kubecost estimated savings dashboard
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.20 – Kubecost 估算节省仪表盘
- en: Click on the arrow button next to one of the saving categories and review the
    recommendations to optimize your cluster cost.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击节省类别旁边的箭头按钮，查看优化集群成本的建议。
- en: Now we have learned how to identify monthly cluster costs, resource efficiency,
    cost allocation, and potential savings by optimizing request sizes, cleaning up
    abandoned workloads, and using many other ways to manage underutilized nodes in
    our cluster.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何通过优化请求大小、清理废弃工作负载以及采用多种方法管理集群中的未充分利用节点，来识别每月集群成本、资源效率、成本分配和潜在节省。
- en: Summary
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored Kubernetes operation best practices and covered
    cluster maintenance topics such as upgrades, backups, and disaster recovery. We
    learned how to validate our cluster configuration to avoid cluster and application
    problems. Finally, we learned ways to detect and improve resource allocation and
    the cost of our cluster resources.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们探讨了 Kubernetes 操作的最佳实践，并涵盖了集群维护主题，如升级、备份和灾难恢复。我们学习了如何验证我们的集群配置，以避免集群和应用程序问题。最后，我们了解了如何检测和改善资源分配以及集群资源的成本。
- en: By completing this last chapter in the book, we now have the complete knowledge
    to build and manage production-grade Kubernetes infrastructure following the industry
    best practices and well-proven techniques learned from early technology adopters
    and real-life, large-scale Kubernetes deployments. Kubernetes offers a very active
    user and partner ecosystem. In this book, we focused on the best practices known
    today. Although principles will not change quickly, as with every new technology,
    there will be new solutions and new approaches to solving the same problems. Please
    let us know how we can improve this book in the future by reaching out to us via
    the methods mentioned in the *Preface* section.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完成本书的最后一章，我们现在具备了建立和管理符合生产级别的 Kubernetes 基础设施的完整知识，这些基础设施遵循行业最佳实践，并借鉴了早期技术采用者和现实中大规模
    Kubernetes 部署的成熟技术。Kubernetes 提供了一个非常活跃的用户和合作伙伴生态系统。在本书中，我们重点介绍了目前已知的最佳实践。尽管原则不会迅速改变，但与每一项新技术一样，始终会有新的解决方案和新方法来解决相同的问题。如果您有任何建议，请通过*前言*部分提到的方式联系我们，帮助我们改进本书。
- en: Further reading
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can refer to the following links for more information on the topics covered
    in this chapter:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下链接，获取更多关于本章所涉及主题的信息：
- en: '*Amazon EKS Kubernetes release calendar*: https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-release-calendar'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Amazon EKS Kubernetes 发布日历*: https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-release-calendar'
- en: 'Disaster recovery for multi-region Kafka at Uber: [https://eng.uber.com/kafka/](https://eng.uber.com/kafka/)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Uber 的多区域 Kafka 灾难恢复: [https://eng.uber.com/kafka/](https://eng.uber.com/kafka/)'
- en: '*Disaster Recovery Preparedness for Your Kubernetes Clusters*: [https://rancher.com/blog/2020/disaster-recovery-preparedness-kubernetes-clusters](https://rancher.com/blog/2020/disaster-recovery-preparedness-kubernetes-clusters)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为您的 Kubernetes 集群做好灾难恢复准备*: [https://rancher.com/blog/2020/disaster-recovery-preparedness-kubernetes-clusters](https://rancher.com/blog/2020/disaster-recovery-preparedness-kubernetes-clusters)'
- en: 'The official website of the Velero project: [https://velero.io/](https://velero.io/)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Velero 项目的官方网站: [https://velero.io/](https://velero.io/)'
- en: 'The official website of the Sonobuoy project: [https://sonobuoy.io/](https://sonobuoy.io/)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sonobuoy 项目的官方网站: [https://sonobuoy.io/](https://sonobuoy.io/)'
- en: 'KubeDR, an alternative open source Kubernetes cluster backup solution: [https://github.com/catalogicsoftware/kubedr](https://github.com/catalogicsoftware/kubedr)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'KubeDR，一个替代的开源 Kubernetes 集群备份解决方案: [https://github.com/catalogicsoftware/kubedr](https://github.com/catalogicsoftware/kubedr)'
- en: 'Kasten, an alternative Kubernetes backup, disaster recovery, and mobility solution:
    [https://www.kasten.io/](https://www.kasten.io/%20)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kasten，一个替代的 Kubernetes 备份、灾难恢复和移动性解决方案: [https://www.kasten.io/](https://www.kasten.io/%20)'
