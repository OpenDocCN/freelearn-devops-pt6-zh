- en: '*Chapter 10*: Operating and Maintaining Efficient Kubernetes Clusters'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we learned about production best practices for automating
    Kubernetes and its infrastructure components. We discussed challenges with provisioning
    stateless workloads in our clusters, including getting persistent storage up and
    running, choosing container images, and deployment strategies. We also learned
    about important observability tools in the ecosystem and building monitoring and
    logging stacks in our cluster to provide a solid base for our troubleshooting
    needs. Once we have a production-ready cluster and have started to serve workloads,
    it is vital to have efficient operations to oversee the cluster maintenance, availability,
    and other **service-level objectives** (**SLOs**).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on Kubernetes operation best practices and cover
    topics related to cluster maintenance, such as upgrades and rotation, backups,
    disaster recovery and avoidance, cluster and troubleshooting failures of the cluster
    control plane, workers, and applications. Finally, we will learn about the solutions
    available to validate and improve our cluster's quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning about cluster maintenance and upgrades
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing for backups and disaster recovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating cluster quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should have the following tools installed from previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS CLI v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS IAM authenticator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helm 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metrics-server`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MinIO instance (optional as an S3 target for backups)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to have an up-and-running Kubernetes cluster as per the instructions
    in [*Chapter 3*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073), *Provisioning
    Kubernetes Clusters Using AWS and Terraform*.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter is located at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/3aAdPzl](https://bit.ly/3aAdPzl)'
  prefs: []
  type: TYPE_NORMAL
- en: Learning about cluster maintenance and upgrades
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about upgrading our Kubernetes clusters in production.
    Generally, a new major Kubernetes version is announced quarterly, and every minor
    version is supported around 12 months after its initial release date. Following
    the rule of thumb for software upgrades, it is not common to upgrade to a new
    version immediately after its release unless it is a severe time-sensitive security
    patch. Cloud providers also follow the same practice and run their conformance
    tests before releasing a new image to the public. Therefore, cloud providers'
    Kubernetes releases usually follow a couple of versions behind the upstream release
    of Kubernetes. If you'd like to read about the latest releases, you can find the
    Kubernetes release notes on the official Kubernetes documentation site at https://kubernetes.io/docs/setup/release/notes/.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073), *Provisioning
    Kubernetes Clusters Using AWS and Terraform*, we learned about cluster deployment
    and rollout strategies. We also learned that cluster deployment is not a one-time
    task. It is a continuous process that affects the cluster's quality, stability,
    and operations, as well as the products and services on top of it. In previous
    chapters, we established a solid infrastructure deployment strategy, and now we
    will follow it with production-grade upgrade best practices in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073), *Provisioning
    Kubernetes Clusters Using AWS and Terraform*, we automated our cluster deployment
    using Terraform. Let's use the same cluster and upgrade it to a newer Kubernetes
    release.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading kubectl
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we will upgrade `kubectl` to the latest version. Your `kubectl` version
    should be at least equal to or greater than the Kubernetes version you are planning
    to upgrade to:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the latest `kubectl` binary and copy it to the `bin` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Confirm that the `kubectl` binary is updated to the newer version by executing
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, check your node status and version by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding command should look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.1 – The kubectl command showing the node status and its version](img/B16192_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – The kubectl command showing the node status and its version
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have updated `kubectl` to the latest version. Let's move on to the
    next step and upgrade our cluster version.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading the Kubernetes control plane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS EKS clusters can be upgraded one version at a time. This means that if we
    are on version 1.15, we can upgrade to 1.16, then to 1.17, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You can find the complete source code at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/terraform](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/terraform).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s upgrade our controller nodes using the Terraform scripts we also used
    in [*Chapter 3*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073), *Provisioning
    Kubernetes Clusters Using AWS and Terraform*, to deploy our clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit the `terraform.tfvars` file under the `Chapter10/terraform/packtclusters`
    directory and increase the `cluster_version` value to the next release version
    number. In our example, we have increased the version from `1.15` to `1.16`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the `terraform plan` command to validate the planned changes before applying
    them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Execute the `terraform apply` command. Enter `yes` when you get a prompt to
    approve the in-place update:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'While an upgrade is in progress, we can track the progress both from the command
    line or in the AWS console. The cluster status in the AWS console will look similar
    to the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.3 – AWS console output showing the cluster status as Updating'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16192_10_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – AWS console output showing the cluster status as Updating
  prefs: []
  type: TYPE_NORMAL
- en: 'You will get the following result after the `terraform apply` command completes
    successfully. By then, Terraform has successfully changed one AWS resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – The terraform apply command output](img/B16192_10_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – The terraform apply command output
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have updated our Kubernetes control plane to the next version available.
    Let's move on to the next step and upgrade our node groups.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading Kubernetes components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Upgrading the Kubernetes control plane doesn''t upgrade the worker nodes or
    our Kubernetes add-ons, such as `kube-proxy`, CoreDNS, and the Amazon VPC CNI
    plugin. Therefore, after upgrading the control plane, we need to carefully upgrade
    each and every component to a supported version if needed. You can read more about
    the supported component versions and Kubernetes upgrade prerequisites on the Amazon
    EKS documentation site at [https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html](https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html).
    The following figure shows an example support matrix table for the upgrade path
    we will follow in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – An example of a Kubernetes component support matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16192_10_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – An example of a Kubernetes component support matrix
  prefs: []
  type: TYPE_NORMAL
- en: Some version upgrades may also require changes in your application's YAML manifest
    to reference the new APIs. It is highly recommended to test your application behavior
    using a continuous integration workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our EKS control plane is upgraded, let''s upgrade `kube-proxy`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the current version of the `kube-proxy` component by executing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding command should look as follows. Note that your
    account ID and region will be different:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, upgrade the `kube-proxy` image to the supported version from *Figure 10.5*
    by using the output of the previous command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the command from *step 1* to confirm the version change. This time, the
    output of the preceding command should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s learn how we can upgrade `coredns` when needed. Note that only an upgrade
    from 1.17 to 1.18 requires the `coredns` version to be at 1.7.0\. Confirm that
    your cluster uses `coredns` as the DNS provider by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding command should look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.6 – CoreDNS pods running on the Kubernetes cluster](img/B16192_10_006.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 10.6 – CoreDNS pods running on the Kubernetes cluster
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Get the current version of the `coredns` component by executing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding command should look as follows. Note that your
    account ID and region will be different:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, upgrade the `coredns` image to the supported version from *Figure 10.5*
    by using the output of the previous command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the command from *step 1* to confirm the version change. This time, the
    output of the preceding command should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we have updated our Kubernetes components to the next version available.
    Let's move on to the next step and upgrade our worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading Kubernetes worker nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After upgrading AWS EKS controllers, we will follow with adding new worker nodes
    using updated AMI images. We will drain the old nodes and help Kubernetes to migrate
    workloads to the newly created nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s upgrade our worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit the `config.tf` file under the `Chapter03/terraform/packtclusters` directory
    and change the name of the workers AMI ID increased version from `1.15` to `1.16`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Edit the `terraform.tfvars` file under the `Chapter03/terraform/packtclusters`
    directory and increase `workers_number_min` if you like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the `terraform plan` command to validate the planned changes before applying
    them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Execute the `terraform apply` command. Enter `yes` when you get a prompt to
    approve the in-place update:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Execute the `kubectl get nodes` command to get the name of your old nodes. You
    will get the following output and as we can see, two out of three nodes in our
    cluster are still on v1.15.12:![Figure 10.9 – The kubectl output showing node
    names and version
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16192_10_009.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.9 – The kubectl output showing node names and version
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we''ve confirmed one new node is added to our cluster, we need to
    move our pods from the old nodes to the new nodes. First, one by one, taint the
    old nodes and drain them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, remove the old nodes from your cluster. New nodes will be automatically
    created and added to our cluster. Let''s confirm all nodes are upgraded by executing
    the `kubectl get nodes` command. The output of the command should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.10 – The kubectl output showing updated node version](img/B16192_10_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – The kubectl output showing updated node version
  prefs: []
  type: TYPE_NORMAL
- en: We have now learned how to upgrade the Kubernetes control plane and workers
    using Terraform. It is a production best practice to have a regular backup of
    persistent data and applications from our clusters. In the next section, we will
    focus on taking a backup of applications and preparing our clusters for disaster
    recovery.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing for backups and disaster recovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be taking a complete, instant, or scheduled backup
    of the applications running in our cluster. Not every application requires or
    can even take advantage of regular backups. Stateless application configuration
    is usually stored in a Git repository and can be easily deployed as part of the
    **Continuous Integration and Continuous Delivery** (**CI/CD**) pipelines when
    needed. Of course, this is not the case for stateful applications such as databases,
    user data, and content. Our business running online services can be challenged
    to meet legal requirements and industry-specific regulations and retain copies
    of data for a certain time.
  prefs: []
  type: TYPE_NORMAL
- en: For reasons external or internal to our clusters, we can lose applications or
    the whole cluster and may need to recover services as quickly as possible. In
    that case, for disaster recovery use cases, we will learn how to use our backup
    data stored in an S3 target location to restore services.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will use the open source Velero project as our backup solution.
    We will learn how to install Velero to take a scheduled backup of our data and
    restore it.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Velero on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional backup solutions and similar services offered by cloud vendors focus
    on protecting node resources. In Kubernetes, an application running on nodes can
    dynamically move across nodes, therefore taking a backup of node resources does
    not fulfill the requirements of a container orchestration platform. Cloud-native
    applications require a granular, application-aware backup solution. This is exactly
    the kind of solution cloud-native backup solutions such as Velero focus on. Velero
    is an open source project to back up and restore Kubernetes resources and their
    persistent volumes. Velero can be used to perform migration operations and disaster
    recovery on Kubernetes resources. You can read more about Velero and its concepts
    on the official Velero documentation site at [https://velero.io/docs/main/](https://velero.io/docs/main/).
  prefs: []
  type: TYPE_NORMAL
- en: Information
  prefs: []
  type: TYPE_NORMAL
- en: You can find the complete source code at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s install Velero using its latest version and prepare our cluster
    to start taking a backup of our resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get the latest release version tag of `velero` and keep it in a variable
    called `VELEROVERSION`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, download the latest `velero` release binary and install by executing the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Confirm that the `velero` command can execute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the credentials file for Velero to access your S3 target in this `chapter10/velero/credentials-velero`
    path. Replace `aws_access_key_id` and `aws_secret_access_key` with your AWS ID
    and access key and save the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before you run the following command, update `s3Url` with your AWS S3 bucket
    address or S3-compatible object storage, such as a MinIO object storage server
    address. Install the Velero server components by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding command should look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.11 – Velero installer output showing successful installation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16192_10_011.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.11 – Velero installer output showing successful installation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Confirm that the Velero server components are successfully installed by executing
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding command should look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.12 – Velero deployments status showing ready](img/B16192_10_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 – Velero deployments status showing ready
  prefs: []
  type: TYPE_NORMAL
- en: Now we have Velero installed and configured to take a backup of resources to
    an S3 target. Next, we will learn how to take a bundled backup of Kubernetes resources.
  prefs: []
  type: TYPE_NORMAL
- en: Taking a backup of specific resources using Velero
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s follow the steps here to get a backup of Kubernetes resources we would
    like to protect. For this example, we will need a stateful application. We will
    deploy a MinIO object storage workload, upload some files on it, and take a backup
    of all resources to demonstrate the backup and restoration capabilities. You can
    apply the same steps to any application you wish:'
  prefs: []
  type: TYPE_NORMAL
- en: Information
  prefs: []
  type: TYPE_NORMAL
- en: You can find the complete source code at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero/backup](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero/backup).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you already have a stateful application with persistent volumes to protect,
    you can skip to *step 4*. Otherwise, execute the following command to deploy a
    MinIO instance to continue with the scenario:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that MinIO pods, service, and persistent volumes are created by executing
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding command should look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.13 – Status of the MinIO pods, service, and persistent volume](img/B16192_10_013.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 10.13 – Status of the MinIO pods, service, and persistent volume
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we will create a backup for all resources that have the label `app=minio`.
    Make sure to match the selector if you are using different labels. Execute the
    following command to create a backup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following command to verify that the backup job is completed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As an alternative, we can back up resources in an entire namespace. Let''s
    make another backup, this time using a namespace, by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we have learned how to create a backup of our first resource group and
    namespace in Kubernetes. Let's simulate a disaster scenario and test recovering
    our application.
  prefs: []
  type: TYPE_NORMAL
- en: Restoring an application resource from its backup using Velero
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s follow these steps to completely remove resources in a namespace and
    restore the previous backup to recover them. You can apply the same steps on any
    application to migrate from one cluster to another. This method can also serve
    as a cluster upgrade strategy to reduce upgrade time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Delete all resources in a namespace of your application by executing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new namespace and execute the following command to restore the application
    and its resources from its backup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Wait for a couple of second for resources to be restored and verify that your
    MinIO pods, service, and persistent volume are restored by executing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding command should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Status of the MinIO pods, service, and persistent volume](img/B16192_10_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 – Status of the MinIO pods, service, and persistent volume
  prefs: []
  type: TYPE_NORMAL
- en: Now we have learned how to restore a resource group backup of the service in
    our production Kubernetes clusters. Let's take a look at how we can improve by
    continuously validating the quality of our clusters and troubleshooting issues.
  prefs: []
  type: TYPE_NORMAL
- en: Validating cluster quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn about some of the best practices and tools in
    the ecosystem to improve different aspects of our Kubernetes clusters. Continuous
    improvement is a wide-ranging concept that encompasses everything from providing
    a smooth platform to services on Kubernetes and setting a particular **Quality
    of Service** (**QoS**) for resources, to making sure resources are evenly distributed
    and unused resources are released to reduce the pressure on cluster resources
    and the overall cost of providing services. The definition of improvement itself
    is gradually getting more granular, and it is not limited to the practices that
    we will discuss here. Before we learn about the conformance and cost management
    tools, let''s learn about a few common-sense quality best practices we should
    consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generate state-of-cluster reports**: Although it is expected that Kubernetes
    clusters should behave the same whether it''s a managed Kubernetes service provided
    by a public cloud provider, a distribution provided by a specific vendor, or a
    self-managed cluster based on upstream Kubernetes, the reality is there may be
    limitations and configuration differences that we should validate. Conformance
    testing is a great way to ensure that the clusters we support are properly configured
    and conform to official Kubernetes specifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Guaranteed`, `Burstable`, or `BestEffort`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduce latency to closer to users'' location**: There is a reason why cloud
    providers offer clusters in different geographic locations. It is common to start
    locally and observe end user latencies and traffic before spinning clusters in
    different geographies. Observe issues and bottlenecks, and expand to additional
    regions closer to users when needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define storage classes with different QoSes**: In a Kubernetes cluster, the
    CPU, memory, and also to a degree, the network, QoS can be managed by Kubernetes.
    Storage QoS is expected to be handled by storage providers. Storage can be provided
    by the external storage of the cluster or hyper-converged **Container Attached
    Storage** (**CAS**) outside. A best practice is to abstract data management from
    specific storage vendors to provide vendor-agnostic service flexibility with storage
    classes. Different storage classes can be used to provide cold storage, SSD, or
    NVMe-backed storage depending on the application''s needs. We learned about tuning
    Kubernetes storage and choosing the storage solution in [*Chapter 7*](B16192_07_Final_PG_ePub.xhtml#_idTextAnchor157),
    *Managing Storage and Stateful Applications*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimize container images**: It is recommended to continuously monitor your
    cluster resources'' top consumers, improve their consumption, and look for ways
    to optimize their consumption. Optimizing container images can have a significant
    impact on resource utilization and performance. You can read more about the challenges
    and best practices of improving container images in [*Chapter 8*](B16192_08_Final_PG_ePub.xhtml#_idTextAnchor177),
    *Deploying Seamless and Reliable Applications*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimize cluster resource spend**: In theory, the only limit on the cloud
    provider''s resources is your budget. It is recommended to monitor the cost of
    resources and project allocation to get the full cost of running a product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we have learned the best practices for improving the quality of our cluster;
    we have touched on some of the topics in previous chapters. Let's look into the
    remaining areas that we haven't covered yet, including how we can validate cluster
    resources in a non-destructive manner and monitoring the cost of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Generating compliance reports
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many ways and tools to get a Kubernetes cluster up and running. It
    is an administrative challenge to maintain a proper configuration. Fortunately,
    there are tools to validate reports and detect configuration problems. Sonobuoy
    is one of the popular open source tools available to run Kubernetes conformance
    tests and validate our cluster''s health. Sonobuoy is cluster-agnostic and can
    generate reports of our cluster''s characteristics. These reports are used to
    ensure the best practices applied by eliminating distribution-specific issues
    and conforming clusters can be ported into our clusters. You can read more about
    custom data collection capabilities using plugins and integrated **end-to-end**
    (**e2e**) testing at Sonobuoy''s official documentation site, https://sonobuoy.io/docs/v0.20.0/.
    Now, let''s install the latest version of Sonobuoy and validate our cluster by
    running a Kubernetes conformance test:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get the latest release version tag of Sonobuoy and keep it in a variable
    called `SONOBUOYVERSION`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, download the latest `sonobuoy` release binary and install by executing
    the following command ([https://github.com/vmware-tanzu/sonobuoy/releases/download/v0.20.0/sonobuoy_0.20.0_linux_amd64.tar.gz](https://github.com/vmware-tanzu/sonobuoy/releases/download/v0.20.0/sonobuoy_0.20.0_linux_amd64.tar.gz)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Confirm that Sonobuoy is installed, and the command can execute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make sure that the cluster has enough resources to execute all the tests. You
    can find a specific suggestion for every provider on Sonobuoy''s source repository
    at [https://github.com/cncf/k8s-conformance/tree/master/v1.16](https://github.com/cncf/k8s-conformance/tree/master/v1.16).
    For EKS, the suggested cluster size is 10 `c5.xlarge` worker instances. Start
    the conformance tests on your EKS cluster by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To shorten testing and validate the configuration rather than full certified
    conformance, we can run the test with the `--mode quick` option:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Validation will take up to an hour to complete depending on the tests executed
    on the cluster. Once finished, execute the following command to get the plugins''
    results and inspect the results for failures. For a detailed list of options to
    inspect results, see the documentation at [https://sonobuoy.io/docs/v0.20.0/results/](https://sonobuoy.io/docs/v0.20.0/results/):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding command should look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.15 – Sonobuoy validation results](img/B16192_10_015.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 10.15 – Sonobuoy validation results
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Delete the Sonobuoy components from the cluster and clean up the resources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we have learned how to validate our Kubernetes cluster configuration. Let's
    look into how we can detect overprovisioned, idle resources and optimize our cluster's
    total cost.
  prefs: []
  type: TYPE_NORMAL
- en: Managing and improving the cost of cluster resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Monitoring project cost and team chargeback and managing total cluster spending
    are some of the big challenges of managing Kubernetes on public cloud providers.
    Since we have a theoretically unlimited scale available through cloud vendors,
    utilization fees can quickly go up and become a problem if not managed. Kubecost
    helps you monitor and continuously improve the cost of Kubernetes clusters. You
    can read more about the cost and capacity management capabilities of Kubecost
    at Kubecost''s official documentation site: https://docs.kubecost.com/.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s install Kubecost using Helm and start analyzing cost allocation
    in our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a namespace called `kubecost`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the `cost-analyzer` Helm Chart repository to your local repository list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update the Helm Chart repositories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install `cost-analyzer` from its Helm repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify successful installation by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we have `cost-analyzer` installed. Let''s access the Kubecost dashboard.
    Create port forwarding to access the Kubecost interface locally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Open a browser window and visit `http://localhost:9090`, which is forwarded
    to the `kubecost-cost-analyzer` service running in the cluster. The dashboard
    will immediately show the running monthly cost of your cluster, similar to the
    following:![Figure 10.17 – Kubecost Available Clusters screen
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16192_10_017.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.17 – Kubecost Available Clusters screen
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on your cluster from the list and access the Kubecost dashboard. The top
    part of the dashboard will show a summary of the total cost and any potential
    identified savings, similar to that in the following screenshot:![Figure 10.18
    – Kubecost dashboard
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16192_10_018.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.18 – Kubecost dashboard
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's scroll down the dashboard screen to find a summary of the controller component
    and service allocation. At the bottom of the dashboard, we will see the health
    scores. A health score is an assessment of infrastructure reliability and performance
    risks:![Figure 10.19 – Kubecost dashboard showing the cluster health assessment
    score](img/B16192_10_019.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 10.19 – Kubecost dashboard showing the cluster health assessment score
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The most important quick summary pages on the dashboard are the health assessment
    and estimated saving detail pages. Let's click on each to get to the areas where
    you can improve your cluster's cost and performance. In the following screenshot,
    we can see an example of a significant saving suggestion from Kubecost after analyzing
    our cluster:![Figure 10.20 – Kubecost estimated savings dashboard](img/B16192_10_020.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 10.20 – Kubecost estimated savings dashboard
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the arrow button next to one of the saving categories and review the
    recommendations to optimize your cluster cost.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we have learned how to identify monthly cluster costs, resource efficiency,
    cost allocation, and potential savings by optimizing request sizes, cleaning up
    abandoned workloads, and using many other ways to manage underutilized nodes in
    our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored Kubernetes operation best practices and covered
    cluster maintenance topics such as upgrades, backups, and disaster recovery. We
    learned how to validate our cluster configuration to avoid cluster and application
    problems. Finally, we learned ways to detect and improve resource allocation and
    the cost of our cluster resources.
  prefs: []
  type: TYPE_NORMAL
- en: By completing this last chapter in the book, we now have the complete knowledge
    to build and manage production-grade Kubernetes infrastructure following the industry
    best practices and well-proven techniques learned from early technology adopters
    and real-life, large-scale Kubernetes deployments. Kubernetes offers a very active
    user and partner ecosystem. In this book, we focused on the best practices known
    today. Although principles will not change quickly, as with every new technology,
    there will be new solutions and new approaches to solving the same problems. Please
    let us know how we can improve this book in the future by reaching out to us via
    the methods mentioned in the *Preface* section.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following links for more information on the topics covered
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Amazon EKS Kubernetes release calendar*: https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-release-calendar'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disaster recovery for multi-region Kafka at Uber: [https://eng.uber.com/kafka/](https://eng.uber.com/kafka/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Disaster Recovery Preparedness for Your Kubernetes Clusters*: [https://rancher.com/blog/2020/disaster-recovery-preparedness-kubernetes-clusters](https://rancher.com/blog/2020/disaster-recovery-preparedness-kubernetes-clusters)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official website of the Velero project: [https://velero.io/](https://velero.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official website of the Sonobuoy project: [https://sonobuoy.io/](https://sonobuoy.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KubeDR, an alternative open source Kubernetes cluster backup solution: [https://github.com/catalogicsoftware/kubedr](https://github.com/catalogicsoftware/kubedr)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kasten, an alternative Kubernetes backup, disaster recovery, and mobility solution:
    [https://www.kasten.io/](https://www.kasten.io/%20)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
