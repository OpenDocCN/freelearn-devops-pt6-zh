<html><head></head><body>
		<div id="_idContainer351">
			<h1 id="_idParaDest-158"><em class="italic"><a id="_idTextAnchor157"/>Chapter 11</em>: Machine Learning on Kubernetes</h1>
			<p>Throughout the chapters, you have learned about the differences between a traditional software development process and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>). You have learned about the ML life cycle and you understand that it is pretty different from the conventional software development life cycle. We have shown you how open source software can be used to build a complete ML platform on Kubernetes. We presented to you the life cycle of ML projects, and by doing the activities, you have experienced how each phase of the project life cycle is executed. </p>
			<p>In this chapter, we will show you some of the key ideas that we wanted to bring forth to further your knowledge on the subject. The following topics will be covered in this chapter:</p>
			<ul>
				<li>Identifying ML platform use cases</li>
				<li>Operationalizing ML</li>
				<li>Running on Kubernetes</li>
			</ul>
			<p>These topics will help you decide when and where to use the ML platform that we presented in this book and help you set up the right organizational structure for running and maintaining the platform in production.</p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor158"/>Identifying ML platform use cases</h1>
			<p>As discussed in the earlier chapters, it is imperative to understand what ML is and how it differs from other <a id="_idIndexMarker970"/>closely related disciplines, such as data analytics and data science. Data science may be required as a precursor to ML. It is instrumental in the research and exploration phase where you are unsure whether an ML algorithm can solve the problem. In the previous chapters, you have employed data science practices such as problem definitions, isolation of business metrics, and algorithm comparison. While data science is essential, there are also ML use cases that do not require as many data science activities. An example of such cases is the use of AutoML frameworks, which we will talk about in the next section.</p>
			<p>Identifying whether ML can best solve the problem and selecting the ML platform is a bit of a chicken and egg problem. This is because, in order to be sure that an ML algorithm can best solve a certain business problem, it requires some data science work such as data <a id="_idIndexMarker971"/>exploration, and thus requires a platform to work on. If you are in this situation, your best bet is to choose an open source platform such as <strong class="bold">Open Data Hub</strong> (<strong class="bold">ODH</strong>), which we presented in this book. Because it is fully open source, there are no required commercial agreements and licenses to start <a id="_idIndexMarker972"/>installing and using the platform, and you have already seen how capable the platform is. Once you have a platform, you can then use it to initiate your research and data exploration until you can conclude whether ML is the right approach to solving the business problem or not. You can then either continue using the platform for the remainder of the project life cycle or abandon it without incurring any platform costs.</p>
			<p>In some cases, you may already know that the business problem can be solved by ML because you have seen a similar implementation somewhere else. In such cases, choosing the ML platform we have presented is also a good option. However, you could also be in a situation where you do not have a strong data science team. You may have a few data engineers and ML engineers who understand the process of model development but are not confident about their data science skills. This is where AutoML comes into the picture as a consideration.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor159"/>Considering AutoML</h2>
			<p>To define it in its simplest form, AutoML is about automatically producing ML models, with little to no <a id="_idIndexMarker973"/>data science work needed. To elaborate a bit, it is about <a id="_idIndexMarker974"/>automatic algorithm selection, automatic hyperparameter tuning, and automatic model evaluation.</p>
			<p>AutoML technology comes as a framework or a software library that can generate an ML model from a given dataset. There are several AutoML frameworks already available on the market as of writing this book. The following list shows some of the popular AutoML frameworks currently available. There are many other AutoML frameworks not listed here, and we <a id="_idIndexMarker975"/>encourage you to explore them:</p>
			<ul>
				<li><strong class="bold">BigML</strong> – An end-to-end AutoML <a id="_idIndexMarker976"/>enterprise platform sold commercially. </li>
				<li><strong class="bold">MLJAR</strong> – An open <a id="_idIndexMarker977"/>source AutoML framework.</li>
				<li><strong class="bold">H2O.ai</strong> – An open source full <a id="_idIndexMarker978"/>ML platform that includes an AutoML framework. </li>
				<li><strong class="bold">TPOT</strong> – Considers itself as a data scientist assistant. It's an open source AutoML framework developed <a id="_idIndexMarker979"/>by the Computational Genetics Lab at the University of Pennsylvania.</li>
				<li><strong class="bold">MLBox</strong> – An open <a id="_idIndexMarker980"/>source AutoML Python library.</li>
				<li><strong class="bold">Ludwig</strong> – A toolbox featuring <a id="_idIndexMarker981"/>zero code ML model development <a id="_idIndexMarker982"/>that includes AutoML.</li>
				<li><strong class="bold">Auto-sklearn</strong> – An open <a id="_idIndexMarker983"/>source AutoML toolkit based on scikit-learn ML libraries.</li>
				<li><strong class="bold">Auto-PyTorch</strong> – An open source AutoML framework that features an automatic neural network <a id="_idIndexMarker984"/>architecture search. It can <a id="_idIndexMarker985"/>automatically optimize neural network architectures.</li>
				<li><strong class="bold">AutoKeras</strong> – An open source AutoML framework <a id="_idIndexMarker986"/>based on Keras ML libraries.</li>
			</ul>
			<p>It is also important to note that <a id="_idIndexMarker987"/>some of these frameworks and libraries can be used <a id="_idIndexMarker988"/>within, or in conjunction with, our ML platform or any ML platform.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor160"/>Commercial platforms</h2>
			<p>Commercial vendors of ML platforms, including cloud providers, also include AutoML products <a id="_idIndexMarker989"/>and services in their portfolio. Google has Google Cloud AutoML, Microsoft has Azure Machine Learning, Amazon has Sagemaker Autopilot, and IBM has Watson Studio with AutoML and AutoAI components. However, these vendors sell their AutoML products and services as part of their ML platform product, which means you will have to use their ML platform to take advantage of the AutoML features.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor161"/>ODH</h2>
			<p>You have seen how the ODH allows you to choose which components to install and it also allows you to <a id="_idIndexMarker990"/>replace one component with another by updating the <strong class="source-inline">kfdef</strong> manifest file. This adds <a id="_idIndexMarker991"/>additional flexibility as to what components you choose to be part of your platform. For example, suppose you only need JupyterHub and MLflow for your data science team to start exploring the possibility of using ML to solve your business problem. In that case, you can choose to install only these components. This will save you compute resources and, therefore, reduce cloud computing bills.</p>
			<p>Regardless of which ML platform you choose, it is also essential that the path to operationalizing your ML platform is clearly established. This includes finding the right people who can run the platform in production and mapping the personas in the ML life cycle to the existing organization. This also includes establishing some processes and communication channels, which brings us to our next topic.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor162"/>Operationalizing ML</h1>
			<p>As discussed in earlier chapters, you can enjoy the full benefits of ML in your business if your models get <a id="_idIndexMarker992"/>deployed and used in the production environment. Operationalization is more than just deploying the ML model. There are also other things that need to be addressed to have successful ML-enabled applications in production. Let's get into it.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor163"/>Setting the business expectations</h2>
			<p>It is extremely important to ensure that the business stakeholders understand the risk of making business <a id="_idIndexMarker993"/>decisions using the ML model's predictions. You do not want to be in a situation where your organization fails because of ML. Zillow, a real estate company that invested a lot in ML with their product <em class="italic">Zestimate,</em> lost 500 million dollars due to incorrect price estimates of real properties. They ended up buying properties at prices set by their ML model that they eventually ended up selling for a much lower price.</p>
			<p>ML models are not perfect; they make mistakes. The business must accept this fact and must not rely entirely on the ML model's prediction without looking at other data sources. If the business fails to accept this fact, this could lead to irreparable damages caused by wrong expectations. These damages include reputational damages, loss of trust by the business, and even regulatory fines and penalties.</p>
			<p>Another case is that some algorithms, particularly deep learning, are not explainable. It must be communicated to the business because, in some cases, an explainable algorithm may be required for regulatory purposes. Some regulators may need you to explain the reason behind the business decision. For example, suppose an ML model decided that a new bank customer is not a risky individual and it turned out to be a black-listed or sanctioned individual by some regulators; the financial organization may need to explain the reasoning behind this decision to the regulators during the investigation and the post-mortem analysis. Or, even worse, the organization could get fined millions of dollars. </p>
			<p>Avoid over-promising results to the business. IBM Watson had the idea that ML could diagnose cancer by making sense of diagnostic data from several medical institutions and potentially <a id="_idIndexMarker994"/>assisting, or even replacing, doctors in performing a more reliable cancer diagnosis in the future. This has gained a lot of attention, and many organizations invested in the idea. However, it turned out to be a very difficult task. It did not only result in losses, but it also somehow damaged the brand. </p>
			<p>To summarize, before deciding whether to use ML models to predict business decisions, make sure that the business understands the risks and consequences if the model does not behave as expected. Set the expectations right. Be transparent about what is possible and what is hard. Some ML models may be able to replace a human in a particular business process, but not all ML models will achieve superhuman abilities.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor164"/>Dealing with dirty real-world data</h2>
			<p>The data you used for model training comes as prepared datasets tested in a controlled environment. However, this is not the case in the real-world setting. After your model gets <a id="_idIndexMarker995"/>deployed to production, you must expect dirty data. You may receive wrongly structured data, and most of the data is new and has never been seen by the model during training. To ensure that your model is fit for production, avoid overfitting, and test the model thoroughly with datasets that are as close as the ones it will see in production. If possible, use data augmentation techniques or even manufactured data to simulate production scenarios. For example, a model that works well in diagnosing a patient utilizing chest X-ray scans may work well in one clinic, but it may not work in another clinic using older medical equipment. There is a real story behind this, and the reason it did not work was that the X-ray scanners generated scans that showed dust particles present in the machine's sensors.</p>
			<p>To summarize, avoid overfitting. Have a solid data cleaning process as part of your inference <a id="_idIndexMarker996"/>pipeline. Prepare for the worst possible input data by having suitable datasets from various sources. Be ready when your model does not return what is expected of it.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor165"/>Dealing with incorrect results</h2>
			<p>Imagine you have a credit card fraud detection and it marks a routine transaction as fraudulent. There could be many reasons for this, such as your model may not be aware of higher-than-normal spending during Christmas. You need the capability to investigate such <a id="_idIndexMarker997"/>scenarios and that's why it is crucial to have logging in place. This will allow you to recall the model's answer to a particular question thrown to it in production. You will need this to investigate model issues.</p>
			<p>When this happens, you must be prepared to face the consequences of the wrong information your model returned. But also, you must be able to address the erroneous result in the future by updating the model with new sets of data from time to time. You must also have the ability to track the model's performance over time. You have seen in the previous chapter how monitoring is done. The change in model performance over time is also called a <strong class="bold">drift</strong>. There are two kinds of drift. <strong class="bold">Data drift</strong> happens when the model starts <a id="_idIndexMarker998"/>receiving new types of data that it has not been trained on. For example, an insurance fraud <a id="_idIndexMarker999"/>detection model worked well until it started seeing new data that included a new insurance product that the model hadn't seen before. In this case, the model will not produce a reliable result. In other words, your model performance has degraded. Another example is that your model was trained on a certain demographic or age group, and then suddenly a new age group started appearing. Similarly, there is a higher chance that the ML model will return an unreliable result. <strong class="bold">Concept drift</strong> is when the functional relationship between the input data and the label <a id="_idIndexMarker1000"/>has changed. For example, in a fraud detection model, a transaction that was not previously considered fraudulent is now labeled as fraudulent or anomalous according to the new regulations. This means the model will produce more false-negative results, which renders the model unreliable.</p>
			<p>In these scenarios, you must have a process set for addressing these problems. You must have a process for when to manually retrain the model, or even automatically retrain the model when it detects a drift. You may also want to implement anomaly detection <a id="_idIndexMarker1001"/>in the input data. This ensures that your model only gives up results if the input data make sense. This avoids abuse or attacks on the model as well. These automation requirements can be integrated as part of your continuous integration and deployment pipelines.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor166"/>Maintaining continuous delivery</h2>
			<p>You have seen how to run model builds and model deployments in the platform manually. You have also seen how to automate the deployment workflow using Airflow. Although the data <a id="_idIndexMarker1002"/>scientists or ML engineers in the team can manually perform or trigger such operations, in the real world, you will also need someone or a team to maintain these pipelines to make sure they are always working. You may want to have a dedicated platform team to maintain the underlying platform that executes the pipelines, or you may assign this responsibility to the data engineering team. Whatever approach you choose, the important thing is that someone must be responsible for ensuring that the deployment pipelines are always working.</p>
			<p>Although the ODH operator completely manages the ML platform, you will still need someone responsible for maintaining it. Ensure that the Kubernetes operators are up to date. Apply security patches whenever necessary.</p>
			<p>For some critical workloads, you may not be able to deploy to production automatically. There will be manual approvals required before you can ship updates to a model in production. In this case, you need to establish this approval workflow by either embedding this process into the platform or through mutual agreement with manual approval processes. Nevertheless, the objective is to have someone accountable for maintaining continuous delivery services.</p>
			<p>In summary, continuous delivery must always work so that the model development life cycle can have a faster feedback cycle. Also, if drift is detected, you will always have a ready-to-go delivery pipeline that can ship a more up-to-date version of the model.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor167"/>Managing security</h2>
			<p>Security is another critical area to focus on when operationalizing ML projects. You have seen in the preceding <a id="_idIndexMarker1003"/>chapters that the ML platform can be secured by using <strong class="bold">OpenID Connect</strong> (<strong class="bold">OIDC</strong>) or <strong class="bold">OAuth2</strong>, a standard <a id="_idIndexMarker1004"/>authentication mechanism. Different platform components can utilize <a id="_idIndexMarker1005"/>the same authentication mechanism for a more seamless user <a id="_idIndexMarker1006"/>experience. You have used an open source tool called Keycloak, an industry-standard implementation of the <strong class="bold">identity and access management</strong> (<strong class="bold">IAM</strong>) system that mainly supports OIDC, <strong class="bold">Security Assertion Markup Language</strong> (<strong class="bold">SAML</strong>), and more. The Seldon Core API allows the REST-exposed ML models to be protected behind the same authentication mechanism. Refer to the Seldon Core documentation for more details.</p>
			<p>To summarize, the ML platform must be protected by an authentication mechanism, preferably OIDC. This also allows for the implementation of <strong class="bold">single sign-on</strong> (<strong class="bold">SSO</strong>). Additionally, you also <a id="_idIndexMarker1007"/>need to protect your deployed models to ensure that only the intended audiences <a id="_idIndexMarker1008"/>can access your ML models. And finally, there must be someone responsible for maintaining the Keycloak instance that your platform uses and someone, or a team, managing the access to the platform resources.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor168"/>Adhering to compliance policies</h2>
			<p>In some business settings, compliance is at the center of the operation. Financial institutions <a id="_idIndexMarker1009"/>have a whole department managing compliance. These compliance rules typically come from the regulatory bodies that oversee the financial institution's operations. Depending on which country your ML platform will be used and hosted in, regulatory policies may prevent you from moving data out of the on-premises data centers. Or, there could be a requirement for encrypting data at rest. </p>
			<p>The good news is that your platform is flexible enough to be configured for such compliance measures. It can run on-premises or in any cloud provider, thanks to Kubernetes. You can also run the ML platform in the cloud while having the storage on-premises or take advantage of hybrid-cloud strategies.</p>
			<p>Another thing is that each of the components in the platform is replaceable and pluggable. For example, instead of using a dedicated instance of Keycloak, you could use an existing regulator-approved OIDC provider.</p>
			<p>Compliance could often become an impediment in progressing with ML projects. If you plan to use a commercial platform rather than the one you built in this book, always consider <a id="_idIndexMarker1010"/>the compliance or regulatory requirements before deciding. Some commercial platforms in the cloud may not be able to comply with data sovereignty, especially in countries where the major cloud providers do not yet have a local data center.</p>
			<p>In other words, always consider compliance requirements when planning for the architecture of your ML platform.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor169"/>Applying governance</h2>
			<p>After taking into account the preceding considerations, another important area that needs to be cleared out to operationalize your ML platform is <strong class="bold">governance</strong>. This is where you <a id="_idIndexMarker1011"/>will design the organizational structure, roles and responsibilities, collaboration model, and escalation points. The authors advocate for a more cross-functional team with very high collaboration levels. However, this is not always possible in the real world. There are organizations with very well-defined hierarchies and silos that refuse to change the way things are. If you are in this type of organization, you may face several hurdles in implementing the ML platform we have presented here.</p>
			<p>One of the platform's main features is that it is a self-service platform. It allows data scientists, ML engineers, and data engineers to spin up their notebook servers and Spark clusters. However, this will also lead to less predictable cloud billings or operating costs. If you are the data architect of the project, part of your job is to convince the leadership team and the platform teams to trust their data scientists and ML engineers.</p>
			<p>Ideally, the best way to design the organizational structure around the ML project is to have a platform team. This team is responsible for running the ML platform. This team then acts as a service provider to the data and application teams, also called the <strong class="bold">stream-aligned teams</strong>, in a <strong class="bold">software as a service</strong> (<strong class="bold">SaaS</strong>) model. The platform team's objective is to <a id="_idIndexMarker1012"/>ensure that the stream-aligned teams can perform <a id="_idIndexMarker1013"/>their work on the platform as smoothly and as quickly as possible. The data science and data engineering teams can be the stream-aligned teams, and they are the main users of the platform and the main customers of the platform team. The DevSecOps or DevOps teams may sit together in the same organizational unit, as the platform team provides DevOps services to the stream-aligned teams. <em class="italic">Figure 11.1</em> shows an example of an organizational structure that you could implement to run an ML project using the Team Topologies notation:</p>
			<div>
				<div id="_idContainer349" class="IMG---Figure">
					<img src="image/B18332_11_01.jpg" alt="Figure 11.1 – Example ML project team structure&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – Example ML project team structure</p>
			<p>In <em class="italic">Figure 11.1</em>, there are a total of three stream-aligned teams, namely, the data science team, the data engineering team, and the software engineering team. All three stream-aligned teams <a id="_idIndexMarker1014"/>are collaborating with each other with the objective of delivering an ML-enabled application in production. There are also three platform teams. The cloud infrastructure team is providing a cloud <strong class="bold">platform as a service</strong> (<strong class="bold">PaaS</strong>) to the two other platform teams: the ML platform team <a id="_idIndexMarker1015"/>and the MLOps team. Both the ML platform team and the MLOps team are providing ML PaaS and MLOps as a service to all the three stream-aligned teams. The purple box represents an enabling team. This is where the SMEs and product owners sit. This team enables and provides support to all the stream-aligned teams.</p>
			<p>You must take note that this is just an example; you may want to combine the ML platform team and MLOps team together, or the data science and data engineering teams, and that's perfectly okay.</p>
			<p>If you want to learn more about this type of organizational design notation, you may want to read about Team Topologies. </p>
			<p>We can summarize as follows:</p>
			<ul>
				<li>Use the ML life cycle diagram that you have seen in <em class="italic">Figure 2.7</em> in <a href="B18332_02_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Understanding MLOps</em>, to map the current organizational structure of your teams.</li>
				<li>Communicate the roles and responsibilities clearly.</li>
				<li>Set the collaboration channels and feedback points, such as design spike meetings and chatgroups.</li>
			</ul>
			<p>Suppose you cannot break the silos; set up regular meetings between the silos and establish a more <a id="_idIndexMarker1016"/>streamlined handover process. However, if you want to take advantage of the full potential of the ML platform, we strongly recommend that you form a cross-functional and self-organizing team to deliver your ML project.</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor170"/>Running on Kubernetes</h1>
			<p>Using the ODH operator, the ML platform truly unlocks the full potential of Kubernetes as the infrastructure layer of your ML platform. The <strong class="bold">Operator Lifecycle Management</strong> (<strong class="bold">OLM</strong>) framework enables the ODH operator to simplify the operation and maintenance <a id="_idIndexMarker1017"/>of the ML platform. Almost all <a id="_idIndexMarker1018"/>operational work is done in a Kubernetes-native way, and you can even spin up multiple ML platforms <a id="_idIndexMarker1019"/>with a few clicks. Kubernetes and the OLM also allow you to implement the <strong class="bold">Platform as Code</strong> (<strong class="bold">PaC</strong>) approach, enabling you to implement GitOps practices.</p>
			<p>The ML platform you've seen in this book works well with vanilla Kubernetes instances or any other flavors of Kubernetes or even a Kubernetes-based platform. In fact, the original ODH repository was mainly designed and built for Red Hat OpenShift.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor171"/>Avoiding vendor lock-ins</h2>
			<p>Kubernetes protects you from vendor lock-ins. Because of the extra layer of containerization and container orchestration, all your workloads do not run directly on the infrastructure layer but through containers. This allows the ML platform to be hosted in any <a id="_idIndexMarker1020"/>capable infrastructure. Whether on-premises or in the cloud, the operations will be the same. This also allows you to seamlessly switch to a different cloud provider when needed. This is one of the advantages of using this ML platform when compared to the commercial platforms provided by cloud vendors. You are not subject to vendor lock-in.</p>
			<p>For example, if you use Azure ML as your platform of choice, you will be stuck with using Azure as your infrastructure provider. You will not be able to move your entire ML project to another cloud vendor without changing the platform and deployment architecture. In other words, the cost of switching to a different cloud vendor is so high that you are basically stuck with the original vendor.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor172"/>Considering other Kubernetes platforms</h2>
			<p>It is not mandatory for this ML platform to run on the vanilla Kubernetes platform only. As mentioned in <a id="_idIndexMarker1021"/>the previous section, the original ODH was designed to run on Red Hat OpenShift, whereas in this book, you managed to make it run on minikube, a single-node vanilla Kubernetes.</p>
			<p>There are many other Kubernetes platforms out there, including those provided by the major cloud providers. The following list includes the most common ones in no particular order, but <a id="_idIndexMarker1022"/>other emerging Kubernetes-based platforms have just <a id="_idIndexMarker1023"/>entered the market or are either <a id="_idIndexMarker1024"/>in beta or <a id="_idIndexMarker1025"/>in development <a id="_idIndexMarker1026"/>as of this writing:</p>
			<ul>
				<li><strong class="bold">Kubernetes</strong></li>
				<li><strong class="bold">Red Hat</strong> <strong class="bold">OpenShift Container Platform</strong> (<strong class="bold">OCP</strong>)</li>
				<li><strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>)</li>
				<li><strong class="bold">Amazon</strong> <strong class="bold">Elastic Kubernetes Engine</strong> (<strong class="bold">EKS</strong>)</li>
				<li><strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>)</li>
				<li><strong class="bold">VMware Tanzu</strong></li>
				<li><strong class="bold">Docker Enterprise Edition</strong> (<strong class="bold">Docker EE</strong>)</li>
			</ul>
			<p>Although we <a id="_idIndexMarker1027"/>have tested this platform in Kubernetes and Red Hat OpenShift, the ML platform <a id="_idIndexMarker1028"/>that you built in minikube can <a id="_idIndexMarker1029"/>also be built in any of the above Kubernetes platforms, and others. But, what about in the future? Where is ODH heading?</p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor173"/>Roadmap</h1>
			<p>ODH is an active open source project primarily maintained by Red Hat, the largest open source company in the world. ODH will keep getting updated to bring more and more features to the <a id="_idIndexMarker1030"/>product. However, because the ML and MLOps space is also relatively new and still evolving, it is not unnatural to see significant changes and pivots in the project over time.</p>
			<p>As of writing this book, the next version of ODH includes the following changes (as shown in <em class="italic">Figure 11.2</em>):</p>
			<div>
				<div id="_idContainer350" class="IMG---Figure">
					<img src="image/B18332_11_02.jpg" alt="Figure 11.2 – ODH's next release&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – ODH's next release</p>
			<p>There are other features of ODH that you have not yet explored because they are more geared toward data engineering and the data analytics space. One example is data virtualization and <a id="_idIndexMarker1031"/>visualization using Trino and Superset. If you want to learn more about these features, you can explore them in the same ML platform you built by simply updating the <strong class="source-inline">kfdef</strong> file to include Trino and Superset as components of your ML platform. You will find some examples of these <strong class="source-inline">kfdef</strong> files in the ODH GitHub project.</p>
			<p>You can look for future roadmaps of ODH at the following URL: <a href="https://opendatahub.io/docs/roadmap/future.html">https://opendatahub.io/docs/roadmap/future.html</a>.</p>
			<p>In the future, there could be another open source ML platform project that will surface on the market. Keep an open mind, and never stop exploring other open source projects.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor174"/>Summary</h1>
			<p>The knowledge that you have gained in this book about ML, data science and data engineering, MLOps, and the ML life cycle applies to any other ML platforms as well. You have not only gained important insights and knowledge about running ML projects in Kubernetes but also gained the experience of building the platform from scratch. In the later chapters, you were able to gain hands-on experience and wear the hats of a data engineer, data scientist, and MLOps engineer.</p>
			<p>While writing this book, we realized that the subject is vast and that going deep into each of the topics covered in the book may be too much for some. Although we have touched upon most of the components of the ML platform, there is still a lot more to learn about each of the components, especially Seldon Core, Apache Spark, and Apache Airflow. To further your knowledge of these applications, we recommend going through the official documentation pages.</p>
			<p>ML, AI, and MLOps are still evolving. On the other hand, even though Kubernetes is almost 8 years old, it is still relatively new to most enterprise organizations. Because of this, most professionals in this space are still learning, while at the same time establishing new standards.</p>
			<p>Keep yourself updated on the latest ML and Kubernetes trends. You already have enough knowledge to advance your learning in this subject on your own.</p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor175"/>Further reading</h1>
			<ul>
				<li><em class="italic">Seldon core documentation</em>: <a href="https://docs.seldon.io/projects/seldon-core/en/latest/index.html">https://docs.seldon.io/projects/seldon-core/en/latest/index.html</a></li>
				<li><em class="italic">Team topologies</em>: <a href="https://teamtopologies.com">https://teamtopologies.com</a></li>
				<li><em class="italic">Open Data Hub</em>: <a href="https://opendatahub.io">https://opendatahub.io</a> </li>
			</ul>
		</div>
	</body></html>