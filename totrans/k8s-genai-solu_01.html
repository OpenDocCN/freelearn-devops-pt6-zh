<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer021">
			<h1 id="_idParaDest-16" class="chapter-number"><a id="_idTextAnchor015"/>1</h1>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Generative AI Fundamentals</h1>
			<p> <strong class="bold">Generative AI</strong> (<strong class="bold">GenAI</strong>) has<a id="_idIndexMarker000"/> revolutionized our world and has grabbed everyone’s attention since the introduction of ChatGPT in November of 2022 by<a id="_idIndexMarker001"/> OpenAI (<a href="https://openai.com/index/chatgpt/">https://openai.com/index/chatgpt/</a>). However, the foundational concepts of this technology have been around for quite some time. In this chapter, we will introduce the key concepts of GenAI and how it has evolved over time. We will then discuss how to think about a GenAI project and align it with the business objectives, covering the entire process for developing and deploying GenAI workloads, along with potential use cases across <span class="No-Break">different industries.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Artificial intelligence <span class="No-Break">versus GenAI</span></li>
				<li>The evolution of <span class="No-Break">machine learning</span></li>
				<li><span class="No-Break">Transformer architecture</span></li>
				<li>The GenAI project <span class="No-Break">life cycle</span></li>
				<li>The GenAI <span class="No-Break">deployment stack</span></li>
				<li>GenAI project <span class="No-Break">use cases</span></li>
			</ul>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Artificial Intelligence versus GenAI</h1>
			<p>Before we dive<a id="_idIndexMarker002"/> deeper into GenAI concepts, let’s <a id="_idIndexMarker003"/>discuss the differences between <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>), <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>), <strong class="bold">Deep Learning</strong> (<strong class="bold">DL</strong>), and<a id="_idIndexMarker004"/> GenAI, as these terms are often <span class="No-Break">used</span><span class="No-Break"><a id="_idIndexMarker005"/></span><span class="No-Break"> interchangeably.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.1</em> shows the relationships between <span class="No-Break">these concepts.</span></p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B31108_01_01.jpg" alt="Figure 1.1 – Relationships between AI, ML, DL, and GenAI" width="1213" height="720"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Relationships between AI, ML, DL, and GenAI</p>
			<p>Let’s<a id="_idIndexMarker006"/> learn <a id="_idIndexMarker007"/>more about <span class="No-Break">these relationships:</span></p>
			<ul>
				<li><strong class="bold">AI</strong>: AI refers to a system or algorithm that is capable of performing tasks that would otherwise typically require human intelligence. These tasks include reasoning, learning, problem-solving, perception, and language understanding. AI is a broad category and can include rule-based systems, expert systems, neural networks, and GenAI algorithms. The evolution of AI algorithms has provided machines with human-like senses and capabilities, such as vision to analyze the world around them, listening and speaking to understand natural language and respond verbally, and using sensor data to understand the external environment and <span class="No-Break">respond accordingly.</span></li>
				<li><strong class="bold">ML</strong>: ML is a subset <a id="_idIndexMarker008"/>of AI that involves algorithms and models that enable machines to learn from data and make predictions without requiring explicit coding. In traditional programming, developers write explicit instructions for a computer to execute, whereas in ML, algorithms learn from the patterns and relationships in data and make predictions. ML can further be divided into the <span class="No-Break">following sub-categories:</span><ul><li><strong class="bold">Supervised learning</strong>: This uses<a id="_idIndexMarker009"/> labeled datasets to train the models. It can further be subdivided into classification and <span class="No-Break">regression problems:</span><ul><li><strong class="bold">Classification problems</strong> use labeled data, such as labeled pictures of dogs and cats, to train the model. Once the model is trained, it can classify a user-provided picture using the classes it has been <span class="No-Break">trained on.</span></li><li><strong class="bold">Regression problems</strong>, on the other hand, use numerical data to understand the relationship between dependent and independent variables, such as house pricing based on different attributes. Once a model establishes a relationship, it can then forecast the pricing for different sets of attributes, even if the model has not been trained on these specific attributes. Some popular regression algorithms are linear regression, logistic regression, and <span class="No-Break">polynomial regression.</span></li></ul></li><li><strong class="bold">Unsupervised learning</strong>: This <a id="_idIndexMarker010"/>uses ML algorithms to analyze and cluster unlabeled datasets to discover hidden patterns in data. Unsupervised learning can<a id="_idIndexMarker011"/> further <a id="_idIndexMarker012"/>be divided into the following <span class="No-Break">two sub-categories:</span><ul><li><strong class="bold">Clustering algorithms</strong> group data based on similarities or differences. A popular clustering algorithm is the <strong class="bold">k-means clustering algorithm</strong>, which <a id="_idIndexMarker013"/>uses Euclidian distances between data points to measure the similarity between data points and assign them in <em class="italic">k</em> distinct, non-overlapping clusters. It iterates to refine the clusters to minimize the variance within each cluster. A typical use case is segmenting customers based on purchasing behavior, demographics, or preferences to target marketing <span class="No-Break">strategies effectively.</span></li><li><strong class="bold">Dimensionality reduction</strong> is another form of unsupervised learning, which is used to reduce the number of features/dimensions in a given dataset. It aims to simplify models, reduce computational costs, and improve overall model <a id="_idIndexMarker014"/>performance. <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>) (<a href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c">https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c</a>) is a popular algorithm used for dimensionality reduction. It achieves this by finding a new set of features called components, which are composites of the original features that are uncorrelated with <span class="No-Break">one another.</span></li></ul></li><li><strong class="bold">Semi-supervised learning</strong>: This is <a id="_idIndexMarker015"/>a type of ML that combines supervised and unsupervised learning by leveraging both labeled and unlabeled data for training. This is particularly useful when obtaining labeled data is time-consuming and expensive because you can use small amounts of labeled data for training and then iteratively apply it to the large amounts of unlabeled data. This can be applied in both classification and regression<a id="_idIndexMarker016"/> use <a id="_idIndexMarker017"/>cases, such as spam/image/object detection, speech recognition, <span class="No-Break">and forecasting.</span></li><li><strong class="bold">Reinforcement learning</strong>: In reinforcement learning, there is an agent and reward <a id="_idIndexMarker018"/>system, and algorithms learn by trial and error to maximize the reward for the agent. An <strong class="bold">agent</strong> is an autonomous<a id="_idIndexMarker019"/> system, like a computer program or robot, that can make decisions and act in response to its environment without direct human instructions. <strong class="bold">Rewards</strong> are <a id="_idIndexMarker020"/>given from the environment when agent actions lead to a positive outcome. For example, if we want to train a robot to walk without falling over, positive rewards are given for actions that help the robot to remain upright, and negative rewards are given for actions that cause it to fall over. The robot begins by trying different actions randomly, such as leaning forward, moving its legs, or shifting its weight. As it performs these actions, it observes the resulting changes in its state. The robot uses feedback (rewards) to update its understanding of which actions are beneficial and thus learns to walk <span class="No-Break">over time.</span></li></ul><p class="list-inset">We have summarized the different categories of ML in <span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p></li>
			</ul>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B31108_01_02.jpg" alt="Figure 1.2 – Different categories of ML" width="1302" height="399"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Different categories of ML</p>
			<ul>
				<li><strong class="bold">DL</strong>: DL is a <a id="_idIndexMarker021"/>subset of ML that involves deep neural networks with many layers. Conceptually, it is inspired by the human brain, which has billions of deeply connected neurons and provides humans with very advanced cognition. Some popular examples of deep neural nets are <strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>), used for <a id="_idIndexMarker022"/>image processing, and <strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>), which are used for analyzing time<a id="_idIndexMarker023"/> series data or natural <span class="No-Break">language processing.</span></li>
				<li><strong class="bold">GenAI</strong>: GenAI is <a id="_idIndexMarker024"/>a<a id="_idIndexMarker025"/> further subset of DL and focuses on <a id="_idIndexMarker026"/>creating new data, such as text, images, music, and other forms of content. Lots of generative applications are based on <strong class="bold">Foundational Models</strong> (<strong class="bold">FMs</strong>), which<a id="_idIndexMarker027"/> are large-scale AI models trained on vast amounts of diverse data, serving as a base for a wide range of downstream tasks. They are pre-trained on broad datasets and can be fine-tuned for specific <a id="_idIndexMarker028"/>applications. <strong class="bold">Large Language Models</strong> (<strong class="bold">LLMs</strong>) are a subset of FMs specifically designed for understanding and generating human language. GenAI is the primary focus of this book; we will be diving into its details later in <span class="No-Break">the book.</span></li>
			</ul>
			<p>Now that we understand the key differences between AI, ML, DL, and GenAI, let’s explore the evolution of ML and how transformer architecture has revolutionized the ML landscape, particularly in the<a id="_idIndexMarker029"/> field of <strong class="bold">Natural Language </strong><span class="No-Break"><strong class="bold">Processing</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">NLP</strong></span><span class="No-Break">).</span></p>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>Evolution of machine learning</h1>
			<p>Since this book is <a id="_idIndexMarker030"/>about GenAI, what could be a better way to start it than asking ChatGPT to summarize the evolution of AI and ML over the <span class="No-Break">last decade?</span></p>
			<p><strong class="bold">Prompt</strong>: <strong class="source-inline">"Why did the chicken cross the road?" Describe how that question's answer evolved using AI/ML over the </strong><span class="No-Break"><strong class="source-inline">last decade.</strong></span></p>
			<p><strong class="bold">ChatGPT Response (ChatGPT-4o, June </strong><span class="No-Break"><strong class="bold">16th, 2024)</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
The evolution of AI/ML responses to the question "Why did the chicken cross the road?" over the past decade reflects significant advancements in language processing and understanding. Here's a detailed description of how these responses have evolved:
Early 2010s: Rule-Based Systems
Response Type: Traditional, rule-based
Example Answer: "To get to the other side."
Description: In the early 2010s, AI systems were primarily rule-based. These systems relied on pre-defined patterns and rules coded by developers. When asked "Why did the chicken cross the road?" the AI would output the standard punchline "To get to the other side" because it was programmed to recognize this classic joke and respond accordingly. The response lacked creativity and was limited to what was explicitly coded into the system.
Mid 2010s: Statistical and Early Machine Learning Models
Response Type: Basic machine learning-generated variations
Example Answer: "To find some food on the other side."
Description: As machine learning techniques began to develop, early models could generate responses based on statistical patterns in the data they were trained on. These models, such as simple neural networks or early natural language processing (NLP) algorithms, could create variations of the joke. However, the answers were still relatively straightforward and sometimes nonsensical, as these models had limited understanding of context and nuance.
Late 2010s: Advanced Neural Networks and NLP
Response Type: Context-aware and more sophisticated
Example Answer: "Because it wanted to explore the other side of the road."
Description: The late 2010s saw the rise of advanced neural networks and NLP (natural language processing) techniques, such as LSTM (Long Short-Term Memory) networks and early transformer models. These models had a better grasp of context and could generate more sophisticated responses. They could recognize the structure of the joke but also provide contextually appropriate variations that made sense and were more engaging.
Early 2020s: Transformer-Based Models
Response Type: Highly fluent and creative
Example Answer: "To escape from the boredom of farm life and seek adventure!"
Description: With the advent of transformer-based models like GPT-3 and BERT, AI's ability to generate human-like text improved dramatically. These models, trained on vast amounts of internet text, could produce responses that were not only fluent but also creative and contextually rich. They could understand humor better and generate answers that went beyond the traditional punchline, incorporating more complex wordplay or humor styles.
Present Day: LLMs and Personalization
Response Type: Personalized and contextually relevant
Example Answer: "Because its AI model predicted a 95% chance of finding a better grain on the other side."
Description: Today's LLMs, such as GPT-4, are capable of highly personalized and contextually aware responses. They can take into account previous conversational history, the user's preferences, and cultural context. The AI might generate responses that are tailored to the individual user, offering a wide range of humorous and inventive answers that reflect a deep understanding of language and context. This level of personalization and creativity was unimaginable a decade ago.</pre>			<p>As can be seen from the preceding example, transformers revolutionized NLP by enabling parallel processing and improving scalability and efficiency; however, prior to transformers, deep neural networks, such as CNNs and RNNs, dominated the DL field since being introduced <a id="_idIndexMarker031"/>in the 1980s. Here are some brief descriptions of these <span class="No-Break">neural networks:</span></p>
			<ul>
				<li><strong class="bold">CNNs</strong> are similar in functionality to how our visual cortex functions. Our brain processes images <a id="_idIndexMarker032"/>from the retina by using specialized neurons that handle specific types of information or features. Similarly, the different filters in a CNN can detect various sets of features in an image or dataset. To learn more, refer to the paper by Yann LeCun et al. <em class="italic">Backpropagation Applied to Handwritten Zip Code Recognition</em> (<a href="https://ieeexplore.ieee.org/document/6795724">https://ieeexplore.ieee.org/document/6795724</a>), presented in 1989. CNNs are still commonly used for <span class="No-Break">image analysis.</span></li>
				<li><strong class="bold">RNNs</strong> are commonly<a id="_idIndexMarker033"/> used for sequences of data, or time series data, to analyze patterns and potentially forecast future events, such as analyzing historical stock market data to predict future trade options. RNNs are also frequently used in NLP, as natural language is a sequence of words where the order matters and can significantly impact <span class="No-Break">the meaning.</span><p class="list-inset">The concept of RNNs was introduced in the 1986 paper by <em class="italic">David Rumelhart, Geoffrey Hinton, et al., Learning representations by back-propagating errors</em> <em class="italic">(</em><a href="https://www.nature.com/articles/323533a0">https://www.nature.com/articles/323533a0</a><em class="italic">)</em>. This seminal paper introduced the concept of the backpropagation algorithm, based on the gradient descent concept, which is an essential technology for training neural networks and has revolutionized the entire AI field. In <em class="italic">Appendix 1A</em>, we have included a brief <a id="_idIndexMarker034"/>mathematical introduction to RNNs and their <a id="_idIndexMarker035"/>popular variants, such as <strong class="bold">Long Short-Term Memory</strong> (<strong class="bold">LSTM</strong>) networks <a id="_idIndexMarker036"/>and <strong class="bold">Gated Recurrent </strong><span class="No-Break"><strong class="bold">Units</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">GRUs</strong></span><span class="No-Break">).</span></p></li>
			</ul>
			<p>In 2007, Fei-Fei Li, then a professor of Computer Science at Stanford University, started the ImageNet competition (<a href="https://www.image-net.org/">https://www.image-net.org/</a>), which included a massive dataset of images available on the internet that were labeled for training and testing. Every year, different AI/ML teams try to automate the prediction and improve the accuracy of their models using this <span class="No-Break">training dataset.</span></p>
			<p>Until 2011, the state-of-the-art technologies in the ImageNet competition were based on classical ML approaches, such<a id="_idIndexMarker037"/> as <strong class="bold">Support Vector Machines</strong> (<strong class="bold">SVMs</strong>), which tried to create an isolation plane between two different categories with the maximum margin in between. A<a id="_idIndexMarker038"/> breakthrough came with the introduction of AlexNet in 2012, developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. This paper (<a href="https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html</a>) won the ImageNet competition using deep CNNs and brought GPU programming to the forefront of <span class="No-Break">AIML development.</span></p>
			<ul>
				<li><strong class="bold">Transformers</strong>: In 2017, the transformer architecture was introduced by Vaswani et al. in their seminal paper <em class="italic">Attention Is All You Need</em> (<a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>). It revolutionized NLP by enabling parallel processing and improving the scalability and <a id="_idIndexMarker039"/>efficiency of NLP. <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>), developed by Google, significantly improved the understanding of context in language models. Since then, there have been lots of LLMs introduced by different companies, such as the GPT series by OpenAI and Claude <span class="No-Break">by Anthropic.</span></li>
			</ul>
			<p>Over the last two decades, ML has evolved from basic algorithmic models that were rule-based and <a id="_idIndexMarker040"/>dependent on manually curated features to advanced, context-aware models using deep learning frameworks such as neural networks and transformers. Let’s take a closer look at the transformer <span class="No-Break">architecture now.</span></p>
			<h1 id="_idParaDest-20"><a id="_idTextAnchor019"/>Transformer architecture</h1>
			<p>A <strong class="bold">transformer model</strong> uses an <strong class="bold">encoder-decoder</strong> architecture, where<a id="_idIndexMarker041"/> the encoder maps the<a id="_idIndexMarker042"/> input<a id="_idIndexMarker043"/> sequences/tokens through a self-attention mechanism. This mapped data is used by the decoder to generate the output sequence. The mapping of input tokens retains not only their intrinsic values but also their context and weight in the original sequence. Let’s go through some key aspects of the encoder architecture in the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B31108_01_03.jpg" alt="Figure 1.3 – Transformer architecture from the Attention Is All You Need paper" width="736" height="901"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – Transformer architecture from the Attention Is All You Need paper</p>
			<p>Here is a breakdown of the concepts highlighted in <span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="bold">Input embeddings</strong>: Marked as <strong class="bold">1</strong> in the figure, this is a key part of the transformer model, which<a id="_idIndexMarker044"/> converts input sequences/tokens into high-dimensional vector embeddings. In real-world applications, output embeddings from a trained model may be stored in high-dimensional vector databases, such as Elasticsearch, Milvus, or PineCone. Vector databases help to find similar searches in high-dimensional space using either Euclidian distance or cosine similarity, and similar objects are assigned closer to each other in this high-dimensional vector space, as shown in the <span class="No-Break">following figure.</span></li>
			</ul>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B31108_01_04.jpg" alt="Figure 1.4 – Assignment of similar objects in high-dimensional space" width="551" height="492"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – Assignment of similar objects in high-dimensional space</p>
			<ul>
				<li><strong class="bold">Positional encoding</strong>: Positional encoding, marked as <strong class="bold">2</strong> in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.2</em>, provides information about the order of tokens in an input sequence. Unlike RNNs, which have the knowledge of time step <em class="italic">t</em> or the notion of the sequence, transformer models rely on self-attention mechanisms and lack awareness of intrinsic token order. Positional<a id="_idIndexMarker045"/> encoding injects sequential information into the <span class="No-Break">token embeddings.</span></li>
			</ul>
			<p>For example, if the input sequence is <strong class="source-inline">The Brown hat</strong>, here is how the mechanism <span class="No-Break">would work:</span></p>
			<ul>
				<li><strong class="bold">Token embeddings</strong>: Each word in the sentence is converted into a vector representation (embedding). Let’s say we have these simple embeddings <span class="No-Break">for demonstration:</span><ul><li><strong class="source-inline">The</strong> -&gt; [<span class="No-Break">0.1, 0.2]</span></li><li><strong class="source-inline">Brown</strong> -&gt; [<span class="No-Break">0.3, 0.4]</span></li><li><strong class="source-inline">hat</strong> -&gt; [<span class="No-Break">0.5, 0.6]</span></li></ul></li>
				<li><strong class="bold">Positional encoding vectors</strong>: We generate positional encodings for each position in the sentence. Transformer models typically use sinusoidal functions for positional encoding, however for illustration purposes, let’s use the <span class="No-Break">following example:</span><ul><li><strong class="bold">Position 0</strong>: [<span class="No-Break">0.01, 0.02]</span></li><li><strong class="bold">Position 1</strong>: [<span class="No-Break">0.03, 0.04]</span></li><li><strong class="bold">Position 2</strong>: [<span class="No-Break">0.05, 0.06]</span></li></ul></li>
				<li><strong class="bold">Adding positional encodings to token embeddings</strong>: We add the positional encoding<a id="_idIndexMarker046"/> vectors to the token embeddings to incorporate <span class="No-Break">positional information:</span><ul><li><strong class="source-inline">The</strong> + Position 0: [0.1, 0.2] + [0.01, 0.02] = [<span class="No-Break">0.11, 0.22]</span></li><li><strong class="source-inline">Brown</strong> + Position 1: [0.3, 0.4] + [0.03, 0.04] = [<span class="No-Break">0.33, 0.44]</span></li><li><strong class="source-inline">hat</strong> + Position 2: [0.5, 0.6] + [0.05, 0.06] = [<span class="No-Break">0.55, 0.66]</span></li></ul></li>
			</ul>
			<p class="list-inset">Now, each word has a unique vector that includes both the word’s meaning and its position in <span class="No-Break">the sentence.</span></p>
			<ul>
				<li><strong class="bold">Multi-head attention mechanism</strong>: In a multi-head attention mechanism, marked as <strong class="bold">3</strong> in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.2</em>, the model calculates the query, key, and value vectors for every I/P sequence and each attention head. A possible analogy for thinking about the multi-head attention mechanism is a team of detectives solving a very complex case, where each detective is an attention head. Every attention head has its own query vectors, which is an aspect of the case that a detective is focused upon, such as motive for the crime, weapon used, and so on. Finally, the key is the clues or pieces of evidence related to that given motive. So, if the query is the weapon used, the detective would investigate the crime scene for anything that could be used as a weapon, and the value would be the insights gained from each bit of evidence (e.g., fingerprints on a weapon). By having multiple attention heads, a model can focus on different parts of the input simultaneously, capturing various patterns or dependencies in the data, such as word meanings, words’ relative positions, and sentence structures, similar to our analogy, where different detectives are focusing on different aspects of the case. Besides that, multi-head attention also allows the parallel processing of inputs, making it highly efficient and speeding up both training and inference by distributing them across <span class="No-Break">different devices.</span><p class="list-inset">In <em class="italic">Appendix 1B</em>, we cover the mathematical model and complexities of key-value pairs, as well as feed-forward networks and the concept of temperature in <span class="No-Break">transformer models.</span></p></li>
			</ul>
			<p>Now that we<a id="_idIndexMarker047"/> understand how transformer architecture has revolutionized DL by using an encoder-decoder framework that relies on self-attention mechanisms to enable the parallel processing of input data, let’s explore a typical GenAI project <span class="No-Break">life cycle.</span></p>
			<h1 id="_idParaDest-21"><a id="_idTextAnchor020"/>GenAI project life cycle</h1>
			<p>Enterprise spending on <a id="_idIndexMarker048"/>GenAI projects has been growing exponentially since 2023, with c-suite executives planning to spend even more on GenAI projects (<a href="https://www.gartner.com/en/newsroom/press-releases/2023-10-11-gartner-says-more-than-80-percent-of-enterprises-will-have-used-generative-ai-apis-or-deployed-generative-ai-enabled-applications-by-2026">https://www.gartner.com/en/newsroom/press-releases/2023-10-11-gartner-says-more-than-80-percent-of-enterprises-will-have-used-generative-ai-apis-or-deployed-generative-ai-enabled-applications-by-2026</a>). However, there is growing concern about how to quantify <a id="_idIndexMarker049"/>the <strong class="bold">Return on Investment</strong> (<strong class="bold">ROI</strong>) of these efforts, such as revenue impact, efficiency, and accuracy gains. Moving forward, ROI will become a critical part of the conversation, as enterprises look for new GenAI projects. So, before starting a new GenAI project, it is recommended to think about the entire project life cycle. In this section, we will be covering the project <span class="No-Break">life cycle.</span></p>
			<p>Let’s first look at the following figure, which outlines the end-to-end GenAI project life cycle, starting from defining business objectives, or KPIs. This is followed by selecting or training FMs and optimizing them using techniques such as fine tuning and prompt tuning. Then, the model is <a id="_idIndexMarker050"/>evaluated, deployed, and continuously monitored to ensure that business goals <span class="No-Break">are met.</span></p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B31108_01_05.jpg" alt="Figure1.5 – GenAI project life cycle" width="1586" height="742"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure1.5 – GenAI project life cycle</p>
			<p>Let’s take a closer look at each stage of the GenAI project <span class="No-Break">life cycle:</span></p>
			<ul>
				<li><strong class="bold">Business objectives and FM selection</strong>: The <a id="_idIndexMarker051"/>GenAI project life cycle starts with the business objective/problem statement we are trying to solve with GenAI. Business objectives could be increasing customer conversion or retention, creating personalized marketing campaigns, or creating a customer chatbox using enterprise <span class="No-Break">internal data:</span><ul><li><strong class="bold">Critical KPIs</strong>: After the use case, the next consideration is business-critical KPIs, such as cost per inference. For example, if we are creating a personalized marketing campaign, we should ensure that the cost per customization is less than the product of the <strong class="bold">Lifetime Value</strong> (<strong class="bold">LTV</strong>) of the customer and the probability of <a id="_idIndexMarker052"/>customer conversion. If the cost of inference is more than the business value the project is expected to deliver, the project might not have long-term sustainability. Other KPIs to think about could include latency, as sub-second response times might be needed for certain use cases or throughput, if a certain number of tokens is expected <span class="No-Break">per second.</span></li><li><strong class="bold">Selecting and training a FM</strong>: The next step is to either pick an existing FM or train a new one. Training a new FM could be extremely resource intensive and cost billions of dollars and significant time, so in the majority of applications, it is better to pick an existing FM and do domain-specific optimization. When picking an existing model, you can select either an open source model or a proprietary model that can be accessed through web interfaces or over APIs, such as Claude or ChatGPT. It is always a good idea to check the licensing terms of these models to ensure that they meet application<a id="_idIndexMarker053"/> requirements and can scale with your <span class="No-Break">enterprise needs.</span></li></ul></li>
				<li><strong class="bold">Model optimization</strong>: Once the FM is selected, the next step is to optimize the model for the business <a id="_idIndexMarker054"/>use case using techniques such as fine tuning, prompt tuning, Reinforcement Learning from Human Feedback (RLHF), <span class="No-Break">and DPO:</span><ul><li><strong class="bold">Fine tuning</strong>: In fine tuning, a domain-specific labeled dataset with prompt and completion<a id="_idIndexMarker055"/> data is used to train the model for a specific domain or set of domains. Since all model weights can be updated in fine tuning, the compute requirements for fine tuning are similar to those for full training. However, the training time is smaller, as we are training the model on a smaller dataset. To reduce the training resources, there are options such as <strong class="bold">Performance-Efficient Fine Tuning</strong> (<strong class="bold">PEFT</strong>), which<a id="_idIndexMarker056"/> only updates a small set of weights and thus reduces the compute requirements. <strong class="bold">Low Rank Adoption</strong> (<strong class="bold">LoRA</strong>) is<a id="_idIndexMarker057"/> a very popular form of PEFT, where the original model matrix is reparametrized using a low-rank representation to significantly reduce the number of model parameters to be updated. As an example, in Vaswani’s paper, each attention head has the dimensionality [512, 64], that is, 32,768 parameters to train per head. With LoRA, we train much smaller weight matrices to offer 80% or higher weight reduction. We will cover this topic in detail in <a href="B31108_04.xhtml#_idTextAnchor049"><em class="italic">Chapter 4</em></a>. Another version of LoRA is <strong class="bold">QLoRA </strong>(<strong class="bold">Quantized Lower Rank Adoption</strong>). In QLoRA, we use quantization to <a id="_idIndexMarker058"/>compress the model weights <a id="_idIndexMarker059"/>from 32-bit precision to 8-bit or 4-bit precision, which dramatically reduces the model size and makes it easier to run on GPUs with <span class="No-Break">less memory.</span></li><li><strong class="bold">Prompt tuning</strong> is a low-cost <a id="_idIndexMarker060"/>technique where soft prompt tokens are added to the input query to optimize model performance for domain-specific tasks. These tokens are optimized as the model is retrained on the labeled domain-specific data. Unlike traditional fine tuning, which adjusts the model’s parameters across numerous training iterations, prompt tuning focuses on refining the prompts to guide the model <span class="No-Break">more effectively.</span></li><li>In <strong class="bold">RLHF</strong> (<a href="https://huggingface.co/blog/rlhf">https://huggingface.co/blog/rlhf</a>), human<a id="_idIndexMarker061"/> feedback is used to train LLMs to align with human preferences using RL techniques. In this approach, the<a id="_idIndexMarker062"/> LLM generates multiple responses to a variety of prompts, which humans evaluate and rank based on criteria such as accuracy, relevance, and ethical standards. Using the ranked responses, a reward model is trained to predict the human preference score for any given LLM response, and the LLM is fine-tuned using RL algorithms guided by the developed <span class="No-Break">reward model.</span></li><li>In <strong class="bold">Direct Preference Optimization</strong> (<strong class="bold">DPO</strong>) (<a href="https://huggingface.co/papers/2305.18290">https://huggingface.co/papers/2305.18290</a>), a <a id="_idIndexMarker063"/>policy is trained with a simple classification objective to best align with human preferences without using RL. Similar to RLHF, multiple outputs are generated by the LLM for a set of input prompts. Human evaluators compare these outputs in pairs and indicate which output they prefer. This creates a dataset of preference pairs, such as (Ap, Anp), where Ap indicates a preferred answer and Anp indicates a not-preferred answer. A loss function is then designed to maximize the probability that the preferred output is ranked higher than the less-preferred output as the LLM output. The model parameters are optimized to minimize this loss function over all collected preference pairs. This directly tunes the model to produce outputs that are more aligned with <span class="No-Break">human preferences.</span></li><li><strong class="bold">Evaluation</strong>: After the models are trained, you need to evaluate them for accuracy using metrics such as <em class="italic">ROUGE</em> (<a href="https://huggingface.co/spaces/evaluate-metric/rouge">https://huggingface.co/spaces/evaluate-metric/rouge</a>) or <em class="italic">BLEU</em> (<a href="https://huggingface.co/spaces/evaluate-metric/bleu">https://huggingface.co/spaces/evaluate-metric/bleu</a>), based on the <span class="No-Break">use case:</span><ul><li><strong class="bold">Bilingual Evaluation Understudy</strong> (<strong class="bold">BLEU</strong>) measures how closely machine-generated <a id="_idIndexMarker064"/>text matches a set of reference texts. This metric was originally designed for machine translation and is also commonly used to evaluate text generation tasks, such<a id="_idIndexMarker065"/> as <strong class="bold">n-grams</strong> (contiguous sequences of <em class="italic">n</em> words) in generated text against <span class="No-Break">reference texts.</span></li><li><strong class="bold">Recall-Oriented Understudy for Gisting Evaluation</strong> (<strong class="bold">ROUGE</strong>) is a set of metrics<a id="_idIndexMarker066"/> used to evaluate the quality of summaries and translations <a id="_idIndexMarker067"/>generated by models. It compares the overlap between the generated text and a set of reference texts. ROUGE metrics are particularly popular in evaluating the performance of <span class="No-Break">summarization systems.</span></li></ul></li></ul></li>
			</ul>
			<p>Besides those metrics, developers should also evaluate generated responses against the <strong class="bold">Honesty, Harmlessness, and Helpfulness</strong> (<strong class="bold">3H</strong>) metric and ensure that training data is free of<a id="_idIndexMarker068"/> biases and <span class="No-Break">harmful content.</span></p>
			<ul>
				<li><strong class="bold">Deployment optimization</strong>: Once the model training/fine tuning is complete, the next step is to explore options to reduce the model size and optimize it for latency and cost. Some possible options are quantization, distillation, <span class="No-Break">and pruning:</span><ul><li><strong class="bold">Quantization</strong>: In quantization, we can explore model precision tradeoffs, such as changing model weights from FP32 (floating point, 32 bit), which requires 4 bytes of memory per parameter, to FP16 or Bfloat16, which requires 2 bytes of memory per parameter, or even Int 8, which requires only 1 byte per parameter. So, by moving from FP32 to Int8, we could reduce memory requirements by 4X; however, that could impact model accuracy. So, we need to evaluate model performance/accuracy to see whether these trade-offs <span class="No-Break">are acceptable.</span></li><li><strong class="bold">Distillation</strong>: In distillation, we can train a student model with a smaller size than the original model by minimizing the distillation loss. The goal of distillation is to create a student model that approximates the performance of the teacher model, while being more efficient in terms of computational resources, such as memory and <span class="No-Break">inference time.</span></li><li><strong class="bold">Pruning</strong>: In pruning, we try to reduce the size and complexity of a model by removing less important parameters, such as weights close to zero, while maintaining/improving the model’s performance. The main goal of pruning is to create a more efficient model that requires fewer computational resources for inference and training without significantly <span class="No-Break">affecting accuracy.</span></li></ul></li>
				<li><strong class="bold">Deployment options</strong>: Once the model is ready, the next choice is to pick the deployment<a id="_idIndexMarker069"/> options, such as cloud, on-premises, or hybrid approach. The selection depends on criteria such as hardware availability, cost, CapEx versus OpEx, and data residency requirements. Usually, cloud deployment offers the most simplicity due to the managed services and scalability; however, there could be data  residency requirements, requiring either on-premises or <span class="No-Break">hybrid deployments.</span></li>
				<li><strong class="bold">Inference size improvements</strong>: After the model is ready and deployed, we can focus on imporving results on the inference side. Two options for this are <strong class="bold">Retrieval Augmentation Generation</strong> (<strong class="bold">RAG</strong>) and <span class="No-Break"><strong class="bold">prompt engineering</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">RAG</strong>: In RAG, we<a id="_idIndexMarker070"/> provide the model with relevant information and documents besides the prompt to fix knowledge cutoff issues. This technique provides real-time information within the prompt without the need for continuous model training, thus reducing the overall training costs. RAG combines the capabilities of retrieval-based models and generation-based models to improve the quality of text generation tasks. We will discuss RAG implementation in detail in <a href="B31108_04.xhtml#_idTextAnchor049"><span class="No-Break"><em class="italic">Chapter 4</em></span></a><span class="No-Break">.</span></li><li><strong class="bold">Prompt engineering</strong>: In prompt<a id="_idIndexMarker071"/> engineering, we provide the model with some examples or context while asking a question. It could be zero-shot learning, where the model is given direct instructions without examples, such as <strong class="source-inline">summarize this text</strong>. In few-shot learning, the user includes a few examples within the input prompt, which helps the model learn the desired <a id="_idIndexMarker072"/>output format <span class="No-Break">and style.</span></li></ul></li>
			</ul>
			<p>Once the model is deployed, we need to keep monitoring it to ensure that the model’s results don’t drift or get stale over time. We will explore model monitoring, performance, and drift in detail in <a href="B31108_11.xhtml#_idTextAnchor145"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><span class="No-Break">.</span></p>
			<p>In this section, we looked at the various stages of the GenAI project life cycle. It starts with defining the business objectives and KPIs, which is followed by model selection and various fine-tuning techniques. We evaluate model accuracy using metrics such as BLEU and ROGUE, and finally we deploy and continuously optimize the model. Now, let’s look at the various layers of the GenAI deployment stack for <span class="No-Break">deploying models.</span></p>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/>GenAI deployment stack</h1>
			<p>As we discuss GenAI application<a id="_idIndexMarker073"/> development and deployment over Kubernetes, it is a good idea to understand the entire deployment stack, which can help us to think about the right infrastructure, orchestration platform, and libraries. The following figure shows the various layers of the GenAI deployment stack, from the foundational infrastructure layer comprising compute, storage, and networking through to the orchestration, tools, and <span class="No-Break">deployment layers.</span></p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B31108_01_06.jpg" alt="" width="1034" height="1079"/>
				</div>
			</div>
			<p class="IMG---Figure">Figure1.6 – Deployment stack for GenAI applications</p>
			<p>Let’s a closer look<a id="_idIndexMarker074"/> at each of <span class="No-Break">these layers:</span></p>
			<ul>
				<li><strong class="bold">Infrastructure layer</strong>: We will start from the foundation layer of the stack and move upward. The foundation of this stack is the infrastructure layer, which covers compute, networking, and <span class="No-Break">storage options:</span><ul><li><strong class="bold">Compute</strong>: For compute, we can use options such as CPUs, GPUs, custom accelerators, or a combination of these. As explained previously, LLMs are very computationally intensive. GPUs offer massively parallel matrix multiplication capabilities and are mostly favored for training workloads. For inference, both CPUs and GPUs are used, but for LLMs with billions of model parameters, GPUs are often needed for inference as well. Besides CPUs and GPUs, there are custom accelerators, such as AWS Inferentia and Trainium, which are custom silicon chips specially designed for ML and highly optimized for <span class="No-Break">mathematical operations.</span></li><li><strong class="bold">Networking</strong>: Networking is the next critical infrastructure component. For language models that are very large, both training and inference could become a distributed<a id="_idIndexMarker075"/> system problem. To explain this, let’s look at recent LLM <span class="No-Break">model trends:</span></li></ul></li>
			</ul>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Year</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Model</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Model Size (in </strong><span class="No-Break"><strong class="bold">billion parameters)</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">2018</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">BERT-L</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.34</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">2019</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">T5-L</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.77</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">2019</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT2</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">1.5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">2020</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT3</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">175</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">2023</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT4</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Trillions</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.1 – Evolution of GenAI model sizes</p>
			<p class="list-inset">It’s evident here that GenAI models are growing exponentially, and more parameters generally mean a more complex model that can capture more intricate patterns in data, thus requiring more computational resources for training <span class="No-Break">and inference.</span></p>
			<p class="list-inset">To clarify, if we say a model has 1 billion parameters, it usually refers to model weights after training has been completed. However, for training these models, we need the model weights, gradients, Adam Optimizers, activations terms, and some temporary variables throughout the training <span class="No-Break">epochs (</span><a href="https://huggingface.co/docs/transformers/v4.33.3/perf_train_gpu_one#batch-size-choice"><span class="No-Break">https://huggingface.co/docs/transformers/v4.33.3/perf_train_gpu_one#batch-size-choice</span></a><span class="No-Break">).</span></p>
			<p class="list-inset">So, during training, we might need to store up to <em class="italic">six parameters</em> per training weight, which could take up to 24 bytes at FP32 precision, 12 bytes at FP16 (or Bfloat16) precision, or 6 bytes at FP8 or Int8. Actual precision depends on the accuracy requirements versus the <span class="No-Break">training cost.</span></p>
			<p class="list-inset">Now, let’s say we are training or fine tuning a 70B-parameter model, such as Llama3. It would require 840 GB (70B*12 bytes) of memory to store all the model weights and temporary variables with Bfloat16 or FP16 for precision. If we are using the latest NVIDIA H200 GPUs, which were introduced in 2024 and offer up to 141 GB of memory, we would still need about six GPUs to store these model parameters during training or full fine tuning, assuming the model is <span class="No-Break">fully sharded.</span></p>
			<p class="list-inset">In reality, the actual GPU<a id="_idIndexMarker076"/> count could be higher, if we would like to train these models in a reasonable time. This explains that East-West traffic, that is, the traffic flowing within the data center nodes or GPUs, could become a performance bottleneck for large model training or fine tuning. For this reason, non-blocking networking technologies such as <strong class="bold">memory coherence</strong> and <strong class="bold">Remote Direct Memory Access</strong> (<strong class="bold">RDMA</strong>) could help scale performance <a id="_idIndexMarker077"/>across nodes. RDMA is a technology that lets nodes in a distributed <a id="_idIndexMarker078"/>system access the memory of other nodes without involving either the core processor or operating system. Similarly, memory coherence technology ensures that all the caches in the system have up-to-date memory information and that write operations by one node to memory are visible to all the nodes/caches that are connected coherently. These two technologies result in lower latency and increased throughput for distributed training and <span class="No-Break">fine-tuning problems.</span></p>
			<ul>
				<li><strong class="bold">Storage</strong>: After networking, the next infrastructure choice is storage, with choices such as block storage, file storage, or Lustre. In a <strong class="bold">block storage system</strong>, such<a id="_idIndexMarker079"/> as Amazon S3, data is stored in data blocks. These block sizes range from 512 bytes to 64 KB, and multiple blocks can be accessed in parallel, which could provide higher I/O bandwidth. In a <strong class="bold">file storage</strong> system, data<a id="_idIndexMarker080"/> is stored in files and directories, making it easier to manage and structure the data. However, a file storage system creates an additional overhead of managing the file hierarchy and metadata. <strong class="bold">Lustre</strong> is a <a id="_idIndexMarker081"/>popular storage system that is now gaining traction for GenAI applications and has been in use for quite some time in high-performance computing. Lustre provides a massively parallel file storage system and can be scaled horizontally by adding <span class="No-Break">more resources.</span><p class="list-inset">If you are planning to store data in databases, you might choose SQL for fixed schema data or NoSQL databases for unstructured data, such as images <span class="No-Break">and videos.</span></p></li>
				<li><strong class="bold">Compute unit</strong>: After infrastructure, the next thing is to select the compute unit. The possible options are <strong class="bold">bare-metal machines</strong>, <strong class="bold">Virtual Machines</strong> (<strong class="bold">VMs</strong>), or <strong class="bold">containers</strong>. Containers <a id="_idIndexMarker082"/>pack all software <a id="_idIndexMarker083"/>dependencies, such <a id="_idIndexMarker084"/>as language runtimes and libraries, into an image, and multiple containers can share the same node/kernel. This allows much tighter bin packing or resource utilization as<a id="_idIndexMarker085"/> compared to VMs. For VMs, each VM requires a separate OS and runtime before the application can be deployed. Bare-metal machines are physical servers that are dedicated to a single tenant or customer, unlike VMs, which run on shared physical servers through <span class="No-Break">a hypervisor.</span></li>
				<li><strong class="bold">Orchestration platform</strong>: After compute unit selection, we choose the orchestration platform to manage the underlying infrastructure life cycle. This orchestration platform needs to be capable of scaling up or down based on workload demand and should be able to withstand network outages or hardware <a id="_idIndexMarker086"/>failures. <strong class="bold">Kubernetes</strong> (<strong class="bold">K8s</strong>) has emerged as a leading orchestration platform for containers and will be the primary focus in this book, as lots of leading companies, such as OpenAI (<a href="https://openai.com/research/scaling-kubernetes-to-7500-nodes">https://openai.com/research/scaling-kubernetes-to-7500-nodes</a>) and Anthropic, are using it for their GenAI workload orchestration. OpenStack is an open source orchestration platform <span class="No-Break">for VMs.</span></li>
				<li><strong class="bold">Frameworks</strong>: After the orchestration platform comes the AI framework, such as TensorFlow or PyTorch. PyTorch<strong class="bold"> Fully Sharded Data Parallel </strong>(<strong class="bold"> FSDP</strong> <strong class="bold"> </strong>) and TensorFlow <a id="_idIndexMarker087"/>distributed libraries allow model parameters and training data to be distributed across multiple GPUs and help the system to scale for large <span class="No-Break">model sizes.</span></li>
				<li><strong class="bold">IDEs</strong>: After orchestration, the next selection is what <strong class="bold">Integrated Development Environment</strong> (<strong class="bold">IDE</strong>) to use, such <a id="_idIndexMarker088"/>as JupyterHub, as well as what libraries to use, such as cuDNN, NumPy, or pandas, based on the infrastructure selection <span class="No-Break">made earlier.</span></li>
				<li><strong class="bold">Endpoints</strong>: Finally, the user can select the final deployment endpoint, such as offering models as an API, a platform, or a workload. Considering this stack for scalability, cost, latency, and disaster recovery could help users avoid very expensive rearchitecting once application use starts to scale. In our experience, data science teams often pick a simple architecture for proofs of concept, but these implementations sometimes don’t scale well and are very expensive <span class="No-Break">during deployment.</span></li>
			</ul>
			<p>In this section, we discussed various layers of the GenAI deployment stack, from the foundational infrastructure layer to the high-level abstraction layer. We also looked at the challenges these <a id="_idIndexMarker089"/>models present as model sizes grow and the different ways to solve them. Let’s take a look at GenAI use cases <span class="No-Break">across industries.</span></p>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor022"/>GenAI use cases</h1>
			<p>GenAI is<a id="_idIndexMarker090"/> transforming all industries. As per McKinsey (<a href="https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier#key-insights">https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier#key-insights</a>), it is <a id="_idIndexMarker091"/>expected to add trillions of dollars to the economy by 2030. The following are some of the industry verticals affected and use cases. It is by no stretch a comprehensive list, as the list of applications is growing very rapidly. </p>
			<p>However, learning about some of these <a id="_idIndexMarker092"/>will give you an idea of the potential <span class="No-Break">GenAI carries:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Retail/e-commerce</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Product design</strong>: GenAI can be used with every stage of product design, such as summarizing market and user research, creating visuals/animations of possible product options, refining concepts based on user feedback, and creating product descriptions and <span class="No-Break">marketing campaigns.</span></li><li><strong class="bold">Personalized recommendation</strong>: GenAI can enable more natural engagement for e-commerce websites. Users can ask in natural language for product recommendations, such as <strong class="source-inline">the best running shoes under $100 with red stripes</strong>. GenAI can comprehend the user’s request and provide <span class="No-Break">recommendations accordingly.</span></li><li><strong class="bold">Review summarization</strong>: GenAI can help summarize user reviews and overall sentiments, so users don’t have to read through all the <span class="No-Break">different reviews.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Finance</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Financial reporting/analysis</strong>: GenAI can help create financial reports and summaries based on the data and <span class="No-Break">trend analysis.</span></li><li><strong class="bold">Customer service</strong>: GenAI can help with personalized marketing and chatbots to address user questions based on <span class="No-Break">their data.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Healthcare</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Drug discovery</strong>: GenAI models can predict how different drugs interact with various biological targets (proteins, enzymes, etc.) by analyzing existing interaction data. This can help in identifying promising drug candidates more efficiently. They can also propose novel molecular structures that have the potential to be effective drugs. These models can be further optimized for properties such as binding affinity, bioavailability, <span class="No-Break">and toxicity.</span></li><li><strong class="bold">Personalized medicine</strong>: LLMs can integrate and analyze diverse types of patient data, such as medical records, lab results, imaging data, and genetic information, to create comprehensive patient profiles and identify genetic biomarkers that predict a patient’s response to <span class="No-Break">particular treatments.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Education</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Personalized learning</strong>: GenAI can help develop personalized learning paths, based on user journeys <span class="No-Break">and responses.</span></li><li><strong class="bold">New language learning</strong>: GenAI can help create personalized content and <span class="No-Break">conversation examples.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Legal</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Document review and summarization</strong>: GenAI can help automate the review and summarization of legal documents and prior similar cases for <span class="No-Break">legal precedents.</span></li><li><strong class="bold">Contract generation</strong>: GenAI can help create legal contracts based on the <span class="No-Break">parameters provided.</span></li><li><strong class="bold">Customer interaction</strong>: GenAI can help in developing chatbots for client inquiries <span class="No-Break">and support.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Entertainment</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Content creation</strong>: GenAI can help create scripts, music, artwork, <span class="No-Break">and characters.</span></li><li><strong class="bold">Virtual reality</strong>: GenAI can help create immersive VR environments <span class="No-Break">and experiences.</span></li><li><strong class="bold">Personalized content</strong>: GenAI can help in tailoring entertainment content and recommendations to <span class="No-Break">individual preferences.</span></li></ul></li>
			</ul>
			<p>In this section, we looked at<a id="_idIndexMarker093"/> various GenAI use cases across different industries, from enhancing customer experience in retail to drug discovery and personalized medicine in healthcare. This is not an exhaustive list and it continues to grow as technology and <span class="No-Break">models evolve.</span></p>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor023"/>Summary</h1>
			<p>In this chapter, we covered the differences between AI and GenAI. AI is a very broad term that refers to technologies that enable machines to emulate human intelligence and encompasses a broad range of applications, including GenAI, whereas GenAI specifically focuses on creating new content, such as text, images, <span class="No-Break">and videos.</span></p>
			<p>We then looked at the evolution of ML, understanding its progression from CNNs/RNNs to the transformer architecture introduced in 2017. Transformers have revolutionized AI with their ability to process sequences of data efficiently, making them fundamental to many GenAI applications, particularly <span class="No-Break">in NLP.</span></p>
			<p>The chapter also outlined the life cycle of a GenAI project, which includes business objectives and KPIs, foundational model selection, model training, evaluation, and deployment. Each stage is critical, with continuous iterations based on <span class="No-Break">performance feedback.</span></p>
			<p>Finally, the chapter covered various use cases of GenAI across different sectors, including retail/e-commerce, finance, healthcare, and legal, that can leverage GenAI for summarization, recommendation, and personalization. This exploration underscores the versatile and transformative potential of GenAI in augmenting human creativity and transforming our day-to-day lives. In the next chapter, we will introduce the concepts of containers, K8s, and cover how K8s can manage the deployment, scaling, and operations of containerized workloads. We will also cover the specific advantages of using K8s in GenAI projects and what makes it attractive for <span class="No-Break">GenAI applications.</span></p>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor024"/>Appendix 1A – RNNs</h1>
			<p>In this section, we will provide a basic overview of how RNNs work, including a mathematical explanation of their functionality. RNNs handle sequential data by maintaining a hidden state (or memory cell) that can capture information from previous time steps. The following is a very simplistic and mathematical representation of <span class="No-Break">an RNN.</span></p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B31108_01_07.jpg" alt="Figure 1.7 – Simple representation of an RNN" width="475" height="416"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – Simple representation of an RNN</p>
			<p>In this figure, <strong class="bold">ht</strong> represents the hidden state at a given time step <strong class="bold">t</strong> and can be <span class="No-Break">presented as:</span></p>
			<p><strong class="bold">ht = f(wh * ht-1  +wx *</strong><span class="No-Break"><strong class="bold">Xt )</strong></span></p>
			<p>Where <strong class="bold">wh</strong> is the weight for the hidden stage and <strong class="bold">ht-1</strong> is the output of this hidden stage at step <strong class="bold">t-1</strong>. <strong class="bold">Xt</strong> is the input, <strong class="bold">wx</strong> is the weight of the input stage, and <strong class="bold">f</strong> is the <span class="No-Break">activation function.</span></p>
			<p><strong class="bold">Output Yt = wy * ht + </strong><span class="No-Break"><strong class="bold">by</strong></span></p>
			<p>In RNNs, the output at time <strong class="bold">t</strong> depends upon the hidden stage, which includes the weighted output of the prior steps, such as <strong class="bold">ht-1</strong>, <strong class="bold">ht-2</strong>, and so on. This architecture can process inputs of any length, and the model size does not increase with the size of the input. However, this implementation is sequential in nature and can’t be accelerated beyond a point through <span class="No-Break">parallel processing.</span></p>
			<p>There are four different types of <span class="No-Break">RNN topologies:</span></p>
			<ul>
				<li><strong class="bold">Sequence-to-sequence RNN</strong>: For this RNN topology, both input and output are a sequence, such as stock <span class="No-Break">market analysis</span></li>
				<li><strong class="bold">Sequence-to-vector RNN</strong>: This involves examples such as sentiment analysis by analyzing a statement <span class="No-Break">or text</span></li>
				<li><strong class="bold">Vector-to-sequence analysis</strong>: This includes practical scenarios, such as creating a caption from <span class="No-Break">an image</span></li>
				<li><strong class="bold">Encoder-to-decoder</strong>: This can be used for machine translation from one language <span class="No-Break">to another</span></li>
			</ul>
			<p>Significant advancements in RNNs came with the introduction of LSTM networks. LSTM networks addressed the problem of vanishing and exploding gradients in standard RNNs, making it possible to learn long-range dependencies in sequences more effectively. LSTM cells maintain separate short-term and long-term states. GRU is another optimization over LSTM and gives better <span class="No-Break">training performance.</span></p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor025"/>Appendix 1B – Transformer mathematical models for the self-attention mechanism</h1>
			<p>In this section, we will provide a basic overview of how the transformer model works, including a mathematical explanation of its functionality. We discussed the concepts of queues, keys, and values as part of transformer analysis earlier in this chapter. For a given attention head <strong class="bold">i</strong>, the following are the query, key, and <span class="No-Break">value vectors:</span></p>
			<p><strong class="bold">Q= </strong><span class="No-Break"><strong class="bold">X* Wi</strong></span><span class="No-Break"><span class="superscript">Q</span></span></p>
			<p><strong class="bold">K= </strong><span class="No-Break"><strong class="bold">X* Wi</strong></span><span class="No-Break"><span class="superscript">K</span></span></p>
			<p><strong class="bold">V= </strong><span class="No-Break"><strong class="bold">X* Wi</strong></span><span class="No-Break"><span class="superscript">V</span></span></p>
			<p>Where W<span class="subscript">i</span><span class="superscript">Q</span>, W<span class="subscript">i</span><span class="superscript">K</span>, and W<span class="subscript">i</span><span class="superscript">V </span>are the weight vectors for the attention head <strong class="bold">i</strong> for the query, key, and values. These weights are the parameters that we optimize as we train <span class="No-Break">the model.</span></p>
			<p>To understand the computational complexity of these calculations, let’s look over the dimensionality of <span class="No-Break">these vectors:</span></p>
			<ul>
				<li>X= [n, d<span class="subscript">model</span>], where <em class="italic">n</em> is the number of tokens in the input sequence and d<span class="subscript">model </span>is the dimensionality of the <span class="No-Break">multi-dimensional space.</span></li>
				<li><strong class="bold">Weight vectors</strong>: W<span class="subscript">i</span><span class="superscript">Q </span>W<span class="subscript">i</span><span class="superscript">k </span>W<span class="subscript">i</span><span class="superscript">v </span><span class="subscript">= </span>[d<span class="subscript">model </span>,d<span class="subscript">k </span>], where d<span class="subscript">k </span>=d<span class="subscript">model</span> / # of <span class="No-Break">attention heads</span></li>
			</ul>
			<p>In the<em class="italic"> Attention Is All You Need</em> paper, d<span class="subscript">model </span>was 512 and the number of attention heads was 8, so d<span class="subscript">k </span>=<span class="No-Break">512/8 =64.</span></p>
			<p>During a forward pass, each of the Q, K, and V vector calculations would require ~262,144 multiplications (8*64*512) and 261,632 additions (8*64*511). During training, model goes through multiple forward and backward passes for each head across multiple sets of training data. This explains the complexity and compute resource requirements for these <span class="No-Break">transformer models.</span></p>
			<p>For each attention head, the attention score is calculated with the <span class="No-Break">following equation:</span></p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B31108_01_08.jpg" alt="" width="295" height="50"/>
				</div>
			</div>
			<p>After computing the attention outputs for each head, they are concatenated to create multi-head attention, where W<span class="superscript">o</span> is the weight matrix for <span class="No-Break">the output:</span></p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B31108_01_09.jpg" alt="" width="763" height="43"/>
				</div>
			</div>
			<p>This final output is then used in subsequent layers of the transformer model to perform various tasks, such as translation, text generation, <span class="No-Break">or classification.</span></p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>Understanding the temperature parameter for GenAI use cases</h2>
			<p>In the <em class="italic">softmax</em> function used as a part of the attention score, <strong class="bold">temperature</strong> refers to a parameter that controls the smoothness or sharpness of the probability distribution produced by the softmax function. Adjusting the temperature can influence how confident the model is about its prediction of <span class="No-Break">the output:</span></p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B31108_01_10.jpg" alt="" width="337" height="67"/>
				</div>
			</div>
			<p>Lower temperature settings make the softmax distribution sharper (more confident predictions), while higher temperatures make the distribution smoother (less confident predictions) and could lead to more <span class="No-Break">creative responses.</span></p>
			<p>The following is a sample response from ChatGPT for two temperature settings with the input prompt of <strong class="source-inline">What is the purpose </strong><span class="No-Break"><strong class="source-inline">of Life?</strong></span></p>
			<p><strong class="bold">Temperature = </strong><span class="No-Break"><strong class="bold">2.0</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
The purpose of life is like an ever-changing kaleidoscope, bursting with colors and patterns that shift with every moment. For some, it's about weaving tales of adventure and discovery, chasing after the mysteries that lie just beyond the horizon. For others, it's the serene pursuit of inner peace, finding harmony in the gentle ebb and flow of existence.</pre>			<p><strong class="bold">Temperature = </strong><span class="No-Break"><strong class="bold">0.75</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
The purpose of life is a question that has intrigued humanity for centuries, and it can be seen from various perspectives. For many, the purpose of life is to seek happiness and fulfillment, to find and pursue passions that bring joy and meaning. This can involve forming deep connections with family and friends, contributing to the well-being of others, and making a positive impact on the world.</pre>		</div>
	</div></div></body></html>