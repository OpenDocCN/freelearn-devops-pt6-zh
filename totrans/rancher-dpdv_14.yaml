- en: '*Chapter 10*: Monitoring and Logging'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapters covered cluster configuration, backup, and recovery. This
    chapter will cover Rancher monitoring and how Rancher uses Prometheus and Grafana
    to collect metrics for a cluster and then appoint them. Then, we will cover Rancher
    logging and how Rancher uses the Banzai Cloud Logging operator and Fluent Bit
    to collect the logs from the Kubernetes components and collect application logs,
    including filtering logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Prometheus and Grafana?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Rancher's monitoring stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding custom application metrics to Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating alert rules in Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Grafana dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the Banzai Cloud Logging operator?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Fluent Bit?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Rancher logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering application logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing logs to multiple log servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Prometheus and Grafana?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll be covering the most popular monitoring solution for
    Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prometheus** is an open source monitoring and alerting framework that the
    Kubernetes community has widely adopted. Prometheus was initially created by SoundCloud
    back in 2012 before it was accepted by the **Cloud Native Computing Foundation**
    (**CNCF**) as its second incubated project after Kubernetes. Prometheus was built
    from the ground up to work with Kubernetes, the core idea being that everything
    should be discoverable via the Kubernetes API. At this point, Prometheus will
    pull the metrics and store them as time-series key-value pairs.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the first question that always comes up is, *what are metrics?* In
    the simplest terms, it's a numerical measurement of a resource. For example, it
    can be the current memory usage of a pod or the current number of connections
    to a database server. It is important to note that Prometheus doesn't support
    anything but an integer or floating-point number for the values. You can't set
    a value to something such as the words *up* or *down* for the value of a metric.
    For example, if you want to check whether a job was successful or failed, you
    might output that value as `0` for a successful status and `1` for a failed status.
  prefs: []
  type: TYPE_NORMAL
- en: The other central point with metrics is they should be a point-in-time value.
    For example, you might want the average number of connections at a given time.
    So, you would define a metrics endpoint at the pod level. One of the traps for
    new players is to add the metric endpoint to a service record. This might be easier
    but is not recommended for the long term because you might want a different rule
    in the future. For example, you start with just finding an average number of connections
    over the last 5 minutes, and then you want to change that to 15 minutes. Do you
    change the value of the current metric, which might affect historical reporting,
    or do you add another metric, which then means you are collecting duplicate data?
    The best approach is to output the raw data as a metric and then process it inside
    Prometheus and Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: The next question that comes up is, *how does Prometheus get its data?* Because
    Prometheus uses a `pull` instead of a `push` model, this is done by running a
    web server that exports the metrics as a simple text output of key-value pairs.
    This is commonly called an exporter in Prometheus. These exporters can be built
    directly into your application, as in the case of most of the core components
    of Kubernetes. For example, etcd has a built-in metrics exporter that runs on
    a different port, `2379`. It is common to run metrics on a different port than
    the main application because Prometheus, by default, will try making a `GET` request
    without authentication. Prometheus can query an endpoint that requires authentication,
    but setting up the tokens or credentials requires additional work and maintenance.
    So, most users will avoid it and use the fact that metrics are only exposed internally
    to the cluster and not to the public as *good enough* security.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, Prometheus uses exporters to collect metrics, so the question of
    what exporters are available comes up. And luckily, because of the open source
    community, there are a significant number of third-party exporters for most standard
    applications. For example, almost all major open source databases have an exporter
    such as MySQL, CouchDB, MongoDB, MSSQL, Oracle DB, and PostgreSQL. You can find
    the official list at https://prometheus.io/docs/instrumenting/exporters/#databases.
    It's the same with standard web servers such as Apache and NGINX, with the complete
    list available at https://prometheus.io/docs/instrumenting/exporters/#http. And,
    of course, almost all Kubernetes native applications such as CoreDNS, Longhorn,
    Linkerd, and OPA Gatekeeper have Prometheus exporters built right into the application.
  prefs: []
  type: TYPE_NORMAL
- en: For application developers, there are several libraries available for Go, Java/JVM,
    Python, and Node.js that allow even custom applications to add built-in support
    for Prometheus. Of course, if you can't find a premade exporter for your application,
    upstream Prometheus provides excellent resources for writing your exporter, including
    naming standards, example code, and different technical aspects for handling use
    cases. All this can be found at [https://prometheus.io/docs/instrumenting/writing_exporters/](https://prometheus.io/docs/instrumenting/writing_exporters/).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, one of the newer features added to Prometheus is alerting. Because
    Prometheus is already collecting your environment and applications data, it makes
    sense to add alerting into Prometheus using AlertManager. The core concept is
    that you define a set of queries that will run inside the Prometheus server that,
    if violated, will trigger an alert, which will be sent to Alertmanager, which
    will forward that alert to several external services such as email, Slack, and
    PagerDuty. Later in this chapter, we'll cover creating alerts in Prometheus along
    with some examples.
  prefs: []
  type: TYPE_NORMAL
- en: The one main feature that Prometheus is missing is a way to visualize your data.
    This is where Grafana comes into the picture. Grafana allows you to visualize
    data stored in Prometheus and other data sources such as MySQL, Loki, and InfluxDB.
    The main idea behind Grafana is that you create a dashboard that will query a
    data source (Prometheus, in this case) and then use this data to develop a range
    of graphs, charts, gauges, and so on. It is important to note that Gradana doesn't
    store any data outside of caching query results. Grafana also supports exploring
    logs from sources such as Loki and Elasticsearch. It also has a notification system
    that can trigger alerts based on queries, as Prometheus does. This can be helpful
    for application teams to create custom alerts.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Rancher's monitoring stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With Rancher, there are two main versions of monitoring, v1 and v2\. The original
    v1 monitoring that came with Rancher 2.0 to 2.4 is based on Prometheus and Grafana.
    But with the Rancher server and UI managing the deployment and configuration of
    the monitoring stack, the basic idea is to deploy Prometheus at the cluster level
    and additional Prometheus servers for each Rancher project. This approach was
    fine if you had a small number of projects that didn't need to be controlled via
    automation. This was mainly done because, initially, all the configurations of
    the Prometheus server were done by changing configmap. This required a great deal
    of work to manage the monitoring settings as clusters and applications grew in
    size and complexity.
  prefs: []
  type: TYPE_NORMAL
- en: With the creation of the Prometheus operator, it all changed. The core idea
    is that the Prometheus operator monitors a set of **Custom Resource Definitions**
    (**CRDs**). This includes the description of the Prometheus server and its related
    services such as node-exporter and Alertmanager. It is important to note that
    monitoring v1 and v2 has built-in rules and dashboards but, most importantly,
    the configuration of probes, alerts, and other related Prometheus settings, with
    Prometheus operator handling, creating, and updating the configuration files used
    by Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: In October 2020, Rancher 2.5 migrated from monitoring v1 to v2, with v2 being
    built on the operator model. It is important to note that both Prometheus and
    Grafana moved to this new model. This also included Rancher using the standard
    upstream Prometheus and Grafana image instead of the Rancher customized images.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are currently running the old v1 monitoring, migrating to the new v2
    monitoring is recommended. The official process can be found at [https://rancher.com/docs/rancher/v2.5/en/monitoring-alerting/guides/migrating/](https://rancher.com/docs/rancher/v2.5/en/monitoring-alerting/guides/migrating/),
    but the process can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: You need to delete all the current settings and configurations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, uninstall the old Prometheus server and its components.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, you can install v2 monitoring and reconfigure all the settings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You must make sure that nothing is left behind from v1 monitoring before installing
    v2\. Luckily, one of the engineers at Rancher named Bastian Hofmann created a
    script that handles the process of collecting all the alerts and dashboards and
    migrating them over to v2 ([https://github.com/bashofmann/rancher-monitoring-v1-to-v2](https://github.com/bashofmann/rancher-monitoring-v1-to-v2)).
    It is important to note that it is not an official script, and you should take
    an etcd backup before starting this process.
  prefs: []
  type: TYPE_NORMAL
- en: For deploying monitoring v1, log into the Rancher UI, go to **Tools** | **Monitoring**,
    and click the **Enable** button. At this point, the Rancher server will take over
    deploying the Prometheus server and node exporters. Then, all the monitoring configuration
    will be done via the Rancher UI. For example, if you wanted to view the CPU usage
    of a pod, you would browse to the pod in the Rancher UI, and Grafana graphs will
    be displayed right inside the UI. For additional details about the workload metrics
    that can be collected, please see the official Rancher documentation at [https://rancher.com/docs/rancher/v2.0-v2.4/en/cluster-admin/tools/cluster-monitoring/cluster-metrics/](https://rancher.com/docs/rancher/v2.0-v2.4/en/cluster-admin/tools/cluster-monitoring/cluster-metrics/).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that cluster monitoring is only designed to be used
    by users that have full view access to the cluster. If you want to scope monitoring
    to a single project, you''ll need to enable project monitoring by going to the
    project and selecting **Monitoring** from the **Tools** menu. This will cause
    the Rancher server to deploy an additional Prometheus server with its own namespace
    inside the project. This Prometheus server is scoped to the project and its namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Rancher monitoring v1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_10_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 – Rancher monitoring v1
  prefs: []
  type: TYPE_NORMAL
- en: 'For deploying monitoring v2, you have a couple of different options. The first
    one is to go to **Cluster explorer** | **Cluster Tools** and click **Install**
    next to **Monitoring**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Rancher monitoring v2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_10_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – Rancher monitoring v2
  prefs: []
  type: TYPE_NORMAL
- en: 'This will deploy Rancher''s monitoring chart via the app catalog. This chart
    is just a repackage of upstream images with no code changes. The only real difference
    is to use Rancher''s Docker Hub repositories in place of upstreams. Also, the
    default namespace is set to `cattle-monitoring-system`, but this can be customized
    if you so choose. Because monitoring v2 is a Helm chart, you can choose to deploy
    it directly via the `helm` command, which can be very helpful when managing clusters
    at scale, using tools such as Rancher''s fleet. The following is an example command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can find the complete command and `values.yaml` for installing Rancher monitoring
    via the `helm` command at [https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-monitoring-v2](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-monitoring-v2).
  prefs: []
  type: TYPE_NORMAL
- en: 'The second option is to deploy the upstream Helm chart, commonly called `kube-prometheus-stack`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It is important to note that, at the time of writing, this chart is still in
    beta and is subject to change, and Rancher doesn't support all the versions available
    in the upstream charts. So, it recommends reviewing Rancher's support matrix at
    [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/).
    You can also find a complete list of the configuration options by viewing the
    chart value at https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#configuration.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should have Prometheus and Grafana installed on your cluster.
    It is important to note that it can take approximately 5–10 minutes for all the
    pods and services that Prometheus needs to start entirely. It is also important
    to note that, at the time of writing, Rancher does not fully support Prometheus
    federation – the idea being that you can have a central Prometheus server that
    scans all other Prometheus servers across your other clusters. If you would like
    to learn more about this, I recommend looking at the official documentation at
    [https://prometheus.io/docs/prometheus/latest/federation/](https://prometheus.io/docs/prometheus/latest/federation/),
    but I would note that this is still a new feature and still evolving.
  prefs: []
  type: TYPE_NORMAL
- en: Adding custom application metrics to Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, now that you have Prometheus and Grafana all installed and working,
    the question becomes, *how do you get metrics from our applications into Prometheus?*
    In this section, we will cover two main ways of doing this.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way is to use a community-created chart such as Bitnami's MariaDB
    chart, including the `metrics.enabled=true` option. This option enables a sidecar
    that adds `mysqld-exporter` to the deployment, with many community-created charts
    using this model of having the exporter be a sidecar container to the main container.
    It is important to note that you should read the documentation for the Helm chart
    to see if any additional steps need to be taken when enabling metrics in your
    chart, as some applications will require a service account or permissions to be
    set for the exporter to work correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Besides enabling the metrics, you'll also see an annotation section with the
    `prometheus.io/scrape=true` and `prometheus.io/port=9000` key pair. The port might
    be different, but it is a standard practice to set it to something in this range.
    These two annotations are significant, as they are what Prometheus uses when discovering
    all the different pods that should be scraped.
  prefs: []
  type: TYPE_NORMAL
- en: But let's assume that you are using a custom-made application and want to capture
    metrics from this application. The following are a couple of examples of different
    applications where the metrics exporter is installed.
  prefs: []
  type: TYPE_NORMAL
- en: With GoLang, Prometheus provides an official library located at [https://github.com/prometheus/client_golang/](https://github.com/prometheus/client_golang/).
    This library handles most of the heavy lifting when generating the metrics output.
    You can find an example Go application and deployment at [https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/go](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/go).
    You need to run the `kubectl apply -f deploy.yaml` command in order to deploy
    the example application. If you curl the pod IP address with the path/metrics,
    you'll find that the application returns a list of different metrics (for example,
    curl `10.42.7.23:8080/metrics`). Once the application is up and running, you can
    send a `GET` request to `/ping`, which will return the word `pong`. Then, inside
    the application, it will increase a counter called `ping_request_count`, which
    is a custom metric that is being exposed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of the metrics output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The full output can be found at [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/go/output.txt.](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/go/output.txt.)
  prefs: []
  type: TYPE_NORMAL
- en: With Apache2, we need to take the sidecar option to add the exporter to the
    deployment. In our example, we are going to use a popular third-party exporter
    called `apache_exporter`. You can read more about this project at [https://github.com/Lusitaniae/apache_exporter](https://github.com/Lusitaniae/apache_exporter).
    The basic idea behind this project is to act as a translation layer between the
    Apache `mod_status` module and Prometheus. We need to install/enable the `mod_status`
    module to the primary web server container in the example deployment. Then, we
    need to expose the `server-status` page to the sidecar container that hosts the
    exporter. You can find the example and deployment file at [https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/apache](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/apache).
    You need to run the `kubectl apply -f deploy.yaml` command to deploy the example
    application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of the metrics output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The full output can be found at [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/apache/output.txt](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/apache/output.txt).
  prefs: []
  type: TYPE_NORMAL
- en: With NGINX, we will use a similar process as we did with Apache, but this time,
    we will use an exporter provided by NGINX. You can find the example and deployment
    file at [https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/nginx](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/nginx).
    You simply need to run the `kubectl apply -f deploy.yaml` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of the metrics output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The full output can be found at [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/nginx/output.txt.](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/nginx/output.txt.).
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can monitor the metrics of our different applications, but
    we're missing the ability to create alerts based on these metrics. In the next
    section, we'll dive into alert rules for Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Creating alert rules in Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Prometheus operator defines alert rules via the CRD PrometheusRule. At its
    core, all an alert is is an expression with a trigger. Let's look at the following
    example alert. This alert is from Longhorn, which we'll cover in the next chapter.
    As you can see, the expression is denoted by the `expr` field, which has a formula
    to take the actual size of the volume, divided by the capacity, and convert it
    to a percentage. Then, if that value is greater than 90%, the expression is `true`,
    which will trigger an alert. The `description` section is mainly for the end user.
    Still, it's important to note that you can have variables inside the description
    because the alert will contain the same explanation as the summary, typically
    used for the subject line. For example, when sending an email alert, the email's
    subject will be set to the subject of the alert, with the body of the email being
    the description.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of an alert:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: It is important to note that Prometheus will only find rules located in the
    same namespace as the server by default. This can cause issues, as application
    teams might need access to the namespace to add/edit/remove their alerts. To work
    around this issue, you'll need to add the following settings in your `values.yaml`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of `values.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we have Prometheus up and running. It's collecting all the data
    about our cluster, but the built-in UI doesn't give you a way to visualize this
    data in a useful way. In the next section, we'll be diving into Grafana to bring
    dashboards to our data.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Grafana dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we should have Prometheus and Grafana up and running, with the
    server collecting all the data about our cluster. Still, you can''t see most of
    the data unless you use Rancher''s monitoring charts, including some prebuilt
    dashboards that are mostly related to the cluster and its core services such as
    etcd, kube-apiserver, and CoreDNS. But, of course, a question comes up: *How do
    I create my own?*'
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward answer is to find a premade dashboard and let someone
    else do all the hard work for you. The Grafana Labs dashboard repository is the
    most extensive resource, located at [https://grafana.com/grafana/dashboards/](https://grafana.com/grafana/dashboards/).
    Their search tool lets you filter results by applications, data sources, and so
    on. But the coolest part is their dashboard ID system. All dashboards on the official
    site have an ID number – for example, the NGINX Ingress controller dashboard has
    an ID of `9614`, and all you need to do to use this dashboard is copy that ID
    number and go to the Grafana UI. Browse to `admin/prom-operator`. Then, paste
    the ID number in, and you're done.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, Rancher monitoring provides some example dashboards bundled into
    the rancher-monitoring chart. You can find the raw JSON files at [https://github.com/rancher/system-charts/tree/dev-v2.6/charts/rancher-monitoring/v0.3.1/charts/grafana/dashboards](https://github.com/rancher/system-charts/tree/dev-v2.6/charts/rancher-monitoring/v0.3.1/charts/grafana/dashboards).
    Additionally, you can add baseboards by ID too, with some of the most important
    ones being the component and etcd dashboards, which can be used to provide a great
    deal of insight into cluster performance issues.
  prefs: []
  type: TYPE_NORMAL
- en: But let's say the application you are deploying is a community-created application
    that doesn't have a dashboard on the official site. Most repositories will have
    the dashboard defined as a JSON file. You'll copy and paste it into the UI and
    import it with this file. But let's say you are deploying a custom in-house application
    and want to start from zero. I recommend watching the *Getting started with Grafana
    dashboard design* video at [https://grafana.com/go/webinar/guide-to-dashboard-design/](https://grafana.com/go/webinar/guide-to-dashboard-design/).
    I use a lot of community-created dashboards and tune them to my needs. You can
    click the share button at the top and export a dashboard as a JSON file, at which
    point you can copy and paste the parts you like. It is also imperative that you
    save your work when making changes to your dashboard by clicking the save icon
    at the top-right corner. If you close the page without clicking that icon, all
    your changes will be lost.
  prefs: []
  type: TYPE_NORMAL
- en: So far in this chapter, we have been talking about monitoring and alerting.
    In the next section, we will shift gears and focus on the other half of the equation,
    logging.
  prefs: []
  type: TYPE_NORMAL
- en: What is the Banzai Cloud Logging operator?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Along with migrating to monitoring v2 in Rancher 2.5, Rancher also migrated
    to logging v2 for mostly the same reasons. In v1, logging is built on Fluentd
    and uses plugins to ship logs to different logging services such as Elasticsearch,
    Splunk, Kafka, and Syslog. With v1, the Rancher server was in complete control
    of the logging deployments, which made customizing and tuning the logging solution
    complicated. Most of the settings were hard coded inside Rancher. This is where
    Banzai's Logging operator comes into the picture.
  prefs: []
  type: TYPE_NORMAL
- en: The Logging operator uses the CRD model, just like the Prometheus operator,
    wherein you'll define your Fluent Bit deployment and its setting via a CRD. The
    operator takes over pushing out your changes. Because everything is a CRD, including
    your settings, you can let your application teams define their logging settings.
    For example, one team might want their logs sent to a cloud log service such as
    Splunk, while another team might have the legal requirement for everything to
    stay running on RKE or another K8s cluster hosted on-premises, and you can do
    this because of the Logging operator. The idea is that you have logging flows
    that are a set of application pods going to an output, which can be any number
    of logging servers/services.
  prefs: []
  type: TYPE_NORMAL
- en: What is Fluent Bit and Fluentd?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When talking about logging, two of the questions are, *what is Fluent Bit?*
    and *what is Fluentd?*
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into Fluent Bit, let's talk about Fluentd, which came first. Fluentd
    is an open source project written in Ruby by the Treasure Data team back in 2011\.
    Its core idea was that all logs should be JSON objects. With Fluentd and Docker,
    the basic process used to collect logs from the containers is to use the default
    Docker log driver that writes the container logs to a file on the disk. Then,
    Fluentd will read the whole log file and bring forward the events onto the server,
    at which point Fluentd will open a tail file handler that will hold the log file
    open and read all writes to it.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that Fluentd has a process for handling log rotation,
    so it is recommended to enable log rotation in Docker Engine. The following is
    an example configuration. You can find the complete documentation at [https://docs.docker.com/config/containers/logging/configure/](https://docs.docker.com/config/containers/logging/configure/).
    Docker will wait until the `logs` file is 100 MB before rotating the file in the
    following example. This is done to prevent the loss of events for applications
    that create large amounts of events. Fluentd needs to read all the events and
    forward them to the log server before the rotation.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we are defining the log options for the `json-file`
    log driver, which is built into Docker Engine by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example at `/etc/docker/daemon.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we are allowing the log file for all containers to reach a maximum
    size of 100 MB before rotating the file. Then, we'll only be keeping the last
    three rotated files.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's go over Fluent Bit. Basically, Fluentd was designed to be a simple
    tool that is fast and lightweight. Fluent Bit is built on top of Fluentd and is
    intended to provide the additional filtering and routing that Banzai logging needs.
    You can find more about the differences at [https://docs.fluentbit.io/manual/about/fluentd-and-fluent-bit](https://docs.fluentbit.io/manual/about/fluentd-and-fluent-bit).
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Rancher logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With Rancher logging, it is recommended to deploy via the Apps and Marketplace
    in the Rancher UI by going to `cattle-logging-system` namespace. It is important
    to note that you'll see two applications, `rancher-logging` and `rancher-logging-crd`,
    in the **Installed** section after the installation is complete. Also, depending
    on the size of the cluster, it might take 5 to 15 minutes for all the pods to
    start up and go into the *ready* state. Once Rancher logging is installed on the
    cluster, we will be able to configure filtering and log flows, which we'll cover
    in the next two sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because logging v2 is a Helm chart, you can choose to deploy it directly via
    the `helm` command, which can be very helpful when managing clusters at scale,
    using tools such as Rancher''s Fleet. The following is an example command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: You can find the full commands and `values.yaml` to install Rancher monitoring
    via the `helm` command at [https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-logging-v2](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-logging-v2).
  prefs: []
  type: TYPE_NORMAL
- en: Filtering application logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first setting that most people configure with Rancher logging is **ClusterFlows**
    and **ClusterOutput**. ClusterFlow is designed to be scoped to all namespaces
    and can set the default logging policy for the cluster as a whole. To configure
    this setting, you'll go to the Rancher UI and browse to **Logging** and then **ClusterFlows**.
    From there, you'll fill out the form. Then, once that is done, you'll want to
    define ClusterOutput, where you define the target location for your logs, ElasticSearch,
    Splunk, Syslog, and so on. For an example of each of the different logging providers,
    please see Rancher's official documentation at [https://rancher.com/docs/rancher/v2.5/en/logging/custom-resource-config/outputs/](https://rancher.com/docs/rancher/v2.5/en/logging/custom-resource-config/outputs/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have ClusterFlows and ClusterOutput configured, you can call it done.
    But if you want to customize the logging for an application, you need to repeat
    the process. Still, this time, you''ll be configuring the Flows and Outputs, with
    the main difference being setting the selector rules or what the documentation
    calls `include` or `exclude` labels that you can use to limit the scope of the
    Flows and Outputs. The following is an example YAML for a NGINX application in
    the default namespace. It is important to note that Flows are namespace-scoped:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a Flow example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we should have logging configured on our cluster and be forwarding
    the logs to a log server. In the next section, we'll cover a more advanced setup
    that a number of users use in their environments to log to multiple servers.
  prefs: []
  type: TYPE_NORMAL
- en: Writing logs to multiple log servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because you can define as many outputs as you would like, suppose you wanted
    to send logs to multiple log servers, such as sending to a local Syslog and Splunk
    server. It is imperative to note that this will duplicate your logs, so this is
    not recommended as an `failed to send` logs, so you can run into memory pressure
    issues if you have a misconfigured log server or the log server is offline for
    long periods.
  prefs: []
  type: TYPE_NORMAL
- en: You can find YAML examples in Rancher's official documentation, located at [https://rancher.com/docs/rancher/v2.6/en/logging/custom-resource-config/outputs/](https://rancher.com/docs/rancher/v2.6/en/logging/custom-resource-config/outputs/).
    It's important to note that, as of Rancher 2.6.3, the logging settings in the
    Rancher UI are still buggy (for example, [https://github.com/rancher/rancher/issues/36516](https://github.com/rancher/rancher/issues/36516),
    where the **ClusterOutput** field is failing to update in the UI), so it's recommended
    to use YAML files as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about Rancher monitoring and logging. This includes
    how Prometheus, Grafana, Fluentd, and Fluent Bit work. We learned how to install
    Rancher monitoring and logging. We finally went into detail about some example
    dashboards. We ended the chapter by talking about customizing application logging
    and its flows.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will cover Rancher's storage project to provide storage to
    Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
