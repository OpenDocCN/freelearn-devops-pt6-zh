<html><head></head><body>
		<div id="_idContainer017">
			<h1 class="chapter-number" id="_idParaDest-53"><a id="_idTextAnchor053"/>3</h1>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor054"/>Getting Hands-On with Kubernetes</h1>
			<p>In this chapter, we will get hands-on experience with Kubernetes by deploying both a local and cloud-based Kubernetes cluster, and then deploying sample applications into those clusters. First, you will deploy a local cluster using <strong class="bold">Kubernetes in Docker</strong> (<strong class="bold">Kind</strong>). Then, you will deploy managed<a id="_idIndexMarker137"/> Kubernetes clusters on AWS, GCP, and Azure. For the cloud options, we will provide the minimal account setup required to deploy the clusters. Feel free to choose the cloud provider you are most comfortable with; the core Kubernetes functionality will be <span class="No-Break">the same.</span></p>
			<p>After deploying a cluster, this chapter will be divided into two parts. In the first part, you will take the simple API application you developed in <a href="B21927_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a> and deploy it into your Kubernetes cluster. You will learn how to containerize applications and work with Kubernetes deployments and services to expose your application. In the second part, you will deploy the simple data processing batch job from <span class="No-Break"><em class="italic">Chapter 1</em></span> into Kubernetes. This will demonstrate how to run one-off jobs <span class="No-Break">using Kubernetes.</span></p>
			<p>By the end of this chapter, you will have first-hand experience deploying applications into Kubernetes. You will understand how to package and deploy containerized applications, expose them via services and ingress, and leverage Kubernetes for running both long-running services and batch jobs. With these skills, you will be ready to deploy applications into production <span class="No-Break">Kubernetes environments.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li><span class="No-Break">Installing </span><span class="No-Break"><strong class="source-inline">kubectl</strong></span></li>
				<li>Deploying a local K8s cluster <span class="No-Break">with Kind</span></li>
				<li>Deploying an AWS <span class="No-Break">EKS cluster</span></li>
				<li>Deploying a Google Cloud <span class="No-Break">GKE cluster</span></li>
				<li>Deploying an Azure <span class="No-Break">AKS cluster</span></li>
				<li>Running your API <span class="No-Break">on Kubernetes</span></li>
				<li>Running a data processing job <span class="No-Break">in Kubernetes</span></li>
			</ul>
			<p>Let’s get <span class="No-Break">to it.</span></p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor055"/>Technical requirements</h1>
			<p>In this chapter, you will be required to have Docker installed (the instructions for this can be found in <a href="B21927_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>). The main hands-on activities we are going to do will be cloud-based, but you can choose to do them locally with Kind. You will learn how to deploy a Kubernetes cluster locally using Kind or in the cloud (AWS, Google Cloud, and Azure) in this chapter. Finally, you will need <strong class="source-inline">kubectl</strong> to interact with your Kubernetes cluster. You’ll learn how to install it in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor056"/>Installing kubectl</h1>
			<p><strong class="source-inline">kubectl</strong> is a CLI that we are going<a id="_idIndexMarker138"/> to use to send commands to a Kubernetes cluster. You must have this installed so that you can interact with the cluster (regardless of whether it’s running locally or in <span class="No-Break">the cloud):</span></p>
			<ul>
				<li>To install <strong class="source-inline">kubectl</strong> on macOS with Homebrew, use the <span class="No-Break">following command:</span><pre class="source-code">
brew install kubectl</pre></li>				<li>To install <strong class="source-inline">kubectl</strong> in a Linux distribution, you can use <strong class="source-inline">curl</strong> to download the <span class="No-Break">binary executable:</span><pre class="source-code">
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"</pre></li>				<li>To install it in a Windows system, you can use the <strong class="source-inline">chocolatey</strong> <span class="No-Break">package manager:</span><pre class="source-code">
choco install kubernetes-cli</pre></li>			</ul>
			<p>From this point on, every <strong class="source-inline">kubectl</strong> command will be the same, regardless of the OS you’re using. To check if the installation went successfully, run the following command. This will give you a nice, formatted view of the version of <strong class="source-inline">kubectl</strong> that’s running on <span class="No-Break">your system:</span></p>
			<pre class="source-code">
kubectl version –client –output=yaml</pre>			<p>Now, let’s move and <span class="No-Break">install </span><span class="No-Break"><strong class="source-inline">kind</strong></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor057"/>Deploying a local cluster using Kind</h1>
			<p>Deploying a local<a id="_idIndexMarker139"/> Kubernetes cluster <a id="_idIndexMarker140"/>can be extremely helpful for learning, testing, and preparing deployments for a production environment as Kubernetes is the same, wherever you run it. Let’s start by deploying a single-node local cluster <span class="No-Break">using Kind.</span></p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor058"/>Installing kind</h2>
			<p>Kind is a tool that allows you to run Kubernetes<a id="_idIndexMarker141"/> on your local machine inside Docker containers. Besides being light and easy to set up, Kind delivers performance with the same Kubernetes standards. Kind clusters pass upstream Kubernetes <span class="No-Break">conformance testing.</span></p>
			<p>Kind is distributed as a single binary file. You can install it easily with package managers (make sure you have Docker <span class="No-Break">already installed):</span></p>
			<ul>
				<li>If you’re using macOS, use Homebrew to install <strong class="source-inline">kind</strong>, <span class="No-Break">like so:</span><pre class="source-code">
brew install kind</pre></li>				<li>If you’re using a Linux distribution (Ubuntu, for instance), you can install it <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break">:</span><pre class="source-code">
$ [ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64
$ chmod +x ./kind
$ sudo mv ./kind /usr/local/bin/kind</pre></li>				<li>If you’re using a Windows<a id="_idIndexMarker142"/> system, install it with <span class="No-Break"><strong class="source-inline">chocolatey</strong></span><span class="No-Break"> (</span><a href="https://chocolatey.org/packages/kind):"><span class="No-Break">https://chocolatey.org/packages/kind):</span></a><pre class="source-code">
choco install kind</pre></li>			</ul>
			<p>After the installation is finished, you can check if it was installed correctly by running the <span class="No-Break">following </span><span class="No-Break"><a id="_idIndexMarker143"/></span><span class="No-Break">command:</span></p>
			<pre class="console">
kind version</pre>			<h2 id="_idParaDest-59"><a id="_idTextAnchor059"/>Deploying the cluster</h2>
			<p>Now, to deploy a single-node<a id="_idIndexMarker144"/> local cluster, just run the <span class="No-Break">following command:</span></p>
			<pre class="console">
kind create cluster</pre>			<p>If this is the first time you’ve run this command, Kind will download the control plane image, create a Docker container for it, and configure a single-node Kubernetes cluster for you. The first time it runs, Kind will take 1-2 minutes to complete as it downloads the Kubernetes image. The next runs will be <span class="No-Break">much faster.</span></p>
			<p>To verify that the cluster is up, run the <span class="No-Break">following command:</span></p>
			<pre class="console">
kubectl cluster-info</pre>			<p>This will print connection details for the local cluster. And, that’s it! You’re good <span class="No-Break">to go.</span></p>
			<p>If you’re willing to work with a cloud-based Kubernetes cluster, in the next few sections, we will deploy a cluster on AWS, Google Cloud, <span class="No-Break">and Azure.</span></p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor060"/>Deploying an AWS EKS cluster</h1>
			<p><strong class="bold">Amazon Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>) allows you to easily deploy<a id="_idIndexMarker145"/> and manage Kubernetes<a id="_idIndexMarker146"/> clusters on AWS. EKS handles provisioning and maintaining the Kubernetes control plane, while you can use standard Kubernetes tooling such as <strong class="source-inline">kubectl</strong> to manage <span class="No-Break">worker nodes.</span></p>
			<p>To get started with EKS, you need an AWS account. Go to <a href="http://aws.amazon.com">http://aws.amazon.com</a> and click <strong class="bold">Create an AWS account</strong>. Follow the steps to sign up for a new account; note that you will be requested to provide your credit card information. AWS offers a free usage tier that provides limited resources at no charge for 12 months. This is usually sufficient to run small workloads but not Kubernetes (although costs will not be high if you manage it wisely). AWS charges 73 dollars per month for every running cluster (assuming that the cluster is running for the whole month; if it runs just for a couple of days or hours, billing should be a fraction of that accordingly) plus the proper charging for each node that is running according to the size of the <span class="No-Break">chosen machines.</span></p>
			<p>After setting up the account, you must access it with your <strong class="source-inline">root</strong> user. We need to create an IAM user with specific permissions as this is the recommended usage. In the AWS console, go to the <strong class="bold">IAM</strong> service and click <strong class="bold">Users</strong> in the left panel. You should see a screen <span class="No-Break">like this:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer016">
					<img alt="Figure 3.1 – The Users menu in IAM" src="image/B21927_03_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – The Users menu in IAM</p>
			<p>Then, define the username<a id="_idIndexMarker147"/> and password for access. When you click <strong class="bold">Next</strong>, choose to attach policies directly (since this is only a study account – this configuration is not suited for a production environment). Choose <strong class="bold">AdministratorAccess</strong> from the list (which will permit you to do everything in AWS) and click <strong class="bold">Next</strong> to review. Then, click the <strong class="bold">Create user</strong> button to finish the process. When all is set, remember to download the AWS secrets (AWS access key ID and AWS secret access key). You will need these secrets to authenticate in AWS from your computer. Now, log off the console and access it again with your new IAM user to validate that your new IAM user has been <span class="No-Break">configured correctly.</span></p>
			<p>Before we look at the tools for setting up a Kubernetes cluster, we need to install the AWS CLI to interact with AWS from our local terminal. To do so, type <span class="No-Break">the following:</span></p>
			<pre class="console">
pip3 install awscli</pre>			<p>After installed, now we run aws configure to set up our <span class="No-Break">AWS credentials:</span></p>
			<pre class="console">
aws configure</pre>			<p>You will be prompted for your AWS credentials (AWS Access Key ID and AWS Secret Access Key). Copy and paste those credentials as you are asked for them. Then, the configuration will ask for the default AWS region. You can work in us-east-1. For the output format, <strong class="source-inline">json</strong> is a <span class="No-Break">good choice.</span></p>
			<p>This will save a config file with your credentials at <strong class="source-inline">~/.aws/credentials</strong>. Now, you can run AWS CLI commands to interact with <span class="No-Break">AWS services.</span></p>
			<p>Once you have an AWS account and your IAM user and the AWS CLI have been configured, install the <strong class="source-inline">eksctl</strong> command-line tool. This tool simplifies and automates the process of deploying EKS clusters. Go to <a href="https://eksctl.io/installation/">https://eksctl.io/installation/</a> and follow the<a id="_idIndexMarker148"/> installation instructions. Note that the documentation page lists the permissions you should have to deploy a cluster with <strong class="source-inline">eksctl</strong>. The <strong class="bold">AdministratorAccess</strong> policy should englobe any of the permissions <span class="No-Break">you need.</span></p>
			<p>After the installation, to check if <strong class="source-inline">eksctl</strong> has been set up correctly, run the <span class="No-Break">following command:</span></p>
			<pre class="console">
eksctl version</pre>			<p>You should see some output<a id="_idIndexMarker149"/> showing the version. Now, let’s create a cluster from scratch with a <span class="No-Break">single command:</span></p>
			<pre class="console">
eksctl create cluster \
    --managed --alb-ingress-access \
    --node-private-networking --full-ecr-access \
    --name=studycluster \
    --instance-types=m6i.xlarge \
    --region=us-east-1 \
    --nodes-min=2 --nodes-max=4 \
    --nodegroup-name=ng-studycluster</pre>			<p>The base of this command is <strong class="source-inline">eksctl create cluster</strong>. From the second line on, we are stating some options. Let’s look <span class="No-Break">at them:</span></p>
			<ul>
				<li>The <strong class="source-inline">--managed</strong> option tells <strong class="source-inline">eksctl</strong> to create a fully managed cluster. It will handle creating the EKS control plane, node groups, networking, <span class="No-Break">and more.</span></li>
				<li>The <strong class="source-inline">--alb-ingress-access</strong> option will configure the cluster to allow inbound traffic from load balancers. This is required for load balancer <span class="No-Break">type services.</span></li>
				<li>The <strong class="source-inline">--node-private-networking</strong> option enables private networking between worker nodes. Nodes will have private IP <span class="No-Break">addresses only.</span></li>
				<li>The <strong class="source-inline">--full-ecr-access</strong> option gives worker nodes full access to ECR container image repositories. This is needed to pull <span class="No-Break">container images.</span></li>
				<li>The <strong class="source-inline">--name</strong> option sets a user-defined name for <span class="No-Break">the cluster.</span></li>
				<li>The <strong class="source-inline">--instance-type</strong> option configures the instance type that will be used for worker nodes. In this case, we are choosing to use the <strong class="source-inline">m6i.xlarge</strong> <span class="No-Break">EC2 instance.</span></li>
				<li>The <strong class="source-inline">--region</strong> option sets the AWS region – in this case, N. <span class="No-Break">Virginia (us-east-1).</span></li>
				<li>The <strong class="source-inline">--nodes-min</strong> and <strong class="source-inline">--nodes-max</strong> options set an autoscaling rule for the node group. Here, we set the minimum to <strong class="source-inline">2</strong> instances and the maximum to <span class="No-Break"><strong class="source-inline">4</strong></span><span class="No-Break"> instances.</span></li>
				<li>The <strong class="source-inline">--nodegroup</strong> option sets the node group’s name <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">ng-studycluster</strong></span><span class="No-Break">.</span></li>
			</ul>
			<p>This will take around 10-15 minutes to complete. <strong class="source-inline">eksctl</strong> handles<a id="_idIndexMarker150"/> all the details of creating VPCs, subnets, security groups, IAM policies, and other AWS resources needed to run an <span class="No-Break">EKS cluster.</span></p>
			<p>Under the hood, <strong class="source-inline">eksctl</strong> uses CloudFormation to provision the AWS infrastructure. The <strong class="source-inline">eksctl</strong> command generates a CloudFormation template based on the parameters provided. It then deploys this template to create the <span class="No-Break">necessary resources.</span></p>
			<p>The following are some key components that are created <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">eksctl</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="bold">Virtual private cloud (VPC)</strong>: A VPC network where<a id="_idIndexMarker151"/> your cluster resources will run. This includes public and private subnets across multiple <span class="No-Break">availability zones</span></li>
				<li><strong class="bold">EKS cluster</strong>: The Kubernetes control plane, which consists of the API server, etcd, controller manager, scheduler, and more. AWS operates and <span class="No-Break">manages this</span></li>
				<li><strong class="bold">Worker node groups</strong>: Managed node groups containing EC2 instances that will run your <span class="No-Break">Kubernetes workloads</span></li>
				<li><strong class="bold">Security groups</strong>: Firewall rules to control access to <span class="No-Break">the cluster</span></li>
				<li><strong class="bold">IAM roles and policies</strong>: Access policies to allow worker nodes and Kubernetes to access AWS APIs <span class="No-Break">and resources</span></li>
			</ul>
			<p>Once the <strong class="source-inline">eksctl</strong> command <a id="_idIndexMarker152"/>completes, your EKS cluster will be ready to use. You can view details about the cluster by running the <span class="No-Break">following command:</span></p>
			<pre class="console">
eksctl get cluster --name studycluster --region us-east-1</pre>			<p>Next, we are going to create a Kubernetes cluster using <span class="No-Break">Google Cloud.</span></p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor061"/>Deploying a Google Cloud GKE cluster</h1>
			<p>To deploy a Kubernetes cluster<a id="_idIndexMarker153"/> on Google Cloud, we need to set up<a id="_idIndexMarker154"/> a Google Cloud account and install the <strong class="source-inline">gcloud</strong> <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>). To do that, go to <a href="https://cloud.google.com/">https://cloud.google.com/</a> and click on <strong class="bold">Start free</strong> to create a new account. Follow the instructions and, when your new account is created, navigate to the console at <a href="https://console.cloud.google.com/">https://console.cloud.google.com/</a>. This is where you can manage all your Google <span class="No-Break">Cloud resources.</span></p>
			<p>Before we can deploy a GKE cluster, we need to enable the necessary APIs. Click on the navigation menu icon in the top left and go to <strong class="bold">APIs &amp; Services</strong>. Search for <strong class="source-inline">Kubernetes Engine API</strong> and click on it. Make sure it is enabled. It is also a good practice to enable <strong class="bold">Cloud Resource Manager API</strong> and <strong class="bold">Cloud Billing API</strong>. This will allow us to create and manage the cluster resources. Next, we need to install and initialize the <strong class="source-inline">gcloud</strong> CLI. This will allow us to manage Google Cloud resources from the <span class="No-Break">command line.</span></p>
			<p>The <strong class="source-inline">gcloud</strong> CLI can be installed on Linux, macOS, and Windows. In a Linux distribution, you can download the installation script <span class="No-Break">like so:</span></p>
			<pre class="console">
wget https://dl.google.com/dl/cloudsdk/release/google-cloud-sdk.tar.gz</pre>			<p>Then, extract <span class="No-Break">the archive:</span></p>
			<pre class="console">
tar -xzf google-cloud-sdk.tar.gz</pre>			<p>Next, run the <span class="No-Break">install script:</span></p>
			<pre class="console">
./google-cloud-sdk/install.sh</pre>			<p>For a macOS installation, you can <span class="No-Break">use Homebrew:</span></p>
			<pre class="console">
brew install --cask google-cloud-sdk</pre>			<p class="callout-heading">Note</p>
			<p class="callout">For Windows, you can<a id="_idIndexMarker155"/> download the <strong class="source-inline">gcloud</strong> installer <span class="No-Break">from </span><a href="https://cloud.google.com/sdk/docs/install#windows"><span class="No-Break">https://cloud.google.com/sdk/docs/install#windows</span></a><span class="No-Break">.</span></p>
			<p>Once the installation is done, you can check it by running the <span class="No-Break">following command:</span></p>
			<pre class="console">
gcloud --version</pre>			<p>Once you’ve installed it, you must initiate <span class="No-Break">the CLI:</span></p>
			<pre class="console">
gcloud init</pre>			<p>This will walk you through logging in with your Google account and <span class="No-Break">configuring </span><span class="No-Break"><strong class="source-inline">gcloud</strong></span><span class="No-Break">.</span></p>
			<p>Now that <strong class="source-inline">gcloud</strong> is installed<a id="_idIndexMarker156"/> and initialized, we can deploy a Kubernetes cluster on GKE. First, choose a Google Cloud project to deploy the cluster under. You can create a new project and set it as active <span class="No-Break">like so:</span></p>
			<pre class="console">
gcloud projects create [PROJECT_ID]
gcloud config set project [PROJECT_ID]</pre>			<p>Then, you must set a compute zone to deploy the cluster in. Here, we will work in the <span class="No-Break"><strong class="source-inline">us-central1-a</strong></span><span class="No-Break"> zone:</span></p>
			<pre class="console">
gcloud config set compute/zone us-central1-a</pre>			<p>Now, we can deploy a Kubernetes cluster by running the <span class="No-Break">following command:</span></p>
			<pre class="console">
gcloud container clusters create studycluster --num-nodes=2</pre>			<p>The <strong class="source-inline">–num-nodes</strong> parameter controls how many nodes you are going to be deployed. Note that this will take several minutes to complete as Google Cloud will set up all the cluster components, networking, and so on. Once deployed, gather credentials to interact with <span class="No-Break">the cluster:</span></p>
			<pre class="console">
gcloud container clusters get-credentials studycluster</pre>			<p>This will save credentials to your Kubernetes config file. Finally, you can verify that you can connect to the cluster by running the <span class="No-Break">following command:</span></p>
			<pre class="console">
kubectl cluster-info</pre>			<p>You should see information<a id="_idIndexMarker157"/> about the Kubernetes master and the cloud provider. And that’s it! You now have a fully functional Kubernetes cluster running on <span class="No-Break">Google Cloud.</span></p>
			<p>Next, we will do the same with Microsoft <span class="No-Break">Azure Cloud.</span></p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor062"/>Deploying an Azure AKS cluster</h1>
			<p>In this section, we will walk through<a id="_idIndexMarker158"/> the steps to deploy <a id="_idIndexMarker159"/>a Kubernetes cluster using <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>). To get started with AKS, you need an Azure account. First, go to <a href="https://azure.com">https://azure.com</a>, click <strong class="bold">Try Azure for free</strong>, and then click <strong class="bold">Start free</strong>. This will allow you to start a free trial account on Azure. You will need to provide some basic information, such as your email and phone number, to set up the account. Make sure you use a valid email as Azure will send a verification code to complete the signup process. Once your account has been created, you will be directed to the Azure portal. This is the main dashboard for managing all your <span class="No-Break">Azure resources.</span></p>
			<p>At this point, it’s recommended to install the Azure CLI on your local machine. The Azure CLI allows you to manage Azure resources from the command line. Follow<a id="_idIndexMarker160"/> the instructions at <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli">https://docs.microsoft.com/en-us/cli/azure/install-azure-cli</a> to install it on Linux, macOS, <span class="No-Break">or Windows.</span></p>
			<p>After installing the CLI, run <strong class="source-inline">az login</strong> and follow the prompts to authenticate with your account. This will allow you to run Azure commands from your terminal. With the Azure account set up and the CLI installed, you are ready to deploy an <span class="No-Break">AKS cluster.</span></p>
			<p>The first thing you must do is create a resource group. Resource groups in Azure allow you to logically group resources such as AKS clusters, storage accounts, virtual networks, and more. Let’s start by creating a resource group for our <span class="No-Break">AKS cluster:</span></p>
			<pre class="console">
az group create --name myResourceGroup --location eastus</pre>			<p>This will create a resource group named <strong class="source-inline">myResourceGroup</strong> in the <strong class="source-inline">eastus</strong> region. You can specify any region close to you. Now, we can create an AKS cluster in this resource group. The basic command to create a cluster is <span class="No-Break">as follows:</span></p>
			<pre class="console">
az aks create --resource-group myResourceGroup --name studycluster--node-count 2 --generate-ssh-keys</pre>			<p>This will create a Kubernetes cluster named <strong class="source-inline">studycluster</strong> with two nodes. We generate SSH keys automatically to set up access to the nodes. Some other options you can specify are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">--node-vm-size</strong>: The size of the virtual machines for nodes. The default <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">Standard_D2s_v3</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">--kubernetes-version</strong>: The Kubernetes version to use for the cluster. It defaults to the <span class="No-Break">latest version.</span></li>
				<li><strong class="source-inline">--enable-addons</strong>: Enables add-ons such as monitoring, virtual nodes, <span class="No-Break">and more.</span></li>
			</ul>
			<p>The <strong class="source-inline">az aks create</strong> command handles setting up the Kubernetes cluster, virtual machines, networking, storage, and more automatically. The process may take 5-10 minutes <span class="No-Break">to complete.</span></p>
			<p>When it is done, you can connect to the cluster by running the <span class="No-Break">following command:</span></p>
			<pre class="console">
kubectl get nodes</pre>			<p>This will display the nodes<a id="_idIndexMarker161"/> that are part of your AKS cluster. At this point, you have successfully deployed an AKS cluster and are ready to deploy our API and batch processing job <span class="No-Break">on it.</span></p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor063"/>Running your API on Kubernetes</h1>
			<p>From this point, you can choose<a id="_idIndexMarker162"/> the Kubernetes deployment type you like the most (local or cloud-based) to run our API. The following examples will be shown with AWS but you also can choose another cloud provider. Feel free to <span class="No-Break">do so.</span></p>
			<p>Now, it is time to retake the API we built in <a href="B21927_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a> and ship it to production. We developed a simple API that, when requested, can say hello to you or answer with a (very <span class="No-Break">cool) joke.</span></p>
			<p>We already have the code for the API (<a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/app/main.py">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/app/main.py</a>) and the Dockerfile to build the container <span class="No-Break">image (</span><a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/Dockerfile"><span class="No-Break">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/Dockerfile</span></a><span class="No-Break">).</span></p>
			<p>For the image to be accessible to Kubernetes, it should be available on a container registry. Each cloud provider has a registry but to make things simple, we<a id="_idIndexMarker163"/> are working with DockerHub (<a href="https://hub.docker.com/">https://hub.docker.com/</a>). So long as your images are public, you can store as many images as you want for free. Let’s <span class="No-Break">get started:</span></p>
			<ol>
				<li>To get started, in your terminal, type the <span class="No-Break">following command:</span><pre class="source-code">
docker login</pre><p class="list-inset">This will prompt you for your Docker username and password. If you don’t have a DockerHub account yet, it will give you a website to create one. After logging in, we must build the image locally, <span class="No-Break">like so:</span></p><pre class="source-code">docker build –t &lt;USERNAME&gt;/jokeapi:v1 .</pre><p class="list-inset">Remember to replace <strong class="source-inline">&lt;USERNAME&gt;</strong> with your real DockerHub username. We are changing the image name to <strong class="source-inline">jokeapi</strong> so that it’s easier to locate. If you’re running Docker on a Mac M1, it is important to set the <strong class="source-inline">--platform</strong> parameter to make the container image compatible with AMD64 machines. To do so, run the <span class="No-Break">following command:</span></p><pre class="source-code">docker build --platform linux/amd64 –t &lt;USERNAME&gt;/jokeapi:v1 .</pre></li>				<li>Now, we can push the image <span class="No-Break">to DockerHub:</span><pre class="source-code">
docker push &lt;USERNAME&gt;/jokeapi:v1</pre></li>				<li>Go to <a href="https://hub.docker.com/">https://hub.docker.com/</a> and log in. You should see your image listed in the <span class="No-Break"><strong class="bold">Repositories</strong></span><span class="No-Break"> section.</span></li>
			</ol>
			<p>Now, the image is available<a id="_idIndexMarker164"/> for Kubernetes. Next, we need to define some Kubernetes resources to run our API. We will create a deployment and <span class="No-Break">a service.</span></p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor064"/>Creating the deployment</h2>
			<p>Follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>First, we’ll create <a id="_idIndexMarker165"/>the deployment. This specifies how many Pod replicas to run and <span class="No-Break">their configuration:</span><p class="list-inset"><span class="No-Break"><strong class="bold">deployment_api.yaml</strong></span></p><pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
    name: jokeapi
spec:
    replicas: 2
    selector:
    matchLabels:
      app: jokeapi
    template:
    metadata:
      labels:
        app: jokeapi
    spec:
      containers:
      - name: jokeapi
        image: &lt;USERNAME&gt;/jokeapi:v1
        imagePullPolicy: Always
        ports:
        - containerPort: 8087</pre><p class="list-inset">This will run two Pod replicas using the Docker image we built. Note that we are opening port <strong class="source-inline">8087</strong> in the container. This is similar to the <strong class="source-inline">EXPOSE</strong> command in <span class="No-Break">the Dockerfile.</span></p></li>				<li>Next, we will create a namespace to separate and organize our resources and apply <span class="No-Break">the deployment:</span><pre class="source-code">
kubectl create namespace jokeapi
kubectl apply -f deployment_api.yaml -n jokeapi</pre></li>				<li>This will create the deployment<a id="_idIndexMarker166"/> and the two Pod replicas in the <strong class="source-inline">jokeapi</strong> namespace. We can check that everything is working by running the <span class="No-Break">following command:</span><pre class="source-code">
kubectl get deployments -n jokeapi</pre><p class="list-inset">You should see the <span class="No-Break">following output:</span></p><pre class="source-code">NAME      READY   UP-TO-DATE    AVAILABLE      AGE
jokeapi    0/2        2              0          2s</pre></li>				<li>Now, let’s check if the Pods are <span class="No-Break">running properly:</span><pre class="source-code">
kubectl get pods -n jokeapi</pre><p class="list-inset">You should see an output <span class="No-Break">like this:</span></p><pre class="source-code">NAME                       READY   STATUS    RESTARTS
jokeapi-7d9877598d-bsj5b    1/1     Running    0
jokeapi-7d9877598d-qb8vs    1/1     Running    0</pre></li>			</ol>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor065"/>Creating a service</h2>
			<p>Follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>Here, we’ll specify<a id="_idIndexMarker167"/> a service to expose the Pods in <span class="No-Break">the cluster:</span><p class="list-inset"><span class="No-Break"><strong class="bold">service_api.yaml</strong></span></p><pre class="source-code">
apiVersion: v1
kind: Service
metadata:
    name: jokeapi
spec:
    selector:
    app: jokeapi
    ports:
    - protocol: TCP
      port: 80
      targetPort: 8087</pre><p class="list-inset">This creates a ClusterIP service that exposes the API Pods on an internal IP address in the cluster. Note that the <strong class="source-inline">type</strong> parameter is not specified within the <strong class="source-inline">spec</strong> section of the YAML file, so, it defaults <span class="No-Break">to ClusterIP.</span></p></li>				<li>To make the API accessible externally, we can create a load balancer (not possible with a local <strong class="source-inline">kind</strong> cluster, only with <span class="No-Break">cloud-based clusters):</span><p class="list-inset"><span class="No-Break"><strong class="bold">lb_api.yaml</strong></span></p><pre class="source-code">
apiVersion: v1
kind: Service
metadata:
    name: jokeapi
spec:
    selector:
    app: jokeapi
    type: LoadBalancer
    ports:
    - protocol: TCP
      port: 80
      targetPort: 8087</pre></li>				<li>Now, the <strong class="source-inline">type</strong> definition is set and this code will define a load balancer and assign an external IP. Next, we will deploy the load <span class="No-Break">balancer service:</span><pre class="source-code">
kubectl apply -f lb_api.yaml -n jokeapi</pre></li>				<li>Now, we can test if the API<a id="_idIndexMarker168"/> is accessible. First, we must get the load balancer’s URL (here, we are working in AWS) by running the <span class="No-Break">following command:</span><pre class="source-code">
kubectl get services -n jokeapi</pre><p class="list-inset">You should see the <span class="No-Break">following output:</span></p><pre class="source-code">NAME      TYPE          CLUSTER-IP      EXTERNAL-IP
jokeapi  LoadBalancer  10.100.251.249  &lt;DNS&gt;.amazonaws.com</pre></li>				<li>Copy the content under <strong class="source-inline">EXTERNAL-IP</strong>, paste the URL in a browser, and add <strong class="source-inline">/joke</strong>. For instance, in my implementation here, I got <strong class="source-inline">ab1cdd20ce1a349bab9af992211be654-1566834308.us-east-1.elb.amazonaws.com/joke</strong>. You should see the following response on <span class="No-Break">your screen:</span><pre class="source-code">
{"joke":"Have you heard about the chocolate record player? It sounds pretty sweet."}</pre></li>			</ol>
			<p>Success! We have a (great) joke in our browser! Now, we will deploy the API with an ingress instead of a load<a id="_idIndexMarker169"/> balancer (for cloud-based <span class="No-Break">clusters only).</span></p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/>Using an ingress to access the API</h2>
			<p>For this deployment, we will use<a id="_idIndexMarker170"/> the NGINX ingress controller<a id="_idIndexMarker171"/> and connect it to the load Balancer provided by AWS (the process is very similar if you are working with any cloud provider). Follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>First, we’ll create a new namespace for NGINX and deploy the controller <span class="No-Break">on Kubernetes:</span><pre class="source-code">
kubectl create namespace ingress-nginx
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.3/deploy/static/provider/baremetal/deploy.yaml -n ingress-nginx</pre></li>				<li>This will deploy the NGINX controller using the official manifests. Now, we have to edit one line in the deployment to make sure it uses the load balancer as an ingress deployment and not <strong class="source-inline">NodePort</strong>, its default. In your terminal, type <span class="No-Break">the following:</span><pre class="source-code">
kubectl edit service ingress-nginx-controller -n ingress-nginx</pre></li>				<li>Search for the <strong class="source-inline">spec.type</strong> field and change its value to <strong class="source-inline">LoadBalancer</strong>.<strong class="source-inline"> </strong>After saving the file, let’s check the services that have been deployed with <span class="No-Break">the controller:</span><pre class="source-code">
kubectl get services -n ingress-nginx</pre></li>				<li>We will see that the <strong class="source-inline">ingress-nginx-controller</strong> service is set to<strong class="source-inline"> LoadBalancer</strong> and has an external IP related to it. Now, it is easy to set up an ingress that points to this ingress controller. First, we’ll create a service defined in the <strong class="source-inline">service_api.yaml</strong> file. This service should be set to a ClusterIP type (see the code in the previous section). Then, we can define an ingress with the <span class="No-Break">following code:</span><p class="list-inset"><span class="No-Break"><strong class="bold">ingress.yaml</strong></span></p><pre class="source-code">
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
    name: jokeapi-ingress
spec:
    rules:
    - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: jokeapi
            port:
              number: 80</pre></li>				<li>This ingress will route<a id="_idIndexMarker172"/> external traffic<a id="_idIndexMarker173"/> to the internal service IP. Once the ingress has an external IP assigned, we should be able to access our API by hitting that URL. Type <span class="No-Break">the following</span><pre class="source-code">
kubectl get services –n ingress-nginx</pre></li>				<li>Get the external<a id="_idIndexMarker174"/> URL for the controller<a id="_idIndexMarker175"/> and add <strong class="source-inline">/joke</strong> <span class="No-Break">to it:</span><pre class="source-code">
{"joke":"What do you call a fish wearing a bowtie? Sofishticated."}</pre></li>			</ol>
			<p>Et voilà! In the next section, we will deploy our data processing job <span class="No-Break">on Kubernetes.</span></p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor067"/>Running a data processing job in Kubernetes</h1>
			<p>In this section, we will deploy<a id="_idIndexMarker176"/> the simple data processing job<a id="_idIndexMarker177"/> from <a href="B21927_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a> on Kubernetes. We have already developed the job (<a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/run.py">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/run.py</a>) and built a Dockerfile to package it into a container <span class="No-Break">image (</span><a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/Dockerfile_job"><span class="No-Break">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/Dockerfile_job</span></a><span class="No-Break">).</span></p>
			<p>Now, we have to build a Docker image and push it to a repository that’s accessible <span class="No-Break">to Kubernetes.</span></p>
			<pre class="console">
docker build --platform linux/amd64 –f Dockerfile_job –t &lt;USERNAME&gt;/dataprocessingjob:v1 .
docker push &lt;USERNAME&gt;/dataprocessingjob:v1</pre>			<p>Now, we can create a Kubernetes job to run our data processing task. Here’s an example <span class="No-Break">job manifest:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">job.yaml</p>
			<pre class="source-code">
apiVersion: batch/v1
kind: Job
metadata:
    name: dataprocessingjob
spec:
    template:
    spec:
      containers:
      - name: dataprocessingjob
        image: &lt;USERNAME&gt;/dataprocessingjob:v1
      restartPolicy: Never
    backoffLimit: 4</pre>			<p>This configures a job named <strong class="source-inline">dataprocessingjob</strong> that will run one replica of the <strong class="source-inline">&lt;USERNAME&gt;/dataprocessingjob:v1</strong> image. Now, we can create a new namespace and deploy the job, <span class="No-Break">like so:</span></p>
			<pre class="console">
kubectl create namespace datajob
kubectl apply -f job.yaml -n datajob</pre>			<p>This defines a job<a id="_idIndexMarker178"/> called <strong class="source-inline">dataprocessingjob</strong> that will run a single Pod <a id="_idIndexMarker179"/>using our Docker image. We set <strong class="source-inline">restartPolicy: Never</strong> since we want the container to run to completion rather <span class="No-Break">than restart.</span></p>
			<p>We can check the status of the job <span class="No-Break">like so:</span></p>
			<pre class="console">
kubectl get jobs –n datajob</pre>			<p>After the job has been completed, we will <span class="No-Break">see </span><span class="No-Break"><strong class="source-inline">1/1</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
NAME                COMPLETIONS    DURATION      AGE
dataprocessingjob      1/1           8s          11s</pre>			<p>To view the logs from our job, we can use <strong class="source-inline">kubectl logs</strong> on the Pod created by <span class="No-Break">the job:</span></p>
			<pre class="console">
kubectl get pods -n datajob
kubectl logs &lt;NAMEOFTHEPOD&gt; -n datajob</pre>			<p>In my case, I typed <span class="No-Break">the following:</span></p>
			<pre class="console">
kubectl logs dataprocessingjob-g8lkm -n datajob</pre>			<p>I got the <span class="No-Break">following results:</span></p>
			<pre class="console">
Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 'newcolumn'], dtype='object')
   0    1   2   3    4     5      6   7  8    newcolumn
0  6  148  72  35    0  33.6  0.627  50  1       67.2
1  1   85  66  29    0  26.6  0.351  31  0       53.2
2  8  183  64   0    0  23.3  0.672  32  1       46.6
3  1   89  66  23   94  28.1  0.167  21  0       56.2
4  0  137  40  35  168  43.1  2.288  33  1       86.2
(768, 10)</pre>			<p>This will print the application<a id="_idIndexMarker180"/> output from our Python program<a id="_idIndexMarker181"/> so that we can verify it ran correctly. And that’s it! You ran a data processing job <span class="No-Break">inside Kubernetes!</span></p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor068"/>Summary</h1>
			<p>In this chapter, we gained hands-on experience deploying Kubernetes clusters and running applications in them. We started by installing <strong class="source-inline">kubectl</strong> and deploying a local Kubernetes cluster using Kind. Then, we deployed managed Kubernetes clusters on AWS, GCP, and Azure. While the cloud providers differ, Kubernetes provides a consistent environment to <span class="No-Break">run containers.</span></p>
			<p>After setting up our clusters, we containerized and deployed the simple API application from <a href="B21927_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>. This demonstrated how to define Kubernetes deployments, services, ingress, and load balancers to run web applications. Then, we deployed the data processing batch job from <a href="B21927_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a> as a Kubernetes job. This showed us how to leverage Kubernetes for running one-off tasks <span class="No-Break">and jobs.</span></p>
			<p>By going through the process of deploying clusters and applications end-to-end, you now have first-hand experience with Kubernetes. You understand how to package applications as containers, expose them via services, ingress, or load balancers, and leverage Kubernetes abstractions such as deployments and jobs. With these skills, you are equipped to run applications and workloads on Kubernetes in development or <span class="No-Break">production environments.</span></p>
			<p>In the next chapter, we are going to take a closer look at the modern data stack, understanding each technology, why they are important, and how they link together to build a <span class="No-Break">data solution.</span></p>
		</div>
	

		<div class="Content" id="_idContainer018">
			<h1 id="_idParaDest-69" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor069"/>Part 2:  Big Data Stack</h1>
		</div>
		<div id="_idContainer019">
			<p>In this part, you will dive into the core technologies that make up the <strong class="bold">modern data stack</strong>, a set of tools and architectures designed for building robust and scalable data pipelines. You will gain a solid understanding of the Lambda architecture and its components, and gain some hands-on experience with powerful big data tools such as Apache Spark, Apache Airflow, and <span class="No-Break">Apache Kafka.</span></p>
			<p>This part contains the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B21927_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a>, <em class="italic">The Modern Data Stack</em></li>
				<li><a href="B21927_05.xhtml#_idTextAnchor092"><em class="italic">Chapter 5</em></a>, <em class="italic">Big Data Processing with Apache Spark</em></li>
				<li><a href="B21927_06.xhtml#_idTextAnchor112"><em class="italic">Chapter 6</em></a>, <em class="italic">Apache Airflow for Building Pipelines</em></li>
				<li><a href="B21927_07.xhtml#_idTextAnchor122"><em class="italic">Chapter 7</em></a>, <em class="italic">Apache Kafka for Real-Time Events and Data Ingestion</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer020">
			</div>
		</div>
		<div>
			<div class="Basic-Graphics-Frame" id="_idContainer021">
			</div>
		</div>
	</body></html>