- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Optimizing GPU Resources for GenAI Applications in Kubernetes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes中优化GPU资源以适应GenAI应用
- en: This chapter will cover strategies to maximize **graphics processing unit**
    (**GPU**) ([https://aws.amazon.com/what-is/gpu/](https://aws.amazon.com/what-is/gpu/))
    efficiency in K8s when deploying GenAI applications as GPU instances are very
    expensive and often underutilized. We will also cover GPU resource management,
    scheduling best practices, and partitioning options such as **Multi-Instance GPU**
    (**MIG**) ([https://www.nvidia.com/en-us/technologies/multi-instance-gpu/](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)),
    **Multi-Process Service** (**MPS**) ([https://docs.nvidia.com/deploy/mps/index.html](https://docs.nvidia.com/deploy/mps/index.html)),
    and **GPU time-slicing** ([https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html)).
    Finally, we’ll discuss monitoring GPU performance, balancing workloads across
    nodes, and auto-scaling GPU resources to handle dynamic GenAI workloads effectively.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论在K8s中部署GenAI应用时最大化**图形处理单元**（**GPU**）（[https://aws.amazon.com/what-is/gpu/](https://aws.amazon.com/what-is/gpu/)）效率的策略，因为GPU实例非常昂贵，且经常被低效使用。我们还将讨论GPU资源管理、调度最佳实践，以及分区选项，如**多实例GPU**（**MIG**）（[https://www.nvidia.com/en-us/technologies/multi-instance-gpu/](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)）、**多进程服务**（**MPS**）（[https://docs.nvidia.com/deploy/mps/index.html](https://docs.nvidia.com/deploy/mps/index.html)）和**GPU时间切片**（[https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html)）。最后，我们将讨论如何监控GPU性能、跨节点平衡工作负载以及自动扩展GPU资源以有效应对动态GenAI工作负载。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下主要内容：
- en: GPUs and custom accelerators
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU和自定义加速器
- en: Allocating GPU resources in K8s
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在K8s中分配GPU资源
- en: Understanding GPU utilization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解GPU利用率
- en: Techniques for partitioning and sharing GPUs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割和共享GPU的技术
- en: Scaling and optimization considerations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展性和优化考虑
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be using the following tools, some of which require
    you to set up an account and create an access token:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下工具，其中一些工具需要你设置账户并创建访问令牌：
- en: '**Hugging** **Face**: [https://huggingface.co/join](https://huggingface.co/join).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging** **Face**: [https://huggingface.co/join](https://huggingface.co/join)。'
- en: 'The **Llama-3.2-1B** model, which can be accessed via Hugging Face: [https://huggingface.co/meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Llama-3.2-1B**模型，可以通过Hugging Face访问：[https://huggingface.co/meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B)。'
- en: An **Amazon EKS cluster**, as illustrated in [*Chapter 3*](B31108_03.xhtml#_idTextAnchor039).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如[ *第3章* ](B31108_03.xhtml#_idTextAnchor039)所示的**Amazon EKS集群**。
- en: '**AWS Service Quotas** to run family EC2 instances. You can request a quota
    increase in the **AWS** **console** ([https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html](https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html)).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS服务配额**用于运行家庭EC2实例。你可以在**AWS** **控制台**中申请配额增加（[https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html](https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html)）。'
- en: GPUs and custom accelerators
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU和自定义加速器
- en: Deploying GenAI workloads in K8s requires selecting the right hardware based
    on the computational requirements of the workload, such as training, inference,
    or microservices implementation. Compute options include CPUs, GPUs, or custom
    accelerators such as **Inferentia** ([https://aws.amazon.com/ai/machine-learning/inferentia/](https://aws.amazon.com/ai/machine-learning/inferentia/))
    and **Trainium** ([https://aws.amazon.com/ai/machine-learning/trainium/](https://aws.amazon.com/ai/machine-learning/trainium/)),
    as well as accelerators from AWS or **tensor processing units** (**TPUs**) ([https://cloud.google.com/tpu](https://cloud.google.com/tpu))
    from Google Cloud.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在K8s中部署GenAI工作负载需要根据工作负载的计算需求（如训练、推理或微服务实现）选择合适的硬件。计算选项包括CPU、GPU，或者自定义加速器，如**Inferentia**（[https://aws.amazon.com/ai/machine-learning/inferentia/](https://aws.amazon.com/ai/machine-learning/inferentia/)）和**Trainium**（[https://aws.amazon.com/ai/machine-learning/trainium/](https://aws.amazon.com/ai/machine-learning/trainium/)），以及来自AWS的加速器或**张量处理单元**（**TPU**）（[https://cloud.google.com/tpu](https://cloud.google.com/tpu)）来自Google
    Cloud。
- en: CPUs are usually the *default compute resource* in K8s and are suitable for
    lightweight GenAI tasks, including small-scale inference, data preprocessing,
    exposing APIs, and implementing classical ML algorithms such as **XGBoost** for
    decision trees ([https://xgboost.readthedocs.io/en/stable/](https://xgboost.readthedocs.io/en/stable/)).
    However, they are less efficient for tasks that require high parallelism and a
    very large number of matrix multiplications, such as training foundational models.
    K8s lets you define both CPU requests and limits, ensuring a fair and efficient
    allocation of resources among all workloads.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 通常是 K8s 中的*默认计算资源*，适合轻量级的 GenAI 任务，包括小规模推理、数据预处理、暴露 API 和实现经典的机器学习算法，如**XGBoost**（[https://xgboost.readthedocs.io/en/stable/](https://xgboost.readthedocs.io/en/stable/)）决策树。然而，它们在需要高并行度和大量矩阵乘法的任务（例如训练基础模型）上效率较低。K8s
    允许您定义 CPU 请求和限制，确保所有工作负载之间的资源分配公平且高效。
- en: GPUs excel at **massively parallel processing** (**MPP**) because they feature
    thousands of cores and very high memory bandwidth, allowing them to handle the
    matrix multiplications and linear algebra computations that are central to deep
    learning far more efficiently than CPUs. However, GPUs are not recognized natively
    by K8s. Thanks to its extensible architecture, device vendors can develop **device
    plugins** ([https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/))
    that expose GPU resources to the K8s control plane. These plugins are typically
    deployed as **DaemonSets** ([https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/))
    in the cluster, enabling the K8s scheduler to identify, allocate, and manage GPUs
    for containerized workloads properly. One of the most popular device plugins is
    the **NVIDIA device plugin for Kubernetes** ([https://github.com/NVIDIA/k8s-device-plugin](https://github.com/NVIDIA/k8s-device-plugin)),
    which exposes NVIDIA GPUs attached to each K8s worker node and continuously tracks
    their health.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 擅长于**大规模并行处理**（**MPP**），因为它们拥有成千上万的核心和非常高的内存带宽，这使得它们比 CPU 更有效地处理深度学习中核心的矩阵乘法和线性代数计算。然而，GPU
    并未在 K8s 中被原生识别。得益于其可扩展的架构，设备厂商可以开发**设备插件**（[https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)），将
    GPU 资源暴露给 K8s 控制平面。这些插件通常作为**守护进程集**（**DaemonSets**）（[https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/)）在集群中部署，使得
    K8s 调度器能够正确地识别、分配和管理容器化工作负载的 GPU。最受欢迎的设备插件之一是**NVIDIA Kubernetes 设备插件**（[https://github.com/NVIDIA/k8s-device-plugin](https://github.com/NVIDIA/k8s-device-plugin)），它将每个
    K8s 工作节点上附加的 NVIDIA GPU 暴露出来，并持续监控其健康状态。
- en: As an alternative to GPUs, many companies are investing in creating *purpose-built
    accelerators* tailored for AI/ML workloads. Examples include **AWS Inferentia**
    and **Trainium**, **Google TPUs**, **field-programmable gate arrays** (**FPGAs**)
    ([https://www.arm.com/glossary/fpga](https://www.arm.com/glossary/fpga)), and
    various **application-specific integrated circuits** (**ASICs**) ([https://www.arm.com/glossary/asic](https://www.arm.com/glossary/asic)),
    each designed to excel at core operations such as matrix multiplication, utilizing
    transformer-based models, delivering higher performance, and ensuring lower energy
    consumption. Similar to GPUs, these accelerators integrate with K8s through *custom
    device plugins* provided by hardware vendors. These plugins discover, allocate,
    and monitor the specialized hardware resources attached to K8s worker nodes, enabling
    seamless scheduling and management alongside other compute resources.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 GPU 的替代方案，许多公司正在投资创建*专用加速器*，专为 AI/ML 工作负载量身定制。例子包括**AWS Inferentia** 和 **Trainium**，**Google
    TPUs**，**现场可编程门阵列**（**FPGAs**）（[https://www.arm.com/glossary/fpga](https://www.arm.com/glossary/fpga)），以及各种**特定应用集成电路**（**ASICs**）（[https://www.arm.com/glossary/asic](https://www.arm.com/glossary/asic)），它们都设计用于擅长核心操作，如矩阵乘法、利用基于
    Transformer 的模型，提供更高的性能，并确保更低的能耗。与 GPU 类似，这些加速器通过硬件厂商提供的*自定义设备插件*与 K8s 集成。这些插件可以发现、分配并监控附加到
    K8s 工作节点上的专用硬件资源，从而与其他计算资源一起实现无缝调度和管理。
- en: Custom accelerators are particularly effective for large-scale training or low-latency
    inference. For example, **AWS Trainium** ([https://aws.amazon.com/ai/machine-learning/trainium/](https://aws.amazon.com/ai/machine-learning/trainium/))
    is a family of AI chips developed by AWS to enhance GenAI training by delivering
    high performance while reducing costs. The first-generation Trainium chips powered
    **Amazon EC2 Trn1 instances** ([https://aws.amazon.com/ec2/instance-types/trn1/](https://aws.amazon.com/ec2/instance-types/trn1/))
    and offer up to 50% lower training costs compared to comparable EC2 instances.
    The Trainium2 chips, featured in **Amazon EC2 Trn2 instances** and **Trn2 UltraServers**
    ([https://aws.amazon.com/ec2/instance-types/trn2/](https://aws.amazon.com/ec2/instance-types/trn2/)),
    are the most powerful EC2 instances for training and inferencing of GenAI models
    with hundreds of billions to trillions of parameters. They provide up to four
    times the performance of their predecessors and 30% to 40% better price performance
    than EC2 P5e and P5en family instances.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义加速器特别适用于大规模训练或低延迟推理。例如，**AWS Trainium**([https://aws.amazon.com/ai/machine-learning/trainium/](https://aws.amazon.com/ai/machine-learning/trainium/))是一系列由
    AWS 开发的 AI 芯片，通过提供高性能并降低成本来增强 GenAI 训练。第一代 Trainium 芯片驱动了**Amazon EC2 Trn1 实例**([https://aws.amazon.com/ec2/instance-types/trn1/](https://aws.amazon.com/ec2/instance-types/trn1/))，与类似的
    EC2 实例相比，提供了最多 50% 的训练成本节省。Trainium2 芯片，出现在**Amazon EC2 Trn2 实例**和**Trn2 UltraServers**([https://aws.amazon.com/ec2/instance-types/trn2/](https://aws.amazon.com/ec2/instance-types/trn2/))中，是训练和推理
    GenAI 模型（拥有数百亿到万亿参数）时最强大的 EC2 实例。它们提供了比前代产品高达四倍的性能，并且比 EC2 P5e 和 P5en 系列实例提供了
    30% 至 40% 更好的价格性能比。
- en: Custom accelerators often rely on specialized hardware architectures and instruction
    sets that differ from general-purpose CPUs or GPUs. Because of this, AI/ML frameworks
    such as **TensorFlow** and **PyTorch** cannot natively translate high-level operations
    into low-level instructions that accelerators understand. The **Neuron SDK** ([https://aws.amazon.com/ai/machine-learning/neuron/](https://aws.amazon.com/ai/machine-learning/neuron/))
    is a software development kit designed by AWS to run and optimize AI/ML workloads
    efficiently on AWS’s custom AI accelerators, such as AWS Trainium and Inferentia.
    It includes a compiler, runtime, training and inference libraries, and profiling
    tools. Neuron supports customers throughout their end-to-end ML development life
    cycle, including building and deploying deep learning and AI models. The Neuron
    SDK provides seamless integration with popular ML frameworks such as **PyTorch**
    ([https://pytorch.org/](https://pytorch.org/)), **TensorFlow** ([https://www.tensorflow.org/](https://www.tensorflow.org/)),
    and **JAX** ([https://jax.readthedocs.io/](https://jax.readthedocs.io/)) while
    supporting over 100,000 models, including those from Hugging Face. Customers,
    such as *Databricks*, have reported significant performance improvements and cost
    savings of up to 30% when using Trainium-powered instances ([https://aws.amazon.com/ai/machine-learning/trainium/customers/](https://aws.amazon.com/ai/machine-learning/trainium/customers/)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义加速器通常依赖于与通用 CPU 或 GPU 不同的专用硬件架构和指令集。因此，像**TensorFlow**和**PyTorch**这样的 AI/ML
    框架无法将高级操作直接转换为加速器可以理解的低级指令。[**Neuron SDK**](https://aws.amazon.com/ai/machine-learning/neuron/)是
    AWS 设计的软件开发工具包，用于在 AWS 的定制 AI 加速器上高效运行和优化 AI/ML 工作负载，如 AWS Trainium 和 Inferentia。它包括编译器、运行时、训练和推理库以及性能分析工具。Neuron
    在整个 ML 开发生命周期中为客户提供支持，包括构建和部署深度学习和 AI 模型。Neuron SDK 与流行的 ML 框架无缝集成，如**PyTorch**([https://pytorch.org/](https://pytorch.org/))、**TensorFlow**([https://www.tensorflow.org/](https://www.tensorflow.org/))和**JAX**([https://jax.readthedocs.io/](https://jax.readthedocs.io/))，同时支持超过
    100,000 个模型，包括来自 Hugging Face 的模型。客户，如*Databricks*，报告称使用 Trainium 驱动的实例时，性能显著提升，成本节省高达
    30%([https://aws.amazon.com/ai/machine-learning/trainium/customers/](https://aws.amazon.com/ai/machine-learning/trainium/customers/))。
- en: Choosing between CPUs, GPUs, and accelerators requires balancing performance
    needs, workload intensity, and budget constraints to optimize resource utilization
    for GenAI workloads. With this overview of custom accelerators, let’s dive deeper
    into allocating GPU resources to GenAI applications in K8s.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择 CPU、GPU 和加速器时，需要平衡性能需求、工作负载强度和预算限制，以优化 GenAI 工作负载的资源利用率。通过对自定义加速器的概述，让我们更深入地探讨如何在
    K8s 中为 GenAI 应用分配 GPU 资源。
- en: Allocating GPU resources in K8s
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 K8s 中分配 GPU 资源
- en: To use GPU and custom accelerator resources in K8s, you must use the corresponding
    device plugins. For instance, the **NVIDIA device plugin for Kubernetes** makes
    NVIDIA GPUs recognizable and schedulable by the K8s cluster, while the **Neuron
    device plugin** does the same for AWS Trainium and Inferentia accelerators. This
    mechanism ensures that any custom accelerator is discovered, allocated, and managed
    properly within the K8s cluster.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 K8s 中使用 GPU 和自定义加速器资源，必须使用相应的设备插件。例如，**NVIDIA Kubernetes 设备插件**使得 K8s 集群能够识别和调度
    NVIDIA GPU，而 **Neuron 设备插件**则使 AWS Trainium 和 Inferentia 加速器也能被 K8s 集群识别和调度。此机制确保任何自定义加速器都能在
    K8s 集群中正确地被发现、分配和管理。
- en: Apart from installing the device plugin, you should also ensure that the respective
    GPU/accelerator drivers are present on the underlying operating system. AWS offers
    accelerated AMIs for NVIDIA and the Trainium and Inferentia accelerators, all
    of which you can use to launch K8s worker nodes. These AMIs include NVIDIA, Neuron
    drivers, **nvidia-container-toolkit** ([https://github.com/NVIDIA/nvidia-container-toolkit](https://github.com/NVIDIA/nvidia-container-toolkit)),
    and others on top of the standard EKS-optimized AMI. Please refer to the AWS documentation
    at [https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html#gpu-ami](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html#gpu-ami)
    for more information.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除了安装设备插件，还应确保底层操作系统上已安装相应的 GPU/加速器驱动程序。AWS 提供了用于 NVIDIA 以及 Trainium 和 Inferentia
    加速器的加速 AMI，您可以使用这些 AMI 启动 K8s 工作节点。这些 AMI 包含 NVIDIA 和 Neuron 驱动程序，**nvidia-container-toolkit**
    ([https://github.com/NVIDIA/nvidia-container-toolkit](https://github.com/NVIDIA/nvidia-container-toolkit))
    等，都是基于标准的 EKS 优化 AMI。有关更多信息，请参考 AWS 文档 [https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html#gpu-ami](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html#gpu-ami)。
- en: 'A high-level architecture of the K8s device plugin framework is depicted in
    *Figure 10**.1*. Let’s look at what steps are involved in this process:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: K8s 设备插件框架的高级架构如*图 10.1*所示。让我们看看这个过程涉及的步骤：
- en: K8s device plugins are deployed as **DaemonSets**, ensuring that all (or some)
    nodes run a copy of the plugin Pod. Typically, these plugins are scheduled on
    worker nodes with specific labels (GPU, custom accelerator) to optimize resource
    utilization.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K8s 设备插件作为 **DaemonSets** 部署，确保所有（或部分）节点都运行插件 Pod 的副本。通常，这些插件会被调度到具有特定标签（GPU、自定义加速器）的工作节点上，以优化资源利用。
- en: Upon initialization, the device plugin performs various vendor-specific initialization
    tasks and ensures devices are in a ready state.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在初始化时，设备插件会执行各种特定厂商的初始化任务，并确保设备处于就绪状态。
- en: The plugin registers itself with the kubelet and declares the custom resources
    it manages – for example, `nvidia.com/gpu` for NVIDIA GPUs and `aws.amazon.com/neuroncore`
    for AWS Trainium/Inferentia devices.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插件将自己注册到 kubelet，并声明它管理的自定义资源——例如，`nvidia.com/gpu` 用于 NVIDIA GPU，`aws.amazon.com/neuroncore`
    用于 AWS Trainium/Inferentia 设备。
- en: 'After successful registration, the plugin provides the kubelet with the list
    of managed devices. The kubelet then performs a node status update to advertise
    these resources to the K8s API server. This can be verified by running the following
    command:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注册成功后，插件会向 kubelet 提供托管设备的列表。然后，kubelet 会执行节点状态更新，将这些资源通告给 K8s API 服务器。可以通过运行以下命令来验证：
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When a Pod is scheduled on the worker node, the kubelet notifies the device
    plugin of the container’s requirement (the number of GPUs) so that it can perform
    the necessary preparation tasks to allocate the requested resources.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 Pod 被调度到工作节点时，kubelet 会通知设备插件容器的需求（GPU 数量），以便它执行必要的准备任务来分配请求的资源。
- en: The device plugin continuously monitors the health of the devices it manages
    and updates the kubelet accordingly to ensure optimal resource availability.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设备插件会持续监控它所管理设备的健康状态，并相应地更新 kubelet，以确保资源的最佳可用性。
- en: '![Figure 10.1 – K8s device plugin architecture](img/B31108_10_1.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – K8s 设备插件架构](img/B31108_10_1.jpg)'
- en: Figure 10.1 – K8s device plugin architecture
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – K8s 设备插件架构
- en: 'In our walkthrough in [*Chapter 5*](B31108_05.xhtml#_idTextAnchor062), we installed
    the NVIDIA device plugin using the `eks-data-addons` Terraform module, as shown
    here:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们在[*第 5 章*](B31108_05.xhtml#_idTextAnchor062)的演示中，我们使用 `eks-data-addons` Terraform
    模块安装了 NVIDIA 设备插件，如下所示：
- en: '[PRE1]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It used the Terraform Helm provider to deploy the **NVIDIA device plugin Helm
    chart** ([https://github.com/NVIDIA/k8s-device-plugin?tab=readme-ov-file#deployment-via-helm](https://github.com/NVIDIA/k8s-device-plugin?tab=readme-ov-file#deployment-via-helm))
    in the EKS cluster. You can verify which Helm release and DeamonSet are being
    used by running the following commands:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它使用 Terraform Helm 提供程序在 EKS 集群中部署了**NVIDIA 设备插件 Helm 图表**([https://github.com/NVIDIA/k8s-device-plugin?tab=readme-ov-file#deployment-via-helm](https://github.com/NVIDIA/k8s-device-plugin?tab=readme-ov-file#deployment-via-helm))。您可以通过运行以下命令验证正在使用的
    Helm 版本和 DaemonSet：
- en: '[PRE2]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The next step is to launch GPU worker nodes and register them with the cluster.
    This was covered in the [*Chapter 5*](B31108_05.xhtml#_idTextAnchor062) walkthrough,
    where we created a new EKS-managed node group called `eks-gpu-mng` using the G6
    family of EC2 instances. Additionally, K8s taints were applied to these nodes
    to ensure that only Pods requiring GPU resources were scheduled on them:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是启动 GPU 工作节点并将其注册到集群中。这在[*第5章*](B31108_05.xhtml#_idTextAnchor062)的操作指南中有介绍，我们创建了一个名为`eks-gpu-mng`的新
    EKS 管理节点组，使用的是 G6 系列的 EC2 实例。此外，还对这些节点应用了 K8s 污点，以确保只有需要 GPU 资源的 Pods 会被调度到这些节点上：
- en: '[PRE3]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'GPU worker nodes can also be labeled so that advanced scheduling can be implemented
    – for example, we can use `hardware-type=gpu` to identify GPU-enabled nodes. This
    makes it possible to target specific nodes when scheduling workloads that require
    GPUs. We can do this by using the `kubectl` command or running the necessary Terraform
    code:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPU 工作节点还可以进行标记，从而实现高级调度。例如，我们可以使用 `hardware-type=gpu` 来标识启用了 GPU 的节点。这使得在调度需要
    GPU 的工作负载时可以针对特定节点。我们可以通过使用 `kubectl` 命令或运行必要的 Terraform 代码来实现：
- en: '[PRE4]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can also label the K8s worker nodes using Terraform, as shown in the following
    code snippet:'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还可以使用 Terraform 对 K8s 工作节点进行标记，如以下代码片段所示：
- en: '[PRE5]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The next step is to allocate GPU resources for containerized workloads, such
    as GenAI training and inference workloads. To request GPUs, you need to specify
    resource requests and limits in your K8s deployment and Pod specifications. The
    following code snippet demonstrates how to do the following:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是为容器化工作负载分配 GPU 资源，例如 GenAI 训练和推理工作负载。要请求 GPU 资源，需要在 K8s 部署和 Pod 规格中指定资源请求和限制。以下代码片段演示了如何执行以下操作：
- en: Assign tolerations to the K8s Pod that match the node taints
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 K8s Pod 分配与节点污点匹配的容忍度
- en: 'Schedule the K8s Pods specifically on nodes labeled with `hardware-type: gpu`'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '将 K8s Pods 专门调度到标有 `hardware-type: gpu` 的节点上'
- en: 'Request one GPU resource using the `nvidia.com/gpu` attribute:'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `nvidia.com/gpu` 属性请求一个 GPU 资源：
- en: '[PRE6]'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When requesting GPU resources in K8s, you must always define them in the *limits*
    section of the specification, either alone or with matching request values; specifying
    only *requests* without *limits* is not allowed. Similarly, when using AWS Trainium/Inferentia
    accelerators, you can use the `aws.amazon.com/neuroncore` attribute to request
    the resources.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 K8s 中请求 GPU 资源时，必须始终在规格的*limits*部分定义它们，可以单独定义，也可以与匹配的请求值一起定义；仅指定*requests*而不指定*limits*是不允许的。同样，在使用
    AWS Trainium/Inferentia 加速器时，可以使用 `aws.amazon.com/neuroncore` 属性来请求资源。
- en: In this section, we started by looking at the K8s device plugin architecture
    and the steps involved in creating the worker nodes in an EKS cluster. We also
    looked at K8s scheduling techniques such as taints, tolerations, and node selectors,
    all of which we can use to schedule the GPU Pods to their respective nodes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本节开始时，我们研究了 K8s 设备插件架构以及在 EKS 集群中创建工作节点的步骤。我们还了解了 K8s 调度技术，如污点、容忍度和节点选择器，这些都可以用于将
    GPU Pods 调度到各自的节点上。
- en: Understanding GPU utilization
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 GPU 利用率
- en: GPUs constitute a major cost of running GenAI workloads, so utilizing them effectively
    is paramount in achieving optimal performance and cost-efficiency. Without proper
    monitoring, underutilized GPUs can result in compute inefficiencies and increased
    operational expenses, while overutilized GPUs risk request throttling and potential
    application failures. In this section, we will explore solutions for monitoring
    GPU utilization while focusing on exporting metrics and leveraging them to implement
    efficient autoscaling strategies.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 构成了运行 GenAI 工作负载的主要成本，因此有效利用它们对于实现最佳性能和成本效益至关重要。如果没有适当的监控，GPU 未充分利用可能导致计算效率低下和运营成本增加，而
    GPU 过度利用则可能导致请求限制和潜在的应用程序故障。本节将探讨监控 GPU 利用率的解决方案，重点介绍如何导出指标并利用它们实现高效的自动扩展策略。
- en: NVIDIA Data Center GPU Manager (DCGM)
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NVIDIA 数据中心 GPU 管理器（DCGM）
- en: '**DCGM** ([https://developer.nvidia.com/dcgm](https://developer.nvidia.com/dcgm))
    is a lightweight library and agent designed to simplify the management of NVIDIA
    GPUs, enabling users, developers, and system administrators to monitor and manage
    GPUs across clusters or data centers.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**DCGM** ([https://developer.nvidia.com/dcgm](https://developer.nvidia.com/dcgm))
    是一个轻量级的库和代理，旨在简化NVIDIA GPU的管理，使用户、开发人员和系统管理员能够监控和管理跨集群或数据中心的GPU。'
- en: DCGM provides functionality such as GPU health diagnostics, behavior monitoring,
    configuration management, telemetry collection, and policy automation. It operates
    both as a *standalone service* through the NVIDIA host engine and as an *embedded
    component* within third-party management tools. Its key features include GPU health
    diagnostics, job-level telemetry, group-centric resource management for multiple
    GPUs or hosts, and automated management policies to enhance reliability and simplify
    administration tasks. DCGM can integrate seamlessly with K8s tools such as the
    NVIDIA GPU Operator, allowing for telemetry collection and health checks in containerized
    environments. With support for exporting metrics to systems such as Prometheus,
    DCGM also facilitates real-time visualization and analysis of GPU data in tools
    such as Grafana.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: DCGM提供了GPU健康诊断、行为监控、配置管理、遥测收集和策略自动化等功能。它既可以作为通过NVIDIA主机引擎的*独立服务*运行，也可以作为嵌入式组件在第三方管理工具中运行。其主要特性包括GPU健康诊断、作业级遥测、多GPU或主机的组中心资源管理，以及自动化管理策略，以增强可靠性并简化管理任务。DCGM可以与K8s工具如NVIDIA
    GPU Operator无缝集成，实现容器化环境中的遥测收集和健康检查。通过支持将度量数据导出到诸如Prometheus等系统，DCGM还能够促进GPU数据在Grafana等工具中的实时可视化和分析。
- en: 'DCGM can be implemented in K8s clusters in the following ways:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: DCGM可以通过以下方式在K8s集群中实施：
- en: '`gpu-operator` ([https://github.com/NVIDIA/gpu-operator](https://github.com/NVIDIA/gpu-operator))
    leverages the **operator pattern** ([https://kubernetes.io/docs/concepts/extend-kubernetes/operator/](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/))
    within K8s to automate the process of managing all NVIDIA software components
    required for provisioning GPUs. These components include NVIDIA drivers (to enable
    CUDA), the K8s device plugin for GPUs, NVIDIA Container Runtime, automatic node
    labeling, DCGM-based monitoring, and more. This approach is ideal for environments
    where you want a fully automated solution for managing NVIDIA GPU resources, including
    installation, updates, and configuration.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gpu-operator` ([https://github.com/NVIDIA/gpu-operator](https://github.com/NVIDIA/gpu-operator))
    利用K8s中的**operator模式** ([https://kubernetes.io/docs/concepts/extend-kubernetes/operator/](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/))
    自动化管理为GPU提供所需的所有NVIDIA软件组件。这些组件包括NVIDIA驱动程序（以启用CUDA）、K8s设备插件（用于GPU）、NVIDIA容器运行时、自动节点标记、基于DCGM的监控等。此方法非常适合那些希望为NVIDIA
    GPU资源提供全面自动化解决方案的环境，包括安装、更新和配置。'
- en: '**DCGM-Exporter** ([https://github.com/NVIDIA/dcgm-exporter](https://github.com/NVIDIA/dcgm-exporter)):
    Built on top of the NVIDIA DCGM framework, DCGM-Exporter collects and exposes
    a wide range of GPU performance and health metrics, such as utilization, memory
    usage, temperature, and power consumption, in a Prometheus-compatible format.
    This facilitates seamless integration with popular monitoring and visualization
    tools such as Prometheus and Grafana. DCGM-Exporter is well-suited for scenarios
    where the necessary GPU components are already installed, and you need to enable
    GPU monitoring without additional overhead.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DCGM-Exporter** ([https://github.com/NVIDIA/dcgm-exporter](https://github.com/NVIDIA/dcgm-exporter))：建立在NVIDIA
    DCGM框架之上，DCGM-Exporter收集并以Prometheus兼容格式暴露广泛的GPU性能和健康度量数据，如利用率、内存使用、温度和功耗等。这有助于与Prometheus和Grafana等流行的监控和可视化工具无缝集成。DCGM-Exporter非常适用于那些GPU组件已安装，且您需要启用GPU监控而不增加额外负担的场景。'
- en: Please refer to the NVIDIA documentation at [https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/amazon-eks.html](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/amazon-eks.html)
    for detailed guidance on selecting the right approach based on your operational
    needs.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考NVIDIA文档中的[https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/amazon-eks.html](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/amazon-eks.html)以获取关于根据您的操作需求选择正确方法的详细指导。
- en: So far in our walkthrough, we have already installed the necessary components,
    such as the NVIDIA device plugin and relevant drivers. Therefore, we will use
    the second approach (DCGM-Exporter) to monitor the GPU’s health and utilization
    metrics. For the first approach, you can refer to the NVIDIA documentation at
    [https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operator-install-guide](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operator-install-guide)
    for detailed setup instructions. To proceed, we will install DCGM-Exporter using
    the Helm provider in Terraform. Begin by downloading the `aiml-addons.tf` file
    from [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/aiml-addons.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/aiml-addons.tf).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在我们的操作过程中，我们已经安装了必要的组件，如NVIDIA设备插件和相关驱动程序。因此，我们将使用第二种方法（DCGM-Exporter）来监控GPU的健康状况和使用情况指标。对于第一种方法，您可以参考NVIDIA文档，网址为[https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operator-install-guide](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operator-install-guide)，以获取详细的设置说明。接下来，我们将使用Terraform中的Helm提供者来安装DCGM-Exporter。首先，从[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/aiml-addons.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/aiml-addons.tf)下载`aiml-addons.tf`文件。
- en: 'The `dcgm-exporter` Helm chart will be deployed in the `dcgm-exporter` namespace
    from the [https://nvidia.github.io/dcgm-exporter/](https://nvidia.github.io/dcgm-exporter/)
    Helm repository, as shown in the following code snippet:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`dcgm-exporter` Helm图表将从[https://nvidia.github.io/dcgm-exporter/](https://nvidia.github.io/dcgm-exporter/)
    Helm仓库中部署到`dcgm-exporter`命名空间，如下所示的代码片段：'
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Execute the following commands to deploy the `dcgm-exporter` Helm chart in
    the EKS cluster and verify its installation using the following `kubectl` command.
    The output should confirm that `dcgm-exporter` has been deployed as a *DaemonSet*
    and its Pods are *Running*:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下命令将`dcgm-exporter` Helm图表部署到EKS集群，并使用以下`kubectl`命令验证其安装。输出应确认`dcgm-exporter`已作为*DaemonSet*部署，并且其Pod正在*运行*：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that DCGM-Exporter is functional, you can access the GPU’s health and utilization
    metrics by connecting to the DCGM-Exporter service. Execute the following commands
    to connect to the service locally and use a `curl` command to query the `/metrics`
    endpoint to view the GPU metrics:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，DCGM-Exporter已经正常运行，您可以通过连接到DCGM-Exporter服务来访问GPU的健康状况和利用率指标。执行以下命令以本地连接到该服务，并使用`curl`命令查询`/metrics`端点以查看GPU指标：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In [*Chapter 12*](B31108_12.xhtml#_idTextAnchor160), we will deploy and configure
    a Prometheus agent that will scrape these GPU metrics and visualize them using
    Grafana dashboards. Once the metrics are available in Prometheus, we can install
    the **Prometheus adapter** ([https://github.com/kubernetes-sigs/prometheus-adapter](https://github.com/kubernetes-sigs/prometheus-adapter))
    to create *autoscaling policies*, allowing GPU workloads to scale dynamically
    for optimal resource utilization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第12章*](B31108_12.xhtml#_idTextAnchor160)中，我们将部署并配置一个Prometheus代理，来抓取这些GPU指标并通过Grafana仪表板进行可视化。一旦指标在Prometheus中可用，我们可以安装**Prometheus适配器**([https://github.com/kubernetes-sigs/prometheus-adapter](https://github.com/kubernetes-sigs/prometheus-adapter))，以创建*自动扩展策略*，使GPU工作负载能够动态扩展，从而实现最佳资源利用。
- en: GPU utilization challenges
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU利用率挑战
- en: In K8s, allocating a GPU to a Pod reserves that GPU exclusively for the Pod’s
    entire life cycle, even if the Pod is not actively using it. K8s does not support
    sharing GPUs among multiple Pods or assigning partial GPUs (< 1) per Pod by default.
    This design ensures isolation and avoids conflicts as most GPUs are not inherently
    designed to handle concurrent workloads without specialized software.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在K8s中，将GPU分配给Pod会将该GPU独占分配给该Pod的整个生命周期，即使该Pod没有在积极使用它。默认情况下，K8s不支持在多个Pod之间共享GPU或为每个Pod分配部分GPU（<
    1）。这种设计确保了隔离，避免了冲突，因为大多数GPU本身并未设计为处理没有专用软件的并发工作负载。
- en: 'However, exclusive GPU allocation can lead to underutilization if a Pod does
    not fully utilize its assigned GPU. This challenge is especially pronounced in
    GenAI scenarios, where the following factors often come into play:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，独占GPU分配可能会导致资源利用不足，如果Pod没有充分利用其分配的GPU。这个挑战在生成AI场景中尤为突出，因为以下因素常常发挥作用：
- en: '**Varying model sizes (large versus small LLMs)**: While **state-of-the-art**
    (**SOTA**) LLMs may require an entire GPU or even multiple GPUs, there is a growing
    demand for smaller or distilled versions of these models. Depending on their size,
    these smaller LLMs may require a fraction of the GPU’s memory and compute capacity.
    With K8s’s default “whole GPU” allocation, even a smaller model that needs only
    part of a GPU will be allocated an entire device, leading to over-provisioning.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不同模型大小（大模型与小LLM）**：尽管**最先进的**（**SOTA**）LLM可能需要整个GPU甚至多个GPU，但对这些模型的更小或蒸馏版本的需求也在增加。根据它们的大小，这些较小的LLM可能只需要GPU的一小部分内存和计算能力。在K8s的默认“整块GPU”分配模式下，即使是只需要部分GPU的小模型，也会被分配整个设备，导致资源过度配置。'
- en: '**Dynamic workload patterns**: GenAI workloads such as fine-tuning or training
    LLMs or running inference for complex diffusion-based models can create bursty
    GPU usage patterns. During training/fine-tuning/inference, GPU usage can spike
    to up to 100% for compute-intensive operations (matrix multiplications, backpropagation,
    etc.,) but will drop significantly in between epochs or data loading/processing
    steps. Because K8s does not support fractional GPU resource allocation by default,
    these *peaks and valleys* can lead to inefficient GPU utilization.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态工作负载模式**：GenAI工作负载，如微调或训练LLM，或运行复杂的基于扩散的模型推理，可能会产生突发性的GPU使用模式。在训练/微调/推理过程中，GPU使用率可能会飙升至100%，用于计算密集型操作（矩阵乘法、反向传播等），但在周期或数据加载/处理步骤之间会显著下降。由于K8s默认不支持按比例分配GPU资源，这些*波动峰谷*会导致GPU利用率低效。'
- en: '**Scheduling complexity and fragmentation**: K8s’s default scheduler lacks
    the intelligence of advanced GPU-sharing strategies. Even with techniques such
    as node affinity, taints, and tolerations, there is no out-of-the-box method to
    dynamically reassign underutilized GPUs to another Pod. Consequently, smaller
    LLMs might monopolize an entire GPU, even if they need a fraction of its capacity.
    As more Pods are deployed, multiple GPUs become partially utilized but fully reserved,
    causing fragmented GPU resources across the cluster.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度复杂性和碎片化**：K8s的默认调度器缺乏先进GPU共享策略的智能。即使有节点亲和性、污点和容忍度等技术，也没有现成的方法可以动态地将未充分利用的GPU重新分配给另一个Pod。因此，较小的LLM可能会独占整个GPU，即使它们只需要其中一小部分的计算能力。随着更多Pod的部署，多个GPU会变成部分利用，但被完全预留，从而导致集群中的GPU资源碎片化。'
- en: 'We can implement a few approaches to address these utilization challenges,
    such as NVIDIA’s **MIG**, **MPS**, and **GPU time-slicing** options. In a case
    study on delivering video content using GPU-sharing techniques, up to a 95% improvement
    in price performance was achieved. For more detailed benchmarking data regarding
    this, please refer to the following AWS blog: [https://aws.amazon.com/blogs/containers/delivering-video-content-with-fractional-gpus-in-containers-on-amazon-eks/](https://aws.amazon.com/blogs/containers/delivering-video-content-with-fractional-gpus-in-containers-on-amazon-eks/).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采取一些方法来应对这些利用率挑战，比如NVIDIA的**MIG**、**MPS**和**GPU时间切片**选项。在一项使用GPU共享技术提供视频内容的案例研究中，价格性能改善最高可达95%。有关此方面的更详细基准数据，请参考以下AWS博客：[https://aws.amazon.com/blogs/containers/delivering-video-content-with-fractional-gpus-in-containers-on-amazon-eks/](https://aws.amazon.com/blogs/containers/delivering-video-content-with-fractional-gpus-in-containers-on-amazon-eks/)。
- en: In this section, we explored why monitoring GPU health and metrics is crucial
    and deployed the NVIDIA DCGM-Exporter add-on in our K8s cluster to track GPU performance
    and health metrics in real time. We also looked at the challenges of GPU utilization
    and the factors contributing to inefficiency in the GenAI space. Next, we will
    dive into GPU partitioning techniques that we can use to address these issues.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们探讨了为什么监控GPU健康状况和指标至关重要，并在我们的K8s集群中部署了NVIDIA DCGM-Exporter插件，以实时跟踪GPU性能和健康指标。我们还探讨了GPU利用率的挑战，以及在GenAI领域中造成低效的因素。接下来，我们将深入研究可用于解决这些问题的GPU划分技术。
- en: Techniques for partitioning and sharing GPUs
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU划分和共享技术
- en: GPU partitioning and sharing techniques are often vendor-specific, meaning they
    may not be available for every accelerator out there. In this section, we will
    explore some of the most common approaches provided by NVIDIA for its GPUs, such
    as MIG, MPS, and time-slicing, and discuss how they can help improve GPU utilization
    for GenAI workloads.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: GPU划分和共享技术通常是厂商特定的，这意味着它们可能并不适用于所有加速器。在这一节中，我们将探讨NVIDIA为其GPU提供的一些常见方法，如MIG、MPS和时间切片，并讨论它们如何帮助提高GenAI工作负载的GPU利用率。
- en: NVIDIA MIG
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NVIDIA MIG
- en: '**MIG** ([https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html))
    is a feature that was introduced in NVIDIA’s **Ampere** and later architectures
    (A100, H100, etc.) that allows a single physical GPU to be partitioned into multiple
    independent GPU instances. Each MIG instance has its own dedicated memory, compute
    cores, and other GPU resources, providing strict isolation between workloads.
    MIG minimizes interference among instances and ensures predictable performance,
    ultimately enabling more efficient and flexible use of GPU capacity.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**MIG**（[https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html)）是
    NVIDIA 在 **Ampere** 及后续架构（A100、H100 等）中引入的一项功能，允许将单个物理 GPU 划分为多个独立的 GPU 实例。每个
    MIG 实例拥有自己的专用内存、计算核心和其他 GPU 资源，从而在工作负载之间提供严格的隔离。MIG 最小化了实例之间的干扰，并确保可预测的性能，最终实现了更高效、灵活的
    GPU 容量利用。'
- en: 'The process of implementing MIG involves defining GPU instances that bundle
    a portion of the GPU’s memory and compute capacity. For example, on an NVIDIA
    A100 GPU with 40 GB of memory, you can configure up to seven instances each with
    5 GB of dedicated memory and a corresponding share of compute resources; this
    can be seen in *Figure 10**.2*. In this example, we can have multiple workloads,
    such as Jupyter notebooks, ML jobs, and more, that can run on separate GPU partitions,
    allowing for efficient utilization of the GPU instance:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 MIG 的过程包括定义 GPU 实例，这些实例捆绑了 GPU 的一部分内存和计算能力。例如，在具有 40 GB 内存的 NVIDIA A100 GPU
    上，您可以配置最多七个实例，每个实例拥有 5 GB 的专用内存和相应的计算资源份额；这可以在*图 10.2*中看到。在这个示例中，我们可以在不同的 GPU
    分区上运行多个工作负载，例如 Jupyter 笔记本、机器学习任务等，从而实现 GPU 实例的高效利用：
- en: '![Figure 10.2 – Multi-instance GPU in an A100 NVIDIA GPU](img/B31108_10_2.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2 – A100 NVIDIA GPU 中的多实例 GPU](img/B31108_10_2.jpg)'
- en: Figure 10.2 – Multi-instance GPU in an A100 NVIDIA GPU
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – A100 NVIDIA GPU 中的多实例 GPU
- en: 'These instances are specified by **MIG profiles** ([https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-mig-profiles](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-mig-profiles)),
    which outline the size and shape of each instance – for example, a *1g.5gb* profile
    allocates 5 GB of memory and a proportional share of GPU cores. These instances
    function as isolated GPU instances, each with guaranteed performance and minimal
    interference from others. The following table shows the MIG profiles for the latest
    NVIDIA H200 GPUs from the NVIDIA MIG user guide ([https://docs.nvidia.com/datacenter/tesla/pdf/NVIDIA_MIG_User_Guide.pdf](https://docs.nvidia.com/datacenter/tesla/pdf/NVIDIA_MIG_User_Guide.pdf)):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实例由**MIG 配置文件**指定（[https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-mig-profiles](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-mig-profiles)），该配置文件概述了每个实例的大小和形状——例如，*1g.5gb*
    配置文件分配 5 GB 内存和相应份额的 GPU 核心。这些实例作为独立的 GPU 实例运行，每个实例具有保证的性能并且相互之间的干扰最小。下表显示了 NVIDIA
    MIG 用户指南中最新 NVIDIA H200 GPU 的 MIG 配置文件（[https://docs.nvidia.com/datacenter/tesla/pdf/NVIDIA_MIG_User_Guide.pdf](https://docs.nvidia.com/datacenter/tesla/pdf/NVIDIA_MIG_User_Guide.pdf)）：
- en: '| **MIG Profile** | **GPU Slices** | **GPU** **Memory (GB)** | **Number** **of
    Instances** |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| **MIG 配置文件** | **GPU 切片** | **GPU 内存（GB）** | **实例数量** |'
- en: '| 1g.18gb | 1 | 18 | 7 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 1g.18gb | 1 | 18 | 7 |'
- en: '| 1g.18gb+me | 1 | 18 | 1 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 1g.18gb+me | 1 | 18 | 1 |'
- en: '| 1g.35gb | 1 | 35 | 4 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 1g.35gb | 1 | 35 | 4 |'
- en: '| 2g.35gb | 2 | 35 | 3 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 2g.35gb | 2 | 35 | 3 |'
- en: '| 3g.70gb | 3 | 70 | 2 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 3g.70gb | 3 | 70 | 2 |'
- en: '| 4g.70gb | 4 | 70 | 1 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 4g.70gb | 4 | 70 | 1 |'
- en: '| 7g.141gb | 7 | 141 | 1 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 7g.141gb | 7 | 141 | 1 |'
- en: Table 10.1 – GPU instance profiles for a NVIDIA H200 GPU
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.1 – NVIDIA H200 GPU 的 GPU 实例配置文件
- en: In this table, *GPU Slices* represent the fraction of the GPU allocated to the
    MIG profile, *GPU Memory (GB)* represents the amount of memory allocated to that
    MIG instance in GB, and *Number of Instances* is the instance count that can be
    created for the given profile.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在此表中，*GPU 切片*表示分配给 MIG 配置文件的 GPU 部分，*GPU 内存（GB）*表示分配给该 MIG 实例的内存量（以 GB 为单位），*实例数量*表示可以为给定配置文件创建的实例数。
- en: In K8s, you can use the **NVIDIA GPU Operator** to simplify the MIG setup that
    deploys MIG Manager to manage the MIG configuration and other essential configurations
    on the GPU nodes. Please refer to the NVIDIA documentation at [https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html)
    for step-by-step instructions on setting up MIG on K8s clusters.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在 K8s 中，你可以使用**NVIDIA GPU Operator**来简化 MIG 设置，该设置会部署 MIG Manager 以管理 GPU 节点上的
    MIG 配置及其他关键配置。有关如何在 K8s 集群中设置 MIG 的详细步骤，请参考 NVIDIA 文档：[https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html)。
- en: 'K8s identifies the individual MIG instances with a unique **GPU instance ID**
    and **compute instance ID**. These instances are exposed to K8s as extended resources
    (e.g., *nvidia.com/mig-1g.18gb*). The device plugin labels the node with the available
    MIG profiles and their quantities, enabling the K8s scheduler to match Pod resource
    requests to the appropriate MIG instance. For example, the following code snippet
    shows a K8s Pod requesting that *nvidia.com/mig-1g.18gb* be scheduled to a node
    that has an available instance of that specific configuration:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: K8s 使用独特的**GPU 实例 ID**和**计算实例 ID**来识别各个 MIG 实例。这些实例作为扩展资源（例如，*nvidia.com/mig-1g.18gb*）暴露给
    K8s。设备插件会为节点标注可用的 MIG 配置及其数量，从而使 K8s 调度器能够将 Pod 资源请求匹配到相应的 MIG 实例。例如，下面的代码片段显示了一个
    K8s Pod 请求将 *nvidia.com/mig-1g.18gb* 调度到具有该特定配置可用实例的节点：
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now that we’ve learned how individual MIG instances are identified and allocated
    within K8s, let’s explore the different ways these partitions can be configured.
    Primarily, there are two strategies: *single* and *mixed*.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了在 K8s 中如何识别和分配各个 MIG 实例，接下来让我们探索这些分区可以如何配置。主要有两种策略：*单一*和*混合*。
- en: With the **single MIG strategy** ([https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html#example-single-mig-strategy](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html#example-single-mig-strategy)),
    all GPU slices on a node are of the same size. For example, on a p5.48xlarge EC2
    instance that features eight H100 GPUs, each with 80 GB of memory, you could create
    56 slices of 1g.10gb, 24 slices of 2g.20gb, 16 slices of 3g.40gb, or even eight
    slices of 4g.40gb or 7g.80gb. This uniform approach is especially useful if multiple
    teams have similar GPU requirements. In such cases, you can allocate the same
    sized slice to each team, ensuring fair access and maximizing the utilization
    of the p5.48xlarge instance for workloads such as fine-tuning and inference, where
    the need for GPU capacity is consistent across tasks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**单一 MIG 策略**([https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html#example-single-mig-strategy](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html#example-single-mig-strategy))时，节点上的所有
    GPU 切片大小相同。例如，在一个配有八个 80 GB 内存的 H100 GPU 的 p5.48xlarge EC2 实例上，你可以创建 56 个 1g.10gb
    切片，24 个 2g.20gb 切片，16 个 3g.40gb 切片，甚至是 8 个 4g.40gb 或 7g.80gb 切片。这种统一的策略特别适用于多个团队有相似
    GPU 需求的情况。在这种情况下，你可以为每个团队分配相同大小的切片，确保公平访问，并最大化 p5.48xlarge 实例的利用率，适用于诸如微调和推理等任务，这些任务对
    GPU 容量的需求在各个任务中是一致的。
- en: In contrast, the **mixed MIG strategy** ([https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html#example-mixed-mig-strategy](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html#example-mixed-mig-strategy))
    offers greater flexibility by allowing GPU slices of varying sizes to be created
    within each GPU on a node. For example, on a p5.24xlarge EC2 instance, which is
    equipped with eight NVIDIA H100 GPUs, you can combine different MIG slice configurations,
    such as 16 slices of 1g.10gb + 8 slices of 2g.20gb + 8 slices of 3g.40gb. This
    heterogeneous allocation is particularly beneficial for clusters that handle a
    wide range of workloads with diverse GPU requirements.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，**混合 MIG 策略**([https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html#example-mixed-mig-strategy](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html#example-mixed-mig-strategy))通过允许在节点上的每个
    GPU 中创建不同大小的 GPU 切片，提供了更大的灵活性。例如，在一个配备有八个 NVIDIA H100 GPU 的 p5.24xlarge EC2 实例上，你可以结合不同的
    MIG 切片配置，例如 16 个 1g.10gb 切片 + 8 个 2g.20gb 切片 + 8 个 3g.40gb 切片。这种异构分配对于处理具有不同 GPU
    需求的工作负载的集群尤为有益。
- en: 'Consider an AI startup with three specialized workloads: an image recognition
    app, a natural language processing app, and a video analytics app. Using the mixed
    strategy, the startup can customize GPU allocations to match each app’s needs.
    For instance, the image recognition application might be assigned two 1g.10gb
    slices, the natural language processing application might utilize one 2g.20gb
    slice, and the video analytics application might benefit from one 3g.40gb slice,
    all operating concurrently on the same H100 GPU. This approach ensures that each
    application receives the appropriate level of GPU resources without overprovisioning,
    thereby maximizing the overall utilization and efficiency of the GPU resources.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个 AI 初创公司，拥有三种专门化工作负载：一个图像识别应用、一个自然语言处理应用和一个视频分析应用。通过混合策略，该初创公司可以根据每个应用的需求定制
    GPU 分配。例如，图像识别应用可能会分配两个 1g.10gb 的切片，自然语言处理应用可能会使用一个 2g.20gb 的切片，视频分析应用可能会从一个 3g.40gb
    的切片中受益，所有应用都在同一 H100 GPU 上并行运行。这种方法确保每个应用获得适当的 GPU 资源，而不会过度配置，从而最大化 GPU 资源的整体利用率和效率。
- en: NVIDIA MPS
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NVIDIA MPS
- en: '**MPS** ([https://docs.nvidia.com/deploy/mps/index.html](https://docs.nvidia.com/deploy/mps/index.html))
    is a software feature that was designed to optimize GPU resource sharing across
    multiple processes. It enables multiple processes to submit work concurrently,
    reducing GPU idle time and improving resource utilization. As depicted in *Figure
    10**.3*, each process retains its own isolated GPU memory space, while compute
    resources are shared dynamically, allowing for low-latency scheduling and better
    performance for workloads that might not fully utilize GPU resources individually:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**MPS** ([https://docs.nvidia.com/deploy/mps/index.html](https://docs.nvidia.com/deploy/mps/index.html))
    是一种软件功能，旨在优化多个进程间的 GPU 资源共享。它使多个进程能够并发提交任务，从而减少 GPU 空闲时间并提高资源利用率。如 *图 10.3* 所示，每个进程保留自己的独立
    GPU 内存空间，同时计算资源动态共享，这使得低延迟调度成为可能，并且对于那些可能无法单独完全利用 GPU 资源的工作负载来说，能够提供更好的性能：'
- en: '![Figure 10.3 – NVIDIA MPS](img/B31108_10_3.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3 – NVIDIA MPS](img/B31108_10_3.jpg)'
- en: Figure 10.3 – NVIDIA MPS
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – NVIDIA MPS
- en: In MPS, different processes running concurrently share GPU global memory, which
    introduces challenges that require careful consideration. *Synchronization overhead*
    can arise as processes or threads must be carefully managed to avoid race conditions
    or inconsistent states when accessing shared memory simultaneously. Additionally,
    resource contention may occur when multiple processes or threads compete for the
    same memory space, potentially creating performance bottlenecks. Balancing the
    use of shared global memory with private allocations adds complexity to programming,
    requiring careful planning and a deep understanding of CUDA ([https://developer.nvidia.com/about-cuda](https://developer.nvidia.com/about-cuda))
    memory management to ensure both efficiency and correctness.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MPS 中，不同的并发运行的进程共享 GPU 全局内存，这带来了需要仔细考虑的挑战。当进程或线程必须小心管理，以避免在同时访问共享内存时发生竞争条件或不一致的状态时，*同步开销*可能会出现。此外，当多个进程或线程争夺同一内存空间时，可能会发生资源争用，进而导致性能瓶颈。共享全局内存与私有分配的平衡增加了编程的复杂性，需要仔细规划并深入理解
    CUDA ([https://developer.nvidia.com/about-cuda](https://developer.nvidia.com/about-cuda))
    内存管理，以确保效率和正确性。
- en: The memory isolation issue with MPS has been partially addressed, initially
    by the **Volta architecture** ([https://docs.nvidia.com/deploy/mps/index.html#volta-mps](https://docs.nvidia.com/deploy/mps/index.html#volta-mps)).
    Volta GPUs introduced individual GPU address spaces for each client, ensuring
    that memory allocations by one process are not directly accessible to others,
    a significant improvement in memory isolation. Additionally, clients can submit
    work directly to the GPU without using a shared context, reducing contention and
    enabling finer resource allocation. Execution resource provisioning also ensures
    better control over GPU compute resources, preventing any single client from monopolizing
    them. However, limitations remain, with global memory bandwidth still being shared
    among all processes, potentially leading to performance degradation if one process
    overuses it. Additionally, MPS lacks fault isolation, meaning critical errors
    in one process can still disrupt others. While these improvements make MPS in
    Volta and newer GPUs more suitable for multi-process workloads with less stringent
    isolation requirements, NVIDA’s MIG remains the recommended solution when complete
    fault isolation is needed.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MPS的内存隔离问题已部分解决，最初由**Volta架构**([https://docs.nvidia.com/deploy/mps/index.html#volta-mps](https://docs.nvidia.com/deploy/mps/index.html#volta-mps))解决。Volta
    GPU为每个客户端引入了独立的GPU地址空间，确保一个进程的内存分配不会被其他进程直接访问，这是内存隔离的一大进步。此外，客户端可以直接向GPU提交工作，而无需使用共享上下文，减少了竞争并实现了更精细的资源分配。执行资源的配置也确保了对GPU计算资源的更好控制，防止单个客户端垄断资源。然而，仍然存在一些限制，所有进程共享全局内存带宽，如果某个进程过度使用，可能导致性能下降。此外，MPS缺乏故障隔离，意味着一个进程中的关键错误仍然可能干扰其他进程。尽管这些改进使得Volta及更新版GPU中的MPS更适合处理多进程工作负载，并且在隔离要求不那么严格时表现更佳，但当需要完全的故障隔离时，NVIDIA的MIG仍然是推荐的解决方案。
- en: 'The NVIDIA device plugin for K8s does not currently support MPS partitioning,
    and it is tracked under this GitHub issue #443 ([https://github.com/NVIDIA/k8s-device-plugin/issues/443](https://github.com/NVIDIA/k8s-device-plugin/issues/443)).
    However, there is a forked version of the plugin at [https://github.com/nebuly-ai/k8s-device-plugin](https://github.com/nebuly-ai/k8s-device-plugin)
    that enables MPS support in K8s clusters. Refer to the following post by *Medium*
    for step-by-step instructions: [https://medium.com/data-science/how-to-increase-gpu-utilization-in-kubernetes-with-nvidia-mps-e680d20c3181](https://medium.com/data-science/how-to-increase-gpu-utilization-in-kubernetes-with-nvidia-mps-e680d20c3181).
    Once MPS is enabled using this forked NVIDIA device plugin, multiple K8s Pods
    can share a single GPU. MPS dynamically manages compute resource allocation while
    maintaining memory isolation between Pods. This functionality enables fractional
    GPU requests in Pod resource definitions, allowing K8s to schedule multiple Pods
    on the same GPU efficiently.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，NVIDIA的K8s设备插件不支持MPS分区，并且这一问题正在GitHub上的问题追踪中进行跟踪，编号为#443([https://github.com/NVIDIA/k8s-device-plugin/issues/443](https://github.com/NVIDIA/k8s-device-plugin/issues/443))。不过，存在一个分支版本的插件，地址为[https://github.com/nebuly-ai/k8s-device-plugin](https://github.com/nebuly-ai/k8s-device-plugin)，它支持在K8s集群中启用MPS。请参考*Medium*上的以下文章，获取逐步操作指南：[https://medium.com/data-science/how-to-increase-gpu-utilization-in-kubernetes-with-nvidia-mps-e680d20c3181](https://medium.com/data-science/how-to-increase-gpu-utilization-in-kubernetes-with-nvidia-mps-e680d20c3181)。一旦通过此分支的NVIDIA设备插件启用MPS，多个K8s
    Pod可以共享单个GPU。MPS动态管理计算资源的分配，同时保持Pod之间的内存隔离。此功能使Pod资源定义中支持按比例请求GPU，从而使K8s能够高效地将多个Pod调度到同一GPU上。
- en: GPU time-slicing
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU时间切片
- en: 'For NVIDIA GPUs, **time-slicing** ([https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html))
    is another technique that allows multiple processes or applications to share GPU
    resources dynamically by dividing execution time into slices, enabling sequential
    access to the GPU. *Figure 10**.4* illustrates how the GPU alternates between
    different processes over time, enabling them to share resources while each process
    typically retains its respective memory allocation:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NVIDIA GPU，**时间切片**([https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html))是另一种技术，通过将执行时间分割为多个时间片，允许多个进程或应用动态共享GPU资源，从而实现对GPU的顺序访问。*图10.4*展示了GPU如何随着时间的推移在不同进程之间交替，使它们能够共享资源，同时每个进程通常保持各自的内存分配：
- en: '![Figure 10.4 – GPU time-slicing](img/B31108_10_4.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图10.4 – GPU时间切片](img/B31108_10_4.jpg)'
- en: Figure 10.4 – GPU time-slicing
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 – GPU时间切片
- en: Time-slicing supports workloads that would otherwise require exclusive access
    to GPUs, providing shared access but without the memory or fault isolation capability
    of NVIDIA’s MIG feature. The time-slicing feature is especially beneficial for
    older GPUs that do not support the MIG feature. It can also complement MIG by
    enabling multiple processes to share the resources of a single MIG partition.
    However, time-slicing may introduce latency as processes have to wait for their
    turn. This creates context-switching overhead, which involves saving and restoring
    the state of each process before switching to the next process.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 时间分片支持那些通常需要独占GPU访问的工作负载，提供共享访问，但没有NVIDIA MIG特性的内存或故障隔离能力。时间分片功能特别适用于不支持MIG特性的老旧GPU。它还可以补充MIG，使多个进程共享单一MIG分区的资源。然而，时间分片可能会引入延迟，因为进程需要等待轮到它们执行。这会带来上下文切换开销，涉及在切换到下一个进程之前保存和恢复每个进程的状态。
- en: Time-slicing is well suited for general-purpose, multi-process GPU usage and
    works effectively in virtualization, multi-tenant systems, and mixed workloads.
    However, for scenarios requiring strict resource isolation or high responsiveness,
    MIG might be a more appropriate solution.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 时间分片非常适合通用、多进程GPU使用，并且在虚拟化、多租户系统和混合工作负载中表现有效。然而，对于需要严格资源隔离或高响应性的场景，MIG可能是更合适的解决方案。
- en: 'Here is a high-level comparison of the GPU sharing techniques mentioned so
    far – that is, MIG, MPS, and time-slicing:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是前面提到的GPU共享技术（即MIG、MPS和时间分片）的高级对比：
- en: '| **Feature** | **MIG** | **MPS** | **Time-Slicing** |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| **特性** | **MIG** | **MPS** | **时间分片** |'
- en: '| **Resource** **isolation** | Strong (hardware-level) | None or limited for
    Volta+ GPUs | None |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| **资源** **隔离** | 强（硬件级别） | 对于Volta+ GPU没有或有限 | 无 |'
- en: '| **Performance** | Predictable | Improved GPU utilization with added latency
    | Dependent on workload characteristics |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| **性能** | 可预测 | 提高了GPU利用率，但增加了延迟 | 依赖于工作负载特性 |'
- en: '| **Scalability** | Limited by supported partition count | High | High |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| **可扩展性** | 受支持分区数限制 | 高 | 高 |'
- en: '| **Overhead** | Minimal | Low | Higher with context switching |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **开销** | 最小 | 低 | 随着上下文切换增加开销 |'
- en: '| **Use vase** | Multi-tenant, inference | HPC, multi-process workloads | Non-critical
    tasks |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **使用场景** | 多租户，推理 | 高性能计算，多个进程工作负载 | 非关键任务 |'
- en: '| **Compatibility** | Ampere+ GPUs | All CUDA-capable GPUs | All CUDA-capable
    GPUs |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| **兼容性** | Ampere+ GPUs | 所有支持CUDA的GPU | 所有支持CUDA的GPU |'
- en: Table 10.2 – Comparison of NVIDIA GPU sharing techniques
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 10.2 – NVIDIA GPU共享技术对比
- en: 'The NVIDIA time-slicing feature in K8s enables GPU oversubscription, allowing
    multiple workloads to share a single GPU dynamically by interleaving their execution.
    This is achieved through the NVIDIA GPU Operator and enhanced configuration options
    in the NVIDIA device plugin for K8s. To enable the time-slicing feature in our
    EKS cluster setup, we need to create a `time-slicing-config`. In the following
    example, we are creating 10 replicas (virtual “time-sliced” GPUs) so that each
    K8s Pod requesting one `nvidia.com/gpu` resource will be allocated to one of these
    virtual GPUs and time-sliced on the underlying physical GPU:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: K8s中的NVIDIA时间分片特性支持GPU超额分配，允许多个工作负载通过交错执行动态共享单个GPU。这是通过NVIDIA GPU Operator和在K8s中增强的NVIDIA设备插件配置选项实现的。为了在EKS集群设置中启用时间分片特性，我们需要创建一个`time-slicing-config`。在以下示例中，我们创建了10个副本（虚拟的“时间分片”GPU），这样每个请求一个`nvidia.com/gpu`资源的K8s
    Pod将被分配到这些虚拟GPU中的一个，并在底层物理GPU上进行时间分片：
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: $ kubectl apply -f nvidia-ts.yaml
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: $ kubectl apply -f nvidia-ts.yaml
- en: configmap/time-slicing-config created
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: configmap/time-slicing-config 创建完成
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: module "eks_data_addons" {
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: module "eks_data_addons" {
- en: source = "aws-ia/eks-data-addons/aws"
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: source = "aws-ia/eks-data-addons/aws"
- en: '...'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: enable_nvidia_device_plugin = true
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: enable_nvidia_device_plugin = true
- en: nvidia_device_plugin_helm_config = {
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: nvidia_device_plugin_helm_config = {
- en: name    = "nvidia-device-plugin"
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: name    = "nvidia-device-plugin"
- en: values = [
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: values = [
- en: '...'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: 'config:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 'config:'
- en: 'name: time-slicing-config'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 'name: time-slicing-config'
- en: '...'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: $ terraform apply -auto-approve
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: $ terraform apply -auto-approve
- en: '[PRE14]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: $ kubectl get nodes -o custom-columns=NAME:.metadata.name,INSTANCE:.metadata.labels."node\.kubernetes\.io/instance-type",GPUs:.status.allocatable."nvidia\.com/gpu"
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: $ kubectl get nodes -o custom-columns=NAME:.metadata.name,INSTANCE:.metadata.labels."node\.kubernetes\.io/instance-type",GPUs:.status.allocatable."nvidia\.com/gpu"
- en: NAME                                        INSTANCE     GPUs
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: NAME                                        INSTANCE     GPUs
- en: ip-10-0-40-57.us-west-2.compute.internal    g6.2xlarge   10
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ip-10-0-40-57.us-west-2.compute.internal    g6.2xlarge   10
- en: '[PRE15]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: $ kubectl apply -f llama32-deploy.yaml
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: $ kubectl apply -f llama32-deploy.yaml
- en: deployment/my-llama32-deployment created
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: deployment/my-llama32-deployment 创建完成
- en: $ kubectl get pods -o wide
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: $ kubectl get pods -o wide
- en: <Displays 5 pods running on same node>
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: <显示在同一节点上运行的 5 个 Pod>
- en: '[PRE16]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'apiVersion: autoscaling/v2'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiVersion: autoscaling/v2'
- en: 'kind: HorizontalPodAutoscaler'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 'kind: HorizontalPodAutoscaler'
- en: 'metadata:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 'metadata:'
- en: 'name: genai-training-hpa'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 'name: genai-training-hpa'
- en: 'spec:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 'spec:'
- en: 'scaleTargetRef:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 'scaleTargetRef:'
- en: 'apiVersion: apps/v1'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiVersion: apps/v1'
- en: 'kind: Deployment'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 'kind: Deployment'
- en: 'name: genai-training'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 'name: genai-training'
- en: 'minReplicas: 1'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 'minReplicas: 1'
- en: 'maxReplicas: 10'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 'maxReplicas: 10'
- en: 'metrics:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 'metrics:'
- en: '- type: Object'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '- type: Object'
- en: 'object:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 'object:'
- en: 'metricName: DCGM_FI_DEV_GPU_UTIL'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 'metricName: DCGM_FI_DEV_GPU_UTIL'
- en: 'targetAverageValue: 80'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 'targetAverageValue: 80'
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Part 3: Operating GenAI Workloads on K8s'
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 部分：在 K8s 上运行 GenAI 工作负载
- en: This section addresses the day-to-day operations of GenAI applications in production
    K8s environments, covering critical aspects from automated pipelines to resilience
    strategies. This section explores GenAIOps practices, comprehensive observability
    implementations using industry-standard tools, and strategies for high availability
    and disaster recovery. It also highlights the transformative impact of GenAI coding
    assistants on automating and managing K8s clusters, concluding with recommendations
    for further reading.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了 GenAI 应用在生产 K8s 环境中的日常操作，涵盖了从自动化流水线到弹性策略的关键方面。本节探讨了 GenAIOps 实践、使用行业标准工具的全面可观察性实现，以及高可用性和灾难恢复策略。还强调了
    GenAI 编程助手在自动化和管理 K8s 集群方面的变革性影响，并以进一步阅读建议作为结尾。
- en: 'This part has the following chapters:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 11*](B31108_11.xhtml#_idTextAnchor145), *GenAIOps: Data Management
    and GenAI Automation Pipeline*'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第 11 章*](B31108_11.xhtml#_idTextAnchor145)，*GenAIOps：数据管理与 GenAI 自动化流水线*'
- en: '[*Chapter 12*](B31108_12.xhtml#_idTextAnchor160), *Observability – Getting
    Visibility into GenAI on K8s*'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第 12 章*](B31108_12.xhtml#_idTextAnchor160)，*可观察性 – 获取 GenAI 在 K8s 上的可见性*'
- en: '[*Chapter 13*](B31108_13.xhtml#_idTextAnchor176), *High Availability and Disaster
    Recovery for GenAI Applications*'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第 13 章*](B31108_13.xhtml#_idTextAnchor176)，*GenAI 应用的高可用性和灾难恢复*'
- en: '[*Chapter 14*](B31108_14.xhtml#_idTextAnchor183), *Wrapping-up**: GenAI Coding
    Assistants and Further Reading*'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第 14 章*](B31108_14.xhtml#_idTextAnchor183)，*总结*：GenAI 编程助手与进一步阅读'
