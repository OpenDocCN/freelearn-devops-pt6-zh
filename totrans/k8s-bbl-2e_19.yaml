- en: '19'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '19'
- en: Advanced Techniques for Scheduling Pods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pods 调度的高级技术
- en: At the beginning of the book, in *Chapter 2*, *Kubernetes Architecture – from
    Container Images to Running Pods*, we explained the principles behind the Kubernetes
    scheduler (`kube-scheduler`) control plane component and its crucial role in the
    cluster. In short, its responsibility is to schedule container workloads (Kubernetes
    Pods) and assign them to healthy nodes that fulfill the criteria required for
    running a particular workload.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的 *第二章*，*Kubernetes 架构 – 从容器镜像到运行 Pods* 中，我们解释了 Kubernetes 调度器（`kube-scheduler`）控制平面组件背后的原理及其在集群中的关键作用。简而言之，它的责任是调度容器工作负载（Kubernetes
    Pods），并将它们分配到满足特定工作负载运行要求的健康节点。
- en: This chapter will cover how you can control the criteria for scheduling Pods
    in the cluster. We will pay particular attention to Node **affinity**, **taints**,
    and **tolerations** for Pods. We will also take a closer look at **scheduling
    policies**, which give `kube-scheduler` flexibility in how it prioritizes Pod
    workloads. You will find all of these concepts important in running production
    clusters at the cloud scale.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍如何控制集群中调度 Pods 的标准。我们将特别关注 Node **亲和性**、**污点**和 **容忍度**。我们还将深入探讨 **调度策略**，这为
    `kube-scheduler` 提供了如何优先考虑 Pod 工作负载的灵活性。你会发现这些概念在云规模的生产集群运行中非常重要。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下内容：
- en: Refresher – What is kube-scheduler?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温故 – 什么是 kube-scheduler？
- en: Managing Node affinity
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理 Node 亲和性
- en: Using Node taints and tolerations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Node 污点和容忍度
- en: Understanding Static Pods in Kubernetes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 Kubernetes 中的静态 Pods
- en: Extended Scheduler Configurations in Kubernetes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 中的扩展调度器配置
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will need the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要以下内容：
- en: A *multi-node* Kubernetes cluster is required. Having a multi-node cluster will
    make understanding Node affinity, taints, and tolerations much easier.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要一个 *多节点* Kubernetes 集群。拥有一个多节点集群将使理解 Node 亲和性、污点和容忍度变得更加容易。
- en: The Kubernetes CLI (`kubectl`) installed on your local machine and configured
    to manage your Kubernetes cluster.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地机器上已安装并配置用于管理 Kubernetes 集群的 Kubernetes CLI (`kubectl`)。
- en: Basic Kubernetes cluster deployment (local and cloud-based) and `kubectl` installation
    have been covered in *Chapter 3*, *Installing Your First Kubernetes Cluster*.
    The previous chapters *15*, *16*, and *17* have provided you an overview of how
    to deploy a fully functional Kubernetes cluster on different cloud platforms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的 Kubernetes 集群部署（本地和云端）以及 `kubectl` 安装已在 *第三章*，*安装你的第一个 Kubernetes 集群* 中进行讲解。之前的
    *第 15、16、17* 章为你提供了如何在不同的云平台上部署一个功能完整的 Kubernetes 集群的概述。
- en: 'You can download the latest code samples for this chapter from the official
    GitHub repository: [https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter19](https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter19).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从官方 GitHub 仓库下载本章的最新代码示例：[https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter19](https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter19)。
- en: Refresher – What is kube-scheduler?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 温故 – 什么是 kube-scheduler？
- en: In Kubernetes clusters, kube-scheduler is a critical component of the control
    plane. The main responsibility of this component is scheduling container workloads
    (Pods) and **assigning** them to healthy compute nodes (also known as worker nodes)
    that fulfill the criteria required for running a particular workload. To recap,
    a Pod is a group of one or more containers with a shared network and storage and
    is the smallest **deployment unit** in the Kubernetes system. You usually use
    different Kubernetes controllers, such as Deployment objects and StatefulSet objects,
    to manage your Pods, but it is kube-scheduler that eventually assigns the created
    Pods to particular Nodes in the cluster.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 集群中，kube-scheduler 是控制平面中的一个关键组件。该组件的主要职责是调度容器工作负载（Pods），并将它们**分配**到满足特定工作负载运行要求的健康计算节点（也称为工作节点）上。简而言之，Pod
    是一个或多个共享网络和存储的容器组，是 Kubernetes 系统中最小的**部署单元**。你通常会使用不同的 Kubernetes 控制器，例如 Deployment
    对象和 StatefulSet 对象，来管理 Pods，但最终是 kube-scheduler 将创建的 Pods 分配到集群中的特定节点。
- en: For managed Kubernetes clusters in the cloud, such as **Azure Kubernetes Service**
    (**AKS**) or Amazon **Elastic Kubernetes Service** (**EKS**), you typically do
    not have access to the control plane or controller nodes as they are managed by
    the cloud service provider. This means you won’t have direct access to components
    like `kube-scheduler` or control over its configuration, such as scheduling policies.
    However, you can still control all the parameters of Pods that influence their
    scheduling.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于云中的托管 Kubernetes 集群，例如 **Azure Kubernetes Service**（**AKS**）或 Amazon **Elastic
    Kubernetes Service**（**EKS**），通常无法访问控制平面或控制节点，因为它们由云服务提供商管理。这意味着你无法直接访问 `kube-scheduler`
    等组件，也无法控制其配置（如调度策略）。然而，你仍然可以控制所有影响 Pod 调度的参数。
- en: kube-scheduler queries the **Kubernetes API Server** (`kube-apiserver`) at regular
    intervals in order to list the Pods that have not been *scheduled*. At creation,
    Pods are marked as *not* scheduled – this means no Node was elected to run them.
    A Pod that is not scheduled will be registered in the `etcd` cluster state but
    without any Node assigned to it and, thus, no running kubelet will be aware of
    this Pod. Ultimately, no container described in the Pod specification will run
    at this point.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: kube-scheduler 会定期查询 **Kubernetes API Server**（`kube-apiserver`），以列出尚未 *调度*
    的 Pods。在创建时，Pods 会被标记为 *未* 调度—这意味着没有节点被选举来运行它们。一个未调度的 Pod 将在 `etcd` 集群状态中注册，但没有分配任何节点，因此没有运行的
    kubelet 会知道这个 Pod。最终，Pod 规范中描述的容器此时不会运行。
- en: Internally, the Pod object, as it is stored in `etcd`, has a property called
    `nodeName`. As the name suggests, this property should contain the name of the
    Node that will host the Pod. When this property is set, we say the Pod is in a
    `scheduled` state; otherwise, the Pod is in a `pending` state.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，Pod 对象在 `etcd` 中存储时具有一个名为 `nodeName` 的属性。顾名思义，这个属性应该包含将要托管该 Pod 的节点的名称。当这个属性被设置时，我们说
    Pod 处于 `scheduled` 状态；否则，Pod 处于 `pending` 状态。
- en: 'We need to find a way to fill this `nodeName` value, and this is the role of
    kube-scheduler. For this, kube-scheduler polls kube-apiserver at regular intervals.
    It looks for Pod resources with an empty `nodeName` property. Once it finds such
    Pods, it will execute an algorithm to elect a Node and will update the `nodeName`
    property in the Pod object by issuing a request to kube-apiserver. When selecting
    a Node for the Pod, `kube-scheduler` will take into account its internal scheduling
    policies and criteria that you defined for the Pods. Finally, the kubelet that
    is responsible for running Pods on the selected Node will notice that there is
    a new Pod in the `scheduled` state for the Node and will attempt to start the
    Pod. These principles have been visualized in the following diagram:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要找到一种方法来填充这个`nodeName`值，而这正是 kube-scheduler 的作用。为此，kube-scheduler 会定期轮询 kube-apiserver。它会查找具有空
    `nodeName` 属性的 Pod 资源。一旦找到这样的 Pod，它将执行算法来选举一个节点，并通过向 kube-apiserver 发出请求来更新 Pod
    对象中的 `nodeName` 属性。在为 Pod 选择节点时，`kube-scheduler` 会考虑其内部的调度策略和你为 Pod 定义的标准。最后，负责在选定节点上运行
    Pod 的 kubelet 会注意到该节点的 `scheduled` 状态下有一个新 Pod，并会尝试启动该 Pod。这些原则在下图中已可视化：
- en: '![](img/B22019_19_01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_19_01.png)'
- en: 'Figure 19.1: Interactions of kube-scheduler and kube-apiserver'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.1：kube-scheduler 和 kube-apiserver 的交互
- en: 'The scheduling process for a Pod is performed in two phases:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 的调度过程分为两个阶段：
- en: '**Filtering**: kube-scheduler determines the set of Nodes that are capable
    of running a given Pod. This includes checking the actual state of the Nodes and
    verifying any resource requirements and criteria specified by the Pod definition.
    At this point, if there are no Nodes that can run a given Pod, the Pod cannot
    be scheduled and remains pending.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤**：kube-scheduler 确定能够运行给定 Pod 的节点集合。这包括检查节点的实际状态，并验证 Pod 定义中指定的资源需求和标准。在此阶段，如果没有节点能够运行给定的
    Pod，Pod 将无法调度，保持在待处理状态。'
- en: '**Scoring**: kube-scheduler assigns scores to each Node based on a set of **scheduling
    policies**. Then, the Pod is assigned by the scheduler to the Node with the highest
    score. We will cover scheduling policies in a later section of this chapter.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评分**：kube-scheduler 根据一组 **调度策略** 为每个节点分配分数。然后，调度器将 Pod 分配给得分最高的节点。我们将在本章后面的部分讨论调度策略。'
- en: '`kube-scheduler` will consider criteria and configuration values you can optionally
    pass in the Pod specification. By using these configurations, you can control
    precisely how kube-scheduler will elect a Node for the Pod. To control where a
    Pod runs, you can set constraints to restrict it to specific nodes or indicate
    preferred nodes. We have learned that, typically, Kubernetes will handle Pod placement
    effectively without any manual constraints, ensuring Pods are spread across Nodes
    to prevent resource shortages. However, there are times when you might need to
    influence Pod placement, such as ensuring a Pod runs on a Node with an SSD or
    co-locating Pods that communicate frequently within the same availability zone.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-scheduler` 将考虑你可以选择传递到 Pod 规格中的标准和配置值。通过使用这些配置项，你可以精确控制 kube-scheduler
    如何为 Pod 选择节点。为了控制 Pod 运行的位置，你可以设置约束，将其限制在特定节点上，或指明优先选择的节点。我们已经了解到，通常情况下，Kubernetes
    会有效地处理 Pod 的分配，不需要任何手动约束，确保 Pods 在节点间分布，以避免资源短缺。然而，有时你可能需要影响 Pod 的位置，比如确保 Pod
    运行在带有 SSD 的节点上，或将频繁通信的 Pods 安排在同一可用区内。'
- en: The decisions of kube-scheduler are valid precisely at the point in time when
    the Pod is scheduled. Once the Pod is scheduled and running, kube-scheduler will
    not perform any rescheduling operations while it is running (which can be for
    days or even months). So, even if the Pod no longer matches the Node according
    to your rules, it will remain running. Rescheduling will only happen if the Pod
    is terminated, and a new Pod needs to be scheduled.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: kube-scheduler 的决策只在 Pod 被调度的时刻有效。一旦 Pod 被调度并运行，kube-scheduler 就不会在 Pod 运行期间进行任何重新调度操作（这个过程可能持续数天甚至数月）。因此，即使
    Pod 不再符合你的规则与节点匹配，它仍然会继续运行。只有当 Pod 被终止，并且需要调度新 Pod 时，才会进行重新调度。
- en: 'You can use the following methods to influence Pod scheduling in Kubernetes:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下方法来影响 Kubernetes 中的 Pod 调度：
- en: Use the `nodeSelector`field to match against node labels.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `nodeSelector` 字段来匹配节点标签。
- en: Set affinity and anti-affinity rules.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置亲和性和反亲和性规则。
- en: Specify the `nodeName` field.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定 `nodeName` 字段。
- en: Define Pod topology spread constraints.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义 Pod 拓扑分布约束。
- en: Taints and tolerations.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 污点和容忍。
- en: In the next sections, we will discuss these configurations to control the scheduling
    of Pods. Before we jump into the hands-on practices, make sure you have a multi-node
    Kubernetes cluster to experience the node scheduling scenarios.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论这些配置项以控制 Pods 的调度。在开始实际操作之前，请确保你有一个多节点的 Kubernetes 集群，以体验节点调度场景。
- en: 'In our case, we are using a multi-node Kubernetes cluster using `minikube`
    as follows (you may change the driver to kvm2, Docker, or Podman):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们正在使用一个多节点的 Kubernetes 集群，并通过 `minikube` 进行部署，如下所示（你可以将驱动程序更改为 kvm2、Docker
    或 Podman）：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `--nodes=3` argument will trigger minikube to deploy a Kubernetes cluster
    with the first node as a controller node (or Master node) and the second and third
    nodes as compute nodes (or worker nodes), as shown below:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`--nodes=3` 参数将触发 minikube 部署一个 Kubernetes 集群，第一个节点作为控制节点（或主节点），第二个和第三个节点作为计算节点（或工作节点），如下所示：'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you are using any other Kubernetes cluster for learning, then you may skip
    this minikube cluster setup.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用其他 Kubernetes 集群进行学习，可以跳过此 minikube 集群的设置。
- en: Now, let’s take a look at Node affinity, together with Node name and Node selector.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看节点亲和性，以及节点名称和节点选择器。
- en: Managing Node affinity
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理节点亲和性
- en: To understand how **Node affinity** works in Kubernetes, we first need to take
    a look at the most basic scheduling options, which use **Node name** and **Node
    selector** for Pods.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解 **节点亲和性** 在 Kubernetes 中是如何工作的，我们首先需要看一下最基本的调度选项，这些选项使用 **节点名称** 和 **节点选择器**
    来调度 Pods。
- en: Using nodeName for Pods
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 nodeName 来为 Pods 分配节点
- en: As we mentioned before, each Pod object has a `nodeName` field, which is usually
    controlled by kube-scheduler. Nevertheless, it is possible to set this property
    directly in the YAML manifest when you create a Pod or create a controller that
    uses a Pod template. This is the simplest form of statically scheduling Pods on
    a given Node and is generally *not recommended* – it is not flexible and does
    not scale at all. The names of Nodes can change over time, and you risk running
    out of resources on the Node.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，每个 Pod 对象都有一个 `nodeName` 字段，通常由 kube-scheduler 控制。然而，在你创建 Pod 时，或者创建使用
    Pod 模板的控制器时，可以直接在 YAML 清单中设置该属性。这是将 Pod 静态调度到给定节点的最简单形式，通常*不推荐*使用——它不够灵活，也无法扩展。节点的名称可能随时间变化，而且你有可能在该节点上耗尽资源。
- en: You may find setting `nodeName` explicitly useful in debugging scenarios when
    you want to run a Pod on a specific Node.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在调试场景中，当你想在特定节点上运行 Pod 时，明确设置 `nodeName` 可能会很有用。
- en: We are going to demonstrate all scheduling principles on an example Deployment
    object that we introduced in *Chapter 11*, *Using Kubernetes Deployments for Stateless
    Workloads*. This is a simple Deployment that manages *five* Pod replicas of an
    `nginx` webserver.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将基于 *第11章* 中介绍的示例 Deployment 对象演示所有调度原理，*使用 Kubernetes Deployment 管理无状态工作负载*。这是一个简单的
    Deployment，管理着一个 `nginx` Web 服务器的 *五个* Pod 副本。
- en: 'Before we use `nodeName` in the Deployment manifest, we need to know what Nodes
    we have in the cluster so that we can understand how they are scheduled and how
    we can influence the scheduling of Pods. You can get the list of Nodes using the
    `kubectl get nodes` command, as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们在 Deployment 清单中使用 `nodeName` 之前，我们需要知道集群中有哪些节点，以便理解它们是如何调度的，以及我们如何能影响 Pod
    的调度。你可以使用 `kubectl get nodes` 命令获取节点列表，如下所示：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In our example, we are running a three-Node cluster (remember to refer to your
    correct cluster Node names in the manifest later). For simplicity, let’s refer
    to `minikube` as `Node1`, `minikube-m02` as `Node2`, and `minikube-m02` as `Node3`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们运行的是一个三节点集群（稍后记得在清单中引用你正确的集群节点名称）。为了简化，我们将 `minikube` 称为 `Node1`，`minikube-m02`
    称为 `Node2`，`minikube-m02` 称为 `Node3`。
- en: 'For the demonstration, we want to schedule all five nginx Pods to `minikube-m02`.
    Create the following YAML manifest named `n01_nodename/nginx-deployment.yaml`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个演示中，我们希望将所有五个 nginx Pods 调度到 `minikube-m02`。创建以下 YAML 清单，并命名为 `n01_nodename/nginx-deployment.yaml`：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Apply the Deployment YAML as usual:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样应用 Deployment YAML：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The Deployment object will create five Pod replicas. Use `kubectl get pods
    -o wide` to see the Pods and Node names. Let’s use a customized output as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Deployment 对象将创建五个 Pod 副本。使用 `kubectl get pods -o wide` 来查看 Pod 和节点名称。我们将使用如下的自定义输出：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, by default, the Pods have been distributed uniformly – Node1
    has received one Pod, Node2 two Pods, and Node3 two Pods. This is a result of
    the default scheduling policies enabled in `kube-scheduler` for filtering and
    scoring.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，默认情况下，Pods 已经均匀分配——Node1 收到了一个 Pod，Node2 收到了两个 Pods，Node3 收到了两个 Pods。这是
    `kube-scheduler` 中启用的默认调度策略进行过滤和评分的结果。
- en: If you are running a **non-managed** Kubernetes cluster, you can inspect the
    logs for the kube-scheduler Pod using the `kubectl logs` command, or even directly
    in the control plane nodes in `/var/log/kube-scheduler.log`. This may also require
    increased verbosity of logs for the kube-scheduler process. You can read more
    at [https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在运行一个 **非托管** 的 Kubernetes 集群，你可以使用 `kubectl logs` 命令检查 kube-scheduler Pod
    的日志，或者直接在控制平面节点中的 `/var/log/kube-scheduler.log` 查看。这可能还需要增加 kube-scheduler 进程的日志详细级别。你可以在
    [https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/)
    阅读更多信息。
- en: At this point, the Pod template in `.spec.template.spec` does not contain any
    configurations that affect the scheduling of the Pod replicas.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`.spec.template.spec` 中的 Pod 模板并不包含任何影响 Pod 副本调度的配置。
- en: 'We will now **forcefully** assign all Pods in the Deployment to **Node2** (`minikube-m02`
    in our case) in the cluster using the `nodeName` field in the Pod template. Change
    the `nginx-deployment.yaml` YAML manifest so that it has this property set with
    the correct Node name for *your* cluster:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将通过在 Pod 模板中使用 `nodeName` 字段，**强制**将 Deployment 中的所有 Pod 分配到 **Node2**（在我们的例子中是
    `minikube-m02`）。请更改 `nginx-deployment.yaml` YAML 清单，将该属性设置为 *你* 集群的正确节点名称：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Notice the line `nodeName: minikube-m02`; we are explicitly stating that `minikube-m02`
    should be used as the Node for deploying our nginx Pods.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '注意这一行 `nodeName: minikube-m02`；我们明确指出 `minikube-m02` 应该作为部署 nginx Pods 的节点。'
- en: 'Apply the manifest to the cluster using the `kubectl apply -f ./nginx-deployment.yaml`
    command:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl apply -f ./nginx-deployment.yaml` 命令将清单应用到集群中：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, inspect the Pod status and Node assignment again:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，再次检查 Pod 状态和节点分配情况：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As expected, *all five* Pods are now running on Node2 (`minikube-m02`). These
    are all new Pods – when you change the Pod template in the Deployment specification,
    it causes an internal rollout using a new ReplicaSet object, while the old ReplicaSet
    object is scaled down, as explained in *Chapter 11*, *Using Kubernetes Deployments
    for Stateless Workloads*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，*所有五个* Pod 现在都运行在 Node2（`minikube-m02`）上。这些都是新的 Pod——当你在 Deployment
    规范中更改 Pod 模板时，它会导致一个新的 ReplicaSet 对象的内部回滚，而旧的 ReplicaSet 对象则会缩小，如 *第 11 章* 中所述，*使用
    Kubernetes Deployments 管理无状态工作负载*。
- en: In this way, we have actually *bypassed* `kube-scheduler`. If you inspect events
    for one of the Pods using the `kubectl describe pod` command, you will see that
    it lacks any events with `Scheduled` as a reason.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们实际上已经 *绕过* 了 `kube-scheduler`。如果你使用 `kubectl describe pod` 命令检查其中一个
    Pod 的事件，你会看到它缺少任何带有 `Scheduled` 作为原因的事件。
- en: Next, we are going to take a look at another basic method of scheduling Pods,
    which is `nodeSelector`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看另一种基本的 Pod 调度方法，即 `nodeSelector`。
- en: Using nodeSelector for Pods
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 nodeSelector 为 Pods 调度
- en: Pod specification has a special field, `.spec.nodeSelector`, that gives you
    the ability to schedule your Pod only on Nodes that have certain label values.
    This concept is similar to **label selectors** for Deployments and StatefulSets,
    but the difference is that it allows only simple *equality-based* comparisons
    for labels. You cannot do advanced *set-based* logic.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 规范有一个特殊字段 `.spec.nodeSelector`，该字段使你能够仅在具有特定标签值的节点上调度 Pod。这个概念类似于 **标签选择器**
    用于 Deployments 和 StatefulSets，但不同之处在于，它仅允许对标签进行简单的 *等式比较*。你不能进行复杂的 *基于集合* 的逻辑操作。
- en: 'This is especially useful in:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下情况下特别有用：
- en: '**Hybrid clusters**: Ensure Windows containers run on Windows nodes and Linux
    containers run on Linux nodes by specifying the operating system as a scheduling
    criterion.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合集群**：通过将操作系统指定为调度标准，确保 Windows 容器在 Windows 节点上运行，而 Linux 容器在 Linux 节点上运行。'
- en: '**Resource allocation**: Target Pods to nodes with specific resources (CPU,
    memory, storage) to optimize resource utilization.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源分配**：将 Pods 定向到具有特定资源（CPU、内存、存储）的节点，以优化资源利用率。'
- en: '**Hardware requirements**: Schedule Pods requiring special hardware (e.g.,
    GPUs) only on nodes with those capabilities.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件要求**：仅在具有相关能力的节点上调度需要特殊硬件（例如 GPU）的 Pod。'
- en: '**Security zones**: Define security zones with labels and use `nodeSelector`
    to restrict Pods to specific zones for enhanced security.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全区域**：通过标签定义安全区域，并使用`nodeSelector`限制 Pods 只能在特定区域内运行，以增强安全性。'
- en: 'Every Kubernetes Node comes by default with a set of labels, which include
    the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Kubernetes 节点默认都带有一组标签，包括以下内容：
- en: '`kubernetes.io/arch`: Describes the Node’s processor architecture, for example,
    `amd64` or `arm`. This is also defined as `beta.kubernetes.io/arch`.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubernetes.io/arch`：描述节点的处理器架构，例如 `amd64` 或 `arm`。这也被定义为 `beta.kubernetes.io/arch`。'
- en: '`kubernetes.io/os`: Has a value of `linux` or `Windows`. This is also defined
    as `beta.kubernetes.io/os`.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubernetes.io/os`：该标签的值为 `linux` 或 `Windows`。这也被定义为 `beta.kubernetes.io/os`。'
- en: '`node-role.kubernetes.io/control-plane`: The role of the node in the Kubernetes
    cluster.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`node-role.kubernetes.io/control-plane`：节点在 Kubernetes 集群中的角色。'
- en: 'If you inspect the labels for one of the Nodes, you will see that there are
    plenty of them. In our case, some of them are specific to `minikube` clusters:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检查其中一个节点的标签，你会看到有很多标签。在我们的例子中，其中一些是特定于`minikube`集群的：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Of course, you can define your *own* labels for the Nodes and use them to control
    scheduling. Please note that in general you should use semantic labeling for your
    resources in Kubernetes, rather than give them special labels just for the purpose
    of scheduling. Let’s demonstrate how to do that by following these steps:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以为节点定义 *自己的* 标签，并使用这些标签来控制调度。请注意，通常你应该在 Kubernetes 中使用语义标签来标识资源，而不是仅仅为了调度的目的给资源添加特殊标签。让我们通过以下步骤演示如何操作：
- en: 'Use the `kubectl label nodes` command to add a `node-type` label with a `superfast`
    value to `Node` `1` and `Node` `2` in the cluster:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `kubectl label nodes` 命令，将一个 `node-type` 标签（值为 `superfast`）添加到集群中的 `Node`
    `1` 和 `Node` `2`：
- en: '[PRE10]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Verify the node labels as follows:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式验证节点标签：
- en: '[PRE11]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Edit the `./nginx-deployment.yaml` Deployment manifest (or create another one
    called `02_nodeselector/nginx-deployment.yaml`) so that `nodeSelector` in the
    Pod template is set to `node-type: superfast`, as follows:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '编辑 `./nginx-deployment.yaml` 部署清单（或创建另一个名为 `02_nodeselector/nginx-deployment.yaml`
    的清单），使 Pod 模板中的 `nodeSelector` 设置为 `node-type: superfast`，如下所示：'
- en: '[PRE12]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Apply the manifest to the cluster using the `kubectl apply -f 02_nodeselector/nginx-deployment.yaml`
    command and inspect the Pod status and Node assignment again. You may need to
    wait a while for the Deployment rollout to finish:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `kubectl apply -f 02_nodeselector/nginx-deployment.yaml` 命令将清单应用到集群中，然后再次检查
    Pod 状态和节点分配。您可能需要等一段时间，直到 Deployment 完成滚动更新：
- en: '[PRE13]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see in the preceding output, Pods are now assigned to `minikube-m02`
    and `minikube-m03` (`minikube-m02` has been assigned with three Pods and `minikube-m02`
    with two Pods). The Pods have been distributed among Nodes that have the `node-type=superfast`
    label.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的输出所示，Pod 现在已分配给 `minikube-m02` 和 `minikube-m03`（`minikube-m02` 已分配了三个 Pod，`minikube-m03`
    分配了两个 Pod）。这些 Pod 已经分布在具有 `node-type=superfast` 标签的节点上。
- en: 'In contrast, if you change the `./nginx-deployment.yaml` manifest so that `nodeSelector`
    in the Pod template is set to `node-type: slow`, which no Node in the cluster
    has assigned, we will see that Pods could not be scheduled and the Deployment
    will be stuck. Edit the manifest (or copy to a new file called `02_nodeselector/nginx-deployment-slow.yaml`):'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '相反，如果您修改 `./nginx-deployment.yaml` 清单，使 Pod 模板中的 `nodeSelector` 设置为 `node-type:
    slow`，而集群中没有节点分配此标签，我们将看到 Pod 无法调度，Deployment 会被卡住。编辑清单（或将其复制到一个名为 `02_nodeselector/nginx-deployment-slow.yaml`
    的新文件中）：'
- en: '[PRE14]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Apply the manifest to the cluster as follows:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以如下方式将清单应用到集群中：
- en: '[PRE15]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Inspect the Pod status and Node assignment again:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 再次检查 Pod 状态和节点分配：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The reason why three new Pods are pending and four old Pods are still running
    is the default configuration of rolling updates in the Deployment object. By default,
    `maxSurge` is set to `25%` of Pod replicas (the absolute number is *rounded up*),
    so in our case, two Pods are allowed to be created above the desired number of
    five Pods. In total, we now have seven Pods. At the same time, `maxUnavailable`
    is also `25%` of Pod replicas (but the absolute number is *rounded down*), so
    in our case, one Pod out of five cannot be available. In other words, four Pods
    must be `Running`. And because the new `Pending` Pods cannot get a Node in the
    process of scheduling, the Deployment is stuck waiting and not progressing. Normally,
    in this case, you need to either perform a rollback to the previous version for
    the Deployment or change `nodeSelector` to one that matches existing Nodes properly.
    Of course, there is also an alternative of adding a new Node with matching labels
    or adding missing labels to the existing ones without performing a rollback.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 三个新 Pod 正处于挂起状态，四个旧 Pod 仍在运行的原因是 Deployment 对象中滚动更新的默认配置。默认情况下，`maxSurge` 设置为
    Pod 副本的 `25%`（绝对数量会*向上取整*），因此在我们的情况下，允许创建超过五个 Pod 所需数量的两个 Pod。总的来说，现在我们有七个 Pod。同时，`maxUnavailable`
    也是 Pod 副本的 `25%`（但绝对数量会*向下取整*），所以在我们的情况下，五个 Pod 中有一个 Pod 是不可用的。换句话说，四个 Pod 必须是
    `Running` 状态。而且，由于新创建的 `Pending` 状态 Pod 在调度过程中无法获取节点，Deployment 被卡住，无法继续进行。通常，在这种情况下，您需要要么将
    Deployment 回滚到之前的版本，要么将 `nodeSelector` 更改为与现有节点匹配的值。当然，您也可以选择添加一个带有匹配标签的新节点，或者向现有节点添加缺失的标签，而不进行回滚。
- en: 'We will now continue the topic of scheduling Pods by looking at the first of
    some more advanced techniques: **Node affinity**.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将继续讨论 Pod 调度的话题，首先介绍一些更高级的技术：**Node 亲和性**。
- en: Using the nodeAffinity configuration for Pods
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Pod 的 nodeAffinity 配置
- en: The concept of Node affinity expands the `nodeSelector` approach and provides
    a richer language for defining which Nodes are preferred or avoided for your Pod.
    In everyday life, the word “affinity” is defined as “*a natural liking for and
    understanding of someone or something*,” and this best describes the purpose of
    Node affinity for Pods. That is, you can control which Nodes your Pod will be
    *attracted* to or *repelled* by.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Node 亲和性的概念扩展了 `nodeSelector` 方法，并提供了一个更丰富的语言来定义哪些节点是 Pod 更倾向或避免的。在日常生活中，“affinity”
    一词的定义是“*对某人或某事的自然喜好和理解*”，这正好描述了 Node 亲和性对 Pod 的目的。也就是说，您可以控制 Pod 将被*吸引*到哪些节点，或者哪些节点会被*排斥*。
- en: 'With Node affinity, represented in `.spec.affinity.nodeAffinity` for the Pod,
    you get the following enhancements over simple `nodeSelector`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 Node 亲和性，表示在 Pod 的 `.spec.affinity.nodeAffinity` 中，您可以获得比简单的 `nodeSelector`
    更强大的功能：
- en: You get a richer language for expressing the rules for matching Pods to Nodes.
    For example, you can use the `In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt`, and
    `Lt` operators for labels.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将获得一个更丰富的语言来表达匹配 Pod 到节点的规则。例如，您可以使用 `In`、`NotIn`、`Exists`、`DoesNotExist`、`Gt`
    和 `Lt` 操作符来处理标签。
- en: Similar to `nodeAffinity`, it is possible to do scheduling using `inter-Pod`
    affinity (`podAffinity`) and additionally `anti-affinity` (`podAntiAffinity`).
    Anti-affinity has the opposite effect of affinity – you can define rules that
    repel Pods. In this way, you can make your Pods be attracted to Nodes that *already
    run* certain Pods. This is especially useful if you want to collocate Pods to
    decrease latency.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与`nodeAffinity`类似，也可以使用`Pod间`亲和性（`podAffinity`）以及额外的`反亲和性`（`podAntiAffinity`）进行调度。反亲和性与亲和性具有相反的效果——你可以定义排斥Pod的规则。通过这种方式，你可以让Pod被吸引到*已经运行*某些Pod的节点上。如果你想将Pod协同调度以降低延迟，这尤其有用。
- en: It is possible to define **soft** affinity and anti-affinity rules that represent
    a *preference* instead of a **hard** rule. In other words, the scheduler can still
    schedule the Pod, even if it cannot match the soft rule. Soft rules are represented
    by the `preferredDuringSchedulingIgnoredDuringExecution` field in the specification,
    whereas hard rules are represented by the `requiredDuringSchedulingIgnoredDuringExecution`
    field.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以定义**软性**亲和性和反亲和性规则，表示*偏好*，而不是**硬性**规则。换句话说，即使调度器无法匹配软性规则，它仍然可以调度Pod。软性规则在规范中由`preferredDuringSchedulingIgnoredDuringExecution`字段表示，而硬性规则由`requiredDuringSchedulingIgnoredDuringExecution`字段表示。
- en: Soft rules can be **weighted**, and it is possible to add multiple rules with
    different weight values. The scheduler will consider this weight value together
    with the other parameters to make a decision on the affinity.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软性规则可以**加权**，并且可以添加多个不同权重值的规则。调度器将考虑这些权重值以及其他参数来做出亲和性决策。
- en: 'Even though there is no Node anti-affinity field provided by a separate field
    in the spec, as in the case of inter-Pod anti-affinity you can still achieve similar
    results by using the `NotIn` and `DoesNotExist` operators. In this way, you can
    make Pods be repelled from Nodes with specific labels, also in a soft way. Refer
    to the documentation to learn more: [https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管规范中没有提供一个专门的字段用于节点反亲和性，与Pod间反亲和性类似，你仍然可以通过使用`NotIn`和`DoesNotExist`操作符来实现类似效果。通过这种方式，你可以让Pod从带有特定标签的节点上被排斥，同样以软性方式进行。有关更多信息，请参见文档：[https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)。
- en: The use cases and scenarios for defining the Node affinity and inter-Pod affinity/anti-affinity
    rules are *unlimited*. It is possible to express all kinds of requirements in
    this way, provided that you have enough labeling on the Nodes. For example, you
    can model requirements like scheduling the Pod only on a Windows Node with an
    Intel CPU and premium storage in the West Europe region but currently not running
    Pods for MySQL, or try not to schedule the Pod in availability zone 1, but if
    it is not possible, then availability zone 1 is still OK.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 定义节点亲和性和Pod间亲和性/反亲和性规则的使用场景和案例是*无限的*。只要节点上有足够的标签，你就可以通过这种方式表达各种要求。例如，你可以建模类似的需求：仅将Pod调度到带有Intel
    CPU和高端存储的Windows节点上，并且位于西欧区域，但当前没有运行MySQL的Pod，或者尽量不将Pod调度到可用区1，但如果无法避免，那么可用区1仍然可以接受。
- en: 'To demonstrate Node affinity, we will try to model the following requirements
    for our Deployment: “*Try* to schedule the Pod only on Nodes with a `node-type`
    label with a `fast` or `superfast` value, but if this is not possible, use any
    Node but *strictly* not with a `node-type` label with an `extremelyslow` value.”
    For this, we need to use:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示节点亲和性，我们将尝试为我们的部署建模以下需求：“*尽量*将Pod调度到带有`node-type`标签且值为`fast`或`superfast`的节点上，但如果无法满足这个条件，可以使用任何节点，但*严格*避免使用`node-type`标签值为`extremelyslow`的节点。”为此，我们需要使用：
- en: '**A soft Node affinity** rule of type `preferredDuringSchedulingIgnoredDuringExecution`
    to match `fast` and `superfast` Nodes.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软性节点亲和性**规则，类型为`preferredDuringSchedulingIgnoredDuringExecution`，用于匹配`fast`和`superfast`节点。'
- en: '**A hard Node affinity** rule of type `requiredDuringSchedulingIgnoredDuringExecution`
    to repel the Pod strictly from Nodes with `node-type` as `extremelyslow`. We need
    to use the `NotIn` operator to get the anti-affinity effect.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬性节点亲和性**规则，类型为`requiredDuringSchedulingIgnoredDuringExecution`，用于严格排除带有`node-type`标签且值为`extremelyslow`的节点。我们需要使用`NotIn`操作符来实现反亲和性效果。'
- en: 'In our cluster, we are going to first have the following labels for Nodes:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的集群中，我们将首先为节点定义以下标签：
- en: '`Node1`: `slow`'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Node1`: `slow`'
- en: '`Node2`: `fast`'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Node2`: `fast`'
- en: '`Node3`: `superfast`'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Node3`：`superfast`'
- en: As you can see, according to our requirements, the Deployment Pods should be
    scheduled on Node2 and Node3, unless something is preventing them from being allocated
    there, like a lack of CPU or memory resources. In that case, Node1 would also
    be allowed as we use the soft affinity rule.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，根据我们的要求，Deployment Pods应该被调度到Node2和Node3，除非有某些因素阻止它们被分配到那里，比如缺乏CPU或内存资源。在这种情况下，Node1也会被允许，因为我们使用了软性亲和性规则。
- en: 'Next, we will relabel the Nodes in the following way:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将按照以下方式重新标记节点：
- en: '`Node1`: `slow`'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Node1`：`slow`'
- en: '`Node2`: `extremelyslow`'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Node2`：`extremelyslow`'
- en: '`Node3`: `extremelyslow`'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Node3`：`extremelyslow`'
- en: Subsequently, we will need to redeploy our Deployment (for example, scale it
    down to zero and up to the original replica count, or use the `kubectl rollout
    restart` command) to reschedule the Pods again. After that, looking at our requirements,
    kube-scheduler should assign all Pods to Node1 (because it is still allowed by
    the soft rule) but avoid *at all costs* Node2 and Node3\. If, by any chance, Node1
    has no resources to run the Pod, then the Pods will be stuck in the `Pending`
    state.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要重新部署我们的Deployment（例如，将其缩放为零，然后恢复到原始副本数，或者使用`kubectl rollout restart`命令），以便重新调度Pods。之后，根据我们的要求，kube-scheduler应该将所有Pods调度到Node1（因为它仍然符合软性规则），但*无论如何都要避免*Node2和Node3。如果Node1没有资源来运行Pod，那么Pods将处于`Pending`状态。
- en: 'To solve the issue of rescheduling already running Pods (in other words, to
    make kube-scheduler consider them again), there is an incubating Kubernetes project
    named **Descheduler**. You can find out more here: [https://github.com/kubernetes-sigs/descheduler](https://github.com/kubernetes-sigs/descheduler).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决已经运行的Pods重新调度的问题（换句话说，让kube-scheduler重新考虑它们），有一个正在孵化的Kubernetes项目叫做**Descheduler**。你可以在这里了解更多信息：[https://github.com/kubernetes-sigs/descheduler](https://github.com/kubernetes-sigs/descheduler)。
- en: 'To do the demonstration, please follow these steps:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行演示，请按照以下步骤操作：
- en: 'Use the `kubectl label nodes` command to add a `node-type` label with a `slow`
    value for Node1, a `fast` value for Node2, and a `superfast` value for Node3:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`kubectl label nodes`命令为Node1添加一个`node-type`标签，值为`slow`，为Node2添加`fast`值标签，为Node3添加`superfast`值标签：
- en: '[PRE17]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Edit the `03_affinity/nginx-deployment.yaml` Deployment manifest and define
    the soft Node affinity rule as follows:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑`03_affinity/nginx-deployment.yaml` Deployment清单，并按如下方式定义软性节点亲和性规则：
- en: '[PRE18]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As you can see, we have used `nodeAffinity` (not `podAffinity` or `podAntiAffinity`)
    with `preferredDuringSchedulingIgnoredDuringExecution` set so that it has only
    one soft rule: `node-type` should have a `fast` value or a `superfast` value.
    This means that if there are no resources on such Nodes, they can still be scheduled
    on other Nodes. Additionally, we specify one hard anti-affinity rule in `requiredDuringSchedulingIgnoredDuringExecution`,
    which says that `node-type` *must not* be `extremelyslow`. You can find the full
    specification of Pod’s `.spec.affinity` in the official documentation: [https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们使用了`nodeAffinity`（而不是`podAffinity`或`podAntiAffinity`），并设置了`preferredDuringSchedulingIgnoredDuringExecution`，这样它只有一个软性规则：`node-type`应该具有`fast`值或`superfast`值。这意味着，如果这些节点上没有资源，它们仍然可以调度到其他节点。此外，我们在`requiredDuringSchedulingIgnoredDuringExecution`中指定了一个硬性反亲和性规则，要求`node-type`*不能是*`extremelyslow`。你可以在官方文档中找到Pod的`.spec.affinity`的完整规范：[https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)。
- en: 'Apply the manifest to the cluster using the `kubectl apply -f 03_affinity/nginx-deployment.yaml`
    command and inspect the Pod status and Node assignment again. You may need to
    wait a while for the Deployment rollout to finish:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`kubectl apply -f 03_affinity/nginx-deployment.yaml`命令将清单应用到集群中，然后再次检查Pod的状态和节点分配情况。你可能需要等待一段时间，直到部署完成：
- en: '[PRE19]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Our Node affinity rules were defined to prefer Nodes that have `node-type` set
    to either `fast` or `superfast`, and indeed the Pods were scheduled for Node2
    and Node3 only.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的节点亲和性规则被定义为偏好具有`node-type`为`fast`或`superfast`的节点，实际上Pods只会调度到Node2和Node3。
- en: 'Now, we will perform an experiment to demonstrate how the soft part of Node
    affinity works together with the hard part of Node anti-affinity. We will relabel
    the Nodes as described in the introduction, redeploy the Deployment, and observe
    what happens. Please follow these steps:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将进行一次实验，演示节点亲和性的软性部分是如何与节点反亲和性的硬性部分协同工作的。我们将按照引言中描述的方式重新标记节点，重新部署 Deployment，并观察发生的变化。请按照以下步骤操作：
- en: 'Use the `kubectl label nodes` command to add a `node-type` label with a `slow`
    value for `Node 0`, an `extremelyslow` value for `Node1`, and an `extremelyslow`
    value for `Node2`:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `kubectl label nodes` 命令为 `Node 0` 添加 `node-type` 标签，并将其值设置为 `slow`，为 `Node1`
    设置值为 `extremelyslow`，为 `Node2` 设置值为 `extremelyslow`：
- en: '[PRE20]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'At this point, if you were to check Pod assignments using `kubectl get pods`,
    there would be no difference. This is because, as we explained before, a Pod’s
    assignment to Nodes is valid only at the time of scheduling, and after that, it
    is not changed unless they are restarted. To force the restart of Pods, we could
    scale the Deployment down to zero replicas and then back to five. But there is
    an easier way, which is to use an imperative `kubectl rollout restart` command.
    This approach has the benefit of not making the Deployment unavailable, and it
    performs a rolling restart of Pods without a decrease in the number of available
    Pods. Execute the following command:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，如果你使用 `kubectl get pods` 检查 Pod 分配情况，结果将没有变化。因为正如我们之前解释的，Pod 的节点分配只在调度时有效，之后除非
    Pod 被重启，否则不会更改。为了强制重启 Pods，我们可以将 Deployment 缩放到零副本，然后再恢复到五个副本。但有一种更简单的方法，即使用 `kubectl
    rollout restart` 命令。这种方法的好处是不会使 Deployment 不可用，并且会进行滚动重启，而不会减少可用 Pod 的数量。执行以下命令：
- en: '[PRE21]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Inspect the Pod status and Node assignment again. You may need to wait a while
    for the Deployment rollout to finish:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次检查 Pod 状态和节点分配。你可能需要等待一段时间，直到部署完成：
- en: '[PRE22]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The output shows that, as expected, all Pods have been scheduled to Node1, which
    is labeled with `node-type=slow`. We allow such Nodes if there is nothing better,
    and in this case, Node2 and Node3 have the `node-type=extremelyslow` label, which
    is prohibited by the hard Node anti-affinity rule.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果显示，如预期所示，所有 Pod 都已调度到标记为 `node-type=slow` 的 Node1 上。如果没有更好的节点可用，我们允许 Pod
    调度到这样的节点，而此时，Node2 和 Node3 被标记为 `node-type=extremelyslow`，这被硬性节点反亲和性规则禁止。
- en: 'To achieve even higher granularity and control of Pod scheduling, you can use
    *Pod topology spread constraints*. More details are available in the official
    documentation: [https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现更高的粒度和对 Pod 调度的控制，你可以使用 *Pod 拓扑分布约束*。更多细节请参考官方文档：[https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/)。
- en: Congratulations, you have successfully configured Node affinity for our Deployment
    Pods! We will now explore another way of scheduling Pods – *T**aints and tolerations*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，你已经成功为我们的部署 Pod 配置了节点亲和性！接下来，我们将探索另一种调度 Pod 的方法——*污点和容忍*。
- en: Using Node taints and tolerations
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用节点污点和容忍
- en: Using the Node and inter-Pod affinity mechanism for scheduling Pods is very
    powerful, but sometimes you need a simpler way of specifying which Nodes should
    *repel* Pods. Kubernetes has two slightly older and simpler features for this
    purpose – **taints** and **tolerations**. You apply a taint to a given Node (which
    describes some kind of limitation) and the Pod must have a specific toleration
    defined to be schedulable on the tainted Node. If the Pod has a toleration, it
    does not mean that the taint is *required* on the Node. The definition of *taint*
    is “a trace of a bad or undesirable substance or quality,” and this reflects the
    idea pretty well – all Pods will *avoid* a Node if there is a taint set for them,
    but we can instruct Pods to *tolerate* a specific taint.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用节点和 Pod 之间的亲和性机制调度 Pods 非常强大，但有时你需要一种更简单的方式来指定哪些节点应该 *排斥* Pods。Kubernetes
    提供了两个稍微旧一些、但更简单的功能来实现这一点——**污点**和**容忍**。你可以对某个节点应用污点（描述某种限制），而 Pod 必须定义一个特定的容忍才可以调度到这个被污点标记的节点上。如果
    Pod 有容忍，并不意味着该节点上必须存在污点。*污点*的定义是“有不良或不希望出现的物质或特质的痕迹”，这个定义很符合我们的想法——如果节点上有污点，所有
    Pod 都会 *避开* 这个节点，但我们可以指示 Pod *容忍* 特定的污点。
- en: If you look closely at how taints and tolerations are described, you can see
    that you can achieve similar results with Node labels and Node hard and soft affinity
    rules with the `NotIn` operator. There is one catch – you can define taints with
    a `NoExecute` effect, which will result in the termination of the Pod if it cannot
    tolerate it. You cannot get similar results with affinity rules unless you restart
    the Pod manually.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Taints for Nodes have the following structure: `<key>=<value>:<effect>`. The
    **key** and **value** pair *identifies* the taint and can be used for more granular
    tolerations, for example, tolerating all taints with a given key and any value.
    This is similar to labels, but please remember that taints are separate properties,
    and defining a taint does not affect Node labels. In our example demonstration,
    we will use our taint with a `machine-check-exception` key and a `memory` value.
    This is, of course, a theoretical example where we want to indicate that there
    is a hardware issue with memory on the host, but you could also have a taint with
    the same key and instead a `cpu` or `disk` value.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: In general, your taints should *semantically* label the type of issue that the
    Node is experiencing. There is nothing preventing you from using any keys and
    values for creating taints, but if they make semantic sense, it is much easier
    to manage them and define tolerations.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'The taint can have different effects:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '`NoSchedule` – kube-scheduler *will not schedule* Pods to this Node. Similar
    behavior can be achieved using a hard Node affinity rule.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PreferNoSchedule` – kube-scheduler *will try to not schedule* Pods to this
    Node. Similar behavior can be achieved using a soft Node affinity rule.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NoExecute` – kube-scheduler *will not schedule* Pods to this Node and *evict*
    (terminate and reschedule) running Pods from this Node. You cannot achieve similar
    behavior using Node affinity rules. Note that when you define a toleration for
    a Pod for this type of taint, it is possible to control how long the Pod will
    tolerate the taint before it gets evicted, using `tolerationSeconds`.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes manages quite a few `NoExecute` taints automatically by monitoring
    the Node hosts. The following taints are built in and managed by **NodeController**
    or the `kubelet`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/not-ready`: Added when NodeCondition `Ready` has a `false`
    status.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/unreachable`: Added when NodeCondition `Ready` has an `Unknown`
    status. This happens when `NodeController` cannot reach the Node.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/memory-pressure`: Node is experiencing memory pressure.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/disk-pressure`: Node is experiencing disk pressure.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/network-unavailable`: Network is currently down on the
    Node.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/unschedulable`: Node is currently in an `unschedulable`
    state.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.cloudprovider.kubernetes.io/uninitialized`: Intended for Nodes that are
    prepared by an external cloud provider. When the Node gets initialized by `cloud-controller-manager`,
    this taint is removed.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To add a taint on a Node, you use the `kubectl taint node` command in the following
    way:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'So, for example, if we want to use key `machine-check-exception` and a `memory`
    value with a `NoExecute` effect for Node1, we will use the following command:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To remove the same taint, you need to use the following command (bear in mind
    the - character at the end of the taint definition):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can also remove all taints with a specified key:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To counteract the effect of the taint on a Node for specific Pods, you can
    define tolerations in their specification. In other words, you can use tolerations
    to ignore taints and still schedule the Pods to such Nodes. If a Node has multiple
    taints applied, the Pod must tolerate all of its taints. Tolerations are defined
    under `.spec.tolerations` in the Pod specification and have the following structure:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The operator can be either `Equal` or `Exists`. `Equal` means that the `key`
    and `value` of the taint must match exactly, whereas `Exists` means that just
    `key` must match and `value` is not considered. In our example, if we want to
    ignore the taint, the toleration will need to look like this:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Please note, you can define multiple tolerations for a Pod to ensure the correct
    Pod placement.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: In the case of `NoExecute` tolerations, it is possible to define an additional
    field called `tolerationSeconds`, which specifies how long the Pod will tolerate
    the taint until it gets evicted. So, this is a way of having partial toleration
    of taint with a timeout. Please note that if you use `NoExecute` taints, you usually
    also need to add a `NoSchedule` taint. In this way, you can prevent any **eviction
    loops** from happening when the Pod has a `NoExecute` toleration with `tolerationSeconds`
    set. This is because the taint has no effect for a specified number of seconds,
    which also includes *not* preventing the Pod from being scheduled for the tainted
    Node.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: When Pods are created in the cluster, Kubernetes automatically adds two `Exists`
    tolerations for `node.kubernetes.io/not-ready` and `node.kubernetes.io/unreachable`
    with `tolerationSeconds` set to `300`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have learned about taints and tolerations, we will put this knowledge
    into practice with a few demonstrations. Please follow the next steps to go through
    the taints and tolerations exercise:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have the `nginx-app` Deployment with Node affinity defined still running
    from the previous section, it will currently have all Pods running on Node1 (`minikube`).
    The Node affinity rules are constructed in such a way that the Pods cannot be
    scheduled on Node2 and Node3\. Let’s see what happens if you taint Node1 with
    `machine-check-exception=memory:NoExecute`:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Check the Pod status and Node assignment:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: All Deployment Pods are now in the `Pending` state because kube-scheduler is
    unable to find a Node that can run them.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit the `./nginx-deployment.yaml` Deployment manifest (or check `04_taints/nginx-deployment.yaml`)
    and remove `affinity`. Instead, define taint toleration for `machine-check-exception=memory:NoExecute`
    with a timeout of 60 seconds as follows:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: When this manifest is applied to the cluster, the old Node affinity rules which
    prevented scheduling to Node2 and Node3 will be gone. The Pods will be able to
    schedule on Node2 and Node3, but Node1 has taint `machine-check-exception=memory:NoExecute`.
    So, the Pods should *not* be scheduled to `Node0`, as `NoExecute` implies `NoSchedule`,
    *right*? Let’s check that.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the manifest to the cluster using the `kubectl apply -f 04_taints/nginx-deployment.yaml`
    command and inspect the Pod status and Node assignment again. You may need to
    wait a while for the Deployment rollout to finish:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This result may be a bit surprising. As you can see, we got Pods scheduled on
    Node2 and Node3, but at the same time Node1 has received Pods, and they are in
    an eviction loop every 60 seconds! The explanation for this is that `tolerationSeconds`
    for the `NoExecute` taint implies that the whole taint is ignored for 60 seconds.
    So, kube-scheduler can schedule the Pod on Node1, even though it will get evicted
    later.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fix this behavior by applying a recommendation to use a `NoSchedule`
    taint whenever you use a `NoExecute` taint. In this way, the evicted Pods will
    have no chance to be scheduled on the tainted Node again, unless, of course, they
    start tolerating this type of taint too. Execute the following command to taint
    `Node0`:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Inspect the Pod status and Node assignment again:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the output, you can see that the Pods are now distributed between Node2 and
    Node3 – exactly as we wanted.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, remove *both* taints from Node1:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Restart the Deployment to reschedule the Pods using the following command:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Inspect the Pod status and Node assignment again:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The Pods are again distributed evenly between all three Nodes.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, let’s see how the combination of the `NoExecute` and `NoSchedule`
    taints work, with `tolerationSeconds` for `NoExecute` set to `60`. Apply two taints
    to Node1 again:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Immediately after that, start watching Pods with their Node assignments. Initially,
    you will see that the Pods are still running on Node1 for some time. But after
    60 seconds, you will see:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As we expected, the Pods were evicted after 60 seconds and there were no eviction-schedule
    loops.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: This has demonstrated a more advanced use case for taints that you cannot easily
    substitute with Node affinity rules.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Let’s learn about static Pods in the next section.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Static Pods in Kubernetes
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Static Pods offer a different way to manage Pods within a Kubernetes cluster.
    Unlike regular Pods, which are controlled by the cluster’s Pod schedulers and
    API server, static Pods are managed directly by the kubelet daemon on a specific
    node. The API server isn’t aware of these Pods, except through mirror Pods that
    the kubelet creates for them.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '**Key characteristics**:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '**Node-specific**: Static Pods are tied to a single node and can’t be moved
    elsewhere in the cluster.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubelet management**: The kubelet on the designated node handles starting,
    stopping, and restarting static Pods.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mirror Pods**: The kubelet creates mirror Pods on the API server to reflect
    the state of static Pods, but these mirror Pods can’t be controlled through the
    API.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Static Pods can be created using two main methods. The first method is the filesystem-hosted
    configuration, where you place Pod definitions in YAML or JSON format in a specific
    directory on the node. The kubelet scans this directory regularly and manages
    Pods based on the files present. The second method is the web-hosted configuration,
    where the Pod definition is hosted in a YAML file on a web server. The kubelet
    is configured with the URL of this file and periodically downloads it to manage
    the static Pods.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Static Pods are often used for bootstrapping essential cluster components, like
    the API server or controller manager, on each node. However, for running Pods
    on every node in a cluster, DaemonSets are usually recommended. Static Pods have
    limitations, such as not being able to reference other Kubernetes objects like
    Secrets or ConfigMaps and not supporting ephemeral containers. Understanding static
    Pods can be useful in scenarios where tight control over Pod placement on individual
    nodes is needed.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have covered different mechanisms to control the Pod scheduling and
    placement. In the next section, we will give a short overview of other scheduler
    configurations and features.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Extended scheduler configurations in Kubernetes
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the scheduler customizations, Kubernetes also supports some advanced
    scheduling configurations, which we will discuss in this section.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler configuration
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can customize this scheduling behavior using a configuration file. This
    file defines how the scheduler prioritizes Nodes for Pods based on various criteria.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '**Key concepts**:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '**Scheduling profiles**: The configuration file can specify multiple scheduling
    profiles. Each profile has a distinct name and can be configured with its own
    set of plugins.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scheduling plugins**: Plugins are like building blocks that perform specific
    tasks during the scheduling process. They can filter Nodes based on resource availability,
    hardware compatibility, or other factors.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extension points**: These are stages within the scheduling process where
    plugins can be hooked in. Different plugins are suited for different stages, such
    as filtering unsuitable Nodes or scoring suitable Nodes.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IMPORTANT: Scheduling Policies (Pre-v1.23 Kubernetes)**'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes versions before v1.23 allowed specifying scheduling policies via
    kube-scheduler flags or ConfigMaps. These policies defined how the scheduler selected
    nodes for Pods using predicates (filtering criteria) and priorities (scoring functions).
    As of v1.23, this functionality is replaced by scheduler configuration. This new
    approach allows more flexibility and control over scheduling behavior.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes scheduler configuration file provides several benefits for managing
    Pod placement within your cluster. It provides the flexibility to tailor scheduling
    behavior to your needs. For instance, you can prioritize Pods requiring GPUs to
    land on Nodes with those resources. Additionally, you can develop custom plugins
    to handle unique scheduling requirements not addressed by default plugins. Finally,
    the ability to define multiple profiles allows you to create granular controls
    by assigning different scheduling profiles to different types of Pods.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Before we conclude the chapter, let’s look at the Node restrictions feature
    in Kubernetes scheduling.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Node isolation and restrictions
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes allows you to isolate Pods on specific Nodes using node labels. These
    labels can define properties like security requirements or regulatory compliance.
    This ensures Pods are only scheduled on Nodes that meet these criteria. To prevent
    a compromised node from manipulating labels for its own benefit, the `NodeRestriction`
    admission plugin restricts the `kubelet` from modifying labels with a specific
    prefix (e.g., `node-restriction.kubernetes.io/`). To leverage this functionality,
    you’ll need to enable the `NodeRestriction` plugin and `Node authorizer`. Then,
    you can add labels with the restricted prefix to your Nodes and reference them
    in your Pod’s `nodeSelector` configuration. This ensures your Pods only run on
    pre-defined, isolated environments.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Tuning Kubernetes scheduler performance
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In large Kubernetes clusters, efficient scheduler performance is crucial. Let’s
    explore a key tuning parameter: `percentageOfNodesToScore`.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The `percentageOfNodesToScore` setting determines how many Nodes the scheduler
    considers when searching for a suitable Pod placement. A higher value means the
    scheduler examines more Nodes, potentially finding a better fit but taking longer.
    Conversely, a lower value leads to faster scheduling but might result in suboptimal
    placement. You can configure this value in the kube-scheduler configuration file.
    The valid range is 1% to 100%, with a default calculated based on cluster size
    (50% for 100 nodes, 10% for 5,000 nodes).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'To set `percentageOfNodesToScore` to 50% for a cluster with hundreds of nodes,
    you’d include the following configuration in the scheduler file:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The optimal value depends on your priorities. If fast scheduling is critical,
    a lower value might be acceptable. However, if ensuring the best possible placement
    outweighs speed concerns, a higher value is recommended. Avoid setting it too
    low to prevent the scheduler from overlooking potentially better Nodes.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have finished this chapter, and we have learned about different
    mechanisms and strategies available in Kubernetes to control Pod placement on
    Nodes, including `nodeName`, `nodeSelector`, `nodeAffinity`, taints and tolerations,
    and also other useful advanced scheduler configurations.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has given an overview of advanced techniques for Pod scheduling
    in Kubernetes. First, we recapped the theory behind kube-scheduler implementation
    and explained the process of scheduling Pods. Next, we introduced the concept
    of Node affinity in Pod scheduling. We discussed the basic scheduling methods,
    which use Node names and Node selectors, and based on that, we explained how more
    advanced Node affinity works. We also explained how to use the affinity concept
    to achieve anti-affinity, and what inter-Pod affinity/anti-affinity is. After
    that, we discussed taints for Nodes and tolerations specified by Pods. You learned
    about some different effects of taints, and put that knowledge into practice in
    an advanced use case involving `NoExecute` and `NoSchedule` taints on a Node.
    Lastly, we discussed some advanced scheduling features in Kubernetes such as scheduler
    configurations, Node isolation, and static pods.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to discuss the **autoscaling** of Pods and
    Nodes in Kubernetes. This is a topic that shows how flexibly Kubernetes can run
    workloads in cloud environments.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes Scheduler: [https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/
    )'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Assigning Pods to Nodes: [https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    )'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taints and Tolerations: [https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    )'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scheduler Configuration: [https://kubernetes.io/docs/reference/scheduling/config/](https://kubernetes.io/docs/reference/scheduling/config/
    )'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more information regarding Pod scheduling in Kubernetes, please refer to
    the following PacktPub books:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '*The Complete Kubernetes Guide*, by *Jonathan Baier*, *Gigi Sayfan*, *Jesse
    White* ([https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346](https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346))'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Getting Started with Kubernetes – Third Edition*, by *Jonathan Baier*, *Jesse
    White* ([https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263](https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263))'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kubernetes for Developers*, by *Joseph Heck* ([https://www.packtpub.com/en-in/product/kubernetes-for-developers-9781788830607](https://www.packtpub.com/en-in/product/kubernetes-for-developers-9781788830607))'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can also refer to official documents:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes documentation ([https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)),
    which is always the most up-to-date source of knowledge about Kubernetes in general.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node affinity is covered at [https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/).
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taint and tolerations are covered at [https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/).
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod priorities and preemption (which we have not covered in this chapter) are
    described at [https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/).
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced kube-scheduler configuration using scheduling profiles (which we have
    not covered in this chapter) is described at [https://kubernetes.io/docs/reference/scheduling/config](https://kubernetes.io/docs/reference/scheduling/config).
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code119001106479081656.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
