- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Resisting Component Failure Using HA Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at how to enable Linkerd or Istio service
    mesh add-ons and inject sidecars into a sample application. We also looked at
    the dashboards that allow us to look at telemetry data in order to troubleshoot,
    manage, and improve applications. We then looked at how metrics, distributed traces,
    and access logs can help with overall service mesh observability. We additionally
    looked at some of the most common service mesh use cases today, as well as some
    recommendations for how to choose the correct service mesh. We also covered a
    list of service mesh configuration best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Through the use of dynamic container scheduling, Kubernetes offers higher reliability
    and resiliency for distributed applications. But how can you ensure that Kubernetes
    itself remains operational when a component, or even an entire data center site,
    fails? In this chapter, we will look into our next use case on how to configure
    Kubernetes for a **high-availability (HA)** cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '**HA** keeps applications up and running even if the site fails partially or
    completely. The basic goal of HA is to eliminate potential points of failure.
    It can be achieved at many levels of infrastructure and within different cluster
    components. However, the amount of availability that fits a particular situation
    is determined by a number of factors, including your business needs, service-level
    agreements with your customers, and resource availability.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes aims to provide HA for both applications and infrastructure. Each
    of the control plane (master) components can be configured for multi-node replication
    (multi-master setup) to improve availability. However, it’s important to remember
    that HA and a multi-master setup are not synonymous. Even if you have three or
    more control plane nodes and only one NGINX instance front-load balancing to those
    masters, you have a multi-master cluster setup, but not an HA setup, because NGINX
    could still go down at any time and cause failures.
  prefs: []
  type: TYPE_NORMAL
- en: Control plane nodes are vital because they operate the services that control,
    monitor, and maintain the Kubernetes cluster’s state. The API server, cluster
    state storage, the scheduler, and the controller manager are all part of the control
    plane. If only one control plane node fails in a cluster, the cluster’s operation
    and stability may be seriously impaired. HA clusters address this by running numerous
    control plane nodes at the same time, and while this doesn’t completely eliminate
    risk, it reduces it greatly.
  prefs: []
  type: TYPE_NORMAL
- en: MicroK8s’ HA option has been simplified and enabled by default. This means that
    a cluster can survive a node failure and continue to serve workloads without interruption.
    HA is a critical feature for enterprises wishing to deploy containers and pods
    that can provide the level of stability required when working at scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'Canonical’s lightweight Dqlite SQL database is used to enable the HA clustering
    functionality. By embedding the database into Kubernetes, Dqlite reduces the cluster’s
    memory footprint and eliminates process overhead. This is significant for IoT
    and Edge applications. The deployment of resilient Kubernetes clusters at the
    edge is simplified when Dqlite is used as the Kubernetes datastore. Edge applications
    can now achieve exceptional reliability at a low cost on x86 or ARM commodity
    appliances such as clusters of Intel NUCs or Raspberry Pi boards. In this chapter,
    we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of HA topologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up an HA Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes HA best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of HA topologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll look at the two most common HA topologies for enabling
    HA clusters. The Kubernetes cluster’s control plane is mostly stateless. The cluster
    datastore, which operates as the one source of truth for the whole cluster, is
    the only stateful component of the control plane. Internal and external consumers
    can access and alter the state through the API server, which serves as a gateway
    to the cluster datastore. MicroK8s uses Dqlite, a distributed and highly accessible
    variant of SQLite, as the key-value database to preserve the cluster’s state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before looking at the HA topologies, let us look at potential failure scenarios
    that could hamper cluster operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loss of control plane (master) node**: Loss of the master node or its services
    will have a major impact. The cluster will be unable to respond to API commands
    or the deployment of nodes. Each service in the master node, as well as the storage
    layer, is crucial and must be designed for HA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss of cluster datastore**: Whether the cluster datastore is run on the
    master node or set up separately, losing cluster data is disastrous because it
    contains all cluster information. To avoid this, the cluster datastore must be
    configured in an HA cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Worker node(s) failure**: In most circumstances, Kubernetes will be able
    to identify and failover pods automatically. The end users of the application
    may not notice any difference, depending on how the services are load-balanced.
    If any pods on a node become unresponsive, kubelet will detect this and notify
    the master to start another pod.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network failures**: Network outages and partitions can cause the master and
    worker nodes in a Kubernetes cluster to become unreachable. In some circumstances,
    they will be classified as node failures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve seen some of the probable failure situations, let’s look at how
    they can be mitigated using HA topologies that can withstand the failure of one
    or more master nodes while running Kubernetes production workloads.
  prefs: []
  type: TYPE_NORMAL
- en: The topology of an HA Kubernetes cluster can be configured in two ways, depending
    on how the cluster datastore is configured. The first topology is based on a stacked
    cluster design, in which each node hosts a Dqlite instance, as well as the control
    plane. The `kube-apiserver` instance, the `kube-scheduler` instance, and the `kube-controller-manager`
    instance are running on each control plane node. A load balancer exposes the `kube-apiserver`
    instance to worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Each control plane node produces a local Dqlite member, which exclusively communicates
    with this node’s `kube-apiserver` instance. The local `kube-controller-manager`
    instance and the `kube-scheduler` instance are the same.
  prefs: []
  type: TYPE_NORMAL
- en: The control planes and local Dqlite members are linked on the same node in this
    topology. It is easier to set up and administer for replication. However, a stacked
    cluster is vulnerable to failed coupling. When one node fails, the local Dqlite
    members and a control plane instance are lost, putting redundancy at risk. This
    threat can be reduced by adding more control plane nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence for an HA Kubernetes cluster, this design necessitates at least three
    stacked control plane nodes, as depicted in *Figure 13.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – A stacked control plane topology ](img/Figure_13.01_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – A stacked control plane topology
  prefs: []
  type: TYPE_NORMAL
- en: The second topology makes use of an external Dqlite cluster that is installed
    and controlled on a separate set of hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each control plane node in this architecture runs a `kube-apiserver` instance,
    a `kube-scheduler` instance, and a `kube-controller-manager` instance, with each
    Dqlite host communicating with the `kube-apiserver` instance of each control plane
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – The topology of an external cluster datastore ](img/Figure_13.02_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.2 – The topology of an external cluster datastore
  prefs: []
  type: TYPE_NORMAL
- en: The control plane and local Dqlite member are decoupled in this topology. As
    a result, it provides an HA configuration in which losing a control plane instance
    or Dqlite member has less of an impact and does not influence cluster redundancy
    as much as the stacked HA architecture.
  prefs: []
  type: TYPE_NORMAL
- en: This design, however, requires twice as many hosts as the stacked HA topology.
    An HA cluster with this topology requires a minimum of three hosts for control
    plane nodes and three hosts for Dqlite nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to walk through the steps involved in setting
    up an HA cluster using the stacked cluster HA topology. All that is necessary
    for HA MicroK8s is three or more nodes in the cluster, after which Dqlite becomes
    highly available automatically. If the cluster has more than three nodes, additional
    nodes will be designated as standby candidates for the datastore and promoted
    automatically if one of the data store’s nodes fails. The automatic promotion
    of standby nodes into the Dqlite voting cluster makes HA MicroK8s self-sufficient
    and ensures that a quorum is maintained even if no administrative action is performed.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an HA Kubernetes cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to configure and implement an HA MicroK8s Kubernetes cluster utilizing
    the stacked cluster HA topology that we discussed before. We’ll use the three
    nodes to install and configure MicroK8s on each of the nodes and simulate node
    failure to see whether the cluster is resisting component failures and functioning
    as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, a control plane is run by all the nodes in the HA cluster. A portion
    (at least three) of the cluster nodes keeps a copy of the Kubernetes cluster datastore
    (the Dqlite database). A voting procedure is used to pick a leader for database
    maintenance. Aside from the voting nodes, there are non-voting nodes that store
    a copy of the database discreetly. These nodes are ready to replace a leaving
    voter. Finally, some nodes do not vote or duplicate the database. These are known
    as spare nodes. To summarize, the three node roles are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Voters**: Replicating the database, participating in leader election'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standby**: Replicating the database, *not* participating in leader election'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spare**: *Not* replicating the database, *not* participating in leader election'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The administrator doesn’t need to monitor how cluster formation, database syncing,
    or voter and leader elections are all done since it’s transparent and taken care
    of. *Figure 13.3* depicts our Raspberry Pi cluster setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – A fully functional HA cluster setup ](img/Figure_13.03_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 – A fully functional HA cluster setup
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what we want to do, let’s look at the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you begin, the following are the prerequisites for building a Raspberry
    Pi Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: A microSD card (4 GB minimum, 8 GB recommended)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A computer with a microSD card drive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Raspberry Pi 2, 3, or 4 (with three nodes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A micro-USB power cable (a USB-C cable for the Pi 4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Wi-Fi network or an Ethernet cable with an internet connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A monitor with an HDMI interface (optional)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An HDMI cable for the Pi 2 and 3 and a micro-HDMI cable for the Pi 4 (optional)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A USB keyboard (optional)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve established what the requirements are for setting up an HA MicroK8s
    Kubernetes cluster, we’ll move on to the step-by-step instructions on how to complete
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Creating the MicroK8s Raspberry Pi cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Please follow the steps that we covered in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070),
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters*,
    to create the MicroK8s Raspberry Pi cluster. Here’s a quick refresher:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install an OS image to the SD card:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure Wi-Fi access settings.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure remote access settings.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure control group settings.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure a hostname.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Install and configure MicroK8s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add additional control plane nodes and worker nodes to the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the MicroK8s 1.23 release, there is now an option to add worker-only
    nodes. This type of node does not execute the control plane and does not contribute
    to the cluster’s HA. They, on the other hand, utilize fewer resources and are
    hence appropriate for low-end devices. Worker-only nodes are also appropriate
    in systems where the nodes executing the Kubernetes workloads are unreliable or
    cannot be trusted to hold the control plane.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add a worker-only node to the cluster, use the `--worker` flag when running
    the `microk8s join` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '**microk8s join 192.168.1.8:25000/92b2db237428470dc4fcfc4ebbd9dc81/ 2c0cb3284b05
    --worker**'
  prefs: []
  type: TYPE_NORMAL
- en: A Traefik load balancer runs on a worker node, allowing communication between
    local services (kubelet and kube-proxy) and API servers operating on several control
    plane nodes. When adding a worker node, MicroK8s tries to discover all API server
    endpoints in the cluster and correctly set up the new node. We will not use worker-only
    nodes in this section, but worker nodes that also host the control plane instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll repeat the methods set out in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070),
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters*,
    for our present setup, as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 13.1 – A Raspberry Pi cluster setup ](img/011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 13.1 – A Raspberry Pi cluster setup
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’re clear on our goals, we’ll take installing and configuring MicroK8s
    on each Raspberry Pi board step by step and then combine multiple deployments
    to build a fully functional cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring MicroK8s
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SSH into your control plane node and install the MicroK8s snap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output confirms that the MicroK8s snap has
    been installed successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4 – MicroK8s installation ](img/Figure_13.04_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.4 – MicroK8s installation
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command execution output confirms that MicroK8s is running successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5 – Inspect your MicroK8s cluster ](img/Figure_13.05_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5 – Inspect your MicroK8s cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'If the installation is successful, then you should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.6 – Verify whether the node is in a ready state ](img/Figure_13.06_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6 – Verify whether the node is in a ready state
  prefs: []
  type: TYPE_NORMAL
- en: Repeat the MicroK8s installation process on the other nodes as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to add a control plane node and worker node to the cluster.
    Open PuTTY shell to control plane node and run the following command to generate
    the connection string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output validates that the command was successfully
    executed and provides instructions for the connection string:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.7 – Generate connection string for adding nodes ](img/Figure_13.07_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.7 – Generate connection string for adding nodes
  prefs: []
  type: TYPE_NORMAL
- en: As indicated by the preceding command execution output, the connection string
    is generated in the form of `<control plane_ip>:<port>/<token>`.
  prefs: []
  type: TYPE_NORMAL
- en: Adding additional control plane nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now have the connection string to join with the control plane node. Open
    the PuTTY shell to the `controlplane1` node and run the `join` command to add
    it to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The command was successfully executed, and the node has joined the cluster,
    as shown in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.8 – Adding an additional control plane node to the cluster ](img/Figure_13.08_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.8 – Adding an additional control plane node to the cluster
  prefs: []
  type: TYPE_NORMAL
- en: Our next step is to add a worker node to the cluster now that we have added
    an additional control plane node so that we can simulate node failure to see whether
    the cluster is able to resist component failures and function as we expect.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a worker node
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now have the connection string to join with the control plane node. Open
    the PuTTY shell to the worker node and run the `join` command to add it to the
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The command was successfully executed, and the node has joined the cluster,
    as shown in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.9 – Adding a worker node to the cluster ](img/Figure_13.09_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.9 – Adding a worker node to the cluster
  prefs: []
  type: TYPE_NORMAL
- en: As indicated by the preceding command execution output, you should be able to
    see the new node in a few seconds on the control plane.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to verify whether the new node has been added to
    the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output shows that `controlplane`, `controlplane1`,
    and `worker2` are part of the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.10 – The cluster is ready, and control planes and worker2 are part
    of the cluster ](img/Figure_13.10_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.10 – The cluster is ready, and control planes and worker2 are part
    of the cluster
  prefs: []
  type: TYPE_NORMAL
- en: A fully functional multi-node Kubernetes cluster would look like that shown
    in *Figure 13.3*. To summarize, we have installed MicroK8s on the Raspberry Pi
    boards and joined multiple deployments to form the cluster. We’ve also added control
    plane nodes and worker nodes to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a fully functional cluster, we will move on to the next step
    of examining the HA setup.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – Examining the HA setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we have more than one node running the control plane, MicroK8s’ HA would
    be achieved automatically. An HA Kubernetes cluster requires three conditions
    to be satisfied:'
  prefs: []
  type: TYPE_NORMAL
- en: At any given time, there must be more than one node available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The control plane must run on more than one node so that the cluster does not
    become unusable, even if a single node fails.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cluster state must be stored in a highly accessible datastore.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can check the current state of the HA cluster using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output confirms that HA has been achieved and
    also displays the datastore master nodes. Standby nodes are set to `none` since
    we have only three nodes; additional nodes will be designated as standby candidates
    for the datastore and will be promoted automatically if one of the datastore’s
    nodes fails:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.11 – Inspecting the MicroK8s HA cluster ](img/Figure_13.11_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.11 – Inspecting the MicroK8s HA cluster
  prefs: []
  type: TYPE_NORMAL
- en: Congrats! You now have a secure, distributed, highly available Kubernetes cluster
    that’s ready for a production-grade MicroK8s cluster environment. In the next
    section, we are going to deploy a sample application on the MicroK8s cluster that
    we just created.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Deploying a sample containerized application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will deploy the NGINX deployment from the Kubernetes `examples`
    repository in our multi-node HA MicroK8s cluster setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command will deploy the sample application deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output indicates that there is no error in
    the deployment and in the next steps, we can verify this using the `get deployments`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.12 – A sample application deployment ](img/Figure_13.12_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.12 – A sample application deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command execution output displays the information about the deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.13 – The sample application deployments are in ready state ](img/Figure_13.13_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.13 – The sample application deployments are in ready state
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us also check where the pods are running using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output indicates that pods are equally distributed
    between the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.14 – The pod distribution across the nodes ](img/Figure_13.14_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.14 – The pod distribution across the nodes
  prefs: []
  type: TYPE_NORMAL
- en: Great! We have just deployed our sample application on the Raspberry multi-node
    HA cluster. To summarize, we built a Kubernetes Raspberry Pi cluster and used
    it to deploy a sample application. We’ll perform some of the tests to check whether
    our cluster is resistant to failures in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In the case that add-ons are enabled on a multi-node HA cluster, if the client
    binaries are downloaded and installed for an add-on, those binaries will only
    be available on the specific node from which the add-on was enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Simulating control plane node failure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To simulate node failure, we will use the `cordon` command to mark the node
    as `unschedulable`. If the node is unschedulable, the Kubernetes controller will
    not schedule new pods on this node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use `cordon` on the `controlplane1` node so that we can simulate control
    plane failure. Use the following command to cordon the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output shows that `controlplane1` has been
    cordoned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.15 – Cordoning the controlplane1 node ](img/Figure_13.15_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.15 – Cordoning the controlplane1 node
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though the `controplane1` node is cordoned, existing pods will still run.
    We can now use the `drain` command to delete all the pods. Use the following command
    to drain the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Use the `--ignore-daemonsets` flag to drain the nodes that contain the pods
    that are managed by DaemonSet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command execution output shows that the pods running on `controlplane1`
    have been deleted successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.16 – Draining the controlplane1 node ](img/Figure_13.16_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.16 – Draining the controlplane1 node
  prefs: []
  type: TYPE_NORMAL
- en: Because the control plane is run by all nodes in the HA cluster, if one of the
    control plane nodes (`controlplane1`, for example) fails, cluster decisions can
    switch over to another control plane node and continue working without much disruption.
  prefs: []
  type: TYPE_NORMAL
- en: As part of the new control plane decisions, the Kubernetes controller will now
    recreate a new pod and schedule it in a different node as soon as the pod is deleted.
    It cannot be placed on the same node because the scheduling is disabled (since
    we have cordoned the `controlplane1` node).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us inspect where the pods are running using the `kubectl get pods` command.
    The following command execution output shows that the new pod has been rescheduled
    to the `controlplane` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.17 – Pod redistribution across the nodes ](img/Figure_13.17_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.17 – Pod redistribution across the nodes
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command execution output shows that the deployments have also
    been restored, despite the failure of one of the control plane nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.18 – The deployments are in a ready state ](img/Figure_13.18_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.18 – The deployments are in a ready state
  prefs: []
  type: TYPE_NORMAL
- en: 'Almost all HA cluster administration is invisible to the administrator and
    requires minimum configuration. Only the administrator has the ability to add
    and remove nodes. The following parameters should be considered to ensure the
    cluster’s health:'
  prefs: []
  type: TYPE_NORMAL
- en: If the leader node is *removed*, such as by crashing and never returning, the
    cluster may take up to 5 seconds to elect a new leader.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can take up to 30 seconds to convert a non-voter into a voter. This promotion
    occurs when a new node joins the cluster or when a voter fails.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To summarize, we used the stacked cluster HA topology to configure and implement
    a highly available MicroK8s Kubernetes cluster. We used the three nodes to install
    and configure MicroK8s on each one, as well as simulating node failure to see
    whether the cluster can withstand component failures and continue to function
    as expected. In the next section, we will touch upon some of the best practices
    for implementing Kubernetes for a production-grade Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes HA best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As people become more acquainted with Kubernetes, there are trends toward more
    advanced use of the platform, such as users deploying Kubernetes in an HA architecture
    to ensure full production uptime. According to the recent *Kubernetes and cloud
    native operations report, 2022* ([https://juju.is/cloud-native-kubernetes-usage-report-2022](https://juju.is/cloud-native-kubernetes-usage-report-2022)),
    many respondents appear to utilize Kubernetes’ HA architecture for highly secure,
    data-sensitive applications.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will go over some of the best practices for deploying HA
    apps in Kubernetes. These guidelines build upon what we have seen in [*Chapter
    5*](B18115_05.xhtml#_idTextAnchor070), *Creating and Implementing Updates on Multi-Node
    Raspberry Pi Kubernetes Clusters*.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you may be aware, deploying a basic app setup in Kubernetes is a piece of
    cake. Trying to make your application available and fault-tolerant, on the other
    hand, implies a slew of challenges and problems. In general, implementing HA in
    any capacity requires the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Determining your application’s intended level of availability**: The allowable
    level of downtime varies by application and business objectives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A redundant and reliable control plane for your application**: The control
    plane manages the cluster state and contributes to the availability of your applications
    to users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A redundant and reliable data plane for your application**: This entails
    duplicating the data across all cluster nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are a lot of considerations and decisions to make while deploying Kubernetes’
    HA that might have an impact on the apps and how they operate and consume storage
    resources. Here, we will look at some of the considerations to make:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use of replicas**: Use replicas instead of pods to deploy HA apps. Using
    replicas ensures that your application is operating on a consistent set of pods
    at all times. For the application to be declared minimally accessible, it must
    have at least two replicas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ready` state to 75% of their pre-update level. As a result, during the update,
    an application’s compute capacity may drop to 75% of its normal level, resulting
    in a partial failure (degraded application performance). The `RollingUpdate.maxUnavailable`
    parameter lets you choose the maximum percentage of pods that can go down during
    an upgrade. As a result, either ensure that your application operates properly,
    even if 25% of your pods are unavailable, or reduce the `maxUnavailable` option.
    Based on the application needs, other deployment strategies, such as blue-green
    and canary, among others, can also be evaluated for a much better alternative
    to the default strategy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Right sizing of the nodes**: The maximum amount of RAM that can be allocated
    to pods is determined by the size of the nodes. For production clusters, the size
    of the nodes is large enough (2.5 GB or more) that they can absorb the workload
    of any crashed nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node pools for HA**: Node pools with production workloads should contain
    at least three nodes to provide HA. This allows the cluster to distribute and
    schedule work on other nodes if one becomes unavailable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`request` and `limit` objects in your application spec for all deployments:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Requests**: Specifies how much of a resource (such as CPU and memory resources)
    a pod may require before it is scheduled on a node. The pod will not be scheduled
    if the node lacks the required resources. This keeps pods from being scheduled
    on nodes that are already overburdened.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Limits**: Specifies how many resources (such as CPU and RAM) a pod is permitted
    to use on a node. This stops pods from potentially slowing down the operation
    of other pods.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Set pod disruption budgets**: To avoid interruptions to production, such
    as during cluster upgrades, you can configure a pod disruption budget, which restricts
    the number of replicated pods that can be down at the same time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensure a cluster is upgraded**: Ensure to take advantage of the latest features,
    security patches, and stability improvements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoid single points of failure**: Kubernetes enhances dependability by providing
    repeating components and ensuring that application containers can be scheduled
    across various nodes. For HA, use anti-affinity or node selection to help disperse
    your applications across the Kubernetes cluster. Based on labels, node selection
    allows you to specify which nodes in your cluster are eligible to run your application.
    Labels often describe node attributes such as bandwidth or specialized resources
    such as GPUs. For example, to properly commit mutations to data, Apache ZooKeeper
    requires a quorum of servers. Two servers in a three-server ensemble must be healthy
    for writes to succeed. As a result, a resilient deployment must make sure that
    servers are distributed across failure domains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use liveness and readiness probes**: By default, Kubernetes will transfer
    traffic to application containers instantaneously. You may improve the robustness
    of your application by configuring health checks to notify Kubernetes when your
    application pods are ready to receive traffic or have become unresponsive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`startupProbe` or `readinessProbe`, you can use `initContainers` to check for
    external dependencies. Changes to the application code are not required for `initContainers`.
    It is not necessary to embed additional tools in order to utilize them to examine
    external dependencies in application containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use plenty of descriptive labels**: Labels are extremely powerful since they
    are arbitrary key-value pairs and enable you to logically organize all your Kubernetes
    workloads in your clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use sidecars for proxies and watchers**: Sometimes, a set of processes is
    required to communicate with another process. However, you do not want all of
    these to operate in a single container but rather in a pod. This is also the case
    when you are running a proxy or a watcher on which your processes rely. For example,
    with a database on which your processes rely, the credentials would not be hardcoded
    onto each container. Instead, you can deploy the credentials as a proxy inside
    a sidecar that handles the connection securely.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automate your CI/CD pipeline and avoid manual Kubernetes deployments**: Because
    there may be numerous deployments per day, this strategy saves the team considerable
    time by eliminating manual error-prone tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Prod`, `Dev`, and `Test` namespaces in the same cluster, and you can also
    use namespaces to limit the number of resources so that one defective process
    does not consume all of the cluster resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring the control plane**: This helps in the identification of issues
    or threats within the cluster and increases latency. It is also advised to employ
    automated monitoring tools rather than managing alerts manually.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To recap, we’ve gone through some of the best practices to optimize your Kubernetes
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to set up an HA MicroK8s Kubernetes cluster
    using the stacked cluster HA topology. We utilized the three nodes to install
    and configure MicroK8s on each of them, as well as simulating node failure to
    see whether the cluster could tolerate component failures and still continue to
    function normally.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed some of the best practices for implementing Kubernetes applications
    on your production-ready cluster. We also covered the fact that MicroK8s’ HA option
    has been simplified and enabled by default.
  prefs: []
  type: TYPE_NORMAL
- en: HA is a vital feature for organizations looking to deploy containers and pods
    that can deliver the kind of reliability required at scale. We also recognized
    the value of Canonical's lightweight Dqlite SQL database, which is used to provide
    HA clustering. By embedding the database into Kubernetes, Dqlite reduces the cluster’s
    memory footprint and eliminates process overhead. For IoT or Edge applications,
    this is critical.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll look at how to use Kata Containers, a secure container
    runtime, to provide stronger workload isolation by leveraging hardware virtualization
    technology.
  prefs: []
  type: TYPE_NORMAL
