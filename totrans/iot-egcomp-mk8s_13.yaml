- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Resisting Component Failure Using HA Clusters
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用高可用性集群抵御组件故障
- en: In the previous chapter, we looked at how to enable Linkerd or Istio service
    mesh add-ons and inject sidecars into a sample application. We also looked at
    the dashboards that allow us to look at telemetry data in order to troubleshoot,
    manage, and improve applications. We then looked at how metrics, distributed traces,
    and access logs can help with overall service mesh observability. We additionally
    looked at some of the most common service mesh use cases today, as well as some
    recommendations for how to choose the correct service mesh. We also covered a
    list of service mesh configuration best practices.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了如何启用Linkerd或Istio服务网格附加组件并将边车注入到示例应用程序中。我们还查看了允许我们查看遥测数据的仪表板，以便进行故障排除、管理和改进应用程序。然后，我们了解了指标、分布式追踪和访问日志如何帮助整体服务网格的可观察性。我们还查看了当前一些最常见的服务网格用例，并提供了一些关于如何选择正确服务网格的建议。我们还涵盖了服务网格配置的最佳实践清单。
- en: Through the use of dynamic container scheduling, Kubernetes offers higher reliability
    and resiliency for distributed applications. But how can you ensure that Kubernetes
    itself remains operational when a component, or even an entire data center site,
    fails? In this chapter, we will look into our next use case on how to configure
    Kubernetes for a **high-availability (HA)** cluster.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通过动态容器调度，Kubernetes为分布式应用程序提供了更高的可靠性和韧性。但如何确保当组件，甚至整个数据中心站点故障时，Kubernetes本身仍能保持运行呢？在本章中，我们将探讨如何为**高可用性（HA）**集群配置Kubernetes的下一个用例。
- en: '**HA** keeps applications up and running even if the site fails partially or
    completely. The basic goal of HA is to eliminate potential points of failure.
    It can be achieved at many levels of infrastructure and within different cluster
    components. However, the amount of availability that fits a particular situation
    is determined by a number of factors, including your business needs, service-level
    agreements with your customers, and resource availability.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**高可用性（HA）**确保即使站点部分或完全故障，应用程序仍能持续运行。高可用性的基本目标是消除潜在的故障点。它可以在许多基础设施层级和不同的集群组件中实现。然而，适合特定情况的可用性水平由多个因素决定，包括你的业务需求、与客户的服务级别协议以及资源可用性。'
- en: Kubernetes aims to provide HA for both applications and infrastructure. Each
    of the control plane (master) components can be configured for multi-node replication
    (multi-master setup) to improve availability. However, it’s important to remember
    that HA and a multi-master setup are not synonymous. Even if you have three or
    more control plane nodes and only one NGINX instance front-load balancing to those
    masters, you have a multi-master cluster setup, but not an HA setup, because NGINX
    could still go down at any time and cause failures.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes旨在为应用程序和基础设施提供高可用性。每个控制平面（主节点）组件都可以配置为多节点复制（多主节点设置），以提高可用性。然而，重要的是要记住，高可用性和多主节点设置并不等同。即使你有三个或更多的控制平面节点，并且只有一个NGINX实例在这些主节点前进行负载均衡，你也有一个多主节点集群设置，但不是高可用性设置，因为NGINX仍然可能随时发生故障并导致故障。
- en: Control plane nodes are vital because they operate the services that control,
    monitor, and maintain the Kubernetes cluster’s state. The API server, cluster
    state storage, the scheduler, and the controller manager are all part of the control
    plane. If only one control plane node fails in a cluster, the cluster’s operation
    and stability may be seriously impaired. HA clusters address this by running numerous
    control plane nodes at the same time, and while this doesn’t completely eliminate
    risk, it reduces it greatly.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面节点至关重要，因为它们运行控制、监视和维护Kubernetes集群状态的服务。API服务器、集群状态存储、调度器和控制器管理器都是控制平面的一部分。如果集群中的某个控制平面节点发生故障，集群的操作和稳定性可能会受到严重影响。高可用性集群通过同时运行多个控制平面节点来解决这个问题，虽然这不能完全消除风险，但可以大大减少风险。
- en: MicroK8s’ HA option has been simplified and enabled by default. This means that
    a cluster can survive a node failure and continue to serve workloads without interruption.
    HA is a critical feature for enterprises wishing to deploy containers and pods
    that can provide the level of stability required when working at scale.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: MicroK8s的高可用性选项已经简化并默认启用。这意味着集群可以在节点故障的情况下继续为工作负载提供服务，不会中断。高可用性是企业部署容器和Pod时所需的关键特性，能够提供在大规模工作时所需的稳定性。
- en: 'Canonical’s lightweight Dqlite SQL database is used to enable the HA clustering
    functionality. By embedding the database into Kubernetes, Dqlite reduces the cluster’s
    memory footprint and eliminates process overhead. This is significant for IoT
    and Edge applications. The deployment of resilient Kubernetes clusters at the
    edge is simplified when Dqlite is used as the Kubernetes datastore. Edge applications
    can now achieve exceptional reliability at a low cost on x86 or ARM commodity
    appliances such as clusters of Intel NUCs or Raspberry Pi boards. In this chapter,
    we’re going to cover the following main topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Canonical的轻量级Dqlite SQL数据库用于实现HA集群功能。通过将数据库嵌入Kubernetes，Dqlite减少了集群的内存占用并消除了进程开销。这对于物联网和边缘应用程序来说至关重要。当Dqlite作为Kubernetes数据存储使用时，边缘的弹性Kubernetes集群部署得以简化。边缘应用程序现在可以在x86或ARM的普通设备上，如Intel
    NUC或树莓派板等集群，低成本地实现卓越的可靠性。本章将涵盖以下主要内容：
- en: An overview of HA topologies
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HA拓扑概述
- en: Setting up an HA Kubernetes cluster
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置HA Kubernetes集群
- en: Kubernetes HA best practices
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes HA最佳实践
- en: An overview of HA topologies
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HA拓扑概述
- en: In this section, we’ll look at the two most common HA topologies for enabling
    HA clusters. The Kubernetes cluster’s control plane is mostly stateless. The cluster
    datastore, which operates as the one source of truth for the whole cluster, is
    the only stateful component of the control plane. Internal and external consumers
    can access and alter the state through the API server, which serves as a gateway
    to the cluster datastore. MicroK8s uses Dqlite, a distributed and highly accessible
    variant of SQLite, as the key-value database to preserve the cluster’s state.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍两种最常见的HA拓扑，以启用HA集群。Kubernetes集群的控制平面大多是无状态的。作为整个集群唯一真实数据来源的集群数据存储是控制平面中唯一有状态的组件。内外部用户可以通过API服务器访问和修改状态，API服务器作为集群数据存储的网关。MicroK8s使用Dqlite，这是一种分布式且高可访问性的SQLite变体，作为关键值数据库来保持集群的状态。
- en: 'Before looking at the HA topologies, let us look at potential failure scenarios
    that could hamper cluster operations:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看HA拓扑之前，让我们先来看一些可能会妨碍集群操作的潜在故障场景：
- en: '**Loss of control plane (master) node**: Loss of the master node or its services
    will have a major impact. The cluster will be unable to respond to API commands
    or the deployment of nodes. Each service in the master node, as well as the storage
    layer, is crucial and must be designed for HA.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制平面（主节点）丧失**：主节点或其服务的丧失将产生重大影响。集群将无法响应API命令或节点的部署。主节点中的每个服务，以及存储层，都是至关重要的，必须设计为HA。'
- en: '**Loss of cluster datastore**: Whether the cluster datastore is run on the
    master node or set up separately, losing cluster data is disastrous because it
    contains all cluster information. To avoid this, the cluster datastore must be
    configured in an HA cluster.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群数据存储丢失**：无论集群数据存储是运行在主节点上，还是单独设置，丢失集群数据将是灾难性的，因为它包含所有集群信息。为了避免这种情况，集群数据存储必须配置在HA集群中。'
- en: '**Worker node(s) failure**: In most circumstances, Kubernetes will be able
    to identify and failover pods automatically. The end users of the application
    may not notice any difference, depending on how the services are load-balanced.
    If any pods on a node become unresponsive, kubelet will detect this and notify
    the master to start another pod.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作节点故障**：在大多数情况下，Kubernetes将能够自动识别并故障转移Pods。应用程序的最终用户可能不会察觉到任何差异，具体取决于服务的负载均衡方式。如果某个节点上的Pods变得无响应，kubelet将检测到这一点并通知主节点启动另一个Pod。'
- en: '**Network failures**: Network outages and partitions can cause the master and
    worker nodes in a Kubernetes cluster to become unreachable. In some circumstances,
    they will be classified as node failures.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络故障**：网络中断和分区可能导致Kubernetes集群中的主节点和工作节点变得无法访问。在某些情况下，它们将被归类为节点故障。'
- en: Now that we’ve seen some of the probable failure situations, let’s look at how
    they can be mitigated using HA topologies that can withstand the failure of one
    or more master nodes while running Kubernetes production workloads.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了几个可能的故障情况，接下来让我们看看如何通过HA拓扑来缓解这些问题，这些拓扑可以在运行Kubernetes生产工作负载时，承受一个或多个主节点的故障。
- en: The topology of an HA Kubernetes cluster can be configured in two ways, depending
    on how the cluster datastore is configured. The first topology is based on a stacked
    cluster design, in which each node hosts a Dqlite instance, as well as the control
    plane. The `kube-apiserver` instance, the `kube-scheduler` instance, and the `kube-controller-manager`
    instance are running on each control plane node. A load balancer exposes the `kube-apiserver`
    instance to worker nodes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用 Kubernetes 集群的拓扑可以通过两种方式进行配置，具体取决于集群数据存储的配置方式。第一种拓扑基于堆叠式集群设计，每个节点承载一个 Dqlite
    实例以及控制平面。在每个控制平面节点上运行 `kube-apiserver` 实例、`kube-scheduler` 实例和 `kube-controller-manager`
    实例。负载均衡器将 `kube-apiserver` 实例暴露给工作节点。
- en: Each control plane node produces a local Dqlite member, which exclusively communicates
    with this node’s `kube-apiserver` instance. The local `kube-controller-manager`
    instance and the `kube-scheduler` instance are the same.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 每个控制平面节点生成一个本地 Dqlite 成员，该成员仅与该节点的 `kube-apiserver` 实例进行通信。本地的 `kube-controller-manager`
    实例和 `kube-scheduler` 实例相同。
- en: The control planes and local Dqlite members are linked on the same node in this
    topology. It is easier to set up and administer for replication. However, a stacked
    cluster is vulnerable to failed coupling. When one node fails, the local Dqlite
    members and a control plane instance are lost, putting redundancy at risk. This
    threat can be reduced by adding more control plane nodes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种拓扑结构中，控制平面和本地 Dqlite 成员在同一节点上相连。该设计更容易设置和管理用于复制。然而，堆叠式集群容易受到失效耦合的影响。当一个节点发生故障时，本地
    Dqlite 成员和控制平面实例会丢失，从而使冗余性面临风险。通过增加更多的控制平面节点，可以降低这种威胁。
- en: 'Hence for an HA Kubernetes cluster, this design necessitates at least three
    stacked control plane nodes, as depicted in *Figure 13.1*:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于高可用 Kubernetes 集群，这种设计至少需要三个堆叠式控制平面节点，如*图 13.1*所示：
- en: '![Figure 13.1 – A stacked control plane topology ](img/Figure_13.01_B18115.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.1 – 堆叠式控制平面拓扑](img/Figure_13.01_B18115.jpg)'
- en: Figure 13.1 – A stacked control plane topology
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 – 堆叠式控制平面拓扑
- en: The second topology makes use of an external Dqlite cluster that is installed
    and controlled on a separate set of hosts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种拓扑利用了安装并由一组独立主机控制的外部 Dqlite 集群。
- en: 'Each control plane node in this architecture runs a `kube-apiserver` instance,
    a `kube-scheduler` instance, and a `kube-controller-manager` instance, with each
    Dqlite host communicating with the `kube-apiserver` instance of each control plane
    node:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构中，每个控制平面节点运行一个 `kube-apiserver` 实例、一个 `kube-scheduler` 实例和一个 `kube-controller-manager`
    实例，每个 Dqlite 主机与每个控制平面节点的 `kube-apiserver` 实例进行通信：
- en: '![Figure 13.2 – The topology of an external cluster datastore ](img/Figure_13.02_B18115.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.2 – 外部集群数据存储的拓扑结构](img/Figure_13.02_B18115.jpg)'
- en: Figure 13.2 – The topology of an external cluster datastore
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 – 外部集群数据存储的拓扑结构
- en: The control plane and local Dqlite member are decoupled in this topology. As
    a result, it provides an HA configuration in which losing a control plane instance
    or Dqlite member has less of an impact and does not influence cluster redundancy
    as much as the stacked HA architecture.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种拓扑结构中，控制平面和本地 Dqlite 成员是解耦的。因此，它提供了一种高可用配置，其中失去一个控制平面实例或 Dqlite 成员的影响较小，并且不像堆叠式高可用架构那样影响集群的冗余性。
- en: This design, however, requires twice as many hosts as the stacked HA topology.
    An HA cluster with this topology requires a minimum of three hosts for control
    plane nodes and three hosts for Dqlite nodes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种设计需要比堆叠式高可用拓扑多出两倍的主机数量。采用这种拓扑的高可用集群至少需要三台主机作为控制平面节点，以及三台主机作为 Dqlite 节点。
- en: In the next section, we are going to walk through the steps involved in setting
    up an HA cluster using the stacked cluster HA topology. All that is necessary
    for HA MicroK8s is three or more nodes in the cluster, after which Dqlite becomes
    highly available automatically. If the cluster has more than three nodes, additional
    nodes will be designated as standby candidates for the datastore and promoted
    automatically if one of the data store’s nodes fails. The automatic promotion
    of standby nodes into the Dqlite voting cluster makes HA MicroK8s self-sufficient
    and ensures that a quorum is maintained even if no administrative action is performed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍使用堆叠式集群高可用拓扑设置高可用集群的步骤。高可用 MicroK8s 所需的仅仅是集群中的三个或更多节点，之后 Dqlite 会自动变为高可用。如果集群有超过三个节点，额外的节点将被指定为数据存储的备用候选节点，并在数据存储的某个节点发生故障时自动提升。备用节点自动提升为
    Dqlite 投票集群，使得高可用 MicroK8s 自给自足，并确保即使没有执行任何管理操作，也能保持法定人数。
- en: Setting up an HA Kubernetes cluster
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 HA Kubernetes 集群
- en: We are going to configure and implement an HA MicroK8s Kubernetes cluster utilizing
    the stacked cluster HA topology that we discussed before. We’ll use the three
    nodes to install and configure MicroK8s on each of the nodes and simulate node
    failure to see whether the cluster is resisting component failures and functioning
    as expected.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将配置并实施一个高可用性（HA）MicroK8s Kubernetes 集群，采用之前讨论过的堆叠集群 HA 拓扑结构。我们将使用三台节点，在每个节点上安装并配置
    MicroK8s，并模拟节点故障，查看集群是否能抵抗组件故障并按预期运行。
- en: 'To recap, a control plane is run by all the nodes in the HA cluster. A portion
    (at least three) of the cluster nodes keeps a copy of the Kubernetes cluster datastore
    (the Dqlite database). A voting procedure is used to pick a leader for database
    maintenance. Aside from the voting nodes, there are non-voting nodes that store
    a copy of the database discreetly. These nodes are ready to replace a leaving
    voter. Finally, some nodes do not vote or duplicate the database. These are known
    as spare nodes. To summarize, the three node roles are as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，控制平面由 HA 集群中的所有节点运行。集群中的一部分节点（至少三个）保留 Kubernetes 集群数据存储的副本（Dqlite 数据库）。使用投票程序选举数据库维护的领导节点。除了投票节点之外，还有非投票节点，它们悄悄地存储数据库的副本。这些节点准备替代离开的投票节点。最后，有些节点既不投票也不复制数据库，这些节点被称为备用节点。总结一下，三个节点角色如下：
- en: '**Voters**: Replicating the database, participating in leader election'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投票节点**：复制数据库，参与选举领导节点'
- en: '**Standby**: Replicating the database, *not* participating in leader election'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**待命节点**：复制数据库，*不*参与选举领导节点'
- en: '**Spare**: *Not* replicating the database, *not* participating in leader election'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**备用节点**：*不*复制数据库，*不*参与选举领导节点'
- en: 'The administrator doesn’t need to monitor how cluster formation, database syncing,
    or voter and leader elections are all done since it’s transparent and taken care
    of. *Figure 13.3* depicts our Raspberry Pi cluster setup:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 管理员不需要监控集群的形成、数据库同步或投票和领导选举的过程，因为这些都是透明的并且已经处理好了。*图 13.3* 展示了我们的 Raspberry Pi
    集群设置：
- en: '![Figure 13.3 – A fully functional HA cluster setup ](img/Figure_13.03_B18115.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.3 – 完全功能的 HA 集群设置](img/Figure_13.03_B18115.jpg)'
- en: Figure 13.3 – A fully functional HA cluster setup
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3 – 完全功能的 HA 集群设置
- en: Now that we know what we want to do, let’s look at the requirements.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了想要做什么，让我们来看一下要求。
- en: Requirements
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 要求
- en: 'Before you begin, the following are the prerequisites for building a Raspberry
    Pi Kubernetes cluster:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，以下是构建 Raspberry Pi Kubernetes 集群的先决条件：
- en: A microSD card (4 GB minimum, 8 GB recommended)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一张 microSD 卡（至少 4 GB，推荐 8 GB）
- en: A computer with a microSD card drive
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台带有 microSD 卡槽的计算机
- en: A Raspberry Pi 2, 3, or 4 (with three nodes)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台 Raspberry Pi 2、3 或 4（至少三台节点）
- en: A micro-USB power cable (a USB-C cable for the Pi 4)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一根 micro-USB 电源线（Pi 4 使用 USB-C 电缆）
- en: A Wi-Fi network or an Ethernet cable with an internet connection
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一条 Wi-Fi 网络或带有互联网连接的以太网电缆
- en: A monitor with an HDMI interface (optional)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台带有 HDMI 接口的显示器（可选）
- en: An HDMI cable for the Pi 2 and 3 and a micro-HDMI cable for the Pi 4 (optional)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一根适用于 Pi 2 和 Pi 3 的 HDMI 电缆，或适用于 Pi 4 的 micro-HDMI 电缆（可选）
- en: A USB keyboard (optional)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一只 USB 键盘（可选）
- en: Now that we’ve established what the requirements are for setting up an HA MicroK8s
    Kubernetes cluster, we’ll move on to the step-by-step instructions on how to complete
    it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了设置 HA MicroK8s Kubernetes 集群的要求，接下来将进入逐步说明如何完成这一过程。
- en: Step 1 – Creating the MicroK8s Raspberry Pi cluster
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1 – 创建 MicroK8s Raspberry Pi 集群
- en: 'Please follow the steps that we covered in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070),
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters*,
    to create the MicroK8s Raspberry Pi cluster. Here’s a quick refresher:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照我们在 [*第 5 章*](B18115_05.xhtml#_idTextAnchor070) 中介绍的步骤，*创建并实现多节点 Raspberry
    Pi Kubernetes 集群的更新*，来创建 MicroK8s Raspberry Pi 集群。以下是简要回顾：
- en: 'Install an OS image to the SD card:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将操作系统镜像安装到 SD 卡中：
- en: Configure Wi-Fi access settings.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置 Wi-Fi 访问设置。
- en: Configure remote access settings.
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置远程访问设置。
- en: Configure control group settings.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置控制组设置。
- en: Configure a hostname.
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置主机名。
- en: Install and configure MicroK8s.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装并配置 MicroK8s。
- en: Add additional control plane nodes and worker nodes to the cluster.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向集群添加额外的控制平面节点和工作节点。
- en: Note
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Starting from the MicroK8s 1.23 release, there is now an option to add worker-only
    nodes. This type of node does not execute the control plane and does not contribute
    to the cluster’s HA. They, on the other hand, utilize fewer resources and are
    hence appropriate for low-end devices. Worker-only nodes are also appropriate
    in systems where the nodes executing the Kubernetes workloads are unreliable or
    cannot be trusted to hold the control plane.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从 MicroK8s 1.23 版本开始，现在可以选择添加仅作为工作节点的节点。这类节点不执行控制平面，也不为集群的高可用性做出贡献。另一方面，它们消耗较少的资源，因此适用于低端设备。仅作为工作节点的节点在执行
    Kubernetes 工作负载的节点不可靠或无法信任其承载控制平面的系统中也非常适用。
- en: 'To add a worker-only node to the cluster, use the `--worker` flag when running
    the `microk8s join` command:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要将一个仅作为工作节点的节点添加到集群中，请在运行 `microk8s join` 命令时使用 `--worker` 标志：
- en: '**microk8s join 192.168.1.8:25000/92b2db237428470dc4fcfc4ebbd9dc81/ 2c0cb3284b05
    --worker**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**microk8s join 192.168.1.8:25000/92b2db237428470dc4fcfc4ebbd9dc81/ 2c0cb3284b05
    --worker**'
- en: A Traefik load balancer runs on a worker node, allowing communication between
    local services (kubelet and kube-proxy) and API servers operating on several control
    plane nodes. When adding a worker node, MicroK8s tries to discover all API server
    endpoints in the cluster and correctly set up the new node. We will not use worker-only
    nodes in this section, but worker nodes that also host the control plane instead.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Traefik 负载均衡器运行在工作节点上，允许本地服务（kubelet 和 kube-proxy）与多个控制平面节点上运行的 API 服务器之间的通信。当添加一个工作节点时，MicroK8s
    会尝试发现集群中所有 API 服务器的端点，并正确设置新节点。在本节中，我们将不使用仅作为工作节点的节点，而是使用同时承载控制平面的工作节点。
- en: 'We’ll repeat the methods set out in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070),
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters*,
    for our present setup, as shown in the following table:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重复在[*第 5 章*](B18115_05.xhtml#_idTextAnchor070)中介绍的方法，*创建和实施多节点 Raspberry
    Pi Kubernetes 集群的更新*，以用于我们当前的设置，具体内容如下表所示：
- en: '![Table 13.1 – A Raspberry Pi cluster setup ](img/011.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![表 13.1 – Raspberry Pi 集群设置](img/011.jpg)'
- en: Table 13.1 – A Raspberry Pi cluster setup
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13.1 – Raspberry Pi 集群设置
- en: Now that we’re clear on our goals, we’ll take installing and configuring MicroK8s
    on each Raspberry Pi board step by step and then combine multiple deployments
    to build a fully functional cluster.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经明确了目标，我们将逐步在每个 Raspberry Pi 板上安装和配置 MicroK8s，然后将多个部署结合起来，构建一个功能完整的集群。
- en: Installing and configuring MicroK8s
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装和配置 MicroK8s
- en: 'SSH into your control plane node and install the MicroK8s snap:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: SSH 登录到控制平面节点并安装 MicroK8s Snap：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following command execution output confirms that the MicroK8s snap has
    been installed successfully:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出确认 MicroK8s Snap 已成功安装：
- en: '![Figure 13.4 – MicroK8s installation ](img/Figure_13.04_B18115.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.4 – MicroK8s 安装](img/Figure_13.04_B18115.jpg)'
- en: Figure 13.4 – MicroK8s installation
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4 – MicroK8s 安装
- en: 'The following command execution output confirms that MicroK8s is running successfully:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出确认 MicroK8s 正在成功运行：
- en: '![Figure 13.5 – Inspect your MicroK8s cluster ](img/Figure_13.05_B18115.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.5 – 检查您的 MicroK8s 集群](img/Figure_13.05_B18115.jpg)'
- en: Figure 13.5 – Inspect your MicroK8s cluster
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 – 检查您的 MicroK8s 集群
- en: 'If the installation is successful, then you should see the following output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果安装成功，您应该会看到以下输出：
- en: '![Figure 13.6 – Verify whether the node is in a ready state ](img/Figure_13.06_B18115.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.6 – 验证节点是否处于准备就绪状态](img/Figure_13.06_B18115.jpg)'
- en: Figure 13.6 – Verify whether the node is in a ready state
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6 – 验证节点是否处于准备就绪状态
- en: Repeat the MicroK8s installation process on the other nodes as well.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他节点上重复 MicroK8s 安装过程。
- en: 'The next step is to add a control plane node and worker node to the cluster.
    Open PuTTY shell to control plane node and run the following command to generate
    the connection string:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将控制平面节点和工作节点添加到集群中。打开 PuTTY shell 连接到控制平面节点并运行以下命令以生成连接字符串：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following command execution output validates that the command was successfully
    executed and provides instructions for the connection string:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出验证该命令已成功执行，并提供连接字符串的说明：
- en: '![Figure 13.7 – Generate connection string for adding nodes ](img/Figure_13.07_B18115.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.7 – 生成用于添加节点的连接字符串](img/Figure_13.07_B18115.jpg)'
- en: Figure 13.7 – Generate connection string for adding nodes
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7 – 生成用于添加节点的连接字符串
- en: As indicated by the preceding command execution output, the connection string
    is generated in the form of `<control plane_ip>:<port>/<token>`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面命令执行的输出所示，连接字符串以 `<control plane_ip>:<port>/<token>` 的形式生成。
- en: Adding additional control plane nodes
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加额外的控制平面节点
- en: 'We now have the connection string to join with the control plane node. Open
    the PuTTY shell to the `controlplane1` node and run the `join` command to add
    it to the cluster:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有与控制平面节点连接的连接字符串。打开 PuTTY shell 连接到 `controlplane1` 节点，运行 `join` 命令将其添加到集群：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The command was successfully executed, and the node has joined the cluster,
    as shown in the output:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 命令成功执行，节点已加入集群，输出如下所示：
- en: '![Figure 13.8 – Adding an additional control plane node to the cluster ](img/Figure_13.08_B18115.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.8 – 向集群添加额外的控制平面节点](img/Figure_13.08_B18115.jpg)'
- en: Figure 13.8 – Adding an additional control plane node to the cluster
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8 – 向集群添加额外的控制平面节点
- en: Our next step is to add a worker node to the cluster now that we have added
    an additional control plane node so that we can simulate node failure to see whether
    the cluster is able to resist component failures and function as we expect.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经添加了一个额外的控制平面节点，接下来我们将向集群添加一个工作节点，以便模拟节点故障，看看集群是否能抵抗组件故障并按预期工作。
- en: Adding a worker node
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加一个工作节点
- en: 'We now have the connection string to join with the control plane node. Open
    the PuTTY shell to the worker node and run the `join` command to add it to the
    cluster:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有与控制平面节点连接的连接字符串。打开 PuTTY shell 连接到工作节点，运行 `join` 命令将其添加到集群：
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The command was successfully executed, and the node has joined the cluster,
    as shown in the output:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 命令成功执行，节点已加入集群，输出如下所示：
- en: '![Figure 13.9 – Adding a worker node to the cluster ](img/Figure_13.09_B18115.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.9 – 向集群添加一个工作节点](img/Figure_13.09_B18115.jpg)'
- en: Figure 13.9 – Adding a worker node to the cluster
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9 – 向集群添加一个工作节点
- en: As indicated by the preceding command execution output, you should be able to
    see the new node in a few seconds on the control plane.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面命令执行的输出所示，您应该能够在几秒钟内在控制平面上看到新节点。
- en: 'Use the following command to verify whether the new node has been added to
    the cluster:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令验证新节点是否已被添加到集群：
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following command execution output shows that `controlplane`, `controlplane1`,
    and `worker2` are part of the cluster:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出显示 `controlplane`、`controlplane1` 和 `worker2` 是集群的一部分：
- en: '![Figure 13.10 – The cluster is ready, and control planes and worker2 are part
    of the cluster ](img/Figure_13.10_B18115.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.10 – 集群已准备就绪，控制平面和worker2是集群的一部分](img/Figure_13.10_B18115.jpg)'
- en: Figure 13.10 – The cluster is ready, and control planes and worker2 are part
    of the cluster
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.10 – 集群已准备就绪，控制平面和worker2是集群的一部分
- en: A fully functional multi-node Kubernetes cluster would look like that shown
    in *Figure 13.3*. To summarize, we have installed MicroK8s on the Raspberry Pi
    boards and joined multiple deployments to form the cluster. We’ve also added control
    plane nodes and worker nodes to the cluster.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完全功能的多节点 Kubernetes 集群看起来就像*图 13.3*所示。总结一下，我们已经在 Raspberry Pi 板上安装了 MicroK8s，并将多个部署加入集群。我们还向集群中添加了控制平面节点和工作节点。
- en: Now that we have a fully functional cluster, we will move on to the next step
    of examining the HA setup.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个完全功能的集群，我们将继续下一步，检查高可用性（HA）设置。
- en: Step 2 – Examining the HA setup
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2 – 检查 HA 设置
- en: 'Since we have more than one node running the control plane, MicroK8s’ HA would
    be achieved automatically. An HA Kubernetes cluster requires three conditions
    to be satisfied:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有多个节点运行控制平面，MicroK8s 的 HA 会自动实现。要实现 HA Kubernetes 集群，需要满足三个条件：
- en: At any given time, there must be more than one node available.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任何时候，都必须有多个节点可用。
- en: The control plane must run on more than one node so that the cluster does not
    become unusable, even if a single node fails.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制平面必须运行在多个节点上，以确保即使一个节点失败，集群也不会变得不可用。
- en: The cluster state must be stored in a highly accessible datastore.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群状态必须存储在一个高度可访问的数据存储中。
- en: 'We can check the current state of the HA cluster using the following command:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令检查当前 HA 集群的状态：
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following command execution output confirms that HA has been achieved and
    also displays the datastore master nodes. Standby nodes are set to `none` since
    we have only three nodes; additional nodes will be designated as standby candidates
    for the datastore and will be promoted automatically if one of the datastore’s
    nodes fails:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出确认 HA 已经实现，并显示了数据存储主节点。由于我们只有三台节点，备用节点被设置为 `none`；如果增加节点，这些额外的节点将会被指定为数据存储的备用候选节点，如果其中一个数据存储节点发生故障，将会自动晋升为主节点：
- en: '![Figure 13.11 – Inspecting the MicroK8s HA cluster ](img/Figure_13.11_B18115.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.11 - 检查 MicroK8s 高可用集群](img/Figure_13.11_B18115.jpg)'
- en: Figure 13.11 – Inspecting the MicroK8s HA cluster
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11 - 检查 MicroK8s 高可用集群
- en: Congrats! You now have a secure, distributed, highly available Kubernetes cluster
    that’s ready for a production-grade MicroK8s cluster environment. In the next
    section, we are going to deploy a sample application on the MicroK8s cluster that
    we just created.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你现在已经拥有一个安全、分布式、高可用的 Kubernetes 集群，已经准备好作为生产级别的 MicroK8s 集群环境使用。在下一节中，我们将会在我们刚刚创建的
    MicroK8s 集群上部署一个示例应用程序。
- en: Step 3 – Deploying a sample containerized application
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步 - 部署示例容器化应用程序
- en: In this section, we will deploy the NGINX deployment from the Kubernetes `examples`
    repository in our multi-node HA MicroK8s cluster setup.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在我们的多节点高可用 MicroK8s 集群环境中，从 Kubernetes `examples` 仓库部署 NGINX 部署。
- en: 'The following command will deploy the sample application deployment:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将部署示例应用程序：
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following command execution output indicates that there is no error in
    the deployment and in the next steps, we can verify this using the `get deployments`
    command:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出表明部署没有错误，在接下来的步骤中，我们可以使用 `get deployments` 命令验证这一点：
- en: '![Figure 13.12 – A sample application deployment ](img/Figure_13.12_B18115.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.12 - 示例应用程序部署](img/Figure_13.12_B18115.jpg)'
- en: Figure 13.12 – A sample application deployment
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12 - 示例应用程序部署
- en: 'The following command execution output displays the information about the deployment:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出显示了部署的相关信息：
- en: '![Figure 13.13 – The sample application deployments are in ready state ](img/Figure_13.13_B18115.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.13 - 示例应用程序部署已处于就绪状态](img/Figure_13.13_B18115.jpg)'
- en: Figure 13.13 – The sample application deployments are in ready state
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13 - 示例应用程序部署已处于就绪状态
- en: 'Let us also check where the pods are running using the following command:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令检查 Pods 的运行位置：
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following command execution output indicates that pods are equally distributed
    between the nodes:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出表明 Pods 在各节点之间分布均匀：
- en: '![Figure 13.14 – The pod distribution across the nodes ](img/Figure_13.14_B18115.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.14 - Pods 在各节点上的分布](img/Figure_13.14_B18115.jpg)'
- en: Figure 13.14 – The pod distribution across the nodes
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.14 - Pods 在各节点上的分布
- en: Great! We have just deployed our sample application on the Raspberry multi-node
    HA cluster. To summarize, we built a Kubernetes Raspberry Pi cluster and used
    it to deploy a sample application. We’ll perform some of the tests to check whether
    our cluster is resistant to failures in the next step.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们刚刚在 Raspberry 多节点高可用集群上部署了示例应用程序。总结来说，我们构建了一个 Kubernetes Raspberry Pi
    集群，并使用它来部署了一个示例应用程序。接下来，我们将进行一些测试，检查我们的集群在面对故障时的抗压能力。
- en: Note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In the case that add-ons are enabled on a multi-node HA cluster, if the client
    binaries are downloaded and installed for an add-on, those binaries will only
    be available on the specific node from which the add-on was enabled.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在多节点高可用集群上启用了附加组件，并且为附加组件下载并安装了客户端二进制文件，则这些二进制文件只会在启用了附加组件的特定节点上可用。
- en: Step 3 – Simulating control plane node failure
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步 - 模拟控制平面节点故障
- en: To simulate node failure, we will use the `cordon` command to mark the node
    as `unschedulable`. If the node is unschedulable, the Kubernetes controller will
    not schedule new pods on this node.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟节点故障，我们将使用`cordon`命令将节点标记为`unschedulable`。如果节点处于不可调度状态，Kubernetes 控制器将不会在该节点上调度新的
    Pods。
- en: 'Let’s use `cordon` on the `controlplane1` node so that we can simulate control
    plane failure. Use the following command to cordon the node:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 `controlplane1` 节点上使用 `cordon`，以模拟控制平面故障。使用以下命令将节点标记为不可调度：
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following command execution output shows that `controlplane1` has been
    cordoned:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出显示 `controlplane1` 已被标记为不可调度：
- en: '![Figure 13.15 – Cordoning the controlplane1 node ](img/Figure_13.15_B18115.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.15 - 将 controlplane1 节点标记为不可调度](img/Figure_13.15_B18115.jpg)'
- en: Figure 13.15 – Cordoning the controlplane1 node
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.15 - 将 controlplane1 节点标记为不可调度
- en: 'Even though the `controplane1` node is cordoned, existing pods will still run.
    We can now use the `drain` command to delete all the pods. Use the following command
    to drain the node:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 `controlplane1` 节点已被隔离，现有的 Pod 仍然会继续运行。现在我们可以使用 `drain` 命令删除所有 Pod。使用以下命令来排空该节点：
- en: '[PRE9]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Use the `--ignore-daemonsets` flag to drain the nodes that contain the pods
    that are managed by DaemonSet.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `--ignore-daemonsets` 标志来排空包含由 DaemonSet 管理的 Pod 的节点。
- en: 'The following command execution output shows that the pods running on `controlplane1`
    have been deleted successfully:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出显示，运行在 `controlplane1` 上的 Pod 已成功删除：
- en: '![Figure 13.16 – Draining the controlplane1 node ](img/Figure_13.16_B18115.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.16 – 排空 controlplane1 节点](img/Figure_13.16_B18115.jpg)'
- en: Figure 13.16 – Draining the controlplane1 node
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.16 – 排空 controlplane1 节点
- en: Because the control plane is run by all nodes in the HA cluster, if one of the
    control plane nodes (`controlplane1`, for example) fails, cluster decisions can
    switch over to another control plane node and continue working without much disruption.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 因为控制平面由 HA 集群中的所有节点运行，如果某个控制平面节点（例如 `controlplane1`）失败，集群决策可以切换到另一个控制平面节点，并继续正常工作，几乎不会有中断。
- en: As part of the new control plane decisions, the Kubernetes controller will now
    recreate a new pod and schedule it in a different node as soon as the pod is deleted.
    It cannot be placed on the same node because the scheduling is disabled (since
    we have cordoned the `controlplane1` node).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 作为新控制平面决策的一部分，Kubernetes 控制器将在 Pod 被删除后尽快重建新的 Pod，并将其调度到其他节点。由于调度已被禁用（因为我们已经隔离了
    `controlplane1` 节点），新 Pod 不能被调度到同一节点上。
- en: 'Let us inspect where the pods are running using the `kubectl get pods` command.
    The following command execution output shows that the new pod has been rescheduled
    to the `controlplane` node:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `kubectl get pods` 命令查看 Pod 正在运行的位置。以下命令执行输出显示，新 Pod 已重新调度到 `controlplane`
    节点：
- en: '![Figure 13.17 – Pod redistribution across the nodes ](img/Figure_13.17_B18115.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.17 – Pod 在节点间的重新分配](img/Figure_13.17_B18115.jpg)'
- en: Figure 13.17 – Pod redistribution across the nodes
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.17 – Pod 在节点间的重新分配
- en: 'The following command execution output shows that the deployments have also
    been restored, despite the failure of one of the control plane nodes:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出显示，尽管控制平面节点之一失败，部署也已恢复：
- en: '![Figure 13.18 – The deployments are in a ready state ](img/Figure_13.18_B18115.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.18 – 部署已处于就绪状态](img/Figure_13.18_B18115.jpg)'
- en: Figure 13.18 – The deployments are in a ready state
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.18 – 部署已处于就绪状态
- en: 'Almost all HA cluster administration is invisible to the administrator and
    requires minimum configuration. Only the administrator has the ability to add
    and remove nodes. The following parameters should be considered to ensure the
    cluster’s health:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的 HA 集群管理对于管理员来说都是不可见的，并且需要最小的配置。只有管理员才能添加和移除节点。为了确保集群的健康，应该考虑以下参数：
- en: If the leader node is *removed*, such as by crashing and never returning, the
    cluster may take up to 5 seconds to elect a new leader.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果领导节点被*移除*，比如崩溃后永远无法恢复，集群可能需要最多 5 秒钟的时间来选举新的领导者。
- en: It can take up to 30 seconds to convert a non-voter into a voter. This promotion
    occurs when a new node joins the cluster or when a voter fails.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将非选民节点转换为选民节点最多可能需要 30 秒。当一个新节点加入集群或选民节点失败时，就会发生这个提升。
- en: To summarize, we used the stacked cluster HA topology to configure and implement
    a highly available MicroK8s Kubernetes cluster. We used the three nodes to install
    and configure MicroK8s on each one, as well as simulating node failure to see
    whether the cluster can withstand component failures and continue to function
    as expected. In the next section, we will touch upon some of the best practices
    for implementing Kubernetes for a production-grade Kubernetes cluster.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们使用堆叠式集群 HA 拓扑配置并实现了一个高可用的 MicroK8s Kubernetes 集群。我们使用三个节点，在每个节点上安装并配置了
    MicroK8s，同时模拟节点故障以查看集群是否能承受组件故障并继续按预期工作。在下一节中，我们将讨论一些实施生产级 Kubernetes 集群的最佳实践。
- en: Kubernetes HA best practices
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes HA 最佳实践
- en: As people become more acquainted with Kubernetes, there are trends toward more
    advanced use of the platform, such as users deploying Kubernetes in an HA architecture
    to ensure full production uptime. According to the recent *Kubernetes and cloud
    native operations report, 2022* ([https://juju.is/cloud-native-kubernetes-usage-report-2022](https://juju.is/cloud-native-kubernetes-usage-report-2022)),
    many respondents appear to utilize Kubernetes’ HA architecture for highly secure,
    data-sensitive applications.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人们对 Kubernetes 的熟悉，平台的使用趋势趋向于更高级的应用，比如用户在高可用性架构中部署 Kubernetes，以确保生产环境的持续正常运行。根据最近的
    *Kubernetes 和云原生操作报告，2022*（[https://juju.is/cloud-native-kubernetes-usage-report-2022](https://juju.is/cloud-native-kubernetes-usage-report-2022)），许多受访者似乎正在利用
    Kubernetes 的高可用架构来处理对数据安全性要求较高的应用。
- en: In this section, we will go over some of the best practices for deploying HA
    apps in Kubernetes. These guidelines build upon what we have seen in [*Chapter
    5*](B18115_05.xhtml#_idTextAnchor070), *Creating and Implementing Updates on Multi-Node
    Raspberry Pi Kubernetes Clusters*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些在 Kubernetes 中部署高可用应用的最佳实践。这些指南基于我们在 [*第 5 章*](B18115_05.xhtml#_idTextAnchor070)
    中看到的内容，*在多节点 Raspberry Pi Kubernetes 集群上创建和实现更新*。
- en: 'As you may be aware, deploying a basic app setup in Kubernetes is a piece of
    cake. Trying to make your application available and fault-tolerant, on the other
    hand, implies a slew of challenges and problems. In general, implementing HA in
    any capacity requires the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，在 Kubernetes 中部署一个基本的应用设置非常简单。然而，要使你的应用程序可用且具有容错能力，则意味着会遇到一系列挑战和问题。通常，实施高可用性（HA）需要以下几个方面：
- en: '**Determining your application’s intended level of availability**: The allowable
    level of downtime varies by application and business objectives.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确定应用程序的预期可用性水平**：允许的停机时间因应用程序和业务目标而异。'
- en: '**A redundant and reliable control plane for your application**: The control
    plane manages the cluster state and contributes to the availability of your applications
    to users.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为你的应用提供冗余且可靠的控制平面**：控制平面管理集群状态，并有助于应用程序对用户的可用性。'
- en: '**A redundant and reliable data plane for your application**: This entails
    duplicating the data across all cluster nodes.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为你的应用提供冗余且可靠的数据平面**：这意味着将数据复制到所有集群节点。'
- en: 'There are a lot of considerations and decisions to make while deploying Kubernetes’
    HA that might have an impact on the apps and how they operate and consume storage
    resources. Here, we will look at some of the considerations to make:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署 Kubernetes 高可用性时，需要考虑和决策的因素有很多，这些因素可能会影响应用程序及其操作和存储资源的消耗。以下是一些需要考虑的事项：
- en: '**Use of replicas**: Use replicas instead of pods to deploy HA apps. Using
    replicas ensures that your application is operating on a consistent set of pods
    at all times. For the application to be declared minimally accessible, it must
    have at least two replicas.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用副本**：使用副本而非 Pods 来部署高可用应用。使用副本可以确保你的应用始终在一致的 Pod 集合上运行。为了使应用程序被声明为最小可访问，必须至少有两个副本。'
- en: '`Ready` state to 75% of their pre-update level. As a result, during the update,
    an application’s compute capacity may drop to 75% of its normal level, resulting
    in a partial failure (degraded application performance). The `RollingUpdate.maxUnavailable`
    parameter lets you choose the maximum percentage of pods that can go down during
    an upgrade. As a result, either ensure that your application operates properly,
    even if 25% of your pods are unavailable, or reduce the `maxUnavailable` option.
    Based on the application needs, other deployment strategies, such as blue-green
    and canary, among others, can also be evaluated for a much better alternative
    to the default strategy.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Ready` 状态下降至更新前 75% 的水平。因此，在更新过程中，应用程序的计算能力可能降至正常水平的 75%，从而导致部分失败（应用程序性能降级）。`RollingUpdate.maxUnavailable`
    参数让你选择升级过程中可以下线的最大 Pod 百分比。因此，要么确保你的应用程序在 25% 的 Pod 不可用的情况下仍能正常运行，要么降低 `maxUnavailable`
    参数。根据应用的需求，还可以评估其他部署策略，如蓝绿部署和金丝雀发布等，以获得比默认策略更好的替代方案。'
- en: '**Right sizing of the nodes**: The maximum amount of RAM that can be allocated
    to pods is determined by the size of the nodes. For production clusters, the size
    of the nodes is large enough (2.5 GB or more) that they can absorb the workload
    of any crashed nodes.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正确的节点大小配置**：可以分配给 Pod 的最大内存量由节点的大小决定。对于生产集群，节点的大小通常足够大（2.5 GB 或更大），能够承载任何宕机节点的工作负载。'
- en: '**Node pools for HA**: Node pools with production workloads should contain
    at least three nodes to provide HA. This allows the cluster to distribute and
    schedule work on other nodes if one becomes unavailable.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用于 HA 的节点池**：生产工作负载的节点池应至少包含三个节点，以提供高可用性（HA）。这可以让集群在某个节点不可用时，能够将工作调度到其他节点。'
- en: '`request` and `limit` objects in your application spec for all deployments:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在应用程序规范中为所有部署使用 `request` 和 `limit` 对象：
- en: '**Requests**: Specifies how much of a resource (such as CPU and memory resources)
    a pod may require before it is scheduled on a node. The pod will not be scheduled
    if the node lacks the required resources. This keeps pods from being scheduled
    on nodes that are already overburdened.'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**请求**：指定一个 Pod 在节点上调度前所需的资源数量（如 CPU 和内存）。如果节点缺乏所需资源，Pod 将无法调度。这可以防止将 Pod 调度到已经超负荷的节点上。'
- en: '**Limits**: Specifies how many resources (such as CPU and RAM) a pod is permitted
    to use on a node. This stops pods from potentially slowing down the operation
    of other pods.'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**限制**：指定一个 Pod 在节点上允许使用的资源数量（如 CPU 和内存）。这可以防止 Pod 占用过多资源，从而可能影响其他 Pod 的运行。'
- en: '**Set pod disruption budgets**: To avoid interruptions to production, such
    as during cluster upgrades, you can configure a pod disruption budget, which restricts
    the number of replicated pods that can be down at the same time.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设置 Pod 中断预算**：为了避免生产环境中的中断，例如在集群升级时，可以配置 Pod 中断预算，限制同一时间内可以停机的副本 Pod 数量。'
- en: '**Ensure a cluster is upgraded**: Ensure to take advantage of the latest features,
    security patches, and stability improvements.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确保集群已升级**：确保利用最新的功能、安全补丁和稳定性改进。'
- en: '**Avoid single points of failure**: Kubernetes enhances dependability by providing
    repeating components and ensuring that application containers can be scheduled
    across various nodes. For HA, use anti-affinity or node selection to help disperse
    your applications across the Kubernetes cluster. Based on labels, node selection
    allows you to specify which nodes in your cluster are eligible to run your application.
    Labels often describe node attributes such as bandwidth or specialized resources
    such as GPUs. For example, to properly commit mutations to data, Apache ZooKeeper
    requires a quorum of servers. Two servers in a three-server ensemble must be healthy
    for writes to succeed. As a result, a resilient deployment must make sure that
    servers are distributed across failure domains.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免单点故障**：Kubernetes 通过提供冗余组件并确保应用容器可以跨多个节点调度，从而提高可靠性。为了实现高可用性（HA），可以使用反亲和性或节点选择来帮助将应用分散到
    Kubernetes 集群中。基于标签，节点选择允许你指定集群中哪些节点有资格运行你的应用。标签通常描述节点属性，如带宽或专用资源，如 GPU。例如，为了正确地提交数据变更，Apache
    ZooKeeper 需要一个服务器的多数节点。一个三节点的集群中，两个节点必须是健康的，写操作才会成功。因此，可靠的部署必须确保服务器分布在不同的故障域中。'
- en: '**Use liveness and readiness probes**: By default, Kubernetes will transfer
    traffic to application containers instantaneously. You may improve the robustness
    of your application by configuring health checks to notify Kubernetes when your
    application pods are ready to receive traffic or have become unresponsive.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用存活性和就绪性探针**：默认情况下，Kubernetes 会立即将流量转发到应用程序容器。通过配置健康检查，可以在应用程序 Pod 准备好接收流量或变得不可响应时通知
    Kubernetes，从而提高应用程序的稳定性。'
- en: '`startupProbe` or `readinessProbe`, you can use `initContainers` to check for
    external dependencies. Changes to the application code are not required for `initContainers`.
    It is not necessary to embed additional tools in order to utilize them to examine
    external dependencies in application containers.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`startupProbe` 或 `readinessProbe`，可以使用 `initContainers` 来检查外部依赖项。对于 `initContainers`，无需更改应用程序代码。也不需要在容器中嵌入额外工具来检查应用程序容器中的外部依赖项。'
- en: '**Use plenty of descriptive labels**: Labels are extremely powerful since they
    are arbitrary key-value pairs and enable you to logically organize all your Kubernetes
    workloads in your clusters.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用大量描述性标签**：标签非常强大，因为它们是任意的键值对，可以帮助你在集群中逻辑地组织所有 Kubernetes 工作负载。'
- en: '**Use sidecars for proxies and watchers**: Sometimes, a set of processes is
    required to communicate with another process. However, you do not want all of
    these to operate in a single container but rather in a pod. This is also the case
    when you are running a proxy or a watcher on which your processes rely. For example,
    with a database on which your processes rely, the credentials would not be hardcoded
    onto each container. Instead, you can deploy the credentials as a proxy inside
    a sidecar that handles the connection securely.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 sidecar 进行代理和监视**：有时，需要一组进程与另一个进程进行通信。然而，你不希望所有这些进程都在一个容器中运行，而是希望它们在一个
    Pod 中运行。当你在运行一个代理或监视程序，而你的进程依赖于它时，这种情况也是如此。例如，对于一个依赖的数据库，凭据不会硬编码到每个容器中。相反，你可以将凭据作为一个代理部署在
    sidecar 中，确保安全地处理连接。'
- en: '**Automate your CI/CD pipeline and avoid manual Kubernetes deployments**: Because
    there may be numerous deployments per day, this strategy saves the team considerable
    time by eliminating manual error-prone tasks.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化你的 CI/CD 流水线，避免手动 Kubernetes 部署**：因为每天可能有大量部署，这种策略通过消除易出错的手动任务，为团队节省了大量时间。'
- en: '`Prod`, `Dev`, and `Test` namespaces in the same cluster, and you can also
    use namespaces to limit the number of resources so that one defective process
    does not consume all of the cluster resources.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在同一集群中使用 `Prod`、`Dev` 和 `Test` 命名空间，你还可以利用命名空间限制资源数量，从而避免一个有问题的进程占用所有集群资源。
- en: '**Monitoring the control plane**: This helps in the identification of issues
    or threats within the cluster and increases latency. It is also advised to employ
    automated monitoring tools rather than managing alerts manually.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控控制平面**：这有助于识别集群中的问题或威胁，并增加延迟。建议使用自动化监控工具，而不是手动管理警报。'
- en: To recap, we’ve gone through some of the best practices to optimize your Kubernetes
    environment.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们回顾了优化 Kubernetes 环境的一些最佳实践。
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at how to set up an HA MicroK8s Kubernetes cluster
    using the stacked cluster HA topology. We utilized the three nodes to install
    and configure MicroK8s on each of them, as well as simulating node failure to
    see whether the cluster could tolerate component failures and still continue to
    function normally.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何使用堆叠集群 HA 拓扑设置 HA MicroK8s Kubernetes 集群。我们利用三台节点在每台节点上安装和配置 MicroK8s，并模拟节点故障，查看集群是否能够容忍组件故障并继续正常运行。
- en: We discussed some of the best practices for implementing Kubernetes applications
    on your production-ready cluster. We also covered the fact that MicroK8s’ HA option
    has been simplified and enabled by default.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了一些在生产就绪集群中实现 Kubernetes 应用程序的最佳实践。我们还提到，MicroK8s 的 HA 选项已经简化，并默认启用。
- en: HA is a vital feature for organizations looking to deploy containers and pods
    that can deliver the kind of reliability required at scale. We also recognized
    the value of Canonical's lightweight Dqlite SQL database, which is used to provide
    HA clustering. By embedding the database into Kubernetes, Dqlite reduces the cluster’s
    memory footprint and eliminates process overhead. For IoT or Edge applications,
    this is critical.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用性（HA）是组织在部署容器和 Pod 时，提供大规模可靠性所需的重要特性。我们还认识到 Canonical 的轻量级 Dqlite SQL 数据库的价值，它用于提供
    HA 集群。通过将数据库嵌入 Kubernetes，Dqlite 减少了集群的内存占用，并消除了进程开销。对于物联网（IoT）或边缘计算应用，这一点至关重要。
- en: In the next chapter, we’ll look at how to use Kata Containers, a secure container
    runtime, to provide stronger workload isolation by leveraging hardware virtualization
    technology.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何使用 Kata Containers，一种安全的容器运行时，通过利用硬件虚拟化技术提供更强的工作负载隔离。
