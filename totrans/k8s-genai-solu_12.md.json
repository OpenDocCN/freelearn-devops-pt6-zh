["```\n\n$ kubectl logs <pod-name>\n```", "```\n\n$ kubectl logs <pod-name> -c <container-name>\n```", "```\n\n--audit-log-path=/var/log/kubernetes/kube-apiserver-audit.log\n--audit-policy-file=/etc/kubernetes/audit-policy.yaml\n```", "```\n\napiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n  - level: Metadata\n```", "```\n\nmodule \"eks_blueprints_addons\" {\n  source = \"aws-ia/eks-blueprints-addons/aws\"\n  ...\n  enable_kube_prometheus_stack = true\n  kube_prometheus_stack = {\n    namespace = \"monitoring\"\n    values = [\n      templatefile(\"${path.module}/kube-prometheus.yaml\", {\n        storage_class_type = kubernetes_storage_class.default_gp3.id\n      })]\n  ...\n  helm_releases = {\n    \"prometheus-adapter\" = {\n      repository = \"https://prometheus-community.github.io/helm-charts\"\n      chart      = \"prometheus-adapter\"\n      namespace = module.eks_blueprints_addons.kube_prometheus_stack.namespace\n...\n```", "```\n\n$ terraform init\n$ terraform plan\n$ terraform apply -auto-approve\n```", "```\n\n$ kubectl get ds -n monitoring\nNAME                                             READY\nkube-prometheus-stack-prometheus-node-exporter   6\n$ kubectl get deploy -n monitoring\nNAME                                       READY\nkube-prometheus-stack-grafana              1/1\nkube-prometheus-stack-kube-state-metrics   1/1\nkube-prometheus-stack-operator             1/1\nprometheus-adapter                         1/1\n$ kubectl get sts -n monitoring\nNAME                                          READY\nprometheus-kube-prometheus-stack-prometheus   1/1\n```", "```\n\nresource \"helm_release\" \"dcgm_exporter\" {\n  name       = \"dcgm-exporter\"\n...\n  values = [\n      <<-EOT\n        serviceMonitor:\n          terraform commands to update the NVIDIA DCGM exporter Helm chart in the EKS cluster:\n\n```", "```\n\n\t\t\tWe can verify the scraping status by launching the Prometheus dashboard and checking the scraping target’s health. By default, the Prometheus service is internal and not exposed outside of the cluster, so let’s use the `kubectl port-forward` mechanism to connect to it from the local machine. Run the following command to initiate a port-forward connection from local port `9090` to Prometheus service port `9090` in the `monitoring` namespace:\n\n```", "```\n\n\t\t\tNow, launch the [http://localhost:9090/targets?search=dcgm-exporter](http://localhost:9090/targets?search=dcgm-exporter) URL in your browser to check the health of the DCGM exporter targets. *Figure 12**.6* shows that all three `dcgm-exporter` endpoints are in the **UP** state, with each corresponding to a GPU worker node in the cluster:\n\n\t\t\t\t\t![Figure 12.6 – Prometheus target health status](img/B31108_12_06.jpg)\n\n\t\t\tFigure 12.6 – Prometheus target health status\n\t\t\tNext, we can query the `dcgm-exporter` metrics using PromQL. For example, *Figure 12**.7* shows the average GPU utilization across multiple GPUs over the past minute grouped by the K8s Pod, where `DCGM_FI_DEV_GPU_UTIL` is the metric exposed by `dcgm-exporter`.\n\n\t\t\t\t\t![Figure 12.7 – Querying GPU metrics using PromQL](img/B31108_12_07.jpg)\n\n\t\t\tFigure 12.7 – Querying GPU metrics using PromQL\n\t\t\tSimilarly, we will create additional Service Monitors to collect metrics from both the Ray Serve cluster deployed in [*Chapter 11*](B31108_11.xhtml#_idTextAnchor145) and various e-commerce chatbot application components. You can download the corresponding K8s manifest files from the GitHub repository at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch12/monitoring](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch12/monitoring) and apply each file using the `kubectl apply -f` command to deploy them to the EKS cluster:\n\n```", "```\n\n\t\t\tAfter applying the files, navigate to the Prometheus dashboard and verify the status of the newly discovered scraping targets. You will notice both `ray-workers` and `ray-head` Pod targets in the dashboard, as shown in *Figure 12**.8*.\n\n\t\t\t\t\t![Figure 12.8 – Prometheus target health status](img/B31108_12_08.jpg)\n\n\t\t\tFigure 12.8 – Prometheus target health status\n\t\t\tIn this section, we explored various tools to monitor GenAI applications in K8s, such as Fluentd, Fluent Bit, Loki, OTel, and Prometheus. We also deployed `kube-prometheus-stack` in the EKS cluster and set up the Service/Pod monitors to collect metrics from `dcgm-exporter` and other application components in our EKS cluster. In the next section, we will dive into visualization tools such as Grafana to view these metrics.\n\t\t\tVisualization and debugging\n\t\t\tIn this section, we will explore key practices for enhancing the observability of GenAI applications deployed on K8s. We will begin by demonstrating how Grafana can be used to visualize essential metrics such as GPU utilization, system performance, and the status of GenAI applications, to provide real-time operational insights. Additionally, we will dive into debugging strategies for Gen AI workloads by examining tools such as **Langfuse** ([https://github.com/langfuse/langfuse](https://github.com/langfuse/langfuse)), an open source LLM engineering platform designed to aid in debugging and the analysis of LLM applications.\n\t\t\tGrafana\n\t\t\t**Grafana** ([https://grafana.com/](https://grafana.com/)) is an open source visualization and analytics platform widely used for monitoring K8s environments. It provides a centralized interface for querying, visualizing, and alerting on metrics collected from various data sources, such as Prometheus, Amazon CloudWatch, and Azure Monitor.\n\t\t\tGrafana provides prebuilt and customizable dashboards to monitor K8s components such as API servers, etcd databases, nodes, Pods, and namespaces. These dashboards help visualize metrics such as CPU and memory utilization, network activity, and application-specific metrics. Grafana allows users to configure alerts with notifications sent to channels such as Slack, email, PagerDuty, or **Microsoft Teams**. Alerts are typically based on threshold conditions, though you can also implement anomaly detection through custom queries or external integrations.\n\t\t\tGrafana provides the ability to define roles and permissions using **role-based access control** (**RBAC**), allowing fine-grained control over who can view or edit dashboards and alerts. Grafana supports a wide range of community-contributed plugins ([https://grafana.com/grafana/plugins/](https://grafana.com/grafana/plugins/)), custom visualization panels ([https://grafana.com/grafana/plugins/panel-plugins/](https://grafana.com/grafana/plugins/panel-plugins/)), and dashboards ([https://grafana.com/grafana/dashboards/](https://grafana.com/grafana/dashboards/)), enabling users to extend its functionality and adapt it to specific use cases.\n\t\t\tGrafana best practices\n\t\t\tThe following lists some Grafana best practices:\n\n\t\t\t\t*   Ensure Grafana dashboards and settings persist across Pod restarts by using a **Persistent Volume** (**PV**) and a PVC. This helps maintain the state even when Grafana Pods are rescheduled.\n\t\t\t\t*   Automate dashboard provisioning using ConfigMaps or **infrastructure as code** (**IaC**) tools, such as Terraform, to maintain consistent observability setups across environments.\n\t\t\t\t*   Enable robust authentication mechanisms ([https://grafana.com/docs/grafana/latest/setup-grafana/configure-security/configure-authentication/](https://grafana.com/docs/grafana/latest/setup-grafana/configure-security/configure-authentication/)) such as OAuth, SAML, and LDAP, to control user access. Use RBAC to manage user permissions effectively.\n\t\t\t\t*   When exposing the Grafana dashboard outside the cluster, use an Ingress controller with TLS termination to secure network communications.\n\t\t\t\t*   Leverage Grafana’s rich plugin ecosystem to integrate with external data sources and specialized visualizations.\n\t\t\t\t*   Enhance observability by combining metrics, logs, and traces. Integrate Grafana with Loki for centralized K8s logging alongside metrics visualization.\n\t\t\t\t*   Monitor Grafana’s resource usage and performance within the K8s cluster. This includes setting up alerts for abnormal behavior, which helps maintain optimal performance and availability.\n\t\t\t\t*   Use managed Grafana offerings such as **Amazon Managed Grafana** ([https://aws.amazon.com/grafana/](https://aws.amazon.com/grafana/)), **Grafana Cloud** ([https://grafana.com/products/cloud/](https://grafana.com/products/cloud/)), and **Azure Managed Grafana** ([https://azure.microsoft.com/en-us/products/managed-grafana](https://azure.microsoft.com/en-us/products/managed-grafana)) to offload operational tasks such as scaling, patching, and security management. This enables us to focus on creating dashboards and analyzing data. These services also provide seamless cloud-native integrations, auto-scaling capabilities, and cost efficiencies.\n\n\t\t\tSetting up Grafana dashboards\n\t\t\tIn our setup, we deployed Grafana as part of the `kube-prometheus-stack` installation earlier in this chapter. Alternatively, Grafana Helm charts can be leveraged to deploy as a standalone option in K8s; refer to [https://grafana.com/docs/grafana/latest/setup-grafana/installation/helm/](https://grafana.com/docs/grafana/latest/setup-grafana/installation/helm/) for step-by-step instructions. By default, the Grafana service is accessible within the cluster unless it is exposed outside via the K8s `LoadBalancer` service type or an Ingress resource. So, let’s use the `kubectl port-forward` mechanism to connect to the Grafana console:\n\n```", "```\n\n\t\t\tNow, open [http://localhost:8080/](http://localhost:8080/) in your browser to access the Grafana console. When prompted for credentials, note that a default admin user is automatically created during the Helm chart installation. You can retrieve the credentials from the K8s secret named `kube-prometheus-stack-grafana` in the `monitoring` namespace by running the following command:\n\n```", "```\n\n\t\t\tOnce logged in to the Grafana console, navigate to **Connections** | **Data sources** in the left side menu bar to view and manage the connected data sources. You will notice the local Prometheus server is already added to the data sources, as shown in *Figure 12**.9*.\n\n\t\t\t\t\t![Figure 12.9 – Grafana connected data sources](img/B31108_12_09.jpg)\n\n\t\t\tFigure 12.9 – Grafana connected data sources\n\t\t\tAs Grafana has access to Prometheus metrics, let’s start exploring the Grafana dashboards. Navigate to `kube-prometheus-stack` installation, as shown in *Figure 12**.10*.\n\n\t\t\t\t\t![Figure 12.10 – Grafana dashboards list](img/B31108_12_10.jpg)\n\n\t\t\tFigure 12.10 – Grafana dashboards list\n\t\t\tThese Grafana dashboards provide preconfigured monitoring views tailored for various K8s components, such as **CoreDNS**, **API server**, **Namespace**, **Pod**, **Workload**, **Node**, **Scheduler**, **Controller Manager**, and **kubelet**. You can select and explore these dashboards to gain insights into the performance, resource usage, and health of the K8s environments. For example, select the **Kubernetes** | **Compute Resources** | **Cluster** dashboard to view an overview of resource utilization, including CPU, memory, and storage metrics across the entire K8s cluster, helping you monitor and optimize cluster performance, as shown in *Figure 12**.11*.\n\n\t\t\t\t\t![Figure 12.11 – Kubernetes Compute Resources dashboard](img/B31108_12_11.jpg)\n\n\t\t\tFigure 12.11 – Kubernetes Compute Resources dashboard\n\t\t\tIn this dashboard, we can see the CPU, memory quota, and usage metrics aggregated by each K8s namespace at the cluster level.\n\t\t\tWhile the default dashboards offer comprehensive insights into core K8s components, you may also need visibility into specialized resources, depending on your workload. Now, let’s take a look at how to visualize GPU metrics using the NVIDIA DCGM exporter dashboard.\n\t\t\tNVIDIA DCGM dashboard\n\t\t\tEarlier in this chapter, we enabled metrics collection from the NVIDIA DCGM exporter add-on using Prometheus Service Monitor resources. Now, we will visualize these metrics using Grafana dashboards. NVIDIA published a Grafana dashboard at [https://grafana.com/grafana/dashboards/12239-nvidia-dcgm-exporter-dashboard/](https://grafana.com/grafana/dashboards/12239-nvidia-dcgm-exporter-dashboard/) to monitor GPU utilization metrics.\n\t\t\tYou can import this dashboard to your Grafana instance using the instructions at [https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/import-dashboards/](https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/import-dashboards/). Once the import is successful, you will be able to visualize the GPU metrics, as shown in *Figure 12**.12*.\n\n\t\t\t\t\t![Figure 12.12 – DCGM exporter Grafana dashboard](img/B31108_12_12.jpg)\n\n\t\t\tFigure 12.12 – DCGM exporter Grafana dashboard\n\t\t\tThis dashboard provides real-time visibility into key GPU performance metrics such as temperature, power usage, clock speeds, and utilization. With this information, you can quickly identify performance bottlenecks, detect potential issues, and optimize resource usage.\n\t\t\tIn addition to dashboards, we can also define Prometheus alerting rules to monitor GPU health and performance. For example, we can create a `gpu-rules.yaml` from our GitHub repository at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch12/monitoring/gpu-rules.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch12/monitoring/gpu-rules.yaml) and run the following command to configure them in our setup:\n\n```", "```\n\n\t\t\tOnce deployed, we can visualize the rules in the Prometheus or Grafana console. In the Grafana console, navigate to **Alerting** | **Alert rules** in the left side menu bar to view the status of the alert rules, as shown in *Figure 12**.13*.\n\n\t\t\t\t\t![Figure 12.13 – NVIDIA GPU alert rules in the Grafana console](img/B31108_12_13.jpg)\n\n\t\t\tFigure 12.13 – NVIDIA GPU alert rules in the Grafana console\n\t\t\tAs shown in *Figure 12**.13*, one of the GPU alerting rules is in a **Firing** state due to low GPU utilization on one of our worker nodes. To investigate further, we can expand the alert rule to view detailed information such as the worker node, GPU identifier, and the associated K8s Pod, as illustrated in *Figure 12**.14*.\n\n\t\t\t\t\t![Figure 12.14 – GPU alert rule details in the Grafana console](img/B31108_12_14.jpg)\n\n\t\t\tFigure 12.14 – GPU alert rule details in the Grafana console\n\t\t\tWhile the DCGM dashboard provides deep visibility into GPU performance, it’s also important to monitor the higher-level services that rely on these resources, especially in GenAI workloads. One such example is Ray Serve, which plays a key role in serving models such as Llama 3 in our deployment. Let’s now set up a dedicated Grafana dashboard to monitor the performance and resource usage of Ray Serve components.\n\t\t\tRay Serve dashboard\n\t\t\tIn [*Chapter 11*](B31108_11.xhtml#_idTextAnchor145), we deployed a Ray cluster in our EKS cluster and used the Ray Serve framework to expose the Llama 3 model. Earlier in this chapter, we also created Prometheus Service and Pod Monitor resources to gather metrics from both the Ray cluster and Ray Serve deployments. Now, we will create a `ray-serve-dashboard.json`) from our GitHub repository at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch12/dashboards](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch12/dashboards).\n\t\t\tOpen the Grafana console, navigate to the `ray-serve-dashboard.json` file from your local filesystem and click the **Import** button. Once imported, select **Ray Serve Dashboard** from the dashboards list to view real-time information about your Ray Serve deployments, helping you identify bottlenecks and optimize performance, as shown in *Figure 12**.15*.\n\n\t\t\t\t\t![Figure 12.15 – Ray Serve Grafana dashboard](img/B31108_12_15.jpg)\n\n\t\t\tFigure 12.15 – Ray Serve Grafana dashboard\n\t\t\tJust like GPU alerting rules in the previous section, we can also define Prometheus alerting rules to monitor the health and performance of Ray Serve deployments. For example, we can configure a Prometheus rule to trigger alerts based on conditions such as high error rates, increased latency, Ray worker node failures, response latency spikes, or low throughput. To create these rules, download the `ray-serve-rules.yaml` file from our GitHub repository at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch12/monitoring/ray-serve-rules.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch12/monitoring/ray-serve-rules.yaml) and run the following command to configure them in our setup:\n\n```", "```\n\n\t\t\tOnce deployed, we can visualize the rules in the Prometheus or Grafana console. In the Grafana console, navigate to **Alerting** | **Alert rules** in the left side menu bar to view the status of the alert rules, as shown in *Figure 12**.16*.\n\n\t\t\t\t\t![Figure 12.16 – Ray Serve alert rules in Grafana](img/B31108_12_16.jpg)\n\n\t\t\tFigure 12.16 – Ray Serve alert rules in Grafana\n\t\t\tAs we have covered the different observability tools for monitoring GenAI workloads in K8s, let’s now explore how to extend these concepts for GenAI frameworks, such as LangChain.\n\t\t\tLangChain observability\n\t\t\t**LangChain** ([https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)) is a framework for building applications with LLMs, which we covered in [*Chapter 4*](B31108_04.xhtml#_idTextAnchor049). It integrates with various tools to enable observability and debugging.\n\t\t\tLangChain provides built-in capabilities to log and trace the execution of chains, agents, and tools. These features allow developers and operators to understand how prompts, responses, and workflows behave during execution. The `verbose=True` in chains, agents, or tools.\n\t\t\tLangChain can integrate with **LangChainTracer** to collect execution data, including steps, timing information, errors, and retries. The tracer can be used via **LangSmith** ([https://www.langchain.com/langsmith](https://www.langchain.com/langsmith)) or deployed as a self-hosted server in K8s.\n\t\t\tThe following code snippet defines a custom debugging and observability callback handler for LangChain, which helps track the execution flow of a chain. It logs when a chain starts, when it completes, and the time taken for execution, and handles errors gracefully:\n\n```", "```\n\n\t\t\tThe `DebugCallbackHandler` class extends `BaseCallbackHandler`, making it compatible with LangChain’s callback system. The constructor initializes a `self.start_time` variable, which is used to track the execution duration.\n\t\t\tThe `on_chain_error` method is called when an error occurs during chain execution. It prints an error message along with details about the encountered exception.\n\t\t\tThis callback handler is useful for debugging and performance monitoring in LangChain applications by providing real-time logs for execution tracking, timing analysis, and error handling.\n\t\t\tWhile LangChain’s built-in logging and tracing capabilities provide a solid foundation for understanding the internal workings of LLM chains and agents, there are scenarios where more advanced observability tools are needed, especially for production-grade applications. This is where platforms such as LangFuse come into play, offering richer insights, distributed tracing, and powerful dashboards tailored for LLM workflows. Let’s take a closer look at how LangFuse enhances observability for GenAI applications in the next section.\n\t\t\tLangFuse\n\t\t\t**LangFuse** ([https://langfuse.com/](https://langfuse.com/)) is an open source observability and monitoring platform tailored for LLM applications. It provides deep insights into the execution of AI workflows by tracking user interactions, prompts, responses, and application performance. LangFuse supports key observability features such as logging, tracing, metrics collection, and visualization, making it invaluable for debugging and optimizing LLM-based applications.\n\t\t\tLangFuse benefits from K8s-native capabilities such as scalability, auto-healing, and seamless integration with managed services. LangFuse collects and visualizes critical metrics and traces related to prompts, model latency, response accuracy, and system health. It supports distributed tracing, allowing developers to trace user interactions across multiple components, such as API gateways, vector databases, and LLM endpoints, to diagnose performance bottlenecks or errors.\n\t\t\tKey features of LangFuse include the following:\n\n\t\t\t\t*   It logs detailed information about requests, including the input prompt, LLM-generated responses, and associated metadata such as token usage and model-specific parameters\n\t\t\t\t*   It captures the end-to-end lifecycle of interactions, enabling you to monitor every step in workflows, from user input to database queries and LLM outputs\n\t\t\t\t*   It provides interactive dashboards to visualize system performance, latency trends, error rates, and other **key performance** **indicators** (**KPIs**)\n\t\t\t\t*   It links errors or delays in AI pipelines to specific users, prompts, or workflows for faster debugging and resolution\n\t\t\t\t*   It easily integrates with Prometheus, Grafana, OTel, and other K8s monitoring tools to enhance existing observability stacks\n\n\t\t\tFor K8s deployments, LangFuse offers flexibility in deployment configurations, enabling you to run the observability stack alongside your AI workloads. It is compatible with Helm charts, ensuring smooth deployment and configuration in cloud-native environments. Detailed deployment instructions and configurations for K8s and EKS are available in the LangFuse documentation at [https://langfuse.com/self-hosting/kubernetes-helm](https://langfuse.com/self-hosting/kubernetes-helm).\n\t\t\tIn this section, we explored various visualization and debugging tools for monitoring GenAI applications in K8s, including Grafana, LangChain, and LangFuse. We deployed Grafana in our EKS cluster and imported prebuilt dashboards to view the key performance metrics of various components, such as the API server, Ray Serve deployments, and so on. Additionally, LangChain and LangFuse provide advanced debugging and observability features for GenAI workloads, enabling you to trace LLM calls, monitor model outputs, and optimize prompt configurations.\n\t\t\tSummary\n\t\t\tIn this chapter, we covered key observability concepts for monitoring GenAI applications in K8s. We understood why monitoring is critical for optimizing GenAI workloads, examining both system-level metrics and application-specific signals. We explored a comprehensive monitoring framework using tools such as Prometheus for metrics collection, Grafana for visualization, and LangFuse and LangChain for debugging.\n\t\t\tIn K8s, various tools cater to different facets of the observability framework. Prometheus excels at collecting and querying time-series metrics, offering built-in alerting capabilities and seamless integration with K8s. Fluentd and Fluent Bit serve as a unified logging layer, collecting data from diverse sources and routing it to multiple destinations. OpenTelemetry provides a vendor-neutral set of APIs and libraries for generating and processing telemetry data, spanning metrics, logs, and traces.\n\t\t\tGrafana provides an intuitive interface to view and analyze metrics, logs, and traces, making it easy to detect anomalies and investigate performance bottlenecks. LangFuse specializes in detailed logging and observability of LLM-based requests, capturing prompts, responses, and metadata to facilitate faster debugging. LangChain offers a framework for orchestrating and experimenting with LLM workflows, helping us better understand and refine prompt engineering, chaining logic, and model responses.\n\t\t\tIn the next chapter, we will explore how to set up high availability and disaster recovery for GenAI applications on K8s.\n\n```"]