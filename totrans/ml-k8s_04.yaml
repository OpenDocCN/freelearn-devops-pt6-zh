- en: '*Chapter 5*: Data Engineering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data engineering, in general, refers to the management and organization of data
    and data flows across an organization. It involves data gathering, processing,
    versioning, data governance, and analytics. It is a huge topic that revolves around
    the development and maintenance of data processing platforms, data lakes, data
    marts, data warehouses, and data streams. It is an important practice that contributes
    to the success of **big data** and **machine learning** (**ML**) projects. In
    this chapter, you will learn about the ML-specific topics of data engineering.
  prefs: []
  type: TYPE_NORMAL
- en: A sizable number of ML tutorials/books start with a clean dataset and a CSV
    file to build your model against. The real world is different. Data comes in many
    shapes and sizes, and it is important that you have a well-defined strategy to
    harvest, process, and prepare data at scale. This chapter will discuss open source
    tools that can provide the foundations for data engineering in ML projects. You
    will learn how to install the open source toolsets on the Kubernetes platform
    and how these tools will enable you and your team to be more efficient and agile.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Keycloak for authentication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Open Data Hub components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding and using the JupyterHub IDE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the basics of Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how Open Data Hub provisions on-demand Apache Spark clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing and running a Spark application from Jupyter Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter includes some hands-on setup and exercises. You will need a running
    Kubernetes cluster configured with **Operator Lifecycle Manager**. Building such
    a Kubernetes environment is covered in [*Chapter 3*](B18332_03_ePub.xhtml#_idTextAnchor040),
    *Exploring Kubernetes*. Before attempting the technical exercises in this chapter,
    please make sure that you have a working Kubernetes cluster and **Open Data Hub**
    (**ODH**) installed on your Kubernetes cluster. Installing the ODH is covered
    in [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The Anatomy of a Machine
    Learning Platform*. You can find all the code associated with this book at [https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes](https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Keycloak for authentication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before you start using any component of your platform, you need to configure
    the authentication system to be associated with the platform components. As mentioned
    in [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The Anatomy of a Machine
    Learning Platform*, you will use Keycloak, an open source software to provide
    authentication services.
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, import the configuration from `chapter5/realm-export.json`,
    which is available in the code repository associated with this book. This file
    contains the configuration required to associate the OAuth2 capabilities for the
    platform components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though this book is not a Keycloak guide by any means, we will provide some
    basic definitions for you to understand the high-level taxonomy of the Keycloak
    server:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Realm**: A Keycloak realm is an object that manages the users, roles, groups,
    and client applications that belong to the same domain. One Keycloak server can
    have multiple realms, so you have multiple sets of configurations, such as one
    realm for internal applications and one for external applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clients**: Clients are entities that can request user authentication. A Keycloak
    client object is associated with a realm. All the applications in our platform
    that require **single sign-on** (**SSO**) will be registered as **clients** in
    the Keycloak server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Users and groups**: These two terms are self-explanatory, and you will be
    creating a new user in the following steps and using it to log into different
    software of the platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step is to configure Keycloak to provide OAuth capabilities to our
    ML platform component.
  prefs: []
  type: TYPE_NORMAL
- en: Importing the Keycloak configuration for the ODH components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will import the clients and group configurations onto
    the Keycloak server running on your Kubernetes cluster. The following steps will
    import everything onto the master realm of the Keycloak server:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to your Keycloak server using the username `admin` and the password
    `admin`. Click on the **Import** link on the left-hand sidebar under the **Manage**
    heading:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Keycloak Master realm'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Keycloak Master realm
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Select file** button on the screen, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Keycloak import configuration page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – Keycloak import configuration page
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the `chapter5/realm-export.json` file from the pop-up window. After
    that, select **Skip** for the **If a resource exists** drop-down options, and
    click **Import**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Keycloak import configuration page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Keycloak import configuration page
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that the records have been imported successfully onto your Keycloak
    server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Keycloak import configuration results page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 – Keycloak import configuration results page
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that there are four clients created by clicking on the **Clients**
    item on the left-hand side menu. The following client IDs should exist: **aflow**,
    **mflow**, **grafana**, and **jhub**. The **aflow** client is for the workflow
    engine of the platform, which is an instance of **Apache Airflow**. The **mflow**
    client is for the model registry and training tracker tool and is an instance
    of **MLflow**. The **grafana** client is for monitoring UI and is an instance
    of **Grafana**. And last, the **jhub** client is for the **JupyterHub** server
    instance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Keycloak clients page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – Keycloak clients page
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that a group called **ml-group** has been created by clicking on the
    **Groups** link on the left-hand panel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Keycloak groups page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – Keycloak groups page
  prefs: []
  type: TYPE_NORMAL
- en: You will use this user group to create a user of the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Great work! You have just configured multiple Keycloak clients for the ML platform.
    The next step is to create a user in Keycloak that you will be using for the rest
    of this book. It is important to note that Keycloak can be hooked with your enterprise
    directory or any other database and to use them as a source of the users. Keep
    in mind that the realm configuration we are using here is very basic and is not
    recommended for production use.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Keycloak user
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will create a new user and associate the newly created
    user with the group imported in the preceding section. Associating the user with
    the group gives the roles required for the different ODH software:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the left-hand side of the Keycloak page, click on the **Users** link to
    come to this page. To add a new user, click the **Add user** button on the right:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Keycloak users list'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 – Keycloak users list
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the username `mluser` and make sure the **User Enabled** and **Email Verified**
    toggle buttons are set to **ON**. In **Groups**, select the **ml-group** group
    and fill in the **Email**, **First Name**, and **Last Name** fields, as shown
    in *Figure 5.8*, and then hit the **Save** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Keycloak Add user page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8 – Keycloak Add user page
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Credentials** tab to set the password for your user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Keycloak Credentials page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9 – Keycloak Credentials page
  prefs: []
  type: TYPE_NORMAL
- en: Type in the password of your choice, then disable the **Temporary** flag, and
    hit the **Set Password** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You have just created and configured a user in Keycloak. Your Keycloak server
    is now ready to be used by the ML platform components. The next step is to explore
    the component of the platform that provides the main coding environment for all
    personas in the ML project.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring ODH components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The Anatomy of a Machine
    Learning Platform*, you have installed the ODH operator. Using the ODH operator,
    you will now configure an instance of ODH that will automatically install the
    components of the ML platform. ODH executes **Kustomize** scripts to install the
    components of the ML platform. As part of the code for this book, we have provided
    templates to install and configure all the components required to run the platform.
  prefs: []
  type: TYPE_NORMAL
- en: You can also configure what components ODH operators install for you through
    a `manifests` file. You can pass the specific configuration to the manifests and
    choose the components you need. One such manifest is available in the code repository
    of the book at `manifests/kfdef/ml-platform.yaml`. This YAML file is configured
    for the ODH operator to do its magic and install the software we need to be part
    of the platform. You will need to make some modifications to this file, as you
    will see in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 'This file defines the components of your platform and the location from where
    these components will get their settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name**: Defines the name of the component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path` property where you define the relative path location of the files required
    to configure this component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KEYCLOAK_URL` and `JUPYTERHUB_HOST` will need to be changed as per your configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`manifests/jupytherhub/overlays` folder in the code repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Repos**: This configuration section is specific to each manifest file and
    applies to all the components in the manifest. It defines the location and version
    of the Git repository that contains all the files being referred to by this manifest
    file. If you want the manifest to reference your own files for the installation,
    you need to refer here to the right Git repository (the repository that contains
    your files).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 5.10* shows the part of the manifest file that holds the definition
    of the JupyterHub component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – A component in the ODH manifest file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10 – A component in the ODH manifest file
  prefs: []
  type: TYPE_NORMAL
- en: You will use the provided manifest file to create an instance of the ML platform.
    You may also tweak configurations or add or remove components of the platform
    as you wish by modifying this file. However, for the exercises in the book, we
    do not recommend changing this unless you are instructed to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have seen the ODH manifest file, it's time to make good use of
    it to create your first ML platform on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Installing ODH
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we can install the data engineering components of the platform, we first
    need to create an instance of ODH. An ODH instance is a curated collection of
    related toolsets that serve as the components of an ML platform. Although the
    ML platform may contain components other than what is provided by ODH, it is fair
    to say that an instance of ODH is an instance of the ML platform. You may also
    run multiple instances of ODH on the same Kubernetes cluster as long as they run
    on their own isolated Kubernetes namespaces. This is useful when multiple teams
    or departments in your organization are sharing a single Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps you need to follow to create an instance of ODH
    on your Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new namespace in your Kubernetes cluster using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – New namespace in your Kubernetes cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.11 – New namespace in your Kubernetes cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that the ODH operator is running by issuing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response. Make sure the status says `Running`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Status of the ODH operator'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.12 – Status of the ODH operator
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the IP address of your `minikube` environment. This IP address will be
    used to create ingress for different components of the platform the same way we
    did for Keycloak. Note that your IP may be different for each `minikube` instance
    depending on your underlying infrastructure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command should give you the IP address of your `minikube` cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `manifests/kfdef/ml-platform.yaml` file and change the value of the
    following parameters to a NIP (`nip.io`) domain name of your `minikube` instance.
    Only replace the IP address part of the domain name. For example, `KEYCLOAK_URL
    keycloak.<IP Address>.nip.io` should become `keycloak.192.168.61.72.nip.io`. Note
    that these parameters may be referenced in more than one place in this file. In
    a full Kubernetes environment, `<IP Address>` should be the domain name of your
    Kubernetes cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`KEYCLOAK_URL`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`JUPYTERHUB_HOST`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AIRFLOW_HOST`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`MINIO_HOST`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`MLFLOW_HOST`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`GRAFANA_HOST`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Apply the manifest file to your Kubernetes cluster using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Result from applying manifests for the ODH components'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.13 – Result from applying manifests for the ODH components
  prefs: []
  type: TYPE_NORMAL
- en: 'Start watching the pods being created in the `ml-workshop` namespace by using
    the following command. It will take a while for all the components to be installed.
    After several minutes, all the Pods will be in a running state. While the pods
    are being created, you may see some pods throw errors. This is normal because
    some pods are dependent on other pods. Be patient as all the components come together
    and the pods will come into a running state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response when all the pods are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – CLI response showing the ODH components running on the Kubernetes
    cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.14 – CLI response showing the ODH components running on the Kubernetes
    cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what does this command do? The `kfdef` `ml-workshop` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You should see all the objects that were created in the `ml-workshop` namespace
    by the ODH operator.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have just created a fresh instance of ODH. Now that you
    have seen the process of creating an instance of the ML platform from a manifest
    file, it is time to take a look at each of the components of the platform that
    the data engineers will use for their activities.
  prefs: []
  type: TYPE_NORMAL
- en: Minikube Using Podman Driver
  prefs: []
  type: TYPE_NORMAL
- en: Note that for some `minikube` setups that use `podman` drivers in Linux, the
    Spark operator may fail due to the limit of the number of threads. To solve this
    problem, you need to use a `kvm2` driver in your `minikube` configuration. You
    can do this by adding the `--driver=kvm2` parameter to your `minikube start` command.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and using JupyterHub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jupyter Notebook has become an extremely popular tool for writing code for ML
    projects. JupyterHub is a software that facilitates the self-service provisioning
    of computing environments that includes spinning up pre-configured Jupyter Notebook
    servers and provisioning the associated compute resources on the Kubernetes platform.
    On-demand end users such as data engineers and data scientists can provision their
    own instances of Jupyter Notebook dedicated only to them. If a requesting user
    already has his/her own running instance of Jupyter Notebook, the hub will just
    direct the user to the existing instance, avoiding duplicated environments. From
    the end user's perspective, the whole interaction is seamless. You will see this
    in the next section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: When a user requests an environment in JupyterHub, they are also given the option
    to choose a pre-configured sizing of hardware resources such as CPU, memory, and
    storage. This allows for a flexible way for developers, data engineers, and data
    scientists to provision just the right amount of computing resources for a given
    task. This dynamic allocation of resources is facilitated by the underlying Kubernetes
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: Different users may require different frameworks, libraries, and flavors of
    coding environments. Some data scientists may want to use TensorFlow while others
    want to use scikit-learn or PyTorch. Some data engineers may prefer to use pandas
    while some may need to run their data pipelines in PySpark. In JupyterHub, they
    can configure multiple pre-defined environments for such scenarios. Users can
    then select a predefined configuration when they request a new environment. These
    predefined environments are actually container images. This means that the platform
    operator or platform administrator can prepare several predefined container images
    that will serve as the end user's computing environment. This feature also enables
    the standardization of environments. How many times do you have to deal with different
    versions of the libraries on different developer computers? The standardization
    of environments can reduce the number of problems related to library version inconsistencies
    and generally reduce the *it works on my machine* issues.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.15* shows the three-step process of provisioning a new JupyterHub
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – Workflow for creating a new environment in JupyterHub'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.15 – Workflow for creating a new environment in JupyterHub
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know what JupyterHub can do, let's see it in action.
  prefs: []
  type: TYPE_NORMAL
- en: Validating the JupyterHub installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every data engineer in the team follows a simple and standard workflow of provisioning
    an environment. No more manual installations and fiddling with their workstation
    configurations. This is great for autonomous teams and will definitely help improve
    your team's velocity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ODH operator has already installed the JupyterHub for you in the previous
    sections. Now, you will spin up a new Jupyter Notebook environment, as a data
    engineer, and write your data pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the ingress objects created in your Kubernetes environment using the following
    command. We are running this command to find the URL of JupyterHub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following example response. Take note of the JupyterHub
    URL as displayed in the `HOSTS` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – All ingresses in your cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.16 – All ingresses in your cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a browser from the same machine where minikube is running and navigate
    to the JupyterHub URL. The URL looks like https://jupyterhub.<MINIKUBE IP ADDRESS>.nip.io.
    This URL will take you to the Keycloak login page to perform SSO authentication.
    Make sure that you replace the IP address with your minikube IP address in this
    URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.17 – SSO challenge for JupyterHub'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_017.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.17 – SSO challenge for JupyterHub
  prefs: []
  type: TYPE_NORMAL
- en: Type `mluser` for the username, then type whatever password you have set up
    for this user, and click **Sign In**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will see the landing page of the JupyterHub server where it allows you to
    select the notebook container image that you want to use and also a predefined
    size of computing resources you need.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook image section contains the standard notebooks that you have provisioned
    using the ODH manifests from the `manifests/jupyterhub-images` folder of the code
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: The container size drop-down allows you to select the right size of environment
    for your need. This configuration is also controlled via the `manifests/jupyterhub/jupyterhub/overlays/mlops/jupyterhub-singleuser-profiles-sizes-configmap.yaml`
    manifest file.
  prefs: []
  type: TYPE_NORMAL
- en: We encourage you to look into these files to familiarize yourself with what
    configuration you can set for each manifest.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – JupyterHub landing page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_018.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.18 – JupyterHub landing page
  prefs: []
  type: TYPE_NORMAL
- en: Select **Base Elyra Notebook Image** and the **Default** container size and
    hit **Start server**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that a new pod has been created for your user by issuing the following
    command. Jupyter Notebook instance names start with `jupyter-nb-` and are suffixed
    with the username of the user. This allows for a unique name of notebook pods
    for each user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19 – Jupyter Notebook pod created by JupyterHub'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_019.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.19 – Jupyter Notebook pod created by JupyterHub
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You are now running your own self-provisioned Jupyter Notebook
    server on the Kubernetes platform.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.20 – Jupyter Notebook landing page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_020.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.20 – Jupyter Notebook landing page
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s stop the notebook server. Click on the **File > Hub Control Panel**
    menu option to go to the **Hub Control Panel** page shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.21 – Menu option to see the Hub Control Panel'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_021.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.21 – Menu option to see the Hub Control Panel
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Stop My Server** button. This is how you stop your instance of
    Jupyter Notebook. You may want to start it back again later for the next steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.22 – Hub Control Panel'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_022.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.22 – Hub Control Panel
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that a new pod has been destroyed for your user by issuing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There should be no output for this command because the Jupyter Notebook pod
    has been destroyed by JupyterHub.
  prefs: []
  type: TYPE_NORMAL
- en: We leave it up to you to explore the different bits of the configuration of
    the notebook in your environment. You will write code using this Jupyter notebook
    in the later sections of this chapter and the next few chapters of this book,
    so if you just want to continue reading, you will not miss anything.
  prefs: []
  type: TYPE_NORMAL
- en: Running your first Jupyter notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that your Jupyter notebook is running, it is time to write the `Hello World!`
    program. In the code repository of this book, we have provided one such program,
    and in the following steps, you will check out the code using Git and run the
    program. Before we start these steps, make sure that you can access your Jupyter
    notebook using the browser, as mentioned in the preceding section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the Git icon on the left-hand side menu on your Jupyter notebook.
    The icon is the third from the top. It will display three buttons for different
    operations. Click on the **Clone a Repository** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.23 – Git operations in the Jupyter notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_024.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.23 – Git operations in the Jupyter notebook
  prefs: []
  type: TYPE_NORMAL
- en: In the **Clone a repo** pop-up box, type in the location of the code repository
    of this book, [https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git](https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git),
    and hit **CLONE**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.24 – Git clone a repo in the Jupyter notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_025.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.24 – Git clone a repo in the Jupyter notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see that the code repository is cloned onto your Jupyter notebook''s
    file system. As shown in *Figure 5.25*, navigate to the `chapter5/helloworld.ipynb`
    file and open it in your notebook. Click on the little play icon on the top bar
    to run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.25 – Notebook on your Jupyter environment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_026.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.25 – Notebook on your Jupyter environment
  prefs: []
  type: TYPE_NORMAL
- en: Et voila! You have just executed a Python code in your own self-provisioned
    Jupyter Notebook server running on Kubernetes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shut down your notebook by selecting the **File > Hub Control Panel** menu option.
    Click on the **Stop My Server** button to shut down your environment. Note that
    ODH will save your disk and next time you start your notebook, all your saved
    files will be available.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations! Now, you can run your code on the platform. Next, we'll get
    some basics refreshed for the Apache Spark engine.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basics of Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is an open source data processing engine designed for distributed
    large-scale processing of data. This means that if you have smaller datasets,
    say 10s or even a few 100s of GB, a tuned traditional database may provide faster
    processing times. The main differentiator for Apache Spark is its capability to
    perform in-memory intermediate computations, which makes Apache Spark much faster
    than Hadoop MapReduce.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark is built for speed, flexibility, and ease of use. Apache Spark
    offers more than 70 high-level data processing operators that make it easy for
    data engineers to build data applications, so it is easy to write data processing
    logic using Apache Spark APIs. Being flexible means that Spark works as a unified
    data processing engine and works on several types of data workloads such as batch
    applications, streaming applications, interactive queries, and even ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.26* shows the Apache Spark components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26 – Apache Spark components'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_027.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.26 – Apache Spark components
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Apache Spark job execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most data engineers now know that Apache Spark is a massively parallel data
    processing engine. It is one of the most successful projects of the Apache Software
    Foundation. Spark traditionally runs on a cluster of multiple **virtual machines**
    (**VMs**) or bare metal servers. However, with the popularity of containers and
    Kubernetes, Spark added support for running Spark clusters on containers on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: There are two most common ways of running Spark on Kubernetes. The first, and
    the native way, is by using the Kubernetes engine itself to orchestrate the Kubernetes
    worker pods. In this approach, the Spark cluster instance is always running and
    the Spark applications are submitted to the Kubernetes API that will schedule
    the submitted application. We will not dig deeper into how this is implemented.
    The second approach is through Kubernetes operators. Operators take advantage
    of the Kubernetes CRDs to create Spark objects natively in Kubernetes. In this
    approach, the Spark cluster is created on the fly by the Spark operator. Instead
    of submitting a Spark application to an existing cluster, the operator spins up
    spark clusters on-demand.
  prefs: []
  type: TYPE_NORMAL
- en: A Spark cluster follows a manager/worker architecture. The Spark cluster manager
    knows where the workers are located, and the resources available for the worker.
    The Spark cluster manages the resources for the cluster of worker nodes where
    your application will run. Each worker has one or more executors that run the
    assigned jobs through an executor.
  prefs: []
  type: TYPE_NORMAL
- en: Spark applications have two parts, the driver component, and the data processing
    logic. A driver component is responsible for executing the flow of data processing
    operations. The driver run first talks to the cluster manager to find out what
    worker nodes will run the application logic. The driver transforms all the application
    operations into tasks, schedules them, and assigns tasks directly to the executor
    processes on the worker node. One executor can run multiple tasks that are associated
    with the same Spark context.
  prefs: []
  type: TYPE_NORMAL
- en: If your application requires you to collect the computed result and merge them,
    the driver is the one who will be responsible for this activity. As a data engineer,
    all this activity is abstracted from you via the SparkSession object. You only
    need to write the data processing logic. Did we mention Apache Spark aims to be
    simple?
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.27* shows the relationship between the Spark driver, Spark cluster
    manager, and Spark worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.27 – Relationship between Apache Spark components'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_028.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.27 – Relationship between Apache Spark components
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how ODH provisions Apache Spark cluster on-demand
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have talked about how the ODH allows you to create a dynamic and flexible
    development environment to write code such as data pipelines using Jupyter Notebook.
    We have noticed that data developers need to interact with IT to get time on the
    data processing clusters such as Apache Spark. These interactions reduce the agility
    of the team, and this is one of the problems the ML platform solves. To adhere
    to this scenario, ODH provides the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: A Spark operator that spawns the Apache Spark cluster. For this book, we have
    forked the original Spark operator provided by ODH and radanalytics to adhere
    to the latest changes to the Kubernetes API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A capability in JupyterHub to issue a request for a new Spark cluster to the
    Spark operator when certain notebook environments are created by the user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a data engineer, when you spin up a new notebook environment using certain
    notebook images, JupyterHub not only spawns a new notebook server, it also creates
    the Apache Spark cluster dedicated for you through the Spark operator.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Spark cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's first see how the Spark operator works on the Kubernetes cluster. ODH
    creates the Spark controller. You can see the configuration in the `chapter5/ml-platform.yaml`
    file under the name `radanalyticsio-spark-cluster`, as shown in *Figure 5.28*.
    You can see this is another set of Kubernetes YAML files that defines the `manifests/radanalyticsio`
    folder in the code repository of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.28 – Snippet of the section of the manifest that installs the Spark
    operator'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_029.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.28 – Snippet of the section of the manifest that installs the Spark
    operator
  prefs: []
  type: TYPE_NORMAL
- en: 'When you need to spin up an Apache Spark cluster, you can do this by creating
    a Kubernetes custom resource called **SparkCluster**. Upon receiving the request,
    the Spark operator will provision a new Spark cluster as per the required configuration.
    The following steps will show you the steps for provisioning a Spark cluster on
    your platform:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that the Spark operator pod is running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.29 – Spark operator pod'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_030.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.29 – Spark operator pod
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a simple Spark cluster with one worker node using the file available
    at `chapter5/simple-spark-cluster.yaml`. You can see that this file is requesting
    a Spark cluster with one master and one worker node. Through this custom resource,
    you can set several Spark configurations, as we shall see in the next section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.30 – Spark custom resource'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_031.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.30 – Spark custom resource
  prefs: []
  type: TYPE_NORMAL
- en: 'Create this Spark cluster custom resource in your Kubernetes cluster by running
    the following command. The Spark operator constantly scans for this resource in
    the Kubernetes platform and automatically creates a new instance of Apache Spark
    cluster for each given Spark cluster custom resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.31 – Response to creating a Spark cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_032.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.31 – Response to creating a Spark cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that the Spark cluster pods are running in your cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response. There are two pods created by the Spark
    operator, one for the Spark master node and another for the worker node. The number
    of worker pods depends on the value of the `instances` parameters in the `SparkCluster`
    resource. It may take some time for the pods to come to a running state the first
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.32 – List of running Spark cluster pods'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_033.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.32 – List of running Spark cluster pods
  prefs: []
  type: TYPE_NORMAL
- en: Now, you know how the Spark operator works on the Kubernetes cluster. The next
    step is to see how JupyterHub is configured to request the cluster dynamically
    while provisioning a new notebook for you.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how JupyterHub creates a Spark cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Simply put, JupyterHub does what you did in the preceding section. JupyterHub
    creates a `SparkCluster` resource in Kubernetes so that the Spark operator can
    provision the Apache Spark cluster for your use. This `SparkCluster` resource
    configuration is a Kubernetes `ConfigMap` file and can be found at `manifests/jupyterhub/jupyterhub/base/jupyterhub-spark-operator-configmap.yaml`.
    Look for `sparkClusterTemplate` in this file, as shown in *Figure 5.33*. You can
    see that it looks like the file you have created in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.33 – JupyterHub template for Spark resources'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_034.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.33 – JupyterHub template for Spark resources
  prefs: []
  type: TYPE_NORMAL
- en: Some of you might have noticed that this is a template, and it needs the values
    for specific variables mentioned in this template. Variables such as `{{ user
    }}` and `{{ worker_nodes }}` and so on. Recall that we have mentioned that JupyterHub
    creates the `SparkCluster` request while it is provisioning a container for your
    notebook. JupyterHub uses this file as a template and fills in the values while
    creating your notebook. How does JupyterHub decide to create a Spark cluster?
    This configuration is called `ConfigMap` file in `manifests/jupyterhub/jupyterhub/overlays/spark3/jupyterhub-singleuser-profiles-configmap.yaml`.
    This looks like the file shown in *Figure 5.33*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that the `image` field specifies the name of the container image
    on which this profile will be triggered. So, as a data engineer, when you select
    this notebook image from the JupyterHub landing page, JupyterHub will apply this
    profile. The second thing in the profile is the `env` section, which specifies
    the environment variables that will be pushed to the notebook container instance.
    The `configuration` object defines the values that will be applied to the template
    that is mentioned in the `resources` key:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.34 – JupyterHub profile for Spark resources'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_035.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.34 – JupyterHub profile for Spark resources
  prefs: []
  type: TYPE_NORMAL
- en: As you may appreciate, there is a lot of work done behind the scenes to make
    a streamlined experience for you and your team, and in the true sense of open
    source, you can configure everything and even give back to the project if you
    come up with any modifications or new features.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will see how easy it is to write and run a Spark application
    on your platform running these components.
  prefs: []
  type: TYPE_NORMAL
- en: Writing and running a Spark application from Jupyter Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before you run the following steps, make sure that you grasped the components
    and their interactions that we have introduced in the previous section of this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that the Spark operator pod is running by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.35 – Spark operator pod'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_036.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.35 – Spark operator pod
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that the JupyterHub pod is running by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.36 – JupyterHub pod'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_037.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.36 – JupyterHub pod
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you start the notebook, let''s delete the Spark cluster you have created
    in the previous sections by running the following command. This is to demonstrate
    that JupyterHub will automatically create a new instance of Spark cluster for
    you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Log in to your JupyterHub server. Refer to the *Validating JupyterHub configuration*
    section earlier in this chapter. You will get the landing page of your server.
    Select the `manifests/jupyterhub/jupyterhub/overlays/spark3/jupyterhub-singleuser-profiles-configmap.yaml`
    file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on **Start server**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.37 – JupyterHub landing page showing Elyra Notebook Image with Spark'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_038.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.37 – JupyterHub landing page showing Elyra Notebook Image with Spark
  prefs: []
  type: TYPE_NORMAL
- en: The notebook you have just started will also trigger the creation of a dedicated
    Spark cluster for you. It may take some time for the notebook to start because
    it has to wait for the Spark cluster to be ready.
  prefs: []
  type: TYPE_NORMAL
- en: Also, you may have noticed that the image you have configured in the `jupyterhub-singleuser-profiles-configmap.yaml`
    file is `quay.io/ml-aml-workshop/elyra-spark:0.0.4`, while the name we have selected
    is `manifests/jupyterhub-images/elyra-notebook-spark3-imagestream.yaml` file.
    You will find the descriptive text displayed on the JupyterHub landing page coming
    from the *annotations* section of this file. If you want to add your own images
    with specific libraries, you can just add another file here and it will be available
    for your team. This feature of JupyterHub enables the standardization of notebook
    container images, which allows everyone in the teams to have the same environment
    configurations and the same set of libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the notebook has started, validate that the Spark cluster is provisioned
    for you. Note that this is the Spark cluster for the user of this notebook and
    is dedicated to this user only:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response. The response contains a notebook pod
    and two Spark pods; the one with a little `-m` character is the master, while
    the other is the worker. Notice how your username (`mluser`) is associated with
    the pod names:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.38 – Jupyter Notebook and Spark cluster pods'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_039.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.38 – Jupyter Notebook and Spark cluster pods
  prefs: []
  type: TYPE_NORMAL
- en: Now, everyone in your team will get their own developer environment with dedicated
    Spark instances to write and test the data processing code.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark provides a UI through which you can monitor applications and data
    processing jobs. The ODH-provisioned Spark cluster provides this GUI, and it is
    available at `https://spark-cluster-mluser.192.168.61.72.nip.io`. Make sure to
    change the IP address to your minikube IP address. You may also notice that the
    username you have used to log in to JupyterHub, `mluser`, is part of the URL.
    If you have used a different username, you may need to adjust the URL accordingly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.39 – Spark UI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_040.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.39 – Spark UI
  prefs: []
  type: TYPE_NORMAL
- en: The preceding UI mentions that you have one worker in the cluster, and you can
    click on the worker node to find out the executors running inside the worker node.
    If you want to refresh your knowledge of the Spark cluster, please refer to the
    *Understanding the basics of Apache Spark* section earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `chapter5/hellospark.ipynb` file from your notebook. This is quite
    a simple job that calculates the square of the given array. Remember that Spark
    will automatically schedule the job and distribute it among executors. The notebook
    here is the Spark Driver program, which talks to the Spark cluster, and all of
    this is abstracted via the `SparkSession` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the second code cell of this notebook, you are creating a `SparkSession`
    object. The `getOrCreateSparkSession` utility function will connect to the Spark
    cluster provisioned for you by the platform.
  prefs: []
  type: TYPE_NORMAL
- en: The last cell is where your data processing logic resides. In this example,
    the logic was to take the data and calculate the square of each element in the
    array. Once the data is processed, the `collect` method will bring the response
    to the driver that is running in the Spark application in your notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.40 – A notebook with a simple Spark application'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_041.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.40 – A notebook with a simple Spark application
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Run > Run All cells** menu option, and the notebook will connect
    to the Spark cluster, and submit and execute your job.
  prefs: []
  type: TYPE_NORMAL
- en: While the job is progressing, open the Spark UI at [https://spark-cluster-mluser.192.168.61.72.nip.io](https://spark-cluster-mluser.192.168.61.72.nip.io).
    Remember to adjust the IP address as per your settings, and click on the table
    with the **Application ID** heading under the **Running Applications** heading
    on this page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.41 – Apache Spark UI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_042.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.41 – Apache Spark UI
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the details page of the Spark application. Note that the application
    title, **Hello from ODH**, has been set up in your notebook. Click on the **Application
    Detail UI** link:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.42 – Spark UI showing the submitted Spark job'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_043.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.42 – Spark UI showing the submitted Spark job
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see a page showing the detailed metrics of the job that you have
    just executed on the Spark cluster from your Jupyter notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.43 – Spark UI showing the submitted job details'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_044.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.43 – Spark UI showing the submitted job details
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you are done with your work, go to the **File > Hub Control Panel** menu
    option and click on the **Stop My Server** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.44 – Jupyter Notebook control panel'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_045.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.44 – Jupyter Notebook control panel
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that the Spark cluster has been terminated by issuing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should not see a response because the pods are terminated by the Spark operator
    on your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: You have finally run a basic data processing job in an on-demand ephemeral Spark
    cluster that is running on Kubernetes. Note that you have done all this from a
    Jupyter notebook running on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: With this capability in the platform, data engineers can perform huge data processing
    tasks directly from the browser. This capability also allows them to collaborate
    easily with each other to provide transformed, cleaned, high-quality data for
    your ML project.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have just created your first ML platform. You have configured
    the ODH components via the ODH Kubernetes operator. You have seen how a data engineer
    persona will use JupyterHub to provision the Jupyter notebook and the Apache Spark
    cluster instance while the platform provides the provisioning of the environments
    automatically. You have also seen how the platform enables standardization of
    the operating environment via the container images, which bring consistency and
    security. You have seen how a data engineer could run Apache Spark jobs from the
    Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: All these capabilities allow the data engineer to work autonomously and in a
    self-serving fashion. You have seen that all these components were available autonomously
    and on-demand. The elastic and self-serving nature of the platform will allow
    teams to be more productive and agile while responding to the ever-changing requirements
    of the data and the ML world.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will see how data scientists can benefit from the platform
    and be more efficient.
  prefs: []
  type: TYPE_NORMAL
