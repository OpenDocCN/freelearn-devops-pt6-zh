- en: '*Chapter 5*: Data Engineering'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第5章*：数据工程'
- en: Data engineering, in general, refers to the management and organization of data
    and data flows across an organization. It involves data gathering, processing,
    versioning, data governance, and analytics. It is a huge topic that revolves around
    the development and maintenance of data processing platforms, data lakes, data
    marts, data warehouses, and data streams. It is an important practice that contributes
    to the success of **big data** and **machine learning** (**ML**) projects. In
    this chapter, you will learn about the ML-specific topics of data engineering.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程通常是指在组织中管理和组织数据及数据流的过程。它涉及数据收集、处理、版本控制、数据治理和分析。这是一个庞大的主题，围绕数据处理平台、数据湖、数据集市、数据仓库和数据流的开发与维护展开。数据工程是一个重要的实践，它为**大数据**和**机器学习**（**ML**）项目的成功做出了贡献。本章将介绍数据工程在机器学习中的具体应用。
- en: A sizable number of ML tutorials/books start with a clean dataset and a CSV
    file to build your model against. The real world is different. Data comes in many
    shapes and sizes, and it is important that you have a well-defined strategy to
    harvest, process, and prepare data at scale. This chapter will discuss open source
    tools that can provide the foundations for data engineering in ML projects. You
    will learn how to install the open source toolsets on the Kubernetes platform
    and how these tools will enable you and your team to be more efficient and agile.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习教程/书籍从一个干净的数据集和 CSV 文件开始，构建您的模型。现实世界则不同。数据呈现出多种形态和规模，因此您需要有一个明确的策略来收集、处理并大规模准备数据。本章将讨论开源工具，这些工具可以为机器学习项目中的数据工程奠定基础。您将学习如何在
    Kubernetes 平台上安装这些开源工具集，以及这些工具如何帮助您和您的团队提高效率和灵活性。
- en: 'In this chapter, you will learn about the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习以下内容：
- en: Configuring Keycloak for authentication
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 Keycloak 进行身份验证
- en: Configuring Open Data Hub components
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 Open Data Hub 组件
- en: Understanding and using the JupyterHub IDE
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解并使用 JupyterHub IDE
- en: Understanding the basics of Apache Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Apache Spark 的基础知识
- en: Understanding how Open Data Hub provisions on-demand Apache Spark clusters
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Open Data Hub 如何按需提供 Apache Spark 集群
- en: Writing and running a Spark application from Jupyter Notebook
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Jupyter Notebook 编写并运行 Spark 应用程序
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter includes some hands-on setup and exercises. You will need a running
    Kubernetes cluster configured with **Operator Lifecycle Manager**. Building such
    a Kubernetes environment is covered in [*Chapter 3*](B18332_03_ePub.xhtml#_idTextAnchor040),
    *Exploring Kubernetes*. Before attempting the technical exercises in this chapter,
    please make sure that you have a working Kubernetes cluster and **Open Data Hub**
    (**ODH**) installed on your Kubernetes cluster. Installing the ODH is covered
    in [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The Anatomy of a Machine
    Learning Platform*. You can find all the code associated with this book at [https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes](https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括一些动手设置和练习。您将需要一个已配置好**操作生命周期管理器（Operator Lifecycle Manager）**的 Kubernetes
    集群。如何构建这样的 Kubernetes 环境已在[*第3章*](B18332_03_ePub.xhtml#_idTextAnchor040)《探索 Kubernetes》中介绍。在进行本章的技术练习之前，请确保您已经在
    Kubernetes 集群上搭建了运行中的 Kubernetes 集群，并安装了**Open Data Hub**（**ODH**）。ODH 的安装可参考[*第4章*](B18332_04_ePub.xhtml#_idTextAnchor055)《机器学习平台的构成》。您可以在[https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes](https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes)找到本书的所有相关代码。
- en: Configuring Keycloak for authentication
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 Keycloak 进行身份验证
- en: Before you start using any component of your platform, you need to configure
    the authentication system to be associated with the platform components. As mentioned
    in [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The Anatomy of a Machine
    Learning Platform*, you will use Keycloak, an open source software to provide
    authentication services.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用平台的任何组件之前，您需要配置与平台组件相关联的身份验证系统。如在[*第4章*](B18332_04_ePub.xhtml#_idTextAnchor055)《机器学习平台的构成》中所述，您将使用
    Keycloak，这是一款开源软件，用于提供身份验证服务。
- en: As a first step, import the configuration from `chapter5/realm-export.json`,
    which is available in the code repository associated with this book. This file
    contains the configuration required to associate the OAuth2 capabilities for the
    platform components.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从 `chapter5/realm-export.json` 导入配置，该文件可在本书关联的代码库中找到。此文件包含将 OAuth2 功能关联到平台组件所需的配置。
- en: 'Though this book is not a Keycloak guide by any means, we will provide some
    basic definitions for you to understand the high-level taxonomy of the Keycloak
    server:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '**Realm**: A Keycloak realm is an object that manages the users, roles, groups,
    and client applications that belong to the same domain. One Keycloak server can
    have multiple realms, so you have multiple sets of configurations, such as one
    realm for internal applications and one for external applications.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clients**: Clients are entities that can request user authentication. A Keycloak
    client object is associated with a realm. All the applications in our platform
    that require **single sign-on** (**SSO**) will be registered as **clients** in
    the Keycloak server.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Users and groups**: These two terms are self-explanatory, and you will be
    creating a new user in the following steps and using it to log into different
    software of the platform.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step is to configure Keycloak to provide OAuth capabilities to our
    ML platform component.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Importing the Keycloak configuration for the ODH components
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will import the clients and group configurations onto
    the Keycloak server running on your Kubernetes cluster. The following steps will
    import everything onto the master realm of the Keycloak server:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to your Keycloak server using the username `admin` and the password
    `admin`. Click on the **Import** link on the left-hand sidebar under the **Manage**
    heading:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Keycloak Master realm'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_001.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Keycloak Master realm
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Select file** button on the screen, as follows:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Keycloak import configuration page'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_002.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – Keycloak import configuration page
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the `chapter5/realm-export.json` file from the pop-up window. After
    that, select **Skip** for the **If a resource exists** drop-down options, and
    click **Import**:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Keycloak import configuration page'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_003.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Keycloak import configuration page
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that the records have been imported successfully onto your Keycloak
    server:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Keycloak import configuration results page'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_004.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 – Keycloak import configuration results page
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that there are four clients created by clicking on the **Clients**
    item on the left-hand side menu. The following client IDs should exist: **aflow**,
    **mflow**, **grafana**, and **jhub**. The **aflow** client is for the workflow
    engine of the platform, which is an instance of **Apache Airflow**. The **mflow**
    client is for the model registry and training tracker tool and is an instance
    of **MLflow**. The **grafana** client is for monitoring UI and is an instance
    of **Grafana**. And last, the **jhub** client is for the **JupyterHub** server
    instance.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Keycloak clients page'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_005.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – Keycloak clients page
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that a group called **ml-group** has been created by clicking on the
    **Groups** link on the left-hand panel:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证是否已创建名为 **ml-group** 的组，方法是在左侧面板点击 **Groups** 链接：
- en: '![Figure 5.6 – Keycloak groups page'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.6 – Keycloak 组页面'
- en: '](img/B18332_05_006.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_006.jpg)'
- en: Figure 5.6 – Keycloak groups page
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – Keycloak 组页面
- en: You will use this user group to create a user of the platform.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用该用户组来创建平台用户。
- en: Great work! You have just configured multiple Keycloak clients for the ML platform.
    The next step is to create a user in Keycloak that you will be using for the rest
    of this book. It is important to note that Keycloak can be hooked with your enterprise
    directory or any other database and to use them as a source of the users. Keep
    in mind that the realm configuration we are using here is very basic and is not
    recommended for production use.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！你刚刚为 ML 平台配置了多个 Keycloak 客户端。下一步是在 Keycloak 中创建一个用户，你将使用该用户进行本书余下部分的操作。需要注意的是，Keycloak
    可以与企业目录或任何其他数据库集成，并将其用作用户源。请记住，我们在这里使用的领域配置非常基础，不建议用于生产环境。
- en: Creating a Keycloak user
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Keycloak 用户
- en: 'In this section, you will create a new user and associate the newly created
    user with the group imported in the preceding section. Associating the user with
    the group gives the roles required for the different ODH software:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将创建一个新用户，并将新创建的用户与前一节中导入的组关联。将用户与组关联将授予不同 ODH 软件所需的角色：
- en: 'On the left-hand side of the Keycloak page, click on the **Users** link to
    come to this page. To add a new user, click the **Add user** button on the right:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Keycloak 页面左侧，点击 **Users** 链接进入该页面。要添加新用户，点击右侧的 **Add user** 按钮：
- en: '![Figure 5.7 – Keycloak users list'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7 – Keycloak 用户列表'
- en: '](img/B18332_05_007.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_007.jpg)'
- en: Figure 5.7 – Keycloak users list
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – Keycloak 用户列表
- en: 'Add the username `mluser` and make sure the **User Enabled** and **Email Verified**
    toggle buttons are set to **ON**. In **Groups**, select the **ml-group** group
    and fill in the **Email**, **First Name**, and **Last Name** fields, as shown
    in *Figure 5.8*, and then hit the **Save** button:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加用户名 `mluser`，确保 **User Enabled** 和 **Email Verified** 切换按钮设置为 **ON**。在 **Groups**
    中，选择 **ml-group** 组，并填写 **Email**、**First Name** 和 **Last Name** 字段，如 *图 5.8*
    所示，然后点击 **Save** 按钮：
- en: '![Figure 5.8 – Keycloak Add user page'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.8 – Keycloak 添加用户页面'
- en: '](img/B18332_05_008.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_008.jpg)'
- en: Figure 5.8 – Keycloak Add user page
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – Keycloak 添加用户页面
- en: 'Click on the **Credentials** tab to set the password for your user:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **Credentials** 标签页为你的用户设置密码：
- en: '![Figure 5.9 – Keycloak Credentials page'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.9 – Keycloak 凭证页面'
- en: '](img/B18332_05_009.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_009.jpg)'
- en: Figure 5.9 – Keycloak Credentials page
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – Keycloak 凭证页面
- en: Type in the password of your choice, then disable the **Temporary** flag, and
    hit the **Set Password** button.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入你选择的密码，然后禁用 **Temporary** 标志，点击 **Set Password** 按钮。
- en: You have just created and configured a user in Keycloak. Your Keycloak server
    is now ready to be used by the ML platform components. The next step is to explore
    the component of the platform that provides the main coding environment for all
    personas in the ML project.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚在 Keycloak 中创建并配置了一个用户。现在你的 Keycloak 服务器已经可以被 ML 平台组件使用。下一步是探索平台中为所有角色提供主要编码环境的组件。
- en: Configuring ODH components
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 ODH 组件
- en: In [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The Anatomy of a Machine
    Learning Platform*, you have installed the ODH operator. Using the ODH operator,
    you will now configure an instance of ODH that will automatically install the
    components of the ML platform. ODH executes **Kustomize** scripts to install the
    components of the ML platform. As part of the code for this book, we have provided
    templates to install and configure all the components required to run the platform.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 4 章*](B18332_04_ePub.xhtml#_idTextAnchor055)《机器学习平台的构成》中，你已经安装了 ODH 操作符。使用
    ODH 操作符，你将配置一个 ODH 实例，该实例将自动安装 ML 平台的组件。ODH 执行 **Kustomize** 脚本来安装 ML 平台的组件。作为本书代码的一部分，我们提供了模板来安装和配置运行平台所需的所有组件。
- en: You can also configure what components ODH operators install for you through
    a `manifests` file. You can pass the specific configuration to the manifests and
    choose the components you need. One such manifest is available in the code repository
    of the book at `manifests/kfdef/ml-platform.yaml`. This YAML file is configured
    for the ODH operator to do its magic and install the software we need to be part
    of the platform. You will need to make some modifications to this file, as you
    will see in the following section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过 `manifests` 文件配置 ODH 操作员为你安装哪些组件。你可以将特定的配置传递给清单文件，并选择所需的组件。一本书的代码仓库中有一个这样的清单，路径为
    `manifests/kfdef/ml-platform.yaml`。这个 YAML 文件为 ODH 操作员配置了所需的安装软件，使其成为平台的一部分。你需要对这个文件进行一些修改，接下来你将看到具体操作。
- en: 'This file defines the components of your platform and the location from where
    these components will get their settings:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件定义了平台的组件以及这些组件获取设置的位置：
- en: '**Name**: Defines the name of the component.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**名称**：定义组件的名称。'
- en: '`path` property where you define the relative path location of the files required
    to configure this component.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path` 属性定义了配置此组件所需文件的相对路径位置。'
- en: '`KEYCLOAK_URL` and `JUPYTERHUB_HOST` will need to be changed as per your configuration.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KEYCLOAK_URL` 和 `JUPYTERHUB_HOST` 需要根据你的配置进行更改。'
- en: '`manifests/jupytherhub/overlays` folder in the code repository.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`manifests/jupytherhub/overlays` 文件夹在代码仓库中。'
- en: '**Repos**: This configuration section is specific to each manifest file and
    applies to all the components in the manifest. It defines the location and version
    of the Git repository that contains all the files being referred to by this manifest
    file. If you want the manifest to reference your own files for the installation,
    you need to refer here to the right Git repository (the repository that contains
    your files).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Repos**：此配置部分特定于每个清单文件，并适用于清单中的所有组件。它定义了包含所有被清单文件引用文件的 Git 仓库的位置和版本。如果你希望清单引用你自己的安装文件，需在此处指定正确的
    Git 仓库（包含你文件的仓库）。'
- en: '*Figure 5.10* shows the part of the manifest file that holds the definition
    of the JupyterHub component:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.10* 展示了清单文件中定义 JupyterHub 组件的部分：'
- en: '![Figure 5.10 – A component in the ODH manifest file'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.10 – ODH 清单文件中的组件'
- en: '](img/B18332_05_010.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_010.jpg)'
- en: Figure 5.10 – A component in the ODH manifest file
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – ODH 清单文件中的组件
- en: You will use the provided manifest file to create an instance of the ML platform.
    You may also tweak configurations or add or remove components of the platform
    as you wish by modifying this file. However, for the exercises in the book, we
    do not recommend changing this unless you are instructed to do so.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用提供的清单文件来创建 ML 平台的实例。你也可以根据需要修改此文件，通过调整配置或添加、移除平台组件来进行个性化设置。但是，在书中的练习中，我们不建议你进行更改，除非有特别指示。
- en: Now that you have seen the ODH manifest file, it's time to make good use of
    it to create your first ML platform on Kubernetes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了 ODH 清单文件，是时候充分利用它来创建你的第一个 Kubernetes 上的 ML 平台了。
- en: Installing ODH
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 ODH
- en: Before we can install the data engineering components of the platform, we first
    need to create an instance of ODH. An ODH instance is a curated collection of
    related toolsets that serve as the components of an ML platform. Although the
    ML platform may contain components other than what is provided by ODH, it is fair
    to say that an instance of ODH is an instance of the ML platform. You may also
    run multiple instances of ODH on the same Kubernetes cluster as long as they run
    on their own isolated Kubernetes namespaces. This is useful when multiple teams
    or departments in your organization are sharing a single Kubernetes cluster.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们安装平台的数据工程组件之前，我们首先需要创建一个 ODH 实例。ODH 实例是一个经过精心策划的相关工具集的集合，作为 ML 平台的组件。尽管 ML
    平台可能包含 ODH 提供之外的组件，但可以公平地说，ODH 实例就是 ML 平台的一个实例。你也可以在同一个 Kubernetes 集群上运行多个 ODH
    实例，只要它们运行在各自隔离的 Kubernetes 命名空间中。这在组织中的多个团队或部门共享单一 Kubernetes 集群时非常有用。
- en: 'The following are the steps you need to follow to create an instance of ODH
    on your Kubernetes cluster:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你需要按照的步骤，以在 Kubernetes 集群上创建 ODH 实例：
- en: 'Create a new namespace in your Kubernetes cluster using the following command:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在 Kubernetes 集群中创建一个新命名空间：
- en: '[PRE0]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You should see the following response:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 5.11 – New namespace in your Kubernetes cluster'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.11 – Kubernetes 集群中的新命名空间'
- en: '](img/B18332_05_011.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_011.jpg)'
- en: Figure 5.11 – New namespace in your Kubernetes cluster
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 你的Kubernetes集群中的新命名空间
- en: 'Make sure that the ODH operator is running by issuing the following command:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保ODH操作符正在运行，执行以下命令：
- en: '[PRE1]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You should see the following response. Make sure the status says `Running`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应。确保状态显示为`Running`：
- en: '![Figure 5.12 – Status of the ODH operator'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.12 – ODH操作符的状态'
- en: '](img/B18332_05_012.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_012.jpg)'
- en: Figure 5.12 – Status of the ODH operator
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – ODH操作符的状态
- en: 'Get the IP address of your `minikube` environment. This IP address will be
    used to create ingress for different components of the platform the same way we
    did for Keycloak. Note that your IP may be different for each `minikube` instance
    depending on your underlying infrastructure:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取你`minikube`环境的IP地址。这个IP地址将用于为平台的不同组件创建入口，就像我们为Keycloak所做的那样。请注意，依据你的底层基础设施，每个`minikube`实例的IP可能会不同：
- en: '[PRE2]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This command should give you the IP address of your `minikube` cluster.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令应该会给出你`minikube`集群的IP地址。
- en: 'Open the `manifests/kfdef/ml-platform.yaml` file and change the value of the
    following parameters to a NIP (`nip.io`) domain name of your `minikube` instance.
    Only replace the IP address part of the domain name. For example, `KEYCLOAK_URL
    keycloak.<IP Address>.nip.io` should become `keycloak.192.168.61.72.nip.io`. Note
    that these parameters may be referenced in more than one place in this file. In
    a full Kubernetes environment, `<IP Address>` should be the domain name of your
    Kubernetes cluster:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`manifests/kfdef/ml-platform.yaml`文件，并将以下参数的值更改为你`minikube`实例的NIP（`nip.io`）域名。只需要替换域名中的IP地址部分。例如，`KEYCLOAK_URL
    keycloak.<IP Address>.nip.io`应该改为`keycloak.192.168.61.72.nip.io`。注意，这些参数在文件中可能会被多次引用。在完整的Kubernetes环境中，`<IP
    Address>`应该是你的Kubernetes集群的域名：
- en: '`KEYCLOAK_URL`'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`KEYCLOAK_URL`'
- en: '`JUPYTERHUB_HOST`'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`JUPYTERHUB_HOST`'
- en: '`AIRFLOW_HOST`'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`AIRFLOW_HOST`'
- en: '`MINIO_HOST`'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MINIO_HOST`'
- en: '`MLFLOW_HOST`'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MLFLOW_HOST`'
- en: '`GRAFANA_HOST`'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`GRAFANA_HOST`'
- en: 'Apply the manifest file to your Kubernetes cluster using the following command:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将清单文件应用到你的Kubernetes集群：
- en: '[PRE3]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should see the following response:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 5.13 – Result from applying manifests for the ODH components'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.13 – 应用ODH组件清单后的结果'
- en: '](img/B18332_05_013.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_013.jpg)'
- en: Figure 5.13 – Result from applying manifests for the ODH components
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 – 应用ODH组件清单后的结果
- en: 'Start watching the pods being created in the `ml-workshop` namespace by using
    the following command. It will take a while for all the components to be installed.
    After several minutes, all the Pods will be in a running state. While the pods
    are being created, you may see some pods throw errors. This is normal because
    some pods are dependent on other pods. Be patient as all the components come together
    and the pods will come into a running state:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令开始监控在`ml-workshop`命名空间中创建的Pods。安装所有组件需要一些时间。几分钟后，所有Pods将进入运行状态。在Pods创建过程中，你可能会看到一些Pods抛出错误。这是正常现象，因为某些Pods依赖于其他Pods。耐心等待，所有组件最终会安装完毕，Pods将进入运行状态：
- en: '[PRE4]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You should see the following response when all the pods are running:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有Pods都在运行时，你应该看到以下响应：
- en: '![Figure 5.14 – CLI response showing the ODH components running on the Kubernetes
    cluster'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.14 – CLI响应，展示ODH组件在Kubernetes集群上运行的情况'
- en: '](img/B18332_05_014.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_014.jpg)'
- en: Figure 5.14 – CLI response showing the ODH components running on the Kubernetes
    cluster
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 – CLI响应，展示ODH组件在Kubernetes集群上运行的情况
- en: 'So, what does this command do? The `kfdef` `ml-workshop` namespace:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这个命令是做什么的呢？`kfdef` `ml-workshop`命名空间：
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You should see all the objects that were created in the `ml-workshop` namespace
    by the ODH operator.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到ODH操作符在`ml-workshop`命名空间中创建的所有对象。
- en: Congratulations! You have just created a fresh instance of ODH. Now that you
    have seen the process of creating an instance of the ML platform from a manifest
    file, it is time to take a look at each of the components of the platform that
    the data engineers will use for their activities.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你刚刚创建了一个新的ODH实例。现在你已经看过了从清单文件创建ML平台实例的过程，是时候看看数据工程师将用来开展工作的平台各个组件了。
- en: Minikube Using Podman Driver
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Podman驱动的Minikube
- en: Note that for some `minikube` setups that use `podman` drivers in Linux, the
    Spark operator may fail due to the limit of the number of threads. To solve this
    problem, you need to use a `kvm2` driver in your `minikube` configuration. You
    can do this by adding the `--driver=kvm2` parameter to your `minikube start` command.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于某些使用 `podman` 驱动程序的 `minikube` 设置（在 Linux 上），由于线程数的限制，Spark 操作符可能会失败。为了解决这个问题，你需要在
    `minikube` 配置中使用 `kvm2` 驱动程序。你可以通过在 `minikube start` 命令中添加 `--driver=kvm2` 参数来实现。
- en: Understanding and using JupyterHub
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解和使用 JupyterHub
- en: Jupyter Notebook has become an extremely popular tool for writing code for ML
    projects. JupyterHub is a software that facilitates the self-service provisioning
    of computing environments that includes spinning up pre-configured Jupyter Notebook
    servers and provisioning the associated compute resources on the Kubernetes platform.
    On-demand end users such as data engineers and data scientists can provision their
    own instances of Jupyter Notebook dedicated only to them. If a requesting user
    already has his/her own running instance of Jupyter Notebook, the hub will just
    direct the user to the existing instance, avoiding duplicated environments. From
    the end user's perspective, the whole interaction is seamless. You will see this
    in the next section of this chapter.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter Notebook 已成为编写机器学习项目代码的极受欢迎的工具。JupyterHub 是一款软件，它便于自助提供计算环境，包括启动预配置的
    Jupyter Notebook 服务器并在 Kubernetes 平台上提供相关的计算资源。数据工程师和数据科学家等按需终端用户可以为自己提供专用的 Jupyter
    Notebook 实例。如果请求的用户已经有了自己运行的 Jupyter Notebook 实例，Hub 将直接将用户引导到现有实例，从而避免重复的环境。对终端用户而言，整个过程是无缝的。你将在本章的下一部分看到这一点。
- en: When a user requests an environment in JupyterHub, they are also given the option
    to choose a pre-configured sizing of hardware resources such as CPU, memory, and
    storage. This allows for a flexible way for developers, data engineers, and data
    scientists to provision just the right amount of computing resources for a given
    task. This dynamic allocation of resources is facilitated by the underlying Kubernetes
    platform.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户请求 JupyterHub 中的一个环境时，他们还可以选择预配置的硬件资源（如 CPU、内存和存储）大小。这为开发者、数据工程师和数据科学家提供了一种灵活的方式，使他们能够为特定任务分配合适的计算资源。这种资源的动态分配是通过底层的
    Kubernetes 平台来实现的。
- en: Different users may require different frameworks, libraries, and flavors of
    coding environments. Some data scientists may want to use TensorFlow while others
    want to use scikit-learn or PyTorch. Some data engineers may prefer to use pandas
    while some may need to run their data pipelines in PySpark. In JupyterHub, they
    can configure multiple pre-defined environments for such scenarios. Users can
    then select a predefined configuration when they request a new environment. These
    predefined environments are actually container images. This means that the platform
    operator or platform administrator can prepare several predefined container images
    that will serve as the end user's computing environment. This feature also enables
    the standardization of environments. How many times do you have to deal with different
    versions of the libraries on different developer computers? The standardization
    of environments can reduce the number of problems related to library version inconsistencies
    and generally reduce the *it works on my machine* issues.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的用户可能需要不同的框架、库和编程环境的版本。一些数据科学家可能想使用 TensorFlow，而其他人可能想使用 scikit-learn 或 PyTorch。一些数据工程师可能更喜欢使用
    pandas，而其他人可能需要在 PySpark 中运行数据管道。在 JupyterHub 中，他们可以为这些场景配置多个预定义的环境。用户在请求新环境时，可以选择一个预定义的配置。这些预定义的环境实际上是容器镜像。这意味着平台操作员或管理员可以准备多个预定义的容器镜像，作为终端用户的计算环境。此功能还支持环境的标准化。你有多少次在不同开发者计算机上处理不同版本库的情况？环境的标准化可以减少与库版本不一致相关的问题，并通常减少
    *它在我的机器上能运行* 的问题。
- en: '*Figure 5.15* shows the three-step process of provisioning a new JupyterHub
    environment:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.15* 展示了为 JupyterHub 环境配置的新环境的三步流程：'
- en: '![Figure 5.15 – Workflow for creating a new environment in JupyterHub'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.15 – 创建 JupyterHub 新环境的工作流程]'
- en: '](img/B18332_05_015.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_015.jpg)'
- en: Figure 5.15 – Workflow for creating a new environment in JupyterHub
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – 创建 JupyterHub 新环境的工作流程
- en: Now that you know what JupyterHub can do, let's see it in action.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道了 JupyterHub 能做什么，我们来看看它是如何运作的。
- en: Validating the JupyterHub installation
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证 JupyterHub 安装
- en: Every data engineer in the team follows a simple and standard workflow of provisioning
    an environment. No more manual installations and fiddling with their workstation
    configurations. This is great for autonomous teams and will definitely help improve
    your team's velocity.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 团队中的每个数据工程师都遵循一个简单而标准的环境配置工作流。不再需要手动安装和调试工作站配置。这对于自主团队来说非常好，肯定能提高团队的效率。
- en: 'The ODH operator has already installed the JupyterHub for you in the previous
    sections. Now, you will spin up a new Jupyter Notebook environment, as a data
    engineer, and write your data pipelines:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ODH 运维人员已经为你在之前的部分安装了 JupyterHub。现在，你将以数据工程师身份启动一个新的 Jupyter Notebook 环境，并编写数据管道：
- en: 'Get the ingress objects created in your Kubernetes environment using the following
    command. We are running this command to find the URL of JupyterHub:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令获取在 Kubernetes 环境中创建的 ingress 对象。我们运行此命令以查找 JupyterHub 的 URL：
- en: '[PRE6]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You should see the following example response. Take note of the JupyterHub
    URL as displayed in the `HOSTS` column:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下示例响应。请注意在 `HOSTS` 列中显示的 JupyterHub URL：
- en: '![Figure 5.16 – All ingresses in your cluster'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.16 – 集群中的所有 ingress'
- en: '](img/B18332_05_016.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_016.jpg)'
- en: Figure 5.16 – All ingresses in your cluster
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 – 集群中的所有 ingress
- en: 'Open a browser from the same machine where minikube is running and navigate
    to the JupyterHub URL. The URL looks like https://jupyterhub.<MINIKUBE IP ADDRESS>.nip.io.
    This URL will take you to the Keycloak login page to perform SSO authentication.
    Make sure that you replace the IP address with your minikube IP address in this
    URL:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从与 minikube 运行在同一台机器上的浏览器中，访问 JupyterHub URL。该 URL 看起来像是 https://jupyterhub.<MINIKUBE
    IP ADDRESS>.nip.io。此 URL 会将你带到 Keycloak 登录页面进行 SSO 认证。确保在这个 URL 中将 IP 地址替换为你 minikube
    的 IP 地址：
- en: '![Figure 5.17 – SSO challenge for JupyterHub'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.17 – JupyterHub 的 SSO 挑战'
- en: '](img/B18332_05_017.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_017.jpg)'
- en: Figure 5.17 – SSO challenge for JupyterHub
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 – JupyterHub 的 SSO 挑战
- en: Type `mluser` for the username, then type whatever password you have set up
    for this user, and click **Sign In**.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入 `mluser` 作为用户名，然后输入你为该用户设置的密码，点击 **Sign In**。
- en: You will see the landing page of the JupyterHub server where it allows you to
    select the notebook container image that you want to use and also a predefined
    size of computing resources you need.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到 JupyterHub 服务器的登录页面，允许你选择你想使用的 notebook 容器镜像，以及你所需的预定义计算资源大小。
- en: The notebook image section contains the standard notebooks that you have provisioned
    using the ODH manifests from the `manifests/jupyterhub-images` folder of the code
    repository.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: notebook 镜像部分包含了你通过 ODH 清单文件从代码库中的 `manifests/jupyterhub-images` 文件夹中配置的标准 notebooks。
- en: The container size drop-down allows you to select the right size of environment
    for your need. This configuration is also controlled via the `manifests/jupyterhub/jupyterhub/overlays/mlops/jupyterhub-singleuser-profiles-sizes-configmap.yaml`
    manifest file.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 容器大小下拉菜单允许你选择适合你需求的环境大小。此配置也通过 `manifests/jupyterhub/jupyterhub/overlays/mlops/jupyterhub-singleuser-profiles-sizes-configmap.yaml`
    清单文件进行控制。
- en: We encourage you to look into these files to familiarize yourself with what
    configuration you can set for each manifest.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励你查看这些文件，了解你可以为每个清单配置哪些设置。
- en: '![Figure 5.18 – JupyterHub landing page'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.18 – JupyterHub 登录页面'
- en: '](img/B18332_05_018.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_018.jpg)'
- en: Figure 5.18 – JupyterHub landing page
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18 – JupyterHub 登录页面
- en: Select **Base Elyra Notebook Image** and the **Default** container size and
    hit **Start server**.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 **Base Elyra Notebook Image** 和 **Default** 容器大小，然后点击 **Start server**。
- en: 'Validate that a new pod has been created for your user by issuing the following
    command. Jupyter Notebook instance names start with `jupyter-nb-` and are suffixed
    with the username of the user. This allows for a unique name of notebook pods
    for each user:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令验证是否为你的用户创建了一个新的 Pod。Jupyter Notebook 实例的名称以 `jupyter-nb-` 开头，并以用户名作为后缀。这使得每个用户的
    notebook pod 拥有独特的名称：
- en: '[PRE7]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You should see the following response:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 5.19 – Jupyter Notebook pod created by JupyterHub'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.19 – JupyterHub 创建的 Jupyter Notebook pod'
- en: '](img/B18332_05_019.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_019.jpg)'
- en: Figure 5.19 – Jupyter Notebook pod created by JupyterHub
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19 – JupyterHub 创建的 Jupyter Notebook pod
- en: Congratulations! You are now running your own self-provisioned Jupyter Notebook
    server on the Kubernetes platform.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 恭喜！你现在正在 Kubernetes 平台上运行自己的自供给 Jupyter Notebook 服务器。
- en: '![Figure 5.20 – Jupyter Notebook landing page'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.20 – Jupyter Notebook 登录页面'
- en: '](img/B18332_05_020.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_020.jpg)'
- en: Figure 5.20 – Jupyter Notebook landing page
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.20 – Jupyter Notebook 登录页面
- en: 'Now, let''s stop the notebook server. Click on the **File > Hub Control Panel**
    menu option to go to the **Hub Control Panel** page shown as follows:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们停止 notebook 服务器。点击 **File > Hub Control Panel** 菜单选项，进入以下所示的 **Hub 控制面板**
    页面：
- en: '![Figure 5.21 – Menu option to see the Hub Control Panel'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.21 – 查看 Hub 控制面板的菜单选项'
- en: '](img/B18332_05_021.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_021.jpg)'
- en: Figure 5.21 – Menu option to see the Hub Control Panel
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.21 – 查看 Hub 控制面板的菜单选项
- en: Click on the **Stop My Server** button. This is how you stop your instance of
    Jupyter Notebook. You may want to start it back again later for the next steps.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **Stop My Server** 按钮。这是停止你 Jupyter Notebook 实例的方式。你可能稍后想要重新启动它以继续下一步。
- en: '![Figure 5.22 – Hub Control Panel'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.22 – Hub 控制面板'
- en: '](img/B18332_05_022.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_022.jpg)'
- en: Figure 5.22 – Hub Control Panel
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.22 – Hub 控制面板
- en: 'Validate that a new pod has been destroyed for your user by issuing the following
    command:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过发出以下命令验证新 pod 是否已为你的用户销毁：
- en: '[PRE8]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There should be no output for this command because the Jupyter Notebook pod
    has been destroyed by JupyterHub.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令应该没有输出，因为 Jupyter Notebook pod 已被 JupyterHub 销毁。
- en: We leave it up to you to explore the different bits of the configuration of
    the notebook in your environment. You will write code using this Jupyter notebook
    in the later sections of this chapter and the next few chapters of this book,
    so if you just want to continue reading, you will not miss anything.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将留给你自己探索在你环境中 notebook 的不同配置。你将在本章和接下来的几章中使用这个 Jupyter notebook 编写代码，所以如果你只是想继续阅读，你不会错过任何重要内容。
- en: Running your first Jupyter notebook
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行你的第一个 Jupyter notebook
- en: 'Now that your Jupyter notebook is running, it is time to write the `Hello World!`
    program. In the code repository of this book, we have provided one such program,
    and in the following steps, you will check out the code using Git and run the
    program. Before we start these steps, make sure that you can access your Jupyter
    notebook using the browser, as mentioned in the preceding section:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你的 Jupyter notebook 已经运行，是时候编写 `Hello World!` 程序了。在本书的代码仓库中，我们提供了这样的一个程序，在接下来的步骤中，你将使用
    Git 检出代码并运行该程序。在开始这些步骤之前，请确保你可以通过浏览器访问你的 Jupyter notebook，如前一节所述：
- en: 'Click on the Git icon on the left-hand side menu on your Jupyter notebook.
    The icon is the third from the top. It will display three buttons for different
    operations. Click on the **Clone a Repository** button:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 Jupyter notebook 左侧菜单中的 Git 图标。该图标是从顶部数起的第三个。它会显示三个不同操作的按钮。点击 **Clone a Repository**
    按钮：
- en: '![Figure 5.23 – Git operations in the Jupyter notebook'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.23 – Jupyter notebook 中的 Git 操作'
- en: '](img/B18332_05_024.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_024.jpg)'
- en: Figure 5.23 – Git operations in the Jupyter notebook
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.23 – Jupyter notebook 中的 Git 操作
- en: In the **Clone a repo** pop-up box, type in the location of the code repository
    of this book, [https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git](https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git),
    and hit **CLONE**.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **Clone a repo** 弹出框中，输入本书的代码仓库位置，[https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git](https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git)，然后点击
    **CLONE**。
- en: '![Figure 5.24 – Git clone a repo in the Jupyter notebook'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.24 – 在 Jupyter notebook 中克隆 Git 仓库'
- en: '](img/B18332_05_025.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_025.jpg)'
- en: Figure 5.24 – Git clone a repo in the Jupyter notebook
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.24 – 在 Jupyter notebook 中克隆 Git 仓库
- en: 'You will see that the code repository is cloned onto your Jupyter notebook''s
    file system. As shown in *Figure 5.25*, navigate to the `chapter5/helloworld.ipynb`
    file and open it in your notebook. Click on the little play icon on the top bar
    to run the cell:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会看到代码仓库已被克隆到 Jupyter notebook 的文件系统中。如 *图 5.25* 所示，导航到 `chapter5/helloworld.ipynb`
    文件并在 notebook 中打开它。点击顶部栏上的小播放图标运行该单元：
- en: '![Figure 5.25 – Notebook on your Jupyter environment'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.25 – 在你的 Jupyter 环境中的 notebook'
- en: '](img/B18332_05_026.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_026.jpg)'
- en: Figure 5.25 – Notebook on your Jupyter environment
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.25 – 在你的 Jupyter 环境中的 notebook
- en: Et voila! You have just executed a Python code in your own self-provisioned
    Jupyter Notebook server running on Kubernetes.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Voilà！你刚刚在自己配置的 Kubernetes 上运行的 Jupyter Notebook 服务器中执行了 Python 代码。
- en: Shut down your notebook by selecting the **File > Hub Control Panel** menu option.
    Click on the **Stop My Server** button to shut down your environment. Note that
    ODH will save your disk and next time you start your notebook, all your saved
    files will be available.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过选择 **文件 > Hub 控制面板** 菜单选项关闭你的笔记本。点击 **停止我的服务器** 按钮来关闭你的环境。请注意，ODH 会保存你的磁盘，下次启动笔记本时，所有保存的文件都会可用。
- en: Congratulations! Now, you can run your code on the platform. Next, we'll get
    some basics refreshed for the Apache Spark engine.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！现在，你可以在平台上运行你的代码。接下来，我们将回顾一下 Apache Spark 引擎的一些基础知识。
- en: Understanding the basics of Apache Spark
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Apache Spark 的基础知识
- en: Apache Spark is an open source data processing engine designed for distributed
    large-scale processing of data. This means that if you have smaller datasets,
    say 10s or even a few 100s of GB, a tuned traditional database may provide faster
    processing times. The main differentiator for Apache Spark is its capability to
    perform in-memory intermediate computations, which makes Apache Spark much faster
    than Hadoop MapReduce.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个开源的数据处理引擎，旨在分布式大规模数据处理。这意味着，如果你处理的是较小的数据集，比如 10s 或者几百 GB，经过优化的传统数据库可能会提供更快的处理时间。Apache
    Spark 的主要特点是其能够在内存中进行中间计算，这使得 Apache Spark 比 Hadoop MapReduce 更加高效。
- en: Apache Spark is built for speed, flexibility, and ease of use. Apache Spark
    offers more than 70 high-level data processing operators that make it easy for
    data engineers to build data applications, so it is easy to write data processing
    logic using Apache Spark APIs. Being flexible means that Spark works as a unified
    data processing engine and works on several types of data workloads such as batch
    applications, streaming applications, interactive queries, and even ML algorithms.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 的设计目标是速度、灵活性和易用性。Apache Spark 提供了超过 70 个高级数据处理操作符，使数据工程师可以轻松构建数据应用程序，因此使用
    Apache Spark API 编写数据处理逻辑变得非常简单。灵活性意味着 Spark 作为一个统一的数据处理引擎，可以处理多种数据工作负载，如批处理应用程序、流处理应用程序、交互式查询，甚至是机器学习算法。
- en: '*Figure 5.26* shows the Apache Spark components:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.26* 显示了 Apache Spark 组件：'
- en: '![Figure 5.26 – Apache Spark components'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.26 – Apache Spark 组件]'
- en: '](img/B18332_05_027.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_027.jpg)'
- en: Figure 5.26 – Apache Spark components
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.26 – Apache Spark 组件
- en: Understanding Apache Spark job execution
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Apache Spark 作业执行
- en: Most data engineers now know that Apache Spark is a massively parallel data
    processing engine. It is one of the most successful projects of the Apache Software
    Foundation. Spark traditionally runs on a cluster of multiple **virtual machines**
    (**VMs**) or bare metal servers. However, with the popularity of containers and
    Kubernetes, Spark added support for running Spark clusters on containers on Kubernetes.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在大多数数据工程师都知道 Apache Spark 是一个大规模并行数据处理引擎。它是 Apache 软件基金会最成功的项目之一。Spark 通常运行在多个
    **虚拟机**（**VMs**）或裸金属服务器的集群上。然而，随着容器和 Kubernetes 的流行，Spark 增加了对在 Kubernetes 上运行
    Spark 集群的支持。
- en: There are two most common ways of running Spark on Kubernetes. The first, and
    the native way, is by using the Kubernetes engine itself to orchestrate the Kubernetes
    worker pods. In this approach, the Spark cluster instance is always running and
    the Spark applications are submitted to the Kubernetes API that will schedule
    the submitted application. We will not dig deeper into how this is implemented.
    The second approach is through Kubernetes operators. Operators take advantage
    of the Kubernetes CRDs to create Spark objects natively in Kubernetes. In this
    approach, the Spark cluster is created on the fly by the Spark operator. Instead
    of submitting a Spark application to an existing cluster, the operator spins up
    spark clusters on-demand.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上运行 Spark 的两种最常见方式如下。第一种，也是原生方式，是通过 Kubernetes 引擎本身来协调 Kubernetes
    工作节点。在这种方法中，Spark 集群实例始终运行，Spark 应用程序被提交到 Kubernetes API，然后 Kubernetes API 调度提交的应用程序。我们不会深入探讨这部分实现。第二种方法是通过
    Kubernetes 运维工具（operators）。运维工具利用 Kubernetes CRD 来在 Kubernetes 中本地创建 Spark 对象。在这种方法中，Spark
    集群是通过 Spark 运维工具动态创建的。与其将 Spark 应用程序提交到现有的集群，不如通过运维工具按需启动 Spark 集群。
- en: A Spark cluster follows a manager/worker architecture. The Spark cluster manager
    knows where the workers are located, and the resources available for the worker.
    The Spark cluster manages the resources for the cluster of worker nodes where
    your application will run. Each worker has one or more executors that run the
    assigned jobs through an executor.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 集群采用管理/工作架构。Spark 集群管理器知道工作节点的位置，以及工作节点可用的资源。Spark 集群管理着将运行应用程序的工作节点集群的资源。每个工作节点都有一个或多个执行进程，通过执行进程运行分配的任务。
- en: Spark applications have two parts, the driver component, and the data processing
    logic. A driver component is responsible for executing the flow of data processing
    operations. The driver run first talks to the cluster manager to find out what
    worker nodes will run the application logic. The driver transforms all the application
    operations into tasks, schedules them, and assigns tasks directly to the executor
    processes on the worker node. One executor can run multiple tasks that are associated
    with the same Spark context.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 应用程序有两部分：驱动程序组件和数据处理逻辑。驱动程序组件负责执行数据处理操作的流程。驱动程序首先与集群管理器交互，找出哪些工作节点将运行应用程序逻辑。驱动程序将所有应用操作转换为任务，调度它们，并将任务直接分配给工作节点上的执行进程。一个执行进程可以运行多个与相同
    Spark 上下文相关的任务。
- en: If your application requires you to collect the computed result and merge them,
    the driver is the one who will be responsible for this activity. As a data engineer,
    all this activity is abstracted from you via the SparkSession object. You only
    need to write the data processing logic. Did we mention Apache Spark aims to be
    simple?
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的应用程序需要收集计算结果并进行合并，驱动程序将负责此项工作。作为数据工程师，这一切操作都通过 SparkSession 对象进行了抽象，你只需要编写数据处理逻辑。我们提到过
    Apache Spark 旨在简化操作吗？
- en: '*Figure 5.27* shows the relationship between the Spark driver, Spark cluster
    manager, and Spark worker nodes:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.27* 显示了 Spark 驱动程序、Spark 集群管理器和 Spark 工作节点之间的关系：'
- en: '![Figure 5.27 – Relationship between Apache Spark components'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.27 – Apache Spark 组件关系'
- en: '](img/B18332_05_028.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_028.jpg)'
- en: Figure 5.27 – Relationship between Apache Spark components
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.27 – Apache Spark 组件关系
- en: Understanding how ODH provisions Apache Spark cluster on-demand
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 ODH 如何按需提供 Apache Spark 集群
- en: 'We have talked about how the ODH allows you to create a dynamic and flexible
    development environment to write code such as data pipelines using Jupyter Notebook.
    We have noticed that data developers need to interact with IT to get time on the
    data processing clusters such as Apache Spark. These interactions reduce the agility
    of the team, and this is one of the problems the ML platform solves. To adhere
    to this scenario, ODH provides the following components:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了 ODH 如何帮助你创建一个动态灵活的开发环境，使用 Jupyter Notebook 编写代码，如数据管道。我们注意到，数据开发人员需要与
    IT 部门互动，以便获得数据处理集群（如 Apache Spark）上的时间。这些互动降低了团队的敏捷性，而这是 ML 平台解决的一个问题。为了符合这一场景，ODH
    提供了以下组件：
- en: A Spark operator that spawns the Apache Spark cluster. For this book, we have
    forked the original Spark operator provided by ODH and radanalytics to adhere
    to the latest changes to the Kubernetes API.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Spark 操作符，用于启动 Apache Spark 集群。本书中，我们基于 ODH 和 radanalytics 提供的原始 Spark 操作符进行了分支，以适应
    Kubernetes API 的最新变化。
- en: A capability in JupyterHub to issue a request for a new Spark cluster to the
    Spark operator when certain notebook environments are created by the user.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JupyterHub 中的一项功能，当用户创建特定的笔记本环境时，向 Spark 操作符发出请求，要求创建一个新的 Spark 集群。
- en: As a data engineer, when you spin up a new notebook environment using certain
    notebook images, JupyterHub not only spawns a new notebook server, it also creates
    the Apache Spark cluster dedicated for you through the Spark operator.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据工程师，当你使用某些笔记本镜像启动新的笔记本环境时，JupyterHub 不仅会启动一个新的笔记本服务器，还会通过 Spark 操作符为你创建一个专用的
    Apache Spark 集群。
- en: Creating a Spark cluster
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Spark 集群
- en: Let's first see how the Spark operator works on the Kubernetes cluster. ODH
    creates the Spark controller. You can see the configuration in the `chapter5/ml-platform.yaml`
    file under the name `radanalyticsio-spark-cluster`, as shown in *Figure 5.28*.
    You can see this is another set of Kubernetes YAML files that defines the `manifests/radanalyticsio`
    folder in the code repository of this book.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看看 Spark 操作符在 Kubernetes 集群中的工作方式。ODH 创建了 Spark 控制器。你可以在 `chapter5/ml-platform.yaml`
    文件中查看配置，文件名为 `radanalyticsio-spark-cluster`，如 *图 5.28* 所示。你可以看到，这是另一组 Kubernetes
    YAML 文件，定义了本书代码库中的 `manifests/radanalyticsio` 文件夹。
- en: '![Figure 5.28 – Snippet of the section of the manifest that installs the Spark
    operator'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.28 – 安装Spark操作符的清单部分代码片段'
- en: '](img/B18332_05_029.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_029.jpg)'
- en: Figure 5.28 – Snippet of the section of the manifest that installs the Spark
    operator
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.28 – 安装Spark操作符的清单部分代码片段
- en: 'When you need to spin up an Apache Spark cluster, you can do this by creating
    a Kubernetes custom resource called **SparkCluster**. Upon receiving the request,
    the Spark operator will provision a new Spark cluster as per the required configuration.
    The following steps will show you the steps for provisioning a Spark cluster on
    your platform:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要启动一个Apache Spark集群时，可以通过创建一个名为**SparkCluster**的Kubernetes自定义资源来实现。收到请求后，Spark操作符将根据所需的配置来配置一个新的Spark集群。以下步骤将向你展示如何在你的平台上配置Spark集群：
- en: 'Validate that the Spark operator pod is running:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证Spark操作符Pod是否正在运行：
- en: '[PRE9]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You should see the following response:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到如下响应：
- en: '![Figure 5.29 – Spark operator pod'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.29 – Spark操作符Pod'
- en: '](img/B18332_05_030.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_030.jpg)'
- en: Figure 5.29 – Spark operator pod
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.29 – Spark操作符Pod
- en: 'Create a simple Spark cluster with one worker node using the file available
    at `chapter5/simple-spark-cluster.yaml`. You can see that this file is requesting
    a Spark cluster with one master and one worker node. Through this custom resource,
    you can set several Spark configurations, as we shall see in the next section:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`chapter5/simple-spark-cluster.yaml`文件创建一个包含一个工作节点的简单Spark集群。你可以看到该文件请求一个包含一个主节点和一个工作节点的Spark集群。通过这个自定义资源，你可以设置多个Spark配置，正如我们将在下一节中看到的那样：
- en: '![Figure 5.30 – Spark custom resource'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.30 – Spark自定义资源'
- en: '](img/B18332_05_031.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_031.jpg)'
- en: Figure 5.30 – Spark custom resource
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.30 – Spark自定义资源
- en: 'Create this Spark cluster custom resource in your Kubernetes cluster by running
    the following command. The Spark operator constantly scans for this resource in
    the Kubernetes platform and automatically creates a new instance of Apache Spark
    cluster for each given Spark cluster custom resource:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的Kubernetes集群中通过运行以下命令创建这个Spark集群自定义资源。Spark操作符会不断扫描Kubernetes平台上的此资源，并为每个指定的Spark集群自定义资源自动创建一个新的Apache
    Spark集群实例：
- en: '[PRE10]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You should see the following response:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到如下响应：
- en: '![Figure 5.31 – Response to creating a Spark cluster'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.31 – 创建Spark集群后的响应'
- en: '](img/B18332_05_032.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_032.jpg)'
- en: Figure 5.31 – Response to creating a Spark cluster
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.31 – 创建Spark集群后的响应
- en: 'Validate that the Spark cluster pods are running in your cluster:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证Spark集群的Pod是否在你的集群中运行：
- en: '[PRE11]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You should see the following response. There are two pods created by the Spark
    operator, one for the Spark master node and another for the worker node. The number
    of worker pods depends on the value of the `instances` parameters in the `SparkCluster`
    resource. It may take some time for the pods to come to a running state the first
    time:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到如下响应。Spark操作符已创建两个Pod，一个用于Spark主节点，另一个用于工作节点。工作节点的Pod数量取决于`SparkCluster`资源中的`instances`参数值。首次启动时，Pod可能需要一些时间才能进入运行状态：
- en: '![Figure 5.32 – List of running Spark cluster pods'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.32 – 正在运行的Spark集群Pod列表'
- en: '](img/B18332_05_033.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_033.jpg)'
- en: Figure 5.32 – List of running Spark cluster pods
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.32 – 正在运行的Spark集群Pod列表
- en: Now, you know how the Spark operator works on the Kubernetes cluster. The next
    step is to see how JupyterHub is configured to request the cluster dynamically
    while provisioning a new notebook for you.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经了解了Spark操作符在Kubernetes集群中的工作原理。下一步是查看JupyterHub是如何配置以动态请求集群，并在为你配置新笔记本时提供Spark集群的。
- en: Understanding how JupyterHub creates a Spark cluster
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解JupyterHub如何创建Spark集群
- en: 'Simply put, JupyterHub does what you did in the preceding section. JupyterHub
    creates a `SparkCluster` resource in Kubernetes so that the Spark operator can
    provision the Apache Spark cluster for your use. This `SparkCluster` resource
    configuration is a Kubernetes `ConfigMap` file and can be found at `manifests/jupyterhub/jupyterhub/base/jupyterhub-spark-operator-configmap.yaml`.
    Look for `sparkClusterTemplate` in this file, as shown in *Figure 5.33*. You can
    see that it looks like the file you have created in the previous section:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，JupyterHub执行了你在前一节中所做的操作。JupyterHub在Kubernetes中创建一个`SparkCluster`资源，以便Spark操作符可以为你配置Apache
    Spark集群。这个`SparkCluster`资源配置是一个Kubernetes `ConfigMap`文件，位于`manifests/jupyterhub/jupyterhub/base/jupyterhub-spark-operator-configmap.yaml`。请查找文件中的`sparkClusterTemplate`，如*图5.33*所示。你可以看到它看起来与前一节中你创建的文件相似：
- en: '![Figure 5.33 – JupyterHub template for Spark resources'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.33 – JupyterHub的Spark资源模板'
- en: '](img/B18332_05_034.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_034.jpg)'
- en: Figure 5.33 – JupyterHub template for Spark resources
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.33 – JupyterHub用于Spark资源的模板
- en: Some of you might have noticed that this is a template, and it needs the values
    for specific variables mentioned in this template. Variables such as `{{ user
    }}` and `{{ worker_nodes }}` and so on. Recall that we have mentioned that JupyterHub
    creates the `SparkCluster` request while it is provisioning a container for your
    notebook. JupyterHub uses this file as a template and fills in the values while
    creating your notebook. How does JupyterHub decide to create a Spark cluster?
    This configuration is called `ConfigMap` file in `manifests/jupyterhub/jupyterhub/overlays/spark3/jupyterhub-singleuser-profiles-configmap.yaml`.
    This looks like the file shown in *Figure 5.33*.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 你们中的一些人可能已经注意到这是一个模板，它需要填写模板中提到的特定变量的值。诸如`{{ user }}`和`{{ worker_nodes }}`等变量。回想一下，我们提到过JupyterHub在为你的笔记本配置容器时，会创建`SparkCluster`请求。JupyterHub使用这个文件作为模板，并在创建笔记本时填充相应的值。JupyterHub是如何决定创建Spark集群的呢？这个配置文件在`manifests/jupyterhub/jupyterhub/overlays/spark3/jupyterhub-singleuser-profiles-configmap.yaml`中，称为`ConfigMap`文件。它看起来像*图5.33*所示的文件。
- en: 'You can see that the `image` field specifies the name of the container image
    on which this profile will be triggered. So, as a data engineer, when you select
    this notebook image from the JupyterHub landing page, JupyterHub will apply this
    profile. The second thing in the profile is the `env` section, which specifies
    the environment variables that will be pushed to the notebook container instance.
    The `configuration` object defines the values that will be applied to the template
    that is mentioned in the `resources` key:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，`image`字段指定了该配置文件将触发的容器镜像名称。因此，作为数据工程师，当你从JupyterHub登录页选择该笔记本镜像时，JupyterHub将应用此配置文件。配置文件中的第二部分是`env`部分，它指定了将被推送到笔记本容器实例的环境变量。`configuration`对象定义了将应用于在`resources`键中提到的模板的值：
- en: '![Figure 5.34 – JupyterHub profile for Spark resources'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.34 – JupyterHub针对Spark资源的配置文件'
- en: '](img/B18332_05_035.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_035.jpg)'
- en: Figure 5.34 – JupyterHub profile for Spark resources
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.34 – JupyterHub针对Spark资源的配置文件
- en: As you may appreciate, there is a lot of work done behind the scenes to make
    a streamlined experience for you and your team, and in the true sense of open
    source, you can configure everything and even give back to the project if you
    come up with any modifications or new features.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经感受到的那样，在后台有很多工作在进行，旨在为你和你的团队提供一个流畅的体验，并且从开源的真正意义上讲，你可以配置所有内容，甚至如果你有任何修改或新功能，也可以回馈项目。
- en: In the next section, you will see how easy it is to write and run a Spark application
    on your platform running these components.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将看到如何在运行这些组件的平台上编写和运行Spark应用程序是多么容易。
- en: Writing and running a Spark application from Jupyter Notebook
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Jupyter Notebook编写和运行Spark应用程序
- en: 'Before you run the following steps, make sure that you grasped the components
    and their interactions that we have introduced in the previous section of this
    chapter:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行以下步骤之前，请确保你理解了我们在本章前面部分介绍的组件及其相互关系：
- en: 'Validate that the Spark operator pod is running by running the following command:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令，验证Spark操作员Pod是否正在运行：
- en: '[PRE12]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You should see the following response:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到以下响应：
- en: '![Figure 5.35 – Spark operator pod'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.35 – Spark操作员Pod'
- en: '](img/B18332_05_036.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_036.jpg)'
- en: Figure 5.35 – Spark operator pod
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.35 – Spark操作员Pod
- en: 'Validate that the JupyterHub pod is running by running the following command:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令，验证JupyterHub Pod是否正在运行：
- en: '[PRE13]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You should see the following response:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到以下响应：
- en: '![Figure 5.36 – JupyterHub pod'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.36 – JupyterHub Pod'
- en: '](img/B18332_05_037.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_037.jpg)'
- en: Figure 5.36 – JupyterHub pod
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.36 – JupyterHub Pod
- en: 'Before you start the notebook, let''s delete the Spark cluster you have created
    in the previous sections by running the following command. This is to demonstrate
    that JupyterHub will automatically create a new instance of Spark cluster for
    you:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在启动笔记本之前，让我们通过运行以下命令删除你在前面部分创建的Spark集群。这样可以演示JupyterHub将自动为你创建一个新的Spark集群实例：
- en: '[PRE14]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Log in to your JupyterHub server. Refer to the *Validating JupyterHub configuration*
    section earlier in this chapter. You will get the landing page of your server.
    Select the `manifests/jupyterhub/jupyterhub/overlays/spark3/jupyterhub-singleuser-profiles-configmap.yaml`
    file.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到你的JupyterHub服务器。参考本章前面提到的*验证JupyterHub配置*部分。你将看到服务器的登录页面。选择`manifests/jupyterhub/jupyterhub/overlays/spark3/jupyterhub-singleuser-profiles-configmap.yaml`文件。
- en: 'Click on **Start server**:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **启动服务器**：
- en: '![Figure 5.37 – JupyterHub landing page showing Elyra Notebook Image with Spark'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.37 – 显示 Elyra Notebook 图像与 Spark 的 JupyterHub 登录页面](img/B18332_05_039.jpg)'
- en: '](img/B18332_05_038.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_038.jpg)'
- en: Figure 5.37 – JupyterHub landing page showing Elyra Notebook Image with Spark
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.37 – 显示 Elyra Notebook 图像与 Spark 的 JupyterHub 登录页面
- en: The notebook you have just started will also trigger the creation of a dedicated
    Spark cluster for you. It may take some time for the notebook to start because
    it has to wait for the Spark cluster to be ready.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚启动的笔记本也会触发为你创建一个专用的 Spark 集群。启动笔记本可能需要一些时间，因为它需要等待 Spark 集群准备好。
- en: Also, you may have noticed that the image you have configured in the `jupyterhub-singleuser-profiles-configmap.yaml`
    file is `quay.io/ml-aml-workshop/elyra-spark:0.0.4`, while the name we have selected
    is `manifests/jupyterhub-images/elyra-notebook-spark3-imagestream.yaml` file.
    You will find the descriptive text displayed on the JupyterHub landing page coming
    from the *annotations* section of this file. If you want to add your own images
    with specific libraries, you can just add another file here and it will be available
    for your team. This feature of JupyterHub enables the standardization of notebook
    container images, which allows everyone in the teams to have the same environment
    configurations and the same set of libraries.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可能已经注意到，在 `jupyterhub-singleuser-profiles-configmap.yaml` 文件中配置的镜像是 `quay.io/ml-aml-workshop/elyra-spark:0.0.4`，而我们选择的文件名称是
    `manifests/jupyterhub-images/elyra-notebook-spark3-imagestream.yaml`。你会发现显示在 JupyterHub
    登录页面上的描述性文本来自该文件的 *annotations* 部分。如果你想添加包含特定库的自定义镜像，只需在这里添加另一个文件，它就会对你的团队可用。JupyterHub
    的这一功能使笔记本容器镜像的标准化成为可能，从而使团队中的每个人都能拥有相同的环境配置和相同的库集。
- en: 'After the notebook has started, validate that the Spark cluster is provisioned
    for you. Note that this is the Spark cluster for the user of this notebook and
    is dedicated to this user only:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动笔记本后，验证 Spark 集群是否已经为你配置好。请注意，这是为该笔记本用户专用的 Spark 集群，仅限于该用户使用：
- en: '[PRE15]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You should see the following response. The response contains a notebook pod
    and two Spark pods; the one with a little `-m` character is the master, while
    the other is the worker. Notice how your username (`mluser`) is associated with
    the pod names:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应。该响应包含一个笔记本 Pod 和两个 Spark Pod；其中带有 `-m` 字符的是主节点，另一个是工作节点。注意，你的用户名（`mluser`）是如何与
    Pod 名称关联的：
- en: '![Figure 5.38 – Jupyter Notebook and Spark cluster pods'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.38 – Jupyter Notebook 和 Spark 集群 Pod](img/B18332_05_038.jpg)'
- en: '](img/B18332_05_039.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_039.jpg)'
- en: Figure 5.38 – Jupyter Notebook and Spark cluster pods
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.38 – Jupyter Notebook 和 Spark 集群 Pod
- en: Now, everyone in your team will get their own developer environment with dedicated
    Spark instances to write and test the data processing code.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你团队中的每个人都会获得自己的开发环境，并配备专用的 Spark 实例来编写和测试数据处理代码。
- en: Apache Spark provides a UI through which you can monitor applications and data
    processing jobs. The ODH-provisioned Spark cluster provides this GUI, and it is
    available at `https://spark-cluster-mluser.192.168.61.72.nip.io`. Make sure to
    change the IP address to your minikube IP address. You may also notice that the
    username you have used to log in to JupyterHub, `mluser`, is part of the URL.
    If you have used a different username, you may need to adjust the URL accordingly.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Apache Spark 提供了一个用户界面，通过该界面你可以监控应用程序和数据处理作业。ODH 提供的 Spark 集群提供了这个图形用户界面，并且它可以通过
    `https://spark-cluster-mluser.192.168.61.72.nip.io` 访问。确保将 IP 地址更改为你自己的 minikube
    IP 地址。你可能还会注意到，登录 JupyterHub 使用的用户名 `mluser` 是 URL 的一部分。如果你使用了不同的用户名，可能需要相应地调整
    URL。
- en: '![Figure 5.39 – Spark UI'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.39 – Spark 用户界面](img/B18332_05_039.jpg)'
- en: '](img/B18332_05_040.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_05_040.jpg)'
- en: Figure 5.39 – Spark UI
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.39 – Spark 用户界面
- en: The preceding UI mentions that you have one worker in the cluster, and you can
    click on the worker node to find out the executors running inside the worker node.
    If you want to refresh your knowledge of the Spark cluster, please refer to the
    *Understanding the basics of Apache Spark* section earlier in this chapter.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的用户界面提到你在集群中有一个工作节点，你可以点击该工作节点查看在其中运行的执行器。如果你想刷新你对 Spark 集群的了解，请参考本章前面的 *理解
    Apache Spark 基础知识* 部分。
- en: Open the `chapter5/hellospark.ipynb` file from your notebook. This is quite
    a simple job that calculates the square of the given array. Remember that Spark
    will automatically schedule the job and distribute it among executors. The notebook
    here is the Spark Driver program, which talks to the Spark cluster, and all of
    this is abstracted via the `SparkSession` object.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the second code cell of this notebook, you are creating a `SparkSession`
    object. The `getOrCreateSparkSession` utility function will connect to the Spark
    cluster provisioned for you by the platform.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: The last cell is where your data processing logic resides. In this example,
    the logic was to take the data and calculate the square of each element in the
    array. Once the data is processed, the `collect` method will bring the response
    to the driver that is running in the Spark application in your notebook.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.40 – A notebook with a simple Spark application'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_041.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.40 – A notebook with a simple Spark application
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Run > Run All cells** menu option, and the notebook will connect
    to the Spark cluster, and submit and execute your job.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: While the job is progressing, open the Spark UI at [https://spark-cluster-mluser.192.168.61.72.nip.io](https://spark-cluster-mluser.192.168.61.72.nip.io).
    Remember to adjust the IP address as per your settings, and click on the table
    with the **Application ID** heading under the **Running Applications** heading
    on this page.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.41 – Apache Spark UI'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_042.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.41 – Apache Spark UI
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the details page of the Spark application. Note that the application
    title, **Hello from ODH**, has been set up in your notebook. Click on the **Application
    Detail UI** link:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.42 – Spark UI showing the submitted Spark job'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_043.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.42 – Spark UI showing the submitted Spark job
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see a page showing the detailed metrics of the job that you have
    just executed on the Spark cluster from your Jupyter notebook:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.43 – Spark UI showing the submitted job details'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_044.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.43 – Spark UI showing the submitted job details
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you are done with your work, go to the **File > Hub Control Panel** menu
    option and click on the **Stop My Server** button:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.44 – Jupyter Notebook control panel'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_05_045.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.44 – Jupyter Notebook control panel
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that the Spark cluster has been terminated by issuing the following
    command:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You should not see a response because the pods are terminated by the Spark operator
    on your cluster.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: You have finally run a basic data processing job in an on-demand ephemeral Spark
    cluster that is running on Kubernetes. Note that you have done all this from a
    Jupyter notebook running on Kubernetes.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: With this capability in the platform, data engineers can perform huge data processing
    tasks directly from the browser. This capability also allows them to collaborate
    easily with each other to provide transformed, cleaned, high-quality data for
    your ML project.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 通过平台的这一功能，数据工程师可以直接从浏览器执行大规模的数据处理任务。这一功能还使他们能够轻松地协作，提供转换、清洗过的高质量数据，以支持你的ML项目。
- en: Summary
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you have just created your first ML platform. You have configured
    the ODH components via the ODH Kubernetes operator. You have seen how a data engineer
    persona will use JupyterHub to provision the Jupyter notebook and the Apache Spark
    cluster instance while the platform provides the provisioning of the environments
    automatically. You have also seen how the platform enables standardization of
    the operating environment via the container images, which bring consistency and
    security. You have seen how a data engineer could run Apache Spark jobs from the
    Jupyter notebook.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章节中，你刚刚创建了你的第一个ML平台。你通过ODH Kubernetes操作员配置了ODH组件。你已经看到，数据工程师如何使用JupyterHub来配置Jupyter
    notebook和Apache Spark集群实例，同时平台自动提供环境的配置。你还看到，平台如何通过容器镜像实现操作环境的标准化，确保一致性和安全性。你也看到了，数据工程师如何从Jupyter
    notebook中运行Apache Spark作业。
- en: All these capabilities allow the data engineer to work autonomously and in a
    self-serving fashion. You have seen that all these components were available autonomously
    and on-demand. The elastic and self-serving nature of the platform will allow
    teams to be more productive and agile while responding to the ever-changing requirements
    of the data and the ML world.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些功能使得数据工程师能够自主工作，并以自服务的方式操作。你已经看到，所有这些组件都是自主且按需提供的。平台的弹性和自服务特性将使团队在应对数据和ML领域不断变化的需求时更加高效和敏捷。
- en: In the next chapter, you will see how data scientists can benefit from the platform
    and be more efficient.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章节中，你将看到数据科学家如何从平台中受益并提高效率。
