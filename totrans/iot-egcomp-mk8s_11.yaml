- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing Storage Replication with OpenEBS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at two serverless frameworks that are available
    with MicroK8s, both of which are Kubernetes-based platforms for designing, deploying,
    and managing modern serverless workloads. We also noticed that the ease with which
    serverless frameworks can be implemented appears to be tied to the ease with which
    MicroK8s can be deployed. Some guiding principles to remember when creating and
    deploying serverless apps were also highlighted. We also realized that we needed
    to follow best practices to safeguard our resources, apps, and infrastructure
    service provider accounts.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look into the next use case for supporting cloud-native
    storage solutions, such as OpenEBS, to provide persistent storage for our container
    applications. Cloud-native storage solutions enable comprehensive storage mechanisms.
    These solutions mimic the properties of cloud environments, such as scalability,
    reliability, container architecture, and high availability. These features make
    it simple to interface with the container management platform and provide persistent
    storage for container-based applications.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will look at the Kubernetes storage basics before diving into OpenEBS
    concepts. Containers are ephemeral, which means they are established for a specific
    reason and then shut down after that task is completed. Containers do not maintain
    state data on their own, and a new container instance has no memory/state of prior
    ones. Although a container provides storage, it is only ephemeral storage, so
    it is wiped when the container is turned off. Developers will need to manage persistent
    storage as part of containerized applications as they adopt containers for new
    use cases. A developer, for example, may want to operate a database in a container
    and store the data in a volume that survives the container’s shutdown process.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides a variety of management options for clusters of containers.
    The ability to manage persistent storage is one of these capabilities. Administrators
    can use Kubernetes persistent storage to keep track of both persistent and non-persistent
    data in a Kubernetes cluster. Multiple applications that operate on the cluster
    can then utilize storage resources dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help manage persistent storage, Kubernetes supports two primary mechanisms:'
  prefs: []
  type: TYPE_NORMAL
- en: A **PersistentVolume** (**PV**) is a storage element that can be created manually
    or dynamically, depending on the storage class. It has a life cycle that is unaffected
    by the life cycle of Kubernetes pods. A pod can mount a PV, but the PV remains
    after the pod has shut down, and its data can still be accessed. Each PV can have
    its own set of parameters, such as disc type, storage tier, and performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **PersistentVolumeClaim** (**PVC**) is a storage request that’s made by a
    Kubernetes user. Based on the custom parameters, any application operating on
    a container can request storage and define the size and other properties of the
    storage it requires (for example, the specific type of storage, such as SSD storage).
    Based on the available storage resources, the Kubernetes cluster can provision
    a PV.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StorageClass` is a Kubernetes API object for configuring storage parameters.
    It’s a way of configuring a dynamic setup that generates new volumes based on
    demand. `StorageClass` defines the name of the volume plugin, as well as any external
    providers and a **Container Storage Interface** (**CSI**) driver, which allows
    containers to communicate with storage devices. CSI is a standard that allows
    containerized workloads to access any block and file storage systems.'
  prefs: []
  type: TYPE_NORMAL
- en: '`StorageClass` can be defined and PVs assigned by Kubernetes administrators.
    Each `StorageClass` denotes a different form of storage, such as fast SSD storage
    versus traditional magnetic drives or remote cloud storage. This enables a Kubernetes
    cluster to supply different types of storage based on the workload’s changing
    requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic volume provisioning is a feature of Kubernetes that allows storage volumes
    to be created on-demand. Administrators no longer need to manually build new storage
    volumes in their cloud or storage provider, then create PV objects to make them
    available in the cluster. When users request a specific storage type, the entire
    process is automated and provisioned. `StorageClass` objects are defined by the
    cluster administrator as needed. A volume plugin such as OpenEBS, also known as
    a provisioner, is referenced by each `StorageClass`. When a storage volume is
    automatically provisioned, the volume plugin provides a set of parameters and
    passes them to the provisioner.
  prefs: []
  type: TYPE_NORMAL
- en: The administrator can define many `StorageClass`, each of which can represent
    a distinct type of storage or the same storage with different specifications.
    This allows users to choose from a variety of storage solutions without having
    to worry about the implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: '**Container Attached Storage** (**CAS**) is quickly gaining traction as a viable
    option for managing stateful workloads and is becoming the favored method for
    executing durable, fault-tolerant stateful applications. CAS was brought to the
    Kubernetes platform via the OpenEBS project. It can be readily deployed in on-premises
    clusters, managed clusters in the public cloud, and even isolated air-gapped clusters.
    MicroK8s offers in-built support for OpenEBS via an add-on, making it the best
    solution for running Kubernetes clusters in air-gapped Edge/IoT scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of OpenEBS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring and implementing a PostgreSQL stateful workload
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes storage best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of OpenEBS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Kubernetes, storage is often integrated as an OS kernel module with individual
    nodes. Even the PVs are monolithic and legacy resources since they are strongly
    tied to the underlying components. CAS allows Kubernetes users to treat storage
    entities as microservices. CAS is made up of two parts: the control plane and
    the data plane. The control plane is implemented as a set of **Custom Resource
    Definitions** (**CRDs**) that deal with low-level storage entities. The data plane
    runs as a collection of pods close to the workload. It is in charge of the I/O
    transactions, which translate into read and write operations.'
  prefs: []
  type: TYPE_NORMAL
- en: The clean separation of the control plane and data plane provides the same benefits
    as running microservices on Kubernetes. This architecture decouples persistence
    from the underlying storage entities, allowing workloads to be more portable.
    It also adds scale-out capabilities to storage, allowing administrators and operators
    to dynamically expand volumes in response to the workload. Finally, CAS ensures
    that the data (PV) and compute (pod) are always co-located in a hyper-converged
    mode to maximize throughput and fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data is copied across many nodes using the synchronous replication feature
    of OpenEBS. The failure of a node would only affect the volume replicas on that
    node. The data on other nodes would remain available at the same performance levels,
    allowing applications to be more resilient to failures:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Synchronous replication  ](img/Figure_11.01_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Synchronous replication
  prefs: []
  type: TYPE_NORMAL
- en: Creating instantaneous snapshots are also possible with the OpenEBS CAS architecture.
    These can be made and managed with the regular `kubectl` command. This extensive
    integration with Kubernetes allows for job portability and easier data backup
    and migration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the typical components of OpenEBS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – OpenEBS control plane and data plane ](img/Figure_11.02_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – OpenEBS control plane and data plane
  prefs: []
  type: TYPE_NORMAL
- en: OpenEBS is a well-designed system built on CAS concepts. We’ll look at the architecture
    in more detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Control plane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The control plane, disk manager, and data plane are assigned to each storage
    volume that’s been installed. The control plane is closer to the storage infrastructure;
    it keeps track of the storage volumes that are joined to each cluster node through
    SAN or block storage. Provisioning volumes, initiating snapshots, creating clones,
    creating storage policies, enforcing storage policies, and exporting volume metrics
    to other systems such as Prometheus are all handled directly by the control plane.
  prefs: []
  type: TYPE_NORMAL
- en: An OpenEBS storage administrator interacts with the control plane to manage
    cluster-wide storage activities. Through an API server, the OpenEBS control plane
    is accessible to the outside world. A pod exposes the REST API for controlling
    resources such as volumes and policies. The declaration is initially submitted
    as a YAML file to the API server, which then starts the workflow. The API server
    communicates with the Kubernetes master’s API server to schedule volume pods in
    the data plane.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic provisioning is implemented via the control plane’s provisioner component
    using the standard Kubernetes external storage plugin. When an application builds
    a PVC from an existing storage class, the OpenEBS provisioner constructs a PV
    from the primitives in the storage class and binds it to the PVC.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenEBS control plane relies heavily on the `etcd` database, which serves
    as the cluster’s single source of truth.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen what the control plane it’s, let’s learn more about the
    data plane.
  prefs: []
  type: TYPE_NORMAL
- en: Data plane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data plane is close to the workload, which remains in the volume’s I/O path.
    It manages the life cycle of the PV and PVCs while running in the user space.
    A variety of storage engines with varied capabilities are available on the data
    plane. **Jiva**, **cStor**, and **Local PV** are the three storage engines that
    are available at the time of writing. Jiva provides standard storage capabilities
    (block storage) and is typically used for smaller-scale workloads compared to
    cStor, which offers enterprise-grade functionality and extensive snapshot features.
    Local PV, on the other hand, provides performance through advanced features such
    as replication and snapshots.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at each storage engine.
  prefs: []
  type: TYPE_NORMAL
- en: Storage engines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenEBS’s preferred storage engine is cStor. It’s a feature-rich and lightweight
    storage engine designed for high-availability workloads such as databases. It
    includes enterprise-level capabilities such as synchronous data replication, snapshots,
    clones, thin data provisioning, high data resiliency, data consistency, and on-demand
    capacity or performance increases. With just a single replica, cStor’s synchronous
    replication ensures excellent availability for stateful Kubernetes deployments.
    When a stateful application requires high data availability, cStor is set up with
    three replicas, with data written synchronously to each of the three replicas.
    Terminating and scheduling a new pod in a different node does not result in data
    loss because data is written to multiple replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Jiva was the first storage engine to be included in early OpenEBS versions.
    Jiva is the simplest of the options, as it runs entirely in user space and has
    conventional block storage features such as synchronous replication. Smaller applications
    running on nodes without the ability to install extra block storage devices benefit
    from Jiva. As a result, it is not appropriate for mission-critical tasks that
    require high performance or advanced storage capacities.
  prefs: []
  type: TYPE_NORMAL
- en: '**Local persistent volume** (**Local PV**) is OpenEBS’s third and simplest
    storage engine. Local PV is a local disc that’s attached directly to a single
    Kubernetes mode. Kubernetes applications can now consume high-performance local
    storage using the traditional volume APIs. OpenEBS’s Local PV is a storage engine
    that may build PVs on worker nodes using local discs or host paths. Local PV can
    be used by cloud-native apps that do not require advanced storage features such
    as replication, snapshots, or clones. A StatefulSet that manages replication and
    HA on its own, for example, can set up a Local PV based on OpenEBS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the storage engines mentioned previously, the **Mayastor data
    engine**, a low latency engine that is currently in development, has a declarative
    data plane, which provides flexible, persistent storage for stateful applications.
    It is Kubernetes-native and provides fast, redundant storage that works in any
    Kubernetes cluster. The Mayastor add-on will become available with MicroK8s 1.24:
    [https://microk8s.io/docs/addon-mayastor](https://microk8s.io/docs/addon-mayastor).'
  prefs: []
  type: TYPE_NORMAL
- en: Another optional and popular feature of OpenEBS is copy-on-write snapshots.
    Snapshots are created instantly, and there is no limit to the number of snapshots
    that can be created. The incremental snapshot feature improves data migration
    and portability across Kubernetes clusters, as well as between cloud providers
    or data centers. Common application scenarios include efficient replication for
    backups and the use of clones for troubleshooting or development against a read-only
    copy of data.
  prefs: []
  type: TYPE_NORMAL
- en: OpenEBS volumes also support backup and restore facilities that are compatible
    with Kubernetes backup and restore solutions such as Velero ([https://velero.io/](https://velero.io/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more, you can check out my blog post on how to back up and restore
    Kubernetes cluster resources, including PVs: [https://www.upnxtblog.com/index.php/2019/12/16/how-to-back-up-and-restore-your-kubernetes-cluster-resources-and-persistent-volumes/](https://www.upnxtblog.com/index.php/2019/12/16/how-to-back-up-and-restore-your-kubernetes-cluster-resources-and-persistent-volumes/).'
  prefs: []
  type: TYPE_NORMAL
- en: Through the container attached storage technique, OpenEBS extends the benefits
    of software-defined storage to cloud-native applications. For a thorough comparison
    and preferred use cases for each of the storage engines, see the OpenEBS documentation
    at [https://openebs.io/docs/](https://openebs.io/docs/).
  prefs: []
  type: TYPE_NORMAL
- en: To recap, OpenEBS creates local or distributed Kubernetes PVs from any storage
    available to Kubernetes worker nodes. This makes it simple for application and
    platform teams to implement Kubernetes stateful workloads that require fast, reliable,
    and scalable CAS. It also ensures that each storage volume has a separate pod
    and a set of replica pods, which are managed and deployed in Kubernetes like any
    other container or microservice. OpenEBS is also installed as a container, allowing
    for convenient storage service allocation on a per-application, cluster, or container
    basis.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s learn how to configure and implement a PostgreSQL stateful application
    while utilizing the OpenEBS Jiva storage engine.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and implementing a PostgreSQL stateful workload
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll configure and implement a PostgreSQL stateful workload
    while utilizing the OpenEBS storage engine. We’ll be using the Jiva storage engine
    for PostgreSQL persistence, creating test data, simulating node failure to see
    if the data is still intact, and confirming that OpenEBS replication is functioning
    as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that understand OpenEBS, we will delve into the steps of configuring and
    deploying OpenEBS on the cluster. The following diagram depicts our Raspberry
    Pi cluster setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – MicroK8s Raspberry Pi cluster ](img/Figure_11.03_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – MicroK8s Raspberry Pi cluster
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what we want to do, let’s look at the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you begin, you will need the following prerequisites to build a Raspberry
    Pi Kubernetes cluster and configure OpenEBS:'
  prefs: []
  type: TYPE_NORMAL
- en: A microSD card (4 GB minimum, 8 GB recommended)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A computer with a microSD card drive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Raspberry Pi 2, 3, or 4 (1 or more)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A micro-USB power cable (USB-C for the Pi 4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Wi-Fi network or an ethernet cable with an internet connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Optional) A monitor with an HDMI interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Optional) An HDMI cable for the Pi 2 and 3 and a micro-HDMI cable for the Pi
    4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Optional) A USB keyboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve established what the requirements are for testing a PostgresSQL
    stateful workload backed by OpenEBS, let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Creating the MicroK8s Raspberry Pi cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Please follow the steps that we covered in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070),
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters*,
    to create the MicroK8s Raspberry Pi cluster; here’s a quick refresher:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the OS image on the SD card:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the Wi-Fi access settings.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the remote access settings.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the control group settings.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the hostname.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Install and configure MicroK8s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a worker node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A fully functional multi-node Kubernetes cluster would look as follows. To
    summarize, we have installed MicroK8s on the Raspberry Pi boards and joined multiple
    deployments to form the cluster. We’ve also added nodes to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Fully functional MicroK8s Raspberry Pi cluster ](img/Figure_11.04_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Fully functional MicroK8s Raspberry Pi cluster
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s enable the OpenEBS add-on.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – Enabling the OpenEBS add-on
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The OpenEBS add-on is available with MicroK8s by default. Use the following
    command to enable OpenEBS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the following command indicates that the `iscsid` controller
    must be enabled as a prerequisite. For storage management, OpenEBS uses the **Internet
    Small Computer System Interface** (**iSCSI**) technology. The iSCSI protocol is
    a TCP/IP-based protocol for creating storage area networks and establishing and
    managing interconnections between IP storage devices, hosts, and clients (SANs).
    These SANs allow the SCSI protocol to be used in high-speed data transmission
    networks with block-level data transfer between different data storage networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Enabling the OpenEBS add-on ](img/Figure_11.05_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Enabling the OpenEBS add-on
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to enable the `iscsid` controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates that `iscsid` has been installed successfully.
    Now, we can enable the OpenEBS add-on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Enabling the iSCSI controller ](img/Figure_11.06_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Enabling the iSCSI controller
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output indicates that the OpenEBS add-on has been enabled successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Enabling the OpenEBS add-on ](img/Figure_11.07_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Enabling the OpenEBS add-on
  prefs: []
  type: TYPE_NORMAL
- en: 'The Helm3 add-on is also enabled by default. Before we move on, let’s make
    sure that all of the OpenEBS components are up and running using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates that all the components are `Running`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – The OpenEBS components are up and running ](img/Figure_11.08_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – The OpenEBS components are up and running
  prefs: []
  type: TYPE_NORMAL
- en: Now that the OpenEBS add-on has been enabled, let’s deploy a PostgreSQL stateful
    workload.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Deploying the PostgreSQL stateful workload
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To recap from [*Chapter 1*](B18115_01.xhtml#_idTextAnchor014), *Getting Started
    with Kubernetes*, a StatefulSet is a Kubernetes workload API object for managing
    stateful applications. In a typical deployment, the user is not concerned about
    how the pods are scheduled, so long as it has no negative impact on the deployed
    application. However, to preserve the state in stateful applications with persistent
    storage, pods must be identified. This functionality is provided by StatefulSet,
    which creates pods with a persistent identifier that corresponds to its value
    across rescheduling. This way, even if a pod is recreated, it will be correctly
    mapped to the storage volumes, and the application’s state will be preserved.
  prefs: []
  type: TYPE_NORMAL
- en: With the popularity of deploying database clusters in Kubernetes, managing states
    in a containerized environment have become even more important.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll need to set up the following resources to get the PostgreSQL configuration
    up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: Storage class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PersistentVolume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PersistentVolumeClaim
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StatefulSet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ConfigMap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To manage persistent storage, Kubernetes provides the `PersistentVolume` and
    `PersistentVolumeClaim` storage mechanisms, which we briefly discussed in the
    introduction. Here’s a quick rundown of what they are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PersistentVolume** (**PV**) is stored in a cluster that has been provisioned
    by a cluster administrator or dynamically provisioned using storage classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PersistentVolumeClaim** (**PVC**) is a user’s (developer’s) request for storage.
    It is comparable to a pod. PVCs consume PV resources, while pods consume node
    resources:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.9 – PV and PVC storage basics ](img/Figure_11.09_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – PV and PVC storage basics
  prefs: []
  type: TYPE_NORMAL
- en: Before we create a PV and PVC, let’s look at the storage class that OpenEBS
    has created for us.
  prefs: []
  type: TYPE_NORMAL
- en: '`StorageClass` allows administrators to describe the *classes* of storage that
    they provide. Different classes may correspond to different **Quality-of-Service**
    (**QoS**) levels, backup policies, or arbitrary policies that are determined by
    the cluster administrators.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to retrieve the storage class that has been created
    by OpenEBS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows that three `StorageClass` are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.10 – OpenEBS storage classes ](img/Figure_11.10_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – OpenEBS storage classes
  prefs: []
  type: TYPE_NORMAL
- en: '`openebs-hostpath` and `openebs-device` are recommended for single-node clusters.
    For multi-node clusters, `openebs-jiva-csi-default` is recommended.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must define `PersistentVolume`, which will use the storage class, as
    well as `PersistentVolumeClaim`, which will be used to claim this volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Because we’re utilizing an OpenEBS disc provisioner, we’ll need to specify where
    our data will be saved on the host node. We’ll use `/var/data/` in this case.
    The `accessMode` option is also crucial. We’ll use `ReadWriteOnce` in this case.
    This ensures that only one pod can write at any given moment. As a result, no
    two pods end up with the same writing volume. We can also specify the size of
    this volume, which we chose to be 5 GB.
  prefs: []
  type: TYPE_NORMAL
- en: Note on Access Modes
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though a volume supports several access modes, they can only be mounted
    one at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ReadOnlyMany** (**ROX**): Can be mounted by multiple nodes in read-only mode'
  prefs: []
  type: TYPE_NORMAL
- en: '**ReadWriteOnce** (**RWO**): Can be mounted by a single node in read-write
    mode'
  prefs: []
  type: TYPE_NORMAL
- en: '**ReadWriteMany** (**RWX**): Multiple nodes can be mounted in read-write mode'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to create the PV and PVC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates that `PersistentVolume` and `PersistentVolumeClaim`
    have been created successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11 – PV and PVC created successfully ](img/Figure_11.11_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – PV and PVC created successfully
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, let’s check if PV and PVC are `Bound`. A `Bound` state indicates
    that the application has access to the necessary storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12 – PV and PVC are bound ](img/Figure_11.12_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 – PV and PVC are bound
  prefs: []
  type: TYPE_NORMAL
- en: If a PVC becomes stuck waiting, `StatefulSet` will get stuck as well, as it
    will be unable to access its storage. As a result, double-check that both `StorageClass`
    and `PersistentVolume` have been set up correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve set up the PV and PVC, we’ll set up `ConfigMap` with configurations
    such as the username and password required for our setup. To keep things simple
    in this example, we’ve hardcoded the values inside `ConfigMap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following command to create `ConfigMap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates that the `postgres-configuration.yaml` file’s
    `ConfigMap` has been created successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.13 – PostgreSQL ConfigMap created ](img/Figure_11.13_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – PostgreSQL ConfigMap created
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the `describe` command to fetch the details of the `ConfigMap` object
    that we have created. The following output shows that the configuration required
    for our PostgreSQL setup is ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.14 – PostgreSQL ConfigMap ](img/Figure_11.14_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – PostgreSQL ConfigMap
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve defined our `ConfigMap` and storage volume, we can define a
    `StatefulSet` that will make use of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The definition of a `StatefulSet` is similar to that of deployments. We’ve
    added two more things:'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve loaded the environment variables from `ConfigMap` into the pod.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve defined our volume, which will map to `/var/lib/PostgreSQL/data` within
    our pod. This volume is defined using the PVC that we discussed earlier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we have also created a `Service` resource that will expose our database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to create the `StatefulSet` and `Service` resources
    to expose the database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates that both `StatefulSet` and `Service` have been
    created successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.15 – Postgres deployment succeeded ](img/Figure_11.15_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 – Postgres deployment succeeded
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, let’s verify that the pods and service have been created.
    The following output shows that the pods are `Running`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.16 – The Postgres pods are Running  ](img/Figure_11.16_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 – The Postgres pods are Running
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows that the service has been exposed on port `5432`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.17 – The Postgres service has been exposed ](img/Figure_11.17_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 – The Postgres service has been exposed
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also look at where the `StatefulSet` pods are distributed across the
    cluster using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows that the PostgreSQL database pods are running on
    two nodes (`1` in `controlplane` and `1` in `worker1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.18 – The database pods are running on two nodes ](img/Figure_11.18_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 – The database pods are running on two nodes
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have successfully configured PostgreSQL and it’s up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s create a test database and a table, and add a few records.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – Creating the test data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To create test data, use the `PgSQL` client or log into one of the pods so that
    we can create a test database and table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to log into the PostgreSQL pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows that we can log into the PostgreSQL pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.19 – Logging into one of the PostgreSQL pods ](img/Figure_11.19_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19 – Logging into one of the PostgreSQL pods
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have logged into the pod, we have access to the `psql` PostgreSQL
    client. Use the following command to create the test database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows that our test database, `inventory_mgmt`, has been
    created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.20 – Creating the test database ](img/Figure_11.20_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.20 – Creating the test database
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s switch our connection to the new database we have created using `\c inventory_mgmt`.
    The following output indicates that we have successfully switched to a new database.
    Now, we can create a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.21 – Switching the connection to the new database ](img/Figure_11.21_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.21 – Switching the connection to the new database
  prefs: []
  type: TYPE_NORMAL
- en: 'In the new database, use the `CREATE TABLE` command to create a test table.
    The following output indicates that a new table called `products_master` has been
    created successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.22 – Creating the test table ](img/Figure_11.22_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.22 – Creating the test table
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the test table has been created, use the `INSERT` command to add a
    few records, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.23 – Adding a few records to the test table ](img/Figure_11.23_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.23 – Adding a few records to the test table
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have added records to our test table. Before we move on, let’s use
    the `SELECT` command to list the records, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.24 – Records from the test table ](img/Figure_11.24_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.24 – Records from the test table
  prefs: []
  type: TYPE_NORMAL
- en: To recap, in this section, we have created a test database, created a new table,
    and added a few records to the table. Now, let’s simulate node failure.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – Simulating node failure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To simulate node failure, we will use the `cordon` command to mark the node
    as `unschedulable`. If the node is `unschedulable`, the Kubernetes controller
    will not schedule new pods on this node.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s locate the PostgreSQL database pod’s node and cordon it off, preventing
    new pods from being scheduled on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows that the database pods are running on `2` nodes
    (`1` in `controlplane` and `1` in `worker1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.25 – PostgreSQL database pods ](img/Figure_11.18_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.25 – PostgreSQL database pods
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use `cordon` on the `worker1` node so that new pods are prevented from
    being scheduled on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows that `worker1` has been cordoned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.26 – Cordoned Worker1 node ](img/Figure_11.26_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.26 – Cordoned Worker1 node
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though the `worker1` node has been cordoned, existing pods will still
    run, so we can use the `drain` command to delete all the pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows that `worker1` can’t be drained due to pods with
    local storage provisioned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.27 – Draining the Worker1 node ](img/Figure_11.27_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.27 – Draining the Worker1 node
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will use the `kubectl delete` command to delete the pod that is
    currently running on the cordoned node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows that pods running on `worker1` have been deleted
    successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.28 – Deleting the pods running on the Worker1 node ](img/Figure_11.28_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.28 – Deleting the pods running on the Worker1 node
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes controller will now recreate a new pod and schedule it in a different
    node as soon as the pod is deleted. It cannot be placed on the same node since
    scheduling has been disabled; this is because we cordoned the `worker1` node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s inspect where the pods are running using the `kubectl get pods` command.
    The following output shows that the new pod has been rescheduled to the `controlplane`
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.29 – PostgreSQL database pods ](img/Figure_11.29_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.29 – PostgreSQL database pods
  prefs: []
  type: TYPE_NORMAL
- en: Even though the PVC has a `ReadWriteOnce` access mode and is mounted by a specific
    node for read-write access, the new pod that has been recreated can use the same
    PVC that has been abstracted by the underlying `OpenEBS` volumes into a single
    storage layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify if the new pod is using the same PVC, let’s connect to the new pod
    and see if the data is still intact by using the `kubectl exec` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.30 – Logging into the PostgreSQL pod ](img/Figure_11.30_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.30 – Logging into the PostgreSQL pod
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows that the data is intact even after deleting the
    pod and rescheduling it on a different node. This confirms that the replication
    `OpenEBS` is working properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.31 – The data is intact ](img/Figure_11.31_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.31 – The data is intact
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, data engines are responsible for maintaining the actual state
    generated by stateful applications, as well as providing sufficient storage capacity
    to retain the information and ensure that it remains intact over time. For example,
    the state can be created once, accessed over the next few minutes or days, updated,
    or simply left to be retrieved months or years later. You can use **Local PV**,
    **Jiva**, **cStor**, or **Mayastor**, depending on the type of storage associated
    with your Kubernetes worker nodes and your application performance needs.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an engine is entirely dependent on your platform (resources and storage
    type), the application workload, and the application’s current and future capacity
    and/or performance growth. In the next section, we’ll look at some Kubernetes
    storage best practices, as well as some recommendations for data engines.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes storage best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For modern containerized applications deployed on Kubernetes, storage is a
    crucial concern. Kubernetes has progressed from local node filesystems mounted
    in containers to NFS, and finally to native storage, as described by the CSI specification,
    which allows for data durability and sharing. In this section, we’ll look at some
    of the best practices to take into consideration when configuring a PV:'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid statically creating and allocating PVs to decrease management costs and
    facilitate scaling. Use dynamic provisioning instead. Define an appropriate reclaim
    policy in your storage class to reduce storage costs when pods are deleted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each node can only support a certain number of sizes, so different node sizes
    provide varying amounts of local storage and capacity. To install the optimum
    node sizes, plan accordingly for your application’s demands.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The life cycle of a PV is independent of any individual container in the cluster.
    A PVC is a request for a specific type of storage made by a container user or
    application. Kubernetes documentation suggests the following for building a PV:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PVCs should always be included in the container setup.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: PVs should never be used in container configuration since they will bind a container
    to a specific volume.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: PVCs that don’t specify a specific class will fail if they don’t have a default
    `Storage Class`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Give Storage Classes names that are meaningful.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: At the namespace level, resource quotas are also provided, giving you another
    level of control over cluster resource utilization. The total amount of CPU, memory,
    and storage resources that all the containers executing in the namespace can utilize
    is limited by resource limits. It can also set storage resource limits based on
    service levels or backup requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistent storage hardware comes in a variety of shapes and sizes. SSDs, for
    example, outperform HDDs in terms of read/write performance, and NVMe SSDs are
    especially well-suited to high workloads. QoS criteria are added to the description
    of a PVC by some of the Kubernetes providers. This means that it prioritizes read/write
    volumes for specific installations, allowing for higher performance if the application
    requires it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s look at some of the guidelines for selecting OpenEBS data engines.
  prefs: []
  type: TYPE_NORMAL
- en: Guidelines on choosing OpenEBS data engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each storage engine has its advantages, as shown in the following table. Choosing
    an engine is entirely dependent on your platform (resources and storage type),
    the application workload, and the application’s current and future capacity and/or
    performance growth. The following guidelines will assist you in selecting an engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 11.1 – Choosing OpenEBS data engines ](img/Table_11.01_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 11.1 – Choosing OpenEBS data engines
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, OpenEBS offers a set of data engines, each of which is designed
    and optimized for executing stateful workloads with varied capabilities on Kubernetes
    nodes with varying resource levels. In a Kubernetes cluster, platform SREs or
    administrators often choose one or more data engines. These data engines are chosen
    based on node capabilities or stateful application capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how Kubernetes persistent storage provides a convenient
    way for Kubernetes applications to request and consume storage resources. The
    PVC is declared by the user’s pod, and Kubernetes will find a PV to pair it with.
    If there is no PV to pair with, then it will go to the corresponding `StorageClass`
    and assist it in creating a PV before binding it to the PVC. The newly created
    PV must use the attached master node to create a remote disc for the host and
    then mount the attached remote disc to the host directory using the `kubelet`
    component of each node.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has made significant improvements to facilitate running stateful
    workloads by giving platform (or cluster administrators) and application developers
    the necessary abstractions. These abstractions ensure that different types of
    file and block storage (whether ephemeral or persistent, local or remote) are
    available wherever a container is scheduled (including provisioning/creating,
    attaching, mounting, unmounting, detaching, and deleting volumes), storage capacity
    management (container ephemeral storage usage, volume resizing, and generic operations),
    and influencing container scheduling based on storage (data gravity, availability,
    and so on).
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to deploy the Istio and Linkerd service
    mesh. You will also learn how to deploy and run a sample application, as well
    as how to configure and access dashboards.
  prefs: []
  type: TYPE_NORMAL
