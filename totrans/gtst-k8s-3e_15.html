<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Kubernetes Infrastructure Management</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll discuss how to make changes to the infrastructure that powers your Kubernetes infrastructure, whether or not it is a purely public cloud platform or a hybrid installation. We'll discuss methods for handling underlying instance and resource instability, and strategies for running highly available workloads on partially available underlying hardware. We'll cover a few key topics in this chapter in order to build your understanding of how to manage infrastructure in this way:</p>
<ul>
<li style="font-weight: 400">How do we plan to deploy Kubernetes components?</li>
<li style="font-weight: 400">How do we secure Kubernetes infrastructure?</li>
<li style="font-weight: 400">How do we upgrade the cluster and <kbd>kubeadm</kbd>?</li>
<li style="font-weight: 400">How do we scale up the cluster?</li>
<li>What external resources are available to us?</li>
</ul>
<p>In this chapter, you'll learn about the following:</p>
<ul>
<li class="mce-root">Cluster upgrades</li>
<li class="mce-root">How to manage <kbd>kubeadm</kbd></li>
<li class="mce-root">Cluster scaling</li>
<li class="mce-root">Cluster maintenance</li>
<li class="mce-root">The SIG Cluster Lifecycle group</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You'll need to have your Google Cloud Platform account enabled and logged in, or you can use a local Minikube instance of Kubernetes. You can also use Play with Kubernetes over the web: <a href="https://labs.play-with-k8s.com/">https://labs.play-with-k8s.com/</a>.</p>
<p class="mce-root"/>
<p>Here's the GitHub repository for this chapter: <a href="https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter15">https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter15</a><a href="https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code%20files/Chapter%2015">.</a></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Planning a cluster</h1>
                </header>
            
            <article>
                
<p>Looking back over the work we've done up till now in this book, there are a lot of options when it comes to building a cluster with Kubernetes. Let's briefly highlight the options you have available to you when you're planning on building your cluster. We have a few key areas to investigate when planning ahead.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Picking what's right</h1>
                </header>
            
            <article>
                
<p>The first and arguably most important step when choosing a cluster is to pick the right hosted platform for your Kubernetes cluster. At a high level, here are the choices you have:</p>
<ul>
<li style="font-weight: 400">Local solutions include the following:
<ul>
<li style="font-weight: 400"><strong>Minikube</strong>: A single-node Kubernetes cluster</li>
<li style="font-weight: 400"><strong>Ubuntu on LXD</strong>: This uses LXD to deploy a nine-instance cluster of Kubernetes</li>
<li style="font-weight: 400"><strong>IBM's Cloud Private-CE</strong>: This uses VirtualBox to deploy Kubernetes on <em>n+1</em> instances</li>
<li style="font-weight: 400"><kbd>kubeadm-dind</kbd> (Docker-in-Docker): This allows for multi-node Kubernetes clusters</li>
</ul>
</li>
<li style="font-weight: 400">Hosted solutions include the following:
<ul>
<li style="font-weight: 400">Google Kubernetes Engine</li>
<li style="font-weight: 400">Amazon Elastic Container Services</li>
<li style="font-weight: 400">Azure Kubernetes Service</li>
<li style="font-weight: 400">Stackpoint</li>
<li style="font-weight: 400">Openshift online</li>
<li style="font-weight: 400">IBM Cloud Kubernetes Services</li>
<li style="font-weight: 400">Giant Swarm</li>
</ul>
</li>
<li style="font-weight: 400">On all of the aforementioned clouds and more, there are many turnkey solutions that allow you to spin up full clusters with community-maintained scripts</li>
</ul>
<p>As of this book's publishing, here's a list of projects and solutions:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="Images/fda0d0fd-7b11-4640-bd88-83cbf844a383.png" style="width:26.92em;height:46.00em;" width="402" height="686"/></p>
<div class="mce-root packt_infobox"><span>Check out this link for more turnkey solutions: </span><a href="https://kubernetes.io/docs/setup/pick-right-solution/#turnkey-cloud-solutions">https://kubernetes.io/docs/setup/pick-right-solution/#turnkey-cloud-solutions</a>.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Securing the cluster</h1>
                </header>
            
            <article>
                
<p>As we've discussed, there are several areas of focus when securing a cluster. Ensure that you have read through and made configuration changes (in code) to your cluster configuration in the following areas:</p>
<ul>
<li><strong>Logging</strong>: Ensure that your Kubernetes logs are enabled. You can read more about audit logging here: <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">https://kubernetes.io/docs/tasks/debug-application-cluster/audit/</a>.</li>
<li>Make sure you have authentication enabled so that your users, operators, and services identify themselves as unique identifiers. Read more about authentication here: <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/">https://kubernetes.io/docs/reference/access-authn-authz/authentication/</a>.</li>
<li>Ensure that you have proper separation of duties, role-based access control, and fine grained privileges using authorization. You can read more about HTTP-based controls here: <a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/">https://kubernetes.io/docs/reference/access-authn-authz/authorization/</a>.</li>
<li>Make sure that you have locked down the API to specific permissions and groups. You can read more about access to the API here: <a href="https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/">https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/</a>.</li>
<li>When appropriate, enable an admission controller to further re-validate requests after they pass through the authentication and authorization controls. These controllers can take additional, business-logic based validation steps in order to secure your cluster further. Read more about admission controllers here: <a href="https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/">https://kubernetes.io/docs/reference/access-authn-authz/controlling-access</a>.</li>
<li>Tune Linux system parameters via the <kbd>sysctl</kbd> interface. This allows you to modify kernel parameters for node-level and namespaced <kbd>sysctl</kbd> features. There are safe and unsafe system parameters. There are several subsystems that can be tweaked with <kbd>sysctls</kbd>. Possible parameters are as follows:
<ul>
<li style="font-weight: 400"><kbd>abi</kbd>: Execution domains and personalities</li>
<li style="font-weight: 400"><kbd>fs</kbd>: Specific filesystems, filehandle, inode, dentry, and quota tuning</li>
<li style="font-weight: 400"><kbd>kernel</kbd>: Global kernel information/tuning</li>
<li style="font-weight: 400"><kbd>net</kbd>: Networking</li>
<li style="font-weight: 400"><kbd>sunrpc</kbd>: SUN <strong>Remote Procedure Call</strong> (<strong>RPC</strong>)</li>
<li style="font-weight: 400"><kbd>vm</kbd>: Memory management tuning, buffer, and cache management</li>
<li style="font-weight: 400"><kbd>user</kbd>: Per user per user namespace limits</li>
</ul>
</li>
</ul>
<p style="padding-left: 60px">You can read more about <kbd>sysctl</kbd> calls here: <a href="https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/">https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/</a>.</p>
<div class="packt_tip"><span>You can enable unsafe <kbd>sysctl</kbd> values by running the following command:<br/></span><br/>
<kbd><span>kubelet --allowed-unsafe-sysctls ‘net.ipv4.route.min_pmtu'</span></kbd></div>
<p>Here's a diagram of the authorization, authentication, and admission control working together:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="Images/5c3dd62f-3d30-428a-b3cb-95efa32abfcc.png" width="1871" height="786"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Tuning examples</h1>
                </header>
            
            <article>
                
<p>If you'd like to experiment with modifying <kbd>sysctls</kbd>, you can set a security context as follows, per pod:</p>
<pre>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/> name: sysctl-example<br/>spec:<br/> securityContext:<br/>   sysctls:<br/>   - name: kernel.shm_rmid_forced<br/>     value: "0"<br/>   - name: net.core.somaxconn<br/>     value: "10000"<br/>   - name: kernel.msgmax<br/>     value: "65536"<br/>   - name: ipv4.ip_local_port_range<br/>      value: ‘1024 65535'</pre>
<p>You can also tune variables such as the ARP cache, as Kubernetes consumes a lot of IPs at scale, which can exhaust space in the ARP cache. Changing these settings is common in large scale HPC clusters and can help with address exhaustion with Kubernetes as well. You can set these values, as follows:</p>
<pre>net.ipv4.neigh.default.gc_thresh1 = 90000<br/>net.ipv4.neigh.default.gc_thresh2 = 100000<br/>net.ipv4.neigh.default.gc_thresh3 = 120000</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Upgrading the cluster</h1>
                </header>
            
            <article>
                
<p>In order to run your cluster over long periods of time, you'll need to update your cluster as needed. There are several ways to manage cluster upgrades, and the difficulty level of the upgrades is determined by the platform you've chosen previously. As a general rule, hosted <strong>Platform as a service</strong> (<strong>PaaS</strong>) options are simpler, while roll your own options rely on you to manage your cluster upgrades.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Upgrading PaaS clusters</h1>
                </header>
            
            <article>
                
<p>Upgrading PaaS clusters is a lot simpler than updating your hand-rolled clusters. Let's check out how the major cloud service providers update their hosted Kubernetes platforms.</p>
<p>With Azure, it's relatively straightforward to manage an upgrade of both the control plane and nodes of your cluster. You can check which upgrades are available for your cluster with the following command:</p>
<pre><strong>az aks get-upgrades --name “myAKSCluster” --resource-group myResourceGroup --output table</strong><br/><strong>Name ResourceGroup MasterVersion NodePoolVersion Upgrades</strong><br/><br/><strong>------- --------------- --------------- ----------------- -------------------</strong><br/><br/><strong>default gsw-k8s-aks 1.8.10 1.8.10 1.9.1, 1.9.2, 1.9.6</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>When upgrading AKS clusters, you have to upgrade through minor versions. AKS handles adding a new node to your cluster and manages to cordon and drain process in order to prevent any disruption to your running applications. You can see how the drain process works in a following section.</p>
<p>You can run the <kbd>upgrade</kbd> command as follows. You should experiment with this feature before running on production workloads so you can understand the impact on running applications:</p>
<pre><strong>az aks upgrade --name myAKSCluster --resource-group myResourceGroup --kubernetes-version 1.9.6</strong></pre>
<p>You should see a lot of output that identifies the update, which will look something like this:</p>
<pre>{<br/>  "id": "/subscriptions/&lt;Subscription ID&gt;/resourcegroups/myResourceGroup/providers/Microsoft.ContainerService/managedClusters/myAKSCluster",<br/>  "location": "eastus",<br/>  "name": "myAKSCluster",<br/>  "properties": {<br/>    "accessProfiles": {<br/>      "clusterAdmin": {<br/>        "kubeConfig": "..."<br/>      },<br/>      "clusterUser": {<br/>        "kubeConfig": "..."<br/>      }<br/>    },<br/>    "agentPoolProfiles": [<br/>      {<br/>        "count": 1,<br/>        "dnsPrefix": null,<br/>        "fqdn": null,<br/>        "name": "myAKSCluster",<br/>        "osDiskSizeGb": null,<br/>        "osType": "Linux",<br/>        "ports": null,<br/>        "storageProfile": "ManagedDisks",<br/>        "vmSize": "Standard_D2_v2",<br/>        "vnetSubnetId": null<br/>      }<br/>    ],<br/>    "dnsPrefix": "myK8sClust-myResourceGroup-4f48ee",<br/>    "fqdn": "myk8sclust-myresourcegroup-4f48ee-406cc140.hcp.eastus.azmk8s.io",<br/>    "kubernetesVersion": "1.9.6",<br/>    "linuxProfile": {<br/>      "adminUsername": "azureuser",<br/>      "ssh": {<br/>        "publicKeys": [<br/>          {<br/>            "keyData": "..."<br/>          }<br/>        ]<br/>      }<br/>    },<br/>    "provisioningState": "Succeeded",<br/>    "servicePrincipalProfile": {<br/>      "clientId": "e70c1c1c-0ca4-4e0a-be5e-aea5225af017",<br/>      "keyVaultSecretRef": null,<br/>      "secret": null<br/>    }<br/>  },<br/>  "resourceGroup": "myResourceGroup",<br/>  "tags": null,<br/>  "type": "Microsoft.ContainerService/ManagedClusters"<br/>}</pre>
<p>You can additionally show the current version:</p>
<pre><strong>az aks show --name myAKSCluster --resource-group myResourceGroup --output table</strong></pre>
<p>To upgrade a GCE cluster, you'll follow a similar procedure. In GCE's case, there are two mechanisms that allow you update your cluster:</p>
<ul>
<li style="font-weight: 400">For manager node upgrades, GCP deletes and recreates the master nodes using the same <strong>Persistent Disk</strong> (<strong>PD</strong>) to preserve your state across upgrades</li>
<li style="font-weight: 400">With your worker nodes, you'll use GCP's manage instance groups and perform a rolling upgrade of your cluster, wherein each node is destroyed and replaced to avoid interruption to your workloads</li>
</ul>
<p>You can upgrade your cluster master to a specific version:</p>
<pre><strong>cluster/gce/upgrade.sh -M v1.0.2</strong></pre>
<p>Or, you can update your full cluster with this command:</p>
<pre><strong>cluster/gce/upgrade.sh -M v1.0.2</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To upgrade a Google Kubernetes Engine cluster, you have a simple, user-initiated option. You'll need to set your project ID:</p>
<pre><strong>gcloud config set project [PROJECT_ID]</strong></pre>
<p>And, make sure that you have the latest set of <kbd>gcloud</kbd> components:</p>
<pre><strong>gcloud components update</strong></pre>
<p>When updating Kubernetes clusters on GCP, you get the following benefits. You can downgrade your nodes, but you cannot downgrade your master:</p>
<ul>
<li style="font-weight: 400">GKE will handle node and pod drainage without application interruption</li>
<li style="font-weight: 400">Replacement nodes will be recreated with the same node and configuration as their predecessors</li>
<li style="font-weight: 400">GKE will update software for the following pieces of the cluster:
<ul>
<li style="font-weight: 400"><kbd>kubelet</kbd></li>
<li style="font-weight: 400"><kbd>kube-proxy</kbd></li>
<li style="font-weight: 400">Docker daemon</li>
<li style="font-weight: 400">OS</li>
</ul>
</li>
</ul>
<p>You can see what options your server has for upgrades with this command:</p>
<pre><strong>gcloud container get-server-config</strong></pre>
<p>Keep in mind that data stored in the <kbd>hostPath</kbd> and <kbd>emptyDir</kbd> directories will be deleted during the upgrade, and only PDs will be preserved during it. You can turn on automatic node updates for your cluster with GKE, or you can perform them manually.</p>
<div class="packt_tip packt_infobox">To turn on automatic node automatic upgrades read this: <strong><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-upgrades">https://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-upgrades</a>.</strong></div>
<p>You can also create clusters with this set to default with the <kbd>--enable-autoupgrade</kbd> command:</p>
<pre><strong>gcloud container clusters create [CLUSTER_NAME] --zone [COMPUTE_ZONE] \</strong><br/><strong> --enable-autoupgrade</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>If you'd like to update your clusters manually, you can issue specific commands. It is recommended for production systems to turn off automatic upgrades and to perform them during periods of low traffic or during maintenance windows to ensure minimal disruption for your applications. Once you build confidence in updates, you may be able to experiment with auto-upgrades.</p>
<p>To manually kick off a node upgrade, you can run the following command:</p>
<pre><strong>gcloud container clusters upgrade [CLUSTER_NAME]</strong></pre>
<p>If you'd like to upgrade to a specific version of Kubernetes, you can add the <kbd>--cluster-version</kbd> tag.</p>
<p>You can see a running list of operations on your cluster to keep track of the update operation:</p>
<pre><strong>gcloud beta container operations list</strong><br/><strong>NAME TYPE ZONE TARGET STATUS_MESSAGE STATUS START_TIME END_TIME</strong><br/><strong>operation-1505407677851-8039e369 CREATE_CLUSTER us-west1-a my-cluster DONE 20xx-xx-xxT16:47:57.851933021Z 20xx-xx-xxT16:50:52.898305883Z</strong><br/><strong>operation-1505500805136-e7c64af4 UPGRADE_CLUSTER us-west1-a my-cluster DONE 20xx-xx-xxT18:40:05.136739989Z 20xx-xx-xxT18:41:09.321483832Z</strong><br/><strong>operation-1505500913918-5802c989 DELETE_CLUSTER us-west1-a my-cluster DONE 20xx-xx-xxT18:41:53.918825764Z 20xx-xx-xxT18:43:48.639506814Z</strong></pre>
<p>You can then describe your particular upgrade operation with the following:</p>
<pre><strong>gcloud beta container operations describe [OPERATION_ID]</strong></pre>
<p>The previous command will tell you details about the cluster upgrade action:</p>
<pre><strong>gcloud beta container operations describe operation-1507325726639-981f0ed6</strong><br/><strong>endTime: '20xx-xx-xxT21:40:05.324124385Z'</strong><br/><strong>name: operation-1507325726639-981f0ed6</strong><br/><strong>operationType: UPGRADE_CLUSTER</strong><br/><strong>selfLink: https://container.googleapis.com/v1/projects/.../kubernetes-engine/docs/zones/us-central1-a/operations/operation-1507325726639-981f0ed6</strong><br/><strong>startTime: '20xx-xx-xxT21:35:26.639453776Z'</strong><br/><strong>status: DONE</strong><br/><strong>targetLink: https://container.googleapis.com/v1/projects/.../kubernetes-engine/docs/zones/us-central1-a/clusters/...</strong><br/><strong>zone: us-central1-a</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scaling the cluster</h1>
                </header>
            
            <article>
                
<p>As with PaaS versus hosted clusters, you have several options for scaling up your production Kubernetes cluster.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">On GKE and AKS</h1>
                </header>
            
            <article>
                
<p>When upgrading a GKE cluster, all you need to do is issue a scaling command that modifies the number of instances in your minion group. You can resize the node pools that control your cluster with the following:</p>
<pre><strong><span>gcloud container clusters resize [CLUSTER_NAME] \</span></strong><br/><strong><span> --node-pool [POOL_NAME]</span></strong><br/><strong><span> --size [SIZE]</span></strong></pre>
<p>Keep in mind that new nodes are created with the same configuration as the current machines in your node pool. When additional pods are scheduled, they'll be scheduled on the new nodes. Existing pods will not be relocated or rebalanced to the new nodes.</p>
<p>Scaling up the AKS cluster engine is a similar exercise, where you'll need to specify the <kbd>--resource-group</kbd> node count to your required number of nodes:</p>
<pre><strong>az aks scale --name myAKSCluster --resource-group gsw-k8s-group --node-count 1</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">DIY clusters</h1>
                </header>
            
            <article>
                
<p>When you add resources to your hand-rolled Kubernetes cluster, you'll need to do more work. In order to have nodes join in as you add them automatically via a scaling group, or manually via Infrastructure as code, you'll need to ensure that automatic registration of nodes is enabled via th<span>e</span> <kbd>--register-node</kbd> <span>flag. If  this flag is turned on, new nodes will attempt to auto-register themselves. This is the default behavior.</span></p>
<p>You can also join nodes manually, using a pre-vetted token, to your clusters. If you initialize <kbd>kubeadm</kbd> with the following token:</p>
<pre><strong>kubeadm init --token=101tester101 --kubernetes-version $(kubeadm version -o short)</strong></pre>
<p>You can then add additional nodes to your clusters with this command:</p>
<pre><strong>kubeadm join --discovery-token-unsafe-skip-ca-verification --token=101tester101:6443</strong></pre>
<p>Normally in a production install of <kbd>kubeadm</kbd>, you would not specify the token and need to extract it and store it from the <kbd>kubeadm init</kbd> command.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Node maintenance</h1>
                </header>
            
            <article>
                
<p>If you're scaling your cluster up or down, it's essential to know how the manual process of node deregistration and draining works. We'll use the <kbd>kubectl</kbd> drain command here to remove all pods from your node before removing the node from your cluster. Removing all pods from your nodes ensures that there are not running workloads on your instance or VM when you remove it.</p>
<p>Let's get a list of available nodes using the following command:</p>
<pre><strong>kubectl get nodes</strong></pre>
<p>Once we have the node list, the command to drain nodes is fairly simple:</p>
<pre><strong>kubectl drain &lt;node&gt;</strong></pre>
<p>This command will take some time to execute, as it has to reschedule the workloads on the node onto other machines that have available resources. Once the draining is complete, you can remove the node via your preferred programmatic API. If you're merely removing the node for maintenance, you can add it back to the available nodes with the <kbd>uncordon</kbd> command:</p>
<pre><strong>kubectl uncordon &lt;node&gt;</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Additional configuration options</h1>
                </header>
            
            <article>
                
<p>Once you've built up an understanding of how Kubernetes cluster configuration is managed, it's a good idea to explore the additional tools that offer enhanced mechanisms or abstractions to configure the state of your clusters.</p>
<p>ksonnet is one such tool, which allows you to build a structure around your various configurations in order to keep many environments configured. ksonnet uses another powerful tool called Jsonnet in order to maintain the state of the cluster. ksonnet is a different approach to cluster management that's different from the Helm approach we discussed in earlier chapters, in that it doesn't define packages by dependency, but instead takes a composable prototype approach, where you build JSON templates that are rendered by the ksonnet CLI to apply state on the cluster. You start with parts that create prototypes, which becomes a component once it's configured, and those components can then get combined into applications. This helps avoid repeated code in your code base. Check it out here: <a href="https://ksonnet.io/">https://ksonnet.io/</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed how to make changes to the infrastructure that provides compute, storage, and networking capacity to your Kubernetes infrastructure, whether it be a purely public cloud platform or a hybrid installation. In observing the public cloud platforms, we discussed methods for handling underlying instance and resource instability, and strategies for running highly available workloads on partially available underlying hardware.</p>
<p>Additionally, we covered a key topic on how to build infrastructure using tools such as <kbd>kubeadm</kbd>, <kbd>kubectl</kbd>, and public cloud provider tools that can scale up and down your clusters.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li style="font-weight: 400">Name two available local solutions for Kubernetes</li>
<li style="font-weight: 400">Name three hosted solutions for Kubernetes</li>
<li style="font-weight: 400">What are four of the key areas for securing your cluster?</li>
<li style="font-weight: 400">What is the command to upgrade each of the major CSPs hosted Kubernetes clusters?</li>
<li style="font-weight: 400">Which cloud provider has the most production ready PaaS for Kubernetes?</li>
<li style="font-weight: 400">Which command is use to take a node out of rotation?</li>
<li style="font-weight: 400">Which command is used to add it back in?</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>If you'd like to learn more about Jsonnet, check out this link: <a href="https://jsonnet.org/">https://jsonnet.org/</a>.</p>


            </article>

            
        </section>
    </div>



  </body></html>