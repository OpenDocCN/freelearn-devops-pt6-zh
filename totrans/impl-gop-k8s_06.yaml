- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GitOps Architectural Designs and Operational Control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the rapidly evolving landscape of cloud-native technologies, understanding
    and effectively implementing various architectural frameworks becomes crucial
    for organizations seeking to harness the full potential of Kubernetes. As we dive
    deeper into this topic in this chapter, we will explore diverse architectures
    that not only enable multi-cluster management – a theme partially introduced in
    the previous chapter – but also facilitate effective **GitOps** implementations
    for service and product deployments utilized by various companies.
  prefs: []
  type: TYPE_NORMAL
- en: Our journey will take us through real-world scenarios and practical insights
    from projects that have employed different architectural approaches. By examining
    how various companies have successfully integrated GitOps methodologies to deploy
    and manage their services and products, we gain valuable perspectives on what
    works in different contexts. This chapter will particularly benefit **platform
    engineers**, **SREs**, and **internal developer platform** builders as it focuses
    on deploying various operational models used by teams to provide their workloads
    or the platform context.
  prefs: []
  type: TYPE_NORMAL
- en: We will delve into the nuances of managing Kubernetes clusters and workloads
    using tools such as **Argo CD**, **Flux CD**, and **Cluster API**. These tools
    are at the forefront of enabling efficient and scalable management of Kubernetes
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a comprehensive understanding of how
    different architectural choices impact the effectiveness and efficiency of Kubernetes
    deployments, particularly in the context of GitOps. Whether you’re a platform
    engineer crafting the infrastructure, an SRE ensuring its reliability, or a developer
    building internal platforms, the insights shared here will be invaluable in your
    pursuit of operational excellence in cloud-native environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'As such, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring diverse GitOps architectural frameworks for Kubernetes environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining the impact of architectural choices on GitOps’ effectiveness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tailoring designs for scalability, resilience, and efficiency in cloud-native
    deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralized control – managing clusters with a solo Argo instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dedicated instances – instance per cluster with Argo CD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dedicated instances – instance per cluster with Flux CD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The middle way – instance per logical group with Argo CD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cockpit and fleet approach with Argo CD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralized Kubernetes cluster creation – leveraging Cluster API and Argo CD
    for streamlined cluster deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A deep dive into Cluster API and GitOps – hands-on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring diverse GitOps architectural frameworks for Kubernetes environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring diverse GitOps architectural frameworks for Kubernetes environments
    is crucial for organizations looking to streamline their deployment pipelines
    and operational workflows. GitOps, a term coined by **Weaveworks**, emphasizes
    the use of Git as the single source of truth for declarative infrastructure and
    applications. In Kubernetes environments, this translates to a series of best
    practices and patterns that guide the management and automation of container orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: Several architectural frameworks within GitOps cater to different organizational
    needs and technical contexts. The choice of framework often depends on the complexity
    of the environment, the scale of the operations, and the governance requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The adoption of GitOps influences architectural decisions in Kubernetes in
    several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Infrastructure as Code (IaC)**: With GitOps, the entire Kubernetes architecture
    is defined as code – typically YAML files that describe the desired state of the
    system. This approach enables developers and operations teams to collaborate on
    infrastructure changes, which can be versioned, reviewed, and audited just like
    application code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Immutable infrastructure**: The architectural frameworks that embrace GitOps
    often prioritize immutability. Once a resource is deployed, it should not be changed
    manually in the running environment. Instead, any modifications are made in the
    Git repository, which triggers a deployment process to update the infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modular design**: GitOps encourages a modular approach to infrastructure.
    Each module, or set of Kubernetes resources, can be managed as a separate project
    within Git. This modularization aligns with Kubernetes’ architectural philosophy
    of microservices, where each service can be deployed, scaled, and managed independently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated deployment strategies**: Architectural frameworks under GitOps
    often incorporate advanced deployment strategies such as canary releases, blue-green
    deployments, and A/B testing. GitOps tooling automates the rollout and monitoring
    of these strategies, making it easier to implement them in a controlled manner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment parity**: GitOps ensures that each environment – from development
    to staging to production – can be replicated with a high degree of fidelity. This
    is achieved by using the same declarative configurations across environments,
    reducing the “works on my machine” syndrome.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and compliance**: By defining architectural elements as code in
    a Git repository, GitOps enables the application of security policies and compliance
    checks as part of the **continuous integration/continuous deployment** (**CI/CD**)
    pipeline. This means that security becomes a part of the architecture by design,
    not an afterthought.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single repository versus multiple repositories**: Some organizations opt
    for a single repository containing all configurations and applications, which
    simplifies management but may not scale well with large teams or complex applications.
    Others prefer multiple repositories and separating configurations and applications
    to provide finer-grained access control and clearer separation of concerns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Push versus pull deployment models**: In a push-based model, changes are
    pushed from the repository to the Kubernetes clusters, often through a CI/CD pipeline.
    The pull-based model, conversely, involves a Kubernetes operator within the cluster
    monitoring the repository and pulling in changes when they’re detected. While
    the push model offers immediacy, the pull model is praised for its alignment with
    the Kubernetes declarative philosophy and enhanced security posture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monolithic versus microservices architectures**: When it comes to application
    architectures within Kubernetes, GitOps can be applied to both monolithic and
    microservices patterns. Monolithic architectures may be easier to manage through
    GitOps due to their singular nature, but microservices architectures benefit from
    GitOps through the ability to independently deploy and scale services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architectural frameworks for Kubernetes, empowered by GitOps, are evolving
    to facilitate more robust, scalable, and secure application deployments. Embracing
    GitOps not only streamlines the operational workflow but also enforces best practices
    in software architecture. As organizations adopt these frameworks, they must remain
    flexible and willing to adapt to the rapidly changing landscape of cloud-native
    technologies.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at the effects the choice of architecture
    has on working with GitOps.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the impact of architectural choices on GitOps’ effectiveness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GitOps is inherently aligned with Kubernetes’ declarative approach to managing
    infrastructure, where the desired state of the system is described in code. This
    state is checked into a Git repository, which then serves as the single source
    of truth. The effectiveness of GitOps is contingent on how well the architectural
    choices support a declarative model that enables the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Version control**: Tracking changes over time, providing a historical context,
    and enabling rollback to previous states'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Change management**: Facilitating peer reviews and approvals for changes
    to infrastructure code, enhancing the quality and security of deployments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated synchronization**: Ensuring that the actual state of the system
    automatically converges to the desired state defined in the repository'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architectural choices impacting GitOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When considering the impact of architectural choices on GitOps, several key
    factors come into play:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Repository structure**: Choosing between a monolithic (single repository)
    versus a multi-repository (one per service or team) approach can significantly
    affect the manageability and scalability of applications. A monolithic repository
    might simplify dependency tracking and versioning but could become unwieldy with
    scale. Multi-repository strategies enhance modularity and separation of concerns
    but require more sophisticated synchronization mechanisms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment strategies**: The architecture must support a variety of deployment
    strategies, such as canary, blue-green, or rolling updates. GitOps tools automate
    the execution of these strategies, and the choice of strategy can impact resource
    utilization, downtime during deployments, and the ability to test changes in production-like
    environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment isolation**: Architectural decisions on how to isolate and manage
    environments (development, staging, and production) will affect the GitOps workflow.
    Environment-specific configurations can be handled via separate branches, directories,
    or even separate repositories, each with implications for access control, traceability,
    and complexity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: As organizations scale, the architecture should facilitate
    a GitOps approach that can handle increased workloads, more complex deployments,
    and a growing number of services. This may involve partitioning clusters, adopting
    multi-cluster strategies, or leveraging cloud-native tools that specifically address
    scalability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security considerations**: Architectural choices must ensure that security
    is embedded in the GitOps workflow. This includes everything from securing access
    to Git repositories to encrypting sensitive data and automatically enforcing policies
    throughout the CI/CD pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making informed architectural decisions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To ensure GitOps effectiveness, organizations must make informed architectural
    decisions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assess organizational needs**: Understand the organization’s requirements
    in terms of scale, complexity, compliance, and team workflows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluate tooling compatibility**: Select GitOps tooling that is compatible
    with the chosen architecture and can support the required deployment strategies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Promote collaboration**: Architectures should encourage collaboration between
    development, operations, and security teams to leverage the collective expertise
    in support of GitOps workflows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuously refine**: Architectural choices should be revisited and refined
    based on feedback from ongoing operations so that they can adapt to new challenges
    and opportunities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, the architectural choices that are made when setting up Kubernetes
    environments have far-reaching implications for the success of a GitOps approach.
    By fostering an architecture that embraces version control, change management,
    and automated synchronization, organizations can leverage GitOps to enhance the
    agility and stability of their infrastructure. Making informed decisions about
    repository structures, deployment strategies, environment isolation, scalability,
    and security will position teams to harness the full potential of GitOps, leading
    to a more resilient and responsive infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Tailoring designs for scalability, resilience, and efficiency in cloud-native
    deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tailoring architectural designs to achieve scalability, resilience, and efficiency
    is fundamental for cloud-native deployments, where the dynamic nature of the cloud
    environment can present both opportunities and challenges. Cloud-native architectures
    enable systems to be resilient to failures, adaptable to changing loads, and efficient
    in resource utilization. When incorporating GitOps practices, these designs can
    be systematically enforced and continuously improved.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability in cloud-native architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cloud-native deployments are expected to handle varying loads gracefully. This
    flexibility is crucial for maintaining performance during demand spikes and optimizing
    costs during quieter periods. Here are a few ways you can achieve high scalability:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Horizontal scaling**: Architectures should be designed to allow for horizontal
    scaling, which involves adding more instances of an application to handle increased
    load'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microservices**: Breaking down applications into microservices enables individual
    components to scale independently, providing granular control over resource allocation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stateless applications**: Stateless applications are inherently more scalable
    since any instance can handle any request, allowing for straightforward horizontal
    scaling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitOps can manage the deployment and scaling of these services by automatically
    adjusting the number of instances based on the load, as defined in the Git repository.
  prefs: []
  type: TYPE_NORMAL
- en: Resilience through redundancy and isolation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A resilient system can withstand and recover from failures without significant
    downtime or data loss. Here are a few ways you can achieve stronger resilience:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High availability**: Architectures must be designed for high availability,
    with redundant components that can take over in case of failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault isolation**: Microservices architectures naturally lend themselves
    to fault isolation. A problem in one service should not cascade and cause system-wide
    failures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disaster recovery**: A robust backup and recovery strategy, along with multi-region
    deployments, can ensure that applications survive even catastrophic events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In GitOps workflows, the desired state in the repository reflects these high-availability
    configurations, enabling the system to self-heal by automatically re-deploying
    failed components.
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency with proactive optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Efficiency in cloud-native deployments is about doing more with less – less
    time, less resources, and less manual intervention. You can achieve early efficiency
    if the following aspects are optimized in advance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Auto-scaling**: Implement auto-scaling policies to adjust resources in response
    to real-time metrics, ensuring efficient use of infrastructure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load balancing**: Effective load balancing distributes traffic across instances
    to optimize resource utilization and ensure consistent performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource limits**: Setting appropriate resource limits and requests in Kubernetes
    helps prevent any single service from consuming more than its fair share of resources,
    leading to a more efficient system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitOps automates the process of enforcing these policies by triggering actions
    based on the configurations defined in the Git repository.
  prefs: []
  type: TYPE_NORMAL
- en: Tailoring designs with GitOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Designing architectures with scalability, resilience, and efficiency in mind
    requires careful planning and the right set of tools to manage the deployment
    and operation of cloud-native applications. Consider the following aspects when
    tailoring the design:'
  prefs: []
  type: TYPE_NORMAL
- en: '**IaC**: Define your infrastructure and policies as code to maintain a clear
    and auditable trail of how resources are allocated and managed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observability**: Implement comprehensive logging, monitoring, and alerting
    to gain insights into the system’s performance and health, informing decisions
    about design adjustments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous improvement**: Use GitOps to continuously deploy updates and improvements
    to the architecture, ensuring it evolves to meet changing needs and challenges'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tailoring designs for scalability, resilience, and efficiency is vital for cloud-native
    deployments to thrive in the elastic and often unpredictable cloud environment.
    By leveraging GitOps, teams can ensure that these design principles are consistently
    applied across all environments, enabling them to respond quickly to changes and
    maintain robust, efficient systems. As cloud technologies continue to evolve,
    so too must the architectures and practices that support them, with GitOps providing
    a framework for that ongoing evolution.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 11*](B22100_11.xhtml#_idTextAnchor209), we’ll explore the practical
    use of different GitOps approaches to deploy real-world applications. The next
    section delves into the significance of application design in GitOps, emphasizing
    the importance of the operational setup. A team’s focus shouldn’t just be on deploying
    applications, but also on choosing the right GitOps instance strategy for effective
    deployment. Before implementing GitOps with tools such as Argo CD, you must carefully
    consider the required number of clusters. We briefly introduced single instance
    approaches in the previous chapter while focusing on scalability. The next section
    will examine various approaches, their real-world application by organizations,
    and the advantages and disadvantages that are experienced in these scenarios,
    with a particular emphasis on the operational control of GitOps instances.
  prefs: []
  type: TYPE_NORMAL
- en: Centralized control – managing clusters with a solo Argo instance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As organizations grow and evolve, the demand for scalable, secure, and efficient
    deployment strategies becomes increasingly critical. **Argo CD**, a key player
    in the GitOps field [*1*, *2*], stands out for its comprehensive capabilities
    in scaling across various aspects, such as performance, security, usability, and
    failover processes. In this section, we’ll delve into the nuances of different
    architectural models and the intricacies of operational management. However, before
    we dive into these topics, it’s essential to address a few preliminary points
    that significantly influence decision-making in this realm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Security considerations**: Security in scaling with Argo CD involves a robust
    combination of **role-based access control** (**RBAC**) and **single sign-on**
    (**SSO**) mechanisms. A key security feature is the client-side rendering of manifests,
    which reduces the threat landscape. However, the power of tools such as **Helm**,
    **Kustomize**, and **Jsonnet** in manifest generation introduces potential risks
    as they allow arbitrary code to be executed. This necessitates careful consideration,
    especially in larger instances, to prevent abuse in manifest generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usability at scale**: Argo CD’s reputation for ease of use and extensibility
    remains intact even as it scales. However, managing applications across numerous
    Kubernetes instances can lead to complexities, such as the need for unique application
    naming conventions and the challenge of managing a large number of applications
    within a single UI instance. The tool’s UI filters, while powerful, are not savable,
    leading teams to find creative solutions such as using bookmarks for saved filters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failover strategies**: The GitOps approach, embodied by Argo CD, facilitates
    rapid changes and recovery through simple git commits. However, this ease of setup
    and teardown also brings to light the potential for significant impact due to
    misconfigurations. For instance, a minor error in updating a config management
    plugin could lead to widespread application failures. This raises the question
    of the “blast radius” – the extent of impact that a single misconfiguration could
    have.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance scaling**: Argo CD is available in two versions: the standard
    and the **high-availability** (**HA**) version. The latter is specifically designed
    for scalability, deploying multiple replicas of key components such as the repo-server.
    Argo CD’s scalability is evident in its capacity to support, without major adjustments,
    up to 1,500 applications, 14,000 objects, 50 clusters, and 200 developers. This
    benchmark, though conservative, accounts for variations in applications, such
    as their object count, manifest complexity, and update frequency. These figures,
    sourced from a KubeCon talk by *Joseph Sandoval* and from *Adobe and Dan Garfield
    from Codefresh* [*3*], provide a foundational guideline for planning scaling needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, while Argo CD is a robust tool that’s capable of handling the complexities
    of scaling in modern software environments, it requires careful consideration
    in terms of its performance capabilities, security risks, usability challenges,
    and failover strategies. The correct architectural choices, tailored to an organization’s
    specific needs, can leverage Argo CD’s strengths while mitigating potential risks
    associated with scaling.
  prefs: []
  type: TYPE_NORMAL
- en: The approach – centralized control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This approach may be familiar to you. In the managed cluster approach, a single
    Argo CD instance is utilized by various teams for different purposes. The platform
    team employs this shared instance to deploy the necessary platform context for
    other teams.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The term platform context, tailored to individual company needs, includes essential
    tools such as Ingress Controllers and Cert-Managers for effective Kubernetes platform
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Developers use the same Argo CD instance to deploy their applications. Similarly,
    **SRE teams** leverage it to deploy tools that help in identifying bottlenecks,
    analyzing performance issues, and more. The security team also makes use of this
    shared Argo CD instance to deploy its policies and enforce them through a policy
    engine. Additionally, they deploy security tools such as **kubeclarity** [*4*]
    or the **trivy operator** [*5*] to monitor vulnerabilities in images running in
    the cluster, track used packages, and check licenses. However, the significant
    change now is that instead of one Argo CD instance managing the platform and applications
    on a shared control and workload cluster, there is one Argo CD instance managing
    multiple clusters (*Figure 6**.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Difference between centralized control and one cockpit to rule
    them all](img/B22100_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Difference between centralized control and one cockpit to rule
    them all
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table outlines the advantages and disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Advantages** | **Disadvantages** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Centralized view and control**: Unified view for deployment activities
    across all clusters | **Scaling and performance**: Scaling necessitates tuning
    individual components |'
  prefs: []
  type: TYPE_TB
- en: '| **Simplified management**: Managing a single Argo CD instance for multiple
    Kubernetes clusters eases administrative tasks | **Single point of failure**:
    Potential single point of failure for deployments |'
  prefs: []
  type: TYPE_TB
- en: '| **API/CLI integration**: With only one server URL, API and CLI integration
    becomes more straightforward | **Security implications**: Centralization of admin
    credentials for all clusters |'
  prefs: []
  type: TYPE_TB
- en: '|  | **Network traffic and cost implications**: The application controller,
    responsible for Kubernetes watches, can incur significant network costs, especially
    if clusters are located in different regions |'
  prefs: []
  type: TYPE_TB
- en: Table 6.1 – Advantages and disadvantages of the centralized control approach
  prefs: []
  type: TYPE_NORMAL
- en: The single control plane approach involves one Argo CD instance managing all
    clusters, a popular approach for offering a unified application view and enhancing
    developer experience.
  prefs: []
  type: TYPE_NORMAL
- en: For organizations that delegate access based on environment and are concerned
    about managing all applications under one instance, **RBAC** policies and **AppProjects**
    can establish necessary boundaries, defining deployment locations and access controls.
  prefs: []
  type: TYPE_NORMAL
- en: This architecture also mandates establishing and maintaining a dedicated management
    cluster to host the Argo CD control plane, with direct access to all other clusters.
    The location of this management cluster could lead to security concerns, especially
    if it involves public exposure.
  prefs: []
  type: TYPE_NORMAL
- en: The key question here is how to ensure tenant separation while maintaining collaboration.
    In the GitOps approach with Argo CD, built-in **custom resources** such as **projects**,
    **roles**, and **groups** are utilized to implement a multitenancy framework.
    However, determining who is responsible for implementing, maintaining, and extending
    this approach to optimize the shared Argo CD instance is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: If the platform team is in charge, they must also manage the security, governance,
    and compliance aspects, especially if they have admin rights over the cluster.
    The feasibility of this depends on the team’s resources and expertise. For instance,
    a well-resourced platform team with core knowledge of the platform and specialized
    skills in areas such as security and **FinOps** can manage this effectively. However,
    smaller teams may find it challenging to maintain security.
  prefs: []
  type: TYPE_NORMAL
- en: 'A solution some companies adopt involves collaboration between different teams:'
  prefs: []
  type: TYPE_NORMAL
- en: Platform team ↔ security team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform team ↔ developers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform team ↔ FinOps team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform team ↔ SRE teams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The platform team is responsible for liaising with these teams and implementing
    commitments. They must also justify necessary changes enforced by the security
    team. This model works well for smaller companies with up to 30-50 mixed IT employees.
    Beyond 50 employees, the increased interaction between the platform team and developer/SRE
    teams can slow down development.
  prefs: []
  type: TYPE_NORMAL
- en: When to use the centralized control approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The centralized control approach is often the initial choice for teams operationalizing
    Argo CD across many clusters. It’s particularly effective for managing dev, staging,
    and production environments within a small team framework. The model supports
    high availability, scalable components, RBAC, and SSO, making it suitable for
    smaller-scale operations and straightforward network configurations.
  prefs: []
  type: TYPE_NORMAL
- en: When to avoid the centralized control approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Larger organizations with multiple independent teams, extensive networks, or
    a need for high flexibility should be cautious. The model’s potential for a large
    “blast radius” during critical failures and its limited flexibility with large
    numbers of users can be detrimental. For networks hosting Kubernetes in virtual
    private clouds or behind firewalls, while possible, the addition of network tunnels
    can add complexity.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the single control plane approach of the centralized control approach,
    while offering numerous advantages in terms of simplicity and ease of management,
    carries risks related to security, scalability, and performance. Organizations
    must weigh these factors carefully while considering their specific needs, team
    size, and network architecture before adopting this model.
  prefs: []
  type: TYPE_NORMAL
- en: The next section deals with dedicated Argo CD instances per Kubernetes cluster
    and the associated challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Dedicated instances – instance per cluster with Argo CD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An Argo CD instance is installed and co-located with the cluster it manages,
    meaning each cluster has its own dedicated Argo CD instance. This approach provides
    several advantages and challenges. This section aims to help you understand and
    implement standalone Argo CD instances in your Kubernetes environment. Each Argo
    CD instance will be installed and co-located with the cluster it manages, providing
    a dedicated instance per cluster, as shown in *Figure 6**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each cluster and each team benefits from having a dedicated Argo CD instance.
    This means that every cluster can be configured, managed, and monitored independently,
    allowing for tailored management strategies that align with the specific needs
    of each cluster. But this approach also means that each Argo CD instance necessitates
    its own set of resources. Ensuring that each cluster has the necessary resources
    to support its Argo CD instance is crucial. This requirement calls for detailed
    planning and assessment to allocate resources efficiently and avoid potential
    shortages or imbalances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Example of dedicated Argo CD instances per cluster](img/B22100_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Example of dedicated Argo CD instances per cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at some pros and cons to better understand this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Advantages** | **Disadvantages** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Reliability improvement**: Each cluster operates independently, enhancing
    overall reliability | **Management complexity**: Each instance requires individual
    management and updates, increasing the overall complexity |'
  prefs: []
  type: TYPE_TB
- en: '| **Isolation of concerns**: This setup offers better security as each cluster
    is self-contained | **Access complexity**: Providing access to users across multiple
    instances can be challenging |'
  prefs: []
  type: TYPE_TB
- en: '| **No external networking access required**: Standalone Argo CD instances
    operate independently of external network access, which is crucial for edge deployments
    and even air-gapped environments where updates might occur via a USB drive | **Disaster
    recovery considerations**: Special planning is needed for disaster recovery due
    to the decentralized nature of the setup |'
  prefs: []
  type: TYPE_TB
- en: '| **Suitability for Edge deployments**: Its standalone nature is ideal for
    clusters at the Edge, ensuring each operates completely independently |  |'
  prefs: []
  type: TYPE_TB
- en: Table 6.2 – Advantages and disadvantages of the dedicated instances approach
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, it appears that the advantages would outweigh the disadvantages.
    Two significant benefits have been proven in practice over the years.
  prefs: []
  type: TYPE_NORMAL
- en: Heightened security is perhaps the most standout benefit. By employing individual
    Argo CD instances, security measures can be precisely tailored to meet the unique
    requirements and vulnerabilities of each cluster. This level of customization
    ensures that the security protocols are not only robust but also intricately designed
    to address specific threats, providing a fortified defense for each unique environment.
  prefs: []
  type: TYPE_NORMAL
- en: Another critical advantage is **isolated risk management**. In conventional
    setups, a single failure or breach could potentially escalate into a system-wide
    crisis. However, with dedicated Argo CD instances, such risks are contained within
    the affected cluster, significantly reducing the likelihood of widespread issues.
    This isolation of risk is vital in a landscape where a single vulnerability can
    lead to significant operational disruptions.
  prefs: []
  type: TYPE_NORMAL
- en: However, from practical experience, there is a disadvantage that outweighs many
    advantages, especially as the number of clusters and Argo CD instances increases
    and maintenance effort escalates significantly.
  prefs: []
  type: TYPE_NORMAL
- en: The maintenance overhead is another significant factor. Managing multiple Argo
    CD instances means that each one demands individual attention – from updates and
    configuration tweaks to regular monitoring. This increased workload can place
    a strain on IT teams, necessitating more robust and efficient management strategies
    to handle the additional administrative tasks.
  prefs: []
  type: TYPE_NORMAL
- en: When to use dedicated Argo CD instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Standalone Argo CD** instances are most beneficial in scenarios where the
    greatest reliability and accessibility are required, especially in situations
    where external networking is limited or non-existent. These instances are the
    default choice for deploying to clusters at the edge, given their complete operational
    independence. To scale these deployments, integration with infrastructure management
    tools such as Crossplane or Terraform is often employed to streamline setup and
    teardown. Another variation involves using a hub-and-spoke model to manage multiple
    standalone Argo CD instances across numerous clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: When to avoid dedicated Argo CD instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: However, standalone instances come with significant management overhead. In
    scenarios where a single team manages a simple staging-to-production workflow,
    it might not be efficient to separate Argo CD for each cluster. Testing and implementing
    changes across multiple instances also present challenges. Adopting a canary release
    approach to update a fleet of instances, while effective, adds another layer of
    complexity to the process.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, the decision to use standalone Argo CD instances should be based
    on the specific needs and capabilities of the organization. While they offer improved
    reliability and security, the complexities in management, updates, and disaster
    recovery planning must be carefully considered. For certain environments, especially
    those at the Edge, the standalone approach is ideal, but for simpler setups or
    smaller teams, this approach might introduce unnecessary complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding to use an Argo CD instance per cluster depends on your organization’s
    specific needs and capabilities. While they offer greater reliability and security,
    this approach requires careful consideration in terms of the complexities in management,
    updates, and disaster recovery. This strategy is highly effective for Edge deployments
    and environments with limited external networking but may be too complex for simpler
    setups or smaller teams.
  prefs: []
  type: TYPE_NORMAL
- en: The next section deals with dedicated Flux CD instances per Kubernetes cluster
    and the associated challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Dedicated instances – instance per cluster with Flux CD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the realm of GitOps tools, the distinction between Argo CD and **Flux CD**
    is crucial, particularly when considering their application in dedicated instances
    per cluster. While Argo CD is a well-known entity in the GitOps conversation,
    Flux CD holds a significant place, with a robust community and a substantial user
    base. This diversity of tools is essential to understanding the range of options
    available for Kubernetes cluster management.
  prefs: []
  type: TYPE_NORMAL
- en: My journey in GitOps began with Flux CD, a tool that served effectively over
    a long period, especially in projects where scaling and managing multiple clusters
    wasn’t a requirement. This context-specific suitability of Flux CD stems from
    its distinct approach and capabilities compared to Argo CD.
  prefs: []
  type: TYPE_NORMAL
- en: 'At first glance, the use of Flux CD might appear like that of Argo CD, almost
    as if it’s a simple icon swap, as visually represented in *Figure 6**.3*, where
    there’s a dedicated Flux CD instance per cluster. However, practical experience
    with Flux CD reveals a deeper layer of complexity. Unlike Argo CD, Flux CD requires
    a higher level of expertise in Kubernetes and Helm, demanding proficiency from
    teams in tools such as *Helm*, *Kustomize*, and *Kubernetes*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3: Example of dedicated Flux CD instances per cluster](img/B22100_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Example of dedicated Flux CD instances per cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Flux CD’s approach to managing deployments revolves around Helm releases and
    the Helm controller. These elements are crucial for handling package deployments
    and life cycle management in Kubernetes. The Helm controller in Flux CD offers
    a declarative way to install, upgrade, and manage Helm charts in a Kubernetes
    environment, aligning with the GitOps principles. This requires teams to have
    a solid understanding of Helm charts and their management.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Flux CD utilizes **Kustomizations** for applying Kubernetes manifests.
    This feature allows resources to be customized before they are applied to the
    cluster, providing a powerful tool for managing complex deployments. Understanding
    and effectively using Flux Kustomization requires a deep knowledge of how Kubernetes
    manifests work and how they can be customized for specific deployment needs.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of organizational structure, adopting Flux CD effectively usually involves
    small, autonomous teams, typically consisting of 5-7 members. These teams handle
    projects as independent units within a larger organization, delivering specific
    products or services. This structure, while beneficial for focused and efficient
    delivery, poses challenges in onboarding due to the complexity of Flux CD and
    the advanced skills it demands.
  prefs: []
  type: TYPE_NORMAL
- en: The key advantages of using Flux CD for individual cluster management per team
    lie in its flexibility and adaptability, making it an ideal choice for complex
    deployment scenarios. Flux CD’s support for advanced customization options is
    particularly beneficial for teams with comprehensive knowledge of Kubernetes and
    Helm.
  prefs: []
  type: TYPE_NORMAL
- en: However, this sophistication comes with a steep learning curve. The advanced
    functionalities and customization options of Flux CD add to the complexity of
    its setup and operation. As a result, integrating teams into a Flux CD workflow
    can be time-consuming, demanding a higher level of technical proficiency.
  prefs: []
  type: TYPE_NORMAL
- en: This discussion isn’t centered around comparing Flux CD and Argo CD, but rather
    on the fact that the “instance per cluster” approach works effectively with both.
    The previously mentioned pros and cons apply to Flux CD as well since both tools
    operate on Kubernetes and employ native Kubernetes methods to facilitate deployments
    into clusters. However, it’s important to note that while Flux CD demands a higher
    skill level and prior experience with Helm charts or Kustomization, Argo CD can
    also be utilized directly with plain Kubernetes manifests.
  prefs: []
  type: TYPE_NORMAL
- en: This distinction highlights that both Argo CD and Flux CD, despite their different
    complexities and requirements, can be effectively integrated into the instance
    per cluster model. While Argo CD offers a more user-friendly approach suitable
    for less complex scenarios, Flux CD’s adaptability and technical demands make
    it ideal for more intricate deployments, especially for teams well-versed in Kubernetes
    and Helm.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the operational nuances, strengths, and requirements of each tool
    is crucial for organizations looking to optimize their Kubernetes management strategies.
    The choice between Argo CD and Flux CD in the instance per cluster approach should
    be informed by the specific needs of the deployment scenario and the skill level
    of the managing team. By aligning the tool’s capabilities with the team’s expertise
    and the project’s requirements, organizations can achieve efficient and effective
    management of their Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will be an exciting one as various concepts that have already
    been presented will be combined to help you find a middle way.
  prefs: []
  type: TYPE_NORMAL
- en: The middle way – instance per logical group with Argo CD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**The middle way – instance per logical group with Argo CD** is an approach
    that centers around using a control cluster with Argo CD to manage a group of
    clusters. This approach presents a refined architecture that seeks to balance
    scalability, manageability, and efficiency in Kubernetes cluster management. It
    involves running one Argo CD instance per logical group of clusters, such as per
    team, region, or environment, depending on the organizational structure and requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this model, Argo CD is deployed on a control cluster that belongs to a specific
    group. From this central point, Argo CD manages all clusters within that group.
    This arrangement aims to streamline the management process by consolidating control,
    yet it still maintains a level of separation between different groups of clusters
    (*Figure 6**.4*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Example of an instance per logical group in Argo CD-based projects](img/B22100_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Example of an instance per logical group in Argo CD-based projects
  prefs: []
  type: TYPE_NORMAL
- en: This architecture balances the demands of managing multiple clusters by effectively
    partitioning them into logical groups. It offers a solution that alleviates the
    challenges of maintaining too many individual instances while providing a more
    manageable and scalable approach. This grouping not only improves operational
    efficiency but also enhances the security and reliability of the system. The developer
    experience is also improved compared to an instance-per-cluster architecture as
    following a clear and understood convention for grouping reduces the cognitive
    load and simplifies integration processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the possible and most sensible groupings. For example, groups
    can be logically sorted by departments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Department-based**: Different departments such as development, operations,
    or QA each have their own Argo CD instance for managing their specific clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Development**: An instance for developers working on new product features'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operations**: A separate instance for the operations team to manage deployment
    and infrastructure'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**QA**: An instance for the QA team to test and validate products'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: At this point, developers have a development environment with different workload
    clusters that they can use autonomously. The operations department also has cluster
    groups and can operate infrastructure components that are important for the organization,
    such as LDAP servers, DNS servers, ACME servers, databases, and more. The QA department
    can test delivered software features on different clusters, in different versions,
    and conduct load tests with their tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at projects and geographical location:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Project-based**: For companies handling multiple projects, each project can
    be assigned a separate instance, facilitating focused management and autonomy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E-commerce platform**: An instance dedicated to the e-commerce project team'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mobile application development**: A separate instance for teams working on
    mobile apps'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Internal tools**: For teams developing and maintaining internal company tools'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In this scenario, each project is assigned its own Argo CD instance. For instance,
    the team working on the e-commerce platform can fully control their deployment
    pipelines and feature rollouts, tailoring their workflows to the specific needs
    of the project. Similarly, teams dedicated to mobile application development can
    manage their deployments with a focus on mobile-specific requirements and testing
    environments. For internal tools, a dedicated instance allows the team to rapidly
    iterate and deploy updates, ensuring that internal operations run smoothly and
    efficiently.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Geographical location**: Companies with global operations can group clusters
    based on geographical regions for better localization and compliance management:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**North American operations**: An instance for clusters in North American regions'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Europe**: A dedicated instance for managing clusters in European countries'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Asia-Pacific** (**APAC**): An instance focused on the APAC region’s specific
    needs and compliance'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, a dedicated Argo CD instance for North American operations allows
    teams to manage clusters as per local compliance and operational standards. In
    Europe, teams can address specific regional requirements, such as **GDPR compliance**,
    through a Europe-focused instance. Similarly, for the APAC region, an instance
    can cater to the unique operational and regulatory landscape, ensuring that deployments
    are optimized for local preferences and legal requirements. This geographical
    grouping not only enhances efficiency but also ensures adherence to regional regulations
    and cultural nuances, making it an essential strategy for global operations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These groupings are valid, and I have personally seen this setup in various
    companies. However, it’s more related to the organizational structure and how
    projects are managed. I have not yet seen the Argo CD setup in productive use
    in practice; I only know it from concepts or proof of concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Although the centralized controller reduces certain disadvantages, such as the
    potential for a single point of failure in deployments, the centralization of
    admin credentials for all clusters, and the need for tuning individual components
    for scaling, these issues are shifted to the group level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through this setup, the following advantages and disadvantages arise:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Advantages** | **Disadvantages** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Load distribution**: By grouping clusters, this approach distributes the
    workload more evenly across groups, easing the burden on application controllers,
    repo servers, and API servers. | **Multiple instance maintenance**: The approach
    requires maintaining multiple Argo CD instances, one for each group, which can
    add to the administrative workload. |'
  prefs: []
  type: TYPE_TB
- en: '| **Group-specific credentials**: Credentials are scoped per group, which simplifies
    access management while maintaining security. | **Reducing single points of failure**:
    While having a control cluster as the central management point per group reduces
    single points of failure, it also introduces a new vulnerability. If the control
    cluster encounters issues, it could potentially disrupt the management of all
    grouped deployments. |'
  prefs: []
  type: TYPE_TB
- en: '| **Unified view for deployment activities**: Each group has a single view
    for all deployment activities, streamlining the monitoring and management process.
    | **Centralized administration credentials**: Admin credentials for all clusters
    in a group are stored in the control cluster, streamlining access management.
    This centralized approach can enhance security measures as it reduces the number
    of access points that need to be secured. |'
  prefs: []
  type: TYPE_TB
- en: '| **Reduced configuration duplication**: As clusters in a group are likely
    to have similar RBAC, *AppProject*, and other configurations, this model significantly
    reduces the need for duplicate configurations. | **Management cluster requirements**:
    The requirement for a separate management cluster to host Argo CD instances adds
    a layer of infrastructure that needs to be set up and maintained, potentially
    complicating the overall system architecture. |'
  prefs: []
  type: TYPE_TB
- en: Table 6.3 – Advantages and disadvantages of the middle-way approach
  prefs: []
  type: TYPE_NORMAL
- en: When to use the middle-way approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The middle-way approach is particularly suited to organizations that manage
    a diverse range of clusters per logical group. It excels in scenarios where different
    departments, such as development, operations, or QA, require independent control
    over their respective clusters. This method is ideal for businesses handling multiple
    projects, each with unique requirements, allowing for focused and autonomous management.
    Additionally, for multinational companies, this approach facilitates effective
    management of clusters based on geographical locations, ensuring compliance with
    regional standards and operational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: When not to use the middle-way approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite its benefits, this approach may not suit every scenario. For smaller
    organizations with limited resources, the task of maintaining multiple Argo CD
    instances can be daunting and resource-intensive. The need to tune each instance
    at scale, coupled with the necessity of a separate management cluster, adds layers
    of complexity that smaller or less complex environments might not warrant. In
    cases where centralized control is more practical and efficient, particularly
    in smaller setups, this approach might introduce unnecessary complications.
  prefs: []
  type: TYPE_NORMAL
- en: This approach presents a compromise between individual and centralized management
    models, distributing workloads across groups and reducing configuration duplication.
    It enhances the security and reliability of the system by limiting the impact
    of potential failures on specific groups. However, it requires careful planning
    and consideration of the organizational structure, resource availability, and
    the scale of operations to ensure it aligns with the specific needs of the organization.
    This approach, while not universally applicable, offers a flexible and efficient
    solution for medium to large-scale Kubernetes environments.
  prefs: []
  type: TYPE_NORMAL
- en: The next section deals with how to use a central Argo CD instance to provide
    the clusters with the necessary tools and how to ensure developer autonomy with
    dedicated Argo CD instances on the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The cockpit and fleet approach with Argo CD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the dynamic world of Kubernetes and GitOps, The **cockpit and fleet** approach
    offers an innovative solution that combines centralized management with individual
    autonomy. This approach involves a platform team utilizing a central Argo CD instance
    for overarching control while also providing individual Argo CD instances for
    each developer’s cluster (*Figure 6**.5*). This approach is specifically designed
    for organizations that aim to streamline their Kubernetes operations and concurrently
    empower individual teams or departments with autonomy in their cluster management.
    Since July 2023, Flux has had an implementation that allows a similar approach
    and is called Hub and Spoke [*6*]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Example of the cockpit and fleet approach](img/B22100_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Example of the cockpit and fleet approach
  prefs: []
  type: TYPE_NORMAL
- en: 'The central principle of this approach is anchored in two pivotal components
    – the cockpit and the fleet:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The cockpit**: Managed by the platform team, the cockpit involves operating
    a centralized Argo CD instance. This central instance functions as a command-and-control
    center that’s responsible for deploying and managing essential infrastructure
    components across all the Kubernetes clusters within the organization. The primary
    role of the cockpit is to ensure that there’s a uniform application of critical
    infrastructure elements across all clusters. This includes enforcing compliance
    with organizational standards and policies, thereby establishing a consistent
    and secure infrastructure framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The fleet**: In contrast to the centralized nature of the cockpit, the fleet
    provides individual developers or specific teams with dedicated Argo CD instances
    for each of their clusters. This decentralization empowers teams to manage their
    applications’ life cycle independently, from configuration to deployment and updates.
    Such autonomy is vital in fostering innovation and agility, particularly in fast-paced
    development environments where rapid deployment and iterative updates are the
    norm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delving deeper into the approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dual-layered nature of the cockpit and fleet approach is crafted to address
    the diverse needs of large organizations with multiple clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: The centralized cockpit offers a streamlined, holistic view of the organization’s
    Kubernetes infrastructure. This centralization is crucial for large-scale operations
    where consistency in infrastructure management and policy enforcement is necessary.
    By having a unified control point, the platform team can efficiently manage shared
    resources, apply global security policies, and ensure compliance across all clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decentralized fleet, on the other hand, caters to the specific needs of
    individual development teams or departments. Each team has the flexibility to
    tailor its cluster according to its project requirements. This setup is particularly
    beneficial in environments where different teams work on varied projects, each
    with its unique set of requirements and deployment strategies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operational dynamics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Implementing the cockpit and fleet approach necessitates a well-orchestrated
    operational model that balances centralized governance with decentralized autonomy:'
  prefs: []
  type: TYPE_NORMAL
- en: On one side, the platform team must ensure that the centralized cockpit is effectively
    managing the shared components and maintaining the required standards across all
    clusters. This involves regular updates, security patching, and monitoring of
    the centralized infrastructure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other side, individual teams managing their clusters via the fleet model
    need to align their development and deployment strategies with the broader organizational
    goals. They must also ensure their practices comply with the security and policy
    guidelines set by the platform team.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table compares some of the advantages and disadvantages of this
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Advantages** | **Disadvantages** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Centralized view and control**: Offers a unified view for *platform context*
    deployment activities across all clusters. | **Security implications**: Centralizing
    admin credentials for all fleet clusters could pose security risks. |'
  prefs: []
  type: TYPE_TB
- en: '| **Simplified management**: Managing a single Argo CD instance for multiple
    Kubernetes clusters simplifies administrative tasks, easing the process of provisioning
    and maintaining the *platform context* for the fleet clusters. | **Resource consumption**:
    Additional resources are consumed, both for the management cluster and the fleet
    clusters, including hardware and engineering resources. |'
  prefs: []
  type: TYPE_TB
- en: '| **Reliability improvement**: Each fleet cluster operates independently, which
    enhances overall reliability and allows for strict separation between teams. |
    **Single point of failure**: There’s a potential risk of a single point of failure
    for fleet cluster deployments to provide the platform context. |'
  prefs: []
  type: TYPE_TB
- en: '| **Maintenance and expansion of fleet clusters**: For platform teams, it’s
    easier to roll out new tools or policies simultaneously across all clusters. This
    also impacts the process of maintaining or upgrading tools within the platform
    context. | **Scaling and performance**: Scaling requires tuning individual components
    within the platform context. |'
  prefs: []
  type: TYPE_TB
- en: Table 6.4 – Advantages and disadvantages of the cockpit and fleet approach
  prefs: []
  type: TYPE_NORMAL
- en: When to use the cockpit and fleet approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The cockpit and fleet approach is particularly well-suited for large-scale organizations
    managing a diverse range of projects across numerous clusters. This strategy is
    ideal for environments that require a combination of centralized control for shared
    resources and decentralized autonomy for individual teams or departments. It’s
    especially beneficial in complex multi-cluster environments where a streamlined
    operation is needed to manage common infrastructure elements and policies efficiently.
    Moreover, organizations with different teams or departments, each having unique
    operational requirements, can leverage this approach to provide each unit with
    the necessary tools and autonomy for their specific projects. Global companies
    with operations across multiple regions also find this approach advantageous as
    it allows for centralized management of global standards while enabling local
    teams to manage clusters as per regional requirements.
  prefs: []
  type: TYPE_NORMAL
- en: When not to use the cockpit and fleet approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Conversely, the cockpit and fleet approach may not be the most suitable for
    small to medium-sized businesses with a limited number of clusters and less complexity.
    In such cases, the overhead of creating a setup of maintaining both centralized
    and decentralized systems might outweigh the benefits. Organizations with uniform
    cluster needs across the board might find a simpler, more centralized approach
    more efficient and practical. Additionally, companies with limited resources in
    terms of personnel or infrastructure might face challenges in maintaining the
    dual management system effectively. Environments with simplified workflows, where
    development and deployment processes are straightforward and uniform across the
    organization, may not derive significant value from the added complexity of a
    hybrid approach.
  prefs: []
  type: TYPE_NORMAL
- en: The cockpit and fleet approach stands as a testament to the evolving landscape
    of Kubernetes management, offering a solution that is both comprehensive and flexible.
    It adeptly addresses the challenges of managing a vast Kubernetes infrastructure
    in large organizations, balancing the need for centralized control with the agility
    of decentralized management. The approach fosters a collaborative and efficient
    environment where the platform team and individual development teams work in harmony,
    each with their distinct yet interconnected roles. As organizations continue to
    grow and evolve in their Kubernetes journey, approaches such as cockpit and fleet
    become increasingly vital in navigating the complexities of cluster management
    at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right approach for your GitOps needs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deciding on the right approach for GitOps can be challenging as there is no
    one-size-fits-all solution. In my experience across various industries and companies,
    two primary approaches have emerged: the cockpit and fleet approach in larger
    organizations, which scales with project needs, and the dedicated instance per
    cluster approach for smaller, independent teams. However, each approach has its
    drawbacks, particularly concerning security, especially in public cloud operations.
    To address these challenges and mitigate disadvantages, take a look at [*Chapter
    13*](B22100_13.xhtml#_idTextAnchor257), *Security with GitOps*. Companies such
    as **Akuity** have begun offering SaaS and self-hosted solutions, which build
    upon GitOps with Argo CD but invert the principle.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Example of Akuity Platform’s Argo CD SaaS and self-hosted offerings
    [6]](img/B22100_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Example of Akuity Platform’s Argo CD SaaS and self-hosted offerings
    *[6]*
  prefs: []
  type: TYPE_NORMAL
- en: The Akuity Platform ingeniously integrates the cockpit and fleet approach of
    Argo CD management, where the Argo CD instances on fleet clusters register themselves
    with the central Argo CD “cockpit” in the Akuity Platform. This model blends the
    benefits of both instance-per-cluster and single-instance architectures, effectively
    addressing most of their limitations.
  prefs: []
  type: TYPE_NORMAL
- en: In this hybrid agent architecture, an agent runs inside each fleet cluster and
    establishes outbound communication back to the control plane in the cockpit. This
    setup significantly reduces the network traffic between the control plane and
    the cluster, addressing common security concerns as it does not require direct
    cluster access or admin credentials. This architecture is particularly advantageous
    for connecting external Argo CD instances to clusters in restricted environments,
    such as a local development cluster on a laptop.
  prefs: []
  type: TYPE_NORMAL
- en: The Akuity Platform simplifies the operational aspects of Argo CD. Unlike traditional
    models, which require a dedicated management cluster to host Argo CD, the Akuity
    Platform hosts the Argo CD instance and the custom resources. This innovation
    not only streamlines the management process but also introduces automatic snapshotting
    and disaster recovery features, effectively eliminating concerns around single
    points of failure.
  prefs: []
  type: TYPE_NORMAL
- en: From a visibility standpoint, the Akuity Platform offers a centralized view
    of all organizational Argo CD instances, akin to the single-instance architecture.
    The platform enhances open source capabilities by providing a dashboard for each
    instance, showcasing application health metrics and synchronization histories.
    It facilitates the management of settings, allowing configurations, typically
    complex YAML files, to be crafted easily using user-friendly wizards. Additionally,
    the inclusion of an audit log feature for all activity across the Argo CD instances
    greatly simplifies compliance reporting and monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the disadvantages of the cockpit and fleet approach, you can
    see why Akuity provides an innovative solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows how the Akuity Platform has minimized or eliminated
    most of the disadvantages mentioned in *Table 6.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Disadvantages** | **Explanation** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Security implications**: Centralizing admin credentials for all fleet clusters
    could pose security risks. | The agent within the fleet cluster eliminates central
    credentials in the cockpit. It operates with outbound access back to the cockpit,
    removing the need for direct cluster access or admin credentials, thereby mitigating
    security concerns. |'
  prefs: []
  type: TYPE_TB
- en: '| **Resource consumption**: Additional resources are consumed, both for the
    management cluster and the fleet clusters, including hardware and engineering
    resources. | Network traffic between the control plane and fleet clusters is reduced.
    Argo CD no longer needs to establish connections to fleet clusters at specific
    intervals. The syncing of applications is reduced as they now reside on the fleet
    clusters, lessening the load. |'
  prefs: []
  type: TYPE_TB
- en: '| **Single point of failure**: There’s a potential risk of a single point of
    failure for fleet cluster deployments to provide the platform context. | Fleet
    clusters autonomously retrieve and store the platform context through custom resources
    within the fleet cluster itself. This ensures continuity even in case of connection
    loss. |'
  prefs: []
  type: TYPE_TB
- en: '| **Scaling and performance**: Scaling requires tuning individual components
    within the platform context. | No elimination. Tuning requirements shift to different
    aspects, such as setting specific domain filters for External-DNS per fleet cluster,
    which is essential for scaling. This necessitates tuning values so that they can
    be adapted to each cluster’s needs. |'
  prefs: []
  type: TYPE_TB
- en: Table 6.5 – How the Akuity Platform minimizes or eliminates most of the disadvantages
    mentioned in Table 6.4
  prefs: []
  type: TYPE_NORMAL
- en: In essence, the Akuity Platform adopts and enhances the cockpit and fleet approach,
    allowing fleet clusters’ Argo CD instances to connect back to a central “cockpit,”
    thereby providing a seamless, secure, and efficient method of managing large-scale
    Kubernetes environments. Akuity’s approach stands out as an innovative solution,
    particularly for organizations grappling with the complexities and security concerns
    inherent in managing Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have looked at approaches where Kubernetes clusters already exist.
    Next, we’ll take a step back and create the Kubernetes clusters ourselves using
    the GitOps approach.
  prefs: []
  type: TYPE_NORMAL
- en: Centralized Kubernetes cluster creation – leveraging Cluster API and Argo CD
    for streamlined cluster deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the world of modern software deployment, the synergy between Cluster API
    and Argo CD stands as a testament to the power and efficiency of GitOps practices.
    This approach not only facilitates the operation of Argo CD within clusters but
    also harnesses GitOps methodologies for deploying clusters themselves. These clusters
    form the foundational infrastructure for Argo CD and the workloads it manages,
    including various applications deployed through it.
  prefs: []
  type: TYPE_NORMAL
- en: This section delves into how Argo CD can be utilized, or how a self-service
    portal for teams can be provided, to streamline the deployment of Kubernetes clusters.
    The effectiveness of this approach is evidenced by companies such as **Kubermatic**,
    **CLASTIX**, and **Giant Swarm**, which have leveraged it to offer diverse managed
    Kubernetes solutions. These solutions range from standalone products to comprehensive
    managed services, aiming to simplify Kubernetes deployment for their clients.
  prefs: []
  type: TYPE_NORMAL
- en: This strategy allows organizations to centralize and automate the creation and
    management of Kubernetes clusters, ensuring a consistent and reliable infrastructure
    for deploying and managing applications using Argo CD. The use of GitOps in this
    context not only enhances the efficiency of these processes but also offers the
    scalability and flexibility needed to manage complex, multi-cluster environments
    effectively. By integrating Cluster API with Argo CD, organizations can create
    a powerful pipeline for deploying and managing Kubernetes clusters, which, in
    turn, can be used to deploy a wide range of workloads, including the Argo CD toolset
    itself and the application stacks for developers.
  prefs: []
  type: TYPE_NORMAL
- en: This approach represents a significant shift in how Kubernetes clusters are
    provisioned and managed, moving toward a more automated, scalable, and developer-friendly
    environment. It exemplifies the potential of GitOps to streamline not just application
    deployment but also the underlying infrastructure management, thereby enabling
    organizations to focus on innovation and development.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Cluster API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Cluster API** [*7*] project represents a key initiative within the Kubernetes
    ecosystem that focuses on making the setup, update, and oversight of Kubernetes
    clusters more streamlined. Launched by the **Kubernetes Special Interest Group**
    (**SIG**) Cluster Lifecycle, this project utilizes Kubernetes-conformant APIs
    and design principles to automate the process of managing cluster life cycles
    for those responsible for platform operations. It facilitates defining and managing
    underlying infrastructure components – such as virtual machines, network resources,
    load balancers, and **virtual private clouds** (**VPCs**) – in a manner akin to
    how application developers handle application deployments. This approach ensures
    uniform and reliable deployment of clusters across diverse infrastructure settings.
  prefs: []
  type: TYPE_NORMAL
- en: A key aspect of Cluster API is its ability to provision Kubernetes-native, declarative
    infrastructure that applies to AWS. This incorporates principles and experiences
    from previous cluster managers, such as *kops* and *kubicorn*. Its features include
    being able to manage VPCs, gateways, security groups, and instances, support for
    **Elastic Kubernetes Service** (**EKS**), and the ability to deploy Kubernetes
    control planes in private subnets with a separate bastion server. SSH is not used
    for bootstrapping nodes, and only the minimal components are installed to bootstrap
    a control plane and worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster API is licensed under the Apache-2.0 license and offers an active community
    for developers and enthusiasts who wish to contribute to further development.
    There are regular office hours with maintainers where developers can participate
    in discussions and get support.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cluster API offers a range of use cases in the realm of Kubernetes cluster
    management that cater to different needs within cloud-native ecosystems. Here
    are some common scenarios where Cluster API proves to be particularly useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-cluster management**: Cluster API simplifies the management of multiple
    Kubernetes clusters across various environments. It allows for consistent and
    automated provisioning, upgrading, and operational tasks for many clusters, making
    it ideal for organizations managing a vast fleet of Kubernetes clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated cluster life cycle management**: It automates the entire life cycle
    of Kubernetes clusters, including creation, scaling, upgrading, and deletion.
    This automation is particularly beneficial in scenarios where clusters need to
    be frequently scaled up or down based on demand or updated with the latest Kubernetes
    versions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid cloud and multi-cloud deployments**: For organizations that operate
    in a hybrid or multi-cloud environment, Cluster API enables consistent deployment
    and management of Kubernetes clusters across different cloud providers. This uniformity
    is crucial for businesses looking to avoid vendor lock-in and maintain flexibility
    in their cloud strategy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IaC**: Cluster API aligns with the IaC paradigm, allowing teams to define
    and manage clusters declaratively. This approach is beneficial for DevOps teams
    aiming to maintain infrastructure and configuration consistency through code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self-service clusters**: In larger organizations, different teams may require
    their own Kubernetes clusters. Cluster API enables a self-service model where
    teams can provision and manage their clusters autonomously while adhering to centralized
    policies and standards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CI/CD pipelines**: Integrating Cluster API with CI/CD pipelines can streamline
    the process of testing and rolling out new applications or updates. It allows
    for dynamic creation and disposal of clusters as part of the CI/CD process, enabling
    more efficient testing and deployment workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disaster recovery**: Cluster API can be instrumental in disaster recovery
    strategies. Automating the creation of backup clusters and enabling quick replication
    of cluster states helps reduce downtime and ensures high availability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edge computing**: For Edge computing scenarios where Kubernetes clusters
    need to be deployed at multiple Edge locations, Cluster API provides a unified
    way to manage these clusters from a central point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning and experimentation**: For educational purposes or experimentation,
    Cluster API allows users to quickly spin up and tear down Kubernetes clusters.
    This is useful for learning Kubernetes, testing new features, or experimenting
    with different configurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these use cases demonstrates the versatility and utility of Cluster
    API in managing Kubernetes clusters efficiently and at scale, catering to the
    diverse needs of modern cloud-native applications and infrastructures.
  prefs: []
  type: TYPE_NORMAL
- en: How Cluster API is leveraged by different companies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In exploring the diverse landscape of Kubernetes management, it becomes evident
    that different organizations have unique requirements and strategies. These vary
    based on their specific operational needs, infrastructure preferences, and long-term
    technological goals. As a result, various implementations of Cluster API have
    emerged, each tailored to meet these differing demands. Some organizations opt
    for fully managed Kubernetes services, while others lean toward self-managed solutions
    to avoid vendor lock-in and maintain greater control over their infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are only a fraction of the companies that use Cluster API in
    their substructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**VMware Kubernetes solution (vSphere with Tanzu)**: VMware’s integration of
    Kubernetes directly into the vSphere platform demonstrates a deep use of Cluster
    API, particularly with the vSphere provider. This allows developers to deploy
    and manage Kubernetes clusters directly from vSphere.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster API Provider Azure (CAPZ)**: This is Microsoft’s implementation of
    Cluster API for Azure and replaces AKS Engine for self-managed Kubernetes clusters.
    CAPZ leverages Azure’s robust, scalable infrastructure to provide a seamless and
    efficient way to operate Kubernetes, simplifying cluster management tasks and
    enhancing the automation capabilities inherent in Azure’s cloud services. This
    implementation ensures that users can maintain full control over their Kubernetes
    environments while benefiting from the native integrations and services offered
    by Azure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Giant Swarm (Kubernetes platform)**: Giant Swarm uses Cluster API to create
    a unified application point for multiple self-managed Kubernetes clusters across
    different cloud provider endpoints. It offers a managed Kubernetes solution with
    the flexibility to deploy to various target cloud providers, emphasizing low vendor
    lock-in and subscription-based pricing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CLASTIX (Kamaji)**: An entirely open source implementation of Cluster API,
    Kamaji is noted for its efficiency in scaling control planes on a management cluster,
    thereby reducing costs. The approach involves creating worker nodes and enabling
    them to join the respective tenants.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubermatic Kubernetes Platform (KKP)**: This platform, which started early
    with the first version of Cluster API, focuses on creating and managing instances
    for worker nodes and joining them to a cluster. The architecture includes a Master
    Cluster and Seed Clusters with a special machine controller for precise management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these implementations reflects different strategies and priorities,
    such as vendor lock-in considerations, customization capabilities, resource conservation,
    and integration with existing infrastructure. The choice of a specific implementation
    depends on the organization’s requirements, including governance, compliance,
    and operational needs.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, examining how different companies utilize Cluster API provides
    valuable insights into the practical applications and benefits of this tool. For
    instance, the Azure Provider for Cluster API (**CAPZ**) bridges the gap between
    Microsoft Azure’s managed Kubernetes service, AKS, and Kubernetes-native management,
    aligning with Cluster API’s standardized, declarative approach. Meanwhile, open
    source solutions such as **CLASTIX’s Kamaji** emphasize flexibility and control,
    catering to organizations keen on avoiding vendor lock-in. In contrast, **VMware’s
    vSphere with Tanzu** integrates Kubernetes into its platform, streamlining workflows
    and offering a seamless Kubernetes-native environment. Similarly, **Giant Swarm**
    and **Kubermatic Kubernetes Platform** target specific operational needs such
    as compatibility with Cluster API versions and resource conservation. Each of
    these implementations showcases the adaptability of Cluster API, underlining its
    importance in providing flexible, cloud-agnostic Kubernetes solutions in the modern
    technological landscape.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster API, like any technology, comes with its own set of advantages and disadvantages.
    Understanding these can help in determining whether it’s the right tool for a
    specific Kubernetes management scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few advantages of Cluster API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency and standardization**: Cluster API provides a standardized way
    to manage Kubernetes clusters. This consistency is crucial for large-scale and
    multi-cloud environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automation and scalability**: It automates the process of creating, configuring,
    and managing Kubernetes clusters, which is beneficial for organizations that need
    to scale their operations efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Declarative API**: Aligning with the Kubernetes principle of declarative
    configuration, Cluster API allows users to define their desired state for clusters,
    which the system then works to achieve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with the cloud-native ecosystem**: It integrates well with other
    tools in the Kubernetes ecosystem, offering a seamless experience for managing
    clusters as part of the broader cloud-native infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-cloud and hybrid cloud support**: Cluster API supports multiple cloud
    providers, making it easier to manage clusters in a hybrid or multi-cloud environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Community support**: Being a part of the Kubernetes project, it benefits
    from strong community support and ongoing development efforts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are a few disadvantages of Cluster API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complexity**: Cluster API can be complex to understand and implement, especially
    for users new to Kubernetes or cloud-native technologies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited customization in some areas**: While it offers a standardized approach,
    this can sometimes limit customization options for specific use cases or environments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependency on Kubernetes expertise**: Effective use of Cluster API requires
    a good understanding of Kubernetes concepts and architecture'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource overhead**: Running additional controllers and resources for managing
    clusters could lead to increased resource consumption in your Kubernetes environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning curve**: For teams not familiar with Kubernetes’ declarative model
    and API-centric management, there can be a significant learning curve'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, Cluster API is a powerful tool for organizations looking to automate
    and standardize their Kubernetes cluster management, especially across large-scale
    and multi-cloud environments. However, its complexity and the need for Kubernetes
    expertise might pose challenges for some teams. As with any technological decision,
    it’s important to evaluate these factors in the context of specific organizational
    needs and capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will go hands-on and use Cluster API and GitOps to deploy
    Kubernetes clusters on Azure.
  prefs: []
  type: TYPE_NORMAL
- en: A deep dive into Cluster API and GitOps – hands-on
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll use Cluster API to provision a Kubernetes cluster in
    Azure by using the declarative approach with Argo CD on VMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s see what our environment looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure tenant ID
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure subscription
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure app registration with Contributor access to the subscription
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Kubernetes** **Service** (**AKS**)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The necessary Azure infrastructure, including **Virtual Machines Scale Sets**
    (**VMSS**), virtual networks, and more
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Managed cluster – AKS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Argo CD running on a managed cluster
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Workload cluster – VMSS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Control plane
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Nodes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tools:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clusterctl`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`az cli`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`helm`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Initializing the management cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `clusterctl` command takes a list of providers to install as input. When
    executed for the first time, `clusterctl init` automatically includes the `cluster-api`
    core provider in the list. If not specified, it also adds the `kubeadm` bootstrap
    and `kubeadm` control plane providers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The output also mentions that the installation of `cert-manager` is skipped
    because it is already installed. This step is important because `cert-manager`
    plays a critical role in managing certificates within Kubernetes environments,
    ensuring secure communication between cluster components by automating the issuance
    and renewal of TLS certificates.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the output reflects the successful setup of a management cluster with
    essential providers for Kubernetes cluster management, including Cluster API,
    Kubeadm, and Azure infrastructure, each installed in specific namespaces. This
    step is crucial for streamlining Kubernetes operations as workload clusters can
    be created with a simple command, thereby facilitating efficient cluster deployment
    and management.
  prefs: []
  type: TYPE_NORMAL
- en: Creating your first workload cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the management cluster is ready, you can create your first workload cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you select a VM size that is available in your desired location for
    your subscription. To see the available SKUs, use the `az vm list-skus -l <your_location>
    -r virtualMachines -o` `table` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please replace the following variables with your specific values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The following command prepares a YAML manifest for deploying a Kubernetes cluster
    named `capi-quickstart` and specifies Kubernetes version `1.29.0`, one control
    plane machine, and three worker machines. By saving this configuration to `capi-quickstart.yaml`,
    it enables automated and consistent cluster deployment, encapsulating the desired
    state and structure of the cluster in a single file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you are ready to generate the cluster YAML manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, you should have a file that contains the following custom resources
    in the `capi-quickstart.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`KubeadmConfigTemplate`: This is the schema for the `kubeadmconfigtemplates`
    API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AzureClusterIdentity`: This is the schema for the `azureclustersidentities`
    API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AzureMachineTemplate`: These templates define the specifications for creating
    Azure VMs within the cluster. This is the schema for the `azuremachinetemplates`
    API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MachineDeployment`: This custom resource specifies the desired number of worker
    nodes and their properties. It helps in scaling the cluster by automatically managing
    the creation and scaling of worker nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KubeadmControlPlane`: This defines the control plane for the Kubernetes cluster,
    including settings such as the number of control plane nodes and their configurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AzureCluster`: This custom resource represents the Azure-specific details
    of the cluster, such as the network configuration and virtual network details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Cluster`: This defines the high-level cluster configuration, including control
    plane settings, worker node references, and provider-specific details. This is
    the top-level resource that represents the entire Kubernetes cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, you can apply the file using `kubectl`. However, we’ll leverage GitOps
    with Argo CD to maximize the benefits of the declarative approach. So, create
    an *application*, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, you can view the provisioning of the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, view the provisioning state of the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding output, it’s evident that the cluster is partially operational.
    Having `READY True` across various components in the output indicates the operational
    status of the cluster. Specifically, the readiness of the cluster (`Cluster/capi-quickstart`),
    its Azure infrastructure (`AzureCluster/capi-quickstart`), and `KubeadmControlPlane/capi-quickstart-control-plane`
    being marked as `True` shows that these critical parts of the cluster are fully
    operational. The control plane nodes are running, but the worker nodes haven’t
    started yet.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The control plane won’t be ready until we install a **container network** **interface**
    (**CNI**).
  prefs: []
  type: TYPE_NORMAL
- en: The worker nodes are not yet operational because we need to deploy the CNI components.
    It’s important to note that Azure does not currently support Calico networking.
    CAPZ clusters that use the default Calico configuration will experience issues
    with DNS functionality. To address this, we will deploy a Calico spec that utilizes
    VXLAN encapsulation for Pod traffic. You can deploy the Azure **Calico CNI** using
    the template provided here.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain `kubeconfig` so that you can interact with the cluster, follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command retrieves the `kubeconfig` details for the `capi-quickstart`
    Kubernetes cluster and saves it to a file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to install the CNI plugin on the workload cluster using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: kubectl --kubeconfig=./capi-quickstart.kubeconfig get nodes
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '#Output like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NAME                                  STATUS   ROLES           AGE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: capi-quickstart-control-plane-kcqmm   Ready    control-plane   8m26s
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: capi-quickstart-md-0-2kj9c            Ready    <none>          6m58s
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: capi-quickstart-md-0-7krx6            Ready    <none>          6m56s
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: capi-quickstart-md-0-b8m7r            Ready    <none>          7m2s
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now you can add the cluster to your Argo CD cockpit as a fleet ship and continue
    working.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In Azure, due to the peculiarities of CNI, doing this requires some additional
    work, such as setting up a webhook that deploys the CNI plugin as soon as `status
    control-plane=true` is achieved. This slightly restricts the self-service aspect
    and requires extension in the form of **CI/CD and webhooks**, for example. However,
    Cluster API offers various other providers where this may not be necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Workflow visualized](img/B22100_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Workflow visualized
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we demonstrated how to utilize Cluster API to create a declarative
    setup that can be deployed by Argo CD to provision the infrastructure or workload
    clusters. *Figure 6**.7* illustrates this process. Subsequently, Argo CD can be
    layered on top of it using `argocd cluster add`. This allows you to utilize tools
    such as **Kubernetes Service Catalog** (**KSC**) to deploy services based on labels.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we embarked on a comprehensive exploration of GitOps within
    Kubernetes environments, uncovering pivotal insights and strategies that are crucial
    for modern cloud-native deployments. We began by examining the criticality of
    tailoring architectural designs for scalability, resilience, and efficiency, all
    of which are foundational principles in today’s dynamic cloud landscapes. This
    journey through architectural frameworks underscored the indispensability of IaC,
    not only for its collaborative and version control benefits but also for establishing
    immutable infrastructure that resists manual alterations in live environments.
    Emphasizing modular design, we highlighted how efficient microservices management
    can transform operational workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Then, our exploration delved into the depths of architectural choices and their
    significant impact on the effectiveness of GitOps. We learned the importance of
    adopting a declarative model, an approach that seamlessly integrates version control,
    change management, and automated synchronization. This section illuminated the
    considerations necessary when selecting repository structures, weighing the merits
    of monolithic against multi-repository strategies. It also discussed the vital
    role of deployment strategies, environment isolation, scalability, and security,
    each a cornerstone in realizing a robust GitOps implementation.
  prefs: []
  type: TYPE_NORMAL
- en: We then transitioned to understanding the role of GitOps in enforcing systematic
    improvements in cloud-native architectures. Key aspects such as horizontal scaling,
    microservices, stateless applications, high availability, fault isolation, and
    disaster recovery were dissected. We explored how GitOps can manage deployments,
    ensuring resilience and efficiency through tactics such as auto-scaling, load
    balancing, and setting resource limits.
  prefs: []
  type: TYPE_NORMAL
- en: The second part of this chapter shifted our focus to various architectural approaches
    tailored for GitOps in Kubernetes environments. As organizations evolve, the need
    for scalable, secure, and efficient deployment strategies becomes paramount. We
    delved into the world of Argo CD, examining its capabilities in scaling performance,
    security, usability, and failover processes. We compared and contrasted the nuances
    between managing clusters with a centralized Argo CD instance and dedicated instances
    per cluster. The differences between Argo CD and Flux CD were also highlighted,
    providing a balanced view of their respective strengths and weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we explored the innovative cockpit and fleet approach with Argo CD,
    a strategy that goes beyond mere Kubernetes cluster management and includes provisioning
    clusters declaratively using Cluster API.
  prefs: []
  type: TYPE_NORMAL
- en: As we pave the way for the next chapter, we will delve into the necessary cultural
    shifts for successfully implementing and operating GitOps. We’ll explore the transformation
    of treating infrastructure as an application and the principles of immutable infrastructure
    before delving into various **DevOps Research and Assessment** (**DORA**) metrics.
    We’ll also discuss the critical need for continual improvement in GitOps and overcoming
    cultural barriers that may hinder its adoption. This sets the stage for a profound
    understanding that successful GitOps is not just about the right tools and technologies
    but also about cultural adaptation and evolution within the IT landscape.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*1*] [https://akuity.io/blog/argo-cd-architectures-explained](https://akuity.io/blog/argo-cd-architectures-explained)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*2*] [https://codefresh.io/blog/a-comprehensive-overview-of-argo-cd-architectures-2023/](https://codefresh.io/blog/a-comprehensive-overview-of-argo-cd-architectures-2023/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*3*] [https://www.youtube.com/watch?v=p8BluR5WT5w](https://www.youtube.com/watch?v=p8BluR5WT5w)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*4*] [https://github.com/openclarity/kubeclarity](https://github.com/openclarity/kubeclarity)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*5*] [https://github.com/aquasecurity/trivy-operator](https://github.com/aquasecurity/trivy-operator)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*6*] [https://github.com/fluxcd/flux2/releases/tag/v2.0.0](https://github.com/fluxcd/flux2/releases/tag/v2.0.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*7*] [https://cluster-api.sigs.k8s.io](https://cluster-api.sigs.k8s.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
