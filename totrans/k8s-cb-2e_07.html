<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Kubernetes on GCP</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will use <strong>Google Cloud Platform</strong> (<strong>GCP</strong>) in the following recipes:</p>
<ul>
<li>Playing with GCP</li>
<li>Setting up managed Kubernetes via <strong>Google Kubernetes Engine</strong> (<strong>GKE</strong>)</li>
<li>Exploring Kubernetes CloudProvider on GKE</li>
<li>Managing a Kubernetes cluster on GKE</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Playing with GCP</h1>
                </header>
            
            <article>
                
<p>GCP <span>is getting popular in the public cloud industry. It has concepts similar to AWS, such as VPC, a compute engine, persistent disks, load balancing, and several managed services. The most interesting service is GKE, which is the managed Kubernetes cluster. We will explore how to use GCP and GKE.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p> To use GCP, you need to have a Google account such as Gmail (<a href="https://mail.google.com/mail/)">https://mail.google.com/mail/</a>), which many people already have. Then sign up to GCP using your Google account by following these steps:</p>
<ol>
<li>Go to the <a href="https://cloud.google.com">https://cloud.google.com</a> website then click the <span class="packt_screen">Try it free</span> button</li>
<li>Log in <span>to Google using your Google account</span></li>
<li>Register with GCP and enter your personal information and billing information</li>
</ol>
<p>That's it!</p>
<p>Once registration is complete, you'll see the GCP Web Console page. In the beginning, it may ask you to create one project; the default name could be <span class="packt_screen">My First Project</span>. You can keep it, but we will create another project in this chapter, the better to help you understand.</p>
<p>The GCP Web Console is enough as a first step. <span>But to keep using the Web Console is not recommended for DevOps, because human manual input always causes human errors and Google might change the Web Console design in the future.</span></p>
<p>Thus, we will use the CLI<span>. GCP provides a CLI tool</span> called <span>Cloud SDK (</span><a href="https://cloud.google.com/sdk/">https://cloud.google.com/sdk/</a><span>). So, let's create one new GCP project and then install Cloud SDK on your machine.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a GCP project</h1>
                </header>
            
            <article>
                
<p>We will create a new project from scratch by following steps. It will help you to understand how does GCP project works:</p>
<ol>
<li>Go to the project page by clicking the <span class="packt_screen">My First Project</span> link:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-459 image-border" src="assets/a34389e9-cb29-4c1b-8f1f-4df4957081e7.png" style="width:63.08em;height:21.92em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Navigating to the project link</div>
<ol start="2">
<li>You may see your own projects to choose from, but this time click the + button to create a new one:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-460 image-border" src="assets/47198482-4866-4daa-9443-182e99c7c608.png" style="width:68.83em;height:14.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Creating a new project</div>
<ol start="3">
<li>Type the project name as <kbd>Kubernetes Cookbook</kbd>. Then GCP will generate and assign a project ID such as <span class="packt_screen">kubernetes-cookbook-12345</span>. Please remember this project ID.</li>
</ol>
<div class="packt_tip">You may notice that your project ID is NOT <span class="packt_screen">kubernetes-cookbook,</span> like <span class="packt_screen">kubernetes-cookbook-194302</span> in the screenshot as shown in the following screenshot. And even you click <span class="packt_screen">Edit</span> to attempt to change it to <span class="packt_screen">kubernetes-cookbook</span>, it doesn't allow it, because the project ID is a unique string for all GCP users. And we already took the <span class="packt_screen">kubernetes-cookbook</span> project ID.</div>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-461 image-border" src="assets/0fd87e3a-8715-4e46-a992-f368a6bf95e4.png" style="width:55.83em;height:16.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Project name and Project ID</div>
<ol start="4">
<li>After a few minutes, your project is ready to use. Go back to the project selection page on the top banner and then select your <span class="packt_screen">Kubernetes Cookbook</span> project:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-462 image-border" src="assets/93685e0e-2de7-4b92-bc57-d8240fcb7af6.png" style="width:61.42em;height:15.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Selecting a Kubernetes Cookbook project</div>
<p>Done! You can at any time switch to your project and the <span class="packt_screen">Kubernetes Cookbook</span> project. That is is isolated environment; any VPC, VM, IAM users and even billing methods are independent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Cloud SDK</h1>
                </header>
            
            <article>
                
<p>Next, install Cloud SDK on your machine. It supports the Windows, Mac, and Linux platforms. All of these require a Python interpreter version 2.7, but most macOS and Linux installs use the defaults.</p>
<p>On the other hand, Windows does't have the Python interpreter by default. However, in the Cloud SDK installer for Windows, it is possible to install Python. Let's install Cloud SDK on Windows and macOS step by step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Cloud SDK on Windows</h1>
                </header>
            
            <article>
                
<p>Cloud SDK provides an installer for Windows. It also include Python interpreter for Windows as well. Please follow the following steps to install on your Windows machine:</p>
<ol>
<li>Download the Cloud SDK installer on Windows (<a href="https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe">https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe</a>).</li>
<li>Run the Cloud SDK installer.</li>
</ol>
<p style="padding-left: 60px">If you've never installed a Python interpreter on your Windows machine, you have to choose the <span class="packt_screen">Bundled Python</span> option:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-463 image-border" src="assets/f28b2426-efed-4bf7-8cd6-5c7dd9758275.png" style="width:27.58em;height:20.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Cloud SDK installer for Windows</div>
<ol start="3">
<li> Other than that, proceed with the installation with the default options.</li>
<li>Once the installation is done, you can find <span class="packt_screen">Google Cloud SDK Shell</span> in the <span class="packt_screen">Google Cloud SDK</span> program group. Click it to launch a <span class="packt_screen">Google Cloud SDK Shell</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-464 image-border" src="assets/1232e920-4d5f-44d9-91c5-52a00344d03e.png" style="width:43.83em;height:20.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Google Cloud SDK Shell in the Google Cloud SDK program group</span></div>
<ol start="5">
<li>Type <kbd>gcloud info</kbd> to check whether you can see the Cloud SDK version:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-465 image-border" src="assets/443c01fd-9b87-450e-a00e-1f2351558d22.png" style="width:46.33em;height:23.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Running the gcloud command on Windows</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Cloud SDK on Linux and macOS</h1>
                </header>
            
            <article>
                
<p>Installing Cloud SDK on both Linux and macOS <span>follows the steps listed here</span>. Let's install Cloud SDK under your home directory:</p>
<ol>
<li>Open the Terminal.</li>
<li>Type the following command to download and run the Cloud SDK installer:</li>
</ol>
<pre style="padding-left: 90px">$ curl https://sdk.cloud.google.com | bash</pre>
<ol start="3">
<li>It asks for your desired installation directory. By default, it is under your home directory. So, type <kbd>return</kbd>:</li>
</ol>
<pre style="padding-left: 90px">Installation directory (this will create a google-cloud-sdk subdirectory) (/Users/saito):</pre>
<ol start="4">
<li>It asks whether to send user usage data; it will send some information when it crashes. Based on your privacy policy, if don't wish to send any data to Google, choose <kbd>n</kbd>. Otherwise choose <kbd>Y</kbd> to improve their quality:</li>
</ol>
<pre style="padding-left: 90px">Do you want to help improve the Google Cloud SDK (Y/n)? <strong>n</strong></pre>
<ol start="5">
<li>It asks whether to update <kbd>.bash_profile</kbd> by adding the <kbd>gcloud</kbd> command to your command search path; type <kbd>y</kbd> to proceed:</li>
</ol>
<pre style="padding-left: 90px">Modify profile to update your $PATH and enable shell command<br/>completion?<br/>Do you want to continue (Y/n)?  <strong>y</strong><br/>The Google Cloud SDK installer will now prompt you to update an rc<br/>file to bring the Google Cloud CLIs into your environment.<br/>Enter a path to an rc file to update, or leave blank to use<br/>[/Users/saito/.bash_profile]:</pre>
<ol start="6">
<li>Open another Terminal or type <kbd>exec -l $SHELL</kbd> to refresh your command search path:</li>
</ol>
<pre style="padding-left: 90px">//reload .bash_profile<br/>$ exec -l $SHELL<br/><br/>//check gcloud command is in your search path<br/>$ which gcloud<br/>/Users/saito/google-cloud-sdk/bin/gcloud</pre>
<ol start="7">
<li>Type <kbd>gcloud info</kbd> to check whether you can see the Cloud SDK version:</li>
</ol>
<pre style="padding-left: 90px">$ gcloud info<br/>Google Cloud SDK [187.0.0]<br/>Platform: [Mac OS X, x86_64] ('Darwin', 'Hideto-Saito-no-MacBook.local', '17.4.0', 'Darwin Kernel Version 17.4.0: Sun Dec 17 09:19:54 PST 2017; root:xnu-4570.41.2~1/RELEASE_X86_64', 'x86_64', 'i386')<br/>Python Version: [2.7.14 (default, Jan 21 2018, 12:22:04)  [GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.38)]]<br/>Python Location: [/usr/local/Cellar/python/2.7.14_2/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python]</pre>
<p>Now you can start to configure Cloud SDK!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring Cloud SDK</h1>
                </header>
            
            <article>
                
<p>You can configure both Cloud SDK for Windows and for Linux/macOS, by using the following steps:</p>
<ol>
<li>Launch <span class="packt_screen">Google Cloud SDK Shell</span> (Windows) or open a Terminal (Linux/macOS).</li>
</ol>
<p> </p>
<ol start="2">
<li>Type <kbd>gcloud init</kbd>; it asks you to log on to your Google account. Type <kbd>y</kbd> and press return:</li>
</ol>
<pre style="padding-left: 90px">You must log in to continue. Would you like to log in (Y/n)? <strong>y</strong></pre>
<ol start="3">
<li>It will open a web browser to navigate to the Google logon page; proceed to log on using your Google Account with the GCP account.</li>
<li>It asks you whether Cloud SDK can access your Google account information. Click the <span class="packt_screen">ALLOW</span> button.</li>
<li>Back to the Terminal—it asks you which project you want to use. Let's choose the <span class="packt_screen">Kubernetes Cookbook</span> project you made:</li>
</ol>
<pre style="padding-left: 90px">Pick cloud project to use:<br/> [1] my-first-project-194302<br/> [2] kubernetes-cookbook<br/> [3] Create a new project<br/>Please enter numeric choice or text value (must exactly match list item):  <strong>2</strong></pre>
<ol start="6">
<li> It asks you whether to configure <kbd>Compute Engine</kbd> or not. Let's type <kbd>n</kbd> to skip it this time:</li>
</ol>
<pre style="padding-left: 90px">Do you want to configure Google Compute Engine<br/>(https://cloud.google.com/compute) settings (Y/n)?  <strong>n</strong></pre>
<p>Now you can start to use Cloud SDK to control GCP. Let's create VPC, subnet, and firewall rules, then launch a VM instance to set up our own GCP infrastructure.</p>
<div class="packt_tip">If you chose the wrong project or you want to try again, at any time you can reconfigure your setup by the <kbd>gcloud init</kbd> command.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>We will go through GCP's basic functionality to set up an infrastructure under the <span class="packt_screen">Kubernetes Cookbook</span> project. By using the <kbd>gcloud</kbd> command, we will create these components:</p>
<ul>
<li>One new VPC</li>
<li>Two subnets (<span><kbd>us-central1</kbd> and <kbd>us-east1</kbd></span>) in the VPC</li>
<li>Three firewall rules (<kbd>public-ssh</kbd>, <kbd>public-http</kbd>, and <kbd>private-ssh</kbd>)</li>
<li>We will add your ssh public key to a project-wide metadata</li>
</ul>
<p><span>Overall, your infrastructure will resemble the following. Let's configure the components one by one:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-467 image-border" src="assets/e360bc7c-6a8d-4dc1-83db-41c845ded19b.png" style="width:32.83em;height:15.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Target infrastructure</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a VPC</h1>
                </header>
            
            <article>
                
<p>VPC in GCP is like AWS, but there's no need to bind a particular region, and also no need to set the CIDR address range. This means you can create a VPC that covers all regions. By default, your Kubernetes Cookbook project has a <span class="packt_screen">default</span> VPC.</p>
<p>However, for a better understanding, let's create a new VPC by following these steps:</p>
<ol>
<li>Run the <kbd>gcloud compute networks</kbd> command to create a new VPC. The name is <kbd>chap7 </kbd>and subnet-mode is <kbd>custom</kbd>, which means subnets are not created automatically. So we will add it manually in the next step:</li>
</ol>
<pre style="padding-left: 90px">$ gcloud compute networks create chap7 --subnet-mode=custom</pre>
<ol start="2">
<li>Check the VPC list; you should have two VPCs, <kbd>default</kbd> VPC and <kbd>chap7</kbd> VPC:</li>
</ol>
<pre style="padding-left: 90px">$ gcloud compute networks list<br/><span>NAME     SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4<br/></span><span><strong>chap7    CUSTOM       REGIONAL</strong><br/></span><span>default  AUTO         REGIONAL</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating subnets</h1>
                </header>
            
            <article>
                
<p>Let's create two subnets under the <kbd>chap7</kbd> VPC (network) by following these steps:</p>
<ol>
<li>In order to create a subnet, you have to choose the region. By typing <kbd>gcloud compute regions list</kbd> you will know which regions are available to you:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-468 image-border" src="assets/9a46d42e-477c-436b-9b2c-fbef497e1ab9.png" style="width:52.25em;height:22.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Displaying a GCP region list</div>
<ol start="2">
<li>Let's choose <kbd>us-central1</kbd> and <kbd>us-east1</kbd> to create two subnets under the <kbd>chap7</kbd> VPC with the following configuration:</li>
</ol>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>Subnet name</strong></td>
<td><strong>VPC</strong></td>
<td><strong>CIDR range</strong></td>
<td><strong>Region</strong></td>
</tr>
<tr>
<td><kbd>chap7-us-central1</kbd></td>
<td><kbd>chap7</kbd></td>
<td><kbd>192.168.1.0/24</kbd></td>
<td><kbd>us-central1</kbd></td>
</tr>
<tr>
<td><kbd>chap7-us-east1</kbd></td>
<td><kbd>chap7</kbd></td>
<td><kbd>192.168.2.0/24</kbd></td>
<td><kbd>us-east1</kbd></td>
</tr>
</tbody>
</table>
<pre style="padding-left: 90px">$ gcloud compute networks subnets create chap7-us-central1 --network=chap7 --range=192.168.1.0/24 --region us-central1<br/><br/>$ gcloud compute networks subnets create chap7-us-east1 --network=chap7 --range=192.168.2.0/24 --region us-east1</pre>
<ol start="3">
<li>Check the following command to see whether subnets are configured properly or not:</li>
</ol>
<pre style="padding-left: 90px">$ gcloud compute networks subnets list --network=chap7<br/><span>NAME               REGION       NETWORK  RANGE<br/></span><span>chap7-us-east1     us-east1     chap7    192.168.2.0/24<br/></span><span>chap7-us-central1  us-central1  chap7    192.168.1.0/24</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating firewall rules</h1>
                </header>
            
            <article>
                
<p>Firewall rules are similar to an AWS Security Group in that you can define incoming and outgoing packet filters. They use a network tag, which is a label, to distinguish between firewall rules and VM instances. So, VM instances can specify zero or some network tags, then the firewall rule will apply to the VM which has the same Network Tag.</p>
<p>Therefore, we need to set a <span class="packt_screen">target network tag</span> while creating the firewall rule. Overall, we will create three firewall rules that have these configurations:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Firewall rule name</strong></p>
</td>
<td>
<p><strong>Target VPC</strong></p>
</td>
<td>
<p><strong>Allow port</strong></p>
</td>
<td>
<p><strong>Allow from</strong></p>
</td>
<td>
<p><strong>Target network tag</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>public-ssh</kbd></p>
</td>
<td>
<p><kbd>chap7</kbd></p>
</td>
<td>
<p><kbd>ssh</kbd> (22/tcp)</p>
</td>
<td>
<p>All (<kbd>0.0.0.0/0</kbd>)</p>
</td>
<td>
<p>public</p>
</td>
</tr>
<tr>
<td>
<p><kbd>public-http</kbd></p>
</td>
<td>
<p><kbd>chap7</kbd></p>
</td>
<td>
<p><kbd>http</kbd> (80/tcp)</p>
</td>
<td>
<p>All (<kbd>0.0.0.0/0</kbd>)</p>
</td>
<td>
<p>public</p>
</td>
</tr>
<tr>
<td>
<p><kbd>private-ssh</kbd></p>
</td>
<td>
<p><kbd>chap7</kbd></p>
</td>
<td>
<p><kbd>ssh</kbd> (22/tcp)</p>
</td>
<td>
<p>Host which has a public network tag</p>
</td>
<td>
<p>private</p>
</td>
</tr>
</tbody>
</table>
<ol>
<li>Create a <kbd>public-ssh</kbd> rule:</li>
</ol>
<pre style="padding-left: 90px">$ gcloud compute firewall-rules create public-ssh --network=chap7 --allow="tcp:22" --source-ranges="0.0.0.0/0" --target-tags="public"</pre>
<ol start="2">
<li>Create a <kbd>public-http</kbd> rule:</li>
</ol>
<pre style="padding-left: 90px">$ gcloud compute firewall-rules create public-http --network=chap7 --allow="tcp:80" --source-ranges="0.0.0.0/0" --target-tags="public"</pre>
<ol start="3">
<li>Create a <kbd>private-ssh</kbd> rule:</li>
</ol>
<pre style="padding-left: 90px">$ gcloud compute firewall-rules create private-ssh --network=chap7 --allow="tcp:22" --source-tags="public" --target-tags="private"</pre>
<ol start="4">
<li>Check all firewall rules:</li>
</ol>
<pre style="padding-left: 90px">$ gcloud compute firewall-rules list --filter='NETWORK=chap7'<br/><span>NAME         NETWORK  DIRECTION  PRIORITY  ALLOW   DENY<br/></span><span>private-ssh  chap7    INGRESS    1000      tcp:22<br/></span><span>public-http  chap7    INGRESS    1000      tcp:80<br/></span><span>public-ssh   chap7    INGRESS    1000      tcp:22</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding your ssh public key to GCP</h1>
                </header>
            
            <article>
                
<p>Before you launch VM instances, you need to upload your ssh public key in order to log on to the VM. If <span>you don't have any</span> ssh keys, you have to run the <kbd>ssh-keygen</kbd> command to generate a key pair (public key and private key). Let's assume you have a public key as <kbd>~/.ssh/id_rsa.pub</kbd> and a private key as <kbd>~/.ssh/id_rsa</kbd></p>
<ol>
<li>Check your login user name by using the <kbd>whoami</kbd> command, then use <kbd>gcloud compute config-ssh</kbd> to upload your key via the following command:</li>
</ol>
<pre style="padding-left: 90px">$ whoami<br/>saito<br/><br/>$ gcloud compute config-ssh --ssh-key-file=~/.ssh/id_rsa</pre>
<ol start="2">
<li>Check your ssh public key is registered as metadata:</li>
</ol>
<pre style="padding-left: 90px">$ gcloud compute project-info describe --format=json<br/><span>{<br/></span><span>  "commonInstanceMetadata": {<br/></span><span>    "fingerprint": "fAqDGp0oSMs=",<br/></span><span>    "items": [<br/></span><span>      {<br/></span><span>        "key": "<strong>ssh-keys</strong>",<br/></span><span>        "value": "<strong>saito</strong>:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDAr1cHrrONuaPgN20sXCPH8uT2lOjWRB3zEncOTxOI2lCW6DM6Mr31boboDe0kAtUMdoDU43yyMe4r734SmtMuh...<br/></span></pre>
<p>That's all. These are minimal configurations in order to launch a VM instance. So, let's launch some VM instances on this infrastructure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Now you have your own VPC, subnet, and firewall rules. This infrastructure will be used by the compute engine (VM instances), Kubernetes Engine, and some other GCP products. Let's deploy two VM instances onto your VPC, as in the following diagram, to see how it works:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-469 image-border" src="assets/061df439-c2c1-48b2-9aeb-9645e75b1deb.png" style="width:56.25em;height:26.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Final state</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Launching VM instances</h1>
                </header>
            
            <article>
                
<p>We will launch two VM instances on both <kbd>us-central1</kbd> and <kbd>us-east1</kbd> by using the following configuration:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>VM Instance name</strong></p>
</td>
<td>
<p><strong>Target VPC</strong></p>
</td>
<td>
<p><strong>zone (see <span>the following steps</span>)</strong></p>
</td>
<td>
<p><strong>Target Subnet</strong></p>
</td>
<td>
<p><strong>Assign Network Tag</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>chap7-public</kbd></p>
</td>
<td>
<p><kbd>chap7</kbd></p>
</td>
<td>
<p><kbd>us-central1-a</kbd></p>
</td>
<td>
<p><kbd>chap7-us-central1</kbd></p>
</td>
<td>
<p>public</p>
</td>
</tr>
<tr>
<td>
<p><kbd>chap7-private</kbd></p>
</td>
<td>
<p><kbd>chap7</kbd></p>
</td>
<td>
<p><kbd>us-east1-b</kbd></p>
</td>
<td>
<p><kbd>chap7-us-east1</kbd></p>
</td>
<td>
<p>private</p>
</td>
</tr>
</tbody>
</table>
<ol>
<li>Check the available zones in <kbd>us-central1</kbd> and <kbd>us-east1</kbd> by using the following command:</li>
</ol>
<pre style="padding-left: 90px">$ gcloud compute zones list --filter='name:(us-east1,us-central1)'<br/><span>NAME           REGION       STATUS  NEXT_MAINTENANCE  TURNDOWN_DATE<br/></span><span><strong>us-east1-b</strong>     us-east1     UP<br/></span><span>us-east1-c     us-east1     UP<br/></span><span>us-east1-d     us-east1     UP<br/></span><span>us-central1-c  us-central1  UP<br/></span><span><strong>us-central1-a</strong>  us-central1  UP<br/></span><span>us-central1-f  us-central1  UP<br/></span><span>us-central1-b  us-central1  UP</span></pre>
<p style="padding-left: 60px">So, let's choose <kbd>us-central1-a</kbd> for <kbd>chap7-public</kbd> and <kbd>us-east1-b</kbd> for <kbd>chap7-private</kbd>:</p>
<ol start="2">
<li>Type the following command to create two VM instances:</li>
</ol>
<pre style="padding-left: 90px">$ gcloud compute instances create chap7-public --network=chap7 --subnet=chap7-us-central1 --zone=us-central1-a --tags=public --machine-type=f1-micro<br/><br/>$ gcloud compute instances create chap7-private --network=chap7 --subnet=chap7-us-east1 --zone=us-east1-b --tags=private --machine-type=f1-micro</pre>
<ol start="3">
<li>Check the VM instance external IP address via the following command:</li>
</ol>
<pre style="padding-left: 90px">$ gcloud compute instances list<br/><span>NAME           ZONE           MACHINE_TYPE  PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP    STATUS<br/></span><span>chap7-public   us-central1-a  f1-micro                   192.168.1.2  <strong>35.224.14.45</strong>   RUNNING<br/></span><span>chap7-private  us-east1-b     f1-micro                   <strong>192.168.2.2</strong>  35.229.95.179  RUNNING</span></pre>
<ol start="4">
<li>Run <kbd>ssh-agent</kbd> to remember your ssh key:</li>
</ol>
<pre style="padding-left: 90px">$ ssh-add ~/.ssh/id_rsa</pre>
<ol start="5">
<li>ssh from your machine to <kbd>chap7-public</kbd> using the <kbd>-A</kbd> option (forward authentication) and using an external IP address:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-470 image-border" src="assets/01c661b0-0fb2-4541-a527-1d97a51994cd.png" style="width:59.17em;height:21.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">ssh to the public VM instance</div>
<ol start="6">
<li>ssh from <kbd>chap7-public</kbd> to <kbd>chap7-private</kbd> via the internal IP address:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-471 image-border" src="assets/0bdef6f5-f918-4d9f-ab06-2dfdf413ecc4.png" style="width:57.83em;height:19.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">ssh to private VM instance</div>
<ol start="7">
<li>Type the <kbd>exit</kbd> command to go back to the <kbd>chap7-public</kbd> host, then install <kbd>nginx</kbd> by using the <kbd>apt-get</kbd> command:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-472 image-border" src="assets/c932a618-0356-4c6b-aefc-c96873cd71b8.png" style="width:61.83em;height:19.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Installing nginx on a public VM instance</div>
<ol start="8">
<li>Launch <kbd>nginx</kbd> by using the following command:</li>
</ol>
<pre style="padding-left: 90px">$ sudo systemctl start nginx</pre>
<ol start="9">
<li>Access <kbd>chap7-public</kbd> (via the external IP) using your web browser:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-473 image-border" src="assets/e140b946-9757-4ec3-96a3-650eddc6f7ab.png" style="width:63.17em;height:18.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Accessing a nginx web server on a public VM instance</div>
<p>Congratulations! You have finished setting up a GCP VPC, Subnet, and firewall rules, and launch VM instances! These are very basic and common usages of Google Compute Engine. You can login and install software in these machines, or even build a Kubernetes cluster from scratch. However, GCP also has a managed Kubernetes product called Kubernetes Engine. We will explore it in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Playing with Google Kubernetes Engine</h1>
                </header>
            
            <article>
                
<p>Kubernetes was designed by google and widely used internally at Google for years. Google Cloud Platform offers the hosted GKE. With GKE, we don't need to build a cluster from scratch. Instead, clusters can be launched and turned down on demand.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We can use the Kubernetes Engine dashboard in the GCP console or the gcloud CLI to launched and configure a cluster. Using the console is very straightforward and intuitive. However, using CLI is a more flexible way to make the operation repeatable or to integrate it with your existing pipeline. In this recipe, we'll walk through how to use gcloud to launch and set up a Kubernetes cluster, along with some importants concept in GCP.</p>
<p>In GCP, everything is associated with a project. A GCP project is the basic unit for using GCP services, billing, and permission control. At first, we'll have to create a project from the GCP console <a href="https://console.cloud.google.com">https://console.cloud.google.com</a>.</p>
<p>The project ID is globally unique in GCP. After the project is properly created, we'll see there is a unique project number assigned. In the home dashboard, we'll have a clear view of how many resources we've used. We can set permissions, storage, network, billing, and other resources from here. Before we can move forward, we'll need to install gcloud. gcloud is  part of Google Cloud SDK. Other than gcloud, which can do most common operations in GCP, Google Cloud SDK also includes other common GCP tools, such as gsutil (to manage Cloud Storage), bq (a command-line tool for BigQuery), and core (Cloud SDK libraries). The tools are available at the Google cloud SDK download page: <a href="https://cloud.google.com/sdk/docs/#install_the_latest_cloud_tools_version_cloudsdk_current_version">https://cloud.google.com/sdk/docs/#install_the_latest_cloud_tools_version_cloudsdk_current_version</a>. </p>
<p>After gcloud is installed, run gcloud init to log in to set up your identity with gcloud and create a project named<strong> k8s-cookbook-2e</strong>. We can use gcloud to manipulate almost all the services in Google Cloud; the major command group is:</p>
<pre>gcloud container [builds|clusters|images|node-pools|operations] | $COMMAND $FLAG…</pre>
<p>The gcloud container command line set is used to manage our containers and clusters in Google Kuberentes Engine. For launching a cluster, the most important parameters are network settings. Let's spend some time understanding network terminology in GCP here. Just like AWS, GCP has the VPC concept as well. It's a private and safer way to isolate your compute, storage, and cloud resources with the public internet. It can be peered across projects, or established as a VPN with on-premise datacenters to create a hybrid cloud environment:</p>
<pre>// create GCP VPC, it might take few minutes.<br/># gcloud compute networks create k8s-network<br/>Created [https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/global/networks/k8s-network].<br/>NAME         SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4<br/>k8s-network  AUTO         REGIONAL</pre>
<p>Instances on this network will not be reachable until firewall rules are created. As an example, you can allow all internal traffic between instances as well as SSH, RDP, and ICMP by running:</p>
<pre>$ gcloud compute firewall-rules create &lt;FIREWALL_NAME&gt; --network k8s-network --allow tcp,udp,icmp --source-ranges &lt;IP_RANGE&gt;<br/>$ gcloud compute firewall-rules create &lt;FIREWALL_NAME&gt; --network k8s-network --allow tcp:22,tcp:3389,icmp</pre>
<p>By default, the VPC is created in auto mode, which will create a one subnet per region. We can observe that via the subcommand <kbd>describe</kbd>:</p>
<pre>// gcloud compute networks describe &lt;VPC name&gt;<br/># gcloud compute networks describe k8s-network<br/>autoCreateSubnetworks: true<br/>creationTimestamp: '2018-02-25T13:54:28.867-08:00'<br/>id: '1580862590680993403'<br/>kind: compute#network<br/>name: k8s-network<br/>routingConfig:<br/>  routingMode: REGIONAL<br/>selfLink: https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/global/networks/k8s-network<br/>subnetworks:<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/australia-southeast1/subnetworks/k8s-network<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/europe-west4/subnetworks/k8s-network<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/northamerica-northeast1/subnetworks/k8s-network<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/europe-west1/subnetworks/k8s-network<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/southamerica-east1/subnetworks/k8s-network<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/us-central1/subnetworks/k8s-network<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/us-east1/subnetworks/k8s-network<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/asia-east1/subnetworks/k8s-network<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/us-west1/subnetworks/k8s-network<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/europe-west3/subnetworks/k8s-network<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/asia-southeast1/subnetworks/k8s-network<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/us-east4/subnetworks/k8s-network<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/europe-west2/subnetworks/k8s-network<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/asia-northeast1/subnetworks/k8s-network<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/asia-south1/subnetworks/k8s-network<br/>x_gcloud_bgp_routing_mode: REGIONAL<br/>x_gcloud_subnet_mode: AUTO</pre>
<p>In GCP, each subnet is across a zone. A zone is an isolated location in a region, which is a similar concept to availability zones in AWS.</p>
<div class="packt_infobox">
<p>Alternatively, you could create a network in custom mode by adding the parameter <kbd>--subnet-mode=custom</kbd>, which allows you to define your desired IP range, region, and all the routing rules. For more details, please refer to the previous section.</p>
</div>
<p>Auto mode also helps you set up all default routing rules. A route serves to define the destination for certain IP ranges. For example, this route will direct the packet to virtual network <kbd>10.158.0.0/20</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-474 image-border" src="assets/7fe25562-f068-439c-9e86-1e0873de39b4.png" style="width:50.67em;height:5.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Default route example</div>
<p>There route which is used to direct the packet to the outside world. The next hop of this route is the default internet gateway, similar to the igw in AWS. In GCP, however, you don't need to explicitly create an internet gateway:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-475 image-border" src="assets/532e9bd0-5148-40f5-a51d-a02176ae9ee1.png" style="width:55.42em;height:3.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Default route for internet access</div>
<p>Another important concept in a GCP network is firewall rules, used to control the ingress and egress for your instance. In GCP, the association between firewall rules and VM instances is implemented by network tags.</p>
<div class="packt_infobox">
<p>A firewall rule can also be assigned to all instances in the network or a group of instances with a specific service account (ingress only). The service account is the identity of a VM instance in GCP. One or more roles can be assigned to a service account, so it can have access to other GCP resources. This is similar to <span>AWS instance profiles.</span></p>
</div>
<p>One VM instance can have more than one network tags, which implies multiple network routes could be applied. This diagram shows how tags work. In the following diagram, the first firewall rule is applied to VM1 and VM2, and VM2 has two firewall rules associated with it:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-476 image-border" src="assets/69c75148-13f4-4596-a4d4-dd6b32705650.png" style="width:60.50em;height:20.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Illustration of AWS security groups and GCP firewall rules</div>
<p>In <strong>AWS</strong>, one or more ingress/egress rules are defined in a <strong>Security Group</strong>, and one or more Security Groups can be assigned to a <strong>EC2</strong> instance. In <strong>GCP</strong>, on the other hand, one or more firewall rules are defined, which are associated with one or more tags. One or more tags can be assigned to an instance. By mapping network tags, firewall rules can control and limit  access in and out of your instances.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>We've learned the basic network concept in GCP. Let's launch our first GKE cluster:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Parameter</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
<td>
<p><strong>Value in example</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>--cluster-version</kbd></p>
</td>
<td>
<p>Supported cluster version (Refer to <a href="https://cloud.google.com/kubernetes-engine/release-notes">https://cloud.google.com/kubernetes-engine/release-notes</a>)</p>
</td>
<td>
<p><kbd>1.9.2-gke.1</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>--machine-type</kbd></p>
</td>
<td>
<p>Instance type of nodes (Refer to <a href="https://cloud.google.com/compute/docs/machine-types">https://cloud.google.com/compute/docs/machine-types</a>)</p>
</td>
<td>
<p><kbd>f1-micro</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>--num-nodes</kbd></p>
</td>
<td>
<p>Number of nodes in the cluster</p>
</td>
<td>
<p><kbd>3</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>--network</kbd></p>
</td>
<td>
<p>Target VPC network</p>
</td>
<td>
<p><kbd>k8s-network</kbd> (the one we just created)</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--zone</kbd></p>
</td>
<td>
<p>Target zone</p>
</td>
<td>
<p><kbd>us-central1-a</kbd> (you're free to use any zone)</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--tags</kbd></p>
</td>
<td>
<p>Network tags to be attached to the nodes</p>
</td>
<td>
<p>private</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--service-account | --scopes</kbd></p>
</td>
<td>
<p>Node identity (Refer to <a href="https://cloud.google.com/sdk/gcloud/reference/container/clusters/create">https://cloud.google.com/sdk/gcloud/reference/container/clusters/create</a> for more scope value)</p>
</td>
<td>
<p><kbd>storage-rw</kbd>,<kbd>compute-ro</kbd></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>By referring preceding parameters, let's launch a three nodes cluster by <kbd>gcloud</kbd> command:</p>
<pre>// create GKE cluster<br/>$ gcloud container clusters create my-k8s-cluster --cluster-version 1.9.2-gke.1 --machine-type f1-micro --num-nodes 3 --network k8s-network --zone us-central1-a --tags private --scopes=storage-rw,compute-ro<br/>WARNING: The behavior of --scopes will change in a future gcloud release: service-control and service-management scopes will no longer be added to what is specified in --scopes. To use these scopes, add them explicitly to --scopes. To use the new behavior, set container/new_scopes_behavior property (gcloud config set container/new_scopes_behavior true).<br/>WARNING: Starting in Kubernetes v1.10, new clusters will no longer get compute-rw and storage-ro scopes added to what is specified in --scopes (though the latter will remain included in the default --scopes). To use these scopes, add them explicitly to --scopes. To use the new behavior, set container/new_scopes_behavior property (gcloud config set container/new_scopes_behavior true).<br/>Creating cluster my-k8s-cluster...done.<br/>Created [https://container.googleapis.com/v1/projects/kubernetes-cookbook/zones/us-central1-a/clusters/my-k8s-cluster].<br/>To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-central1-a/my-k8s-cluster?project=kubernetes-cookbook<br/>kubeconfig entry generated for my-k8s-cluster.<br/>NAME            LOCATION       MASTER_VERSION  MASTER_IP    MACHINE_TYPE  NODE_VERSION  NUM_NODES  STATUS<br/>my-k8s-cluster  us-central1-a  1.9.2-gke.1     35.225.24.4  f1-micro      1.9.2-gke.1   3          RUNNING</pre>
<p>After the cluster is up-and-running, we can start to connect to the cluster by configuring <kbd>kubectl</kbd>:</p>
<pre># gcloud container clusters get-credentials my-k8s-cluster --zone us-central1-a --project kubernetes-cookbook<br/>Fetching cluster endpoint and auth data.<br/>kubeconfig entry generated for my-k8s-cluster.</pre>
<p>Let's see if the cluster is healthy:</p>
<pre>// list cluster components<br/># kubectl get componentstatuses<br/>NAME                 STATUS    MESSAGE              ERROR<br/>controller-manager   Healthy   ok<br/>scheduler            Healthy   ok<br/>etcd-0               Healthy   {"health": "true"}<br/>etcd-1               Healthy   {"health": "true"}</pre>
<p>And we can check the nodes inside the cluster:</p>
<pre>// list the nodes in cluster<br/># kubectl get nodes<br/>NAME                                            STATUS    ROLES     AGE       VERSION<br/>gke-my-k8s-cluster-default-pool-7d0359ed-0rl8   Ready     &lt;none&gt;    21m       v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-7d0359ed-1s2v   Ready     &lt;none&gt;    21m       v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-7d0359ed-61px   Ready     &lt;none&gt;    21m       v1.9.2-gke.1</pre>
<p>We can also use <kbd>kubectl</kbd> to check cluster info:</p>
<pre>// list cluster info<br/># kubectl cluster-info<br/>Kubernetes master is running at https://35.225.24.4<br/>GLBCDefaultBackend is running at https://35.225.24.4/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy<br/>Heapster is running at https://35.225.24.4/api/v1/namespaces/kube-system/services/heapster/proxy<br/>KubeDNS is running at https://35.225.24.4/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy<br/>kubernetes-dashboard is running at https://35.225.24.4/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy<br/>Metrics-server is running at https://35.225.24.4/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>Under the hood, gcloud creates a Kubernetes cluster with three nodes, along with a controller manager, scheduler, and etcd cluster with two members. We can also see that the master is launched with some services, including a default backend used by the controller, heapster (used for monitoring) KubeDNS for DNS services in the cluster, a dashboard for Kubernetes UI, and metrics-server for resource usage metrics.</p>
<p>We saw <kbd>Kubernetes-dashboard</kbd> has a URL; let's try and access it:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-477 image-border" src="assets/4e58c86e-2d4e-4495-a79f-3bdc37c3da1b.png" style="width:61.83em;height:12.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Forbidden to access Kubernetes dashboard</div>
<p>We got <kbd>HTTP 403 Forbidden</kbd>. Where do we get the access and credentials though? One way to do it is running a proxy via the <kbd>kubectl proxy</kbd> command. It will bind the master IP to local <kbd>127.0.0.1:8001</kbd>:</p>
<pre># kubectl proxy<br/>Starting to serve on 127.0.0.1:8001</pre>
<p>After that, when we access <kbd>http://127.0.0.1:8001/ui</kbd>, it'll be redirected to <kbd>http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy</kbd>.</p>
<p>Since Kubernetes 1.7, the dashboard has supported user authentication based on a bearer token or <kbd>Kubeconfig</kbd> file:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-478 image-border" src="assets/fd85ef71-b10b-40b8-af0e-31ba8d287dee.png" style="width:39.00em;height:24.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Logging in to the Kubernetes dashboard</div>
<p>You could create a user and bind it to the current context (please refer to the <em>Authentication and authorization</em> recipe in <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml" target="_blank">Chapter 8</a>, <em>Advanced Cluster Administration</em>). Just for convenience, we can check if we have any existing users. Firstly, we need to know our current context name. Context combines of cluster information, users for authentication, and a namespace:</p>
<pre>// check our current context name<br/># kubectl config current-context<br/>gke_kubernetes-cookbook_us-central1-a_my-k8s-cluster</pre>
<p>After we know the context name, we can describe it via the <kbd>kubectl</kbd> config view <kbd>$CONTEXT_NAME</kbd>:</p>
<pre>// kubectl config view $CONTEXT_NAME<br/># kubectl config view gke_kubernetes-cookbook_us-central1-a_my-k8s-cluster<br/>current-context: gke_kubernetes-cookbook_us-central1-a_my-k8s-cluster<br/>kind: Config<br/>preferences: {}<br/>users:<br/>- name: gke_kubernetes-cookbook_us-central1-a_my-k8s-cluster<br/>  user:<br/>    auth-provider:<br/>      config:<br/>        access-token: $ACCESS_TOKEN<br/>        cmd-args: config config-helper --format=json<br/>        cmd-path: /Users/chloelee/Downloads/google-cloud-sdk-2/bin/gcloud<br/>        expiry: 2018-02-27T03:46:57Z<br/>        expiry-key: '{.credential.token_expiry}'<br/>        token-key: '{.credential.access_token}'<br/>      name: gcp</pre>
<p>We may find there is a default user existing in our cluster; using its <kbd>$ACCESS_TOKEN</kbd>, you can glimpse the Kubernetes console.</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-479 image-border" src="assets/6825b49d-6934-41d2-83a8-cb3a098d2675.png" style="width:55.75em;height:28.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Kubernetes dashboard overview</div>
<p>Our cluster in GKE is up-and-running! Let's try and see if we can run a simple deployment on it:</p>
<pre># kubectl run nginx --image nginx --replicas=2<br/>deployment "nginx" created<br/># kubectl get pods<br/>NAME                   READY     STATUS    RESTARTS   AGE<br/>nginx-8586cf59-x27bj   1/1       Running   0          12s<br/>nginx-8586cf59-zkl8j   1/1       Running   0          12s</pre>
<p>Let's check our Kubernetes dashboard:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-480 image-border" src="assets/8a069c1f-b7cb-4441-8f32-90576dcdab69.png" style="width:44.17em;height:23.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Workloads in Kubernetes dashboard</div>
<p>Hurray! The deployment is created and as a result two pods are scheduled and created.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Advanced settings in kubeconfig</em> in <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml" target="_blank">Chapter 8</a>, <em>Advanced Cluster Administration</em></li>
<li><em>Setting resources in nodes</em> in Chapter 8, <em>Advanced Cluster Administration</em></li>
<li><em>Playing with the Web UI</em> in <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml" target="_blank">Chapter 8</a>, <em>Advanced Cluster Administration</em></li>
<li><em>Setting up a DNS server in Kubernetes Cluster</em> in <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml" target="_blank">Chapter 8</a>, <em>Advanced Cluster Administration</em></li>
<li><em>Authentication and authorization</em> in <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml" target="_blank">Chapter 8</a>, <em>Advanced Cluster Administration</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring CloudProvider on GKE</h1>
                </header>
            
            <article>
                
<p>GKE works as a native Kubernetes Cloud Provider, which integrates with resources in Kubernetes seamlessly and allows you to provision on demand, for example, VPC routes for the network, <strong>Persistent Disk</strong> (<strong>PD</strong>) for StorageClass, L4 load balancer for Service, and L4 load balancer for ingress.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>By default, when you create the network and launch a Kubernetes cluster in Google Cloud Platform with proper routes, containers can already talk to each other without an explicit network being set up.Beyond the resources listed previously, we don't need to set any settings explicitly in most cases. GKE will just work.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how convenient GKE offers about storage, network and more.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">StorageClass</h1>
                </header>
            
            <article>
                
<p>In <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml" target="_blank">Chapter 2</a>, <em><span>Walking through Kubernetes Concepts</span></em>, we learned how to declare <kbd>PersistentVolume</kbd> and <kbd>PersistentVolumeClaim</kbd>. With dynamic provisioning, you can define a set of <kbd>StorageClass</kbd> with different physical storage backends and use them in <kbd>PersistentVolume</kbd> or <kbd>PersistentVolumeClaim</kbd>. Let's see how it works.</p>
<p>To check the current default <kbd>StorageClass</kbd>, use <kbd>kubectl get storageclasses</kbd> command:</p>
<pre># kubectl get storageclasses<br/>NAME                 PROVISIONER            AGE<br/>standard (default)   kubernetes.io/gce-pd   1h</pre>
<p>We can see we have a default storage class named standard and its provisioner is GCE PD.</p>
<p>Let's create a <kbd>PersistentVolumeClaim</kbd> request and use the standard <kbd>StorageClass</kbd> as the backend:</p>
<pre># cat gke-pvc.yaml<br/>apiVersion: v1<br/>kind: PersistentVolumeClaim<br/>metadata:<br/>    name: pvc-example-pv<br/>spec:<br/>  storageClassName: standard<br/>  accessModes:<br/>    - ReadWriteOnce<br/>  resources:<br/>    requests:<br/>      storage: 10Gi<br/><br/>// create resources<br/># kubectl create -f gke-pvc.yaml<br/>persistentvolumeclaim "pvc-example-pv" created</pre>
<p><kbd>storageClassName</kbd> is the place to put the name of the <kbd>StorageClass</kbd>. If you put in something that doesn't exist, PVC will not be created, since there is no proper mapped <kbd>StorageClass</kbd> to use:</p>
<pre>// check pvc status<br/># kubectl get pvc<br/>NAME              STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE<br/>pvc-example-pv    Bound     pvc-1491b08e-1cfc-11e8-8589-42010a800360   10Gi       RWO            standard       12m<br/><br/>// describe the details of created PVC<br/># kubectl describe pvc pvc-example-pv<br/>Name:          pvc-example-pv<br/>Namespace:     default<br/>StorageClass:  standard<br/>Status:        Bound<br/>Volume:        pvc-1491b08e-1cfc-11e8-8589-42010a800360<br/>Labels:        &lt;none&gt;<br/>Annotations:   pv.kubernetes.io/bind-completed=yes<br/>               pv.kubernetes.io/bound-by-controller=yes<br/>               volume.beta.kubernetes.io/storage-provisioner=kubernetes.io/gce-pd<br/>Finalizers:    []<br/>Capacity:      10Gi<br/>Access Modes:  RWO<br/>Events:<br/>  Type    Reason                 Age   From                         Message<br/>  ----    ------                 ----  ----                         -------<br/>  Normal  ProvisioningSucceeded  12m   persistentvolume-controller  Successfully provisioned volume pvc-1491b08e-1cfc-11e8-8589-42010a800360 using kubernetes.io/gce-pd</pre>
<p>We can see volume <kbd>pvc-1491b08e-1cfc-11e8-8589-42010a800360</kbd> has been created and bounded. If we list GCP disks, we'll find there was a Persistent Disk created; the suffix of the disk name indicates the volume name in Kubernetes. That's the magic of dynamic volume provisioning:</p>
<pre># gcloud compute disks list<br/>NAME                                                             ZONE           SIZE_GB  TYPE         STATUS<br/>gke-my-k8s-cluster-5ef-pvc-1491b08e-1cfc-11e8-8589-42010a800360  us-central1-a  10       pd-standard  READY</pre>
<div class="packt_tip">
<p>Besides the default <kbd>StorageClass</kbd>, you can also create your own. Recap this in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml" target="_blank">Chapter 2</a>, <span><em>Walking through Kubernetes Concepts</em>.</span></p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Service (LoadBalancer)</h1>
                </header>
            
            <article>
                
<p>A <kbd>LoadBalancer</kbd> service type only works in the cloud environment that supports external load balancers. This allows outside traffic to be routed into target Pods. In GCP, a TCP load balancer will be created by a <kbd>LoadBalancer</kbd> service type:</p>
<ol>
<li>The firewall rules for allowing traffic between the load balancer and nodes will be created automatically:</li>
</ol>
<pre style="padding-left: 90px">// leveraging LoadBalancer service<br/># cat gke-service.yaml<br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: nginx<br/>spec:<br/>  replicas: 1<br/>  selector:<br/>    matchLabels:<br/>      run: nginx<br/>  template:<br/>    metadata:<br/>      labels:<br/>        run: nginx<br/>    spec:<br/>      containers:<br/>        - image: nginx<br/>          name: nginx<br/>          ports:<br/>            - containerPort: 80<br/>---<br/>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: nginx<br/>spec:<br/>  ports:<br/>    - port: 80<br/>      targetPort: 80<br/>  type: LoadBalancer<br/>  selector:<br/>    run: nginx<br/><br/>// create resources<br/># kubectl create -f gke-service.yaml<br/>deployment "nginx" created<br/>service "nginx" created</pre>
<ol start="2">
<li>Let's check the service. The <kbd>EXTERNAL-IP</kbd> will show <kbd>&lt;pending&gt;</kbd> if the load balancer is still provisioning. Wait a while and the load balancer IP will present itself eventually:</li>
</ol>
<pre style="padding-left: 90px"># kubectl get svc nginx<br/>NAME      TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)        AGE<br/>nginx     LoadBalancer   10.35.250.183   35.225.223.151   80:30383/TCP   11m</pre>
<ol start="3">
<li>Let's curl <kbd>$EXTERNAL-IP:80</kbd>, to see if it works properly:</li>
</ol>
<pre style="padding-left: 90px"># curl -I 35.225.223.151<br/>HTTP/1.1 200 OK<br/>Server: nginx/1.13.9<br/>Date: Thu, 01 Mar 2018 03:57:05 GMT<br/>Content-Type: text/html<br/>Content-Length: 612<br/>Last-Modified: Tue, 20 Feb 2018 12:21:20 GMT<br/>Connection: keep-alive<br/>ETag: "5a8c12c0-264"<br/>Accept-Ranges: bytes</pre>
<ol start="4">
<li>If we check the forwarding rules in GCP, we can find a rule that defines how the traffic goes from external IP to the target pool:</li>
</ol>
<pre style="padding-left: 90px"># gcloud compute forwarding-rules list<br/>NAME                              REGION       IP_ADDRESS      IP_PROTOCOL  TARGET<br/>ae1f2ad0c1d0211e8858942010a80036  us-central1  35.225.223.151  TCP          us-central1/targetPools/ae1f2ad0c1d0211e8858942010a80036</pre>
<ol start="5">
<li>A target pool is a set of instances that receive the traffic from forwarding rules. We could inspect the target pool by using the gcloud command as well:</li>
</ol>
<pre style="padding-left: 90px">// list target pools<br/># gcloud compute target-pools list<br/>NAME                              REGION       SESSION_AFFINITY  BACKUP  HEALTH_CHECKS<br/>ae1f2ad0c1d0211e8858942010a80036  us-central1  NONE                      k8s-1a4c86537c370d21-node<br/><br/>// check target pools info, replace $GCP_REGION as your default region.<br/># gcloud compute target-pools describe ae1f2ad0c1d0211e8858942010a80036 --region=$GCP_REGION<br/>creationTimestamp: '2018-02-28T19:45:46.052-08:00'<br/>description: '{"kubernetes.io/service-name":"default/nginx"}'<br/>healthChecks:<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/global/httpHealthChecks/k8s-1a4c86537c370d21-node<br/>id: '3515096241941432709'<br/>instances:<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/zones/us-central1-a/instances/gke-my-k8s-cluster-default-pool-36121894-71wg<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/zones/us-central1-a/instances/gke-my-k8s-cluster-default-pool-36121894-04rv<br/>- https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/zones/us-central1-a/instances/gke-my-k8s-cluster-default-pool-36121894-3mxm<br/>kind: compute#targetPool<br/>name: ae1f2ad0c1d0211e8858942010a80036<br/>region: https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/us-central1<br/>selfLink: https://www.googleapis.com/compute/v1/projects/kubernetes-cookbook/regions/us-central1/targetPools/ae1f2ad0c1d0211e8858942010a80036<br/>sessionAffinity: NONE</pre>
<p>We can see there are three nodes inside the pool. Those are the same three nodes in our Kubernetes cluster. Load balancer will dispatch the traffic to a node based on a hash of the source/definition IP and port. A service with <kbd>LoadBalancer</kbd> type looks handy; however, it can't do path-based routing. It's time for ingress to come into play. I<span>ngress </span>supports virtual hosts, path-based routing, and TLS termination, which is a more flexible approach to your web services.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ingress </h1>
                </header>
            
            <article>
                
<p>In <a href="669edaf0-c274-48fa-81d8-61150fa36df5.xhtml" target="_blank">Chapter 5</a>, <em><span>Building Continuous Delivery Pipelines</span></em>, we learned about the concept of <span>ingress </span>, and when and how to use it. I<span>ngress </span> defines a set of rules allowing the inbound connection to access Kubernetes cluster services. It routes the traffic into cluster at L7, and the controller brings the traffic to the nodes. When GCP is the cloud provider, a L7 load balancer will be created if an <span>ingress </span>is created, as well as related firewall rules, health checks, backend services, forwarding rules, and a URL map. A URL map in GCP is a mechanism that contains a set of rules and forwards requests to the corresponding backend services.</p>
<p>In this recipe, we'll reuse the examples from <a href="669edaf0-c274-48fa-81d8-61150fa36df5.xhtml" target="_blank">Chapter 5</a>, <em><span>Building Continuous Delivery Pipelines</span></em>, <kbd>Nodeport-deployment.yaml</kbd> and <kbd>echoserver.yaml</kbd>. Next is an illustration of how these two services work from <a href="669edaf0-c274-48fa-81d8-61150fa36df5.xhtml" target="_blank">Chapter 5</a>, <em><span>Building Continuous Delivery Pipelines</span></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-481 image-border" src="assets/9396e211-b955-4cd9-bb43-1154a5cc13ae.jpg" style="width:21.50em;height:18.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>I</span><span>ngress </span>illustration</div>
<p>We will create an i<span>ngress </span>for nginx and echoserver, that routes to different services. When the traffic comes in, the pod ingress controller will decide with service to route to.</p>
<p>Here is an example for i<span>ngress </span>. Please note that you might want to add the host name inside the rules section if you want the underlying services to always be visited from a certain host name:</p>
<pre># cat INGRESS.yaml<br/>apiVersion: extensions/v1beta1<br/>kind: INGRESS<br/>metadata:<br/>  name: my-INGRESS<br/>  annotations:<br/>    INGRESS.kubernetes.io/rewrite-target: /<br/>spec:<br/>  rules:<br/>    - http:<br/>        paths:<br/>          - path: /<br/>            # default backend<br/>            backend:<br/>              serviceName: nodeport-svc<br/>              servicePort: 8080<br/>          - path: /nginx<br/>            # nginx service<br/>            backend:<br/>              serviceName: nodeport-svc<br/>              servicePort: 8080<br/>          - path: /echoserver<br/>            # echoserver service<br/>            backend:<br/>              serviceName: echoserver-svc<br/>              servicePort: 8080<br/><br/>// create nodeport-svc (nginx) service<br/># kubectl create -f nodeport-deployment.yaml<br/>deployment "nodeport-deploy" created<br/>service "nodeport-svc" created<br/><br/>// create echoserver-svc (echoserver) service<br/># kubectl create -f echoserver.yaml<br/>deployment "echoserver-deploy" created<br/>service "echoserver-svc" created<br/><br/>// create INGRESS<br/># kubectl create -f INGRESS.yaml<br/>INGRESS "my-INGRESS" created</pre>
<div class="packt_infobox">
<p>Please double-check that the underlying service is configured as a <kbd>NodePort</kbd> type. Otherwise you might encounter errors such as <kbd>googleapi: Error 400: Invalid value for field 'namedPorts[1].port': '0'. Must be greater than or equal to 1, invalid error </kbd>from <kbd>loadbalancer-controller</kbd>.</p>
</div>
<p>After a few minutes, the L7 load balancer will be created and you'll be able to see it from the GCP console or by using the gcloud command. Let's use <kbd>kubectl</kbd> to check if the backend service in INGRESS is healthy:</p>
<pre>// kubectl describe INGRESS $INGRESS_name<br/># kubectl describe INGRESS my-INGRESS<br/><br/>curl Name:             my-INGRESS<br/>Namespace:        default<br/>Address:          35.190.46.137<br/>Default backend:  default-http-backend:80 (10.32.2.3:8080)<br/>Rules:<br/>  Host  Path  Backends<br/>  ----  ----  --------<br/>  *<br/>        /             nodeport-svc:8080 (&lt;none&gt;)<br/>        /nginx        nodeport-svc:8080 (&lt;none&gt;)<br/>        /echoserver   echoserver-svc:8080 (&lt;none&gt;)<br/>Annotations:<br/>  backends:         {"k8s-be-31513--91cf30ccf285becb":"HEALTHY","k8s-be-31780--91cf30ccf285becb":"HEALTHY","k8s-be-32691--91cf30ccf285becb":"HEALTHY"}<br/>  forwarding-rule:  k8s-fw-default-my-INGRESS--91cf30ccf285becb<br/>  rewrite-target:   /<br/>  target-proxy:     k8s-tp-default-my-INGRESS--91cf30ccf285becb<br/>  url-map:          k8s-um-default-my-INGRESS--91cf30ccf285becb<br/>Events:<br/>  Type    Reason   Age               From                     Message<br/>  ----    ------   ----              ----                     -------<br/>  Normal  Service  2m (x11 over 1h)  loadbalancer-controller  no user specified default backend, using system default</pre>
<p>We can see the three backends are healthy and the related forwarding rules, target proxy, and URL map have been all created. We can get a comprehensive view from the GCP console by visiting discovery and load balancing in GKE or the Load balancing tab in network services:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-482 image-border" src="assets/c1509f93-dc69-4e09-b6c6-0c80c63776cc.png" style="width:45.92em;height:36.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Discovery and Load balancing</span></div>
<p>The backend is illustrated here:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-484 image-border" src="assets/69386610-e730-4510-94ff-cda84aa27e43.png" style="width:62.75em;height:32.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Backend services</span></div>
<p>From time to time, your i<span>ngress </span>resource might encounter updates. When you redeploy it, there is no guarantee that GCP will allocate the same IP address to your load balancer. This might introduce a problem when the IP address is associated with a DNS name. The target IP address will need to be updated every time the IP is changed. This could be resolved by a static external IP address plus <kbd>kubernetes.io/INGRESS.global-static-ip-name</kbd> annotation:</p>
<pre>// allocate static IP as my-external-ip<br/># gcloud compute addresses create my-external-ip –global<br/><br/><br/>// check external-ip<br/># gcloud compute addresses list<br/>NAME            REGION  ADDRESS        STATUS<br/>my-external-ip          130.211.37.61  RESERVED<br/>After external IP is prepared, we could start launching our INGRESS now.<br/># cat INGRESS-static-ip.yaml<br/>apiVersion: extensions/v1beta1<br/>kind: INGRESS<br/>metadata:<br/>  name: my-INGRESS-static-ip<br/>  annotations:<br/>    INGRESS.kubernetes.io/rewrite-target: /<br/>    kubernetes.io/INGRESS.global-static-ip-name: my-external-ip<br/>spec:<br/>  rules:<br/>    - http:<br/>        paths:<br/>          - path: /<br/>            # default backend<br/>            backend:<br/>              serviceName: nodeport-svc<br/>              servicePort: 8080<br/>          - path: /nginx<br/>            # nginx service<br/>            backend:<br/>              serviceName: nodeport-svc<br/>              servicePort: 8080<br/>          - path: /echoserver<br/>            # echoserver service<br/>            backend:<br/>              serviceName: echoserver-svc<br/>              servicePort: 8080<br/><br/><br/># kubectl create -f INGRESS-static-ip.yaml<br/>INGRESS "my-INGRESS-stati-ip" created</pre>
<p>Let's describe <kbd>my-INGRESS</kbd> and see if it binds properly with the external IP we created :</p>
<pre># kubectl describe INGRESS my-INGRESS<br/>Name:             my-INGRESS<br/>Namespace:        default<br/>Address:          130.211.37.61<br/>Default backend:  default-http-backend:80 (10.32.2.3:8080)<br/>Rules:<br/>  Host  Path  Backends<br/>  ----  ----  --------<br/>  *        /             nodeport-svc:8080 (&lt;none&gt;)<br/>        /nginx        nodeport-svc:8080 (&lt;none&gt;)        /echoserver   echoserver-svc:8080 (&lt;none&gt;)Annotations:<br/>  backends:         {"k8s-be-31108--91cf30ccf285becb":"HEALTHY","k8s-be-31250--91cf30ccf285becb":"HEALTHY","k8s-be-32691--91cf30ccf285becb":"HEALTHY"}  forwarding-rule:  k8s-fw-default-my-INGRESS--91cf30ccf285becb  rewrite-target:   /  target-proxy:     k8s-tp-default-my-INGRESS--91cf30ccf285becb  url-map:          k8s-um-default-my-INGRESS--91cf30ccf285becbEvents:  Type    Reason   Age               From                     Message  ----    ------   ----              ----                     -------  Normal  ADD      27m               loadbalancer-controller  default/my-INGRESS  Normal  CREATE   25m               loadbalancer-controller  ip: 130.211.37.61<br/>  Normal  Service  4m (x6 over 25m)  loadbalancer-controller  no user specified default backend, using system default</pre>
<p>We're all set. <kbd>Nginx</kbd> and <kbd>echoserver</kbd> can be visited via the external static IP <kbd>130.211.37.61</kbd>, and we're able to associate a DNS name with it by using the cloud DNS service in GCP.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>In Kubernetes v.1.9, the Kubernetes cloud controller manager was promoted to alpha. Cloud controller manager aims to make the cloud provider release feature support via its own release cycles, which could be independent from the Kubernetes release cycle. Then it could be independent with Kubernetes core release cycle. It provides common interfaces that each cloud provider can implement, which decoupling with Kubernetes Core logic. In the near future, we'll see more comprehensive support from different cloud providers!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Working with services</em> in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml" target="_blank">Chapter 2</a>, <em>Walking through Kubernetes Concepts</em></li>
<li><em>Working with volumes</em> in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml" target="_blank">Chapter 2</a>, <em>Walking through Kubernetes Concepts</em></li>
<li><em>Forwarding container ports</em> in <a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml" target="_blank">Chapter 3</a>, <em>Playing with Containers</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managing Kubernetes clusters on GKE</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">Google Kubernetes Engines offers us the seamless experience of running Kubernetes; it also makes Kubernetes administration so easy. Depending on the expected peak time, we might want to scale the Kubernetes nodes out or in. Alternatively, we could use Autoscaler to do auto-scaling for the nodes. Kubernetes is an evolving platform. The release pace is fast. We might want to upgrade the cluster version from time to time, which is very easy to do. We could also use the Autoupgrade feature to upgrade the cluster by enabling automatically schedule feature in GKE. Let's see how to do it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Before setting up the administration features that GCP offers, we'll have to have a cluster up and running. We'll reuse the cluster we created in the Playing with the Google Kubernetes Engine recipe in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>In this recipe, we'll introduce how to manage the number of nodes based on usage and requirements. Also, we'll learn how to deal with cluster upgrades. Finally, we'll see how to provision a multi-zone cluster in GKE, in order to prevent a physical zone outage.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Node pool</h1>
                </header>
            
            <article>
                
<p>A node pool is a set of instances in GCP that share the same configuration. When we launch a cluster from the <kbd>gcloud</kbd> command, we pass <kbd>--num-node=3</kbd> and the rest of the arguments. Then three instances will be launched inside the same pool, sharing the same configuration, using the same method:</p>
<pre class="mce-root"># gcloud compute instance-groups list NAME LOCATION SCOPE NETWORK MANAGED INSTANCES gke-my-k8s-cluster-default-pool-36121894-grp us-central1-a zone k8s-network Yes 3 </pre>
<p class="NormalPACKT">Assume there is an expected heavy peak time for your service. As a Kubernetes administrator, you might want to resize your node pool inside the cluster.</p>
<pre># gcloud container clusters resize my-k8s-cluster --size 5 --zone us-central1-a --node-pool default-pool<br/>Pool [default-pool] for [my-k8s-cluster] will be resized to 5.<br/>Do you want to continue (Y/n)?  y<br/>Resizing my-k8s-cluster...done.<br/>Updated [https://container.googleapis.com/v1/projects/kubernetes-cookbook/zones/us-central1-a/clusters/my-k8s-cluster].<br/># kubectl get nodes<br/>NAME                                               STATUS    ROLES     AGE       VERSION<br/>gke-my-k8s-cluster-default-pool-36121894-04rv      Ready     &lt;none&gt;    6h        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-36121894-71wg      Ready     &lt;none&gt;    6h        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-36121894-8km3      Ready     &lt;none&gt;    39s       v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-36121894-9j9p      Ready     &lt;none&gt;    31m       v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-36121894-9jmv      Ready     &lt;none&gt;    36s       v1.9.2-gke.1</pre>
<p>The resize command can help you scale out and in. If the node count after resizing is less than before, the scheduler will migrate the pods to run on available nodes.</p>
<p>You can set the compute resource boundary for each container in the spec. You set requests and limits to a pod container. Assume we have a super nginx which requires 1024 MB memory:</p>
<pre># cat super-nginx.yaml<br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: super-nginx<br/>  labels:<br/>    app: nginx<br/>spec:<br/>  replicas: 1<br/>  selector:<br/>    matchLabels:<br/>      app: nginx<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: nginx<br/>    spec:<br/>      containers:<br/>      - name: nginx<br/>        image: nginx<br/>        resources:<br/>          requests:<br/>            memory: 1024Mi <br/><br/>// create super nginx deployment<br/># kubectl create -f super-nginx.yaml<br/>deployment "super-nginx" created<br/><br/># kubectl get pods<br/>NAME                           READY     STATUS    RESTARTS   AGE<br/>super-nginx-df79db98-5vfmv      0/1       Pending   0          10s<br/># kubectl describe po super-nginx-779494d88f-74xjp<br/>Name:           super-nginx-df79db98-5vfmv<br/>Namespace:      default<br/>Node:           &lt;none&gt;<br/>Labels:         app=nginx<br/>                pod-template-hash=89358654<br/>Annotations:    kubernetes.io/limit-ranger=LimitRanger plugin set: cpu request for container nginx<br/>Status:         PendingIP:<br/>Controlled By:  ReplicaSet/super-nginx-df79db98<br/>...<br/>Events:<br/>  Type     Reason            Age                From               Message<br/>  ----     ------            ----               ----               -------<br/>  Warning  FailedScheduling  11s (x5 over 18s)  default-scheduler  0/5 nodes are available: 5 Insufficient memory.</pre>
<p>The node size we created is <kbd>f1-miro</kbd>, which only has 0.6 GM memory per node. It means the scheduler will never find a node with sufficient memory to run <kbd>super-nginx</kbd>. In this case, we can add more nodes with higher memory to the cluster by creating another node pool. We'll use <kbd>g1-small</kbd> as an example, which contains 1.7 GB memory:</p>
<pre>// create a node pool named larger-mem-pool with n1-standard-1 instance type<br/># gcloud container node-pools create larger-mem-pool --cluster my-k8s-cluster --machine-type n1-standard-1 --num-nodes 2 --tags private --zone us-central1-a --scopes=storage-rw,compute-ro<br/>...<br/>Creating node pool larger-mem-pool...done.<br/>Created [https://container.googleapis.com/v1/projects/kubernetes-cookbook/zones/us-central1-a/clusters/my-k8s-cluster/nodePools/larger-mem-pool].<br/>NAME             MACHINE_TYPE   DISK_SIZE_GB  NODE_VERSION<br/>larger-mem-pool  n1-standard-1  100           1.9.2-gke.1<br/><br/>// check node pools<br/># gcloud container node-pools list --cluster my-k8s-cluster --zone us-central1-a<br/>NAME             MACHINE_TYPE   DISK_SIZE_GB  NODE_VERSION<br/>default-pool     f1-micro       100           1.9.2-gke.1<br/>larger-mem-pool  n1-standard-1  100           1.9.2-gke.1<br/><br/>// check current nodes<br/># kubectl get nodes<br/>NAME                                               STATUS    ROLES     AGE       VERSION<br/>gke-my-k8s-cluster-default-pool-36121894-04rv      Ready     &lt;none&gt;    7h        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-36121894-71wg      Ready     &lt;none&gt;    7h        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-36121894-8km3      Ready     &lt;none&gt;    9m        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-36121894-9j9p      Ready     &lt;none&gt;    40m       v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-36121894-9jmv      Ready     &lt;none&gt;    9m        v1.9.2-gke.1<br/>gke-my-k8s-cluster-larger-mem-pool-a51c8da3-f1tb   Ready     &lt;none&gt;    1m        v1.9.2-gke.1<br/>gke-my-k8s-cluster-larger-mem-pool-a51c8da3-scw1   Ready     &lt;none&gt;    1m        v1.9.2-gke.1</pre>
<p class="NormalPACKT">Looks like we have two more powerful nodes. Let's see the status of our super nginx:</p>
<pre># kubectl get pods<br/>NAME                         READY     STATUS    RESTARTS   AGE<br/>super-nginx-df79db98-5vfmv   1/1       Running   0          23m</pre>
<p>It's running! Kubernetes scheduler <span>will always try to find</span><span> sufficient resources to schedule pods. In this case, there are two new nodes added to the cluster that can fulfill the resource requirement, so the pod is scheduled and run:</span></p>
<pre>// check the event of super nginx<br/># kubectl describe pods super-nginx-df79db98-5vfmv<br/>...<br/>Events:<br/>  Warning  FailedScheduling       3m (x7 over 4m)     default-scheduler                                          0/5 nodes are available: 5 Insufficient memory.<br/>  Normal   Scheduled              1m                  default-scheduler                                          Successfully assigned super-nginx-df79db98-5vfmv to gke-my-k8s-cluster-larger-mem-pool-a51c8da3-scw1<br/>  Normal   SuccessfulMountVolume  1m                  kubelet, gke-my-k8s-cluster-larger-mem-pool-a51c8da3-scw1  MountVolume.SetUp succeeded for volume "default-token-bk8p2"<br/>  Normal   Pulling                1m                  kubelet, gke-my-k8s-cluster-larger-mem-pool-a51c8da3-scw1  pulling image "nginx"<br/>  Normal   Pulled                 1m                  kubelet, gke-my-k8s-cluster-larger-mem-pool-a51c8da3-scw1  Successfully pulled image "nginx"<br/>  Normal   Created                1m                  kubelet, gke-my-k8s-cluster-larger-mem-pool-a51c8da3-scw1  Created container<br/>  Normal   Started                1m                  kubelet, gke-my-k8s-cluster-larger-mem-pool-a51c8da3-scw1  Started container</pre>
<p>From the events of the pod, we know what path it ran through. Originally, it couldn't find any nodes with sufficient resources and eventually it's scheduled to the new node named <kbd>gke-my-k8s-cluster-larger-mem-pool-a51c8da3-scw1</kbd>.</p>
<div class="packt_infobox">
<div>
<p>For making the user preference on scheduling pods on certain nodes, <kbd>nodeSelector</kbd> was introduced. You could either use built-in node labels, such as <kbd>beta.kubernetes.io/instance-type: n1-standard-1</kbd> in pod spec, or use customized labels to achieve it. For more information, please refer to <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node">https://kubernetes.io/docs/concepts/configuration/assign-pod-node</a>.</p>
</div>
</div>
<p>Kubernetes also supports <strong>cluster autoscaler</strong>, which automatically resizes your cluster based on capacity if all nodes have insufficient resources to run the requested pods. To do that, we add <kbd>–enable-autoscaling</kbd> and specify the maximum and minimum node count when we create the new node pool:</p>
<pre># cloud container node-pools create larger-mem-pool --cluster my-k8s-cluster --machine-type n1-standard-1 --tags private --zone us-central1-a --scopes=storage-rw,compute-ro --enable-autoscaling --min-nodes 1 --max-nodes 5<br/>...<br/>Creating node pool larger-mem-pool...done.<br/>Created [https://container.googleapis.com/v1/projects/kubernetes-cookbook/zones/us-central1-a/clusters/my-k8s-cluster/nodePools/larger-mem-pool].<br/>NAME             MACHINE_TYPE   DISK_SIZE_GB  NODE_VERSION<br/>larger-mem-pool  n1-standard-1  100           1.9.2-gke.1</pre>
<p>After a few minutes, we can see there is a new node inside our cluster:</p>
<pre>#  kubectl get nodes<br/>NAME                                               STATUS    ROLES     AGE       VERSION<br/>gke-my-k8s-cluster-default-pool-36121894-04rv      Ready     &lt;none&gt;    8h        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-36121894-71wg      Ready     &lt;none&gt;    8h        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-36121894-8km3      Ready     &lt;none&gt;    1h        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-36121894-9j9p      Ready     &lt;none&gt;    1h        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-36121894-9jmv      Ready     &lt;none&gt;    1h        v1.9.2-gke.1<br/>gke-my-k8s-cluster-larger-mem-pool-a51c8da3-s6s6   Ready     &lt;none&gt;    15m       v1.9.2-gke.1</pre>
<p>Now, let's change the replica of our super-nginx from 1 to 4 by using <kbd>kubectl</kbd> edit or creating a new deployment:</p>
<pre>// check current pods<br/># kubectl get pods<br/>NAME                         READY     STATUS    RESTARTS   AGE<br/>super-nginx-df79db98-5q9mj   0/1       Pending   0          3m<br/>super-nginx-df79db98-72fcz   1/1       Running   0          3m<br/>super-nginx-df79db98-78lbr   0/1       Pending   0          3m<br/>super-nginx-df79db98-fngp2   1/1       Running   0          3m</pre>
<p>We find there are two pods with a pending status:</p>
<pre>// check nodes status<br/># kubectl get nodes<br/>NAME                                               STATUS     ROLES     AGE       VERSION<br/>gke-my-k8s-cluster-default-pool-36121894-04rv      Ready   &lt;none&gt;    8h        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-36121894-71wg      Ready      &lt;none&gt;    8h        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-36121894-9j9p      Ready      &lt;none&gt;    2h        v1.9.2-gke.1<br/>gke-my-k8s-cluster-larger-mem-pool-a51c8da3-d766   Ready      &lt;none&gt;    4m        v1.9.2-gke.1<br/>gke-my-k8s-cluster-larger-mem-pool-a51c8da3-gtsn   Ready      &lt;none&gt;    3m        v1.9.2-gke.1<br/>gke-my-k8s-cluster-larger-mem-pool-a51c8da3-s6s6   Ready      &lt;none&gt;    25m       v1.9.2-gke.1</pre>
<p>After a few minutes, we see that there are new members in our larger mem pool, and all our pods get to run:</p>
<pre>// check pods status<br/># kubectl get pods<br/>NAME                         READY     STATUS    RESTARTS   AGE<br/>super-nginx-df79db98-5q9mj   1/1       Running   0          3m<br/>super-nginx-df79db98-72fcz   1/1       Running   0          3m<br/>super-nginx-df79db98-78lbr   1/1       Running   0          3m<br/>super-nginx-df79db98-fngp2   1/1       Running   0          3m</pre>
<p>Cluster autoscaler comes in handy and is cost-effective. When the nodes are over-provisioned, the additional node in the node pool will be terminated automatically.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multi-zone and regional clusters</h1>
                </header>
            
            <article>
                
<p>Our <kbd>my-k8s-cluster</kbd> is currently deployed in the <kbd>us-central1-a</kbd> zone. While a zone is a physically isolated location in a region, it may suffer an outage. Google Kubernetes Engine supports multi-zone and regional deployment. Multi-zone clusters create a single master in a zone and provision nodes in multiple zones; on the other hand, a regional cluster creates multiple masters across three zones and provisions nodes in multiple zones.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multi-zone clusters</h1>
                </header>
            
            <article>
                
<p>To enable multi-zone cluster, add -<kbd>-additional-zones $zone2, $zone3, …</kbd> into the command when you create the cluster.</p>
<div class="packt_tip">
<div>
<p>Just like AWS, GCP has service quota limits as well. You could use <kbd>gcloud compute project-info describe –project $PROJECT_NAME</kbd> to check the quota and request an increase from the GCP console if needed.</p>
</div>
</div>
<p class="mce-root">Let's launch a two-nodes cluster per zone first:</p>
<pre class="mce-root">// launch a multi-zone cluster with 2 nodes per zone.<br/># gcloud container clusters create my-k8s-cluster --cluster-version 1.9.2-gke.1 --machine-type f1-micro --num-nodes 2 --network k8s-network --tags private --scopes=storage-rw,compute-ro --zone us-central1-a --additional-zones us-central1-b,us-central1-c<br/>Creating cluster my-k8s-cluster...done.<br/>Created [https://container.googleapis.com/v1/projects/kubernetes-cookbook/zones/us-central1-a/clusters/my-k8s-cluster].<br/>To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-central1-a/my-k8s-cluster?project=kubernetes-cookbook<br/>kubeconfig entry generated for my-k8s-cluster.<br/>NAME            LOCATION       MASTER_VERSION  MASTER_IP      MACHINE_TYPE  NODE_VERSION  NUM_NODES  STATUS<br/>my-k8s-cluster  us-central1-a  1.9.2-gke.1     35.226.67.179  f1-micro      1.9.2-gke.1   6          RUNNING</pre>
<p>We find we have six nodes now:</p>
<pre># kubectl get nodes<br/>NAME                                            STATUS    ROLES     AGE       VERSION<br/>gke-my-k8s-cluster-default-pool-068d31a2-q909   Ready     &lt;none&gt;    8m        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-068d31a2-rqzw   Ready     &lt;none&gt;    8m        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-64a6ead8-qf6z   Ready     &lt;none&gt;    8m        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-64a6ead8-x8cc   Ready     &lt;none&gt;    8m        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-798c4248-2r4p   Ready     &lt;none&gt;    8m        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-798c4248-skdn   Ready     &lt;none&gt;    8m        v1.9.2-gke.1</pre>
<p class="NormalPACKT">Let's check if the nodes are spread across the three zones we specified:</p>
<pre># gcloud compute instance-groups list NAME LOCATION SCOPE NETWORK MANAGED INSTANCES gke-my-k8s-cluster-default-pool-068d31a2-grp us-central1-a zone k8s-network Yes 2 gke-my-k8s-cluster-default-pool-64a6ead8-grp us-central1-c zone k8s-network Yes 2 gke-my-k8s-cluster-default-pool-798c4248-grp us-central1-b zone k8s-network Yes 2 </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regional clusters</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">Regional clusters are still in the beta phase. To use these, we'll have to enable the gcloud beta command. We can enable it via this command:</p>
<pre># export CLOUDSDK_CONTAINER_USE_V1_API_CLIENT=false # gcloud config set container/use_v1_api false <br/>Updated property [container/use_v1_api].</pre>
<p class="NormalPACKT">Then we should be able to use the <kbd>gcloud v1beta</kbd> command to launch the regional cluster:</p>
<pre># gcloud beta container clusters create my-k8s-cluster --cluster-version 1.9.2-gke.1 --machine-type f1-micro --num-nodes 2 --network k8s-network --tags private --scopes=storage-rw,compute-ro --region us-central1 <br/><br/>Creating cluster my-k8s-cluster...done. Created [https://container.googleapis.com/v1beta1/projects/kubernetes-cookbook/zones/us-central1/clusters/my-k8s-cluster]. To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-central1/my-k8s-cluster?project=kubernetes-cookbook <br/><br/>kubeconfig entry generated for my-k8s-cluster. NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS my-k8s-cluster us-central1 1.9.2-gke.1 35.225.71.127 f1-micro 1.9.2-gke.1 6 RUNNING</pre>
<p class="NormalPACKT">The command is quite similar to the one that creates a cluster, just with two differences: a <span class="KeyWordPACKT">beta</span> flag is added before the group name <span class="KeyWordPACKT">container</span> which indicates it's a <kbd>v1beta</kbd> command. The second difference is changing <kbd>--zone</kbd> to <kbd>--region</kbd>:</p>
<pre>// list instance groups<br/># gcloud compute instance-groups list<br/>NAME                                          LOCATION       SCOPE  NETWORK      MANAGED  INSTANCES<br/>gke-my-k8s-cluster-default-pool-074ab64e-grp  us-central1-a  zone   k8s-network  Yes      2<br/>gke-my-k8s-cluster-default-pool-11492dfc-grp  us-central1-c  zone   k8s-network  Yes      2<br/>gke-my-k8s-cluster-default-pool-f2c90100-grp  us-central1-b  zone   k8s-network  Yes      2</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cluster upgrades</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">Kubernetes is a fast-release project. GKE also keeps supporting new versions. It's not uncommon to have multiple minor version updates within a month. check the GKE console:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-485 image-border" src="assets/7474190c-b187-407f-aac8-f92c7e3c9ec3.png" style="width:49.33em;height:15.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Upgrade available information in the GCP console</div>
<div class="CDPAlignCenter CDPAlign">
<p>We see that an upgrade is available. <span class="packt_screen">1.9.3-gke.1</span> in the screenshot has just been released and our cluster is able to upgrade:</p>
<img class="alignnone size-full wp-image-486 image-border" src="assets/bd70a627-254a-4882-b45a-7f1c15f9b782.png" style="width:39.25em;height:19.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Upgrade available to 1.9.3-gke.0</div>
<div>
<p>We can upgrade the cluster via the GKE console, or using gcloud command. We'll use the single zone (<kbd>us-central1-a</kbd>) cluster to demonstrate how to upgrade in the next example. When upgrading the cluster, the master is always the first citizen to do the upgrade. The desired node version cannot be greater than the current master version.</p>
<pre># gcloud container clusters upgrade my-k8s-cluster --zone us-central1-a --cluster-version 1.9.3-gke.0 –master<br/>Master of cluster [my-k8s-cluster] will be upgraded from version<br/>[1.9.2-gke.1] to version [1.9.3-gke.0]. This operation is long-running<br/> and will block other operations on the cluster (including delete)<br/>until it has run to completion.<br/>Do you want to continue (Y/n)?  y<br/>Upgrading my-k8s-cluster...done.<br/>Updated [https://container.googleapis.com/v1/projects/kubernetes-cookbook/zones/us-central1-a/clusters/my-k8s-cluster].</pre>
<p>Let's check the master's version:</p>
<pre># kubectl version<br/>...<br/>Server Version: version.Info{Major:"1", Minor:"9+", GitVersion:"v1.9.3-gke.0", GitCommit:"a7b719f7d3463eb5431cf8a3caf5d485827b4210", GitTreeState:"clean", BuildDate:"2018-02-16T18:26:01Z", GoVersion:"go1.9.2b4", Compiler:"gc", Platform:"linux/amd64"}</pre>
<p>Looks good. The master has been upgraded to <kbd>v1.9.3-gke.0</kbd>, but our nodes didn't get upgrade yet:</p>
<pre># kubectl get nodes<br/>NAME                                            STATUS    ROLES     AGE       VERSION<br/>gke-my-k8s-cluster-default-pool-978ca614-3jxx   Ready     &lt;none&gt;    8m        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-978ca614-njrs   Ready     &lt;none&gt;    8m        v1.9.2-gke.1<br/>gke-my-k8s-cluster-default-pool-978ca614-xmlw   Ready     &lt;none&gt;    8m        v1.9.2-gke.1</pre>
<p>For the node upgrade, instead of upgrading them all at once, GKE performs rolling upgrade. It will first drain and deregister a node from the node pool, delete an old instance, and provision a new instance with the desired version, then add it back to the cluster:</p>
<pre>// perform node upgrades.<br/># gcloud container clusters upgrade my-k8s-cluster --zone us-central1-a --cluster-version 1.9.3-gke.0<br/>All nodes (3 nodes) of cluster [my-k8s-cluster] will be upgraded from<br/>version [1.9.2-gke.1] to version [1.9.3-gke.0]. This operation is<br/>long-running and will block other operations on the cluster (including<br/> delete) until it has run to completion.<br/>Do you want to continue (Y/n)?  y<br/>Upgrading my-k8s-cluster...done.<br/>Updated [https://container.googleapis.com/v1/projects/kubernetes-cookbook/zones/us-central1-a/clusters/my-k8s-cluster].</pre></div>
<div class="packt_infobox"><span>The node pool can be configured to auto-upgrade via the <kbd>--enable-autoupgrade</kbd> flag during cluster creation, or using the gcloud container <kbd>node-pools</kbd> update command to update existing node pools. For more information, please refer to <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-upgrades">https://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-upgrades</a>.</span></div>
<div>
<p>It will take more than 10 minutes. After that, all the nodes in the cluster are upgraded to <kbd>1.9.3-gke.0</kbd>.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Advanced settings in kubeconfig</em> in <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml" target="_blank">Chapter 8</a>, <em><span>Advanced Cluster Administration</span></em></li>
<li><em>Setting resources in nodes</em> in <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml" target="_blank">Chapter 8</a>, <em><span>Advanced Cluster Administration</span></em></li>
</ul>


            </article>

            
        </section>
    </body></html>