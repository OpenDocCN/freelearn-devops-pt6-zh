- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Provisioning Backing Services for Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we saw how to build application container images with
    ample security and without much operational overhead using **Tanzu Build Service**
    (**TBS**). These container images are the essential building blocks to run our
    cloud-native applications on container orchestration platforms such as **Kubernetes**.
    We can deploy those container images on a Kubernetes cluster and run our applications.
    However, in real life, things are not that straightforward. In the majority of
    cases, business applications depend on backing services such as databases, queues,
    caches, and others. Additionally, there is an increasing trend to also deploy
    such off-the-shelf backing services as containers on Kubernetes-like platforms
    for various good reasons.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will take a deep dive into **VMware Application Catalog**
    (**VAC**), which provides a secure, fast, and reliable way to use such open source
    backing services in a containerized environment. We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why VMware Application Catalog?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What VMware Application Catalog is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with VMware Application Catalog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common day-two activities with VMware Application Catalog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a lot of ground to cover. So, let’s get started exploring what business
    and technical challenges are addressed by VAC.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are some technical requirements that need to be fulfilled before we start
    using VAC. These requirements are covered later in this chapter, at the beginning
    of the *Getting started with VMware Application Catalog* section. However, you
    may not need them to understand the application and the details of this tool.
  prefs: []
  type: TYPE_NORMAL
- en: Why VMware Application Catalog?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following are the key areas where VAC addresses detailed challenges with
    its capabilities for delivering better developer productivity, security, and operational
    practices when it comes to providing a way to consume popular **open source software**
    (**OSS**) and deploy it as running containers.
  prefs: []
  type: TYPE_NORMAL
- en: Using the right tool for the right purpose with the flexibility of choice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed previously, most business applications depend on one or more
    backing services, and depending on the nature of the application, the need for
    such backing services can be different. We have seen that using a relational database
    as a backend data store has been the most common backing service for the past
    several years. But some modern cloud-native applications could perform better
    with other data stores such as NoSQL. Similarly, if an application needs a queue
    as a backing service, we can use either Kafka or RabbitMQ. But both Kafka and
    RabbitMQ have their own niche use cases where one might be a better option than
    the other depending on the application’s needs. Similarly, such options exist
    for tools such as caches, logging, **continuous integration/continuous deployment**
    (**CI/CD**) automation, and many other aspects of running cloud-native applications.
    For these use cases, there are strong and mature open source software solutions
    available today that are very popular and widely adopted. *Figure 4**.1* shows
    how OSS tools have become more popular than proprietary tools in the recent past:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The increasing trend of open source data store popularity](img/B18145_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – The increasing trend of open source data store popularity
  prefs: []
  type: TYPE_NORMAL
- en: Despite the vast choice and proven track record, providing the required freedom
    to adopt such OSS tools internally to application teams is often challenging.
    There are two possible reasons for this. Firstly, it takes a lot of operational
    overhead to get such tools approved for usage and create a secure supply chain
    of container images for these tools. And secondly, the fact that their respective
    container images are available on public container repositories means that they
    are not always trustworthy. Because of that, development teams suffer either because
    of a lack of choice or the loss of productivity results in wait times.
  prefs: []
  type: TYPE_NORMAL
- en: VAC addresses this challenge by providing a huge catalog of OSS tools that enterprises
    can select from. Once a custom catalog of OSS tools is prepared, the VAC service
    creates an automated supply chain to stream container images and Helm charts of
    those selected tools and delivers them to a targeted container registry that is
    deployed either internally or externally, such as **Google Container Registry**
    (**GCR**). Later, we would keep getting the newer versions of those OSS tools
    in the form of their newer container images and Helm charts. These can all be
    configured with minimal operational overhead and provide a lot of choices for
    application teams. Once we start getting the artifacts, container images, and
    Helm charts for our catalog items, we can expose that catalog for internal consumption.
    Then, the authorized internal members may use the catalog to provision those OSS
    tools on Kubernetes within minutes. Such flexibility of choice without the operational
    overhead to developers should encourage the usage of the right backing services
    for the right use case without affecting the productivity of their users.
  prefs: []
  type: TYPE_NORMAL
- en: Enhanced security and transparency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using OSS deployment on container platforms is very quick, easy, and becoming
    popular. All major public container repositories such as Docker Hub and GCR have
    the container images for all major OSS. We can simply provide the name of the
    container image and download it in seconds. However, despite several benefits
    of pulling container images from such public container repositories, almost all
    enterprises, with some level of mature security practices, would not allow this.
    The following are a few reasons for this:'
  prefs: []
  type: TYPE_NORMAL
- en: It is difficult to determine which container images on public repositories are
    hosted by legitimate sources. You may find several different container images
    for the same software hosted by different organizations or common users. In this
    case, it would be very difficult to create a whitelist of authenticated sources
    to pull the images hosted by them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if there is a way to find the authenticated sources whose container images
    can be used, it is very difficult to create a governance model around it to restrict
    the usage of the container images hosted by unknown sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also very difficult to get a **bill of material** (**BOM**) to know what
    such externally provided container images contain. Such third-party images often
    act as black boxes for enterprises to obtain the required confidence to allow
    using them in any environment. Auditing the environments running such black-box
    container images would be very difficult.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any reasonably mature organization has a standard set of **operating systems**
    (**OSes**) that they allow in their infrastructure. These OS requirements are
    often applicable to container images too. However, there are no controls and enough
    choices for selecting the OS when it comes to the container images provided by
    third parties. This could be a single significant reason that corporates disapprove
    of the use of the container images of OSS tools hosted on public container registries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are several audit and compliance requirements when it comes to information
    security standards for a security-first mindset organization. To achieve the required
    confidence to run an OSS container in the production environment of a corporate
    entity, there should be several details available, including **Common Vulnerabilities
    and Exposures** (**CVE**) scan reports, test results, anti-virus scans, and other
    details for all the workloads deployed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To meet these requirements, companies usually take control of curating their
    own OSS container image-building process. And when they add more OSS into their
    catalog of the in-house image curation process, they often later realize that
    such efforts are not very scalable, fast enough to fulfill the demand, efficient,
    or secure. Here are a few reasons for this: when there are new OSS tools required
    to be added to the internally managed catalog, it requires building a new automation
    pipeline, test cases, infrastructure capacity, and more:'
  prefs: []
  type: TYPE_NORMAL
- en: Such in-house and bespoke automation efforts are often understood and maintained
    by very few engineers. Out rotation of such key people in the team creates a vacuum
    that is difficult to fill at times, and that creates a knowledge gap.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the catalog becomes bigger, the maintenance effort increases exponentially.
    Because every newer version of each catalog item requires building a new corresponding
    container image for internal consumption. Because of such added overhead, the
    platform operations team may fall behind in keeping up with the latest patched
    versions of the OSS tools. Such delays in producing the latest patched container
    image of the tool increase security risks by allowing unpatched CVEs to be available
    for enterprise-wide consumption until the newer container image is ready.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of the amount of effort required to provision a new catalog item for
    internal consumption, there could be a potential pushback to adding new items.
    Such pushbacks could either reduce developer productivity as they spend time waiting
    or affect the business application’s quality by not using the right tool for the
    right use case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though the platform team agrees to add a new item to the catalog, it would
    take a long time before the actual consumers would get a production-certified
    container image that they can use. Such delays again waste the valuable productive
    time of an important workforce. Or the workforce finds workarounds by using externally
    available but insecure sources for such container images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The time of the people working on such internal automation efforts and the infrastructure
    capacity utilized for this reason could be better used for a more business outcome-driven
    endeavor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To address these challenges, VAC comes into the picture and helps with the
    following benefits to provide a secure solution to increase developer productivity
    and operational efficiency:'
  prefs: []
  type: TYPE_NORMAL
- en: VAC allows enterprises to use their own golden OS image layer for all their
    selected OSS container images. This is a significant benefit as the client organization
    can use their hardened OS layer with desired security configurations on their
    selection of OS flavor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VAC creates an automation pipeline for the creation and distribution of every
    catalog item’s container image and Helm chart (if applicable). Because of such
    automation, VAC can quickly supply newer patched versions of the catalog items
    to subscribers soon after the newer upstream version becomes available. Such a
    quick supply of the patched versions provides a good preventative security posture
    against hacking attacks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VAC supplies the following artifacts with all container images delivered to
    enhance consumers’ confidence and increase the transparency of the container images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asset specification detailing information about the content of the container
    image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Automation test case results for a test run executed before delivery
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: CVE scan report
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: CVE scan report in CVRF format
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Antivirus scan report
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: User documentation references
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: VAC pushes all the artifacts to a private container registry as provided by
    the clients, which creates a trustable source of all container images and Helm
    charts for internal consumption.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the CVRF?
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Common Vulnerability Reporting Framework** (**CVRF**) Version 1.1 was
    released in May 2012\. CVRF is an XML-based language that enables different stakeholders
    across different organizations to share critical security-related information
    in a single format, speeding up information exchange and digestion. Reference:
    [https://www.icasi.org/the-common-vulnerability-reporting-framework-cvrf-v1-1/](https://www.icasi.org/the-common-vulnerability-reporting-framework-cvrf-v1-1/).'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous points, we discussed most of the important benefits of VAC.
    Presently, VAC is only applicable to OSS tools. However, many large organizations
    do not use OSS tools without enterprise-grade support, especially in a production
    environment. It is important to understand that VAC, as a solution, only supports
    the secure supply chain of OSS container images and Helm charts using client-selected
    OS layers. But VAC does not support the underlying OSS tools as a part of the
    subscription. For example, if an organization requires PostgreSQL DB container
    images via VAC, then VAC will support the packaging and the timely distribution
    of the container images of PostgreSQL DB upstream versions. But VAC would not
    support the underlying PostgreSQL DB itself. Hence, for the support of PostgreSQL,
    the enterprise may need to either use a vendor-supported offering such as VMware’s
    **Tanzu** data management service subscription, which supports open source PostgreSQL
    and MySQL DB. Alternatively, they could use a vendor-specific flavor of the open
    source solution such as the one provided by Crunchy Data for PostgreSQL, for example.
    In this case, organizations may get container images from respective third-party
    vendors such as Crunchy Data. For such cases, VAC would not be useful. But if
    the enterprises wanted to use the vanilla upstream version of PostgreSQL, which
    is supported by a vendor such as VMware or **EnterpriseDB** (**EDB**), then they
    may use VAC to benefit from all the listed benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Upstream versus a vendor-specific flavored OSS
  prefs: []
  type: TYPE_NORMAL
- en: Using upstream OSS distributions directly without adding a vendor-specific flavor
    helps avoid potential vendor lock-ins, which is the first and foremost reason
    to use OSS technology. VAC makes the adoption of such OSS tools easier for enterprises.
    Despite that, many organizations still use vendor-specific flavors of OSS because
    of the additional features and functionalities not available in the upstream OSS
    distributions. Hence, there are pros and cons to both approaches.
  prefs: []
  type: TYPE_NORMAL
- en: After understanding how and where VAC is beneficial and where it is not applicable,
    let’s now take a deeper look into what VAC is and what it contains.
  prefs: []
  type: TYPE_NORMAL
- en: What VMware Application Catalog is
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After delivering comprehensive detail on what business, security, and technological
    challenges VAC can address and where it will not be a good use case, let’s now
    understand this tool in a bit more detail to see what it contains. But before
    that, let’s look into the background of VAC to learn more about it.
  prefs: []
  type: TYPE_NORMAL
- en: The history of VMware Application Catalog
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In late 2019, with a vision of curating a comprehensive portfolio of modern
    application development and management tools, VMware decided to acquire a popular
    OSS packaging and distribution company named **Bitnami**. Bitnami had several
    years of experience working in this space, initially providing well-curated and
    consumable OSS tools in the form of binaries, virtual machine images, and container
    images. After the acquisition, VMware rebranded Bitnami as **Tanzu Application
    Catalog** to define an enterprise-grade OSS container image distribution offering
    that can customize the image specification as per the enterprise client’s needs.
    In 2021, VMware decided to also include the **Open Virtual Appliance** (**OVA**)
    image catalog to build virtual machines in addition to just the container images
    and Helm charts, which was the original idea behind Tanzu Application Catalog.
    As we see in this book, VMware’s Tanzu product portfolio contains all the tools
    and technologies around containers and Kubernetes. This was the reason why this
    offering was also initially given a *Tanzu* name. But with the recent announcement
    by VMware to also expand this offering to cover OVA images along with container
    images and Helm charts, this offering was renamed **VMware Application Catalog**
    (**VAC**), as it is not just about the container ecosystem anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The main focus of this book is Tanzu and its surrounding ecosystem. Additionally,
    at the time of writing this book, the offerings around virtual machine images
    are still evolving to get to the level of container images with respect to VAC.
    Hence, we will only cover details around container image catalog management and
    consumption in this chapter and not for virtual machine images.
  prefs: []
  type: TYPE_NORMAL
- en: After tapping into the history of VAC and knowing why it has *VMware* and not
    *Tanzu* in the name, let’s now understand what the key parts of this product offering
    are.
  prefs: []
  type: TYPE_NORMAL
- en: Components of VMware Application Catalog
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following tools are the components of VAC. Let’s review them.
  prefs: []
  type: TYPE_NORMAL
- en: VMware Application Catalog portal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main component of this offering is the VAC portal, where we can curate and
    manage the application catalog. It’s a **Software as a Service** (**SaaS**) component
    that is hosted and managed by VMware. VAC clients can use this portal using their
    VMware Cloud ([https://console.cloud.vmware.com/](https://console.cloud.vmware.com/))
    account to access the VAC service. A catalog administrator may create new catalogs,
    add new OSS offerings in the catalog, and download supporting elements related
    to the catalog items, including test result logs, CVE reports, anti-virus scan
    reports, and other such items. In summary, the VAC portal provides a web-based
    user interface for securely curating a catalog of OSS tools that can be freely
    used by the internal users of the enterprise. We will cover more details about
    this portal later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeapps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once a catalog administrator defines a catalog of supported OSS tools with Helm
    charts on the VAC portal, developers or operators may use Kubeapps ([https://kubeapps.com/](https://kubeapps.com/))
    to consume the published catalog items for internal use. Kubeapps is an OSS tool
    under the **Cloud Native Computing Foundation** (**CNCF**) umbrella. This project
    was started by Bitnami to provide a **graphical user interface** (**GUI**) to
    deploy software using Helm charts on top of Kubernetes clusters. Since the acquisition
    of Bitnami, VMware actively maintains it. It is a very lightweight application
    that can be deployed on a Kubernetes cluster running in the organization’s environment.
    The users of Kubeapps can select software to be deployed from the accessible catalog,
    change required deployment configurations (for example, user credentials, storage
    size, security configuration, and things of that nature), and finally deploy it
    as running containers on the targeted namespace of the selected Kubernetes cluster.
    Once a new version of the software is available in the catalog, the user can quickly
    upgrade it or remove the deployment if no longer required. To sum up, if the VAC
    portal provides the required controls for securely exposing a catalog of OSS tools,
    the Kubeapps provides the desired flexibility and productivity to developers or
    other users of the catalogs to life cycle various OSS tools as and when required
    using a published catalog.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand how these components work together to provide the required
    functionalities. *Figure 4**.2* describes the overall process to define the OSS
    catalog on the VAC portal and consume those catalog artifacts either using Kubeapps
    or **continuous deployment** (**CD**) automation in its place.
  prefs: []
  type: TYPE_NORMAL
- en: The VAC management and consumption process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After reviewing the key components of this offering in the previous section,
    let’s now understand how everything works together to provide end-to-end functionality
    of catalog curation and consumption using VAC. The following points correspond
    to each number given in *Figure 4**.2* and describe what happens during that step
    of the process. The cloud in *Figure 4**.2* depicts the SaaS infrastructure of
    VAC, and the rectangle defines the infrastructure boundary of the client organization.
    This client infrastructure may be either a private data center, a public cloud,
    or a combination of both:'
  prefs: []
  type: TYPE_NORMAL
- en: A catalog administrator within the client organization with access to the VMware
    Cloud account defines a new catalog of selected OSS tools. In this step, the catalog
    admin also selects the base container OS to be used, whether to include or exclude
    Helm charts for the selected items and provides a container repository reference
    to where the catalog artifacts will be pushed for secure consumption.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the catalog administrator submits a catalog configuration, VAC automation
    processes take it forward to deploy the required automation pipelines to create
    a stream of container images and Helm charts that can be generated as per the
    specifications provided by the catalog administrator in *step 1*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The VAC automation process pulls required OSS binaries from authorized third-party
    sources based on each selected OSS tool. After getting the required binaries,
    VAC automation performs certain operations, including automation testing of the
    OSS tool’s version, packaging the tool using the client-specified OS image layer,
    preparing a container specification report, and performing anti-virus and CVE
    scans. This step gets repeated for each newer version of each OSS tool covered
    by VAC.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once a catalog item is ready for consumption, it is pushed to a target container
    registry as specified by the catalog administrator in *step 1*. This step is also
    repeated for each OSS tool’s version that is prepared in *step 3*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once a catalog item is pushed in the target container registry, a catalog administrator
    may pull the required reports that were prepared in *step 3* for the published
    artifacts such as CVE scan reports and others. The VAC portal also specifies required
    CLI commands to use the container images and Helm charts for the published artifacts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this step, the catalog administrator configures the published catalog on
    Kubeapps for internal consumption.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this step, Kubeapps pulls the required details of the catalog to publish
    on the GUI for consumption.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the catalog is configured on Kubeapps, a catalog consumer may access Kubeapps
    GUI to deploy the required OSS tool as a Kubernetes deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.2 – The VAC management and consumption process](img/B18145_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – The VAC management and consumption process
  prefs: []
  type: TYPE_NORMAL
- en: Upon receiving the request from a catalog consumer to deploy a tool using its
    Helm chart on the Kubeapps GUI, Kubeapps pulls the required Helm chart and container
    images for the deployment of the tool from the container registry where the artifacts
    were pushed in *step 4*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Helm installer triggered by Kubeapps deploys the OSS tool on the targeted
    Kubernetes cluster using the configuration supplied by the catalog consumer in
    *step 8*. At the end of this step, we have a running instance of the OSS tool
    in the targeted Kubernetes cluster. In most cases, this step gets completed within
    a few minutes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This step describes an alternative way of consuming the VAC-supplied artifacts
    using a CD automation process. This step can be configured to be triggered every
    time there is a newer version of the artifact available in the container registry
    to initiate an automated deployment process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this step, the CD process deploys the downloaded OSS tool in the targeted
    Kubernetes cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following figure shows all these steps as a summary highlighting what VAC
    is and how it is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – VAC at a high level (https://docs.vmware.com/)](img/B18145_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – VAC at a high level (https://docs.vmware.com/)
  prefs: []
  type: TYPE_NORMAL
- en: With this, we covered the details required to understand the history of VAC,
    how VAC got its current name, the key components of VAC, and the end-to-end process
    involving catalog creation to consumption. Now let’s get started using VAC to
    better understand how to consume it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with VMware Application Catalog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this part of the chapter, we will cover the following details:'
  prefs: []
  type: TYPE_NORMAL
- en: How to configure an application catalog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to install Kubeapps on a Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to configure Kubeapps to use a catalog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let’s get started with hands-on work. But before that, we need the following
    prerequisites fulfilled.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following points list prerequisites to operationalize VAC:'
  prefs: []
  type: TYPE_NORMAL
- en: A VMware Cloud Services ([https://console.cloud.vmware.com/](https://console.cloud.vmware.com/))
    account with VAC access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the following container repositories that can be accessed by VAC to
    push catalog items:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GCR
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Container Registry
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Harbor
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Kubernetes cluster with the following attributes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Version 1.19 or later
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Outbound internet access
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Container registry access that is used by VAC
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated **Persistent Volume** (**PV**) creation based on **Persistent Volume**
    **Claims** (**PVC**)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A workstation with either Linux or macOS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helm v3.x installed on the workstation machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helm installation
  prefs: []
  type: TYPE_NORMAL
- en: 'Use this documentation if you need help with the Helm installation: [https://docs.bitnami.com/kubernetes/get-started-kubernetes/#step-4-install-and-configure-helm](https://docs.bitnami.com/kubernetes/get-started-kubernetes/#step-4-install-and-configure-helm).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by defining a catalog of backend services that can be accessed by
    various applications. We will select MySQL, a relational database, as an example
    of an OSS catalog item to describe various details later in the chapter. In real
    life, we may add many other OSS tools to the catalog and use them in a similar
    way. You can find a broader list of available OSS tools on the VAC portal for
    catalog creation.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a catalog on the VAC portal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create a catalog on the VAC portal, take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to your VMware Cloud Services account and select **VMware Application
    Catalog** from the available services. If you do not see that service listed,
    then you may need to reach out to your VMware account team member to get access:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Select VAC from the list of services](img/B18145_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Select VAC from the list of services
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see an empty catalog page, as shown in the following figure as there
    will be no catalog items previously added. Click on the **ADD NEW APPLICATIONS**
    button shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.5 – The ADD NEW APPLICATIONS button in an empty catalog](img/B18145_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – The ADD NEW APPLICATIONS button in an empty catalog
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the base OS layer for the catalog artifacts. As mentioned before in
    this chapter, we will focus on the Kubernetes-based application catalog and not
    on virtual machines. As a simple example, **Ubuntu 18.04** is selected as the
    base OS layer, but you may also select the **Custom Base Image** option. For more
    details on that visit the product documentation for VAC at [https://docs.vmware.com/en/VMware-Application-Catalog/services/main/GUID-get-started-get-started-vmware-application-catalog.html#step-3-create-custom-catalogs-5](https://docs.vmware.com/en/VMware-Application-Catalog/services/main/GUID-get-started-get-started-vmware-application-catalog.html#step-3-create-custom-catalogs-5):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Base OS selection for catalog items](img/B18145_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Base OS selection for catalog items
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the required OSS items from the available options to include in the
    catalog. You may also search for them if required:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Select catalog items](img/B18145_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Select catalog items
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the destination container registry by clicking on the **ADD REGISTRY**
    button to get the required catalog artifacts delivered:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Add the container registry](img/B18145_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Add the container registry
  prefs: []
  type: TYPE_NORMAL
- en: 'Provide the required details for the container registry to allow VAC to push
    catalog artifacts to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Add registry details](img/B18145_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Add registry details
  prefs: []
  type: TYPE_NORMAL
- en: 'Though *Figure 4**.9* shows the details for GCR, it also supports other registries,
    including Azure Container Registry and Harbor. You can get more information regarding
    other registries at: [https://docs.vmware.com/en/VMware-Application-Catalog/services/main/GUID-get-started-get-started-vmware-application-catalog.html#step-3-create-custom-catalogs-5](https://docs.vmware.com/en/VMware-Application-Catalog/services/main/GUID-get-started-get-started-vmware-application-catalog.html#step-3-create-custom-catalogs-5).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Give a name and description to this catalog:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Add name and description for the catalog](img/B18145_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Add name and description for the catalog
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify the input summary and submit the catalog request:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Submit the catalog request](img/B18145_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Submit the catalog request
  prefs: []
  type: TYPE_NORMAL
- en: After this, we will see a message confirming our catalog request submission.
    It may take about a couple of weeks to review and process this request before
    we start getting our catalog items delivered to our selected container registry
    destination.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it. We have our catalog of OSS tools defined so that we can publish to
    internal consumers for easy and quick access. But how can they access this catalog?
    Let’s check that out in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Consuming VAC using Kubeapps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we learned how to request an application catalog on
    the VAC portal. Once we start getting our Helm charts and container images delivered
    in our container registry, we can access those tools using Kubeapps, a GUI to
    manage Kubernetes-based software deployments. We discussed Kubeapps in detail
    in the *Components of VAC* section covered previously in this chapter. Let’s see
    how to install and configure it now.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeapps installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubeapps is itself a Kubernetes-based deployment to manage other Kubernetes
    deployments that can be deployed using Helm charts and operators. Since we are
    planning to use Kubeapps to consume our VAC-supplied distributions that include
    Helm charts and container images, we will not cover the usage of operators in
    this section. As a deployment topology, we can install Kubeapps on any Kubernetes
    cluster to deploy catalog items on the same or any other Kubernetes clusters and
    their Kubernetes namespaces that are linked with Kubeapps. In this chapter, we
    will use a single Kubernetes cluster to minimize configuration complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to fulfill some requirements to move forward, as they are covered
    previously in the *Prerequisites* section of this chapter. The following steps
    describe the installation and configuration of Kubeapps on a Kubernetes cluster
    with sufficient resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a Bitnami Helm chart repository to your local Helm library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a Kubernetes namespace for Kubeapps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install Kubeapps in the `kubeapps` namespace using a Helm chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding output is truncated for conciseness. With this, we have Kubeapps
    running in our cluster. Let’s verify it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify the Kubeapps deployment to see if everything is running fine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding command should list `pods`, `services`, `deployments`, `replicasets`,
    `statefulsets`, and `jobs` that are not listed here for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a temporary service account to access the Kubeapps GUI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Link the service account with a Kubernetes role to allow required access:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Retrieve the access token for the account to log in to the Kubeapps GUI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This long command will print a long string of characters, which is the token
    that we will use to log in to the Kubeapps GUI. Save this token for future reference.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The previous three commands to create access permissions for Kubeapps use a
    very primitive approach for simplicity and as an easy learning reference. In a
    production-grade implementation, we may need to use a more sophisticated approach
    to configure real enterprise users who can access Kubeapps in the Kubernetes namespace.
    Such user permissions are generally managed using an external integration with
    either an OIDC or LDAP identity provider. Additionally, using the `cluster-admin`
    role is not a secure approach and should not be used other than for such learning
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Expose the Kubeapps GUI to access it locally in a browser. For a production-grade
    deployment, the GUI should be assigned a proper domain name and exposed outside
    the Kubernetes cluster using a load balancer. In this step, we will use Kubernetes
    port forwarding for quick and simple access to the deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Access Kubeapps in your local browser using `http://localhost:8080/`. This
    should open the following screen of Kubeapps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Kubeapps GUI – authentication page](img/B18145_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Kubeapps GUI – authentication page
  prefs: []
  type: TYPE_NORMAL
- en: 'Access Kubeapps using the token retrieved in *step 7*. Paste the token value
    on the login page and submit. This should open Kubeapps GUI as shown in *Figure
    4**.13*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Kubeapps GUI – initial landing page](img/B18145_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Kubeapps GUI – initial landing page
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The screenshots and steps described in this chapter are based on the presently
    available versions of VAC and Kubeapps. Depending on when this book is read, the
    content and experience could be different based on the future changes incorporated
    into these products.
  prefs: []
  type: TYPE_NORMAL
- en: As you will see, Kubeapps comes with a configuration to access the publicly
    available generic Bitnami catalog. Once our customer catalog defined on the VAC
    portal is ready, and we start getting its artifacts, we may configure Kubeapps
    to use the same. We will cover the linking of our custom application catalog on
    Kubeapps later in day-two activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, we conclude this section of the chapter on getting started with
    VAC and Kubeapps. We have seen how to create a catalog on the VAC portal and install
    Kubeapps in a Kubernetes cluster. In the next section, we will see the following
    items, covering common day-two activities around VAC:'
  prefs: []
  type: TYPE_NORMAL
- en: How to inspect delivered catalog artifacts and obtain required reports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to link VAC with Kubeapps to publish it for consumption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to consume catalog items via automation pipeline using Kubeapps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to deploy MySQL as a backend service running on the Kubernetes cluster using
    Kubeapps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to manage catalog items
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now learn how to use VAC.
  prefs: []
  type: TYPE_NORMAL
- en: Common day-two activities with VAC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the details covered in the previous section of the chapter, we now have
    an OSS tool catalog request placed on the VAC portal and a Kubeapps instance running
    in the Kubernetes cluster. Let’s now review some of the key day-two activities
    that can be performed by the catalog administrator to ensure security and compliance
    of the OSS tool usage, and by the catalog consumers to unleash productivity and
    flexibility to quickly deploy and use these OSS tools in different ways that are
    part of the VAC catalog.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting catalog deliverables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once our catalog request is submitted to Vmware using the VAC portal, as we
    covered in the previous section, we can check the status of our catalog deliverables
    using the VAC portal as shown in the following figure under the **My** **Requests**
    tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Checking VAC request status](img/B18145_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Checking VAC request status
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have our catalog request completed, we start seeing our delivered artifacts
    under the **My Applications** page, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Listing catalog applications on the VAC portal](img/B18145_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – Listing catalog applications on the VAC portal
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on our selection during the catalog creation request, we may see
    Helm charts and their container images, as shown in *Figure 4**.15*. Let’s now
    check the details of the MySQL container image by clicking on the **DETAILS**
    link given for the item on the right. The following screenshot shows the details
    for the MySQL container image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Details of a catalog item on the VAC portal](img/B18145_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Details of a catalog item on the VAC portal
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the following details from different sections of a catalog item’s
    details page, as shown in *Figure 4**.16*. The numbers in the following list correspond
    to the numbers given in *Figure 4**.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Digest**: This is the name and the location of the artifact that is placed
    in our destination container registry that we supplied during catalog creation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Consume your Container Image**: This is the Docker command to pull this container
    image into a Docker runtime environment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Container Tags**: These are different tag alias for this container image
    that we can use in a Kubernetes deployment manifest file to pull this container
    image for running this application on Kubernetes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Validation Reports**: This allows us to download the automation test result
    log file that was generated to test this version of MySQL before creating this
    container image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Build Time Reports**: This section contains various container build reports,
    including anti-virus and CVE scan reports and an asset specification (bill of
    material) report containing the list of software with their versions used in the
    container image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Release Relationship**: This section shows the dependent Helm charts that
    use this image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Like the details of a container image, the details of the corresponding Helm
    chart include required `helm` CLI commands to deploy the chart, test results for
    the chart, asset specification, and container image dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Using the application catalog
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After inspecting the required details for the artifacts supplied for our catalog
    items on the VAC portal, it’s time to use them to deploy those tools in our environment.
    There are various ways we can use those container images provided by VAC. Depending
    on the requirement of the tool, we can simply run some tools in our workstation’s
    container runtime environment, such as Docker. However, for an enterprise-grade
    deployment, we need several supporting components to run a tool. For example,
    we saw that all Kubernetes resources were created to deploy and run our Kubeapps
    instance that we deployed previously in this chapter. One possible way to deploy
    such tools is to use deployment automation using custom scripts and CI/CD tools
    such as Jenkins and/or **ArgoCD** using a **GitOps**-based deployment model. Another
    possible option is to use a Kubernetes packaging tool such as Helm charts. A Helm
    chart bundles all required dependencies to deploy and run corresponding objects
    for a tool on a Kubernetes cluster. Using a tool such as Helm charts makes it
    very easy to configure and quickly deploy the tool with all its required components
    with minimal effort and within a few minutes. As we have seen previously, VAC
    allows us to select containers as well as Helm charts for our catalog items wherever
    applicable. As a part of application packaging, Helm charts also allow exposing
    certain configuration properties that we may need to change for different deployments.
    We can use these Helm charts provided by VAC to deploy those tools with our custom
    configuration requirements. For example, for a MySQL database deployment, we can
    change attributes such as its name, login credentials, storage volume size, and
    many more.
  prefs: []
  type: TYPE_NORMAL
- en: What is GitOps?
  prefs: []
  type: TYPE_NORMAL
- en: 'GitOps upholds the principle that Git is the one and only source of truth.
    GitOps requires the desired state of the system to be stored in version control
    so that anyone can view the history of changes. All changes to the desired state
    are performed through Git *commits*. Source: [https://blogs.vmware.com/management/2020/08/ops-powered-by-git-gitops-in-vrealize-automation.html](https://blogs.vmware.com/management/2020/08/ops-powered-by-git-gitops-in-vrealize-automation.html).'
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed previously, Kubeapps is a tool to deploy and manage our catalog
    of Helm charts that we have configured on VAC. So, let’s check out how we can
    link those Helm charts provided by VAC with our Kubeapps instance that we deployed
    previously in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the application catalog to Kubeapps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following steps describe how to configure a new catalog on our Kubeapps
    instance for the Helm charts provided by VAC:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtain the chart repository location where the Helm charts are located. You
    can find the same on the details page of a Helm chart item in your catalog on
    the VAC portal, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.17 – Getting the Helm chart repository location](img/B18145_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – Getting the Helm chart repository location
  prefs: []
  type: TYPE_NORMAL
- en: 'The highlighted URL portion is the location of all the Helm charts for our
    catalog that we defined on the VAC portal. Make a note of this URL as we will
    use it in one of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate an API token for the VAC account on the VAC portal, which we will
    use to authenticate our Kubeapps instance to pull the Helm charts for selected
    catalog items. The following sub-steps describe how to generate an API token on
    the VAC portal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go to the **My Account** page on the VAC portal using the drop-down menu in
    the top-right corner, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.18 – Go to the VAC My Account settings](img/B18145_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – Go to the VAC My Account settings
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **API** **Tokens** tab:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.19 – Go to the API Tokens list page](img/B18145_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 – Go to the API Tokens list page
  prefs: []
  type: TYPE_NORMAL
- en: 'If there is already a token listed on this page that has access to the VAC
    service, then you can skip the following steps to generate a new API token and
    jump directly to *step 3* to add a repository in Kubeapps. Otherwise, click on
    the **GENERATE A NEW API TOKEN** link as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '-'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20 – Go to the API Tokens configuration page](img/B18145_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – Go to the API Tokens configuration page
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the token configuration, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.21 – Generate a New API Token for VAC](img/B18145_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – Generate a New API Token for VAC
  prefs: []
  type: TYPE_NORMAL
- en: You can change the name and duration of the token to your requirements. However,
    the scope is important to allow access to VAC from Kubeapps. It may not be a read-only
    or support role. You can also generate a generic token that can be used for all
    VMware Cloud Service offerings. However, it will be very broad in nature allowing
    all types of access to all services. Hence, it is not recommended.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Save the generated token for future usage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.22 – Save the generated token](img/B18145_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 – Save the generated token
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to the Kubeapps instance that we deployed and open its configuration
    menu using the top-right corner icon followed by the **App Repositories** option
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.23 – Add the Helm chart repository to Kubeapps](img/B18145_04_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.23 – Add the Helm chart repository to Kubeapps
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **ADD APP REPOSITORY** button to configure the details of a new
    repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.24 – Go to a new repository configuration page](img/B18145_04_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.24 – Go to a new repository configuration page
  prefs: []
  type: TYPE_NORMAL
- en: 'Give it a name, add the Helm chart repository URL captured in *step 1*, paste
    the API token generated in *step 2*, select the **Skip TLS Verification** option,
    and click the **INSTALL** **REPO** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.25 – Install the VAC Helm repo in Kubeapps](img/B18145_04_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.25 – Install the VAC Helm repo in Kubeapps
  prefs: []
  type: TYPE_NORMAL
- en: We had to select the **Skip TLS Verification** option, as shown in *Figure 4**.25*,
    as our Kubeapps deployment is not assigned an external facing domain name and
    a TLS certificate. In an enterprise-grade environment, this is not a recommended
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you get a success message, then the catalog should be integrated with Kubeapps.
    To check that, click on the **Catalog** tab on Kubeapps and select the **demo-catalog**
    option, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.26 – A newly configured catalog on Kubeapps](img/B18145_04_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.26 – A newly configured catalog on Kubeapps
  prefs: []
  type: TYPE_NORMAL
- en: With this, our Kubeapps deployment is fully integrated with our custom application
    catalog that we created on VAC. Let’s now learn to deploy a MySQL service instance
    from our custom catalog using Kubeapps.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a service using Kubeapps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this part, we will deploy a MySQL database instance on our Kubernetes cluster
    using Kubeapps. The following steps describes how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **MySQL** tile that is shown in *Figure 4**.26* to get started
    with the installation on your Kubernetes cluster. We will see the following screen
    after clicking there:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.27 – Deployment instruction page for MySQL on Kubeapps](img/B18145_04_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.27 – Deployment instruction page for MySQL on Kubeapps
  prefs: []
  type: TYPE_NORMAL
- en: This page shows detailed instructions on how to use a MySQL Helm chart. We can
    use these instructions to manually deploy it using the Helm chart command-line
    tool. However, we will see how we can use the Kubeapps GUI for such configuration
    and installation. You can scroll down the page to see the details of all the configuration
    parameters that this Helm chart allows us to change to customize our MySQL database
    instance. By clicking the highlighted **DEPLOY** button in *Figure 4**.27*, we
    get a screen to update these attributes for our custom needs and to trigger the
    installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the type of Helm chart, we may get a form like a GUI to modify
    some of the most common attributes for the deployment. For example, the following
    form in *Figure 4**.28* for MySQL DB instance configuration shows that we can
    select deployment architecture and the size of the primary and secondary database
    storage volumes. But we will use a detailed YAML-based configuration approach
    to demonstrate that as well. So, click on the highlighted **YAML** tab in the
    following screenshot to move forward:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.28 – Helm configuration form for MySQL DB on Kubeapps](img/B18145_04_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.28 – Helm configuration form for MySQL DB on Kubeapps
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the destination Kubernetes namespace, update the **YAML** configuration,
    and deploy the service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.29 – Detailed Helm chart configuration for MySQL DB deployment using
    Kubeapps](img/B18145_04_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.29 – Detailed Helm chart configuration for MySQL DB deployment using
    Kubeapps
  prefs: []
  type: TYPE_NORMAL
- en: Section 1 in *Figure 4.29* shows where we can select the destination for this
    installation. As discussed before, we may use Kubeapps to deploy Helm charts in
    many other connected Kubernetes clusters and their namespaces depending on the
    Kubeapps configurations and the privileges of the Kubeapps user on those clusters
    and namespaces. In this case, we have only one option – **default** – that corresponds
    to the Kubernetes cluster where Kubeapps is deployed. Similarly, we may also select
    or create a namespace for this deployment if required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section 2 in *Figure 4.29* shows the YAML configuration for the Helm chart,
    where we may customize our deployment configuration. The deployment used in this
    book has only changed for user credentials, keeping all other attributes to their
    default values to keep it simple. Once the required configuration changes are
    done, we can verify them using the **Changes** tab to ensure there are only intended
    changes in the deployment configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the required changes are made, section 3 in *Figure 4.29* shows the button
    to trigger the deployment on our Kubernetes cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will get the following page after triggering the Helm chart deployment to
    update the status of the installation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.30 – MySQL DB deployment status on Kubeapps](img/B18145_04_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.30 – MySQL DB deployment status on Kubeapps
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.30* contains the following details:'
  prefs: []
  type: TYPE_NORMAL
- en: Section 1 shows the different buttons to perform the life cycle operations for
    the deployment, including upgrading it to a newer version of MySQL, rolling back
    to a previous version, or deleting the deployment if not required. The button
    triggers their corresponding Kubernetes Deployment life cycle operations behind
    the scenes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section 2 shows the number of healthy running pods for our deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section 3 shows the authentication credentials that were provided during the
    installation configuration step covered previously, as shown in *Figure 4.29*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section 4 shows different useful tips and details for using the deployed tool.
    For example, in our case, it shows how to connect to the MySQL DB instance deployed
    here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section 5 shows the different Kubernetes resources that were deployed as part
    of this installation. We can view details of the resources by clicking on their
    corresponding tabs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section 6 shows the final installation configuration that was used to deploy
    this instance of MySQL DB. We can save the YAML configuration in a file for future
    usage to deploy the same type of MySQL instance again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this, we have covered the required details to see how we can use Kubeapps
    to consume a custom application catalog created using VAC to quickly deploy several
    OSS tools in minutes and in a self-service way. Such a setup given to developers
    to deploy required application backend services can boost their productivity,
    reduce overall application development time, and as a result, improve an enterprise’s
    innovation speed. And using Kubeapps and VAC is not just for developers but also
    for DevOps, platform, and infrastructure engineers to deploy and use many popular
    OSS tools using container images and Helm charts delivered from a trusted source.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now visit one more day-two activity around VAC that the catalog administrators
    will need to perform – updating the catalog on the VAC portal.
  prefs: []
  type: TYPE_NORMAL
- en: Updating the application catalog
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier in the chapter, we saw, as a part of day-two activities, how to obtain
    different reports and logs for the supplied artifacts from the VAC portal. Then
    we covered how to consume a catalog of OSS tools that were supplied by VAC using
    either an automation pipeline or Kubeapps. Now let’s visit the last major day-two
    activity around VAC, which is to update the catalog items.
  prefs: []
  type: TYPE_NORMAL
- en: Adding new catalog items
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To add new OSS applications to the corporate catalog, catalog administrators
    can go to their corporate VAC account using VMware Cloud Services credentials.
    Once on the VAC portal, the catalog administrators can select the **ADD NEW APPLICATIONS**
    button under the **Applications** section, as shown in *Figure 4**.31*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.31 – Add a new item in the catalog on the VAC portal](img/B18145_04_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.31 – Add a new item in the catalog on the VAC portal
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to add new catalog items after clicking the highlighted button in
    *Figure 4**.31* are the same as we covered during the creation of the new catalog
    earlier in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.32 – Request status tracking on the VAC portal](img/B18145_04_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.32 – Request status tracking on the VAC portal
  prefs: []
  type: TYPE_NORMAL
- en: Newly added items will appear under **My Requests**, as shown in *Figure 4**.32*.
  prefs: []
  type: TYPE_NORMAL
- en: Making additional catalog changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Other than adding new items in the catalog, we may also need to remove unwanted
    tools from it, update the base OS layer, update destination repository details,
    and include/exclude Helm charts from the listed items. At the time of writing
    this book, all these operations can only be performed by contacting VAC support
    as the VAC portal does not currently provide an interface to do it in a self-service
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have covered most of the day-two activities around VAC. Let’s
    recap to summarize our learning from this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw that the popularity of using OSS tools has dramatically
    increased in recent years. There are several mature OSS tools available today
    that many organizations confidently use in their production environments. Also,
    running software such as containers on Kubernetes alongside containerized client
    applications is a quick way to equip the applications with appropriate backend
    technologies. But using container images available on public container registries
    such as Docker Hub is not a secure way of deploying such OSS tools. And hence,
    most organizations do not encourage such practices and try to employ some internal
    mechanisms to generate internally curated container images for such OSS consumption.
    Having such efforts undertaken internally not only wastes a lot of resources but
    also discourages developer teams from experimenting with various tools or wasting
    their productivity in waiting for getting them ready for consumption. And that
    is where VAC comes into the picture. We saw how VAC can handle such challenges
    by providing a way to curate a custom catalog of required OSS tools that can be
    consumed internally with full confidence.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned how VAC works and what its key components are. Following this,
    we learned how we can get started with VAC to define a catalog and set up Kubeapps
    to consume the Helm charts and container images delivered under that custom catalog.
    And finally, we went through some of the key day-two activities around catalog
    consumption and management. We saw how quickly we can deploy an instance of MySQL
    database using its Helm chart supplied as a part of the catalog. We also reviewed
    how catalog administrators can check out and obtain copies of the CVE scanning
    report, anti-virus scan logs, test logs, and asset specification (bill of material)
    for all the container images and Helm charts delivered through the VAC service.
  prefs: []
  type: TYPE_NORMAL
- en: After seeing how to build our apps quickly with the vast choice of OSS tools
    to be used as backends for our apps, in the next chapter, we will learn how to
    build and manage the API endpoints exposed by our applications using the Tanzu
    application tools.
  prefs: []
  type: TYPE_NORMAL
