<html><head></head><body>
		<div>
			<div id="_idContainer183" class="Content">
			</div>
		</div>
		<div id="_idContainer184" class="Content">
			<h1 id="_idParaDest-83">7. <a id="_idTextAnchor084"/>Monitoring the AKS cluster and the application</h1>
		</div>
		<div id="_idContainer232" class="Content">
			<p>Now that you know how to deploy applications on an AKS cluster, let's focus on how you can ensure that your cluster and applications remain available. In this chapter, you will learn how to monitor your cluster and the applications running on it. You'll explore how Kubernetes makes sure that your applications are running reliably using readiness and liveness probes.</p>
			<p>You will also learn how <strong class="bold">AKS Diagnostics</strong> and <strong class="bold">Azure Monitor</strong> are used, and how they are integrated within the Azure portal. You will see how you can use AKS Diagnostics to monitor the status of the cluster itself, and how Azure Monitor helps monitor the pods on the cluster and allows you to get access to the logs of the pods at scale.</p>
			<p>In brief, the following topics will be covered in this chapter:</p>
			<ul>
				<li>Monitoring and debugging applications using <strong class="inline">kubectl</strong></li>
				<li>Reviewing metrics reported by Kubernetes	</li>
				<li>Reviewing metrics from Azure Monitor</li>
			</ul>
			<p>Let's start the chapter by reviewing some of the commands in <strong class="inline">kubectl</strong> that you can use to monitor your applications.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor085"/>Commands for monitoring applications</h2>
			<p>Monitoring the health of applications deployed on Kubernetes as well as the Kubernetes infrastructure itself is essential for providing a reliable service to your customers. There are two primary use cases for monitoring:</p>
			<ul>
				<li>Ongoing monitoring to get alerts if something is not behaving as expected</li>
				<li>Troubleshooting and debugging application errors</li>
			</ul>
			<p>When observing an application running on top of a Kubernetes cluster, you'll need to examine multiple things in parallel, including containers, pods, services, and the nodes in the cluster. For ongoing monitoring, you'll need a monitoring system such as Azure Monitor or Prometheus. Azure Monitor will be introduced later in this chapter. Prometheus (<a href="https://prometheus.io/">https://prometheus.io/</a>) is a popular open-source solution within the Kubernetes ecosystem to monitor Kubernetes environments. For troubleshooting, you'll need to interact with the live cluster. The most common commands used for troubleshooting are as follows:</p>
			<p class="snippet">kubectl get &lt;resource type&gt; &lt;resource name&gt;</p>
			<p class="snippet">kubectl describe &lt;resource type&gt; &lt;resource name&gt;</p>
			<p class="snippet">kubectl logs &lt;pod name&gt;</p>
			<p>Each of these commands will be described in detail later in this chapter. </p>
			<p>To begin with the practical examples, recreate the guestbook example again using the following command:</p>
			<p class="snippet">kubectl create -f guestbook-all-in-one.yaml</p>
			<p>While the <strong class="inline">create</strong> command is running, you will watch its progress in the following sections. Let's start by exploring the <strong class="inline">get</strong> command.</p>
			<h3 id="_idParaDest-85"><a id="_idTextAnchor086"/>The kubectl get command</h3>
			<p>To see the overall picture of deployed applications, <strong class="inline">kubectl</strong> provides the <strong class="inline">get</strong> command. The <strong class="inline">get</strong> command lists the resources that you specify. Resources can be pods, ReplicaSets, ingresses, nodes, deployments, secrets, and so on. You have already run this command in the previous chapters to verify that an application was ready for use. </p>
			<p>Perform the following steps:</p>
			<ol>
				<li>Run the following <strong class="inline">get</strong> command, which will get us the resources and their statuses:<p class="snippet">kubectl get all</p><p>This will show you all the deployments, ReplicaSets, pods, and services in your namespace:</p><div id="_idContainer185" class="IMG---Figure"><img src="image/B17338_07_01.jpg" alt="Output displaying all the resources running in the default namespace"/></div><p class="figure">Figure 7.1: All the resources running in the default namespace</p></li>
				<li>Focus your attention on the pods in your deployment. You can get the status of the pods with the following command:<p class="snippet">kubectl get pods</p><p>You will see that only the pods are shown, as seen in <em class="italics">Figure 7.2</em>. Let's investigate this in detail:</p><div id="_idContainer186" class="IMG---Figure"><img src="image/B17338_07_02.jpg" alt="Checking the status of all the pods in the namespace"/></div><p class="figure">Figure 7.2: All the pods in your namespace</p><p>The first column indicates the pod name, for example, <strong class="inline">frontend-766d4f77cb-ds6gb</strong>. The second column indicates how many containers in the pod are ready against the total number of containers in the pod. Readiness is defined via a readiness probe in Kubernetes. There is a dedicated section called <em class="italics">Readiness and liveness probes</em> later in this chapter.</p><p>The third column indicates the status, for example, <strong class="inline">Pending</strong>, <strong class="inline">ContainerCreating</strong>, <strong class="inline">Running</strong>, and so on. The fourth column indicates the number of restarts, while the fifth column indicates the age when the pod was asked to be created.</p></li>
				<li>If you need more information about your pod, you can add extra columns to the output of a <strong class="inline">get</strong> command by adding <strong class="inline">-o wide</strong> to the command like this:<p class="snippet">kubectl get pods -o wide</p><p>This will show you additional information, as shown in <em class="italics">Figure 7.3</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/B17338_07_03.jpg" alt="Getting additional details of all the pods in the namespace"/>
				</div>
			</div>
			<p class="figure">Figure 7.3: Adding -o wide shows more details on the pods</p>
			<p>The extra columns include the IP address of the pod, the node it is running on, the nominated node, and readiness gates. A nominated node is only set when a higher-priority pod preempts a lower-priority pod. The nominated node field would then be set on the higher-priority pod. It signifies the node that the higher-priority pod will be scheduled once the lower-priority pod has terminated gracefully. A readiness gate is a way to introduce external system components as the readiness for a pod.</p>
			<p>Executing a <strong class="inline">get pods</strong> command only shows the state of the current pod. As we will see next, things can fail at any of the states, and we need to use the <strong class="inline">kubectl describe</strong> command to dig deeper.</p>
			<h3 id="_idParaDest-86"><a id="_idTextAnchor087"/>The kubectl describe command</h3>
			<p>The <strong class="inline">kubectl describe</strong> command gives you a detailed view of the object you are describing. It contains the details of the object itself, as well as any recent events related to that object. While the <strong class="inline">kubectl get events</strong> command lists all the events for the entire namespace, with the <strong class="inline">kubectl describe</strong> command, you would get only the events for that specific object. If you are interested in just pods, you can use the following command:</p>
			<p class="snippet">kubectl describe pods</p>
			<p>The preceding command lists all the information pertaining to all pods. This is typically too much information to contain in a typical shell.</p>
			<p>If you want information on a particular pod, you can type the following:</p>
			<p class="snippet">kubectl describe pod/&lt;pod-name&gt;</p>
			<h4>Note</h4>
			<p class="callout">You can either use a slash or a space in between <strong class="inline">pod</strong> and <strong class="inline">&lt;pod-name&gt;</strong>. The following two commands will have the same output:</p>
			<p class="callout"><strong class="inline">kubectl describe pod/&lt;pod-name&gt;</strong></p>
			<p class="callout"><strong class="inline">kubectl describe pod &lt;pod-name&gt;</strong></p>
			<p>You will get an output similar to <em class="italics">Figure 7.4</em>, which will be explained in detail later:</p>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/B17338_07_04.jpg" alt="Describing an individual pod to show a detailed output of that object"/>
				</div>
			</div>
			<p class="figure">Figure 7.4: Describing an object shows the detailed output of that object</p>
			<p>From the description, you can get the node on which the pod is running, how long it has been running, its internal IP address, the Docker image name, the ports exposed, the <strong class="inline">env</strong> variables, and the events (from within the past hour).</p>
			<p>In the preceding example, the pod name is <strong class="inline">frontend-766d4f77cb-ds6gb</strong>. As mentioned in <em class="italics">Chapter 1, Introduction to containers and Kubernetes</em>, it has the <strong class="inline">&lt;ReplicaSet name&gt;-&lt;random 5 chars&gt;</strong> format. The <strong class="inline">replicaset</strong> name itself is randomly generated from the deployment name front end: <strong class="inline">&lt;deployment name&gt;-&lt;random-string&gt;</strong>.</p>
			<p><em class="italics">Figure 7.5</em> shows the relationship between a deployment, a ReplicaSet, and pods:</p>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="image/B17338_07_05.jpg" alt="A flowchart describing the relationship between a deployment, a ReplicaSet, and pods"/>
				</div>
			</div>
			<p class="figure">Figure 7.5: Relationship between a deployment, a ReplicaSet, and pods</p>
			<p>The namespace under which this pod runs is <strong class="inline">default</strong>. So far, you have just been using the <strong class="inline">default</strong> namespace, appropriately named <strong class="inline">default</strong>. </p>
			<p>Another section that is important from the preceding output is the <strong class="inline">node</strong> section:</p>
			<p class="snippet">Node:         aks-agentpool-39838025-vmss000000/10.240.0.4 </p>
			<p>The <strong class="inline">node</strong> section lets you know which physical node/VM the pod is running on. If the pod is repeatedly restarting or having issues running and everything else seems OK, there might be an issue with the node itself. Having this information is essential to perform advanced debugging.</p>
			<p>The following is the time the pod was initially scheduled:</p>
			<p class="snippet">Start Time:   Tue, 26 Jan 2021 02:10:33 +0000 </p>
			<p>This doesn't mean that the pod has been running since that time, so the time can be misleading in that sense. If a health event occurs (for example, a container crashes), the pod will reset automatically.</p>
			<p>You can add more information about a workload in Kubernetes using <strong class="inline">Labels</strong>, as shown here:</p>
			<p class="snippet">Labels:app=guestbook</p>
			<p class="snippet">pod-template-hash=57d8c9fb45</p>
			<p class="snippet">tier=frontend</p>
			<p>Labels are a commonly used functionality in Kubernetes. For example, this is how links between objects, such as <strong class="inline">service</strong> to <strong class="inline">pod</strong> and <strong class="inline">deployment</strong> to <strong class="inline">ReplicaSet</strong> to <strong class="inline">pod</strong> (<em class="italics">Figure 7.5</em>), are made. If you see that traffic is not being routed to a pod from a service, this is the first thing you should check. Also, you'll notice that the <strong class="inline">pod-template-hash</strong> label also occurs in the pod name. This is how the link between the ReplicaSet and the pod is made. If the labels don't match, the resources won't attach.</p>
			<p>The following shows the internal IP of the pod and its status:</p>
			<p class="snippet">Status:       Running</p>
			<p class="snippet">IP:           10.244.0.44</p>
			<p class="snippet">IPs:</p>
			<p class="snippet">  IP:           10.244.0.44 </p>
			<p>As mentioned in previous chapters, when building out your application, the pods can be moved to different nodes and get a different IP, so you should avoid using these IP addresses. However, when debugging application issues, having a direct IP for a pod can help with troubleshooting. Instead of connecting to your application through a service object, you can connect directly from one pod to another using the other pod's IP address to test connectivity.</p>
			<p>The containers running in the pod and the ports that are exposed are listed in the following block:</p>
			<p class="snippet">Containers:</p>
			<p class="snippet">  php-redis:</p>
			<p class="snippet">    ...</p>
			<p class="snippet">    Image:          gcr.io/google-samples/gb-frontend:v4</p>
			<p class="snippet">    ...</p>
			<p class="snippet">    Port:           80/TCP</p>
			<p class="snippet">    ...</p>
			<p class="snippet">    Requests:</p>
			<p class="snippet">      cpu:     10m</p>
			<p class="snippet">      memory:  10Mi</p>
			<p class="snippet">    Environment:</p>
			<p class="snippet">      GET_HOSTS_FROM:  dns</p>
			<p class="snippet">    ...</p>
			<p>In this case, you are getting the <strong class="inline">gb-frontend</strong> container with the <strong class="inline">v4</strong> tag from the <strong class="inline">gcr.io</strong> container registry, and the repository name is <strong class="inline">google-samples</strong>.</p>
			<p>Port <strong class="inline">80</strong> is exposed to outside traffic. Since each pod has its own IP, the same port can be exposed for multiple instances of the same pod even when running on the same host. For instance, if you had two pods running a web server on the same node, both could use port <strong class="inline">80</strong>, since each pod has its own IP address. This is a huge management advantage as you don't have to worry about port collisions on the same node. </p>
			<p>Any events that occurred in the previous hour show up here:</p>
			<p class="snippet">Events:</p>
			<p>Using <strong class="inline">kubectl describe</strong> is very useful to get more context about the resources you are running. The final section contains events related to the object you were describing. You can get all events in your cluster using the <strong class="inline">kubectl get events</strong> command.</p>
			<p>To see the events for all resources in the system, run the following command:</p>
			<p class="snippet">kubectl get events</p>
			<h4>Note</h4>
			<p class="callout">Kubernetes maintains events for only 1 hour by default.</p>
			<p>If everything goes well, you should have an output similar to <em class="italics">Figure 7.6</em>:</p>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="image/B17338_07_06.jpg" alt="Running the kubectl get events command to display all events from the past hour"/>
				</div>
			</div>
			<p class="figure">Figure 7.6: Getting the events shows all events from the past hour</p>
			<p><em class="italics">Figure 7.6</em> only shows the event for one pod, but as you can see in your output, the output for this command contains the events for all resources that were recently created, updated, or deleted. </p>
			<p>In this section, you have learned about the commands you can use to inspect a Kubernetes application. In the next section, you'll focus on debugging application failures.</p>
			<h3 id="_idParaDest-87"><a id="_idTextAnchor088"/>Debugging applications</h3>
			<p>Now that you have a basic understanding of how to inspect applications, you can start seeing how you can debug issues with deployments.</p>
			<p>In this section, common errors will be introduced, and you'll determine how to debug and fix them.</p>
			<p>If you haven't implemented the Guestbook application already, run the following command:</p>
			<p class="snippet">kubectl create -f guestbook-all-in-one.yaml</p>
			<p>After a couple of seconds, the application should be up and running.</p>
			<h3>Image pull errors</h3>
			<p>In this section, you are going to introduce image pull errors by setting the image tag value to a non-existent one. An image pull error occurs when Kubernetes cannot download the image for the container it needs to run.</p>
			<ol>
				<li value="1">Run the following command on Azure Cloud Shell:<p class="snippet">kubectl edit deployment/frontend</p><p>Next, change the image tag from <strong class="inline">v4</strong> to <strong class="inline">v_non_existent</strong> by executing the following steps.</p></li>
				<li>Type <strong class="inline">/gb-frontend</strong> and hit the <em class="italics">Enter</em> key to have your cursor brought to the image definition.<p>Hit the <em class="italics">I</em> key to go into insert mode. Delete <strong class="inline">v4</strong> and type <strong class="inline">v_non_existent</strong> as shown in <em class="italics">Figure 7.7</em>:</p><div id="_idContainer191" class="IMG---Figure"><img src="image/B17338_07_07.jpg" alt="Using Azure Cloud Shell to change the image tag from v4 to v_non_existent"/></div><p class="figure"> </p><p class="figure">Figure 7.7: Changing the image tag from v4 to v_non_existent</p></li>
				<li>Now, close the editor by first hitting the <em class="italics">Esc</em> key, then type <strong class="inline">:wq!</strong> and hit <em class="italics">Enter</em>.</li>
				<li>Run the following command to list all the pods in the current namespace:<p class="snippet">kubectl get pods</p><p>The preceding command should indicate errors, as shown in <em class="italics">Figure 7.8</em>:</p><div id="_idContainer192" class="IMG---Figure"><img src="image/B17338_07_08.jpg" alt="Displaying all the pods in the current namespace to indicate status errors"/></div><p class="figure">Figure 7.8: One of the pods has the status of either ErrImagePull or ImagePullBackOff</p><p>You might see either a status called <strong class="inline">ErrImagePull</strong> or <strong class="inline">ImagePullBackOff</strong>. Both errors refer to the fact that Kubernetes cannot pull the image from the registry. The <strong class="inline">ErrImagePull</strong> error describes just this; <strong class="inline">ImagePullBackOff</strong> describes that Kubernetes will back off (wait) before retrying to download the image. This back-off has an exponential delay, going from 10 to 20 to 40 seconds and beyond, up to 5 minutes.</p></li>
				<li>Run the following command to get the full error details:<p class="snippet">kubectl describe pods/&lt;failed pod name&gt;</p><p>A sample error output is shown in <em class="italics">Figure 7.9</em>. The key error message is highlighted in red:</p><div id="_idContainer193" class="IMG---Figure"><img src="image/B17338_07_09.jpg" alt="Describing an individual pod to display more details on the error"/></div><p class="figure">Figure 7.9: Using describe shows more details on the error</p><p>The events clearly show that the image does not exist. Errors such as passing invalid credentials to private Docker repositories will also show up here.</p></li>
				<li>Let's fix the error by setting the image tag back to <strong class="inline">v4</strong>. First, type the following command in Cloud Shell to edit the deployment:<p class="snippet">kubectl edit deployment/frontend</p></li>
				<li>Type <strong class="inline">/gb-frontend</strong> and hit <em class="italics">Enter</em> to have your cursor brought to the image definition.</li>
				<li>Hit the <em class="italics">I</em> key to go into insert mode. Delete <strong class="inline">v_non_existent</strong>, and type <strong class="inline">v4</strong>.</li>
				<li>Now, close the editor by first hitting the <em class="italics">Esc</em> key, then type <strong class="inline">:wq!</strong> and hit <em class="italics">Enter</em>.</li>
				<li>This should automatically fix the deployment. You can verify it by getting the events for the pods again.<h4>Note</h4><p class="callout">Because Kubernetes did a rolling update, the front end was continuously available with zero downtime. Kubernetes recognized a problem with the new specification and stopped rolling out additional changes automatically.</p></li>
			</ol>
			<p>Image pull errors can occur when images aren't available or when you don't have access to the container registry. In the next section, you'll explore an error within the application itself.</p>
			<h3>Application errors</h3>
			<p>You will now see how to debug an application error. The errors in this section will be self- induced, similar to the last section. The method for debugging the issue is the same as the one we used to debug errors on running applications.</p>
			<ol>
				<li value="1">To start, get the public IP of the front-end service:<p class="snippet">kubectl get service </p></li>
				<li>Connect to the service by pasting its public IP in a browser. Create a couple of entries:</li>
			</ol>
			<div>
				<div id="_idContainer194" class="IMG---Figure">
					<img src="image/B17338_07_10.jpg" alt="Creating entries in the guestbook application"/>
				</div>
			</div>
			<p class="figure">Figure 7.10: Make a couple of entries in the guestbook application</p>
			<p>You now have an instance of the guestbook application running. To improve the experience with the example, it's best to scale down the front end so there is only a single replica running.</p>
			<h3>Scaling down the front end</h3>
			<p>In <em class="italics">Chapter 3</em>, <em class="italics">Application deployment on AKS</em>, you learned how the deployment of the front end has a configuration of <strong class="inline">replicas=3</strong>. This means that the requests the application receives can be handled by any of the pods. To introduce the application error and note the errors, you'll need to make changes in all three of them.</p>
			<p>But to make this example easier, set <strong class="inline">replicas</strong> to <strong class="inline">1</strong>, so that you have to make changes to only one pod:</p>
			<p class="snippet">kubectl scale --replicas=1 deployment/frontend</p>
			<p>Having only one replica running will make introducing the error easier. Let's now introduce this error.</p>
			<h3>Introducing an app error</h3>
			<p>In this case, you are going to make the <strong class="bold">Submit</strong> button fail to work. You will need to modify the application code for this:</p>
			<h4>Note:</h4>
			<p class="callout">It is not advised to make production changes to your application by using <strong class="inline">kubectl exec</strong> to execute commands in your pods. If you need to make changes to your application, the preferred way is to create a new container image and update your deployment.</p>
			<ol>
				<li value="1">You will use the <strong class="inline">kubectl exec</strong> command. This command lets you run commands on the command line of that pod. With the <strong class="inline">-it</strong> option, it attaches an interactive terminal to the pod and gives you a shell that you can run commands on. The following command launches a Bash terminal on the pod:<p class="snippet">kubectl exec -it &lt;frontend-pod-name&gt; -- bash</p><p>This will enter a Bash shell environment as shown in <em class="italics">Figure 7.11</em>:</p><div id="_idContainer195" class="IMG---Figure"><img src="image/B17338_07_11.jpg" alt="Executing a command to launch and enter a Bash terminal on the pod"/></div><p class="figure">Figure 7.11: Getting a pod's name and getting access to a shell inside the pod</p></li>
				<li>Once you are in the container shell, run the following command:<p class="snippet">apt update</p><p class="snippet">apt install -y vim</p><p>The preceding code installs the <strong class="inline">vim</strong> editor so that we can edit the file to introduce an error.</p></li>
				<li>Now, use <strong class="inline">vim</strong> to open the <strong class="inline">guestbook.php</strong> file:<p class="snippet">vim guestbook.php</p></li>
				<li>Add the following code at line 17, below the line <strong class="inline">if ($_GET['cmd'] == 'set') {</strong>. Remember, to edit a line in <strong class="inline">vim</strong>, you hit the <em class="italics">I</em> key. After you are done editing, you can exit by hitting <em class="italics">Esc</em>, and then type <strong class="inline">:wq!</strong> and press <em class="italics">Enter</em>:<p class="snippet">$host = 'localhost';</p><p class="snippet">if(!defined('STDOUT')) define('STDOUT', fopen('php://stdout', 'w'));</p><p class="snippet">fwrite(STDOUT, "hostname at the beginning of 'set' command "); fwrite(STDOUT, $host);</p><p class="snippet">fwrite(STDOUT, "\n");</p><p>The file will look like <em class="italics">Figure 7.12</em>:</p><div id="_idContainer196" class="IMG---Figure"><img src="image/B17338_07_12.jpg" alt="Output displaying the updated code that introduced an error and additional logging"/></div><p class="figure">Figure 7.12: The updated code that introduced an error and additional logging</p></li>
				<li>You have now introduced an error where reading messages will work, but not writing them. You have done this by asking the front end to connect to the Redis master at the non-existent localhost server. The writes should fail. At the same time, to make this demo more visual, we added some additional logging to this section of the code.<p>Open your guestbook application by browsing to its public IP, and you should see the entries from earlier:</p><div id="_idContainer197" class="IMG---Figure"><img src="image/B17338_07_10.jpg" alt="Guestbook application displaying entries from earlier"/></div><p class="figure">Figure 7.13: The entries from earlier are still present</p></li>
				<li>Now, create a new message by typing a message and hitting the <strong class="bold">Submit</strong> button:<div id="_idContainer198" class="IMG---Figure"><img src="image/B17338_07_14.jpg" alt="Creating a new message in the guestbook application"/></div><p class="figure">Figure 7.14: A new message was created</p><p>Submitting a new message makes it appear in the application. If you did not know any better, you would have thought the entry was written successfully to the database. However, if you refresh your browser, you will see that the message is no longer there.</p></li>
				<li>To verify that the message has not been written to the database, hit the <strong class="bold">Refresh</strong> button in your browser; you will see just the initial entries, and the new entry has disappeared:</li>
			</ol>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/B17338_07_15.jpg" alt="Refreshing the webpage to confirm there the new message wasn’t persisted"/>
				</div>
			</div>
			<p class="figure">Figure 7.15: The new message has disappeared</p>
			<p>As an app developer or operator, you'll probably get a ticket like this: <strong class="inline">After the new deployment, new entries are not persisted. Fix it</strong>.</p>
			<h3>Using logs to identify the root cause </h3>
			<p>The first step toward resolution is to get the logs. </p>
			<ol>
				<li value="1">Exit out of the front-end pod for now and get the logs for this pod:<p class="snippet">exit</p><p class="snippet">kubectl logs &lt;frontend-pod-name&gt;</p><h4>Note:</h4><p class="callout">You can add the <strong class="inline">-f</strong> flag after <strong class="inline">kubectl logs</strong> to get a live log stream, as follows: <strong class="inline">kubectl logs &lt;pod-name&gt; -f</strong>. This is useful during live debugging sessions.</p></li>
				<li>You will see entries such as those seen in <em class="italics">Figure 7.16</em>:<div id="_idContainer200" class="IMG---Figure"><img src="image/B17338_07_16.jpg" alt="Output displaying the new message as part of the application logs"/></div><p class="figure">Figure 7.16: The new message shows up as part of the application logs</p></li>
				<li>Hence, you know that the error is somewhere when writing to the database in the <strong class="inline">set</strong> section of the code. When you see the entry <strong class="inline">hostname at the beginning of 'set' command localhost</strong>, you know that the error is between this line and the start of the client, so the setting of <strong class="inline">$host = 'localhost'</strong> must be the offending error. This error is not as uncommon as you would think and, as you just saw, could have easily gone through QA unless there had been a specific instruction to refresh the browser. It could have worked perfectly well for the developer, as they could have a running Redis server on the local machine.</li>
			</ol>
			<p>Now that you have used logs in Kubernetes to root cause the issue, let's get to resolving the error and getting our application back to a healthy state.</p>
			<h3>Solving the issue</h3>
			<p>There are two options to fix this bug you introduced: you can either navigate into the pod and make the code changes, or you can ask Kubernetes to give us a healthy new pod. It is not recommended to make manual changes to pods, so in the next step, you will use the second approach. Let's fix this bug by deleting the faulty pod:</p>
			<p class="snippet">kubectl delete pod &lt;podname&gt;</p>
			<p>As there is a ReplicaSet that controls the pods, you should immediately get a new pod that has started from the correct image. Try to connect to the guestbook again and verify that messages persist across browser refreshes.</p>
			<p>The following points summarize what was covered in this section on how to identify an error and how to fix it:</p>
			<ul>
				<li>Errors can come in many shapes and forms.</li>
				<li>Most of the errors encountered by the deployment team are configuration issues.</li>
				<li>Use logs to identify the root cause.</li>
				<li>Using <strong class="inline">kubectl exec</strong> on a container is a useful debugging strategy.</li>
				<li>Note that broadly allowing <strong class="inline">kubectl exec</strong> is a serious security risk, as it lets the Kubernetes operator execute commands directly in the pods they have access to. Make sure that only a subset of operators has the ability to use the <strong class="inline">kubectl exec</strong> command. You can use role-based access control to manage this access restriction, as you'll learn in <em class="italics">Chapter 8, Role-based access control in AKS</em>.</li>
				<li>Anything printed to <strong class="inline">stdout</strong> and <strong class="inline">stderr</strong> shows up in the logs (independent of the application/language/logging framework).</li>
			</ul>
			<p>In this section, you introduced an application error to the guestbook application and leveraged Kubernetes logs to pinpoint the issue in the code. In the next section, you will learn about a powerful mechanism in Kubernetes called <strong class="bold">readiness</strong> and <strong class="bold">liveness probes</strong>.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor089"/>Readiness and liveness probes</h2>
			<p>Readiness and liveness probes were briefly touched upon in the previous section. In this section, you'll explore them in more depth.</p>
			<p>Kubernetes uses liveness and readiness probes to monitor the availability of your applications. Each probe serves a different purpose:</p>
			<ul>
				<li>A <strong class="bold">liveness probe</strong> monitors the availability of an application while it is running. If a liveness probe fails, Kubernetes will restart your pod. This could be useful to catch deadlocks, infinite loops, or just a "stuck" application.</li>
				<li>A <strong class="bold">readiness probe</strong> monitors when your application becomes available. If a readiness probe fails, Kubernetes will not send any traffic to the unready pods. This is useful if your application has to go through some configuration before it becomes available, or if your application has become overloaded but is recovering from the additional load. By having a readiness probe fail, your application will temporarily not get any more traffic, giving it the ability to recover from the increased load.</li>
			</ul>
			<p>Liveness and readiness probes don't need to be served from the same endpoint in your application. If you have a smart application, that application could take itself out of rotation (meaning no more traffic is sent to the application) while still being healthy. To achieve this, it would have the readiness probe fail but have the liveness probe remain active.</p>
			<p>Let's build this out in an example. You will create two nginx deployments, each with an index page and a health page. The index page will serve as the liveness probe.</p>
			<h3 id="_idParaDest-89"><a id="_idTextAnchor090"/>Building two web containers</h3>
			<p>For this example, you'll use a couple of web pages that will be used to connect to a readiness and a liveness probe. The files are present in the code files for this chapter. Let's first create <strong class="inline">index1.html</strong>:</p>
			<p class="snippet">&lt;!DOCTYPE html&gt;</p>
			<p class="snippet">&lt;html&gt;</p>
			<p class="snippet">  &lt;head&gt;</p>
			<p class="snippet">    &lt;title&gt;Server 1&lt;/title&gt;</p>
			<p class="snippet">  &lt;/head&gt;</p>
			<p class="snippet">  &lt;body&gt;</p>
			<p class="snippet">    Server 1</p>
			<p class="snippet">  &lt;/body&gt;</p>
			<p class="snippet">&lt;/html&gt;</p>
			<p>After that, create <strong class="inline">index2.html</strong>:</p>
			<p class="snippet">&lt;!DOCTYPE html&gt;</p>
			<p class="snippet">&lt;html&gt;</p>
			<p class="snippet">  &lt;head&gt;</p>
			<p class="snippet">    &lt;title&gt;Server 2&lt;/title&gt;</p>
			<p class="snippet">  &lt;/head&gt;</p>
			<p class="snippet">  &lt;body&gt;</p>
			<p class="snippet">    Server 2</p>
			<p class="snippet">  &lt;/body&gt;</p>
			<p class="snippet">&lt;/html&gt;</p>
			<p>Let's also create a health page, <strong class="inline">healthy.html</strong>:</p>
			<p class="snippet">&lt;!DOCTYPE html&gt;</p>
			<p class="snippet">&lt;html&gt;</p>
			<p class="snippet">  &lt;head&gt;</p>
			<p class="snippet">    &lt;title&gt;All is fine here&lt;/title&gt;</p>
			<p class="snippet">  &lt;/head&gt;</p>
			<p class="snippet">  &lt;body&gt;</p>
			<p class="snippet">    OK</p>
			<p class="snippet">  &lt;/body&gt;</p>
			<p class="snippet">&lt;/html&gt;</p>
			<p>In the next step, you'll mount these files to your Kubernetes deployments. To do this, you'll turn each of these into a <strong class="inline">configmap</strong> that you will connect to your pods. You've already learned about configmaps in <em class="italics">Chapter 3, Application deployment on AKS</em>. Use the following commands to create the <strong class="inline">configmap</strong>:</p>
			<p class="snippet">kubectl create configmap server1 --from-file=index1.html</p>
			<p class="snippet">kubectl create configmap server2 --from-file=index2.html</p>
			<p class="snippet">kubectl create configmap healthy --from-file=healthy.html</p>
			<p>With that out of the way, you can go ahead and create your two web deployments. Both will be very similar, with just the <strong class="inline">configmap</strong> changing. The first deployment file (<strong class="inline">webdeploy1.yaml</strong>) looks like this:</p>
			<p class="snippet">1   apiVersion: apps/v1</p>
			<p class="snippet">2   kind: Deployment</p>
			<p class="snippet">...</p>
			<p class="snippet">17     spec:</p>
			<p class="snippet">18       containers:</p>
			<p class="snippet">19         - name: nginx-1</p>
			<p class="snippet">20           image: nginx:1.19.6-alpine</p>
			<p class="snippet">21           ports:</p>
			<p class="snippet">22             - containerPort: 80</p>
			<p class="snippet">23           livenessProbe:</p>
			<p class="snippet">24             httpGet:</p>
			<p class="snippet">25               path: /healthy.html</p>
			<p class="snippet">26               port: 80</p>
			<p class="snippet">27             initialDelaySeconds: 3</p>
			<p class="snippet">28             periodSeconds: 3</p>
			<p class="snippet">29           readinessProbe:</p>
			<p class="snippet">30             httpGet:</p>
			<p class="snippet">31               path: /index.html</p>
			<p class="snippet">32               port: 80</p>
			<p class="snippet">33             initialDelaySeconds: 3</p>
			<p class="snippet">34             periodSeconds: 3</p>
			<p class="snippet">35           volumeMounts:</p>
			<p class="snippet">36             - name: html</p>
			<p class="snippet">37               mountPath: /usr/share/nginx/html</p>
			<p class="snippet">38             - name: index</p>
			<p class="snippet">39               mountPath: /tmp/index1.html</p>
			<p class="snippet">40               subPath: index1.html</p>
			<p class="snippet">41             - name: healthy</p>
			<p class="snippet">42               mountPath: /tmp/healthy.html</p>
			<p class="snippet">43               subPath: healthy.html</p>
			<p class="snippet">44           command: ["/bin/sh", "-c"]</p>
			<p class="snippet">45           args: ["cp /tmp/index1.html /usr/share/nginx/html/index.html; cp /tmp/healthy.html /usr/share/nginx/html/healthy.html; nginx; sleep inf"]</p>
			<p class="snippet">46       volumes:</p>
			<p class="snippet">47         - name: index</p>
			<p class="snippet">48           configMap:</p>
			<p class="snippet">49             name: server1</p>
			<p class="snippet">50         - name: healthy</p>
			<p class="snippet">51           configMap:</p>
			<p class="snippet">52             name: healthy</p>
			<p class="snippet">53         - name: html</p>
			<p class="snippet">54           emptyDir: {}</p>
			<p>There are a few things to highlight in this deployment:</p>
			<ul>
				<li><strong class="bold">Lines 23-28</strong>: This is the liveness probe. The liveness probe points to the health page. Remember, if the health page fails, the container will restart.</li>
				<li><strong class="bold">Lines 29-32</strong>: This is the readiness probe. The readiness probe in our case points to the index page. If this page fails, the pod will temporarily not be sent any traffic but will remain running.</li>
				<li><strong class="bold">Lines 44-45</strong>: These two lines contain a couple of commands that get executed when the container starts. Instead of simply running the nginx server, this copies the index and ready files in the right location, then starts nginx, and then uses a <strong class="inline">sleep</strong> command (so the container keeps running).</li>
			</ul>
			<p>You can create this deployment using the following command. You can also deploy the second version for <strong class="inline">server 2</strong>, which is similar to <strong class="inline">server 1</strong>:</p>
			<p class="snippet">kubectl create -f webdeploy1.yaml</p>
			<p class="snippet">kubectl create -f webdeploy2.yaml</p>
			<p>Finally, you can also create a service (<strong class="inline">webservice.yaml</strong>) that routes traffic to both deployments:</p>
			<p class="snippet">1   apiVersion: v1</p>
			<p class="snippet">2   kind: Service</p>
			<p class="snippet">3   metadata:</p>
			<p class="snippet">4     name: web</p>
			<p class="snippet">5   spec:</p>
			<p class="snippet">6     selector:</p>
			<p class="snippet">7       app: web-server</p>
			<p class="snippet">8     ports:</p>
			<p class="snippet">9     - protocol: TCP</p>
			<p class="snippet">10     port: 80</p>
			<p class="snippet">11     targetPort: 80</p>
			<p class="snippet">12   type: LoadBalancer</p>
			<p>You can create that service using the following:</p>
			<p class="snippet">kubectl create -f webservice.yaml</p>
			<p>You now have the application up and running. In the next section, you'll introduce some failures to verify the behavior of the liveness and readiness probes.</p>
			<h3 id="_idParaDest-90"><a id="_idTextAnchor091"/>Experimenting with liveness and readiness probes</h3>
			<p>In the previous section, the functionality of the liveness and readiness probes was explained, and you created a sample application. In this section, you will introduce errors in this application and verify the behavior of the liveness and readiness probes. You will see how a failure of the readiness probe will cause the pod to remain running but no longer accept traffic. After that, you will see how a failure of the liveness probe will cause the pod to be restarted.</p>
			<p>Let's start by failing the readiness probe.</p>
			<h3>Failing the readiness probe causes traffic to temporarily stop</h3>
			<p>Now that you have a simple application up and running, you can experiment with the behavior of the liveness and readiness probes. To start, let's get the service's external IP to connect to our web server using the browser:</p>
			<p class="snippet">kubectl get service</p>
			<p>If you hit the external IP in the browser, you should see a single line that either says <strong class="bold">Server 1</strong> or <strong class="bold">Server 2</strong>:</p>
			<div>
				<div id="_idContainer201" class="IMG---Figure">
					<img src="image/B17338_07_17.jpg" alt="Browsing to the external IP in the browser shows the application returning traffic from server 1 "/>
				</div>
			</div>
			<p class="figure">Figure 7.17: Our application is returning traffic from server 1</p>
			<p>During the upcoming tests, you'll use a small script called <strong class="inline">testWeb.sh</strong> that has been provided in the code samples for this chapter to connect to your web page 50 times, so you can monitor a good distribution of results between servers 1 and 2. You'll first need to make that script executable, and then you can run that script while your deployment is fully healthy:</p>
			<p class="snippet">chmod +x testWeb.sh</p>
			<p class="snippet">./testWeb.sh &lt;external-ip&gt;</p>
			<p>During healthy operations, we can see that server 1 and server 2 are hit almost equally, with <strong class="inline">24</strong> hits for server 1 and <strong class="inline">26</strong> for server 2:</p>
			<div>
				<div id="_idContainer202" class="IMG---Figure">
					<img src="image/B17338_07_18.jpg" alt="Output displaying a healthy application with its traffic load-balanced between server 1 and server 2"/>
				</div>
			</div>
			<p class="figure">Figure 7.18: While the application is healthy, traffic is load-balanced between server 1 and server 2</p>
			<p>Let's now move ahead and fail the readiness probe in server 1. To do this, you will use the <strong class="inline">kubectl exec</strong> command to move the index file to a different location:</p>
			<p class="snippet">kubectl get pods #note server1 pod name</p>
			<p class="snippet">kubectl exec &lt;server1 pod name&gt; -- \</p>
			<p class="snippet">  mv /usr/share/nginx/html/index.html \</p>
			<p class="snippet">  /usr/share/nginx/html/index1.html</p>
			<p>Once this is executed, we can view the change in the pod status with the following command:</p>
			<p class="snippet">kubectl get pods -w</p>
			<p>You should see the readiness state of the server 1 pod change to <strong class="inline">0/1</strong>, as shown in <em class="italics">Figure 7.19</em>:</p>
			<div>
				<div id="_idContainer203" class="IMG---Figure">
					<img src="image/B17338_07_19.jpg" alt="First, a command is executed to stop directing traffic to server 1. Then, using kubectl get pods -w, the ready attribute of the server 1 pod changes from 1/1 to 0/1"/>
				</div>
			</div>
			<p class="figure">Figure 7.19: The failing readiness probes causes server 1 to not have any READY containers</p>
			<p>This should direct no more traffic to the server 1 pod. Let's verify that:</p>
			<p class="snippet">./testWeb.sh &lt;external-ip&gt;</p>
			<p>Traffic should be redirected to server 2:</p>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="image/B17338_07_20.jpg" alt="Output displaying all traffic is directed to server 2"/>
				</div>
			</div>
			<p class="figure">Figure 7.20: All traffic is now served by server 2</p>
			<p>You can now restore the state of server 1 by moving the file back to its rightful place:</p>
			<p class="snippet">kubectl exec &lt;server1 pod name&gt; -- mv \</p>
			<p class="snippet">  /usr/share/nginx/html/index1.html \</p>
			<p class="snippet">  /usr/share/nginx/html/index.html</p>
			<p>This will return the pod to a <strong class="bold">Ready</strong> state and should again split traffic equally:</p>
			<p class="snippet">./testWeb.sh &lt;external-ip&gt;</p>
			<p>This will show an output similar to <em class="italics">Figure 7.21</em>:</p>
			<div>
				<div id="_idContainer205" class="IMG---Figure">
					<img src="image/B17338_07_21.jpg" alt="After restoring the readiness probe, traffic is load-balanced again"/>
				</div>
			</div>
			<p class="figure">Figure 7.21: Restoring the readiness probe causes traffic to be load-balanced again</p>
			<p>A failing readiness probe will cause Kubernetes to no longer send traffic to the failing pod. You have verified this by causing a readiness probe in your example application to fail. In the next section, you'll explore the impact of a failing liveness probe.</p>
			<h3>A failing liveness probe restarts the pod</h3>
			<p>You can repeat the previous process with the liveness probe as well. When the liveness probe fails, Kubernetes is expected to restart that pod. Let's try this by deleting the health file:</p>
			<p class="snippet">kubectl exec &lt;server 2 pod name&gt; -- \</p>
			<p class="snippet">  rm /usr/share/nginx/html/healthy.html</p>
			<p>Let's see what this does to the pod:</p>
			<p class="snippet">kubectl get pods -w</p>
			<p>You should see that the pod restarts within a couple of seconds:</p>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="image/B17338_07_22.jpg" alt="Output displaying a failing liveness probe that causes the pod to restart"/>
				</div>
			</div>
			<p class="figure">Figure 7.22: A failing liveness probe will cause the pod to be restarted</p>
			<p>As you can see in <em class="italics">Figure 7.22</em>, the pod was successfully restarted, with limited impact. You can inspect what was going on in the pod by running a <strong class="inline">describe</strong> command:</p>
			<p class="snippet">kubectl describe pod &lt;server2 pod name&gt;</p>
			<p>The preceding command will give you an output similar to <em class="italics">Figure 7.23</em>:</p>
			<div>
				<div id="_idContainer207" class="IMG---Figure">
					<img src="image/B17338_07_23.jpg" alt="More details on the pod showing the failing liveness probe caused the pod to be restarted"/>
				</div>
			</div>
			<p class="figure">Figure 7.23: More details on the pod showing how the liveness probe failed</p>
			<p>In the <strong class="inline">describe</strong> command, you can clearly see that the pod failed the liveness probe. After three failures, the container was killed and restarted.</p>
			<p>This concludes the experiment with liveness and readiness probes. Remember that both are useful for your application: a readiness probe can be used to temporarily stop traffic to your pod, so it has to deal with less load. A liveness probe is used to restart your pod if there is an actual failure in the pod.</p>
			<p>Let's also make sure to clean up the deployments you just created:</p>
			<p class="snippet">kubectl delete deployment server1 server2</p>
			<p class="snippet">kubectl delete service web</p>
			<p>Liveness and readiness probes are useful to ensure that only healthy pods will receive traffic in your cluster. In the next section, you will explore different metrics reported by Kubernetes that you can use to verify the state of your application.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor092"/>Metrics reported by Kubernetes</h2>
			<p>Kubernetes reports multiple metrics. In this section, you'll first use a number of <strong class="inline">kubectl</strong> commands to get these metrics. Afterward, you'll look into Azure Monitor for containers to see how Azure helps with container monitoring.</p>
			<h3 id="_idParaDest-92"><a id="_idTextAnchor093"/>Node status and consumption</h3>
			<p>The nodes in your Kubernetes are the servers running your application. Kubernetes will schedule pods to different nodes in the cluster. You need to monitor the status of your nodes to ensure that the nodes themselves are healthy and that the nodes have enough resources to run new applications.</p>
			<p>Run the following command to get information about the nodes on the cluster:</p>
			<p class="snippet">kubectl get nodes</p>
			<p>The preceding command lists their name, status, and age:</p>
			<div>
				<div id="_idContainer208" class="IMG---Figure">
					<img src="image/B17338_07_24.jpg" alt="Running the kubectl get nodes command to get information about the nodes on the cluster"/>
				</div>
			</div>
			<p class="figure">Figure 7.24: There are two nodes in this cluster</p>
			<p>You can get more information by passing the <strong class="inline">-o wide</strong> option:</p>
			<p class="snippet">kubectl get -o wide nodes</p>
			<p>The output lists the underlying <strong class="inline">OS-IMAGE</strong> and <strong class="inline">INTERNAL-IP</strong>, and other useful information, which can be viewed in <em class="italics">Figure 7.25</em>:</p>
			<div>
				<div id="_idContainer209" class="IMG---Figure">
					<img src="image/B17338_07_25.jpg" alt="Adding the -o wide option to the command to display more details about the nodes"/>
				</div>
			</div>
			<p class="figure">Figure 7.25: Using -o wide adds more details about the nodes</p>
			<p>You can find out which nodes are consuming the most resources by using the following command:</p>
			<p class="snippet">kubectl top nodes</p>
			<p>It shows the CPU and memory usage of the nodes:</p>
			<div>
				<div id="_idContainer210" class="IMG---Figure">
					<img src="image/B17338_07_26.jpg" alt="Output displaying CPU and memory utilization of the nodes"/>
				</div>
			</div>
			<p class="figure">Figure 7.26: CPU and memory utilization of the nodes</p>
			<p>Note that this is the actual consumption at that point in time, not the number of requests a certain node has. To get the requests, you can execute the following:</p>
			<p class="snippet">kubectl describe node &lt;node name&gt;</p>
			<p>This will show you the requests and limits per pod, as well as the cumulative amount for the whole node:</p>
			<div>
				<div id="_idContainer211" class="IMG---Figure">
					<img src="image/B17338_07_27.jpg" alt="Output displaying requests and limits per pod, as well as the total of allocated resources"/>
				</div>
			</div>
			<p class="figure">Figure 7.27: Describing the nodes shows details on requests and limits</p>
			<p>As you can see in <em class="italics">Figure 7.27</em>, the <strong class="inline">describe node</strong> command outputs the requests and limits per pod, across namespaces. This is a good way for cluster operators to verify how much load is being put on the cluster, across all namespaces.</p>
			<p>You now know where you can find information about the utilization of your nodes. In the next section, you will look into how you can get the same metrics for individual pods.</p>
			<h3 id="_idParaDest-93"><a id="_idTextAnchor094"/>Pod consumption</h3>
			<p>Pods consume CPU and memory resources from an AKS cluster. Requests and limits are used to configure how much CPU and memory a pod can consume. Requests are used to reserve a minimum amount of CPU and memory, while limits are used to set a maximum amount of CPU and memory per pod.</p>
			<p>In this section, you will learn how you can use <strong class="inline">kubectl</strong> to get information about the CPU and memory utilization of pods.</p>
			<p>Let's start by exploring how you can see the requests and limits for a pod that you currently have running:</p>
			<ol>
				<li value="1">For this example, you will use the pods running in the <strong class="inline">kube-system</strong> namespace. Get all the pods in this namespace:<p class="snippet">kubectl get pods -n kube-system</p><p>This should show something similar to <em class="italics">Figure 7.28</em>:</p><div id="_idContainer212" class="IMG---Figure"><img src="image/B17338_07_28.jpg" alt="Output displaying pods running in the kube-system namespace"/></div><p class="figure">Figure 7.28: The pods running in the kube-system namespace</p></li>
				<li>Let's get the requests and limits for one of the <strong class="inline">coredns</strong> pods. This can be done using the <strong class="inline">describe</strong> command:<p class="snippet">kubectl describe pod coredns-&lt;pod id&gt; -n kube-system</p><p>In the <strong class="inline">describe</strong> command, there should be a section similar to <em class="italics">Figure 7.29</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer213" class="IMG---Figure">
					<img src="image/B17338_07_29.jpg" alt="Displaying the limits and requests for the CoreDNS pod"/>
				</div>
			</div>
			<p class="figure">Figure 7.29: Limits and requests for the CoreDNS pod</p>
			<p>This shows you that this pod has a memory limit of <strong class="inline">170Mi</strong>, no CPU limit, and has a request for 100 m CPU (which means 0.1 CPU) and <strong class="inline">70Mi</strong> of memory. This means that if this pod were to consume more than 170 MiB of memory, Kubernetes would restart that pod. Kubernetes has also reserved 0.1 CPU core and 70 MiB of memory for this pod.</p>
			<p>Requests and limits are used to perform capacity management in a cluster. You can also get the actual CPU and memory consumption of a pod. Run the following command and you'll get the actual pod consumption in all namespaces:</p>
			<p class="snippet">kubectl top pods --all-namespaces</p>
			<p>This should show you anoutput similar to <em class="italics">Figure 7.30</em>:</p>
			<div>
				<div id="_idContainer214" class="IMG---Figure">
					<img src="image/B17338_07_30.jpg" alt="Running a command to get the actual pod consumption in all namespaces"/>
				</div>
			</div>
			<p class="figure">Figure 7.30: Seeing the CPU and memory consumption of pods</p>
			<p>Using the <strong class="inline">kubectl top</strong> command shows the CPU and memory consumption at the point in time when the command was run. In this case, you can see that the <strong class="inline">coredns</strong> pods are using <strong class="inline">3m</strong> CPU and <strong class="inline">10Mi</strong> of memory.</p>
			<p>In this section, you have used the <strong class="inline">kubectl</strong> command to get an insight into the resource utilization of the nodes and pods in your cluster. This is useful information, but it is limited to that specific point in time. In the next section, you'll use the Azure portal to get more detailed information on the cluster and the applications on top of the cluster. You'll start by exploring the <strong class="bold">AKS Diagnostics</strong> pane.</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor095"/>Using AKS Diagnostics</h2>
			<p>When you are experiencing issues in AKS, a good place to start your exploration is the <strong class="bold">AKS Diagnostics</strong> pane. It provides you with tools that help investigate any issues related to underlying infrastructure or system cluster components. </p>
			<h4>Note:</h4>
			<p class="callout">AKS Diagnostics is in preview at the time of writing this book. This means functionality might be added or removed.</p>
			<p>To access AKS Diagnostics, hit the <strong class="bold">Diagnose and solve problems</strong> option in the AKS menu. This will open up Diagnostics, as shown in <em class="italics">Figure 7.31</em>:</p>
			<div>
				<div id="_idContainer215" class="IMG---Figure">
					<img src="image/B17338_07_31.jpg" alt="An overview of the AKS Diagnostics pane in the Azure portal"/>
				</div>
			</div>
			<p class="figure">Figure 7.31: Accessing AKS Diagnostics</p>
			<p>AKS Diagnostics gives you two tools to diagnose and explore issues. One is <strong class="bold">Cluster Insights</strong>, and the other is <strong class="bold">Networking</strong>. Cluster Insights uses cluster logs and configuration on your cluster to perform a health check and compare your cluster against best practices. It contains useful information and relevant health indicators in case anything is misconfigured in your cluster. An example output of Cluster Insights is shown in <em class="italics">Figure 7.32</em>:</p>
			<div>
				<div id="_idContainer216" class="IMG---Figure">
					<img src="image/B17338_07_32.jpg" alt="Example output of Cluster Insights as seen in the Azure portal"/>
				</div>
			</div>
			<p class="figure">Figure 7.32: Example output from Cluster Insights</p>
			<p>The <span class="P---Screen-Text">Networking</span> section of AKS Diagnostics allows you to interactively troubleshoot networking issues in your cluster. As you open the <span class="P---Screen-Text">Networking</span> view, you are presented with several questions that will then trigger network health checks and configuration reviews. Once you select one of those options, the interactive tool will give you the output from those checks, as shown in <em class="italics">Figure 7.33</em>:</p>
			<div>
				<div id="_idContainer217" class="IMG---Figure">
					<img src="image/B17338_07_33.jpg" alt="Diagnosing networking issues using AKS Diagnostics"/>
				</div>
			</div>
			<p class="figure">Figure 7.33: Diagnosing networking issues using AKS Diagnostics</p>
			<p>Using AKS Diagnostics is very useful when you are facing infrastructure issues on your cluster. The tool does a scan of your environment and verifies whether everything is running and configured well. However, it does not scan your applications. That is where Azure Monitor comes in; it allows you to monitor your application and access your application logs.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor096"/>Azure Monitor metrics and logs</h2>
			<p>Previously in this chapter, you explored the status and metrics of nodes and pods in your cluster using the <strong class="inline">kubectl</strong> command-line tool. In Azure, you can get more metrics from nodes and pods and explore the logs from pods in your cluster. Let's start by exploring AKS Insights in the Azure portal.</p>
			<h3 id="_idParaDest-96"><a id="_idTextAnchor097"/>AKS Insights</h3>
			<p>The <strong class="bold">Insights</strong> section of the AKS pane provides most of the metrics you need to know about your cluster. It also has the ability to drill down to the container level. You can also see the logs of the container.</p>
			<h4>Note:</h4>
			<p class="callout">The Insights section of the AKS pane relies on Azure Monitor for containers. If you created the cluster using the portal defaults, this is enabled by default.</p>
			<p>Kubernetes makes metrics available but doesn't store them. Azure Monitor can be used to store these metrics and make them available to query over time. To collect the relevant metrics and logs into Insights, Azure connects to the Kubernetes API to collect the metrics and logs to then store them in Azure Monitor.</p>
			<h4>Note:</h4>
			<p class="callout">Logs of a container could contain sensitive information. Therefore, the rights to review logs should be controlled and audited.</p>
			<p>Let's explore the <strong class="bold">Insights</strong> tab of the AKS pane, starting with the cluster metrics.</p>
			<h3>Cluster metrics</h3>
			<p><strong class="bold">Insights</strong> shows the cluster metrics. <em class="italics">Figure 7.34</em> shows the CPU utilization and the memory utilization of all the nodes in the cluster. You can optionally add additional filters to filter to a particular namespace, node, or node pool. There also is a live option, which gives you more real-time information on your cluster status:</p>
			<div>
				<div id="_idContainer218" class="IMG---Figure">
					<img src="image/B17338_07_34.jpg" alt="Viewing CPU and memory utilization for the cluster in the Cluster tab"/>
				</div>
			</div>
			<p class="figure">Figure 7.34: The Cluster tab shows CPU and memory utilization for the cluster</p>
			<p>The cluster metrics also show the node count and the number of active pods. The node count is important, as you can track whether you have any nodes that are in a <strong class="bold">Not Ready</strong> state:</p>
			<div>
				<div id="_idContainer219" class="IMG---Figure">
					<img src="image/B17338_07_35.jpg" alt="Checking the node count and the number of active pods in the Cluster tab"/>
				</div>
			</div>
			<p class="figure">Figure 7.35: The Cluster tab shows the node count and the number of active pods</p>
			<p>The <strong class="bold">Cluster</strong> tab can be used to monitor the status of the nodes in the cluster. Next, you'll explore the <strong class="bold">Reports</strong> tab.</p>
			<h3>Reports</h3>
			<p>The <strong class="bold">Reports</strong> tab in AKS Insights gives you access to a number of preconfigured monitoring workbooks. These workbooks combine text, log queries, metrics, and parameters together and give you rich interactive reports. You can drill down into each individual report to get more information and prebuilt log queries. The available reports are shown in <em class="italics">Figure 7.36</em>:</p>
			<h4>Note</h4>
			<p class="callout">The Reports functionality is in preview at the time of writing this book.</p>
			<div>
				<div id="_idContainer220" class="IMG---Figure">
					<img src="image/B17338_07_36.jpg" alt="An overview of the Reports tab that allows access to preconfigured monitoring workbooks"/>
				</div>
			</div>
			<p class="figure">Figure 7.36: The Reports tab gives you access to preconfigured monitoring workbooks</p>
			<p>As an example, you can explore the <strong class="bold">Deployments</strong> workbook. This is shown in <em class="italics">Figure 7.37</em>:</p>
			<div>
				<div id="_idContainer221" class="IMG---Figure">
					<img src="image/B17338_07_37.jpg" alt="An overview of the Deployments workbook, which shows the status of all the deployments"/>
				</div>
			</div>
			<p class="figure">Figure 7.37: The Deployments workbook shows you the status of your deployments</p>
			<p>This shows you all the deployments by default, their health, and up-to-date status. As you can see, it shows you that <strong class="bold">server1</strong> was temporarily unavailable when you were doing the exploration with liveness and readiness probes earlier in this chapter.</p>
			<p>You can drill down further into the status of the individual deployments. If you click on the <strong class="bold">Log</strong> button highlighted in <em class="italics">Figure 7.37</em>, you get redirected to Log Analytics with a prebuilt query. You can then modify this query and get deeper insights into your workload, as shown in <em class="italics">Figure 7.38</em>.</p>
			<div>
				<div id="_idContainer222" class="IMG---Figure">
					<img src="image/B17338_07_38.jpg" alt="Drilling down in Log Analytics to get more details on the deployments"/>
				</div>
			</div>
			<p class="figure">Figure 7.38: Drilling down in Log Analytics to get more details on your deployments</p>
			<h4>Note:</h4>
			<p class="callout">The queries used in Log Analytics make use of the <strong class="bold">Kusto Query Language </strong>(<strong class="bold">KQL</strong>). To learn more about KQL, please refer to the documentation: <a href="https://docs.microsoft.com/azure/data-explorer/kusto/concepts/">https://docs.microsoft.com/azure/data-explorer/kusto/concepts/</a> </p>
			<p>The <strong class="bold">Reports</strong> tab in AKS Insights gives you a number of prebuilt monitoring workbooks. The next tab is the <strong class="bold">Nodes</strong> tab.</p>
			<h3>Nodes</h3>
			<p>The <strong class="bold">Nodes</strong> view shows you detailed metrics for your nodes. It also shows you which pods are running on each node, as you can see in <em class="italics">Figure 7.39</em>:</p>
			<div>
				<div id="_idContainer223" class="IMG---Figure">
					<img src="image/B17338_07_39.jpg" alt="Detailed metrics of the nodes as seen in the Nodes pane"/>
				</div>
			</div>
			<p class="figure">Figure 7.39: Detailed metrics of the nodes in the Nodes pane</p>
			<p>Note that different metrics can be viewed from the dropdown menu right next to the search bar. If you need even more details, you can click through and get Kubernetes event logs from your nodes as well:</p>
			<div>
				<div id="_idContainer224" class="IMG---Figure">
					<img src="image/B17338_07_40.jpg" alt="Clicking on the View Kubernetes event logs option to get the logs from a cluster"/>
				</div>
			</div>
			<p class="figure">Figure 7.40: Click on View Kubernetes event logs to get the logs from a cluster</p>
			<p>This will open Azure Log Analytics and will have pre-created a query for you that shows the logs for your node. In the example in <em class="italics">Figure 7.41</em>, you can see that the node was rebooted a couple of times and hit an <strong class="inline">InvalidDiskCapacity</strong> warning as well:</p>
			<div>
				<div id="_idContainer225" class="IMG---Figure">
					<img src="image/B17338_07_41.jpg" alt="An overview of Log Analytics displaying a pre-created query to show the logs for your node"/>
				</div>
			</div>
			<p class="figure">Figure 7.41: Log Analytics showing the logs for the nodes</p>
			<p>This gives you information about the status of your nodes. Next, you'll explore the <strong class="bold">Controllers</strong> tab.</p>
			<h3>Controllers</h3>
			<p>The <strong class="bold">Controllers</strong> tab shows you details on all the controllers (that is, ReplicaSets, DaemonSets, and so on) on your cluster and the pods running in them. This shows you a controller-centric view of running pods. For instance, you can find the <strong class="bold">server1</strong> ReplicaSet and see all the pods and containers running in it, as shown in <em class="italics">Figure 7.42</em>:</p>
			<div>
				<div id="_idContainer226" class="IMG---Figure">
					<img src="image/B17338_07_42.jpg" alt="An overview of the Controllers tab displaying all the pods running in a ReplicaSet"/>
				</div>
			</div>
			<p class="figure">Figure 7.42: The Controllers tab shows you all the pods running in a ReplicaSet</p>
			<p>The next tab is the <strong class="bold">Containers</strong> tab, which will show you the metrics, logs, and environment variables for a container.</p>
			<h3>Container metrics, logs, and environment variables</h3>
			<p>Clicking on the <strong class="bold">Containers</strong> tab lists the container metrics, environment variables, and access to its logs, as shown in <em class="italics">Figure 7.43</em>:</p>
			<div>
				<div id="_idContainer227" class="IMG---Figure">
					<img src="image/B17338_07_43.jpg" alt="An overview of the Containers tab displaying all the individual containers"/>
				</div>
			</div>
			<p class="figure">Figure 7.43: The Containers tab shows us all the individual containers</p>
			<h4>Note:</h4>
			<p class="callout">You might notice a couple of containers with an <strong class="inline">Unknown</strong> state. If a container in the <strong class="bold">Insights</strong> pane has an <strong class="inline">unknown</strong> status, that is because Azure Monitor has logs and information about that container, but the container is no longer running on the cluster.</p>
			<p>You can get access to the container's logs from this view as well:</p>
			<div>
				<div id="_idContainer228" class="IMG---Figure">
					<img src="image/B17338_07_44.jpg" alt="Clicking the View container logs option to access the logs from the containers tab itself"/>
				</div>
			</div>
			<p class="figure">Figure 7.44: Access the container's logs</p>
			<p>This will show you all the logs that Kubernetes logged from your application. Earlier in the chapter, you used <strong class="inline">kubectl</strong> to get access to container logs. Using this approach can be a lot more productive, as you can edit the log queries and correlate logs from different pods and applications in a single view:</p>
			<div>
				<div id="_idContainer229" class="IMG---Figure">
					<img src="image/B17338_07_45.jpg" alt="Displaying logs that Kubernetes logged from the application in a single view"/>
				</div>
			</div>
			<p class="figure">Figure 7.45: Logs are collected and can be queried</p>
			<p>Apart from the logs, this view also shows the environment variables that are set for the container. To see the environment variables, scroll down in the right cell of the <strong class="bold">Containers</strong> view:</p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<img src="image/B17338_07_46.jpg" alt="Viewing the environment variables set for the container"/>
				</div>
			</div>
			<p class="figure">Figure 7.46: The environment variables set for the container</p>
			<p>The final tab in AKS Insights is the <strong class="bold">Deployments</strong> tab, which you'll explore next.</p>
			<h3>Deployments</h3>
			<p>The final tab is the <strong class="bold">Deployments</strong> tab. This tab gives you an overview of all deployments in the cluster and allows you to get the definition of the deployment by selecting it. As you can see in <em class="italics">Figure 7.47</em>, you can get this view either in <strong class="bold">Describe</strong> (in text format) or in <strong class="bold">RAW</strong> (YAML format):</p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<img src="image/B17338_07_47.jpg" alt="An overview of the Deployments tab in the AKS Insights pane"/>
				</div>
			</div>
			<p class="figure">Figure 7.47: The Deployments tab in AKS Insights</p>
			<p>By using the <strong class="bold">Insights</strong> pane in AKS, you can get detailed information about your cluster. You explored the different tabs in this section and learned how you can drill down and get access to customizable log queries to get even more information.</p>
			<p>And that concludes this section. Let's make sure to clean up all the resources created in this chapter by using the following command:</p>
			<p class="snippet">kubectl delete -f </p>
			<p>In this section, you explored monitoring applications running on top of Kubernetes. You used the AKS <strong class="bold">Insights</strong> tab in the Azure portal to get a detailed view of your cluster and the containers running on the cluster.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor098"/>Summary</h2>
			<p>You started this chapter by learning how to use different <strong class="inline">kubectl</strong> commands to monitor an application. Then, you explored how logs created in Kubernetes can be used to debug that application. The logs contain all the information that is written to <strong class="inline">stdout</strong> and <strong class="inline">stderr</strong>. </p>
			<p>After that, you switched to the Azure portal and started using AKS Diagnostics to explore infrastructure issues. Lastly, you explored the use of Azure Monitor and AKS Insights to show the AKS metrics and environment variables, as well as logs with log filtering. </p>
			<p>In the next chapter, you will learn how to connect an AKS cluster to Azure PaaS services. You will specifically focus on how you can connect an AKS cluster to a MySQL database managed by Azure.</p>
		</div>
	</body></html>