<html><head></head><body>
  <div id="_idContainer147" class="Basic-Text-Frame">
    <h1 class="chapterNumber">4</h1>
    <h1 id="_idParaDest-179" class="chapterTitle">Securing Kubernetes</h1>
    <p class="normal">In <em class="chapterRef">Chapter 3</em>, <em class="italic">High Availability and Reliability</em>, we looked at reliable and highly available Kubernetes clusters, the basic concepts, the best practices, and the many design trade-offs regarding scalability, performance, and cost.</p>
    <p class="normal">In this chapter, we will explore the important topic of security. Kubernetes clusters are complicated systems composed of multiple layers of interacting components. The isolation and compartmentalization of different layers is very important when running critical applications. To secure a system and ensure proper access to resources, capabilities, and data, we must first understand the unique challenges facing Kubernetes as a general-purpose orchestration platform that runs unknown workloads. Then we can take advantage of various securities, isolation, and access control mechanisms to make sure the cluster, the applications running on it, and the data are all safe. We will discuss various best practices and when it is appropriate to use each mechanism.</p>
    <p class="normal">This chapter will explore the following main topics:</p>
    <ul>
      <li class="bulletList">Understanding Kubernetes security challenges</li>
      <li class="bulletList">Hardening Kubernetes</li>
      <li class="bulletList">Running multi-tenant clusters</li>
    </ul>
    <p class="normal">By the end of this chapter, you will have a good understanding of Kubernetes security challenges. You will gain practical knowledge of how to harden Kubernetes against various potential attacks, establishing defense in depth, and will even be able to safely run a multi-tenant cluster while providing different users full isolation as well as full control over their part of the cluster.</p>
    <h1 id="_idParaDest-180" class="heading-1">Understanding Kubernetes security challenges</h1>
    <p class="normal">Kubernetes <a id="_idIndexMarker418"/>is a very flexible system that manages very low-level resources in a generic way. Kubernetes itself can be deployed on many <a id="_idIndexMarker419"/>operating systems and hardware or virtual machine solutions, on-premises, or in the cloud. Kubernetes runs workloads implemented by runtimes it interacts with through a well-defined runtime interface, but without understanding how they are implemented. Kubernetes manipulates critical resources such as networking, DNS, and resource allocation on behalf of or in service of applications it knows nothing about. </p>
    <p class="normal">This means that Kubernetes is faced with the difficult task of providing good security mechanisms and capabilities in a way that application developers and cluster administrators can utilize, while protecting itself, the developers, and the administrators from common mistakes.</p>
    <p class="normal">In this section, we will discuss security challenges in several layers or components of a Kubernetes cluster: nodes, network, images, pods, and containers. Defense in depth is an important security concept that requires systems to protect themselves at each level, both to mitigate attacks that penetrate other layers and to limit the scope and damage of a breach. Recognizing the challenges in each layer is the first step toward defense in depth.</p>
    <p class="normal">This is often described as the 4 Cs of cloud-native security:</p>
    <figure class="mediaobject"><img src="../Images/B18998_04_01.png" alt="Table  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 4.1: The 4 Cs of cloud-native security</p>
    <p class="normal">However, the 4 Cs model is a <a id="_idIndexMarker420"/>coarse-grained approach for security. Another approach is building a threat <a id="_idIndexMarker421"/>model based on the security challenges across different dimensions, such as:</p>
    <ul>
      <li class="bulletList">Node challenges</li>
      <li class="bulletList">Network challenges</li>
      <li class="bulletList">Image challenges</li>
      <li class="bulletList">Deployment and configuration challenges</li>
      <li class="bulletList">Pod and container challenges</li>
      <li class="bulletList">Organizational, cultural, and process challenges</li>
    </ul>
    <p class="normal">Let’s examine these challenges.</p>
    <h2 id="_idParaDest-181" class="heading-2">Node challenges</h2>
    <p class="normal">The nodes <a id="_idIndexMarker422"/>are the hosts of the runtime engines. If an attacker gets access to a node, this is a serious threat. They can control at least the host itself and all the workloads running on it. But it gets worse. </p>
    <p class="normal">The node has a kubelet running that talks to the API server. A sophisticated attacker can replace the kubelet with a modified version and effectively evade detection by communicating normally with the Kubernetes API server while running their own workloads instead of the scheduled workloads, collecting information about the overall cluster, and disrupting the API server and the rest of the cluster by sending malicious messages. The node will have access to shared resources and to secrets that may allow the attacker to infiltrate even deeper. A node breach is very serious, because of both the possible damage and the difficulty of detecting it after the fact.</p>
    <p class="normal">Nodes can be compromised at the physical level too. This is more relevant on bare-metal machines where you can tell which hardware is assigned to the Kubernetes cluster.</p>
    <p class="normal">Another attack vector is resource drain. Imagine that your nodes become part of a bot network that, unrelated to your Kubernetes cluster, just runs its own workloads like cryptocurrency mining and drains CPU and memory. The danger here is that your cluster will choke and run out of resources to run your workloads or, alternatively, your infrastructure may scale automatically and allocate more resources.</p>
    <p class="normal">Another problem is the installation of debugging and troubleshooting tools or modifying the configuration outside of an automated deployment. Those are typically untested and, if left behind and active, can lead to at least degraded performance, but can also cause more sinister problems. At the least, it increases the attack surface.</p>
    <p class="normal">Where <a id="_idIndexMarker423"/>security is concerned, it’s a numbers game. You want to understand the attack surface of the system and where you’re vulnerable. Let’s list some possible node challenges:</p>
    <ul>
      <li class="bulletList">An attacker takes control of the host</li>
      <li class="bulletList">An attacker replaces the kubelet</li>
      <li class="bulletList">An attacker takes control of a node that runs master components (such as the API server, scheduler, or controller manager)</li>
      <li class="bulletList">An attacker gets physical access to a node</li>
      <li class="bulletList">An attacker drains resources unrelated to the Kubernetes cluster</li>
      <li class="bulletList">Self-inflicted damage occurs through the installation of debugging and troubleshooting tools or a configuration change</li>
    </ul>
    <p class="normal">Mitigating node challenges requires several layers of defense such as controlling physical access, preventing privilege escalation, and reducing the attack surface by controlling the OS and software installed on the node.</p>
    <h2 id="_idParaDest-182" class="heading-2">Network challenges</h2>
    <p class="normal">Any <a id="_idIndexMarker424"/>non-trivial Kubernetes cluster spans at least one network. There are many challenges related to networking. You need to understand how your system components are connected at a very fine level. Which components are supposed to talk to each other? What network protocols do they use? What ports? What data do they exchange? How is your cluster connected to the outside world?</p>
    <p class="normal">There is a complex chain of exposing ports and capabilities or services:</p>
    <ul>
      <li class="bulletList">Container to host</li>
      <li class="bulletList">Host to host within the internal network</li>
      <li class="bulletList">Host to the world</li>
    </ul>
    <p class="normal">Using overlay networks (which will be discussed more in <em class="chapterRef">Chapter 10</em>, <em class="italic">Exploring Kubernetes Networking</em>) can help with defense in depth where, even if an attacker gains access to a container, they are sandboxed and can’t escape to the underlay network’s infrastructure.</p>
    <p class="normal">Discovering <a id="_idIndexMarker425"/>components is a big challenge too. There are several options here, such as DNS, dedicated discovery services, and load balancers. Each comes with a set of pros and cons that take careful planning and insight to get right for your situation. Making sure two containers can find each other and exchange information is not trivial.</p>
    <p class="normal">You need to decide which resources and endpoints should be publicly accessible. Then you need to come up with a proper way to authenticate users, services, and authorize them to operate on resources. Often you may want to control access between internal services too.</p>
    <p class="normal">Sensitive data must be encrypted on the way into and out of the cluster and sometimes at rest, too. That means key management and safe key exchange, which is one of the most difficult problems to solve in security.</p>
    <p class="normal">If your cluster shares networking infrastructure with other Kubernetes clusters or non-Kubernetes processes, then you have to be diligent about isolation and separation.</p>
    <p class="normal">The ingredients are network policies, firewall rules, and <strong class="keyWord">software-defined networking</strong> (<strong class="keyWord">SDN</strong>). The recipe is often customized. This is especially challenging with on-premises and bare-metal clusters. Here are some of the network challenges you will face:</p>
    <ul>
      <li class="bulletList">Coming up with a connectivity plan</li>
      <li class="bulletList">Choosing components, protocols, and ports</li>
      <li class="bulletList">Figuring out dynamic discovery</li>
      <li class="bulletList">Public versus private access</li>
      <li class="bulletList">Authentication and authorization (including between internal services)</li>
      <li class="bulletList">Designing firewall rules</li>
      <li class="bulletList">Deciding on a network policy</li>
      <li class="bulletList">Key management and exchange</li>
      <li class="bulletList">Encrypted communication</li>
    </ul>
    <p class="normal">There is a constant tension between making it easy for containers, users, and services to find and talk to each other at the network level versus locking down access and preventing attacks through the network or attacks on the network itself.</p>
    <p class="normal">Many of these challenges are not Kubernetes-specific. However, the fact that Kubernetes is a generic platform that manages key infrastructure and deals with low-level networking makes it necessary to think about dynamic and flexible solutions that can integrate <a id="_idIndexMarker426"/>system-specific requirements into Kubernetes. These solutions often involve monitoring and automatically injecting firewall rules or applying network policies based on namespaces and pod labels.</p>
    <h2 id="_idParaDest-183" class="heading-2">Image challenges</h2>
    <p class="normal">Kubernetes<a id="_idIndexMarker427"/> runs containers that comply with one of its runtime engines. It has no idea what these containers are doing (except collecting metrics). You can put certain limits on containers via quotas. You can also limit their access to other parts of the network via network policies. But, in the end, containers do need access to host resources, other hosts in the network, distributed storage, and external services. The image determines the behavior of a container. The infamous software supply chain problem is at the core of how these container images are created. There are two categories of problems with images:</p>
    <ul>
      <li class="bulletList">Malicious images</li>
      <li class="bulletList">Vulnerable images</li>
    </ul>
    <p class="normal">Malicious images are images that contain code or configuration that was designed by an attacker to do some harm, collect information, or just take advantage of your infrastructure for their purposes (for example, crypto mining). Malicious code can be injected into your image preparation pipeline, including any image repositories you use. Alternatively, you may install third-party images that were compromised themselves and now contain malicious code.</p>
    <p class="normal">Vulnerable images are images you designed (or third-party images you install) that just happen to contain some vulnerability that allows an attacker to take control of the running container or cause some other harm, including injecting their own code later.</p>
    <p class="normal">It’s hard to tell which category is worse. At the extreme, they are equivalent because they allow seizing total control of the container. The other defenses that are in place (remember defense in depth?) and the restrictions you put on the container will determine how much damage it can do. Minimizing the danger of bad images is very challenging. Fast-moving companies utilizing microservices may generate many images daily. Verifying an image is not an easy task either. In addition, some containers require wide permissions to do their legitimate job. If such a container is compromised it can do extreme damage.</p>
    <p class="normal">The base images that contain the operating system may become vulnerable any time a new vulnerability is discovered. Moreover, if you rely on base images prepared by someone else (very common) then malicious code may find its way into those base images, which you have no control over and you trust implicitly.</p>
    <p class="normal">When a<a id="_idIndexMarker428"/> vulnerability in a third-party dependency is discovered, ideally there is already a fixed version and you should patch it as soon as possible.</p>
    <p class="normal">We can summarize the image challenges that developers are likely to face as follows:</p>
    <ul>
      <li class="bulletList">Kubernetes doesn’t know what containers are doing</li>
      <li class="bulletList">Kubernetes must provide access to sensitive resources for the designated function</li>
      <li class="bulletList">It’s difficult to protect the image preparation and delivery pipeline (including image repositories)</li>
      <li class="bulletList">The speed of development and deployment of new images conflict with the careful review of changes</li>
      <li class="bulletList">Base images that contain the OS or other common dependencies can easily get out of date and become vulnerable</li>
      <li class="bulletList">Base images are often not under your control and might be more prone to the injection of malicious code</li>
    </ul>
    <p class="normal">Integrating a static image analyzer like CoreOS Clair or the Anchore Engine into your CI/CD pipeline can help a lot. In addition, minimizing the blast radius by limiting the resource access of containers only to what they need to perform their job can reduce the impact on your system if a container gets compromised. You must also be diligent about patching known vulnerabilities.</p>
    <h2 id="_idParaDest-184" class="heading-2">Configuration and deployment challenges</h2>
    <p class="normal">Kubernetes <a id="_idIndexMarker429"/>clusters are administered remotely. Various manifests and policies determine the state of the cluster at each point in time. If an attacker gets access to a machine with administrative control over the cluster, they can wreak havoc, such as collecting information, injecting bad images, weakening security, and tampering with logs. As usual, bugs and mistakes can be just as harmful; by neglecting important security measures, you leave the cluster open for attack. It is very common these days for employees with administrative access to the cluster to work remotely from home or from a coffee shop and have their laptops with them, where you are just one kubectl command from opening the floodgates.</p>
    <p class="normal">Let’s reiterate the challenges:</p>
    <ul>
      <li class="bulletList">Kubernetes is administered remotely</li>
      <li class="bulletList">An attacker with remote administrative access can gain complete control over the cluster</li>
      <li class="bulletList">Configuration and deployment is typically more difficult to test than code</li>
      <li class="bulletList">Remote or out-of-office employees risk extended exposure, allowing an attacker to gain access to their laptops or phones with administrative access</li>
    </ul>
    <p class="normal">There are<a id="_idIndexMarker430"/> some best practices to minimize this risk, such as a layer of indirection in the form of a jump box where a developer connects from the outside to a dedicated machine in the cluster with tight controls that manages secure interaction with internal services, requiring a VPN connection (which authenticates and encrypts all communication), and using multi-factor authentication and one-time passwords to protect against trivial password cracking attacks.</p>
    <h2 id="_idParaDest-185" class="heading-2">Pod and container challenges</h2>
    <p class="normal">In <a id="_idIndexMarker431"/>Kubernetes, pods are the unit of work and contain one or more containers. The pod is a grouping and deployment construct. But often, containers that are deployed together in the same pod interact through direct mechanisms. The containers all share the same localhost network and often share mounted volumes from the host. This easy integration between containers in the same pod can result in exposing parts of the host to all the containers. This might allow one bad container (either malicious or just vulnerable) to open the way for an escalated attack on other containers in the pod, later taking over the node itself and the entire cluster. Control plane add-ons are often collocated with control plane components and present that kind of danger, especially because many of them are experimental. The same goes for DaemonSets that run pods on every node. The practice of sidecar containers where additional containers are deployed in a pod along with your application container is very popular, especially with service meshes. This increases that risk because the sidecar containers are often outside your control, and if compromised, can provide access to your infrastructure.</p>
    <p class="normal">Multi-container pod challenges include the following:</p>
    <ul>
      <li class="bulletList">The same pod containers share the localhost network</li>
      <li class="bulletList">The same pod containers sometimes share a mounted volume on the host filesystem</li>
      <li class="bulletList">Bad containers might poison other containers in the pod</li>
      <li class="bulletList">Bad containers have an easier time attacking the node if collocated with another container that accesses crucial node resources</li>
      <li class="bulletList">Experimental add-ons that are collocated with master components might be experimental and less secure</li>
      <li class="bulletList">Service meshes introduce a sidecar container that might become an attack vector</li>
    </ul>
    <p class="normal">Consider<a id="_idIndexMarker432"/> carefully the interaction between containers running in the same pod. You should realize that a bad container might try to compromise its sibling containers in the same pod as its first attack. This means that you should be able to detect rogue containers injected into a pod (e.g. by a malicious admission control webhook or a compromised CI/CD pipeline). You should also apply the least privilege principle and minimize the damage such a rogue container can do.</p>
    <h2 id="_idParaDest-186" class="heading-2">Organizational, cultural, and process challenges</h2>
    <p class="normal">Security <a id="_idIndexMarker433"/>is often held in contrast to productivity. This is a normal trade-off and nothing to worry about. Traditionally, when developers and operations were separate, this conflict was managed at an organizational level. Developers pushed for more productivity and treated security requirements as the cost of doing business. Operations controlled the production environment and were responsible for access and security procedures. The DevOps movement brought down the wall between developers and operations. Now, speed of development often takes a front-row seat. Concepts such as continuous deployment deploying multiple times a day without human intervention were unheard of in most organizations. Kubernetes was designed for this new world of cloud-native applications. But, it was developed based on Google’s experience. </p>
    <p class="normal">Google had a lot of time and skilled experts to develop the proper processes and tooling to balance rapid deployments with security. For smaller organizations, this balancing act might be very challenging and security could be weakened by focusing too much on productivity.</p>
    <p class="normal">The challenges facing organizations that adopt Kubernetes are as follows:</p>
    <ul>
      <li class="bulletList">Developers that control the operation of Kubernetes might be less security-oriented</li>
      <li class="bulletList">The speed of development might be considered more important than security</li>
      <li class="bulletList">Continuous deployment might make it difficult to detect certain security problems before they reach production</li>
      <li class="bulletList">Smaller organizations might not have the knowledge and expertise to manage security properly in Kubernetes clusters</li>
    </ul>
    <p class="normal">There<a id="_idIndexMarker434"/> are no easy answers here. You should be deliberate in striking the right balance between security and agility. I recommend having a dedicated security team (or at least one person focused on security) participate in all planning meetings and advocate for security. It’s important to bake security into your system from the get-go.</p>
    <p class="normal">In this section, we reviewed the many challenges you face when you try to build a secure Kubernetes cluster. Most of these challenges are not specific to Kubernetes, but using Kubernetes means there is a large part of your system that is generic and unaware of what the system is doing.</p>
    <p class="normal">This can pose problems when trying to lock down a system. The challenges are spread across different levels:</p>
    <ul>
      <li class="bulletList">Node challenges</li>
      <li class="bulletList">Network challenges</li>
      <li class="bulletList">Image challenges</li>
      <li class="bulletList">Configuration and deployment challenges</li>
      <li class="bulletList">Pod and container challenges</li>
      <li class="bulletList">Organizational and process challenges</li>
    </ul>
    <p class="normal">In the next section, we will look at the facilities Kubernetes provides to address some of those challenges. Many of the challenges require solutions at the larger system scope. It is important to realize that just utilizing all of Kubernetes’ security features is not enough.</p>
    <h1 id="_idParaDest-187" class="heading-1">Hardening Kubernetes</h1>
    <p class="normal">The <a id="_idIndexMarker435"/>previous section cataloged and listed the variety of security challenges facing developers and administrators deploying and maintaining Kubernetes clusters. In this section, we will hone in on the design aspects, mechanisms, and features offered by Kubernetes to address some of the challenges. You can get to a pretty good state of security by judicious use of capabilities such as service accounts, network policies, authentication, authorization, admission control, AppArmor, and secrets.</p>
    <p class="normal">Remember that a <a id="_idIndexMarker436"/>Kubernetes cluster is one part of a bigger system that includes other software systems, people, and processes. Kubernetes can’t solve all problems. You should always keep in mind general security principles, such as defense in depth, a need-to-know basis, and the principle of least privilege. </p>
    <p class="normal">In addition, log everything you think may be useful in the event of an attack and have alerts for early detection when the system deviates from its state. It may be just a bug or it may be an attack. Either way, you want to know about it and respond.</p>
    <h2 id="_idParaDest-188" class="heading-2">Understanding service accounts in Kubernetes</h2>
    <p class="normal">Kubernetes has <a id="_idIndexMarker437"/>regular users that are managed outside the cluster for humans connecting to the cluster (for example, via the <code class="inlineCode">kubectl</code> command), and it has service accounts.</p>
    <p class="normal">Regular user accounts <a id="_idIndexMarker438"/>are global and can access multiple namespaces in the cluster. Service accounts are constrained to one namespace. This is important. It ensures namespace isolation, because whenever the API server receives a request from a pod, its credentials will apply only to its own namespace.</p>
    <p class="normal">Kubernetes manages service accounts on behalf of the pods. Whenever Kubernetes instantiates a pod, it assigns the pod a service account unless the service account or the pod explicitly opted out by setting <code class="inlineCode">automountServiceAccountToken</code> to <code class="inlineCode">False</code>. The service account identifies all the pod processes when they interact with the API server. Each service account has a set of credentials mounted in a secret volume. Each namespace has a default service account called default. When you create a pod, it is automatically assigned the default service account unless you specify a different service account.</p>
    <p class="normal">You can create additional service accounts if you want different pods to have different identities and permissions. Then you can bind different service accounts to different roles.</p>
    <p class="normal">Create a file called <code class="inlineCode">custom-service-account.yaml</code> with the following content:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">custom-service-account</span>
</code></pre>
    <p class="normal">Now type<a id="_idIndexMarker439"/> the following:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl create -f custom-service-account.yaml
serviceaccount/custom-service-account created
</code></pre>
    <p class="normal">Here is the <a id="_idIndexMarker440"/>service account listed alongside the default service account:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl get serviceaccounts
NAME                     SECRETS   AGE
custom-service-account   1         6s
default                  1         2m28s
</code></pre>
    <p class="normal">Note that a secret was created automatically for your new service account:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl get secret
NAME                                                    TYPE                                                   DATA   AGE
custom-service-account-token-vbrbm   kubernetes.io/service-account-token   3          62s
default-token-m4nfk                              kubernetes.io/service-account-token   3           3m24s
</code></pre>
    <p class="normal">To get more detail, type the following:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl get serviceAccounts/custom-service-account -o yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: "2022-06-19T18:38:22Z"
  name: custom-service-account
  namespace: default
  resourceVersion: "784"
  uid: f70f70cf-5b42-4a46-a2ff-b07792bf1220
secrets:
- name: custom-service-account-token-vbrbm
</code></pre>
    <p class="normal">You can see the secret itself, which includes a <code class="inlineCode">ca.crt</code> file and a token, by typing the following:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl get secret custom-service-account-token-vbrbm -o yaml
</code></pre>
    <h3 id="_idParaDest-189" class="heading-3">How does Kubernetes manage service accounts?</h3>
    <p class="normal">The API server<a id="_idIndexMarker441"/> has a dedicated component called the service account admission controller. It is responsible for checking, at pod creation time, if the API server has a custom service account and, if it does, that the custom service account exists. If there is no service account specified, then it assigns the default service account.</p>
    <p class="normal">It also ensures the pod has <code class="inlineCode">ImagePullSecrets</code>, which are necessary when images need to be pulled from a remote image registry. If the pod spec doesn’t have any secrets, it uses the service account’s <code class="inlineCode">ImagePullSecrets</code>.</p>
    <p class="normal">Finally, it adds a <code class="inlineCode">volume</code> with an API token for API access and a <code class="inlineCode">volumeSource</code> mounted at <code class="inlineCode">/var/run/secrets/kubernetes.io/serviceaccount</code>.</p>
    <p class="normal">The API token is created and added to the secret by another component called the <strong class="keyWord">token controller</strong> whenever a service account is created. The token controller also monitors secrets and adds or removes tokens wherever secrets are added to or removed from a service account.</p>
    <p class="normal">The <strong class="keyWord">service account controller</strong> ensures the default service account exists for every namespace.</p>
    <h2 id="_idParaDest-190" class="heading-2">Accessing the API server</h2>
    <p class="normal">Accessing the<a id="_idIndexMarker442"/> API server requires a chain of steps that include authentication, authorization, and admission control. At each stage, the request may be rejected. Each stage consists of multiple plugins that are chained together.</p>
    <p class="normal">The following diagram illustrates this:</p>
    <figure class="mediaobject"><img src="../Images/B18998_04_02.png" alt="Diagram  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 4.2: Accessing the API server</p>
    <h3 id="_idParaDest-191" class="heading-3">Authenticating users</h3>
    <p class="normal">When <a id="_idIndexMarker443"/>you first create the cluster, some keys and certificates are created for you to authenticate against the cluster. These credentials are typically stored in the file <code class="inlineCode">~/.kube/config</code>, which may contain credentials for multiple clusters. You can also have multiple configuration files and control which file will be used by setting the <code class="inlineCode">KUBECONFIG</code> environment variable or passing the <code class="inlineCode">--kubeconfig</code> flag to kubectl. kubectl uses the credentials to authenticate itself to the API server and vice versa over TLS (an encrypted HTTPS connection). Let’s create a new KinD cluster and store its credentials in a dedicated config file by setting the <code class="inlineCode">KUBECONFIG</code> environment variable:</p>
    <pre class="programlisting gen"><code class="hljs">$ export KUBECONFIG=~/.kube/kind-config
$ kind create cluster
Creating cluster "kind" ...
 <img src="../Images/B18998_04_001.png" alt=""/> Ensuring node image (kindest/node:v1.23.4) <img src="../Images/B18998_04_002.png" alt=""/>
 <img src="../Images/B18998_04_003.png" alt=""/> Preparing nodes <img src="../Images/B18998_04_004.png" alt=""/>
 <img src="../Images/B18998_04_003.png" alt=""/> Writing configuration <img src="../Images/B18998_04_006.png" alt=""/>
 <img src="../Images/B18998_04_001.png" alt=""/> Starting control-plane <img src="../Images/B18998_04_008.png" alt=""/>
 <img src="../Images/B18998_04_001.png" alt=""/> Installing CNI <img src="../Images/B18998_04_010.png" alt=""/>
 <img src="../Images/B18998_04_003.png" alt=""/> Installing StorageClass <img src="../Images/B18998_04_012.png" alt=""/>
Set kubectl context to "kind-kind"
You can now use your cluster with:
kubectl cluster-info --context kind-kind
</code></pre>
    <p class="normal">You can <a id="_idIndexMarker444"/>view your configuration using this command:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://127.0.0.1:61022
  name: kind-kind
contexts:
- context:
    cluster: kind-kind
    user: kind-kind
  name: kind-kind
current-context: kind-kind
kind: Config
preferences: {}
users:
- name: kind-kind
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
</code></pre>
    <p class="normal">This is the configuration for a KinD cluster. It may look different for other types of clusters.</p>
    <p class="normal">Note that if multiple users need to access the cluster, the creator should provide the necessary client certificates and keys to the other users in a secure manner.</p>
    <p class="normal">This is just establishing basic trust with the Kubernetes API server itself. You’re not authenticated yet. Various authentication modules may look at the request and check for various additional client certificates, passwords, bearer tokens, and JWT tokens (for service accounts). Most requests require an authenticated user (either a regular user or a service account), although there are some anonymous requests too. If a request fails to authenticate with all the authenticators it will be rejected with a 401 HTTP status code (unauthorized, which is a bit of a misnomer).</p>
    <p class="normal">The cluster<a id="_idIndexMarker445"/> administrator determines what authentication strategies to use by providing various command-line arguments to the API server:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">--client-ca-file=</code> (for x509 client certificates specified in a file)</li>
      <li class="bulletList"><code class="inlineCode">--token-auth-file=</code> (for bearer tokens specified in a file)</li>
      <li class="bulletList"><code class="inlineCode">--basic-auth-file=</code> (for user/password pairs specified in a file)</li>
      <li class="bulletList"><code class="inlineCode">--enable-bootstrap-token-auth</code> (for bootstrap tokens used by kubeadm)</li>
    </ul>
    <p class="normal">Service accounts use an automatically loaded authentication plugin. The administrator may provide two optional flags:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">--service-account-key-file=</code> (If not specified, the API server’s TLS private key will be utilized as the PEM-encoded key for signing bearer tokens.)</li>
      <li class="bulletList"><code class="inlineCode">--service-account-lookup</code> (When enabled, the revocation of tokens will take place if they are deleted from the API.)</li>
    </ul>
    <p class="normal">There are several other methods, such as OpenID Connect, webhooks, Keystone (the OpenStack identity service), and an authenticating proxy. The main theme is that the authentication stage is extensible and can support any authentication mechanism.</p>
    <p class="normal">The various authentication plugins will examine the request and, based on the provided credentials, will associate the following attributes:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Username</strong> (a user-friendly name)</li>
      <li class="bulletList"><strong class="keyWord">UID</strong> (a unique identifier and more consistent than the username)</li>
      <li class="bulletList"><strong class="keyWord">Groups</strong> (a set of group names the user belongs to)</li>
      <li class="bulletList"><strong class="keyWord">Extra fields</strong> (these map string keys to string values)</li>
    </ul>
    <p class="normal">In Kubernetes 1.11, kubectl gained the ability to use credential plugins to receive an opaque token from a provider such as an organizational LDAP server. These credentials are sent by kubectl to the API server that typically uses a webhook token authenticator to authenticate the credentials and accept the request.</p>
    <p class="normal">The authenticators have no knowledge whatsoever of what a particular user is allowed to do. They just map a set of credentials to a set of identities. The authenticators run in an unspecified order; the first authenticator to accept the passed credentials will associate an identity with the incoming request and the authentication is considered successful. If all authenticators reject the credentials then authentication failed.</p>
    <p class="normal">It’s interesting to note that Kubernetes has no idea who its regular users are. There is no list of users in etcd. Authentication <a id="_idIndexMarker446"/>is granted to any user who presents a valid certificate that has been signed by the <strong class="keyWord">certificate authority</strong> (<strong class="keyWord">CA</strong>) associated with the cluster.</p>
    <h3 id="_idParaDest-192" class="heading-3">Impersonation</h3>
    <p class="normal">It is <a id="_idIndexMarker447"/>possible for users to impersonate different users (with proper authorization). For example, an admin may want to troubleshoot some issue as a different user with fewer privileges. This requires passing impersonation headers to the API request. The headers are as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Impersonate-User</strong>: Specifies the username to act on behalf of.</li>
      <li class="bulletList"><strong class="keyWord">Impersonate-Group</strong>: Specifies a group name to act on behalf of. Multiple groups can be provided by specifying this option multiple times. This option is optional but requires Impersonate-User to be set.</li>
      <li class="bulletList"><strong class="keyWord">Impersonate-Extra-(extra name)</strong>: A dynamic header used to associate additional fields with the user. This option is optional but requires Impersonate-User to be set.</li>
    </ul>
    <p class="normal">With kubectl, you pass the <code class="inlineCode">–-as</code> and <code class="inlineCode">--as-group</code> parameters.</p>
    <p class="normal">To impersonate a service account, type the following:</p>
    <pre class="programlisting gen"><code class="hljs">kubectl --as system:serviceaccount:&lt;namespace&gt;:&lt;service account name&gt;
</code></pre>
    <h3 id="_idParaDest-193" class="heading-3">Authorizing requests</h3>
    <p class="normal">Once a user is <a id="_idIndexMarker448"/>authenticated, authorization commences. Kubernetes has generic authorization semantics. A set of authorization modules receives the request, which includes information such as the authenticated username and the request’s verb (list, get, watch, create, and so on). Unlike authentication, all authorization plugins will get a shot at any request. If a single authorization plugin rejects the request or no plugin had an opinion then it will be rejected with a 403 HTTP status code (forbidden). A request will continue only if at least one plugin accepts it and no other plugin rejected it.</p>
    <p class="normal">The cluster administrator determines what authorization plugins to use by specifying the <code class="inlineCode">--authorization-mode</code> command-line flag, which is a comma-separated list of plugin names.</p>
    <p class="normal">The following modes are supported:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">--authorization-mode=AlwaysDeny</code> rejects all requests. Use if you don’t need authorization.</li>
      <li class="bulletList"><code class="inlineCode">--authorization-mode=AlwaysAllow</code> allows all requests. Use if you don’t need authorization. This is useful during testing.</li>
      <li class="bulletList"><code class="inlineCode">--authorization-mode=ABAC</code> allows for a simple, local-file-based, user-configured authorization policy. ABAC stands for Attribute-Based Access Control.</li>
      <li class="bulletList"><code class="inlineCode">--authorization-mode=RBAC</code> is a role-based mechanism where authorization policies are stored and driven by the Kubernetes API. RBAC stands for Role-Based Access Control.</li>
      <li class="bulletList"><code class="inlineCode">--authorization-mode=Node</code> is a special mode designed to authorize API requests made by kubelets.</li>
      <li class="bulletList"><code class="inlineCode">--authorization-mode=Webhook</code> allows for authorization to be driven by a remote service using REST.</li>
    </ul>
    <p class="normal">You can <a id="_idIndexMarker449"/>add your own custom authorization plugin by implementing the following straightforward Go interface:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">type</span> Authorizer <span class="hljs-keyword">interface</span> {
    Authorize(ctx context.Context, a Attributes) (authorized Decision, reason <span class="hljs-type">string</span>, err <span class="hljs-type">error</span>)
}
</code></pre>
    <p class="normal">The <code class="inlineCode">Attributes</code> input argument is also an interface that provides all the information you need to make an authorization decision:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">type</span> Attributes <span class="hljs-keyword">interface</span> {
  GetUser() user.Info
  GetVerb() <span class="hljs-type">string</span>
  IsReadOnly() <span class="hljs-type">bool</span>
  GetNamespace() <span class="hljs-type">string</span>
  GetResource() <span class="hljs-type">string</span>
  GetSubresource() <span class="hljs-type">string</span>
  GetName() <span class="hljs-type">string</span>
  GetAPIGroup() <span class="hljs-type">string</span>
  GetAPIVersion() <span class="hljs-type">string</span>
  IsResourceRequest() <span class="hljs-type">bool</span>
  GetPath() <span class="hljs-type">string</span>
}
</code></pre>
    <p class="normal">You can find the source code at <a href="https://github.com/kubernetes/apiserver/blob/master/pkg/authorization/authorizer/interfaces.go"><span class="url">https://github.com/kubernetes/apiserver/blob/master/pkg/authorization/authorizer/interfaces.go</span></a>.</p>
    <p class="normal">Using<a id="_idIndexMarker450"/> the <code class="inlineCode">kubectl can-i</code> command, you can check what actions you can perform and even impersonate other users:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl auth can-i create deployments
Yes
$ kubectl auth can-i create deployments --as jack
no
</code></pre>
    <p class="normal">kubectl supports plugins. We will discuss plugins in depth later in <em class="chapterRef">Chapter 15</em>, <em class="italic">Extending Kubernetes</em>. In the meantime, I’ll just mention that one of my favorite plugins is rolesum. This plugin gives you a summary of all the permissions a user or service account has. Here is an example:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl rolesum job-controller -n kube-system
ServiceAccount: kube-system/job-controller
Secrets:
• */job-controller-token-tp72d
Policies:
• [CRB] */system:controller:job-controller <img src="../Images/B18998_04_013.png" alt=""/>  [CR] */system:controller:job-controller
  Resource                 Name  Exclude  Verbs  G L W C U P D DC
  events.[,events.k8s.io]  [*]     [-]     [-]   <img src="../Images/B18998_04_014.png" alt=""/>
  jobs.batch               [*]     [-]     [-]   <img src="../Images/B18998_04_015.png" alt=""/>
  jobs.batch/finalizers    [*]     [-]     [-]   <img src="../Images/B18998_04_016.png" alt=""/>
  jobs.batch/status        [*]     [-]     [-]   <img src="../Images/B18998_04_016.png" alt=""/>
  pods                     [*]     [-]     [-]   <img src="../Images/B18998_04_018.png" alt=""/>
</code></pre>
    <p class="normal">Check it out here: <a href="https://github.com/Ladicle/kubectl-rolesum"><span class="url">https://github.com/Ladicle/kubectl-rolesum</span></a>.</p>
    <h3 id="_idParaDest-194" class="heading-3">Using admission control plugins</h3>
    <p class="normal">OK. The<a id="_idIndexMarker451"/> request was authenticated and authorized, but there is one more step before it can be executed. The request must go through a gauntlet of admission-control plugins. Similar to the authorizers, if a single admission controller rejects a request, it is denied. Admission controllers are a neat concept. The idea is that there may be global cluster concerns that could be grounds for rejecting a request. Without admission controllers, all authorizers would have <a id="_idIndexMarker452"/>to be aware of these concerns and reject the request. But, with admission controllers, this logic can be performed once. In addition, an admission controller may modify the request. Admission controllers run in either validating mode or mutating mode. As usual, the cluster administrator decides which admission control plugins run by providing a command-line argument called <code class="inlineCode">admission-control</code>. The value is a comma-separated and ordered list of plugins. Here is the list of recommended plugins for Kubernetes &gt;= 1.9 (the order matters):</p>
    <pre class="programlisting gen"><code class="hljs">--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,DefaultTolerationSeconds
</code></pre>
    <p class="normal">Let’s look at some available plugins (more are added all the time):</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">DefaultStorageClass</code>: Adds a default storage class to requests for the creation of a <code class="inlineCode">PersistentVolumeClaim</code> that doesn’t specify a storage class.</li>
      <li class="bulletList"><code class="inlineCode">DefaultTolerationSeconds</code>: Sets the default toleration of pods for taints (if not set already): <code class="inlineCode">notready:NoExecute</code> and <code class="inlineCode">notreachable:NoExecute</code>.</li>
      <li class="bulletList"><code class="inlineCode">EventRateLimit</code>: Limits flooding of the API server with events.</li>
      <li class="bulletList"><code class="inlineCode">ExtendedResourceToleration</code>: Combines taints on nodes with special resources such as GPUs and <strong class="keyWord">Field Programmable Gate Arrays</strong> (<strong class="keyWord">FPGAs</strong>) with toleration on pods that request those resources. The end result is that the node with the extra resources will be dedicated for pods with the proper toleration.</li>
      <li class="bulletList"><code class="inlineCode">ImagePolicyWebhook</code>: This complicated plugin connects to an external backend to decide whether a request should be rejected based on the image.</li>
      <li class="bulletList"><code class="inlineCode">LimitPodHardAntiAffinity</code>: In the <code class="inlineCode">requiredDuringSchedulingRequiredDuringExecution</code> field, any pod that specifies an AntiAffinity topology key other than <code class="inlineCode">kubernetes.io/hostname</code> will be denied.</li>
      <li class="bulletList"><code class="inlineCode">LimitRanger</code>: Rejects requests that violate resource limits.</li>
      <li class="bulletList"><code class="inlineCode">MutatingAdmissionWebhook</code>: Calls registered mutating webhooks that are able to modify their target object. Note that there is no guarantee that the change will be effective due to potential changes by other mutating webhooks.</li>
      <li class="bulletList"><code class="inlineCode">NamespaceAutoProvision</code>: Creates the namespace in the request if it doesn’t exist already.</li>
      <li class="bulletList"><code class="inlineCode">NamespaceLifecycle</code>: Rejects object creation requests in namespaces that are in the process of being terminated or don’t exist.</li>
      <li class="bulletList"><code class="inlineCode">ResourceQuota</code>: Rejects requests that violate the namespace’s resource quota.</li>
      <li class="bulletList"><code class="inlineCode">ServiceAccount</code>: Automation for service accounts.</li>
      <li class="bulletList"><code class="inlineCode">ValidatingAdmissionWebhook</code>: The admission controller invokes validating webhooks that match the request. The matching webhooks are called concurrently, and if any of them reject the request, the overall request fails.</li>
    </ul>
    <p class="normal">As you <a id="_idIndexMarker453"/>can see, the admission control plugins have very diverse functionality. They support namespace-wide policies and enforce the validity of requests mostly from the resource management and security points of view. This frees up the authorization plugins to focus on valid operations. <code class="inlineCode">ImagePolicyWebHook</code> is the gateway to validating images, which is a big challenge. <code class="inlineCode">MutatingAdmissionWebhook</code> and <code class="inlineCode">ValidatingAdmissionWebhook</code> are the gateways to dynamic admission control, where you can deploy your own admission controller without compiling it into Kubernetes. Dynamic admission control is suitable for tasks like semantic validation of resources (do all pods have the standard set of labels?). We will discuss dynamic admission control in depth later, in <em class="chapterRef">Chapter 16</em>, <em class="italic">Governing Kubernetes</em>, as this is the foundation of policy management governance in Kubernetes.</p>
    <p class="normal">The division of responsibility for validating an incoming request through the separate stages of authentication, authorization, and admission, each with its own plugins, makes a complicated process much more manageable to understand, use, and extend.</p>
    <p class="normal">The mutating admission controllers provide a lot of flexibility and the ability to automatically enforce certain policies without burdening the users (for example, creating a namespace automatically if it doesn’t exist).</p>
    <h2 id="_idParaDest-195" class="heading-2">Securing pods</h2>
    <p class="normal">Pod security <a id="_idIndexMarker454"/>is a major concern, since Kubernetes schedules the pods and lets them run. There are several independent mechanisms for securing pods and containers. Together these mechanisms support defense in depth, where, even if an attacker (or a mistake) bypasses one mechanism, it will get blocked by another.</p>
    <h3 id="_idParaDest-196" class="heading-3">Using a private image repository</h3>
    <p class="normal">This<a id="_idIndexMarker455"/> approach gives you a lot of confidence that your cluster will only pull images that you have previously vetted, and you can manage upgrades better. In light of the rise in software supply chain attacks it is an important countermeasure. You can configure your <code class="inlineCode">HOME/.docker/config.json</code> on each node. But, on many cloud providers, you can’t do this because nodes are provisioned automatically for you.</p>
    <h3 id="_idParaDest-197" class="heading-3">ImagePullSecrets</h3>
    <p class="normal">This <a id="_idIndexMarker456"/>approach is recommended for clusters on cloud providers. The idea is that the credentials for the registry will be provided by the pod, so it doesn’t matter what node it is scheduled to run on. This circumvents the problem with <code class="inlineCode">.dockercfg</code> at the node level.</p>
    <p class="normal">First, you need to create a secret object for the credentials:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl create secret docker-registry the-registry-secret \
    --docker-server=&lt;docker registry server&gt; \
    --docker-username=&lt;username&gt; \ 
    --docker-password=&lt;password&gt; \
    --docker-email=&lt;email&gt;
secret 'docker-registry-secret' created.
</code></pre>
    <p class="normal">You can create secrets for multiple registries (or multiple users for the same registry) if needed. The kubelet will combine all <code class="inlineCode">ImagePullSecrets</code>.</p>
    <p class="normal">But, since pods can access secrets only in their own namespace, you must create a secret on each namespace where you want the pod to run. Once the secret is defined, you can add it to the pod spec and run some pods on your cluster. The pod will use the credentials from the secret to pull images from the target image registry:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">cool-pod</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">the-namespace</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
 <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cool-container</span>
   <span class="hljs-attr">image:</span> <span class="hljs-string">cool/app:v1</span>
   <span class="hljs-attr">imagePullSecrets:</span>
   <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">the-registry-secret</span>
</code></pre>
    <h3 id="_idParaDest-198" class="heading-3">Specifying a security context for pods and containers</h3>
    <p class="normal">Kubernetes <a id="_idIndexMarker457"/>allows setting a security context at the pod level and additional security contexts at the container level. The pod security context is a set of operating-system-level security settings such as UID, GID, capabilities, and SELinux role. The pod security context can also apply its security settings (in particular, <code class="inlineCode">fsGroup</code> and <code class="inlineCode">seLinuxOptions</code>) to volumes.</p>
    <p class="normal">Here is an example of a pod security context:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">name:</span> <span class="hljs-string">some-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">securityContext:</span>
    <span class="hljs-attr">fsGroup:</span> <span class="hljs-number">1234</span>
    <span class="hljs-attr">supplementalGroups:</span> [<span class="hljs-number">5678</span>]
    <span class="hljs-attr">seLinuxOptions:</span>
      <span class="hljs-attr">level:</span> <span class="hljs-string">'s0:c123,c456'</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-string">...</span>
</code></pre>
    <p class="normal">For the complete list of pod security context fields, check out <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#podsecuritycontext-v1-core"><span class="url">https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#podsecuritycontext-v1-core</span></a>.</p>
    <p class="normal">The container security context is applied to each container and it adds container-specific settings. Some fields of the container security context overlap with fields in the pod security context. If the container security context specifies these fields they override the values in the pod security context. Container context settings can’t be applied to volumes, which remain at the pod level even if mounted into specific containers only.</p>
    <p class="normal">Here is a pod with a container security context:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">some-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">some-container</span>
    <span class="hljs-string">...</span>
    <span class="hljs-attr">securityContext:</span>
      <span class="hljs-attr">privileged:</span> <span class="hljs-literal">true</span>
      <span class="hljs-attr">seLinuxOptions:</span>
        <span class="hljs-attr">level:</span> <span class="hljs-string">'s0:c123,c456'</span>
</code></pre>
    <p class="normal">For the <a id="_idIndexMarker458"/>complete list of container security context fields, check out <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#securitycontext-v1-core"><span class="url">https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#securitycontext-v1-core</span></a>.</p>
    <h3 id="_idParaDest-199" class="heading-3">Pod security standards</h3>
    <p class="normal">Kubernetes <a id="_idIndexMarker459"/>defines security profiles that are appropriate for different security needs and aggregates recommended settings. The privileged profile provides all permissions and is, unfortunately, the default. The baseline profile is a minimal security profile that just prevents privilege escalation. The restricted profile follows hardening best practices.</p>
    <p class="normal">See more info here: <a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/"><span class="url">https://kubernetes.io/docs/concepts/security/pod-security-standards/</span></a>.</p>
    <h3 id="_idParaDest-200" class="heading-3">Protecting your cluster with AppArmor</h3>
    <p class="normal">AppArmor<a id="_idIndexMarker460"/> is a Linux kernel security module. With <a id="_idIndexMarker461"/>AppArmor, you can restrict a process running in a container to a limited set of resources such as network access, Linux capabilities, and file permissions. You configure AppArmor through profiles.</p>
    <h4 class="heading-4">AppArmor requirements</h4>
    <p class="normal">AppArmor <a id="_idIndexMarker462"/>support was added as beta in Kubernetes 1.4. It is not available for every OS, so you must choose a supported OS distribution in order to take advantage of it. Ubuntu and SUSE Linux support AppArmor and enable it by default. Other distributions have optional support. </p>
    <p class="normal">To check if AppArmor is enabled connect to a node (e.g. via <code class="inlineCode">ssh</code>) and type the following:</p>
    <pre class="programlisting gen"><code class="hljs">$ cat /sys/module/apparmor/parameters/enabled 
Y
</code></pre>
    <p class="normal">If the result is <code class="inlineCode">Y</code> then it’s enabled. If the file doesn’t exist or the result is not <code class="inlineCode">Y</code> it is not enabled.</p>
    <p class="normal">The profile <a id="_idIndexMarker463"/>must be loaded into the kernel. Check the following file:</p>
    <pre class="programlisting gen"><code class="hljs">/sys/kernel/security/apparmor/profiles
</code></pre>
    <p class="normal">Kubernetes doesn’t provide a built-in mechanism to load profiles to nodes. You typically need a DaemonSet with node-level privileges to load the necessary AppArmor profiles into the nodes.</p>
    <p class="normal">Check out the following link for more details on loading AppArmor profiles into nodes: <a href="https://kubernetes.io/docs/tutorials/security/apparmor/#setting-up-nodes-with-profiles"><span class="url">https://kubernetes.io/docs/tutorials/security/apparmor/#setting-up-nodes-with-profiles</span></a>.</p>
    <h4 class="heading-4">Securing a pod with AppArmor</h4>
    <p class="normal">Since <a id="_idIndexMarker464"/>AppArmor is still in beta, you specify the metadata as<a id="_idIndexMarker465"/> annotations and not as bonafide fields. When it gets out of beta, this will change.</p>
    <p class="normal">To apply a profile to a container, add the following annotation:</p>
    <pre class="programlisting code"><code class="hljs-code">container.apparmor.security.beta.kubernetes.io/&lt;container name&gt;: &lt;profile reference&gt;
</code></pre>
    <p class="normal">The profile reference can be either the <code class="inlineCode">default</code> profile, <code class="inlineCode">runtime/default</code>, or a profile file on the host/localhost.</p>
    <p class="normal">Here is a sample profile that prevents writing to files:</p>
    <pre class="programlisting gen"><code class="hljs">&gt; \#include \&lt;tunables/global\&gt;
&gt;
&gt; profile k8s-apparmor-example-deny-write flags=(attach\\\_disconnected)
&gt; {
&gt;
&gt; \#include \&lt;abstractions/base\&gt;
&gt;
&gt; file,
&gt;
&gt; \# Deny all file writes.
&gt;
&gt; deny /\\\*\\\* w,
&gt;
&gt; }
</code></pre>
    <p class="normal">AppArmor is not a Kubernetes resource, so the format is not the YAML or JSON you’re familiar with.</p>
    <p class="normal">To verify the <a id="_idIndexMarker466"/>profile was attached correctly, check the attributes<a id="_idIndexMarker467"/> of process 1:</p>
    <pre class="programlisting gen"><code class="hljs">kubectl exec &lt;pod-name&gt; cat /proc/1/attr/current
</code></pre>
    <p class="normal">Pods can be scheduled on any node in the cluster by default. This means the profile should be loaded into every node. This is a classic use case for a DaemonSet.</p>
    <h2 id="_idParaDest-201" class="heading-2">Writing AppArmor profiles</h2>
    <p class="normal">Writing <a id="_idIndexMarker468"/>profiles for AppArmor by hand is not trivial. There are some tools that can help: <code class="inlineCode">aa-genprof</code> and <code class="inlineCode">aa-logprof</code> can generate a profile for you and assist in fine-tuning it by running your application with AppArmor in complain mode. The tools keep track of your application’s activity and AppArmor warnings and create a corresponding profile. This approach works, but it feels clunky.</p>
    <p class="normal">My favorite tool is bane, which generates AppArmor profiles from a simpler profile language based on the TOML syntax. bane profiles are very readable and easy to grasp. Here is a sample bane profile:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># name of the profile, we will auto prefix with `docker-`</span>
<span class="hljs-comment"># so the final profile name will be `docker-nginx-sample`</span>
Name = <span class="hljs-string">"nginx-sample"</span>
[Filesystem]
<span class="hljs-comment"># read only paths for the container</span>
ReadOnlyPaths = [
    <span class="hljs-string">"/bin/**"</span>,
    <span class="hljs-string">"/boot/**"</span>,
    <span class="hljs-string">"/dev/**"</span>,
    <span class="hljs-string">"/etc/**"</span>,
    <span class="hljs-string">"/home/**"</span>,
    <span class="hljs-string">"/lib/**"</span>,
    <span class="hljs-string">"/lib64/**"</span>,
    <span class="hljs-string">"/media/**"</span>,
    <span class="hljs-string">"/mnt/**"</span>,
    <span class="hljs-string">"/opt/**"</span>,
    <span class="hljs-string">"/proc/**"</span>,
    <span class="hljs-string">"/root/**"</span>,
    <span class="hljs-string">"/sbin/**"</span>,
    <span class="hljs-string">"/srv/**"</span>,
    <span class="hljs-string">"/tmp/**"</span>,
    <span class="hljs-string">"/sys/**"</span>,
    <span class="hljs-string">"/usr/**"</span>,
]
<span class="hljs-comment"># paths where you want to log on write</span>
LogOnWritePaths = [
    <span class="hljs-string">"/**"</span>
]
<span class="hljs-comment"># paths where you can write</span>
WritablePaths = [
    <span class="hljs-string">"/var/run/nginx.pid"</span>
]
<span class="hljs-comment"># allowed executable files for the container</span>
AllowExec = [
    <span class="hljs-string">"/usr/sbin/nginx"</span>
]
<span class="hljs-comment"># denied executable files</span>
DenyExec = [
    <span class="hljs-string">"/bin/dash"</span>,
    <span class="hljs-string">"/bin/sh"</span>,
    <span class="hljs-string">"/usr/bin/top"</span>
]
<span class="hljs-comment"># allowed capabilities</span>
[Capabilities]
Allow = [
    <span class="hljs-string">"chown"</span>,
    <span class="hljs-string">"dac_override"</span>,
    <span class="hljs-string">"setuid"</span>,
    <span class="hljs-string">"setgid"</span>,
    <span class="hljs-string">"net_bind_service"</span>
]
[Network]
<span class="hljs-comment"># if you don't need to ping in a container, you can probably</span>
<span class="hljs-comment"># set Raw to false and deny network raw</span>
Raw = false
Packet = false
Protocols = [
    <span class="hljs-string">"tcp"</span>,
    <span class="hljs-string">"udp"</span>,
    <span class="hljs-string">"icmp"</span>
]
</code></pre>
    <p class="normal">The generated<a id="_idIndexMarker469"/> AppArmor profile is pretty gnarly (verbose and complicated).</p>
    <p class="normal">You can find more information about bane here: <a href="https://github.com/genuinetools/bane"><span class="url">https://github.com/genuinetools/bane</span></a>.</p>
    <h2 id="_idParaDest-202" class="heading-2">Pod Security Admission</h2>
    <p class="normal">Pod Security Admission<a id="_idIndexMarker470"/> is an admission controller that is responsible for managing the Pod Security Standards (<a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/"><span class="url">https://kubernetes.io/docs/concepts/security/pod-security-standards/</span></a>). The pod security restrictions are applied at the namespace level. All pods in the target namespace will be checked for the same security profile (<strong class="keyWord">privileged</strong>, <strong class="keyWord">baseline</strong>, or <strong class="keyWord">restricted</strong>).</p>
    <p class="normal">Note that Pod Security Admission doesn’t set the relevant security contexts. It only validates that the pod conforms to the target policy.</p>
    <p class="normal">There are three modes:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">enforce</code>: Policy violations will result in the rejection of the pod.</li>
      <li class="bulletList"><code class="inlineCode">audit</code>: Policy violations will result in the addition of an audit annotation to the event recorded in the audit log, but the pod will still be allowed.</li>
      <li class="bulletList"><code class="inlineCode">warn</code>: Policy violations will trigger a warning for the user, but the pod will still be allowed.</li>
    </ul>
    <p class="normal">To activate Pod Security Admission on a namespace you simply add a label to the target namespace:</p>
    <pre class="programlisting gen"><code class="hljs">$ MODE=warn # One of enforce, audit, or warn
$ LEVEL=baseline # One of privileged, baseline, or restricted
$ kubectl label namespace/ns-1 pod-security.kubernetes.io/${MODE}: ${LEVEL}
namespace/ns-1 created      
</code></pre>
    <h2 id="_idParaDest-203" class="heading-2">Managing network policies</h2>
    <p class="normal">Node, pod, and <a id="_idIndexMarker471"/>container security are imperative, but it’s not enough. Network segmentation is critical to design secure Kubernetes clusters that allow multi-tenancy, as well as to minimize the impact of security breaches. Defense in depth mandates that you compartmentalize parts of the system that don’t need to talk to each other, while also carefully managing the direction, protocols, and ports of network traffic.</p>
    <p class="normal">Network policies <a id="_idIndexMarker472"/>allow the fine-grained control and proper network segmentation of your cluster. At the core, a network policy is a set of firewall rules applied to a set of namespaces and pods selected by labels. This is very flexible because labels can define virtual network segments and be managed at the Kubernetes resource level.</p>
    <p class="normal">This is a huge improvement over trying to segment your network using traditional approaches like IP address ranges and subnet masks, where you often run out of IP addresses or allocate too many just in case.</p>
    <p class="normal">However, if you use a service mesh you may not need to use network policies since the service mesh can fulfill the same role. More on service meshes later, in <em class="chapterRef">Chapter 14</em>, <em class="italic">Utilizing Service Meshes</em>.</p>
    <h3 id="_idParaDest-204" class="heading-3">Choosing a supported networking solution</h3>
    <p class="normal">Some<a id="_idIndexMarker473"/> networking backends (network plugins) don’t support network policies. For example, the popular Flannel can’t be used to apply policies. This is critical. You will be able to define network policies even if your network plugin doesn’t support them. Your policies will simply have no effect, giving you a false sense of security. Here is a list of network plugins that support network policies (both ingress and egress):</p>
    <ul>
      <li class="bulletList">Calico</li>
      <li class="bulletList">WeaveNet</li>
      <li class="bulletList">Canal</li>
      <li class="bulletList">Cillium</li>
      <li class="bulletList">Kube-Router</li>
      <li class="bulletList">Romana</li>
      <li class="bulletList">Contiv</li>
    </ul>
    <p class="normal">If you run your cluster on a managed Kubernetes service then the choice has already been made for you, although you can also install custom CNI plugins on some managed Kubernetes offerings.</p>
    <p class="normal">We will explore the ins and outs of network plugins in <em class="chapterRef">Chapter 10</em>, <em class="italic">Exploring Kubernetes Networking</em>. Here we focus on network policies.</p>
    <h3 id="_idParaDest-205" class="heading-3">Defining a network policy</h3>
    <p class="normal">You define a<a id="_idIndexMarker474"/> network policy using a standard YAML manifest. The supported protocols are TCP, UDP, and SCTP (since Kubernetes 1.20).</p>
    <p class="normal">Here is a sample policy:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">the-network-policy</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">podSelector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">role:</span> <span class="hljs-string">db</span>
  <span class="hljs-attr">ingress:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">from:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">namespaceSelector:</span>
        <span class="hljs-attr">matchLabels:</span>
          <span class="hljs-attr">project:</span> <span class="hljs-string">cool-project</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">podSelector:</span>
        <span class="hljs-attr">matchLabels:</span>
          <span class="hljs-attr">role:</span> <span class="hljs-string">frontend</span>
    <span class="hljs-attr">ports:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
      <span class="hljs-attr">port:</span> <span class="hljs-number">6379</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">spec</code> part has two important parts, the <code class="inlineCode">podSelector</code> and the <code class="inlineCode">ingress</code>. The <code class="inlineCode">podSelector</code> governs which pods this network policy applies to. The <code class="inlineCode">ingress</code> governs which namespaces and pods can access these pods and which protocols and ports they can use.</p>
    <p class="normal">In the preceding sample network policy, the pod selector specified the target for the network policy to be all the pods that are labeled <code class="inlineCode">role: db</code>. The <code class="inlineCode">ingress</code> section has a <code class="inlineCode">from</code> sub-section with a namespace selector and a pod selector. All the namespaces in the cluster that are labeled <code class="inlineCode">project: cool-project</code>, and within these namespaces, all the pods that are labeled <code class="inlineCode">role: frontend</code>, can access the target pods labeled <code class="inlineCode">role: db</code>. The <code class="inlineCode">ports</code> section defines a list of pairs (protocol and port) that further restrict what protocols and ports are allowed. In this case, the protocol is <code class="inlineCode">tcp</code> and the port is <code class="inlineCode">6379</code> (the standard Redis port). If you want to target a range of ports, you can use <code class="inlineCode">endPort</code>, as in:</p>
    <pre class="programlisting code"><code class="hljs-code">  <span class="hljs-attr">ports:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
    <span class="hljs-attr">port:</span> <span class="hljs-number">6379</span>
    <span class="hljs-attr">endPort:</span> <span class="hljs-number">7000</span>
</code></pre>
    <p class="normal">Note that<a id="_idIndexMarker475"/> the network policy is cluster-wide, so pods from multiple namespaces in the cluster can access the target namespace. The current namespace is always included, so even if it doesn’t have the <code class="inlineCode">project:cool</code> label, pods with <code class="inlineCode">role:frontend</code> can still have access.</p>
    <p class="normal">It’s important to realize that the network policy operates in a whitelist fashion. By default, all access is forbidden, and the network policy can open certain protocols and ports to certain pods that match the labels. However, the whitelist nature of the network policy applies only to pods that are selected for at least one network policy. If a pod is not selected it will allow all access. Always make sure all your pods are covered by a network policy.</p>
    <p class="normal">Another implication of the whitelist nature is that, if multiple network policies exist, then the unified effect of all the rules applies. If one policy gives access to port <code class="inlineCode">1234</code> and another gives access to port <code class="inlineCode">5678</code> for the same set of pods, then a pod may be accessed through either <code class="inlineCode">1234</code> or <code class="inlineCode">5678</code>.</p>
    <p class="normal">To use network policies responsibly, consider starting with a <code class="inlineCode">deny-all</code> network policy:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">deny-all</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">podSelector:</span> {}
  <span class="hljs-attr">policyTypes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">Ingress</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">Egress</span>
</code></pre>
    <p class="normal">Then, start adding network policies to allow ingress to specific pods explicitly. Note that you must apply the <code class="inlineCode">deny-all</code> policy for each namespace:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -n ${NAMESPACE} -f deny-all-network-policy.yaml
</code></pre>
    <h3 id="_idParaDest-206" class="heading-3">Limiting egress to external networks</h3>
    <p class="normal">Kubernetes 1.8 added egress network<a id="_idIndexMarker476"/> policy support, so you can control outbound traffic too. Here is an example that prevents access to the external IP <code class="inlineCode">1.2.3.4</code>. The <code class="inlineCode">order: 999</code> ensures the policy is applied before other policies:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">default-deny-egress</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">order:</span> <span class="hljs-number">999</span>
  <span class="hljs-attr">egress:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">action:</span> <span class="hljs-string">deny</span>
    <span class="hljs-attr">destination:</span>
      <span class="hljs-attr">net:</span> <span class="hljs-number">1.2.3.4</span>
    <span class="hljs-attr">source:</span> {}
</code></pre>
    <h3 id="_idParaDest-207" class="heading-3">Cross-namespace policies</h3>
    <p class="normal">If you <a id="_idIndexMarker477"/>divide your cluster into multiple namespaces, it can come in handy sometimes if pods can communicate across namespaces. You can specify the <code class="inlineCode">ingress.namespaceSelector</code> field in your network policy to enable access from multiple namespaces. This is useful, for example, if you have production and staging namespaces and you periodically populate your staging environments with snapshots of your production data.</p>
    <h3 id="_idParaDest-208" class="heading-3">The costs of network policies</h3>
    <p class="normal">Network policies <a id="_idIndexMarker478"/>are not free. Your CNI plugin may install additional components in your cluster and on every node. These components use precious resources and in addition may cause your pods to get evicted due to insufficient capacity. For example, the Calico CNI plugin installs several deployments in the <code class="inlineCode">kube-system</code> namespace:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get deploy -n kube-system -o name | grep calico 
deployment.apps/calico-node-vertical-autoscaler 
deployment.apps/calico-typha 
deployment.apps/calico-typha-horizontal-autoscaler 
deployment.apps/calico-typha-vertical-autoscaler
</code></pre>
    <p class="normal">It also provisions a DaemonSet that runs a pod on every node:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get ds -n kube-system -o name | grep calico-node 
daemonset.apps/calico-node
</code></pre>
    <h2 id="_idParaDest-209" class="heading-2">Using secrets</h2>
    <p class="normal">Secrets are <a id="_idIndexMarker479"/>paramount in secure systems. They can be credentials such as usernames and passwords, access tokens, API keys, certificates, or crypto keys. Secrets are typically small. If you <a id="_idIndexMarker480"/>have large amounts of data you want to protect, you should encrypt it and keep the encryption/decryption keys as secrets.</p>
    <h3 id="_idParaDest-210" class="heading-3">Storing secrets in Kubernetes</h3>
    <p class="normal">Kubernetes <a id="_idIndexMarker481"/>used to store secrets in etcd as plaintext by default. This means that direct access to etcd should be limited and carefully guarded. Starting with Kubernetes 1.7, you can now encrypt your secrets at rest (when they’re stored by etcd).</p>
    <p class="normal">Secrets are managed at the namespace level. Pods can mount secrets either as files via secret volumes or as environment variables. From a security standpoint, this means that any user or service that can create a pod in a namespace can have access to any secret managed for that namespace. If you want to limit access to a secret, put it in a namespace accessible to a limited set of users or services.</p>
    <p class="normal">When a secret is mounted into a container, it is never written to disk. It is stored in <code class="inlineCode">tmpfs</code>. When the kubelet communicates with the API server, it normally uses TLS, so the secret is protected in transit.</p>
    <p class="normal">Kubernetes secrets are limited to 1 MB.</p>
    <h3 id="_idParaDest-211" class="heading-3">Configuring encryption at rest</h3>
    <p class="normal">You need<a id="_idIndexMarker482"/> to pass this argument when you start the API server: <code class="inlineCode">--encryption-provider-config</code>.</p>
    <p class="normal">Here is a sample encryption config:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apiserver.config.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">EncryptionConfiguration</span>
<span class="hljs-attr">resources:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">resources:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">secrets</span>
    <span class="hljs-attr">providers:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">identity:</span> {}
      <span class="hljs-bullet">-</span> <span class="hljs-attr">aesgcm:</span>
          <span class="hljs-attr">keys:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">key1</span>
              <span class="hljs-attr">secret:</span> <span class="hljs-string">c2VjcmV0IGlzIHNlY3VyZQ==</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">key2</span>
              <span class="hljs-attr">secret:</span> <span class="hljs-string">dGhpcyBpcyBwYXNzd29yZA==</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">aescbc:</span>
          <span class="hljs-attr">keys:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">key1</span>
              <span class="hljs-attr">secret:</span> <span class="hljs-string">c2VjcmV0IGlzIHNlY3VyZQ==</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">key2</span>
              <span class="hljs-attr">secret:</span> <span class="hljs-string">dGhpcyBpcyBwYXNzd29yZA==</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">secretbox:</span>
          <span class="hljs-attr">keys:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">key1</span>
              <span class="hljs-attr">secret:</span> <span class="hljs-string">YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=</span>
</code></pre>
    <h3 id="_idParaDest-212" class="heading-3">Creating secrets</h3>
    <p class="normal">Secrets<a id="_idIndexMarker483"/> must be created before you try to create a pod that requires them. The secret must exist; otherwise, the pod creation will fail.</p>
    <p class="normal">You can create secrets with the following command: <code class="inlineCode">kubectl create secret</code>.</p>
    <p class="normal">Here I create a generic secret called <code class="inlineCode">hush-hush</code>, which contains two keys, a username and password:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create secret generic hush-hush \
    --from-literal=username=tobias  \ 
    --from-literal=password=cutoffs
secret/hush-hush created    
</code></pre>
    <p class="normal">The resulting secret is opaque:</p>
    <pre class="programlisting gen"><code class="hljs">$ k describe secrets/hush-hush
Name:         hush-hush
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Type:  Opaque
Data
====
password:  7 bytes
username:  6 bytes
</code></pre>
    <p class="normal">You can create<a id="_idIndexMarker484"/> secrets from files using <code class="inlineCode">--from-file</code> instead <code class="inlineCode">--from-literal</code>, and you can also create secrets manually if you encode the secret value as base64.</p>
    <p class="normal">Key names inside a secret must follow the rules for DNS sub-domains (without the leading dot).</p>
    <h3 id="_idParaDest-213" class="heading-3">Decoding secrets</h3>
    <p class="normal">To get the <a id="_idIndexMarker485"/>content of a secret you can use <code class="inlineCode">kubectl get secret</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get secrets/hush-hush -o yaml
apiVersion: v1
data:
  password: Y3V0b2Zmcw==
  username: dG9iaWFz
kind: Secret
metadata:
  creationTimestamp: "2022-06-20T19:49:56Z"
  name: hush-hush
  namespace: default
  resourceVersion: "51831"
  uid: 93e8d6d1-4c7f-4868-b146-32d1eb02b0a6
type: Opaque
</code></pre>
    <p class="normal">The values are base64-encoded. You need to decode them yourself:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get secrets/hush-hush -o jsonpath='{.data.password}' | base64 --decode
cutoffs
</code></pre>
    <h2 id="_idParaDest-214" class="heading-2">Using secrets in a container</h2>
    <p class="normal">Containers can <a id="_idIndexMarker486"/>access secrets as files by mounting volumes from the pod. Another approach is to access the secrets as environment variables. Finally, a container (given that its service account has the permission) can access the Kubernetes API directly or use <code class="inlineCode">kubectl get secret</code>.</p>
    <p class="normal">To use a secret mounted as a volume, the pod manifest should declare the volume and it should be mounted in the container’s spec:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-with-secret</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">container-with-secret</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/py-kube:0.3</span>
    <span class="hljs-attr">command:</span> [<span class="hljs-string">"/bin/bash"</span>, <span class="hljs-string">"-c"</span>, <span class="hljs-string">"while true ; do sleep 10 ; done"</span>]
    <span class="hljs-attr">volumeMounts:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">secret-volume</span>
      <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/mnt/hush-hush"</span>
      <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">volumes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">secret-volume</span>
    <span class="hljs-attr">secret:</span>
      <span class="hljs-attr">secretName:</span> <span class="hljs-string">hush-hush</span>
</code></pre>
    <p class="normal">The <a id="_idIndexMarker487"/>volume name (secret-volume) binds the pod volume to the mount in the container. Multiple containers can mount the same volume. When this pod is running, the username and password are available as files under <code class="inlineCode">/etc/hush-hush</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f pod-with-secret.yaml
pod/pod-with-secret created
$ k exec pod-with-secret -- cat /mnt/hush-hush/username
tobias
$ k exec pod-with-secret -- cat /mnt/hush-hush/password
cutoffs
</code></pre>
    <h2 id="_idParaDest-215" class="heading-2">Managing secrets with Vault</h2>
    <p class="normal">Kubernetes<a id="_idIndexMarker488"/> secrets are a good foundation for storing and managing sensitive data on Kubernetes. However, storing encrypted data in etcd is just the tip of the iceberg of an industrial-strength secret management solution. This is where Vault comes in.</p>
    <p class="normal">Vault is an identity-based source secret management system developed by HashiCorp since 2015. Vault is considered best in class and doesn’t really have any significant non-proprietary competitors. It exposes an HTTP API, CLI, and UI to manage your secrets. Vault is a mature and battle-tested solution that is being used by a large number of enterprise organizations as well as smaller companies. Vault has well-defined security and threat models that cover a lot of ground. Practically, as long as you can ensure the physical security of your Vault deployment, Vault will keep your secrets safe and make it easy to manage and audit them.</p>
    <p class="normal">When<a id="_idIndexMarker489"/> running Vault on Kubernetes there are several other important measures to ensure the Vault security model remains intact such as:</p>
    <ul>
      <li class="bulletList">Considerations for multi-tenant clusters (single Vault will be shared by all tenants)</li>
      <li class="bulletList">End-to-end TLS (Kubernetes may skip TLS under some conditions)</li>
      <li class="bulletList">Turn off process core dumps to avoid revealing Vault encryption keys</li>
      <li class="bulletList">Ensure mlock is enabled to avoid swapping memory to disk and revealing Vault encryption keys</li>
      <li class="bulletList">Container supervisor and pods should run as non-root</li>
    </ul>
    <p class="normal">You can find a lot of information about Vault here: <a href="https://www.vaultproject.io/"><span class="url">https://www.vaultproject.io/</span></a>.</p>
    <p class="normal">Deploying and configuring Vault is pretty straightforward. If you want to try it out, follow this tutorial: <a href="https://learn.hashicorp.com/tutorials/vault/kubernetes-minikube-raft"><span class="url">https://learn.hashicorp.com/tutorials/vault/kubernetes-minikube-raft</span></a>.</p>
    <h1 id="_idParaDest-216" class="heading-1">Running a multi-tenant cluster</h1>
    <p class="normal">In this section, we will<a id="_idIndexMarker490"/> look briefly at the option to use a single cluster to host systems for multiple users or multiple user communities (which is also known as multi-tenancy). The idea is that those users are totally isolated and may not even be aware that they share the cluster with other users. </p>
    <p class="normal">Each user community will have its own resources, and there will be no communication between them (except maybe through public endpoints). The Kubernetes namespace concept is the ultimate expression of this idea. But, they don’t provide absolute isolation. Another solution is to use virtual clusters where each namespace appears as a completely independent cluster to the users.</p>
    <p class="normal">Check out <a href="https://www.vcluster.com/"><span class="url">https://www.vcluster.com/</span></a> for more details about virtual clusters.</p>
    <h2 id="_idParaDest-217" class="heading-2">The case for multi-tenant clusters</h2>
    <p class="normal">Why<a id="_idIndexMarker491"/> should you run a single cluster for multiple isolated users or deployments? Isn’t it simpler to just have a dedicated cluster for each user? There are two main reasons: cost and operational complexity. If you have many relatively small deployments and you want to create a dedicated cluster for each one, then you’ll have a separate control plane node and possibly a three-node etcd cluster for each one. The<a id="_idIndexMarker492"/> cost can add up. Operational complexity is very important too. Managing tens, hundreds, or thousands of independent clusters is no picnic. Every upgrade and every patch needs to be applied to each cluster. Operations might fail and you’ll have to manage a fleet of clusters where some of them are in a slightly different state than the others. Meta-operations across all clusters may be more difficult. You’ll have to aggregate and write your tools to perform operations and collect data from all clusters.</p>
    <p class="normal">Let’s look at some use cases and requirements for multiple isolated communities or deployments:</p>
    <ul>
      <li class="bulletList">A platform or service provider for software-as-a-service</li>
      <li class="bulletList">Managing separate testing, staging, and production environments</li>
      <li class="bulletList">Delegating responsibility to community/deployment admins</li>
      <li class="bulletList">Enforcing resource quotas and limits on each community</li>
      <li class="bulletList">Users see only resources in their community</li>
    </ul>
    <h2 id="_idParaDest-218" class="heading-2">Using namespaces for safe multi-tenancy</h2>
    <p class="normal">Kubernetes <a id="_idIndexMarker493"/>namespaces are a good start for safe multi-tenant clusters. This is not a surprise, as this was one of the design goals of namespaces.</p>
    <p class="normal">You can easily create namespaces in addition to the built-in <code class="inlineCode">kube-system</code> and default. Here is a YAML file that will create a new namespace called <code class="inlineCode">custom-namespace</code>. All it has is a metadata item called name. It doesn’t get any simpler:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">custom-namespace</span>
</code></pre>
    <p class="normal">Let’s create the namespace:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f custom-namespace.yaml
namespace/custom-namespace created
$ k get ns
NAME                 STATUS   AGE
custom-namespace     Active   5s
default              Active   24h
kube-node-lease      Active   24h
kube-public          Active   24h
kube-system          Active   24h
</code></pre>
    <p class="normal">We can see the default namespace, our new <code class="inlineCode">custom-namespace</code>, and a few other system namespaces prefixed with <code class="inlineCode">kube-</code>.</p>
    <p class="normal">The status<a id="_idIndexMarker494"/> field can be <code class="inlineCode">Active</code> or <code class="inlineCode">Terminating</code>. When you delete a namespace, it will move into the <code class="inlineCode">Terminating</code> state. When the namespace is in this state, you will not be able to create new resources in this namespace. This simplifies the clean-up of namespace resources and ensures the namespace is really deleted. Without it, the replication controllers might create new pods when existing pods are deleted.</p>
    <p class="normal">Sometimes, namespaces may hang during termination. I wrote a little Go tool called k8s-namespace-deleter to delete stubborn namespaces.</p>
    <p class="normal">Check it out here: <a href="https://github.com/the-gigi/k8s-namespace-deleter"><span class="url">https://github.com/the-gigi/k8s-namespace-deleter</span></a> </p>
    <p class="normal">It can also be used as kubectl plugin.</p>
    <p class="normal">To work with a namespace, you add the <code class="inlineCode">--namespace</code> (or <code class="inlineCode">-n</code> for short) argument to kubectl commands. Here is how to run a pod in interactive mode in the <code class="inlineCode">custom-namespace</code> namespace:</p>
    <pre class="programlisting gen"><code class="hljs">$ k run trouble -it -n custom-namespace --image=g1g1/py-kube:0.3 bash
If you don't see a command prompt, try pressing enter. 
root@trouble:/# 
</code></pre>
    <p class="normal">Listing pods in the <code class="inlineCode">custom-namespace</code> returns only the pod we just launched:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po -n custom-namespace
NAME      READY   STATUS    RESTARTS      AGE
trouble   1/1     Running   1 (15s ago)   57s
</code></pre>
    <h2 id="_idParaDest-219" class="heading-2">Avoiding namespace pitfalls</h2>
    <p class="normal">Namespaces <a id="_idIndexMarker495"/>are great, but they can add some friction. When you use just the default namespace, you can simply omit the namespace. When using multiple namespaces, you must qualify everything with the namespace. This can add some burden but doesn’t present any danger.</p>
    <p class="normal">However, if some users (for example, cluster administrators) can access multiple namespaces, then you’re open to accidentally modifying or querying the wrong namespace. The best way to avoid this situation is to hermetically seal the namespace and require different users and credentials for each namespace, just like you should use a user account for most operations on your machine or remote machines and use root via sudo only when you have to.</p>
    <p class="normal">In addition, you <a id="_idIndexMarker496"/>should use tools that help make it clear what namespace you’re operating on (for example, shell prompt if working from the command line or listing the namespace prominently in a web interface). One of the most popular tools is kubens (available along with kubectx), available at <a href="https://github.com/ahmetb/kubectx"><span class="url">https://github.com/ahmetb/kubectx</span></a>.</p>
    <p class="normal">Make sure that users that can operate on a dedicated namespace don’t have access to the default namespace. Otherwise, every time they forget to specify a namespace, they’ll operate quietly on the default namespace.</p>
    <h2 id="_idParaDest-220" class="heading-2">Using virtual clusters for strong multi-tenancy</h2>
    <p class="normal">Namespaces <a id="_idIndexMarker497"/>are fine, but they don’t really cut it for strong multi-tenancy. Namespace isolation obviously works for namespaced resources only. But, Kubernetes has many cluster-level resources (in particular CRDs). Tenants will share those. In addition, the control plane version, security, and audit will be shared.</p>
    <p class="normal">One trivial solution is not to use multi-tenancy. Just have a separate cluster for each tenant. But, that is not efficient especially if you have a lot of small tenants.</p>
    <p class="normal">The vcluster project (<a href="https://www.vcluster.com"><span class="url">https://www.vcluster.com</span></a>) from Loft.sh utilizes an innovative approach where a physical Kubernetes cluster can host multiple virtual clusters that appear as regular Kubernetes clusters to their users, totally isolated from the other virtual clusters and the host cluster. This reaps all benefits of multi-tenancy without the downsides of namespace-level isolation.</p>
    <p class="normal">Here is the vcluster architecture: </p>
    <figure class="mediaobject"><img src="../Images/B18998_04_03.png" alt="Timeline  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 4.3: vcluster architecture</p>
    <p class="normal">Let’s create a <a id="_idIndexMarker498"/>couple of virtual clusters. First install the vcluster CLI: <a href="https://www.vcluster.com/docs/getting-started/setup"><span class="url">https://www.vcluster.com/docs/getting-started/setup</span></a>.</p>
    <p class="normal">Make sure it’s installed correctly:</p>
    <pre class="programlisting gen"><code class="hljs">$ vcluster version 
vcluster version 0.10.1
</code></pre>
    <p class="normal">Now, we can create some virtual clusters. You can create virtual clusters using the vcluster CLI, Helm, or kubectl. Let’s go with the vcluster CLI.</p>
    <pre class="programlisting gen"><code class="hljs">$ vcluster create tenant-1
info   Creating namespace vcluster-tenant-1
info   Detected local kubernetes cluster kind. Will deploy vcluster with a NodePort
info   Create vcluster tenant-1...
done √ Successfully created virtual cluster tenant-1 in namespace vcluster-tenant-1
info   Waiting for vcluster to come up...
warn   vcluster is waiting, because vcluster pod tenant-1-0 has status: ContainerCreating
info   Starting proxy container...
done √ Switched active kube context to vcluster_tenant-1_vcluster-tenant-1_kind-kind
- Use `vcluster disconnect` to return to your previous kube context
- Use `kubectl get namespaces` to access the vcluster
</code></pre>
    <p class="normal">Let’s create another virtual cluster:</p>
    <pre class="programlisting gen"><code class="hljs">$ vcluster create tenant-2
? You are creating a vcluster inside another vcluster, is this desired?
  [Use arrows to move, enter to select, type to filter]
&gt; No, switch back to context kind-kind
  Yes
</code></pre>
    <p class="normal">Oops. After <a id="_idIndexMarker499"/>creating the <code class="inlineCode">tenant-1 </code>virtual cluster the Kubernetes context changed to this cluster. When I tried to create <code class="inlineCode">tenant-2</code>, the vcluster CLI was smart enough to warn me. Let’s try again:</p>
    <pre class="programlisting gen"><code class="hljs">$ k config use-context kind-kind
Switched to context "kind-kind".
$ vcluster create tenant-2
info   Creating namespace vcluster-tenant-2
info   Detected local kubernetes cluster kind. Will deploy vcluster with a NodePort
info   Create vcluster tenant-2...
done √ Successfully created virtual cluster tenant-2 in namespace vcluster-tenant-2
info   Waiting for vcluster to come up...
info   Stopping docker proxy...
info   Starting proxy container...
done √ Switched active kube context to vcluster_tenant-2_vcluster-tenant-2_kind-kind
- Use `vcluster disconnect` to return to your previous kube context
- Use `kubectl get namespaces` to access the vcluster
</code></pre>
    <p class="normal">Let’s check our clusters: </p>
    <pre class="programlisting gen"><code class="hljs">$ k config get-contexts -o name
kind-kind
vcluster_tenant-1_vcluster-tenant-1_kind-kind
vcluster_tenant-2_vcluster-tenant-2_kind-kind
</code></pre>
    <p class="normal">Yes, our two virtual clusters are available. Let’s see the namespaces in our host <code class="inlineCode">kind</code> cluster:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get ns --context kind-kind
NAME                 STATUS   AGE
custom-namespace     Active   3h6m
default              Active   27h
kube-node-lease      Active   27h
kube-public          Active   27h
kube-system          Active   27h
local-path-storage   Active   27h
vcluster-tenant-1    Active   15m
vcluster-tenant-2    Active   3m48s
</code></pre>
    <p class="normal">We can see the two new namespaces for the virtual clusters. Let’s see what’s running in the <code class="inlineCode">vcluster-tenant-1</code> namespace:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get all -n vcluster-tenant-1 --context kind-kind
NAME                                                    READY   STATUS    RESTARTS   AGE
pod/coredns-5df468b6b7-rj4nr-x-kube-system-x-tenant-1   1/1     Running   0          16m
pod/tenant-1-0                                          2/2     Running   0          16m
NAME                                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns-x-kube-system-x-tenant-1   ClusterIP   10.96.200.106   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   16m
service/tenant-1                            NodePort    10.96.107.216   &lt;none&gt;        443:32746/TCP            16m
service/tenant-1-headless                   ClusterIP   None            &lt;none&gt;        443/TCP                  16m
service/tenant-1-node-kind-control-plane    ClusterIP   10.96.235.53    &lt;none&gt;        10250/TCP                16m
NAME                        READY   AGE
statefulset.apps/tenant-1   1/1     16m
</code></pre>
    <p class="normal">Now, let’s see <a id="_idIndexMarker500"/>what namespaces are in the virtual cluster:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get ns --context vcluster_tenant-1_vcluster-tenant-1_kind-kind
NAME              STATUS   AGE
kube-system       Active   17m
default           Active   17m
kube-public       Active   17m
kube-node-lease   Active   17m
</code></pre>
    <p class="normal">Just the default namespaces of a k3s cluster (vcluster is based on k3s). Let’s create a new namespace and verify it shows up only in the virtual cluster:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create ns new-ns --context vcluster_tenant-1_vcluster-tenant-1_kind-kind
namespace/new-ns created
$ k get ns new-ns --context vcluster_tenant-1_vcluster-tenant-1_kind-kind
NAME     STATUS   AGE
new-ns   Active   19s
$ k get ns new-ns --context vcluster_tenant-2_vcluster-tenant-2_kind-kind
 Error from server (NotFound): namespaces "new-ns" not found
$ k get ns new-ns --context kind-kind
Error from server (NotFound): namespaces "new-ns" not found
</code></pre>
    <p class="normal">The new <a id="_idIndexMarker501"/>namespace is only visible in the virtual cluster it was created in as expected.</p>
    <p class="normal">In this section, we covered multi-tenant clusters, why they are useful, and different approaches for isolating tenants such as namespaces and virtual clusters.</p>
    <h1 id="_idParaDest-221" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we covered the many security challenges facing developers and administrators building systems and deploying applications on Kubernetes clusters. But we also explored the many security features and the flexible plugin-based security model that provides many ways to limit, control, and manage containers, pods, and nodes. Kubernetes already provides versatile solutions to most security challenges, and it will only get better as capabilities such as AppArmor and various plugins move from alpha/beta status to general availability. Finally, we considered how to use namespaces and virtual clusters to support multi-tenant communities or deployments in the same Kubernetes cluster.</p>
    <p class="normal">In the next chapter, we will look in detail into many Kubernetes resources and concepts and how to use them and combine them effectively. The Kubernetes object model is built on top of a solid foundation of a small number of generic concepts such as resources, manifests, and metadata. This empowers an extensible, yet surprisingly consistent, object model to expose a very diverse set of capabilities for developers and administrators.</p>
    <h1 id="_idParaDest-222" class="heading-1">Join us on Discord!</h1>
    <p class="normal">Read this book alongside other users, cloud experts, authors, and like-minded professionals.</p>
    <p class="normal">Ask questions, provide solutions to other readers, chat with the authors via. Ask Me Anything sessions and much more.</p>
    <p class="normal">Scan the QR code or visit the link to join the community now.</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img src="../Images/QR_Code844810820358034203.png" alt=""/></p>
  </div>
</body></html>