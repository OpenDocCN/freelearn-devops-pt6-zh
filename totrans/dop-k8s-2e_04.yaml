- en: Managing Stateful Workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](a5cf080a-372a-406e-bb48-019af313c676.xhtml), *Getting Started
    with Kubernetes,* we introduced the basic functions of Kubernetes. Once you start
    to deploy containers with Kubernetes, you'll need to consider the application's
    data life cycle and CPU/memory resource management.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How a container behaves with volumes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Kubernetes' volume functionalities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices and pitfalls of Kubernetes' persistent volume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Submitting a short-lived application as a Kubernetes Job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes volume management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes and Docker use a local disk by default. The Docker application may
    store and load any data onto the disk, for example, log data, temporary files,
    and application data. As long as the host has enough space and the application
    has the necessary permission, the data will exist as long as a container exists.
    In other words, when a container terminates, exits, crashes, or is reassigned
    to another host, the data will be lost.
  prefs: []
  type: TYPE_NORMAL
- en: Container volume life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to understand Kubernetes'' volume management, you''ll need to understand
    the Docker volume life cycle. The following example is how Docker behaves with
    a volume when a container restarts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In Kubernetes, you also need to take care of pod restart. In the case of a resource
    shortage, Kubernetes may stop a container and then restart a container on the
    same or another Kubernetes node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows how Kubernetes behaves when there is a resource
    shortage. One pod is killed and restarted when an out of memory error is received:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Sharing volume between containers within a pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 3](a5cf080a-372a-406e-bb48-019af313c676.xhtml), *Getting Started with
    Kubernetes,* stated that multiple containers within the same Kubernetes pod can
    share the same pod IP address, network port, and IPC. Therefore, applications
    can communicate with each other through a localhost network. However, the filesystem
    is segregated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows that **Tomcat** and **nginx** are in the same pod.
    Those applications can communicate with each other via localhost. However, they
    can''t access each other''s `config` files:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/495d3ce3-fbe2-4547-8a35-dec6534ab5d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Some applications won't affect these scenarios and behavior, but some applications
    may have some use cases that require them to use a shared directory or file. Consequently,
    developers and Kubernetes administrators need to be aware of the different types
    of stateless and stateful applications.
  prefs: []
  type: TYPE_NORMAL
- en: Stateless and stateful applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stateless applications don't need to preserve the application or user data on
    the disk volume. Although stateless applications may write the data to the filesystem
    while a container exists, it is not important in terms of the application's life
    cycle.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the `tomcat` container runs some web applications. It also writes
    an application log under `/usr/local/tomcat/logs/`, but it won't be affected if
    it loses a `log` file.
  prefs: []
  type: TYPE_NORMAL
- en: But what if you need to persist an application log for analysis or auditing?
    In this scenario, Tomcat can still be stateless but share the `/usr/local/tomcat/logs`
    volume with another container such as Logstash ([https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)).
    Logstash will then send a log to the chosen analytic store, such as Elasticsearch
    ([https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the `tomcat` container and `logstash` container must be in the
    same Kubernetes pod and share the `/usr/local/tomcat/logs` volume, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e69e5d06-bcea-44d3-9c22-a784dddfa08c.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows how Tomcat and Logstash can share the `log` file
    using the Kubernetes `emptyDir` volume ([https://kubernetes.io/docs/concepts/storage/volumes/#emptydir](https://kubernetes.io/docs/concepts/storage/volumes/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Tomcat and Logstash didn''t use the network via localhost, but they did share
    the filesystem between `/usr/local/tomcat/logs` from the Tomcat container and
    `/mnt` from the `logstash` container, through Kubernetes'' `emptyDir` volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create `tomcat` and `logstash` pod, and then see whether Logstash can
    see the Tomcat application log under `/mnt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this scenario, Elasticsearch must be stateful in the final destination, meaning
    that it uses a persistent volume. The Elasticsearch container must preserve the
    data even if the container is restarted. In addition, you do not need to configure
    the Elasticsearch container within the same pod as Tomcat/Logstash. Because Elasticsearch
    should be a centralized log datastore, it can be separated from the Tomcat/Logstash
    pod and scaled independently.
  prefs: []
  type: TYPE_NORMAL
- en: Once you determine that your application needs a persistent volume, there are
    some different types of volumes and different ways to manage persistent volumes
    to look at.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes' persistent volume and dynamic provisioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes supports a variety of persistent volumes, for example, public cloud
    storage such as AWS EBS and Google persistent disks. It also supports network
    (distributed) filesystems such as NFS, GlusterFS, and Ceph. In addition, it can
    also support a block device such as iSCSI and Fibre Channel. Based on the environment
    and infrastructure, a Kubernetes administrator can choose the best matching types
    of persistent volume.
  prefs: []
  type: TYPE_NORMAL
- en: The following example is using a GCP persistent disk as a persistent volume.
    The first step is to create a GCP persistent disk and name it `gce-pd-1`.
  prefs: []
  type: TYPE_NORMAL
- en: If you use AWS EBS, Google persistent disk, or Azure disk storage, the Kubernetes
    node must be in the same cloud platform. In addition, Kubernetes has a limit on
    maximum volumes per node. Please look at the Kubernetes documentation at [https://kubernetes.io/docs/concepts/storage/storage-limits/](https://kubernetes.io/docs/concepts/storage/storage-limits/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c4fd27c-12fe-4a90-94c8-a7f3290615f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, specify the name `gce-pd-1` in the `Deployment` definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This will mount the persistent disk from the GCE persistent disk to `/usr/local/tomcat/logs`,
    which can persist Tomcat application logs.
  prefs: []
  type: TYPE_NORMAL
- en: Abstracting the volume layer with a persistent volume claim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Directly specifying a persistent volume in a configuration file makes a tight
    couple with a particular infrastructure. For the preceding example (`tomcat-log`
    volume), `pdName` is `gce-pd-1` and Volume type is `gcePersistentDisk`. From a
    container management point of view, the pod definition shouldn't be locked into
    the specific environment because the infrastructure could be different based on
    the environment. The ideal pod definition should be flexible or abstract the actual
    infrastructure and specify only volume name and mount point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, Kubernetes provides an abstraction layer that associates the
    pod with the persistent volume, which is called the **Persistent Volume Claim**
    (**PVC**). This allows us to decouple from the infrastructure. The Kubernetes
    administrator just needs to pre-allocate the size of the persistent volume in
    advance. Then Kubernetes will bind the persistent volume and the PVC as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe65323a-e6db-489a-b13f-98f0f3c2be7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following example is a definition of a pod that uses a PVC; let''s reuse
    the previous example (`gce-pd-1`) to register with Kubernetes first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that we assign `storageClassName` as `my-10g-pv-1`, as an identifier that
    the PVC can bind to by specifying the same name.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a PVC that associates with the persistent volume (`pv-1`).
  prefs: []
  type: TYPE_NORMAL
- en: The `storageClassName` parameter lets Kubernetes use static provisioning. This
    is because some Kubernetes environments, such as **Google Container Engine** (**GKE**),
    are already set up with dynamic provisioning. If we don't specify `storageClassName`,
    Kubernetes will allocate a new `PersistentVolume` and then bind to `PersistentVolumeClaim`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the `tomcat` setting has been decoupled from the GCE persistent volume,
    and bound to the abstracted volume, `pvc-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Dynamic provisioning and StorageClass
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PVC a degree of flexibility for persistent volume management. However, pre-allocating
    some persistent volume pools might not be cost-efficient, especially in a public
    cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes also assists in this kind of situation by supporting dynamic provisioning
    for persistent volumes. The Kubernetes administrator defines the provisioner of
    the persistent volume, which is called `StorageClass`. Then, the PVC asks `StorageClass`
    to dynamically allocate a persistent volume, and then associates it with the PVC
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc67e1e5-9606-478f-867d-b539c1ae2f16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following example, AWS EBS is used as the `StorageClass`. When creating
    the PVC, the `StorageClass` dynamically creates an EBS then registers it as **Persistent
    Volume** (**PV**), and then attaches it to the PVC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Once `StorageClass` has been successfully created, then, create a PVC without
    PV, but specify the `StorageClass` name. In this example, this would be `aws-sc`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the EBS after submitting to StorageClass to
    create a PVC. AWS console shows a new EBS which is created by StorageClass:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cd3cac5-a5bd-4a17-901c-ad5687be8449.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that managed Kubernetes services such as Amazon EKS ([https://aws.amazon.com/eks/](https://aws.amazon.com/eks/)), Google
    Kubernetes Engine ([https://cloud.google.com/container-engine/](https://cloud.google.com/container-engine/)),
    and Azure Kubernetes Service ([https://azure.microsoft.com/en-us/services/kubernetes-service/](https://azure.microsoft.com/en-us/services/kubernetes-service/))
    create `StorageClass` by default. For example, Google Kubernetes Engine sets up
    a default storage class as a Google Cloud persistent disk. For more information,
    please refer to [Chapter 10](f55d3fa8-e791-4473-83ba-ed8c4f848a90.xhtml), *Kubernetes
    on AWS*, [Chapter 11](d4de05e3-eb24-4e8e-bfd3-e68819b5e66c.xhtml)*, Kubernetes
    on* *GCP*, and [Chapter 12](89891610-4ca4-4216-9d76-2613d186421c.xhtml), *Kubernetes
    on Azure*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Problems with ephemeral and persistent volume settings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may determine your application as stateless because the `datastore` function
    is handled by another pod or system. However, there are some pitfalls in that
    sometimes; applications actually store important files that you aren't aware of.
    For example, Grafana ([https://grafana.com/grafana](https://grafana.com/grafana))
    connects time series datasources such as Graphite ([https://graphiteapp.org](https://graphiteapp.org))
    and InfluxDB ([https://www.influxdata.com/time-series-database/](https://www.influxdata.com/time-series-database/)),
    so people may misunderstand that Grafana is a stateless application.
  prefs: []
  type: TYPE_NORMAL
- en: Actually, Grafana itself also uses databases to store user, organization, and
    dashboard metadata. By default, Grafana uses SQLite3 components and stores the
    database as `/var/lib/grafana/grafana.db`. Therefore, when a container is restarted,
    the Grafana settings will all be reset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates how Grafana behaves with an ephemeral volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, navigate to the Grafana web console to create the Grafana `Organizations` named
    `kubernetes org`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5eb22b7-fce0-4566-8f1a-15bad07ff121.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, look at the `Grafana` directory. Here, there is a database file (`/var/lib/grafana/grafana.db`)
    with a timestamp that has been updated after creating a Grafana `Organizations`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'When the pod is deleted, the `Deployment` will start a new pod and check whether
    a Grafana `Organizations` exists or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It looks like the `sessions` directory has disappeared and `grafana.db` has
    also been recreated by the Docker image again. If you access the web console,
    the Grafana `organization` will also disappear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b0906787-27fb-4d3b-b177-b42955fa24ef.png)'
  prefs: []
  type: TYPE_IMG
- en: How about just attaching a persistent volume to Grafana? You'll soon find that
    mounting a persistent volume on a pod controlled by a `Deployment` doesn't scale
    properly as every new pod attempts to mount the same persistent volume. In most
    cases, only the first pod can mount the persistent volume. Other pods will try
    to mount the volume, and they will give up and freeze if they can't. This happens
    if the persistent volume is capable of only RWO (read write once; only one pod
    can write).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, Grafana uses a persistent volume to mount `/var/lib/grafana`;
    however, it can''t scale because the Google persistent disk is RWO:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Even if the persistent volume has the RWX capability (read write many; many
    pods can mount to read and write simultaneously), such as NFS, it won''t complain
    if multiple pods try to bind the same volume. However, we still need to consider
    whether multiple application instances can use the same folder/file or not. For
    example, if it replicates Grafana to two or more pods, it will be conflicted,
    with multiple Grafana instances that try to write to the same `/var/lib/grafana/grafana.db`,
    and then the data could be corrupted, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eeb24510-e346-4bd1-941b-f4f4b1fe91e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this scenario, Grafana must use backend databases such as MySQL or PostgreSQL,
    instead of SQLite3, as follows. It allows multiple Grafana instances to read/write
    Grafana metadata properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7cb8468-f326-4dde-8750-2f76fad6baa3.png)'
  prefs: []
  type: TYPE_IMG
- en: Because RDBMS basically supports connecting with multiple application instances
    via a network, this scenario is perfectly suited to being used by multiple pods.
    Note that Grafana supports using RDBMS as a backend metadata store; however, not
    all applications support RDBMS.
  prefs: []
  type: TYPE_NORMAL
- en: For the Grafana configuration that uses MySQL/PostgreSQL, please see the online
    documentation at
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.grafana.org/installation/configuration/#database](http://docs.grafana.org/installation/configuration/#database).'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the Kubernetes administrator needs to carefully monitor how an application
    behaves with volumes, and understand that in some use cases, just using a persistent
    volume may not help because of issues that might arise when scaling pods.
  prefs: []
  type: TYPE_NORMAL
- en: If multiple pods need to access the centralized volume, then consider using
    the database as previously shown, if applicable. On the other hand, if multiple
    pods need an individual volume, consider using `StatefulSet`.
  prefs: []
  type: TYPE_NORMAL
- en: Replicating pods with a persistent volume using StatefulSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`StatefulSet` was introduced in Kubernetes 1.5; this consists of a bond between
    the pod and the persistent volume. When scaling a pod that increases or decreases,
    pod and persistent volume are created or deleted together.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, the pod-creation process is serial. For example, when requesting
    Kubernetes to scale two additional `StatefulSet`s, Kubernetes creates **Persistent
    Volume Claim 1** and **Pod 1** first, and then creates **Persistent Volume Claim
    2** and **Pod 2**, but not simultaneously. This helps the administrator if an
    application registers to a registry during the application bootstrap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/871b9722-059b-4ee2-8a53-00e066f19cd2.png)'
  prefs: []
  type: TYPE_IMG
- en: Even if one pod is dead, `StatefulSet` preserves the position of the pod (Kubernetes
    metadata, such as the pod name) and the persistent volume. Then it attempts to
    recreate a container that it reassigns to the same pod and mounts the same persistent
    volume.
  prefs: []
  type: TYPE_NORMAL
- en: If you run the headless service with `StatefulSet`, Kubernetes also assigns
    and preserves the FQDN for the pod as well.
  prefs: []
  type: TYPE_NORMAL
- en: Headless services will be described in detail in [Chapter 6](fc67e008-b601-45a6-8297-d2fa28360b1f.xhtml)*,
    Kubernetes Network*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This helps to keep track of and maintain the number of pods/persistent volumes
    and the application remains online using the Kubernetes scheduler:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddea9f11-3327-4aee-abc7-42a522b8a9db.png)'
  prefs: []
  type: TYPE_IMG
- en: '`StatefulSet` with a persistent volume requires dynamic provisioning and `StorageClass`
    because `StatefulSet` can be scalable. Kubernetes needs to know how to provision
    the persistent volume when adding more pods.'
  prefs: []
  type: TYPE_NORMAL
- en: Submitting Jobs to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, the application is designed to be long-lived in the same way as
    the daemon process. A typical long-lived application opens the network port and
    keeps it running. It is required to keep running the application. If it fails,
    you will need to restart to recover the state. Therefore, using Kubernetes deployment
    is the best option for the long-lived application.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, some applications are designed to be short-lived, such as
    a command script. It is expected to exit the application even if it is successful
    in order to finish the tasks. Therefore, a Kubernetes deployment is not the right
    choice, because a deployment tries to keep the process running.
  prefs: []
  type: TYPE_NORMAL
- en: No worries; Kubernetes also supports short-lived applications. You can submit
    a container as a **Job** or **Scheduled Job**, and Kubernetes will dispatch it
    to an appropriate node and execute your container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes supports several types of Jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: Single Job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeatable Job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel Job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduled Job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last one is also called a **CronJob**. Kubernetes supports these different
    types of Jobs that are used differently to pods to utilize your resources.
  prefs: []
  type: TYPE_NORMAL
- en: Submitting a single Job to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Job-like pod is suitable to run for batch programs such as collecting data,
    querying the database, generating a report, and so on. Although this is referred
    to as short-lived, it doesn't matter how long is spent on it. This may need to
    run for a few seconds, or perhaps a few days, in order to complete. It will eventually
    exit an application, which means it has an end state.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is capable of monitoring a short-lived application as a Job, and
    in the case of failure, Kubernetes will create a new pod for the Job that tries
    to accomplish your application to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to submit a Job to Kubernetes, you need to write a Job template that
    specifies the pod configuration. The following example demonstrates how to check
    the `dpkg` installed in Ubuntu Linux:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Job definition is similar to pod definition, but the important settings are
    `activeDeadlineSeconds` and `restartPolicy`. The `activeDeadlineSeconds `parameter sets
    the maximum timescale for the pod to run. If exceeded, the pod will be terminated.
    The `restartPolicy` parameter defines how Kubernetes behaves in the case of failure.
    For example, when the pod is crashed if you specify `Never`, Kubernetes doesn't
    restart; if you specify `OnFailure`, Kubernetes attempts to resubmit the Job until
    successfully completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `kubectl` command to submit a Job to see how Kubernetes manages the
    pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Because this Job (the `dpkg-query -l` command) is short-lived, it will `exit()`
    eventually. Therefore, if the `dpkg-query` command completes gracefully, Kubernetes
    doesn''t attempt to restart, even if no active pod exists. So, when you type `kubectl
    get pods`, the pod status will be `completed` after finishing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Although no active pod exists, you still have an opportunity to check the application
    log by typing the `kubectl logs` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Submitting a repeatable Job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Users can also decide the number of tasks that should be finished in a single
    Job. This is helpful for solving some random sampling problems. Let''s reuse the
    previous template and add `spec.completions` to see the differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, three pods are created to complete this Job. This is useful
    if you need to run your program repeatedly at particular times. However, as you
    may have noticed from the Age column in the preceding result, these pods ran sequentially
    one by one. In the preceding result, the ages are 7 seconds, 4 seconds, then 2
    seconds. This means that the second Job was started after the first Job was completed,
    and the third Job was started after the second Job was completed.
  prefs: []
  type: TYPE_NORMAL
- en: If it is the case that a Job runs for a longer period (such as a few days),
    but if there is no correlation needs between the 1^(st), 2^(nd), and 3^(rd) Job,
    then it does not make sense to run them sequentially. In this case, use a parallel
    Job.
  prefs: []
  type: TYPE_NORMAL
- en: Submitting a parallel Job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If your batch Job doesn''t have a state or dependency between Jobs, you may
    consider submitting Jobs in parallel. To do so, similar to the `spec.completions`
    parameter, the Job template has a `spec.parallelism` parameter to specify how
    many Jobs you want to run in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As you see from the `AGE` column through the `kubectl get pods` command, it
    indicates that the three pods ran at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: In this setting, Kubernetes can dispatch to an available node to run your application
    and that easily scales your Jobs. This is useful if you want to run something
    like a worker application to distribute a bunch of pods to different nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, if you no longer need to check the Job''s results anymore, delete the
    resource by using the `kubectl delete` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Scheduling running a Job using CronJob
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are familiar with **UNIX CronJob** or **Java Quartz** ([http://www.quartz-scheduler.org](http://www.quartz-scheduler.org)),
    Kubernetes CronJob is a very straightforward tool that you can use to define a
    particular timing to run your Kubernetes Job repeatedly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scheduling format is very simple; it specifies the following five items:'
  prefs: []
  type: TYPE_NORMAL
- en: Minutes (0 to 59)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hours (0 to 23)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Days of the month (1 to 31)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Months (1 to 12)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Days of the week (0: Sunday to 6: Saturday)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if you only want to run your Job at 9:00 am on November 12th every
    year to send a birthday greeting to me, the schedule format would be `0 9 12 11
    *`
  prefs: []
  type: TYPE_NORMAL
- en: 'You may also use a slash (`/`) to specify a step value. Running the previous
    Job example at five-minute intervals would have the following schedule format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following template is using a `CronJob` to run the `package-check` command
    every five minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You may notice that the template format is slightly different from the Job
    template here. However, there is one parameter we need to pay attention to: `spec.concurrencyPolicy`.
    With this, you can specify a behavior if the previous Job is not finished but
    the next Job''s schedule is approaching. This determines how the next Job runs.
    You can set the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Allow**: Allow execution of the next Job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Forbid**: Skip execution of the next Job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replace**: Delete the current Job, then execute the next Job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you set `Allow`, there might be the potential risk of accumulating some unfinished
    Jobs in the Kubernetes cluster. Therefore, during the testing phase, you should
    set either `Forbid` or `Replace` to monitor Job execution and completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a few moments, the Job will be triggered by your desired timing—in this
    case, every five minutes. You may then see the Job entry with the `kubectl get
    jobs` and `kubectl get pods` commands, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`CronJob` will remain until you delete it. This means that, every five minutes,
    `CronJob` will create a new Job entry and related pods will also keep getting
    created. This will impact the consumption of Kubernetes resources. Therefore,
    by default, `CronJob` will keep up to three successful Jobs (with `spec.successfulJobsHistoryLimit`)
    and one failed Job (with `spec.failedJobsHistoryLimit`). You can change these
    parameters based on your requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, `CronJob` allows Jobs to automatically run in your application with
    the desired timing. You can utilize `CronJob` to run report-generation Jobs, daily
    or weekly batch Jobs, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered stateful applications that use persistent volumes.
    Compared to ephemeral volumes, they have some pitfalls when an application restarts
    or a pod scales. In addition, persistent volume management on Kubernetes has been
    enhanced to make it easier, as you can see from tools such as `StatefulSet` and
    dynamic provisioning.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, Jobs and CronJobs are special utilities for pods. Compared to deployment/ReplicaSets,
    this has a desired number of pods running, which is against Job's ideal situation
    in which the pods should be deleted once they finish their tasks. This kind of
    short-lived application can be managed by Kubernetes as well.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](17a735e1-3810-4b77-a9ac-fac0120b0b90.xhtml), *Cluster Administration
    and Extension*, we will discuss the cluster administration such as authentication,
    authorization, and admission control. We will also introduce the **Custom Resource
    Definition** (**CRD**) that how to control Kubernetes object by your own code.
  prefs: []
  type: TYPE_NORMAL
