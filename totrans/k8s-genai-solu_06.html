<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer056" epub:type="chapter">&#13;
			<h1 id="_idParaDest-76" class="chapter-number"><a id="_idTextAnchor075"/>6</h1>&#13;
			<h1 id="_idParaDest-77"><a id="_idTextAnchor076"/>Scaling GenAI Applications on Kubernetes</h1>&#13;
			<p>In this chapter, we will cover <strong class="bold">application scaling</strong> strategies and best practices for Kubernetes. Application <a id="_idIndexMarker470"/>scaling is a process where K8s can dynamically scale the resources to match the application demand, ensuring efficient and cost-effective resource utilization and optimal application performance. Kubernetes provides different scaling mechanisms to scale applications based on metrics such as CPU usage, memory, or <span class="No-Break">custom metrics.</span></p>&#13;
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break"><strong class="bold">Scaling metrics</strong></span></li>&#13;
				<li><span class="No-Break"><strong class="bold">HorizontalPodAutoscaler</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">HPA</strong></span><span class="No-Break">)</span></li>&#13;
				<li><span class="No-Break"><strong class="bold">VerticalPodAutoscaler</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">VPA</strong></span><span class="No-Break">)</span></li>&#13;
				<li><strong class="bold">Kubernetes Event-Driven </strong><span class="No-Break"><strong class="bold">Autoscaler</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">KEDA</strong></span><span class="No-Break">)</span></li>&#13;
				<li><strong class="bold">Cluster </strong><span class="No-Break"><strong class="bold">Autoscaler</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">CA</strong></span><span class="No-Break">)</span></li>&#13;
				<li><span class="No-Break"><strong class="bold">Karpenter</strong></span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-78"><a id="_idTextAnchor077"/>Scaling metrics</h1>&#13;
			<p>To scale applications<a id="_idIndexMarker471"/> correctly, it is essential to choose the right metrics to ensure efficient resource utilization and a seamless end user experience. These metrics can be divided into conventional and <span class="No-Break">custom metrics.</span></p>&#13;
			<h2 id="_idParaDest-79"><a id="_idTextAnchor078"/>Conventional metrics</h2>&#13;
			<p>These are common<a id="_idIndexMarker472"/> metrics in Kubernetes used<a id="_idIndexMarker473"/> for horizontal or <span class="No-Break">vertical scaling:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">CPU usage</strong>: This measures the percentage of CPU utilization. High CPU utilization might indicate that the application is under heavy load, requiring more instances (Pods) to handle the demand, whereas a constantly low CPU usage might indicate the overprovisioning of resources, and the number of Pods can be <span class="No-Break">scaled down.</span></li>&#13;
				<li><strong class="bold">Memory usage</strong>: This measures the amount of memory consumed by a Pod. Like CPU, high memory usage can signal that the application is handling a large amount of data, meaning more resources are needed to prevent memory shortages. When a process inside a container exceeds the memory limit, the container will be terminated by the container runtime (CRI), which is different from the CPU limit. If a CPU limit is set and a container exceeds that limit, the container will not get terminated but rather throttled or slowed down. Generally, it is not a best practice to use memory usage as a scaling metric because applications are often poor at <span class="No-Break">memory management.</span></li>&#13;
			</ul>&#13;
			<h2 id="_idParaDest-80"><a id="_idTextAnchor079"/>Custom metrics</h2>&#13;
			<p>Custom metrics<a id="_idIndexMarker474"/> allow K8s scaling based on more <a id="_idIndexMarker475"/>application or use case-specific metrics, allowing better granular control over the application’s performance. Some examples of custom metrics are <span class="No-Break">as follows:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">HTTP requests rate</strong>: This measures the number of HTTP/HTTPS requests or API calls the application receives per second. We could use monitoring tools, such as <strong class="bold">Prometheus</strong>, to <a id="_idIndexMarker476"/>track the request rates and scale the application when requests spike and exceed a <span class="No-Break">certain threshold.</span></li>&#13;
				<li><strong class="bold">Queue length</strong>: This measures the number of unprocessed jobs or messages in a queue, such as <strong class="bold">Amazon SQS</strong> or <strong class="bold">RabbitMQ</strong>. <em class="italic">Queue backlogs</em> indicate that the application<a id="_idIndexMarker477"/> is not <a id="_idIndexMarker478"/>able to keep up with the load and needs more resources to process jobs in a timely manner. It is a critical metric, especially <a id="_idIndexMarker479"/>in <strong class="bold">event-driven architecture</strong> (<strong class="bold">EDA</strong>). K8s scaling mechanisms, such as KEDA, support scaling based on <span class="No-Break">queue metrics.</span></li>&#13;
				<li><strong class="bold">Latency/response time</strong>: This measures the time it takes for the application to respond to requests. High latency often signals that the application is struggling under the current load, and scaling out additional instances can help maintain low <span class="No-Break">response times.</span></li>&#13;
				<li><strong class="bold">Error rate</strong>: This measures the number of failed requests. An increase in the error rate could indicate that the current number of resources is insufficient to handle the load, leading to failures; scaling up the resources might <span class="No-Break">be required.</span></li>&#13;
				<li><strong class="bold">Concurrency/active sessions</strong>: This measures the number of active connections, users, or sessions interacting with the application. For applications such as online games or video streaming platforms, the number of active users could be a critical indicator of the <span class="No-Break">application load.</span></li>&#13;
				<li><strong class="bold">GPU utilization</strong>: This measures the percentage of GPU capacity consumed by an application. When scaling GenAI applications on K8s, using GPU utilization as a scaling metric is effective because of heavy reliance and the indication of the load on the application. With <strong class="bold">NVIDIA GPUs</strong>, metrics<a id="_idIndexMarker480"/> are exported using<a id="_idIndexMarker481"/> the <strong class="bold">DCGM-Exporter</strong> addon (<a href="https://github.com/NVIDIA/dcgm-exporter">https://github.com/NVIDIA/dcgm-exporter</a>), which can be installed via Helm, allowing an observability agent (such as <strong class="bold">Prometheus</strong>) to scrape these metrics. We will configure this in our EKS cluster as part of <a href="B31108_12.xhtml#_idTextAnchor160"><span class="No-Break"><em class="italic">Chapter 12</em></span></a><span class="No-Break">.</span></li>&#13;
			</ul>&#13;
			<p>These are some of the <a id="_idIndexMarker482"/>commonly used metrics used in K8s to <a id="_idIndexMarker483"/>scale the resources. Some applications may require a mix of metrics. For example, a web application might use network I/O and request rate together to ensure optimal resource utilization and performance. Custom metrics are not available by default for K8s autoscaling; a <em class="italic">custom metrics adapter</em> needs to be installed to make them available from respective metric sources. This adapter acts as a bridge between the metrics system and K8s, exposing the metrics via K8s custom metrics API. Some<a id="_idIndexMarker484"/> examples are <strong class="bold">prometheus-adapter</strong> (<a href="https://github.com/kubernetes-sigs/prometheus-adapter">https://github.com/kubernetes-sigs/prometheus-adapter</a>) and <strong class="bold">Datadog Cluster </strong><span class="No-Break"><strong class="bold">Agent</strong></span><span class="No-Break"> (</span><a href="https://docs.datadoghq.com/containers/guide/cluster_agent_autoscaling_metrics"><span class="No-Break">https://docs.datadoghq.com/containers/guide/cluster_agent_autoscaling_metrics</span></a><span class="No-Break">).</span></p>&#13;
			<p>In this<a id="_idIndexMarker485"/> section, we discussed different types of scaling metrics, such as conventional and custom metrics, and had a look at some examples. We also discussed the custom GPU utilization metric and saw that it is an effective measure to scale the GenAI workloads. Next, let’s explore<a id="_idIndexMarker486"/> horizontal Pod autoscaling and see <a id="_idIndexMarker487"/>how these metrics can be used to autoscale <span class="No-Break">K8s Pods.</span></p>&#13;
			<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/>HorizonalPodAutoscaler (HPA)</h1>&#13;
			<p>HPA (<a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/</a>) is a K8s feature that<a id="_idIndexMarker488"/> adjusts the number of Pods in a deployment based on user-defined metrics, such as CPU or memory utilization. The primary goal of HPA is to ensure that applications can handle varying loads by dynamically scaling in or out the number of Pods. HPA does not apply to objects that can’t be scaled, such <span class="No-Break">as </span><span class="No-Break"><em class="italic">DaemonSets</em></span><span class="No-Break">.</span></p>&#13;
			<p>HPA uses a metrics server or monitoring system, such as Prometheus, to collect real-time data on the defined metrics. HPA has a <strong class="bold">controller</strong> component<a id="_idIndexMarker489"/> that runs in the Kubernetes control plane. It periodically checks the current metrics of the target application, such as deployment, and compares it to the desired thresholds specified in the HPA <span class="No-Break">resource configuration.</span></p>&#13;
			<p>Based on the metrics, the controller adjusts the desired number of Pods. If resource usage, such as CPU utilization, exceeds the threshold, HPA increases the number of Pods, whereas if the usage drops below the threshold, HPA decreases the number of Pods, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer053" class="IMG---Figure">&#13;
					<img src="image/B31108_06_1.jpg" alt="Figure 6.1 – HPA overview" width="1650" height="429"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – HPA overview</p>&#13;
			<p>The following YAML file indicates how HPA can be implemented for our e-commerce chatbot UI <a id="_idIndexMarker490"/>deployment <span class="No-Break">in K8s:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: autoscaling/v2&#13;
kind: HorizontalPodAutoscaler&#13;
metadata:&#13;
  name: chatbot-ui-hpa&#13;
spec:&#13;
  scaleTargetRef:&#13;
    apiVersion: apps/v1&#13;
    kind: Deployment&#13;
    name: chatbot-ui-deployment&#13;
  minReplicas: 1&#13;
  maxReplicas: 5&#13;
  metrics:&#13;
  - type: Resource&#13;
    resource:&#13;
      name: cpu&#13;
      target:&#13;
        type: Utilization&#13;
        averageUtilization: 70</pre>			<p>In this example, we are setting <strong class="source-inline">kind</strong> to <strong class="source-inline">HorizontalPodAutoscaler</strong>, which specifies that this manifest is for an HPA, and setting its <strong class="source-inline">name</strong> to <strong class="source-inline">chabot-ui-hpa</strong>. In the <strong class="source-inline">spec</strong> section, we are setting the scaling target for this HPA as <strong class="source-inline">chatbot-ui-deployment</strong>, which is a deployment, and <strong class="source-inline">apps/v1</strong> is the API version for the <span class="No-Break">target resource.</span></p>&#13;
			<p>Next, we set the minimum number of replicas to <strong class="source-inline">1</strong> (<strong class="source-inline">minReplicas: 1</strong>) and the maximum number of replicas to <strong class="source-inline">5</strong> (<strong class="source-inline">maxReplicas: 5</strong>). Finally, we set the metrics that HPA can monitor and use to make scaling decisions. In this example, we are using average CPU utilization across all the <em class="italic">chatbot-ui-deployment</em> deployment Pods as <span class="No-Break">the metric.</span></p>&#13;
			<p>In the <strong class="source-inline">target</strong> specification, <strong class="source-inline">averageUtilization: 70</strong> sets the target CPU utilization to <strong class="source-inline">70</strong>. If the average CPU utilization exceeds 70%, HPA will start scaling up the number of replicas to meet this target; however, it will not exceed five replicas due to the maximum limit we defined. Once the CPU utilization drops below 70%, it will start scaling down but will ensure that there is still one replica running all the time (<strong class="source-inline">minReplicas</strong>). You <a id="_idIndexMarker491"/>can also use K8s imperative commands to create and manage HPA resources. For example commands, refer to official K8s documentation <span class="No-Break">at </span><a href="https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-command/"><span class="No-Break">https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-command/</span></a><span class="No-Break">.</span></p>&#13;
			<p>You can download the HPA manifest from the GitHub repository at <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/chatbot-ui-hpa.yaml">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/chatbot-ui-hpa.yaml</a> and execute the following command to apply the HPA policy in our <span class="No-Break">EKS cluster.</span></p>&#13;
			<pre class="source-code">&#13;
$ kubectl apply -f chatbot-ui-hpa.yaml&#13;
horizontalpodautoscaler.autoscaling/chatbot-ui-hpa created</pre>			<p>To prevent frequent scaling up and down, the following HPA behavior configuration can <span class="No-Break">be used:</span></p>&#13;
			<pre class="source-code">&#13;
behavior:&#13;
  scaleDown:&#13;
    stabilizationWindowSeconds: 180&#13;
    policies:&#13;
    - type: Percent&#13;
      value: 50&#13;
      periodSeconds: 15&#13;
  scaleUp:&#13;
    stabilizationWindowSeconds: 0&#13;
    policies:&#13;
    - type: Pods&#13;
      value: 2&#13;
      periodSeconds: 15&#13;
    selectPolicy: Max</pre>			<p>The <strong class="source-inline">scaleDown</strong> section controls how fast the HPA can scale down or remove the replicas. The maximum allowed scale-down has been set to 50% in this example, which means that HPA can remove up to 50% of the current replicas during each scale-down event. <strong class="source-inline">periodSeconds</strong> defines the time window in seconds over which the scaling rule is evaluated. <strong class="source-inline">stabilizationWindowSeconds</strong> specifies the amount of time (in seconds) that HPA will wait before scaling down after detecting lower resource utilization. This helps prevent frequent and aggressive scaling down that might occur due to temporary drops <span class="No-Break">in usage.</span></p>&#13;
			<p>In this case, the <a id="_idIndexMarker492"/>stabilization window is set to 180 seconds, meaning that HPA will wait for 3 minutes before it reduces the number of replicas after a drop <span class="No-Break">in load.</span></p>&#13;
			<p>The <strong class="source-inline">scaleUp</strong> section of behavior classification controls how fast the HPA can scale up or add new Pods. In this case, we are defining scale-up policies using a fixed number of Pods instead of percentage based approach. A percentage based policy would increase the number of Pods by a certain percentage of the current replica count. For example, if we have 5 Pods and the policy allows a 60% increase, HPA could scale up by <span class="No-Break">3 Pods.</span></p>&#13;
			<p>In this example, scaling is done in fixed increments of two Pods and HPA will allow adding up to two Pods within every 15-second window, if more replicas are required. We have set <strong class="source-inline">stabilizationWindowSeconds</strong> to <strong class="source-inline">0</strong>, meaning there’s no delay before <span class="No-Break">scaling up.</span></p>&#13;
			<p>In this walkthrough, we created an HPA policy based on the Pod-level metrics, which aggregate the resource usage of all containers within a Pod. However, in multi-container Pods, this metric might not accurately represent the performance of an individual container. To address this, K8s introduced container resource metrics that allow HPA to track the resource usage of specific containers across Pods when scaling the target resource. This approach enables you to set scaling thresholds for the containers that are most critical to your application. For example, if your Pod includes a web application and a sidecar container that provides logging, you can configure HPA to scale based solely on the web application’s CPU utilization while ignoring the sidecar’s resource use. The following code snippet demonstrates how to use the CPU utilization of the <strong class="source-inline">web-app</strong> container to scale the <span class="No-Break">overall deployment:</span></p>&#13;
			<pre class="source-code">&#13;
type: ContainerResource&#13;
containerResource:&#13;
  name: cpu&#13;
  container: <strong class="bold">web-app</strong>&#13;
  target:&#13;
    type: Utilization&#13;
    averageUtilization: 60</pre>			<p>In this section, we explored HPA, a K8s feature designed to automatically adjust the number of Pods in a K8s Deployment, StatefulSet, or ReplicaSet based on user-defined metrics such as CPU, memory, or<a id="_idIndexMarker493"/> GPU utilization. Its primary goal is to maintain an adequate number of K8s Pods to handle dynamic workloads effectively. We also covered how to configure HPA policies and customize the scaling behavior. Next, let’s dive <span class="No-Break">into VPA.</span></p>&#13;
			<h1 id="_idParaDest-82"><a id="_idTextAnchor081"/>VerticalPodAutoscaler (VPA)</h1>&#13;
			<p>VPA can adjust the resource<a id="_idIndexMarker494"/> requests and limits for the CPU and memory of the Pods based on actual usage and configuration. This differs from HPA, which adjusts the number of Pods based on the metrics defined. VPA has four <span class="No-Break">operating modes:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Off</strong>: In this mode, VPA only<a id="_idIndexMarker495"/> provides resource recommendations, but does not apply <span class="No-Break">any changes.</span></li>&#13;
				<li><strong class="bold">Auto</strong>: VPA applies changes to resource requests and restarts the Pods, <span class="No-Break">if needed.</span></li>&#13;
				<li><strong class="bold">Recreate</strong>: VPA applies changes on Pod creation and updates them on existing Pods by <span class="No-Break">evicting them.</span></li>&#13;
				<li><strong class="bold">Initial</strong>: VPA applies resource recommendations only when new Pods are created or existing Pods are restarted, without interfering with the <span class="No-Break">running Pods.</span></li>&#13;
			</ul>&#13;
			<p>VPA collects resource usage data via the K8s Metrics API and suggests optimal CPU and memory values for resource requests and limits. In <em class="italic">Auto</em> mode, it can automatically evict Pods so that they are rescheduled with updated resource requests, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer054" class="IMG---Figure">&#13;
					<img src="image/B31108_06_2.jpg" alt="Figure 6.2 – VPA overview" width="1650" height="490"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – VPA overview</p>&#13;
			<p>Unlike HPA, VPA is not included with K8s by default; it is a separate project available on GitHub at <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler</a>. Install the VPA add-on by following the instructions provided <span class="No-Break">at </span><a href="https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/docs/installation.md"><span class="No-Break">https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/docs/installation.md</span></a><span class="No-Break">.</span></p>&#13;
			<p>The following YAML<a id="_idIndexMarker496"/> file implements VPA in our e-commerce chatbot UI application deployed <span class="No-Break">in K8s:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: autoscaling.k8s.io/v1&#13;
kind: VerticalPodAutoscaler&#13;
metadata:&#13;
  name: chatbot-ui-vpa&#13;
spec:&#13;
  targetRef:&#13;
    apiVersion: apps/v1&#13;
    kind: Deployment&#13;
    name: chatbot-ui-deployment&#13;
  updatePolicy:&#13;
    updateMode: "Auto"&#13;
  resourcePolicy:&#13;
    containerPolicies:&#13;
    - containerName: "*"&#13;
      minAllowed:&#13;
        cpu: "1000m"&#13;
        memory: "2Gi"&#13;
      maxAllowed:&#13;
        cpu: "2000m"&#13;
        memory: "4Gi"&#13;
      controlledValues: "RequestsAndLimits"</pre>			<p>You can download the VPA manifest from the GitHub repository at <a href="https://github.com/PacktPublishing/Kubernetes-for-Gen-AI-Models/blob/main/ch6/chatbot-ui-vpa.yaml"/><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/chatbot-ui-vpa.yaml">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/chatbot-ui-vpa.yaml</a> and execute<a id="_idIndexMarker497"/> the following command to apply the VPA policy in our <span class="No-Break">EKS cluster:</span></p>&#13;
			<pre class="source-code">&#13;
$ kubectl apply -f chatbot-ui-vpa.yaml&#13;
verticalpodautoscaler.autoscaling.k8s.io/chatbot-ui-vpa created</pre>			<p>In this example, we are setting <strong class="source-inline">kind</strong> to <strong class="source-inline">VerticalPodAutoscaler</strong> and <strong class="source-inline">updateMode</strong> to <strong class="source-inline">Auto</strong>, which means that VPA will automatically adjust resource requests and limits. The <strong class="source-inline">containerName *</strong> wildcard indicates that the policy should be applied to all containers in <span class="No-Break">the Pod.</span></p>&#13;
			<p><strong class="source-inline">maxAllowed</strong> limits ensure that VPA does not set resource values beyond the specified range. In this configuration, the <strong class="source-inline">minAllowed</strong> limits are defined as <strong class="source-inline">1000m</strong> (equivalent to one vCPU) and 2 GB of memory, while the maximum allowed CPU is <strong class="source-inline">2000m</strong> (or two vCPUs) with 4 GB of memory. Defining <strong class="source-inline">controlledValues</strong> as <strong class="source-inline">RequestsAndLimits</strong> means that VPA should manage both resource requests and limits. Resource requests are the amount of CPU or memory that a Pod requests from the Kubernetes <a id="_idIndexMarker498"/>scheduler when it starts. We can also set this to <strong class="source-inline">RequestsOnly</strong> if we want VPA to adjust the resource requests only, but not <span class="No-Break">the limits.</span></p>&#13;
			<h2 id="_idParaDest-83"><a id="_idTextAnchor082"/>Combining HPA and VPA</h2>&#13;
			<p>It is recommended <a id="_idIndexMarker499"/>not to combine HPA and <a id="_idIndexMarker500"/>VPA in the same cluster (unless VPA is set to “off”), as it can result in potential conflicts. For example, VPA adjusts the CPU/memory resource requests and limits for a Pod, while HPA scales the number of Pods based on current utilization. If HPA and VPA are used simultaneously, VPA changes might confuse HPA, as the resource usage of a single Pod fluctuates frequently, affecting the metrics used by HPA <span class="No-Break">to scale.</span></p>&#13;
			<p>In this section, we explored VPA and its operating modes, along with an example manifest. VPA monitors resource usage of workloads and automatically adjusts the resource requests to optimize resource allocation in a K8s cluster. We also discussed potential conflicts that arise by combining HPA and VPA in auto-operating <span class="No-Break">mode simultaneously.</span></p>&#13;
			<h1 id="_idParaDest-84"><a id="_idTextAnchor083"/>KEDA</h1>&#13;
			<p>With cloud adoption, there<a id="_idIndexMarker501"/> is a growing trend of microservice and EDA adoption. In this implementation, different building blocks are divided into microservice-based implementations, which are self-contained and talk to each other only through API calls. This implementation allows easier updates and the flexibility to add <span class="No-Break">new features.</span></p>&#13;
			<p><strong class="bold">KEDA</strong> (<a href="https://keda.sh/">https://keda.sh/</a>) is an <a id="_idIndexMarker502"/>open source project that brings event-driven autoscaling to Kubernetes. It extends Kubernetes’ built-in HPA by allowing applications to scale based on external event sources, such as message queue depth, event stream size, or any <span class="No-Break">custom metrics.</span></p>&#13;
			<p>A KEDA <strong class="bold">ScaledObject</strong> is a custom resource that defines how a target (e.g., a Deployment) should be autoscaled based on event-driven or external metrics. When we define a ScaledObject in KEDA, it automatically creates an HPA resource behind the scenes. The HPA resource then scales the deployment based on the metrics provided by KEDA. The ScaledObject defines the scaling logic (e.g., which external metric to use, scaling thresholds, and min/max replicas), and KEDA takes care of managing the HPA based on <span class="No-Break">this configuration.</span></p>&#13;
			<p>KEDA is particularly useful for event-driven architectures where workloads may be sporadic and need scaling only when there’s an event, such as a new customer signing in or a new item being added to <span class="No-Break">the cart.</span></p>&#13;
			<p>KEDA supports <a id="_idIndexMarker503"/>various <strong class="bold">scalers</strong>, which are integrations with external services, such as Amazon SQS, Apache <a id="_idIndexMarker504"/>Kafka, and Prometheus. These scalers watch for changes in metrics or events, such as the number of messages in a queue or the rate of <span class="No-Break">HTTP requests.</span></p>&#13;
			<p class="callout-heading">Note</p>&#13;
			<p class="callout">Refer to the KEDA documentation at <a href="https://keda.sh/docs/latest/scalers/">https://keda.sh/docs/latest/scalers/</a> for a list of <span class="No-Break">available scalers.</span></p>&#13;
			<p>One unique feature of KEDA is that it can scale to zero when there are no events to process, which is beneficial in serverless and <span class="No-Break">event-driven applications.</span></p>&#13;
			<p>To illustrate this behavior, let’s look at a<a id="_idIndexMarker505"/> sample <strong class="bold">ScaledObject</strong> configuration that enables KEDA to scale a deployment based on number of messages in an Amazon <span class="No-Break">SQS queue</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: keda.sh/v1alpha1&#13;
kind: ScaledObject&#13;
metadata:&#13;
  name: amazonsqs-scaler&#13;
spec:&#13;
  scaleTargetRef:&#13;
    apiVersion: apps/v1&#13;
    kind: Deployment&#13;
    name: demo-deployment&#13;
  minReplicaCount: 0&#13;
  maxReplicaCount: 10&#13;
  pollingInterval: 15&#13;
  cooldownPeriod: 180&#13;
  triggers:&#13;
  - type: aws-sqs-queue&#13;
    metadata:&#13;
      queueURL: "http://&lt;aws-sqs-url"&#13;
      awsRegion: "us-east-1"&#13;
      queueLength: "10"&#13;
  authenticationRef:&#13;
    name: keda-service-account</pre>			<p>We are defining a ScaledObject, <strong class="source-inline">amazonsqs-scaler</strong>, for Amazon <span class="No-Break">SQS-based scaling.</span></p>&#13;
			<p><strong class="source-inline">minReplicaCount</strong> set to <strong class="source-inline">0</strong> defines the minimum number of replicas for the deployment. In this example, KEDA can scale the deployment down to zero, when there are no messages in the Amazon SQS queue. This helps conserve resources when there is no workload to<a id="_idIndexMarker506"/> process. <strong class="source-inline">maxReplicaCount</strong> set to <strong class="source-inline">10</strong> specifies the maximum number of replicas that KEDA can scale the deployment up to. This ensures that the deployment does not scale beyond 10 Pods, even if the queue <span class="No-Break">size increases.</span></p>&#13;
			<p><strong class="source-inline">pollingInterval</strong> set to <strong class="source-inline">15</strong> makes KEDA check the queue length every 15 seconds. In this case, KEDA will query the Amazon SQS API every 15 seconds to check the size of the queue. <strong class="source-inline">cooldownPeriod</strong> set to 180 seconds states that after scaling up, KEDA will wait for 3 minutes before scaling down the deployment, even if the workload drops. This prevents rapid scaling down after a temporary traffic spike and allows a more <span class="No-Break">stable scaling.</span></p>&#13;
			<p>We are using <strong class="source-inline">aws-sqs-queue</strong> as the type of scaler, which is compatible with the Amazon SQS service. <strong class="source-inline">queueURL</strong> defines the management endpoint for the Amazon SQS service, where KEDA can query the queue depth. <strong class="source-inline">queueLength</strong> set to <strong class="source-inline">10</strong> defines the threshold for scaling. KEDA will trigger scaling when the number of messages in the queue exceeds 10. Lastly, <strong class="source-inline">keda-service-account</strong> refers to the service account that KEDA will use to authenticate with the Amazon <span class="No-Break">SQS service.</span></p>&#13;
			<p>Following is an example ScaledObject that automatically scales GenAI model deployments in K8s. It scales based on two triggers: the first one is based on the average number of incoming requests, and the second is based on the GPU utilization of the inference Pods. Both metrics are sourced from the local Prometheus setup, which collects these metrics<a id="_idIndexMarker507"/> from the NVIDIA DCGM exporter and the application <span class="No-Break">metrics endpoint:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: keda.sh/v1alpha1&#13;
kind: ScaledObject&#13;
metadata:&#13;
  name: my-llama-deployment-scaler&#13;
spec:&#13;
  scaleTargetRef:&#13;
    apiVersion: apps/v1&#13;
    kind: Deployment&#13;
    name: my-llama-deployment&#13;
...&#13;
  triggers:&#13;
    - type: prometheus&#13;
      metadata:&#13;
        serverAddress: http://prometheus-server.default.svc:9090&#13;
        metricName: http_requests_total&#13;
        threshold: "10"&#13;
        query: sum(rate(http_requests_total[1m]))&#13;
    - type: prometheus&#13;
      metadata:&#13;
        serverAddress: http://prometheus-server.default.svc:9090&#13;
        metricName: DCGM_FI_DEV_GPU_UTIL&#13;
        threshold: "70"&#13;
        query: avg(DCGM_FI_DEV_GPU_UTIL{pod=~"my-llama-.*"}[30s])</pre>			<p>In this section, we explored the KEDA project, which brings event-driven autoscaling to K8s. KEDA extends the built-in HPA by enabling applications to scale based on custom metrics from various sources, including event-driven metrics such as message queue depth. By integrating <a id="_idIndexMarker508"/>KEDA into K8s, we can dynamically scale K8s workloads in response to external events, making resource allocation more responsive <span class="No-Break">and efficient.</span></p>&#13;
			<h1 id="_idParaDest-85"><a id="_idTextAnchor084"/>Cluster Autoscaler (CA)</h1>&#13;
			<p>The Kubernetes CA is a<a id="_idIndexMarker509"/> tool that adjusts the number of nodes in a Kubernetes cluster based on the needs of the workloads running in the cluster. It scales the cluster up or down by adding or removing nodes to meet the resource demands of pending or <span class="No-Break">underutilized Pods.</span></p>&#13;
			<p>When HPA detects that the resource usage exceeds the configured threshold, it increases the number of Pod replicas. However, if the existing nodes in the cluster don’t have enough capacity to schedule the new Pods, these Pods will <span class="No-Break">remain unschedulable.</span></p>&#13;
			<p>That’s where CA comes into the picture. CA detects that there are unschedulable Pods and provisions more nodes to accommodate these newly created Pods, which are unscheduled. Once the nodes are ready, the pending Pods are scheduled and start running on the <span class="No-Break">new nodes.</span></p>&#13;
			<p>CA supports both <em class="italic">scale-up</em>, when Pods are unschedulable due to insufficient resources, and <em class="italic">scale-down</em>; when the nodes are underutilized, the CA can remove them to optimize resource utilization and <span class="No-Break">reduce costs.</span></p>&#13;
			<p>CA continuously monitors the Kubernetes cluster and interacts with the cloud provider’s API to add or remove nodes based on the current resource usage. It focuses on pending Pods that cannot be scheduled due to a lack of resources and underutilized nodes that have <span class="No-Break">spare capacity.</span></p>&#13;
			<p>The CA identifies a node group, such as an AWS Auto Scaling Group, that is capable of provisioning additional nodes with the necessary resources. Once the new node becomes available, the pending Pods are scheduled to run <span class="No-Break">on it:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Scale-down</strong>: When <a id="_idIndexMarker510"/>nodes are underutilized, meaning they are running with low resource usage or without any significant workloads, the CA checks whether the Pods on the underutilized nodes can be safely rescheduled on other nodes in <span class="No-Break">the cluster.</span></li>&#13;
				<li><strong class="bold">Scale-up</strong>: When the<a id="_idIndexMarker511"/> application experiences high resource demand and existing nodes cannot accommodate new or pending workloads, the CA triggers scale-up. It provisions additional nodes to ensure workload scheduling. Once new nodes join the cluster, pending Pods are scheduled on them to maintain the <span class="No-Break">desired performance.</span></li>&#13;
			</ul>&#13;
			<p>While CA has been the traditional choice for scaling Kubernetes clusters, it has certain limitations, such as relying on predefined node groups and a polling-based mechanism for scaling decisions. This<a id="_idIndexMarker512"/> is where <strong class="bold">Karpenter</strong> can help. Karpenter is designed to address the inefficiencies of traditional autoscaling methods and will be covered in the <span class="No-Break">next section.</span></p>&#13;
			<h1 id="_idParaDest-86"><a id="_idTextAnchor085"/>Karpenter</h1>&#13;
			<p><strong class="bold">Karpenter</strong> is an open <a id="_idIndexMarker513"/>source, flexible, high-performance Kubernetes CA built by AWS. It was first introduced in 2021 during the AWS re:Invent (<a href="https://aws.amazon.com/blogs/aws/introducing-karpenter-an-open-source-high-performance-kubernetes-cluster-autoscaler/">https://aws.amazon.com/blogs/aws/introducing-karpenter-an-open-source-high-performance-kubernetes-cluster-autoscaler/</a>) conference, with its primary purpose to improve and simplify K8s <span class="No-Break">autoscaling experience.</span></p>&#13;
			<p>Unlike the native Kubernetes CA, which primarily focuses on scaling nodes in response to pending Pods, Karpenter dynamically provisions right-sized compute resources based on the specific needs of workloads. Karpenter optimizes for both efficiency and performance in the <span class="No-Break">following ways:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Faster and more efficient scaling</strong>: Karpenter can directly communicate with the Kubernetes API server to understand the pending Pod requirements and can launch new nodes faster, reducing scheduling delays. Karpenter makes these scaling decisions in near-real-time by analyzing the specific needs of pending Pods. This means that nodes are provisioned based on the immediate requirements, which reduces latency and increases responsiveness to <span class="No-Break">workload changes.</span></li>&#13;
				<li><strong class="bold">Better utilization of nodes</strong>: Unlike the CA, which typically provisions nodes from pre-configured instance groups or node pools, Karpenter can dynamically select the best instance types. It can pick instance sizes and types that match the resource requirements of the pending Pods, reducing wasted capacity and optimizing <span class="No-Break">resource allocation.</span></li>&#13;
				<li><strong class="bold">Consolidation capabilities</strong>: Karpenter continuously monitors the cluster and consolidates workloads by re-packing them onto fewer nodes when possible, terminating underutilized nodes. This consolidation helps to reduce costs by making better use of available node resources, whereas the CA generally scales down nodes based on pre-configured thresholds without aggressively <span class="No-Break">consolidating workloads.</span></li>&#13;
				<li><strong class="bold">Support for multiple instance types</strong>: Karpenter can select from a wide range of instance types, including different generations and sizes. It does this based on current availability and pricing, ensuring that Pods are scheduled on the most cost-effective and <span class="No-Break">resource-appropriate nodes.</span></li>&#13;
				<li><strong class="bold">Drift</strong>: Karpenter automatically detects the nodes that have drifted from the desired state and replaces them in a rolling manner. This functionality can be used to perform patch upgrades or K8s version upgrades of the <span class="No-Break">Karpenter-managed nodes.</span></li>&#13;
			</ul>&#13;
			<p>Karpenter looks for <a id="_idIndexMarker514"/>pending Pods in the cluster that are marked <em class="italic">unschedulable</em> by the kube-scheduler and aggregates the resource and scheduling requirements of those Pods to make decisions to launch new worker nodes. It performs <em class="italic">bin-packing</em> to ensure the correct size and number of nodes are provisioned. It also actively looks for opportunities to reduce the overall cluster costs by terminating worker nodes that are empty, under-utilized, or can be replaced with cheaper nodes, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer055" class="IMG---Figure">&#13;
					<img src="image/B31108_06_3.jpg" alt="Figure 6.3 – Karpenter overview" width="1650" height="865"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Karpenter overview</p>&#13;
			<p>Karpenter provides two custom<a id="_idIndexMarker515"/> resources, <strong class="bold">NodePools</strong> (<a href="https://karpenter.sh/v1.4/concepts/nodepools/">https://karpenter.sh/v1.4/concepts/nodepools/</a>) and <strong class="bold">NodeClasses</strong> (<a href="https://karpenter.sh/v1.4/concepts/nodeclasses/">https://karpenter.sh/v1.4/concepts/nodeclasses/</a>), for<a id="_idIndexMarker516"/> configuration. NodePools define a set of provisioning <a id="_idIndexMarker517"/>constraints for the nodes created by Karpenter, such as instance types, availability zones, CPU architecture, capacity type, taints, and labels. NodeClasses configure cloud-provider-specific settings such as VPC subnets, IAM role, AMI ID, and EC2 Security groups. Multiple NodePools can be created to cater to different workload requirements, such as a GPU-specific NodePool to launch GPU worker nodes for the GenAI training and inference applications, and generic NodePools to accommodate webservers and microservices. Always ensure that NodePools are created with distinct configurations that are mutually exclusive; if multiple NodePools are matched, Karpenter will randomly pick one to launch the <span class="No-Break">worker nodes.</span></p>&#13;
			<p>Karpenter can be installed in the EKS cluster as a Helm chart, refer to the Getting Started guide at <a href="https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/">https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/</a> for the instructions. In our setup, we installed the Karpenter using Terraform helm provider in <span class="No-Break">Chapter 3</span> as part of the EKS <span class="No-Break">cluster setup.</span></p>&#13;
			<p>You can verify the installation using the following command, which displays the status, version, and other details of <span class="No-Break">the deployment:</span></p>&#13;
			<pre class="source-code">&#13;
$ helm list -n kube-system&#13;
NAME                NAMESPACE             STATUS&#13;
karpenter           kube-system           deployed</pre>			<p>Now, let’s use the power of Karpenter to automatically launch GPU instances for our GenAI workloads. In <a href="B31108_05.xhtml#_idTextAnchor062"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, we created a dedicated EKS Managed Node group of G6 EC2 instances to deploy the Llama 3 fine-tuning job and inference applications. A disadvantage of that approach is that the GPU worker node always remains attached to the cluster, regardless of whether GenAI applications are running, which is an inefficient use of the most <a id="_idIndexMarker518"/>expensive resources. Let’s go ahead and delete the node group and configure the Karpenter to manage the GPU <span class="No-Break">instance provisioning.</span></p>&#13;
			<p>Comment the following code in the <strong class="source-inline">eks.tf</strong> file and run the Terraform commands to delete the <strong class="source-inline">eks-gpu-mng</strong> node group. The complete file is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks.tf"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks.tf</span></a><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
module "eks" {&#13;
  source = "terraform-aws-modules/eks/aws"&#13;
  ...&#13;
  eks_managed_node_groups = {&#13;
    ...&#13;
    # eks-gpu-mng = {&#13;
      # instance_types = ["g6.2xlarge"]&#13;
      # ami_type = "AL2_x86_64_GPU"&#13;
      # max_size = 2&#13;
      # desired_size = 1&#13;
      # capacity_type = "SPOT"&#13;
      # disk_size = 100&#13;
      # labels = {&#13;
      #   "hub.jupyter.org/node-purpose" = "user"&#13;
      # }&#13;
      # taints = {&#13;
      #   gpu = {&#13;
      #     key    = "nvidia.com/gpu"&#13;
      #     value = "true"&#13;
      #     effect = "NO_SCHEDULE"&#13;
      #   }&#13;
      # }&#13;
    # }&#13;
 ...&#13;
$ terraform plan&#13;
$ terraform apply -auto-approve</pre>			<p>You can confirm that<a id="_idIndexMarker519"/> the node group has been deleted by checking the <strong class="bold">Compute</strong> tab in EKS console or by using the <span class="No-Break">following command:</span></p>&#13;
			<pre class="source-code">&#13;
$ kubectl get nodes -l nvidia.com/gpu.present</pre>			<p>Now that we have deleted the GPU worker nodes in the cluster, let’s go ahead and configure the Karpenter by creating NodePool and EC2NodeClass resources to launch the GPU instances. Create a GPU NodePool called <strong class="source-inline">eks-gpu-np</strong> with a set of following requirements to pick G6 instance generation instances, from on-demand or spot capacity. The complete file is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks-gpu-np.yaml"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks-gpu-np.yaml</span></a><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: karpenter.sh/v1&#13;
kind: NodePool&#13;
metadata:&#13;
  name: eks-gpu-np&#13;
spec:&#13;
...&#13;
      nodeClassRef:&#13;
        group: karpenter.k8s.aws/v1&#13;
        kind: EC2NodeClass&#13;
        name: default&#13;
      requirements:&#13;
        - key: "karpenter.k8s.aws/instance-generation"&#13;
          operator: In&#13;
          values: ["g6"]&#13;
        - key: "karpenter.sh/capacity-type"&#13;
          operator: In&#13;
          values: ["spot", "on-demand"]&#13;
  disruption:&#13;
    consolidationPolicy: WhenEmptyOrUnderutilized&#13;
...&#13;
$ kubectl apply -f eks-gpu-np.yaml&#13;
nodepool.karpenter.sh/eks-gpu-np created</pre>			<p>Next, create<a id="_idIndexMarker520"/> the <strong class="source-inline">default-gpu</strong> EC2NodeClass to configure the AMI, IAM role, VPC subnets, EC2 security groups, and so on. The complete file is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks-gpu-nc.yaml"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks-gpu-nc.yaml</span></a><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: karpenter.k8s.aws/v1&#13;
kind: EC2NodeClass&#13;
metadata:&#13;
  name: default-gpu&#13;
spec:&#13;
  amiFamily: AL2023&#13;
...&#13;
  role: "eks-demo"&#13;
...&#13;
$ kubectl apply -f eks-gpu-nc.yaml&#13;
ec2nodeclass.karpenter.k8s.aws/default-gpu created</pre>			<p>Now that we have configured Karpenter, let’s rerun the fine-tuning job and see the compute autoscaling live in action. Execute the following commands to initiate the fine-tuning job. You can download the K8s manifest is available at <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/llama-finetuning-job.yaml">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/llama-finetuning-job.yaml</a>. Replace the container image, Hugging Face token, and model <a id="_idIndexMarker521"/>assets S3 bucket name values before running <span class="No-Break">the commands:</span></p>&#13;
			<pre class="source-code">&#13;
$ kubectl delete job my-llama-job&#13;
job.batch "my-llama-job" deleted&#13;
$ kubectl apply -f llama-finetuning-job.yaml&#13;
job.batch/my-llama-job is created</pre>			<p>Given that the cluster doesn’t have any GPU worker nodes, kube-scheduler will mark the fine-tuning Pod as unschedulable. We can verify that by using the following commands. The first one will output the <em class="italic">Pending</em> status, and the second one shows the reason for <span class="No-Break">this status.</span></p>&#13;
			<pre class="source-code">&#13;
$ kubectl get pods -l app=my-llama-job&#13;
NAME                    READY     STATUS        RESTARTS         AGE&#13;
my-llama-job-plgb5      0/1       Pending       0                22s&#13;
$ kubectl describe pod -l app=my-llama-job | grep Scheduling&#13;
Warning  FailedScheduling  101s  default-scheduler  0/2 nodes are available: 2 Insufficient nvidia.com/gpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod</pre>			<p>Karpenter is actively looking for pending Pods that are unschedulable and launches the G6 family EC2 instance in <a id="_idIndexMarker522"/>response to our fine-tuning job. Let’s verify that by looking at the <span class="No-Break">Karpenter logs:</span></p>&#13;
			<pre class="source-code">&#13;
$ kubectl logs --selector app.kubernetes.io/instance=karpenter -n kube-system&#13;
{"level":"INFO","time":"2025-01-31T14:34:03.899Z","logger":"controller","message":"found provisionable pod(s)","commit":"b897114","controller":"provisioner","namespace":"","name":"","reconcileID":"bbdfdd41-e86f-4cfe-8fb5-e161f3ce4a72","Pods":"default/my-llama-job-plgb5","duration":"33.662142ms"}&#13;
...&#13;
{"level":"INFO","time":"2025-01-31T14:36:01.277Z","logger":"controller","message":"initialized nodeclaim","commit":"b897114","controller":"nodeclaim.lifecycle","controllerGroup":"karpenter.sh","controllerKind":"NodeClaim","NodeClaim":{"name":"eks-gpu-np-gkx7t"},"namespace":"","name":"eks-gpu-np-gkx7t","reconcileID":"f2b28556-4808-4577-a559-f946a451b46c","provider-id":"aws:///us-west-2c/i-02e475780d3aed0a1","Node":{"name":"ip-10-0-32-176.us-west-2.compute.internal"},"allocatable":{"cpu":"15890m","ephemeral-storage":"95551679124","hugepages-1Gi":"0","hugepages-2Mi":"0","memory":"60398040Ki","nvidia.com/gpu":"1","pods":"234"}}</pre>			<p>Once the node has been initiated and joined the cluster, the fine-tuning job will be scheduled on it. You can verify by using the <span class="No-Break">following commands:</span></p>&#13;
			<pre class="source-code">&#13;
$ kubectl get pods -l app=my-llama-job&#13;
NAME                 READY   STATUS    RESTARTS   AGE&#13;
my-llama-job-plgb5   1/1     Running   0          9m20s&#13;
$ kubectl get nodes -l nvidia.com/gpu.present&#13;
NAME                                        STATUS   ROLES&#13;
ip-10-0-32-176.us-west-2.compute.internal   Ready    &lt;none&gt;</pre>			<p>A few minutes after the job is complete, Karpenter will automatically detect the empty node, apply a taint to prevent new workloads from being scheduled, evict the existing Pods, and then terminate<a id="_idIndexMarker523"/> the node. You can verify that by checking the <span class="No-Break">Karpenter logs:</span></p>&#13;
			<pre class="source-code">&#13;
$ kubectl logs --selector app.kubernetes.io/instance=karpenter -n kube-system&#13;
"level":"INFO","time":"2025-01-31T14:50:18.104Z","logger":"controller","message":"tainted node","commit":"b897114","controller":"node.termination","controllerGroup":"","controllerKind":"Node","Node":{"name":"ip-10-0-32-176.us-west-2.compute.internal"},"namespace":"","name":"ip-10-0-32-176.us-west-2.compute.internal","reconcileID":"874424bd-d2f7-45ab-a399-41e5314fb3d3","taint.Key":"karpenter.sh/disrupted","taint.Value":"","taint.Effect":"NoSchedule"}&#13;
{"level":"INFO","time":"2025-01-31T14:54:03.281Z","logger":"controller","message":"deleted node","commit":"b897114","controller":"node.termination","controllerGroup":"","controllerKind":"Node","Node":{"name":"ip-10-0-32-176.us-west-2.compute.internal"},"namespace":"","name":"ip-10-0-32-176.us-west-2.compute.internal","reconcileID":"eb99f292-2ea2-4aba-ac88-528746cc7e89"}</pre>			<p>In this section, we looked at the advantages of using Karpenter and installed it in the EKS cluster using Terraform and Helm. Then, we configured Karpenter to launch G6 instances for our Llama 3 fine-tuning job and inference workloads. Karpenter launched a G6 instance in response <a id="_idIndexMarker524"/>to the fine-tuning job and terminated it automatically after the <span class="No-Break">job completion.</span></p>&#13;
			<h1 id="_idParaDest-87"><a id="_idTextAnchor086"/>Summary</h1>&#13;
			<p>In this chapter, we discussed scaling strategies and best practices for K8s applications to ensure efficient resource utilization and optimal performance. The chapter covers key scaling topics, including metrics, HPA, KEDA, VPA, CA, <span class="No-Break">and Karpenter.</span></p>&#13;
			<p>Scaling in Kubernetes involves selecting the right scaling metrics. Conventional metrics help determine the need for adding or removing Pods. Custom metrics are used for more granular control in <span class="No-Break">scaling decisions.</span></p>&#13;
			<p>HPA can automatically adjust the number of Pods in a deployment based on metrics such as CPU or memory usage, whereas VPA can adjust the resource requests and limits for individual Pods. VPA ensures optimal resource allocation but may conflict with HPA if both are <span class="No-Break">used simultaneously.</span></p>&#13;
			<p>KEDA brings event-driven autoscaling to K8s, enabling scaling based on external events such as message queue depths. It creates an HPA resource that manages scaling in response to event triggers such as spikes in API calls. KEDA can scale applications to zero, making it highly suitable for serverless and event-driven use cases. CA can adjust the number of nodes in a cluster based on Pod requirements. CA works closely with cloud provider APIs to manage <span class="No-Break">nodes dynamically.</span></p>&#13;
			<p>Lastly, we covered Karpenter, an alternative to CA. Karpenter can dynamically provision the right-sized compute resources to handle pending Pods. It optimizes for both performance and cost efficiency by selecting suitable instance types and terminating underutilized nodes to reduce costs. To demonstrate its functionality, we reran the Llama 3 fine-tuning job, during which Karpenter launched a node with GPU capabilities in response to the resource requirements, and automatically terminated the node once the job was complete. In the next chapter, we will discuss different strategies to optimize the overall cost of running GenAI applications <span class="No-Break">on K8s.</span></p>&#13;
		</div>&#13;
	</div></div></body></html>