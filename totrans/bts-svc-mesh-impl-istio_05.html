<html><head></head><body>
		<div id="_idContainer060">
			<h1 id="_idParaDest-86" class="chapter-number"><a id="_idTextAnchor085"/>5</h1>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor086"/>Managing Application Resiliency</h1>
			<p>Application resiliency<a id="_idIndexMarker457"/> is the ability of software applications to withstand faults and failures without any noticeable degradation of quality and level of service for its consumer. The move from monoliths to microservices has exacerbated the need for application resiliency in software design and architecture. In monolith applications, there is a single code base and single deployment whereas, in microservice-based architecture, there are many independent code bases each with its own deployment. When leveraging Kubernetes and other similar platforms, you also need to cater to deployment flexibility and the fact that multiple application instances are being deployed and scaled elastically; these dynamic instances need to not only coordinate with each other but also coordinate with all <span class="No-Break">other microservices.</span></p>
			<p>In this chapter, we will read about how to make use of Istio to increase the application resiliency of microservices. As we go through each section, we will discuss various aspects of application resiliency and how they are addressed <span class="No-Break">by Istio.</span></p>
			<p>In a nutshell, the following topics will be covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Application resiliency using <span class="No-Break">fault injection</span></li>
				<li>Application resiliency using timeouts <span class="No-Break">and retries</span></li>
				<li>Building application resiliency using <span class="No-Break">load balancing</span></li>
				<li><span class="No-Break">Rate limiting</span></li>
				<li>Circuit breaker and <span class="No-Break">outlier detection</span></li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">The technical prerequisites for this chapter are the same as the <span class="No-Break">previous chapter.</span></p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor087"/>Application resiliency using fault injection</h1>
			<p>Fault injections are<a id="_idIndexMarker458"/> used for testing the recovery capability of applications in case of any kind of failure. In principle, every microservice should be designed with in-built resiliency for internal and external failures but often, that is not the case. The most complex and difficult work of building resiliency is usually at design and <span class="No-Break">test time.</span></p>
			<p>During design time, you <a id="_idIndexMarker459"/>must identify all known and unknown <a id="_idIndexMarker460"/>scenarios to which you need to cater. For example, you must address <span class="No-Break">the following:</span></p>
			<ul>
				<li>What kind of known and unknown errors might happen inside and outside of <span class="No-Break">the microservice?</span></li>
				<li>How should each of those errors be handled by the <span class="No-Break">application code?</span></li>
			</ul>
			<p>During test time, you should be able to simulate these scenarios to validate the contingencies built into the <span class="No-Break">application code:</span></p>
			<ul>
				<li>Mimic in real-time different failure scenarios in the behavior of other upstream services to test the overall <span class="No-Break">application behavior</span></li>
				<li>Mimic not just application failures but also network failures such as delays, outages, etc., as well as <span class="No-Break">infrastructure failure</span></li>
			</ul>
			<p>Chaos engineering<a id="_idIndexMarker461"/> is the software engineering term used to refer to the discipline of testing software systems by introducing adverse conditions into software systems and their execution and communication environments and ecosystems. You can read more about <a id="_idIndexMarker462"/>Chaos engineering at <a href="https://hub.packtpub.com/chaos-engineering-managing-complexity-by-breaking-things/">https://hub.packtpub.com/chaos-engineering-managing-complexity-by-breaking-things/</a>. Various tools are available for generating chaos – as in, failures – mostly at the infrastructure level. One such popular tool is<a id="_idIndexMarker463"/> Chaos Monkey. It is available at <a href="https://netflix.github.io/chaosmonkey/">https://netflix.github.io/chaosmonkey/</a>. AWS also provides AWS Fault Injection Simulator<a id="_idIndexMarker464"/> for running fault injection simulations. You can read about AWS Fault Injection Simulator at <a href="https://aws.amazon.com/fis/">https://aws.amazon.com/fis/</a>. Another popular open source software is Litmus, which is a chaos engineering platform to identify weaknesses and potential outages in infrastructures by inducing <a id="_idIndexMarker465"/>chaos tests in a controlled way. You can read more about <span class="No-Break">it </span><a href="https://litmuschaos.io/"><span class="No-Break">https://litmuschaos.io/</span></a><span class="No-Break">.</span></p>
			<p>Istio provides fine-grained control for injecting failures because of its access and knowledge of application traffic. With Istio fault injection, you can use application-layer fault injection; this, combined with infrastructure-level fault injectors such as Chaos Monkey and AWS Fault Simulator, provides a very robust capability for testing <span class="No-Break">application resiliency.</span></p>
			<p>Istio supports the following <a id="_idIndexMarker466"/>types of <span class="No-Break">fault injections:</span></p>
			<ul>
				<li><span class="No-Break">HTTP delay</span></li>
				<li><span class="No-Break">HTTP abort</span></li>
			</ul>
			<p>In the following sections, we will discuss both fault injection types in <span class="No-Break">more detail.</span></p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor088"/>What is HTTP delay?</h2>
			<p>Delays <a id="_idIndexMarker467"/>are timing failures. They mimic increased request turnaround times caused either by network latency or an overloaded upstream service. These delays are injected via <span class="No-Break">Istio VirtualServices.</span></p>
			<p>In our sock shop example, let’s inject an<a id="_idIndexMarker468"/> HTTP delay between the<a id="_idIndexMarker469"/> frontend service and catalog service and test how the frontend service behaves when it cannot get images from the catalog service. We will do this specifically for one image rather than <span class="No-Break">all images.</span></p>
			<p>You will find the VirtualService definition in <strong class="source-inline">Chapter5/02-faultinjection_delay.yaml</strong> <span class="No-Break">on GitHub.</span></p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B17989_05_01.jpg" alt="Figure 5.1 – HTTP delay injection in catalogue VirtualService"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – HTTP delay injection in catalogue VirtualService</p>
			<p>The <a id="_idIndexMarker470"/>following is the<a id="_idIndexMarker471"/> snippet from the <span class="No-Break">VirtualService definition:</span></p>
			<pre class="source-code">
spec:
  hosts:
  - "catalogue.sock-shop.svc.cluster.local"
  gateways:
  - mesh
  http:
  - match:
    - uri:
        prefix: "/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b"
      ignoreUriCase: true
    fault:
      delay:
        percentage:
          value: 100.0
        fixedDelay: 10s
    route:
    - destination:
        host: catalogue.sock-shop.svc.cluster.local
        port:
          number: 80</pre>
			<p>The<a id="_idIndexMarker472"/> first thing to <a id="_idIndexMarker473"/>note is the <strong class="source-inline">fault</strong> definition, which is used to inject a delay and/or abort the fault before forwarding the request to the destination specified in the route. In this case, we are injecting a <strong class="source-inline">delay</strong> type of fault, which is used to emulate slow <span class="No-Break">response times.</span></p>
			<p>In the <strong class="source-inline">delay</strong> configuration, the following fields <span class="No-Break">are defined:</span></p>
			<ul>
				<li><strong class="source-inline">fixedDelay</strong>: Specifies the delay duration, the value can be in hours, minutes, seconds, or milliseconds, specified by the <strong class="source-inline">h</strong>, <strong class="source-inline">m</strong>, <strong class="source-inline">s</strong>, or <strong class="source-inline">ms</strong> <span class="No-Break">suffix, respectively</span></li>
				<li><strong class="source-inline">percentage</strong>: Specifies the percentage of requests for which the delay will <span class="No-Break">be injected</span></li>
			</ul>
			<p>The other thing to note is that the VirtualService is associated with the <strong class="source-inline">mesh</strong> gateway; you may have noticed that we did not define an Ingress or Egress gateway with <strong class="source-inline">mesh</strong>. So, you must be wondering where this came from, <strong class="source-inline">mesh</strong> is a reserved word used to refer all the sidecars in the mesh. This is also the default value for the gateway configuration, so if you don’t provide a value for the gateway, then the VirtualService by default associates itself with all the sidecars in <span class="No-Break">the mesh.</span></p>
			<p>So, let’s summarize what we <span class="No-Break">have configured.</span></p>
			<p>The <strong class="source-inline">sock-shop</strong> VirtualService is associated with all sidecars in the mesh and is applied for requests destined for <strong class="source-inline">catalogue.sock-shop.svc.cluster.local</strong>, the VirtualService injects a delay of 10 seconds for all requests prefixed with <strong class="source-inline">/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b</strong>, and it then forwards them to the <strong class="source-inline">catalogue.sock-shop.svc.cluster.local</strong> service. Requests that don’t have the <strong class="source-inline">/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b</strong> prefix are forwarded as is to the <span class="No-Break"><strong class="source-inline">catalogue.sock-shop.svc.cluster.local</strong></span><span class="No-Break"> service.</span></p>
			<p>Create a namespace for <strong class="source-inline">Chapter5</strong> with Istio <span class="No-Break">injection enabled:</span></p>
			<pre class="console">
$ kubectl create ns chapter5
$ kubectl label ns chapter5 istio-injection=enabled</pre>
			<p>Create the Ingress gateway and VirtualService configuration for <strong class="source-inline">sockshop.com</strong> using <span class="No-Break">the following:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter5/01-sockshop-istio-gateway.yaml</pre>
			<p>After that, apply the<a id="_idIndexMarker474"/> VirtualService<a id="_idIndexMarker475"/> configuration for the <span class="No-Break">catalog service:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter5/02-faultinjection_delay.yaml</pre>
			<p>After this, open <strong class="source-inline">sockshop.com</strong> in your browser using the Ingress ELB and custom host headers. Enable the developer tools and search for requests with the <strong class="source-inline">/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b</strong> prefix. You will notice those particular requests are taking more than 10 seconds <span class="No-Break">to process.</span></p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B17989_05_02.jpg" alt="Figure 5.2 – HTTP delay causing requests to take more than 10 seconds"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – HTTP delay causing requests to take more than 10 seconds</p>
			<p>You can also <a id="_idIndexMarker476"/>check the <a id="_idIndexMarker477"/>sidecar injected in the <strong class="source-inline">front-end</strong> Pod to get the access logs for <span class="No-Break">this request:</span></p>
			<pre class="source-code">
% kubectl logs -l name=front-end -c istio-proxy -n sock-shop | grep /catalogue/3395a43e-2d88-40de-b95f-e00e1502085b
[2022-08-27T00:39:09.547Z] "GET /catalogue/3395a43e-2d88-40de-b95f-e00e1502085b HTTP/1.1" 200 DI via_upstream - "-" 0 286 <strong class="bold">10005</strong> 4 "-" "-" "d83fc92e-4781-99ec-91af-c523e55cdbce" "catalogue" "10.10.10.170:80" outbound|80||catalogue.sock-shop.svc.cluster.local 10.10.10.155:59312 172.20.246.13:80 10.10.10.155:40834 - -</pre>
			<p>Highlighted in the preceding code block is the request that, in this instance, took <strong class="source-inline">10005</strong> milliseconds <span class="No-Break">to process.</span></p>
			<p>In this section, we injected the latency of 10 seconds, but you may have also noticed that the front-end web page functioned without any noticeable delays. All the images loaded asynchronously and any latency was limited to the catalogue section of the page. However, by configuring the<a id="_idIndexMarker478"/> delay, you are able to test the end-to-end behavior of the application <a id="_idIndexMarker479"/>in case of any unforeseen delays in the network or processing in the <span class="No-Break">catalog service.</span></p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor089"/>What is HTTP abort?</h2>
			<p>HTTP abort is the<a id="_idIndexMarker480"/> second type of fault that can be injected using Istio. HTTP abort prematurely aborts the processing of the request; you can also specify the error code that needs to <a id="_idIndexMarker481"/>be <span class="No-Break">returned downstream.</span></p>
			<p>The following is the <a id="_idIndexMarker482"/>snippet from the <strong class="source-inline">VirtualService</strong> definition with the <strong class="source-inline">abort</strong> configuration for the catalog service. The configuration is available <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">Chapter5/03-faultinjection_abort.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
spec:
  hosts:
  - "catalogue.sock-shop.svc.cluster.local"
  gateways:
  - mesh
  http:
  - match:
    - uri:
        prefix: "/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b"
      ignoreUriCase: true
    fault:
      abort:
        httpStatus: 500
        percentage:
          value: 100.0
    route:
    - destination:
        host: catalogue.sock-shop.svc.cluster.local
        port:
          number: 80</pre>
			<p>Under <strong class="source-inline">fault</strong>, there is another configuration called <strong class="source-inline">abort</strong> with the <span class="No-Break">following parameters:</span></p>
			<ul>
				<li><strong class="source-inline">httpStatus</strong>: Specifies the HTTP status code that needs to be <span class="No-Break">returned downstream</span></li>
				<li><strong class="source-inline">percentage</strong>: Specifies <a id="_idIndexMarker483"/>the percentage of requests that <a id="_idIndexMarker484"/>need to <span class="No-Break">be aborted</span></li>
			</ul>
			<p>The following is a list of the additional configuration that can be applied to the <span class="No-Break">gRPC request:</span></p>
			<ul>
				<li><strong class="source-inline">grpcStatus</strong>: The gRPC status code that needs to be returned when aborting <span class="No-Break">gRPC requests</span></li>
			</ul>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B17989_05_03.jpg" alt="Figure 5.3 – HTTP abort injection in catalog VirtualService"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – HTTP abort injection in catalog VirtualService</p>
			<p>In <strong class="source-inline">Chapter5/03-faultinjection_abort.yaml</strong>, we have configured a VirtualService rule for all calls originating from within the mesh to <a href="http://catalogue.sock-shop.svc.cluster.local/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b">http://catalogue.sock-shop.svc.cluster.local/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b</a> to be<a id="_idIndexMarker485"/> aborted with an HTTP status <a id="_idIndexMarker486"/>code <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">500</strong></span><span class="No-Break">.</span></p>
			<p>Apply the <span class="No-Break">following configuration:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter5/03-faultinjection_abort.yaml</pre>
			<p>When loading <strong class="source-inline">sock-shop.com</strong> from the browser, you will notice that one image doesn’t load. Looking at the <strong class="source-inline">istio-proxy</strong> access logs for the <strong class="source-inline">front-end</strong> Pod, you will find <span class="No-Break">the following:</span></p>
			<pre class="console">
% kubectl logs -l name=front-end -c istio-proxy -n sock-shop | grep /catalogue/3395a43e-2d88-40de-b95f-e00e1502085b
[2022-08-27T00:42:45.260Z] "GET /catalogue/3395a43e-2d88-40de-b95f-e00e1502085b HTTP/1.1" <strong class="source-inline">500 FI fault_filter_abort -</strong> "-" 0 18 0 - "-" "-" "b364ca88-cb39-9501-b4bd-fd9ea143fa2e" "catalogue" "-" outbound|80||catalogue.sock-shop.svc.cluster.local - 172.20.246.13:80 10.10.10.155:57762 - -</pre>
			<p>This concludes <a id="_idIndexMarker487"/>fault injection and you have now practiced how to inject <strong class="source-inline">delay</strong> and <strong class="source-inline">abort</strong> into a<a id="_idIndexMarker488"/> <span class="No-Break">Service Mesh.</span></p>
			<p class="callout-heading">Reminder</p>
			<p class="callout">Please clean up <strong class="source-inline">Chapter5/02-faultinjection_delay.yaml</strong> and <strong class="source-inline">Chapter5/03-faultinjection_abort.yaml</strong> to avoid conflict with the upcoming exercises in <span class="No-Break">this chapter.</span></p>
			<p>In this section, we read about how to inject faults into a mesh so that we can then test the microservices for their resiliency and design them to withstand any fault caused by latency and delays due to upstream service communications. In the following section, we will read about implementing timeouts and retries in the <span class="No-Break">Service Mesh.</span></p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor090"/>Application resiliency using timeouts and retries</h1>
			<p>With communication between multiple microservices, several things can go wrong, network and infrastructure being the most common causes of service degradation and outages. A service too slow to respond can cause cascading failures across other services and have a ripple effect across the whole application. So, microservices design must be prepared for unexpected delays by setting <strong class="bold">timeouts</strong> when sending requests to <span class="No-Break">other microservices.</span></p>
			<p>The timeout<a id="_idIndexMarker489"/> is the amount of time for which a service can wait for a response from other services; beyond the timeout duration, the response has no significance to the requestor. Once a timeout happens, the microservices will follow contingency methods, which may include servicing the response from the cache or letting the request <span class="No-Break">gracefully fail.</span></p>
			<p>Sometimes, issues are transient, and it makes sense to make another attempt to get a response. This approach is<a id="_idIndexMarker490"/> called a <strong class="bold">retry</strong>, where a microservice can retry a request based on certain conditions. In this section, we will discuss how Istio enables service timeouts and retries without requiring any code changes for microservices. We will start with <span class="No-Break">timeouts first.</span></p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor091"/>Timeouts</h2>
			<p>A timeout<a id="_idIndexMarker491"/> is the amount of time for which an <strong class="source-inline">istio-proxy</strong> sidecar should wait for replies from a given service. Timeouts help ensure that microservices are not waiting unreasonably long for replies and that calls succeed or fail within a predictable timeframe. Istio lets you easily adjust timeouts dynamically on a per-service basis using <strong class="source-inline">VirtualServices</strong> without having to edit your <span class="No-Break">service code.</span></p>
			<p>As an example, we will configure a <a id="_idIndexMarker492"/>timeout of 1 second on the <strong class="source-inline">order</strong> service <a id="_idIndexMarker493"/>and we will generate a <strong class="source-inline">delay</strong> of 10 seconds in the <strong class="source-inline">payment</strong> service. The <strong class="source-inline">order</strong> service calls the <strong class="source-inline">payment</strong> service during check out, so we are simulating a slow payment service and implementing resiliency in the frontend service by configuring a timeout during invocation of <span class="No-Break"><strong class="source-inline">order</strong></span><span class="No-Break"> service:</span></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B17989_05_04.jpg" alt="Figure 5.4 – Timeout for order and delay fault in the payment service"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Timeout for order and delay fault in the payment service</p>
			<p>We will<a id="_idIndexMarker494"/> start by first configuring a timeout in the <strong class="source-inline">order</strong> service, which <a id="_idIndexMarker495"/>also happens via the <strong class="source-inline">VirtualService</strong>. You can find the full configuration in <strong class="source-inline">Chapter5/04-request-timeouts.yaml</strong> <span class="No-Break">on GitHub:</span></p>
			<pre class="source-code">
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: orders
  namespace: chapter5
spec:
  hosts:
  - "orders.sock-shop.svc.cluster.local"
  gateways:
  - mesh
  http:
  - timeout: 1s
    route:
    - destination:
        host: orders.sock-shop.svc.cluster.local
        port:
          number: 80</pre>
			<p>Here, we have created a new VirtualService called <strong class="source-inline">orders</strong> and we have configured a <strong class="source-inline">timeout</strong> of <strong class="source-inline">1</strong> second to any request for <strong class="source-inline">orders.sock-shop.svc.cluster.local</strong> made from within the mesh. The timeout is part of the <strong class="source-inline">http</strong> <span class="No-Break">route configuration.</span></p>
			<p>Following this, we are also injecting a delay of <strong class="source-inline">10</strong> seconds to all requests to the <strong class="source-inline">payment</strong> service. For details, you can refer to the <span class="No-Break"><strong class="source-inline">Chapter5/04-request-timeouts.yaml</strong></span><span class="No-Break"> file.</span></p>
			<p>Go ahead and apply <span class="No-Break">the changes:</span></p>
			<pre class="console">
% kubectl apply -f Chapter5/04-request-timeouts.yaml</pre>
			<p>From the<a id="_idIndexMarker496"/> sockshop website, add any items to the cart and check <a id="_idIndexMarker497"/>out. Observe the behavior before and after applying the changes in <span class="No-Break">this section.</span></p>
			<p>Check the logs for the sidecar in the <span class="No-Break">order Pods:</span></p>
			<pre class="console">
% kubectl logs --follow -l name=orders -c istio-proxy -n sock-shop | grep payment
[2022-08-28T01:24:31.968Z] "POST /paymentAuth HTTP/1.1" 200 - via_upstream - "-" 326 51 2 2 "-" "Java/1.8.0_111-internal" "ce406513-fd29-9dfc-b9cd-cb2b3dbd24a6" "payment" "10.10.10.171:80" outbound|80||payment.sock-shop.svc.cluster.local 10.10.10.229:60984 172.20.93.36:80 10.10.10.229:40816 - -
[2022-08-28T01:25:55.244Z] "POST /paymentAuth HTTP/1.1" 200 DI via_upstream - "-" 326 51 10007 2 "-" "Java/1.8.0_111-internal" "ae00c14e-409c-94b1-8cfb-951a89411246" "payment" "10.10.10.171:80" outbound|80||payment.sock-shop.svc.cluster.local 10.10.10.229:36752 172.20.93.36:80 10.10.10.229:52932 - -</pre>
			<p>Notice that the actual request to the <strong class="source-inline">payment</strong> Pod was processed in <strong class="source-inline">2</strong> milliseconds but the overall time taken was <strong class="source-inline">10007</strong> milliseconds due to the injection of the <span class="No-Break"><strong class="source-inline">delay</strong></span><span class="No-Break"> fault.</span></p>
			<p>Further, check the <strong class="source-inline">istio-proxy</strong> logs in the <span class="No-Break"><strong class="source-inline">front-end</strong></span><span class="No-Break"> Pods:</span></p>
			<pre class="console">
% kubectl logs --follow -l name=front-end -c istio-proxy -n sock-shop | grep orders
[2022-08-28T01:25:55.204Z] "POST /orders HTTP/1.1" 504 UT upstream_response_timeout - "-" 232 24 1004 - "-" "-" "b02ea4a2-b834-95a6-b5be-78db31fabf28" "orders" "10.10.10.229:80" outbound|80||orders.sock-shop.svc.cluster.local 10.10.10.155:49808 172.20.11.100:80 10.10.10.155:55974 - -
[2022-08-28T01:25:55.173Z] "POST /orders HTTP/1.1" 504 - via_upstream - "-" 0 26 1058 1057 "10.10.10.217" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36" "a8cb6589-4fd1-9b2c-abef-f049cd1f6beb" "sockshop.com" "10.10.10.155:8079" inbound|8079|| 127.0.0.6:36185 10.10.10.155:8079 10.10.10.217:0 outbound_.80_._.front-end.sock-shop.svc.cluster.local default</pre>
			<p>Here, we can see the request was returned after a little over <strong class="source-inline">1</strong> second with an HTTP status code of <strong class="source-inline">504</strong>. Although the underlying request for payment was processed in <strong class="source-inline">10</strong> seconds, the request to the <strong class="source-inline">order</strong> service timed out after <span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break"> second.</span></p>
			<p>In this scenario, we can see that the error was not gracefully handled by the website. Instead of returning a graceful message such as “<strong class="bold">Your order has been accepted and we will notify you about its status</strong>” or something similar, the checkout button became unresponsive. We<a id="_idIndexMarker498"/> can say that the website has no inbuilt resiliency to any failure in <a id="_idIndexMarker499"/>the <span class="No-Break"><strong class="source-inline">order</strong></span><span class="No-Break"> service.</span></p>
			<p class="callout-heading">Reminder</p>
			<p class="callout">Don’t forget to clean up <strong class="source-inline">Chapter5/04-request-timeouts.yaml</strong> to avoid conflict with <span class="No-Break">upcoming exercises.</span></p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor092"/>Retries</h2>
			<p>Timeouts<a id="_idIndexMarker500"/> are good firewalls to stop delays from cascading to other parts of an application, but the root cause of a delay is sometimes transient. In these scenarios, it may make sense to retry the requests at least a couple of times. The number of retries and the interval between them depends on the reason for the delay, which is determined by the error codes returned in <span class="No-Break">the response.</span></p>
			<p>In this section, we will look at how to <a id="_idIndexMarker501"/>inject retries into the Service<a id="_idIndexMarker502"/> Mesh. To keep things simple so that we can focus on learning about the concepts in this section, we will make use of the <strong class="source-inline">envoydummy</strong> service we created in the previous chapter. Envoy has many filters to simulate various delays, which we will <a id="_idIndexMarker503"/>leverage to mimic <span class="No-Break">application failures.</span></p>
			<p>First, configure the <span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break"> service:</span></p>
			<pre class="console">
$ kubectl create ns utilities
$ kubectl label ns utilities istio-injection=enabled</pre>
			<p>After that deploy the <strong class="source-inline">envoy</strong> service <span class="No-Break">and Pods:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter5/envoy-proxy-01.yaml</pre>
			<p>We then deploy the gateway <span class="No-Break">and VirtualService:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter5/mockshop-ingress_01.yaml</pre>
			<p>Test the service and see whether it is <span class="No-Break">working OK.</span></p>
			<p>After this, we <a id="_idIndexMarker504"/>will configure the <strong class="source-inline">envoy</strong> config to abort half of the calls and return an error code of <strong class="source-inline">503</strong>. Notice the similarity between the Istio config and the <span class="No-Break"><strong class="source-inline">envoy</strong></span><span class="No-Break"> config.</span></p>
			<p>Configure <strong class="source-inline">envoydummy</strong> to abort half of the <span class="No-Break">API calls:</span></p>
			<pre class="source-code">
  http_filters:
              - name: envoy.filters.http.fault
                typed_config:
                  "@type": type.googleapis.com/envoy.extensions.filters.http.fault.v3.HTTPFault
                  abort:
                    http_status: 503
                    percentage:
                      numerator: 50
                      denominator: HUNDRED</pre>
			<p>The complete file is available <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">Chapter5/envoy-proxy-02-abort-02.yaml</strong></span><span class="No-Break">.</span></p>
			<p>Apply <span class="No-Break">the changes:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter5/envoy-proxy-02-abort-02.yaml</pre>
			<p>Conduct a few tests and you will notice that the API calls are working OK, although we have configured <strong class="source-inline">envoydummy</strong> to fail half of the calls. This is because Istio has already enabled default retries. Although the requests are aborted by <strong class="source-inline">envoydummy</strong>, the sidecar retries them for a default of two times and eventually gets a successful response. The interval between retries (over 25 ms) is variable and is determined automatically by Istio, preventing the called service from being overwhelmed by requests. This is made possible by the Envoy proxy in the sidecar, which uses a fully jittered exponential back-off algorithm for retries with a configurable base interval with a default value of 25 ms. If the base interval is C and N is the number of retry attempts, then the back-off for the retry is in the range of [0,(2^N−1)C). For example, for an interval of 25 ms and a retry attempt of 2, then the first retry will be delayed randomly by 0-24 ms and the<a id="_idIndexMarker505"/> second by <span class="No-Break">0-74 ms.</span></p>
			<p>To disable <strong class="source-inline">retries</strong>, make the<a id="_idIndexMarker506"/> following changes in the <span class="No-Break"><strong class="source-inline">mockshop</strong></span><span class="No-Break"> VirtualService:</span></p>
			<pre class="source-code">
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        port:
          number: 80
        host: envoydummy.utilities.svc.cluster.local
    retries:
      attempts: 0</pre>
			<p>Here, we have configured the number of <strong class="source-inline">retries</strong> to <strong class="source-inline">0</strong>. Apply the changes and this time, you will notice that half of the API calls <span class="No-Break">return </span><span class="No-Break"><strong class="source-inline">503</strong></span><span class="No-Break">.</span></p>
			<p>We will be making the changes as depicted in the <span class="No-Break">following illustration:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B17989_05_05.jpg" alt="Figure 5.5 – Request retries"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Request retries</p>
			<p>Make the<a id="_idIndexMarker507"/> following changes to the <strong class="source-inline">retry</strong> block. You can find the full configuration <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">Chapter5/05-request-retry.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
    retries:
      attempts: 2
      perTryTimeout: 2s
      retryOn: 5xx,gateway-error,reset,connect-failure,refused-stream,retriable-4xx</pre>
			<p>In the <strong class="source-inline">retry</strong> block, we <a id="_idIndexMarker508"/>define the <span class="No-Break">following configurations:</span></p>
			<ul>
				<li><strong class="source-inline">attempts</strong>: The number of retries for a <span class="No-Break">given request</span></li>
				<li><strong class="source-inline">perTryTimeout</strong>: The timeout for <span class="No-Break">every attempt</span></li>
				<li><strong class="source-inline">retryOn</strong>: The condition for which the request should <span class="No-Break">be retried</span></li>
			</ul>
			<p>Apply the configuration and you will notice that the requests are working <span class="No-Break">as usual:</span></p>
			<pre class="console">
$ kubectl apply -f  Chapter5/05-request-retry.yaml</pre>
			<p class="callout-heading">Reminder</p>
			<p class="callout">Please clean up <strong class="source-inline">Chapter5/05-request-retry.yaml</strong> to avoid conflict with <span class="No-Break">upcoming exercises.</span></p>
			<p>This <a id="_idIndexMarker509"/>concludes this section, where you have learned how to set<a id="_idIndexMarker510"/> timeouts and retries in the Service Mesh to improve application resiliency. In the next section, we will explore various strategies for <span class="No-Break">load balancing.</span></p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor093"/>Building application resiliency using load balancing</h1>
			<p>Load balancing<a id="_idIndexMarker511"/> is another technique to improve application resiliency. Istio load balancing policies help you maximize the availability of your application by distributing network traffic efficiently across the microservices or underlying services. Load balancing uses destination rules. Destination rules define the policy that controls how the traffic <a id="_idIndexMarker512"/>should be handled by the service <a id="_idIndexMarker513"/>after routing <span class="No-Break">has occurred.</span></p>
			<p>In the previous chapter, we used destination rules for traffic management purposes. In this section, we will go through various load balancing strategies provided by Istio and how to configure them using <span class="No-Break">destination rules.</span></p>
			<p>Deploy another <strong class="source-inline">envoydummy</strong> Pod but with an additional label of <strong class="source-inline">version:v2</strong> this time and an output of <strong class="source-inline">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong>. The config is available <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">Chapter5/envoy-proxy-02.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter5/envoy-proxy-02.yaml</pre>
			<p>Istio supports the following load <span class="No-Break">balancing strategies.</span></p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor094"/>Round-robins</h2>
			<p>Round robins<a id="_idIndexMarker514"/> are one of the simplest ways of distributing loads, where requests are forwarded <a id="_idIndexMarker515"/>one by one to underlying backend services. Although simple to use, they don’t necessarily result in the most efficient distribution of traffic, because with round-robin load balancing, every upstream is treated the same, as if they are handling the same kind of traffic, are equally performant, and experience similar <span class="No-Break">environmental constraints.</span></p>
			<p>In <strong class="source-inline">Chapter5/06-loadbalancing-roundrobbin.yaml</strong>, we have created a destination rule with <strong class="source-inline">trafficPolicy</strong> <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: envoydummy
spec:
  host: envoydummy
  trafficPolicy:
       loadBalancer:
         simple: ROUND_ROBIN</pre>
			<p>In <strong class="source-inline">DestinationRule</strong>, you can define multiple parameters, which we will uncover one by one in this section and <span class="No-Break">subsequent sections.</span></p>
			<p>For <a id="_idIndexMarker516"/>round-robin <a id="_idIndexMarker517"/>load balancing, we have defined the following destination rules <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">trafficPolicy</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="source-inline">simple</strong>: Defines the load balancing algorithms to be used – the possible values are <span class="No-Break">as follows:</span><ul><li><strong class="source-inline">UNSPECIFIED</strong>: Istio will select an <span class="No-Break">appropriate default</span></li><li><strong class="source-inline">RANDOM</strong>: Istio will select a healthy host <span class="No-Break">at random.</span></li><li><strong class="source-inline">PASSTHROUGH</strong>: This option allows the client to ask for a specific upstream and the load balancing policy will forward the request to the <span class="No-Break">requested upstream.</span></li><li><strong class="source-inline">ROUND_ROBIN</strong>: Istio will send requests in a round-robin fashion to <span class="No-Break">upstream services.</span></li><li><strong class="source-inline">LEAST_REQUEST</strong>: This distributes the load across endpoints depending on how many requests are outstanding on each endpoint. This policy is the most efficient of all the load <span class="No-Break">balancing policies.</span></li></ul></li>
			</ul>
			<p>Apply the configuration <span class="No-Break">using this:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter5/06-loadbalancing-roundrobbin.yaml</pre>
			<p>Test the <a id="_idIndexMarker518"/>endpoints and you will notice that you are receiving an equal amount of responses from<a id="_idIndexMarker519"/> both versions <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">envoydummy</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor095"/>RANDOM</h2>
			<p>When using<a id="_idIndexMarker520"/> the <strong class="source-inline">RANDOM</strong> load balancing policy, Istio selects a <a id="_idIndexMarker521"/>destination host at random. You can find an example of the <strong class="source-inline">RANDOM</strong> load balancing policy in the <strong class="source-inline">Chapter5/07-loadbalancing-random.yaml</strong> file. The following is the destination rule with a <strong class="source-inline">RANDOM</strong> load <span class="No-Break">balancing policy:</span></p>
			<pre class="source-code">
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: envoydummy
spec:
  host: envoydummy
  trafficPolicy:
       loadBalancer:
         simple: RANDOM</pre>
			<p>Apply <span class="No-Break">the configuration:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter5/07-loadbalancing-random.yaml</pre>
			<p>Make a few requests to the endpoints and you will notice that the response doesn’t have any <span class="No-Break">predictable patterns.</span></p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor096"/>LEAST_REQUEST</h2>
			<p>As mentioned earlier, in<a id="_idIndexMarker522"/> the <strong class="source-inline">LEAST_REQUEST</strong> load<a id="_idIndexMarker523"/> balancing policy, Istio routes the traffic that has the least amount of outstanding <span class="No-Break">requests upstream.</span></p>
			<p>To mimic this scenario, we will create another service that specifically sends all requests to an <strong class="source-inline">envoydummy</strong> version 2 Pod. The configuration is available <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">Chapter5/08-loadbalancing-leastrequest.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
apiVersion: v1
kind: Service
metadata:
  name: envoydummy2
  labels:
    name: envoydummy2
    service: envoydummy2
  namespace: chapter5
spec:
  ports:
    # the port that this service should serve on
  - port: 80
    targetPort: 10000
  selector:
    name: envoydummy</pre>
			<p>We have also made changes to <strong class="source-inline">DestinationRule</strong> to send the request to the host that has the fewest <span class="No-Break">active connections:</span></p>
			<pre class="source-code">
  trafficPolicy:
       loadBalancer:
         simple: LEAST_REQUEST
    version: v2</pre>
			<p>Apply <span class="No-Break">the configuration:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter5/08-loadbalancing-leastrequest.yaml</pre>
			<p>Using <strong class="source-inline">kubectl port-forward</strong>, we can send a request to the <strong class="source-inline">envoydummy2</strong> service <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">localhost</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ kubectl port-forward svc/envoydummy2 10000:80 -n utilities</pre>
			<p>After this, we will generate a request targeted for version 2 of the <strong class="source-inline">envoydummy</strong> service using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ for ((i=1;i&lt;1000000000;i++)); do curl -v "http://localhost:10000"; done</pre>
			<p>While the load is in progress, access the request using the <strong class="source-inline">mockshop</strong> endpoint and you will notice the majority, if not all the requests, are served by version 1 of the <strong class="source-inline">envoydummy</strong> Pods<a id="_idIndexMarker524"/> because of the <strong class="source-inline">LEAST_REQUEST</strong> load <a id="_idIndexMarker525"/><span class="No-Break">balancing policy:</span></p>
			<pre class="console">
$ curl -Hhost:mockshop.com http://a816bb2638a5e4a8c990ce790b47d429-1565783620.us-east-1.elb.amazonaws.com/
V1----------Bootstrap Service Mesh Implementation with Istio----------V1
V1----------Bootstrap Service Mesh Implementation with Istio----------V1
V1----------Bootstrap Service Mesh Implementation with Istio----------V1
V1----------Bootstrap Service Mesh Implementation with Istio----------V1</pre>
			<p>In the preceding example, you saw how Istio routes all requests for <strong class="source-inline">mockshop</strong> to version 1 of <strong class="source-inline">envoydummy</strong> because <strong class="source-inline">v1</strong> had the fewest <span class="No-Break">active connections.</span></p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor097"/>Defining multiple load balancing rules</h2>
			<p>Istio has provisions to apply <a id="_idIndexMarker526"/>multiple load balancing rules for every subset. In the <strong class="source-inline">Chapter5/09-loadbalancing-multirules.yaml</strong> file, we are defining the default load balancing policy of <strong class="source-inline">ROUND_ROBIN</strong>, the <strong class="source-inline">LEAST_REQUEST</strong> policy for the <strong class="source-inline">v1</strong> subset, and <strong class="source-inline">RANDOM</strong> for the <span class="No-Break"><strong class="source-inline">v2</strong></span><span class="No-Break"> subset.</span></p>
			<p>The following is the snippet from the configuration defined <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">Chapter5/09-loadbalancing-multirules.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
  host: envoydummy
  trafficPolicy:
       loadBalancer:
         simple: ROUND_ROBIN
  subsets:
  - name: v1
    labels:
      version: v1
    trafficPolicy:
      loadBalancer:
        simple: LEAST_REQUEST
  - name: v2
    labels:
      version: v2
    trafficPolicy:
      loadBalancer:
        simple: RANDOM</pre>
			<p>In the preceding code block, we have applied the <strong class="source-inline">LEAST_REQUEST</strong> load balancing policy to the <strong class="source-inline">v1</strong> subset, the <strong class="source-inline">RANDOM</strong> load balancing policy to the <strong class="source-inline">v2</strong> subset, and <strong class="source-inline">ROUND_ROBBIN</strong> to any other subset not specified in the <span class="No-Break">code block.</span></p>
			<p>Being able to define <a id="_idIndexMarker527"/>multiple load balancing rules for workloads enables you to apply fine-grained control on traffic distribution at the level of the destination rule subset. In the next section, we will go over another important aspect of application resiliency <a id="_idIndexMarker528"/>called <strong class="bold">rate limiting</strong> and how it can be implemented <span class="No-Break">in Istio.</span></p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor098"/>Rate limiting</h1>
			<p>Another important technique for application resiliency is rate limiting<a id="_idIndexMarker529"/> and circuit breaking. Rate limiting helps provide the following controls to handle traffic from consumers without breaking down the <span class="No-Break">provider system:</span></p>
			<ul>
				<li>Surge protection to prevent a system from being overloaded by a sudden spike <span class="No-Break">in traffic</span></li>
				<li>Aligning the rate of incoming requests with the available capacity to <span class="No-Break">process requests</span></li>
				<li>Protecting slow providers from <span class="No-Break">fast consumers</span></li>
			</ul>
			<p>Rate limiting <a id="_idIndexMarker530"/>is performed by configuring destination rules with a connection pool for connections to upstream services. Connection pool settings can be applied at the TCP level as well as the HTTP level, as described in the following configuration <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">Chapter5/10-connection-pooling.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: envoydummy
  namespace: chapter5
spec:
  host: envoydummy
  trafficPolicy:
      connectionPool:
        http:
          http2MaxRequests: 1
          maxRequestsPerConnection: 1
          http1MaxPendingRequests: 0</pre>
			<p>The following are the key attributes of the connection <span class="No-Break">pool configuration:</span></p>
			<ul>
				<li><strong class="source-inline">http2MaxRequests</strong>: The maximum number of active requests to a destination; the default value <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">1024</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">maxRequestsPerConnection</strong>: The maximum number of requests per connection to upstream. A value of <strong class="source-inline">1</strong> disables keep-alive whereas <strong class="source-inline">0</strong> <span class="No-Break">is unlimited.</span></li>
				<li><strong class="source-inline">http1MaxPendingRequests</strong>: The maximum number of requests that will be queued while waiting for a connection from the connection pool; the default value <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">1024</strong></span><span class="No-Break">.</span></li>
			</ul>
			<p>We have configured a maximum of <strong class="source-inline">1</strong> request per connection to upstream, a maximum of <strong class="source-inline">1</strong> active connection at any point of time, and no queuing for connection <span class="No-Break">requests allowed.</span></p>
			<p>Testing the <a id="_idIndexMarker531"/>rate limit, circuit breakers, and outlier detection are not as straightforward as testing other features of application resiliency. Fortunately, there is a very handy load testing utility <a id="_idIndexMarker532"/>called <strong class="source-inline">fortio</strong> available at <a href="https://github.com/fortio/fortio">https://github.com/fortio/fortio</a> and packaged in the Istio sample directory. We will use <strong class="source-inline">fortio</strong> for generating load and testing <span class="No-Break">rate limit.</span></p>
			<p>Deploy <strong class="source-inline">fortio</strong> from the <span class="No-Break">Istio directory:</span></p>
			<pre class="console">
$ kubectl apply -f samples/httpbin/sample-client/fortio-deploy.yaml -n utilities</pre>
			<p>Apply one of the load balancing policies to test <span class="No-Break">normal behavior:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter5/06-loadbalancing-roundrobbin.yaml</pre>
			<p>Generate a load <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">fortio</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ kubectl exec -it fortio-deploy-7dcd84c469-xpggh -n utilities -c fortio -- /usr/bin/fortio load -qps 0 -c 2 -t 1s -H "Host:mockshop.com" http://a816bb2638a5e4a8c990ce790b47d429-1565783620.us-east-1.elb.amazonaws.com/</pre>
			<p>In the previous request, we are configuring <strong class="source-inline">folio</strong> to generate a load test for <strong class="source-inline">1</strong> second with <strong class="source-inline">2</strong> parallel connections with a maximum query per second (<strong class="source-inline">qps</strong>) rate of <strong class="source-inline">0</strong>, meaning no waits/the maximum <span class="No-Break"><strong class="source-inline">qps</strong></span><span class="No-Break"> rate.</span></p>
			<p>In the output, you will notice that all requests were <span class="No-Break">successfully processed:</span></p>
			<pre class="source-code">
Code 200 : 486 (100.0 %)
Response Header Sizes: count 486 avg 151.01235 +/- 0.1104 min 151 max 152 sum 73392
Response Body/Total Sizes : count 486 avg 223.01235 +/- 0.1104 min 223 max 224 sum 108384
All done 486 calls (plus 2 warmup) 4.120 ms avg, 484.8 qps</pre>
			<p>In this case, a total of <strong class="source-inline">486 calls</strong> were made with a 100% success rate. Next, we will apply the changes to enforce <span class="No-Break">rate limiting:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter5/10-connection-pooling.yaml</pre>
			<p>Run the test <a id="_idIndexMarker533"/>again with <span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break"> connection:</span></p>
			<pre class="console">
kubectl exec -it fortio-deploy-7dcd84c469-xpggh -n utilities -c fortio -- /usr/bin/fortio load -qps 0 -c 1 -t 1s -H "Host:mockshop.com" http://a816bb2638a5e4a8c990ce790b47d429-1565783620.us-east-1.elb.amazonaws.com/
Code 200 : 175 (100.0 %)
Response Header Sizes : count 175 avg 151.01143 +/- 0.1063 min 151 max 152 sum 26427
Response Body/Total Sizes : count 175 avg 223.01143 +/- 0.1063 min 223 max 224 sum 39027
All done 175 calls (plus 1 warmup) 5.744 ms avg, 174.0 qps</pre>
			<p>Run the test again; this time, with <span class="No-Break">two connections:</span></p>
			<pre class="source-code">
Code 200 : 193 (66.6 %)
Code 503 : 97 (33.4 %)
Response Header Sizes : count 290 avg 100.55517 +/- 71.29 min 0 max 152 sum 29161
Response Body/Total Sizes : count 290 avg 240.45517 +/- 24.49 min 223 max 275 sum 69732
All done 290 calls (plus 2 warmup) 6.915 ms avg, 288.8 qps</pre>
			<p>You can see <a id="_idIndexMarker534"/>that <strong class="source-inline">33.4%</strong> of the calls failed with a 503 error code because the destination rule enforces the rate <span class="No-Break">limiting rules.</span></p>
			<p>In this section, you saw an example of rate limiting, which, in turn, is also circuit breaking based on a rate limiting condition. In the next section, we will read about circuit breaking by <span class="No-Break">detecting outliers.</span></p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor099"/>Circuit breakers and outlier detection</h1>
			<p>In this section, we will look at outlier detection and circuit breaker patterns. <strong class="bold">A circuit breaker</strong> is a<a id="_idIndexMarker535"/> design pattern in which you continuously monitor the response processing behavior of upstream systems and when the behavior is unacceptable, you stop sending any further requests upstream until the behavior has become <span class="No-Break">acceptable again.</span></p>
			<p>For example, you can monitor the average response time of the upstream system and when it crosses a certain threshold, you may decide to stop sending any further requests to the system; this is called tripping the circuit breaker. Once the circuit breaker has been tripped, you leave it that way for a certain duration so that the upstream service can heal. After the circuit breaker duration has elapsed, you can reset the circuit breaker to let the traffic pass <span class="No-Break">through again.</span></p>
			<p>While circuit breaking is the part that handles the flow of traffic, <strong class="bold">outlier detection</strong> is a set of policies to identify the conditions when the circuit <a id="_idIndexMarker536"/>breaker <span class="No-Break">should trip.</span></p>
			<p>We will configure <a id="_idIndexMarker537"/>one of <strong class="source-inline">envoy</strong> Pods to return a <strong class="source-inline">503</strong> error at random. We will reuse <strong class="source-inline">Chapter5/envoy-proxy-02-abort-02.yaml</strong>, in which we configured a version of <strong class="source-inline">envoydummy</strong> to return a <strong class="source-inline">503</strong> error for 50% of <span class="No-Break">the requests.</span></p>
			<p>To avoid any confusion, delete all previous deployments of <strong class="source-inline">envoydummy</strong> in the <strong class="source-inline">utilities</strong> namespace and any Istio config we have executed prior to <span class="No-Break">this section.</span></p>
			<p>Perform the following in the same order <span class="No-Break">as shown:</span></p>
			<pre class="console">
Kubectl apply -f Chapter5/envoy-proxy-02.yaml
Kubectl apply -f Chapter5/envoy-proxy-02-abort-02.yaml</pre>
			<p>At this stage, we have two <strong class="source-inline">envoydummy</strong> Pods. For the Pod labeled <strong class="source-inline">version:v1</strong>, returning <strong class="source-inline">V1----------Bootstrap Service Mesh Implementation with Istio----------V1</strong>, we have modified it to abort 50% of the request and <span class="No-Break">return </span><span class="No-Break"><strong class="source-inline">503</strong></span><span class="No-Break">.</span></p>
			<p>Perform the following command to disable the default automated retries <span class="No-Break">by Istio:</span></p>
			<pre class="console">
$ kubectl apply -f Chapter5/11-request-retry-disabled.yaml</pre>
			<p>Test the request and you will notice that the response is a mixed bag of <strong class="source-inline">v1</strong>, <strong class="source-inline">v2</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">503</strong></span><span class="No-Break">.</span></p>
			<p>Now, the task at hand is to <a id="_idIndexMarker538"/>define an outlier detection policy to detect <strong class="source-inline">v1</strong> as the outlier because of its erroneous behavior of returning a <strong class="source-inline">503</strong> error code. We will do that via destination rules <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: envoydummy
  namespace: utilities
spec:
  host: envoydummy.utilities.svc.cluster.local
  trafficPolicy:
      connectionPool:
        http:
          http2MaxRequests: 2
          maxRequestsPerConnection: 1
          http1MaxPendingRequests: 0
      outlierDetection:
        baseEjectionTime: 5m
        consecutive5xxErrors: 1
        interval: 1s
        maxEjectionPercent: 100</pre>
			<p>In <strong class="source-inline">outlierDetection</strong>, the<a id="_idIndexMarker539"/> following parameters <span class="No-Break">are provided:</span></p>
			<ul>
				<li><strong class="source-inline">baseEjectionTime</strong>: The minimum ejection duration per ejection, which is then multiplied by the number of times an upstream is found to be unhealthy. For example, if a host is found to be an outlier five times, then it will be ejected from the connection pool <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">baseEjectionTime*5</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">consecutive5xxErrors</strong>: The number of 5xx errors that need to occur to qualify the upstream to be <span class="No-Break">an outlier.</span></li>
				<li><strong class="source-inline">interval</strong>: The time between the checks when Istio scans upstream for health status. The interval is specified in hours, mins, <span class="No-Break">or seconds.</span></li>
				<li><strong class="source-inline">maxEjectionPercent</strong>: The maximum number of hosts in the connection pool that can <span class="No-Break">be ejected.</span></li>
			</ul>
			<p>In the destination rule, we have configured Istio to scan upstream at an interval of 1 second. If 1 or more 5xx errors are consecutively returned, then the upstream will be ejected from the connection pool for 5 minutes and if needed, all hosts can be ejected from the <span class="No-Break">connection pool.</span></p>
			<p>The following parameters can also be defined for outlier detection, but we have not used them in <span class="No-Break">our example:</span></p>
			<ul>
				<li><strong class="source-inline">splitExternalLocalOriginErrors</strong>: This flag tells Istio whether it should consider the local service behavior to determine whether the upstream service is an outlier. For example, <strong class="source-inline">404</strong> may be returned, which is a valid response, but returning it too frequently can also mean that there may be a problem. Maybe the upstream service has an error but due to bad error handling, the upstream is returning <strong class="source-inline">404</strong>, which, in turn, makes the downstream service return a 5XX error. To summarize, this flag enables outlier detection not just based on the response codes returned upstream but also on how the downstream system perceived <span class="No-Break">the response.</span></li>
				<li><strong class="source-inline">consecutiveLocalOriginFailures</strong>: This is the number of consecutive local errors before the upstream service is ejected from the <span class="No-Break">connection pool.</span></li>
				<li><strong class="source-inline">consecutiveGatewayErrors</strong>: This is the number of gateway errors before an upstream service is ejected. This might be caused by unhealthy connections or misconfiguration between a gateway and the upstream service. When an upstream host is accessed over HTTP then, HTTP status codes of <strong class="source-inline">502</strong>, <strong class="source-inline">503</strong>, or <strong class="source-inline">504</strong> are usually returned due to communication issues between the gateway and <span class="No-Break">upstream services.</span></li>
				<li><strong class="source-inline">minHealthPercent</strong>: This field defines the minimum number of healthy upstream systems available in the load balancing pool for outlier detection to be enabled. Once the number of healthy upstream systems drops below this level, outlier detection is disabled to maintain <span class="No-Break">service availability.</span></li>
			</ul>
			<p>The <a id="_idIndexMarker540"/>configuration defined in <strong class="source-inline">Chapter5/12-outlier-detection.yaml</strong> enables us to quickly observe the effects of outlier detection but when deploying this in a non-experimental scenario, the values need to be tuned and configured as per the <span class="No-Break">resiliency requirements.</span></p>
			<p>Apply the updated <span class="No-Break">destination rule:</span></p>
			<pre class="console">
$ kubectl apply -f  Chapter5/12-outlier-detection.yaml</pre>
			<p>After applying the changes, test the request a few times. you will notice that apart from just a few responses with <strong class="source-inline">V1----------Bootstrap Service Mesh Implementation with Istio----------V1</strong>, most of the response contains <strong class="source-inline">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong> because Istio detected the <strong class="source-inline">v1</strong> Pod returning <strong class="source-inline">503</strong> and marked it as an outlier in the <span class="No-Break">connection pool.</span></p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor100"/>Summary</h1>
			<p>In this chapter, we read about how Istio enables application resiliency and testing by providing options to inject delays and faults into request processing. Fault injection assists in validating an application’s resiliency when there is unexpected degradation in underlying services, as well as the network and infrastructure. After fault injection, we read about request timeouts and how they improve application resiliency. For transient failures, it might be a wise idea to make a few retries before giving up on the request and hence, we practiced configuring Istio to perform service retries. Fault injection, timeouts, and retries are properties of VirtualServices and are carried out before routing a request to <span class="No-Break">upstream services.</span></p>
			<p>In the second part of the chapter, we read about various load balancing policies and how you can configure load balancing policies based on the dynamic behavior of an upstream service. Load balancing helps to distribute traffic to upstream services, with <strong class="source-inline">LEAST_REQUEST</strong> policy being the most effective policy for distributing traffic based on the number of requests being handled upstream at any point in time. Load balancing is configured in destination rules because it happens as part of the request routing to upstream services. After load balancing, we read about rate limiting and how it is based on the connection pool configuration in the destination rules. In the final part of the chapter, we read about how to configure destination rules to implement <span class="No-Break">outlier detection.</span></p>
			<p>The most noticeable element of everything that we read in this chapter was the ability to implement application resiliency via timeouts, retries, load balancing, circuit breaking, and outlier detection without ever needing to change the application code. Applications benefit from these resiliency strategies just by being part of the Service Mesh. Various types of software and utilities are used by software engineers to perform chaos engineering to understand the resiliency of an application suffering from failures. You can use these chaos engineering tools to test the application resiliency provided by the <span class="No-Break">Service Mesh.</span></p>
			<p>The next chapter is very exciting and intense because we will read about how to make use of Istio to implement iron-clad security for applications running in <span class="No-Break">the mesh.</span></p>
		</div>
		<div>
			<div id="_idContainer061" class="IMG---Figure">
			</div>
		</div>
	</body></html>