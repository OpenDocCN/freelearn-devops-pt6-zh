<html><head></head><body>
		<div id="_idContainer056">
			<h1 class="chapter-number" id="_idParaDest-134"><a id="_idTextAnchor134"/>8</h1>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor135"/>Deploying the Big Data Stack on Kubernetes</h1>
			<p>In this chapter, we will cover the deployment of key big data technologies – Spark, Airflow, and Kafka – on Kubernetes. As container orchestration and management have become critical for running data workloads efficiently, Kubernetes has emerged as the de facto standard. By the end of this chapter, you will be able to successfully deploy and manage big data stacks on Kubernetes for building robust data pipelines <span class="No-Break">and applications.</span></p>
			<p>We will start by deploying Apache Spark on Kubernetes using the Spark operator. You will learn how to configure and monitor Spark jobs running as Spark applications on your Kubernetes cluster. Being able to run Spark workloads on Kubernetes brings important benefits such as dynamic scaling, versioning, and unified <span class="No-Break">resource management.</span></p>
			<p>Next, we will deploy Apache Airflow on Kubernetes. You will configure Airflow on Kubernetes, link its logs to S3 for easier debugging and monitoring, and set it up to orchestrate data pipelines built using tools such as Spark. Running Airflow on Kubernetes improves reliability, scaling, and <span class="No-Break">resource utilization.</span></p>
			<p>Finally, we will deploy Apache Kafka on Kubernetes, which is critical for streaming data pipelines. Running Kafka on Kubernetes simplifies operations, scaling, and <span class="No-Break">cluster management.</span></p>
			<p>By the end of this chapter, you will have hands-on experience with deploying and managing big data stacks on Kubernetes. This will enable you to build robust, reliable data applications leveraging Kubernetes as your container <span class="No-Break">orchestration platform.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Deploying Spark <span class="No-Break">on Kubernetes</span></li>
				<li>Deploying Airflow <span class="No-Break">on Kubernetes</span></li>
				<li>Deploying Kafka <span class="No-Break">on Kubernetes</span></li>
			</ul>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor136"/>Technical requirements</h1>
			<p>For the activities in this chapter, you should have an AWS account and <strong class="source-inline">kubectl</strong>, <strong class="source-inline">eksctl</strong>, and <strong class="source-inline">helm</strong> installed. For instructions on how to set up an AWS account and <strong class="source-inline">kubectl</strong> and <strong class="source-inline">eksctl</strong> installation, refer to <a href="B21927_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>. For <strong class="source-inline">helm</strong> installation instructions, <span class="No-Break">access </span><a href="https://helm.sh/docs/intro/install/"><span class="No-Break">https://helm.sh/docs/intro/install/</span></a><span class="No-Break">.</span></p>
			<p>We will also be using the Titanic dataset for our exercises. You can find the version we will use <span class="No-Break">at </span><a href="https://github.com/neylsoncrepalde/titanic_data_with_semicolon"><span class="No-Break">https://github.com/neylsoncrepalde/titanic_data_with_semicolon</span></a><span class="No-Break">.</span></p>
			<p>All code in this chapter is available in the GitHub repository at <a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes">https://github.com/PacktPublishing/Bigdata-on-Kubernetes</a>, in the <span class="No-Break"><strong class="source-inline">Chapter08</strong></span><span class="No-Break"> folder.</span></p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor137"/>Deploying Spark on Kubernetes</h1>
			<p>To help us <a id="_idIndexMarker494"/>deploy<a id="_idIndexMarker495"/> resources on Kubernetes, we are going to <a id="_idIndexMarker496"/>use <strong class="bold">Helm</strong>. Helm is a package manager for Kubernetes that helps install applications and services. Helm uses templates <a id="_idIndexMarker497"/>called <strong class="bold">Charts</strong>, which package up installation configuration, default settings, dependencies, and more, into an <span class="No-Break">easy-to-deploy bundle.</span></p>
			<p>On the other hand, we have <strong class="bold">Operators</strong>. Operators <a id="_idIndexMarker498"/>are custom controllers that extend the Kubernetes API to manage applications and their components. They provide a declarative way to create, configure, and manage complex stateful applications <span class="No-Break">on Kubernetes.</span></p>
			<p>Some key benefits of using operators include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Simplified application deployment and lifecycle management</strong>: Operators abstract <a id="_idIndexMarker499"/>away low-level details and provide high-level abstractions for deploying applications without needing to understand the intricacies <span class="No-Break">of Kubernetes</span></li>
				<li><strong class="bold">Integration with monitoring tools</strong>: Operators expose custom metrics and logs, enabling integration with monitoring stacks such as Prometheus <span class="No-Break">and Grafana</span></li>
				<li><strong class="bold">Kubernetes native</strong>: Operators leverage Kubernetes’ extensibility and are written specifically <a id="_idIndexMarker500"/>for Kubernetes, allowing them to <span class="No-Break">be cloud-agnostic.</span></li>
			</ul>
			<p>Operators extend Kubernetes by <a id="_idIndexMarker501"/>creating <strong class="bold">Custom Resource Definitions</strong> (<strong class="bold">CRDs</strong>) and controllers. A CRD allows you to define a new resource type in Kubernetes. For example, the SparkOperator defines a <span class="No-Break">SparkApplication resource.</span></p>
			<p>The operator then creates a controller that watches for these custom resources and performs actions based on the <span class="No-Break">resource </span><span class="No-Break"><strong class="source-inline">spec</strong></span><span class="No-Break">.</span></p>
			<p>For example, when a SparkApplication resource is created, the SparkOperator controller will do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Create the driver and executor Pods based on <span class="No-Break">the spec</span></li>
				<li>Mount <span class="No-Break">storage volumes</span></li>
				<li>Monitor the status of <span class="No-Break">the application</span></li>
				<li>Perform logging <span class="No-Break">and monitoring</span></li>
			</ul>
			<p>Now, let’s get <a id="_idIndexMarker502"/><span class="No-Break">to</span><span class="No-Break"><a id="_idIndexMarker503"/></span><span class="No-Break"> it:</span></p>
			<ol>
				<li>To start, let’s create an AWS EKS cluster <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">eksctl</strong></span><span class="No-Break">:</span><pre class="source-code">
eksctl create cluster --managed --alb-ingress-access --node-private-networking --full-ecr-access --name=studycluster --instance-types=m6i.xlarge --region=us-east-1 --nodes-min=3 --nodes-max=4 --nodegroup-name=ng-studycluster</pre><p class="list-inset">Remember that this line of code takes several minutes to complete. Now, there are some important configurations to give our Kubernetes cluster permission to create volumes on our behalf. For this, we need to install the AWS EBS CSI driver. This is not required for deploying Spark applications, but it will be very important for <span class="No-Break">Airflow deployment.</span></p></li>				<li>First, we need to associate the IAM OIDC provider with the EKS cluster, which allows IAM roles and users to authenticate to the Kubernetes API. To do that, in the terminal, type <span class="No-Break">the following:</span><pre class="source-code">
eksctl utils associate-iam-oidc-provider --region=us-east-1 --cluster=studycluster --approve</pre></li>				<li>Next, we will create an IAM service account called <strong class="source-inline">ebs-csi-controller-sa</strong> in the <strong class="source-inline">kube-system</strong> namespace, with the specified IAM role and policy attached. This service account will be used by the EBS <span class="No-Break">CSI driver:</span><pre class="source-code">
eksctl create iamserviceaccount --name ebs-csi-controller-sa --namespace kube-system --cluster studycluster --role-name AmazonEKS_EBS_CSI_DriverRole --role-only --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy --approve</pre></li>				<li>Finally, we will enable the EBS CSI driver in the cluster and link it to the service account and <a id="_idIndexMarker504"/>role<a id="_idIndexMarker505"/> created earlier. Remember to change <strong class="source-inline">&lt;YOUR_ACCOUNT_NUMBER&gt;</strong> for the <span class="No-Break">real value:</span><pre class="source-code">
eksctl create addon --name aws-ebs-csi-driver --cluster studycluster --service-account-role-arn arn:aws:iam::&lt;YOUR_ACCOUNT_NUMBER&gt;:role/AmazonEKS_EBS_CSI_DriverRole --force</pre></li>				<li>Now, let’s start the actual Spark operator deployment. We will create a namespace to organize our <span class="No-Break">resources next:</span><pre class="source-code">
kubectl create namespace spark-operator</pre></li>				<li>Next, we will use the SparkOperator Helm chart available online to deploy <span class="No-Break">the operator:</span><pre class="source-code">
helm install spark-operator https://github.com/kubeflow/spark-operator/releases/download/spark-operator-chart-1.1.27/spark-operator-1.1.27.tgz --namespace spark-operator --set webhook.enable=true</pre></li>				<li>Check whether the operator was <span class="No-Break">correctly deployed:</span><pre class="source-code">
kubectl get pods -n spark-operator</pre><p class="list-inset">You should see output <span class="No-Break">like this:</span></p><pre class="source-code"><strong class="bold">NAME                                READY   STATUS</strong>
<strong class="bold">spark-operator-74db6fcf98-grhdw     1/1     Running</strong>
<strong class="bold">spark-operator-webhook-init-mw8gf   0/1     Completed</strong></pre></li>				<li>Next, we need to register our AWS credentials as a Kubernetes Secret to make them available for Spark. This will allow our Spark applications to access resources <span class="No-Break">in AWS:</span><pre class="source-code">
kubectl create secret generic aws-credentials --from-literal=aws_access_key_id=&lt;YOUR_ACCESS_KEY_ID&gt; --from-literal=aws_secret_access_key="&lt;YOUR_SECRET_ACCESS_KEY&gt;" -n spark-operator</pre></li>				<li>Now, it’s time to develop our Spark code. By now, you should have the Titanic dataset stored on Amazon S3. At <a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.py">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.py</a>, you will find simple code that reads the Titanic dataset from the S3 bucket and writes it into another <a id="_idIndexMarker506"/>bucket (this second S3 <a id="_idIndexMarker507"/>bucket must have been created previously – you can do it in the <span class="No-Break">AWS console).</span></li>
				<li>Save this file as <strong class="source-inline">spark_job.py</strong> and upload it to a different S3 bucket. This is where the SparkOperator is going to look for the code to run the application. Note that this PySpark code is slightly different from what we saw earlier, in <a href="B21927_05.xhtml#_idTextAnchor092"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>. Here, we are setting Spark configurations separately from the Spark session. We will go through those configurations <span class="No-Break">in detail:</span><ul><li><strong class="source-inline">.set("spark.cores.max", "2")</strong>: This limits the maximum number of cores this Spark application will use to two. This prevents the overallocation <span class="No-Break">of resources.</span></li><li><strong class="source-inline">.set("spark.executor.extraJavaOptions", "-Dcom.amazonaws.services.s3.enableV4=true")</strong> and <strong class="source-inline">.set("spark.driver.extraJavaOptions", "-Dcom.amazonaws.services.s3.enableV4=true")</strong>: These enable support for reading and writing to S3 using Signature Version 4 authentication, which is <span class="No-Break">more secure.</span></li><li><strong class="source-inline">.set("spark.hadoop.fs.s3a.fast.upload", True)</strong>: This property enables the fast upload feature of the S3A connector which improves performance when saving data <span class="No-Break">to S3.</span></li><li><strong class="source-inline">.set("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")</strong>: This configuration sets the S3 FileSystem implementation to use the newer, optimized <strong class="source-inline">s3a</strong> instead of the older <span class="No-Break"><strong class="source-inline">s3</strong></span><span class="No-Break"> connector.</span></li><li><strong class="source-inline">.set("spark.hadoop.fs.s3a.aws.crendentials.provider", "com.amazonaws.auth.EnvironmentVariablesCredentials")</strong>: This configures Spark to obtain AWS credentials from the environment variables, rather than needing to specify them directly <span class="No-Break">in code.</span></li><li><strong class="source-inline">.set("spark.jars.packages", "org.apache.hadoop:hadoop-aws:2.7.3")</strong>: This adds a dependency on the Hadoop AWS module so Spark can <span class="No-Break">access S3.</span></li></ul><p class="list-inset">Also note that, by <a id="_idIndexMarker508"/>default, Spark <a id="_idIndexMarker509"/>uses log-level <strong class="source-inline">INFO</strong>. In this code, we set it to <strong class="source-inline">WARN</strong> to reduce logging and improve logs’ readability. Remember to change <strong class="source-inline">&lt;YOUR_BUCKET&gt;</strong> for your own <span class="No-Break">S3 buckets.</span></p></li>
				<li>After uploading this code to S3, it’s time to create a YAML file with the SparkApplication definitions. The content for the code is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.yaml"><span class="No-Break">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.yaml</span></a><span class="No-Break">.</span><p class="list-inset">The code defines a new SparkApplication resource. This is only possible because the SparkOperator created the SparkApplication custom resource. Let’s take a closer look at what this YAML definition <span class="No-Break">is doing.</span></p><ul><li>The first block of the YAML file specifies the apiVersion and the kind of resource as Spark application. It also sets a name and namespace for <span class="No-Break">the application.</span></li><li>The second block defines a volume mount called “ivy” that will be used to cache dependencies and avoid fetching them for each job run. It mounts to <strong class="source-inline">/tmp</strong> in the driver <span class="No-Break">and executors.</span></li><li>The third block configures Spark properties, enabling the Ivy cache directory and setting the resource allocation batch size <span class="No-Break">for Kubernetes.</span></li><li>The fourth block configures Hadoop properties to use the S3A file <span class="No-Break">system implementation.</span></li><li>The fifth block sets this Spark application as a Python one, the Python version to use, running in cluster mode, and the Docker image to use – in this case, a previously prepared Spark image that integrates with AWS and Kafka. It also defines that the image will always be<a id="_idIndexMarker510"/> pulled <a id="_idIndexMarker511"/>from Docker Hub, even if it is already present in <span class="No-Break">the cluster.</span></li><li>The sixth block specifies the main Python application file location in S3 and the Spark version – in this <span class="No-Break">case, 3.1.1.</span></li><li>The seventh block sets <strong class="source-inline">restartPolicy</strong> to <strong class="source-inline">Never</strong>, so the application runs <span class="No-Break">only once.</span></li></ul><p class="list-inset">The remaining blocks set configuration for the driver and executor Pods. Here, we set up AWS access key secrets for accessing S3, we request one core and 1 GB memory for the driver and the same resources for the executors, we mount an <strong class="source-inline">emptyDir</strong> volume called “ivy” for caching dependencies, and we set Spark and driver Pod labels <span class="No-Break">for tracking.</span></p></li>
				<li>Once this file is saved on your computer and you already have the <strong class="source-inline">.py</strong> file on S3, it’s time to run the Spark Application. In the terminal, type <span class="No-Break">the following:</span><pre class="source-code">
kubectl apply -f spark_job.yaml -n spark-operator</pre><p class="list-inset">We can check whether the application was successfully submitted <span class="No-Break">with this:</span></p><pre class="source-code">kubectl get sparkapplication -n spark-operator</pre><p class="list-inset">We can get a few more details on the application using <span class="No-Break">the following:</span></p><pre class="source-code">kubectl describe sparkapplication/test-spark-job -n spark-operator</pre><p class="list-inset">To see the logs of our Spark Application, <span class="No-Break">type this:</span></p><pre class="source-code">kubectl logs test-spark-job-driver -n spark-operator</pre></li>			</ol>
			<p>And that’s it! You just ran your first Spark application on Kubernetes! Kubernetes won’t let you deploy another<a id="_idIndexMarker512"/> job<a id="_idIndexMarker513"/> with the same name, so, to test again, you should delete <span class="No-Break">the application:</span></p>
			<pre class="source-code">
kubectl delete sparkapplication/test-spark-job -n spark-operator</pre>			<p>Now, let’s see how to deploy Airflow on Kubernetes using the official <span class="No-Break">Helm chart.</span></p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor138"/>Deploying Airflow on Kubernetes</h1>
			<p>Airflow deployment<a id="_idIndexMarker514"/> on Kubernetes is very straightforward. Nevertheless, there<a id="_idIndexMarker515"/> are some important details in the Helm chart configuration that we need to pay <span class="No-Break">attention to.</span></p>
			<p>First, we will download the most recent Helm chart to our <span class="No-Break">local environment:</span></p>
			<pre class="source-code">
helm repo add apache-airflow https://airflow.apache.org</pre>			<p>Next, we need to configure a <strong class="source-inline">custom_values.yaml</strong> file to change the default deployment configurations for the chart. An example of this YAML file is available at <a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/custom_values.yaml">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/custom_values.yaml</a>. We will not go through the entire file but just the most important configurations that are needed for <span class="No-Break">this deployment:</span></p>
			<ol>
				<li>In the <strong class="source-inline">defaultAirflowTag</strong> and <strong class="source-inline">airflowVersion</strong> parameters, make sure to set <strong class="source-inline">2.8.3</strong>. This is the latest Airflow version available for the 1.13.1 Helm <span class="No-Break">chart version.</span></li>
				<li>The <strong class="source-inline">executor</strong> parameter should be set to <strong class="source-inline">KubernetesExecutor</strong>. This guarantees that Airflow will use Kubernetes infrastructure to launch <span class="No-Break">tasks dynamically.</span></li>
				<li>In the <strong class="source-inline">env</strong> section, we will configure “remote logging” to allow Airflow to save logs in S3. This is best practice for auditing and saving Kubernetes storage resources. Here, we configure three environment variables for Airflow. The first one sets remote logging to <strong class="source-inline">"True"</strong>; the second one defines in which S3 bucket and folder Airflow will write logs, and the last one defines a “connection” that Airflow will use to authenticate in AWS. We will have to set this in the Airflow UI later. This is <a id="_idIndexMarker516"/>an <a id="_idIndexMarker517"/>example of what this block should <span class="No-Break">look like:</span><pre class="source-code">
env:
    - name: "AIRFLOW__LOGGING__REMOTE_LOGGING"
    value: "True"
    - name: "AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER"
    value: "s3://airflow-logs-&lt;YOUR_ACCOUNT_NUMBER&gt;/airflow-logs/"
    - name: "AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID"
    value: "aws_conn"</pre></li>				<li>In the webserver block, we must configure the first user credentials and the type of service. The <strong class="source-inline">service</strong> parameter should be set to “LoadBalancer” so we can access the Airflow UI from a browser. The <strong class="source-inline">defaultUser</strong> block should look <span class="No-Break">like this:</span><pre class="source-code">
    defaultUser:
      enabled: true
      role: Admin
      username: &lt;YOUR_USERNAME&gt;
      email: admin@example.com
      firstName: NAME
      lastName: LASTNAME
      password: admin</pre><p class="list-inset">It is important to have a simple password in the values file and change it in the UI as soon as the deployment is ready. This way, your credentials do not get stored in plain text. This would be a major <span class="No-Break">security incident.</span></p></li>				<li>The <strong class="source-inline">redis.enabled</strong> parameter should be set to <strong class="source-inline">false</strong>. As we are using the Kubernetes executor, Airlfow will not need Redis to manage tasks. If we don’t set this parameter to <strong class="source-inline">false</strong>, Helm will deploy a Redis <span class="No-Break">Pod anyway.</span></li>
				<li>Lastly, in the <strong class="source-inline">dags</strong> block, we will configure <strong class="source-inline">gitSync</strong>. This is the easiest way to send our DAG files to Airflow and keep them updated in GitHub (or any other Git repository<a id="_idIndexMarker518"/> you prefer). First, you <a id="_idIndexMarker519"/>should create a GitHub repository with a folder named <strong class="source-inline">"dags"</strong> to store Python DAG files. Then, you should configure the <strong class="source-inline">gitSync</strong> block <span class="No-Break">as follows:</span><pre class="source-code">
gitSync:
    enabled: true
    repo: https://github.com/&lt;USERNAME&gt;/&lt;REPO_NAME&gt;.git
    branch: main
    rev: HEAD
    ref: main
    depth: 1
    maxFailures: 0
    subPath: "dags"</pre><p class="list-inset">Note that we omitted several comments in the original file for readability. The <strong class="source-inline">custom_values.yaml</strong> file is ready for deployment. We can now proceed with this command in <span class="No-Break">the terminal:</span></p><pre class="source-code">helm install airflow apache-airflow/airflow --namespace airflow --create-namespace -f custom_values.yaml</pre></li>			</ol>
			<p>This deployment can take a few minutes to finish because Airflow will do a database migration job before making the <span class="No-Break">UI available.</span></p>
			<p>Next, we need to get the URL for the UI’s LoadBalancer. In the terminal, <span class="No-Break">type this:</span></p>
			<pre class="source-code">
kubectl get svc -n airflow</pre>			<p>In the columns <strong class="source-inline">EXTERNAL-IP</strong>, you will notice one <strong class="source-inline">not empty</strong> value for the <strong class="source-inline">airflow-webserver</strong> service. Copy this URL and paste it into your browser, adding <strong class="source-inline">":8080"</strong> to access Airflow’s <span class="No-Break">correct port.</span></p>
			<p>After logging in to the UI, click on <strong class="bold">Admin</strong> and <strong class="bold">Connections</strong> in the menu to configure an AWS connection. Set the name to <strong class="source-inline">aws_conn</strong> (as we stated in the values file), choose <strong class="bold">Amazon Web Services</strong> for the connection type, and enter your access key ID and the secret <a id="_idIndexMarker520"/>access <a id="_idIndexMarker521"/>key. (At this point, you should have your AWS credentials stored locally – if you don’t, in the AWS console, go to IAM and generate new credentials for your user. You will not be able to see the old <span class="No-Break">credentials onscreen.)</span></p>
			<p>Finally, we will use DAG code adapted from <a href="B21927_05.xhtml#_idTextAnchor092"><span class="No-Break"><em class="italic">Chapter 5</em></span></a> that will run smoothly on Kubernetes. This DAG will download the Titanic dataset automatically from the internet, perform simple calculations, and print the results, which will be accessed on the “logs” page. The content for the code is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/dags/titanic_dag.py"><span class="No-Break">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/dags/titanic_dag.py</span></a><span class="No-Break">.</span></p>
			<p>Upload a copy of this Python code to your GitHub repository and, in a few seconds, it will show on the Airflow UI. Now, activate your DAG (click the <strong class="source-inline">switch</strong> button) and follow the execution (<span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer054">
					<img alt="Figure 8.1 – DAG successfully executed" src="image/B21927_08.1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – DAG successfully executed</p>
			<p>Then click on any task to<a id="_idIndexMarker522"/> select <a id="_idIndexMarker523"/>it and click on <strong class="bold">Logs</strong>. You will see Airflow logs being read directly from S3 (<span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer055">
					<img alt="Figure 8.2 – Airflow logs in S3" src="image/B21927_08_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Airflow logs in S3</p>
			<p>Congratulations! You now have Airflow up and running on Kubernetes. In <a href="B21927_10.xhtml#_idTextAnchor154"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, we will put all these pieces together to build a fully automated <span class="No-Break">data pipeline.</span></p>
			<p>Next, we will deploy <a id="_idIndexMarker524"/>a Kafka <a id="_idIndexMarker525"/>cluster on Kubernetes using the Strimzi operator. Let’s get <span class="No-Break">to it.</span></p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor139"/>Deploying Kafka on Kubernetes</h1>
			<p>Strimzi is an open<a id="_idIndexMarker526"/> source operator that facilitates the <a id="_idIndexMarker527"/>deployment and management of Kafka clusters on Kubernetes, creating new CRDs. It is developed and maintained by the Strimzi project, which is part <a id="_idIndexMarker528"/>of the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>). The Strimzi operator provides a declarative approach to managing Kafka clusters on Kubernetes. Instead of manually creating and configuring Kafka components, you define the desired state of your Kafka cluster using Kubernetes custom resources. The operator then takes care of deploying and managing the Kafka components according to the <span class="No-Break">specified configuration:</span></p>
			<ol>
				<li>To deploy Strimzi in Kubernetes, first, we need to install its <span class="No-Break">Helm chart:</span><pre class="source-code">
helm repo add strimzi <a href="https://strimzi.io/charts/">https://strimzi.io/charts/</a></pre></li>				<li>Next, we install the operator with the <span class="No-Break">following command:</span><pre class="source-code">
helm install kafka strimzi/strimzi-kafka-operator --namespace kafka --create-namespace --version 0.40.0</pre></li>				<li>We can check whether the deployment was successful with <span class="No-Break">the following:</span><pre class="source-code">
helm status kafka -n kafka
kubectl get pods -n kafka</pre></li>				<li>Now, it’s time to configure the deployment of our Kafka cluster. Here is a YAML file to configure a Kafka cluster using the new CRDs. Let’s break it into pieces for <span class="No-Break">better understanding:</span><p class="list-inset"><span class="No-Break"><strong class="bold">kafka_jbod.yaml</strong></span></p><pre class="source-code">
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
    name: kafka-cluster
spec:
    kafka:
    version: 3.7.0
    replicas: 3</pre><p class="list-inset">The first part of the code specifies the API version and the kind of resource being defined. In this case, it’s a Kafka resource managed by the Strimzi operator. Then, we define metadata for the Kafka resource, specifically its name, which is set to <strong class="source-inline">kafka-cluster</strong>. The next block specifies the configuration for the Kafka brokers. We’re setting the Kafka version to 3.7.0 and specifying that <a id="_idIndexMarker529"/>we<a id="_idIndexMarker530"/> want three replicas (Kafka broker instances) in <span class="No-Break">the cluster:</span></p><pre class="source-code">    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
      - name: external
        port: 9094
        type: loadbalancer
        tls: false</pre><p class="list-inset">Next, we define the listeners for the Kafka brokers. We’re configuring <span class="No-Break">three listeners:</span></p><ul><li><strong class="source-inline">plain</strong>: An internal listener on port <strong class="source-inline">9092</strong> without <span class="No-Break">TLS encryption</span></li><li><strong class="source-inline">tls</strong>: An internal listener on port <strong class="source-inline">9093</strong> with TLS <span class="No-Break">encryption enabled</span></li><li><strong class="source-inline">external</strong>: An external listener on port <strong class="source-inline">9094</strong> exposed as a LoadBalancer service, without <span class="No-Break">TLS encryption</span><pre class="source-code">    readinessProbe:
      initialDelaySeconds: 15
      timeoutSeconds: 5
    livenessProbe:
      initialDelaySeconds: 15
      timeoutSeconds: 5</pre></li></ul><p class="list-inset">The next block<a id="_idIndexMarker531"/> configures<a id="_idIndexMarker532"/> the readiness and liveness probes for the Kafka brokers. The readiness probe checks whether the broker is ready to accept traffic, while the liveness probe checks whether the broker is still running. The <strong class="source-inline">initialDelaySeconds</strong> parameter specifies the number of seconds to wait before performing the first probe, and <strong class="source-inline">timeoutSeconds</strong> specifies the number of seconds after which the probe is <span class="No-Break">considered failed:</span></p><pre class="source-code">    config:
      default.replication.factor: 3
      num.partitions: 9
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 1
      log.message.format.version: "3.7"
      inter.broker.protocol.version: "3.7"
      min.insync.replicas: 2
      log.retention.hours: 2160</pre><p class="list-inset">This <strong class="source-inline">kafka.config</strong> block specifies various configuration options for the Kafka brokers, such as the default replication factor, the number of partitions for new topics, the replication factor for the offsets and transaction state log topics, the log message format version, and the log retention period (in hours). The default log retention for Kafka is 7 days (168 hours), but we can change this parameter according to our needs. It is important to remember that longer retention<a id="_idIndexMarker533"/> periods<a id="_idIndexMarker534"/> imply more disk storage usage, so <span class="No-Break">be careful:</span></p><pre class="source-code">    storage:
      type: jbod
      volumes:
      - id: 0
        type: persistent-claim
        size: 15Gi
        deleteClaim: false
      - id: 1
        type: persistent-claim
        size: 15Gi
        deleteClaim: false</pre><p class="list-inset">The <strong class="source-inline">kafka.storage</strong> block configures the storage for the Kafka brokers. We’re using<a id="_idIndexMarker535"/> the <strong class="bold">Just a Bunch of Disks</strong> (<strong class="bold">JBOD</strong>) storage type, which allows us to specify multiple persistent volumes for each broker. In this case, we’re defining two persistent volume claims of 15 GiB each, with <strong class="source-inline">deleteClaim</strong> set to <strong class="source-inline">false</strong> to prevent the persistent volume claims from being deleted when the Kafka cluster <span class="No-Break">is deleted:</span></p><pre class="source-code">    resources:
      requests:
        memory: 512Mi
        cpu: "500m"
      limits:
        memory: 1Gi
        cpu: "1000m"</pre><p class="list-inset">Next, the <strong class="source-inline">kafka.resources</strong> block specifies the resource requests and limits for the Kafka brokers. We’re requesting 512 MiB of memory and 500 millicpu, and setting the<a id="_idIndexMarker536"/> memory<a id="_idIndexMarker537"/> limit to 1 GiB and the CPU limit to <span class="No-Break">1 </span><span class="No-Break"><strong class="source-inline">cpu</strong></span><span class="No-Break">:</span></p><pre class="source-code">    zookeeper:
    replicas: 3
    storage:
        type: persistent-claim
        size: 10Gi
        deleteClaim: false
    resources:
      requests:
        memory: 512Mi
        cpu: "250m"
      limits:
        memory: 1Gi
        cpu: "500m"</pre><p class="list-inset">Finally, we have a <strong class="source-inline">zookeeper</strong> block that configures the ZooKeeper ensemble used by the Kafka cluster. We’re specifying three replicas for ZooKeeper, using a 10 GiB persistent volume claim for storage, and setting resource requests and limits similar to the <span class="No-Break">Kafka brokers.</span></p></li>				<li>Once the configuration file is ready on your machine, type the following command to deploy the cluster <span class="No-Break">to Kubernetes:</span><pre class="source-code">
kubectl apply -f kafka_jbod.yaml -n kafka</pre><p class="list-inset">Let’s check whether the Kafka cluster was <span class="No-Break">correctly deployed:</span></p><pre class="source-code">kubectl get kafka -n kafka</pre><p class="list-inset">This yields<a id="_idIndexMarker538"/> the <a id="_idIndexMarker539"/><span class="No-Break">following output:</span></p><pre class="source-code">NAME          DESIRED KAFKA REPLICAS   DESIRED ZK REPLICAS
kafka-class   3                        3</pre><p class="list-inset">We can also get detailed information about <span class="No-Break">the deployment:</span></p><pre class="source-code">kubectl describe kafka -n kafka</pre><p class="list-inset">Now, check <span class="No-Break">the Pods:</span></p><pre class="source-code">kubectl get pods -n kafka</pre></li>			</ol>
			<p>The output shows the three Kafka brokers and the <span class="No-Break">ZooKeeper instances:</span></p>
			<pre class="console">
NAME                      READY   STATUS
kafka-class-kafka-0       1/1     Running
kafka-class-kafka-1       1/1     Running
kafka-class-kafka-2       1/1     Running
kafka-class-zookeeper-0   1/1     Running
kafka-class-zookeeper-1   1/1     Running
kafka-class-zookeeper-2   1/1     Running</pre>			<p>Congrats! You have a fully operational Kafka cluster running on Kubernetes. Now, the next step is to deploy a Kafka Connect cluster and make everything ready for a real-time data pipeline. We<a id="_idIndexMarker540"/> will not<a id="_idIndexMarker541"/> do that right now for cloud cost efficiency, but we will get back to this configuration in <a href="B21927_10.xhtml#_idTextAnchor154"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor140"/>Summary</h1>
			<p>In this chapter, you learned how to deploy and manage key big data technologies such as Apache Spark, Apache Airflow, and Apache Kafka on Kubernetes. Deploying these tools on Kubernetes provides several benefits, including simplified operations, better resource utilization, scaling, high availability, and unified <span class="No-Break">cluster management.</span></p>
			<p>You started by deploying the Spark operator on Kubernetes and running a Spark application to process data from Amazon S3. This allows you to leverage Kubernetes for running Spark jobs in a cloud-native way, taking advantage of dynamic resource allocation <span class="No-Break">and scaling.</span></p>
			<p>Next, you deployed Apache Airflow on Kubernetes using the official Helm chart. You configured Airflow to run with the Kubernetes executor, enabling it to dynamically launch tasks on Kubernetes. You also set up remote logging to Amazon S3 for easier monitoring and debugging. Running Airflow on Kubernetes improves reliability, scalability, and resource utilization for orchestrating <span class="No-Break">data pipelines.</span></p>
			<p>Finally, you deployed Apache Kafka on Kubernetes using the Strimzi operator. You configured a Kafka cluster with brokers, a ZooKeeper ensemble, persistent storage volumes, and internal/external listeners. Deploying Kafka on Kubernetes simplifies operations, scaling, high availability, and cluster management for building streaming <span class="No-Break">data pipelines.</span></p>
			<p>Overall, you now have hands-on experience with deploying and managing the key components of a big data stack on Kubernetes. This will enable you to build robust, scalable data applications and pipelines leveraging the power of container orchestration with Kubernetes. The skills learned in this chapter are crucial for running big data workloads efficiently in <span class="No-Break">cloud-native environments.</span></p>
			<p>In the next chapter, we will see how to build a data consumption layer on top of Kubernetes and how to connect those layers with tools to visualize and <span class="No-Break">use data.</span></p>
		</div>
	</body></html>