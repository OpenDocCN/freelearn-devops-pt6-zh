- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting Hands-On with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will get hands-on experience with Kubernetes by deploying
    both a local and cloud-based Kubernetes cluster, and then deploying sample applications
    into those clusters. First, you will deploy a local cluster using **Kubernetes
    in Docker** (**Kind**). Then, you will deploy managed Kubernetes clusters on AWS,
    GCP, and Azure. For the cloud options, we will provide the minimal account setup
    required to deploy the clusters. Feel free to choose the cloud provider you are
    most comfortable with; the core Kubernetes functionality will be the same.
  prefs: []
  type: TYPE_NORMAL
- en: After deploying a cluster, this chapter will be divided into two parts. In the
    first part, you will take the simple API application you developed in [*Chapter
    1*](B21927_01.xhtml#_idTextAnchor015) and deploy it into your Kubernetes cluster.
    You will learn how to containerize applications and work with Kubernetes deployments
    and services to expose your application. In the second part, you will deploy the
    simple data processing batch job from *Chapter 1* into Kubernetes. This will demonstrate
    how to run one-off jobs using Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have first-hand experience deploying applications
    into Kubernetes. You will understand how to package and deploy containerized applications,
    expose them via services and ingress, and leverage Kubernetes for running both
    long-running services and batch jobs. With these skills, you will be ready to
    deploy applications into production Kubernetes environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing `kubectl`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a local K8s cluster with Kind
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying an AWS EKS cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a Google Cloud GKE cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying an Azure AKS cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running your API on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a data processing job in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get to it.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will be required to have Docker installed (the instructions
    for this can be found in [*Chapter 1*](B21927_01.xhtml#_idTextAnchor015)). The
    main hands-on activities we are going to do will be cloud-based, but you can choose
    to do them locally with Kind. You will learn how to deploy a Kubernetes cluster
    locally using Kind or in the cloud (AWS, Google Cloud, and Azure) in this chapter.
    Finally, you will need `kubectl` to interact with your Kubernetes cluster. You’ll
    learn how to install it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Installing kubectl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`kubectl` is a CLI that we are going to use to send commands to a Kubernetes
    cluster. You must have this installed so that you can interact with the cluster
    (regardless of whether it’s running locally or in the cloud):'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `kubectl` on macOS with Homebrew, use the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To install `kubectl` in a Linux distribution, you can use `curl` to download
    the binary executable:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To install it in a Windows system, you can use the `chocolatey` package manager:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From this point on, every `kubectl` command will be the same, regardless of
    the OS you’re using. To check if the installation went successfully, run the following
    command. This will give you a nice, formatted view of the version of `kubectl`
    that’s running on your system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s move and install `kind`.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a local cluster using Kind
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying a local Kubernetes cluster can be extremely helpful for learning,
    testing, and preparing deployments for a production environment as Kubernetes
    is the same, wherever you run it. Let’s start by deploying a single-node local
    cluster using Kind.
  prefs: []
  type: TYPE_NORMAL
- en: Installing kind
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kind is a tool that allows you to run Kubernetes on your local machine inside
    Docker containers. Besides being light and easy to set up, Kind delivers performance
    with the same Kubernetes standards. Kind clusters pass upstream Kubernetes conformance
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kind is distributed as a single binary file. You can install it easily with
    package managers (make sure you have Docker already installed):'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re using macOS, use Homebrew to install `kind`, like so:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you’re using a Linux distribution (Ubuntu, for instance), you can install
    it with `curl`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you’re using a Windows system, install it with `chocolatey` ([https://chocolatey.org/packages/kind):](https://chocolatey.org/packages/kind):)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the installation is finished, you can check if it was installed correctly
    by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Deploying the cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, to deploy a single-node local cluster, just run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If this is the first time you’ve run this command, Kind will download the control
    plane image, create a Docker container for it, and configure a single-node Kubernetes
    cluster for you. The first time it runs, Kind will take 1-2 minutes to complete
    as it downloads the Kubernetes image. The next runs will be much faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify that the cluster is up, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This will print connection details for the local cluster. And, that’s it! You’re
    good to go.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re willing to work with a cloud-based Kubernetes cluster, in the next
    few sections, we will deploy a cluster on AWS, Google Cloud, and Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an AWS EKS cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`kubectl` to manage worker nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: To get started with EKS, you need an AWS account. Go to [http://aws.amazon.com](http://aws.amazon.com)
    and click **Create an AWS account**. Follow the steps to sign up for a new account;
    note that you will be requested to provide your credit card information. AWS offers
    a free usage tier that provides limited resources at no charge for 12 months.
    This is usually sufficient to run small workloads but not Kubernetes (although
    costs will not be high if you manage it wisely). AWS charges 73 dollars per month
    for every running cluster (assuming that the cluster is running for the whole
    month; if it runs just for a couple of days or hours, billing should be a fraction
    of that accordingly) plus the proper charging for each node that is running according
    to the size of the chosen machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'After setting up the account, you must access it with your `root` user. We
    need to create an IAM user with specific permissions as this is the recommended
    usage. In the AWS console, go to the **IAM** service and click **Users** in the
    left panel. You should see a screen like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – The Users menu in IAM](img/B21927_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – The Users menu in IAM
  prefs: []
  type: TYPE_NORMAL
- en: Then, define the username and password for access. When you click **Next**,
    choose to attach policies directly (since this is only a study account – this
    configuration is not suited for a production environment). Choose **AdministratorAccess**
    from the list (which will permit you to do everything in AWS) and click **Next**
    to review. Then, click the **Create user** button to finish the process. When
    all is set, remember to download the AWS secrets (AWS access key ID and AWS secret
    access key). You will need these secrets to authenticate in AWS from your computer.
    Now, log off the console and access it again with your new IAM user to validate
    that your new IAM user has been configured correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we look at the tools for setting up a Kubernetes cluster, we need to
    install the AWS CLI to interact with AWS from our local terminal. To do so, type
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After installed, now we run aws configure to set up our AWS credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You will be prompted for your AWS credentials (AWS Access Key ID and AWS Secret
    Access Key). Copy and paste those credentials as you are asked for them. Then,
    the configuration will ask for the default AWS region. You can work in us-east-1\.
    For the output format, `json` is a good choice.
  prefs: []
  type: TYPE_NORMAL
- en: This will save a config file with your credentials at `~/.aws/credentials`.
    Now, you can run AWS CLI commands to interact with AWS services.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have an AWS account and your IAM user and the AWS CLI have been configured,
    install the `eksctl` command-line tool. This tool simplifies and automates the
    process of deploying EKS clusters. Go to [https://eksctl.io/installation/](https://eksctl.io/installation/)
    and follow the installation instructions. Note that the documentation page lists
    the permissions you should have to deploy a cluster with `eksctl`. The **AdministratorAccess**
    policy should englobe any of the permissions you need.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the installation, to check if `eksctl` has been set up correctly, run
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see some output showing the version. Now, let’s create a cluster
    from scratch with a single command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The base of this command is `eksctl create cluster`. From the second line on,
    we are stating some options. Let’s look at them:'
  prefs: []
  type: TYPE_NORMAL
- en: The `--managed` option tells `eksctl` to create a fully managed cluster. It
    will handle creating the EKS control plane, node groups, networking, and more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `--alb-ingress-access` option will configure the cluster to allow inbound
    traffic from load balancers. This is required for load balancer type services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `--node-private-networking` option enables private networking between worker
    nodes. Nodes will have private IP addresses only.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `--full-ecr-access` option gives worker nodes full access to ECR container
    image repositories. This is needed to pull container images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `--name` option sets a user-defined name for the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `--instance-type` option configures the instance type that will be used
    for worker nodes. In this case, we are choosing to use the `m6i.xlarge` EC2 instance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `--region` option sets the AWS region – in this case, N. Virginia (us-east-1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `--nodes-min` and `--nodes-max` options set an autoscaling rule for the
    node group. Here, we set the minimum to `2` instances and the maximum to `4` instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `--nodegroup` option sets the node group’s name to `ng-studycluster`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will take around 10-15 minutes to complete. `eksctl` handles all the details
    of creating VPCs, subnets, security groups, IAM policies, and other AWS resources
    needed to run an EKS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, `eksctl` uses CloudFormation to provision the AWS infrastructure.
    The `eksctl` command generates a CloudFormation template based on the parameters
    provided. It then deploys this template to create the necessary resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some key components that are created by `eksctl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Virtual private cloud (VPC)**: A VPC network where your cluster resources
    will run. This includes public and private subnets across multiple availability
    zones'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EKS cluster**: The Kubernetes control plane, which consists of the API server,
    etcd, controller manager, scheduler, and more. AWS operates and manages this'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Worker node groups**: Managed node groups containing EC2 instances that will
    run your Kubernetes workloads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security groups**: Firewall rules to control access to the cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IAM roles and policies**: Access policies to allow worker nodes and Kubernetes
    to access AWS APIs and resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the `eksctl` command completes, your EKS cluster will be ready to use.
    You can view details about the cluster by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Next, we are going to create a Kubernetes cluster using Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Google Cloud GKE cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To deploy a Kubernetes cluster on Google Cloud, we need to set up a Google Cloud
    account and install the `gcloud` **command-line interface** (**CLI**). To do that,
    go to [https://cloud.google.com/](https://cloud.google.com/) and click on **Start
    free** to create a new account. Follow the instructions and, when your new account
    is created, navigate to the console at [https://console.cloud.google.com/](https://console.cloud.google.com/).
    This is where you can manage all your Google Cloud resources.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can deploy a GKE cluster, we need to enable the necessary APIs. Click
    on the navigation menu icon in the top left and go to `Kubernetes Engine API`
    and click on it. Make sure it is enabled. It is also a good practice to enable
    `gcloud` CLI. This will allow us to manage Google Cloud resources from the command
    line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `gcloud` CLI can be installed on Linux, macOS, and Windows. In a Linux
    distribution, you can download the installation script like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, extract the archive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, run the install script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'For a macOS installation, you can use Homebrew:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For Windows, you can download the `gcloud` installer from [https://cloud.google.com/sdk/docs/install#windows](https://cloud.google.com/sdk/docs/install#windows).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the installation is done, you can check it by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you’ve installed it, you must initiate the CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This will walk you through logging in with your Google account and configuring
    `gcloud`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that `gcloud` is installed and initialized, we can deploy a Kubernetes
    cluster on GKE. First, choose a Google Cloud project to deploy the cluster under.
    You can create a new project and set it as active like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you must set a compute zone to deploy the cluster in. Here, we will work
    in the `us-central1-a` zone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can deploy a Kubernetes cluster by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `–num-nodes` parameter controls how many nodes you are going to be deployed.
    Note that this will take several minutes to complete as Google Cloud will set
    up all the cluster components, networking, and so on. Once deployed, gather credentials
    to interact with the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This will save credentials to your Kubernetes config file. Finally, you can
    verify that you can connect to the cluster by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: You should see information about the Kubernetes master and the cloud provider.
    And that’s it! You now have a fully functional Kubernetes cluster running on Google
    Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will do the same with Microsoft Azure Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an Azure AKS cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will walk through the steps to deploy a Kubernetes cluster
    using **Azure Kubernetes Service** (**AKS**). To get started with AKS, you need
    an Azure account. First, go to [https://azure.com](https://azure.com), click **Try
    Azure for free**, and then click **Start free**. This will allow you to start
    a free trial account on Azure. You will need to provide some basic information,
    such as your email and phone number, to set up the account. Make sure you use
    a valid email as Azure will send a verification code to complete the signup process.
    Once your account has been created, you will be directed to the Azure portal.
    This is the main dashboard for managing all your Azure resources.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it’s recommended to install the Azure CLI on your local machine.
    The Azure CLI allows you to manage Azure resources from the command line. Follow
    the instructions at [https://docs.microsoft.com/en-us/cli/azure/install-azure-cli](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli)
    to install it on Linux, macOS, or Windows.
  prefs: []
  type: TYPE_NORMAL
- en: After installing the CLI, run `az login` and follow the prompts to authenticate
    with your account. This will allow you to run Azure commands from your terminal.
    With the Azure account set up and the CLI installed, you are ready to deploy an
    AKS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing you must do is create a resource group. Resource groups in
    Azure allow you to logically group resources such as AKS clusters, storage accounts,
    virtual networks, and more. Let’s start by creating a resource group for our AKS
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a resource group named `myResourceGroup` in the `eastus` region.
    You can specify any region close to you. Now, we can create an AKS cluster in
    this resource group. The basic command to create a cluster is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a Kubernetes cluster named `studycluster` with two nodes.
    We generate SSH keys automatically to set up access to the nodes. Some other options
    you can specify are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--node-vm-size`: The size of the virtual machines for nodes. The default is
    `Standard_D2s_v3`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--kubernetes-version`: The Kubernetes version to use for the cluster. It defaults
    to the latest version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--enable-addons`: Enables add-ons such as monitoring, virtual nodes, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `az aks create` command handles setting up the Kubernetes cluster, virtual
    machines, networking, storage, and more automatically. The process may take 5-10
    minutes to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it is done, you can connect to the cluster by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This will display the nodes that are part of your AKS cluster. At this point,
    you have successfully deployed an AKS cluster and are ready to deploy our API
    and batch processing job on it.
  prefs: []
  type: TYPE_NORMAL
- en: Running your API on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From this point, you can choose the Kubernetes deployment type you like the
    most (local or cloud-based) to run our API. The following examples will be shown
    with AWS but you also can choose another cloud provider. Feel free to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is time to retake the API we built in [*Chapter 1*](B21927_01.xhtml#_idTextAnchor015)
    and ship it to production. We developed a simple API that, when requested, can
    say hello to you or answer with a (very cool) joke.
  prefs: []
  type: TYPE_NORMAL
- en: We already have the code for the API ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/app/main.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/app/main.py))
    and the Dockerfile to build the container image ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/Dockerfile](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/Dockerfile)).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the image to be accessible to Kubernetes, it should be available on a container
    registry. Each cloud provider has a registry but to make things simple, we are
    working with DockerHub ([https://hub.docker.com/](https://hub.docker.com/)). So
    long as your images are public, you can store as many images as you want for free.
    Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, in your terminal, type the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remember to replace `<USERNAME>` with your real DockerHub username. We are
    changing the image name to `jokeapi` so that it’s easier to locate. If you’re
    running Docker on a Mac M1, it is important to set the `--platform` parameter
    to make the container image compatible with AMD64 machines. To do so, run the
    following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can push the image to DockerHub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Go to [https://hub.docker.com/](https://hub.docker.com/) and log in. You should
    see your image listed in the **Repositories** section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, the image is available for Kubernetes. Next, we need to define some Kubernetes
    resources to run our API. We will create a deployment and a service.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll create the deployment. This specifies how many Pod replicas to
    run and their configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**deployment_api.yaml**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will run two Pod replicas using the Docker image we built. Note that we
    are opening port `8087` in the container. This is similar to the `EXPOSE` command
    in the Dockerfile.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will create a namespace to separate and organize our resources and
    apply the deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will create the deployment and the two Pod replicas in the `jokeapi` namespace.
    We can check that everything is working by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s check if the Pods are running properly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see an output like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Creating a service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we’ll specify a service to expose the Pods in the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**service_api.yaml**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This creates a ClusterIP service that exposes the API Pods on an internal IP
    address in the cluster. Note that the `type` parameter is not specified within
    the `spec` section of the YAML file, so, it defaults to ClusterIP.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To make the API accessible externally, we can create a load balancer (not possible
    with a local `kind` cluster, only with cloud-based clusters):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**lb_api.yaml**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, the `type` definition is set and this code will define a load balancer
    and assign an external IP. Next, we will deploy the load balancer service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can test if the API is accessible. First, we must get the load balancer’s
    URL (here, we are working in AWS) by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Copy the content under `EXTERNAL-IP`, paste the URL in a browser, and add `/joke`.
    For instance, in my implementation here, I got `ab1cdd20ce1a349bab9af992211be654-1566834308.us-east-1.elb.amazonaws.com/joke`.
    You should see the following response on your screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Success! We have a (great) joke in our browser! Now, we will deploy the API
    with an ingress instead of a load balancer (for cloud-based clusters only).
  prefs: []
  type: TYPE_NORMAL
- en: Using an ingress to access the API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this deployment, we will use the NGINX ingress controller and connect it
    to the load Balancer provided by AWS (the process is very similar if you are working
    with any cloud provider). Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll create a new namespace for NGINX and deploy the controller on
    Kubernetes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will deploy the NGINX controller using the official manifests. Now, we
    have to edit one line in the deployment to make sure it uses the load balancer
    as an ingress deployment and not `NodePort`, its default. In your terminal, type
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Search for the `spec.type` field and change its value to `LoadBalancer`.After
    saving the file, let’s check the services that have been deployed with the controller:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will see that the `ingress-nginx-controller` service is set to `LoadBalancer`
    and has an external IP related to it. Now, it is easy to set up an ingress that
    points to this ingress controller. First, we’ll create a service defined in the
    `service_api.yaml` file. This service should be set to a ClusterIP type (see the
    code in the previous section). Then, we can define an ingress with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**ingress.yaml**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This ingress will route external traffic to the internal service IP. Once the
    ingress has an external IP assigned, we should be able to access our API by hitting
    that URL. Type the following
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the external URL for the controller and add `/joke` to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Et voilà! In the next section, we will deploy our data processing job on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Running a data processing job in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will deploy the simple data processing job from [*Chapter
    1*](B21927_01.xhtml#_idTextAnchor015) on Kubernetes. We have already developed
    the job ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/run.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/run.py))
    and built a Dockerfile to package it into a container image ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/Dockerfile_job](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter01/Dockerfile_job)).
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have to build a Docker image and push it to a repository that’s accessible
    to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create a Kubernetes job to run our data processing task. Here’s
    an example job manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: job.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This configures a job named `dataprocessingjob` that will run one replica of
    the `<USERNAME>/dataprocessingjob:v1` image. Now, we can create a new namespace
    and deploy the job, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This defines a job called `dataprocessingjob` that will run a single Pod using
    our Docker image. We set `restartPolicy: Never` since we want the container to
    run to completion rather than restart.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check the status of the job like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'After the job has been completed, we will see `1/1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the logs from our job, we can use `kubectl logs` on the Pod created
    by the job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'In my case, I typed the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'I got the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: This will print the application output from our Python program so that we can
    verify it ran correctly. And that’s it! You ran a data processing job inside Kubernetes!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we gained hands-on experience deploying Kubernetes clusters
    and running applications in them. We started by installing `kubectl` and deploying
    a local Kubernetes cluster using Kind. Then, we deployed managed Kubernetes clusters
    on AWS, GCP, and Azure. While the cloud providers differ, Kubernetes provides
    a consistent environment to run containers.
  prefs: []
  type: TYPE_NORMAL
- en: After setting up our clusters, we containerized and deployed the simple API
    application from [*Chapter 1*](B21927_01.xhtml#_idTextAnchor015). This demonstrated
    how to define Kubernetes deployments, services, ingress, and load balancers to
    run web applications. Then, we deployed the data processing batch job from [*Chapter
    1*](B21927_01.xhtml#_idTextAnchor015) as a Kubernetes job. This showed us how
    to leverage Kubernetes for running one-off tasks and jobs.
  prefs: []
  type: TYPE_NORMAL
- en: By going through the process of deploying clusters and applications end-to-end,
    you now have first-hand experience with Kubernetes. You understand how to package
    applications as containers, expose them via services, ingress, or load balancers,
    and leverage Kubernetes abstractions such as deployments and jobs. With these
    skills, you are equipped to run applications and workloads on Kubernetes in development
    or production environments.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to take a closer look at the modern data stack,
    understanding each technology, why they are important, and how they link together
    to build a data solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Big Data Stack'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you will dive into the core technologies that make up the **modern
    data stack**, a set of tools and architectures designed for building robust and
    scalable data pipelines. You will gain a solid understanding of the Lambda architecture
    and its components, and gain some hands-on experience with powerful big data tools
    such as Apache Spark, Apache Airflow, and Apache Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B21927_04.xhtml#_idTextAnchor070), *The Modern Data Stack*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B21927_05.xhtml#_idTextAnchor092), *Big Data Processing with
    Apache Spark*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B21927_06.xhtml#_idTextAnchor112), *Apache Airflow for Building
    Pipelines*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B21927_07.xhtml#_idTextAnchor122), *Apache Kafka for Real-Time
    Events and Data Ingestion*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
