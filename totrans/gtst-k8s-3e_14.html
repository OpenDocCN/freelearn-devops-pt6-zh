<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Hardening Kubernetes</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we'll look at considerations for moving to production. We will also show you some helpful tools and third-party projects that are available in the Kubernetes community at large and where you can go to get more help.</span></p>
<p><span>This chapter will discuss the following topics:</span></p>
<ul>
<li>Production characteristics</li>
<li style="font-weight: 400">Lessons learned from Kubernetes production</li>
<li style="font-weight: 400">Hardening the cluster</li>
<li style="font-weight: 400">The Kubernetes ecosystem</li>
<li>Where can you get help?</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Ready for production</h1>
                </header>
            
            <article>
                
<p>So far in this book, we have walked through a number of typical operations using Kubernetes. As we have been, K8s offers a variety of fea<span>tures and abstractions that ease the burden of day-to-day management for container depl</span><span>oyments.</span></p>
<p>There are many characteristics that define a production-ready system for containers. The following diagram provides a high-level view of the major concerns for production-ready clusters. This is by no means an exhaustive list, but it's meant to provide some solid ground for heading into production operations:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/8bb4fd04-edf2-4be7-ad9e-f996021c0426.png" style="width:45.83em;height:34.50em;" width="978" height="736"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Production characteristics for container operations</div>
<p>We saw how the core concepts and abstractions of Kubernetes address a few of these concerns. The service abstraction has built-in service discovery and health checking at both the service and application level. We also get seamless application updates and scalability from the replication controller and deployment constructs. All of the core abstractions of services, replication controllers, replica sets, and pods work with a core scheduling and affinity rulesets and give us easy service and application composition.</p>
<p class="mce-root"/>
<p>There is built-in support for a variety of persistent storage options, and the networking model provides manageable network operations with options to work with other third-party providers. We also took a brief look at CI/CD integration with some of the popular tools in the marketplace.</p>
<p>Furthermore, we have built-in system events tracking, and with the major cloud providers, an out-of-the-box setup for monitoring and logging. We also saw how this can be extended to third-party providers such as Stackdriver and Sysdig. These services also address overall node health and proactive trend deviation alerts.</p>
<p>The core constructs also help us address high availability in our application and service layers. The scheduler can be used with autoscaling mechanisms to provide this at a node level. Then, there is support for making the Kubernetes master itself highly available. In <a href="92883e26-5c4a-466e-bfe6-1a5e0d0997f9.xhtml">Chapter 12</a>, <em>Cluster Federation and Multi-Tenancy</em>, we took a brief look at the new federation capabilities that promise a multi-cloud and multi-data center model for the future.</p>
<p>Finally, we explored a new breed of operating systems that give us a slim base to build on and secure update mechanisms for patching and updates. The slim base, together with scheduling, can help us with efficient resource utilization. In addition, we looked at some hardened concerns and explored the image trust and verification tools available. Security is a wide topic and capability matrices exist for this topic alone.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Ready, set, go</h1>
                </header>
            
            <article>
                
<p>While there are still some gaps, a variety of the remaining security and operation concerns are actively being addressed by third-party companies, as we will see in the following section. Going forward, the Kubernetes project will continue to evolve, and the community of projects and partners around K8s and Docker will also grow. The community is closing the remaining gaps at a phenomenal pace.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Lessons learned from production</h1>
                </header>
            
            <article>
                
<p>Kubernetes has been around long enough now that there are a number of companies running Kubernetes. In our day jobs, we've seen Kubernetes run in production across a number of different industry verticals and in numerous configurations. Let's explore what folks across the industry are doing when providing customer-facing workloads. At a high level, there are several key areas:</p>
<ul>
<li style="font-weight: 400">Make sure to set limits in your cluster.</li>
<li style="font-weight: 400">Use the appropriate workload types for your application.</li>
<li style="font-weight: 400">Label everything! Labels are very flexible and can contain a lot of information that can help identify an object, route traffic, or determine placement.</li>
<li style="font-weight: 400">Don't use default values.</li>
<li style="font-weight: 400">Tweak the default values for the core Kubernetes components.</li>
<li style="font-weight: 400">Use load balancers as opposed to exposing services directly on a node's port.</li>
<li style="font-weight: 400">Build your Infrastructure as Code and use provisioning tools such as CloudFormation or Terraform, and configuration tools such as Chef, Ansible, or Puppet.</li>
<li style="font-weight: 400">Consider not running stateful workloads in production clusters until you build up expertise in Kubernetes.</li>
<li style="font-weight: 400">Investigate higher-function templating languages to maintain the state of your cluster. We'll explore a few options for an immutable infrastructure in the following chapter.</li>
<li style="font-weight: 400">Use RBAC, the principle of least privilege, and separation of concerns wherever possible.</li>
<li style="font-weight: 400">Use TLS-enabled communications for all inter-cluster chatter. You can set up TLS and certificate rotation for the <kbd>kubelet</kbd> communication in your cluster.</li>
<li style="font-weight: 400">Until you're comfortable with managing Kubernetes, build lots of small clusters. It's more operational overhead, but it will get you into the deep end of experience faster so that you see more failure and experience the operator burden more heavily.</li>
<li style="font-weight: 400">As you get better at Kubernetes, build bigger clusters that use namespaces, network segmentation, and the authorization features to break up your cluster into pieces.</li>
<li style="font-weight: 400">Once you're running a few large clusters, manage them with <kbd>kubefed</kbd>.</li>
<li style="font-weight: 400">If you can, use the features of your given cloud service provider's built-in high availability on a Kubernetes platform. For example, run Regional Clusters on GCP, with GKE. This feature spreads your nodes across several availability zones in a region. This allows for resilience against a single zone failure, and provides the conceptual building blocks for the zero downtime upgrades of your master nodes.</li>
</ul>
<p>In the next section, we'll explore one of these concepts <span>–</span> limits <span>– </span>in more detail.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Setting limits</h1>
                </header>
            
            <article>
                
<p>If you've done work with containers before, you will know that one of the first and easiest things to set up for your containers is resource limits in the form of the following metrics:</p>
<ul>
<li style="font-weight: 400">CPU</li>
<li style="font-weight: 400">Memory</li>
<li style="font-weight: 400">Requests</li>
</ul>
<p>You may be familiar with setting runtime limits on resources with Docker's CLI, which specify flags to limit these items and more:</p>
<pre><strong>docker run --it --cpu-rt-runtime=950000 \</strong><br/><strong> --ulimit rtprio=99 \</strong><br/><strong> --memory=1024m \</strong><br/><strong> --cpus=".5"</strong><br/><strong> alpine /bin/sh</strong></pre>
<p>Here, we're setting a runtime parameter, creating a <kbd>ulimit</kbd>, and setting memory and CPU quotas. The story evolves a bit in Kubernetes, as you can create these limits to a specific namespace, which allows you to characterize your limits by the domains of your cluster. You have four overarching parameters so that you can work resource limits in Kubernetes:</p>
<pre>spec.containers[].resources.limits.cpu<br/>spec.containers[].resources.requests.cpu<br/>spec.containers[].resources.limits.memory<br/>spec.containers[].resources.requests.memory</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scheduling limits</h1>
                </header>
            
            <article>
                
<p>When you create a pod with a memory limit, Kubernetes looks for a node with the right labels and selectors that has enough of the resource types, CPU, and memory, that the pod requires. Kubernetes is in charge of ensuring that the total memory request of the pods on a node is not less than the pod's total resources. This can sometimes result in unexpected outcomes, as you can have node limitations reached in terms of capacity, even if the net utilization of a pod is low. This is a design of the system in order to accommodate varying load levels.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>You can look through pod logs to find out when this has occurred:</p>
<pre><strong>$ kubectl describe pod web| grep -C 3 Events</strong><br/><strong>Events:</strong><br/><strong>FirstSeen LastSeen Count From Subobject PathReason Message</strong><br/><strong>74s 15s 2 {scheduler } FailedScheduling Failed for reason PodExceedsFreeCPU and possibly others</strong></pre>
<p>You can address these issues by removing unneeded pods, ensuring that your pod isn't larger as a whole than any one available node, or simply add more resources to the cluster.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Memory limit example</h1>
                </header>
            
            <article>
                
<p>Let's walk through an example. First, we'll create a namespace to house our memory limit:</p>
<pre><strong>master $ kubectl create namespace low-memory-area</strong><br/><strong>namespace "low-memory-area" created</strong></pre>
<p>Once we've created the namespace, we can create a file that sets a <kbd>LimitRange</kbd> object, which will allow us to enforce a default for memory limits and requests. Create a file called <kbd>memory-default.yaml</kbd> with the following contents:</p>
<pre>apiVersion: v1<br/>kind: LimitRange<br/>metadata:<br/> name: mem-limit-range<br/>spec:<br/> limits:<br/> - default:<br/>     memory: 512Mi<br/>   defaultRequest:<br/>     memory: 256Mi<br/>   type: Container</pre>
<p>And now, we can create it in the namespace:</p>
<pre><strong>master $ kubectl create -f test.ym --namespace=low-memory-area</strong><br/><strong>limitrange "mem-limit-range" created</strong></pre>
<p><span>Let's create a pod without a memory limit, in the low-memory-area namespace, and see what happens.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Create the following <kbd>low-memory-pod.yaml</kbd> file:</p>
<pre>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/> name: low-mem-demo<br/>spec:<br/> containers:<br/> - name: low-mem-demo<br/>   image: redis</pre>
<p><span>Then, we can create the pod with this command:</span></p>
<pre><strong>kubectl create -f low-memory-pod.yaml --namespace=low-memory-area</strong><br/><strong>pod "low-mem-demo" created</strong></pre>
<p><span>Let's see if our resource constraints were added to the pod's configuration for containers, without having to explicitly specify it in the pod configuration. Notice the memory limits in place! We've removed some of the informational output for readability:</span></p>
<pre><strong>kubectl get pod low-mem-demo --output=yaml --namespace=low-memory-area</strong></pre>
<p>Here's the output of the preceding code:</p>
<pre><strong>apiVersion: v1</strong><br/><strong>kind: Pod</strong><br/><strong>metadata:</strong><br/><strong> annotations:</strong><br/><strong>   kubernetes.io/limit-ranger: 'LimitRanger plugin set: memory request for container</strong><br/><strong>     low-mem-demo; memory limit for container low-mem-demo'</strong><br/><strong> creationTimestamp: 2018-09-20T01:41:40Z</strong><br/><strong> name: low-mem-demo</strong><br/><strong> namespace: low-memory-area</strong><br/><strong> resourceVersion: "1132"</strong><br/><strong> selfLink: /api/v1/namespaces/low-memory-area/pods/low-mem-demo</strong><br/><strong> uid: 52610141-bc76-11e8-a910-0242ac11006a</strong><br/><strong>spec:</strong><br/><strong> containers:</strong><br/><strong> - image: redis</strong><br/><strong>   imagePullPolicy: Always</strong><br/><strong>   name: low-mem-demo</strong><br/><strong>   resources:</strong><br/><strong>     limits:</strong><br/><strong>       memory: 512Mi</strong><br/><strong>     requests:</strong><br/><strong>       memory: 256Mi</strong><br/><strong>   terminationMessagePath: /dev/termination-log</strong><br/><strong>   terminationMessagePolicy: File</strong><br/><strong>   volumeMounts:</strong><br/><strong>   - mountPath: /var/run/secrets/kubernetes.io/serviceaccount</strong><br/><strong>     name: default-token-t6xqm</strong><br/><strong>     readOnly: true</strong><br/><strong> dnsPolicy: ClusterFirst</strong><br/><strong> nodeName: node01</strong><br/><strong> restartPolicy: Always</strong><br/><strong> schedulerName: default-scheduler</strong><br/><strong> securityContext: {}</strong><br/><strong> serviceAccount: default</strong><br/><strong> serviceAccountName: default</strong><br/><strong> terminationGracePeriodSeconds: 30</strong><br/><strong> tolerations:</strong><br/><strong> - effect: NoExecute</strong><br/><strong>   key: node.kubernetes.io/not-ready</strong><br/><strong>   operator: Exists</strong><br/><strong>   tolerationSeconds: 300</strong><br/><strong> - effect: NoExecute</strong><br/><strong>   key: node.kubernetes.io/unreachable</strong><br/><strong>   operator: Exists</strong><br/><strong>   tolerationSeconds: 300</strong><br/><strong> volumes:</strong><br/><strong> - name: default-token-t6xqm</strong><br/><strong>   secret:</strong><br/><strong>     defaultMode: 420</strong><br/><strong>     secretName: default-token-t6xqm</strong><br/><strong> hostIP: 172.17.1.21</strong><br/><strong> phase: Running</strong><br/><strong> podIP: 10.32.0.3</strong><br/><strong> qosClass: Burstable</strong><br/><strong> startTime: 2018-09-20T01:41:40Z</strong><br/><br/></pre>
<p><span>You can delete the pod with the following command:</span></p>
<pre><strong>Kubectl delete pod low-mem-demo --namespace=low-memory-area</strong><br/><strong>pod "low-mem-demo" delete</strong></pre>
<p>There are a lot of options for configuring resource limits. If you create a memory limit, but don't specify the default request, the request will be set to the maximum available memory, which will correspond to the memory limit. That will look like the following:</p>
<pre>resources:<br/> limits:<br/>   memory: 4096m<br/> requests:<br/>   memory: 4096m</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>In a cluster with diverse workloads and API-driven relationships, it's incredibly important to set memory limits with your containers and their corresponding applications in order to prevent misbehaving applications from disrupting your cluster. Services don't implicitly know about each other, so they're very susceptible to resource exhaustion if you don't configure limits correctly.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scheduling CPU constraints</h1>
                </header>
            
            <article>
                
<p>Let's look at another type of resource management, the constraint. We'll use the CPU dimension here, and we'll explore how to set the maximum and minimum values for available resources for a given container and pod in a namespace. There are a number of reasons you might want to limit CPU on a Kubernetes cluster:</p>
<ul>
<li style="font-weight: 400">If you have a namespaced cluster that has different levels of production and non-production workloads, you may want to specify higher limits for your production workloads. You can allow quad-core CPU consumption for production; put pin development, staging, or UAT-type workloads to a single CPU; or stagger them according to environment needs.</li>
<li style="font-weight: 400">You can also ban requests from pods that require more CPU resources than your nodes have available. If you're running a certain type of machine on a cloud service provider, you can ensure that workloads that require X cores aren't scheduled on machines with &lt;X cores.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">CPU constraints example</h1>
                </header>
            
            <article>
                
<p>Let's go ahead and create another namespace in which to hold our example:</p>
<pre><strong>kubectl create namespace cpu-low-area</strong></pre>
<p>Now, let's set up a <kbd>LimitRange</kbd> for CPU constraints, which uses the measurement of millicpus. If you're requesting 500 m, it means that you're asking for 500 millicpus or millicores, which is equivalent to 0.5 in notational form. When you request 0.5 or 500 m, you're asking for half of a CPU in whatever form your platform provides (vCPU, Core, Hyper Thread, vCore, or vCPU).</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>As we did previously, let's create a <kbd>LimitRange</kbd> for our CPU constraints:</p>
<pre>apiVersion: v1<br/>kind: LimitRange<br/>metadata:<br/> name: cpu-demo-range<br/>spec:<br/> limits:<br/> - max:<br/>     cpu: "500m"<br/>   min:<br/>     cpu: "300m"<br/>   type: Container</pre>
<p>Now, we can create the <kbd><span>LimitRange</span></kbd>:</p>
<pre><strong>kubectl create -f cpu-constraint.yaml --namespace=cpu-low-area</strong></pre>
<p>Once we create the <kbd>LimitRange</kbd>, we can inspect it. What you'll notice is that the <kbd>defaultRequest</kbd> is specified as the same as the maximum, because we didn't specify it. Kubernetes sets the <kbd>defaultRequest</kbd> to max:</p>
<pre><strong>kubectl get limitrange cpu-demo-range --output=yaml --namespace=cpu-low-area</strong><br/><br/><strong>limits:</strong><br/><strong>- default:</strong><br/><strong>   cpu: 500m</strong><br/><strong> defaultRequest:</strong><br/><strong>   cpu: 500m</strong><br/><strong> max:</strong><br/><strong>   cpu: 500m</strong><br/><strong> min:</strong><br/><strong>   cpu: 300m</strong><br/><strong> type: Container</strong></pre>
<p>This is the intended behavior. When further containers are scheduled in this namespace, Kubernetes first checks to see whether the pod specifies a request and limit. If it doesn't, the defaults are applied. Next, the controller confirms that the CPU request is more than the lower bound in the <kbd>LimitRange</kbd>, 300 m. Additionally, it checks for the upper bound to make sure that the object is not asking for more than 500 m.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>You can check the container constraints again by looking at the YAML output of the pod:</p>
<pre><strong>kubectl get pod cpu-demo-range --output=yaml --namespace=cpu-low-area<br/><br/>resources:</strong><br/><strong> limits:</strong><br/><strong>   cpu: 500m</strong><br/><strong> requests:</strong><br/><strong>   cpu: 300m</strong></pre>
<p>Now, don't forget to delete the pod:</p>
<pre><strong>kubectl delete pod cpu-demo-range --namespace=cpu-low-area</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Securing a cluster</h1>
                </header>
            
            <article>
                
<p>Let's look at some other common recommendations for hardening your cluster in production. These use cases cover both intentional, malicious actions against your cluster, as well as accidental misuse. Let's take a look at what we can do to secure things.</p>
<p>First off, you want to ensure that access to the Kubernetes API is controlled. Given that all actions in Kubernetes are API-driven, we should secure this interface first. We can control access to this API with several settings:</p>
<ul>
<li><strong>Encode all traffic</strong>: In order to keep communication secure, you should make sure that <strong>Transport Level Security</strong> (<strong>TLS</strong>) is set up for API communication in the cluster. Most of the installation methods we've reviewed in this book create the necessary component certificates, but it's always on the cluster operators to identify all in-use local ports that may not use the more secure settings.</li>
<li><strong>Authenticate your access</strong>: Just as with any large scale computer system, you want to ensure that the identity of a user is established. For small clusters, you can use certs or tokens, while larger production clusters should use OpenID or LDAP.</li>
<li><strong>Control your access</strong>: After you've established the identity of the role accessing your API, you always want to ensure that you pass your authenticated access request through an authorization filter with Kubernetes' built-in <strong>role-based-access-control</strong> (<strong>RBAC</strong>), which helps operators limit control and access by roles and users. There are two authorizer plugins, node and RBAC, that can be used, along with the <kbd>NodeRestriction</kbd> admission plugin. A key point to keep in mind is that role granularity should increase as cluster size increases, and even more so from non-production environments toward production environments.</li>
</ul>
<p class="mce-root"/>
<p>By default, authentication to use the <kbd>kubelet</kbd> is turned off. You can enable authorization/authentication on the <kbd>kubelet</kbd> by turning on certificate rotation.</p>
<div class="packt_tip">You can read more about certificate rotation here: <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/">https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/</a>.</div>
<p>We can also modify the usage of Kubernetes at runtime:</p>
<ul>
<li style="font-weight: 400">Long-time operators of Kubernetes in production will recognize this as a reference point to our previous discussions on limits and policies. As discussed previously, resource quotas limit the number of resources provided within a namespace, while limits ranges between restrict minimum and maximum sizes.</li>
<li style="font-weight: 400">You can determine the privileges of your pods by defining a security context. Here, you can specify things like a particular Linux user, group, volume mount access, allowing privilege escalation, and more.</li>
<li style="font-weight: 400">You can also restrict access to logical partitions of your cluster by using network policies. You can ensure that certain namespaces are off limits to users, or determine whether or not they're able to set up services with specific load balancer configuration or open ports on host nodes.</li>
</ul>
<p>While the preceding patterns are useful for operations inside of Kubernetes, there are also several actions that you should take when securing your cluster from an external perspective:</p>
<ul>
<li style="font-weight: 400">First off, enable and monitor your logs! While this seems like a no-brainer, we see a lot of problems cropping up from people that aren't watching their logs, or who haven't created alerts based off these logs. Another hint: don't store logs inside of your cluster! If your cluster is breached, then those logs will be an invaluable source of information for malicious agents.</li>
<li style="font-weight: 400">Make double sure that you restrict access to your etcd cluster. This can come in the form of setting up security groups or firewalls and ensuring that your etcd cluster nodes have the appropriate identity and access management from an infrastructure perspective. From a cluster perspective, make sure that you're always using TLS certificates for authentication and strong credentials. In no case should any components inside your cluster have read/write access to the full etcd key/value space.</li>
<li style="font-weight: 400">Make sure to vet alpha/beta components and review third-party integrations. Make sure that you know what you're using when you enable it, and what it does when you turn it on! Emerging features or third-party tools may create attack surfaces or threat models where you're not aware of what dependencies they have. Beware of any tools that need to do work inside the kube-system, as it's a particularly powerful portion of the cluster.</li>
<li style="font-weight: 400">Encrypt your secrets at rest in etcd. This is good advice for any computerized system, and Kubernetes is no different here. The same goes for your backups to ensure that an attacker can't gain access to your cluster via inspection of those resources.</li>
</ul>
<p>For your production cluster, you should also be doing things such as scanning your container images, running static analysis of your YAML files, running containers as non-root users where possible, and running an <strong>intrusion detection system</strong> (<strong>IDS</strong>). Once you have all of this in place, you can begin to explore the functional capabilities of the service meshes out in the wild.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Third-party companies</h1>
                </header>
            
            <article>
                
<p>Since the Kubernetes project's initial release, there has been a growing ecosystem of partners. We looked at CoreOS, Sysdig, and many others in the previous chapters, but there are a variety of projects and companies in this space. We will highlight a few that may be useful as you move toward production. This is by no means an exhaustive list and it is merely meant to provide some interesting starting points.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Private registries</h1>
                </header>
            
            <article>
                
<p>In many situations, organizations will not want to place their applications and/or intellectual property in public repositories. For those cases, a private registry solution is helpful in securely integrating deployments end to end.</p>
<p>Google Cloud offers the Google Container Registry at <a href="https://cloud.google.com/container-registry/" target="_blank">https://cloud.google.com/container-registry/</a>.</p>
<p>Docker has its own trusted registry offering at <a href="https://www.docker.com/docker-trusted-registry" target="_blank">https://www.docker.com/docker-trusted-registry</a>.</p>
<p>Quay also provides secure private registries, vulnerability scanning, and comes from the CoreOS team, and can be found at <a href="https://quay.io/" target="_blank">https://quay.io/</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Google Kubernetes Engine</h1>
                </header>
            
            <article>
                
<p>Google was the main author of the original Kubernetes project and is still a major contributor. Although this book has mostly focused on running Kubernetes on our own, Google also offers a fully managed container service through the Google Cloud Platform.</p>
<p>Find more information on the <strong>Google Kubernetes Engine</strong> (<strong>GKE</strong>) website at <a href="https://cloud.google.com/container-engine/" target="_blank">https://cloud.google.com/container-engine/</a>.</p>
<p>Kubernetes will be installed on GKE and will be managed by Google engineers. They also provide private registries and integration with your existing private networks.</p>
<p>You create your first GKE cluster by using the following steps:</p>
<ol>
<li>From the GCP console, in <span class="packt_screen">Compute</span>, click on <span class="packt_screen">Container Engine</span>, and then on <span class="packt_screen">Container Clusters</span>.</li>
<li>If this is your first time creating a cluster, you'll have an information box in the middle of the page. Click on the <span class="packt_screen">Create</span> <span class="packt_screen">a container cluster</span> button.</li>
<li>Choose a name for your cluster and the zone. You'll also be able to choose the machine type (instance size) for your nodes and how many nodes (cluster size) you want in your cluster. You'll also see a choice for node image, which lets you choose the base OS and machine image for the nodes themselves. The master is managed and updated by the Google team themselves.</li>
<li>Leave Stackdriver logging and Stackdriver monitoring checked. Click on <span class="packt_screen">Create</span>, and in a few minutes, you'll have a new cluster ready for use.</li>
<li>You'll need <kbd>kubectl</kbd>, which is included with the Google SDK, to begin using your GKE cluster. Refer to <a href="446f901d-70fa-4ebe-be8a-0de14248f99c.xhtml">Chapter 1</a>, <em>Introduction to Kubernetes</em>, for details on installing the SDK. Once we have the SDK, we can configure <kbd>kubectl</kbd> and the SDK for our cluster using the steps outlined at <a href="https://cloud.google.com/container-engine/docs/before-you-begin#install_kubectl">https://cloud.google.com/container-engine/docs/before-you-begin#install_kubectl</a>.</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Azure Kubernetes Service</h1>
                </header>
            
            <article>
                
<p>Another cloud-managed offering is Microsoft's <strong>Azure Kubernetes Service</strong> (<strong>AKS</strong>). AKS is really nice because it allows you to choose from industry standard tools such as Docker Swarm, Kubernetes, and Mesos. It then creates a managed cluster for you, but uses one of these toolsets as the foundation. The advantage is that you can still use the tool's native API and management tools, but leave the management of the cloud infrastructure to Azure.</p>
<p>You can find out more about ACS at <a href="https://azure.microsoft.com/en-us/services/container-service/" target="_blank">https://azure.microsoft.com/en-us/services/container-service/</a>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">ClusterHQ</h1>
                </header>
            
            <article>
                
<p>ClusterHQ provides a solution for bringing stateful data into your containerized applications. They provide Flocker, a tool for managing persistent storage volumes with containers, and FlockerHub, which provides a storage repository for your data volumes.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Portworx</h1>
                </header>
            
            <article>
                
<p>Portworx is another player in the storage space. It provides solutions for bringing persistence storage to your containers. Additionally, it has features for snapshotting, encryption, and even multi-cloud replication.</p>
<p>Please refer to the Portworx website for more information: <a href="https://portworx.com/" target="_blank">https://portworx.com/</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Shippable</h1>
                </header>
            
            <article>
                
<p>Shippable is a continuous integration, continuous deployment, and release automation platform that has built-in support for a variety of modern container environments. The product touts support for any language with a uniform support for packaging and test.</p>
<p>Please refer to the Shippable website for more information: <a href="https://app.shippable.com/" target="_blank">https://app.shippable.com/</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Twistlock</h1>
                </header>
            
            <article>
                
<p>Twistlock.io is a vulnerability and hardening tool that's tailor-made for containers. It provides the ability to enforce policies, hardens according to CIS standards, and scans images in any popular registry for vulnerabilities. It also provides scan integration with popular CI/CD tools and RBAC solutions for many orchestration tools such as Kubernetes.</p>
<p>Please refer to the Twistlock website for more information: <a href="https://www.twistlock.com/">https://www.twistlock.com/</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Aqua Sec</h1>
                </header>
            
            <article>
                
<p>Aqua Sec is another security tool that provides a variety of features. Image scanning with popular registries, policy enforcement, user access control, and container hardening are all covered. Additionally, Aqua Sec has some interesting functionality in network segmentation.</p>
<p>Please refer to the Aqua's website for more information: <a href="https://www.aquasec.com/" target="_blank">https://www.aquasec.com/</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Mesosphere (Kubernetes on Mesos)</h1>
                </header>
            
            <article>
                
<p>Mesosphere itself is building a commercially supported product around the open source Apache Mesos project. Apache Mesos is a cluster management system that offers scheduling and resource sharing, a bit like Kubernetes itself, but at a much higher level. The open source project is used by several well-known companies, such as Twitter and Airbnb.</p>
<p>You can find out more information about the Mesos OS project and the Mesosphere offerings at the following sites:</p>
<ul>
<li><a href="http://mesos.apache.org/" target="_blank">http://mesos.apache.org/<br/></a></li>
<li><a href="https://mesosphere.com/" target="_blank">https://mesosphere.com/</a></li>
</ul>
<p>Mesos, by its nature, is modular, and allows the use of different frameworks for a variety of platforms. A Kubernetes framework is now available, so we can take advantage of the cluster management in Mesos while still maintaining the useful application-level abstractions in K8s. Refer to the following link for more information: <a href="https://github.com/kubernetes-incubator/kube-mesos-framework" target="_blank">https://github.com/kubernetes-incubator/kube-mesos-framework</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deis</h1>
                </header>
            
            <article>
                
<p>The Deis project provides an open source <strong>Platform as a Service</strong> (<strong>PaaS</strong>) solution based on and around Kubernetes. This allows companies to deploy their own PaaS on-premise or on the public cloud. Deis provides tools for application composition and deployment, package management (at the pod level), and service brokering.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">OpenShift</h1>
                </header>
            
            <article>
                
<p>Another PaaS solution is OpenShift from Red Hat. The OpenShift platform uses the Red Hat Atomic platform as a secure and slim OS for running containers. In version 3, Kubernetes was added as the orchestration layer for all container operations on your PaaS. This is a great combination for managing PaaS installations at a large scale.</p>
<p>More information on OpenShift can be found at <a href="https://enterprise.openshift.com/" target="_blank">https://enterprise.openshift.com/.</a></p>
<p class="mce-root"/>
<p><a href="https://enterprise.openshift.com/" target="_blank"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we left a few breadcrumbs to guide you on your continuing journey with Kubernetes. You should have a solid set of production characteristics to get you started. There is a wide community in both the Docker and Kubernetes worlds. There are also a few additional resources that we provided if you need a friendly face along the way.</p>
<p>By now, you have seen the full spectrum of container operations with Kubernetes. You should be more confident in how Kubernetes can streamline the management of your container deployments and how you can plan to move containers off developer laptops and onto production servers. Now get out there and start shipping your containers!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ul>
<li>What are some of the key characteristics of production systems?</li>
<li>What are two examples of third-party monitoring systems?</li>
<li>Which tools can help you build Infrastructure as Code?</li>
<li>What is RBAC?</li>
<li>What limits can you set in a Kubernetes cluster?</li>
<li>In which dimensions can constraints be set?</li>
<li>Which technology should be used to secure communication within a cluster?</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>The Kubernetes project is an open source effort, so there is a broad community of contributors and enthusiasts. One great resource in order to find more assistance is the Kubernetes Slack channel: <a href="http://slack.kubernetes.io/">http://slack.kubernetes.io/</a>.</p>
<p>There is also a Kubernetes group on Google groups. You can join it at <a href="https://groups.google.com/forum/#!forum/kubernetes-users" target="_blank">https://groups.google.com/forum/#!forum/kubernetes-users</a>.</p>
<p>If you enjoyed this book, you can find more of my articles, how-tos, and various musings on my blogs and Twitter page:</p>
<ul>
<li><a href="https://medium.com/@grizzbaier" target="_blank">https://medium.com/@grizzbaier</a></li>
<li><a href="https://twitter.com/grizzbaier" target="_blank">https://twitter.com/grizzbaier</a></li>
</ul>


            </article>

            
        </section>
    </div>



  </body></html>