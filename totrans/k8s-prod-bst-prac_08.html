<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer043">
			<h1 id="_idParaDest-143"><em class="italic"><a id="_idTextAnchor177"/>Chapter 8</em>: Deploying Seamless and Reliable Applications</h1>
			<p>In previous chapters, we learned how to prepare our platform and infrastructure components for production usage. We also learned Kubernetes data management considerations and storage best practices to deploy our first stateful application using the Operator Framework. One of the most underestimated topics in container orchestration is container image management. Although developing applications in Kubernetes is out of the scope of this book, we need to understand the critical components of our images. There are multiple sources, public container registries, and vendors where we can find ready-to-consume application images. Mishandling container images can not only cause overutilization of our cluster resources but, more importantly, can also impact the reliability and security of our services.</p>
			<p>In this chapter, we will discuss topics such as containers and image management. We will learn about the technical challenges when selecting or creating our application images that affect the Kubernetes cluster's stability and security. We will focus on application rollout best practices when deploying and creating our production services before hosting on our cluster to avoid creating instability or misuse of the cluster. This will help us to get the full benefits of using Kubernetes to orchestrate our services securely.</p>
			<p>In this chapter, we're going to cover the following main topics: </p>
			<ul>
				<li>Understanding the challenges with container images</li>
				<li>Learning application deployment strategies</li>
				<li>Scaling applications and achieving higher availability</li>
			</ul>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor178"/>Technical requirements</h1>
			<p>You should have the following tools installed from previous chapters:</p>
			<ul>
				<li><strong class="source-inline">kubectl</strong></li>
				<li><strong class="source-inline">metrics-server</strong></li>
			</ul>
			<p>You need to have an up-and-running Kubernetes cluster as per the instructions in <a href="B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 3</em></a>, <em class="italic">Provisioning Kubernetes Clusters Using AWS and Terraform</em>.</p>
			<p>The code for this chapter is located at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter08">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter08</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/3rpWeRN">https://bit.ly/3rpWeRN</a></p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor179"/>Understanding the challenges with container images</h1>
			<p>In <a id="_idIndexMarker522"/>this section, we will learn about the considerations and best practices followed by industry experts when building or selecting the right container images. Before we discuss the challenges and get into our options, let's learn what goes into a container image.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor180"/>Exploring the components of container images</h2>
			<p>To <a id="_idIndexMarker523"/>understand the behavior of a container image, we need to have basic <a id="_idIndexMarker524"/>knowledge of the <strong class="bold">Operating System</strong> (<strong class="bold">OS</strong>) and hierarchical protection domains. For security segregation purposes, the OS handles virtual memory in two layers <a id="_idIndexMarker525"/>called <strong class="bold">kernel space</strong> and <strong class="bold">user space</strong>. Basically, the kernel runs in the most privileged<a id="_idIndexMarker526"/> protection ring, called <strong class="bold">Ring 0</strong>, and interacts directly with critical resources such as CPU and <a id="_idIndexMarker527"/>memory. The kernel needs to be stable since any problem or instability would cause instability in the overall system and bring everything to a panic state. As we can see in <em class="italic">Figure 8.1</em>, drivers, low-level system components, and all user applications run in the least privileged protection rings and in user space:</p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="Images/B16192_08_001.jpg" alt="Figure 8.1 – Privilege rings, also called hierarchical protection domains&#13;&#10;" width="520" height="535"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Privilege rings, also called hierarchical protection domains</p>
			<p>To learn about the user space, please check <a id="_idIndexMarker528"/>out the detailed explanation here: <a href="https://debian-handbook.info/browse/stable/sect.user-space.html">https://debian-handbook.info/browse/stable/sect.user-space.html</a>.</p>
			<p>Linux <a id="_idIndexMarker529"/>containers take the segregation of security one step further and allow us to manage application and OS <a id="_idIndexMarker530"/>dependencies separately in what is called<a id="_idIndexMarker531"/> the <strong class="bold">container host</strong> and <strong class="bold">container image</strong>. </p>
			<p>The container host is where the OS runs along <a id="_idIndexMarker532"/>with the <strong class="bold">container runtime</strong> (some<a id="_idIndexMarker533"/> of<a id="_idIndexMarker534"/> the <a id="_idIndexMarker535"/>popular container <a id="_idIndexMarker536"/>runtimes include <strong class="bold">containerd</strong>, <strong class="bold">CRI-O</strong>, <strong class="bold">Firecracker</strong>, and <strong class="bold">Kata</strong>) and <strong class="bold">container engine</strong> (some of the popular container engines include <a id="_idIndexMarker537"/>Docker and <a id="_idIndexMarker538"/>the <strong class="bold">Linux Container Daemon</strong> (<strong class="bold">LXD</strong>)). In this book, we will not discuss the differences between container runtimes and engines, since most of the time they are part of the platform, which is outside of our scope. In traditional monolithic architectures, we run applications on top of the OS along with OS dependencies and other applications, whereas in cloud-native microservices architectures, we run applications and their dependencies inside a container image (see <em class="italic">Figure 8.2</em>):</p>
			<p class="figure-caption">  </p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="Images/B16192_08_002.jpg" alt="Figure 8.2 – Comparison of monolithic and microservices architecture" width="890" height="718"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Comparison of monolithic and microservices architecture </p>
			<p>When we <a id="_idIndexMarker539"/>run an application in Kubernetes, such as NGINX, Cassandra, Kafka, MongoDB, and so on, our container engine pulls the container image from its container registry to the local registry, then it wraps one or more containers into an <a id="_idIndexMarker540"/>object called a <strong class="bold">pod</strong> and <strong class="bold">schedules</strong> it on<a id="_idIndexMarker541"/> an available worker node. </p>
			<p>The container image (most of the time, this term is misused instead of <em class="italic">base image</em>) used in this process is a layered image consisting of the user application and the container base image. </p>
			<p>The container base image contains the interchangeable user space components of the OS. The container image is packaged following the Docker image or <strong class="bold">Open Container Initiative</strong> (<strong class="bold">OCI</strong>) industry<a id="_idIndexMarker542"/> standards. This is where our options and challenges come in. Most container base images contain a root filesystem with the minimal user space applications of an OS distribution, some other external libraries, utilities, and files. Container images are typically used for software development and provide a functional application written in common programming languages. Programming languages, including both compiled and interpreted ones, depend on external drivers and libraries. These dependencies make the container base image selection critically important. </p>
			<p>Before we build our application or run an application based on an existing image in production, we need to understand the critical differences between the popular container base images. Now that you've learned what goes into container images, let's learn the differences between the common container base images and some of the best practices for choosing the right image type.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor181"/>Choosing the right container base image</h2>
			<p>Choosing a container base<a id="_idIndexMarker543"/> image is not much different than choosing your container hosts' Linux distribution. Similar criteria such as security, performance, dependencies, core utilities, package managers, the size of its community and ecosystem, and the security response and support must be considered.</p>
			<p>I would like to highlight the five notable container image challenges that we will try to address in this chapter:</p>
			<ul>
				<li><strong class="bold">Image size</strong>: One of the important benefits of container images is portability. A smaller container image size reduces the build and rollout times since pulling the image itself will be faster. Smaller images are achieved by limiting extra binaries, which also bring a minimized attack surface and increased security benefits.</li>
				<li><strong class="bold">Stability</strong>: Updating base images is not fun, but updating every container image is the worst. Container images that only include your application and its runtime dependencies, such as distroless images, may sound attractive. Still, when it comes to <a id="_idIndexMarker544"/>patching <strong class="bold">Common Vulnerabilities and Exposures</strong> (<strong class="bold">CVEs</strong>), you will need to update all your containers that can introduce stability issues.<p class="callout-heading">Important note</p><p class="callout">Distroless images are container images that don't contain package managers or any other application. You can read more about distroless Docker images<a id="_idIndexMarker545"/> and watch a presentation here: <a href="https://github.com/GoogleContainerTools/distroless">https://github.com/GoogleContainerTools/distroless</a>.</p></li>
				<li><strong class="bold">Security</strong>: Every binary that is added to our container images adds unpredictable risks to the overall platform security. When choosing base images, their update frequency, ecosystem and community size, and vulnerability tracking methods such as a CVE database and <strong class="bold">Open Vulnerability and Assessment Language</strong> (<strong class="bold">OVAL</strong>) data are<a id="_idIndexMarker546"/> important <a id="_idIndexMarker547"/>factors to consider. Check the properties of executables such as the <strong class="bold">Position Independent Executable</strong> (<strong class="bold">PIE</strong>), <strong class="bold">Relocation Read-Only</strong> (<strong class="bold">RELRO</strong>), <strong class="bold">Patches for the Linux Kernel</strong> (<strong class="bold">PaX</strong>), canaries, <strong class="bold">Address Space Layout Randomization</strong> (<strong class="bold">ASLR</strong>), FORTIFY_SOURCE, and <a id="_idIndexMarker548"/>the RPATH <a id="_idIndexMarker549"/>and<a id="_idIndexMarker550"/> RUNPATH <a id="_idIndexMarker551"/>runtime search paths.<p class="callout-heading">Important note</p><p class="callout">You can find the Bash script to check the properties of the binary hardening tools at <a href="https://github.com/slimm609/checksec.sh">https://github.com/slimm609/checksec.sh</a>.</p></li>
				<li><strong class="bold">Speed/performance</strong>: Popular container base images may not always be the fastest. Although Alpine is famous for its size and is recommended in some cases, it may cause serious build performance issues. Alpine might be acceptable if you are using the Go language. If you are using Python instead, you will quickly notice that Alpine images will sometimes get two to three times larger, are more than 10 times slower to build, and might even cause build problems. <p class="callout-heading">Important note</p><p class="callout">You can find the Kubernetes-related performance test tools<a id="_idIndexMarker552"/> here: <a href="https://github.com/kubernetes/perf-tests">https://github.com/kubernetes/perf-tests</a>.</p></li>
				<li><strong class="bold">Dependencies</strong>: The C library used in the container image should not be underestimated. While most base images use <strong class="source-inline">glibc</strong>, Alpine includes <strong class="source-inline">muslc</strong> and can show implementation differences. Also, utilities included in the image for troubleshooting and support need to be considered.</li>
			</ul>
			<p>The following are some of the common container base image options compared by their size, security, <a id="_idTextAnchor182"/>and support options:</p>
			<ul>
				<li><strong class="bold">Alpine (alpine:3.12)</strong>: A very <a id="_idIndexMarker553"/>popular lightweight container base image mainly used to reduce image size. Technically, it is <strong class="source-inline">busybox</strong> with a package manager. <strong class="source-inline">glibc</strong>/<strong class="source-inline">musl</strong> library differences are known to cause problems and performance issues that are hard to track down:<p>- <strong class="bold">Size</strong>: 2.6 MB.</p><p>- <strong class="bold">Security</strong>: Community-updated; Alpine Linux bug tracker available at <a href="https://bugs.alpinelinux.org/projects/alpine/issues">https://bugs.alpinelinux.org/projects/alpine/issues</a>.</p><p>- <strong class="bold">Support</strong>: Support via<a id="_idIndexMarker554"/> community. 386, AMD64, ARMv6, ARMv7, ARM64v8, ppc64le, and S390x architectu<a id="_idTextAnchor183"/>res supported.</p></li>
				<li><strong class="bold">Amazon Linux 2 (amazonlinux:2)</strong>: A Linux <a id="_idIndexMarker555"/>image maintained by <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) to be used<a id="_idIndexMarker556"/> on Amazon EC2 instances. It is binary-compatible with RHEL and CentOS:<p>- <strong class="bold">Size</strong>: 59.14 MB.</p><p>- <strong class="bold">Security</strong>: Vendor-updated; Amazon Linux Security Center available at <a href="https://alas.aws.amazon.com/alas2.html">https://alas.aws.amazon.com/alas2.html</a>.</p><p>- <strong class="bold">Support</strong>: LTS support included with AWS EC2; AMD64 and ARM64v8 <a id="_idTextAnchor184"/>architectures supported.</p></li>
				<li><strong class="bold">CentOS (centos:8)</strong>: Community-driven<a id="_idIndexMarker557"/> container base image of the popular Linux distribution. Due to the rollout of CentOS Stream, its future is unknown. At this point, it is better to wait for the replacement Rocky Linux base images or use Amazon Linux 2:<p>- <strong class="bold">Size</strong>: 71.7 MB.</p><p>- <strong class="bold">Security</strong>: Community-updated; CentOS security alerts can be found here: <a href="https://lwn.net/Alerts/CentOS/">https://lwn.net/Alerts/CentOS/</a>.</p><p>- <strong class="bold">Support</strong>: Support via community only. AMD64, ARM64v8, and ppc64le <a id="_idTextAnchor185"/>architectures supported.</p></li>
				<li><strong class="bold">Debian (debian:buster-slim)</strong>: A large<a id="_idIndexMarker558"/> community-driven container base image of the popular Debian Linux distribution. Debian is preferred over Alpine due to a more compatible C library (<strong class="source-inline">libc</strong>) included in Debian images: <p>- <strong class="bold">Size</strong>: 26.47 MB </p><p>- <strong class="bold">Security</strong>: Community-updated; Security Bug Tracker (<a href="https://security-tracker.debian.org/tracker/">https://security-tracker.debian.org/tracker/</a>) and OVAL at <a href="https://www.debian.org/security/oval/">https://www.debian.org/security/oval/</a></p><p>- <strong class="bold">Support</strong>: Support via community only. 386, AMD64, and ARM64v<a id="_idTextAnchor186"/>5 architectures supported</p></li>
				<li><strong class="bold">Ubuntu (ubuntu:21.04)</strong>: A Debian-based<a id="_idIndexMarker559"/> larger community and enterprise-supported Linux <a id="_idIndexMarker560"/>distribution base image: <p>- <strong class="bold">Size</strong>: 29.94 MB </p><p>- <strong class="bold">Security</strong>: Ubuntu CVE Tracker at <a href="https://people.canonical.com/~ubuntu-security/cve/">https://people.canonical.com/~ubuntu-security/cve/</a> and cloud image bug tracker at <a href="https://bugs.launchpad.net/cloud-images">https://bugs.launchpad.net/cloud-images</a> </p><p>- <strong class="bold">Support</strong>: Community and commercial support. AMD64, ARMv7, and ARM64v8 architectures supported</p></li>
				<li><strong class="bold">Red Hat Universal Base Image (UBI) (registry.redhat.io/ubi8/ubi-minimal:8.3)</strong>: A <strong class="bold">Red Hat Enterprise Linux</strong> (<strong class="bold">RHEL</strong>)-based stripped-down image that uses <strong class="source-inline">microdnf</strong> as a package<a id="_idIndexMarker561"/> manager. It is preferred when running <a id="_idIndexMarker562"/>applications on the <strong class="bold">Red Hat OpenShift</strong> platform. Red Hat UBI provides three base images, minimal (<strong class="source-inline">ubi-minimal</strong>), standard (<strong class="source-inline">ubi</strong>), and multi-service (<strong class="source-inline">ubi-init</strong>), for different use cases:<p>- <strong class="bold">Size</strong>: 37.6 MB.</p><p>- <strong class="bold">Security</strong>: The best container base image in terms of completeness of vulnerability checks. Errata provided at <a href="https://access.redhat.com/errata">https://access.redhat.com/errata</a> and OVAL data provided at <a href="https://www.redhat.com/security/data/oval/">https://www.redhat.com/security/data/oval/</a>.</p><p>- <strong class="bold">Support</strong>: Community and commercial support. AMD64, ARM64v8, ppc64le, and S390x architectures supported.</p></li>
				<li><strong class="bold">Distroless (gcr.io/distroless/base-debian10)</strong>: Builds<a id="_idIndexMarker563"/> on the Debian distribution by Google. They don't contain package managers or shells. Preferred for security and size. Additional builds can be found at <a href="https://console.cloud.google.com/gcr/images/distroless/GLOBAL">https://console.cloud.google.com/gcr/images/distroless/GLOBAL</a>:<p>- <strong class="bold">Size</strong>: 75.1 MB</p><p>- <strong class="bold">Security</strong>: Avoids image vulnerabilities, but introduces another challenge where dependent library updates need to be carefully tracked for every container image</p><p>- <strong class="bold">Support</strong>: Support via <a id="_idIndexMarker564"/>community only. AMD64, ARM, ARM64, ppc64le, and S390x architectures supported</p></li>
			</ul>
			<p>Now you have learned about the challenges we deal with when choosing the right container base image and how the most common popular base images compare. Let's find out some of the best practices for reducing your final image size and scanning your container images for vulnerabilities. </p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor187"/>Reducing container image size</h2>
			<p>An excellent way to<a id="_idIndexMarker565"/> achieve smaller container images would be by starting with small base images such as Alpine, <strong class="source-inline">ubi-minimal</strong>, or distroless base images. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">For reproducible builds and deployment, you can also use the Nix package manager and create slim builds. There is a lot of enthusiasm around Nix, but since there is a steep learning curve and custom expression language is involved, we will not discuss Nix in this book. You can learn about building container images<a id="_idIndexMarker566"/> using Nix here at the official NixOS documentation page: <a href="https://nixos.org/guides/building-and-running-docker-images.html">https://nixos.org/guides/building-and-running-docker-images.html</a>. </p>
			<p>Excluding some of the<a id="_idIndexMarker567"/> unnecessary files, using a <strong class="source-inline">.dockerignore</strong> file can help us to reduce our image size. Here is an example of a <strong class="source-inline">.dockerignore</strong> file:</p>
			<p class="source-code"># ignoring git folder</p>
			<p class="source-code">.git</p>
			<p class="source-code">#ignoring visual studio code related temp data</p>
			<p class="source-code">.vs</p>
			<p class="source-code">.vscode</p>
			<p class="source-code"># other files and CI manifests</p>
			<p class="source-code">.DS_Store</p>
			<p class="source-code">.dockerignore</p>
			<p class="source-code">.editorconfig</p>
			<p class="source-code">.gitignore</p>
			<p class="source-code">.gitlab-ci.yml</p>
			<p class="source-code">.travis.yml</p>
			<p class="source-code"># ignore all files and directories starting with temp </p>
			<p class="source-code"># in any subdirectory </p>
			<p class="source-code">*/temp*</p>
			<p class="source-code"># ignore all files and directories starting with temp</p>
			<p class="source-code"># in any subdirectory two levels below root</p>
			<p class="source-code">*/*/temp*</p>
			<p class="source-code"># ignore all files and directories starting with temp</p>
			<p class="source-code"># followed by any character </p>
			<p class="source-code">temp? </p>
			<p>Size-optimized images can be achieved by utilizing multistage builds and avoiding extra layers. Multistage builds add a couple of new syntaxes and allow us to use a <strong class="source-inline">FROM</strong> section in our Dockerfile multiple times to start a new stage of the build and copy only the artifacts we want to take from previous stages. You can learn more about the multistage build<a id="_idIndexMarker568"/> on the official Docker documentation website at <a href="https://docs.docker.com/develop/develop-images/multistage-build/">https://docs.docker.com/develop/develop-images/multistage-build/</a>.</p>
			<p>Here is an example of a <a id="_idIndexMarker569"/>Dockerfile with two stages:</p>
			<p class="source-code">FROM node:14.15 AS base</p>
			<p class="source-code">ADD . /app</p>
			<p class="source-code">WORKDIR /app</p>
			<p class="source-code">RUN npm ins<a id="_idTextAnchor188"/>tall</p>
			<p class="source-code">FROM gcr.io/distroless/nodejs AS stage2</p>
			<p class="source-code">COPY --from=base /app /app</p>
			<p class="source-code">WORKDIR /app</p>
			<p class="source-code">EXPOSE 8080</p>
			<p class="source-code">CMD ["server.js"]</p>
			<p>In our preceding example, the first stage, <strong class="source-inline">base</strong>, starts with the <strong class="source-inline">node:14.15</strong> Node.js base image. We copy our application code to the <strong class="source-inline">/app</strong> directory and execute the <strong class="source-inline">npm install</strong> command. </p>
			<p>We move to the second stage, called <strong class="source-inline">stage2</strong>, this time using a <strong class="source-inline">distroless/nodejs</strong> base image. Then, we copy our application code and our <strong class="source-inline">node_modules</strong> from the first stage using the <strong class="source-inline">COPY --from=base /app /app</strong> syntax. This way, we are reducing our container image size as well as the attack surface since distroless images do not contain <strong class="source-inline">bash</strong> or other tools that can be maliciously executed. </p>
			<p>You can read about the best practices for writing Dockerfiles<a id="_idIndexMarker570"/> at <a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/">https://docs.docker.com/develop/develop-images/dockerfile_best-practices/</a>.</p>
			<p>Now we have learned a few techniques for reducing our container image size. Let's look at how we can proactively scan our images against security vulnerabilities and patch them in a timely manner before running them in production.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor189"/>Scanning container images for vulnerabilities</h2>
			<p>We've built our <a id="_idIndexMarker571"/>container images or pulled some of the<a id="_idIndexMarker572"/> vendor-provided images to our local registry and now we are ready to run in our production environment. How do we know they are safe to run? How do we know they have the latest security vulnerabilities patched? Most <strong class="bold">Continuous Integration and</strong> <strong class="bold">Continuous Delivery</strong> (<strong class="bold">CI/CD</strong>) solutions<a id="_idIndexMarker573"/> today have additional security scanning tools. It is one of the golden rules not to roll out any service into production before going through a quick image validation during our pipeline. For this<a id="_idIndexMarker574"/> purpose, we will now learn about a popular open source solution called <strong class="bold">Trivy</strong>. </p>
			<p>Trivy is a<a id="_idIndexMarker575"/> comprehensive vulnerability scanner for container images. Trivy is capable of detecting vulnerabilities in most images based on popular base images, including Alpine, CentOS, and Red Hat UBI, and application package dependencies such as <strong class="source-inline">npm</strong>, <strong class="source-inline">yarn</strong>, <strong class="source-inline">bundler</strong>, and <strong class="source-inline">composer</strong>.</p>
			<p>Here, we will manually install the <strong class="source-inline">trivy</strong> binaries and run a vulnerability analysis:</p>
			<ol>
				<li>Let's get the latest release version tag of <strong class="source-inline">trivy</strong> and keep it in a variable called <strong class="source-inline">TRIVYVERSION</strong>:<p class="source-code"><strong class="bold">$ TRIVYVERSION=$(curl –silent "https://api.github.com/repos/aquasecurity/trivy/releases/latest" | grep '"tag_name":' | \</strong></p><p class="source-code"><strong class="bold">sed -E 's/.*"v([^"]+)".*/\1/')</strong></p></li>
				<li>Now, download<a id="_idIndexMarker576"/> the latest <strong class="source-inline">trivy</strong> binary <a id="_idIndexMarker577"/>and install it:<p class="source-code"><strong class="bold">$ curl --silent --location "https://github.com/aquasecurity/trivy/releases/download/v${TRIVYVERSION}/trivy_${TRIVYVERSION}_Linux-64bit.tar.gz" | tar xz -C /tmp</strong></p><p class="source-code"><strong class="bold">$ sudo mv /tmp/trivy /usr/local/bin</strong></p></li>
				<li>Confirm that the installation is successfully completed by executing the following command:<p class="source-code"><strong class="bold">$ trivy --version</strong></p><p class="source-code"><strong class="bold">Version: 0.14.0</strong></p></li>
				<li>Run <strong class="source-inline">trivy</strong> checks with a target image location and its tag. In our example, we scanned the <strong class="source-inline">alpine:3.12</strong> base image from its official Docker Hub repository:<p class="source-code"><strong class="bold">$ trivy alpine:3.12</strong></p><p>The output of the preceding command should look as follows since no issues are found in the particular container image:</p><div id="_idContainer040" class="IMG---Figure"><img src="Images/B16192_08_003.jpg" alt="Figure 8.3 – Trivy results of a container image with no known vulnerabilities&#13;&#10;" width="590" height="143"/></div><p class="figure-caption">Figure 8.3 – Trivy results of a container image with no known vulnerabilities</p></li>
				<li>Now, let's<a id="_idIndexMarker578"/> scan a publicly available version of the popular MongoDB database container image. MongoDB is used by many modern cloud-native applications and services:<p class="source-code"><strong class="bold">$ trivy mongo:4.4</strong></p></li>
				<li>You will notice that Trivy<a id="_idIndexMarker579"/> returned <strong class="source-inline">93</strong> known vulnerabilities, including <strong class="source-inline">2</strong> high and <strong class="source-inline">28</strong> medium severity issues:</li>
			</ol>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="Images/B16192_08_004.jpg" alt="Figure 8.4 – Trivy results showing vulnerabilities&#13;&#10;" width="922" height="307"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – Trivy results showing vulnerabilities</p>
			<p>In the long analysis returned by the Trivy scanner, you can find vulnerability IDs and severity URLs to learn more about the issues. You can also see that some issues come from the Ubuntu 18.04 base image used in the container image and can be resolved by just updating the base image of the container.</p>
			<p>Trivy supports most CI tools, including Travis CI, CircleCI, Jenkins, and GitLab CI. To learn more about Trivy<a id="_idIndexMarker580"/> and integration details, you can read the official documentation at <a href="https://github.com/aquasecurity/trivy">https://github.com/aquasecurity/trivy</a>. </p>
			<p>Now we have<a id="_idIndexMarker581"/> learned how to test container images against<a id="_idIndexMarker582"/> known vulnerabilities. It is highly recommended to have test conditions in your build pipelines. Let's look into how we can test the impact of container image downloads from public repositories.  </p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor190"/>Testing the download speed of a container image</h2>
			<p>CI is a key <a id="_idIndexMarker583"/>component of automation, and the reduction of every second spent in the pipeline execution will be important. Download time also impacts the speed of the new container image rollout to the production environment. Therefore, we need to consider the download speeds of the container images used.</p>
			<p>Here, we will use the <strong class="source-inline">time</strong> command in Linux to execute <strong class="source-inline">docker run</strong> in a specified container base image and compare the summary of the real-time user CPU time and system CPU time spent during the process:</p>
			<ol>
				<li value="1">Install the <strong class="source-inline">curl</strong> utility in the <strong class="source-inline">debian:buster-slim</strong> Debian base image:<p class="source-code"><strong class="bold">$ time docker run --rm debian:buster-slim sh -c "apt-get update &amp;&amp; apt-get install curl -y"</strong></p><p class="source-code"><strong class="bold">real    0m43.837s</strong></p><p class="source-code"><strong class="bold">user    0m0.024s</strong></p><p class="source-code"><strong class="bold">sys     0m0.043s</strong></p></li>
				<li>For comparison, let's now run the same command in the <strong class="source-inline">alpine:3.12</strong> image: <p class="source-code"><strong class="bold">$ time docker run --rm alpine:3.12 sh -c "apk update &amp;&amp; apk add --update curl"</strong></p><p class="source-code"><strong class="bold">real    0m2.644s</strong></p><p class="source-code"><strong class="bold">user    0m0.034s</strong></p><p class="source-code"><strong class="bold">sys     0m0.021s</strong></p></li>
			</ol>
			<p>Note that both images were not available in the local registry and were pulled for the first time from the public Docker Hub location. As you can see, the Alpine image completed the task in close to 2 seconds, whereas the same request took more than 40 seconds longer to finish using the Debian image. </p>
			<p>Now we have learned about measuring the command execution speed in containers based on different base <a id="_idIndexMarker584"/>images. Let's summarize everything we have learned in this section into a short list of simple container image best practices.</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor191"/>Applying container base images best practices</h2>
			<p>Technically, most applications <a id="_idIndexMarker585"/>will run in containers layered on top of all the common and popular container base images. This may be acceptable for development and test purposes, but before rolling out any container images into production, there are a few common-sense best practices we should consider: </p>
			<ul>
				<li>The size of the container image is important as long as the container base image does not introduce a performance tax and vulnerabilities. Using a stable, compatible, and supported base image is preferred over saving a few megabytes.</li>
				<li>Never use the <strong class="source-inline">latest</strong> tag to pull base images when building your container images. </li>
				<li>Make sure to use container images with the exact tested and validated version of the image. You can also specify its digest by replacing <strong class="source-inline">&lt;image-name&gt;:&lt;tag&gt;</strong> with <strong class="source-inline">&lt;image-name&gt;@&lt;digest&gt;</strong> to generate stable reproducible builds.</li>
				<li>Check <strong class="source-inline">imagePullPolicy</strong> in your application manifests. Unless required otherwise, it is suggested to use <strong class="source-inline">IfNotPresent</strong>.</li>
				<li>When possible, use the same base OS in your container host and container images.</li>
				<li>Integrate image vulnerability scanners into your CI pipelines and make sure to clear at least high and critical severity vulnerabilities before rolling your images into production.</li>
				<li>Monitor container image size changes over time and notify maintainers of sudden large size changes.</li>
				<li>When using public container registries, store your container images in multiple registries. Some public registries include Docker Hub, GitLab Container Registry, Red Hat Quay, Amazon ECR, Azure Container Registry, and Google Cloud Container Registry.</li>
				<li>For increased security, use a<a id="_idIndexMarker586"/> private container registry and monitor public container registry pulls into the production environment. </li>
			</ul>
			<p>Now we have learned about the challenges of choosing container images and production best practices. Let's look at different deployment strategies and their use cases.</p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor192"/>Learning application deployment strategies</h1>
			<p>Organizations without<a id="_idIndexMarker587"/> the expertise to design an application deployment strategy before getting their services to production users can face great operational complexity when managing their application life cycle. Many users still face container and microservices adoption issues later in their digital transformation journey and end up going back to the more expensive <strong class="bold">Database as a Service</strong> (<strong class="bold">DbaaS</strong>) model <a id="_idIndexMarker588"/>or even using traditional deployment methods in VMs. To avoid common mistakes and production anti-patterns, we need to be aware of some of the common strategies that will ensure our success in deploying and managing applications on Kubernetes. </p>
			<p>We learned about the differences between different Kubernetes controllers such as Deployments, ReplicaSets, and StatefulSets in the <em class="italic">Deploying stateful applications</em> section in <a href="B16192_07_Final_PG_ePub.xhtml#_idTextAnchor157"><em class="italic">Chapter 7</em></a>, <em class="italic">Managing Storage and Stateful Applications</em>.</p>
			<p>In this section, we will learn about the following containerized application deployment best practices:</p>
			<ul>
				<li>Choosing the deployment model</li>
				<li>Monitoring deployments</li>
				<li>Using readiness <a id="_idIndexMarker589"/>and liveness probes</li>
			</ul>
			<p>Let's discuss each of them in the following sections.</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor193"/>Choosing the deployment model</h2>
			<p>In Kubernetes, applications <a id="_idIndexMarker590"/>can be rolled out following various deployment procedures. Choosing the right strategy is not always easy since it really depends on your services and how your applications are accessed by users. Now, we will review the most common models:</p>
			<ul>
				<li>A/B testing</li>
				<li>Blue/green</li>
				<li>Canary release</li>
				<li>Clean deployment</li>
				<li>Incremental deployment</li>
			</ul>
			<p>Let's learn about the advantages of each of them in the following sections.</p>
			<h3>A/B testing</h3>
			<p>A/B testing deployments<a id="_idIndexMarker591"/> allow routing groups of users to a new deployment based on conditions such as HTTP headers, location, browser cookies, or other user metadata. A/B testing deployments are preferred when a specific feature of the application needs to be tested on a certain group of users and rollout needs to continue based on the conversation rate. Price and UX testing are also done using A/B testing. Other than the complexity of the parameters that need to be managed, it is the most flexible model with low cloud cost, minimum impact on users, and quick rollback times.</p>
			<h3>Blue/green</h3>
			<p>In the blue/green deployment model, an equal <a id="_idIndexMarker592"/>amount of instances of each application is deployed on your cluster. This model can be executed either by traffic switching or by traffic mirroring when a service mesh such as Istio is used. It is preferred when service changes need to be tested for load and compliance with no impact on actual users. When the metrics return successful data, a new deployment (green) gets promoted. This model cannot be used to target a specific group of users and can be expensive in terms of cloud resource consumption cost due to its full deployment model.</p>
			<h3>Canary release</h3>
			<p>Canary deployments <a id="_idIndexMarker593"/>gradually shift traffic from one deployment to another based on percentage, sometimes triggered by metrics such as success rate or health. Canary releases are preferred when confidence in the new releases is not high or when deploying releases on a completely new platform. Groups of users cannot be targeted. This method doesn't increase public cloud costs and rollback times can be rather quick. </p>
			<h3>Clean deployment</h3>
			<p>In this method, one version of the<a id="_idIndexMarker594"/> application is destroyed and a new version is deployed. It is preferred in deployment since it is the simplest method, although this method should not be used in production unless the service is not in use. If the deployment fails compared to the other methods, the rollback time would be the highest, and the service downtime would be the longest.</p>
			<h3>Incremental deployment</h3>
			<p>In this method, a new version of the application is deployed in a rolling update fashion and slowly migrated. The only advantage of this model compared to a clean deployment is that incremental deployment does not introduce downtime. </p>
			<p>Some of the methods can only be implemented with the<a id="_idIndexMarker595"/> help of <strong class="bold">service mesh</strong> solutions, such as Istio, Linkerd, or AWS App Mesh, and ingress controllers, including Contour, Gloo, NGINX, or Traefik. </p>
			<p>Orchestration of multiple deployment strategies can turn into a complex configuration puzzle. In this case, the usage of an application delivery operator can be very useful. <strong class="bold">Flagger</strong> is one<a id="_idIndexMarker596"/> of the most complete progressive delivery Kubernetes operators in the Kubernetes ecosystem. Flagger can automate complex rollover scenarios using Istio, Linkerd, App Mesh, NGINX, Skipper, Contour, Gloo, or Traefik based on the metrics analysis from the metrics collected by Prometheus. To learn more about Flagger operators and a tutorial covering the models discussed here, you can read the official <a id="_idIndexMarker597"/>documentation at <a href="https://docs.flagger.app/">https://docs.flagger.app/</a>.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor194"/>Monitoring deployments</h2>
			<p>Smooth, production-ready application<a id="_idIndexMarker598"/> deployment and canary analysis cannot be achieved without monitoring the application usage metrics. We can monitor our applications using tools such as Prometheus, Datadog, or Splunk. </p>
			<p>We will cover monitoring, visualizations, logging, tracing solutions, and how to make visualization dashboards relevant to serve our production needs in <a href="B16192_09_Final_PG_ePub.xhtml#_idTextAnchor200"><em class="italic">Chapter 9</em></a>, <em class="italic">Monitoring, Logging, and Observability</em>. </p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor195"/>Using readiness and liveness container probes</h2>
			<p>When a new pod is scheduled in our<a id="_idIndexMarker599"/> Kubernetes cluster, its phase is represented by the <strong class="source-inline">PodStatus</strong> object. These phases reported as <strong class="source-inline">Pending</strong>, <strong class="source-inline">Running</strong>, <strong class="source-inline">Succeeded</strong>, <strong class="source-inline">Failed</strong>, or <strong class="source-inline">U<a id="_idTextAnchor196"/>nknown</strong> do not represent or guarantee our application's intended function. You can read more about the pod life cycle and its phases on the official Kubernetes documentation site at <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/</a>.</p>
			<p>To monitor our application's real health status inside the container, a regular diagnostic task can be executed. These diagnostic tests performed periodically are<a id="_idIndexMarker600"/> called <strong class="bold">container probes</strong>. <strong class="source-inline">kubelet</strong> can perform three types of container probes, as follows:</p>
			<ul>
				<li><strong class="source-inline">livenessProbe</strong></li>
				<li><strong class="source-inline">readinessProbe</strong></li>
				<li><strong class="source-inline">startupProbe</strong></li>
			</ul>
			<p>It is highly recommended to use at minimum the readiness and liveness probes to control your application's health when starting and periodically after it is scheduled in your Kubernetes cluster. When enabled, <strong class="source-inline">kubelet</strong> can call three different handlers, <strong class="source-inline">ExecAction</strong>, <strong class="source-inline">TCPSocketAction</strong>, and <strong class="source-inline">HTTPGetAction</strong>, inside or against the pod's IP and validate your application's health.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter08/probes/liveness/busybox.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter08/probes/liveness/busybox.yaml</a>.</p>
			<p>In the next code snippet, we will <a id="_idIndexMarker601"/>create a <strong class="source-inline">busybox</strong> pod example that will use <strong class="source-inline">livenessProbe</strong> to execute a command inside the container image to check our pod's liveness. </p>
			<p>Create the template for the <strong class="source-inline">busybox</strong> pod in this <strong class="source-inline">probes/liveness/busybox.yaml</strong> path:</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Pod</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    test: liveness</p>
			<p class="source-code">  name: liveness-execaction</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  containers:</p>
			<p class="source-code">  - name: liveness</p>
			<p class="source-code">    image: k8s.gcr.io/busybox</p>
			<p class="source-code">    args:</p>
			<p class="source-code">    - /bin/sh</p>
			<p class="source-code">    - -c</p>
			<p class="source-code">    - touch /tmp/alive; sleep 30; rm -rf /tmp/alive; sleep 300</p>
			<p class="source-code">    livenessProbe:</p>
			<p class="source-code">      exec:</p>
			<p class="source-code">        command:</p>
			<p class="source-code">        - cat</p>
			<p class="source-code">        - /tmp/alive</p>
			<p class="source-code">      initialDelaySeconds: 10</p>
			<p class="source-code">      periodSeconds: 10</p>
			<p>When the container starts, it executes the command specified under the <strong class="source-inline">args</strong> section. This command first creates a file<a id="_idIndexMarker602"/> under <strong class="source-inline">/tmp/alive</strong>, and then waits 30 seconds and removes it. <strong class="source-inline">livenessProbe</strong>, as specified in the same file, first waits 10 seconds, as defined by the <strong class="source-inline">initialDelaySeconds</strong> parameter, and then periodically, every 10 seconds, as defined by the <strong class="source-inline">periodSeconds</strong> parameter, executes the <strong class="source-inline">cat</strong> <strong class="source-inline">/tmp/alive</strong> command. In the first 30 seconds, our command will be successful and once the file is removed, <strong class="source-inline">livenessProbe</strong> will fail, and our pod will be restarted for losing its liveness state. Make sure you allow enough time for the pod to start by setting a reasonable <strong class="source-inline">initialDelaySeconds</strong> value.</p>
			<p>Similarly, we can add <strong class="source-inline">readinessProbe</strong> by replacing the <strong class="source-inline">livenessProbe</strong> field with <strong class="source-inline">readinessProbe</strong>.</p>
			<p>Now we have learned about the production deployment best practices on Kubernetes. We have also learned about common deployment strategies for rolling production applications and using container probes for verifying the health of our application. Next, we will learn how to scale our applications.</p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor197"/>Scaling applications and achieving higher availability</h1>
			<p>The Kubernetes container<a id="_idIndexMarker603"/> orchestration platform provides a wide range of functionality to <a id="_idIndexMarker604"/>help us deploy our applications in a scalable and highly available way. When designing architecture that will support horizontally scalable services and applications, we need to be aware of some common strategies that will help to successfully scale our applications on Kubernetes clusters. </p>
			<p>In the previous section, <em class="italic">Learning application deployment strategies</em>, we covered some strategies that would help us to scale our applications, including deployment strategies and implementing health checks using container probes. In this section, we will learn about scaling applications <a id="_idIndexMarker605"/>using the <strong class="bold">Horizontal Pod Autoscaler</strong> (<strong class="bold">HPA</strong>).</p>
			<p>When we first deploy our application on Kubernetes clusters, applications will very likely not get accessed immediately<a id="_idIndexMarker606"/> and usage will gradually increase over time. In that case, rolling<a id="_idIndexMarker607"/> out a deployment with many replicas would result in wasting our infrastructure resources. HPA in Kubernetes helps us increase the necessary resources in different scenarios.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter08/hpa/deployment-nginx.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter08/hpa/deployment-nginx.yaml</a>. </p>
			<p>Now, we will learn about configuring a basic HPA based on CPU utilization metrics. You can read more about HPA on the official Kubernetes documentation site at <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/</a>:</p>
			<ol>
				<li value="1">If you haven't installed it before, make sure to<a id="_idIndexMarker608"/> install <strong class="bold">Metrics Server</strong> by executing the following command:<p class="source-code"><strong class="bold">$ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.4.1/components.yaml</strong></p></li>
				<li>Create a deployment named <strong class="source-inline">nginx-hpa</strong> with a <strong class="source-inline">replicas</strong> count of <strong class="source-inline">1</strong> in the <strong class="source-inline">hpa/deployment-nginx.yaml</strong> path. Make sure to have <strong class="source-inline">resources.request.cpu</strong> set, otherwise HPA cannot function. In our example, we used an NGINX deployment. You can instead use any deployment you would like to apply HPA to:<p class="source-code">apiVersion: apps/v1</p><p class="source-code">kind: Deployment</p><p class="source-code">metadata:</p><p class="source-code">  name: nginx-hpa</p><p class="source-code">  namespace: default</p><p class="source-code">spec:</p><p class="source-code">  replicas: 1</p><p class="source-code">  selector:</p><p class="source-code">    matchLabels:</p><p class="source-code">      app: nginx-hpa</p><p class="source-code">  template:</p><p class="source-code">    metadata:</p><p class="source-code">      labels:</p><p class="source-code">        app: nginx-hpa</p><p class="source-code">    spec:</p><p class="source-code">      containers:</p><p class="source-code">      - name: nginx-hpa</p><p class="source-code">        image: nginx:1.19.6</p><p class="source-code">        ports:</p><p class="source-code">        - containerPort: 80</p><p class="source-code">        resources:</p><p class="source-code">          requests:</p><p class="source-code">            cpu: "200m"</p></li>
				<li>Execute the<a id="_idIndexMarker609"/> following command<a id="_idIndexMarker610"/> to create the deployment:<p class="source-code"><strong class="bold">$ kubectl apply -f deployment-nginx.yaml</strong></p></li>
				<li>Confirm that your deployment is successful by checking its state:<p class="source-code"><strong class="bold">$ kubectl get deployments</strong></p><p class="source-code"><strong class="bold">NAME        READY   UP-TO-DATE   AVAILABLE   AGE</strong></p><p class="source-code"><strong class="bold">nginx-hpa   1/1     1            1           11s</strong></p></li>
				<li>Now<a id="_idIndexMarker611"/> create an <a id="_idIndexMarker612"/>HPA named <strong class="source-inline">nginx-autoscale</strong> with a <strong class="source-inline">minReplicas</strong> count of <strong class="source-inline">1</strong>, a <strong class="source-inline">maxReplicas</strong> count of <strong class="source-inline">5</strong>, and <strong class="source-inline">targetCPUUtilizationPercentage</strong> set to <strong class="source-inline">50</strong> in the <strong class="source-inline">hpa/hpa-nginx.yaml</strong> path:<p class="source-code"><strong class="bold">apiVersion: autoscaling/v1</strong></p><p class="source-code"><strong class="bold">kind: HorizontalPodAutoscaler</strong></p><p class="source-code"><strong class="bold">metadata:</strong></p><p class="source-code"><strong class="bold">  name: nginx-autoscale </strong></p><p class="source-code"><strong class="bold">  namespace: default</strong></p><p class="source-code"><strong class="bold">spec:</strong></p><p class="source-code"><strong class="bold">  scaleTargetRef:</strong></p><p class="source-code"><strong class="bold">    apiVersion: apps/v1</strong></p><p class="source-code"><strong class="bold">    kind: Deployment</strong></p><p class="source-code"><strong class="bold">    name: nginx-hpa</strong></p><p class="source-code"><strong class="bold">  minReplicas: 1</strong></p><p class="source-code"><strong class="bold">  maxReplicas: 5</strong></p><p class="source-code"><strong class="bold">  targetCPUUtilizationPercentage: 50</strong></p></li>
				<li>Execute the following command to create the deployment:<p class="source-code"><strong class="bold">$ kubectl apply -f hpa-nginx.yaml</strong></p></li>
				<li>Confirm that our HPA is successfully created:<p class="source-code"><strong class="bold">$ kubectl get hpa</strong></p><p class="source-code"><strong class="bold">NAME              REFERENCE              TARGETS         MINPODS   MAXPODS   REPLICAS   AGE</strong></p><p class="source-code"><strong class="bold">nginx-autoscale   Deployment/nginx-hpa   0%/50%   1         5         0          15s</strong></p></li>
				<li>The output of the preceding command should look as follows:</li>
			</ol>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="Images/B16192_08_005.jpg" alt="Figure 8.5 – HPA monitoring for CPU metrics to scale the application" width="684" height="40"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – HPA monitoring for CPU metrics to scale the application</p>
			<p>In the preceding<a id="_idIndexMarker613"/> example, we used CPU utilization as our metric. HPA can use multiple <a id="_idIndexMarker614"/>metrics, including CPU, memory, and other custom external metrics such as service latency and I/O load, using <strong class="bold">custom metrics adapters</strong>. In addition<a id="_idIndexMarker615"/> to HPA, we can use <strong class="bold">Pod Disruption Budgets</strong> (<strong class="bold">PDBs</strong>) to<a id="_idIndexMarker616"/> avoid voluntary and involuntary disruptions to provide higher availability. You can read more about specifying a PDB for your application at <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">https://kubernetes.io/docs/tasks/run-application/configure-pdb/</a>.</p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor198"/>Summary</h1>
			<p>In this chapter, we explored the components of container images, best practices for creating container images, and choosing the right base image type. We reduced our container image size by removing unnecessary files and using multistage builds. We learned how to scan our container images for vulnerabilities proactively. We learned about application deployment strategies to test and roll out new features and releases of our applications. We created an HPA to scale our applications. All the recommendations and best practices mentioned in this chapter help us reduce the attack surface and increase stability to improve efficiency in our production environment.</p>
			<p>In the next chapter, we will learn about Kubernetes observability and key metrics to monitor in production. We will learn about the tools and stacks to use or build, compare the best tools in the ecosystem, and learn how to deal with observability from a site reliability perspective.</p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor199"/>Further reading</h1>
			<p>You can refer to the following links for more information on the topics covered in this chapter:</p>
			<ul>
				<li><em class="italic">A Practical Introduction to Container Terminology</em>: <a href="https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction/">https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction/</a></li>
				<li>Open Container Initiative: <a href="https://opencontainers.org/">https://opencontainers.org/</a> </li>
				<li><em class="italic">Hardening ELF binaries using Relocation Read-Only (RELRO)</em>: <a href="https://www.redhat.com/en/blog/hardening-elf-binaries-using-relocation-read-only-relro">https://www.redhat.com/en/blog/hardening-elf-binaries-using-relocation-read-only-relro</a> </li>
				<li><em class="italic">A Comparison of Linux Container Images</em>:<em class="italic"> </em><a href="http://crunchtools.com/comparison-linux-container-images/">http://crunchtools.com/comparison-linux-container-images/</a></li>
				<li><em class="italic">Alpine makes Python Docker builds way too (50×) slower, and images double (2×) larger</em>: <a href="https://lih-verma.medium.com/alpine-makes-python-docker-builds-way-too-50-slower-and-images-double-2-larger-61d1d43cbc79">https://lih-verma.medium.com/alpine-makes-python-docker-builds-way-too-50-slower-and-images-double-2-larger-61d1d43cbc79</a> </li>
				<li><em class="italic">Why Elastic moved from Alpine to CentOS base images</em>: <a href="https://www.elastic.co/blog/docker-base-centos7">https://www.elastic.co/blog/docker-base-centos7</a> </li>
				<li><em class="italic">Introducing multi-architecture container images for Amazon ECR</em>: <a href="https://aws.amazon.com/blogs/containers/introducing-multi-architecture-container-images-for-amazon-ecr/">https://aws.amazon.com/blogs/containers/introducing-multi-architecture-container-images-for-amazon-ecr/</a> </li>
				<li>How to use distroless Docker images: <a href="https://github.com/GoogleContainerTools/distroless">https://github.com/GoogleContainerTools/distroless</a></li>
				<li><em class="italic">Best practices for building containers</em>: <a href="https://cloud.google.com/solutions/best-practices-for-building-containers">https://cloud.google.com/solutions/best-practices-for-building-containers</a> </li>
				<li><em class="italic">Automated rollback of Helm releases based on logs or metrics</em>: <a href="https://blog.container-solutions.com/automated-rollback-helm-releases-based-logs-metrics">https://blog.container-solutions.com/automated-rollback-helm-releases-based-logs-metrics</a></li>
				<li><em class="italic">Kubernetes – A Complete DevOps Cookbook</em> (<a href="B16192_07_Final_PG_ePub.xhtml#_idTextAnchor157"><em class="italic">Chapter 7</em></a>, <em class="italic">Scaling and Upgrading Applications</em>): <a href="https://www.packtpub.com/product/kubernetes-a-complete-devops-cookbook/9781838828042">https://www.packtpub.com/product/kubernetes-a-complete-devops-cookbook/9781838828042</a></li>
			</ul>
		</div>
	</div></body></html>