- en: '*Chapter 6*: Machine Learning Engineering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will move the discussion to the model building and model
    management activities of the **machine learning** (**ML**) engineering lifecycle.
    You will learn about the ML platform's role of providing a self-serving solution
    to data scientist so they can work more efficiently and collaborate with data
    teams and fellow data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: The focus of this chapter is not on building models; instead, it is on showing
    how the platform can bring consistency and security across different environments
    and different members of your teams. You will learn how the platform simplifies
    the work of data scientists in terms of preparing and maintaining their data science
    workspaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding ML engineering?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a custom notebook image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing MLflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using MLflow as an experiment tracking system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using MLflow as a model registry system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter includes some hands-on setup and exercises. You will need a running
    Kubernetes cluster configured with **Operator Lifecycle Manager** (**OLM**). Building
    such a Kubernetes environment is covered in [*Chapter 3*](B18332_03_ePub.xhtml#_idTextAnchor040),
    *Exploring Kubernetes*. Before attempting the technical exercises in this chapter,
    please make sure that you have a working Kubernetes cluster and that **Open Data
    Hub** (**ODH**) is installed on your Kubernetes cluster. Installing ODH is covered
    in [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The Anatomy of a Machine
    Learning Platform*. You can find all the code associated with this book at [https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes](https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding ML engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML engineering is the process of applying software engineering principles and
    practices to ML projects. In the context of this book, ML engineering is also
    a discipline that facilitates applying application development practices to the
    data science lifecycle. When you write a traditional application such as a website
    or a banking system, there are processes and tools to assist you in writing high-quality
    code right from the start. Smart IDEs, standard environments, continuous integration,
    automated testing, and static code analysis are just a few examples. Automation
    and continuous deployment practices enable organizations to deploy applications
    many times in a day and with no downtime.
  prefs: []
  type: TYPE_NORMAL
- en: ML engineering is a loose term that brings the benefits of traditional software
    engineering practices to the model development world. However, most data scientists
    are not developers. They may not be familiar with software engineering practices.
    Also, the tools that the data scientists use may not be the right tools to perform
    ML engineering tasks. Having said that, the model is just another piece of software.
    Therefore, we can also apply existing software engineering approaches to ML models.
    Using containers to package and deploy ML models is one such example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some teams may employ ML engineers to supplement the work of data scientists.
    While the data scientist''s primary responsibility is to build ML or deep learning
    models that solve business problems, ML engineers focus more on the software engineering
    facets. Some of the responsibilities of data engineers include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Model optimization (also about making sure that the built model is optimized
    for the target environment where the model will be hosted).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model packaging (making ML models portable, shippable, executable, and version-controlled).
    Model packaging may also include model serving and containerization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring (establishing an infrastructure for collecting performance metrics,
    logging, alerting, and anomaly detection such as drift and outlier detection).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model testing (including facilitation and automation of A/B testing).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and maintenance of MLOps infrastructure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of continuous integration and continuous deployment pipelines
    for ML models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automation of ML lifecycle processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other responsibilities of ML engineers that are not listed in the
    preceding list, but this list should already give you an idea of how to differentiate
    data science from ML engineering.
  prefs: []
  type: TYPE_NORMAL
- en: The ML platform that you are building will reduce the number of ML engineering
    tasks to be done manually to a point where even the data scientists can do most
    of the ML engineering tasks by themselves.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, you will see how data scientists can track the model development
    iterations to improve model quality and share the learning with the team. You
    will see how teams can apply version control to ML models and other practices
    of software engineering to the ML world.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue our journey of ML engineering into the next chapter, where
    you will see how models can be packaged and deployed in a standard way and see
    how the deployment process can be automated.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with building standard development environments for our data science
    team.
  prefs: []
  type: TYPE_NORMAL
- en: Using a custom notebook image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you have seen in [*Chapter 5*](B18332_05_ePub.xhtml#_idTextAnchor069), *Data
    Engineering*, JupyterHub allows you to spin up Jupyter Notebook-based development
    environments in a self-service manner. You have launched the **Base Elyra Notebook
    Image** container image and used it to write the data processing code using Apache
    Spark. This approach enables your team to use a consistent or standardized development
    environment (for example, same Python versions and same libraries for building
    code) and apply security policies to the known set of software being used by your
    team. However, you may also want to create your own custom images with a different
    set of libraries or a different ML framework. The platform allows you to do that.
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsection, you will build and deploy a custom container image
    to be used within your team.
  prefs: []
  type: TYPE_NORMAL
- en: Building a custom notebook container image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s assume that your team wants to use a specific version of the Scikit
    library along with some other supporting libraries such as `joblib`. You then
    want your team to use this library while developing their data science code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `Dockerfile` provided in the code repository of this book at `chapter6/CustomNotebookDockerfile`.
    This file uses the base image provided and used by ODH and then adds the required
    libraries. The file is shown in *Figure 6.1*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Dockerfile for the custom notebook image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 – Dockerfile for the custom notebook image
  prefs: []
  type: TYPE_NORMAL
- en: Note the first line, which refers to the latest image at the time of writing.
    This image is used by ODH. Lines 4 and 5 install the Python packages defined in
    the `requirements.txt` file. Line 8 installs the dependencies that are not in
    the `requirements.txt` file. If you wish to add additional packages to the image,
    you can simply insert a line in `requirements.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build the image using the file provided in the preceding step. Run the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Output of the container build command'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – Output of the container build command
  prefs: []
  type: TYPE_NORMAL
- en: 'Tag the built image as per your liking. You will need to push this image to
    a registry from where your Kubernetes cluster can access it. We use `quay.io`
    as the public Docker repository of choice, and you can use your preferred repository
    here. Notice that you will need to adjust the following command and change the
    `quay.io/ml-on-k8s/` part before execution of the command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There is no output of the preceding command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Push the image to the Docker repository of your choice. Use the following command
    and make sure to change the repository location as per *Step 3*. This image may
    take some time to be pushed to an internet repository based on your connection
    speed. Be patient:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see the output of this command as shown in *Figure 6.3*. Wait for
    the push to complete.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Pushing the custom notebook image to a Docker repository'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 – Pushing the custom notebook image to a Docker repository
  prefs: []
  type: TYPE_NORMAL
- en: Now, the image is available to be used. You will configure ODH manifests in
    the next steps to use this image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `manifests/jupyterhub-images/base/customnotebook-imagestream.yaml`
    file. This file is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.4 – ImageStream object'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – ImageStream object
  prefs: []
  type: TYPE_NORMAL
- en: JupyterHub from ODH uses a CRD called `manifests/odh-common/base/imagestream-crd.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: Notice on lines 7 and 8, we have defined some annotations. JupyterHub reads
    all the `imagestream` objects and uses these annotations to be displayed on the
    JupyterHub landing page. JupyterHub also looks at the field named `dockerImageReference`
    to load these container images upon request.
  prefs: []
  type: TYPE_NORMAL
- en: We encourage you to fork the code repository of this book onto your own Git
    account and add more images. Keep in mind to change the location of the Git repository
    in the `manifests/kfdef/ml-platform.yaml` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the JupyterHub server to see the newly created image, you will need to
    restart the JupyterHub pod. You can find the pod via the following command and
    delete this pod. After a few seconds, Kubernetes will restart this pod and your
    new image will appear on the JupyterHub landing page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response. Note that the pod name will be different
    for your setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Pods with names containing jupyterhub'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – Pods with names containing jupyterhub
  prefs: []
  type: TYPE_NORMAL
- en: 'Delete the JupyterHub pod by running the following command. Note that you do
    not need to delete this pod for this exercise, because the custom image is already
    present in our manifest files. This step will be required once you add a new customer
    notebook image using the steps mentioned in this section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response. Note that the pod name will be different
    for your setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Output of the delete pod command'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 – Output of the delete pod command
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to JupyterHub and you will see the new notebook image listed there:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.7 – JupyterHub landing page showing the new notebook image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – JupyterHub landing page showing the new notebook image
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn about MLflow, a software that assists teams
    in recording and sharing the outcomes of model training and tuning experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing MLflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simply put, MLflow is there to simplify the model development lifecycle. A lot
    of the data scientist's time is spent finding the right algorithms with the right
    hyperparameters for the given dataset. As a data scientist, you experiment with
    different combinations of parameters and algorithms, then review and compare the
    results to make the right choice. MLflow allows you to record, track, and compare
    these parameters, their results, and associated metrics. The component of MLflow
    that captures the details of each of your experiments is called the **tracking
    server**. The tracking server captures the environment details of your notebook,
    such as the Python libraries and their versions, and the artifacts generated by
    your experiment.
  prefs: []
  type: TYPE_NORMAL
- en: The tracking server allows you to compare the data captured between different
    runs of an experiment, such as the performance metrics (for example, accuracy)
    alongside the hyperparameters used. You can also share this data with your team
    for collaboration.
  prefs: []
  type: TYPE_NORMAL
- en: The second key capability of the MLflow tracking server is the model registry.
    Consider that you have run ten different experiments for the given dataset, while
    each of the experiments resulted in a model. Only one of the models will be used
    for the given problem. The model registry allows you to tag the selected model
    with one of the three stages (**Staging**, **Production**, and **Archived**).
    The model registry has APIs that allow you to access these models from your automated
    jobs. Versioning models in a registry will enable you to roll back to previous
    versions of the model using your automation tools in production if needed.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6.8* shows the two major capabilities of the MLflow software:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – MLflow major capabilities'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.8 – MLflow major capabilities
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know what MLFlow is used for, let's take a look at the components
    that made up MLFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MLflow components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's see what the major components of the MLflow system are and how it fits
    into our ML platform's ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MLflow is deployed as a container, and it contains a backend server, a GUI,
    and an API to interact with it. In the later sections of this chapter, you will
    use the MLflow API to store the experiment data onto it. You will use the GUI
    component to visualize experiment tracking and the model registry parts. You can
    find this configuration at `manifests/mlflow/base/mlflow-dc.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow backend store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The MLflow server needs a backend store to store the metadata about experiments.
    The ODH component automatically provisions a PostgreSQL database to be used as
    a backend store for MLflow. You can find this configuration at `manifests/mlflow/base/mlflow-postgres-statefulset.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The MLflow server supports several types of storage, such as S3 and databases.
    This storage will serve as the persistent storage for the artifacts, such as files
    and model files. In our platform, you will provision an open source S3 compatible
    storage service known as `manifests/minio/base/minio-dc.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow authentication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MLflow does not have an out-of-the-box authentication system at the time of
    writing. In our platform, we have configured a proxy server in front of the MLflow
    GUI that will authenticate the request before forwarding it to the MLflow server.
    We are using the open source component at [https://github.com/oauth2-proxy/oauth2-proxy](https://github.com/oauth2-proxy/oauth2-proxy)
    for this purpose. The proxy has been configured to perform **Single-Sign-On**
    (**SSO**) with the **Keycloak** service of the platform.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – MLflow and associated components in the platform'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.9 – MLflow and associated components in the platform
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in *Figure 6.9*, the MLflow pod has two containers in it: the
    MLflow server and the OAuth2 proxy. The Oauth2 proxy has been configured to use
    the Keycloak instance you installed.'
  prefs: []
  type: TYPE_NORMAL
- en: When you created a new instance of ODH in [*Chapter 5*](B18332_05_ePub.xhtml#_idTextAnchor069),
    *Data Engineering*, it installed many platform components, including MLflow and
    Minio. Now, let's validate the MLflow installation.
  prefs: []
  type: TYPE_NORMAL
- en: Validating the MLflow installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ODH has already installed the MLflow and associated components for you. Now,
    you will use the MLflow GUI to get yourself familiar with the tool. You can imagine
    all the team members will have access to experiments and models, which will improve
    your team''s collaboration:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the ingress objects created in your Kubernetes environment using the following
    command. This is to get the URL of the endpoints where our services are deployed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – All ingress objects in your cluster namespace'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.10 – All ingress objects in your cluster namespace
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the Minio GUI, our S3 component, and validate that there is a bucket available
    for MLflow to be used as its storage. The URL for the Minio component will look
    like `https://minio.192.168.61.72.nip.io`, where you will adjust the IP address
    as per your environment. The password is configured in the manifests file, and
    it is `minio123`. We have added Minio to the manifests to show that there is an
    option available using open source technologies, but making it suitable for production
    is out of scope for this book. Click on the buckets menu item on the left-hand
    side of the screen and you will see the available buckets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Minio bucket list'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.11 – Minio bucket list
  prefs: []
  type: TYPE_NORMAL
- en: How are all of these buckets created? In the manifests, we have a Kubernetes
    job that creates the buckets. You can find the job at `manifests/minio/base/minio-job.yaml`.
    The job is using the Minio command-line client, `mc`, to create the buckets. You
    can find these commands under the `command` field name in this file.
  prefs: []
  type: TYPE_NORMAL
- en: The configuration of S3 that is being used by MLflow is configured at `manifests/mlflow/base/mlflow-dc.yaml
    file`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the settings as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – MLflow configuration to use Minio'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.12 – MLflow configuration to use Minio
  prefs: []
  type: TYPE_NORMAL
- en: Open a browser and paste the `HOSTS` value for the `jupyterhub` ingress into
    your browser. For me, it was [https://mlflow.192.168.61.72.nip.io](https://mlflow.192.168.61.72.nip.io).
    This URL will take you to the Keycloak login page, which is the SSO server as
    shown in the following figure. Make sure that you replace the IP address with
    yours in this URL. Recall that the authentication part of MLflow is being managed
    by a proxy that you have configured in `manifests/mlflow/base/mlflow-dc.yaml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can see the configuration of the OAuth proxy for MLflow as follows. Because
    `oauth-proxy` and MLflow belong to the same pod, all we have done is route the
    traffic from `oauth-proxy` to the MLflow container. This is set up with the `–upstream`
    property. You can also see `oauth-proxy` needs the name of the identity provider
    server, which is Keycloak, and it is configured under the `–oidc-issuer` property:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.13 – OAuth proxy configuration for MLflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.13 – OAuth proxy configuration for MLflow
  prefs: []
  type: TYPE_NORMAL
- en: The landing page of MLflow looks like the page in *Figure 6.14*. You will notice
    there are two sections on the top bar menu. One has the label **Experiments**
    and the other one, **Models**.
  prefs: []
  type: TYPE_NORMAL
- en: Before you see this page, the SSO configuration will display the login page.
    Enter the user ID as `mluser` and the password as `mluser` to log in. The username
    and password were configured in [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055),
    *The* *Anatomy of a Machine Learning Platform*, in the *Creating a Keycloak user*
    section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.14 – MLflow experiment tracking page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.14 – MLflow experiment tracking page
  prefs: []
  type: TYPE_NORMAL
- en: The left-hand side of the **Experiments** screen contains the list of experiments,
    and the right-hand side displays the details of experiment runs. Think of the
    experiment as the data science project you are working on, such as fraud detection
    in consumer transactions, and the **Notes** section captures the combination of
    parameters, algorithms, and other information used to run the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Models** tab to see the landing page of the model registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Models** tab contains the list of models in the registry, their versions,
    and their corresponding stages, which mention what environment the models are
    deployed in.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – MLflow model registry page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.15 – MLflow model registry page
  prefs: []
  type: TYPE_NORMAL
- en: If you can open the MLflow URL and see the pages as described in the preceding
    steps, then you have just validated that MLflow is configured in your platform.
    The next step is to write a notebook that will train a basic model while recording
    the details in your MLflow server.
  prefs: []
  type: TYPE_NORMAL
- en: Using MLFlow as an experiment tracking system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will see how the MLflow library allows you to record your
    experiments with the MLflow server. The custom notebook image, which you saw in
    the first part of this chapter, already has MLflow libraries packaged into the
    container. Please refer to the `chapter6/requirements.txt` file for the exact
    version of the MLflow library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start this activity, it is important to understand two main concepts:
    **experiment** and **run**.'
  prefs: []
  type: TYPE_NORMAL
- en: An experiment is a logical name under which MLflow records and groups the metadata,
    for example, an experiment could be the name of your project. Let's say you are
    working on building a model for predicting credit card fraud for your retail customer.
    This could become the experiment name.
  prefs: []
  type: TYPE_NORMAL
- en: A run is a single execution of an experiment that is tracked in MLflow. A run
    belongs to an experiment. Each run may have a slightly different configuration,
    different hyperparameters, and sometimes, different datasets. You will tweak these
    parameters of the experiment in a Jupyter notebook. Each execution of model training
    is typically considered a run.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow has two main methods of recording the experiment details. The first one,
    which is our preferred method, is to enable the auto-logging features of MLflow
    to work with your ML library. It has integration with Scikit, TensorFlow, PyTorch,
    XGBoost, and a few more. The second way is to record everything manually. You
    will see both options in the following steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps will show you how an experiment run or a model training can be
    recorded in MLflow while executing in a Jupyter notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to JupyterHub and make sure to select the custom container, for example,
    **Scikit v1.10 - Elyra Notebook Image**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before you hit the **Start Server** button, add an environment variable by clicking
    on the **Add more variables** link. This variable may contain sensitive information
    such as passwords. MLflow needs this information to upload the artifacts to the
    Minio S3 server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The landing page will look like the screenshot in *Figure 6.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – JupyterHub with an environment variable'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.16 – JupyterHub with an environment variable
  prefs: []
  type: TYPE_NORMAL
- en: Open the notebook at `chapter6/hellomlflow.ipynb`. This notebook shows you how
    you can record your experiment data onto the MLflow server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Notebook with Mlflow integration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_017.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.17 – Notebook with Mlflow integration
  prefs: []
  type: TYPE_NORMAL
- en: Note that at the first code cell, you have imported the MLflow library. In the
    second code cell, you have set up the location of the MLflow server through the
    `set_tracking_uri` method. Note that because your notebook and the MLflow server
    are running on Kubernetes, we just put the location of the Kubernetes Service
    that is stored in the `HOST` variable name and is being used in this method.
  prefs: []
  type: TYPE_NORMAL
- en: You then set the name of the experiment using the `set_experiment` method. This
    is one important variable through which all your experiment runs will be stored
    in the MLflow server.
  prefs: []
  type: TYPE_NORMAL
- en: The last method in this cell is `sklearn.autolog`, which is a way to tell MLflow
    that we are using the Scikit library for our training, and MLflow will record
    the data through Scikit APIs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Notebook cell with MLflow configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_018.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.18 – Notebook cell with MLflow configuration
  prefs: []
  type: TYPE_NORMAL
- en: In the last cell of this notebook, you are using a simple `DecisionTreeClassifier`
    to train your model. Notice that this is quite a simple model and is used to highlight
    the capabilities of the MLflow server.
  prefs: []
  type: TYPE_NORMAL
- en: Run the notebook by selecting the **Run > Run all cells** menu option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to the MLflow server and click on the experiment name `HelloMlFlow`.
    The URL of MLflow will be like [https://mlflow.192.168.61.72.nip.io](https://mlflow.192.168.61.72.nip.io)
    with the IP address replaced as per your environment. As mentioned earlier in
    this chapter, you get this URL by listing the *ingress* objects of your Kubernetes
    cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see the screen as shown in *Figure 6.19*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – MLflow experiments tracking screen showing an experiment run'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_019.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.19 – MLflow experiments tracking screen showing an experiment run
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that the table on the right-hand side contains one record. This
    is the experiment run you performed in *Step 6*. If you have executed your notebook
    multiple times with different parameters, each run will be recorded as a row in
    this table.
  prefs: []
  type: TYPE_NORMAL
- en: Click on the first row of the table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will get to the details of the run you selected in the previous step. The
    screen will look like the screenshot in *Figure 6.20*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – MLflow run details'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_020.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.20 – MLflow run details
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the information that is available on this screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '`4`, you will see that the parameters that we have used for `DecisionTreeClassifier`
    are recorded here too. One such example is the `max_depth` parameter, as shown
    in *Figure 6.21*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.21 – MLflow run parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_021.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.21 – MLflow run parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`training_accuracy` in the screenshot, as shown in *Figure 6.22*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.22 – MLflow run metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_022.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.22 – MLflow run metrics
  prefs: []
  type: TYPE_NORMAL
- en: '`estimator_class`), which define the type of ML algorithm you have used. Note
    that you can add your own tags if needed. In the next section, we will show how
    to associate a custom tag for your run. *Figure 6.23* shows an example of tags:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.23 – MLflow run tags'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_023.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.23 – MLflow run tags
  prefs: []
  type: TYPE_NORMAL
- en: '`model.pkl` file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.24 – MLflow run artifacts'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_024.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.24 – MLflow run artifacts
  prefs: []
  type: TYPE_NORMAL
- en: 'To validate that these files are indeed stored in the S3 server, log in to
    the Minio server, select **Buckets**, and click on **Browse** button for the MLflow
    bucket. You will find a folder created with the name of your run. This name is
    displayed in the top-left corner of your experiment screen; consult the top-left
    corner of the preceding screen and you will see a label with a combination of
    32 alphanumeric characters. This long number is your **run ID**, and you can see
    a folder label with a combination of 32 alphanumeric characters in your S3 bucket,
    as shown in the following screenshot. You can click on this link to find the artifacts
    stored on the S3 bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Minio bucket location'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_025.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.25 – Minio bucket location
  prefs: []
  type: TYPE_NORMAL
- en: You have just successfully trained a model in JupyterHub and tracked the training
    run in MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: You have seen how MLflow associates the data with each of your runs. You can
    even compare the data between multiple runs by selecting multiple runs from the
    table shown in *Step 6* and clicking on the **Compare** button.
  prefs: []
  type: TYPE_NORMAL
- en: Adding custom data to the experiment run
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s see how we can add more data for each run. You will learn how to
    use the MLflow API to associate custom data with your experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: Start by firing up the Jupyter notebook as you did in the preceding section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the notebook at `chapter6/hellomlflow-custom.ipynb`. This notebook shows
    you how you can customize the recording of your experiment data onto the MLflow
    server. The notebook is similar to the previous notebook, except for the code
    in cell number `6`, which is shown in *Figure 6.26*. This code cell contains the
    functions that show how to associate data with your experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.26 – MLflow customized data collection notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_026.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.26 – MLflow customized data collection notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand these functions in the next few steps. The code snippet in
    code cell number `6` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will include a custom tag labeled `code.repoURL`. This makes
    it easier to trace back the original source code that produced the model in a
    given experiment run.
  prefs: []
  type: TYPE_NORMAL
- en: You can associate any tags while calling the `start_run` function. Tag keys
    that start with *mlflow* are reserved for internal use. You can see that we have
    associated the GIT commit hash with the first property. This will help us in following
    through on what experiment belongs to what code version in your code repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will find that the `code.repoURL` tag contains the Git repository location.
    You can add as many tags as you want. You can see the tags by going to the MLflow
    UI and opening the experiment. Note that the notebook has a different experiment
    name, and it is being referenced as `HelloMlFlowCustom`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the `code.repoURL` in the **Tags** section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 6.27 – MLflow custom tags'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_027.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.27 – MLflow custom tags
  prefs: []
  type: TYPE_NORMAL
- en: The second function that we have used is `record_libraries`. This is a wrapper
    function that internally uses the `mlflow.log_artifact` function to associate
    a file with the run. This utility function is capturing the `pip freeze` output,
    which gives the libraries in the current environment. The utility function then
    writes it to a file and uploads the file to the MLflow experiment. You can look
    at this, and all the other functions, in the `chapter6/mlflow_util.py` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can see in the `pip_freeze.txt`, is available, and it records the output
    of the `pipe freeze` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.28 – MLflow customized artifacts'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_028.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.28 – MLflow customized artifacts
  prefs: []
  type: TYPE_NORMAL
- en: 'The `log_metric` function records the metric name and its associated value.
    Note that the value for the metric is expected to be a number. For the sample
    code, you can see that we have just put a hardcoded value (`1`), however, in the
    real world, this would be a dynamic value that refers to something relative to
    each run of your experiment. You can find your custom metric in the **Metrics**
    section of the page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.29 – MLflow customized metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_029.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.29 – MLflow customized metrics
  prefs: []
  type: TYPE_NORMAL
- en: 'The `log_param` function is like the `log_metric` function, but it can take
    any type of value against a given parameter name. For example, we have recorded
    the Docker image used by the Jupyter notebook. Recall that this is the custom
    image you built to be used by the data scientist team. You can see the following
    `docker_image_name` parameter that contains the desired value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.30 – MLflow customized parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_030.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.30 – MLflow customized parameters
  prefs: []
  type: TYPE_NORMAL
- en: You have used MLflow to track, add custom tags, and custom artifacts to an experiment
    run. In the next section, you will see the capabilities of MLflow as a model registry
    component. Let's dig in.
  prefs: []
  type: TYPE_NORMAL
- en: Using MLFlow as a model registry system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that MLflow has a model registry feature. The registry provides the
    versioning capabilities for your models. Automation tools can get the models from
    the registry to deploy or even roll back your models across different environments.
    You will see in the later chapters that automation tools in our platform fetch
    the model from this registry via the API. For now, let''s see how to use the registry:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to the MLflow server by accessing the UI and clicking on the **Models**
    link. You should see the following screen. Click on the **Create Model** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.31 – MLflow registering a new model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_031.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.31 – MLflow registering a new model
  prefs: []
  type: TYPE_NORMAL
- en: 'Type a name for your model in the pop-up window, as shown in the following
    screenshot, and click on the **Create** button. This name could mention the name
    of the project that this model is serving:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.32 – MLflow model name prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_032.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.32 – MLflow model name prompt
  prefs: []
  type: TYPE_NORMAL
- en: Now, you need to attach a model file to this registered name. Recall from the
    preceding section that you have multiple *runs* in your *experiment*. Each run
    defines the set of configuration parameters and associated models with it. Select
    the experiment and run for which you want to register your model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see a screen like the following. Select the **model** label in the
    **Artifacts** section, and you will notice a **Register Model** button on the
    right-hand side. Click on this button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.33 – MLflow showing the Register Model button'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_033.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.33 – MLflow showing the Register Model button
  prefs: []
  type: TYPE_NORMAL
- en: From the pop-up window, select the model name you created in *Step 1* and click
    on **Register**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.34 – Model name dialog when registering a model in MLflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_034.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.34 – Model name dialog when registering a model in MLflow
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the `mlflowdemo`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.35 – MLflow showing the list of registered models and their versions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_035.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.35 – MLflow showing the list of registered models and their versions
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see the detail screen where you can attach the stage of the model
    as referred to by the **Stage** label. You can also edit other properties, and
    we will leave it to you to explore the data you can associate with this model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.36 – MLflow showing the buttons for promoting registered models
    to higher environments'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_06_036.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.36 – MLflow showing the buttons for promoting registered models to
    higher environments
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have just experienced using MLflow as a model registry!
    You have also seen how the model version can be promoted to the different stages
    of the lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have gained a better understanding of ML engineering and
    how it differs from data science. You have also learned about some of the responsibilities
    of ML engineers. You must take note that the definition of ML engineering and
    the role of ML engineers are still evolving, as more and more techniques are surfacing.
    One such technique that we will not talk about in this book is **online ML**.
  prefs: []
  type: TYPE_NORMAL
- en: You have also learned how to create a custom notebook image and use it to standardize
    notebook environments. You have trained a model in the Jupyter notebook while
    using MLflow to track and compare your model development parameters, training
    results, and metrics. You have also seen how MLflow can be used as a model registry
    and how to promote model versions to different stages of the lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will continue the ML engineering domain and you will package
    and deploy ML models to be consumed as an API. You will then automate the package
    and deploy the process using the tools available in the ML platform.
  prefs: []
  type: TYPE_NORMAL
