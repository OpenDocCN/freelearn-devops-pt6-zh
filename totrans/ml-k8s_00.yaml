- en: '*Chapter 2*: Understanding MLOps'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most people from software engineering backgrounds know about the term **development-operations**
    (**DevOps**). To us, DevOps is about collaboration and shared responsibilities
    across different teams during the **software development life cycle** (**SDLC**).
    The teams are not limited to a few **information technology** (**IT**) teams;
    instead, it involves everyone from the organization who is a stakeholder in the
    project. No more segregation between building software (developers' responsibility)
    and running it in production (operations' responsibility). Instead, the team *owns*
    the product. DevOps is popular because it helps teams increase the velocity and
    reliability of the software being developed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing **machine learning** (**ML**) to traditional programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the benefits of DevOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding **ML operations** (**MLOps**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The role of **open source software** (**OSS**) in ML projects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running ML projects on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we can apply DevOps to ML projects, we must first understand the difference
    between traditional software development and ML development processes.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing ML to traditional programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with traditional application development, an ML project is also a software
    project, but there are fundamental differences in the way they are delivered.
    Let's understand how an ML project is different from a traditional software application.
  prefs: []
  type: TYPE_NORMAL
- en: In traditional software applications, a software developer writes a program
    that holds an explicitly handcrafted set of rules. At runtime or prediction time,
    the built software applies these well-defined rules to the given data, and the
    output of the program is the result calculated based on coded rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the **inputs and outputs** (**I/Os**) for a traditional
    software application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Traditional software development'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_02_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 – Traditional software development
  prefs: []
  type: TYPE_NORMAL
- en: In an ML project, the rules or patterns are not completely known, therefore
    we cannot explicitly describe rules in code as we can in traditional programming.
    In ML, there is a process that extracts rules based on a given sample pair of
    data and its associated expected results. This process is called **model training**.
    In the model-training process, the chosen ML algorithm calculates rules based
    on the given data and the verified answer. The output of this process is the **ML
    model**. This generated model can then be used to infer answers during prediction
    time. In contrast with traditional software development, instead of using explicitly
    written rules, we use a generated ML model to get a result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows that the ML model is generated at training time,
    which is then used to produce answers or results during prediction time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – ML development'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_02_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 – ML development
  prefs: []
  type: TYPE_NORMAL
- en: Though traditional software development and ML are fundamentally different,
    there are some synergies in the engineering processes between the two approaches.
    Given that traditional software development is very mature in the current era,
    we can apply lessons from it to our ML projects. Primarily, of course, both traditional
    programming and ML are software. Whichever processes we apply to build software
    in the traditional world—such as versioning, packaging of software as containers,
    automated deployments, and so on—these can be applied to ML projects too. However,
    we also must accommodate added processes in ML, such as model training.
  prefs: []
  type: TYPE_NORMAL
- en: So, why do we really need DevOps in ML projects? What does it bring to the table?
    Let's have a look at this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the benefits of DevOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DevOps is not just about toolsets. Say you have a tool available that can run
    unit tests for you. However, if the team has no culture of writing test cases,
    the tool would not be useful. DevOps is about how we work together on tasks that
    span across different teams. So, the three primary areas to focus on in DevOps
    are these:'
  prefs: []
  type: TYPE_NORMAL
- en: '**People**: Teams from multiple disciplines to achieve a common goal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processes**: The way teams work together'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Technology**: The tools that facilitate collaboration across different teams'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DevOps is built on top of Agile development practices with the objective of
    streamlining the software development process. DevOps teams are cross-functional,
    and they have the autonomy to build software through **continuous integration/continuous
    delivery** (**CI/CD**). DevOps encourages teams to collaborate over a fast feedback
    loop to improve the efficiency and quality of the software being developed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates a complete DevOps cycle for traditional software
    development projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – A mobius loop showcasing a DevOps process'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_02_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 – A mobius loop showcasing a DevOps process
  prefs: []
  type: TYPE_NORMAL
- en: 'Through DevOps, teams can have well-defined and streamlined development practices
    for building, testing, deploying, and monitoring software in production. All this
    makes it possible to quickly and reliably release software into production. Some
    of the benefits that come out of DevOps practices are presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CI/CD**: CI is a phase through which software is merged and verified as soon
    as the developer pushes it into the code repository. CD is a series of stages
    through which software is built, tested, and packaged in a deployment ready form.
    **Continuous deployment** (also known as **CD**) is a phase where the deployment-ready
    code is picked and deployed to be consumed by end users. In DevOps, all these
    processes are automated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Infrastructure as Code** (**IaC**): IaC is an approach to automate the provisioning
    and configuring of IT infrastructure. This aspect enables the team to request
    and configure infrastructure on an on-demand and as-needed basis. Imagine that
    a data scientist in your team needs a **graphics processing unit** (**GPU**) to
    do their model training. If we follow the practice of configuring and provisioning
    IaC, the request for a GPU can be automatically fulfilled by the system. In the
    next chapters, you will see this capability in action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observability**: Observability relates to how well we understand the state
    of our running system. DevOps makes systems observable via federating logging
    from different components, monitoring the systems (such as **central processing
    unit** (**CPU**), memory, response times, and so on), and providing a way to correlate
    various parts of the system for a given call through call tracing. All these capabilities,
    collectively, provide the basis for understanding the system state and help debug
    any issues without changing the code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Team collaboration**: DevOps is not just about technology. In fact, the key
    focus area for the team is to collaborate. Collaboration is how multiple individuals
    from different teams work toward a common goal. Business, development, and operations
    teams working together is the core of DevOps. For ML-based projects, the team
    will have data scientists and data engineers on top of the aforementioned roles.
    With such a diverse team, communication is critical for building collective understanding
    and ownership of the defined outcome.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, how can we bring the benefits of a DevOps approach to ML projects? The answer
    is MLOps.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MLOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**MLOps** is an emerging domain that takes advantage of the maturity of existing
    software development processes—in other words, DevOps combined with data engineering
    and ML disciplines. MLOps can be simplified as an engineering practice of applying
    DevOps to ML projects. Let''s take a closer look at how these disciplines form
    the foundation of MLOps.'
  prefs: []
  type: TYPE_NORMAL
- en: ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML projects involve activities that are not present in traditional programming.
    You learned in *Figure 2.3* that the bulk of the work in ML projects is not model
    development. Rather, it is more data gathering and processing, data analysis,
    **feature engineering** (**FE**), process management, data analysis, model serving,
    and more. In fact, according to the paper *Hidden Technical Debt in Machine Learning
    Systems* by D. Sculley et al., only 5% of the work is ML model development. Because
    of this, MLOps is not only focused on the ML model development task but mostly
    on the big picture—the entire ML project life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as with DevOps, MLOps focuses on people, processes, and technology. But
    there are some complexities that MLOps has to address and DevOps doesn''t have
    to. Let''s look at some of these complexities in more detail here:'
  prefs: []
  type: TYPE_NORMAL
- en: First, unlike traditional programming, where your only input is code, in ML,
    your input is both code and data. The ML model that is produced in the model development
    stage is highly dependent on data. This means that even if you do not change your
    code, if you train an ML algorithm using a different dataset, the resulting ML
    model will be different and will perform differently. When it comes to **version
    control**, this means that you not only version the code that facilitates model
    training, but you also need to version the data. Data is difficult to version
    because of the huge amount required, unlike code. One approach to address this
    is by using Git to keep track of a dataset version using the hash of the data.
    The actual data is then stored somewhere in remote storage such as a **Simple
    Storage Service** (**S3**) bucket. An open source tool called **Data Version Control**
    (**DVC**) can do this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, there are more personas involved and *more collaboration* required
    in ML projects. You have data scientists, ML engineers, and data engineers collaborating
    with software engineers, business analysts, and operations teams. Sometimes, these
    personas are very diverse. A data scientist may not completely understand what
    production deployment really is. On the other hand, operations people (and sometimes
    even software engineers) do not understand what an ML model is. This makes collaboration
    in ML projects more complicated than a traditional software project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third, the addition of a model development stage adds more pivot points to the
    life cycle. This complicates the whole process. Unlike traditional software development,
    you only need to develop one set of working code. In ML, a data scientist or ML
    engineer may use multiple ML algorithms and generate multiple resulting ML models,
    and because only one model will get selected to be deployed to production, those
    models are compared with each other in terms of performance against other model
    properties. MLOps accommodates this complex workflow of *testing, comparing, and
    selecting models* to be deployed to production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building traditional code to generate an executable binary usually takes a few
    seconds to a few minutes. However, *training an ML algorithm to produce an ML
    model can take hours or days*, sometimes even weeks when you use certain **deep
    learning** (**DL**) algorithms. This makes setting up an Agile iterative time-bound
    cadence a little complicated. An MLOps-enabled team needs to handle this delay
    in their workflow, and one way to do this is to start building the other model
    while waiting for other models to be trained completely. This is very difficult
    to achieve if the data scientists or ML engineers are training their ML algorithms
    using their own laptops. This is where the use of a scalable infrastructure comes
    in handy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, because ML models'' performances rely on the data used during training,
    if this data no longer represents the real-world situation, the model accuracy
    will degrade, resulting in poor prediction performance. This is called **model
    drift**, and this needs to be detected early. This is usually incorporated as
    part of the monitoring process of the ML project life cycle. Aside from the traditional
    metrics that you collect in production, with ML models, you also need to monitor
    model drift and outliers. Outlier detection, however, is much more difficult to
    implement, and sometimes requires you to train and build another ML model. **Outlier
    detection** is about detecting incoming data, in production, that does not look
    like the data the model was trained on: you do not want your model to provide
    irrelevant answers to these non-related questions. Another reason is that this
    could be an attack or an attempt to abuse the system. Once you have detected model
    drift or outliers, what are you going to do with this information? It could very
    well be just about raising an alert, or it could trigger some other automated
    processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of the complexity ML adds when compared to traditional programming,
    the need to address these complexities led to the emergence of MLOps.
  prefs: []
  type: TYPE_NORMAL
- en: DevOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In terms of deployment, think about all the sets of code you write in an ML
    project: the code that performs the data processing, the code that facilitates
    model training and FE, the code that runs the model inference, and the code that
    performs model drift and outlier detection. All of these sets of code need to
    be built, packaged, and deployed for consumption at scale. This code, once running
    in production, needs to be monitored and maintained as well. This is where the
    CI/CD practices of DevOps help. The practice of automating software packaging,
    testing, securing, deploying, and monitoring came from DevOps.'
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every ML project involves **data engineering**, and ML projects deal with a
    lot of data a lot more than code. Therefore, it is mandatory that your infrastructure
    includes data processing capabilities and that it can integrate with existing
    data engineering pipelines in your organization.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering is a huge subject—an entire book could be written about it.
    But what we want to emphasize here is that MLOps intersects with data engineering
    practices, particularly in **data ingestion**, **data cleansing**, **data transformation**,
    and **big data testing**. In fact, your ML project could be just a small ML classification
    model that is a subpart of a much bigger data engineering or data analytics project.
    MLOps adopts the best practices in data engineering and analytics.
  prefs: []
  type: TYPE_NORMAL
- en: 'A representation of MLOps is provided in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – MLOps as the intersection of ML, data engineering, and DevOps'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_02_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 – MLOps as the intersection of ML, data engineering, and DevOps
  prefs: []
  type: TYPE_NORMAL
- en: To put it in another way, MLOps, as shown in *Figure 2.4*, is the convergence
    of **ML**, **DevOps**, and **data engineering** disciplines that focus on running
    ML in production. It is also about encapsulating ML projects in a highly scalable,
    reliable, observable infrastructure. Finally, it is also about establishing repeatable
    processes for teams to perform the tasks required to successfully deliver ML projects,
    as shown in *Figure 2.4*, while supporting collaboration with each other.
  prefs: []
  type: TYPE_NORMAL
- en: With this basic understanding of MLOps, let's dig a little deeper into the ML
    project life cycle. We'll start by defining what are the general stages of an
    ML project.
  prefs: []
  type: TYPE_NORMAL
- en: ML project life cycle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As with DevOps, which provides a series of activities that could be performed
    in a DevOps cycle, you can see a series of steps that could be used to take your
    ML project from start to finish in *Figure 2.5*. These steps or stages will become
    part of your ML projects'' life cycle and provide a consistent way to take your
    ML projects into production. The ML platform that you build in this book is the
    ecosystem that allows you to implement this flow. In later chapters of this book,
    you will use this flow as the basis for the platform. A summary of the stages
    in an ML project could be depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – A ML project life cycle'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_02_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 – A ML project life cycle
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a definition of each stage of the project life cycle presented in the
    preceding diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Codify the problem and define success metrics**: In this stage, the team
    evaluates if the given business problem can be solved using ML. Notice the word
    *team* here, which would consist of data scientists and the business **subject-matter
    expert** (**SME**) at a minimum. The team will then define a success criterion
    to assess the prediction of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ingest, clean, and label data**: In this stage, the team assesses if the
    data required to train the model is available. The team will play an additional
    role, that of data engineers, to help move the project during this stage and beyond.
    The team will build components to ingest data from a variety of sources, clean
    the captured data, possibly label the data, and store it. This data will form
    the basis of ML activities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FE**: FE is about transforming the raw data into features that are more relevant
    to the given problem. Consider you are building a model that predicts if any given
    passenger on the *Titanic* will survive or not. Imagine the dataset you got contains
    the ticket number of the passenger. Do you think ticket numbers have something
    to do with the survival of the passenger? A business SME may mention that ticket
    numbers may be able to provide which class the customer belongs to on the ship,
    and first-class passengers may have easier access to lifeboats on the ship.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model building and tuning**: In this stage, the team starts experimenting
    with different models and different hyperparameters. The team will test the model
    against the given dataset and compare the results of each iteration. The team
    will then determine the best model for the given success metrics and store the
    model in the model registry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model validation**: In this stage, the team validates the model against a
    new set of data that is not available at the training time. This stage is critical
    as it **determines** if the model is generalized enough for the unseen data, or
    if the model only works well on the training data but not on the unseen data—in
    other words, avoiding **overfitting**. Model validation also involves identifying
    **model biases**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model deployment**: In this stage, the team picks the model from the model
    registry, packages it, and deploys it to be consumed. Traditional DevOps processes
    could be used here to make the model available as a service. In this book, we
    will focus on **model as a service** (**MaaS**), where the model is available
    as a **REpresentational State Transfer** (**REST**) service. However, in certain
    scenarios, the model could be packaged as a library for other applications to
    use it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and validation**: In this stage, the model will be continually
    monitored for response times, the accuracy of predictions, and whether the input
    data is like the data on which the model is trained. We have briefly touched on
    outlier detection. In practice, it works like this: imagine that you have trained
    your model for rush-hour vacancy in a public transport system, and the data the
    model is trained against is where citizens use the public transport system for
    over a year. The data will have variances for weekends, public holidays, and any
    other events. Now, imagine if, due to the COVID-19 lockdown, no one is allowed
    to use the public transport system. The real world is not the *same* as compared
    to the data our model is trained upon. Naturally, our model is not particularly
    useful for this changed world. We will need to detect this anomaly and generate
    alerts so that we can retrain our model with the new datasets if possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have just learned the stages of the ML project life cycle. Although the
    stages may look straightforward, in the real world, there are several good reasons
    why you need to go back to previous stages in certain cases.
  prefs: []
  type: TYPE_NORMAL
- en: Fast feedback loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A keen observer may have noticed that a key attribute of the Agile and cross-functional
    teams that we presented in the first chapter is not available in the stages presented
    so far in this chapter. Modern DevOps is all about fast feedback loops to course-correct
    early in the project life cycle. The same concept will bring even more value to
    ML projects because ML projects are more complex than traditional software applications.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see at which stages we can assess and evaluate the progress of the team.
    After evaluation, the team can decide to course-correct by going back to an earlier
    stage or moving on to the next stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the ML project life cycle with feedback checkpoints
    from various stages, denoted by green arrows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – A ML project life cycle with feedback checkpoints'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_02_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 – A ML project life cycle with feedback checkpoints
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at this in more detail here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Checkpoint from the ingest, clean, and label data stage**: After *Stage 1*
    is completed, you have started to process data as defined in the second stage.
    You may find that the actual data is incomplete or not correct. You can take this
    feedback to improve your understanding of data and may need to redefine the success
    criteria of the project or, in worse cases, stop the project because the required
    data is not available. In many scenarios, teams find additional data sources to
    fill the data gap identified in the second stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Checkpoint from the model building and tuning stage**: During this stage,
    the team may find that the features available to train the model may not be enough
    to get the desired metric. At this point, the team may decide to invest more time
    in finding new features or revisit the raw data to determine if more data is needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Checkpoint from the model validation stage**: During this stage, the model
    will be validated against a new dataset that the model has never seen before.
    Poor metrics at this stage may trigger the tuning of the model, or you may decide
    to go back to find more features for better model performance,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Checkpoint from the model monitoring and validation stage**: Once the model
    moves into production, it must be monitored continuously to validate if the model
    is still relevant to the real and changing world. You need to find out if the
    model is still relevant and, if not, how you can make the model more useful. The
    result of this may trigger any other stage in the life cycle; as you can see in
    *Figure 2.6*, you may end up retraining an existing model with new data or going
    to a different model altogether, or even rethinking if this problem *should* be
    tackled by ML. There is no definitive answer on which stage you end up at; just
    as with the real world, it is unpredictable. However, what is important is the
    capability to re-assess and re-evaluate, and to continue to deliver value to the
    business.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have seen the stages of the ML project life cycle and the feedback checkpoints
    from which you decided whether to continue to the next stage or go back to previous
    stages. Now, let's look at the personas involved in each of the stages and their
    collaboration points.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborating over the project life cycle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have defined a streamlined process for building our model. Let's try to define
    how a team of diverse roles and abilities will collaborate on this model. Recall
    from the previous chapter that building a model takes effort from different teams
    with different abilities. It is important to note that in smaller projects, the
    same person may be representing different roles at the same time. For example,
    in a small project, the same person can be both a data scientist and a data engineer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an ML project life cycle with an overlay of feedback
    points and personas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – A ML project life cycle with feedback checkpoints and team roles'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_02_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 – A ML project life cycle with feedback checkpoints and team roles
  prefs: []
  type: TYPE_NORMAL
- en: The ML project within your organization needs collaboration between data scientists
    and the business SMEs in the first stage. Imagine the team wants to predict, based
    on a picture, the probability of a certain type of skin disease.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, a collaboration between data scientists and doctors (the SME
    for this case) is needed to define the problem and the performance metrics. Without
    this collaboration, the project would not be successful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second stage—the data ingestion and cleaning stage—data engineers will
    need to work along with the business SMEs to understand which data is available
    and how to clean and label it correctly. The knowledge the SMEs will bring during
    this stage is critical as this is responsible for creating a useful dataset for
    future stages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the third stage, data scientists, data engineers, and SMEs will collaborate
    to work on the base data from the second stage and process it to extract useful
    features from it. The data scientists and SMEs will provide guidance on which
    data can be extracted, and the data engineer will write processing logic to do
    so.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the fourth and the fifth stages, most of the work will be done by data scientists
    to build and tune the model as per the given criteria. However, based on whether
    or not the model has managed to achieve the defined metric, the team may decide
    to go back to any of the previous stages for better performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the model is built, the DevOps team experts can package, version, and deploy
    the model to the correct environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last stage is critical: the team uses observability capabilities to monitor
    the performance of the model in the production environment. After monitoring the
    model performance in the real world and based on the feedback, the team may again
    decide to go back to any of the previous stages to make the model more useful
    for the business.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you have a good understanding of the challenges we have highlighted
    and how you can overcome these challenges using the ML life cycle, the next phase
    is to have a platform that supports this life cycle while providing a solution
    for each component defined in the big picture (see [*Chapter 1*](B18332_01_ePub.xhtml#_idTextAnchor015),
    *Challenges in Machine Learning*) with self-service and automation capabilities.
    What better way to start a journey while collaborating with the open source community?
  prefs: []
  type: TYPE_NORMAL
- en: The role of OSS in ML projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have a clear understanding of what problems the ML platform is
    expected to solve, let's see why open source is the best place to start. We should
    start with some definitions to set the basics, right?
  prefs: []
  type: TYPE_NORMAL
- en: Free OSS is where *the users have the freedom to run, copy, distribute, study,
    change, and improve the software*.
  prefs: []
  type: TYPE_NORMAL
- en: OSS
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on OSS, see the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.gnu.org/philosophy/free-sw.html](https://www.gnu.org/philosophy/free-sw.html)'
  prefs: []
  type: TYPE_NORMAL
- en: OSS is everywhere. Linux is the most common operating system, running in data
    centers and powering the cloud around the world. Apache Spark and related open
    source technologies are the foundation for the big data revolution for a range
    of organizations. Open source-based **artificial intelligence** (**AI**) technologies
    such as TensorFlow and MLflow are at the forefront of AI advancement and are used
    by hundreds of organizations. Kubernetes, the open source container orchestration
    platform, has become the de facto standard for container platforms.
  prefs: []
  type: TYPE_NORMAL
- en: The top players in computing—such as Amazon, Apple, Facebook, Google, Microsoft,
    and Red Hat, to name a few—have contributed to and owned major open source projects,
    and fresh players are joining all the time. Businesses and governments around
    the world depend on open source to power mission-critical and highly scalable
    systems every day.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most successful open source projects in the cloud computing space
    is **Kubernetes**. Kubernetes was founded in mid-2014 and was followed by the
    release of its version 1.0 in mid-2015\. Since then, it has become the de facto
    standard for container orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the **Cloud Native Computing Foundation** (**CNCF**) was created by
    *The Linux Foundation* with the mission of making cloud computing ubiquitous.
    CNCF does this by bringing together the world's top engineers, developers, end
    users, and vendors. They also run the world's largest open source conferences.
    The foundation was created by using **Kubernetes** as the seed project. This is
    how Kubernetes sets the standard definition of **cloud native**. As of this writing,
    the foundation has 741 member organizations and 130 certified Kubernetes distributions
    and platforms and has graduated 16 very successful open source projects. Among
    those projects is, of course, **Kubernetes** but also the **Operator Framework**,
    which you will learn more about in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Before the explosion of **big data** and **cloud computing**, ML projects were
    mostly academic. They seldom left the boundaries of colleges and universities,
    but this doesn't mean that AI, ML, and **data science** were not progressing forward.
    The academic world has actually created hundreds of open source Python libraries
    for mathematical, scientific, and statistical calculations. These libraries have
    become the foundation modern ML frameworks are built upon. The most popular ML
    frameworks at the time of writing—TensorFlow, PyTorch, scikit-learn, and Spark
    ML—are all open source. The most popular data science and ML development environments
    today—Jupyter Notebook, JupyterLab, JupyterHub, Anaconda, and many more—are also
    all open source.
  prefs: []
  type: TYPE_NORMAL
- en: ML is an evolving field, and it needs the vision of larger communities that
    go beyond any single organization. The process of working in a community-based
    style enables the collaboration and creativity that is required by ML projects,
    and open source is an important part of why ML is progressing at a tremendous
    speed.
  prefs: []
  type: TYPE_NORMAL
- en: You now have a basic understanding of how important OSS is in the AI and ML
    space. Now, let's take a closer look at why you should run ML projects on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Running ML projects on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For building reliable and scalable ML systems, you need a rock-solid base. **Kubernetes**
    provides the foundation for building scalable and reliable distributed systems
    along with the self-service capabilities that are required by our platform. The
    capability of Kubernetes to abstract the hardware infrastructure and consume it
    as a single unit is of great benefit to our platform.
  prefs: []
  type: TYPE_NORMAL
- en: Another key component is the ability of Kubernetes-based software to run anywhere,
    from small on-premises data centers to large hyperscalers (**Amazon Web Services**
    (**AWS**), **Google Cloud Platform** (**GCP**), Azure). This capability will give
    you the portability to run your ML platform anywhere you want. The consistency
    it brings to the consumer of your platform is brilliant as the team can experiment
    with extremely low initial costs on the cloud and then customize the platform
    for a wider audience in your enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: The third and final reason to opt for Kubernetes is its capability to run different
    kinds of workloads. You probably remember from the previous chapter that a successful
    ML project needs not only ML but also infrastructure automation, data life cycle
    management, stateful components, and more. Kubernetes provides a consistent base
    to run diverse types of software components to create an **end-to-end** (**E2E**)
    solution for business use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the layers of an ML platform. Kubernetes provides
    the scaling and abstracting layer on which an ML platform is built. Kubernetes
    offers the freedom of abstracting the underlying infrastructure. Because of this
    flexibility, we can run on a variety of cloud providers and on-premises solutions.
    The ML platform you will build in this book allows operationalization and self-service
    in the three wider areas of an ML project—FE, model development, and DevOps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – An OSS-based ML platform'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_02_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8 – An OSS-based ML platform
  prefs: []
  type: TYPE_NORMAL
- en: 'There you go: your ML platform will be based on OSS and will use Kubernetes
    as the hosting base. The strength of the open source Kubernetes communities will
    help you use the best technologies that will evolve as the field continues to
    mature.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have defined the term *MLOps* and suggested an ML project
    life cycle that is collaborative and provides early feedback. You have learned
    that with this project life cycle, the team can continuously deliver value to
    the business. You have also learned about some of the reasons why building a platform
    based on OSS makes sense and the benefits of community-driven software.
  prefs: []
  type: TYPE_NORMAL
- en: This completes the part of the book about setting the context, learning why
    a platform is needed, and discovering what kinds of problems it is expected to
    solve. In the next chapter, we will examine some basic concepts of the Kubernetes
    system that is at the heart of our ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information regarding the topics that were covered in this chapter,
    take a look at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*DevOps: Breaking the development-operations barrier* [https://www.atlassian.com/devops](https://www.atlassian.com/devops)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
