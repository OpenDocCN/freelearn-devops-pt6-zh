<html><head></head><body>
  <div id="_idContainer200" class="Basic-Text-Frame">
    <h1 class="chapterNumber">11</h1>
    <h1 id="_idParaDest-518" class="chapterTitle">Running Kubernetes on Multiple Clusters</h1>
    <p class="normal">In this chapter, we’ll take it to the next level and consider options for running Kubernetes and deploying workloads on multiple clouds and multiple clusters. Since a single Kubernetes cluster has limits, once you exceed these limits you must run multiple clusters. A typical Kubernetes cluster is a closely-knit unit where all the components run in relative proximity and are connected by a fast network (typically, a physical data center or cloud provider availability zone). This is great for many use cases, but there are several important use cases where systems need to scale beyond a single cluster or a cluster needs to be stretched across multiple availability zones.</p>
    <p class="normal">This is a very active area in Kubernetes these days. In the previous edition of the book, this chapter covered Kubernetes Federation and Gardener. Since then, the Kubernetes Federation project was abandoned. There are now many projects that provide different flavors of multi-cluster solutions, such as direct management, Virtual Kubelet solutions, and the gardener.cloud project, which is pretty unique.</p>
    <p class="normal">The topics we will cover include the following:</p>
    <ul>
      <li class="bulletList">Stretched clusters vs multiple clusters</li>
      <li class="bulletList">The history of cluster federation in Kubernetes</li>
      <li class="bulletList">Cluster API</li>
      <li class="bulletList">Karmada</li>
      <li class="bulletList">Clusternet</li>
      <li class="bulletList">Clusterpedia</li>
      <li class="bulletList">Open Cluster Management</li>
      <li class="bulletList">Virtual Kubelet solutions</li>
      <li class="bulletList">Introduction to the Gardener project</li>
    </ul>
    <h1 id="_idParaDest-519" class="heading-1">Stretched Kubernetes clusters versus multi-cluster Kubernetes</h1>
    <p class="normal">There are several reasons to run<a id="_idIndexMarker1128"/> multiple Kubernetes clusters:</p>
    <ul>
      <li class="bulletList">You want redundancy in case the geographical zone your cluster runs in has some issues</li>
      <li class="bulletList">You need more nodes or pods than a single Kubernetes cluster supports</li>
      <li class="bulletList">You want to isolate workloads across different clusters for security reasons</li>
    </ul>
    <p class="normal">For the first reason it is possible to use a stretched cluster; for the other reasons, you must run multiple clusters.</p>
    <h2 id="_idParaDest-520" class="heading-2">Understanding stretched Kubernetes clusters</h2>
    <p class="normal">A stretched cluster (AKA wide cluster) is a single Kubernetes cluster<a id="_idIndexMarker1129"/> where the control plane nodes and the work nodes are provisioned across multiple geographical availability zones or regions. Cloud providers offer this model for HA-managed Kubernetes clusters.</p>
    <h3 id="_idParaDest-521" class="heading-3">Pros of a stretched cluster</h3>
    <p class="normal">There are several benefits<a id="_idIndexMarker1130"/> to the stretched cluster model:</p>
    <ul>
      <li class="bulletList">Your cluster, with proper redundancy, is protected from data center failures as a <strong class="keyWord">SPOF</strong> (<strong class="keyWord">single point of failure</strong>)</li>
      <li class="bulletList">The simplicity of operating against a single Kubernetes cluster is a huge win (logging, metrics, and upgrades)</li>
      <li class="bulletList">When you run your own unmanaged stretched cluster you can extend it transparently to additional locations (on-prem, edge, and other cloud providers)</li>
    </ul>
    <h3 id="_idParaDest-522" class="heading-3">Cons of a stretched cluster</h3>
    <p class="normal">However, the stretched<a id="_idIndexMarker1131"/> model has its downsides too:</p>
    <ul>
      <li class="bulletList">You can’t exceed the limits of a single Kubernetes cluster</li>
      <li class="bulletList">Degraded performance due to cross-zone networking</li>
      <li class="bulletList">On the cloud cross-zone, networking costs might be substantial</li>
      <li class="bulletList">Cluster upgrades are an all-or-nothing affair</li>
    </ul>
    <p class="normal">In short, it’s good to have the option for stretched clusters, but be prepared to switch to the multi-cluster model if some of the downsides are unacceptable.</p>
    <h2 id="_idParaDest-523" class="heading-2">Understanding multi-cluster Kubernetes</h2>
    <p class="normal">Multi-cluster Kubernetes<a id="_idIndexMarker1132"/> means provisioning multiple Kubernetes clusters. Large-scale systems often can’t be deployed on a single cluster for various reasons mentioned earlier. That means you need to provision multiple Kubernetes clusters and then figure out how to deploy your workloads on all these clusters and how to handle various use cases, such as some clusters being unavailable or having degraded performance. There are many more degrees of freedom.</p>
    <h3 id="_idParaDest-524" class="heading-3">Pros of multi-cluster Kubernetes</h3>
    <p class="normal">Here are some of the benefits <a id="_idIndexMarker1133"/>of the multi-cluster model:</p>
    <ul>
      <li class="bulletList">Scale your system arbitrarily – there are no inherent limits on the number of clusters</li>
      <li class="bulletList">Provide cluster-level isolation to sensitive workloads at the RBAC level</li>
      <li class="bulletList">Utilize multiple cloud providers without incurring excessive costs (as long as most traffic remains within the same cloud provider region)</li>
      <li class="bulletList">Upgrade and perform incremental operations, even for cluster-wide operations</li>
    </ul>
    <h3 id="_idParaDest-525" class="heading-3">Cons of multi-cluster Kubernetes</h3>
    <p class="normal">However, there are some non-trivial downsides<a id="_idIndexMarker1134"/> to the multi-cluster level:</p>
    <ul>
      <li class="bulletList">The very high complexity of provisioning and managing a fleet of clusters</li>
      <li class="bulletList">Need to figure out how to connect all the clusters</li>
      <li class="bulletList">Need to figure out how to store and provide access to data across all the clusters</li>
      <li class="bulletList">A lot of options to shoot yourself in the foot when designing multi-cluster deployments</li>
      <li class="bulletList">Need to work hard<a id="_idIndexMarker1135"/> to provide centralized observability across all clusters</li>
    </ul>
    <p class="normal">There are solutions out there for some of these problems, but at this point in time, there is no clear winner you can just adopt and easily configure for your needs. Instead, you will need to adapt and solve problems depending on the specific issues raised with your organization’s multi-cluster structure.</p>
    <h1 id="_idParaDest-526" class="heading-1">The history of cluster federation in Kubernetes</h1>
    <p class="normal">In the previous editions of the book, we discussed<a id="_idIndexMarker1136"/> Kubernetes Cluster Federation as a solution to managing multiple Kubernetes clusters as a single conceptual cluster. Unfortunately, this project has been inactive since 2019, and the Kubernetes<a id="_idIndexMarker1137"/> multi-cluster <strong class="keyWord">Special Interest Group</strong> (<strong class="keyWord">SIG</strong>) is considering archiving it. Before we describe more modern approaches let’s get some historical context. It’s funny to talk about the history of a project like Kubernetes that didn’t even exist before 2014, but the pace of development and the large number of contributors took Kubernetes through an accelerated evolution. This is especially relevant for the Kubernetes Federation.</p>
    <p class="normal">In March 2015, the first revision of the Kubernetes Cluster Federation proposal was published. It was fondly nicknamed “Ubernetes” back then. The basic idea was to reuse the existing Kubernetes<a id="_idIndexMarker1138"/> APIs to manage multiple clusters. This proposal, now called Federation V1, went through several rounds of revision and implementation but never reached general availability, and the main repo has been retired: <a href="https://github.com/kubernetes-retired/federation"><span class="url">https://github.com/kubernetes-retired/federation</span></a>.</p>
    <p class="normal">The SIG multi-cluster workgroup realized that the multi-cluster problem is more complicated than initially perceived. There are many ways to skin this particular cat and there is no one-size-fits-all solution. The new direction for cluster federation was to use dedicated APIs for federation. A new project and a set of tools were created and implemented as Kubernetes Federation V2: <a href="https://github.com/kubernetes-sigs/kubefed"><span class="url">https://github.com/kubernetes-sigs/kubefed</span></a>.</p>
    <p class="normal">Unfortunately, this didn’t take off either, and the consensus of the multi-cluster SIG is that since the project is not being maintained, it needs to be archived.</p>
    <p class="normal">See the notes for the meeting from 2022-08-09: <a href="https://tinyurl.com/sig-multicluster-notes"><span class="url">https://tinyurl.com/sig-multicluster-notes</span></a>.</p>
    <p class="normal">There are a lot of projects<a id="_idIndexMarker1139"/> out there moving fast to try to solve the multi-cluster problem, and they all operate at different levels. Let’s look at some of the prominent ones. The goal here is just to introduce these projects and what makes them unique. It is beyond the scope of this chapter to fully explore each one. However, we will dive deeper into one of the projects – the Cluster API – in <em class="chapterRef">Chapter 17</em>, <em class="italic">Running Kubernetes in Production</em>.</p>
    <h1 id="_idParaDest-527" class="heading-1">Cluster API</h1>
    <p class="normal">The Cluster API (AKA CAPI) is a project<a id="_idIndexMarker1140"/> from the Cluster Lifecycle SIG. Its goal is to make provisioning, upgrading, and operating multiple Kubernetes clusters easy. It supports both kubeadm-based clusters as well as managed clusters via dedicated providers. It has a cool logo inspired by the famous “It’s turtles all the way down” story. The idea is that the Cluster API uses Kubernetes to manage Kubernetes clusters.</p>
    <figure class="mediaobject"><img src="../Images/B18998_11_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.1: The Cluster API logo</p>
    <h2 id="_idParaDest-528" class="heading-2">Cluster API architecture</h2>
    <p class="normal">The Cluster API<a id="_idIndexMarker1141"/> has a very clean and extensible architecture. The primary components are:</p>
    <ul>
      <li class="bulletList">The management cluster</li>
      <li class="bulletList">The work cluster</li>
      <li class="bulletList">The bootstrap provider</li>
      <li class="bulletList">The infrastructure provider</li>
      <li class="bulletList">The control plane</li>
      <li class="bulletList">Custom resources</li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B18998_11_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.2: Cluster API architecture</p>
    <p class="normal">Let’s understand the<a id="_idIndexMarker1142"/> role of each one of these components and how they interact with each other.</p>
    <h2 id="_idParaDest-529" class="heading-2">Management cluster</h2>
    <p class="normal">The management cluster<a id="_idIndexMarker1143"/> is a Kubernetes cluster that is responsible<a id="_idIndexMarker1144"/> for managing other Kubernetes clusters (work clusters). It runs the Cluster API control plane and providers, and it hosts the Cluster API custom resources that represent the other clusters.</p>
    <p class="normal">The clusterctl CLI can be used to work with the management cluster. The clusterctl CLI is a command-line tool with a lot of commands and options, if you want to experiment with<a id="_idIndexMarker1145"/> the Cluster API through its CLI, visit <a href="https://cluster-api.sigs.k8s.io/clusterctl/overview.html"><span class="url">https://cluster-api.sigs.k8s.io/clusterctl/overview.html</span></a>.</p>
    <h2 id="_idParaDest-530" class="heading-2">Work cluster</h2>
    <p class="normal">A work cluster is just a regular Kubernetes<a id="_idIndexMarker1146"/> cluster. These are the clusters<a id="_idIndexMarker1147"/> that developers use to deploy their workloads. The work clusters don’t need to be aware that they are managed by the Cluster API.</p>
    <h2 id="_idParaDest-531" class="heading-2">Bootstrap provider</h2>
    <p class="normal">When CAPI creates<a id="_idIndexMarker1148"/> a new Kubernetes cluster, it needs certificates<a id="_idIndexMarker1149"/> before it can create the work cluster’s control plane and, finally, the worker nodes. This is the job of the bootstrap provider. It ensures all the requirements are met and eventually joins the worker nodes to the control plane.</p>
    <h2 id="_idParaDest-532" class="heading-2">Infrastructure provider</h2>
    <p class="normal">The infrastructure provider<a id="_idIndexMarker1150"/> is a pluggable component that allows CAPI<a id="_idIndexMarker1151"/> to work in different infrastructure environments, such as cloud providers or bare-metal infrastructure providers. The infrastructure provider implements a set of interfaces as defined by CAPI to provide access to compute and network resources.</p>
    <p class="normal">Check out the current<a id="_idIndexMarker1152"/> providers’ list here: <a href="https://cluster-api.sigs.k8s.io/reference/providers.html"><span class="url">https://cluster-api.sigs.k8s.io/reference/providers.html</span></a>.</p>
    <h2 id="_idParaDest-533" class="heading-2">Control plane</h2>
    <p class="normal">The control plane <a id="_idIndexMarker1153"/>of a Kubernetes cluster consists of the API server, the etcd stat store, the<a id="_idIndexMarker1154"/> scheduler, and the controllers that run the control loops to reconcile the resources in the cluster. The control plane of the work clusters can be provisioned in various ways. CAPI supports the following modes:</p>
    <ul>
      <li class="bulletList">Machine-based – the control plane<a id="_idIndexMarker1155"/> components are deployed as static pods on dedicated machines</li>
      <li class="bulletList">Pod-based – the control plane<a id="_idIndexMarker1156"/> components are deployed via <code class="inlineCode">Deployments</code> and <code class="inlineCode">StatefulSet</code>, and the API server is exposed as a <code class="inlineCode">Service</code></li>
      <li class="bulletList">External – the control plane is provisioned<a id="_idIndexMarker1157"/> and managed by an external provider (typically, a cloud provider)</li>
    </ul>
    <h2 id="_idParaDest-534" class="heading-2">Custom resources</h2>
    <p class="normal">The custom resources<a id="_idIndexMarker1158"/> represent the Kubernetes clusters and machines<a id="_idIndexMarker1159"/> managed by CAPI as well as additional auxiliary resources. There are a lot of custom resources, and some of them are still considered experimental. The primary CRDs are:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Cluster</code></li>
      <li class="bulletList"><code class="inlineCode">ControlPlane</code> (represents control plane machines)</li>
      <li class="bulletList"><code class="inlineCode">MachineSet</code> (represents worker machines)</li>
      <li class="bulletList"><code class="inlineCode">MachineDeployment</code></li>
      <li class="bulletList"><code class="inlineCode">Machine</code></li>
      <li class="bulletList"><code class="inlineCode">MachineHealthCheck</code></li>
    </ul>
    <p class="normal">Some of these generic resources have references to corresponding resources offered by the infrastructure provider.</p>
    <p class="normal">The following diagram illustrates the relationships between the control plane resources that represent the clusters and machine sets:</p>
    <figure class="mediaobject"><img src="../Images/B18998_11_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.3: Cluster API control plane resources</p>
    <p class="normal">CAPI also has an additional set of experimental resources<a id="_idIndexMarker1160"/> that represent a managed cloud provider environment:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">MachinePool</code></li>
      <li class="bulletList"><code class="inlineCode">ClusterResourceSet</code></li>
      <li class="bulletList"><code class="inlineCode">ClusterClass</code></li>
    </ul>
    <p class="normal">See <a href="https://github.com/kubernetes-sigs/cluster-api"><span class="url">https://github.com/kubernetes-sigs/cluster-api</span></a> for more details.</p>
    <h1 id="_idParaDest-535" class="heading-1">Karmada</h1>
    <p class="normal">Karmada is a CNCF sandbox project<a id="_idIndexMarker1161"/> that focuses on deploying and running workloads across multiple Kubernetes clusters. Its claim to fame is that you don’t need to make changes to your application configuration. While CAPI was focused on the lifecycle management of clusters, Karmada picks up when you already have a set of Kubernetes clusters and you want to deploy workloads across all of them. Conceptually, Karmada is a modern take on the abandoned Kubernetes Federation project.</p>
    <p class="normal">It can work with Kubernetes<a id="_idIndexMarker1162"/> in the cloud, on-prem, and on the edge.</p>
    <p class="normal">See <a href="https://github.com/karmada-io/karmada"><span class="url">https://github.com/karmada-io/karmada</span></a>.</p>
    <p class="normal">Let’s look at Karmada’s architecture.</p>
    <h2 id="_idParaDest-536" class="heading-2">Karmada architecture</h2>
    <p class="normal">Karmada is heavily<a id="_idIndexMarker1163"/> inspired by Kubernetes. It provides a multi-cluster control plane with similar components to the Kubernetes control plane:</p>
    <ul>
      <li class="bulletList">Karmada API server</li>
      <li class="bulletList">Karmada controller manager</li>
      <li class="bulletList">Karmada scheduler</li>
    </ul>
    <p class="normal">If you understand how Kubernetes works, then it is pretty easy to understand how Karmada extends it 1:1 to multiple clusters.</p>
    <p class="normal">The following diagram illustrates the Karmada architecture:</p>
    <figure class="mediaobject"><img src="../Images/B18998_11_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.4: Karmada architecture</p>
    <h2 id="_idParaDest-537" class="heading-2">Karmada concepts</h2>
    <p class="normal">Karmada is centered around several concepts implemented as Kubernetes CRDs. You define and update your applications and services using these concepts and Karmada ensures that your workloads are deployed and run in the right place across your multi-cluster system.</p>
    <p class="normal">Let’s look at these concepts.</p>
    <h3 id="_idParaDest-538" class="heading-3">ResourceTemplate</h3>
    <p class="normal">The resource template<a id="_idIndexMarker1164"/> looks just like a regular Kubernetes resource such as <code class="inlineCode">Deployment</code> or <code class="inlineCode">StatefulSet</code>, but it doesn’t actually get deployed to the Karmada control plane. It only serves as a blueprint that will eventually be deployed to member clusters.</p>
    <h3 id="_idParaDest-539" class="heading-3">PropagationPolicy</h3>
    <p class="normal">The propagation policy<a id="_idIndexMarker1165"/> determines where a resource template should be deployed. Here is a simple propagation policy that will place the <code class="inlineCode">nginx</code> Deployment into two clusters, called <code class="inlineCode">member1</code> and <code class="inlineCode">member2</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">policy.karmada.io/v1alpha1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PropagationPolicy</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">cool-policy</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">resourceSelectors:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
      <span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
  <span class="hljs-attr">placement:</span>
    <span class="hljs-attr">clusterAffinity:</span>
      <span class="hljs-attr">clusterNames:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">member1</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">member2</span>
</code></pre>
    <h3 id="_idParaDest-540" class="heading-3">OverridePolicy</h3>
    <p class="normal">Propagation policies operate<a id="_idIndexMarker1166"/> across multiple clusters, but sometimes, there are exceptions. The override policy lets you apply fine-grained rules to override existing propagation policies. There are several types of rules:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">ImageOverrider</code>: Dedicated to overriding images for workloads</li>
      <li class="bulletList"><code class="inlineCode">CommandOverrider</code>: Dedicated to overriding commands for workloads</li>
      <li class="bulletList"><code class="inlineCode">ArgsOverrider</code>: Dedicated to overriding args for workloads</li>
      <li class="bulletList"><code class="inlineCode">PlaintextOverrider</code>: A general-purpose tool to override any kind of resources</li>
    </ul>
    <h2 id="_idParaDest-541" class="heading-2">Additional capabilities</h2>
    <p class="normal">There is much<a id="_idIndexMarker1167"/> more to Karmada, such as:</p>
    <ul>
      <li class="bulletList">Multi-cluster de-scheduling</li>
      <li class="bulletList">Re-scheduling</li>
      <li class="bulletList">Multi-cluster failover</li>
      <li class="bulletList">Multi-cluster service discovery</li>
    </ul>
    <p class="normal">Check the Karmada documentation<a id="_idIndexMarker1168"/> for more details: <a href="https://karmada.io/docs/"><span class="url">https://karmada.io/docs/</span></a>.</p>
    <h1 id="_idParaDest-542" class="heading-1">Clusternet</h1>
    <p class="normal">Clusternet is an interesting<a id="_idIndexMarker1169"/> project. It is centered around the idea of managing multiple Kubernetes clusters as “visiting the internet” (hence the name “Clusternet”). It supports cloud-based, on-prem, edge, and hybrid clusters. The core features<a id="_idIndexMarker1170"/> of Clusternet are:</p>
    <ul>
      <li class="bulletList">Kubernetes multi-cluster management and governance</li>
      <li class="bulletList">Application coordination</li>
      <li class="bulletList">A CLI via the kubectl plugin</li>
      <li class="bulletList">Programmatic access via a wrapper to the Kubernetes Client-Go library</li>
    </ul>
    <h2 id="_idParaDest-543" class="heading-2">Clusternet architecture</h2>
    <p class="normal">The Clusternet architecture<a id="_idIndexMarker1171"/> is similar to Karmada but simpler. There is a parent cluster that runs the Clusternet hub and Clusternet scheduler. On each child cluster, there is a Clusternet agent. The following diagram illustrates the structure and interactions between the components:</p>
    <figure class="mediaobject"><img src="../Images/B18998_11_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.5: Clusternet architecture</p>
    <h3 id="_idParaDest-544" class="heading-3">Clusternet hub</h3>
    <p class="normal">The hub has<a id="_idIndexMarker1172"/> multiple roles. It is responsible for approving cluster registration requests and creating namespaces, service accounts, and RBAC resources for all child clusters. It also serves as an aggregated API server that maintains WebSocket connections to the agent on child clusters. The hub also provides a Kubernetes-like API to proxy requests to each child cluster. Last but not least, the hub coordinates the deployment of applications and their dependencies to multiple clusters from a single set of resources.</p>
    <h3 id="_idParaDest-545" class="heading-3">Clusternet scheduler</h3>
    <p class="normal">The Clusternet scheduler<a id="_idIndexMarker1173"/> is the component that is responsible for ensuring<a id="_idIndexMarker1174"/> that resources (called feeds in Clusternet terminology) are deployed and balanced<a id="_idIndexMarker1175"/> across all the child clusters according to policies called <code class="inlineCode">SchedulingStrategy</code>.</p>
    <h3 id="_idParaDest-546" class="heading-3">Clusternet agent</h3>
    <p class="normal">The Clusternet agent runs<a id="_idIndexMarker1176"/> on every child cluster and communicates with the hub. The agent on a child cluster is the equivalent of the kubelet on a node. It has several roles. The agent registers its child cluster with the parent cluster. The agent provides a heartbeat to the hub that includes a lot of information, such as the Kubernetes version, running platform, health, readiness, and liveness of workloads. The agent also sets up the WebSocket connection to the hub on the parent cluster to allow full-duplex communication channels over a single TCP connection.</p>
    <h2 id="_idParaDest-547" class="heading-2">Multi-cluster deployment</h2>
    <p class="normal">Clusternet models multi-cluster deployment<a id="_idIndexMarker1177"/> as subscriptions and feeds. It provides a <code class="inlineCode">Subscription</code> custom resource that can be used to deploy<a id="_idIndexMarker1178"/> a set of<a id="_idIndexMarker1179"/> resources (called feeds) to multiple clusters (called subscribers) based on different criteria. Here is an example of a <code class="inlineCode">Subscription</code> that deploys a <code class="inlineCode">Namespace</code>, a <code class="inlineCode">Service</code>, and a <code class="inlineCode">Deployment</code> to multiple clusters with a label of <code class="inlineCode">clusters.clusternet.io/cluster-id</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># examples/dynamic-dividing-scheduling/subscription.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps.clusternet.io/v1alpha1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Subscription</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">dynamic-dividing-scheduling-demo</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">subscribers:</span> <span class="hljs-comment"># filter out a set of desired clusters</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">clusterAffinity:</span>
        <span class="hljs-attr">matchExpressions:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">clusters.clusternet.io/cluster-id</span>
            <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>
  <span class="hljs-attr">schedulingStrategy:</span> <span class="hljs-string">Dividing</span>
  <span class="hljs-attr">dividingScheduling:</span>
    <span class="hljs-attr">type:</span> <span class="hljs-string">Dynamic</span>
    <span class="hljs-attr">dynamicDividing:</span>
      <span class="hljs-attr">strategy:</span> <span class="hljs-string">Spread</span> <span class="hljs-comment"># currently we only support Spread dividing strategy</span>
  <span class="hljs-attr">feeds:</span> <span class="hljs-comment"># defines all the resources to be deployed with</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
      <span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">qux</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
      <span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">my-nginx-svc</span>
      <span class="hljs-attr">namespace:</span> <span class="hljs-string">qux</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span> <span class="hljs-comment"># with a total of 6 replicas</span>
      <span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">my-nginx</span>
      <span class="hljs-attr">namespace:</span> <span class="hljs-string">qux</span>
</code></pre>
    <p class="normal">See <a href="https://clusternet.io"><span class="url">https://clusternet.io</span></a> for more<a id="_idIndexMarker1180"/> details.</p>
    <h1 id="_idParaDest-548" class="heading-1">Clusterpedia</h1>
    <p class="normal">Clusterpedia is a CNCF sandbox project. Its central<a id="_idIndexMarker1181"/> metaphor is Wikipedia for Kubernetes clusters. It has a lot of capabilities around multi-cluster search, filtering, field selection, and sorting. This is unusual because it is a read-only project. It doesn’t offer to help with managing the clusters or deploying workloads. It is focused on observing your clusters.</p>
    <h2 id="_idParaDest-549" class="heading-2">Clusterpedia architecture</h2>
    <p class="normal">The architecture<a id="_idIndexMarker1182"/> is similar to other multi-cluster projects. There is a control plane element that runs the Clusterpedia API server and ClusterSynchro manager components. For each observed cluster, there is a dedicated component called cluster syncro that synchronizes the state<a id="_idIndexMarker1183"/> of the clusters into the storage layer of Clusterpedia. One of the most interesting aspects of the architecture is the Clusterpedia aggregated API server, which makes all your clusters seem like a single huge logical cluster. Note that the Clusterpedia API server and the ClusterSynchro manager are loosely coupled and don’t interact directly with each other. They just read and write from a shared storage layer.</p>
    <figure class="mediaobject"><img src="../Images/B18998_11_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.6: Clusterpedia architecture</p>
    <p class="normal">Let’s look at each of the components and understand what their purpose is.</p>
    <h3 id="_idParaDest-550" class="heading-3">Clusterpedia API server</h3>
    <p class="normal">The Clusterpedia API server<a id="_idIndexMarker1184"/> is an aggregated API server. That means that it registers itself with the Kubernetes API server and, in practice, extends the standard Kubernetes API server via custom endpoints. When requests come to the Kubernetes API server, it forwards them to the Clusterpedia API server, which accesses the storage layer to satisfy them. The Kubernetes API server serves as a forwarding layer for the requests that Clusterpedia handles.</p>
    <p class="normal">This is an advanced aspect of Kubernetes. We will discuss API server aggregation in <em class="chapterRef">Chapter 15</em>, <em class="italic">Extending Kubernetes</em>.</p>
    <h3 id="_idParaDest-551" class="heading-3">ClusterSynchro manager</h3>
    <p class="normal">Clusterpedia observes multiple<a id="_idIndexMarker1185"/> clusters to provide its search, filter, and aggregation features. One way to implement it is that whenever a request comes in, Clusterpedia would query all the observed clusters, collect the results, and return them. This approach is very problematic, as some clusters might be slow to respond and similar requests will require returning the same information, which is wasteful and costly. Instead, the ClusterSynchro manager collectively synchronizes the state of each observed cluster into Clusterpedia storage, where the Clusterpedia API server can respond quickly.</p>
    <h3 id="_idParaDest-552" class="heading-3">Storage layer</h3>
    <p class="normal">The storage layer<a id="_idIndexMarker1186"/> is an abstraction layer that stores the state of all observed clusters. It provides a uniform interface that can be implemented by different storage components. The Clusterpedia API server and the ClusterSynchro manager interact with the storage layer interface and never talk to each other directly.</p>
    <h3 id="_idParaDest-553" class="heading-3">Storage component</h3>
    <p class="normal">The storage component is an actual data<a id="_idIndexMarker1187"/> store that implements the storage layer interface and stores the state of observed clusters. Clusterpedia was designed to support different storage components to provide flexibility for their users. Currently, supported storage components include MySQL, Postgres, and Redis.</p>
    <h2 id="_idParaDest-554" class="heading-2">Importing clusters</h2>
    <p class="normal">To onboard clusters<a id="_idIndexMarker1188"/> into Clusterpedia, you define a PediaCluster custom resource. It is pretty straightforward:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">cluster.clusterpedia.io/v1alpha2</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PediaCluster</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">cluster-example</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">apiserver:</span> <span class="hljs-string">"https://10.30.43.43:6443"</span>
  <span class="hljs-attr">kubeconfig:</span>
  <span class="hljs-attr">caData:</span>
  <span class="hljs-attr">tokenData:</span>
  <span class="hljs-attr">certData:</span>
  <span class="hljs-attr">keyData:</span>
  <span class="hljs-attr">syncResources:</span> []
</code></pre>
    <p class="normal">You need to provide credentials to access the cluster, and then Clusterpedia will take over and sync its state.</p>
    <h2 id="_idParaDest-555" class="heading-2">Advanced multi-cluster search</h2>
    <p class="normal">This is where Clusterpedia<a id="_idIndexMarker1189"/> shines. You can access the Clusterpedia cluster via an API or through kubectl. When accessing it through a URL it looks like you hit the aggregated API server endpoint:</p>
    <pre class="programlisting gen"><code class="hljs">kubectl get --raw="/apis/clusterpedia.io/v1beta1/resources/apis/apps/v1/deployments?clusters=cluster-1,cluster-2"
</code></pre>
    <p class="normal">You can specify the target cluster as a query parameter (in this case, <code class="inlineCode">cluster-1</code> and <code class="inlineCode">cluster-2</code>).</p>
    <p class="normal">When accessing through kubectl, you specify the target clusters as a label (in this case, <code class="inlineCode">"search.clusterpedia.io/clusters in (cluster-1,cluster-2)"</code>):</p>
    <pre class="programlisting gen"><code class="hljs">kubectl --cluster clusterpedia get deployments -l "search.clusterpedia.io/clusters in (cluster-1,cluster-2)"
</code></pre>
    <p class="normal">Other search labels and queries exist for namespaces and resource names:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">search.clusterpedia.io/namespaces</code> (query parameter is <code class="inlineCode">namespaces</code>)</li>
      <li class="bulletList"><code class="inlineCode">search.clusterpedia.io/names</code> (query parameter is <code class="inlineCode">names</code>)</li>
    </ul>
    <p class="normal">There is also an experimental fuzzy search label, <code class="inlineCode">internalstorage.clusterpedia.io/fuzzy-name</code>, for resource names, but no query parameter. This is useful as often, resources have generated names with random suffixes.</p>
    <p class="normal">You can also search by creation time:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">search.clusterpedia.io/before</code> (query parameter is <code class="inlineCode">before</code>)</li>
      <li class="bulletList"><code class="inlineCode">search.clusterpedia.io/since</code> (query parameter is <code class="inlineCode">since</code>)</li>
    </ul>
    <p class="normal">Other capabilities<a id="_idIndexMarker1190"/> include filtering by resource labels or field selectors as well as organizing the results using <code class="inlineCode">OrderBy</code> and <code class="inlineCode">Paging</code>.</p>
    <h2 id="_idParaDest-556" class="heading-2">Resource collections</h2>
    <p class="normal">Another important concept<a id="_idIndexMarker1191"/> is resource collections. The standard Kubernetes API offers a straightforward REST API where you can list or get one kind of resource at a time. However, often, users would like to get multiple types of resources at the same time. For example, the <code class="inlineCode">Deployment</code>, <code class="inlineCode">Service</code>, and <code class="inlineCode">HorizontalPodAutoscaler</code> with a specific label. This requires multiple calls via the standard Kubernetes API, even if all these resources are available on one cluster.</p>
    <p class="normal">Clusterpedia defines a <code class="inlineCode">CollectionResource</code> that groups together resources that belong to the following categories:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">any</code> (all resources)</li>
      <li class="bulletList"><code class="inlineCode">workloads</code> (<code class="inlineCode">Deployments</code>, <code class="inlineCode">StatefulSets</code>, and <code class="inlineCode">DaemonSets</code>)</li>
      <li class="bulletList"><code class="inlineCode">kuberesources</code> (all resources other than workloads)</li>
    </ul>
    <p class="normal">You can search for any combination of resources in one API call by passing API groups and resource kinds:</p>
    <pre class="programlisting gen"><code class="hljs">kubectl get --raw "/apis/clusterpedia.io/v1beta1/collectionresources/any?onlyMetadata=true&amp;groups=apps&amp;resources=batch/jobs,batch/cronjobs"
</code></pre>
    <p class="normal">See <a href="https://github.com/clusterpedia-io/clusterpedia"><span class="url">https://github.com/clusterpedia-io/clusterpedia</span></a> for more details.</p>
    <h1 id="_idParaDest-557" class="heading-1">Open Cluster Management</h1>
    <p class="normal"><strong class="keyWord">Open Cluster Management</strong> (<strong class="keyWord">OCM</strong>) is a CNCF sandbox project<a id="_idIndexMarker1192"/> for multi-cluster management, as well as multi-cluster scheduling and workload placement. Its claim to fame is closely following many Kubernetes concepts, extensibility via addons, and strong integration with other open source projects, such as:</p>
    <ul>
      <li class="bulletList">Submariner</li>
      <li class="bulletList">Clusternet (that we covered earlier)</li>
      <li class="bulletList">KubeVela</li>
    </ul>
    <p class="normal">The scope of OCM covers cluster lifecycle, application lifecycle, and governance.</p>
    <p class="normal">Let’s look at OCM’s architecture.</p>
    <h2 id="_idParaDest-558" class="heading-2">OCM architecture</h2>
    <p class="normal">OCM’s architecture<a id="_idIndexMarker1193"/> follows the hub and spokes model. It has a hub cluster, which is the OCM control plane that manages multiple other clusters (the spokes).</p>
    <p class="normal">The control plane’s hub cluster runs two controllers: the registration controller and the placement controller. In addition, the control plane runs multiple management addons, which are the foundation for OCM’s extensibility. On each managed cluster, there is a so-called Klusterlet that has a registration-agent<a id="_idIndexMarker1194"/> and work-agent that interact with the registration controller and placement controller on the hub cluster. Then, there are also addon agents that interact with the addons on the hub cluster.</p>
    <p class="normal">The following diagram illustrates how the different components of OCM communicate:</p>
    <figure class="mediaobject"><img src="../Images/B18998_11_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.7: OCM architecture</p>
    <p class="normal">Let’s look at the different aspects of OCM.</p>
    <h2 id="_idParaDest-559" class="heading-2">OCM cluster lifecycle</h2>
    <p class="normal">Cluster registration<a id="_idIndexMarker1195"/> is a big part of OCM’s secure multi-cluster story. OCM prides itself on the secure double opt-in handshake registration. Since a hub-and-spoke cluster may have different administrators, this model provides protection for each side from undesired requests. Each side can terminate the relationship at any time.</p>
    <p class="normal">The following diagram demonstrates<a id="_idIndexMarker1196"/> the registration process (CSR means certificate signing request):</p>
    <figure class="mediaobject"><img src="../Images/B18998_11_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.8: OCM registration process</p>
    <h2 id="_idParaDest-560" class="heading-2">OCM application lifecycle</h2>
    <p class="normal">The OCM application lifecycle<a id="_idIndexMarker1197"/> supports creating, updating, and deleting resources across multiple clusters.</p>
    <p class="normal">The primary building block is the <code class="inlineCode">ManifestWork</code> custom resource that can define multiple resources. Here is an example that contains only a single <code class="inlineCode">Deployment</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">work.open-cluster-management.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ManifestWork</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">&lt;target</span> <span class="hljs-string">managed</span> <span class="hljs-string">cluster&gt;</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">awesome-workload</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">workload:</span>
    <span class="hljs-attr">manifests:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
        <span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
        <span class="hljs-attr">metadata:</span>
          <span class="hljs-attr">name:</span> <span class="hljs-string">hello</span>
          <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>
        <span class="hljs-attr">spec:</span>
          <span class="hljs-attr">selector:</span>
            <span class="hljs-attr">matchLabels:</span>
              <span class="hljs-attr">app:</span> <span class="hljs-string">hello</span>
          <span class="hljs-attr">template:</span>
            <span class="hljs-attr">metadata:</span>
              <span class="hljs-attr">labels:</span>
                <span class="hljs-attr">app:</span> <span class="hljs-string">hello</span>
            <span class="hljs-attr">spec:</span>
              <span class="hljs-attr">containers:</span>
                <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">hello</span>
                  <span class="hljs-attr">image:</span> <span class="hljs-string">quay.io/asmacdo/busybox</span>
                  <span class="hljs-attr">command:</span>
                    [<span class="hljs-string">"sh"</span>, <span class="hljs-string">"-c"</span>, <span class="hljs-string">'echo "Hello, Kubernetes!" &amp;&amp; sleep 3600'</span>]
</code></pre>
    <p class="normal">The <code class="inlineCode">ManifestWork</code> is created on the hub cluster and is deployed to the target cluster according to the namespace mapping. Each target cluster has a namespace representing it in the hub cluster. A work agent<a id="_idIndexMarker1198"/> running on the target cluster will monitor all <code class="inlineCode">ManifestWork</code> resources on the hub cluster in their namespace and sync changes.</p>
    <h2 id="_idParaDest-561" class="heading-2">OCM governance, risk, and compliance</h2>
    <p class="normal">OCM provides a governance<a id="_idIndexMarker1199"/> model based on policies, policy templates, and policy controllers. The policies can be bound to a specific set of clusters for fine-grained control.</p>
    <p class="normal">Here is a sample policy that requires the existence of a namespace called <code class="inlineCode">Prod</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">policy.open-cluster-management.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Policy</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">policy-namespace</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">policies</span>
  <span class="hljs-attr">annotations:</span>
    <span class="hljs-attr">policy.open-cluster-management.io/standards:</span> <span class="hljs-string">NIST</span> <span class="hljs-string">SP</span> <span class="hljs-number">800-53</span>
    <span class="hljs-attr">policy.open-cluster-management.io/categories:</span> <span class="hljs-string">CM</span> <span class="hljs-string">Configuration</span> <span class="hljs-string">Management</span>
    <span class="hljs-attr">policy.open-cluster-management.io/controls:</span> <span class="hljs-string">CM-2</span> <span class="hljs-string">Baseline</span> <span class="hljs-string">Configuration</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">remediationAction:</span> <span class="hljs-string">enforce</span>
  <span class="hljs-attr">disabled:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">policy-templates:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">objectDefinition:</span>
        <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">policy.open-cluster-management.io/v1</span>
        <span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigurationPolicy</span>
        <span class="hljs-attr">metadata:</span>
          <span class="hljs-attr">name:</span> <span class="hljs-string">policy-namespace-example</span>
        <span class="hljs-attr">spec:</span>
          <span class="hljs-attr">remediationAction:</span> <span class="hljs-string">inform</span>
          <span class="hljs-attr">severity:</span> <span class="hljs-string">low</span>
          <span class="hljs-attr">object-templates:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">complianceType:</span> <span class="hljs-string">MustHave</span>
              <span class="hljs-attr">objectDefinition:</span>
                <span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span> <span class="hljs-comment"># must have namespace 'prod'</span>
                <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
                <span class="hljs-attr">metadata:</span>
                  <span class="hljs-attr">name:</span> <span class="hljs-string">prod</span>
</code></pre>
    <p class="normal">See <a href="https://open-cluster-management.io/"><span class="url">https://open-cluster-management.io/</span></a> for more<a id="_idIndexMarker1200"/> details.</p>
    <h1 id="_idParaDest-562" class="heading-1">Virtual Kubelet</h1>
    <p class="normal">Virtual Kubelet is a fascinating project. It impersonates<a id="_idIndexMarker1201"/> a kubelet to connect Kubernetes to other APIs such as AWS Fargate or Azure ACI. The Virtual Kubelet looks like a node to the Kubernetes cluster, but the compute resources backing it up are abstracted away. The Virtual Kubelet looks like just another node to the Kubernetes cluster:</p>
    <figure class="mediaobject"><img src="../Images/B18998_11_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.9: Virtual Kubelet, which looks like a regular node to the Kubernetes cluster</p>
    <p class="normal">The features<a id="_idIndexMarker1202"/> of the Virtual Kubelet are:</p>
    <ul>
      <li class="bulletList">Creating, updating, and deleting pods</li>
      <li class="bulletList">Accessing container logs and metrics</li>
      <li class="bulletList">Getting a pod, pods, and pod status</li>
      <li class="bulletList">Managing capacity</li>
      <li class="bulletList">Accessing node addresses, node capacity, and node daemon endpoints</li>
      <li class="bulletList">Choosing the operating system</li>
      <li class="bulletList">Supporting your own virtual network</li>
    </ul>
    <p class="normal">See <a href="https://github.com/virtual-kubelet/virtual-kubelet"><span class="url">https://github.com/virtual-kubelet/virtual-kubelet</span></a> for more details.</p>
    <p class="normal">This concept can be used to connect multiple Kubernetes clusters too, and several projects follow this approach. Let’s look briefly at some projects that use Virtual Kubelet for multi-cluster management such as tensile-kube, Admiralty, and Liqo.</p>
    <h2 id="_idParaDest-563" class="heading-2">Tensile-kube</h2>
    <p class="normal">Tensile-kube<a id="_idIndexMarker1203"/> is a sub-project<a id="_idIndexMarker1204"/> of the Virtual Kubelet organization on GitHub.</p>
    <p class="normal">Tensile-kube brings<a id="_idIndexMarker1205"/> the following to the table:</p>
    <ul>
      <li class="bulletList">Automatic discovery of cluster resources</li>
      <li class="bulletList">Async notification of pod modifications</li>
      <li class="bulletList">Full access to pod logs and kubectl exec</li>
      <li class="bulletList">Global scheduling of pods</li>
      <li class="bulletList">Re-scheduling of pods using descheduler</li>
      <li class="bulletList">PV/PVC</li>
      <li class="bulletList">Service</li>
    </ul>
    <p class="normal">Tensile-kube uses the terminology of the upper cluster for the cluster that contains the Virtual Kubelets, and the lower clusters for the clusters that are exposed as virtual nodes in the upper cluster.</p>
    <p class="normal">Here is the<a id="_idIndexMarker1206"/> tensile-kube architecture:</p>
    <figure class="mediaobject"><img src="../Images/B18998_11_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.10: Tensile-kube architecture</p>
    <p class="normal">See <a href="https://github.com/virtual-kubelet/tensile-kube"><span class="url">https://github.com/virtual-kubelet/tensile-kube</span></a> for more details.</p>
    <h2 id="_idParaDest-564" class="heading-2">Admiralty</h2>
    <p class="normal">Admiralty is an<a id="_idIndexMarker1207"/> open source project<a id="_idIndexMarker1208"/> backed by a commercial company. Admiralty takes the Virtual Kubelet concept and builds a sophisticated solution for multi-cluster orchestration and scheduling. Target clusters are represented as virtual nodes in the source cluster. It has a pretty complicated architecture that involves three levels of scheduling. Whenever a pod is created on a proxy, pods are created on the source cluster, candidate pods are created on each target cluster, and eventually, one of the candidate pods is selected and becomes a delegate pod, which is a real pod that actually runs its containers. This is all supported by custom multi-cluster schedulers built on top of the Kubernetes scheduling framework. To schedule workloads on Admiralty, you need to annotate any pod template with <code class="inlineCode">multicluster.admiralty.io/elect=""</code> and Admiralty will take it from there.</p>
    <p class="normal">Here is a diagram that demonstrates<a id="_idIndexMarker1209"/> the interplay between different <a id="_idIndexMarker1210"/>components:</p>
    <figure class="mediaobject"><img src="../Images/B18998_11_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.11: Admiralty architecture</p>
    <p class="normal">Admiralty provides<a id="_idIndexMarker1211"/> the following features:</p>
    <ul>
      <li class="bulletList">Highly available</li>
      <li class="bulletList">Live disaster recovery</li>
      <li class="bulletList">Dynamic <strong class="keyWord">CDN</strong> (<strong class="keyWord">content delivery network</strong>)</li>
      <li class="bulletList">Multi-cluster workflows</li>
      <li class="bulletList">Support for edge computing, IoT, and 5G</li>
      <li class="bulletList">Governance</li>
      <li class="bulletList">Cluster upgrades</li>
      <li class="bulletList">Clusters as cattle abstraction</li>
      <li class="bulletList">Global resource federation</li>
      <li class="bulletList">Cloud bursting and arbitrage</li>
    </ul>
    <p class="normal">See <a href="https://admiralty.io"><span class="url">https://admiralty.io</span></a> for more<a id="_idIndexMarker1212"/> details.</p>
    <h2 id="_idParaDest-565" class="heading-2">Liqo</h2>
    <p class="normal">Liqo is an open source project<a id="_idIndexMarker1213"/> based on the liquid<a id="_idIndexMarker1214"/> computing concept. Let your tasks and data float around and find the best place to run. Its scope is very impressive, as it targets not only the compute aspect of running pods across multiple clusters but also provides network fabric and storage fabric. These aspects of connecting clusters and managing data across clusters are often harder problems to solve than just running workloads.</p>
    <p class="normal">In Liqo’s terminology, the management cluster is called the home cluster and the target clusters are called foreign clusters. The virtual nodes in the home cluster are called “Big” nodes, and they represent the foreign clusters.</p>
    <p class="normal">Liqo utilizes IP address mapping to achieve a flat IP address space across all foreign clusters that may have internal IP conflicts.</p>
    <p class="normal">Liqo filters and batches events from the foreign clusters to reduce pressure on the home cluster.</p>
    <p class="normal">Here is a diagram of the Liqo <a id="_idIndexMarker1215"/>architecture:</p>
    <figure class="mediaobject"><img src="../Images/B18998_11_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.12: Liqo architecture</p>
    <p class="normal">See <a href="https://liqo.io"><span class="url">https://liqo.io</span></a> for more<a id="_idIndexMarker1216"/> details.</p>
    <p class="normal">Let’s move on and take an in-depth look at the Gardener project, which takes a different approach.</p>
    <h1 id="_idParaDest-566" class="heading-1">Introducing the Gardener project</h1>
    <p class="normal">The Gardener project<a id="_idIndexMarker1217"/> is an open source project developed by SAP. It lets you manage thousands (yes, thousands!) of Kubernetes clusters efficiently and economically. Gardener solves a very complex problem, and the solution is elegant but not simple. Gardener is the only project that addresses both the cluster lifecycle and application lifecycle.</p>
    <p class="normal">In this section, we will cover the terminology of Gardener and its conceptual model, dive deep into its architecture, and learn about its extensibility features. The primary theme of Gardener is to use Kubernetes to manage Kubernetes clusters. A good way to think about Gardener is Kubernetes-control-plane-as-a-service.</p>
    <p class="normal">See <a href="https://gardener.cloud"><span class="url">https://gardener.cloud</span></a> for more<a id="_idIndexMarker1218"/> details.</p>
    <h2 id="_idParaDest-567" class="heading-2">Understanding the terminology of Gardener</h2>
    <p class="normal">The Gardener project, as you may have guessed, uses botanical terminology<a id="_idIndexMarker1219"/> to describe the world. There is a garden, which is a Kubernetes cluster responsible for managing seed clusters. A seed is a Kubernetes cluster responsible for managing a set of shoot clusters. A shoot cluster is a Kubernetes cluster that runs actual workloads. </p>
    <p class="normal">The cool idea behind Gardener is that the shoot clusters contain only the worker nodes. The control planes of all the shoot clusters run as Kubernetes pods and services in the seed cluster.</p>
    <p class="normal">The following diagram describes in detail the structure<a id="_idIndexMarker1220"/> of Gardener and the relationships between its components:</p>
    <figure class="mediaobject"><img src="../Images/B18998_11_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.13: The Gardener project structure</p>
    <p class="normal">Don’t panic! Underlying all this complexity is a crystal clear conceptual model.</p>
    <h2 id="_idParaDest-568" class="heading-2">Understanding the conceptual model of Gardener</h2>
    <p class="normal">The architecture diagram of Gardener<a id="_idIndexMarker1221"/> can be overwhelming. Let’s unpack it slowly and surface the underlying principles. Gardener really embraces the spirit of Kubernetes and offloads a lot of the complexity of managing a large set of Kubernetes clusters to Kubernetes itself. At its heart, Gardener is an aggregated API server that manages a set of custom resources using various controllers. It embraces and takes full advantage of Kubernetes’ extensibility. This approach is common in the Kubernetes community. Define a set of custom resources and let Kubernetes manage them for you. The novelty of Gardener is that it takes this approach to the extreme and abstracts away parts of Kubernetes infrastructure itself.</p>
    <p class="normal">In a “normal” Kubernetes cluster, the control plane runs in the same cluster as the worker nodes. Typically, in large clusters, control plane components like the Kubernetes API server and etcd run on dedicated nodes and don’t mix up with the worker nodes. Gardener thinks in terms of many clusters and it takes all the control planes of all the shoot clusters and has a seed cluster to manage them. So the Kubernetes control plane of the shoot clusters is managed in the seed cluster as regular Kubernetes <code class="inlineCode">Deployments</code>, which automatically provides replication, monitoring, self-healing, and rolling updates by Kubernetes.</p>
    <p class="normal">So, the control plane of a Kubernetes shoot cluster is analogous to a <code class="inlineCode">Deployment</code>. The seed cluster, on the other hand, maps to a Kubernetes node. It manages multiple shoot clusters. It is recommended to have a seed cluster per cloud provider. The Gardener developers actually work on a gardenlet controller for seed clusters that is similar to the kubelet on nodes.</p>
    <p class="normal">If the seed clusters are like Kubernetes nodes, then the Garden cluster that manages those seed clusters is like a Kubernetes cluster that manages its worker nodes.</p>
    <p class="normal">By pushing the Kubernetes model this far, the Gardener project leverages the strengths of Kubernetes to achieve robustness<a id="_idIndexMarker1222"/> and performance that would be very difficult to build from scratch.</p>
    <p class="normal">Let’s dive into the architecture.</p>
    <h2 id="_idParaDest-569" class="heading-2">Diving into the Gardener architecture</h2>
    <p class="normal">Gardener creates a Kubernetes<a id="_idIndexMarker1223"/> namespace in the seed cluster for each shoot cluster. It manages the certificates of the shoot clusters as Kubernetes secrets in the seed cluster.</p>
    <h3 id="_idParaDest-570" class="heading-3">Managing the cluster state</h3>
    <p class="normal">The etcd data store<a id="_idIndexMarker1224"/> for each cluster is deployed as a StatefulSet with one replica. In addition, events are stored in a separate etcd instance. The etcd data is periodically snapshotted and stored in remote storage for backup and restore purposes. This enables very fast recovery of clusters that lost their control plane (e.g., when an entire seed cluster becomes unreachable). Note that when a seed cluster goes down, the shoot cluster continues to run as usual.</p>
    <h3 id="_idParaDest-571" class="heading-3">Managing the control plane</h3>
    <p class="normal">As mentioned before, the control plane<a id="_idIndexMarker1225"/> of a shoot cluster X runs in a separate seed cluster, while the worker nodes run in a shoot cluster. This means that pods in the shoot cluster can use internal DNS to locate each other, but communication to the Kubernetes API server running in the seed cluster must be done through an external DNS. This means the Kubernetes API server runs as a <code class="inlineCode">Service</code> of the <code class="inlineCode">LoadBalancer</code> type.</p>
    <h3 id="_idParaDest-572" class="heading-3">Preparing the infrastructure</h3>
    <p class="normal">When creating a new<a id="_idIndexMarker1226"/> shoot cluster, it’s important to provide the necessary infrastructure. Gardener uses Terraform for this task. A Terraform script is dynamically generated based on the shoot cluster specification and stored as a ConfigMap within the seed cluster. To facilitate this process, a dedicated component (Terraformer) runs as a job, performs all the provisioning, and then writes the state into a separate ConfigMap.</p>
    <h3 id="_idParaDest-573" class="heading-3">Using the Machine controller manager</h3>
    <p class="normal">To provision nodes in a provider-agnostic<a id="_idIndexMarker1227"/> manner that can work for private clouds too, Gardener has several custom resources such as <code class="inlineCode">MachineDeployment</code>, <code class="inlineCode">MachineClass</code>, <code class="inlineCode">MachineSet</code>, and <code class="inlineCode">Machine</code>. They work with the Kubernetes Cluster Lifecycle group to unify their abstractions because there is a lot of overlap. In addition, Gardener takes advantage of the cluster auto-scaler to offload the complexity of scaling node pools up and down.</p>
    <h3 id="_idParaDest-574" class="heading-3">Networking across clusters</h3>
    <p class="normal">The seed cluster and shoot clusters can run on<a id="_idIndexMarker1228"/> different cloud providers. The worker nodes in the shoot clusters are often deployed in private networks. Since the control plane needs to interact closely with the worker nodes (mostly the kubelet), Gardener creates a VPN for direct communication.</p>
    <h3 id="_idParaDest-575" class="heading-3">Monitoring clusters</h3>
    <p class="normal">Observability is a big part of operating complex<a id="_idIndexMarker1229"/> distributed systems. Gardener provides a lot of monitoring out of the box using best-of-class open source projects like a central Prometheus server, deployed in the garden cluster that collects information about all seed clusters. In addition, each shoot cluster gets its own Prometheus instance in the seed cluster. To collect metrics, Gardener deploys two <code class="inlineCode">kube-state-metrics</code> instances for each cluster (one for the control plane in the seed and one for the worker nodes in the shoot). The node-exporter is deployed too to provide additional information on the nodes. The Prometheus <code class="inlineCode">AlertManager</code> is used to notify the operator when something goes wrong. Grafana is used to display dashboards with relevant data on the state of the system.</p>
    <h3 id="_idParaDest-576" class="heading-3">The gardenctl CLI</h3>
    <p class="normal">You can manage Gardener<a id="_idIndexMarker1230"/> using only kubectl, but you will have to switch profiles and contexts a lot as you explore different clusters. Gardener provides the <code class="inlineCode">gardenctl</code> command-line tool that offers higher-level abstractions and can operate on multiple clusters at the same time. Here is an example:</p>
    <pre class="programlisting gen"><code class="hljs">$ gardenctl ls shoots
projects:
- project: team-a
  shoots:
  - dev-eu1
  - prod-eu1
$ gardenctl target shoot prod-eu1
[prod-eu1]
$ gardenctl show prometheus
NAME           READY     STATUS    RESTARTS   AGE       IP              NODE
prometheus-0   3/3       Running   0          106d      10.241.241.42   ip-10-240-7-72.eu-central-1.compute.internal
URL: https://user:password@p.prod-eu1.team-a.seed.aws-eu1.example.com
</code></pre>
    <p class="normal">One of the most prominent features of Gardener is its extensibility. It has a large surface area and it supports many environments. Let’s see how extensibility is built into its design.</p>
    <h2 id="_idParaDest-577" class="heading-2">Extending Gardener</h2>
    <p class="normal">Gardener supports<a id="_idIndexMarker1231"/> the following environments:</p>
    <ul>
      <li class="bulletList">AliCloud</li>
      <li class="bulletList">AWS</li>
      <li class="bulletList">Azure</li>
      <li class="bulletList">Equinix Metal</li>
      <li class="bulletList">GCP</li>
      <li class="bulletList">OpenStack</li>
      <li class="bulletList">vSphere</li>
    </ul>
    <p class="normal">It started, like Kubernetes itself, with a lot of provider-specific support in the primary Gardener repository. Over time, it followed the Kubernetes example that externalized cloud providers and migrated the providers to separate Gardener extensions. Providers can be specified<a id="_idIndexMarker1232"/> using a CloudProfile CRD such as:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">core.gardener.cloud/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">CloudProfile</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">aws</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">type:</span> <span class="hljs-string">aws</span>
  <span class="hljs-attr">kubernetes:</span>
    <span class="hljs-attr">versions:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">version:</span> <span class="hljs-number">1.24.3</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">version:</span> <span class="hljs-number">1.23.8</span>
      <span class="hljs-attr">expirationDate:</span> <span class="hljs-string">"2022-10-31T23:59:59Z"</span>
  <span class="hljs-attr">machineImages:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">coreos</span>
    <span class="hljs-attr">versions:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">version:</span> <span class="hljs-number">2135.6.0</span>
  <span class="hljs-attr">machineTypes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">m5.large</span>
    <span class="hljs-attr">cpu:</span> <span class="hljs-string">"2"</span>
    <span class="hljs-attr">gpu:</span> <span class="hljs-string">"0"</span>
    <span class="hljs-attr">memory:</span> <span class="hljs-string">8Gi</span>
    <span class="hljs-attr">usable:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">volumeTypes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">gp2</span>
    <span class="hljs-attr">class:</span> <span class="hljs-string">standard</span>
    <span class="hljs-attr">usable:</span> <span class="hljs-literal">true</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">io1</span>
    <span class="hljs-attr">class:</span> <span class="hljs-string">premium</span>
    <span class="hljs-attr">usable:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">regions:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">eu-central-1</span>
    <span class="hljs-attr">zones:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">eu-central-1a</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">eu-central-1b</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">eu-central-1c</span>
  <span class="hljs-attr">providerConfig:</span>
    <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">aws.provider.extensions.gardener.cloud/v1alpha1</span>
    <span class="hljs-attr">kind:</span> <span class="hljs-string">CloudProfileConfig</span>
    <span class="hljs-attr">machineImages:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">coreos</span>
      <span class="hljs-attr">versions:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">version:</span> <span class="hljs-number">2135.6.0</span>
        <span class="hljs-attr">regions:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">eu-central-1</span>
          <span class="hljs-attr">ami:</span> <span class="hljs-string">ami-034fd8c3f4026eb39</span>
          <span class="hljs-comment"># architecture: amd64 # optional</span>
</code></pre>
    <p class="normal">Then, a shoot cluster will choose<a id="_idIndexMarker1233"/> a provider and configure it with the necessary information:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">gardener.cloud/v1alpha1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Shoot</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">johndoe-aws</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">garden-dev</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">cloudProfileName:</span> <span class="hljs-string">aws</span>
  <span class="hljs-attr">secretBindingName:</span> <span class="hljs-string">core-aws</span>
  <span class="hljs-attr">cloud:</span>
    <span class="hljs-attr">type:</span> <span class="hljs-string">aws</span>
    <span class="hljs-attr">region:</span> <span class="hljs-string">eu-west-1</span>
    <span class="hljs-attr">providerConfig:</span>
      <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">aws.cloud.gardener.cloud/v1alpha1</span>
      <span class="hljs-attr">kind:</span> <span class="hljs-string">InfrastructureConfig</span>
      <span class="hljs-attr">networks:</span>
        <span class="hljs-attr">vpc:</span> <span class="hljs-comment"># specify either 'id' or 'cidr'</span>
        <span class="hljs-comment"># id: vpc-123456</span>
          <span class="hljs-attr">cidr:</span> <span class="hljs-number">10.250.0.0</span><span class="hljs-string">/16</span>
        <span class="hljs-attr">internal:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-number">10.250.112.0</span><span class="hljs-string">/22</span>
        <span class="hljs-attr">public:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-number">10.250.96.0</span><span class="hljs-string">/22</span>
        <span class="hljs-attr">workers:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-number">10.250.0.0</span><span class="hljs-string">/19</span>
      <span class="hljs-attr">zones:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">eu-west-1a</span>
    <span class="hljs-attr">workerPools:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">pool-01</span>
    <span class="hljs-comment"># Taints, labels, and annotations are not yet implemented. This requires interaction with the machine-controller-manager, see</span>
    <span class="hljs-comment"># https://github.com/gardener/machine-controller-manager/issues/174. It is only mentioned here as future proposal.</span>
    <span class="hljs-comment"># taints:</span>
    <span class="hljs-comment"># - key: foo</span>
    <span class="hljs-comment">#   value: bar</span>
    <span class="hljs-comment">#   effect: PreferNoSchedule</span>
    <span class="hljs-comment"># labels:</span>
    <span class="hljs-comment"># - key: bar</span>
    <span class="hljs-comment">#   value: baz</span>
    <span class="hljs-comment"># annotations:</span>
    <span class="hljs-comment"># - key: foo</span>
    <span class="hljs-comment">#   value: hugo</span>
      <span class="hljs-attr">machineType:</span> <span class="hljs-string">m4.large</span>
      <span class="hljs-attr">volume:</span> <span class="hljs-comment"># optional, not needed in every environment, may only be specified if the referenced CloudProfile contains the volumeTypes field</span>
        <span class="hljs-attr">type:</span> <span class="hljs-string">gp2</span>
        <span class="hljs-attr">size:</span> <span class="hljs-string">20Gi</span>
      <span class="hljs-attr">providerConfig:</span>
        <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">aws.cloud.gardener.cloud/v1alpha1</span>
        <span class="hljs-attr">kind:</span> <span class="hljs-string">WorkerPoolConfig</span>
        <span class="hljs-attr">machineImage:</span>
          <span class="hljs-attr">name:</span> <span class="hljs-string">coreos</span>
          <span class="hljs-attr">ami:</span> <span class="hljs-string">ami-d0dcef3</span>
        <span class="hljs-attr">zones:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">eu-west-1a</span>
      <span class="hljs-attr">minimum:</span> <span class="hljs-number">2</span>
      <span class="hljs-attr">maximum:</span> <span class="hljs-number">2</span>
      <span class="hljs-attr">maxSurge:</span> <span class="hljs-number">1</span>
      <span class="hljs-attr">maxUnavailable:</span> <span class="hljs-number">0</span>
  <span class="hljs-attr">kubernetes:</span>
    <span class="hljs-attr">version:</span> <span class="hljs-number">1.11.0</span>
    <span class="hljs-string">...</span>
  <span class="hljs-attr">dns:</span>
    <span class="hljs-attr">provider:</span> <span class="hljs-string">aws-route53</span>
    <span class="hljs-attr">domain:</span> <span class="hljs-string">johndoe-aws.garden-dev.example.com</span>
  <span class="hljs-attr">maintenance:</span>
    <span class="hljs-attr">timeWindow:</span>
      <span class="hljs-attr">begin:</span> <span class="hljs-number">220000</span><span class="hljs-string">+0100</span>
      <span class="hljs-attr">end:</span> <span class="hljs-number">230000</span><span class="hljs-string">+0100</span>
    <span class="hljs-attr">autoUpdate:</span>
      <span class="hljs-attr">kubernetesVersion:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">backup:</span>
    <span class="hljs-attr">schedule:</span> <span class="hljs-string">"*/5 * * * *"</span>
    <span class="hljs-attr">maximum:</span> <span class="hljs-number">7</span>
  <span class="hljs-attr">addons:</span>
    <span class="hljs-attr">kube2iam:</span>
      <span class="hljs-attr">enabled:</span> <span class="hljs-literal">false</span>
    <span class="hljs-attr">kubernetes-dashboard:</span>
      <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">cluster-autoscaler:</span>
      <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">nginx-ingress:</span>
      <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>
      <span class="hljs-attr">loadBalancerSourceRanges:</span> []
    <span class="hljs-attr">kube-lego:</span>
      <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>
      <span class="hljs-attr">email:</span> <span class="hljs-string">john.doe@example.com</span>
</code></pre>
    <p class="normal">But, the extensibility goals of Gardener go far beyond just being provider agnostic. The overall process of standing up a Kubernetes cluster involves many steps. The Gardener project aims to let the operator customize each and every step by defining custom resources and webhooks. Here is the general flow diagram with the CRDs, mutating/validating admission controllers, and webhooks associated with each step:</p>
    <figure class="mediaobject"><img src="../Images/B18998_11_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.14: Flow diagram of CRDs mutating and validating admission controllers</p>
    <p class="normal">Here <a id="_idIndexMarker1234"/>are the CRD categories that comprise the extensibility space of Gardener:</p>
    <ul>
      <li class="bulletList">Providers for DNS management, such as Route53 and CloudDNS</li>
      <li class="bulletList">Providers for blob storage, including S3, GCS, and ABS</li>
      <li class="bulletList">Infrastructure providers like AWS, GCP, and Azure</li>
      <li class="bulletList">Support for various operating systems such as CoreOS Container Linux, Ubuntu, and FlatCar Linux</li>
      <li class="bulletList">Network plugins like Calico, Flannel, and Cilium</li>
      <li class="bulletList">Optional <a id="_idIndexMarker1235"/>extensions, such as Let’s Encrypt’s certificate service</li>
    </ul>
    <p class="normal">We have covered Gardener in depth, which brings us to the end of the chapter.</p>
    <h1 id="_idParaDest-578" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we’ve covered the exciting area of multi-cluster management. There are many projects that tackle this problem from different angles. The Cluster API project has a lot of momentum for solving the sub-problem of managing the lifecycle of multiple clusters. Many other projects take on the resource management and application lifecycle. These projects can be divided into two categories: projects that explicitly manage multiple clusters using a management cluster and managed clusters, and projects that utilize the Virtual Kubelet where whole clusters appear as virtual nodes in the main cluster.</p>
    <p class="normal">The Gardener project has a very interesting approach and architecture. It tackles the problem of multiple clusters from a different perspective and focuses on the large-scale management of clusters. It is the only project that addresses both cluster lifecycle and application lifecycle.</p>
    <p class="normal">At this point, you should have a clear understanding of the current state of multi-cluster management and what the different projects offer. You may decide that it’s still too early or that you want to take the plunge.</p>
    <p class="normal">In the next chapter, we will explore the exciting world of serverless computing on Kubernetes. Serverless can mean two different things: you don’t have to manage servers for your long-running workloads, and also, running functions as a service. Both forms of serverless are available for Kubernetes, and both of them are extremely useful.</p>
  </div>
</body></html>