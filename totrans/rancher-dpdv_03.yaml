- en: '*Chapter 2*: Rancher and Kubernetes High-Level Architecture'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第二章*：Rancher 和 Kubernetes 高级架构'
- en: This chapter will cover the high-level processes of **Rancher**, **Rancher Kubernetes
    Engine** (**RKE**), **RKE2** (also known as **RKE Government**), **K3s**, and
    **RancherD**. We will discuss the core design philosophy of each of these products
    and explore the ways in which they are different. We'll dive into Rancher's high-level
    architecture and see how Rancher server pods communicate with downstream clusters
    using the Cattle agents, which include both the Cattle-cluster-agent and the Cattle-node-agent.
    We'll also look at how the Rancher server uses RKE and how Rancher-machine provisions
    downstream nodes and Kubernetes (**K8s**) clusters. After that, we'll cover the
    high-level architecture of K8s, including **kube-api-server**, **kube-controller-manager**,
    and **kube-scheduler**. We'll also discuss how each of these components maintains
    the state of the cluster. Finally, we'll examine how an end user can change the
    desired state and how the controllers can update the current state.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍**Rancher**、**Rancher Kubernetes Engine**（**RKE**）、**RKE2**（也称为**RKE 政府版**）、**K3s**和**RancherD**的高级过程。我们将讨论这些产品的核心设计理念，并探讨它们的不同之处。我们将深入了解
    Rancher 的高级架构，并查看 Rancher 服务器的 Pod 如何使用 Cattle 代理（包括 Cattle-cluster-agent 和 Cattle-node-agent）与下游集群进行通信。我们还将探讨
    Rancher 服务器如何使用 RKE，以及 Rancher-machine 如何为下游节点和 Kubernetes（**K8s**）集群提供资源。接下来，我们将介绍
    K8s 的高级架构，包括**kube-api-server**、**kube-controller-manager**和**kube-scheduler**。我们还将讨论这些组件如何保持集群的状态。最后，我们将检查最终用户如何更改期望状态，以及控制器如何更新当前状态。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将覆盖以下主要内容：
- en: What is the Rancher server?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher 服务器是什么？
- en: What are RKE and RKE2?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 RKE 和 RKE2？
- en: What is K3s (five less than K8s)?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 K3s（比 K8s 少五个）？
- en: What is RancherD?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 RancherD？
- en: What controllers run inside the Rancher server pods?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher 服务器的 Pod 中运行了哪些控制器？
- en: What does the Cattle agent do?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cattle 代理做什么？
- en: How does Rancher provision nodes and clusters?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher 如何提供节点和集群？
- en: What are kube-apiserver, kube-controller-manager, kube-scheduler, etcd, and
    kubelet?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 kube-apiserver、kube-controller-manager、kube-scheduler、etcd 和 kubelet？
- en: How do the current state and the desired state work?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前状态和期望状态是如何工作的？
- en: What is the Rancher server?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rancher 服务器是什么？
- en: The **Rancher server** forms the core of the Rancher ecosystem, and it contains
    almost everything needed by any other component, product, or tool depending on
    or connecting to the Rancher server via the Rancher API. The *Rancher server*
    is usually shortened to just *Rancher*, and in this section, when I say *Rancher*,
    I will be talking about the *Rancher server*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**Rancher 服务器**构成了 Rancher 生态系统的核心，它包含了任何其他组件、产品或工具所需的几乎所有内容，这些组件、产品或工具依赖于或通过
    Rancher API 连接到 Rancher 服务器。*Rancher 服务器*通常简称为*Rancher*，在本节中，当我说到*Rancher*时，我指的就是*Rancher
    服务器*。'
- en: The heart of Rancher is its API. The **Rancher API** is built on a custom API
    framework called **Norman** that acts as a translation layer between the Rancher
    API and the K8s API. Everything in Rancher uses the Rancher or K8s API to communicate.
    This includes the Rancher **user interface** (**UI**), which is 100% API-driven.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher 的核心是其 API。**Rancher API**建立在一个名为**Norman**的自定义 API 框架之上，Norman 充当 Rancher
    API 和 K8s API 之间的翻译层。Rancher 中的一切都通过 Rancher 或 K8s API 进行通信，包括 100% 由 API 驱动的
    Rancher **用户界面**（**UI**）。
- en: So, how do you connect to the Rancher API? The Rancher API is a standard `CustomResource`
    object. Of course, because everything is being stored in a `CustomResource` object
    in K8s, the Rancher request flow is stateless and doesn't require session persistence.
    Finally, once the `CustomResource` object is created, changed, or deleted, the
    controller for the object type will take over and process that request. We'll
    go deeper into the different controllers later in this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何连接到 Rancher API？Rancher API 是一个标准的 `CustomResource` 对象。当然，因为所有内容都存储在 K8s
    的 `CustomResource` 对象中，所以 Rancher 请求流程是无状态的，不需要会话持久性。最后，一旦 `CustomResource` 对象被创建、修改或删除，该对象类型的控制器将接管并处理该请求。我们将在本章后面深入探讨不同的控制器。
- en: What are RKE and RKE2?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 RKE 和 RKE2？
- en: What do I need, RKE or RKE2? Traditionally, when building a K8s cluster, you
    would need to carry out several steps. First, you'd need to generate a root **CA
    key** as well as the certificates for the different K8s components and push them
    out to every server that was part of the cluster. Second, you'd then install/configure
    **etcd**, and this would include setting up the **systemd** service on your management
    nodes. Next, you would need to bootstrap the etcd cluster and verify that all
    etcd nodes were communicating and replicating correctly. At this point, you would
    install kube-apiserver and connect it back to your etcd cluster. Finally, you
    would need to install kube-controller-manager and kube-scheduler and connect them
    back to the kube-apiserver objects. If you wanted to bring up the control plane
    for your cluster, even more steps would be needed to join your worker nodes to
    the cluster.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要什么，是RKE还是RKE2？传统上，在构建K8s集群时，您需要执行几个步骤。首先，您需要生成根**CA密钥**以及各个K8s组件的证书，并将它们推送到集群中每台服务器上。其次，您需要安装/配置**etcd**，这包括在管理节点上设置**systemd**服务。接下来，您需要启动etcd集群，并验证所有etcd节点是否正确地进行通信和复制。此时，您将安装kube-apiserver并将其连接回etcd集群。最后，您需要安装kube-controller-manager和kube-scheduler，并将它们连接回kube-apiserver对象。如果您想要启动集群的控制平面，您还需要更多步骤将工作节点加入集群。
- en: This process is called *K8s the hard way*, and it's called that for a reason,
    as this process can be very complicated and can change over time. And in the early
    days of K8s, this was the only way to create K8s clusters. Because of this, users
    needed to make large scripts or **Ansible Playbooks** to create their K8s clusters.
    These scripts would need lots of care and feeding to get up and running, with
    even more work required to keep them working as K8s continually changed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为*K8s the hard way*，之所以叫这个名字，是因为这个过程非常复杂，并且随着时间的推移可能会发生变化。在K8s的早期，这是创建K8s集群的唯一方式。正因为如此，用户需要编写大型脚本或**Ansible
    Playbooks**来创建K8s集群。这些脚本需要大量的关注和维护才能顺利启动，并且随着K8s不断变化，还需要更多的工作来保持其正常运行。
- en: Rancher saw this issue and knew that for K8s to become mainstream, it needed
    to be *crazy easy* to build clusters for both end users and the Rancher server.
    Initially, in the Rancher v1.6 days, Rancher would build K8s clusters on its container
    clustering software called **Cattle**. Because of this, everything needed had
    to run as a container, and this was the starting point of RKE.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher看到了这个问题，并意识到为了让K8s成为主流，构建集群需要对最终用户和Rancher服务器来说是*极其简单*的。最初，在Rancher v1.6时代，Rancher会使用它的容器集群软件**Cattle**来构建K8s集群。因此，所需的一切都必须作为容器运行，这也是RKE的起点。
- en: So, what is RKE?
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么，什么是RKE呢？
- en: RKE is Rancher's cluster orchestration tool for creating and managing **Cloud
    Native Computing Foundation** (**CNCF**)-certified K8s clusters on a wide range
    of operating systems with a range of configurations. The core concept of RKE is
    that everything that makes up the K8s cluster should run entirely within **Docker**
    containers. Because of this, RKE doesn't care what operating system it's deployed
    on, as long as it's within a Docker container. This is because RKE is not installing
    binaries on the host, configuring services, or anything similar to this.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: RKE是Rancher的集群编排工具，用于在各种操作系统上创建和管理**云原生计算基金会**（**CNCF**）认证的K8s集群，并支持多种配置。RKE的核心概念是，构成K8s集群的所有组件都应该完全在**Docker**容器中运行。因此，RKE不关心它部署在哪个操作系统上，只要它运行在Docker容器中即可。这是因为RKE并不会在主机上安装二进制文件、配置服务或类似的操作。
- en: How does RKE work?
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RKE是如何工作的？
- en: RKE is a `cluster.yml` (see *Figure 2.1*). RKE then uses that configuration
    file to create all of the containers needed to start the cluster, that is, etcd,
    kube-apiserver, kube-controller-manager, kube-scheduler, and kubelet. Please see
    the *How does Rancher provision nodes and clusters?* section in this chapter for
    further details on nodes and clusters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: RKE是一个`cluster.yml`（见*图2.1*）。RKE随后使用该配置文件来创建启动集群所需的所有容器，即etcd、kube-apiserver、kube-controller-manager、kube-scheduler和kubelet。有关节点和集群的更多详细信息，请参阅本章中的*Rancher如何配置节点和集群？*部分。
- en: '![Figure 2.1 – A code snippet from the cluster.yaml file](img/B18053_02_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – cluster.yaml文件中的代码片段](img/B18053_02_01.jpg)'
- en: Figure 2.1 – A code snippet from the cluster.yaml file
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – cluster.yaml文件中的代码片段
- en: What is RKE2?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是RKE2？
- en: RKE2 is Rancher's next-generation K8s solution and is also known as RKE Government.
    RKE2 was designed to update and address some of the shortfalls of RKE, and it
    also brought the *crazy easy* setup methods from K3s to improve its functionality.
    RKE2 is also a fully CNCF-certified K8s distribution. But RKE2 was created specifically
    for Rancher's US federal government and their customers, as they have several
    special requirements for their K8s use – the first being that it is highly secure
    by default.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: RKE2是Rancher的下一代K8s解决方案，也被称为RKE Government。RKE2的设计旨在更新和解决RKE的一些不足，并且它还引入了K3s的*超简单*安装方法来提高功能性。RKE2也是一个完全CNCF认证的K8s发行版。不过，RKE2是专为Rancher的美国联邦政府及其客户创建的，因为他们对K8s的使用有若干特别要求——第一个要求是默认情况下它必须具有高度安全性。
- en: When setting up RKE, you must follow a hardening guide and take several manual
    steps to comply with **CIS benchmarks**. RKE2, on the other hand, is designed
    to be secure with little to no action required by the cluster administrator. US
    federal customers need their K8s clusters to be **FIPS-enabled** (**FIPS** stands
    for the United States **Federal Information Processing Standards**). Also, because
    RKE2 is built on K3s, it inherits a number of its features – the first being the
    support of **ARM64**-based systems. So, you could set up RKE2 on a **Raspberry
    Pi** if you chose to. This provides users with the flexibility to mix and match
    ARM64 and **AMD64** nodes in the same cluster and that means customers can run
    workloads such as multiple arch builds using the **Drone Continuous Integration
    (CI)** platform inside their cluster. This also provides support for low-power
    and cost-effective ARM64 nodes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置RKE时，你必须遵循加固指南，并采取若干手动步骤以符合**CIS基准**。而RKE2则设计成在几乎不需要集群管理员采取任何行动的情况下就能确保安全。美国联邦客户需要他们的K8s集群是**启用FIPS**的（**FIPS**代表美国**联邦信息处理标准**）。此外，由于RKE2是基于K3s构建的，它继承了K3s的多个特性——其中第一个是支持**ARM64**架构的系统。所以，如果你愿意，可以在**Raspberry
    Pi**上设置RKE2。这为用户提供了在同一个集群中混合使用ARM64和**AMD64**节点的灵活性，这意味着客户可以在集群内运行如**Drone持续集成（CI）**平台等多架构构建的工作负载。它还为低功耗和成本效益高的ARM64节点提供支持。
- en: The second feature inherited from K3s is **self-bootstrapping**. In RKE, you
    would need to define the cluster as YAML and then use the RKE binary to try to
    create and manage the cluster. But with RKE2, once the first node has been created,
    all of the other nodes simply join the cluster using a registration endpoint running
    on the master nodes. Note that this does require an external load balancer or
    a round-robin DNS record to be successful. Because RKE2 can manage itself, it
    allows you to do very cool tasks, such as defining a K8s upgrade with kubectl
    and just letting the cluster take care of it for you.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从K3s继承的第二个特性是**自引导**。在RKE中，你需要定义集群的YAML文件，然后使用RKE二进制文件来尝试创建和管理集群。但在RKE2中，一旦创建了第一个节点，所有其他节点只需通过在主节点上运行的注册端点加入集群。需要注意的是，这确实需要一个外部负载均衡器或轮询DNS记录才能成功。因为RKE2可以自我管理，所以它允许你执行非常酷的任务，例如使用kubectl定义K8s升级，并让集群自行处理。
- en: The third feature that RKE2 inherited from K3s was built-in `kubectl -n kube-system
    get pods`, you can see your etcd containers, and you can even open a shell to
    them or capture logs, just like you would with any other pod.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: RKE2从K3s继承的第三个特性是内置的`kubectl -n kube-system get pods`，你可以查看你的etcd容器，甚至可以像操作其他Pod一样，打开一个shell或者捕获日志。
- en: Last but not the least, the most crucial feature of RKE2 is that it's fully
    open source with no paywall – just like every other Rancher product.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，RKE2的最关键特性是它是完全开源的，没有付费墙——就像Rancher的其他所有产品一样。
- en: What is K3s (five less than K8s)?
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K3s是什么（比K8s少五个版本）？
- en: 'K3s is a fully CNCF-certified K8s distribution. This means that in K3s, the
    YAML you would deploy is just a standard K8s cluster deployed in a K3s cluster.
    K3s was created because traditional K8s clusters – or even RKE clusters – were
    designed to run at scale, meaning that they would require three etcd nodes, two
    control plane nodes, and three or more worker nodes for a standard configuration.
    In this case, the minimum size for nodes would be around four cores, with 8 gigabits
    of RAM for the etcd objects and control plane nodes, with the worker nodes having
    two cores and 4 gigabits of RAM. These would just be the background requirements
    when talking about K8s clusters at the scale of an IE 50 node cluster, with the
    worker nodes having 64 cores and 512 GB of RAM. But when you start looking at
    deploying K8s at the edge, where physical space, power, and compute resources
    are all at a premium, standard K8s and RKE are just too big. So, the question
    is: *how do we shrink K8s?*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: K3s 是一个完全通过 CNCF 认证的 K8s 发行版。这意味着在 K3s 中，你部署的 YAML 文件就是一个标准的 K8s 集群，只不过它被部署在
    K3s 集群中。K3s 的诞生是因为传统的 K8s 集群——甚至是 RKE 集群——是为大规模运行而设计的，意味着它们需要三个 etcd 节点、两个控制平面节点以及三个或更多的工作节点，才能配置一个标准的集群。在这种情况下，节点的最小配置大约是四个核心，etcd
    对象和控制平面节点需要 8 GB 的内存，而工作节点需要两个核心和 4 GB 的内存。这些仅仅是在谈论像 IE 50 节点集群这样规模的 K8s 集群时的背景要求，工作节点的配置可能为
    64 个核心和 512 GB 的内存。但当你开始考虑在边缘部署 K8s 时，考虑到物理空间、电力和计算资源都很紧张，标准的 K8s 和 RKE 就显得过于庞大。所以，问题是：*我们如何缩小
    K8s 的规模？*
- en: 'K3s was based on the following core principles: no legacy code, duplicate code,
    or extras. With RKE and other standard K8s distributions, each component exists
    as its separate code with its own runtime. At Rancher, they asked themselves a
    question:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: K3s 基于以下核心原则：没有遗留代码、重复代码或多余的内容。对于 RKE 和其他标准的 K8s 发行版，每个组件都是独立的代码，并有各自的运行时。在
    Rancher，他们问了自己一个问题：
- en: '*Hey, there is a lot of duplicate code running here. What if we just merged
    kube-apiserver, kube-controller-manager, kube-scheduler, and kubelet into a single
    binary?*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*嘿，这里有很多重复的代码在运行。如果我们将 kube-apiserver、kube-controller-manager、kube-scheduler
    和 kubelet 合并成一个单一的二进制文件会怎么样？*'
- en: And that was how K3s was born. K3s only has *master* and *worker* nodes, with
    the master node running all of the core components. The next big breakthrough
    was what they did with etcd. The etcd object is not small. It eats memory like
    it's going out of style and doesn't play nice when it's in a cluster of one. This
    is where **kind** comes into the picture.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 于是，K3s 就这样诞生了。K3s 只有 *master* 节点和 *worker* 节点，主节点运行所有的核心组件。下一个重大的突破是他们对 etcd
    的处理。etcd 对象并不小，它吃内存的速度快得像要过时一样，而且在单节点集群中它并不好用。这时，**kind** 就进入了画面。
- en: The kind database adapter makes standard **SQL** databases such as **SQLite3**,
    **MySQL**, or **Postgres** look like an etcd database. So, as far as kube-apiserver
    knows, it's talking to an etcd cluster. The CPU and memory footprint is much smaller
    because you can run a database like SQLite3 in place of etcd. It is important
    to note that Rancher does not customize any of the standard K8s libraries in the
    core components. This allows K3s to stay up to date with upstream K8s. The next
    big area of saving in K3s was in-tree storage drivers and cloud providers. Upstream
    K8s has several storage drivers built into the core components. For example, RKE
    has storage drivers to allow K8s to connect to the **AWS** API and use **Amazon**
    **EBS** volumes to provide storage directly to pods. This is great if you are
    running in AWS, but if you are running in **VMware** then this code is just wasting
    resources. It's the same the other way round, with VMware's **vSphere** having
    a storage provider for mounting **Virtual Machine Disks (VMDKs)** to nodes. The
    idea was that most of these storage and cloud providers are not used. For example,
    if I'm running a cluster on Amazon, why do I need libraries and tools for Azure?
    Plus there are out-of-tree alternatives that can be deployed as pods instead of
    being baked in. Also, most of the major storage providers are moving to out-of-tree
    provisioning anyway. So, K3s removes them. This eliminates a significant overhead.
    Because of all these optimizations, K3s clusters can fit on a 40 MB binary file
    and run on a node with only 512 MB of RAM.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据库适配器使得标准**SQL**数据库如**SQLite3**、**MySQL**或**Postgres**看起来像是一个 etcd 数据库。因此，从
    kube-apiserver 的角度来看，它是在与 etcd 集群通信。CPU 和内存占用的足迹要小得多，因为你可以运行像 SQLite3 这样的数据库代替
    etcd。值得注意的是，Rancher 并未定制任何核心组件中的标准 K8s 库，这使得 K3s 可以与上游 K8s 保持同步。K3s 下一个重大节省领域是树内存储驱动程序和云提供商。上游
    K8s 在核心组件中内置了多个存储驱动程序。例如，RKE 有存储驱动程序，允许 K8s 连接到**AWS** API，并直接为 Pod 提供**Amazon
    EBS**卷存储。如果在 AWS 上运行，这非常棒，但如果在**VMware**上运行，这段代码将浪费资源。同样的情况也适用于 VMware 的**vSphere**，它有一个存储提供程序将**虚拟机磁盘
    (VMDKs)**挂载到节点上。这个想法是大多数这些存储和云提供商是不被使用的。例如，如果我在 Amazon 上运行集群，为什么我需要 Azure 的库和工具？此外，有替代方案可以作为
    Pod 部署，而不是作为内置功能。此外，大多数主要的存储提供商正在转向树外的供应方式。因此，K3s 将它们移除了。这消除了显著的开销。由于所有这些优化，K3s
    集群可以放在一个 40 MB 的二进制文件中，并在只有 512 MB RAM 的节点上运行。
- en: The other significant change in K3s to K8s was the idea that it should be *crazy
    easy* to spin up a K3s cluster. For example, creating a single-node K3s cluster
    only requires the `curl -sfL https://get.k3s.io | sh -` command to run, with the
    only dependency being that it's within a Linux ARM64 or AMD64 operating system
    with `curl` installed. Because of this ease of use, K3s is frequently deployed
    in single-node clusters where a user wants to use all of the management tools
    that K8s provides but without the scale. For example, a developer might spin up
    a K3s cluster on their laptop using a **virtual machine** (**VM**) to deploy their
    application just as they would in their production K8s cluster.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: K3s 对 K8s 的另一个重要改变是，它应该*非常简单*地启动一个 K3s 集群。例如，创建一个单节点的 K3s 集群只需要运行`curl -sfL
    https://get.k3s.io | sh -`命令，唯一的依赖是它在安装了`curl`的 Linux ARM64 或 AMD64 操作系统上。由于这种易用性，K3s
    经常被部署在单节点集群中，用户希望使用 K8s 提供的所有管理工具，但规模不大。例如，开发人员可能会在他们的笔记本上通过**虚拟机**（**VM**）快速部署一个
    K3s 集群，以部署他们的应用程序，就像在生产 K8s 集群中一样。
- en: Another great use case for K3s is deploying to a retail environment where you
    might have hundreds or even thousands of locations all over the country (or world)
    and have a single K3s node running on a small PC at each location. K3s helps in
    this situation because it is so tiny, so common problems such as slow internet
    connections are not that big of a problem, and also K3s can keep running even
    if it loses its connection back to a corporate data center. An even more extraordinary
    kind of deployment for K3s is a wind turbine in the middle of nowhere with only
    a **Long-Term Evolution** (**LTE**) connection for internet access. These are
    the kinds of deployments K3s was built for.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: K3s 的另一个优秀用例是在零售环境中部署，可能在全国甚至全球各地都有数百甚至数千个位置，每个位置上都运行着一个小型 PC 上的单个 K3s 节点。K3s
    在这种情况下非常有帮助，因为它体积很小，因此常见的问题如慢速互联网连接并不是大问题，此外，即使失去与企业数据中心的连接，K3s 仍然可以继续运行。对于 K3s
    更加特别的部署是在偏远地区的风力发电机上，仅有一个**长期演进**（**LTE**）连接用于互联网访问。这些是 K3s 设计的部署类型。
- en: What is RancherD?
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RancherD 是什么？
- en: '`curl -sfL https://get.rancher.io | sh –` command on a Linux AMD64 or ARM64
    server. This binary is similar to RKE2 but has been optimized to host the Rancher
    server. RancherD also includes extra tools to support the Rancher server application.
    For example, the `rancherd reset-admin` command will reset the administrator password
    for the Rancher server.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Linux AMD64 或 ARM64 服务器上执行 `curl -sfL https://get.rancher.io | sh –` 命令。这个二进制文件类似于
    RKE2，但经过优化以托管 Rancher 服务器。RancherD 还包括额外的工具来支持 Rancher 服务器应用程序。例如，`rancherd reset-admin`
    命令将重置 Rancher 服务器的管理员密码。
- en: To change this password with a normal RKE or RKE2 cluster, you would need to
    find the Rancher server pod and open a shell into the container. Then you would
    run the `reset-admin` command. The main idea behind RancherD is to make it very
    easy to manage Rancher. It does this by using the RKE2 Helm operator to handle
    deploying the Rancher server pods. And because it uses the same Helm chart that
    you would use in an RKE cluster, all of the customization options are still available
    (the best feature being the ease of management of SSL certificates). In a standard
    Rancher server deployment, you must configure and manage the SSL certificates
    that support the Rancher API. This can be a pain when using internally signed
    certificates, as you need to edit a secret inside the cluster, which can be difficult
    for new K8s users. RancherD solves this problem by simply having the user drop
    the certificate files into `/etc/rancher/ssl/` on one of the RancherD nodes, at
    which point it takes over the process and handles the update for you. Most of
    the time, you'll use RancherD when you don't want to manage the K8s cluster that
    hosts Rancher but can't use a hosted K8s option such as **AWS** **EKS**, **Azure**
    **AKS**, or **Google** **GKE**, or if you need to manage a large number of different
    Rancher installations. For example, if you were running a hosted environment where
    you were providing *Rancher as a service*, you might use RancherD to simplify
    the management of these clusters at scale.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 若要在常规的 RKE 或 RKE2 集群中更改此密码，你需要找到 Rancher 服务器 pod 并进入容器的 shell。然后你可以运行 `reset-admin`
    命令。RancherD 背后的主要理念是让管理 Rancher 变得非常简单。它通过使用 RKE2 Helm 操作符来处理 Rancher 服务器 pod
    的部署实现这一点。而且因为它使用的是你在 RKE 集群中也会使用的相同 Helm chart，所有定制化选项仍然可用（最棒的功能是 SSL 证书的管理便利性）。在标准的
    Rancher 服务器部署中，你必须配置并管理支持 Rancher API 的 SSL 证书。当使用内部签名证书时，这可能会很麻烦，因为你需要编辑集群内的一个
    secret，这对新手 K8s 用户来说可能很困难。RancherD 通过简单地让用户将证书文件放入 `/etc/rancher/ssl/` 目录中的一个
    RancherD 节点来解决这个问题，此时它会接管该过程并为你处理更新。大多数时候，当你不想管理托管 Rancher 的 K8s 集群，但又不能使用诸如 **AWS**
    **EKS**、**Azure** **AKS** 或 **Google** **GKE** 等托管 K8s 选项，或者如果你需要管理大量不同的 Rancher
    安装时，你将使用 RancherD。例如，如果你在托管环境中提供 *Rancher 作为服务*，你可能会使用 RancherD 来简化大规模管理这些集群。
- en: What controllers run inside the Rancher server pods?
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rancher 服务器的 pod 内运行了哪些控制器？
- en: 'Rancher is made of a set of pods – three pods by default – that run in a K8s
    cluster. These pods can service requests from the ingress controller – **ingress-nginx**
    by default – using Norman to translate the Rancher API requests into the K8s API
    requests to access the custom resource objects that Rancher uses. But the Rancher
    server pods also host several controllers, with the primary controllers as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher 由一组 pod 组成——默认情况下是三个 pod——它们在 K8s 集群中运行。这些 pod 可以通过默认的 **ingress-nginx**
    入口控制器处理请求，并使用 Norman 将 Rancher API 请求转换为 K8s API 请求，以访问 Rancher 使用的自定义资源对象。但是，Rancher
    服务器的 pod 还托管多个控制器，主要控制器如下：
- en: '`TokenReview` requests from K8s. This service then calls the LDAP server and
    validates the username and password. If it passes the validation, the service
    will respond with a `200 OK` response, with all other response codes representing
    a failed authentication. Because of this, the setup process can be very complex
    and unreliable. As a result, Rancher chose the approach of building its controller
    to validate the username and password with external authentication providers such
    as LDAP, AD, GitHub, Okta, and more. Once the user has been validated, Rancher
    will give the user a bearer token that they can use to authenticate directly to
    the K8s API. The controller does this by creating matching service accounts, roles,
    and role bindings on the downstream clusters ahead of time. The controller also
    provides some higher-level controls via the Rancher concept of *projects*. You
    can define a group of namespaces called a *project* and manage permissions at
    the project level instead of managing them only at the cluster or namespace level.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TokenReview` 请求来自 K8s。该服务会调用 LDAP 服务器，验证用户名和密码。如果验证通过，服务将以 `200 OK` 响应返回，所有其他响应码表示身份验证失败。由于这一点，设置过程可能非常复杂且不可靠。因此，Rancher
    选择了构建其控制器的方法，通过外部身份验证提供者（如 LDAP、AD、GitHub、Okta 等）验证用户名和密码。一旦用户验证通过，Rancher 将为用户提供一个承载令牌，用户可以直接用它向
    K8s API 进行身份验证。控制器通过提前在下游集群中创建匹配的服务帐户、角色和角色绑定来实现这一点。控制器还通过 Rancher 的 *项目* 概念提供一些更高层次的控制。你可以定义一组命名空间作为一个
    *项目*，并在项目级别管理权限，而不是仅在集群或命名空间级别管理权限。'
- en: '**Rancher Catalog Controller**: This controller is responsible for managing
    the catalogs inside Rancher. But what is a *catalog*? Rancher uses the concept
    of *catalogs* that are repositories for Helm charts. Rancher calls them catalogs
    because they give users a catalog of applications to deploy to their cluster.
    The default catalogs have several great applications including **WordPress**,
    **MySQL**, **Rancher** **Longhorn**, the **Datadog** **Cluster Agent**, and many
    more. All of these catalogs come together in Rancher under what is called the
    **Apps and Marketplace** feature, allowing users to deploy Helm-based applications
    to your cluster. You can also add your repository as a catalog, which is excellent
    for DevOps teams that want to provide their application teams with standardized
    toolsets. For example, if an application team wanted their own monitoring systems,
    they could modify and tune the system based on their preferences. You might create
    a **Prometheus** Helm chart with a basic configuration that the application team
    could simply click to deploy on their cluster.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rancher 目录控制器**：该控制器负责管理 Rancher 内部的目录。那么什么是 *目录* 呢？Rancher 使用 *目录* 的概念作为
    Helm 图表的存储库。Rancher 称它们为目录，因为它们为用户提供了一个可以部署到集群中的应用程序目录。默认目录包含一些很棒的应用程序，包括 **WordPress**、**MySQL**、**Rancher**、**Longhorn**、**Datadog**
    **Cluster Agent** 等等。所有这些目录都汇集在 Rancher 中，组成了所谓的 **应用和市场** 功能，允许用户将基于 Helm 的应用程序部署到集群中。你还可以将自己的存储库添加为目录，这对于希望为其应用程序团队提供标准化工具集的
    DevOps 团队来说非常有用。例如，如果应用程序团队希望拥有自己的监控系统，他们可以根据自己的偏好修改和调整系统。你可能会创建一个 **Prometheus**
    Helm 图表，提供基本配置，应用程序团队只需点击就可以在其集群上部署。'
- en: Another great example of the use of a Helm chart is for environments where there
    might be one primary application – for example, the core application for the business
    that other teams must write their applications to connect to and work with. You
    can create a Helm chart for the monolithic application that an application team
    can quickly spin up to do integration testing with and then spin down to save
    costs. In this case, all of this would be managed by the Rancher catalog controller,
    which handles caching the catalogs (for speed reasons) and for legacy applications,
    deploying the application, that is, running the `helm install` command inside
    the Rancher server pod.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Helm 图表的另一个很好的例子是在可能有一个主应用程序的环境中——例如，业务核心应用程序，其他团队必须编写应用程序以连接并与其一起工作。你可以为这个单体应用程序创建一个
    Helm 图表，应用程序团队可以快速启动进行集成测试，然后关闭以节省成本。在这种情况下，所有这些操作将由 Rancher 目录控制器管理，该控制器负责缓存目录（出于速度考虑），以及对于传统应用程序，部署该应用程序，即在
    Rancher 服务器 pod 内运行 `helm install` 命令。
- en: But with Rancher v2.6, this process has been moved over to Fleet to handle the
    deployment process, where Fleet will spin up a Helm operator pod on the downstream
    cluster and run the Helm commands. Note that this is excellent for speed, scalability,
    and flexibility, as Fleet gives you many options for customizing the Helm chart
    and is part of Rancher's DevOps at scale. Fleet is designed to manage up to a
    million clusters at once. It is important to note that the Rancher catalog controller
    only runs on the Rancher leader pod. If that is deleted or lost, the cache will
    need to be rebuilt, but this process usually only takes a few minutes. This controller
    also synchronizes the cache on a schedule (6 hours by default), but the syncing
    process can be forced to update, with this process running the `helm repo update…`
    command but as Go code instead.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在Rancher v2.6中，这个过程已转移到Fleet来处理部署过程，Fleet将在下游集群上启动一个Helm操作员Pod并运行Helm命令。请注意，这对速度、可扩展性和灵活性非常有利，因为Fleet为您提供了许多自定义Helm图表的选项，是Rancher大规模DevOps的一部分。Fleet的设计能够一次管理最多百万个集群。需要注意的是，Rancher目录控制器仅在Rancher领导者Pod上运行。如果该Pod被删除或丢失，则需要重新构建缓存，但这个过程通常只需要几分钟。此控制器还会按照计划（默认每6小时）同步缓存，但同步过程可以被强制更新，执行`helm
    repo update…`命令，但以Go代码的形式运行。
- en: '`cluster.yaml` and `cluster.rkestate` files for you and handles running `rke
    up` from inside the Rancher leader pod. Note that when troubleshooting the cluster
    if it is stuck when updating status issues, this is the controller we''ll look
    at the most. Please see the *How does Rancher provision nodes and clusters?* section
    later in this chapter for more details on how this controller works.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cluster.yaml`和`cluster.rkestate`文件由您处理，并在Rancher领导者Pod内部运行`rke up`。请注意，在故障排除集群时，如果在更新状态问题时卡住，这是我们最常查看的控制器。更多关于该控制器如何工作的细节，请参见本章稍后的*Rancher如何配置节点和集群？*部分。'
- en: '**Rancher Node Controller**: This controller is responsible for managing the
    Rancher-provisioned nodes. This controller is only used for Rancher-provisioned
    clusters on virtualized platforms such as **Amazon** **EC2**, **Google** **GCP**,
    VMware vSphere, and so on. This controller is built on top of the Go code that
    makes up a Rancher machine, which in turn is built on top of **Docker Machine**.
    This controller''s main function is to handle the creation and deletion of VMs
    in a node pool. Please see the *How does Rancher provision nodes and clusters?*
    section later in this chapter for more details about this process. Note, when
    troubleshooting node provisioning errors such as SSH timeout or configuration
    validation errors, this is the controller we''ll look at the most.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rancher节点控制器**：该控制器负责管理Rancher配置的节点。该控制器仅用于Rancher配置的集群，在虚拟化平台上，如**Amazon**
    **EC2**、**Google** **GCP**、VMware vSphere等。该控制器构建在Rancher机器的Go代码之上，而Rancher机器本身是基于**Docker
    Machine**构建的。该控制器的主要功能是处理节点池中虚拟机的创建和删除。更多关于此过程的细节，请参见本章稍后的*Rancher如何配置节点和集群？*部分。注意，在故障排除节点配置错误时，如SSH超时或配置验证错误时，这是我们最常查看的控制器。'
- en: '`rancher-pipelines.yml` file to configure the pipeline.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rancher-pipelines.yml`文件用于配置管道。'
- en: Note
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: As of Rancher v2.5, Git-based deployment pipelines are now recommended to handle
    Rancher Continuous Delivery, powered by Fleet. As a result, this controller was
    removed in Rancher v2.6.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从Rancher v2.5开始，推荐使用基于Git的部署管道来处理Rancher持续交付，该功能由Fleet提供支持。因此，这个控制器在Rancher
    v2.6中被移除了。
- en: '`vanilla` upstream Prometheus monitoring stack deployment instead of a Rancher
    customized Prometheus deployment.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vanilla`上游Prometheus监控栈部署，而不是Rancher定制的Prometheus部署。'
- en: '**Rancher Logging Controller**: This controller manages the integration between
    the logging systems in the Rancher UI and the **Banzai Cloud** **Logging** operator.
    This controller is a translation layer that allows users to define logging Flows
    and ClusterFlows via the Rancher UI, which get translated into **Custom Resource
    Definition** (**CRD**) objects that the Banzai Cloud Logging operator uses for
    configuring both applications and cluster-level logging. Before Rancher v2.5,
    Rancher used several different logging providers, including **Syslog**, **Splunk**,
    **Apache** **Kafka**, and **Fluentd**. However, this was a custom Rancher solution
    and wasn''t very flexible. So, as part of the logging v2 migration, everything
    was moved over to Banzai to be better aligned with where the industry is heading
    to.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rancher 日志控制器**：这个控制器管理 Rancher UI 中日志系统与 **Banzai Cloud** **Logging** 操作员之间的集成。这个控制器是一个翻译层，允许用户通过
    Rancher UI 定义日志流和集群日志流（ClusterFlows），并将其转换为 **自定义资源定义**（**CRD**）对象，这些对象被 Banzai
    Cloud Logging 操作员用于配置应用程序和集群级别的日志记录。在 Rancher v2.5 之前，Rancher 使用多个不同的日志提供者，包括
    **Syslog**、**Splunk**、**Apache** **Kafka** 和 **Fluentd**。然而，这是一种定制的 Rancher 解决方案，灵活性较差。因此，作为日志
    v2 迁移的一部分，一切都转移到了 Banzai，以更好地与行业的发展趋势对接。'
- en: '`istioctl` binary or the Istio operator. This controller also handles deploying
    **Kiali** for graphing traffic flows throughout the service mesh. This allows
    users to see what applications connect to other applications, including the traffic
    rates and latencies between pods. This can be extremely valuable for application
    owners and teams.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`istioctl` 二进制文件或 Istio 操作员。这个控制器还负责部署**Kiali**，用于在服务网格中绘制流量图。这使得用户可以看到哪些应用程序连接到其他应用程序，包括
    pod 之间的流量速率和延迟。这对应用程序所有者和团队来说非常有价值。'
- en: '`--insecure-bind-address` flag set to something besides `localhost` on kube-apiserver.
    Note that this setting allows requests to bypass the authentication and authorization
    modules, and they must not be exposed outside the node. In this case, Sonobuoy
    would collect this setting and then kube-bench would flag that value as a failed
    check. Finally, the **rancher-cis-benchmark** tool would collect all of the checks
    together in an excellent report that can be sent off by email to the security
    team.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--insecure-bind-address` 标志设置为除 `localhost` 以外的其他值，在 kube-apiserver 上。请注意，这个设置允许请求绕过身份验证和授权模块，且不能暴露到节点外部。在这种情况下，Sonobuoy
    将收集这个设置，然后 kube-bench 会将该值标记为检查失败。最后，**rancher-cis-benchmark** 工具会将所有检查汇总成一个优秀的报告，并通过电子邮件发送给安全团队。'
- en: What do the Cattle agents do?
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cattle 代理做什么？
- en: 'The Cattle agents that Rancher deploys on downstream clusters (that is, clusters
    that Rancher is managing) provide Rancher with access to the cluster and its nodes.
    This is done using two different sets of pods:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher 在下游集群（即 Rancher 正在管理的集群）上部署的 Cattle 代理为 Rancher 提供了访问该集群及其节点的权限。此操作通过两组不同的
    Pod 完成：
- en: '`localhost`. This tunnel will then allow connections for the Rancher server
    pod to the downstream cluster. Because of this, Rancher does not need firewall
    rules to open from the Rancher servers to the downstream cluster, including the
    need to port-forward, which can be a security issue. This WebSocket connection
    is held open by Rancher and the Cattle-cluster-agent, as if this connection drops,
    Rancher will lose access to the cluster until the connection can be restored.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`localhost`。这个隧道将允许 Rancher 服务器 Pod 与下游集群建立连接。因此，Rancher 无需在 Rancher 服务器和下游集群之间打开防火墙规则，也无需端口转发，这通常会带来安全问题。这个
    WebSocket 连接由 Rancher 和 Cattle-cluster-agent 保持打开，因为如果连接中断，Rancher 将失去对集群的访问权限，直到连接恢复。'
- en: '**Cattle-node-agent**: This runs as a **DaemonSet** on all nodes with a toleration
    that ignores just about everything. This pod uses the same kind of WebSocket connection
    as the previous example, with a TCP tunnel back to Rancher. Still, RKE uses this
    connection inside the Rancher server pod to provide a socket connection to the
    Docker Engine running on the node. This is needed for RKE to spin up the non-K8s
    containers that make up an RKE cluster.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cattle-node-agent**：这个代理作为 **DaemonSet** 在所有节点上运行，并具有忽略几乎所有内容的容忍性。这个 Pod
    使用与前面示例相同的 WebSocket 连接方式，并通过 TCP 隧道连接回 Rancher。不过，RKE 在 Rancher 服务器 Pod 内部使用此连接，以提供与节点上运行的
    Docker 引擎的套接字连接。这是 RKE 启动非 K8s 容器以构成 RKE 集群所需的。'
- en: Note
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Cattle-node-agent is only used in clusters where Rancher manages the cluster,
    that is, when Rancher built the cluster using RKE. For imported clusters, such
    as an Amazon EKS cluster, the Cattle-node-agent is not needed.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Cattle-node-agent 仅在 Rancher 管理的集群中使用，也就是说，当 Rancher 使用 RKE 构建集群时需要使用该工具。对于导入的集群，如
    Amazon EKS 集群，则不需要 Cattle-node-agent。
- en: Both agents use HTTPS to connect to the Rancher API. They do this by passing
    some environment variables into the pods. The first variable is `CATTLE_SERVER`;
    this variable is the hostname of the Rancher API. An example hostname is `rancher.example.com`.
    Note that there is no HTTP or HTTPS in this variable, as it is a requirement for
    the agents to connect to Rancher over an HTTPS connection. The second variable
    is `CATTLE_CA_CHECKSUM`, a `CATTLE_CA_CHECKSUM` variable, so long as they match,
    the agents will trust that HTTPS connection. It's important to note that if you
    renew the certificate in place, that is, without changing the chain, the `CATTLE_CA_CHECKSUM`
    variable will not change if you change certificates to a different authority –
    for example, if you are switching from a self-signed certificate to a publicly
    signed certificate from a company such as `CATTLE_CA_CHECKSUM` variable to have
    longer matches, thereby requiring manual work to update the agents. This process
    is documented at [https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool](https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 两个代理都使用 HTTPS 连接到 Rancher API。它们通过将一些环境变量传递到 pod 中来实现。第一个变量是 `CATTLE_SERVER`；该变量是
    Rancher API 的主机名。例如，主机名可以是 `rancher.example.com`。请注意，此变量中不包含 HTTP 或 HTTPS，因为代理需要通过
    HTTPS 连接到 Rancher。第二个变量是 `CATTLE_CA_CHECKSUM`，只要它们匹配，代理将信任该 HTTPS 连接。需要注意的是，如果您在不更改链的情况下更新证书，也就是说，只更换证书的颁发机构，`CATTLE_CA_CHECKSUM`
    变量不会改变—例如，如果您从自签名证书切换到一个像 `CATTLE_CA_CHECKSUM` 这样的公共签名证书，则需要手动更新代理，因为它们会匹配更长时间。该过程可以参考文档
    [https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool](https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool)。
- en: How does Rancher provision nodes and clusters?
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rancher 如何配置节点和集群？
- en: Rancher can provision a number of different nodes and clusters using the following
    methods. There are three main types of clusters in Rancher. *Rancher-created clusters
    using RKE*, *Rancher-created clusters using a hosted provider*, and *imported
    clusters*. Each of these types has subtypes, which we will describe in detail
    here.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher 可以使用以下方法配置不同的节点和集群。Rancher 中有三种主要类型的集群。*使用 RKE 创建的 Rancher 集群*、*使用托管提供商创建的
    Rancher 集群* 和 *导入的集群*。每种类型都有子类型，以下是详细说明。
- en: 'The Rancher-created clusters using RKE are as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RKE 构建的 Rancher 创建的集群如下：
- en: '**Rancher-created nodes**: One of the great things about Rancher is that if
    you choose Rancher, it can build the cluster for you, and they can manage the
    VMs themselves. This is done by using a tool called **Rancher-machine**. This
    tool is based on Docker Machine, which lets you create VMs and install Docker.
    Docker Machine does this by using driver plugins. These driver plugins act as
    a translation layer between Docker Machine and the virtualization provider – for
    example, **Amazon AWS**, **Linode**, **OVHcloud**, or VMware vSphere.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rancher 创建的节点**：Rancher 的一大优点是，如果您选择 Rancher，它可以为您构建集群，并且它们可以自己管理虚拟机。这是通过使用一个名为
    **Rancher-machine** 的工具来实现的。该工具基于 Docker Machine，允许您创建虚拟机并安装 Docker。Docker Machine
    通过使用驱动程序插件来实现这一点。这些驱动程序插件充当 Docker Machine 和虚拟化提供商之间的翻译层—例如，**Amazon AWS**、**Linode**、**OVHcloud**
    或 VMware vSphere。'
- en: How Docker Machine works is that you give it credentials to your virtualization
    provider and define the specifications on the VM, such as how many cores, how
    much RAM, and so on. Then, the driver plugin takes over to call the cloud provider's
    API endpoint to provision the VM. Docker Machine then creates an SSH key pair
    for each VM and then uses the driver plugin to push the SSH key to the VM. It
    then waits for the SSH connection to become available.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Machine 的工作方式是，您向它提供虚拟化提供商的凭据，并定义虚拟机的规格，例如 CPU 核心数、内存等。然后，驱动程序插件接管并调用云提供商的
    API 端点来配置虚拟机。接着，Docker Machine 为每个虚拟机创建一对 SSH 密钥，并使用驱动程序插件将 SSH 密钥推送到虚拟机。它随后等待
    SSH 连接可用。
- en: Once the SSH connection has been created, Docker Machine then installs Docker.
    This is where Rancher-machine comes into the picture. Rancher-machine builds on
    top of Docker Machine by adding additional driver plugins such as **DigitalOcean**
    and **Rackspace**. It then provides additional features such as implementing cloud-init.
    You can run other steps during the node provisioning process such as creating
    a filesystem for Docker or applying customizations to Docker Engine. Rancher provides
    higher-level functions such as defining node templates to deploy nodes in a repeatable
    process that is expanded even more by defining node pools (a group of nodes using
    node templates). Node pools allow Rancher to add and remove nodes from the group
    at will. For example, if a node crashes in the pool and doesn't recover during
    the default 15-minute timeout (customized), Rancher can create a new replacement
    VM and destroy the crashed node. This process can also be used to perform a rolling
    replacement of nodes for use cases where you don't want to *patch in place* but
    want to update your base image and recreate all of your nodes in a rolling fashion.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦SSH连接建立，Docker Machine就会安装Docker。这时，Rancher-machine便发挥作用。Rancher-machine是在Docker
    Machine的基础上，通过添加额外的驱动插件，如**DigitalOcean**和**Rackspace**，来扩展功能。它还提供了更多功能，如实现cloud-init。在节点配置过程中，你可以执行其他步骤，比如为Docker创建文件系统或对Docker引擎应用自定义设置。Rancher提供了更高层次的功能，如定义节点模板，以便在可重复的过程中部署节点，并通过定义节点池（使用节点模板的节点组）进一步扩展这一过程。节点池使Rancher能够随时向组中添加或移除节点。例如，如果池中的某个节点崩溃并且在默认的15分钟超时（可定制）内没有恢复，Rancher可以创建一个新的虚拟机替代节点并销毁崩溃的节点。这个过程还可以用于执行节点的滚动替换，适用于你不希望*就地修补*而是希望更新基础镜像并以滚动方式重建所有节点的使用场景。
- en: '**Bring your own nodes**: These nodes are for use cases where you would like
    or need to create the VMs yourself or use physical servers. In this case, you
    will define your cluster configuration in Rancher. Then, Rancher will create a
    command for you to run that looks like the following:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自带节点**：这些节点适用于你希望或需要自己创建虚拟机或使用物理服务器的场景。在这种情况下，你将会在Rancher中定义集群配置。然后，Rancher会为你生成一个命令，你只需运行这个命令，如下所示：'
- en: '[PRE0]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's break down this command. First, this is a Docker command that can run
    on any Linux host that has Docker installed. The next part is `run`, which says
    to create a new container, with the next flag being `-d`, which says to run in
    *detached* mode. This will start the container and put it in the background. The
    `–privileged` flag then tells Docker that it will be a privileged container –
    meaning that this container can access all of the devices on the host. Think of
    it like running the process directly on the host operating system with little
    to no limits. The `--restart=unless-stopped` flag just tells Docker to keep restarting
    this container until we tell it to stop. Next is the `--net=host` flag, which
    gives the container the same network as the host. Therefore, the container's IP
    will be the host's IP. The next two flags pass the `/etc/kubernetes` and `//var/run`
    directories inside the container. The `/etc/kubernetes` directory is used to store
    node-level configuration files and, most importantly, the SSL certificates used
    for the K8s components.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分解这个命令。首先，这是一个可以在任何已安装Docker的Linux主机上运行的Docker命令。接下来是`run`，它表示创建一个新的容器，紧接着的标志是`-d`，表示以*分离模式*运行。这将启动容器并将其放在后台。`–privileged`标志告诉Docker这是一个特权容器——意味着该容器可以访问主机上的所有设备。可以把它理解为直接在主机操作系统上运行进程，几乎没有任何限制。`--restart=unless-stopped`标志仅表示Docker会不断重启这个容器，直到我们告诉它停止。接下来是`--net=host`标志，它为容器分配与主机相同的网络。因此，容器的IP将是主机的IP。接下来的两个标志将`/etc/kubernetes`和`//var/run`目录挂载到容器内。`/etc/kubernetes`目录用于存储节点级的配置文件，最重要的是存储K8s组件所使用的SSL证书。
- en: 'The following section is the container image and tag. This image will match
    the Rancher version, and this image includes all of the binaries that will be
    needed to bootstrap this node. The `--server` flag is the Rancher API server path.
    This will be passed into the container, creating and tunneling back to the Rancher
    leader pod (please see the *What do the Cattle agents do?* section earlier in
    this chapter for more details). Next, we have the `–token` flag. This is used
    to authenticate the agent to the Rancher server and tie this agent to a cluster.
    Each cluster will have a unique token, but all of the agents in a cluster will
    share the same token. Finally, we have the `role` flags. These flags are used
    to assign the different roles of the RKE cluster to the node. Note that nodes
    can have more than one role, but a cluster requires at least one node for each
    role: one etcd node, one control plane, and one worker node. You can mix and match
    roles as you choose, but there are best practices for this that should be followed.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分是容器镜像和标签。该镜像将与 Rancher 版本相匹配，并且该镜像包含启动此节点所需的所有二进制文件。`--server` 标志是 Rancher
    API 服务器路径。这将被传递到容器中，并返回连接到 Rancher 领导者 Pod（有关更多详细信息，请参阅本章前面的*Cattle 代理做了什么？*部分）。接下来，我们有
    `–token` 标志。它用于将代理认证到 Rancher 服务器并将该代理与集群绑定。每个集群都有一个唯一的令牌，但集群中的所有代理将共享相同的令牌。最后，我们有
    `role` 标志。该标志用于为节点分配 RKE 集群的不同角色。请注意，节点可以拥有多个角色，但集群至少需要每个角色有一个节点：一个 etcd 节点，一个控制平面节点和一个工作节点。您可以根据需要混合和匹配角色，但有一些最佳实践应遵循。
- en: 'In both Rancher-created nodes and *bring your own nodes*, once the bootstrap
    agent has been successfully started on the node, the agent will tunnel back to
    the Rancher leader pod and register the new node in Rancher RKE. It then uses
    the registered nodes to dynamically create the `cluster.yaml` file using the registered
    or registering nodes to the cluster. If this cluster has already been successfully
    started once before, Rancher will also pull `cluster.rkestate` from the CRD `clusters.management.cattle.io`
    object. This file includes the current state of the cluster, the root and server
    certificates, and the authentication tokens that RKE will use to communicate to
    the cluster. Then, the cluster controller will use the port binding on the Rancher
    leader pod to connect the Docker engines on the nodes. At this point, RKE will
    create the certificates and configuration files, deploy them to the nodes, and
    start creating/updating the etcd cluster. RKE performs this process in a serial
    fashion, working on only one node at a time, and if RKE runs into any issues,
    it will throw an error and exit the function. Also, an etcd backup is taken on
    each etcd node for existing clusters before making any changes. Once the etcd
    plane has been successfully started, RKE will begin working on the control plane,
    where RKE will start up the kube-apiserver objects, kube-controller-manager, and
    kube-scheduler, working again in a serial fashion by running one node at a time
    and running health checks as it goes. And again, if any step in this process fails,
    RKE will fail too. Finally, RKE will come to the worker plane. This process is
    different, because it is designed to create a parallel to doing multiple worker
    nodes at once, and it will continue even if a failure happens, so long as the
    settings defined in the `zero downtime` configuration have not been violated.
    The default settings are only one etcd or control plane node down at any given
    time, with up to 10% of the worker nodes down. Note, this number is rounded down
    to the nearest node, with a minimum of one node per batch:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Rancher 创建的节点和 *自带节点* 中，一旦引导代理成功启动，代理将回传到 Rancher 主节点，并在 Rancher RKE 中注册新节点。然后，它使用已注册的节点动态创建
    `cluster.yaml` 文件，利用已注册或正在注册的节点将其加入集群。如果该集群之前已经成功启动过一次，Rancher 还会从 CRD `clusters.management.cattle.io`
    对象中拉取 `cluster.rkestate` 文件。此文件包含集群的当前状态、根证书和服务器证书，以及 RKE 用于与集群通信的身份验证令牌。然后，集群控制器将使用
    Rancher 主节点上的端口绑定连接节点上的 Docker 引擎。在此过程中，RKE 将创建证书和配置文件，部署到节点，并开始创建/更新 etcd 集群。RKE
    会按顺序执行此过程，每次只处理一个节点，如果遇到问题，它将抛出错误并退出功能。同时，在对现有集群进行任何更改之前，RKE 会在每个 etcd 节点上执行备份。等到
    etcd 平面成功启动后，RKE 将开始处理控制平面，此时 RKE 会启动 kube-apiserver 对象、kube-controller-manager
    和 kube-scheduler，依然按顺序操作，每次运行一个节点，并在执行过程中进行健康检查。同样，如果过程中任何步骤失败，RKE 也会失败。最后，RKE
    将进入工作节点平面。这个过程有所不同，因为它旨在并行处理多个工作节点，即使发生故障，也会继续进行，只要没有违反 `零停机` 配置中定义的设置。默认设置为在任何给定时间最多允许一个
    etcd 或控制平面节点宕机，最多 10% 的工作节点宕机。请注意，该数字会向下取整至最近的节点，且每批次至少有一个节点。
- en: '**Rancher-created clusters using a hosted provider**: One of the nice things
    about Rancher is you can use a hosted K8s cluster such as AWS EKS, Google GKE,
    or Azure AKS if you don''t want to deal with VMs and just want to let your cloud
    provider manage the VMs for you. Rancher can help by using the cloud provider''s
    **software development kit** (**SDK**) to provide the cluster for you. This is
    mainly for reasons of convenience and consistency, as there are no unique or hidden
    options that Rancher has that you can''t do yourself. As part of its new hosted
    cluster option in v2, Rancher also allows for the three-way synchronization of
    configurations between Rancher, the cloud provider, and the end user. What is
    remarkable is that if you want to change some settings for your AWS EKS cluster,
    you can manage it directly in the AWS console and your changes will be reflected
    in Rancher. Note that this can be done for RKE clusters too but requires a few
    extra steps.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用托管提供商创建的Rancher集群**：Rancher的一个优点是，如果你不想处理虚拟机，只想让云提供商管理虚拟机，你可以使用托管的K8s集群，如AWS
    EKS、Google GKE或Azure AKS。Rancher可以通过使用云提供商的**软件开发工具包**（**SDK**）为你提供集群。这主要是出于便捷性和一致性的考虑，因为Rancher没有独特或隐藏的选项是你自己无法完成的。作为v2版本中新托管集群选项的一部分，Rancher还允许Rancher、云提供商和最终用户之间的三方配置同步。值得注意的是，如果你想更改AWS
    EKS集群的一些设置，你可以直接在AWS控制台管理，它的更改会在Rancher中反映出来。请注意，RKE集群也可以这样做，但需要额外的步骤。'
- en: '**Imported K8s clusters**: Finally, if you don''t want Rancher to manage your
    clusters whatsoever, but you do want Rancher to be a friendly web UI for your
    cluster, you can utilize the excellent *convenience features* of Rancher such
    as **Active Directory** (**AD**) authentication, web kube-proxy access, and more.
    You can import the cluster where Rancher will deploy the cluster-Cattle-agent
    on the cluster but will not have access to items such as etcd, kubelet, the Docker
    CLI, and more. In this instance, Rancher will only be able to access the kube-apiserver
    endpoint. Note that Rancher supports any certificated K8s distribution for the
    imported cluster option, and this can include *K8s the hard way*, EKS, a self-managed
    RKE cluster, or even a K3s/RKE2 cluster. As of Rancher v2.6.0, K3s and RKE2 clusters
    are unique in that they can be imported into Rancher and Rancher can then take
    over management of the cluster moving forward. Please note that this is still
    a new process and has its limitations and bugs.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**导入的K8s集群**：最后，如果你不希望Rancher管理你的集群，但又希望Rancher作为你集群的友好Web UI，你可以利用Rancher的优秀*便捷功能*，例如**Active
    Directory**（**AD**）身份验证、Web kube-proxy访问等。你可以导入集群，Rancher会在集群上部署cluster-Cattle-agent，但不会访问诸如etcd、kubelet、Docker
    CLI等项。在这种情况下，Rancher只能访问kube-apiserver端点。请注意，Rancher支持任何已认证的K8s发行版作为导入集群选项，这包括*K8s
    the hard way*、EKS、自管理RKE集群，甚至是K3s/RKE2集群。自Rancher v2.6.0起，K3s和RKE2集群有其独特之处，它们可以被导入到Rancher中，Rancher随后可以接管集群的管理。请注意，这仍然是一个新过程，存在一些限制和bug。'
- en: What are kube-apiserver, kube-controller-manager, kube-scheduler, etcd, and
    kubelet?
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是kube-apiserver、kube-controller-manager、kube-scheduler、etcd和kubelet？
- en: The etcd object is a distributed and consistent key-value pair database. **CoreOS**
    initially developed etcd to handle OS upgrades in cluster management systems and
    store configuration files in 2013\. Because of this, etcd needed to be highly
    available and consistent. The etcd object is currently affiliated with the CNCF
    and has been widely adopted in the industry. An etcd cluster is based on the idea
    of maintaining consistency across nodes – most clusters contain three or five
    nodes, and there is a requirement that there be an odd number of nodes. This is
    due to the requirements of the Raft consensus algorithm. This algorithm selects
    a master node, which etcd calls the *leader*. This node is responsible for synchronizing
    data between nodes. If the leader node fails, another election will happen, and
    another node will take over this role. The idea here is that etcd is built on
    the concept of a *quorum*. This means that more than half of the nodes in the
    cluster must be in consensus. In a standard three-node cluster, the etcd cluster
    will continue to accept writes if a single node fails, but if two nodes fail,
    the surviving etcd node will take the safest option and go into read-only mode
    until a quorum can be restored in the cluster. A five-node cluster is the same,
    but it requires three of the five nodes to fail to lose service. All write processes
    go to the etcd leader node, which are written to the Raft log and then broadcast
    to all cluster nodes during operations. Once the majority of the nodes have successfully
    acknowledged the write (that is, two nodes in a three-node cluster and threes
    nodes in a five-node cluster), the Raft log entry is committed, and the write
    is acknowledged back to the client. If a majority of the nodes do not acknowledge
    the write, then the write will fail and will not be committed. Because of Raft,
    adding more nodes to the cluster will increase the fault tolerance, but this also
    increases the load on the leader node without improving performance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: etcd 对象是一个分布式且一致的键值对数据库。**CoreOS** 最初在 2013 年开发 etcd，用于处理集群管理系统中的操作系统升级并存储配置文件。因此，etcd
    需要具备高可用性和一致性。etcd 对象目前隶属于 CNCF，并已在业界广泛采用。etcd 集群基于在节点之间保持一致性的理念——大多数集群包含三个或五个节点，并且有一个要求，即节点数必须为奇数。这是由于
    Raft 共识算法的要求。该算法选择一个主节点，etcd 称之为 *leader*。该节点负责在节点之间同步数据。如果主节点失败，将会进行另一次选举，另一个节点将接管这个角色。这里的理念是，etcd
    构建在 *法定人数*（quorum）的概念上。这意味着集群中超过一半的节点必须达成一致。在一个标准的三节点集群中，如果单个节点失败，etcd 集群将继续接受写入操作，但如果有两个节点失败，幸存的
    etcd 节点将采取最安全的选项，进入只读模式，直到集群能够恢复法定人数。五节点集群也是如此，但需要三个节点失败才会失去服务。所有写入操作都由 etcd leader
    节点处理，这些写入会记录到 Raft 日志中，并在操作过程中广播到所有集群节点。一旦大多数节点成功确认写入（即三节点集群中两个节点，五节点集群中三个节点），Raft
    日志条目就会被提交，写入操作会反馈给客户端。如果大多数节点没有确认写入，则写入会失败，且不会被提交。由于 Raft，向集群中添加更多节点会增加容错能力，但这也会增加
    leader 节点的负载，而不会改善性能。
- en: For now, etcd stores the data because etcd is built on top of **BoltDB**, which
    writes its data into a single memory-mapped file. This means the operating system
    is responsible for handling the data caching and will keep as much data in memory
    as possible – this is why etcd can be a *memory hog* and requires a high-speed
    disk, preferably an **SSD** or **NVME**. Then for the data, etcd uses **multiversion
    concurrency control** (**MVCC**) to handle concurrent write operations safely.
    The MVCC works in conjunction with Raft, where every write is tracked by a revision.
    By keeping a history of the revisions, etcd can provide the version history of
    all of the keys. This impacts read performance because key-value pairs are written
    to disk in the order created in the transaction log, not by an index (as in a
    traditional database). This means that key-value pairs written simultaneously
    are faster to read than key-value pairs written at different times. However, with
    etcd keeping revisions over time, the disk and memory usage can grow very large.
    Even if you delete a large number of keys from etcd, the space will continue to
    grow since the prior history of those keys will still be retained. This is where
    etcd compaction and defragmenting come into the picture wherein etcd will drop
    superseded revisions, that is, older data that has been *overwritten,* where the
    memory-mapped file will have several holes so etcd will then run a defrag to release
    free pages back to the operating system. However, it is essential to note that
    all incoming reads and writes will be blocked during a defragmentation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，etcd 存储数据是因为 etcd 基于 **BoltDB** 构建，BoltDB 将数据写入一个单一的内存映射文件。这意味着操作系统负责处理数据缓存，并尽可能将数据保留在内存中——这就是为什么
    etcd 可能会成为 *内存消耗大户* 并且需要高速磁盘，最好是 **SSD** 或 **NVME**。对于数据，etcd 使用 **多版本并发控制**（**MVCC**）来安全地处理并发写操作。MVCC
    与 Raft 协作，每次写操作都会通过修订版本进行跟踪。通过保留修订历史，etcd 可以提供所有键的版本历史。这会影响读取性能，因为键值对是按事务日志中的创建顺序写入磁盘，而不是按索引（如传统数据库中）。这意味着同时写入的键值对比不同时间写入的键值对更快读取。然而，随着
    etcd 随时间保留修订，磁盘和内存的使用可能会变得非常大。即使从 etcd 删除大量键，空间也会继续增长，因为这些键的历史仍然会被保留。这就是 etcd
    压缩和碎片整理的作用，etcd 会删除被替代的修订，即已被 *覆盖* 的旧数据，内存映射文件会出现多个空洞，etcd 会运行碎片整理以将空闲页面释放回操作系统。然而，必须注意的是，在碎片整理过程中，所有传入的读取和写入都会被阻塞。
- en: kube-apiserver is a critical component in a K8s cluster, as it is the server
    that provides the REST API endpoint for the whole cluster. kube-apiserver is the
    only K8s component that connects to the etcd cluster and acts as an access point
    for all of the other K8s components. Now, kube-apiserver is intended to be relatively
    simple, with most of the business logic being done by other controllers and plugins.
    But one of its primary responsibilities is authentication and **RBAC** (**role-based
    access control**). The default access control behavior is that all clients should
    be authenticated to interact with kube-apiserver.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: kube-apiserver 是 K8s 集群中的关键组件，因为它是为整个集群提供 REST API 端点的服务器。kube-apiserver 是唯一与
    etcd 集群连接的 K8s 组件，并充当所有其他 K8s 组件的访问点。目前，kube-apiserver 旨在保持相对简单，大部分业务逻辑由其他控制器和插件完成。但它的主要责任之一是身份验证和
    **RBAC**（**基于角色的访问控制**）。默认的访问控制行为是，所有客户端都必须通过身份验证才能与 kube-apiserver 交互。
- en: The other central role that kube-apiserver serves is managing secret encryption.
    By default, K8s stores secrets in plain text inside the etcd database. This can
    be a security issue, as secrets store items like passwords, database connection
    strings, and so on. To protect secrets, kube-apiserver supports an encryption
    provider. What happens is, any time a secret is created or updated, kube-apiserver
    will call the encryption provider to access the encryptions algorithm and keys
    to encrypt the data, then send this data to the etcd cluster. Then whenever a
    secret is read from the etcd cluster, kube-apiserver uses the reverse process
    to decrypt the data before sending the response back to the client. Because of
    this, the clients are unaware that secrets are encrypted, with the only impact
    being performance. The standard for Rancher is to use `aescbc` for its encryption
    algorithm, as this provides a good balance between performance and strength, and
    also has the added benefit that most modern CPUs support AES with CBC mode in
    hardware. As a result, encryption and decryption performance are usually not an
    issue.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: kube-apiserver 的另一个核心功能是管理秘密的加密。默认情况下，K8s 将秘密以明文存储在 etcd 数据库中。这可能会带来安全问题，因为秘密存储了像密码、数据库连接字符串等信息。为了保护秘密，kube-apiserver
    支持加密提供者。具体来说，每当创建或更新一个秘密时，kube-apiserver 会调用加密提供者来访问加密算法和密钥以加密数据，然后将数据发送到 etcd
    集群。然后每当从 etcd 集群读取秘密时，kube-apiserver 会使用反向过程解密数据，再将响应发送回客户端。因此，客户端无法察觉秘密已被加密，唯一的影响是性能。Rancher
    的标准是使用 `aescbc` 作为其加密算法，因为它在性能和强度之间提供了良好的平衡，而且大多数现代 CPU 都支持硬件加速 AES CBC 模式。因此，加密和解密性能通常不是问题。
- en: Another one of the critical things to remember about kube-apiserver is that
    it's stateless; besides some in-memory caching, kube-apiserver stores no data.
    This means kube-apiserver is great for horizontal scaling. It also has no leader
    election process as there is no leader node. So, typically, clusters will have
    at least two nodes running kube-apiserver, but you can have more with larger clusters.
    You also don't have the limitation of old numbers of nodes in the way you do with
    etcd. The kube-apiserver is also where a lot of core cluster configuring happens.
    For example, when using a cloud provider such as AWS or VMware vSphere, you need
    to create a configure file and pass that file path into the kube-apiserver component
    as a command-line flag. Note, kube-apiserver does not support hot changing settings
    and requires a restart to alter its configurations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 kube-apiserver 需要记住的另一个关键点是它是无状态的；除了某些内存缓存外，kube-apiserver 不存储任何数据。这意味着 kube-apiserver
    非常适合水平扩展。它也没有领导者选举过程，因为没有领导者节点。因此，通常情况下，集群至少会有两个节点运行 kube-apiserver，但在更大的集群中，你可以有更多的节点。你也不需要像
    etcd 那样面临老旧节点数量的限制。kube-apiserver 还是许多核心集群配置操作发生的地方。例如，当使用 AWS 或 VMware vSphere
    等云提供商时，你需要创建一个配置文件，并将该文件路径作为命令行标志传递给 kube-apiserver 组件。请注意，kube-apiserver 不支持热更改设置，修改其配置需要重启。
- en: The K8s controller manager, kube-controller-manager, is the core controller
    for K8s and is typically called the *controller for controllers*, as its main
    job is to sit in a non-terminating loop that regulates the state of the cluster.
    It connects to the kube-apiserver component and creates several watch handles
    that monitor the current state of the cluster and compare it to the desired state.
    The kube-controller-manager component does this by having several smaller controllers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: K8s 控制器管理器 kube-controller-manager 是 K8s 的核心控制器，通常被称为 *控制器的控制器*，因为它的主要任务是处于一个非终止循环中，调节集群的状态。它连接到
    kube-apiserver 组件，并创建多个监视句柄，监控当前集群的状态，并将其与期望的状态进行比较。kube-controller-manager 组件通过拥有多个较小的控制器来实现这一点。
- en: The first of these smaller controllers is the *replication controller*. This
    controller ensures that the specified number of pods in a **ReplicaSet** are running
    at any given time. An example of this is if a ReplicaSet has five pods in the
    desired state, but it only has hour pods in the current state. The replication
    controller will create a new pod object in the unscheduled state, thereby bringing
    the ReplicaSet back up to the required five pods. Another example is when a node
    fails – here, the replication controller will see that the pod has been disabled,
    deleted, or terminated, and it will create a new pod object again. The replication
    controller also handles terminating pods when the current state is greater than
    the desired state. There is some business logic built inside the termination process.
    Here, the main rule is that pods that are currently not in the ready status, that
    is, they are pending or failed, are the first pods to be set for terminations,
    with the oldest pods being the next in line. Note that the replication controller
    does not delete pods or call nodes directly, or even connect to other controllers.
    All communication happens between the controller and kube-apiserver.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个较小的控制器是*副本控制器*。该控制器确保在任何给定时间，**ReplicaSet**中指定数量的pods都在运行。例如，如果一个ReplicaSet期望有五个pod处于所需状态，但当前只有四个pod，副本控制器将创建一个新的pod对象，处于未调度状态，从而使ReplicaSet恢复到所需的五个pod。另一个例子是当一个节点发生故障时——在这种情况下，副本控制器会发现pod已被禁用、删除或终止，然后它会再次创建一个新的pod对象。副本控制器还负责在当前状态大于所需状态时终止pods。在终止过程中有一些业务逻辑。这里的主要规则是，当前不处于就绪状态的pods（即挂起或失败的pods）会首先被设置为终止，接下来是最旧的pods。需要注意的是，副本控制器不会直接删除pods或调用节点，也不会与其他控制器进行连接。所有通信都是在控制器和kube-apiserver之间进行的。
- en: The second controller is the *endpoints controller*, which is responsible for
    maintaining the endpoints that join services and their assigned pods, with endpoint
    and service records being part of the cluster DNS system. K8s needs to track pods
    being created and deleted in the cluster to update those records with the correct
    IP addresses.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个控制器是*端点控制器*，负责维护连接服务的端点及其分配的pods，其中端点和服务记录是集群DNS系统的一部分。K8s需要跟踪在集群中创建和删除的pods，以便用正确的IP地址更新这些记录。
- en: The third controller is the *service account and token controller*. This controller
    is responsible for creating and managing service accounts in the cluster and creating
    tokens for the service accounts to use to authenticate the kube-apiserver component.
    Note that the tokens are stored as secrets in the namespace where the service
    accounts are hosted.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个控制器是*服务账户和令牌控制器*。该控制器负责在集群中创建和管理服务账户，并为服务账户创建令牌，以便用于身份验证kube-apiserver组件。需要注意的是，令牌作为机密存储在服务账户所在的命名空间中。
- en: The fourth controller is the *node controller*. This controller is responsible
    for watching the status of the nodes in the cluster by watching the node leases
    that kubelet is periodically updating by sending a heartbeat. Suppose a node lease
    violates the node timeout that is five minutes by default. The node controller
    will decide that this node must be down and start the pod eviction process wherein
    the controller updates the pods running on that node with the status of `Unknown`
    and will taint the node object with the taint of `unschedulable`. This will trigger
    the replication controller to begin deleting pods that cannot tolerate that taint.
    It is essential to remember the following things about this process. First, K8s
    has no way of knowing if a node is genuinely down or if it's just having issues
    communicating with the kube-apiserver, which means if a kubelet on the node crashes
    or locks up, then the pods running on the node will continue to run without issue,
    even though the node controller has flagged them and the replication controller
    has deleted them. And also, because K8s has no way for the cluster to block I/O
    to the failed node, you could run into a split-brain issue with the same pod/application
    running in two locations simultaneously. You also must remember that pods that
    have tolerations for the `unschedulable` taint will not be rescheduled.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个控制器是*节点控制器*。这个控制器负责通过监视kubelet定期更新的节点租约来观察集群中节点的状态。假设一个节点租约违反了默认的五分钟节点超时时间，节点控制器将决定该节点必须处于宕机状态，并启动Pod驱逐过程，其中控制器会将该节点上运行的Pod的状态更新为`Unknown`，并会将`unschedulable`的污点标记到节点对象上。这将触发副本控制器开始删除无法容忍该污点的Pod。记住以下几点是非常重要的。首先，K8s无法知道一个节点是否真的宕机，还是仅仅与kube-apiserver通信出现了问题，这意味着如果节点上的kubelet崩溃或卡住，那么该节点上运行的Pod将继续运行，不会有问题，即使节点控制器已经标记它们，并且副本控制器已经删除了它们。此外，因为K8s没有办法阻止集群对失败节点的I/O操作，可能会遇到“脑裂”问题，即相同的Pod/应用程序在两个位置同时运行。还需要记住，具有`unschedulable`污点容忍的Pod将不会被重新调度。
- en: An example of this is a canal pod that has a toleration for any taint placed
    on a node, meaning this pod will be scheduled on a node no matter what. The next
    thing to remember is that the eviction process does have rate limiting, which,
    by default, will evict pods at a rate of ten pods per second. This is to prevent
    a flood of pods from being rescheduled in the cluster. Finally, only a single
    kube-controller-manager is allowed to be active at any one time. It does this
    by using a leader election process. All kube-controller-manager processes try
    to grab a lease in kube-apiserver. One process becomes the leader, allowing the
    other controllers to start taking action in the cluster. The leader will continue
    to refresh this lease while the other nodes continue monitoring the lease, comparing
    the last renewal timestamp to the expiration timestamp. If the lease is ever allowed
    to expire, the standby kube-controller-manager will race to become the new leader.
    All this being said, scaling the kube-controller-manager horizontally only improves
    fault tolerance and will not improve performance.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是一个能够容忍节点上任何污点的canal Pod，这意味着无论如何这个Pod都会被调度到节点上。接下来需要记住的是，驱逐过程是有限速的，默认情况下，Pod驱逐速率为每秒十个Pod。这是为了防止在集群中重新调度大量Pod。最后，任何时候只允许一个kube-controller-manager处于活动状态。它通过使用领导选举过程来实现这一点。所有kube-controller-manager进程都会尝试在kube-apiserver中抢占租约。一个进程成为领导者，允许其他控制器开始在集群中采取行动。领导者将继续刷新此租约，而其他节点则继续监视租约，将最后的续约时间戳与到期时间戳进行比较。如果租约允许过期，备用的kube-controller-manager将争夺成为新的领导者。综上所述，水平扩展kube-controller-manager仅提高了容错性，并不会提高性能。
- en: kube-scheduler is the controller that handles assigning pods to nodes. It does
    this by watching for unscheduled pods, at which point kube-scheduler is evaluating
    nodes. kube-scheduler first builds a list of nodes that meets all of the requirements.
    For example, if a pod requires a node selector rule, only nodes with that label
    will be added to the node candidate list. Next, kube-scheduler will evaluate the
    taints and tolerations of the nodes and pods. For example, by default, a pod will
    not tolerate the node with taint scheduling disabled. This taint is typically
    applied to master, etcd, or control-plane nodes. In this case, the node wouldn't
    be added to the node candidate list. Next, kube-scheduler will create what it
    calls a node *score*. This score is based on the availability of the resources,
    such as CPU, memory, disk, network, and more, that are available on the node.
    For example, a node that is underutilized, that is, with a lot of CPU and memory
    available will score higher than a node that is highly utilized, that is, with
    little to no CPU or memory available. Once all of the scores are calculated, kube-scheduler
    will sort them from the highest to lowest. If there is only one node with the
    highest score, then that node wins. If there is a tie, kube-scheduler will randomly
    pick a winner from the nodes that tied. Finally, kube-scheduler will update the
    pod object in kube-apiserver with its node assignment. An important thing to remember
    is kube-scheduler can be tuned and even replaced with other third-party schedulers.
    This is mainly done for environments with burst workloads where many pods will
    be created at once. This is because kube-scheduler doesn't know about resource
    utilization and its pods over time. It only gets whatever the current value is
    at scheduling. So, what can happen is one node will get flooded with new pods
    and fall over. Those pods are then rescheduled on a new node, which in turn knocks
    that node over as well. But during that event, the first node is recovered, and
    so now all of the new pods will go to that node because it is empty, and the process
    repeats repeatedly. There is also an essential idea to understand here, kube-scheduler
    only touches a pod at the time of creation. Once the pod has been scheduled to
    a node, that's it. The kube-scheduler component does not rebalance or move pods
    around. There are also tools like kube-descheduler that can fill in this gap and
    help you balance your cluster.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: kube-scheduler 是处理将 pods 分配到节点的控制器。它通过监控未调度的 pods 来执行此操作，此时 kube-scheduler 正在评估节点。kube-scheduler
    首先构建一个满足所有要求的节点列表。例如，如果一个 pod 需要节点选择器规则，则只有具有该标签的节点才会被添加到节点候选列表中。接下来，kube-scheduler
    会评估节点和 pods 的污点和容忍度。例如，默认情况下，pod 不会容忍污点调度已禁用的节点。该污点通常应用于 master、etcd 或控制平面节点。在这种情况下，该节点不会被添加到节点候选列表中。接下来，kube-scheduler
    会创建它所称的节点 *评分*。这个评分是基于节点上可用资源的可用性，比如 CPU、内存、磁盘、网络等。例如，一个未充分利用的节点，即有大量可用的 CPU 和内存，其评分会高于一个高度利用的节点，即几乎没有可用的
    CPU 或内存。一旦所有的评分都计算完成，kube-scheduler 会按从高到低的顺序进行排序。如果只有一个节点拥有最高的评分，那么该节点会被选中。如果存在平局，kube-scheduler
    会从平局的节点中随机选择一个胜者。最后，kube-scheduler 会在 kube-apiserver 中更新 pod 对象的节点分配。一个重要的要点是，kube-scheduler
    可以进行调优，甚至可以用其他第三方调度器来替代它。这主要是在处理突发工作负载的环境中进行的，这些环境中会同时创建多个 pods。因为 kube-scheduler
    并不知道资源的利用情况以及它的 pods 随时间的变化。它只会获取调度时的当前值。因此，可能会发生一个节点被新的 pods 填满并崩溃。然后这些 pods
    会被重新调度到一个新的节点，而这个节点也会崩溃。但在此事件期间，第一个节点会恢复，所以现在所有新的 pods 都会去这个节点，因为它已经空了，而这个过程会反复循环。还有一个必须理解的重要概念是，kube-scheduler
    只会在 pod 创建时接触该 pod。一旦 pod 被调度到一个节点上，事情就结束了。kube-scheduler 组件不会重新平衡或移动 pods。还有像
    kube-descheduler 这样的工具可以填补这一空白，帮助你平衡集群。
- en: In simple terms, kubelet is the node agent in K8s. kubelet runs on every worker
    node, and in the case of RKE, all nodes. kubelet has several different roles and
    responsibilities. The first is taking the pod specifications in kube-apiserver
    that have the node assignments of the node where kubelet is running. kubelet then
    compares that pod specification to the current state of the node. kubelet does
    this by connecting to Docker or containerd to gather what containers are currently
    running on the node. Then, if there is a difference, kubelet will create or destroy
    the containers for them to match. The kubelet component's second responsibility
    is *pod health*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，kubelet 是 K8s 中的节点代理（node agent）。kubelet 在每个工作节点上运行，在 RKE 的情况下，所有节点上都有
    kubelet。kubelet 承担了多个不同的角色和职责。首先，它获取在 kube-apiserver 中的 Pod 规范，这些规范包含了 kubelet
    正在运行的节点的节点分配信息。然后，kubelet 将这些 Pod 规范与节点的当前状态进行比较。kubelet 通过连接到 Docker 或 containerd
    来收集当前在节点上运行的容器信息。如果存在差异，kubelet 将创建或销毁容器以使其匹配。kubelet 组件的第二个职责是*Pod 健康*。
- en: Pods can have *probes* defined as a part of their specifications. These include
    *liveness probes*, which are responsible for checking whether the application
    inside a pod is healthy. These checks are simple `HTTP GET` request to `/` every
    5 seconds to confirm that NGINX is up and responding to requests.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 可以在其规范中定义*探针*（probes）。这些探针包括*存活探针*（liveness probes），用于检查 Pod 内部的应用程序是否健康。这些检查是每
    5 秒发送一次简单的 `HTTP GET` 请求到 `/`，以确认 NGINX 是否正常运行并响应请求。
- en: Note
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `kubelet` component only accepts `200 OK` responses as evidence of a healthy
    request. All other response codes will return a failure. Another type of probe
    is called the *startup probe*. This probe is similar to the liveness probe but
    mainly tells `kubelet` that a pod has successfully started.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubelet` 组件只接受 `200 OK` 响应作为健康请求的证据。所有其他响应代码都会返回失败。另一种类型的探针称为*启动探针*（startup
    probe）。这种探针类似于存活探针，但主要告诉 `kubelet` Pod 已成功启动。'
- en: An example might be a database inside a pod where the database could take a
    few minutes to fully start up. If you just used the liveness probes, you would
    need to space out your schedule to allow the database to start before kubelet
    killed the pod entirely. So, you'd want to use a *startup probe* with a delay
    of a minute or two, then, once that is successful, the liveness probe could take
    over and run every few seconds. Finally, the *readiness probe* is very similar
    to the startup probe, but it is used for controlling when K8s can start sending
    traffic to a pod. An example of this could be a web server that might start up
    and be healthy but can't connect to a backend database. In this case, you don't
    want kubelet to kill the pod, as it is fine, but you also don't want to start
    sending traffic to the pod until it can connect to the database.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子可能是一个位于 Pod 内的数据库，其中数据库可能需要几分钟才能完全启动。如果你只使用了存活探针（liveness probes），你需要将时间安排间隔开来，以便在
    kubelet 完全杀死 Pod 之前允许数据库启动。因此，你可能需要使用一个*启动探针*（startup probe），并设置一到两分钟的延迟，之后一旦启动探针成功，存活探针可以接管，并每隔几秒钟运行一次。最后，*就绪探针*（readiness
    probe）与启动探针非常相似，但它用于控制 K8s 何时可以开始向 Pod 发送流量。一个例子可能是一个已经启动且健康的 Web 服务器，但无法连接到后台数据库。在这种情况下，你不希望
    kubelet 杀死 Pod，因为它是健康的，但你也不希望在它能够连接到数据库之前开始向 Pod 发送流量。
- en: How do the current state and the desired state work?
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当前状态与期望状态是如何工作的？
- en: '**Desired state** is one of the core concepts of Rancher and K8s. The idea
    is that you should declare the state of an object (for example, a pod, deployment,
    or volume) as you would like it to be. Then, the cluster should report the current
    state of the object, at which point it''s the role of the controller (the kube-controller-manager
    component in the case of most of K8s core objects) to compare these two states.
    If no difference is found, don''t do anything, but if a discrepancy is found,
    the controller''s job is to create a *plan* for making the current state match
    the desired state.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**期望状态** 是 Rancher 和 K8s 的核心概念之一。其理念是，你应该声明对象的状态（例如，Pod、部署或存储卷）应当是什么样子。然后，集群应报告该对象的当前状态，在此时，控制器（对于大多数
    K8s 核心对象而言，通常是 kube-controller-manager 组件）的角色是比较这两种状态。如果没有发现差异，则不做任何操作；但如果发现不一致，控制器的工作是制定一个*计划*，使当前状态与期望状态一致。'
- en: For example, if you had a deployment with a replica of three pods deployed in
    a cluster, the ReplicaSet controller will see that the replica count is set to
    three (desired state) with an image of *v2*. The controller will then call the
    kube-apiserver component and pull a copy of the current and desired state for
    that ReplicaSet at which point, the controller will start comparing settings.
    In this example, the current state will now have three healthy pods using the
    *v1* image tag, because pods can't be modified after being created. The controller
    will need to create new replacement pods and will need to destroy the old pods.
    It does this by creating a new pod object with the updated image tag. This pod
    object will have the status of *waiting to be scheduled*. At this point, kube-scheduler
    takes over to assign that pod to a node. Then, kubelet takes over to create the
    container(s), IP address, mount volumes, and so on that are needed for the pod.
    Then, once everything has been started and the probes are successful, kubelet
    will update the pod object to the state of `Ready`. This takes us back to the
    ReplicaSet controller, which will then detect that one of the number pods has
    successfully been started. If yes, pick the oldest pod that doesn't meet the spec
    and terminate that pod by setting the status to terminating. This will trigger
    kubelet to destroy the pod and its resources. Then, once everything is cleaned
    up, kube-controller-manager will remove the terminated pod object in kube-apiserver.
    This process then starts again and will repeat until all of the pods in the ReplicaSet
    match the desired state.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: The controllers are designed to always aim to have the current state matching
    the desired state. If the controllers run into an issue, such as the new pods
    keep crashing, the image can't be pulled, or a `configmap` object is missing,
    then after several failed attempts, the controller will keep trying, but it will
    put that object in a **CrashLooping** status. This tells the controller to stop
    fixing that state for a set amount of time (the default is 5 minutes). The controller
    does this to prevent spamming the cluster with requests for failing resources
    (for example, if you had entered a typo in the image tag). We don't want the controller
    to keep creating and deleting the same pod over and over again as fast as it can,
    as this will put a load on kube-apiserver, etcd, kube-scheduler, and so on, which
    would create an extreme case of a large number of pods all crashlooping at the
    same time, and this could take the cluster down.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about Rancher, RKE, RKE2, K3s, and RancherD. We
    went over some of the pros and cons of each product. We then went over how they
    were designed and how they work. Next, we covered all of the controllers that
    make up Rancher and explored how they work behind the scenes. After that, we dove
    into how Rancher uses its Cattle agents for communicating with its clusters and
    nodes. Finally, we went into detail on the different core components of K8s, including
    kube-apiserver, kube-controller-manager, and kube-scheduler.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to install Rancher in a single-node environment.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
