- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Apache Kafka for Real-Time Events and Data Ingestion
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Kafka 用于实时事件和数据摄取
- en: Real-time data and event streaming are crucial components of modern data architectures.
    By leveraging systems such as Apache Kafka, organizations can ingest, process,
    and analyze real-time data to drive timely business decisions and actions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 实时数据和事件流处理是现代数据架构中的关键组成部分。通过利用像Apache Kafka这样的系统，组织可以摄取、处理和分析实时数据，从而推动及时的业务决策和行动。
- en: In this chapter, we will cover Kafka’s fundamental concepts and architecture
    that enable it to be a performant, resilient, and scalable messaging system. You
    will learn how Kafka’s publish-subscribe messaging model works with topics, partitions,
    and brokers. We will demonstrate Kafka setup and configuration, and you will get
    hands-on experience with producing and consuming messages for topics.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍Kafka的基本概念和架构，使其成为一个高性能、可靠且可扩展的消息系统。你将学习Kafka的发布-订阅消息模型是如何通过主题、分区和代理来工作的。我们将演示Kafka的设置和配置，并让你亲身体验如何为主题生产和消费消息。
- en: Additionally, you will understand Kafka’s distributed and fault-tolerant nature
    by experimenting with data replication and topic distribution strategies. We will
    also introduce Kafka Connect for streaming data ingestion from external systems
    such as databases. You will configure Kafka Connect to stream changes from a SQL
    database into Kafka topics.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你将通过实验数据复制和主题分布策略，了解Kafka的分布式和容错特性。我们还将介绍Kafka Connect，用于从外部系统（如数据库）流式摄取数据。你将配置Kafka
    Connect，将SQL数据库中的变更流式传输到Kafka主题。
- en: The highlight of this chapter is combining Kafka with Spark Structured Streaming
    for building real-time data pipelines. You will learn this highly scalable stream
    processing approach by implementing end-to-end pipelines that consume Kafka topic
    data, process it using Spark, and write the output into another Kafka topic or
    external storage system.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的亮点是将Kafka与Spark结构化流处理结合，构建实时数据管道。你将通过实现端到端的管道，学习这种高可扩展的流处理方法，这些管道会消费Kafka主题数据，使用Spark处理数据，并将输出写入另一个Kafka主题或外部存储系统。
- en: By the end of this chapter, you will have gained practical skills to set up
    Kafka clusters and leverage Kafka’s capabilities for building robust real-time
    data streaming and processing architectures. Companies can greatly benefit from
    making timely data-driven decisions, and Kafka enables realizing that objective.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将掌握实际技能，能够设置Kafka集群并利用Kafka的功能构建强大的实时数据流和处理架构。公司可以通过做出及时的数据驱动决策从中获益，而Kafka正是实现这一目标的关键。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主要内容：
- en: Getting started with Kafka
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用Kafka
- en: Exploring the Kafka architecture
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索Kafka架构
- en: Streaming from a database with Kafka Connect
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kafka Connect从数据库流式传输数据
- en: Real-time data processing with Kafka and Spark
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kafka和Spark进行实时数据处理
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will run a Kafka cluster and a Kafka Connect cluster locally
    using `docker-compose` comes with Docker, so no further installation steps are
    necessary. If you find yourself in the need to manually install `docker-compose`,
    please refer to [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用`docker-compose`在本地运行Kafka集群和Kafka Connect集群，而`docker-compose`是随Docker一同提供的，因此无需额外的安装步骤。如果你需要手动安装`docker-compose`，请参考[https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/)。
- en: Additionally, we will process data in real time using **Spark**. For installation
    instructions, please refer to [*Chapter 5*](B21927_05.xhtml#_idTextAnchor092).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将使用**Spark**进行实时数据处理。安装说明请参考[*第5章*](B21927_05.xhtml#_idTextAnchor092)。
- en: All the code for this chapter is available online in this book’s GitHub repository
    ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes))
    in the `Chapter07` folder.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码都可以在本书的GitHub仓库中找到，网址是([https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes))，位于`Chapter07`文件夹中。
- en: Getting started with Kafka
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用Kafka
- en: Kafka is a popular open source platform for building real-time data pipelines
    and streaming applications. In this section, we will learn how to get a basic
    Kafka environment running locally using `docker-compose` so that you can start
    building Kafka producers and consumers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka是一个流行的开源平台，用于构建实时数据管道和流处理应用程序。在本节中，我们将学习如何使用`docker-compose`在本地运行基本的Kafka环境，这样你就可以开始构建Kafka生产者和消费者。
- en: '`docker-compose` is a tool that helps define and run multi-container Docker
    applications. With compose, you use a YAML file to configure your application’s
    services then spin everything up with one command. This allows you to avoid having
    to run and connect containers manually. To run our Kafka cluster, we will define
    a set of nodes using `docker-compose`. First, create a folder called `multinode`
    (just to keep our code organized) and create a new file called `docker-compose.yaml`.
    This is the regular file that `docker-compose` expects to set up the containers
    (the same as Dockerfile for Docker). To improve readability, we will not show
    the entire code (it is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/multinode](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/multinode)),
    but a portion of it. Let’s take a look:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker-compose` 是一个帮助定义和运行多容器 Docker 应用的工具。使用 Compose，你可以通过一个 YAML 文件配置应用的服务，然后通过一个命令启动所有服务。这可以避免手动运行和连接容器。为了运行我们的
    Kafka 集群，我们将使用 `docker-compose` 定义一组节点。首先，创建一个名为 `multinode` 的文件夹（仅为保持代码有序），并创建一个名为
    `docker-compose.yaml` 的新文件。这是 `docker-compose` 用来设置容器的常规文件（类似于 Dockerfile）。为了提高可读性，我们不会显示整个代码（代码可以在
    [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/multinode](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/multinode)
    找到），只展示其中的一部分。让我们来看一下：'
- en: docker-compose.yaml
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: docker-compose.yaml
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The original Docker Compose file is setting up a Kafka cluster with three Kafka
    brokers and three Zookeeper nodes (more details on Kafka architecture in the next
    section). We just left the definition for the first Zookeeper and Kafka brokers
    as the other ones are the same. Here, we’re using Confluent Kafka (an enterprise-ready
    version of Kafka maintained by Confluent Inc.) and Zookeeper images to create
    the containers. For the Zookeeper nodes, the key parameters are as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 Docker Compose 文件正在设置一个包含三个 Kafka 经纪人和三个 Zookeeper 节点的 Kafka 集群（更多关于 Kafka
    架构的细节将在下一节中讲解）。我们只保留了第一个 Zookeeper 和 Kafka 经纪人的定义，因为其他的都是相同的。在这里，我们使用的是 Confluent
    Kafka（由 Confluent Inc. 维护的企业版 Kafka）和 Zookeeper 镜像来创建容器。对于 Zookeeper 节点，关键参数如下：
- en: '`ZOOKEEPER_SERVER_ID`: The unique ID for each Zookeeper server in the ensemble.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ZOOKEEPER_SERVER_ID`：集群中每个 Zookeeper 服务器的唯一 ID。'
- en: '`ZOOKEEPER_CLIENT_PORT`: The port for clients to connect to this Zookeeper
    node. We use different ports for each node.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ZOOKEEPER_CLIENT_PORT`：客户端连接到此 Zookeeper 节点的端口。我们为每个节点使用不同的端口。'
- en: '`ZOOKEEPER_TICK_TIME`: The basic time unit used by Zookeeper for heartbeats.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ZOOKEEPER_TICK_TIME`：Zookeeper 用于心跳的基本时间单位。'
- en: '`ZOOKEEPER_INIT_LIMIT`: The time the Zookeeper servers have to connect to a
    leader.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ZOOKEEPER_INIT_LIMIT`：Zookeeper 服务器必须连接到领导者的时间。'
- en: '`ZOOKEEPER_SYNC_LIMIT`: How far out of date a server can be from a leader.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ZOOKEEPER_SYNC_LIMIT`：服务器可以与领导者相差多远。'
- en: '`ZOOKEEPER_SERVERS`: Lists all Zookeeper servers in the ensemble in `address:leaderElectionPort:followerPort`
    format.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ZOOKEEPER_SERVERS`：以 `address:leaderElectionPort:followerPort` 格式列出集群中的所有
    Zookeeper 服务器。'
- en: 'For the Kafka brokers, the key parameters are as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Kafka 经纪人，关键参数如下：
- en: '`KAFKA_BROKER_ID`: Unique ID for each Kafka broker.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KAFKA_BROKER_ID`：每个 Kafka 经纪人的唯一 ID。'
- en: '`KAFKA_ZOOKEEPER_CONNECT`: Lists the Zookeeper ensemble that Kafka should connect
    to.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KAFKA_ZOOKEEPER_CONNECT`：列出 Kafka 应该连接的 Zookeeper 集群。'
- en: '`KAFKA_ADVERTISED_LISTENERS`: Advertised listener for external connections
    to this broker. We use different ports for each broker.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KAFKA_ADVERTISED_LISTENERS`：对外连接到此经纪人的广告监听器。我们为每个经纪人使用不同的端口。'
- en: The containers are configured to use host networking mode to simplify networking.
    The dependencies ensure Kafka only starts after Zookeeper is ready.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 容器配置为使用主机网络模式以简化网络配置。依赖关系确保 Kafka 仅在 Zookeeper 准备好后才启动。
- en: 'This code creates a fully functional Kafka cluster that can handle replication
    and failures of individual brokers or Zookeepers. Now, we will get those containers
    up and running. In a terminal, move to the `multinode` folder and type the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码创建了一个完全功能的 Kafka 集群，能够处理单个经纪人或 Zookeeper 的复制和故障。现在，我们将启动这些容器。在终端中，进入 `multinode`
    文件夹并输入以下命令：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will tell `docker-compose` to get the containers up. If the necessary images
    are not found locally, they will be automatically downloaded. The `-d` parameter
    makes `docker-compose` run in `-d`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这将告诉 `docker-compose` 启动容器。如果本地未找到必要的镜像，它们将被自动下载。`-d` 参数使得 `docker-compose`
    在 `-d` 模式下运行。
- en: 'To check the logs for one of the Kafka Brokers, run the following command:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看某个 Kafka 经纪人的日志，请运行以下命令：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, `multinode-kafka-1-1` is the name of the first Kafka Broker container
    we defined in the YAML file. With this command, you should be able to visualize
    Kafka’s logs and validate that everything is running correctly. Now, let’s take
    a closer look at Kafka’s architecture and understand how it works.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`multinode-kafka-1-1` 是我们在 YAML 文件中定义的第一个 Kafka Broker 容器的名称。通过这个命令，您应该能够可视化
    Kafka 的日志并验证一切是否正常运行。现在，让我们更详细地了解 Kafka 的架构，并理解它是如何工作的。
- en: Exploring the Kafka architecture
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 Kafka 的架构
- en: Kafka has a distributed architecture that consists of brokers, producers, consumers,
    topics, partitions, and replicas. At a high level, producers publish messages
    to topics, brokers receive those messages and store them in partitions, and consumers
    subscribe to topics and process the messages that are published to them.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 具有分布式架构，包括经纪人、生产者、消费者、主题、分区和副本。在高层次上，生产者向主题发布消息，经纪人接收这些消息并将它们存储在分区中，消费者订阅主题并处理发布给它们的消息。
- en: Kafka relies on an external coordination service called **Zookeeper**, which
    helps manage the Kafka cluster. Zookeeper helps with controller election – selecting
    a broker to be the cluster controller. The controller is responsible for administrative
    operations such as assigning partitions to brokers and monitoring for broker failures.
    Zookeeper also helps brokers coordinate among themselves for operations such as
    leader election for partitions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 依赖于一个称为**Zookeeper**的外部协调服务，帮助管理 Kafka 集群。Zookeeper 帮助进行控制器选举——选择一个经纪人作为集群控制器。控制器负责管理操作，例如将分区分配给经纪人并监视经纪人故障。Zookeeper
    还帮助经纪人协调彼此的操作，例如为分区选举领导者。
- en: Kafka **brokers** are the main components of a Kafka cluster and handle all
    read/write requests from producers/consumers. Brokers receive messages from producers
    and expose data to consumers. Each broker manages data stored on local disks in
    the form of partitions. By default, brokers will evenly distribute partitions
    among themselves. If a broker goes down, Kafka will automatically redistribute
    those partitions to other brokers. This helps prevent data loss and ensures high
    availability. Now, let’s understand how Kafka handles messages in a **publish-subscribe**
    (**PubSub**) design and how it guarantees reliability and scalability for the
    messages’ writing and reading.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka **经纪人**是 Kafka 集群的主要组件，负责处理生产者和消费者的所有读/写请求。经纪人从生产者接收消息并向消费者公开数据。每个经纪人管理以分区形式存储在本地磁盘上的数据。默认情况下，经纪人将分区均匀分布在它们之间。如果经纪人宕机，Kafka
    将自动将这些分区重新分配给其他经纪人。这有助于防止数据丢失并确保高可用性。现在，让我们了解 Kafka 如何在**发布-订阅**（PubSub）设计中处理消息，以及如何为消息的写入和读取保证可靠性和可伸缩性。
- en: The PubSub design
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发布-订阅设计
- en: Kafka relies on a PubSub messaging pattern to enable real-time data streams.
    Kafka organizes messages into categories called **topics**. Topics act as feeds
    or streams of messages. Producers write data to topics and consumers read from
    topics. For example, a “page-visits” topic would record every visit to a web page.
    Topics are always multi-producer and multi-subscriber – they can have zero to
    many producers writing messages to a topic as well as zero to many consumers reading
    messages. This helps coordinate data streams between applications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 依赖于发布-订阅（PubSub）消息模式来实现实时数据流。Kafka 将消息组织成称为**主题（topics）**的类别。主题充当消息的源或流。生产者将数据写入主题，消费者从主题读取数据。例如，“页面访问”主题将记录每次访问网页的情况。主题始终是多生产者和多订阅者的，可以有零到多个生产者向主题写入消息，以及零到多个消费者从主题读取消息。这有助于在应用程序之间协调数据流。
- en: Topics are split into **partitions** for scalability. Each partition acts as
    an ordered, immutable sequence of messages that is continually appended to. By
    partitioning topics into multiple partitions, Kafka can scale topic consumption
    by having multiple consumers reading from a topic in parallel across partitions.
    Partitions allow Kafka to distribute load horizontally across brokers and allow
    for parallelism. Data is kept in the order it was produced within each partition.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 主题被分成**分区**以实现可伸缩性。每个分区充当一个有序、不可变的消息序列，可以不断追加消息。通过将主题分区成多个分区，Kafka 可以通过让多个消费者并行地从多个分区读取主题来扩展主题消费。分区允许
    Kafka 水平分布负载到经纪人之间，并允许并行处理。数据在每个分区内保持生产顺序。
- en: Kafka provides redundancy and fault tolerance by **replicating partitions**
    across a configurable number of brokers. A partition will have one broker designated
    as the “leader” and zero or more brokers acting as “followers.” All reads/writes
    go to the leader. Followers passively replicate the leader by having identical
    copies of the leader’s data. If the leader fails, one of the followers will automatically
    become the new leader.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka通过**复制分区**到可配置数量的代理上来提供冗余和容错性。一个分区将有一个代理被指定为“领导者”，并且有零个或多个代理充当“跟随者”。所有的读写操作都由领导者处理，跟随者通过拥有与领导者数据完全相同的副本来被动地复制领导者的数据。如果领导者失败，某个跟随者将自动成为新的领导者。
- en: Having **replicas** across brokers ensures fault tolerance since data is still
    available for consumption, even if some brokers go down. The replication factor
    controls the number of replicas. For example, a replication factor of three means
    there will be two followers replicating the one leader partition. Common production
    settings have a minimum of three brokers with a replication factor of two or three.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在各个代理（brokers）之间拥有**副本**可以确保容错性，因为即使某些代理出现故障，数据仍然可以被消费。副本因子控制副本的数量。例如，副本因子为三意味着将有两个跟随者复制一个领导者分区。常见的生产环境设置通常至少有三个代理，副本因子为二或三。
- en: Consumers label themselves with **consumer group** names, and each record that’s
    published to a topic is only delivered to one consumer in a group. If there are
    multiple consumers in a group, Kafka will load balance messages across the consumers.
    Kafka guarantees an ordered, at-least-once delivery of messages within a partition
    to a single consumer. Consumer groups allow you to scale out consumers while still
    providing message-ordering guarantees.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者用**消费者组**名称来标识自己，并且发布到主题中的每一条记录只会发送给组中的一个消费者。如果一个组中有多个消费者，Kafka会在消费者之间负载均衡消息。Kafka保证在一个分区内将消息有序、至少一次地传递给一个消费者。消费者组允许你扩展消费者的数量，同时仍然提供消息的顺序保证。
- en: '**Producers** publish records to topic partitions. If there is only one partition,
    all messages will go there. With multiple partitions, producers can choose to
    either publish randomly across partitions or ensure ordering by using the same
    partition. This ordering guarantee only applies within a partition, not across
    partitions.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**生产者**将记录发布到主题的分区中。如果只有一个分区，所有消息都会发送到该分区。如果有多个分区，生产者可以选择随机分配消息到各个分区，或者通过使用相同的分区来确保消息的顺序。这个顺序保证只适用于单个分区内，而不适用于跨分区的消息。'
- en: Producers batch together messages for efficiency and durability. Messages are
    buffered locally and compressed before being sent to brokers. This batching provides
    better efficiency and throughput. Producers can choose to wait until a batch is
    full, or flush based on time or message size thresholds. Producers also replicate
    data by having it acknowledged by all in-sync replicas before confirming a write.
    Producers can choose different acknowledgment guarantees, ranging from committing
    as soon as the leader writes the record or waiting until all followers have replicated.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者为了提高效率和持久性，将消息批量处理。消息在发送到代理之前会在本地缓冲并进行压缩。这样的批处理提供了更好的效率和吞吐量。生产者可以选择等待直到批次满，或根据时间或消息大小的阈值来刷新。生产者还通过在确认写入之前确保所有同步副本都已确认来进行数据复制。生产者可以选择不同的确认保证，从一旦领导者写入记录就提交，或等到所有跟随者都已复制后再提交。
- en: '**Consumers** read records by subscribing to Kafka topics. Consumer instances
    can be in separate processes or servers. Consumers pull data from brokers by periodically
    sending requests for data. Consumers keep track of their position (“offsets”)
    within each partition to start reading from the correct place in case of failures.
    Consumers typically commit offsets periodically. Offsets are also used to allow
    consumers to rewind or skip ahead if needed. How exactly do those offsets work?
    Let’s try to understand them a little deeper.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**消费者**通过订阅Kafka主题来读取记录。消费者实例可以位于不同的进程或服务器上。消费者通过定期发送数据请求从代理中拉取数据。消费者会跟踪它们在每个分区中的位置（“偏移量”），以便在发生故障时从正确的位置开始读取。消费者通常会定期提交偏移量。偏移量还可以让消费者在需要时倒带或跳过一些记录。到底这些偏移量是如何工作的？让我们更深入地了解一下。'
- en: Kafka stores streams of records in categories called topics. Within a topic,
    records are organized into partitions, which allow for parallel processing and
    scalability. Each record within a partition gets an *incremental ID number* called
    an **offset** that uniquely identifies the record within that partition. This
    offset reflects the order of records within a partition. For example, an offset
    of three means it’s the third record.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 将记录流存储在称为主题的类别中。每个主题下的记录被组织成分区，这样就可以进行并行处理和扩展性。每个分区中的记录都有一个称为 **偏移量**
    的 *递增 ID 编号*，它唯一标识该分区内的记录。这个偏移量反映了分区内记录的顺序。例如，偏移量为三意味着它是第三条记录。
- en: When a Kafka consumer reads records from a partition, it keeps track of the
    offset of the last record it has read. This allows the consumer to only read newer
    records it hasn’t processed yet. If the consumer disconnects and reconnects later,
    it will start reading again from the last committed offset. The offsets commit
    log is stored in a Kafka topic named `__consumer_offsets`. This provides durability
    and allows consumers to transparently pick up where they left off in case of failures.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Kafka 消费者从分区读取记录时，它会跟踪已读取的最后一条记录的偏移量。这使得消费者只会读取尚未处理的新记录。如果消费者断开连接并稍后重新连接，它将从最后提交的偏移量处重新开始读取。偏移量提交日志存储在一个名为
    `__consumer_offsets` 的 Kafka 主题中。这样可以保证持久性，并允许消费者在故障发生时透明地从中断处恢复。
- en: Offsets enable multiple consumers to read from the same partition while ensuring
    each record is processed only once by each consumer. The consumers can read at
    their own pace without interfering with each other. This is a key design feature
    that enables Kafka’s scalability. All these features, when used together, allow
    Kafka to deliver **exactly-once semantics**. Let’s take a closer look at this
    concept.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 偏移量使得多个消费者可以从同一分区读取数据，同时确保每个消费者只处理每条记录一次。消费者可以按自己的速度读取数据，彼此之间不会互相干扰。这是 Kafka
    可扩展性的关键设计特性。当这些特性一起使用时，Kafka 可以实现 **精确一次语义**。让我们更详细地了解这个概念。
- en: How Kafka delivers exactly-once semantics
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka 如何实现精确一次语义
- en: 'When processing data streams, three types of semantics are relevant when considering
    guarantees on data delivery:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据流时，考虑数据传输保障时有三种相关的语义：
- en: '**At-least-once semantics**: In this case, each record in the data stream is
    guaranteed to be processed at least once but may be processed more than once.
    This can happen if there is a failure downstream from the data source before the
    processing is acknowledged. When the system recovers, the data source will resend
    the unacknowledged data, causing duplicate processing.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**至少一次语义**：在这种情况下，数据流中的每条记录保证至少被处理一次，但可能会被处理多次。如果数据源下游发生故障，在处理确认之前，系统将重新发送未确认的数据，导致重复处理。'
- en: '**At-most-once semantics**: In this case, each record will either be processed
    once or not at all. This prevents duplicate processing but means that in the event
    of failure, some records may be lost entirely.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最多一次语义**：在这种情况下，每条记录要么被处理一次，要么完全不处理。这可以防止重复处理，但意味着在发生故障时，一些记录可能会完全丢失。'
- en: '**Exactly-once semantics**: This case combines the guarantees of the other
    two and ensures each record is processed one and only one time. This is difficult
    to achieve in practice because it requires coordination between storage and processing
    to ensure no duplicates are introduced during retries.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确一次语义**：此案例结合了其他两种语义的保证，确保每条记录只被处理一次且仅一次。由于需要在存储和处理之间进行协调，以确保在重试过程中不会引入重复项，这在实践中非常难以实现。'
- en: Kafka provides a way to enable exactly-once semantics for event processing through
    a combination of architectural design and integration with stream processing systems.
    Kafka topics are divided into partitions, which enables data parallelism by spreading
    the load across brokers. Events with the same key go to the same partition, enabling
    processing ordering guarantees. Kafka assigns each partition a sequential ID called
    the offset, which uniquely identifies each event within a partition.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 提供了一种方法，通过架构设计和与流处理系统的集成，实现事件处理的精确一次语义。Kafka 主题被划分为多个分区，这使得数据可以通过将负载分散到不同的代理上来实现并行处理。具有相同键的事件会进入同一个分区，从而保证了处理顺序的保证。Kafka
    为每个分区分配一个顺序 ID，称为偏移量，它唯一标识该分区内的每个事件。
- en: Consumers track their position per partition by storing the offset of the last
    processed event. If a consumer fails and restarts, it will resume from the last
    committed offset, ensuring events are not missed or processed twice.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者通过存储最后处理事件的偏移量来跟踪每个分区的位置。如果消费者失败并重新启动，它将从最后提交的偏移量恢复，确保事件不会丢失或被处理两次。
- en: By tightly integrating offset tracking and event delivery with stream processors
    through Kafka’s APIs, Kafka’s infrastructure provides the backbone for building
    exactly-once real-time data pipelines.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将偏移量追踪与流处理器通过Kafka的API紧密集成，Kafka的基础设施为构建精确一次的实时数据管道提供了支撑。
- en: '*Figure 7**.1* shows a visual representation of Kafka’s architecture:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.1*展示了Kafka架构的可视化表示：'
- en: '![Figure 7.1- Kafka’s architecture](img/B21927_07_01.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1- Kafka架构](img/B21927_07_01.jpg)'
- en: Figure 7.1- Kafka’s architecture
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1- Kafka架构
- en: Next, we will do a quick exercise to get started and see Kafka in action.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将做一个简单的练习，以开始并查看Kafka的实际操作。
- en: First producer and consumer
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一个生产者和消费者
- en: 'After setting up Kafka with `docker-compose`, we have to create a topic that
    will hold our events. We can do this from outside the container or we can get
    into the container and run the commands from there. For this exercise, we will
    access the container and run commands from the inside for didactic purposes. Later
    in this book, we will study the other option, which can be very handy, especially
    when Kafka is running on Kubernetes. Let’s get started:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`docker-compose`设置Kafka后，我们需要创建一个主题来保存我们的事件。我们可以在容器外部执行此操作，也可以进入容器并从内部运行命令。为了本次练习，我们将访问容器并从内部运行命令，目的是为了教学。稍后在本书中，我们将研究另一种方法，这在Kafka运行在Kubernetes上时尤其有用。我们开始吧：
- en: 'First, check if all containers are up and running. In a terminal, run the following
    command:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，检查所有容器是否都已启动并正在运行。在终端中，运行以下命令：
- en: '[PRE3]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You should see an output that specifies containers’ names, images, commands,
    and more. Everything seems to be running fine. Note the name of the first Kafka
    broker container.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该看到一个输出，指定了容器的名称、镜像、命令等信息。一切似乎都在正常运行。请注意第一个Kafka代理容器的名称。
- en: 'We will need it to run commands in Kafka from inside the container. To get
    in, run the following command:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要它来从容器内运行Kafka的命令。要进入容器，运行以下命令：
- en: '[PRE4]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we’re creating an environment variable with the first container name (in
    my case, `multinode_kafka-1_1`) and running the `docker exec` command with the
    `-``it` parameter.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们正在创建一个环境变量，存储第一个容器的名称（在我的例子中是`multinode_kafka-1_1`），并使用`docker exec`命令与`-it`参数一起运行。
- en: 'Now, we are in the container. Let’s declare three environment variables that
    will help us manage Kafka:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们进入了容器。让我们声明三个有助于管理Kafka的环境变量：
- en: '[PRE5]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we will use the Kafka CLI to create a topic with the `kafka-topics --create`
    command. Run the following code:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用Kafka命令行工具通过`kafka-topics --create`命令创建一个主题。运行以下代码：
- en: '[PRE6]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will create a topic named `mytopic` with a replication factor of `3` (three
    replicas) and `3` partitions (note that your maximum number of partitions is the
    number of brokers you have).
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将创建一个名为`mytopic`的主题，复制因子为`3`（三个副本），并且有`3`个分区（注意，最大分区数是你拥有的代理数量）。
- en: 'Although we have a confirmation message in the terminal, it’s good to list
    all the topics inside a cluster:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然我们在终端中收到了确认消息，但列出集群中的所有主题还是很有帮助的：
- en: '[PRE7]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You should see `mytopic` as output on the screen.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该在屏幕上看到`mytopic`作为输出。
- en: 'Next, let’s get some information about our topic:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们获取一些关于我们主题的信息：
- en: '[PRE8]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This yields the following output (formatted for better visualization):'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会产生以下输出（已格式化以便更好地可视化）：
- en: '[PRE9]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This topic structure partitioned across all three brokers and with replications
    of each partition in all other brokers is exactly what we have seen in *Figure
    7**.1*.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个主题结构被分配到所有三个代理，并且每个分区在所有其他代理中都有副本，这正是我们在*图 7.1*中看到的。
- en: 'Now, let’s build a simple producer and start sending some messages to this
    topic. In your terminal, type the following:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们构建一个简单的生产者，并开始向这个主题发送一些消息。在终端中，输入以下命令：
- en: '[PRE10]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This starts a simple console producer.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会启动一个简单的控制台生产者。
- en: 'Now, type some messages in the console; they’ll be sent to the topic:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在控制台中输入一些消息，它们将被发送到该主题：
- en: '[PRE11]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can type whatever you want.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以输入任何你想要的内容。
- en: 'Now, open a different terminal and (preferably) put it beside the first terminal
    that’s running the console producer. We must log in to the container the same
    way we did in the first terminal:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，打开一个不同的终端（最好将它放在第一个运行控制台生产者的终端旁边）。我们必须以与第一个终端相同的方式登录到容器：
- en: '[PRE12]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, create the same necessary environment variables:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建相同的必要环境变量：
- en: '[PRE13]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we will start a simple console consumer. We will tell this consumer to
    read all the messages in the topic from the beginning (just for this exercise
    – this is not recommended for topics in production with a huge amount of data
    in them). In the second terminal, run the following command:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将启动一个简单的控制台消费者。我们将指示该消费者从头开始读取主题中的所有消息（仅限此练习——不建议在生产环境中的主题上使用此方法，特别是当数据量非常大时）。在第二个终端中，运行以下命令：
- en: '[PRE14]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You should see all the messages typed on the screen. Note that they are in a
    different order because Kafka only keeps messages in order inside partitions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能在屏幕上看到所有输入的消息。注意它们的顺序不同，因为 Kafka 只会在分区内保持消息的顺序。
- en: 'Across partitions, ordering is not possible (unless you have date-time information
    inside the message, of course). Press *Ctrl* + *C* to stop the consumer. You can
    also press *Ctrl* + *C* in the producer terminal to stop it. Type `exit` in both
    terminals to exit the containers and stop and kill all the containers by running
    the following command:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在跨分区时，无法保持顺序（除非消息内部包含日期和时间信息）。按 *Ctrl* + *C* 停止消费者。你也可以在生产者终端按 *Ctrl* + *C*
    停止它。然后，在两个终端中输入 `exit`，退出容器，并通过运行以下命令停止并杀死所有容器：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can check that all containers were successfully removed with the following
    command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下命令检查所有容器是否已成功删除：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, let’s try something different. One of the most common cases of using Kafka
    is migrating data in real time from tables in a database. Let’s see how we can
    do that simplistically using Kafka Connect.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试做一些不同的事情。使用 Kafka 最常见的一个场景是实时迁移数据库表中的数据。让我们看看如何通过 Kafka Connect 简单地实现这一点。
- en: Streaming from a database with Kafka Connect
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据库流式传输数据到 Kafka Connect
- en: 'In this section, we will read all data that is generated in a Postgres table
    in real time with Kafka Connect. First, it is necessary to build a custom image
    of Kafka Connect that can connect to Postgres. Follow these steps:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Kafka Connect 实时读取在 Postgres 表中生成的所有数据。首先，需要构建一个可以连接到 Postgres 的
    Kafka Connect 自定义镜像。请按照以下步骤操作：
- en: 'Let’s create a different folder for this new exercise. First, create a folder
    named `connect` and another folder inside it named `kafka-connect-custom-image`.
    Inside the custom image folder, we will create a new Dockerfile with the following
    content:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为这个新练习创建一个不同的文件夹。首先，创建一个名为 `connect` 的文件夹，并在其中再创建一个名为 `kafka-connect-custom-image`
    的文件夹。在自定义镜像文件夹内，我们将创建一个新的 Dockerfile，内容如下：
- en: '[PRE17]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This Docker file bases itself on the confluent Kafka Connect image and installs
    two connectors – a JDBC source/sink connector and a sink connector for Amazon
    S3\. The former is necessary to connect to a database while the latter will be
    very handy for delivering events to S3.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个 Docker 文件基于 Confluent Kafka Connect 镜像，并安装了两个连接器——一个 JDBC 源/接收连接器和一个用于 Amazon
    S3 的接收连接器。前者用于连接数据库，而后者则非常方便用于将事件传送到 S3。
- en: 'Build your image with the following commands:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令构建你的镜像：
- en: '[PRE18]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, in the `connec`t folder, you should have a `.env_kafka_connect` file to
    store your AWS credentials. Remember that credentials should *never* be hardcoded
    in any configuration files or code. Your `.env_kafka_connect` file should look
    like this:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，在 `connect` 文件夹中，你应该有一个 `.env_kafka_connect` 文件，用于存储你的 AWS 凭证。请记住，凭证*绝不*应硬编码在任何配置文件或代码中。你的
    `.env_kafka_connect` 文件应如下所示：
- en: '[PRE19]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Save it in the `connect` folder. Then, create a new `docker-compose.yaml` file.
    The content for this file is available in this book’s GitHub repository: [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/docker-compose.yaml](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/docker-compose.yaml).'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其保存在 `connect` 文件夹中。然后，创建一个新的 `docker-compose.yaml` 文件。该文件的内容可以在本书的 GitHub
    仓库中找到：[https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/docker-compose.yaml](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/docker-compose.yaml)。
- en: 'This Docker Compose file sets up an environment for Kafka and Kafka Connect
    along with a Postgres database instance. It defines the following services:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个 Docker Compose 文件为 Kafka 和 Kafka Connect 设置了一个环境，并包含一个 Postgres 数据库实例。它定义了以下服务：
- en: '`zookeeper`: This runs a Zookeeper instance, which Kafka relies on for coordination
    between nodes. It sets up some configurations, such as port, tick time, and a
    client port.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zookeeper`：此容器运行一个 Zookeeper 实例，Kafka 依赖于它进行节点间的协调。它设置了一些配置，如端口、tick 时间和客户端端口。'
- en: '`broker`: This runs a Kafka broker that depends on the Zookeeper service (a
    broker cannot exist until the Zookeepers are all up). It configures things such
    as the broker ID, what Zookeeper instance to connect to, listeners for external
    connections on ports `9092` and `29092`, replication settings for internal topics
    Kafka needs, and some performance tuning.'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`broker`：此容器运行一个 Kafka broker，依赖于 Zookeeper 服务（在所有 Zookeeper 启动之前，broker 无法存在）。它配置了如
    broker ID、连接到的 Zookeeper 实例、用于外部连接的端口 `9092` 和 `29092` 的监听器、Kafka 所需的内部主题的复制设置，以及一些性能调优设置。'
- en: '`schema-registry`: This runs Confluent Schema Registry, which allows us to
    store schemas for topics. It depends on the Kafka broker and sets the URL for
    the Kafka cluster as well as what port to listen on for API requests.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`schema-registry`：此容器运行 Confluent Schema Registry，它允许我们存储主题的模式。它依赖于 Kafka broker，并设置
    Kafka 集群的 URL 以及监听 API 请求的端口。'
- en: '`connect`: This runs our customized image of Confluent Kafka Connect. It depends
    on both the Kafka broker and Schema Registry and sets up bootstrap servers, the
    group ID, internal topics for storing connector configurations, offsets and status,
    key-value converters for serialization, Schema Registry integration, and the plugin
    path for finding more connectors.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`connect`：此容器运行我们定制的 Confluent Kafka Connect 镜像。它依赖于 Kafka broker 和 Schema
    Registry，并设置了启动服务器、组 ID、用于存储连接器配置、偏移量和状态的内部主题、用于序列化的键值转换器、Schema Registry 集成以及查找更多连接器的插件路径。'
- en: '`rest-proxy`: The runs the Confluent REST proxy, which provides a REST interface
    to Kafka. It sets up the Kafka broker connection information and Schema Registry.'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rest-proxy`：此容器运行 Confluent REST 代理，它提供了一个 Kafka 的 REST 接口。它设置了 Kafka broker
    的连接信息和 Schema Registry。'
- en: '`postgres`: This runs a Postgres database instance that’s exposed on port `5432`
    with some basic credentials set. Note that we are saving the database password
    in plain text in our code. This should *never* be done in a production environment
    since it is a security breach. We are only defining the password in this way for
    local testing.'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`postgres`：此容器运行一个 Postgres 数据库实例，并暴露在端口 `5432` 上，设置了一些基本凭据。请注意，我们在代码中以明文保存数据库密码。在生产环境中**绝不**应这样做，因为这是一个安全漏洞。我们这样定义密码仅限于本地测试。'
- en: There is also a custom network defined called `proxynet` that all these services
    join. This allows inter-service communication by hostname instead of exposing
    all services to the host machine network.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 还定义了一个名为 `proxynet` 的自定义网络，所有这些服务都会加入该网络。这允许通过主机名进行服务间通信，而无需将所有服务暴露到主机机器的网络中。
- en: 'To get these containers up and running, run the following command:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启动这些容器，请运行以下命令：
- en: '[PRE20]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: All the containers should be up in a few minutes.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有容器应在几分钟内启动。
- en: Now, we will continuously insert some simulated data into our Postgres database.
    To do that, create a new Python file named `make_fake_data.py`. The code is available
    at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/connect/simulations](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/connect/simulations)
    folder. This code generates fake data for customers (such as name, address, profession,
    and email) and inserts it into a database. For it to work, you should have the
    `faker`, `pandas`, `psycopg2-binary`, and `sqlalchemy` libraries installed. Make
    sure you install them with `pip install` before running the code. A `requirements.txt`
    file, along with the code, is provided in this book’s GitHub repository.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将持续向我们的 Postgres 数据库插入一些模拟数据。为此，创建一个新的 Python 文件，命名为 `make_fake_data.py`。代码可以在
    [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/connect/simulations](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/connect/simulations)
    文件夹中找到。此代码为客户生成假数据（如姓名、地址、职业和电子邮件），并将其插入到数据库中。要使其正常工作，您应安装 `faker`、`pandas`、`psycopg2-binary`
    和 `sqlalchemy` 库。在运行代码之前，请确保通过 `pip install` 安装它们。本书的 GitHub 仓库中提供了一个 `requirements.txt`
    文件和代码。
- en: 'Now, to run the simulations, in a terminal, type the following:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在终端中运行模拟，输入以下命令：
- en: '[PRE21]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This will print the parameters for the simulation (interval of generation, sample
    size, and the connection string) on the screen and start printing the simulated
    data. After a few simulations, you can stop it by pressing *Ctrl* + *C*. Then,
    use your preferred SQL client (DBeaver is one option) to check if the data was
    correctly ingested in the database. Run a simple SQL statement (`select * from
    customers`) to see the data printed in the SQL client.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出模拟参数（生成间隔、样本大小和连接字符串）到屏幕，并开始打印模拟数据。经过几次模拟后，可以通过按 *Ctrl* + *C* 停止它。然后，使用你喜欢的
    SQL 客户端（例如 DBeaver）检查数据是否已正确导入数据库。运行简单的 SQL 语句（`select * from customers`）查看数据是否在
    SQL 客户端中正确显示。
- en: Now, we will register a source JDBC connector to pull data from Postgres. This
    connector will run as a Kafka Connect process that establishes a JDBC connection
    to the source database. It uses this connection to execute SQL queries that select
    data from specific tables. The connector translates the result sets into JSON
    documents and publishes them to configured Kafka topics. Each table has a dedicated
    topic created for it. The query that extracts data can be either a simple `SELECT`
    statement or an incremental query based on timestamp or numeric columns. This
    allows us to capture new or updated rows.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将注册一个源 JDBC 连接器来从 Postgres 拉取数据。此连接器将作为 Kafka Connect 进程运行，建立一个到源数据库的 JDBC
    连接。它使用该连接执行 SQL 查询，从特定表中选择数据。连接器将结果集转换为 JSON 文档，并将其发布到配置的 Kafka 主题。每个表都有一个专门为其创建的主题。提取数据的查询可以是简单的
    `SELECT` 语句，也可以是基于时间戳或数字列的增量查询。这使我们能够捕捉到新增或更新的行。
- en: 'First, we will define a configuration file to deploy the connector on Kafka
    Connect. Create a folder named `connectors` and a new file named `connect_jdbc_pg_json.config`.
    The configuration code is shown here:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个配置文件，以便在 Kafka Connect 上部署连接器。创建一个名为 `connectors` 的文件夹，并创建一个名为 `connect_jdbc_pg_json.config`
    的新文件。配置代码如下所示：
- en: '**connect_jdbc_pg_json.config**'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**connect_jdbc_pg_json.config**'
- en: '[PRE22]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This configuration creates a Kafka connector that will sync rows from the `customers`
    table to JSON-formatted Kafka topics, based on timestamp changes to the rows.
    Let’s take a closer look at the parameters that were used:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此配置创建了一个 Kafka 连接器，将基于行的时间戳变化将 `customers` 表的行同步到 JSON 格式的 Kafka 主题。我们来更详细地了解所使用的参数：
- en: '`name`: Names the connector for management purposes.'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`: 为连接器指定一个名称，以便于管理。'
- en: '`connector.class`: Specifies the JDBC connector class from Confluent to use.'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`connector.class`: 指定使用 Confluent 提供的 JDBC 连接器类。'
- en: '`value.converter`: Specifies that data will be converted into JSON format in
    Kafka.'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value.converter`: 指定数据将在 Kafka 中转换为 JSON 格式。'
- en: '`value.converter.schemas.enable`: Enables schemas to be stored with the JSON
    data.'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value.converter.schemas.enable`: 启用将架构与 JSON 数据一起存储。'
- en: '`tasks.max`: Limits to one task. This parameter can be increased in a production
    environment for scalability, depending on the number of partitions in the topic.'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tasks.max`: 限制为一个任务。此参数可以根据生产环境的扩展性需求，在主题的分区数不同的情况下增加。'
- en: '`connection.url`: Connects to a local PostgreSQL database on port `5432`.'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`connection.url`: 连接到本地的 PostgreSQL 数据库，端口为 `5432`。'
- en: '`connection.user/.password`: PostgreSQL credentials (only in plaintext here
    for this exercise. Credentials should *never* be hardcoded).'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`connection.user/.password`: PostgreSQL 凭证（在本示例中为明文，凭证*绝不*应硬编码）。'
- en: '`mode`: Specifies to use a timestamp column to detect new/changed rows. You
    could also use an `id` column.'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode`: 指定使用时间戳列来检测新增/更改的行。你也可以使用 `id` 列。'
- en: '`timestamp.column.name`: Looks at the `dt_update` column.'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timestamp.column.name`: 查看 `dt_update` 列。'
- en: '`table.whitelist`: Specifies to sync the `customers` table.'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`table.whitelist`: 指定同步 `customers` 表。'
- en: '`topic.prefix`: Output topics will be prefixed with `json-`.'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topic.prefix`: 输出的主题将以 `json-` 为前缀。'
- en: '`validate.non.null`: Allows syncing rows with null values.'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validate.non.null`: 允许同步包含 null 值的行。'
- en: '`poll.interval.ms`: Check for new data every 500ms.'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`poll.interval.ms`: 每 500 毫秒检查一次新数据。'
- en: 'Now, we will create a Kafka topic to store the data from the Postgres table.
    In a terminal, type the following:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个 Kafka 主题来存储来自 Postgres 表的数据。在终端中，输入以下内容：
- en: '[PRE23]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that we are using the `docker-compose` API to execute a command inside
    a container. The first part of the command (`docker-compose exec broker`) tells
    Docker that we want to execute something in the `broker` service defined in the
    `docker-compose.yaml` file. The rest of the command is executed inside the broker.
    We are creating a topic called `json-customers` with two partitions and a replication
    factor of one (one replica per partition). You should see a confirmation message
    in the terminal that the topic was created.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们正在使用 `docker-compose` API 在容器内执行命令。命令的第一部分（`docker-compose exec broker`）告诉
    Docker 我们希望在 `docker-compose.yaml` 文件中定义的 `broker` 服务中执行某些操作。其余命令将在 broker 内部执行。我们正在创建一个名为
    `json-customers` 的主题，具有两个分区和一个副本因子（每个分区一个副本）。你应该在终端中看到主题创建的确认消息。
- en: 'Next, we will register the connector using a simple API call to Kafka Connect.
    We will use the `curl` library to do that. In your terminal, type the following:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用简单的 API 调用来注册连接器到 Kafka Connect。我们将使用 `curl` 库来实现。请在终端中输入以下命令：
- en: '[PRE24]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The name of the connector should be printed in the terminal.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 连接器的名称应在终端中打印出来。
- en: 'Now, do a quick check on the Connect instance logs:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，快速检查 Connect 实例的日志：
- en: '[PRE25]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Roll up a few lines; you should see the output for the connector registration.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 向上滚动几行，你应该会看到连接器注册的输出。
- en: 'Now, let’s try a simple console consumer just to validate that the messages
    are already being migrated to the topic:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们尝试一个简单的控制台消费者，只是验证消息是否已经被迁移到主题中：
- en: '[PRE26]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You should see the messages in JSON format printed on the screen. Press *Ctrl*
    + *C* to stop the consumer and type `exit` to exit the container.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该在屏幕上看到以 JSON 格式打印的消息。按 *Ctrl* + *C* 停止消费者，然后输入 `exit` 退出容器。
- en: Now, we will configure a sink connector to deliver those messages to Amazon
    S3\. First, go to AWS and create a new S3 bucket. S3 bucket names must be unique
    across all AWS. This way, I recommend setting it with the account name as a suffix
    (for instance, `kafka-messages-xxxxxxxx`).
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将配置一个 sink 连接器，将这些消息传输到 Amazon S3。首先，去 AWS 创建一个新的 S3 存储桶。S3 存储桶的名称必须在所有
    AWS 中唯一。因此，我建议将其设置为账户名称作为后缀（例如，`kafka-messages-xxxxxxxx`）。
- en: 'Inside the `connectors` folder, create a new file named `connect_s3_sink.config`:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 `connectors` 文件夹中，创建一个名为 `connect_s3_sink.config` 的新文件：
- en: '**connect_s3_sink.config**'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**connect_s3_sink.config**'
- en: '[PRE27]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s become familiar with the parameters of this connector:'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们来熟悉一下这个连接器的参数：
- en: '`connector.class`: Specifies the connector class to use. In this case, it is
    the Confluent S3 sink connector.'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`connector.class`：指定要使用的连接器类。在这种情况下，它是 Confluent S3 sink 连接器。'
- en: '`format.class`: Specifies the format to use when writing data to S3\. Here,
    we’re using `JsonFormat` so that data will be stored in JSON format.'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`format.class`：指定写入 S3 时使用的格式。这里，我们使用 `JsonFormat`，使数据以 JSON 格式存储。'
- en: '`key.converter` and `value.converter`: Specify the converter classes to use
    for serializing the key and values to JSON, respectively.'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`key.converter` 和 `value.converter`：分别指定用于将键和值序列化为 JSON 的转换器类。'
- en: '`key.converter.schemas.enable` and `value.converter.schemas.enable`: Disable
    schema validation for keys and values.'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`key.converter.schemas.enable` 和 `value.converter.schemas.enable`：禁用键和值的模式验证。'
- en: '`flush.size`: Specifies the number of records the connector should wait before
    performing a flush to S3\. Here, this parameter is set to `1`. However, in production,
    when you have a large message throughput, it is best to set this value higher
    so that more messages get delivered to S3 in a single file.'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flush.size`：指定连接器在执行刷新操作之前应等待的记录数。这里，这个参数设置为 `1`。然而，在生产环境中，当消息吞吐量较大时，最好将此值设置得更高，以便更多的消息在一个文件中一起传输到
    S3。'
- en: '`schema.compatibility`: Specifies the schema compatibility rule to use. Here,
    `FULL` means that schemas must be fully compatible.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`schema.compatibility`：指定要使用的模式兼容性规则。这里，`FULL` 表示模式必须完全兼容。'
- en: '`s3.bucket.name`: The name of the S3 bucket to write data to.'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s3.bucket.name`：要写入数据的 S3 存储桶的名称。'
- en: '`s3.region`: The AWS region where the S3 bucket is located.'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s3.region`：S3 存储桶所在的 AWS 区域。'
- en: '`s3.object.tagging`: Enables S3 object tagging.'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s3.object.tagging`：启用 S3 对象标签。'
- en: '`s3.ssea.name`: The server-side encryption algorithm to use (AES256, S3 managed
    encryption, in this case).'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s3.ssea.name`：要使用的服务器端加密算法（在这种情况下是 AES256，即 S3 管理的加密）。'
- en: '`topics.dir`: Specifies the directory in the S3 bucket to write data to.'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topics.dir`：指定在 S3 存储桶中写入数据的目录。'
- en: '`storage.class`: Specifies the underlying storage class.'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`storage.class`：指定底层存储类别。'
- en: '`tasks.max`: The maximum number of tasks for this connector. This should typically
    be `1` for a sink.'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tasks.max`：此连接器的最大任务数。对于一个接收器，这通常应为`1`。'
- en: '`topics`: A comma-separated list of topics to get data from to write to S3.'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topics`：以逗号分隔的主题列表，用于获取数据并写入 S3。'
- en: 'Now, we can register the sink connector. In your terminal, type the following:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以注册接收器连接器。在你的终端中输入以下命令：
- en: '[PRE28]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Check the logs with `docker logs connect` to validate that the connector was
    correctly registered and there were no errors in its deployment.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `docker logs connect` 查看日志，以验证连接器是否已正确注册，并且在部署过程中没有错误。
- en: And that’s it! You can check the S3 bucket on AWS and see the JSON files coming
    through. If you want, run the `make_fake_data.py` simulator once again to see
    more messages be delivered to S3.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你可以检查 AWS 上的 S3 桶，查看 JSON 文件的传输情况。如果你愿意，可以再次运行 `make_fake_data.py` 模拟器，查看更多消息传递到
    S3。
- en: Now that you know how to set up a real-time message delivery pipeline, let’s
    introduce some real-time processing in it with Apache Spark.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道如何设置实时消息传递管道，让我们通过 Apache Spark 向其中加入一些实时处理功能。
- en: Real-time data processing with Kafka and Spark
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kafka 和 Spark 进行实时数据处理
- en: An extremely important part of real-time data pipelines relates to real-time
    processing. As data gets generated continuously from various sources, such as
    user activity logs, IoT sensors, and more, we need to be able to make transformations
    on these streams of data in real time.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 实时数据管道的一个非常重要的部分是实时处理。随着来自各种来源（如用户活动日志、物联网传感器等）的数据不断生成，我们需要能够在这些数据流上进行实时转换。
- en: Apache Spark’s Structured Streaming module provides a high-level API for processing
    real-time data streams. It builds on top of Spark SQL and provides expressive
    stream processing using SQL-like operations. Spark Structured Streaming processes
    data streams using a micro-batch processing model. In this model, streaming data
    is received and collected into small batches that are processed very quickly,
    typically within milliseconds. This provides low processing latency while retaining
    the scalability of batch processing.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 的结构化流模块提供了一个高层 API，用于处理实时数据流。它建立在 Spark SQL 的基础上，使用类似 SQL 的操作提供丰富的流处理。Spark
    结构化流通过微批处理模型来处理数据流。在这个模型中，流数据会被接收并收集成小批次，通常在毫秒级别内非常快速地处理。这提供了低延迟处理，同时保留了批处理的可扩展性。
- en: We will take from the real-time pipeline that we started with Kafka and build
    real-time processing on top of it. We will use the Spark Structured Streaming
    module for that. Create a new folder called `processing` and a file inside it
    called `consume_from_kafka.py`. The Spark code that processes the data and aggregates
    the results has been provided here.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从使用 Kafka 启动的实时管道中提取数据，并在其上构建实时处理。我们将使用 Spark 结构化流模块来实现这一点。创建一个名为 `processing`
    的新文件夹，并在其中创建一个名为 `consume_from_kafka.py` 的文件。处理数据并聚合结果的 Spark 代码已提供在此。
- en: 'The code is also available in this book’s GitHub repository: [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/processing/consume_from_kafka.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/processing/consume_from_kafka.py).
    This Spark Structured Streaming application is reading from the `json-customers`
    Kafka topic, transforming the JSON data, and computing aggregations on it before
    printing the output to the console:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码也可以在本书的 GitHub 仓库中找到：[https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/processing/consume_from_kafka.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/processing/consume_from_kafka.py)。这个
    Spark 结构化流应用程序正在从 `json-customers` Kafka 主题读取数据，转换 JSON 数据，并在计算聚合后将结果打印到控制台：
- en: consume_from_kafka.py
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: consume_from_kafka.py
- en: '[PRE29]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'First, `SparkSession` is created and configured to use the Kafka connector
    package. Error logging is set to reduce noise and facilitate output visualization
    in the terminal. Next, a DataFrame is created by reading from the `json-customers`
    topic using the `kafka` source. It connects to Kafka running on localhost, starts
    reading from the earliest offset, and represents each message payload as a string:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建并配置 `SparkSession` 来使用 Kafka 连接器包。设置错误日志以减少噪声，并方便在终端中可视化输出。接下来，使用 `kafka`
    源从 `json-customers` 主题读取数据，创建一个 DataFrame。它连接到本地 Kafka，开始从最早的偏移量读取数据，并将每条消息的负载表示为字符串：
- en: '[PRE30]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This second block defines two schemas – `schema1` captures the nested JSON structure
    expected in the Kafka payload, with a schema field and payload field. On the other
    hand, `schema2` defines the actual customer data schema contained in the payload
    field.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这第二个代码块定义了两个模式——`schema1`捕获了Kafka有效载荷中预期的嵌套JSON结构，具有模式字段和有效载荷字段。另一方面，`schema2`定义了包含在有效载荷字段中的实际客户数据模式。
- en: 'The value string field, representing the raw Kafka message payload, is extracted
    from the initial DataFrame. This string payload is parsed as JSON using the defined
    `schema1` to extract just the payload field. The payload string is then parsed
    again using `schema2` to extract the actual customer data fields into a new DataFrame
    called `newdf`:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 从初始DataFrame中提取代表原始Kafka消息有效载荷的值字符串字段。使用定义的`schema1`将此字符串有效载荷解析为JSON，仅提取有效载荷字段。然后使用`schema2`再次解析有效载荷字符串，以提取实际的客户数据字段到名为`newdf`的新DataFrame中：
- en: '[PRE31]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, the transformations occur – the `birthdate` string is cast to `date`,
    the current date is fetched, and the age is calculated using `datediff`. The data
    is aggregated by gender to compute the count, earliest birthdate in data, current
    date, and average age:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，转换发生了——`birthdate`字符串被转换为`date`，获取当前日期，并使用`datediff`计算年龄。按性别聚合数据以计算计数、数据中最早的出生日期、当前日期和平均年龄：
- en: '[PRE32]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Finally, the aggregated DataFrame is written to the console in append output
    mode using Structured Streaming. This query will run continuously until it’s terminated
    by running *Ctrl* + *C*.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，聚合的DataFrame以追加输出模式写入控制台使用Structured Streaming。此查询将持续运行，直到通过运行*Ctrl* + *C*终止。
- en: 'To run the query, in a terminal, type the following command:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行查询，在终端中输入以下命令：
- en: '[PRE33]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You should see the aggregated data in your terminal. Open another terminal
    and run more simulations by running the following command:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在终端中看到聚合的数据。打开另一个终端并通过运行以下命令来运行更多模拟：
- en: '[PRE34]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As new simulations are generated and ingested into Postgres, the Kafka connector
    will automatically pull them to the `json-customers` topic, at which point Spark
    will pull those messages, calculate the aggregations in real time, and print the
    results. After a while, you can hit *Ctrl* + *C* to stop simulations and then
    again to stop the Spark streaming query.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当新的模拟数据生成并被注入到Postgres中时，Kafka连接器将自动将其拉取到`json-customers`主题，此时Spark将拉取这些消息，实时计算聚合结果并打印输出。一段时间后，你可以按下*Ctrl*
    + *C*来停止模拟，然后再次按下来停止Spark流查询。
- en: Congratulations! You ran a real-time data processing pipeline using Kafka and
    Spark! Remember to clean up the created resources with `docker-compose down`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你使用Kafka和Spark运行了一个实时数据处理管道！记得使用`docker-compose down`清理创建的资源。
- en: Summary
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the fundamental concepts and architecture behind
    Apache Kafka – a popular open source platform for building real-time data pipelines
    and streaming applications.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了Apache Kafka背后的基本概念和架构——这是一个用于构建实时数据管道和流式应用程序的流行开源平台。
- en: You learned how Kafka provides distributed, partitioned, replicated, and fault-tolerant
    PubSub messaging through its topics and brokers architecture. Through hands-on
    examples, you gained practical experience with setting up local Kafka clusters
    using Docker, creating topics, and producing and consuming messages. You understood
    offsets and consumer groups that enable fault tolerance and parallel consumption
    from topics.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你了解了Kafka如何通过其主题和代理架构提供分布式、分区、复制和容错的PubSub消息传递。通过实际示例，你获得了使用Docker设置本地Kafka集群、创建主题以及生成和消费消息的实际经验。你理解了偏移量和消费者组，这些使得从主题进行容错和并行消费成为可能。
- en: We introduced Kafka Connect, which allows us to stream data between Kafka and
    external systems such as databases. You implemented a source connector to ingest
    changes from a PostgreSQL database into Kafka topics. We also set up a sink connector
    to deliver the messages from Kafka to object storage in AWS S3 in real time.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了Kafka Connect，它允许我们在Kafka和外部系统（如数据库）之间流动数据。你实现了一个源连接器，用于将PostgreSQL数据库中的更改摄入到Kafka主题中。我们还设置了一个接收器连接器，以实时方式将Kafka消息传递到AWS
    S3中的对象存储。
- en: The highlight was building an end-to-end streaming pipeline with Kafka and Spark
    Structured Streaming. You learned how micro-batch processing on streaming data
    allows low latency while retaining scalability. The example provided showed how
    to consume messages from Kafka, transform them using Spark, and aggregate the
    results in real time.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 亮点是使用 Kafka 和 Spark Structured Streaming 构建端到端的流数据管道。您学到了如何通过流数据上的微批处理实现低延迟，同时保持可扩展性。提供的示例展示了如何从
    Kafka 中消费消息，使用 Spark 进行转换，并实时聚合结果。
- en: Through these hands-on exercises, you gained practical experience with Kafka’s
    architecture and capabilities for building robust and scalable streaming data
    pipelines and applications. Companies can greatly benefit from leveraging Kafka
    to power their real-time data processing needs to drive timely insights and actions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些实践练习，您获得了使用 Kafka 架构和功能的实际经验，以构建强大且可扩展的流数据管道和应用程序。企业可以通过利用 Kafka 来满足其实时数据处理需求，从而推动及时的洞察和行动，获得巨大收益。
- en: In the next chapter, we will finally get all the technologies we’ve studied
    so far into Kubernetes. You will learn how to deploy Airflow, Spark, and Kafka
    in Kubernetes and get them ready to build a fully integrated data pipeline.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将最终将我们迄今为止所学的所有技术引入 Kubernetes。您将学习如何在 Kubernetes 中部署 Airflow、Spark
    和 Kafka，并使它们准备好构建一个完全集成的数据管道。
- en: 'Part 3: Connecting It All Together'
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：将一切连接起来
- en: In this part, you will learn how to deploy and orchestrate the big data tools
    and technologies covered in the previous chapters on Kubernetes. You will build
    scripts to deploy Apache Spark, Apache Airflow, and Apache Kafka on a Kubernetes
    cluster, making them ready for running data processing jobs, orchestrating data
    pipelines, and handling real-time data ingestion, respectively. Additionally,
    you will explore data consumption layers, data lake engines such as Trino, and
    real-time data visualization with Elasticsearch and Kibana, all deployed on Kubernetes.
    Finally, you will bring everything together by building and deploying two complete
    data pipelines, one for batch processing and another for real-time processing,
    on a Kubernetes cluster. The part also covers the deployment of generative AI
    applications on Kubernetes and provides guidance on where to go next in your Kubernetes
    and big data journey.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，您将学习如何在 Kubernetes 上部署和编排前几章中介绍的大数据工具和技术。您将构建脚本来在 Kubernetes 集群上部署 Apache
    Spark、Apache Airflow 和 Apache Kafka，使它们能够分别执行数据处理作业、编排数据管道并处理实时数据摄取。此外，您还将探索数据消费层、数据湖引擎（如
    Trino）以及使用 Elasticsearch 和 Kibana 进行的实时数据可视化，所有这些都将在 Kubernetes 上部署。最后，您将通过在 Kubernetes
    集群上构建和部署两个完整的数据管道来将一切连接起来，一个用于批处理，另一个用于实时处理。本部分还涵盖了在 Kubernetes 上部署生成式 AI 应用程序，并为您在
    Kubernetes 和大数据之旅中下一步的行动提供指导。
- en: 'This part contains the following chapters:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 8*](B21927_08.xhtml#_idTextAnchor134), *Deploying the Big Data Stack
    on Kubernetes*'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B21927_08.xhtml#_idTextAnchor134)，*在Kubernetes上部署大数据栈*'
- en: '[*Chapter 9*](B21927_09.xhtml#_idTextAnchor141), *Data Consumption Layer*'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B21927_09.xhtml#_idTextAnchor141)，*数据消费层*'
- en: '[*Chapter 10*](B21927_10.xhtml#_idTextAnchor154), *Building a Big Data Pipeline
    on Kubernetes*'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B21927_10.xhtml#_idTextAnchor154)，*在 Kubernetes 上构建大数据管道*'
- en: '[*Chapter 11*](B21927_11.xhtml#_idTextAnchor167), *Generative AI on Kubernetes*'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B21927_11.xhtml#_idTextAnchor167)，*Kubernetes上的生成式AI*'
- en: '[*Chapter 12*](B21927_12.xhtml#_idTextAnchor183), *Where To Go From Here*'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第12章*](B21927_12.xhtml#_idTextAnchor183)，*从这里开始*'
