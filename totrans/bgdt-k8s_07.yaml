- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Kafka for Real-Time Events and Data Ingestion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real-time data and event streaming are crucial components of modern data architectures.
    By leveraging systems such as Apache Kafka, organizations can ingest, process,
    and analyze real-time data to drive timely business decisions and actions.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover Kafka’s fundamental concepts and architecture
    that enable it to be a performant, resilient, and scalable messaging system. You
    will learn how Kafka’s publish-subscribe messaging model works with topics, partitions,
    and brokers. We will demonstrate Kafka setup and configuration, and you will get
    hands-on experience with producing and consuming messages for topics.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you will understand Kafka’s distributed and fault-tolerant nature
    by experimenting with data replication and topic distribution strategies. We will
    also introduce Kafka Connect for streaming data ingestion from external systems
    such as databases. You will configure Kafka Connect to stream changes from a SQL
    database into Kafka topics.
  prefs: []
  type: TYPE_NORMAL
- en: The highlight of this chapter is combining Kafka with Spark Structured Streaming
    for building real-time data pipelines. You will learn this highly scalable stream
    processing approach by implementing end-to-end pipelines that consume Kafka topic
    data, process it using Spark, and write the output into another Kafka topic or
    external storage system.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have gained practical skills to set up
    Kafka clusters and leverage Kafka’s capabilities for building robust real-time
    data streaming and processing architectures. Companies can greatly benefit from
    making timely data-driven decisions, and Kafka enables realizing that objective.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the Kafka architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming from a database with Kafka Connect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time data processing with Kafka and Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will run a Kafka cluster and a Kafka Connect cluster locally
    using `docker-compose` comes with Docker, so no further installation steps are
    necessary. If you find yourself in the need to manually install `docker-compose`,
    please refer to [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we will process data in real time using **Spark**. For installation
    instructions, please refer to [*Chapter 5*](B21927_05.xhtml#_idTextAnchor092).
  prefs: []
  type: TYPE_NORMAL
- en: All the code for this chapter is available online in this book’s GitHub repository
    ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes))
    in the `Chapter07` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka is a popular open source platform for building real-time data pipelines
    and streaming applications. In this section, we will learn how to get a basic
    Kafka environment running locally using `docker-compose` so that you can start
    building Kafka producers and consumers.
  prefs: []
  type: TYPE_NORMAL
- en: '`docker-compose` is a tool that helps define and run multi-container Docker
    applications. With compose, you use a YAML file to configure your application’s
    services then spin everything up with one command. This allows you to avoid having
    to run and connect containers manually. To run our Kafka cluster, we will define
    a set of nodes using `docker-compose`. First, create a folder called `multinode`
    (just to keep our code organized) and create a new file called `docker-compose.yaml`.
    This is the regular file that `docker-compose` expects to set up the containers
    (the same as Dockerfile for Docker). To improve readability, we will not show
    the entire code (it is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/multinode](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/multinode)),
    but a portion of it. Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: docker-compose.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The original Docker Compose file is setting up a Kafka cluster with three Kafka
    brokers and three Zookeeper nodes (more details on Kafka architecture in the next
    section). We just left the definition for the first Zookeeper and Kafka brokers
    as the other ones are the same. Here, we’re using Confluent Kafka (an enterprise-ready
    version of Kafka maintained by Confluent Inc.) and Zookeeper images to create
    the containers. For the Zookeeper nodes, the key parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ZOOKEEPER_SERVER_ID`: The unique ID for each Zookeeper server in the ensemble.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ZOOKEEPER_CLIENT_PORT`: The port for clients to connect to this Zookeeper
    node. We use different ports for each node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ZOOKEEPER_TICK_TIME`: The basic time unit used by Zookeeper for heartbeats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ZOOKEEPER_INIT_LIMIT`: The time the Zookeeper servers have to connect to a
    leader.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ZOOKEEPER_SYNC_LIMIT`: How far out of date a server can be from a leader.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ZOOKEEPER_SERVERS`: Lists all Zookeeper servers in the ensemble in `address:leaderElectionPort:followerPort`
    format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the Kafka brokers, the key parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`KAFKA_BROKER_ID`: Unique ID for each Kafka broker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KAFKA_ZOOKEEPER_CONNECT`: Lists the Zookeeper ensemble that Kafka should connect
    to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KAFKA_ADVERTISED_LISTENERS`: Advertised listener for external connections
    to this broker. We use different ports for each broker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The containers are configured to use host networking mode to simplify networking.
    The dependencies ensure Kafka only starts after Zookeeper is ready.
  prefs: []
  type: TYPE_NORMAL
- en: 'This code creates a fully functional Kafka cluster that can handle replication
    and failures of individual brokers or Zookeepers. Now, we will get those containers
    up and running. In a terminal, move to the `multinode` folder and type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This will tell `docker-compose` to get the containers up. If the necessary images
    are not found locally, they will be automatically downloaded. The `-d` parameter
    makes `docker-compose` run in `-d`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the logs for one of the Kafka Brokers, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, `multinode-kafka-1-1` is the name of the first Kafka Broker container
    we defined in the YAML file. With this command, you should be able to visualize
    Kafka’s logs and validate that everything is running correctly. Now, let’s take
    a closer look at Kafka’s architecture and understand how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Kafka architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka has a distributed architecture that consists of brokers, producers, consumers,
    topics, partitions, and replicas. At a high level, producers publish messages
    to topics, brokers receive those messages and store them in partitions, and consumers
    subscribe to topics and process the messages that are published to them.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka relies on an external coordination service called **Zookeeper**, which
    helps manage the Kafka cluster. Zookeeper helps with controller election – selecting
    a broker to be the cluster controller. The controller is responsible for administrative
    operations such as assigning partitions to brokers and monitoring for broker failures.
    Zookeeper also helps brokers coordinate among themselves for operations such as
    leader election for partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka **brokers** are the main components of a Kafka cluster and handle all
    read/write requests from producers/consumers. Brokers receive messages from producers
    and expose data to consumers. Each broker manages data stored on local disks in
    the form of partitions. By default, brokers will evenly distribute partitions
    among themselves. If a broker goes down, Kafka will automatically redistribute
    those partitions to other brokers. This helps prevent data loss and ensures high
    availability. Now, let’s understand how Kafka handles messages in a **publish-subscribe**
    (**PubSub**) design and how it guarantees reliability and scalability for the
    messages’ writing and reading.
  prefs: []
  type: TYPE_NORMAL
- en: The PubSub design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kafka relies on a PubSub messaging pattern to enable real-time data streams.
    Kafka organizes messages into categories called **topics**. Topics act as feeds
    or streams of messages. Producers write data to topics and consumers read from
    topics. For example, a “page-visits” topic would record every visit to a web page.
    Topics are always multi-producer and multi-subscriber – they can have zero to
    many producers writing messages to a topic as well as zero to many consumers reading
    messages. This helps coordinate data streams between applications.
  prefs: []
  type: TYPE_NORMAL
- en: Topics are split into **partitions** for scalability. Each partition acts as
    an ordered, immutable sequence of messages that is continually appended to. By
    partitioning topics into multiple partitions, Kafka can scale topic consumption
    by having multiple consumers reading from a topic in parallel across partitions.
    Partitions allow Kafka to distribute load horizontally across brokers and allow
    for parallelism. Data is kept in the order it was produced within each partition.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka provides redundancy and fault tolerance by **replicating partitions**
    across a configurable number of brokers. A partition will have one broker designated
    as the “leader” and zero or more brokers acting as “followers.” All reads/writes
    go to the leader. Followers passively replicate the leader by having identical
    copies of the leader’s data. If the leader fails, one of the followers will automatically
    become the new leader.
  prefs: []
  type: TYPE_NORMAL
- en: Having **replicas** across brokers ensures fault tolerance since data is still
    available for consumption, even if some brokers go down. The replication factor
    controls the number of replicas. For example, a replication factor of three means
    there will be two followers replicating the one leader partition. Common production
    settings have a minimum of three brokers with a replication factor of two or three.
  prefs: []
  type: TYPE_NORMAL
- en: Consumers label themselves with **consumer group** names, and each record that’s
    published to a topic is only delivered to one consumer in a group. If there are
    multiple consumers in a group, Kafka will load balance messages across the consumers.
    Kafka guarantees an ordered, at-least-once delivery of messages within a partition
    to a single consumer. Consumer groups allow you to scale out consumers while still
    providing message-ordering guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: '**Producers** publish records to topic partitions. If there is only one partition,
    all messages will go there. With multiple partitions, producers can choose to
    either publish randomly across partitions or ensure ordering by using the same
    partition. This ordering guarantee only applies within a partition, not across
    partitions.'
  prefs: []
  type: TYPE_NORMAL
- en: Producers batch together messages for efficiency and durability. Messages are
    buffered locally and compressed before being sent to brokers. This batching provides
    better efficiency and throughput. Producers can choose to wait until a batch is
    full, or flush based on time or message size thresholds. Producers also replicate
    data by having it acknowledged by all in-sync replicas before confirming a write.
    Producers can choose different acknowledgment guarantees, ranging from committing
    as soon as the leader writes the record or waiting until all followers have replicated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Consumers** read records by subscribing to Kafka topics. Consumer instances
    can be in separate processes or servers. Consumers pull data from brokers by periodically
    sending requests for data. Consumers keep track of their position (“offsets”)
    within each partition to start reading from the correct place in case of failures.
    Consumers typically commit offsets periodically. Offsets are also used to allow
    consumers to rewind or skip ahead if needed. How exactly do those offsets work?
    Let’s try to understand them a little deeper.'
  prefs: []
  type: TYPE_NORMAL
- en: Kafka stores streams of records in categories called topics. Within a topic,
    records are organized into partitions, which allow for parallel processing and
    scalability. Each record within a partition gets an *incremental ID number* called
    an **offset** that uniquely identifies the record within that partition. This
    offset reflects the order of records within a partition. For example, an offset
    of three means it’s the third record.
  prefs: []
  type: TYPE_NORMAL
- en: When a Kafka consumer reads records from a partition, it keeps track of the
    offset of the last record it has read. This allows the consumer to only read newer
    records it hasn’t processed yet. If the consumer disconnects and reconnects later,
    it will start reading again from the last committed offset. The offsets commit
    log is stored in a Kafka topic named `__consumer_offsets`. This provides durability
    and allows consumers to transparently pick up where they left off in case of failures.
  prefs: []
  type: TYPE_NORMAL
- en: Offsets enable multiple consumers to read from the same partition while ensuring
    each record is processed only once by each consumer. The consumers can read at
    their own pace without interfering with each other. This is a key design feature
    that enables Kafka’s scalability. All these features, when used together, allow
    Kafka to deliver **exactly-once semantics**. Let’s take a closer look at this
    concept.
  prefs: []
  type: TYPE_NORMAL
- en: How Kafka delivers exactly-once semantics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When processing data streams, three types of semantics are relevant when considering
    guarantees on data delivery:'
  prefs: []
  type: TYPE_NORMAL
- en: '**At-least-once semantics**: In this case, each record in the data stream is
    guaranteed to be processed at least once but may be processed more than once.
    This can happen if there is a failure downstream from the data source before the
    processing is acknowledged. When the system recovers, the data source will resend
    the unacknowledged data, causing duplicate processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**At-most-once semantics**: In this case, each record will either be processed
    once or not at all. This prevents duplicate processing but means that in the event
    of failure, some records may be lost entirely.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exactly-once semantics**: This case combines the guarantees of the other
    two and ensures each record is processed one and only one time. This is difficult
    to achieve in practice because it requires coordination between storage and processing
    to ensure no duplicates are introduced during retries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka provides a way to enable exactly-once semantics for event processing through
    a combination of architectural design and integration with stream processing systems.
    Kafka topics are divided into partitions, which enables data parallelism by spreading
    the load across brokers. Events with the same key go to the same partition, enabling
    processing ordering guarantees. Kafka assigns each partition a sequential ID called
    the offset, which uniquely identifies each event within a partition.
  prefs: []
  type: TYPE_NORMAL
- en: Consumers track their position per partition by storing the offset of the last
    processed event. If a consumer fails and restarts, it will resume from the last
    committed offset, ensuring events are not missed or processed twice.
  prefs: []
  type: TYPE_NORMAL
- en: By tightly integrating offset tracking and event delivery with stream processors
    through Kafka’s APIs, Kafka’s infrastructure provides the backbone for building
    exactly-once real-time data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.1* shows a visual representation of Kafka’s architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1- Kafka’s architecture](img/B21927_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1- Kafka’s architecture
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will do a quick exercise to get started and see Kafka in action.
  prefs: []
  type: TYPE_NORMAL
- en: First producer and consumer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After setting up Kafka with `docker-compose`, we have to create a topic that
    will hold our events. We can do this from outside the container or we can get
    into the container and run the commands from there. For this exercise, we will
    access the container and run commands from the inside for didactic purposes. Later
    in this book, we will study the other option, which can be very handy, especially
    when Kafka is running on Kubernetes. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, check if all containers are up and running. In a terminal, run the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see an output that specifies containers’ names, images, commands,
    and more. Everything seems to be running fine. Note the name of the first Kafka
    broker container.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will need it to run commands in Kafka from inside the container. To get
    in, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we’re creating an environment variable with the first container name (in
    my case, `multinode_kafka-1_1`) and running the `docker exec` command with the
    `-``it` parameter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we are in the container. Let’s declare three environment variables that
    will help us manage Kafka:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will use the Kafka CLI to create a topic with the `kafka-topics --create`
    command. Run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will create a topic named `mytopic` with a replication factor of `3` (three
    replicas) and `3` partitions (note that your maximum number of partitions is the
    number of brokers you have).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Although we have a confirmation message in the terminal, it’s good to list
    all the topics inside a cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see `mytopic` as output on the screen.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, let’s get some information about our topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This yields the following output (formatted for better visualization):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This topic structure partitioned across all three brokers and with replications
    of each partition in all other brokers is exactly what we have seen in *Figure
    7**.1*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s build a simple producer and start sending some messages to this
    topic. In your terminal, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This starts a simple console producer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, type some messages in the console; they’ll be sent to the topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can type whatever you want.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, open a different terminal and (preferably) put it beside the first terminal
    that’s running the console producer. We must log in to the container the same
    way we did in the first terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, create the same necessary environment variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will start a simple console consumer. We will tell this consumer to
    read all the messages in the topic from the beginning (just for this exercise
    – this is not recommended for topics in production with a huge amount of data
    in them). In the second terminal, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see all the messages typed on the screen. Note that they are in a
    different order because Kafka only keeps messages in order inside partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Across partitions, ordering is not possible (unless you have date-time information
    inside the message, of course). Press *Ctrl* + *C* to stop the consumer. You can
    also press *Ctrl* + *C* in the producer terminal to stop it. Type `exit` in both
    terminals to exit the containers and stop and kill all the containers by running
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check that all containers were successfully removed with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s try something different. One of the most common cases of using Kafka
    is migrating data in real time from tables in a database. Let’s see how we can
    do that simplistically using Kafka Connect.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming from a database with Kafka Connect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will read all data that is generated in a Postgres table
    in real time with Kafka Connect. First, it is necessary to build a custom image
    of Kafka Connect that can connect to Postgres. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a different folder for this new exercise. First, create a folder
    named `connect` and another folder inside it named `kafka-connect-custom-image`.
    Inside the custom image folder, we will create a new Dockerfile with the following
    content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This Docker file bases itself on the confluent Kafka Connect image and installs
    two connectors – a JDBC source/sink connector and a sink connector for Amazon
    S3\. The former is necessary to connect to a database while the latter will be
    very handy for delivering events to S3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Build your image with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, in the `connec`t folder, you should have a `.env_kafka_connect` file to
    store your AWS credentials. Remember that credentials should *never* be hardcoded
    in any configuration files or code. Your `.env_kafka_connect` file should look
    like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save it in the `connect` folder. Then, create a new `docker-compose.yaml` file.
    The content for this file is available in this book’s GitHub repository: [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/docker-compose.yaml](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/docker-compose.yaml).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This Docker Compose file sets up an environment for Kafka and Kafka Connect
    along with a Postgres database instance. It defines the following services:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`zookeeper`: This runs a Zookeeper instance, which Kafka relies on for coordination
    between nodes. It sets up some configurations, such as port, tick time, and a
    client port.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`broker`: This runs a Kafka broker that depends on the Zookeeper service (a
    broker cannot exist until the Zookeepers are all up). It configures things such
    as the broker ID, what Zookeeper instance to connect to, listeners for external
    connections on ports `9092` and `29092`, replication settings for internal topics
    Kafka needs, and some performance tuning.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`schema-registry`: This runs Confluent Schema Registry, which allows us to
    store schemas for topics. It depends on the Kafka broker and sets the URL for
    the Kafka cluster as well as what port to listen on for API requests.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connect`: This runs our customized image of Confluent Kafka Connect. It depends
    on both the Kafka broker and Schema Registry and sets up bootstrap servers, the
    group ID, internal topics for storing connector configurations, offsets and status,
    key-value converters for serialization, Schema Registry integration, and the plugin
    path for finding more connectors.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rest-proxy`: The runs the Confluent REST proxy, which provides a REST interface
    to Kafka. It sets up the Kafka broker connection information and Schema Registry.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`postgres`: This runs a Postgres database instance that’s exposed on port `5432`
    with some basic credentials set. Note that we are saving the database password
    in plain text in our code. This should *never* be done in a production environment
    since it is a security breach. We are only defining the password in this way for
    local testing.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: There is also a custom network defined called `proxynet` that all these services
    join. This allows inter-service communication by hostname instead of exposing
    all services to the host machine network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To get these containers up and running, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: All the containers should be up in a few minutes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we will continuously insert some simulated data into our Postgres database.
    To do that, create a new Python file named `make_fake_data.py`. The code is available
    at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/connect/simulations](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/connect/simulations)
    folder. This code generates fake data for customers (such as name, address, profession,
    and email) and inserts it into a database. For it to work, you should have the
    `faker`, `pandas`, `psycopg2-binary`, and `sqlalchemy` libraries installed. Make
    sure you install them with `pip install` before running the code. A `requirements.txt`
    file, along with the code, is provided in this book’s GitHub repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, to run the simulations, in a terminal, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will print the parameters for the simulation (interval of generation, sample
    size, and the connection string) on the screen and start printing the simulated
    data. After a few simulations, you can stop it by pressing *Ctrl* + *C*. Then,
    use your preferred SQL client (DBeaver is one option) to check if the data was
    correctly ingested in the database. Run a simple SQL statement (`select * from
    customers`) to see the data printed in the SQL client.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we will register a source JDBC connector to pull data from Postgres. This
    connector will run as a Kafka Connect process that establishes a JDBC connection
    to the source database. It uses this connection to execute SQL queries that select
    data from specific tables. The connector translates the result sets into JSON
    documents and publishes them to configured Kafka topics. Each table has a dedicated
    topic created for it. The query that extracts data can be either a simple `SELECT`
    statement or an incremental query based on timestamp or numeric columns. This
    allows us to capture new or updated rows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we will define a configuration file to deploy the connector on Kafka
    Connect. Create a folder named `connectors` and a new file named `connect_jdbc_pg_json.config`.
    The configuration code is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**connect_jdbc_pg_json.config**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This configuration creates a Kafka connector that will sync rows from the `customers`
    table to JSON-formatted Kafka topics, based on timestamp changes to the rows.
    Let’s take a closer look at the parameters that were used:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`name`: Names the connector for management purposes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connector.class`: Specifies the JDBC connector class from Confluent to use.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value.converter`: Specifies that data will be converted into JSON format in
    Kafka.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value.converter.schemas.enable`: Enables schemas to be stored with the JSON
    data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tasks.max`: Limits to one task. This parameter can be increased in a production
    environment for scalability, depending on the number of partitions in the topic.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connection.url`: Connects to a local PostgreSQL database on port `5432`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connection.user/.password`: PostgreSQL credentials (only in plaintext here
    for this exercise. Credentials should *never* be hardcoded).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode`: Specifies to use a timestamp column to detect new/changed rows. You
    could also use an `id` column.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timestamp.column.name`: Looks at the `dt_update` column.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`table.whitelist`: Specifies to sync the `customers` table.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topic.prefix`: Output topics will be prefixed with `json-`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validate.non.null`: Allows syncing rows with null values.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`poll.interval.ms`: Check for new data every 500ms.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we will create a Kafka topic to store the data from the Postgres table.
    In a terminal, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we are using the `docker-compose` API to execute a command inside
    a container. The first part of the command (`docker-compose exec broker`) tells
    Docker that we want to execute something in the `broker` service defined in the
    `docker-compose.yaml` file. The rest of the command is executed inside the broker.
    We are creating a topic called `json-customers` with two partitions and a replication
    factor of one (one replica per partition). You should see a confirmation message
    in the terminal that the topic was created.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will register the connector using a simple API call to Kafka Connect.
    We will use the `curl` library to do that. In your terminal, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The name of the connector should be printed in the terminal.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, do a quick check on the Connect instance logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Roll up a few lines; you should see the output for the connector registration.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s try a simple console consumer just to validate that the messages
    are already being migrated to the topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see the messages in JSON format printed on the screen. Press *Ctrl*
    + *C* to stop the consumer and type `exit` to exit the container.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we will configure a sink connector to deliver those messages to Amazon
    S3\. First, go to AWS and create a new S3 bucket. S3 bucket names must be unique
    across all AWS. This way, I recommend setting it with the account name as a suffix
    (for instance, `kafka-messages-xxxxxxxx`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inside the `connectors` folder, create a new file named `connect_s3_sink.config`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**connect_s3_sink.config**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s become familiar with the parameters of this connector:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`connector.class`: Specifies the connector class to use. In this case, it is
    the Confluent S3 sink connector.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`format.class`: Specifies the format to use when writing data to S3\. Here,
    we’re using `JsonFormat` so that data will be stored in JSON format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`key.converter` and `value.converter`: Specify the converter classes to use
    for serializing the key and values to JSON, respectively.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`key.converter.schemas.enable` and `value.converter.schemas.enable`: Disable
    schema validation for keys and values.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flush.size`: Specifies the number of records the connector should wait before
    performing a flush to S3\. Here, this parameter is set to `1`. However, in production,
    when you have a large message throughput, it is best to set this value higher
    so that more messages get delivered to S3 in a single file.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`schema.compatibility`: Specifies the schema compatibility rule to use. Here,
    `FULL` means that schemas must be fully compatible.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s3.bucket.name`: The name of the S3 bucket to write data to.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s3.region`: The AWS region where the S3 bucket is located.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s3.object.tagging`: Enables S3 object tagging.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s3.ssea.name`: The server-side encryption algorithm to use (AES256, S3 managed
    encryption, in this case).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topics.dir`: Specifies the directory in the S3 bucket to write data to.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storage.class`: Specifies the underlying storage class.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tasks.max`: The maximum number of tasks for this connector. This should typically
    be `1` for a sink.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topics`: A comma-separated list of topics to get data from to write to S3.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we can register the sink connector. In your terminal, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Check the logs with `docker logs connect` to validate that the connector was
    correctly registered and there were no errors in its deployment.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it! You can check the S3 bucket on AWS and see the JSON files coming
    through. If you want, run the `make_fake_data.py` simulator once again to see
    more messages be delivered to S3.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to set up a real-time message delivery pipeline, let’s
    introduce some real-time processing in it with Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time data processing with Kafka and Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An extremely important part of real-time data pipelines relates to real-time
    processing. As data gets generated continuously from various sources, such as
    user activity logs, IoT sensors, and more, we need to be able to make transformations
    on these streams of data in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark’s Structured Streaming module provides a high-level API for processing
    real-time data streams. It builds on top of Spark SQL and provides expressive
    stream processing using SQL-like operations. Spark Structured Streaming processes
    data streams using a micro-batch processing model. In this model, streaming data
    is received and collected into small batches that are processed very quickly,
    typically within milliseconds. This provides low processing latency while retaining
    the scalability of batch processing.
  prefs: []
  type: TYPE_NORMAL
- en: We will take from the real-time pipeline that we started with Kafka and build
    real-time processing on top of it. We will use the Spark Structured Streaming
    module for that. Create a new folder called `processing` and a file inside it
    called `consume_from_kafka.py`. The Spark code that processes the data and aggregates
    the results has been provided here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is also available in this book’s GitHub repository: [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/processing/consume_from_kafka.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/processing/consume_from_kafka.py).
    This Spark Structured Streaming application is reading from the `json-customers`
    Kafka topic, transforming the JSON data, and computing aggregations on it before
    printing the output to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: consume_from_kafka.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'First, `SparkSession` is created and configured to use the Kafka connector
    package. Error logging is set to reduce noise and facilitate output visualization
    in the terminal. Next, a DataFrame is created by reading from the `json-customers`
    topic using the `kafka` source. It connects to Kafka running on localhost, starts
    reading from the earliest offset, and represents each message payload as a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This second block defines two schemas – `schema1` captures the nested JSON structure
    expected in the Kafka payload, with a schema field and payload field. On the other
    hand, `schema2` defines the actual customer data schema contained in the payload
    field.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value string field, representing the raw Kafka message payload, is extracted
    from the initial DataFrame. This string payload is parsed as JSON using the defined
    `schema1` to extract just the payload field. The payload string is then parsed
    again using `schema2` to extract the actual customer data fields into a new DataFrame
    called `newdf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the transformations occur – the `birthdate` string is cast to `date`,
    the current date is fetched, and the age is calculated using `datediff`. The data
    is aggregated by gender to compute the count, earliest birthdate in data, current
    date, and average age:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the aggregated DataFrame is written to the console in append output
    mode using Structured Streaming. This query will run continuously until it’s terminated
    by running *Ctrl* + *C*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the query, in a terminal, type the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the aggregated data in your terminal. Open another terminal
    and run more simulations by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As new simulations are generated and ingested into Postgres, the Kafka connector
    will automatically pull them to the `json-customers` topic, at which point Spark
    will pull those messages, calculate the aggregations in real time, and print the
    results. After a while, you can hit *Ctrl* + *C* to stop simulations and then
    again to stop the Spark streaming query.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You ran a real-time data processing pipeline using Kafka and
    Spark! Remember to clean up the created resources with `docker-compose down`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the fundamental concepts and architecture behind
    Apache Kafka – a popular open source platform for building real-time data pipelines
    and streaming applications.
  prefs: []
  type: TYPE_NORMAL
- en: You learned how Kafka provides distributed, partitioned, replicated, and fault-tolerant
    PubSub messaging through its topics and brokers architecture. Through hands-on
    examples, you gained practical experience with setting up local Kafka clusters
    using Docker, creating topics, and producing and consuming messages. You understood
    offsets and consumer groups that enable fault tolerance and parallel consumption
    from topics.
  prefs: []
  type: TYPE_NORMAL
- en: We introduced Kafka Connect, which allows us to stream data between Kafka and
    external systems such as databases. You implemented a source connector to ingest
    changes from a PostgreSQL database into Kafka topics. We also set up a sink connector
    to deliver the messages from Kafka to object storage in AWS S3 in real time.
  prefs: []
  type: TYPE_NORMAL
- en: The highlight was building an end-to-end streaming pipeline with Kafka and Spark
    Structured Streaming. You learned how micro-batch processing on streaming data
    allows low latency while retaining scalability. The example provided showed how
    to consume messages from Kafka, transform them using Spark, and aggregate the
    results in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Through these hands-on exercises, you gained practical experience with Kafka’s
    architecture and capabilities for building robust and scalable streaming data
    pipelines and applications. Companies can greatly benefit from leveraging Kafka
    to power their real-time data processing needs to drive timely insights and actions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will finally get all the technologies we’ve studied
    so far into Kubernetes. You will learn how to deploy Airflow, Spark, and Kafka
    in Kubernetes and get them ready to build a fully integrated data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Connecting It All Together'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you will learn how to deploy and orchestrate the big data tools
    and technologies covered in the previous chapters on Kubernetes. You will build
    scripts to deploy Apache Spark, Apache Airflow, and Apache Kafka on a Kubernetes
    cluster, making them ready for running data processing jobs, orchestrating data
    pipelines, and handling real-time data ingestion, respectively. Additionally,
    you will explore data consumption layers, data lake engines such as Trino, and
    real-time data visualization with Elasticsearch and Kibana, all deployed on Kubernetes.
    Finally, you will bring everything together by building and deploying two complete
    data pipelines, one for batch processing and another for real-time processing,
    on a Kubernetes cluster. The part also covers the deployment of generative AI
    applications on Kubernetes and provides guidance on where to go next in your Kubernetes
    and big data journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B21927_08.xhtml#_idTextAnchor134), *Deploying the Big Data Stack
    on Kubernetes*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B21927_09.xhtml#_idTextAnchor141), *Data Consumption Layer*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B21927_10.xhtml#_idTextAnchor154), *Building a Big Data Pipeline
    on Kubernetes*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B21927_11.xhtml#_idTextAnchor167), *Generative AI on Kubernetes*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B21927_12.xhtml#_idTextAnchor183), *Where To Go From Here*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
