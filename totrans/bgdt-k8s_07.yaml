- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Kafka for Real-Time Events and Data Ingestion
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real-time data and event streaming are crucial components of modern data architectures.
    By leveraging systems such as Apache Kafka, organizations can ingest, process,
    and analyze real-time data to drive timely business decisions and actions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover Kafka’s fundamental concepts and architecture
    that enable it to be a performant, resilient, and scalable messaging system. You
    will learn how Kafka’s publish-subscribe messaging model works with topics, partitions,
    and brokers. We will demonstrate Kafka setup and configuration, and you will get
    hands-on experience with producing and consuming messages for topics.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you will understand Kafka’s distributed and fault-tolerant nature
    by experimenting with data replication and topic distribution strategies. We will
    also introduce Kafka Connect for streaming data ingestion from external systems
    such as databases. You will configure Kafka Connect to stream changes from a SQL
    database into Kafka topics.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: The highlight of this chapter is combining Kafka with Spark Structured Streaming
    for building real-time data pipelines. You will learn this highly scalable stream
    processing approach by implementing end-to-end pipelines that consume Kafka topic
    data, process it using Spark, and write the output into another Kafka topic or
    external storage system.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have gained practical skills to set up
    Kafka clusters and leverage Kafka’s capabilities for building robust real-time
    data streaming and processing architectures. Companies can greatly benefit from
    making timely data-driven decisions, and Kafka enables realizing that objective.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Kafka
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the Kafka architecture
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming from a database with Kafka Connect
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time data processing with Kafka and Spark
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will run a Kafka cluster and a Kafka Connect cluster locally
    using `docker-compose` comes with Docker, so no further installation steps are
    necessary. If you find yourself in the need to manually install `docker-compose`,
    please refer to [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we will process data in real time using **Spark**. For installation
    instructions, please refer to [*Chapter 5*](B21927_05.xhtml#_idTextAnchor092).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: All the code for this chapter is available online in this book’s GitHub repository
    ([https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes))
    in the `Chapter07` folder.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Kafka
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka is a popular open source platform for building real-time data pipelines
    and streaming applications. In this section, we will learn how to get a basic
    Kafka environment running locally using `docker-compose` so that you can start
    building Kafka producers and consumers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '`docker-compose` is a tool that helps define and run multi-container Docker
    applications. With compose, you use a YAML file to configure your application’s
    services then spin everything up with one command. This allows you to avoid having
    to run and connect containers manually. To run our Kafka cluster, we will define
    a set of nodes using `docker-compose`. First, create a folder called `multinode`
    (just to keep our code organized) and create a new file called `docker-compose.yaml`.
    This is the regular file that `docker-compose` expects to set up the containers
    (the same as Dockerfile for Docker). To improve readability, we will not show
    the entire code (it is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/multinode](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/multinode)),
    but a portion of it. Let’s take a look:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: docker-compose.yaml
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The original Docker Compose file is setting up a Kafka cluster with three Kafka
    brokers and three Zookeeper nodes (more details on Kafka architecture in the next
    section). We just left the definition for the first Zookeeper and Kafka brokers
    as the other ones are the same. Here, we’re using Confluent Kafka (an enterprise-ready
    version of Kafka maintained by Confluent Inc.) and Zookeeper images to create
    the containers. For the Zookeeper nodes, the key parameters are as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '`ZOOKEEPER_SERVER_ID`: The unique ID for each Zookeeper server in the ensemble.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ZOOKEEPER_CLIENT_PORT`: The port for clients to connect to this Zookeeper
    node. We use different ports for each node.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ZOOKEEPER_TICK_TIME`: The basic time unit used by Zookeeper for heartbeats.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ZOOKEEPER_INIT_LIMIT`: The time the Zookeeper servers have to connect to a
    leader.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ZOOKEEPER_SYNC_LIMIT`: How far out of date a server can be from a leader.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ZOOKEEPER_SERVERS`: Lists all Zookeeper servers in the ensemble in `address:leaderElectionPort:followerPort`
    format.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the Kafka brokers, the key parameters are as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '`KAFKA_BROKER_ID`: Unique ID for each Kafka broker.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KAFKA_ZOOKEEPER_CONNECT`: Lists the Zookeeper ensemble that Kafka should connect
    to.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KAFKA_ADVERTISED_LISTENERS`: Advertised listener for external connections
    to this broker. We use different ports for each broker.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The containers are configured to use host networking mode to simplify networking.
    The dependencies ensure Kafka only starts after Zookeeper is ready.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'This code creates a fully functional Kafka cluster that can handle replication
    and failures of individual brokers or Zookeepers. Now, we will get those containers
    up and running. In a terminal, move to the `multinode` folder and type the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will tell `docker-compose` to get the containers up. If the necessary images
    are not found locally, they will be automatically downloaded. The `-d` parameter
    makes `docker-compose` run in `-d`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the logs for one of the Kafka Brokers, run the following command:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, `multinode-kafka-1-1` is the name of the first Kafka Broker container
    we defined in the YAML file. With this command, you should be able to visualize
    Kafka’s logs and validate that everything is running correctly. Now, let’s take
    a closer look at Kafka’s architecture and understand how it works.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Kafka architecture
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka has a distributed architecture that consists of brokers, producers, consumers,
    topics, partitions, and replicas. At a high level, producers publish messages
    to topics, brokers receive those messages and store them in partitions, and consumers
    subscribe to topics and process the messages that are published to them.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Kafka relies on an external coordination service called **Zookeeper**, which
    helps manage the Kafka cluster. Zookeeper helps with controller election – selecting
    a broker to be the cluster controller. The controller is responsible for administrative
    operations such as assigning partitions to brokers and monitoring for broker failures.
    Zookeeper also helps brokers coordinate among themselves for operations such as
    leader election for partitions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Kafka **brokers** are the main components of a Kafka cluster and handle all
    read/write requests from producers/consumers. Brokers receive messages from producers
    and expose data to consumers. Each broker manages data stored on local disks in
    the form of partitions. By default, brokers will evenly distribute partitions
    among themselves. If a broker goes down, Kafka will automatically redistribute
    those partitions to other brokers. This helps prevent data loss and ensures high
    availability. Now, let’s understand how Kafka handles messages in a **publish-subscribe**
    (**PubSub**) design and how it guarantees reliability and scalability for the
    messages’ writing and reading.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: The PubSub design
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kafka relies on a PubSub messaging pattern to enable real-time data streams.
    Kafka organizes messages into categories called **topics**. Topics act as feeds
    or streams of messages. Producers write data to topics and consumers read from
    topics. For example, a “page-visits” topic would record every visit to a web page.
    Topics are always multi-producer and multi-subscriber – they can have zero to
    many producers writing messages to a topic as well as zero to many consumers reading
    messages. This helps coordinate data streams between applications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Topics are split into **partitions** for scalability. Each partition acts as
    an ordered, immutable sequence of messages that is continually appended to. By
    partitioning topics into multiple partitions, Kafka can scale topic consumption
    by having multiple consumers reading from a topic in parallel across partitions.
    Partitions allow Kafka to distribute load horizontally across brokers and allow
    for parallelism. Data is kept in the order it was produced within each partition.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Kafka provides redundancy and fault tolerance by **replicating partitions**
    across a configurable number of brokers. A partition will have one broker designated
    as the “leader” and zero or more brokers acting as “followers.” All reads/writes
    go to the leader. Followers passively replicate the leader by having identical
    copies of the leader’s data. If the leader fails, one of the followers will automatically
    become the new leader.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Having **replicas** across brokers ensures fault tolerance since data is still
    available for consumption, even if some brokers go down. The replication factor
    controls the number of replicas. For example, a replication factor of three means
    there will be two followers replicating the one leader partition. Common production
    settings have a minimum of three brokers with a replication factor of two or three.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Consumers label themselves with **consumer group** names, and each record that’s
    published to a topic is only delivered to one consumer in a group. If there are
    multiple consumers in a group, Kafka will load balance messages across the consumers.
    Kafka guarantees an ordered, at-least-once delivery of messages within a partition
    to a single consumer. Consumer groups allow you to scale out consumers while still
    providing message-ordering guarantees.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '**Producers** publish records to topic partitions. If there is only one partition,
    all messages will go there. With multiple partitions, producers can choose to
    either publish randomly across partitions or ensure ordering by using the same
    partition. This ordering guarantee only applies within a partition, not across
    partitions.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Producers batch together messages for efficiency and durability. Messages are
    buffered locally and compressed before being sent to brokers. This batching provides
    better efficiency and throughput. Producers can choose to wait until a batch is
    full, or flush based on time or message size thresholds. Producers also replicate
    data by having it acknowledged by all in-sync replicas before confirming a write.
    Producers can choose different acknowledgment guarantees, ranging from committing
    as soon as the leader writes the record or waiting until all followers have replicated.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '**Consumers** read records by subscribing to Kafka topics. Consumer instances
    can be in separate processes or servers. Consumers pull data from brokers by periodically
    sending requests for data. Consumers keep track of their position (“offsets”)
    within each partition to start reading from the correct place in case of failures.
    Consumers typically commit offsets periodically. Offsets are also used to allow
    consumers to rewind or skip ahead if needed. How exactly do those offsets work?
    Let’s try to understand them a little deeper.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Kafka stores streams of records in categories called topics. Within a topic,
    records are organized into partitions, which allow for parallel processing and
    scalability. Each record within a partition gets an *incremental ID number* called
    an **offset** that uniquely identifies the record within that partition. This
    offset reflects the order of records within a partition. For example, an offset
    of three means it’s the third record.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 将记录流存储在称为主题的类别中。每个主题下的记录被组织成分区，这样就可以进行并行处理和扩展性。每个分区中的记录都有一个称为 **偏移量**
    的 *递增 ID 编号*，它唯一标识该分区内的记录。这个偏移量反映了分区内记录的顺序。例如，偏移量为三意味着它是第三条记录。
- en: When a Kafka consumer reads records from a partition, it keeps track of the
    offset of the last record it has read. This allows the consumer to only read newer
    records it hasn’t processed yet. If the consumer disconnects and reconnects later,
    it will start reading again from the last committed offset. The offsets commit
    log is stored in a Kafka topic named `__consumer_offsets`. This provides durability
    and allows consumers to transparently pick up where they left off in case of failures.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Kafka 消费者从分区读取记录时，它会跟踪已读取的最后一条记录的偏移量。这使得消费者只会读取尚未处理的新记录。如果消费者断开连接并稍后重新连接，它将从最后提交的偏移量处重新开始读取。偏移量提交日志存储在一个名为
    `__consumer_offsets` 的 Kafka 主题中。这样可以保证持久性，并允许消费者在故障发生时透明地从中断处恢复。
- en: Offsets enable multiple consumers to read from the same partition while ensuring
    each record is processed only once by each consumer. The consumers can read at
    their own pace without interfering with each other. This is a key design feature
    that enables Kafka’s scalability. All these features, when used together, allow
    Kafka to deliver **exactly-once semantics**. Let’s take a closer look at this
    concept.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 偏移量使得多个消费者可以从同一分区读取数据，同时确保每个消费者只处理每条记录一次。消费者可以按自己的速度读取数据，彼此之间不会互相干扰。这是 Kafka
    可扩展性的关键设计特性。当这些特性一起使用时，Kafka 可以实现 **精确一次语义**。让我们更详细地了解这个概念。
- en: How Kafka delivers exactly-once semantics
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka 如何实现精确一次语义
- en: 'When processing data streams, three types of semantics are relevant when considering
    guarantees on data delivery:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据流时，考虑数据传输保障时有三种相关的语义：
- en: '**At-least-once semantics**: In this case, each record in the data stream is
    guaranteed to be processed at least once but may be processed more than once.
    This can happen if there is a failure downstream from the data source before the
    processing is acknowledged. When the system recovers, the data source will resend
    the unacknowledged data, causing duplicate processing.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**至少一次语义**：在这种情况下，数据流中的每条记录保证至少被处理一次，但可能会被处理多次。如果数据源下游发生故障，在处理确认之前，系统将重新发送未确认的数据，导致重复处理。'
- en: '**At-most-once semantics**: In this case, each record will either be processed
    once or not at all. This prevents duplicate processing but means that in the event
    of failure, some records may be lost entirely.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最多一次语义**：在这种情况下，每条记录要么被处理一次，要么完全不处理。这可以防止重复处理，但意味着在发生故障时，一些记录可能会完全丢失。'
- en: '**Exactly-once semantics**: This case combines the guarantees of the other
    two and ensures each record is processed one and only one time. This is difficult
    to achieve in practice because it requires coordination between storage and processing
    to ensure no duplicates are introduced during retries.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确一次语义**：此案例结合了其他两种语义的保证，确保每条记录只被处理一次且仅一次。由于需要在存储和处理之间进行协调，以确保在重试过程中不会引入重复项，这在实践中非常难以实现。'
- en: Kafka provides a way to enable exactly-once semantics for event processing through
    a combination of architectural design and integration with stream processing systems.
    Kafka topics are divided into partitions, which enables data parallelism by spreading
    the load across brokers. Events with the same key go to the same partition, enabling
    processing ordering guarantees. Kafka assigns each partition a sequential ID called
    the offset, which uniquely identifies each event within a partition.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 提供了一种方法，通过架构设计和与流处理系统的集成，实现事件处理的精确一次语义。Kafka 主题被划分为多个分区，这使得数据可以通过将负载分散到不同的代理上来实现并行处理。具有相同键的事件会进入同一个分区，从而保证了处理顺序的保证。Kafka
    为每个分区分配一个顺序 ID，称为偏移量，它唯一标识该分区内的每个事件。
- en: Consumers track their position per partition by storing the offset of the last
    processed event. If a consumer fails and restarts, it will resume from the last
    committed offset, ensuring events are not missed or processed twice.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: By tightly integrating offset tracking and event delivery with stream processors
    through Kafka’s APIs, Kafka’s infrastructure provides the backbone for building
    exactly-once real-time data pipelines.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.1* shows a visual representation of Kafka’s architecture:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1- Kafka’s architecture](img/B21927_07_01.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1- Kafka’s architecture
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will do a quick exercise to get started and see Kafka in action.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: First producer and consumer
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After setting up Kafka with `docker-compose`, we have to create a topic that
    will hold our events. We can do this from outside the container or we can get
    into the container and run the commands from there. For this exercise, we will
    access the container and run commands from the inside for didactic purposes. Later
    in this book, we will study the other option, which can be very handy, especially
    when Kafka is running on Kubernetes. Let’s get started:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'First, check if all containers are up and running. In a terminal, run the following
    command:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You should see an output that specifies containers’ names, images, commands,
    and more. Everything seems to be running fine. Note the name of the first Kafka
    broker container.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will need it to run commands in Kafka from inside the container. To get
    in, run the following command:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we’re creating an environment variable with the first container name (in
    my case, `multinode_kafka-1_1`) and running the `docker exec` command with the
    `-``it` parameter.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we are in the container. Let’s declare three environment variables that
    will help us manage Kafka:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we will use the Kafka CLI to create a topic with the `kafka-topics --create`
    command. Run the following code:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will create a topic named `mytopic` with a replication factor of `3` (three
    replicas) and `3` partitions (note that your maximum number of partitions is the
    number of brokers you have).
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Although we have a confirmation message in the terminal, it’s good to list
    all the topics inside a cluster:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You should see `mytopic` as output on the screen.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, let’s get some information about our topic:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This yields the following output (formatted for better visualization):'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This topic structure partitioned across all three brokers and with replications
    of each partition in all other brokers is exactly what we have seen in *Figure
    7**.1*.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s build a simple producer and start sending some messages to this
    topic. In your terminal, type the following:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This starts a simple console producer.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, type some messages in the console; they’ll be sent to the topic:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can type whatever you want.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, open a different terminal and (preferably) put it beside the first terminal
    that’s running the console producer. We must log in to the container the same
    way we did in the first terminal:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, create the same necessary environment variables:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we will start a simple console consumer. We will tell this consumer to
    read all the messages in the topic from the beginning (just for this exercise
    – this is not recommended for topics in production with a huge amount of data
    in them). In the second terminal, run the following command:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You should see all the messages typed on the screen. Note that they are in a
    different order because Kafka only keeps messages in order inside partitions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Across partitions, ordering is not possible (unless you have date-time information
    inside the message, of course). Press *Ctrl* + *C* to stop the consumer. You can
    also press *Ctrl* + *C* in the producer terminal to stop it. Type `exit` in both
    terminals to exit the containers and stop and kill all the containers by running
    the following command:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can check that all containers were successfully removed with the following
    command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, let’s try something different. One of the most common cases of using Kafka
    is migrating data in real time from tables in a database. Let’s see how we can
    do that simplistically using Kafka Connect.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Streaming from a database with Kafka Connect
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will read all data that is generated in a Postgres table
    in real time with Kafka Connect. First, it is necessary to build a custom image
    of Kafka Connect that can connect to Postgres. Follow these steps:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a different folder for this new exercise. First, create a folder
    named `connect` and another folder inside it named `kafka-connect-custom-image`.
    Inside the custom image folder, we will create a new Dockerfile with the following
    content:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This Docker file bases itself on the confluent Kafka Connect image and installs
    two connectors – a JDBC source/sink connector and a sink connector for Amazon
    S3\. The former is necessary to connect to a database while the latter will be
    very handy for delivering events to S3.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Build your image with the following commands:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, in the `connec`t folder, you should have a `.env_kafka_connect` file to
    store your AWS credentials. Remember that credentials should *never* be hardcoded
    in any configuration files or code. Your `.env_kafka_connect` file should look
    like this:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Save it in the `connect` folder. Then, create a new `docker-compose.yaml` file.
    The content for this file is available in this book’s GitHub repository: [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/docker-compose.yaml](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/docker-compose.yaml).'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This Docker Compose file sets up an environment for Kafka and Kafka Connect
    along with a Postgres database instance. It defines the following services:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`zookeeper`: This runs a Zookeeper instance, which Kafka relies on for coordination
    between nodes. It sets up some configurations, such as port, tick time, and a
    client port.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`broker`: This runs a Kafka broker that depends on the Zookeeper service (a
    broker cannot exist until the Zookeepers are all up). It configures things such
    as the broker ID, what Zookeeper instance to connect to, listeners for external
    connections on ports `9092` and `29092`, replication settings for internal topics
    Kafka needs, and some performance tuning.'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`schema-registry`: This runs Confluent Schema Registry, which allows us to
    store schemas for topics. It depends on the Kafka broker and sets the URL for
    the Kafka cluster as well as what port to listen on for API requests.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connect`: This runs our customized image of Confluent Kafka Connect. It depends
    on both the Kafka broker and Schema Registry and sets up bootstrap servers, the
    group ID, internal topics for storing connector configurations, offsets and status,
    key-value converters for serialization, Schema Registry integration, and the plugin
    path for finding more connectors.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rest-proxy`: The runs the Confluent REST proxy, which provides a REST interface
    to Kafka. It sets up the Kafka broker connection information and Schema Registry.'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`postgres`: This runs a Postgres database instance that’s exposed on port `5432`
    with some basic credentials set. Note that we are saving the database password
    in plain text in our code. This should *never* be done in a production environment
    since it is a security breach. We are only defining the password in this way for
    local testing.'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: There is also a custom network defined called `proxynet` that all these services
    join. This allows inter-service communication by hostname instead of exposing
    all services to the host machine network.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To get these containers up and running, run the following command:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: All the containers should be up in a few minutes.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we will continuously insert some simulated data into our Postgres database.
    To do that, create a new Python file named `make_fake_data.py`. The code is available
    at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/connect/simulations](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/connect/simulations)
    folder. This code generates fake data for customers (such as name, address, profession,
    and email) and inserts it into a database. For it to work, you should have the
    `faker`, `pandas`, `psycopg2-binary`, and `sqlalchemy` libraries installed. Make
    sure you install them with `pip install` before running the code. A `requirements.txt`
    file, along with the code, is provided in this book’s GitHub repository.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, to run the simulations, in a terminal, type the following:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This will print the parameters for the simulation (interval of generation, sample
    size, and the connection string) on the screen and start printing the simulated
    data. After a few simulations, you can stop it by pressing *Ctrl* + *C*. Then,
    use your preferred SQL client (DBeaver is one option) to check if the data was
    correctly ingested in the database. Run a simple SQL statement (`select * from
    customers`) to see the data printed in the SQL client.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we will register a source JDBC connector to pull data from Postgres. This
    connector will run as a Kafka Connect process that establishes a JDBC connection
    to the source database. It uses this connection to execute SQL queries that select
    data from specific tables. The connector translates the result sets into JSON
    documents and publishes them to configured Kafka topics. Each table has a dedicated
    topic created for it. The query that extracts data can be either a simple `SELECT`
    statement or an incremental query based on timestamp or numeric columns. This
    allows us to capture new or updated rows.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we will define a configuration file to deploy the connector on Kafka
    Connect. Create a folder named `connectors` and a new file named `connect_jdbc_pg_json.config`.
    The configuration code is shown here:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**connect_jdbc_pg_json.config**'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This configuration creates a Kafka connector that will sync rows from the `customers`
    table to JSON-formatted Kafka topics, based on timestamp changes to the rows.
    Let’s take a closer look at the parameters that were used:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`name`: Names the connector for management purposes.'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connector.class`: Specifies the JDBC connector class from Confluent to use.'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value.converter`: Specifies that data will be converted into JSON format in
    Kafka.'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value.converter.schemas.enable`: Enables schemas to be stored with the JSON
    data.'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tasks.max`: Limits to one task. This parameter can be increased in a production
    environment for scalability, depending on the number of partitions in the topic.'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connection.url`: Connects to a local PostgreSQL database on port `5432`.'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connection.user/.password`: PostgreSQL credentials (only in plaintext here
    for this exercise. Credentials should *never* be hardcoded).'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode`: Specifies to use a timestamp column to detect new/changed rows. You
    could also use an `id` column.'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timestamp.column.name`: Looks at the `dt_update` column.'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`table.whitelist`: Specifies to sync the `customers` table.'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topic.prefix`: Output topics will be prefixed with `json-`.'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validate.non.null`: Allows syncing rows with null values.'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`poll.interval.ms`: Check for new data every 500ms.'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we will create a Kafka topic to store the data from the Postgres table.
    In a terminal, type the following:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that we are using the `docker-compose` API to execute a command inside
    a container. The first part of the command (`docker-compose exec broker`) tells
    Docker that we want to execute something in the `broker` service defined in the
    `docker-compose.yaml` file. The rest of the command is executed inside the broker.
    We are creating a topic called `json-customers` with two partitions and a replication
    factor of one (one replica per partition). You should see a confirmation message
    in the terminal that the topic was created.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will register the connector using a simple API call to Kafka Connect.
    We will use the `curl` library to do that. In your terminal, type the following:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The name of the connector should be printed in the terminal.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, do a quick check on the Connect instance logs:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Roll up a few lines; you should see the output for the connector registration.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s try a simple console consumer just to validate that the messages
    are already being migrated to the topic:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You should see the messages in JSON format printed on the screen. Press *Ctrl*
    + *C* to stop the consumer and type `exit` to exit the container.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we will configure a sink connector to deliver those messages to Amazon
    S3\. First, go to AWS and create a new S3 bucket. S3 bucket names must be unique
    across all AWS. This way, I recommend setting it with the account name as a suffix
    (for instance, `kafka-messages-xxxxxxxx`).
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inside the `connectors` folder, create a new file named `connect_s3_sink.config`:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**connect_s3_sink.config**'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s become familiar with the parameters of this connector:'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`connector.class`: Specifies the connector class to use. In this case, it is
    the Confluent S3 sink connector.'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`format.class`: Specifies the format to use when writing data to S3\. Here,
    we’re using `JsonFormat` so that data will be stored in JSON format.'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`key.converter` and `value.converter`: Specify the converter classes to use
    for serializing the key and values to JSON, respectively.'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`key.converter.schemas.enable` and `value.converter.schemas.enable`: Disable
    schema validation for keys and values.'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flush.size`: Specifies the number of records the connector should wait before
    performing a flush to S3\. Here, this parameter is set to `1`. However, in production,
    when you have a large message throughput, it is best to set this value higher
    so that more messages get delivered to S3 in a single file.'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`schema.compatibility`: Specifies the schema compatibility rule to use. Here,
    `FULL` means that schemas must be fully compatible.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s3.bucket.name`: The name of the S3 bucket to write data to.'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s3.region`: The AWS region where the S3 bucket is located.'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s3.object.tagging`: Enables S3 object tagging.'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s3.ssea.name`: The server-side encryption algorithm to use (AES256, S3 managed
    encryption, in this case).'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topics.dir`: Specifies the directory in the S3 bucket to write data to.'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storage.class`: Specifies the underlying storage class.'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tasks.max`: The maximum number of tasks for this connector. This should typically
    be `1` for a sink.'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topics`: A comma-separated list of topics to get data from to write to S3.'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we can register the sink connector. In your terminal, type the following:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Check the logs with `docker logs connect` to validate that the connector was
    correctly registered and there were no errors in its deployment.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it! You can check the S3 bucket on AWS and see the JSON files coming
    through. If you want, run the `make_fake_data.py` simulator once again to see
    more messages be delivered to S3.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to set up a real-time message delivery pipeline, let’s
    introduce some real-time processing in it with Apache Spark.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Real-time data processing with Kafka and Spark
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An extremely important part of real-time data pipelines relates to real-time
    processing. As data gets generated continuously from various sources, such as
    user activity logs, IoT sensors, and more, we need to be able to make transformations
    on these streams of data in real time.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark’s Structured Streaming module provides a high-level API for processing
    real-time data streams. It builds on top of Spark SQL and provides expressive
    stream processing using SQL-like operations. Spark Structured Streaming processes
    data streams using a micro-batch processing model. In this model, streaming data
    is received and collected into small batches that are processed very quickly,
    typically within milliseconds. This provides low processing latency while retaining
    the scalability of batch processing.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: We will take from the real-time pipeline that we started with Kafka and build
    real-time processing on top of it. We will use the Spark Structured Streaming
    module for that. Create a new folder called `processing` and a file inside it
    called `consume_from_kafka.py`. The Spark code that processes the data and aggregates
    the results has been provided here.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is also available in this book’s GitHub repository: [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/processing/consume_from_kafka.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/processing/consume_from_kafka.py).
    This Spark Structured Streaming application is reading from the `json-customers`
    Kafka topic, transforming the JSON data, and computing aggregations on it before
    printing the output to the console:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: consume_from_kafka.py
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'First, `SparkSession` is created and configured to use the Kafka connector
    package. Error logging is set to reduce noise and facilitate output visualization
    in the terminal. Next, a DataFrame is created by reading from the `json-customers`
    topic using the `kafka` source. It connects to Kafka running on localhost, starts
    reading from the earliest offset, and represents each message payload as a string:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This second block defines two schemas – `schema1` captures the nested JSON structure
    expected in the Kafka payload, with a schema field and payload field. On the other
    hand, `schema2` defines the actual customer data schema contained in the payload
    field.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'The value string field, representing the raw Kafka message payload, is extracted
    from the initial DataFrame. This string payload is parsed as JSON using the defined
    `schema1` to extract just the payload field. The payload string is then parsed
    again using `schema2` to extract the actual customer data fields into a new DataFrame
    called `newdf`:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, the transformations occur – the `birthdate` string is cast to `date`,
    the current date is fetched, and the age is calculated using `datediff`. The data
    is aggregated by gender to compute the count, earliest birthdate in data, current
    date, and average age:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Finally, the aggregated DataFrame is written to the console in append output
    mode using Structured Streaming. This query will run continuously until it’s terminated
    by running *Ctrl* + *C*.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the query, in a terminal, type the following command:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You should see the aggregated data in your terminal. Open another terminal
    and run more simulations by running the following command:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As new simulations are generated and ingested into Postgres, the Kafka connector
    will automatically pull them to the `json-customers` topic, at which point Spark
    will pull those messages, calculate the aggregations in real time, and print the
    results. After a while, you can hit *Ctrl* + *C* to stop simulations and then
    again to stop the Spark streaming query.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You ran a real-time data processing pipeline using Kafka and
    Spark! Remember to clean up the created resources with `docker-compose down`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the fundamental concepts and architecture behind
    Apache Kafka – a popular open source platform for building real-time data pipelines
    and streaming applications.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: You learned how Kafka provides distributed, partitioned, replicated, and fault-tolerant
    PubSub messaging through its topics and brokers architecture. Through hands-on
    examples, you gained practical experience with setting up local Kafka clusters
    using Docker, creating topics, and producing and consuming messages. You understood
    offsets and consumer groups that enable fault tolerance and parallel consumption
    from topics.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: We introduced Kafka Connect, which allows us to stream data between Kafka and
    external systems such as databases. You implemented a source connector to ingest
    changes from a PostgreSQL database into Kafka topics. We also set up a sink connector
    to deliver the messages from Kafka to object storage in AWS S3 in real time.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: The highlight was building an end-to-end streaming pipeline with Kafka and Spark
    Structured Streaming. You learned how micro-batch processing on streaming data
    allows low latency while retaining scalability. The example provided showed how
    to consume messages from Kafka, transform them using Spark, and aggregate the
    results in real time.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Through these hands-on exercises, you gained practical experience with Kafka’s
    architecture and capabilities for building robust and scalable streaming data
    pipelines and applications. Companies can greatly benefit from leveraging Kafka
    to power their real-time data processing needs to drive timely insights and actions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will finally get all the technologies we’ve studied
    so far into Kubernetes. You will learn how to deploy Airflow, Spark, and Kafka
    in Kubernetes and get them ready to build a fully integrated data pipeline.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Connecting It All Together'
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you will learn how to deploy and orchestrate the big data tools
    and technologies covered in the previous chapters on Kubernetes. You will build
    scripts to deploy Apache Spark, Apache Airflow, and Apache Kafka on a Kubernetes
    cluster, making them ready for running data processing jobs, orchestrating data
    pipelines, and handling real-time data ingestion, respectively. Additionally,
    you will explore data consumption layers, data lake engines such as Trino, and
    real-time data visualization with Elasticsearch and Kibana, all deployed on Kubernetes.
    Finally, you will bring everything together by building and deploying two complete
    data pipelines, one for batch processing and another for real-time processing,
    on a Kubernetes cluster. The part also covers the deployment of generative AI
    applications on Kubernetes and provides guidance on where to go next in your Kubernetes
    and big data journey.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapters:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B21927_08.xhtml#_idTextAnchor134), *Deploying the Big Data Stack
    on Kubernetes*'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B21927_09.xhtml#_idTextAnchor141), *Data Consumption Layer*'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B21927_10.xhtml#_idTextAnchor154), *Building a Big Data Pipeline
    on Kubernetes*'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B21927_11.xhtml#_idTextAnchor167), *Generative AI on Kubernetes*'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B21927_12.xhtml#_idTextAnchor183), *Where To Go From Here*'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
