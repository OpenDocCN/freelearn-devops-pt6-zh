- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Kubernetes – Introduction and Integration with GenAI
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes – 简介及与 GenAI 的集成
- en: 'Deploying and managing GenAI workloads at scale presents significant challenges,
    including building the models, packaging them for distribution, and ensuring effective
    deployment and scaling. In this chapter, we will discuss the concepts of containers
    and **Kubernetes** (**K8s**) and why they are emerging as powerful solutions to
    address these complexities. They are becoming the de facto standard for companies
    such as OpenAI ([https://openai.com/index/scaling-kubernetes-to-7500-nodes/](https://openai.com/index/scaling-kubernetes-to-7500-nodes/))
    and Anthropic ([https://youtu.be/c9NJ6GSeNDM?si=xjei4T9VfZvejD5o&t=2412](https://youtu.be/c9NJ6GSeNDM?si=xjei4T9VfZvejD5o&t=2412))
    to deploy GenAI workloads. We will cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模部署和管理 GenAI 工作负载时，会面临诸多挑战，包括构建模型、打包模型进行分发，以及确保有效的部署和扩展。在本章中，我们将讨论容器和**Kubernetes**（**K8s**）的概念，以及它们为什么成为应对这些复杂性强大解决方案的趋势。它们正成为
    OpenAI（[https://openai.com/index/scaling-kubernetes-to-7500-nodes/](https://openai.com/index/scaling-kubernetes-to-7500-nodes/)）和
    Anthropic（[https://youtu.be/c9NJ6GSeNDM?si=xjei4T9VfZvejD5o&t=2412](https://youtu.be/c9NJ6GSeNDM?si=xjei4T9VfZvejD5o&t=2412)）等公司的事实标准，用于部署
    GenAI 工作负载。我们将涵盖以下主要内容：
- en: Understanding containers
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解容器
- en: Why containers for GenAI models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择容器来运行 GenAI 模型
- en: What is Kubernetes (K8s)?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 Kubernetes（K8s）？
- en: Understanding containers
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解容器
- en: Containers have revolutionized the way we manage applications by standardizing
    the packaging format. With the portability of containers, applications are packaged
    as a standard unit of software that packages all of your code and dependencies
    to deploy consistently and reliably across various environments, such as on-premises,
    public, and private clouds. Containers are also considered an evolution of **Virtual
    Machine** (**VM**) technology, where multiple containers are run on the same operating
    system, sharing the underlying kernel to increase the overall server utilization.
    This is a massive advantage of containers as there is no overhead of multiple
    **Operating Systems** (**OSes**) and other OS-level components. So, containers
    can be started and stopped a lot faster while providing isolation.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 容器通过标准化打包格式，彻底改变了我们管理应用程序的方式。借助容器的可移植性，应用程序被打包成一个标准的软件单元，包含所有代码和依赖项，以便在各种环境中（如本地、公共云和私有云）一致且可靠地部署。容器还被认为是**虚拟机**（**VM**）技术的演进，其中多个容器在同一个操作系统上运行，共享底层内核以提高整体服务器利用率。这是容器的巨大优势，因为没有多个**操作系统**（**OSes**）和其他操作系统级组件的开销。因此，容器可以更快地启动和停止，同时提供隔离性。
- en: The following figure illustrates the evolution of computing environments, highlighting
    a shift toward higher levels of abstraction and an increased focus on business
    logic.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了计算环境的演变，突出了向更高层次抽象的转变，以及对业务逻辑的关注日益增强。
- en: '![Figure 2.1 – Evolution of container technology](img/B31108_02_01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 容器技术的演变](img/B31108_02_01.jpg)'
- en: Figure 2.1 – Evolution of container technology
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 容器技术的演变
- en: Physical servers offer the least abstraction, with extensive manual configuration,
    and resource inefficiencies. VMs provide a middle ground, by abstracting underlying
    hardware resources using hypervisor technology. This allows you to run multiple
    VMs on the same physical server, increasing the resource utilization and security.
    However, they are bulky and slow to boot up. Containers provide the highest level
    of abstraction by encapsulating applications and dependencies in portable units,
    allowing us to focus more on developing and optimizing business logic rather than
    managing infrastructure. *Figure 2**.2* illustrates the high-level differences
    between VMs and containers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 物理服务器提供最少的抽象，需大量手动配置，并且存在资源低效问题。虚拟机通过使用虚拟化技术来抽象底层硬件资源，提供一种折中的方案。这使得你可以在同一台物理服务器上运行多个虚拟机，从而提高资源利用率和安全性。然而，它们体积庞大，启动速度较慢。容器通过将应用程序和依赖项封装在便于移植的单元中，提供了最高级别的抽象，让我们可以更多地专注于开发和优化业务逻辑，而不是管理基础设施。*图
    2.2* 展示了虚拟机和容器之间的高层次差异。
- en: '![Figure 2.2 – Virtual machines versus containers](img/B31108_02_02.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 虚拟机与容器的对比](img/B31108_02_02.jpg)'
- en: Figure 2.2 – Virtual machines versus containers
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 虚拟机与容器的对比
- en: Container technology is made possible because of namespaces and cgroups (control
    groups) in the Linux kernel. They form the foundational building blocks to provide
    isolation and resource limits. A **Linux Namespace** ([https://man7.org/linux/man-pages/man7/namespaces.7.html](https://man7.org/linux/man-pages/man7/namespaces.7.html))
    is an abstraction over resources in the operating system. It partitions OS-level
    resources such that different sets of processes see a different set of resources
    (network, file system, etc.,) even though they are running on the same OS kernel.
    **Cgroups** ([https://man7.org/linux/man-pages/man7/cgroups.7.html](https://man7.org/linux/man-pages/man7/cgroups.7.html))
    govern the isolation and usage of system resources, such as CPU, memory, and network,
    for a group of processes and optionally enforce limits and constraints. These
    capabilities let containers abstract the operating system components for modern
    applications.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 容器技术的实现得益于 Linux 内核中的命名空间（namespaces）和控制组（cgroups）。它们构成了提供隔离和资源限制的基础构件。**Linux
    命名空间** ([https://man7.org/linux/man-pages/man7/namespaces.7.html](https://man7.org/linux/man-pages/man7/namespaces.7.html))
    是操作系统资源的一种抽象。它将操作系统级别的资源进行划分，使得不同的进程集合即使在同一操作系统内核上运行，也能看到不同的资源集（如网络、文件系统等）。**控制组**
    ([https://man7.org/linux/man-pages/man7/cgroups.7.html](https://man7.org/linux/man-pages/man7/cgroups.7.html))
    用于管理一组进程的系统资源的隔离和使用，如 CPU、内存、网络等，并可以选择性地强制执行限制和约束。这些能力使容器能够为现代应用程序抽象操作系统组件。
- en: Container terminology
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器术语
- en: 'The following are some terms associated with containers that will be crucial
    in following along with this book:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些与容器相关的术语，它们在阅读本书时至关重要：
- en: '**Container runtime**: This is a host-level process that is responsible for
    creating, stopping, and starting containers. It interacts with low-level container
    runtimes such as runc to set up namespaces and cgroups for containers. Popular
    examples include containerd, CRI-O, and so on.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器运行时**：这是一个主机级别的进程，负责创建、停止和启动容器。它与低级容器运行时（如 runc）交互，为容器设置命名空间和控制组。常见的容器运行时包括
    containerd、CRI-O 等。'
- en: '**Container image**: This is a lightweight, standalone, executable package
    that includes everything needed to run a piece of software, including the code,
    runtime, libraries, environment variables, and configuration files. It is created
    using a Dockerfile, a plain text definition file that includes a set of instructions
    to install dependencies, applications, and so on. Typical characteristics of a
    container image include the following:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器镜像**：这是一个轻量级、独立的可执行包，包含了运行软件所需的一切，包括代码、运行时、库、环境变量和配置文件。它是通过 Dockerfile
    创建的，Dockerfile 是一个包含一组指令的纯文本定义文件，用来安装依赖、应用程序等。容器镜像的典型特征包括以下几点：'
- en: '**Self-contained**: It encapsulates everything needed to run software applications.'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自包含**：它封装了运行软件应用所需的一切。'
- en: '**Immutable**: It is read-only in nature; any changes would require a new image.'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可变**：它是只读的；任何更改都需要创建新的镜像。'
- en: '**Layered**: Images are built in layers, each layer representing a file system.
    This is what makes images highly efficient as common layers can be shared across
    multiple images.'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分层**：镜像是分层构建的，每一层都代表一个文件系统。这使得镜像具有高度的效率，因为相同的层可以在多个镜像之间共享。'
- en: '**Portable**: As the image packages the application and all its dependencies,
    they can be run on any system that supports a container runtime, making them highly
    portable in nature.'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可移植**：由于镜像打包了应用程序及其所有依赖项，因此可以在任何支持容器运行时的系统上运行，使其具有高度的可移植性。'
- en: '**Container registry**: This is a tool used to manage and distribute container
    images. Popular registries include Docker Hub ([https://hub.docker.com/](https://hub.docker.com/)),
    Amazon Elastic Container Registry ([https://aws.amazon.com/ecr/](https://aws.amazon.com/ecr/)),
    Google Artifact Registry ([https://cloud.google.com/artifact-registry](https://cloud.google.com/artifact-registry)),
    and so on. Tools such as Artifactory and Harbor can be used to self-host a registry.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器注册表**：这是一个用于管理和分发容器镜像的工具。常见的注册表包括 Docker Hub ([https://hub.docker.com/](https://hub.docker.com/))、亚马逊弹性容器注册表
    ([https://aws.amazon.com/ecr/](https://aws.amazon.com/ecr/))、谷歌 Artifact Registry
    ([https://cloud.google.com/artifact-registry](https://cloud.google.com/artifact-registry))
    等。工具如 Artifactory 和 Harbor 可以用来自托管注册表。'
- en: '**Container**: This is a running instance or process created from the container
    image by the container runtime.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器**：这是一个通过容器运行时从容器镜像创建的正在运行的实例或进程。'
- en: Let’s now understand the high-level container workflow using Docker, a software
    platform designed to help developers build, share, and run container applications.
    Docker follows traditional client-server architecture. When you install Docker
    on a host, it runs a server component called the Docker daemon and a client component
    – the Docker CLI. The **Daemon** is responsible for creating and managing images,
    running containers using those images, and setting up networking, storage, and
    so on. As depicted in *Figure 2**.3*, the Docker CLI is used to interact with
    the Docker daemon process to build and run containers. Once the images are built,
    the Docker daemon can push and pull those images to/from a container registry.
    Storing the images in a container registry allows them to be portable and they
    can be run anywhere a container runtime is available.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过 Docker 来了解高级容器工作流，Docker 是一个旨在帮助开发人员构建、分享和运行容器应用程序的软件平台。Docker 遵循传统的客户端-服务器架构。当你在主机上安装
    Docker 时，它会运行一个名为 Docker 守护进程的服务器组件和一个客户端组件——Docker CLI。**守护进程**负责创建和管理镜像，使用这些镜像运行容器，并设置网络、存储等。如*图
    2.3*所示，Docker CLI 用于与 Docker 守护进程交互，以构建和运行容器。一旦镜像构建完成，Docker 守护进程可以将这些镜像推送到容器注册表，或从容器注册表拉取镜像。将镜像存储在容器注册表中可以使它们具有可移植性，并且可以在任何有容器运行时的地方运行。
- en: '![Figure 2.3 – Docker architecture overview](img/B31108_02_03.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – Docker 架构概述](img/B31108_02_03.jpg)'
- en: Figure 2.3 – Docker architecture overview
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – Docker 架构概述
- en: Creating a container image
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建容器镜像
- en: 'Let’s create our first hello world container image and run it locally. Use
    the Docker documentation page at [https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/)
    to install Docker Engine on your machine. The following are the links to install
    Docker Desktop for different operating systems:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建第一个 hello world 容器镜像并在本地运行它。请使用 [https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/)
    上的 Docker 文档页面，在你的机器上安装 Docker 引擎。以下是不同操作系统安装 Docker Desktop 的链接：
- en: 'Docker Desktop for Linux: [https://docs.docker.com/desktop/install/linux-install/](https://docs.docker.com/desktop/install/linux-install/)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker Desktop for Linux: [https://docs.docker.com/desktop/install/linux-install/](https://docs.docker.com/desktop/install/linux-install/)'
- en: 'Docker Desktop for Mac (macOS): [https://docs.docker.com/desktop/install/mac-install/](https://docs.docker.com/desktop/install/mac-install/)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker Desktop for Mac (macOS): [https://docs.docker.com/desktop/install/mac-install/](https://docs.docker.com/desktop/install/mac-install/)'
- en: 'Docker Desktop for Windows: [https://docs.docker.com/desktop/install/windows-install/](https://docs.docker.com/desktop/install/windows-install/)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker Desktop for Windows: [https://docs.docker.com/desktop/install/windows-install/](https://docs.docker.com/desktop/install/windows-install/)'
- en: 'In the following Dockerfile, we are creating a simple hello world application
    using the `nginx` server. We will use `nginx` as the parent image and customize
    the `index.html` with our *Hello World!* message. Let’s start by following these
    steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下 Dockerfile 中，我们使用 `nginx` 服务器创建一个简单的 hello world 应用程序。我们将使用 `nginx` 作为父镜像，并将
    `index.html` 自定义为我们的 *Hello World!* 消息。让我们按照以下步骤开始：
- en: 'Create a Dockerfile with the following content:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个具有以下内容的 Dockerfile：
- en: '[PRE0]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Build a container image with a `v1` tag:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `v1` 标签构建容器镜像：
- en: '[PRE1]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can list the local container images using the following command:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以使用以下命令列出本地的容器镜像：
- en: '[PRE2]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Run a container using the `hello-world` image and bind nginx port `80` to the
    `8080` host port:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `hello-world` 镜像运行容器，并将 nginx 的端口 `80` 映射到主机的 `8080` 端口：
- en: '[PRE3]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that we have the image running, we can test the container by calling localhost
    `8080`:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经启动了镜像，我们可以通过访问本地的 `8080` 来测试容器：
- en: '[PRE4]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As an optional step, you can also push the container image to an `xyz` container
    registry so that you can run it anywhere. Replace `xyz` with your container repository
    name. For example, follow the instructions at [https://www.docker.com/blog/how-to-use-your-own-registry-2/](https://www.docker.com/blog/how-to-use-your-own-registry-2/)
    to create the registry in Docker Hub:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作为可选步骤，你也可以将容器镜像推送到 `xyz` 容器注册表，以便你可以在任何地方运行它。将 `xyz` 替换为你的容器仓库名称。例如，按照 [https://www.docker.com/blog/how-to-use-your-own-registry-2/](https://www.docker.com/blog/how-to-use-your-own-registry-2/)
    上的说明创建 Docker Hub 中的注册表：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this section, we learned about the evolution of computing environments, the
    benefits of using containers over traditional physical servers, and VMs. We gained
    an understanding of the overall Docker architecture and various key container
    terminology and built and ran our first hello-world container application. Let’s
    explore why containers are a great fit for GenAI models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了计算环境的演变，使用容器相比传统物理服务器和虚拟机的优势。我们了解了整体的 Docker 架构以及各种关键的容器术语，并构建和运行了我们的第一个
    hello-world 容器应用。接下来，让我们探讨为什么容器是 GenAI 模型的理想选择。
- en: Why containers for GenAI models?
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么为 GenAI 模型选择容器？
- en: A typical challenge with developing ML or GenAI applications involves using
    complex and continuously evolving open source ML frameworks such as PyTorch and
    TensorFlow, ML tool kits such as Hugging Face Transformers, ever-changing GPU
    hardware ecosystems from NVIDIA, and custom accelerators from Amazon, Google,
    and so on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 开发 ML 或 GenAI 应用的一个典型挑战是使用复杂且不断发展的开源 ML 框架，如 PyTorch 和 TensorFlow，ML 工具包如 Hugging
    Face Transformers，以及来自 NVIDIA 的不断变化的 GPU 硬件生态系统和来自亚马逊、谷歌等的定制加速器。
- en: The following figure illustrates various components involved in creating and
    running an ML or GenAI container.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了创建和运行 ML 或 GenAI 容器所涉及的各种组件。
- en: '![Figure 2.4 – A typical GenAI container image](img/B31108_02_04.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 典型的 GenAI 容器镜像](img/B31108_02_04.jpg)'
- en: Figure 2.4 – A typical GenAI container image
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 典型的 GenAI 容器镜像
- en: 'At the top layers, the container encapsulates various software libraries, deep
    learning frameworks, and user-supplied code. The next set of layers contains hardware-layer-specific
    libraries to interact with GPUs or custom accelerators on the host. The container
    runtime can be used to launch the containers from the container image. Let’s dive
    into the significant benefits of using containers for GenAI workloads:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器的最上层，封装了各种软件库、深度学习框架和用户提供的代码。下一组层包含硬件层特定的库，用于与主机上的 GPU 或定制加速器交互。可以使用容器运行时从容器镜像启动容器。让我们深入探讨使用容器处理
    GenAI 工作负载的显著优势：
- en: '**Dependency management**: This could become crucial due to evolving frameworks
    and interdependencies on specific versions. With containers, we can encapsulate
    the GenAI application code and its dependencies in a container image and use it
    on a developer machine or in the test/production environment consistently.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖管理**：由于框架和版本间的相互依赖，依赖管理可能变得至关重要。通过容器，我们可以将 GenAI 应用程序代码及其依赖项封装在容器镜像中，并在开发者机器或测试/生产环境中始终如一地使用。'
- en: '**Resource access**: GenAI/ML apps are computationally intensive, need access
    to single or multiple GPUs or custom accelerators, and adjust resource allocations
    dynamically based on the workload demand. Containers allow for fine-grained control
    over resource allocation, enabling efficient utilization of the available resources
    and avoiding noisy neighbor situations. Containers can also be scaled horizontally
    or vertically to handle increased demand in the applications.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源访问**：GenAI/ML 应用计算密集型，需访问单个或多个 GPU 或定制加速器，并根据工作负载需求动态调整资源分配。容器允许对资源分配进行细粒度控制，能够高效利用可用资源，并避免邻居噪声问题。容器还可以水平或垂直扩展，以应对应用程序需求的增加。'
- en: '**Model versioning and updates**: Managing different versions of the model
    and keeping respective dependencies up to date without disrupting applications
    could be challenging. With containers, different images can be created and versioned,
    making it easy to track changes, manage different model versions, and seamlessly
    execute rollbacks if needed. We will also explore how to use container orchestration
    engines to automate these updates later, in [*Chapter 11*](B31108_11.xhtml#_idTextAnchor145).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型版本控制和更新**：管理模型的不同版本并保持相关依赖项更新而不干扰应用程序可能是一个挑战。使用容器，可以创建和版本化不同的镜像，便于跟踪更改、管理不同的模型版本，并在需要时无缝地执行回滚。我们还将探讨如何使用容器编排引擎来自动化这些更新，稍后将在
    [*第 11 章*](B31108_11.xhtml#_idTextAnchor145)中进行详细介绍。'
- en: '**Security**: Protecting data during the training and inference stages is crucial
    when developing GenAI apps. With containers, we can enforce strict access controls
    and policies for data access, minimize the attack surface by including only the
    necessary components in the container image, and they also provide a layer of
    isolation between the containers and the underlying host.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：在开发GenAI应用时，保护训练和推理阶段的数据至关重要。通过使用容器，我们可以为数据访问实施严格的访问控制和策略，减少攻击面，只在容器镜像中包含必要的组件，还可以在容器与底层主机之间提供一层隔离。'
- en: By providing isolated, consistent, and reproducible environments, containers
    can simplify dependency management, optimize resource efficiency, streamline model
    deployments, and improve overall system security. Container technology comprehensively
    addresses all challenges presented by GenAI application development, thus making
    it a de facto choice.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供隔离、一致和可重现的环境，容器可以简化依赖管理、优化资源效率、简化模型部署并提高整体系统安全性。容器技术全面解决了GenAI应用开发中的所有挑战，因此成为事实上的首选。
- en: Building a GenAI container image
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建GenAI容器镜像
- en: Let’s get a first-hand experience of building our first GenAI container image
    and deploying it locally. To get started, we will download the model files from
    **Hugging Face** ([https://huggingface.co/](https://huggingface.co/)), an AI/ML
    platform that helps users build, deploy, and test machine learning models. Hugging
    Face operates the **Model Hub**, where developers and researchers can share thousands
    of pre-trained models. It also supports various frameworks, including TensorFlow,
    PyTorch, and ONNX.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们亲身体验构建第一个GenAI容器镜像并将其部署到本地。首先，我们将从**Hugging Face**([https://huggingface.co/](https://huggingface.co/))下载模型文件，这是一个帮助用户构建、部署和测试机器学习模型的AI/ML平台。Hugging
    Face运营着**模型中心**，开发者和研究人员可以在这里共享成千上万的预训练模型。它还支持多种框架，包括TensorFlow、PyTorch和ONNX。
- en: 'In this walk-through, we will take one of the popular open source models, **Llama
    2** ([https://llama.meta.com/llama2/](https://llama.meta.com/llama2/)), by **Meta**.
    However, you have to read and comply with the terms and conditions of the model.
    Navigate to the Hugging Face Llama model page ([https://huggingface.co/meta-llama/Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b))
    to request access to the model. The Llama 2 model comes in multiple sizes: 7B,
    13B, and 70B where B stands for billion parameters. The bigger the size of the
    model, the greater the resources needed to host it. Given not all personal laptops
    are equipped with specialized hardware such as GPUs, we will be using the CPU
    version of the Llama 2 model. This was made possible because of **llama.cpp**
    ([https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)),
    an open source project aimed at providing an efficient and portable implementation
    of Llama models. It enables us to deploy these models on various platforms, including
    personal computers, without requiring GPUs. llama.cpp applies a custom quantization
    approach to compress the models in a GGUF format, to reduce the size and resource
    requirements.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用**Meta**的一个流行开源模型**Llama 2**([https://llama.meta.com/llama2/](https://llama.meta.com/llama2/))。不过，您需要阅读并遵守模型的条款和条件。请访问Hugging
    Face的Llama模型页面([https://huggingface.co/meta-llama/Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b))以请求访问该模型。Llama
    2模型有多个版本：7B、13B和70B，其中B代表十亿参数。模型的大小越大，所需的资源就越多。考虑到并非所有个人笔记本电脑都配备了专门的硬件（如GPU），我们将使用Llama
    2模型的CPU版本。这得益于**llama.cpp**([https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp))，这是一个开源项目，旨在提供Llama模型的高效且可移植的实现。它使我们能够在各种平台上（包括个人电脑）部署这些模型，而无需GPU。llama.cpp采用定制量化方法，将模型压缩为GGUF格式，从而减少模型的大小和资源需求。
- en: 'An inference endpoint is created using the `5000` and accepts a JSON request
    with an input prompt, system message, and so on. Input parameters are then passed
    to the Llama model and model output is returned in the JSON format. Let’s begin
    the process:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`5000`创建推理端点，并接受包含输入提示、系统消息等的JSON请求。输入参数将传递给Llama模型，并以JSON格式返回模型输出。让我们开始这个过程：
- en: 'Install the pre-requisites:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装先决条件：
- en: '`huggingface-cli` from [https://huggingface.co/docs/huggingface_hub/en/installation](https://huggingface.co/docs/huggingface_hub/en/installation)'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从[https://huggingface.co/docs/huggingface_hub/en/installation](https://huggingface.co/docs/huggingface_hub/en/installation)安装`huggingface-cli`
- en: '`jq` from [https://jqlang.github.io/jq/download/](https://jqlang.github.io/jq/download/)'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自[https://jqlang.github.io/jq/download/](https://jqlang.github.io/jq/download/)的`jq`
- en: '**Docker Engine** from [https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/)'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自[https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/)的**Docker
    Engine**
- en: Authenticate to the Hugging Face platform by following the instructions at [https://huggingface.co/docs/huggingface_hub/en/guides/cli](https://huggingface.co/docs/huggingface_hub/en/guides/cli).
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照[https://huggingface.co/docs/huggingface_hub/en/guides/cli](https://huggingface.co/docs/huggingface_hub/en/guides/cli)上的说明进行Hugging
    Face平台的身份验证。
- en: 'Download the Llama 2 model using `huggingface-cli`:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`huggingface-cli`下载Llama 2模型：
- en: '[PRE6]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following message is displayed when the model download is complete:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型下载完成后，将显示以下消息：
- en: '[PRE7]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create an `app.py` with our Flask code. This code block sets up a Python Flask
    app with a single route, `/predict`, that accepts HTTP POST requests. It uses
    the `llama_cpp` library to load the Llama 2 model, generates a response based
    on the input prompt and system message from the request, and returns the model’s
    response as a JSON object. You can download the `app.py` code from GitHub at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/app.py](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/app.py):'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含Flask代码的`app.py`。此代码块设置了一个Python Flask应用程序，具有一个单一路由`/predict`，它接受HTTP
    POST请求。它使用`llama_cpp`库加载Llama 2模型，基于输入提示和请求中的系统消息生成响应，并将模型的响应作为JSON对象返回。你可以从GitHub下载`app.py`代码：[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/app.py](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/app.py)：
- en: '[PRE8]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create a Dockerfile that packages the Python source code, Llama 2 model, and
    other dependencies. You can download the Dockerfile code from GitHub at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/Dockerfile](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/Dockerfile):'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个Dockerfile，将Python源代码、Llama 2模型和其他依赖项打包在一起。你可以从GitHub下载Dockerfile代码：[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/Dockerfile](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/Dockerfile)：
- en: '[PRE9]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Build the container image:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建容器镜像：
- en: '[PRE10]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'curl:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'curl:'
- en: '[PRE11]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will run the inference against our local `my-llama` container and return
    the following LLM response:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这将对我们的本地`my-llama`容器进行推理，并返回以下LLM响应：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The input request contains two key attributes:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输入请求包含两个关键属性：
- en: '**Prompt**: Input to the LLM'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**：输入给LLM的内容'
- en: '**System message**: To set the context and guide the behavior of the LLM during
    the request'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统消息**：在请求过程中设置上下文并引导LLM的行为'
- en: Optionally, we can pass additional attributes such as `max_tokens` to limit
    the length of the generated output.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，我们可以传递额外的属性，如`max_tokens`，以限制生成输出的长度。
- en: If you notice the size of the image we built, it will be around 7+ GB. That
    is mainly attributed to the model file and other dependencies. We will explore
    techniques for reducing the size of the image and optimizing the container startup
    time later, in [*Chapter 9*](B31108_09.xhtml#_idTextAnchor113).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你注意到我们构建的镜像大小，它大约是7+ GB。这主要归因于模型文件和其他依赖项。我们将在后续的[*第9章*](B31108_09.xhtml#_idTextAnchor113)中探索减少镜像大小和优化容器启动时间的技术。
- en: It was easy to create and run a single container instance on our local computer.
    Just imagine operating hundreds or thousands of these containers running across
    hundreds or thousands of VMs, and making sure these containers are highly available,
    scalable, load balanced, managing resource allocations, and implementing automated
    deployments and rollbacks. This is where **Container Orchestrators** come to the
    rescue.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们本地计算机上创建并运行一个单一容器实例是很容易的。试想一下，在数百或数千个虚拟机上运行这些容器，并确保这些容器高度可用、可扩展、负载均衡，管理资源分配，并实现自动化部署和回滚。这就是**容器编排器**派上用场的地方。
- en: 'Container orchestrators play a pivotal role in modern software development
    and deployment, addressing all the preceding concerns and managing containerized
    applications at scale. Some of their key benefits are as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 容器编排器在现代软件开发和部署中发挥着至关重要的作用，解决了所有前述问题，并大规模管理容器化应用程序。它们的一些关键优势如下：
- en: '**High availability and fault tolerance**: They continuously monitor the health
    of the containers at regular intervals and automatically restart/replace failed
    containers, thus ensuring the desired number of containers are always up and running.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高可用性和容错性**：它们会定期监控容器的健康状态，自动重启/替换失败的容器，从而确保所需数量的容器始终处于运行状态。'
- en: '**Scaling**: They enable automatic scaling of the applications to match the
    user load/demand. Container instances are automatically created and removed in
    response to the application load. As applications are scaled, underlying compute
    resources are also scaled to accommodate the new containers.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展性**：它们可以根据用户负载/需求自动扩展应用程序。容器实例会根据应用程序负载自动创建和删除。当应用程序进行扩展时，底层计算资源也会进行扩展，以容纳新的容器。'
- en: '**Automated deployments**: They automate the deployment of containerized applications
    across multiple hosts and rollbacks in case of any failures. We can also implement
    advanced traffic routing patterns such as canary releases and blue/green deployments
    to safely roll out new changes.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化部署**：它们自动化容器化应用程序在多个主机上的部署，并在发生故障时进行回滚。我们还可以实现高级流量路由模式，如金丝雀发布和蓝绿部署，以安全地推出新的更改。'
- en: '**Load balancing**: They provide built-in load balancing to distribute incoming
    traffic across multiple container instances, improving performance and reliability.
    They can also integrate with external load-balancing solutions to load-balance
    the traffic.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载均衡**：它们提供内置的负载均衡，将传入的流量分配到多个容器实例，提升性能和可靠性。它们还可以与外部负载均衡解决方案集成，以进行流量的负载均衡。'
- en: '**Service discovery**: As containers are ephemeral in nature, orchestrators
    provide service discovery features to dynamically discover the container endpoints
    to facilitate inter-service communication. They also manage the container networking,
    including IP address management, DNS resolution, and network segmentation.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务发现**：由于容器本质上是短暂的，编排工具提供服务发现功能，以动态发现容器端点，促进服务间通信。它们还管理容器网络，包括 IP 地址管理、DNS
    解析和网络分段。'
- en: '**Observability**: They can integrate with monitoring and logging tools, providing
    visibility into the health and performance of containerized applications.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可观察性**：它们可以与监控和日志工具集成，提供对容器化应用程序健康状况和性能的可视化。'
- en: '**Resource management and advanced scheduling**: They can also manage the resource
    allocation for the containers, including CPU, memory, GPUs, and so on. They enforce
    the resource limits and reservations, preventing noisy neighbor situations. Advanced
    scheduling policies can be used to schedule the applications with special hardware
    needs such as GPUs to the specific hosts.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源管理和高级调度**：它们还可以管理容器的资源分配，包括 CPU、内存、GPU 等。它们强制执行资源限制和预留，防止“吵闹邻居”问题。可以使用高级调度策略，将具有特殊硬件需求（如
    GPU）的应用程序调度到特定主机上。'
- en: 'Now that we understand the need for a container orchestrator, the next question
    is which orchestrator should we pick? There are a number of different open source
    and proprietary orchestrators on the market. Some notable ones are the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了容器编排器的必要性，下一个问题是我们应该选择哪个编排器？市场上有许多不同的开源和专有编排器。一些值得注意的编排器如下：
- en: '**Amazon Elastic Container** **Service** ([https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/))'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Elastic Container** **Service** ([https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/))'
- en: '**Azure Container** **Apps** ([https://azure.microsoft.com/en-us/products/container-apps](https://azure.microsoft.com/en-us/products/container-apps))'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Container** **Apps** ([https://azure.microsoft.com/en-us/products/container-apps](https://azure.microsoft.com/en-us/products/container-apps))'
- en: '**Docker** **Swarm** ([https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/))'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker** **Swarm** ([https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/))'
- en: '**Apache** **Mesos** ([https://mesos.apache.org/](https://mesos.apache.org/))'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache** **Mesos** ([https://mesos.apache.org/](https://mesos.apache.org/))'
- en: Kubernetes ([https://kubernetes.io/](https://kubernetes.io/)), and managed Kubernetes
    offerings such as **Amazon Elastic Kubernetes** **Service** ([https://aws.amazon.com/eks/](https://aws.amazon.com/eks/))
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes ([https://kubernetes.io/](https://kubernetes.io/))，以及像**Amazon Elastic
    Kubernetes** **Service** ([https://aws.amazon.com/eks/](https://aws.amazon.com/eks/))这样的托管
    Kubernetes 服务
- en: '**Google Kubernetes Engine (****GKE)** ([https://cloud.google.com/kubernetes-engine](https://cloud.google.com/kubernetes-engine))'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Kubernetes Engine (GKE)** ([https://cloud.google.com/kubernetes-engine](https://cloud.google.com/kubernetes-engine))'
- en: '**Azure Kubernetes Service (****AKS)** ([https://azure.microsoft.com/en-us/products/kubernetes-service](https://azure.microsoft.com/en-us/products/kubernetes-service))'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Kubernetes Service (****AKS)** ([https://azure.microsoft.com/en-us/products/kubernetes-service](https://azure.microsoft.com/en-us/products/kubernetes-service))'
- en: '**Red Hat** **OpenShift** ([https://www.redhat.com/en/technologies/cloud-computing/openshift](https://www.redhat.com/en/technologies/cloud-computing/openshift))'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Red Hat** **OpenShift** ([https://www.redhat.com/en/technologies/cloud-computing/openshift](https://www.redhat.com/en/technologies/cloud-computing/openshift))'
- en: In this section, we learned about the typical challenges of building GenAI models
    and how we can use container technology to package GenAI code/models, AI/ML frameworks,
    hardware-specific libraries, and other dependencies into an image for consistency,
    reusability, and portability. We also built our first GenAI container image using
    the open source Llama 2 model and ran inference by deploying it locally. Finally,
    we discussed the challenges of managing containerized applications at scale and
    how container orchestrators such as K8s can solve those. In the next section,
    let’s dive into K8s and its architecture, and why it is a great fit to run GenAI
    models.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了构建GenAI模型的典型挑战，以及如何利用容器技术将GenAI代码/模型、AI/ML框架、硬件特定库和其他依赖项打包成镜像，以确保一致性、可重用性和可移植性。我们还使用开源Llama
    2模型构建了我们的第一个GenAI容器镜像，并通过在本地部署运行推理。最后，我们讨论了大规模管理容器化应用程序的挑战，以及K8s等容器编排工具如何解决这些问题。在下一节中，让我们深入了解K8s及其架构，看看它为什么非常适合运行GenAI模型。
- en: What is Kubernetes (K8s)?
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Kubernetes（K8s）？
- en: Kubernetes, commonly referred to as K8s, is an open source container orchestration
    platform that automates the deployment, scaling, and management of containerized
    applications. It is the most widely used container orchestration platform ([https://www.cncf.io/reports/kubernetes-project-journey-report/](https://www.cncf.io/reports/kubernetes-project-journey-report/))
    and has become the de facto choice for many enterprises to run a wide variety
    of workloads.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes，通常称为K8s，是一个开源的容器编排平台，能够自动化容器化应用程序的部署、扩展和管理。它是最广泛使用的容器编排平台 ([https://www.cncf.io/reports/kubernetes-project-journey-report/](https://www.cncf.io/reports/kubernetes-project-journey-report/))，并且已成为许多企业运行各种工作负载的事实标准。
- en: Originally developed by Google engineers Joe Beda, Brendan Burns, and Craig
    McLuckie in 2014 and now maintained by the **Cloud Native Computing Foundation**
    (**CNCF**), its name came from the Ancient Greek for a pilot or helmsman (the
    person at the helm who steers the ship). It has become the second largest open
    source project in the world, after Linux, and is the primary orchestrator tool
    for 71% of Fortune 100 companies ([https://www.cncf.io/reports/kubernetes-project-journey-report/](https://www.cncf.io/reports/kubernetes-project-journey-report/)).
    According to Gartner’s *The CTO’s Guide to Containers and Kubernetes* ([https://www.gartner.com/en/documents/5128231](https://www.gartner.com/en/documents/5128231)),
    by 2027, more than 90% of global organizations will be running containerized applications
    in production.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes最初由Google工程师Joe Beda、Brendan Burns和Craig McLuckie于2014年开发，现在由**云原生计算基金会**（**CNCF**）维护，其名称来源于古希腊语，意为“领航员”或“舵手”（负责掌舵的人）。它已经成为全球第二大开源项目，仅次于Linux，并且是71%财富100强公司使用的主要编排工具
    ([https://www.cncf.io/reports/kubernetes-project-journey-report/](https://www.cncf.io/reports/kubernetes-project-journey-report/))。根据Gartner的*《CTO的容器与Kubernetes指南》*
    ([https://www.gartner.com/en/documents/5128231](https://www.gartner.com/en/documents/5128231))，到2027年，全球90%以上的组织将在生产环境中运行容器化应用程序。
- en: 'Some notable factors in K8s becoming so popular are as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes（K8s）变得如此受欢迎的几个显著因素如下：
- en: '**Rich community and ecosystem**: With 77K+ contributors from 44 different
    countries and contributed to by 8K+ companies, K8s has the most vibrant and active
    community ([https://www.cncf.io/reports/kubernetes-project-journey-report/](https://www.cncf.io/reports/kubernetes-project-journey-report/)).
    The CNCF survey indicates that Kubernetes-related projects (such as Helm, Prometheus,
    and Istio) are also widely adopted, further strengthening its ecosystem.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**丰富的社区和生态系统**：K8s拥有来自44个不同国家的77K+贡献者，并由8K+公司贡献，是最具活力和活跃的社区 ([https://www.cncf.io/reports/kubernetes-project-journey-report/](https://www.cncf.io/reports/kubernetes-project-journey-report/))。CNCF的调查表明，与Kubernetes相关的项目（如Helm、Prometheus和Istio）也得到了广泛采用，进一步加强了其生态系统。'
- en: '**Comprehensive features**: K8s offers a rich set of features including automated
    rollouts and rollbacks, self-healing, horizontal scaling, service discovery, and
    load balancing. These capabilities make it a versatile and powerful tool for managing
    containerized applications.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Portability**: K8s abstracts away the underlying infrastructure, enabling
    applications to be deployed consistently whether on-premises, in public, private,
    or hybrid clouds, or edge locations.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Managed K8s services**: The availability of the managed K8s offerings from
    major cloud providers such as Amazon EKS, GKE, and AKS has significantly lowered
    the barrier to entry. These offerings take away the operational complexities of
    running and operating K8s clusters, allowing enterprises to focus on their core
    business objectives.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strong governance**: K8s has been governed by CNCF since 2016, which fosters
    collaborative development and community participation, enabling contributions
    from a diverse group of developers, organizations, and end users. It follows an
    open source model with well-defined governance structures, including **special
    interest groups** (**SIGs**) that focus on specific areas of development and operations.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Declarative configuration**: K8s uses a declarative approach for configuration
    management, allowing users to define the desired state of their applications and
    infrastructure using YAML or JSON files. As depicted in *Figure 2**.5*, K8s controllers
    continuously monitor the current state of a resource and automatically reconcile
    to match the desired state, simplifying operations and ensuring consistency.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.5 – How K8s controllers work](img/B31108_02_05.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – How K8s controllers work
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '**Extensibility**: This is probably the most significant reason why K8 became
    so popular. K8s is designed with a modular architecture, supporting custom plugins
    and extensions through well-defined APIs. This enabled developers and companies
    to extend or customize the K8s functionality without modifying the upstream code,
    fostering innovation and adaptability.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s look at the architecture of Kubernetes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes architecture
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'K8s architecture is based on running clusters that allow your applications/containers
    to run across multiple hosts. Each cluster consists of two types of nodes:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '**Control plane**: The K8s control plane is the brain behind cluster operations.
    It consists of critical components such as kube-apiserver, etcd, the scheduler
    manager, the controller manager, and the cloud controller manager.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data plane/worker nodes**: The K8s data plane running on the worker nodes
    is composed of several key components, such as kube-proxy, the kubelet, and the
    **Container Network Interface** (**CNI**) plugin.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 2**.6* depicts the high-level K8s cluster architecture. You will notice
    kube-apiserver is the frontend of the K8s control plane and interacts with the
    other control plane components, such as controller managers, etcd, and so on,
    to fulfil the requests. K8s worker nodes host several key components, responsible
    for taking instructions from kube-apiserver, executing them on the worker node,
    and reporting back on the status.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Kubernetes cluster architecture](img/B31108_02_06.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – Kubernetes cluster architecture
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Control plane components
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The primary components of a control plane are as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '**kube-apiserver**: This serves as the entry point or frontend into the K8s
    cluster and the central management component that exposes the K8s API. K8s users,
    administrators, and other components use this API to communicate with the cluster.
    It also communicates with the etcd component to save the state of K8s objects.
    It also handles authentication and authorization, validation, and request processing,
    and communicates with other control plane and data plane components to manage
    the clusters’ states.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**etcd**: This is the distributed key-value store and is used as the K8s backing
    store for all cluster state. It stores the configuration data of all K8s objects
    and any updates made to them and ensures the cluster state is always reliable
    and accessible. It’s critical to take regular backups of the etcd database so
    that clusters can be restored in case of any disruptions.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-controller-manager**: This is responsible for managing various controllers
    in the cluster. This includes the default upstream controllers and any custom-built
    ones. Some examples include the following:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment controller**: This watches for K8s deployment objects and manages
    the updates to K8s Pods.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-scheduler**: This is responsible for scheduling the K8s Pods on the
    worker nodes. It monitors the API server for newly created Pods and assigns a
    worker node based on the resource availability, scheduling requirements defined
    in the Pod configuration such as nodeSelectors, Pod/node affinity, topology spreads,
    and so on. When unable to schedule a Pod due to resource exhaustion and so on,
    it will mark the Pods as *Pending* so that other operational add-ons such as the
    cluster autoscaler can kick in and add/remove compute capacity (worker nodes)
    to the cluster.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cloud-controller-manager**: This manages the cloud-specific controllers to
    handle the cloud provider API calls for resource management. It’s the gateway
    to the Cloud Provider API from the K8s core and is responsible for creating and
    managing cloud-provider-specific resources (such as nodes, LoadBalancers, and
    so on) based on changes to K8s objects (such as nodes, Services, and so on). Some
    examples include the following:'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node controller**: This is responsible for monitoring the health of the worker
    nodes and handling the addition or removal of nodes in the cluster'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service controller**: This watches for service and node object changes, and
    creates, updates, and deletes cloud provider load balancers accordingly'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data plane components
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The primary components of a data plane are as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '**kubelet**: This is an agent that runs on every worker node in the cluster.
    It’s responsible for taking instructions from kube-apiserver, executing them on
    the respective worker node, and reporting the updates on the node components back
    to the cluster control plane. It interacts with other node components such as
    the container runtime to launch container processes, the CNI plugin to set up
    the container networking, and CSI plugins to manage the persistent volumes and
    so on.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-proxy**: This is a network proxy that runs on each worker node in the
    cluster, implementing the K8s service concept. It maintains the network routing
    rules on the worker node, to allow the network communications to and from your
    Pods from within/outside the cluster. It uses the operating system packet filtering
    layer, such as IP tables, IPVS, and so on, to route the traffic to other endpoints
    in the cluster.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container runtime**: containerd is the de facto container runtime responsible
    for launching the containers on the worker node. It is responsible for managing
    the lifecycle of containers in the K8s environment. K8s also supports other container
    runtimes such as CRI-O.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from these components, it’s essential to deploy additional add-on software
    on the K8s cluster for production operations. These add-ons add capabilities such
    as monitoring, security, and networking.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Add-on software components
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are a few examples of add-on software:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '**CNI plugin**: This is a software add-on that implements container network
    specifications. They adhere to the K8s networking tenets and are responsible for
    allocating IP addresses to K8s Pods ([https://kubernetes.io/docs/concepts/workloads/pods/](https://kubernetes.io/docs/concepts/workloads/pods/))
    and enabling them to communicate with each other within the cluster. Popular add-ons
    in this space are Cilium ([https://github.com/cilium/cilium](https://github.com/cilium/cilium)),
    Calico ([https://github.com/projectcalico/calico](https://github.com/projectcalico/calico)),
    and Amazon VPC CNI ([https://github.com/aws/amazon-vpc-cni-k8s](https://github.com/aws/amazon-vpc-cni-k8s)).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CSI plugin**: This is a software add-on that implements container storage
    interface specifications. They are responsible for providing persistent storage
    volumes to K8s Pods and managing the lifecycle of those volumes. A couple of notable
    add-ons are Amazon EBS CSI driver ([https://github.com/kubernetes-sigs/aws-ebs-csi-driver](https://github.com/kubernetes-sigs/aws-ebs-csi-driver))
    and Portworx CSI Driver ([https://docs.portworx.com/portworx-enterprise/operations/operate-kubernetes/storage-operations/csi](https://docs.portworx.com/portworx-enterprise/operations/operate-kubernetes/storage-operations/csi)).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CoreDNS**: This is an essential software add-on that provides DNS resolution
    within the cluster. Containers launched in K8s worker nodes automatically include
    this DNS server in their DNS searches.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring plugins**: This is a software add-on that provides observability
    into the cluster infrastructure and workloads. They extract essential observability
    details such as logs, metrics, and traces and write to monitoring platforms such
    as Prometheus, Amazon CloudWatch, Splunk, Datadog, and New Relic.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Device plugins**: Modern AI/ML apps use specialized hardware devices such
    as GPUs from NVIDIA, Intel, and AMD and custom accelerators from Amazon, Google,
    and Meta. K8s provides a device plugin framework that you can use to advertise
    system hardware resources to the kubelet and control plane so that you can make
    scheduling decisions based on their availability.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is not an exhaustive list of all K8s components and add-ons. We will dive
    into AI/ML-related add-ons in a later part of the book.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we dove into K8s architecture, learned about various control
    plane and data plane components, and explored the advantages of the K8s platform
    and why it became the de facto standard in the community. Let’s understand why
    K8s is a great fit for running GenAI models next.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Why K8s is a great fit for GenAI models
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we understand the K8s architecture, its components, and the advantages
    of the platform, let’s discuss how we apply those to solve common challenges with
    operating GenAI models.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of running GenAI models
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some common challenges of running GenAI models are as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '**Computational requirements**: GenAI models are increasingly becoming large
    and complex, thus requiring substantial computational resources, including GPUs,
    TPUs, and custom accelerators for training and inference. Managing these resources
    efficiently is crucial to ensure performance and cost-efficiency.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: As the demand for AI/ML services increases, scaling GenAI
    models to handle the demand is essential. This requires seamless scaling of computational
    resources without sacrificing the performance and cost of the models.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observability**: As GenAI models proliferate, it’s critical to understand
    their performance by monitoring both business-level KPIs and the health of the
    overall system using logs and metrics.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data management**: GenAI models rely on vast amounts of data for both training
    and inference. Data preparation, security, and management are critical in increasing
    the model’s accuracy and performance.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment complexity**: As we learned earlier in this chapter, all GenAI
    models require custom frameworks, plugin libraries, and other dependencies to
    deploy them. This complexity can lead to deployment issues, delays, and increased
    errors.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K8s advantages
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'K8s offers several advantages for addressing the challenges of running GenAI
    models, such as the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '**Efficient resource management**: K8s has a robust resource management system
    built into kube-scheduler. It automates the distribution of K8s Pods into the
    worker nodes while meeting different scheduling requirements/constraints. Schedulers
    can be configured to operate in lowest-cost, random, or bin-pack modes for flexibility.
    With the K8s extensibility, you can develop custom schedulers and use them for
    scheduling workloads. A common application of this is to schedule training or
    inference workloads on devices with custom devices such as AWS Trainium and Inferentia.
    By using K8s, we can implement dynamic resource allocation based on model requirements,
    thus optimizing costs and improving performance.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seamless scalability**: Training or fine-tuning a GenAI model requires a
    significant number of computational resources. Inference endpoints of these models
    also need to scale horizontally based on the workload demand. This will be achieved
    seamlessly using K8s autoscaling mechanisms such as **Horizontal Pod Autoscaling**
    (**HPA**) ([https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)),
    **Vertical Pod Autoscaling** (**VPA**) ([https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)),
    and **Cluster Autoscaling** ([https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/](https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/)).
    HPA automatically scales a workload resource (such as a Deployment or StatefulSet)
    to match the workload demand. It does this by creating and deploying new K8s Pods
    in response to the demand. VPA automatically adjusts the resource limits (such
    as CPU, memory, and so on) of Pods to match their actual usage. It helps in optimizing
    resource allocation, ensuring workloads are run efficiently. Cluster autoscaling
    is responsible for ensuring the right number of resources are attached to the
    cluster at all times. We will take a deeper look at these mechanisms in [*Chapter
    6*](B31108_06.xhtml#_idTextAnchor075).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extensibility**: Extensibility plays a crucial role in running GenAI workloads
    on K8s. It allows you to extend the functionality of K8s in a scalable manner
    without modifying the upstream code. We can use custom-built add-ons such as **Kubeflow**
    ([https://www.kubeflow.org/](https://www.kubeflow.org/)), an AI/ML platform that
    provides custom resources for managing ML pipelines, model training, and deployment.
    Hardware companies can also leverage this by developing device plugins to manage
    GPU resources in K8s so that GenAI training and inference workloads are scheduled
    accordingly. GenAI workloads often require specific frameworks such as **PyTorch**,
    **TensorFlow**, and **Jupyter Notebook**. We can use custom-built operators to
    integrate these frameworks and tools for seamless development and deployment.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: K8s has many in-built security mechanisms to secure GenAI workloads.
    One can use **Role-Based Access Control** (**RBAC**) ([https://kubernetes.io/docs/reference/access-authn-authz/rbac/](https://kubernetes.io/docs/reference/access-authn-authz/rbac/))
    to limit access to resources, ensuring that only authorized users or applications
    can access sensitive data. K8s Secrets or external secret management solutions
    can be used to safeguard sensitive information. K8s network policies can be used
    to implement network segmentation so that only authorized Pods can access data
    stores. On top of this, we can enable security controls such as encryption, audit
    logging, security scanning, and **Pod Security Standards** (**PSS**) ([https://kubernetes.io/docs/concepts/security/pod-security-standards/](https://kubernetes.io/docs/concepts/security/pod-security-standards/))
    to ensure a robust security posture. We will explore all these features in detail
    in [*Chapter 9*](B31108_09.xhtml#_idTextAnchor113).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High Availability (HA) and fault tolerance**: These play a critical role
    in running GenAI workloads efficiently and reliably. Foundational model training
    often takes weeks or months. If a node or Pod fails, K8s’s in-built self-healing
    mechanism can automatically schedule the job to another node, thus minimizing
    interruptions. AI frameworks can be used along with this to implement a checkpointing
    strategy, to save the training state periodically. For model inferencing, K8s
    can automatically scale inference Pods based on the demand and recover the failed
    ones by launching replacement Pods. K8s can also perform rolling blue/green updates
    to deploy new model versions and seamlessly roll back in case of failures. We
    will take a deeper look at this topic in [*Chapter 13*](B31108_13.xhtml#_idTextAnchor176).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rich ecosystem and add-ons**: The Cloud Native Artificial Intelligence Whitepaper
    ([https://www.cncf.io/wp-content/uploads/2024/03/cloud_native_ai24_031424a-2.pdf](https://www.cncf.io/wp-content/uploads/2024/03/cloud_native_ai24_031424a-2.pdf))
    underscores the growing adoption of Kubernetes-native tools and frameworks to
    streamline the development, training, and deployment of AI models. Notable examples
    include Kubeflow and MLflow for operating end-to-end ML platforms on K8s; KServe,
    Seldon, and RayServe for model serving and scaling; and OpenLLMetry, TruLens,
    and Deepchecks for model observability. This list will continue to grow as the
    industry matures around GenAI use cases.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we learned about the typical challenges of operating GenAI
    models and looked at the advantages of using K8s to address them. K8s extensibility,
    efficient resource management, security, HA, and fault tolerance capabilities
    make it a great fit to run GenAI models at scale.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started with the evolution of compute technologies and how
    containers emerged as a standard to package and ship applications and abstract
    away infrastructure complexities for developers. We discussed the benefits of
    using containers for GenAI models and built and ran our first hello-world, GenAI
    container images. Then we looked at the challenges of running and managing containers
    at scale and how container orchestrator engines such as K8s can help simplify
    that.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: We dove into the high-level K8s architecture and various components that made
    up the control plane and data plane. We also learned how the extensibility, portability,
    declarative nature, and rich community behind K8s made it popular and the de facto
    container orchestrator in the market.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discussed the typical challenges of operating GenAI workloads at
    scale and how K8s is a great fit to address those challenges with its efficient
    resource management, seamless scaling, extensibility, and security capabilities.
    In the next chapter, we will explore how to build a K8s cluster in a cloud environment,
    leverage popular open source tooling to manage GenAI workloads, and deploy our
    *my-llama* container in it.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Appendix
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Following are some great resources for in-depth training on Kubernetes:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes training website: [https://kubernetes.io/training/](https://kubernetes.io/training/)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes course on the Linux Foundation: [https://training.linuxfoundation.org/full-catalog/?_sft_product_type=training&_sft_technology=kubernetes](https://training.linuxfoundation.org/full-catalog/?_sft_product_type=training&_sft_technology=kubernetes)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes Learning Path at KodeKloud: [https://kodekloud.com/learning-path/kubernetes/](https://kodekloud.com/learning-path/kubernetes/)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
