- en: '*Chapter 1*: Introduction to Rancher and Kubernetes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will focus on the history of Rancher and Kubernetes. We will cover
    what products and solutions came before Rancher and Kubernetes and how they have
    evolved into what they are today. At the end of this chapter, you should have
    a good understanding of the origins of Rancher and Kubernetes and their core concepts.
    This knowledge is essential for you to understand why Rancher and Kubernetes are
    what they are.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The history of Rancher Labs as a company
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rancher's earlier products
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Rancher's core philosophy?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where did Kubernetes come from?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What problem is Kubernetes trying to solve?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing Kubernetes with Docker Swarm and OpenShift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The history of Rancher Labs as a company
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rancher Labs was founded in 2014 in Cupertino, California, by Sheng Liang, Shannon
    Williams, Darren Shepherd, and Will Chanas. It was a container management platform
    before Kubernetes was a thing. From the beginning, Rancher was built on the idea
    that everything should be open source and community-driven. With Rancher being
    an open source company, all of the products they have released (including Rancher,
    RancherOS, RKE, K3s, Longhorn, and more) have been 100% open source. Rancher Lab's
    flagship product is Rancher. Primarily, Rancher is a management and orchestration
    platform for containerized workloads both on-premises and in the cloud. Rancher
    can do this because it has always been vendor-neutral; that is, Rancher can deploy
    a workload using physical hardware in your data center to cloud VMs in AWS to
    even a Raspberry Pi in a remote location.
  prefs: []
  type: TYPE_NORMAL
- en: Rancher's earlier products
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When Rancher v1.0 was released in March of 2016, it only supported Docker Swarm
    and Rancher Cattle clusters. Docker Swarm was the early cluster orchestration
    tool that created a number of the core concepts that we still use today; for instance,
    the idea that an application should be defined as a group of containers that can
    be created and destroyed at any time. Another concept is that containers should
    live on a virtual network that is accessible on all nodes in a cluster. You can
    expose your containers via a load balancer which, in the case of Docker Swarm,
    is just a basic TCP load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: While the Rancher server was being created, Rancher Labs was working on their
    own Docker cluster software, called Cattle, which is when Rancher went **General
    Availability (GA**) with the launch of Rancher v1.0\. Cattle was designed to address
    the limitations of Docker Swarm, which spanned several different areas.
  prefs: []
  type: TYPE_NORMAL
- en: The first was networking. Originally, Docker Swarm's networking overlay was
    built on `node01` with an IP address of `192.168.11.22` wants to connect to another
    container hosted on `node02` with an IP address of `192.168.12.33`. The networking
    swarm uses basic Linux routing to route anything inside the `192.168.12.0/24`
    subnet to `node02` over the IPsec tunnel. This core concept is still in use today
    by the majority of Kubernetes's CNI providers. The main issue is in managing the
    health of these tunnels over time and dealing with compatibility issues between
    the nodes. Cattle addressed this issue by moving IPsec into a container and then
    wrapping a management layer to handle the creation, deletion, and monitoring of
    the tunnels.
  prefs: []
  type: TYPE_NORMAL
- en: The second main issue was to do with load balancing. With Docker Swarm, we were
    limited to very basic TCP/layer4 load balancing. We didn't have sessions, SSL,
    or connection management. This is because load balancing was all done by iptable
    rules. Cattle addressed this issue by deploying HAProxy on all nodes in the cluster.
    Following this, Cattle used a custom container, called `rancher-metadata`, to
    dynamically build HAProxy's config every time a container was created or deleted.
  prefs: []
  type: TYPE_NORMAL
- en: The third issue was storage. With Docker Swarm, there weren't any storage options
    outside bind mounting to a host filesystem. This meant that you had to create
    a clustered filesystem or shared network and then manually map them to all of
    your Docker hosts. Cattle addressed this by creating `rancher-nfs`, which is a
    tool that can mount NFS shares inside a container and create a bind mount. As
    Rancher went on, other storage providers were added, such as AWS and VMware.
  prefs: []
  type: TYPE_NORMAL
- en: Then, as time moved forward at Rancher, the next giant leap was when authentication
    providers were added, because Rancher provides access to the clusters that Rancher
    manages by integrating external authentication providers such as Active Directory,
    LDAP, and GitHub. This is unique to Rancher, as Kubernetes still doesn't integrate
    very well with external authentication providers.
  prefs: []
  type: TYPE_NORMAL
- en: What is Rancher's core philosophy?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rancher is built around several core design principles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Open source**: All code, components, and services that make up Rancher or
    come with Rancher must be open source. Because of this, Rancher has a large community
    built around it, with users providing feedback, documentation, and contributing
    code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No lock-ins**: Rancher is designed with no vendor lock-in, including being
    locked inside Rancher. With containerization evolving so quickly, Rancher needs
    to enable users to change technologies with as little impact as possible. A core
    requirement of all products and solutions that Rancher provides is that they can
    be used with or without the Rancher server. An example of this is Longhorn; there
    are zero dependencies between Rancher and Longhorn. This means that at any time,
    a user can uninstall one without impacting the other. This includes the ability
    to uninstall Rancher without losing your clusters. Rancher does this by having
    a process in place for a user to take over the management of a cluster directly
    and kick Rancher out of the picture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clusters.management.cattle.io` and the same with nodes as an object under
    `nodes.management.cattle.io`, which is scoped to a namespace with the cluster
    ID. Because of this, users and applications can directly query Rancher objects
    without needing to talk to Rancher''s API. The reason for this change was mainly
    to do with scalability. With Cattle and MySQL, all cluster-related tasks had to
    go back to the Rancher server. So, as you scaled up the size of your cluster and
    the number of clusters, you had to scale up the Rancher server, too. This resulted
    in customers hitting issues such as "task storms" where a single node rebooting
    in a cluster causes a flood of requests that are sent to the Rancher server, which,
    in turn, causes other tasks to timeout, which then causes more requests. In the
    end, the only thing you can do is to shut everything down and slowly bring it
    back up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Everything is stateless**: Because everything is a Kubernetes object, there
    is no need for a database for Rancher. All Rancher pods are stateless, meaning
    they can be destroyed at any time for any reason. Additionally, Rancher can rely
    on Kubernetes controllers to simply spin up new pods without needing Rancher to
    do anything.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller model**: All Rancher services are designed around the Kubernetes
    controller model. A control loop is always running, watching the current state,
    and comparing it to the desired state. And if any differences are found, it applies
    the application logic to make the current state match the desired state. Alongside
    this, it uses the same leader election process with Kubernetes core components.
    This ensures there is only one source of truth and ensures certain controllers
    will handle failing over after a failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where did Kubernetes come from?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The name **Kubernetes** originates from Greek, meaning **helmsman** or **pilot**.
    Kubernetes is abbreviated to **k8s** due to the number of letters between the
    **K** and **S**. Initially, engineers created Kubernetes at Google from an internal
    project called Borg. Google's Borg system is a cluster manager that was designed
    to run Google's internal applications. These applications are made up of tens
    of thousands of microservices hosted on clusters worldwide, with each cluster
    being made up of tens of thousands of machines. Borg provided three main benefits.
    The first benefit was the abstraction of resource and failure management, so application
    designers could focus on application development. The second benefit was its high
    reliability and availability by design. All parts of Borg were designed, from
    the beginning, to be highly available. This was done by making applications stateless.
    This was done so that any component could be destroyed at any time for any reason
    without impacting availability and, at the same time, could be scaled horizontally
    to hundreds of instances across clusters. The third benefit was an effective workload;
    Borg was designed to have a minimal overhead on the compute resources being managed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes can be traced directly back to Borg, as many of the developers at
    Google that worked on Kubernetes were formerly developers on the Borg project.
    Because of this, many of its core concepts were incorporated into Kubernetes,
    with the only real difference being that Borg was custom-made for Google, and
    its requirements for Kubernetes need to be more generalized and flexible. However,
    there are four main features that have been derived from Borg:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pods**: A pod is the smallest unit of scheduling in Kubernetes. This object
    can include one or more containers, with each container in the pods sharing resources
    such as an IP address, volumes, and other local resources. One of the main design
    principles is that a pod should be disposable and shouldn''t change after creation.
    Another primary principle is that all application configurations should be handled
    at the pod level. For example, a database connection string should be defined
    as part of the pod''s definition instead of the application code. This is done
    so that any changes to the configuration of an application won''t require the
    code to be recompiled and redeployed. Additionally, the pod takes the concept
    of paired processes from Borg, with the classic example being a log collector.
    This is because, typically, your container should only have one primary process
    running inside it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example of this is a web server: the server creates logs, but how do you
    ship those logs to a log server like Splunk? One option is to add a custom agent
    to your application pod which is easy. But now that you manage more than one process
    inside a container, you''ll have duplicate code in your environment, and most
    importantly, you now have to do error handling for both your main application
    and this additional logging agent. This is where sidecars come into play and allow
    you to bolt containers together inside a pod in a repeatable and consistent manner.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Services**: One of Borg''s primary roles was the life cycle management of
    applications and pods. Because of this, the pod name and IP address are ephemeral
    and can change at any time for any reason. So, the concept of a service was created
    as an abstraction level wherein you can define a service object that references
    a pod or pods by using labels. Kubernetes will then handle the mapping of the
    service records to its pods. These benefits load balance the traffic for a service
    among the pods that make up that service. Service records allow Kubernetes to
    add and remove pods without disrupting the applications because the service to
    pod mapping can simply be changed without the requesting client being aware.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"application=web_frontend," "environment=production," "department=marketing"`.
    Note that each of these keys is a different label selector rule that can create
    a service record. This has the side benefit of making the reporting and tracking
    of usage much easier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Every pod has an IP**: When Borg was created, all of the containers on a
    host would share the host''s IP address and then use different ports for each
    container. This allowed Borg to use a standard IP network. However, this created
    a burden on infrastructure and application teams, as Borg needed to schedule ports
    for containers. This required applications to have a set of predefined ports that
    would be needed for their container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What problem is Kubernetes trying to solve?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes was designed to solve several problems. The primary areas are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Availability**: Everyone, from the application owner to the developers, to
    the end users, has come to expect 24x7x365 uptime, with work outages and downtime
    being a four-letter word in IT. With containerization and microservices, this
    bar has only gotten higher. Kubernetes addresses this issue by scheduling containers
    across nodes and using the desired state versus the actual state. The idea is
    that any failures are just a change in the actual state that triggers the controllers
    to schedule pods until the actual state matches the desired state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CI/CD**: Traditional development was carried out using monolithic developments,
    with a few significant releases per year. This required a ton of developers working
    for months to test their releases and build a ton of manual processes to deploy
    their applications. Kubernetes addresses this issue by being driven by the desired
    state and config file. This means implementing a DevOps workflow that allows developers
    to automate steps and continuously integrate, test, and deploy code. All of this
    will enable teams to fail fast and fix fast.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency**: Traditional IT was a black hole that companies threw money
    into. One of the reasons behind this was high availability. For one application,
    you would need at least two servers for each component of your production application.
    Also, you would require additional servers for each of your lower environments
    (such as DEV, QAS, Test, and more). Today, companies want to be as efficient with
    their IT spending as possible. Kubernetes addresses this need by making spinning
    up environments very easy. With CI/CD, you can simply create a new namespace,
    deploy your application, run whatever tests you want, and then tear down the namespace
    to reclaim its resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automate scaling**: Traditionally, you would design and build your environment
    around your peak workload. For instance, let''s say your application is mainly
    busy during business hours and is idle during off-peak hours. You are wasting
    money because you pay the same amount for your compute resources at 100% and 1%.
    However, traditionally, it would take days or even weeks to spin up a new server,
    install your application, config it, and, finally, update the load balancer. This
    made it impossible to scale up and down rapidly. So, some companies just decided
    to scale up and stay there. Kubernetes addresses this issue by making it easy
    to scale up or down, as it just involves a simple change to the desired state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's say that an application currently has two web servers, and you want to
    add a pod to handle the load. Just change the number of replicas to three because
    the current state doesn't match the desired state. The controllers kick up and
    start spinning up a new pod. This can be automated using Kubernetes' built-in
    **horizontal pod autoscaler** (**HPA**), which uses several metrics ranging from
    simple metrics such as CPU and memory to custom metrics such as overall application
    response times. Additionally, Kubernetes can use its **vertical pod autoscaler**
    (**VPA**) to automatically tune your CPU and memory limits over time. Following
    this, Kubernetes can use node scaling to dynamically add and remove nodes to your
    clusters as resources are required. This means your application might have 10
    pods with 10 worker nodes during the day, but it might drop to only 1 pod with
    1 worker node after hours. This means you can save the cost of 9 nodes for 16
    hours per day plus the weekends; all of this without your application having to
    do anything.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Kubernetes with Docker Swarm and OpenShift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will compare both of these in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes versus Docker Swarm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes and Docker Swarm are open source container orchestration platforms
    that have several identical core functions but significant differences.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes is a complex system with several components that all need to work
    together to make the cluster operate, making it more challenging to set up and
    administrate. Kubernetes requires you to manage a database (`etcd`), including
    taking backups and creating SSL certificates for all of the different components.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm is far simpler, with everything just being included in Docker.
    All you need to do is create a manager and join nodes to the swarm. However, because
    everything is baked-in, you don't get the higher-level features such as autoscaling,
    node provisioning, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Networking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes uses a flat network model with all pods sharing a large network subnet
    and another network for creating services. Additionally, Kubernetes allows you
    to customize and change network providers. For example, if you don't like a particular
    canal, or can't do network-level encryption, you can switch to another provider,
    such as Weave, which can do network encryption.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm networking is fundamental. By default, Docker Swarm creates IPsec
    tunnels between all nodes in the cluster using IPsec for the encryption. This
    speed can be good because modern CPUs provide hardware acceleration for AES; however,
    you can still take a performance hit depending on your hardware and workload.
    Additionally, with Docker Swarm, you can't switch network providers as you only
    get what is provided.
  prefs: []
  type: TYPE_NORMAL
- en: Application deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes uses YAML and its API to enable users to define applications and
    their resources. Because of this, there are tools such as Helm that allow application
    owners to define their application in a templatized format, making it very easy
    for applications to be published in a user-friendly format called **Helm charts**.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm is built on the Docker CLI with a minimal API for management. The
    only package management tool is Docker Compose, which hasn't been widely adopted
    due to its limited customization and the high degree of manual work required to
    deploy it.
  prefs: []
  type: TYPE_NORMAL
- en: High availability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes has been built from the ground up to be highly available and to have
    the ability to handle a range of failures, including pods detecting unhealthy
    pods using advanced features such as running commands inside the pods to verify
    their health. This includes all of the management components such as Kube-scheduler,
    Kube-apiserver, and more. Each of these components is designed to be stateless
    with built-in leader election and failover management.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm is highly available mainly by its ability to clone services between
    nodes, with the Swarm manager nodes being in an active-standby configuration in
    the case of a failure.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes pods can be exposed using superficial layer 4 (TCP/UDP mode) load
    balancing services. Then, for external access, Kubernetes has two options. The
    first is node-port, which acts as a simple method of port-forwarding from the
    node's IP address to an internal service record. The second is for more complex
    applications, where Kubernetes can use an ingress controller to provide layer
    7 (HTTP/HTTPS mode) load balancing, routing, and SSL management.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm load balancing is DNS-based, meaning Swarm uses round-robin DNS
    to distribute incoming requests between containers. Because of this, Docker Swarm
    is limited to layer 4 only, with no option to use any of the higher-level features
    such as SSL and host-based routing.
  prefs: []
  type: TYPE_NORMAL
- en: Management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes provides several tools in which to manage the cluster and its applications,
    including `kubectl` for command-line access and even a web UI via the Kubernetes
    dashboard service. It even offers higher-level UIs such as Rancher and Lens. This
    is because Kubernetes is built around a REST API that is highly flexible. This
    means that applications and users can easily integrate their tools into Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm doesn't offer a built-in dashboard. There are some third-party
    dashboards such as **Swarmpit**, but there hasn't been very much adoption around
    these tools and very little standardization.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes provides a built-in RBAC model allowing fine-grained control for
    Kubernetes resources. For example, you can grant pod permission to just one secret
    with another pod being given access to all secrets in a namespace. This is because
    Kubernetes authorization is built on SSL certifications and tokens for authentication.
    This allows Kubernetes to simply pass the certificate and token as a file mounted
    inside a pod. This makes it straightforward for applications to gain access to
    the Kubernetes API.
  prefs: []
  type: TYPE_NORMAL
- en: The Docker Swarm security model is primarily network-based using TLS (mTLS)
    and is missing many fine-grained controls and integrations, with Docker Swarm
    only having the built-in roles of none, view only, restricted control, scheduler,
    and full control. This is because the access model for Docker Swarm was built
    for cluster administration and not application integration. In addition to this,
    originally, the Docker API only supported basic authentication.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes versus OpenShift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both Kubernetes and OpenShift share a lot of features and architectures. Both
    follow the same core design practices, but they differ in terms of how they are
    executed.
  prefs: []
  type: TYPE_NORMAL
- en: Networking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes lacks a built-in networking solution and relies on third-party plug-ins
    such as canal, flannel, and Weave to provide networking for the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift provides a built-in network solution called **Open vSwitch**. This
    is a VXLAN--based software-defined network stack that can easily be integrated
    into RedHat's other products. There is some support for third-party network plugins,
    but they are limited and much harder to support.
  prefs: []
  type: TYPE_NORMAL
- en: Application deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes takes the approach of being as flexible as possible when deploying
    applications to the cluster, allowing users to deploy any Linux distribution they
    choose, including supporting Windows-based images and nodes. This is because Kubernetes
    is vendor-agnostic.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift takes the approach of standardizing the whole stack on RedHat products
    such as RHEL for the node's operating system. Technically, there is little to
    nothing to stop OpenShift from running on other Linux distributions such as Ubuntu.
    Additionally, Openshift puts limits on the types of container images that are
    allowed to run inside the cluster. Again, technically, there isn't much preventing
    a user from deploying an Ubuntu image on the Openshift cluster, but they will
    most likely run into issues around supportably.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes had a built-in tool for pod-level security called **Pod Security
    Policies** (**PSPs**). PSPs were used to enforce limits on pods such as blocking
    a pod from running as root or binding to a host's filesystem. PSPs were deprecated
    in v1.21 due to several limitations of the tool. Now, PSPs are being replaced
    by a third-party tool called **OPA Gatekeeper**, which allows all of the same
    security rules but with a different enforcement model.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift has a much stricter security mindset, with the option to be secure
    as a default, and it doesn't require cluster hardening like Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about Rancher's history and how it got its start.
    Following this, we went over Rancher's core philosophy and how it was designed
    around Kubernetes. Then, we covered where Kubernetes got its start and its core
    philosophy. We then dived into what the core problems are that Kubernetes is trying
    to solve. Finally, we examined the pros and cons of Kubernetes, Docker Swarm,
    and OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover the high-level architecture and processes
    of Rancher and its products, including RKE, K3s, and RancherD.
  prefs: []
  type: TYPE_NORMAL
