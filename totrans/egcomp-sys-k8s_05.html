<html><head></head><body>
		<div id="_idContainer057">
			<h1 id="_idParaDest-98" class="chapter-number"><a id="_idTextAnchor097"/>5</h1>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor098"/>K3s Homelab for Edge Computing Experiments</h1>
			<p>At this point, we have explored essential topics to create your own edge computing cluster. The previous chapters covered how to configure and install a K3s cluster. Building small and big solutions at home involves experimenting. In this chapter, we are going to start building a simple but real cluster, using the knowledge acquired in the previous chapters. We will refer to this environment as the K3s homelab. Once this cluster is created, we are going to deploy a simple application. We will use this as a quickstart method of using Kubernetes with your cluster. In the last part of this chapter, we are going to use the Kubernetes dashboard as a simple UI to manage Kubernetes clusters.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Installing a multi-node K3s cluster on your local network</li>
				<li>Deploying your first application with <strong class="source-inline">kubectl</strong> </li>
				<li>Deploying a simple NGINX server using YAML files</li>
				<li>Adding persistence to your applications</li>
				<li>Deploying a Kubernetes dashboard</li>
			</ul>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor099"/>Technical requirements</h1>
			<p>For this chapter, you need the following hardware to create your K3s homelab for your edge computing applications or experiments:</p>
			<ul>
				<li>Two or more Raspberry Pi 4 B models with a minimum of 4 GB RAM and a 32 GB microSD card with Ubuntu version 20.04 or later. The SanDisk Extreme microSDHC 32 GB UHS-1 A1 V30 or similar is recommended as the microSD card.</li>
				<li>Ethernet cables to connect your Raspberries.</li>
				<li>An Ethernet internet connection for the Raspberries with <strong class="bold">Dynamic Host Configuration Protocol</strong> (<strong class="bold">DHCP</strong>) activated.</li>
				<li>One switch to connect your Raspberry to your local network.</li>
			</ul>
			<p>With this hardware, we are ready to start building our K3s homelab. So, let's get started.</p>
			<p>For more detail and code snippets, check out this resource on GitHub: <a href="https://github.com/PacktPublishing/Edge-Computing-Systems-with-Kubernetes/tree/main/ch5">https://github.com/PacktPublishing/Edge-Computing-Systems-with-Kubernetes/tree/main/ch5</a></p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor100"/>Installing a multi-node K3s cluster on your local network</h1>
			<p>To start <a id="_idIndexMarker291"/>creating this homelab, let's <a id="_idIndexMarker292"/>understand the network topology that we are going to use. Each component in the following diagram is used in the homelab:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B16945_Figure_5.1.jpg" alt="Figure 5.1 – Homelab architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Homelab architecture</p>
			<p>Here is a <a id="_idIndexMarker293"/>small explanation of each component in the figure:</p>
			<ul>
				<li><strong class="bold">Lan</strong>: This is the local network that you are going to use. In our example, the network is defined as <strong class="source-inline">192.168.0.0/24</strong>.</li>
				<li><strong class="bold">Switch</strong>: A switch is also a network connecting device that connects various devices in the same network.</li>
				<li><strong class="bold">Router</strong>: A router connects devices across multiple networks. Typically, home routers are hybrid devices that give local computers access to the internet. It also has small switch capabilities to connect local computers, using wireless or Ethernet ports for your wired devices.</li>
				<li><strong class="bold">Public Interface</strong>: This is the interface of your router that has a public IP.</li>
				<li><strong class="bold">Gateway</strong>: This is an <a id="_idIndexMarker294"/>IP address that is used as the gateway in your private network.</li>
				<li><strong class="bold">DNS</strong>: This is the DNS IP address that your router is going to assign to your devices automatically – in this case, <strong class="source-inline">8.8.8.8</strong> and <strong class="source-inline">1.1.1.1</strong>.</li>
				<li><strong class="bold">Master</strong>: This is the master node of your K3s cluster.</li>
				<li><strong class="bold">Agent</strong>: This is the agent node that acts as a worker in your K3s cluster.</li>
				<li><strong class="bold">Client</strong>: This is your local machine or laptop where you are going to access the cluster using the <strong class="source-inline">kubectl</strong> command.</li>
			</ul>
			<p>Now – a small explanation about how these pieces interact with each other. All your machines will use the <strong class="source-inline">192.168.0.0/24</strong> network; in this case, let's think that your client will use the <strong class="source-inline">192.168.0.2</strong> IP. Using the config files or parameters to install your cluster, you can choose an IP range inside the previous network for your nodes. In this case, the master is using the <strong class="source-inline">192.168.0.11</strong> IP and your agents are using the <strong class="source-inline">192.168.0.12</strong> and <strong class="source-inline">192.168.0.13</strong> IP addresses. Remember that your configuration has set static IP private addresses to <a id="_idIndexMarker295"/>your nodes to prevent <a id="_idIndexMarker296"/>errors in your nodes if the IP address changes. We assume that the nodes are using IP addresses starting from <strong class="source-inline">192.168.0.11</strong> to <strong class="source-inline">192.168.0.13</strong>. We are going to use the <strong class="source-inline">192.168.0.240</strong> to <strong class="source-inline">192.168.0.250</strong> IP address range for load balancers. This is just a simple example of how to organize your IPs for your cluster.</p>
			<p>We are assuming that your router is in the <strong class="source-inline">192.168.0.0/24</strong> network. As we mentioned, home routers have some switch capabilities to auto-assign dynamic IP addresses using a DHCP service configured inside the router, but this isn't healthy for your nodes. That's the main reason for using static IPs for your nodes. We are assuming some public IP to use as an example. We are assuming that we are going to use the <strong class="source-inline">8.8.8.8</strong> and <strong class="source-inline">1.1.1.1</strong> DNS servers.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">All these IP ranges can change, depending on your internet provider or the router device that you are using. We set these values to provide an example of how to organize the network for your cluster.</p>
			<p>To <a id="_idIndexMarker297"/>create your homelab, we have <a id="_idIndexMarker298"/>to complete the following tasks:</p>
			<ol>
				<li>Install Ubuntu image on your Raspberry device.</li>
				<li>Configure your device to run the K3s installer.</li>
				<li>Configure the K3s master node.</li>
				<li>Configure the K3s agent nodes.</li>
				<li>Install <strong class="bold">MetalLB</strong> as the load balancing service.</li>
				<li>Install <strong class="bold">Longhorn</strong> as the default storage class.</li>
				<li>Configure <strong class="source-inline">kubectl</strong> in an external client to access the cluster.</li>
				<li>Deploy your first application using <strong class="source-inline">kubectl</strong> and YAML files.</li>
				<li>Install and configure <strong class="bold">Lens</strong> to manage your cluster.</li>
			</ol>
			<p>So, let's now quickly recap the concepts, starting with how to install Ubuntu on your device.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor101"/>Installing an Ubuntu image on your Raspberry device</h2>
			<p>In this <a id="_idIndexMarker299"/>section, we <a id="_idIndexMarker300"/>are going to install an Ubuntu <a id="_idIndexMarker301"/>image on a Raspberry device. You can skip this section or refer to previous chapters for more information. As a quick summary, you can follow the next steps to install Ubuntu on a Raspberry device:</p>
			<ol>
				<li value="1">Open <em class="italic">Raspberry Pi Imager</em>.</li>
				<li>Click on the <strong class="bold">CHOOSE OS</strong> button to choose the Ubuntu Server 20.04 64-bit for ARM64 operating system, which is located in the <strong class="bold">Other general purpose OS</strong> | <strong class="bold">Ubuntu</strong> menu.</li>
				<li>Then, insert your microSD card (you may have to buy an adapter to read microSD cards); your device appears when you select the <strong class="bold">CHOOSE STORAGE</strong> button.</li>
				<li>Click <a id="_idIndexMarker302"/>on the <strong class="bold">WRITE</strong> button.</li>
				<li>Accept <a id="_idIndexMarker303"/>to write the device; then, Raspberry <a id="_idIndexMarker304"/>Pi Imager will ask you for your username and password in order to continue writing to the microSD card.</li>
				<li>Wait until the writing and verifying process finishes.</li>
				<li>Extract your microSD card.</li>
				<li>Insert the microSD card into your Raspberry Pi and turn it on.</li>
				<li>Repeat these steps for each Raspberry Pi device that will be part of your cluster.</li>
			</ol>
			<p>Now, let's move to configure the network settings and the container support for your device.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor102"/>Configuring your Raspberry Pi to run the K3s installer</h2>
			<p>In <a id="_idIndexMarker305"/>this section, we <a id="_idIndexMarker306"/>are going to configure the <a id="_idIndexMarker307"/>network settings, including your static IP address, DNS, hostname, and hosts files, finalizing with activating the support of the cgroups necessary <a id="_idIndexMarker308"/>to use <strong class="bold">containerd</strong>. Now, follow the next steps to perform the final setup before installing K3s in your nodes; remember that you can customize all these configurations to fit your own network:</p>
			<ol>
				<li value="1">Turn on your device.</li>
				<li>When Ubuntu asks you for your username and password, enter the username and <strong class="source-inline">ubuntu</strong> as the password this is the default password for the first login.</li>
				<li>Now, Ubuntu will ask you to change the default password; let's use <strong class="source-inline">k3s123-</strong> as our password.</li>
				<li>Now, let's configure the network; by default, Ubuntu uses <strong class="source-inline">cloud-init</strong> to configure <a id="_idIndexMarker309"/>the network. Let's deactivate this by creating the <strong class="source-inline">99-disable-network-config.cfg</strong> file with the following commands and content:<p class="source-code"><strong class="bold">$ sudo nano /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg</strong></p></li>
			</ol>
			<p>Here <a id="_idIndexMarker310"/>is an example of the content:</p>
			<p class="source-code"><strong class="bold">network: {config: disabled}</strong></p>
			<ol>
				<li value="5">If you <a id="_idIndexMarker311"/>execute <strong class="source-inline">ifconfig</strong>, you will see that your device is <strong class="source-inline">eth0</strong>, but it can be named <strong class="source-inline">es3</strong> or something similar, so let's modify the <strong class="source-inline">50-cloud-init</strong> file with the following command:<p class="source-code"><strong class="bold">$ sudo nano /etc/netplan/50-cloud-init.yaml</strong></p></li>
				<li>Then, modify the content of the file; it has to look something like this:<p class="source-code">network:</p><p class="source-code">  version: 2</p><p class="source-code">  renderer: networkd</p><p class="source-code">  ethernets:</p><p class="source-code">    eth0:</p><p class="source-code">      dhcp4: no</p><p class="source-code">      addresses:</p><p class="source-code">        - 192.168.0.11/24</p><p class="source-code">      gateway4: 192.168.0.1</p><p class="source-code">      nameservers:</p><p class="source-code">          addresses: [8.8.8.8, 1.1.1.1]</p></li>
				<li>Now, apply the configuration and reboot your device to see whether your IP address is set when the <strong class="bold">Operating System</strong> (<strong class="bold">OS</strong>) starts. To do this, execute the following command:<p class="source-code"><strong class="bold">$ sudo netplan apply </strong></p></li>
				<li>Now, configure <a id="_idIndexMarker312"/>the kernel parameters for the boot by editing the <strong class="source-inline">/boot/firmware/cmdline.txt</strong> file with the following command and content:<p class="source-code"><strong class="bold">$ sudo nano /boot/firmware/cmdline.txt</strong></p></li>
			</ol>
			<p>Add <a id="_idIndexMarker313"/>this content to the end of the line:</p>
			<p class="source-code"><strong class="bold">cgroup_memory=1 cgroup_enable=memory</strong></p>
			<ol>
				<li value="9">Edit <strong class="source-inline">/etc/hostname</strong> using the <strong class="source-inline">master</strong> name for your master node. Use <strong class="source-inline">node01</strong> and <strong class="source-inline">node02</strong> for the <a id="_idIndexMarker314"/>hostnames of your agent nodes; let's edit the file using <strong class="source-inline">nano</strong>:<p class="source-code"><strong class="bold">$ sudo nano /etc/hostname</strong></p></li>
			</ol>
			<p>Here is an example of the content:</p>
			<p class="source-code"><strong class="bold">master</strong></p>
			<ol>
				<li value="10">Edit the <strong class="source-inline">/etc/hosts</strong> file, adding the hostname; at a minimum, you need to have a line like this:<p class="source-code"><strong class="bold">$ sudo nano /etc/hosts</strong></p></li>
			</ol>
			<p>Here is an example of the content:</p>
			<p class="source-code"><strong class="bold">127.0.0.1 localhost master</strong></p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can also use <strong class="source-inline">master.local</strong> instead of <strong class="source-inline">master</strong> to follow <strong class="bold">Internet Engineering Task Force</strong> (<strong class="bold">IETF</strong>) naming <a id="_idIndexMarker315"/>conventions for local networks. This may <a id="_idIndexMarker316"/>also help with zero-configuration <strong class="bold">multicast DNS</strong> (<strong class="bold">mDNS</strong>) setups. For more information, you can check out <a id="_idIndexMarker317"/>this link: <a href="http://www.zeroconf.org">http://www.zeroconf.org</a>.</p>
			<p>Now, reboot your device:</p>
			<p class="source-code"><strong class="bold">$ sudo reboot</strong></p>
			<p>This <a id="_idIndexMarker318"/>configuration <a id="_idIndexMarker319"/>is required to prepare <a id="_idIndexMarker320"/>your device to configure a K3s master or agent nodes. You can also follow IETF recommendations for local network design. In the next section, you will see how to install K3s for your master nodes.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor103"/>Configuring the K3s master node</h2>
			<p>This section <a id="_idIndexMarker321"/>explains how to install your master node <a id="_idIndexMarker322"/>for your K3s cluster; for this, you have to follow these steps:</p>
			<ol>
				<li value="1">Turn on your device and log in with your <strong class="source-inline">ubuntu</strong> user.</li>
				<li>Run the following commands to install your master node using <strong class="source-inline">MASTER_IP</strong> as <strong class="source-inline">192.168.0.11</strong>, as shown in <em class="italic">Figure 5.1</em>, for your K3s cluster:<p class="source-code"><strong class="bold">$ MASTER_IP=&lt;YOUR_PRIVATE_IP&gt;</strong></p><p class="source-code"><strong class="bold">$ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--write-kubeconfig-mode 644 --no-deploy traefik --disable traefik --tls-san "$MASTER_IP" --node-external-ip "$MASTER_IP" --disable servicelb" sh -s -</strong></p></li>
			</ol>
			<p>Now, we have installed the master node. This will be the node with the <strong class="source-inline">192.168.0.11</strong> IP address. Now, let's go ahead and add agent nodes to the cluster in the next section.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor104"/>Configuring the K3s agent nodes</h2>
			<p>This section <a id="_idIndexMarker323"/>explains how to complete <a id="_idIndexMarker324"/>our initial cluster diagram by repeating this section twice to complete the configuration of two agent nodes. Agent nodes will use the <strong class="source-inline">192.168.0.12</strong> and <strong class="source-inline">192.168.0.13</strong> IP addresses. Complete the following steps to configure each agent node:</p>
			<ol>
				<li value="1">Log in to your master node:<p class="source-code"><strong class="bold">$ ssh ubuntu@&lt;MASTER_IP&gt;</strong></p></li>
			</ol>
			<p>We are going to extract the servicer node token to connect the agent nodes. In this case, the master node will be <strong class="source-inline">192.168.0.11</strong>.</p>
			<ol>
				<li value="2">Extract and copy the token to join your agent nodes in the cluster, running the following command:<p class="source-code"><strong class="bold">$ sudo cat /var/lib/rancher/k3s/server/node-token</strong></p></li>
				<li>Log out from your master node. Now, you have the token to join additional nodes to the cluster.</li>
			</ol>
			<p>For each agent node to join the cluster, follow the next steps (the easy way):</p>
			<ol>
				<li value="1">Log in to your agent node that you want to add to the cluster. In this case, <strong class="source-inline">AGENT_IP</strong> will be <strong class="source-inline">192.168.0.12</strong> or <strong class="source-inline">192.168.0.13</strong>:<p class="source-code"><strong class="bold">$ ssh ubuntu@&lt;AGENT_IP&gt;</strong></p></li>
				<li>Set an environment variable with the token that your master generated:<p class="source-code"><strong class="bold">$ export TOKEN=&lt;YOUR_MASTER_TOKEN&gt;</strong></p></li>
				<li>Register your node with the following command; in this case, <strong class="source-inline">MASTER_IP</strong> will be <strong class="source-inline">192.168.0.11</strong>:<p class="source-code"><strong class="bold">$ curl -sfL https://get.k3s.io | sh -s - agent --server https://MASTER_IP:6443 --token ${TOKEN} </strong></p></li>
			</ol>
			<p>Exit from your agent node:</p>
			<p class="source-code"><strong class="bold">$ exit</strong></p>
			<p>Now, we have configured our agent nodes. Let's install MetalLB to start using load balancers for our applications.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor105"/>Installing MetalLB as the load balancing service</h2>
			<p>MetalLB is a <a id="_idIndexMarker325"/>bare metal load balancer that can help when using the load <a id="_idIndexMarker326"/>balancing service of a regular <a id="_idIndexMarker327"/>Kubernetes cluster, with the capabilities of networking designed for bare metal, such as IP address assignment. So, let's get started by installing MetalLB by following these steps:</p>
			<ol>
				<li value="1">Create a MetalLB namespace (<strong class="source-inline">metallb-system</strong>) with the official manifests, executing the following lines:<p class="source-code"><strong class="bold">$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/namespace.yaml</strong></p></li>
				<li>Before running the command to install MetalLB, you have to create a <strong class="source-inline">ConfigMap</strong> resource called <strong class="source-inline">metallb-config</strong> inside the <strong class="source-inline">metallb-system</strong> namespace. Let's call this file <strong class="source-inline">config.yaml</strong>, with the following content:<p class="source-code">apiVersion: v1 </p><p class="source-code">kind: ConfigMap </p><p class="source-code">metadata: </p><p class="source-code">  namespace: metallb-system </p><p class="source-code">  name: config </p><p class="source-code">data: </p><p class="source-code">  config: | </p><p class="source-code">    address-pools: </p><p class="source-code">    - name: default </p><p class="source-code">      protocol: layer2 </p><p class="source-code">      addresses: </p><p class="source-code">      - 192.168.0.240-192.168.0.250</p></li>
				<li>Now, create <strong class="source-inline">ConfigMap</strong>, executing the following command:<p class="source-code"><strong class="bold">$ kubectl apply -f config.yaml</strong></p></li>
				<li>Install <a id="_idIndexMarker328"/>MetalLB with the official manifests by executing the following lines:<p class="source-code"><strong class="bold">$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/metallb.yaml</strong></p></li>
			</ol>
			<p>Now, you <a id="_idIndexMarker329"/>have installed MetalLB. You are ready to install services that use load balancers. These load balancers are commonly found in a lot of Kubernetes software. Now, it is time to add Longhorn for our storage. </p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor106"/>Installing Longhorn with ReadWriteMany mode</h2>
			<p>K3s includes basic storage support. Sometimes, this storage can cause errors when you are <a id="_idIndexMarker330"/>installing software. To prevent this, you will need another storage driver instead of the default <a id="_idIndexMarker331"/>one that comes with K3s. In this <a id="_idIndexMarker332"/>case, you can use Longhorn. With Longhorn, you can install Kubernetes software that looks for regular storage drivers. So, let's install Longhorn in the following steps:</p>
			<ol>
				<li value="1">Log in to your <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) or device:<p class="source-code"><strong class="bold">$ ssh ubuntu@NODE_IP</strong></p></li>
				<li>If you <a id="_idIndexMarker333"/>want to install <strong class="bold">ReadWriteMany</strong> <strong class="bold">Persistent Volume</strong> (<strong class="bold">PVC</strong>) mode, you have to install <strong class="source-inline">nfs-common</strong> on each VM with Ubuntu installed in your cluster. To do this, execute the following command:<p class="source-code"><strong class="bold">$ sudo apt install -y nfs-common</strong></p></li>
				<li>Apply <a id="_idIndexMarker334"/>the official Longhorn manifests, as follows:<p class="source-code"><strong class="bold">$ kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.1.2/deploy/longhorn.yaml</strong></p></li>
			</ol>
			<p>Now, you <a id="_idIndexMarker335"/>have Longhorn installed <a id="_idIndexMarker336"/>and running. Let's move on to learn how to configure <strong class="source-inline">kubectl</strong> on your personal computer to manage your K3s.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor107"/>Extracting the K3s kubeconfig file to access your cluster</h2>
			<p>Now, it's <a id="_idIndexMarker337"/>time to configure the <strong class="source-inline">kubeconfig</strong> file to access your K3s cluster from your computer, using the <strong class="source-inline">kubectl</strong> command. To <a id="_idIndexMarker338"/>configure the connection of your new K3s cluster from the outside, follow these steps:</p>
			<ol>
				<li value="1">Install <strong class="source-inline">kubectl</strong>, following the instructions of the official documentation of Kubernetes (<a href="https://kubernetes.io/docs">https://kubernetes.io/docs</a>); in this case, we are going to use the instructions for Macintosh:<p class="source-code"><strong class="bold">$ curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl"</strong></p><p class="source-code"><strong class="bold">$ chmod +x ./kubectl</strong></p><p class="source-code"><strong class="bold">$ sudo mv ./kubectl /usr/local/bin/kubectl</strong></p><p class="source-code"><strong class="bold">$ sudo chown root: /usr/local/bin/kubectl</strong></p></li>
			</ol>
			<p>Or you can install <strong class="source-inline">kubectl</strong> using <strong class="source-inline">brew</strong> on macOS, using the next command:</p>
			<p class="source-code"><strong class="bold">$ brew install kubectl </strong></p>
			<p>For other custom installations, such as <strong class="source-inline">kubectl</strong> for Apple's new silicon processors, Linux, or Windows, visit the Kubernetes official documentation: <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-macos">https://kubernetes.io/docs/tasks/tools/install-kubectl-macos</a>.</p>
			<ol>
				<li value="2">From the master node, copy the content inside <strong class="source-inline">/etc/rancher/k3s/k3s.yaml</strong> to your local <strong class="source-inline">~/.kube/config</strong> file.</li>
				<li>Change <a id="_idIndexMarker339"/>the permissions of the file with the next command:<p class="source-code"><strong class="bold">$ chmod 0400 ~/.kube/config</strong></p></li>
				<li>Change <a id="_idIndexMarker340"/>part of the server value from <strong class="source-inline">127.0.0.1</strong> to the <strong class="source-inline">MASTER_IP</strong> address of your master node; in this case, it will be <strong class="source-inline">192.168.0.11</strong>:<p class="source-code"><strong class="bold">server: https://127.0.0.1:6443</strong></p></li>
			</ol>
			<p>This changes to the following:</p>
			<p class="source-code"><strong class="bold">server: https://MASTER_IP:6443</strong></p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Remember to install <strong class="source-inline">kubectl</strong> before you copy the Rancher <strong class="source-inline">kubeconfig</strong> file onto your computer. Remember that the content of the <strong class="source-inline">k3s.yaml</strong> file has to be stored inside <strong class="source-inline">~/.kube/config</strong> and needs <a id="_idIndexMarker341"/>the <strong class="source-inline">0400</strong> permissions. To check how to install the <strong class="source-inline">kubectl</strong> command, go to <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-macos">https://kubernetes.io/docs/tasks/tools/install-kubectl-macos</a>.</p>
			<p>Now, we are ready to use the cluster. In the next section, we are going to deploy a basic application with <strong class="source-inline">kubectl</strong> and YAML files, using MetalLB and Longhorn. So, let's start deploying applications, using <strong class="source-inline">kubectl</strong> in the next section.</p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor108"/>Deploying your first application with kubectl</h1>
			<p>This section <a id="_idIndexMarker342"/>covers the basics of Kubernetes. We <a id="_idIndexMarker343"/>are going to deploy an application using <strong class="source-inline">kubectl</strong> first. But before that, let me give you a quick introduction about how Kubernetes works with its basic objects.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor109"/>Basic Kubernetes objects</h2>
			<p>Kubernetes works with objects that provide different functionalities for your application using containers. The goal <a id="_idIndexMarker344"/>of Kubernetes is to orchestrate your containers. Kubernetes uses two ways to create objects. One is using imperative commands – in the case of Kubernetes, the <strong class="source-inline">kubectl</strong> command. The other is using declarative files, where the state of an object is defined, and Kubernetes ensures that this state stays as it was defined throughout its lifetime:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B16945_Figure_5.2.jpg" alt="Figure 5.2 – Kubernetes objects&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Kubernetes objects</p>
			<p>This diagram represents how some of the basic objects interact with each other to deploy and manage an application. So, let's explain each of these objects:</p>
			<ul>
				<li><strong class="bold">Pod</strong> contains <a id="_idIndexMarker345"/>one or more containers, where your application lives; all the containers inside a Pod share the same network, memory, and CPU.</li>
				<li><strong class="bold">ReplicaSet</strong> controls the <a id="_idIndexMarker346"/>number of pods to be the same.</li>
				<li><strong class="bold">Deployment</strong> is an <a id="_idIndexMarker347"/>advanced kind of <strong class="bold">ReplicaSet</strong> object that not only controls the number of Pods and versions but also the changes of the Pods, providing a way to perform rollbacks.</li>
				<li><strong class="bold">Service</strong> is a way <a id="_idIndexMarker348"/>to expose your services. There are different types. <strong class="bold">NodePort</strong> opens a random port on all the nodes, <strong class="bold">ClusterIP</strong> creates a DNS that you can use to communicate with your Pod or deploy with other Pods or deployments, and <strong class="bold">LoadBalancer</strong> creates an exclusive endpoint to publish your app to the outside.</li>
				<li><strong class="bold">Persistent Volume Claim</strong> is the <a id="_idIndexMarker349"/>object in charge of requesting persistent storage and creating stateful deployments.</li>
				<li><strong class="bold">Storage Class</strong> is the <a id="_idIndexMarker350"/>object that defines how you are going to request storage for an application.</li>
			</ul>
			<p>With these pretty basic concepts, let's move on to the practical aspects to understand how each component works. In the next section, we are going to deploy a simple NGINX server using <strong class="source-inline">kubectl</strong>.</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor110"/>Deploying a simple NGINX server with pods using kubectl</h2>
			<p>In this section, we <a id="_idIndexMarker351"/>are going to deploy an NGINX <a id="_idIndexMarker352"/>server, step <a id="_idIndexMarker353"/>by step, using <strong class="source-inline">kubectl</strong>. To do this, follow these steps:</p>
			<ol>
				<li value="1">Create a pod with the <strong class="source-inline">nginx</strong> image:<p class="source-code"><strong class="bold">$ kubectl run myserver --image=nginx --restart=Never</strong></p></li>
				<li>Create a <strong class="source-inline">LoadBalancer</strong> type of service for this Pod to expose and access the NGINX pod:<p class="source-code"><strong class="bold">$ kubectl expose pod/myserver --port=8001 --target-port=80 –type=LoadBalancer</strong></p></li>
				<li>Assign the IP address to your load balancer with the following command:<p class="source-code"><strong class="bold">$ IP_SERVICE=$(kubectl get svc mywebserver --output jsonpath='{.status.loadBalancer.ingress[0].ip}')</strong></p></li>
				<li>Access <a id="_idIndexMarker354"/>the next URL using your browser or the following command:<p class="source-code"><strong class="bold">$ curl IP_SERVICE:8001</strong></p></li>
			</ol>
			<p>Now, you <a id="_idIndexMarker355"/>have an NGINX service <a id="_idIndexMarker356"/>up and running. So, let's move to deploy a <strong class="bold">Redis</strong> database that you can access to store data in the next section.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor111"/>Deploying a Redis NoSQL database with pods</h2>
			<p>Now, we are <a id="_idIndexMarker357"/>going to deploy a Redis NoSQL <a id="_idIndexMarker358"/>key-value database that you can access to store some data. We chose Redis as a basic example as it is quick and easy to use. So, let's deploy Redis using the following commands:</p>
			<ol>
				<li value="1">Create a pod with a <strong class="source-inline">redis</strong> image:<p class="source-code"><strong class="bold">$ kubectl run myredis --image=redis --restart=Never</strong></p></li>
				<li>Create a <strong class="source-inline">ClusterIP</strong> service that you can use to connect to Redis using the name of the service:<p class="source-code"><strong class="bold">$ kubectl expose pod myredis --port=6379 --type=ClusterIP</strong></p></li>
				<li>Let's create an <strong class="source-inline">ubuntu</strong> client with the next command:<p class="source-code"><strong class="bold">$ kubectl run client -it --rm --image=ubuntu:18.04 -- bash</strong></p></li>
				<li>Now, you are inside the client, so let's install the Redis client to get connected to the Redis pods with the following command:<p class="source-code"><strong class="bold">root@client# apt-get update;apt-get install -y redis-tools</strong></p></li>
				<li>Store the variable with the value <strong class="source-inline">1</strong> and get the value from the client, using the following commands:<p class="source-code"><strong class="bold">root@client# redis-cli -h myredis set a 1</strong></p><p class="source-code"><strong class="bold">root@client# redis-cli –h myredis get a</strong></p></li>
			</ol>
			<p>The last command returns the value of the <strong class="source-inline">a</strong> variable, which is <strong class="source-inline">1</strong>.</p>
			<ol>
				<li value="6">Write <strong class="source-inline">exit</strong> and then press <em class="italic">Enter</em> to exit the client. The client will be automatically deleted because of the <strong class="source-inline">--rm</strong> parameter.</li>
				<li>Now, let's expose Redis, using <strong class="source-inline">NodePort</strong> as an example of how to expose a pod using the IPs of your nodes:<p class="source-code"><strong class="bold">$ kubectl expose pod myredis --name=myredis-nodeport --port=6379 --type=NodePort</strong></p></li>
			</ol>
			<p>Now, you can <a id="_idIndexMarker359"/>access your Redis database <a id="_idIndexMarker360"/>using the IP of the host where Redis was deployed.</p>
			<p>You have finished installing a simple database – in this case, Redis. Now, let's explore the deployment objects and storage in the next section.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor112"/>Deploying and scaling an NGINX server with deployments</h2>
			<p>One of the advantages of using deployments is that you manage the changes of your deployment <a id="_idIndexMarker361"/>if the version or the configuration changes. Let's deploy <a id="_idIndexMarker362"/>a simple NGINX server, scale the deployment, change the image, and then perform a rollback to see the power of deployments. Deploy the NGINX server by following these steps:</p>
			<ol>
				<li value="1">Create a deployment with two replicas using the <strong class="source-inline">nginx</strong> image:<p class="source-code"><strong class="bold">$ kubectl create deployment mywebserver --image=nginx --replicas=2</strong></p></li>
				<li>Create a <strong class="source-inline">LoadBalancer</strong> service to expose your deployment:<p class="source-code"><strong class="bold">$ kubectl expose deployment mywebserver --port=8002 --target-port=80 --type=LoadBalancer</strong></p></li>
				<li>Create the IP for <strong class="source-inline">mywebserver</strong>:<p class="source-code"><strong class="bold">$ IP_SERVICE=$(kubectl get svc mywebserver --output jsonpath='{.status.loadBalancer.ingress[0].ip}')</strong></p></li>
				<li>Access the web server using <strong class="source-inline">curl</strong>:<p class="source-code"><strong class="bold">$ curl $IP_SERVICE:8002</strong></p></li>
				<li>Scale <strong class="source-inline">mywebserver</strong> with <strong class="source-inline">0</strong> replicas:<p class="source-code"><strong class="bold">$ kubectl scale deploy/mywebserver --replicas=0</strong></p></li>
				<li>Try to access <strong class="source-inline">mywebserver</strong> again:<p class="source-code"><strong class="bold">$ curl $IP_SERVICE:8002</strong></p></li>
				<li>Scale <strong class="source-inline">mywebserver</strong> with two replicas and wait until the deployment is ready; you can check this with the following:<p class="source-code"><strong class="bold">$ kubectl scale deploy/mywebserver --replicas=2</strong></p><p class="source-code"><strong class="bold">$ kubectl rollout status deploy/mywebserver </strong></p></li>
				<li>Try to <a id="_idIndexMarker363"/>access <strong class="source-inline">mywebserver</strong> again:<p class="source-code"><strong class="bold">$ curl $IP_SERVICE:8002</strong></p></li>
				<li>Let's change <a id="_idIndexMarker364"/>the <strong class="source-inline">nginx</strong> version of the deployment with the wrong version:<p class="source-code"><strong class="bold">$ kubectl set image deployment/mywebserver nginx=nginx:1.16.1.x</strong></p></li>
				<li>Check the changes in the description of the object:<p class="source-code"><strong class="bold">$ kubectl describe deployment mywebserver | grep -i image</strong></p></li>
				<li>Check the current pod status for the <strong class="source-inline">mywebserver</strong> deployment:<p class="source-code"><strong class="bold">$ kubectl get pods</strong></p></li>
			</ol>
			<p>You will see some pods from <strong class="source-inline">mywebserver</strong> with errors.</p>
			<ol>
				<li value="12">Let's roll back to the previous version:<p class="source-code"><strong class="bold">$ kubectl rollout undo deploy/mywebserver</strong></p></li>
				<li>Check the current pod status for the <strong class="source-inline">mywebserver</strong> deployment:<p class="source-code"><strong class="bold">$ kubectl get pods</strong></p></li>
			</ol>
			<p>You will see that the pods with errors have disappeared because you returned to the previous image that the deployment was using – in this case, the correct image name.</p>
			<p>Now, you have <a id="_idIndexMarker365"/>deployed your application using the deployment <a id="_idIndexMarker366"/>object. Let's do something similar using YAML files and add some persistence. To do this, let's move on to the next section.</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor113"/>Deploying a simple NGINX server using 
YAML files</h1>
			<p>At this point, our examples don't store data and the objects are created using imperative <a id="_idIndexMarker367"/>commands. To use declarative files, you can use the <strong class="source-inline">kubectl</strong> command to generate the files. Remember to deploy <a id="_idIndexMarker368"/>your application, using pods or deployments – just choose one of these options. To start, let's create an NGINX pod using YAML files.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor114"/>Deploying an NGINX server using a Pod</h2>
			<p>Now, let's <a id="_idIndexMarker369"/>create an NGINX pod using YAML files. To <a id="_idIndexMarker370"/>do this, follow these steps:</p>
			<ol>
				<li value="1">If you want to use pods, you can use the next YAML file. To generate the file, use the following command:<p class="source-code"><strong class="bold">$ kubectl run nginx --image=nginx --dry-run –o yaml &gt; nginx-pod.yaml</strong></p></li>
			</ol>
			<p>The <strong class="source-inline">nginx-pod.yaml</strong> file will look like this:</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Pod</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: nginx</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    name: nginx</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  containers:</p>
			<p class="source-code">  - name: nginx</p>
			<p class="source-code">    image: nginx</p>
			<p class="source-code">    ports:</p>
			<p class="source-code">      - containerPort: 80</p>
			<ol>
				<li value="2">Apply <a id="_idIndexMarker371"/>the generated file using the following command:<p class="source-code"><strong class="bold">$ kubectl create -f nginx-pod.yaml</strong></p></li>
			</ol>
			<p>Let's move <a id="_idIndexMarker372"/>on to create an NGINX deployment in the next section.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor115"/>Deploying an NGINX server using deployment</h2>
			<p>So, let's get <a id="_idIndexMarker373"/>started creating an NGINX server <a id="_idIndexMarker374"/>using deployment with YAML files. To do this, follow these steps:</p>
			<ol>
				<li value="1">Generate the YAML file for deployment using the following command:<p class="source-code"><strong class="bold">$ kubectl create deployment nginx --image=nginx --replicas=2 --dry-run –o yaml &gt; nginx-deployment.yaml</strong></p></li>
			</ol>
			<p>The file <strong class="source-inline">nginx-deployment.yaml</strong> will look like this:</p>
			<p class="source-code">apiVersion: apps/v1</p>
			<p class="source-code">kind: Deployment</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    app: nginx</p>
			<p class="source-code">  name: nginx</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  replicas: 2</p>
			<p class="source-code">  selector:</p>
			<p class="source-code">    matchLabels:</p>
			<p class="source-code">      app: nginx</p>
			<p class="source-code">  template:</p>
			<p class="source-code">    metadata:</p>
			<p class="source-code">      labels:</p>
			<p class="source-code">        app: nginx</p>
			<p class="source-code">    spec:</p>
			<p class="source-code">      containers:</p>
			<p class="source-code">      - image: nginx</p>
			<p class="source-code">        name: nginx</p>
			<ol>
				<li value="2">Apply <a id="_idIndexMarker375"/>the generated file using the <a id="_idIndexMarker376"/>following command:<p class="source-code"><strong class="bold">$ kubectl create -f nginx-deployment.yaml</strong></p></li>
			</ol>
			<p>Now that we have learned how to create a pod and deployment in Kubernetes, let's move on to the next section to expose these objects using services with YAML files.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor116"/>Exposing your pods using the ClusterIP service and YAML files</h2>
			<p>To communicate <a id="_idIndexMarker377"/>your pod or deployment with <a id="_idIndexMarker378"/>other applications, you may need a DNS record. The <strong class="source-inline">ClusterIP</strong> service <a id="_idIndexMarker379"/>type creates a DNS A <a id="_idIndexMarker380"/>record for your pod or deployment. Using this DNS, other objects in your cluster can access your application. So, let's create a <strong class="source-inline">ClusterIP</strong> service for your application, following these steps:</p>
			<ol>
				<li value="1">To expose your application using YAML files, generate the YAML file for the <strong class="source-inline">ClusterIP</strong> service type: <p class="source-code"><strong class="bold">$ kubectl expose pod/nginx --type=ClusterIP --port=80  --target-port=8001 --dry-run –o yaml &gt; nginx-clusterip.yaml</strong></p></li>
			</ol>
			<p>The <strong class="source-inline">nginx-service.yaml</strong> file <a id="_idIndexMarker381"/>will look like this:</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Service</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  creationTimestamp: null</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    app: nginx</p>
			<p class="source-code">  name: nginx</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  ports:</p>
			<p class="source-code">  - port: 80</p>
			<p class="source-code">    protocol: TCP</p>
			<p class="source-code">    targetPort: 8001</p>
			<p class="source-code">  selector:</p>
			<p class="source-code">    app: nginx</p>
			<p class="source-code">  type: ClusterIP</p>
			<ol>
				<li value="2">Apply <a id="_idIndexMarker382"/>the generated file using the following command:<p class="source-code"><strong class="bold">$ kubectl create -f nginx-pod.yaml</strong></p></li>
			</ol>
			<p>Now that you have learned how to create a <strong class="source-inline">ClusterIP</strong> service using YAML files, let's move on to creating a <strong class="source-inline">NodePort</strong> service for your application in the next section.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor117"/>Exposing your pods using the NodePort service and YAML files</h2>
			<p>To create <a id="_idIndexMarker383"/>a NodePort service for a previously <a id="_idIndexMarker384"/>created pod, follow these steps:</p>
			<ol>
				<li value="1">For <strong class="source-inline">NodePort</strong>, use <a id="_idIndexMarker385"/>the following command: <p class="source-code"><strong class="bold">kubectl expose pod/nginx --type=NodePort --port=80 --target-port=8001 --dry-run -o yaml &gt; nginx-nodeport.yaml</strong></p></li>
			</ol>
			<p>The <strong class="source-inline">nginx-nodeport.yaml</strong> file <a id="_idIndexMarker386"/>will look like this:</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Service</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    app: nginx</p>
			<p class="source-code">  name: nginx</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  ports:</p>
			<p class="source-code">  - port: 80</p>
			<p class="source-code">    protocol: TCP</p>
			<p class="source-code">    targetPort: 8001</p>
			<p class="source-code">  selector:</p>
			<p class="source-code">    app: nginx</p>
			<p class="source-code">  type: NodePort</p>
			<ol>
				<li value="2">Apply the generated file using the next command:<p class="source-code"><strong class="bold">kubectl create -f nginx-pod.yaml</strong></p></li>
			</ol>
			<p>Now that you have learned how to create a <strong class="source-inline">NodePort</strong> service for your application in a pod, it's time to learn how to use <strong class="source-inline">LoadBalancer</strong> services in the next section.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor118"/>Exposing your pods using a LoadBalancer service and YAML files</h2>
			<p>To <a id="_idIndexMarker387"/>create a <strong class="source-inline">LoadBalancer</strong> service to <a id="_idIndexMarker388"/>expose your application inside a pod, follow these steps:</p>
			<ol>
				<li value="1">For <strong class="source-inline">LoadBalancer</strong>, use <a id="_idIndexMarker389"/>the next command:<p class="source-code"><strong class="bold">$ kubectl expose pod/nginx --type=LoadBalancer --port=80 -target-port=8001 --dry-run -o yaml &gt; nginx-lb.yaml</strong></p></li>
			</ol>
			<p>The <a id="_idIndexMarker390"/>generated <strong class="source-inline">nginx-lb.yaml</strong> file will look like this:</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Service</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    app: nginx</p>
			<p class="source-code">  name: nginx</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  ports:</p>
			<p class="source-code">  - port: 80</p>
			<p class="source-code">    protocol: TCP</p>
			<p class="source-code">    targetPort: 8001</p>
			<p class="source-code">  selector:</p>
			<p class="source-code">    app: nginx</p>
			<p class="source-code">  type: LoadBalancer</p>
			<ol>
				<li value="2">Apply the generated file using the next command:<p class="source-code"><strong class="bold">$ kubectl create -f nginx-pod.yaml</strong></p></li>
			</ol>
			<p>You have <a id="_idIndexMarker391"/>learned how to create a <strong class="source-inline">LoadBalancer</strong> service. With <a id="_idIndexMarker392"/>this, we have <a id="_idIndexMarker393"/>covered all the basic services in Kubernetes. Now, we <a id="_idIndexMarker394"/>are ready to learn how to create stateful applications. Let's move on to the next section to add persistence to your applications.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor119"/>Adding persistence to your applications</h1>
			<p>Now, it is time to add storage to your applications; we are going to use the storage classes installed <a id="_idIndexMarker395"/>with Longhorn to provide persistence to your applications. In this section, we are going to explore two examples using persistent volumes. In this part of the book, we are going to discuss the persistent volumes and the process of creating storage for a Pod. But first, we need a persistent volume claim definition to provision this storage.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor120"/>Creating an NGINX pod with a storage volume</h2>
			<p>To create <a id="_idIndexMarker396"/>your NGINX <a id="_idIndexMarker397"/>application using a storage volume <a id="_idIndexMarker398"/>that uses the Longhorn storage class, follow these steps:</p>
			<ol>
				<li value="1">Create <strong class="source-inline">pvc.yaml</strong>:<p class="source-code">apiVersion: v1</p><p class="source-code">kind: PersistentVolumeClaim</p><p class="source-code">metadata:</p><p class="source-code">  name: longhorn-volv-pvc</p><p class="source-code">spec:</p><p class="source-code">  accessModes:</p><p class="source-code">    - ReadWriteMany</p><p class="source-code">  storageClassName: longhorn</p><p class="source-code">  resources:</p><p class="source-code">    requests:</p><p class="source-code">      storage: 2Gi</p></li>
				<li>Apply the <strong class="source-inline">pvc.yaml</strong> YAML file:<p class="source-code"><strong class="bold">$ kubectl create -f pvc.yaml</strong></p></li>
			</ol>
			<p>Now, it's time to create a pod using this PVC that uses the Longhorn storage class. To do this, follow these steps:</p>
			<ol>
				<li value="3">Create <a id="_idIndexMarker399"/>and apply the <strong class="source-inline">pod.yaml</strong> <a id="_idIndexMarker400"/>file to create <a id="_idIndexMarker401"/>a pod using the previously created PVC:<p class="source-code">apiVersion: v1</p><p class="source-code">kind: Pod</p><p class="source-code">metadata:</p><p class="source-code">  name: volume-test</p><p class="source-code">  namespace: default</p><p class="source-code">spec:</p><p class="source-code">  containers:</p><p class="source-code">  - name: volume-test</p><p class="source-code">    image: nginx:stable-alpine</p><p class="source-code">    imagePullPolicy: IfNotPresent</p><p class="source-code">    volumeMounts:</p><p class="source-code">    - name: volv</p><p class="source-code">      mountPath: /data</p><p class="source-code">    ports:</p><p class="source-code">    - containerPort: 80</p><p class="source-code">  volumes:</p><p class="source-code">  - name: volv</p><p class="source-code">    persistentVolumeClaim:</p><p class="source-code">      claimName: longhorn-volv-pvc</p></li>
			</ol>
			<p>This <a id="_idIndexMarker402"/>example has created a pod using a persistent <a id="_idIndexMarker403"/>volume, with the Longhorn <a id="_idIndexMarker404"/>storage class. Let's continue with a second example that shows a database using a storage volume.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor121"/>Creating the database using a persistent volume</h2>
			<p>Now, is <a id="_idIndexMarker405"/>time to use a <a id="_idIndexMarker406"/>persistent volume for a database; in <a id="_idIndexMarker407"/>this example, you are going to learn how to create a Redis database with a persistent volume. So, let's get started with the following steps:</p>
			<ol>
				<li value="1">Create the <strong class="source-inline">redis.yaml</strong> file to create a pod that uses the previous <strong class="source-inline">longhorn-volv-pvc</strong> PVC:<p class="source-code">apiVersion: v1</p><p class="source-code">kind: Pod</p><p class="source-code">metadata:</p><p class="source-code">  name: redis</p><p class="source-code">spec:</p><p class="source-code">  containers:</p><p class="source-code">  - name: redis</p><p class="source-code">    image: redis</p><p class="source-code">    volumeMounts:</p><p class="source-code">    - name: redis-storage</p><p class="source-code">      mountPath: /data/redis</p><p class="source-code">  volumes:</p><p class="source-code">  - name: redis-storage</p><p class="source-code">    persistentVolumeClaim:</p><p class="source-code">      claimName: longhorn-volv-pvc</p></li>
				<li>Apply <a id="_idIndexMarker408"/>the <strong class="source-inline">pod.yaml</strong> YAML file to create the pod:<p class="source-code"><strong class="bold">$ kubectl create -f pod.yaml</strong></p></li>
				<li>Check <a id="_idIndexMarker409"/>and Apply the <strong class="source-inline">pod.yaml</strong> YAML file to create the pod:<p class="source-code"><strong class="bold">$ kubectl create -f pod.yaml</strong></p></li>
				<li>Apply <a id="_idIndexMarker410"/>the <strong class="source-inline">pod.yaml</strong> YAML file to create the pod:<p class="source-code"><strong class="bold">$ kubectl create -f pod.yaml</strong></p></li>
			</ol>
			<p class="callout-heading">Troubleshooting Your Deployments</p>
			<p class="callout">Remember that you can use the <strong class="source-inline">kubectl logs</strong> command to troubleshoot your deployments. For <a id="_idIndexMarker411"/>more information, you can check the next link: <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/">https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/</a>.</p>
			<p>Now, your Redis database is running and using a persistent volume to prevent the loss of data. In the last section, we are going to explore how to install a simple Kubernetes dashboard to manage your cluster using a UI.</p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor122"/>Deploying a Kubernetes dashboard</h1>
			<p>Now, it's time <a id="_idIndexMarker412"/>to install a Kubernetes dashboard. The next steps are based on the official K3s documentation. To start installing the dashboard, follow these steps:</p>
			<ol>
				<li value="1">Install the dashboard using the following commands:<p class="source-code"><strong class="bold">$ GITHUB_URL=https://github.com/kubernetes/dashboard/releases</strong></p><p class="source-code"><strong class="bold">$ VERSION_KUBE_DASHBOARD=$(curl -w '%{url_effective}' -I -L -s -S ${GITHUB_URL}/latest -o /dev/null | sed -e 's|.*/||')</strong></p><p class="source-code"><strong class="bold">$ sudo k3s kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/${VERSION_KUBE_DASHBOARD}/aio/deploy/recommended.yaml</strong></p></li>
			</ol>
			<p>This is going to install the dashboard, but you need to configure how to access this dashboard.</p>
			<ol>
				<li value="2">Create the <strong class="source-inline">dashboard-admin-user.yaml</strong> file to create a service account that provides access to your dashboard. The content of this file will be as follows:<p class="source-code">apiVersion: v1</p><p class="source-code">kind: ServiceAccount</p><p class="source-code">metadata:</p><p class="source-code">  name: admin-user</p><p class="source-code">  namespace: kubernetes-dashboard</p></li>
				<li>Now create the file <strong class="source-inline">dashboard-admin-user-role.yaml</strong>. The content of this file will be the next:<p class="source-code">apiVersion: rbac.authorization.k8s.io/v1</p><p class="source-code">kind: ClusterRoleBinding</p><p class="source-code">metadata:</p><p class="source-code">  name: admin-user</p><p class="source-code">roleRef:</p><p class="source-code">  apiGroup: rbac.authorization.k8s.io</p><p class="source-code">  kind: ClusterRole</p><p class="source-code">  name: cluster-admin</p><p class="source-code">subjects:</p><p class="source-code">- kind: ServiceAccount</p><p class="source-code">  name: admin-user</p><p class="source-code">  namespace: kubernetes-dashboard</p></li>
				<li>Now, apply <a id="_idIndexMarker413"/>the YAML files with the following command:<p class="source-code"><strong class="bold">$ kubectl create -f dashboard-admin-user.yml -f dashboard-admin-user-role.yml</strong></p></li>
				<li>Get the token inside the service account that will be used to access the dashboard:<p class="source-code"><strong class="bold">$ kubectl -n kubernetes-dashboard describe secret admin-user-token | grep '^token'</strong></p></li>
			</ol>
			<p>Copy the token content only.</p>
			<ol>
				<li value="6">Use <strong class="source-inline">kubectl proxy</strong> to expose the Kubernetes API in your localhost, using the following command:<p class="source-code"><strong class="bold">$ sudo kubectl proxy</strong></p></li>
				<li>Access <a id="_idIndexMarker414"/>your browser with the following URL:</li>
			</ol>
			<p><strong class="source-inline">http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</strong></p>
			<p>Sign in with the admin user bearer token that you got. Choose the <strong class="bold">Token</strong> option and enter the token. You will see a screen like this:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B16945_Figure_5.3.jpg" alt="Figure 5.3 – Kubernetes Dashboard sign-in screen&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Kubernetes Dashboard sign-in screen</p>
			<p>After clicking <a id="_idIndexMarker415"/>on the <strong class="bold">Sign In</strong> button, you will see the dashboard. Explore the different menus to see the state of your objects, or click on the plus icon at the lower-right corner to create objects using the YAML files:</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B16945_Figure_5.4.jpg" alt="Figure 5.4  – Kubernetes Dashboard showing CPU and memory usage&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4  – Kubernetes Dashboard showing CPU and memory usage</p>
			<p>We have <a id="_idIndexMarker416"/>now completed all the necessary concepts, giving you a quick introduction to how to use basic objects in Kubernetes with K3s.</p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor123"/>Summary</h1>
			<p>In this chapter, we learned how to set up a K3s cluster with Raspberry Pi devices for our homelab. We also covered how to use basic Kubernetes objects to deploy an application. We deployed sample applications in an imperative way using the <strong class="source-inline">kubectl</strong> command. We also deployed sample applications using YAML files too. At the end of the chapter, we covered how to install a Kubernetes dashboard to manage your cluster. In the next chapter, we are going to continue adding more pieces to this deployment; we are going to use ingress controllers to deploy applications at the edge.</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor124"/>Questions</h1>
			<p>Here are a few questions to validate your new knowledge:</p>
			<ol>
				<li value="1">What are the basic Kubernetes objects that I need to create an application?</li>
				<li>How can I install a K3s cluster for my homelab?</li>
				<li>How can I use <strong class="source-inline">kubectl</strong> to create my applications?</li>
				<li>How can I use YAML files to create my applications?</li>
				<li>How can I use persistent volumes?</li>
				<li>How can I troubleshoot my applications?</li>
			</ol>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor125"/>Further reading</h1>
			<p>You can refer to the following references for more information on the topics covered in this chapter:</p>
			<ul>
				<li>K3s installation options to add custom parameters to your config files: <a href="https://rancher.com/docs/k3s/latest/en/installation/install-options">https://rancher.com/docs/k3s/latest/en/installation/install-options</a> </li>
				<li>Longhorn official page: <a href="https://longhorn.io">https://longhorn.io</a></li>
				<li>MetalLB official page: <a href="https://metallb.universe.tf">https://metallb.universe.tf</a></li>
				<li>Official Kubernetes documentation: <a href="https://kubernetes.io/docs">https://kubernetes.io/docs</a></li>
				<li>Kubernetes Dashboard installation guide: <a href="https://rancher.com/docs/k3s/latest/en/installation/kube-dashboard">https://rancher.com/docs/k3s/latest/en/installation/kube-dashboard</a></li>
				<li>Kubernetes Dashboard installation using Helm: <a href="https://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard">https://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard</a></li>
			</ul>
		</div>
	

		<div id="_idContainer058" class="Content">
			<h1 id="_idParaDest-127"><a id="_idTextAnchor126"/>Part 2: Cloud Native Applications at the Edge</h1>
			<p>Here you will learn how to deploy your applications at the edge using GitOps, service meshes, serverless and event-driven architectures, and different types of databases.</p>
			<p>This part of the book comprises the following chapters:</p>
			<ul>
				<li><a href="B16945_06_Final_PG.xhtml#_idTextAnchor127"><em class="italic">Chapter 6</em></a>, <em class="italic">Exposing Your Applications Using Ingress Controllers and Certificates</em></li>
				<li><a href="B16945_07_Final_PG.xhtml#_idTextAnchor144"><em class="italic">Chapter 7</em></a>, <em class="italic">GitOps with Flux for Edge Applications</em> <em class="italic">  </em></li>
				<li><a href="B16945_08_Final_PG.xhtml#_idTextAnchor163"><em class="italic">Chapter 8</em></a>, <em class="italic">Observability and Traffic Splitting Using Linkerd</em>   </li>
				<li><a href="B16945_09_Final_PG.xhtml#_idTextAnchor181"><em class="italic">Chapter 9</em></a>, <em class="italic">Edge Serverless and Event-Driven Architectures with Knative and Cloud Events</em></li>
				<li><a href="B16945_10_Final_PG.xhtml#_idTextAnchor198"><em class="italic">Chapter 10</em></a>, <em class="italic">SQL and NoSQL Databases at the Edge</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer059">
			</div>
		</div>
		<div>
			<div id="_idContainer060">
			</div>
		</div>
	</body></html>