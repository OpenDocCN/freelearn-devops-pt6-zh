- en: Kubernetes Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](a5cf080a-372a-406e-bb48-019af313c676.xhtml), *Getting Started
    with Kubernetes*, we learned how to deploy containers with different resources
    and also looked at how to use volumes to persist data, dynamic provisioning, different
    storage classes, and advanced administration in Kubernetes. In this chapter, we'll
    learn how Kubernetes routes traffic to make all of this possible. Networking always
    plays an important role in the software world. We'll learn about Kubernetes networking
    step by step, looking at the communication between containers on a single host,
    multiple hosts, and inside a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the topics we''ll cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are plenty of options when it comes to implementing networking in Kubernetes.
    Kubernetes itself doesn''t care about how you implement it, but you must meet
    its three fundamental requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: All containers should be accessible to each other without NAT, regardless of
    which nodes they are on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All nodes should communicate with all containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The IP container should see itself in the same way as others see it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before getting any further into this, we'll first examine how default container
    networking works.
  prefs: []
  type: TYPE_NORMAL
- en: Docker networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now review how docker networking works before getting into Kubernetes
    networking. For container networking, there are different modes: bridge, none,
    overlay, macvlan, and host. We''ve learned about the major modes in [Chapter 2](05e2d0b4-0e70-4480-b5a0-f3860ddb24f2.xhtml), *DevOps
    with Containers. *Bridge is the default networking model. Docker creates and attaches
    a virtual Ethernet device (also known as `veth`) and assigns a network namespace
    to each container.'
  prefs: []
  type: TYPE_NORMAL
- en: The **network namespace** is a feature in Linux that is logically another copy
    of a network stack. It has its own routing tables, ARP tables, and network devices.
    This is a fundamental concept of container networking.
  prefs: []
  type: TYPE_NORMAL
- en: '`Veth` always comes in pairs; one is in the network namespace and the other
    is in the bridge. When the traffic comes into the host network, it will be routed
    into the bridge. The packet will be dispatched to its `veth`, and will go into
    the namespace inside the container, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35ce4055-4fd3-410d-b57e-e795da30a0c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a closer look at this. In the following example, we''ll use a minikube
    node as the docker host. First, we''ll have to use `minikube ssh` to ssh into
    the node because we''re not using Kubernetes yet. After we get into the minikube
    node, we''ll launch a container to interact with us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the implementation of outbound traffic within a container. `docker
    exec <container_name or container_id>` can run a command in a running container.
    Let''s use `ip link list` to list all the interfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that we have three interfaces inside the `busybox` container. One
    has an ID of `53` with the name `eth0@if54`. The number after `if` is the other
    interface ID in the pair. In this case, the pair ID is `54`. If we run the same
    command on the host, we can see that the `veth` in the host is pointing to `eth0`
    inside the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a `veth` on the host named `vethfeec36a@if53`. This pairs with `eth0@if54`
    in the container network namespace. `veth` 54 is attached to the `docker0` bridge,
    and eventually accesses the internet via `eth0`. If we take a look at the `iptables`
    rules, we can find a masquerading rule (also known as SNAT) on the host that docker
    creates for outbound traffic, which will make internet access available for containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, as regards the inbound traffic, docker creates a custom
    filter chain on prerouting and dynamically creates forwarding rules in the `DOCKER`
    filter chain. If we expose a container port, `8080`, and map it to a host port, `8000`,
    we can see that we''re listening to port `8000` on any IP address (`0.0.0.0/0`),
    which will then be routed to container port `8080`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now that we know how a packet goes in/out of containers, let's look at how containers
    in a pod communicate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'User-defined custom bridges:'
  prefs: []
  type: TYPE_NORMAL
- en: As well as the default bridge network, docker also supports user-defined bridges.
    Users can create the custom bridge on the fly. This provides better network isolation,
    supports DNS resolution through embedded DNS server, and can be attached and detached
    from the container at runtime. For more information, please refer to the following
    documentation: [https://docs.docker.com/network/bridge/#manage-a-user-defined-bridge](https://docs.docker.com/network/bridge/#manage-a-user-defined-bridge).
    [](https://docs.docker.com/network/bridge/#manage-a-user-defined-bridge)
  prefs: []
  type: TYPE_NORMAL
- en: Container-to-container communications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pods in Kubernetes have their own real IP addresses. Containers within a pod
    share network namespace, so they see each other as *localhost*. This is implemented
    by the **network container** by default, which acts as a bridge to dispatch traffic
    for every container in a pod. Let''s see how this works in the following example.
    Let''s use the first example from [Chapter 3](a5cf080a-372a-406e-bb48-019af313c676.xhtml),
    *Getting Started with Kubernetes*, which includes two containers, `nginx` and
    `centos`, inside one pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will describe the pod and look at its `Container ID`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, `web` has the container ID `d9bd923572ab`, and `centos` has
    the container ID `f4c019d289d4`. If we go into the `minikube/192.168.99.100` node
    using `docker ps`, we can check how many containers Kubernetes actually launches
    since we''re in minikube, which launches lots of other cluster containers. Check
    out the latest launch time by using the `CREATED` column, where we will find that
    there are three containers that have just been launched:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'There is an additional container, `4ddd3221cc47`, that was launched. Before
    digging into which container it is, let''s check the network mode of our `web`
    container. We will find that the containers in our example pod are running in
    containers with a mapped container mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `4ddd3221cc47` container is the so-called network container in this case.
    This holds the network namespace to let the `web` and `centos` containers join.
    Containers in the same network namespace share the same IP address and network
    configuration. This is the default implementation in Kubernetes for achieving
    container-to-container communications, which is mapped to the first requirement.
  prefs: []
  type: TYPE_NORMAL
- en: Pod-to-pod communications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pod IP addresses are accessible from other pods, no matter which nodes they're
    on. This fits the second requirement. We'll describe the pods' communication within
    the same node and across nodes in the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: Pod communication within the same node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pod-to-pod communication within the same node goes through the bridge by default.
    Let''s say we have two pods that have their own network namespaces. When pod 1
    wants to talk to pod 2, the packet passes through pod 1''s namespace to the corresponding
    `veth` pair, `**vethXXXX**`, and eventually goes to the bridge. The bridge then
    broadcasts the destination IP to help the packet find its way. `vethYYYY` responds with
    the broadcasts. The packet then arrives at pod 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/766d9c4b-d680-429b-af61-4f0f3e5325f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we know how the packet travels in a single node, we will move on and
    talk about how traffic gets routed when the pods are in different nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Pod communication across nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'According to the second requirement, all nodes must communicate with all containers.
    Kubernetes delegates implementation to the **container network interface** (**CNI**).
    Users can choose different implementations, by L2, L3, or overlay. Overlay networking,
    also known as packet encapsulation, is one of the most common solutions. This
    wraps a message before leaving the source, delivers it, and unwraps the message
    at its destination. This leads to a situation in which the overlay increases the
    network latency and complexity. As long as all the containers can access each
    other across nodes, you''re free to use any technology, such as L2 adjacency or
    the L3 gateway. For more information about CNI, refer to its spec ([https://github.com/containernetworking/cni/blob/master/SPEC.md](https://github.com/containernetworking/cni/blob/master/SPEC.md)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8a72fe0-d350-4899-916f-39d07a47b72f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s say we have a packet moving from pod 1 to pod 4\. The packet leaves
    the container interface and reaches the `veth` pair, and then passes through the
    bridge and the node''s network interface. Network implementation comes into play
    in step 4\. As long as the packet can be routed to the target node, you are free
    to use any options. In the following example, we''ll launch minikube with the
    `--network-plugin=cni` option. With CNI enabled, the parameters will be passed
    through kubelet in the node. Kubelet has a default network plugin, but you can
    probe any supported plugin when it starts up. Before starting `minikube`, you
    can use `minikube stop` first if it has been started or `minikube delete` to delete
    the whole cluster thoroughly before doing anything else. Although `minikube` is
    a single node environment that might not completely represent the production scenario
    we''ll encounter, this just gives you a basic idea of how all of this works. We
    will learn about the deployment of networking options in the real world in [Chapter
    9](acaa9855-1a87-4fd4-ad40-0955f5d12f28.xhtml), *Continuous Delivery*, and [Chapter
    10](f55d3fa8-e791-4473-83ba-ed8c4f848a90.xhtml), *Kubernetes on AWS*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When we specify the `network-plugin` option, `minikube` will use the directory
    specified in `--network-plugin-dir` for plugins on startup. In the CNI plugin,
    the default plugin directory is `/opt/cni/net.d`. After the cluster comes up,
    we can log into the node and look at the network interface configuration inside
    the minikube via `minikube ssh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will find that there is one new bridge in the node, and if we create the
    example pod again by using `5-1-1_pod.yml`, we will find that the IP address of
    the pod becomes `10.1.0.x`, which is attaching to `mybridge` instead of `docker0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This is because we have specified that we''ll use CNI as the network plugin,
    and `docker0` will not be used (also known as the **container network model**,
    or **libnetwork**). The CNI creates a virtual interface, attaches it to the underlay
    network, sets the IP address, and eventually routes and maps it to the pods''
    namespace. Let''s take a look at the configuration that''s located at `/etc/cni/net.d/k8s.conf`
    in minikube:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we are using the bridge CNI plugin to reuse the L2 bridge for
    pod containers. If the packet is from `10.1.0.0/16`, and its destination is to
    anywhere, it'll go through this gateway. Just like the diagram we saw earlier,
    we could have another node with CNI enabled with the `10.1.2.0/16` subnet so that
    ARP packets can go out to the physical interface on the node where the target
    pod is located. This then achieves pod-to-pod communication across nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check the rules in `iptables`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: All the related rules have been switched to `10.1.0.0/16` CIDR.
  prefs: []
  type: TYPE_NORMAL
- en: Pod-to-service communications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is dynamic. Pods are created and deleted all the time. The Kubernetes
    service is an abstraction to define a set of pods by label selectors. We normally
    use the service to access pods instead of specifying a pod explicitly. When we
    create a service, an `endpoint` object will be created, which describes a set
    of pod IPs that the label selector in that service has selected.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the `endpoint` object will not be created upon creation of the
    service. For example, services without selectors will not create a corresponding
    `endpoint` object. For more information, refer to the *Service without selectors*
    section in [Chapter 3](a5cf080a-372a-406e-bb48-019af313c676.xhtml), *Getting Started
    with Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how does traffic get from pod to the pod behind the service? By default,
    Kubernetes uses `iptables` to perform this magic, and does so by using `kube-proxy`.
    This is explained in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db7f1486-dc97-4352-8dd9-05253454ae78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s reuse the `3-2-3_rc1.yaml` and `3-2-3_nodeport.yaml` examples from [Chapter
    3](a5cf080a-372a-406e-bb48-019af313c676.xhtml), *Getting Started with Kubernetes*,
    to observe the default behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s observe the `iptable` rules and see how this works. As you can see in
    the following code, our service IP is `10.0.0.167`. The two pods'' IP addresses
    underneath are `10.1.0.4` and `10.1.0.5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s get into the minikube node by using `minikube ssh` and check its `iptables`
    rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The key point here is that the service exposes the cluster IP to outside traffic
    from `KUBE-SVC-37ROJ3MK6RKFMQ2B`, which links to two custom chains, `KUBE-SEP-SVVBOHTYP7PAP3J5`
    and `KUBE-SEP-AYS7I6ZPYFC6YNNF`, with a statistic mode random probability of 0.5\.
    This means that `iptables` will generate a random number and tune it based on
    the probability distribution of 0.5 to the destination. These two custom chains
    have the `DNAT` target set to the corresponding pod IP. The `DNAT` target is responsible
    for changing the packets' destination IP address. By default, conntrack is enabled
    to track the destination and source of connection when the traffic comes in. All
    of this results in a routing behavior. When the traffic comes to the service,
    `iptables` will randomly pick one of the pods to route and modify the destination
    IP from the service IP to the real pod IP. It then gets the response and un-DNAT
    on the reply packets and sends them back to the requester.
  prefs: []
  type: TYPE_NORMAL
- en: 'IPVS-based kube-proxy:'
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes 1.11, the IPVS-based `kube-proxy` feature graduated to GA. This
    could deal with the scaling problem of `iptables` to tens of thousands of services.
    The **IP Virtual Server** (**IPVS**) is part of the Linux kernel, which can direct
    TCP or UDP requests to real servers. `ipvs` proxier is a good fit if your application
    contains a huge number of services. However, it will fall back on `iptables` in
    some specific cases. For more information, please refer to [https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/](https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/).
  prefs: []
  type: TYPE_NORMAL
- en: External-to-service communications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ability to serve external traffic to Kubernetes is critical. Kubernetes
    provides two API objects to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Service**: External network LoadBalancer or NodePort (L4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ingress**: HTTP(S) LoadBalancer (L7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ll learn more about ingress in the next section. For now, we''ll focus
    on the L4 service. Based on what we''ve learned about pod-to-pod communication
    across nodes, the packet goes in and out between the service and pod. The following
    diagram is an illustration of this process. Let''s say we have two services: service
    A has three pods (pod a, pod b, and pod c) and service B gets only one pod (pod
    d). When the traffic comes in from the LoadBalancer, the packet will be dispatched
    to one of the nodes. Most of the LoadBalancer cloud itself is not aware of pods
    or containers; it only knows about the node. If the node passes the health check,
    then it will be the candidate for the destination.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that we want to access service B; this currently only has one
    pod running on one node. However, LoadBalancer sends the packet to another node
    that doesn''t have any of our desired pods running. In this case, the traffic
    route will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/baef9510-6fb1-4435-9ae0-862dc2d234bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The packet routing journey will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: LoadBalancer will choose one of the nodes to forward to the packet. In GCE,
    it selects the instance based on a hash of the source IP and port, destination
    IP and port, and protocol. In AWS, load balancing is based on a round-robin algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, the routing destination will be changed to pod d (DNAT) and will forward
    the packet to the other node, similar to pod-to-pod communication across nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then comes service-to-pod communication. The packet arrives at pod d and pod
    d returns the response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pod-to-service communication is manipulated by `iptables` as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The packet will be forwarded to the original node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The source and destination will be un-DNAT to the LoadBalancer and client, and
    will be sent all the way back to the requester.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From Kubernetes 1.7, there is a new attribute in this service called `externalTrafficPolicy`.
    Here, you can set its value to local, and then, after the traffic goes into a
    node, Kubernetes will route the pods on that node if possible, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**`kubectl patch $service_name nodeport -p ''{"spec":{"externalTrafficPolicy":"Local"}}''`**'
  prefs: []
  type: TYPE_NORMAL
- en: Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pods and services in Kubernetes have their own IPs. However, this is normally
    not the interface you'd provide to the external internet. Though there is a service
    with a node IP configured, the port in the node IP can't be duplicated among the
    services. It is cumbersome to decide which port to manage with which service.
    Furthermore, the node comes and goes; it wouldn't be clever to provide a static
    node IP to an external service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ingress defines a set of rules that allows the inbound connection to access
    Kubernetes cluster services. This brings the traffic into the cluster at L7, and
    allocates and forwards a port on each VM to the service port. This is shown in
    the following diagram. We define a set of rules and post them as source type ingress
    to the API server. When the traffic comes in, the ingress controller will then
    fulfill and route the ingress according to the ingress rules. As shown in the
    following diagram, ingress is used to route external traffic to the kubernetes
    endpoints by different URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48b60b52-c612-47c2-9f77-169aa3f41111.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will go through an example and see how this works. In this example,
    we''ll create two services named `nginx` and `echoserver`, with the ingress paths `/welcome`
    and `/echoserver` configured. We can run this in `minikube`. The old version of
    `minikube` doesn''t enable ingress by default; we''ll have to enable it first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Enabling ingress in `minikube` will create an `nginx` ingress controller ([https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx))
    and a `ConfigMap` to store `nginx` configuration, as well as a `Deployment` and
    service as default HTTP backends for handling unmapped requests. We can observe
    these by adding `--namespace=kube-system` in the `kubectl` command. Next, let''s
    create our backend resources. Here is our `nginx` `Deployment` and `Service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll create another service with `Deployment`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll create the ingress resource. There is an annotation named `nginx.ingress.kubernetes.io/rewrite-target`.
    This is required if the `service` requests are coming from the root URL. Without
    a rewrite annotation, we''ll get 404 as a response. Refer to [https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/rewrite](https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/rewrite) for
    more annotation that''s supported in the `nginx` ingress controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In some cloud providers, the service LoadBalancer controller is supported. This
    can be integrated with ingress via the `status.loadBalancer.ingress` syntax in
    the configuration file. For more information, refer to [https://github.com/kubernetes/contrib/tree/master/service-loadbalancer](https://github.com/kubernetes/contrib/tree/master/service-loadbalancer).
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our host is set to `devops.k8s`, it will only return if we access it
    from that hostname. You could either configure the DNS record in the DNS server,
    or modify the host''s file in local. For simplicity, we''ll just add a line with
    the `ip hostname` format in the host file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we should be able to access our service by the URL directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The pod ingress controller dispatches the traffic based on the URL's path. The
    routing path is similar to external-to-service communication. The packet hops
    between nodes and pods. Kubernetes is pluggable; lots of third-party implementation
    is going on. We have only scratched the surface here, even though `iptables` is
    just a default and common implementation. Networking evolves a lot with every
    single release. At the time of writing, Kubernetes had just released version 1.13.
  prefs: []
  type: TYPE_NORMAL
- en: Network policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The network policy works as a software firewall to the pods. By default, every
    pod can communicate with each other without any boundaries. The network policy
    is one of the isolations you could apply to these pods. This defines who can access
    which pods in which port by namespace selector and pod selector. The network policy
    in a namespace is additive, and once a pod enables the network policy, it denies
    any other ingress (also known as deny all).
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, there are multiple network providers that support the network policy,
    such as Calico ([https://www.projectcalico.org/calico-network-policy-comes-to-kubernetes/](https://www.projectcalico.org/calico-network-policy-comes-to-kubernetes/)),
    Romana ([https://github.com/romana/romana](https://github.com/romana/romana)),
    Weave Net ([https://www.weave.works/docs/net/latest/kube-addon/#npc](https://www.weave.works/docs/net/latest/kube-addon/#npc))),
    Contiv ([http://contiv.github.io/documents/networking/policies.html](http://contiv.github.io/documents/networking/policies.html))),
    and Trireme ([https://github.com/aporeto-inc/trireme-kubernetes](https://github.com/aporeto-inc/trireme-kubernetes)).
    Users are free to choose between any of these. For the purpose of simplicity,
    though, we''re going to use Calico with minikube. To do that, we''ll have to launch
    minikube with the `--network-plugin=cni` option. The network policy is still pretty
    new in Kubernetes at this point. We''re running Kubernetes version v.1.7.0 with
    the v.1.0.7 minikube ISO to deploy Calico by self-hosted solution ([http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/hosted/](http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/hosted/)).
    Calico can be installed with etcd datastore or the Kubernetes API datastore. For
    convenience, we''ll demonstrate how to install Calico with the Kubernetes API
    datastore here. Since rbac is enabled in minikube, we''ll have to configure the
    roles and bindings for Calico:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s deploy Calico:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After doing this, we can list the Calico pods and see whether it''s launched
    successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s reuse `6-2-1_nginx.yaml` for our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We will find that our `nginx` service has an IP address of `10.96.51.143`.
    Let''s launch a simple bash and use `wget` to see whether we can access our `nginx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `--spider` parameter is used to check whether the URL exists. In this case,
    `busybox` can access `nginx` successfully. Next, let''s apply a `NetworkPolicy`
    to our `nginx` pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We can see some important syntax here. The `podSelector` is used to select pods
    that should match the labels of the target pod. Another one is `ingress[].from[].podSelector`,
    which is used to define who can access these pods. In this case, all the pods
    with `project=chapter6` labels are eligible to access the pods with `server=nginx`
    labels. If we go back to our busybox pod, we're unable to contact `nginx` any
    more because, right now, the `nginx` pod has `NetworkPolicy` on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, it is deny all, so busybox won''t be able to talk to `nginx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We can use `kubectl edit deployment busybox` to add the `project=chapter6` label
    in busybox pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we can contact the `nginx` pod again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'With the help of the preceding example, we now have an idea of how to apply
    a network policy. We could also apply some default polices to deny all, or allow
    all, by tweaking the selector to select nobody or everybody. For example, the
    deny all behavior can be achieved as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This way, all pods that don''t match labels will deny all other traffic. Alternatively,
    we could create a `NetworkPolicy` whose ingress is listed everywhere. By doing
    this, the pods running in this namespace can be accessed by anyone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Service mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A service mesh is an infrastructure layer for handling service-to-service communication.
    Especially in the microservice world, the application at hand might contain hundreds
    of thousands of services. The network topology can be very complicated here. A
    service mesh can provide the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Traffic management (such as A/B testing and canary deployment)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security (such as TLS and key management)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observability (such as providing traffic visibility. This is easy to integrate
    with monitoring systems such as Prometheus ([https://prometheus.io/](https://prometheus.io/)),
    tracing systems such as Jaeger ([https://www.jaegertracing.io](https://www.jaegertracing.io))
    or Zipkin ([https://github.com/openzipkin/zipkin](https://github.com/openzipkin/zipkin)),
    and logging systems)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two major service mesh implementations on the market—Istio ([https://istio.io](https://istio.io))
    and Linkerd ([https://linkerd.io](https://linkerd.io)). Both of these deploy network proxy
    containers alongside the application container (the so-called sidecar container)
    and provide Kubernetes support. The following diagram is a simplified common architecture
    of the service mesh:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9227a449-08f6-4594-80bf-ca003f10c738.png)'
  prefs: []
  type: TYPE_IMG
- en: A service mesh normally contains a control plane, which is the brain of the
    mesh. This can manage and enforce the policies for route traffic, as well as collect
    telemetry data that can be integrated with other systems. It also carries out
    identity and credential management for services or end users. The service mesh
    sidecar container, which acts as a network proxy, lives side by side with the
    application container. The communication between services is passed through the
    sidecar container, which means that it can control the traffic by user-defined
    policies, secure the traffic via TLS encryption, do the load balancing and retries,
    control the ingress/egress, collect the metrics, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following section, we''ll use Istio as the example, but you''re free
    to use any implementation in your organization. First, let''s get the latest version
    of Istio. At the time of writing, the latest version is 1.0.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s create a **Custom Resource Definition** (**CRD**) for Istio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following example, we''re installing Istio with default mutual TLS authentication.
    The resource definition is under the `install/kubernetes/istio-demo-auth.yaml` file.
    If you''d like to deploy it without TLS authentication, you can use `install/kubernetes/istio-demo.yaml` instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'After deployment, let''s check that the services and pods have all been deployed
    successfully into the `istio-system` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'After waiting for a few minutes, check that the pods are all in `Running` and
    `Completed` states, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we have `istio-sidecar-injector` deployed, we can simply use `kubectl
    label namespace default istio-injection=enabled` to enable the sidecar container
    injection for every pod in the `default` namespace. `istio-sidecar-injector` acts
    as a mutating admission controller, which will inject the sidecar container to
    the pod if the namespace is labelled with `istio-injection=enabled`. Next, we
    can launch a sample application from the `samples` folder. Helloworld demonstrates
    the use of canary deployment ([https://en.wikipedia.org/wiki/Deployment_environment](https://en.wikipedia.org/wiki/Deployment_environment)),
    which will distribute the traffic to the helloworld-v1 and helloworld-v2 services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If we inspect one of the pods, we''ll find that the `istio-proxy` container
    was injected into it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Taking a closer look, we can see that the `istio-proxy` container was launched
    with the configuration of its control plane address, tracing system address, and
    connection configuration. Istio has now been verified. There are lots of to do
    with Istio traffic management, which is beyond of the scope of this book. Istio
    has a variety of detailed samples for us to try, which can be found in the `istio-1.0.5/samples`
    folder that we just downloaded.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how containers communicate with each other. We also
    introduced how pod-to-pod communication works. A service is an abstraction that
    routes traffic to any of the pods underneath it if the label selectors match.
    We also learned how a service works with a pod using `iptables`. We also familiarized
    ourselves with how packet routes from external services to a pod using DNAT and
    un-DAT packets. In addition to this, we looked at new API objects such as ingress,
    which allows us to use the URL path to route to different services in the backend.
    In the end, another `NetworkPolicy` object was introduced. This provides a second
    layer of security, and acts as a software firewall rule. With the network policy,
    we can make certain pods communicate with certain other pods. For example, only
    data retrieval services can talk to the database container. In the last section,
    we got a glimpse at Istio, one of the popular implementations of service mesh. All
    of these things make Kubernetes more flexible, secure, robust, and powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Before this chapter, we covered the basic concepts of Kubernetes. In [Chapter
    7](9a41a50b-33a5-4ec1-9e40-be08c9ccb1ae.xhtml), *Monitoring and Logging*, we'll
    get a clearer understanding of what is happening inside your cluster by monitoring
    cluster metrics and analyzing applications and system logs for Kubernetes. Monitoring
    and logging tools are essential for every DevOps, which also plays an extremely
    important role in dynamic clusters such as Kubernetes. Consequently, we'll get
    an insight into the activities of the cluster, such as scheduling, deployment,
    scaling, and service discovery. [Chapter 7](9a41a50b-33a5-4ec1-9e40-be08c9ccb1ae.xhtml),
    *Monitoring and Logging*, will help you to better understand the act of operating
    Kubernetes in the real world.
  prefs: []
  type: TYPE_NORMAL
