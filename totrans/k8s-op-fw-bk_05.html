<html><head></head><body>
		<div id="_idContainer018">
			<h1 id="_idParaDest-51"><em class="italic"><a id="_idTextAnchor050"/>Chapter 3</em>: Designing an Operator – CRD, API, and Target Reconciliation</h1>
			<p>The lessons from the previous chapters have helped us understand the foundational basics of the Operator Framework. In <a href="B18147_01_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introducing the Operator Framework</em>, we covered the conceptual pillars of the Operator Framework and the purposes they serve. Then, in <a href="B18147_02_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, <em class="italic">Understanding How Operators Interact with Kubernetes</em>, we discussed some general principles of software design in the context of <strong class="bold">Kubernetes</strong> and the <strong class="bold">Operator Framework</strong>. Together, these chapters have established a baseline understanding of Operators and their development in broad terms. Now, we will be applying this knowledge with examples and begin designing our own Operator.</p>
			<p>We'll start by defining a simple problem that our Operator is going to solve. In this case, that will be managing a basic deployment of an application with a single Pod. Over the next few chapters, we will add functionality to this Operator with specific code examples, but before we can start coding our sample Operator, we must first walk through the design process. Building on the generic definitions and steps we've already discussed with a concrete example will provide a context with which to frame the earlier lessons in practical terms.</p>
			<p>This process will cover a few different steps in order to lay out the core aspects of our Operator. These will include drawing out the <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>), <strong class="bold">CustomResourceDefinitions</strong> (<strong class="bold">CRDs</strong>), and reconciliation logic that will make our Operator work. Along the way, these steps will be related back to the lessons discussed earlier and industry-standard best practices for Operators. We'll break this process into the following steps:</p>
			<ul>
				<li>Describing the problem</li>
				<li>Designing an API and a CRD</li>
				<li>Working with other required resources</li>
				<li>Designing a target reconciliation loop</li>
				<li>Handling upgrades and downgrades</li>
				<li>Using failure reporting</li>
			</ul>
			<p>We won't start writing any actual code yet besides some <strong class="bold">YAML Ain't Markup Language</strong> (<strong class="bold">YAML</strong>) snippets where applicable. However, we will use some pseudocode to better visualize how our actual code will work once we initialize the project with the Operator <strong class="bold">software development kit</strong> (<strong class="bold">SDK</strong>) in <a href="B18147_04_ePub.xhtml#_idTextAnchor066"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing an Operator with the Operator SDK</em>.</p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/>Describing the problem</h1>
			<p>Many software projects can be defined with a user <a id="_idIndexMarker122"/>story in the following format: <em class="italic">As a [user] I want to [action], so that [reason].</em> We'll also do that here, as follows:</p>
			<p class="author-quote"><em class="italic">As a cluster administrator, I want to use an Operator to manage my nginx application so that its health and monitoring are automatically managed for me.</em></p>
			<p>For our use case (designing an Operator), we don't care about the specific application right now. For that reason, our <em class="italic">application</em> is just going to be a basic nginx sample Pod. We will assume that this represents any single-Pod application with basic <strong class="bold">input/output</strong> (<strong class="bold">I/O</strong>) needs. While this may seem too abstract, the focus will be on building an Operator around the application.</p>
			<p>The first thing we have identified from this user story is that we will be building the Operator for cluster administrators. From the previous chapter, we know this means that the Operator's users will have a better understanding of the cluster architecture than most end users and that they will need higher levels of direct control over the Operand. We can also assume that most cluster administrators will be comfortable interacting directly with the Operator rather than through an intermediary frontend application.</p>
			<p>The second part of this user story identifies the functional objective of the Operator. Specifically, this Operator is going to manage the Deployment of a single-Pod application. In this case, <em class="italic">manage</em> is a vague term that we will assume to mean create and maintain the required Kubernetes resources to run the application. These resources will be, at minimum, a Deployment. We will need to expose some of the options of this Deployment through the Operator, such as the container port for nginx.</p>
			<p>Finally, the user story provides our motivation for running the Operator. The cluster administrators want to have the application's <em class="italic">health and monitoring</em> managed by the Operator. Application health can mean a lot of different things, but generally, this comes down to maintaining high uptime for the application and recovering from any crashes, if possible. Monitoring the application can also be done in a number of ways, usually in the form of metrics.</p>
			<p>So, from all of the preceding information, we have identified that we want a very basic Operator that can do the following:</p>
			<ul>
				<li>Deploy an application</li>
				<li>Keep the application running if it fails</li>
				<li>Report on the health status of the application</li>
			</ul>
			<p>These are some of the<a id="_idIndexMarker123"/> simplest functions that an Operator can serve. In later chapters, we'll build on these requests a bit more. But in the interest of starting with a solid foundation upon which to iterate later, this will be<a id="_idIndexMarker124"/> our <strong class="bold">minimum viable product</strong> (<strong class="bold">MVP</strong>). Henceforward, therefore, this is the basic Operator design we will be referencing when referring to our examples.</p>
			<p>Based on these criteria, we can try to define our Operator in terms of the Capability Model covered in <a href="B18147_01_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introducing the Operator Framework</em> (recall that the Capability Model defines five levels of Operator functionality, from <em class="italic">Basic Install</em> to <em class="italic">Auto Pilot</em>). We know that the Operator will be able to install the Operand, as well as manage any additional required resources. We would like it to be able to report on the status of the Operand as well and provide configuration through its CRD. These are all the criteria for a Level I Operator. In addition, it would be great if our Operator could handle upgrades to qualify it as a Level II Operator. </p>
			<p>This is a good start for the initial Operator design. With a full picture of the problem we are trying to solve, we can now begin to brainstorm how we will solve it. To do that, we can start by designing how our Operator will be represented in the cluster API.</p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/>Designing an API and a CRD</h1>
			<p>As we<a id="_idIndexMarker125"/> covered in <a href="B18147_01_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introducing the Operator Framework</em>, and<a id="_idIndexMarker126"/> <a href="B18147_02_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, <em class="italic">Understanding How Operators Interact with Kubernetes</em>, the use of a CRD is a defining characteristic of Operators to create an object for users to interact with. This object creates an interface for controlling the Operator. In this way, the <strong class="bold">Custom Resource</strong> (<strong class="bold">CR</strong>) object is a window into<a id="_idIndexMarker127"/> the Operator's main functions.</p>
			<p>As with any good window, the Operator's CRD must be built well. It must be clear enough to expose the<a id="_idIndexMarker128"/> details of the Operator while being secure enough to keep out harsh weather <a id="_idIndexMarker129"/>and burglars, and as with a window, the CRD's design should follow local building codes to ensure that it is built up to the expected standards of the environment. In our case, those building codes are the Kubernetes API conventions.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor053"/>Following the Kubernetes API design conventions</h2>
			<p>Even though a CRD<a id="_idIndexMarker130"/> is a custom object that can be created by anyone, there are still best practices to keep in mind. This is because the CRD exists within the Kubernetes API, which defines its conventions so that there are certain expectations when interacting with the API. These conventions are documented in the Kubernetes community at <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md">https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md</a>. However, this documentation is extensive and covers the requirements for all kinds of API objects, not just Operator CRDs. There are, however, a few key elements that are relevant to our purpose, as outlined here (Note: some of these fields will be discussed in more detail later in the chapter):</p>
			<ul>
				<li>All API objects must have two fields: <strong class="source-inline">kind</strong> and <strong class="source-inline">apiVersion</strong>. These fields make it possible for Kubernetes API clients to decode the object. <strong class="source-inline">Kind</strong> represents the name of the object type—for example, <strong class="source-inline">MyOperator</strong>—and <strong class="source-inline">apiVersion</strong> is, aptly, the version of the API for that object. For example, your Operator may ship API versions <strong class="source-inline">v1alpha1</strong>, <strong class="source-inline">v1beta1</strong>, and <strong class="source-inline">v1</strong>.</li>
				<li>API objects should also have the following fields (though they are not required):<ul><li><strong class="source-inline">resourceVersion</strong> and <strong class="source-inline">generation</strong>, both of which help to track changes to an object. However, these fields serve different purposes. The <strong class="source-inline">resourceVersion</strong> field is an internal reference that is incremented every time an object is modified, which serves to help with concurrency control. For example, when trying to update an Operand Deployment, you will make two client calls: <strong class="source-inline">Get()</strong> and <strong class="source-inline">Update()</strong>. When calling <strong class="source-inline">Update()</strong>, the API can detect if the object's <strong class="source-inline">resourceVersion</strong> field has changed on the server (which would indicate that another controller has modified the object before we updated it) and reject the update. In contrast, <strong class="source-inline">generation</strong> serves to keep track of the relevant updates to an object. For example, a Deployment that has recently rolled out a new version would have its <strong class="source-inline">generation</strong> field updated. These values can be used to reference older generations or ensure that new ones are of the expected generation number (that is, <em class="italic">current+1</em>).</li><li><strong class="source-inline">creationTimestamp</strong> and <strong class="source-inline">deletionTimestamp</strong>, which serve as helpful reference points for the age of an object. With <strong class="source-inline">creationTimestamp</strong>, for example, you can easily reference the age of an Operator's Deployment based on when its CRD was created. Similarly, <strong class="source-inline">deletionTimestamp</strong> serves to indicate that a deletion request has been sent to the API server for that object.</li><li><strong class="source-inline">labels</strong> and <strong class="source-inline">annotations</strong>, which serve similar purposes but are semantically different. Applying <strong class="source-inline">labels</strong> to an object serves to organize objects by criteria that are easily filtered through the API. On the other hand, <strong class="source-inline">annotations</strong> exposes metadata about the object.</li></ul></li>
				<li>API objects should have <strong class="source-inline">spec</strong> and <strong class="source-inline">status</strong> fields. We will cover <strong class="source-inline">status</strong> in more detail later in the chapter (under <em class="italic">Using failure reporting</em>), but for now, there are some conventions around it to keep in mind, as outlined here:<ul><li>Conditions reported in an object's <strong class="source-inline">status</strong> field should be clearly self-explanatory without the need for additional context to understand them.</li><li>Conditions should respect the same API compatibility rules as any other field. In order to<a id="_idIndexMarker131"/> maintain backward compatibility, condition definitions should not change once defined.</li><li>Conditions can report either <strong class="source-inline">True</strong> or <strong class="source-inline">False</strong> as their normal operating state. There is no set guideline for which one should be the standard mode; it is up to the developer to consider readability in the definition of the condition. For example, a <strong class="source-inline">Ready=true</strong> condition can have the same meaning as one called <strong class="source-inline">NotReady=false</strong>, but the former is much easier to understand.</li><li>Conditions should represent the current known state of the cluster, rather than reporting transitions between states. As we will cover in the <em class="italic">Designing a target reconciliation loop</em> section, many Kubernetes controllers are written with a level-triggered design (meaning they operate based on the current state of the cluster rather than incoming events alone). So, an Operator reporting conditions based on its current state helps to maintain this mutual design assumption of being able to build the current state of the cluster in memory at any time. However, for long transitionary phases, the <strong class="source-inline">Unknown</strong> condition can be used if necessary.</li></ul></li>
				<li>Sub-objects within an API object <a id="_idIndexMarker132"/>should be represented as lists, rather than maps; for example, our nginx deployment may need several different named ports to be specified through the Operator CRD. This means that they should be represented as a list, with each item in the list having fields for <strong class="source-inline">name</strong> and <strong class="source-inline">port</strong>, as opposed to a map where the key for each entry is the <strong class="source-inline">name</strong> of the port. </li>
				<li>Optional fields should be implemented as pointer values in order to easily distinguish between zero and unset values.</li>
				<li>The convention for fields with units is to include the units in the field name—for example, <strong class="source-inline">restartTimeoutSeconds</strong>.</li>
			</ul>
			<p>These are just some of the many API conventions, but they are important to know as you design your Operator's CRD and API. Adhering to the guidelines of API design ensures that other components in the Kubernetes ecosystem (including the platform itself) can make appropriate assumptions about your Operator. With these guidelines in mind, we can move on to the next step of designing our own CRD schema.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor054"/>Understanding a CRD schema</h2>
			<p>We have already discussed CRDs<a id="_idIndexMarker133"/> and their importance to Operators, but up until this point, we haven't looked in depth at how a CRD is composed. Now that we know the problem our example Operator is going to solve, we can begin looking at the options we want to expose through its CRD and get an idea of what those will look like to users.</p>
			<p>First, it is best to see an example CRD and examine each section to understand its purpose, as follows:</p>
			<pre class="source-code">apiVersion: apiextensions.k8s.io/v1</pre>
			<pre class="source-code">kind: CustomResourceDefinition</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: myoperator.operator.example.com</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  group: operator.example.com</pre>
			<pre class="source-code">  names:</pre>
			<pre class="source-code">    kind: MyOperator</pre>
			<pre class="source-code">    listKind: MyOperatorList</pre>
			<pre class="source-code">    plural: myoperators</pre>
			<pre class="source-code">    singular: myoperator</pre>
			<pre class="source-code">  scope: Namespaced</pre>
			<pre class="source-code">  versions:</pre>
			<pre class="source-code">    - name: v1alpha1</pre>
			<pre class="source-code">      schema:</pre>
			<pre class="source-code">        openAPIV3Schema:</pre>
			<pre class="source-code">         ...</pre>
			<pre class="source-code">      served: true</pre>
			<pre class="source-code">      storage: true</pre>
			<pre class="source-code">      subresources:</pre>
			<pre class="source-code">        status: {}</pre>
			<pre class="source-code">status:</pre>
			<pre class="source-code">  acceptedNames:</pre>
			<pre class="source-code">    kind: ""</pre>
			<pre class="source-code">    plural: ""</pre>
			<pre class="source-code">  conditions: []</pre>
			<pre class="source-code">  storedVersions: []</pre>
			<p>The first two fields, <strong class="source-inline">apiVersion</strong> and <strong class="source-inline">kind</strong>, define that this is a CRD. Even though we are trying to define a blueprint of our own custom objects, that blueprint must exist within a <strong class="source-inline">CustomResourceDefinition</strong> object first. From this, the API server will know how to parse the CRD data to create instances of our CR.</p>
			<p>Next, the <strong class="source-inline">metadata.Name</strong> field defines<a id="_idIndexMarker134"/> the name of our CRD (not the custom object created from the CRD). To be more specific, this is the name of the blueprint, not the objects created from the blueprint. For example, we could retrieve this CRD design using <strong class="source-inline">kubectl get crd/myoperator.operator.example.com</strong>.</p>
			<p>Within <strong class="source-inline">spec</strong> is where the CRD begins to actually define the CR objects we want to create. The <strong class="source-inline">group</strong> defines a custom API group to which new objects will belong. Using a unique name here helps to prevent collisions with other objects and APIs in the cluster. </p>
			<p>The <strong class="source-inline">names</strong> section defines different ways in which our objects can be referenced. Here, only <strong class="source-inline">kind</strong> and <strong class="source-inline">plural</strong> are required (as the others can be inferred from these two). Just as any other type of object in the cluster is accessible via its <strong class="source-inline">kind</strong> or <strong class="source-inline">plural</strong> form (for example, <strong class="source-inline">kubectl get pods</strong>), our CRs will be accessible the same way with commands such as <strong class="source-inline">kubectl edit myoperator/foo</strong>. Even though most Operators will (and should) only have one CR object in the cluster, these fields are still required.</p>
			<p>Next, <strong class="source-inline">scope</strong> defines custom objects as namespace- or cluster-scoped. The differences between these two were covered in detail in <a href="B18147_01_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introducing the Operator Framework</em>. The available options for this field are <strong class="source-inline">Cluster</strong> and <strong class="source-inline">Namespaced</strong>.</p>
			<p><strong class="source-inline">Versions</strong> provides a list of the different API versions that will be available for our CR. As your Operator evolves over time and new features are added or removed, you will need to introduce new versions of the Operator's CR. For backward compatibility and support, you should continue to ship older versions of the resource to provide users a transitory period after which the version can be safely deprecated. This is why this field provides a list of versions. The API server is aware of each version and can operate effectively on an object that is created<a id="_idIndexMarker135"/> and used in any available version in this list.</p>
			<p>Each version in the list contains schematic information about the object itself that uniquely identifies the structure of that version in <strong class="source-inline">openAPIV3Schema</strong>. In this example, the <strong class="source-inline">openAPIV3Schema</strong> section has been intentionally omitted. We have done that because this section is usually very long and complex. However, in recent versions of Kubernetes, this section is required in <a id="_idIndexMarker136"/>order to provide a <strong class="bold">structural schema</strong> for the CRD.</p>
			<p>A structural schema is an object schema that is based on <strong class="bold">OpenAPI version 3 (V3) validation</strong>. OpenAPI defines validation rules for each field that can be used to validate field data when objects are <a id="_idIndexMarker137"/>created or updated. These validation rules include the type of data for the field, as well as other information such as allowed string patterns and enumerated values. The structural schema requirement for CRDs ensures consistent, reliably stored representations of the objects.</p>
			<p>Due to the complex nature of OpenAPI <a id="_idIndexMarker138"/>validation schemas, it is not recommended to write them by hand. Instead, the use of generated tools such as <strong class="bold">Kubebuilder</strong> (which is used by the Operator SDK) is recommended. Extensive and flexible validation rules can be defined directly on the Go types for CRDs using the various Kubebuilder markers, which are available for full reference at <a href="https://book.kubebuilder.io/reference/generating-crd.html">https://book.kubebuilder.io/reference/generating-crd.html</a>.</p>
			<p>The next sections of the individual version definitions are <strong class="source-inline">served</strong> and <strong class="source-inline">storage</strong>, which set whether this version is served via REST APIs and if this is the version that should be used as the storage representation. Only one version can be set as the storage version for a CRD.</p>
			<p>The final sections, <strong class="source-inline">subresources</strong> and <strong class="source-inline">status</strong>, are related because they define a <strong class="source-inline">status</strong> field that will be used to report information on the current state of the Operator. We will cover that field and its uses in more detail under <em class="italic">Using failure reporting</em>.</p>
			<p>Now that we have explored the structure of a CRD and have an idea of what one should look like, we can design one for our example nginx Operator.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor055"/>Example Operator CRD</h2>
			<p>From the preceding problem statement, we <a id="_idIndexMarker139"/>know that our Operator is initially going to simply deploy an instance of nginx in the cluster. We also now know that our Operator's CRD will provide a <strong class="source-inline">spec</strong> field with various options to control the Operand Deployment. But what kind of settings should we expose? Our Operand is fairly simple, so let's start with a few basic options that we are defining to configure a simple nginx Pod, as follows:</p>
			<ul>
				<li><strong class="source-inline">port</strong>—This will be the port number that we want to expose the nginx Pod on within the cluster. Since nginx is a web server, this will allow us to modify the accessible port without having to directly touch the nginx Pod, because the Operator will handle safely changing it for us.</li>
				<li><strong class="source-inline">replicas</strong>—This is a bit redundant because the number of replicas for any Kubernetes Deployment can be <a id="_idIndexMarker140"/>adjusted through the Deployment itself. But in the interest of abstracting control of the Operand away into a <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>) behind the management of an Operator, we will provide this option. This way, an administrator (or other application) can scale the Operand with the added handling and reporting of our Operator.</li>
				<li><strong class="source-inline">forceRedeploy</strong>—This field is interesting<a id="_idIndexMarker141"/> because it will effectively be a <strong class="bold">no-operation</strong> (<strong class="bold">no-op</strong>) against the Operand in terms of how it functions. However, including a field in the Operator CRD that can be set to any arbitrary value allows us a way to instruct the Operator to trigger a new rollout of the Operand without having to modify any actual settings. Having that functionality is useful for stuck Deployments where manual intervention can resolve the issue. </li>
			</ul>
			<p>This works because the Operator watches for changes to relevant resources in the cluster, one of which being its own CRD (more on this in the <em class="italic">Designing a target reconciliation loop</em> section). This watch is necessary so that the Operator knows when to update an Operand. So, including a no-op field can be enough for the Operator to know to redeploy the Operand without needing to make any actual changes.</p>
			<p>These three settings together will make the basis of our Operator's CRD <strong class="source-inline">spec</strong>. With this, we know that as an object in the cluster the CR object will look something like this:</p>
			<pre class="source-code">apiVersion: v1alpha1</pre>
			<pre class="source-code">kind: NginxOperator</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: instance</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  port: 80</pre>
			<pre class="source-code">  replicas: 1</pre>
			<pre class="source-code">status:</pre>
			<pre class="source-code">  ...</pre>
			<p>Note that this is the CR itself, not the CRD, as shown in an example earlier. We are using a generic <strong class="source-inline">name: instance</strong>  value <a id="_idIndexMarker142"/>here because we will probably only have one instance of the Operator running in a namespace at a time. We also haven't included the <strong class="source-inline">forceRedeploy</strong> field here because that will be optional.</p>
			<p>This object could be retrieved with the <strong class="source-inline">kubectl get -o yaml nginxoperator/instance</strong> command if we define our CRD right. Thankfully, the Operator SDK and Kubebuilder will help us generate that.</p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor056"/>Working with other required resources</h1>
			<p>Besides the CRD, our Operator <a id="_idIndexMarker143"/>will be responsible for managing a number of other cluster resources as well. Right now, this is the nginx Deployment that will be created as our Operand, as well as a ServiceAccount, Role, and RoleBinding for the Operator. What we need to understand is how the Operator will know the definition of those resources.</p>
			<p>Somewhere, the resources need to be written as Kubernetes cluster objects. Just as you would create a Deployment by hand (for example, with <strong class="source-inline">kubectl create -f</strong>), the definitions of necessary resources can be packaged with the Operator code in a couple of different ways. This can be done easily with templates if you are creating your Operator with Helm or Ansible, but for Operators written in <strong class="bold">Go</strong>, we need to consider our options.</p>
			<p>One way to package these resources so that the Operator can create them is by defining them directly in the Operator's code. All Kubernetes objects are based on corresponding Go type definitions, so we have the <a id="_idIndexMarker144"/>ability to create Deployments (or any resource, for that matter) directly in the Operator by declaring the resources as variables. Here's an example of this:</p>
			<pre class="source-code">…</pre>
			<pre class="source-code">import appsv1 "k8s.io/api/apps/v1"</pre>
			<pre class="source-code">…</pre>
			<pre class="source-code">nginxDeployment := &amp;appsv1.Deployment{</pre>
			<pre class="source-code">  TypeMeta: metav1.TypeMeta{</pre>
			<pre class="source-code">    Kind: "Deployment",</pre>
			<pre class="source-code">    apiVersion: "apps/v1",</pre>
			<pre class="source-code">  },</pre>
			<pre class="source-code">  ObjectMeta: metav1.ObjectMeta{</pre>
			<pre class="source-code">    Name: "nginx-deploy",</pre>
			<pre class="source-code">    Namespace: "nginx-ns",</pre>
			<pre class="source-code">  },</pre>
			<pre class="source-code">  Spec: appsv1.DeploymentSpec{</pre>
			<pre class="source-code">    Replicas: 1</pre>
			<pre class="source-code">    Selector: &amp;metav1.LabelSelector{</pre>
			<pre class="source-code">      MatchLabels: map[string]string{"app":"nginx"},</pre>
			<pre class="source-code">    },</pre>
			<pre class="source-code">    Template: v1.PodTemplateSpec{</pre>
			<pre class="source-code">      Spec: v1.PodSpec{</pre>
			<pre class="source-code">        ObjectMeta: metav1.ObjectMeta{</pre>
			<pre class="source-code">          Name: "nginx-pod",</pre>
			<pre class="source-code">          Namespace: "nginx-ns",</pre>
			<pre class="source-code">          Labels: map[string]string{"app":"nginx"},</pre>
			<pre class="source-code">        },</pre>
			<pre class="source-code">        Containers: []v1.Container{</pre>
			<pre class="source-code">          {</pre>
			<pre class="source-code">             Name: "nginx",</pre>
			<pre class="source-code">             Image: "nginx:latest",</pre>
			<pre class="source-code">             Ports: []v1.ContainerPort{{ContainerPort: int32(80)}},</pre>
			<pre class="source-code">          },</pre>
			<pre class="source-code">        },</pre>
			<pre class="source-code">      },</pre>
			<pre class="source-code">    },</pre>
			<pre class="source-code">  },</pre>
			<pre class="source-code">}</pre>
			<p>The convenience of defining objects in code in this way is helpful for development. This approach provides a transparent definition that is clearly available and immediately usable by the Kubernetes <a id="_idIndexMarker145"/>API clients. However, there are some drawbacks to this. First, it is not very human-readable. Users will be familiar with interacting with Kubernetes objects represented as YAML or <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>), wherein the type<a id="_idIndexMarker146"/> definitions for each field are not present. This information is unnecessary and superfluous for most users. So, any users who are interested in seeing the resource definitions clearly or modifying them may find themselves lost deep in the Operator's code.</p>
			<p>Fortunately, there is an alternative to defining resources as Go types directly. There is a helpful package called <strong class="source-inline">go-bindata</strong> (available at <a href="http://github.com/go-bindata/go-bindata">github.com/go-bindata/go-bindata</a>) that compiles declarative YAML files into your Go binary so that they can be accessible by code. Newer versions of Go (1.16+) also now include the <strong class="source-inline">go:embed</strong> compiler directive to do this without an external tool such as <strong class="source-inline">go-bindata</strong>. So, we can simplify the preceding Deployment definition like so:</p>
			<pre class="source-code">kind: Deployment</pre>
			<pre class="source-code">apiVersion: apps/v1</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: nginx-deploy</pre>
			<pre class="source-code">  namespace: nginx-ns</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  replicas: 1</pre>
			<pre class="source-code">  selector:</pre>
			<pre class="source-code">    matchLabels:</pre>
			<pre class="source-code">      app: nginx</pre>
			<pre class="source-code">  template:</pre>
			<pre class="source-code">    metadata:</pre>
			<pre class="source-code">      labels:</pre>
			<pre class="source-code">        app: nginx</pre>
			<pre class="source-code">    spec:</pre>
			<pre class="source-code">      containers:</pre>
			<pre class="source-code">      - name: nginx</pre>
			<pre class="source-code">         image: nginx:latest</pre>
			<pre class="source-code">         ports:</pre>
			<pre class="source-code">          - containerPort: 80</pre>
			<p>This is much more readable for the average user. It is also easily maintained, and you can provide different versions of <a id="_idIndexMarker147"/>various resources within named directories in your Operator's code base. This is good for code organization, and also simplifies your<a id="_idIndexMarker148"/> options for <strong class="bold">continuous integration</strong> (<strong class="bold">CI</strong>) checks against the validity of these type definitions.</p>
			<p>We will cover how to use <strong class="source-inline">go-bindata</strong> and <strong class="source-inline">go:embed</strong> in more detail in <a href="B18147_04_ePub.xhtml#_idTextAnchor066"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing an Operator with the Operator SDK</em>, but for now, we know how we can package our additional resources to be available in the Operator. This is a key design consideration that benefits our users and maintainers. </p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor057"/>Designing a target reconciliation loop</h1>
			<p>Now that we have defined our<a id="_idIndexMarker149"/> Operator's UI by designing a CRD to represent it in the cluster and itemized the Operand resources that it will manage, we can move on to the core logic of the Operator. This logic is nestled within the main reconciliation loop.</p>
			<p>As described in earlier chapters, Operators function on the premise of reconciling the current state of the cluster with the desired state set by users. They do this by periodically checking what that current state is. These checks are usually triggered by certain events that are related to the Operand. For example, an Operator will monitor the Pods in its target Operand namespace and react to the creation or deletion of a Pod. It is up to the Operator developers to define which events are of interest to the Operator.</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor058"/>Level- versus edge-based event triggering</h2>
			<p>When an event<a id="_idIndexMarker150"/> triggers the Operator's reconciliation loop, the logic does <a id="_idIndexMarker151"/>not receive the context of the whole event. Rather, the Operator must re-evaluate the entire state of the cluster to perform its reconciliation logic. This is known as <strong class="bold">level-based triggering</strong>. The <a id="_idIndexMarker152"/>alternative to this kind of<a id="_idIndexMarker153"/> design is <strong class="bold">edge-based triggering</strong>. In an edge-based system, the Operator logic would function only on the event itself.</p>
			<p>The trade-off between these two system designs is in efficiency for reliability. Edge-based systems are much more efficient because they do not need to re-evaluate the entire state and can only act on the relevant information. However, an edge-based design can suffer from inconsistent and unreliable data—for example, if events are lost. </p>
			<p>Level-based systems, on the other hand, are always aware of the entire state of the system. This makes them more suitable for large-scale distributed systems such as Kubernetes clusters. While these terms originally stem from concepts related to electronic circuits, they also relate well to software design in context. More information is available at <a href="https://venkateshabbarapu.blogspot.com/2013/03/edge-triggered-vs-level-triggered.html">https://venkateshabbarapu.blogspot.com/2013/03/edge-triggered-vs-level-triggered.html</a>.</p>
			<p>Understanding the difference between these design choices allows us to think about how the reconciliation logic will function. By going with a level-based triggering approach, we can be sure that the<a id="_idIndexMarker154"/> Operator will not lose any information or miss any events, as the cluster state representation in its memory will always eventually catch up to reality. However, we must consider the<a id="_idIndexMarker155"/> requirements for implementing a level-based design. Specifically, the Operator must have the information necessary to build the entire relevant cluster state in memory each time an event triggers reconciliation.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor059"/>Designing reconcile logic</h2>
			<p>The reconcile loop is the <a id="_idIndexMarker156"/>core function of the Operator. This is the function that is called when the Operator receives an event, and it's where the main logic of the Operator is written. Additionally, this loop should ideally be designed to manage one CRD, rather than overload a single control loop with multiple responsibilities.</p>
			<p>When using the Operator SDK to scaffold an Operator project, the reconciliation loop will have a function signature like this:</p>
			<pre class="source-code">func (r *Controller) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error)</pre>
			<p>This function is a method of a <strong class="source-inline">Controller</strong> object (which can be any name; we use <strong class="source-inline">Controller</strong> in this example, but it could just as easily be <strong class="source-inline">FooOperator</strong>). This object will be instantiated during the startup of the Operator. It then takes two parameters: <strong class="source-inline">context.Context</strong> and <strong class="source-inline">ctrl.Request</strong>. Finally, it returns a <strong class="source-inline">ctrl.Result</strong> parameter and, if applicable, an <strong class="source-inline">error</strong> parameter, We will go into more detail about these types and their specific roles in <a href="B18147_04_ePub.xhtml#_idTextAnchor066"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing an Operator with the Operator SDK</em>, but for now, understand that the core reconciliation loop of an Operator is built upon <a id="_idIndexMarker157"/>very little information about the event that triggered reconciliation. Note that the Operator's CRD and information about the cluster state are not passed to this loop; nor is anything else.</p>
			<p>Because the Operator is level-driven, the <strong class="source-inline">Reconcile</strong> function should instead build the status of the cluster itself. In pseudocode, this often looks something like this:</p>
			<pre class="source-code">func Reconcile:</pre>
			<pre class="source-code">  // Get the Operator's CRD, if it doesn't exist then return</pre>
			<pre class="source-code">  // an error so the user knows to create it</pre>
			<pre class="source-code">  operatorCrd, error = getMyCRD()</pre>
			<pre class="source-code">  if error != nil {</pre>
			<pre class="source-code">    return error</pre>
			<pre class="source-code">  }</pre>
			<pre class="source-code">  // Get the related resources for the Operator (ie, the</pre>
			<pre class="source-code">  // Operand's Deployment). If they don't exist, create them</pre>
			<pre class="source-code">  resources, error = getRelatedResources()</pre>
			<pre class="source-code">  if error == ResourcesNotFound {</pre>
			<pre class="source-code">    createRelatedResources()</pre>
			<pre class="source-code">  }</pre>
			<pre class="source-code">  // Check that the related resources relevant values match</pre>
			<pre class="source-code">  // what is set in the Operator's CRD. If they don't match,</pre>
			<pre class="source-code">  // update the resource with the specified values.</pre>
			<pre class="source-code">  if resources.Spec != operatorCrd.Spec {</pre>
			<pre class="source-code">    updateRelatedResources(operatorCrd.Spec)</pre>
			<pre class="source-code">  }</pre>
			<p>This will be the basic<a id="_idIndexMarker158"/> layout for our Operator's reconciliation loop as well. The general process, broken into steps, is this:</p>
			<ol>
				<li>First, check for an existing Operator CRD object. As we know, the Operator CRD contains the configuration settings for how the Operator should function. It's considered a best practice that Operators should not manage their own CRD, so if there isn't one on the cluster, then immediately return with an error. This error will show up in the Operator's Pod logs and indicate to the user that they should create a CRD object.</li>
				<li>Second, check for the existence of relevant resources in the cluster. For our current use case, that will be the Operand Deployment. If the Deployment does not exist, then it is the Operator's job to create it.</li>
				<li>Finally, if relevant resources<a id="_idIndexMarker159"/> already existed on the cluster, then check that they are configured in accordance with the settings in the Operator CRD. If not, then update the resources in the cluster with the intended values. While we could just update every time (because we know the desired state without having to look at the current state), it's a good practice to check for differences first. This helps minimize excessive API calls over updating indiscriminately. Making unnecessary updates also increases the chance of an update hot loop, where the Operator's updates to the resources create events that trigger the reconciliation loop that handles that object.</li>
			</ol>
			<p>These three steps rely heavily on access to the Kubernetes API via the standard API clients. The Operator SDK provides functions that help make it easy to instantiate these clients and pass them to the Operator's control loop.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor060"/>Handling upgrades and downgrades</h1>
			<p>As Operator developers, we are <a id="_idIndexMarker160"/>concerned with the versioning of two primary <a id="_idIndexMarker161"/>applications: the Operand and the Operator itself. Seamless upgrades are also the core feature of a Level II Operator, which we have decided is our goal for the initial Operator design. For that reason, we must ensure that our Operator can handle upgrades for both itself and the nginx Operand. For our use case, upgrading the Operand is fairly straightforward. We can simply pull the new image tag and update the Operand Deployment. However, if the Operand changed significantly, then the Operator may also need to be updated in order to properly manage the new Operand version.</p>
			<p>Operator upgrades arise <a id="_idIndexMarker162"/>when changes to the Operator code, API, or both need to be shipped to users. <strong class="bold">Operator Lifecycle Manager</strong> (<strong class="bold">OLM</strong>) makes upgrading Operators with newer released versions easy from a user standpoint. The<a id="_idIndexMarker163"/> Operator's <strong class="bold">ClusterServiceVersion</strong> (<strong class="bold">CSV</strong>) allows developers to define specific upgrade paths for maintainers to provide specific information about new versions that replace older versions. This will be covered in more detail in <a href="B18147_07_ePub.xhtml#_idTextAnchor108"><em class="italic">Chapter 7</em></a>, <em class="italic">Installing and Running Operators with the Operator Lifecycle Manager</em>, when we actually write a CSV for our Operator.</p>
			<p>There may also be a scenario where the Operator's CRD changes in incompatible ways (for example, deprecation of an old field). In this scenario, your Operator's API version should be increased (for example, from <strong class="source-inline">v1alpha1</strong> to <strong class="source-inline">v1alpha2</strong> or <strong class="source-inline">v1beta1</strong>). The new version should also be shipped with the existing version's CRD. This is the reason why the CRD's <strong class="source-inline">versions</strong> field is a list of version definitions, and it allows users the ability to transition from one version to the next thanks to simultaneous support of both.</p>
			<p>Recall, however, that out <a id="_idIndexMarker164"/>of this list of versions there may only be one that is the designated storage version. It is also excessive to ship every previous API version forever (eventually, older versions will need to be completely removed after an appropriate<a id="_idIndexMarker165"/> deprecation timeline has passed). When it is time to permanently remove support for deprecated API versions, the storage version may also need to be updated as well. This can cause issues for users who still have the old version of the Operator CRD installed as the storage version in their cluster. The <strong class="source-inline">kube-storage-version-migrator</strong> tool (<a href="https://github.com/kubernetes-sigs/kube-storage-version-migrator">https://github.com/kubernetes-sigs/kube-storage-version-migrator</a>) helps with this by providing a migration process for existing objects in the cluster. The storage version can be migrated with a <strong class="source-inline">Migration</strong> object, such as this:</p>
			<pre class="source-code">apiVersion: migration.k8s.io/v1alpha1</pre>
			<pre class="source-code">kind: StorageVersionMigration</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: nginx-operator-storage-version-migration</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  resource:</pre>
			<pre class="source-code">    group: operator.example.com</pre>
			<pre class="source-code">    resource: nginxoperators</pre>
			<pre class="source-code">    version: v1alpha2</pre>
			<p>When this object is created, <strong class="source-inline">kube-storage-version-migrator</strong> will see it and update any existing objects that are stored in the cluster to the specified version. This only needs to be done once, and it can even be automated by packaging this object as an additional resource in the Operator. Future versions of Kubernetes will automate this process fully (see <em class="italic">KEP-2855</em>, <a href="https://github.com/kubernetes/enhancements/pull/2856">https://github.com/kubernetes/enhancements/pull/2856</a>).</p>
			<p>Preparing for successful version transitions early on will pay off with future maintenance of your Operator. However, not everything can always go smoothly, and it's impossible to prepare for every possible <a id="_idIndexMarker166"/>scenario. This is why it is important for an<a id="_idIndexMarker167"/> Operator to have sufficient error reporting and handling as well.</p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor061"/>Using failure reporting</h1>
			<p>When it comes to failures, there are<a id="_idIndexMarker168"/> two things we need to worry about: failures in the Operand, and failures in the Operator itself. Sometimes, these two instances may even be related (for example, the Operand is failing in an unexpected way that the Operator does not know how to resolve). When any errors occur, it's an important job of the Operator to report those errors to the user.</p>
			<p>When an error happens during the Operator's reconciliation loop, the Operator must decide what to do next. In implementations with the Operator SDK, the reconcile logic is able to identify when an error has occurred and attempt the loop again. If the error continues to prevent the reconciliation from succeeding, the loop can exponentially back off and wait longer between each attempt in the hope that whichever condition is causing the failure will be resolved. However, when an Operator reaches this state, the error should still be exposed to the user in some way.</p>
			<p>Error reporting can be easily done in a few ways. The main methods for reporting failures are logging, status updates, and events. Each approach offers different advantages, but a sophisticated Operator design will utilize all three in elegant harmony.</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor062"/>Reporting errors with logging</h2>
			<p>The simplest way to report any<a id="_idIndexMarker169"/> error is with basic logging. This is the way that most software projects report information to the user, not just Kubernetes Operators. That's because logged output is fairly easy to implement and intuitive to follow for most users. This reasoning is especially true considering the availability of logging libraries such as <strong class="bold">Klog</strong>, which help standardize the structure of logs specifically for Kubernetes applications. When a Pod is running, the logs are easily retrieved with commands such as <strong class="source-inline">kubectl logs pod/my-pod</strong>. However, there are some downsides to relying on just logging for significant errors.</p>
			<p>First, Kubernetes Pod logs are not persistent. When a Pod crashes and exits, its logs are only available until the failed Pod is cleaned up by the cluster's <strong class="bold">garbage collection</strong> (<strong class="bold">GC</strong>) processes. This<a id="_idIndexMarker170"/> makes debugging a failure particularly difficult as the user is in a race against time. Additionally, if an Operator is working diligently to fix the issue, then the user is also racing against their own automation system, which is supposed to help them, not hinder them.</p>
			<p>Second, logs can be a lot of information to parse. Besides just the relevant Operator logs you may write yourself, your Operator will be built on many libraries and dependencies that inject their own information into the logged output. This can create a cumbersome mess of logs that require work to sort through. While there are, of course, tools such as <strong class="source-inline">grep</strong> that make it relatively easy to search through lots of text, your users may not always know exactly which text to search for in the first place. This can create serious delays when debugging an issue.</p>
			<p>Logs are helpful for tracing <a id="_idIndexMarker171"/>procedural steps in an Operator or for low-level debugging. However, they are not great at bringing failures to the immediate attention of users. Pod logs do not last long, and they are often drowned out by irrelevant logs. In addition, logs themselves usually do not provide much human-readable context for debugging. This is why important failures that require attention are better handled by status updates and events. </p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor063"/>Reporting errors with status updates</h2>
			<p>As mentioned earlier in the<a id="_idIndexMarker172"/> chapter when discussing Kubernetes API conventions and CRD design, an Operator CRD should provide two important fields: <strong class="source-inline">spec</strong> and <strong class="source-inline">status</strong>. While <strong class="source-inline">spec</strong> represents the desired state of the cluster and accepts input from the user, <strong class="source-inline">status</strong> serves to report the current state of the cluster and should only be updated as a form of output.</p>
			<p>By utilizing the <strong class="source-inline">status</strong> field to report the health of your Operator and its Operand, you can easily highlight important state information in a readable format. This format is based on the condition type, which is provided by the Kubernetes API machinery.</p>
			<p>A condition reports its name along with a Boolean value indicating whether the condition is currently present. For example, an Operator could report the <strong class="source-inline">OperandReady=false</strong> condition to show that the Operand is not healthy. There is also a field within the condition called <strong class="source-inline">Reason</strong>, which allows developers to provide a more readable explanation of the current status. As of Kubernetes <strong class="source-inline">1.23</strong>, the <strong class="source-inline">Condition</strong> <strong class="source-inline">Type</strong> field has a maximum length of <strong class="source-inline">316</strong> characters, and its <strong class="source-inline">Reason</strong> field can be up to <strong class="source-inline">1,024</strong> characters.</p>
			<p>The Kubernetes API clients provide functions to report conditions easily, such as <strong class="source-inline">metav1.SetStatusCondition(conditions *[]metav1.Condition, newCondition metav1.Condition</strong>). These functions (and the <strong class="source-inline">Condition</strong> type itself) exist under the <strong class="source-inline">k8s.io/apimachinery/pkg/apis/meta/v1</strong> package.</p>
			<p>In an Operator's CRD <strong class="source-inline">status</strong> field, the conditions look similar to this:</p>
			<pre class="source-code">status:</pre>
			<pre class="source-code">  conditions:</pre>
			<pre class="source-code">    - type: Ready </pre>
			<pre class="source-code">      status: "True"</pre>
			<pre class="source-code">      lastProbeTime: null</pre>
			<pre class="source-code">      lastTransitionTime: 2018-01-01T00:00:00Z</pre>
			<p>For our nginx deployment Operator, we'll start by reporting a condition that is simply called <strong class="source-inline">Ready</strong>. We'll set the status of this condition to <strong class="source-inline">True</strong> on the successful startup of the Operator, and change it to <strong class="source-inline">False</strong> in the event that the Operator fails a reconciliation loop (along with a <strong class="source-inline">Reason</strong> field explaining the failure in more detail). We may end up finding<a id="_idIndexMarker173"/> more Conditions that will make sense to add later, but given the initial simplicity of the Operator, this should be sufficient.</p>
			<p>Using conditions helps show the current state of the Operator and its managed resources, but these only show up in the <strong class="source-inline">status</strong> section of the Operator's CRD. However, we can combine them with events to make the error reporting available throughout the cluster.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor064"/>Reporting errors with events </h2>
			<p>Kubernetes events are a native <a id="_idIndexMarker174"/>API object, just like Pods or any other object in the cluster. Events are aggregated and show up when using <strong class="source-inline">kubectl describe</strong> to describe a Pod. They can also be monitored and filtered by themselves with <strong class="source-inline">kubectl get events</strong>. Their availability within the Kubernetes API makes them understandable by other applications as well, such as alerting systems.</p>
			<p>An example of listing a Pod's events is shown here, where we see five different events:</p>
			<ol>
				<li value="1">A <strong class="source-inline">Warning</strong> event that has occurred three times, showing that the Pod failed to be scheduled.</li>
				<li>A <strong class="source-inline">Normal</strong> event once the Pod was successfully scheduled.</li>
				<li>Three more <strong class="source-inline">Normal</strong> events as the Pod's container images were pulled, created, and successfully started.</li>
			</ol>
			<p>You can see these events in the <a id="_idIndexMarker175"/>following code snippet:</p>
			<pre class="source-code">$ kubectl describe pod/coredns-558bd4d5db-6mqc2 -n kube-system</pre>
			<pre class="source-code">…</pre>
			<pre class="source-code">Events:</pre>
			<pre class="source-code">  Type     Reason            Age                    From               Message</pre>
			<pre class="source-code">  ----     ------            ----                   ----               -------</pre>
			<pre class="source-code">  Warning  FailedScheduling  6m36s (x3 over 6m52s)  default-scheduler            0/1 nodes are available: 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn't tolerate.</pre>
			<pre class="source-code">  Normal   Scheduled         6m31s                  default-scheduler            Successfully assigned kube-system/coredns-558bd4d5db-6mqc2 to kind-control-plane</pre>
			<pre class="source-code">  Normal   Pulled            6m30s                  kubelet, kind-control-plane  Container image "k8s.gcr.io/coredns/coredns:v1.8.0" already present on machine</pre>
			<pre class="source-code">  Normal   Created           6m29s                  kubelet, kind-control-plane  Created container coredns</pre>
			<pre class="source-code">  Normal   Started           6m29s                  kubelet, kind-control-plane  Started container coredns</pre>
			<p>Events can relay more information than conditions thanks to a much more complex object design. While events include a <strong class="source-inline">Reason</strong> and a <strong class="source-inline">Message</strong> field (analogous to conditions' <strong class="source-inline">Type</strong> and <strong class="source-inline">Reason</strong> fields, respectively), they also include information such as <strong class="source-inline">Count</strong> (which shows the number of times this event has occurred), <strong class="source-inline">ReportingController</strong> (which shows the originating controller of the event), and <strong class="source-inline">Type</strong> (which can be used to filter events of different severity levels). </p>
			<p>The <strong class="source-inline">Type</strong> field can currently be used to categorize cluster events as <strong class="source-inline">Normal</strong> or <strong class="source-inline">Warning</strong>. This means that, similar to how a condition can report a successful state, events can also be used to show that certain functions completed successfully (such as startup or upgrades).</p>
			<p>For a Pod to report events to the cluster, the code needs to implement an <strong class="source-inline">EventRecorder</strong> object. This object should be passed throughout the controller and broadcasts events to the<a id="_idIndexMarker176"/> cluster. The Operator SDK and Kubernetes clients provide boilerplate code to set this up properly.</p>
			<p>Besides reporting events, your Operator will also react to events in the cluster. This goes back to the essential foundation of an Operator's event-triggered design. There are code patterns to design which types of events the Operator is interested in reacting to, wherein you can add logic to filter out specific events. This will be covered in detail in later chapters.</p>
			<p>As you can see, a sophisticated error-reporting system utilizes logs, status, and events to provide a full picture of the state of the application. Each method provides its own benefits, and together they weave a beautiful tapestry of debuggability that helps administrators track down failures and resolve issues. </p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor065"/>Summary</h1>
			<p>This chapter outlined the details of an Operator we would like to build. Beginning with a description of the problem (in this case, a simple Operator to manage an nginx Pod) gave a solid foundation of the solutions that<a id="_idIndexMarker177"/> are available to work with. This step even provided enough information to set a goal for the capability level of this Operator (<em class="italic">Level II – Seamless Upgrades</em>).</p>
			<p>The next step was outlining what the Operator CRD will look like. To do this, we first noted some relevant conventions in the Kubernetes API that are helpful to ensure the Operator conforms to expected standards for Kubernetes objects. We then broke down the structure of a CRD and explained how each section relates to the corresponding CR object. Finally, we drafted an example of what the Operator's CR will look like in the cluster to get a concrete idea of the expectation from users.</p>
			<p>After designing the CRD, we considered our options for managing additional resources as well. For an Operator written in Go, it makes sense to package additional resources (such as RoleBinding and ServiceAccount definitions) as YAML files. These files can be compiled into the Operator binary with <strong class="source-inline">go-bindata</strong> and <strong class="source-inline">go:embed</strong>.</p>
			<p>The next step in the design is the target reconciliation loop. This comprises the core logic of the Operator and is what makes the Operator a useful, functional application. This process began with understanding the difference between level- and edge-triggered event processing and why it is better for Operators to be level-based. We then discussed the basic steps of an Operator's reconcile loop.</p>
			<p>The last two sections discussed the topics of upgrades, downgrades, and error reporting. With upgrades and downgrades, we covered the use cases for shipping and supporting multiple API versions simultaneously, as well as the need to occasionally migrate storage versions in existing installations. The section about error reporting focused on the three main ways that applications can expose health information to users: logging, status conditions, and events.</p>
			<p>In the next chapter, we will take everything we have decided on as our initial design and compose it into actual code. This will involve initializing a project with the Operator SDK, generating an API that will become the Operator's CRD, and coding target reconciliation logic. Essentially, we will apply the knowledge from this chapter to a hands-on exercise in Operator development.</p>
		</div>
	</body></html>