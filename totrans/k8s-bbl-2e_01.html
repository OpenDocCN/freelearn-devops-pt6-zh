<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer036">
    <h1 class="chapterNumber">1</h1>
    <h1 class="chapterTitle" id="_idParaDest-17">Kubernetes Fundamentals</h1>
    <p class="normal">Welcome to <em class="italic">The Kubernetes Bible</em>, and we are happy to accompany you on your journey with Kubernetes. If you are working in the software development industry, you have probably heard about Kubernetes. This is normal because the popularity of Kubernetes has grown a lot in recent years.</p>
    <p class="normal">Built by Google, Kubernetes is the leading container orchestrator solution in terms of popularity and adoption: it’s the tool you need if you are looking for a solution to manage containerized applications in production at scale, whether it’s on-premises or on a public cloud. Be focused on the word. Deploying and managing containers at scale is extremely difficult because, by default, container engines such as Docker do not provide any way on their own to maintain the availability and scalability of containers at scale.</p>
    <p class="normal">Kubernetes first emerged as a Google project, and Google has put a lot of effort into building a solution to deploy a huge number of containers on their massively distributed infrastructure. By adopting Kubernetes as part of your stack, you’ll get an open source platform that was built by one of the biggest companies on the internet, with the most critical needs in terms of stability.</p>
    <p class="normal">Although Kubernetes can be used with a lot of different container runtimes, this book is going to focus on the Kubernetes and containers (Docker and Podman) combination.</p>
    <p class="normal">Perhaps you are already using Docker on a daily basis, but the world of container orchestration might be completely unknown to you. It is even possible that you do not even see the benefits of using such technology because everything looks fine to you with just raw Docker. That’s why, in this first chapter, we’re not going to look at Kubernetes in detail. Instead, we will focus on explaining what Kubernetes is and how it can help you manage your application containers in production. It will be easier for you to learn a new technology if you understand why it was built.</p>
    <p class="normal">In this chapter, we’re going to cover the following main topics:</p>
    <ul>
      <li class="bulletList">Understanding monoliths and microservices</li>
      <li class="bulletList">Understanding containers </li>
      <li class="bulletList">How can Kubernetes help you to manage containers?</li>
      <li class="bulletList">Understanding the history of Kubernetes</li>
      <li class="bulletList">Exploring the problems that Kubernetes solves</li>
    </ul>
    <p class="normal">You can download the latest code samples for this chapter from the official GitHub repository at <a href="https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter01"><span class="url">https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter01</span></a></p>
    <h1 class="heading-1" id="_idParaDest-18">Understanding monoliths and microservices</h1>
    <p class="normal">Let’s put Kubernetes and Docker to one side for the moment, and instead, let’s talk a little bit about how the internet and software development evolved together over the past 20 years. This will help you to gain a better understanding of where Kubernetes sits and the problems it solves.</p>
    <h2 class="heading-2" id="_idParaDest-19">Understanding the growth of the internet since the late 1990s</h2>
    <p class="normal">Since the late 1990s, the popularity <a id="_idIndexMarker000"/>of the internet has grown rapidly. Back in the 1990s, and even in the early 2000s, the internet was only used by a few hundred thousand people in the world. Today, almost 2 billion people are using the internet for email, web browsing, video games, and more.</p>
    <p class="normal">There are now a lot of people on the internet, and we’re using it for tons of different needs, and these needs are addressed by dozens of applications deployed on dozens of devices.</p>
    <p class="normal">Additionally, the number of connected devices has increased, as each person can now have several devices of a different nature connected to the internet: laptops, computers, smartphones, TVs, tablets, and more.</p>
    <p class="normal">Today, we can use the internet to shop, to work, to entertain, to read, or to do whatever. It has entered almost every part of our society and has led to a profound paradigm shift in the last 20 years. All of this has given the utmost importance to software development.</p>
    <h2 class="heading-2" id="_idParaDest-20">Understanding the need for more frequent software releases</h2>
    <p class="normal">To cope with this ever-increasing<a id="_idIndexMarker001"/> number of users who are always demanding more in terms of features, the software development industry had to evolve in order to make new software releases faster and more frequent.</p>
    <p class="normal">Indeed, back in the 1990s, you could build an application, deploy it to production, and simply update it once or twice a year. Today, companies must be able to update their software in production, sometimes several times a day, whether to deploy a new feature, integrate with a social media platform, support the resolution of the latest fashionable smartphone, or even release a patch to a security breach identified the day before. Everything is far more complex today, and you must go faster than before.</p>
    <p class="normal">We constantly need to update our software, and in the end, the survival of many companies directly depends on how often they can offer releases to their users. But how do we accelerate software development life cycles so that we can deliver new versions of our software to our users more frequently?</p>
    <p class="normal">IT departments of companies had to evolve, both in an organizational sense and a technical sense. Organizationally, they changed the way they managed projects and teams in order to shift to agile methodologies, and technically, technologies such as cloud computing platforms, containers, and virtualization were adopted widely and helped a lot to align technical agility with organizational agility. All of this is to ensure more frequent software releases! So, let’s focus on this evolution next.</p>
    <h2 class="heading-2" id="_idParaDest-21">Understanding the organizational shift to agile methodologies</h2>
    <p class="normal">From a purely organizational point<a id="_idIndexMarker002"/> of view, agile methodologies such as Scrum, Kanban, and DevOps became the standard way to organize IT teams.</p>
    <p class="normal">Typical IT departments that do not apply agile methodologies are often made of three different teams, each of them having a single responsibility in the development and release process life cycle.</p>
    <div class="note">
      <p class="normal">Rest assured, even though we are currently discussing agile methodologies and the history of the internet, this book is really about Kubernetes! We just need to explain some of the problems that we have faced before introducing Kubernetes for real!</p>
    </div>
    <p class="normal">Before the adoption of<a id="_idIndexMarker003"/> agile methodologies, development and operations often worked in separate silos. This could lead to inefficiency and communication gaps. Agile methodologies helped bridge these gaps and foster collaboration. The three isolated teams are shown below.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">The business team</strong>: They’re like the voice of the customer. Their job is to explain what features are <a id="_idIndexMarker004"/>needed in the app to meet user needs. They translate business goals into clear instructions for the developers. </li>
      <li class="bulletList"><strong class="keyWord">The development team</strong>: These are the engineers who bring the app to life. They translate the<a id="_idIndexMarker005"/> business team’s feature requests into code, building the functionalities and features users will interact with. Clear communication from the business team is crucial. If the instructions aren’t well defined, it can be like a game of telephone – misunderstandings lead to delays and rework. </li>
      <li class="bulletList"><strong class="keyWord">The operation team</strong>: They’re <a id="_idIndexMarker006"/>the keepers of the servers. Their main focus is keeping the app running smoothly. New features can be disruptive because they require updates, which can be risky. In the past, they weren’t always aware of what new features were coming because they weren’t involved in the planning. </li>
    </ul>
    <p class="normal">These are what we call silos, as illustrated in <em class="italic">Figure 1.1</em>:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_01_01.png"/></figure>
    <p class="packt_figref">Figure 1.1: Isolated teams in a typical IT department</p>
    <p class="normal">The roles are clearly defined, people from the different teams do not work together that much, and when something goes wrong, everyone loses time finding the right information from the right person.</p>
    <p class="normal">This kind of siloed organization<a id="_idIndexMarker007"/> has led to major issues:</p>
    <ul>
      <li class="bulletList">A significantly longer development time</li>
      <li class="bulletList">Greater risk in the deployment of a release that might not work at all in production</li>
    </ul>
    <p class="normal">And that’s essentially what agile methodologies and DevOps fixed. The change agile methodologies made was to make people work together by creating multidisciplinary teams.</p>
    <div class="note">
      <p class="normal">DevOps<a id="_idIndexMarker008"/> is a collaborative culture and set of practices that aims to bridge the gap between development (Dev) and operations (Ops) teams. DevOps promotes collaboration and automation throughout the software lifecycle, from development and testing to deployment and maintenance.</p>
    </div>
    <p class="normal">An agile team consists of a product owner describing concrete features by writing them as user stories that are readable by the developers who are working in the same team as them. Developers should have visibility of the production environment and the ability to deploy on top of it, preferably <a id="_idIndexMarker009"/>using a <strong class="keyWord">continuous integration and continuous deployment</strong> (<strong class="keyWord">CI/CD</strong>) approach. Testers should also be part of agile teams in order to write tests.</p>
    <p class="normal">With the collaborative approach, the teams will get better and clearer visibility of the full picture, as illustrated in the following diagram.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_01_02.png"/></figure>
    <p class="packt_figref">Figure 1.2: Team collaboration breaks silos</p>
    <p class="normal">Simply understand that, by adopting agile methodologies and DevOps, these silos were broken and multidisciplinary teams capable of formalizing a need, implementing it, testing it, releasing it, and maintaining it in the production environment were created. <em class="italic">Table 1.1</em> presents a shift from traditional development<a id="_idIndexMarker010"/> to agile and<a id="_idIndexMarker011"/> DevOps methodology.</p>
    <table class="table-container" id="table001-1">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Feature</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Traditional Development</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Agile &amp; DevOps</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Team Structure</p>
          </td>
          <td class="table-cell">
            <p class="normal">Siloed departments (Development, Operations)</p>
          </td>
          <td class="table-cell">
            <p class="normal">Cross-functional, multi-disciplinary teams</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Work Style</p>
          </td>
          <td class="table-cell">
            <p class="normal">Isolated workflows, limited communication</p>
          </td>
          <td class="table-cell">
            <p class="normal">Collaborative, iterative development cycles</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Ownership</p>
          </td>
          <td class="table-cell">
            <p class="normal">Development hands off to Operations for deployment and maintenance</p>
          </td>
          <td class="table-cell">
            <p class="normal">“You Build It, You Run It” - Teams own the entire lifecycle</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Focus</p>
          </td>
          <td class="table-cell">
            <p class="normal">Features and functionality</p>
          </td>
          <td class="table-cell">
            <p class="normal">Business value, continuous improvement</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Release Cycle</p>
          </td>
          <td class="table-cell">
            <p class="normal">Long release cycles, infrequent deployments</p>
          </td>
          <td class="table-cell">
            <p class="normal">Short sprints, frequent releases with feedback loops</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Testing</p>
          </td>
          <td class="table-cell">
            <p class="normal">Separate testing phase after development</p>
          </td>
          <td class="table-cell">
            <p class="normal">Integrated testing throughout the development cycle</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Infrastructure</p>
          </td>
          <td class="table-cell">
            <p class="normal">Static, manually managed <a id="_idIndexMarker012"/>infrastructure</p>
          </td>
          <td class="table-cell">
            <p class="normal">Automated <a id="_idIndexMarker013"/>infrastructure provisioning and management (DevOps)</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 1.1: DevOps vs traditional development – a shift in collaboration</p>
    <p class="normal">So, we’ve covered the organizational transition brought about by the adoption of agile methodologies. Now, let’s discuss the technical evolution that we’ve gone through over the past several years.</p>
    <h2 class="heading-2" id="_idParaDest-22">Understanding the shift from on-premises to the cloud</h2>
    <p class="normal">Having agile teams is very nice, but agility must also be applied to how software is built and hosted.</p>
    <p class="normal">With the aim to always achieve faster and more recurrent releases, agile software development teams had to revise two important aspects of software development and release:</p>
    <ul>
      <li class="bulletList">Hosting</li>
      <li class="bulletList">Software architecture</li>
    </ul>
    <p class="normal">Today, apps are not just for a few hundred users but potentially for millions of users concurrently. Having more users on the internet also means having more computing power capable of handling them. And, indeed, hosting an application became a very big challenge.</p>
    <p class="normal">In the early days of web hosting, businesses primarily relied on two main approaches to housing their applications: one of these approaches is on-premises hosting. This method involved physically owning and managing the servers that ran their applications. There are two main ways to achieve on-premises hosting:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Dedicated Servers</strong>: Renting physical servers from established data center providers: This involved leasing dedicated server<a id="_idIndexMarker014"/> hardware from a hosting company. The hosting provider would manage the physical infrastructure (power, cooling, security) but the responsibility for server configuration, software installation, and ongoing maintenance fell to the business. This offered greater control and customization compared to shared hosting, but still required significant in-house technical expertise.</li>
      <li class="numberedList"><strong class="keyWord">Building Your Own Data Center</strong>: Constructing and maintaining a private data center: This option <a id="_idIndexMarker015"/>involved a massive investment by the company to build and maintain its own physical data center facility. This included purchasing server hardware, networking equipment, and storage solutions, and implementing robust power, cooling, and security measures. While offering the highest level of control and security, this approach was very expensive and resource-intensive and was typically only undertaken by large corporations with significant IT resources.</li>
    </ol>
    <p class="normal">Also note that on-premises hosting also encompasses managing the operating system, security patches, backups, and disaster recovery plans for the servers. Companies often needed a dedicated IT staff to manage and maintain their on-premises infrastructure, adding to the overall cost.</p>
    <p class="normal">When your user base grows, you need to get more powerful machines to handle the load. The solution is to purchase a more powerful server and install your app on it from the start or to order and rack new hardware if you manage your data center. This is not very flexible. Today, a lot of companies are still using an on-premises solution, and often, it’s not very flexible.</p>
    <p class="normal">The game-changer was the adoption of the other approach, which is the public cloud, which is the opposite of on-premises. The idea behind cloud computing is that big companies such as Amazon, Google, and Microsoft, which own a lot of datacenters, decided to build virtualization on top of their massive infrastructure to ensure the creation and management of virtual machines was accessible by APIs. In other words, you can get virtual machines with just a few clicks or just a few commands.</p>
    <p class="normal">The following table provides high-level information about why cloud computing<a id="_idIndexMarker016"/> is good for organizations.</p>
    <table class="table-container" id="table002">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Feature</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">On-Premises</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Cloud</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Scalability</p>
          </td>
          <td class="table-cell">
            <p class="normal">Limited – requires purchasing new hardware when scaling up</p>
          </td>
          <td class="table-cell">
            <p class="normal">Highly scalable – easy to add or remove resources on demand</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Flexibility</p>
          </td>
          <td class="table-cell">
            <p class="normal">Inflexible – changes require physical hardware adjustments</p>
          </td>
          <td class="table-cell">
            <p class="normal">Highly flexible – resources can be provisioned and de-provisioned quickly</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Cost</p>
          </td>
          <td class="table-cell">
            <p class="normal">High upfront cost for hardware, software licenses, and IT staff</p>
          </td>
          <td class="table-cell">
            <p class="normal">Low upfront cost – pay-as-you-go model for resources used</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Maintenance</p>
          </td>
          <td class="table-cell">
            <p class="normal">Requires dedicated IT staff for maintenance and updates</p>
          </td>
          <td class="table-cell">
            <p class="normal">Minimal maintenance required – cloud provider manages infrastructure</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Security</p>
          </td>
          <td class="table-cell">
            <p class="normal">High level of control over security, but requires significant expertise</p>
          </td>
          <td class="table-cell">
            <p class="normal">Robust security measures implemented by cloud providers</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Downtime</p>
          </td>
          <td class="table-cell">
            <p class="normal">Recovery from hardware failures can be time-consuming</p>
          </td>
          <td class="table-cell">
            <p class="normal">Cloud providers offer high availability and disaster recovery features</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Location</p>
          </td>
          <td class="table-cell">
            <p class="normal">Limited to the physical location of datacenter</p>
          </td>
          <td class="table-cell">
            <p class="normal">Access <a id="_idIndexMarker017"/>from anywhere with an internet connection</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 1.2: Importance of cloud computing for organizations</p>
    <p class="normal">We will learn how cloud computing technology has helped organizations scale their IT infrastructure in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-23">Understanding why the cloud is well suited for scalability</h2>
    <p class="normal">Today, virtually anyone can get hundreds or thousands of servers, in just a few clicks, in the form of virtual machines or instances created on physical infrastructure maintained by cloud providers<a id="_idIndexMarker018"/> such as <strong class="keyWord">Amazon Web Services</strong>, <strong class="keyWord">Google Cloud Platform</strong>, and <strong class="keyWord">Microsoft Azure</strong>. A lot of companies <a id="_idIndexMarker019"/>decided to migrate their workloads from<a id="_idIndexMarker020"/> on-premises to a cloud provider, and their adoption has been massive over the last few years.</p>
    <p class="normal">Thanks to that, now, computing power is one of the simplest things you can get.</p>
    <p class="normal">Cloud computing providers are now typical hosting solutions that agile teams possess in their arsenal. The main reason for this is that the cloud is extremely well suited to modern development.</p>
    <p class="normal">Virtual machine configurations, CPUs, OSes, network rules, and more are publicly displayed and fully configurable, so there are no secrets for your team in terms of what the production environment is made of. Because of the programmable nature of cloud providers, it is very easy to replicate a production environment in a development or testing environment, providing more flexibility to teams, and helping them face their challenges when developing software. That’s a useful advantage for an agile development team built around the DevOps philosophy that needs to manage the development, release, and maintenance of applications in production.</p>
    <p class="normal">Cloud providers have <a id="_idIndexMarker021"/>provided many benefits, as follows:</p>
    <ul>
      <li class="bulletList">Elasticity and scalability</li>
      <li class="bulletList">Helping to break up silos and enforcing agile methodologies</li>
      <li class="bulletList">Fitting well with agile methodologies and DevOps</li>
      <li class="bulletList">Low costs and flexible billing models</li>
      <li class="bulletList">Ensuring there is no need to manage physical servers</li>
      <li class="bulletList">Allowing virtual machines to be destroyed and recreated at will</li>
      <li class="bulletList">More flexible compared to renting a bare-metal machine monthly</li>
    </ul>
    <p class="normal">Due to these benefits, the cloud is a wonderful asset in the arsenal of an agile development team. Essentially, you can build and replicate a production environment over and over again without the hassle of managing the physical machine by yourself. The cloud enables you to scale your app based on the number of users using it or the computing resources they are consuming. You’ll make your app highly available and fault tolerant. The result is a better experience for your end users.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">IMPORTANT NOTE</strong></p>
      <p class="normal">Please note that Kubernetes can run both on the cloud and on-premises. Kubernetes is very versatile, and you can even run it on a Raspberry Pi. Kubernetes and the public cloud are a good match, but you are not required or forced to run it on the cloud.</p>
    </div>
    <p class="normal">Now that we have explained the changes the cloud produced, let’s move on to software architecture because, over the years, a few things have also changed there.</p>
    <h2 class="heading-2" id="_idParaDest-24">Exploring the monolithic architecture</h2>
    <p class="normal">In the past, applications<a id="_idIndexMarker022"/> were mostly composed of monoliths. A typical monolith application consists of a simple process, a single binary, or a single package, as shown in <em class="italic">Figure 1.3</em>.</p>
    <p class="normal">This unique component is responsible for the entire implementation of the business logic, to which the software must respond. Monoliths are a good choice if you want to develop simple applications that might not necessarily be updated frequently in production. Why? Well, because monoliths have one major drawback. If your monolith becomes unstable or crashes for some reason, your entire application will become unavailable:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_01_03.png"/></figure>
    <p class="packt_figref">Figure 1.3: A monolith application consists of one big component that contains all your software</p>
    <p class="normal">The monolithic architecture <a id="_idIndexMarker023"/>can allow you to gain a lot of time during your development and that’s perhaps the only benefit you’ll find by choosing this architecture. However, it also has many disadvantages. Here are a few of them:</p>
    <ul>
      <li class="bulletList">A failed deployment to production can break your whole application.</li>
      <li class="bulletList">Scaling activities become difficult to achieve; if you fail to scale, all your applications might become unavailable.</li>
      <li class="bulletList">A failure of any kind on a monolith can lead to a complete outage of your app.</li>
    </ul>
    <p class="normal">In the 2010s, these drawbacks started to cause real problems. With the increase in the frequency of deployments, it became necessary to think of a new architecture that would be capable of supporting frequent deployments and shorter update cycles, while reducing the risk or general unavailability of the application. This is why the microservices architecture was designed.</p>
    <h2 class="heading-2" id="_idParaDest-25">Exploring the microservices architecture</h2>
    <p class="normal">The microservices architecture consists of developing your software application as a suite of independent micro-applications. Each of these applications, which is called a <strong class="keyWord">microservice</strong>, has its own<a id="_idIndexMarker024"/> versioning, life cycle, environment, and dependencies. Additionally, it can have its own deployment life cycle. Each of your microservices must only be responsible for a limited number of business rules, and all your microservices, when used together, make up the application. Think of a microservice as real full-featured software on its own, with its own life cycle and versioning process.</p>
    <p class="normal">Since microservices are only supposed to hold a subset of all the features that the entire application has, they must be accessible in order to expose their functions. You must get data from a microservice, but you might also want to push data into it. You can make your microservice accessible through widely supported protocols such as HTTP or AMQP, and they need to be able to communicate with each other.</p>
    <p class="normal">That’s why microservices are generally built as web services that expose their functionality through well-defined APIs. While HTTP (or HTTPS) REST APIs are a popular choice due to their simplicity and widespread adoption, other protocols, such as GraphQL, AMQP, and gRPC, are gaining traction and are used commonly.</p>
    <p class="normal">The key requirement is that a microservice provides a well-documented and discoverable API endpoint, regardless of the chosen protocol. This allows other microservices to seamlessly interact and exchange data.</p>
    <p class="normal">This is something that greatly<a id="_idIndexMarker025"/> differs from the monolithic architecture:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_01_04.png"/></figure>
    <p class="packt_figref">Figure 1.4: A microservice architecture where different microservices communicate via the HTTP protocol</p>
    <p class="normal">Another key aspect of the microservice architecture is that microservices need to be decoupled: if a microservice becomes unavailable or unstable, it must not affect the other microservices or the entire application’s stability. You must be able to provision, scale, start, update, or stop each microservice independently without affecting anything else. If your microservices need to work with a database engine, bear in mind that even the database must be decoupled. Each microservice should have its own database and so on. So, if the database of <strong class="keyWord">microservice A</strong> crashes, it won’t affect <strong class="keyWord">microservice B</strong>:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_01_05.png"/></figure>
    <p class="packt_figref">Figure 1.5: A microservice architecture where different microservices communicate with each other and with a dedicated database server; this way, the microservices are isolated and have no common dependencies</p>
    <p class="normal">The key rule is to decouple <a id="_idIndexMarker026"/>as much as possible so that your microservices are fully independent. Because they are meant to be independent, microservices can also have completely different technical environments and be implemented in different languages. You can have one microservice implemented in Go, another one in Java, and another one in PHP, and all together they form one application. In the context of a microservice architecture, this is not a problem. Because HTTP is a standard, they will be able to communicate with each other even if their underlying technologies are different.</p>
    <p class="normal">Microservices must be decoupled from other microservices, but they must also be decoupled from the operating system running them. Microservices should not operate at the host system level but at the upper level. You should be able to provision them, at will, on different machines without needing to rely on a strong dependency on the host system; that’s why microservice architectures and containers are a good combination.</p>
    <p class="normal">If you need to release a new feature in production, you simply deploy the microservices that are impacted by the new feature version. The others can remain the same.</p>
    <p class="normal">As you can imagine, the <a id="_idIndexMarker027"/>microservice architecture has tremendous advantages in the context of modern application development:</p>
    <ul>
      <li class="bulletList">It is easier to enforce recurring production deliveries with minimal impact on the stability of the whole application.</li>
      <li class="bulletList">You can only upgrade to a specific microservice each time, not the whole application.</li>
      <li class="bulletList">Scaling activities<a id="_idIndexMarker028"/> are smoother since you might only need to scale specific services.</li>
    </ul>
    <p class="normal">However, on the other hand, the<a id="_idIndexMarker029"/> microservice architecture has a couple of disadvantages too:</p>
    <ul>
      <li class="bulletList">The architecture requires more planning and is hard to develop.</li>
      <li class="bulletList">There are problems in managing each microservice’s dependencies.</li>
    </ul>
    <p class="normal">Microservice applications are considered hard to develop. This approach might be hard to understand, especially for junior developers. Dependency management can also become complex since all microservices can potentially have different dependencies.</p>
    <h2 class="heading-2" id="_idParaDest-26">Choosing between monolithic and microservices architectures</h2>
    <p class="normal">Building a successful software application requires careful planning, and one of the key decisions you’ll face is which architecture to use. Two main approaches dominate the scene: monoliths and microservices:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Monoliths</strong>: Imagine a<a id="_idIndexMarker030"/> compact, all-in-one system. That’s the essence of a monolith. Everything exists in a single codebase, making development and initial deployment simple for small projects or teams with limited resources. Additionally, updates tend to be quick for monoliths because there’s only one system to manage.</li>
      <li class="bulletList"><strong class="keyWord">Microservices</strong>: Think of a complex<a id="_idIndexMarker031"/> application broken down into independent, modular components. Each service can be built, scaled, and deployed separately. This approach shines with large, feature-rich projects and teams with diverse skillsets. Microservices provide flexibility and potentially fast development cycles. However, they also introduce additional complexity in troubleshooting and security management.</li>
    </ul>
    <p class="normal">Ultimately, the choice between a monolith and microservices hinges on your specific needs. Consider your project’s size, team structure, and desired level of flexibility. Don’t be swayed by trends – pick the architecture that empowers your team to develop and manage your application efficiently.</p>
    <p class="normal">Kubernetes provides flexibility. It caters to both fast-moving monoliths and microservices, allowing you to choose the architecture that best suits your project’s needs.</p>
    <p class="normal">In the next section, we will learn about containers and how they help microservice software architectures.</p>
    <h1 class="heading-1" id="_idParaDest-27">Understanding containers</h1>
    <p class="normal">Following this comparison between monolithic and microservice architectures, you should have understood that the architecture that best combines agility and DevOps is the microservice architecture. It is this architecture that we will discuss throughout the book because this is the architecture that Kubernetes manages well.</p>
    <p class="normal">Now, we will move on to discuss how Docker, which is a container engine for Linux, is a good option for managing microservices. If you already know a lot about Docker, you can skip this section. Otherwise, I suggest that you read through it carefully.</p>
    <h2 class="heading-2" id="_idParaDest-28">Understanding why containers are good for microservices</h2>
    <p class="normal">Recall the two important aspects of the<a id="_idIndexMarker032"/> microservice architecture:</p>
    <ol>
      <li class="numberedList" value="1">Each microservice can have its own technical environment and dependencies.</li>
      <li class="numberedList">At the same time, it must be decoupled from the operating system it’s running on.</li>
    </ol>
    <p class="normal">Let’s put the latter point aside for the moment and discuss the first one: two microservices of the same app can be developed in two different languages or be written in the same language but as two different versions. Now, let’s say that you want to deploy these two microservices on the same Linux machine. That would be a nightmare.</p>
    <p class="normal">The reason for this is that you’ll have to install all the versions of the different runtimes, as well as the dependencies, and there might also be different versions or overlaps between the two microservices. Additionally, all of this will be on the same host operating system. Now, let’s imagine you want to remove one of these two microservices from the machine to deploy it on another server and clean the former machine of all the dependencies used by that microservice. Of course, if you are a talented Linux engineer, you’ll succeed in doing this. However, for most people, the risk of conflicts between the dependencies is huge, and in the end, you might just make your app unavailable while running such a nightmarish infrastructure.</p>
    <p class="normal">There is a solution to this: you could build a machine image for each microservice and then put each microservice on a dedicated virtual machine. In other words, you refrain from deploying multiple microservices on the same machine. However, in this example, you will need as many machines as you have microservices. Of course, with the help of AWS or GCP, it’s going to be easy to bootstrap tons of servers, each of them tasked with running one and only one microservice, but it would be a huge waste of money to not mutualize the computing power provided by the host.</p>
    <p class="normal">You have similar solutions in the container world, but not with the default container runtimes because they don’t guarantee complete isolation between microservices. This is exactly how<a id="_idIndexMarker033"/> the <strong class="keyWord">Kata runtime</strong> and the <strong class="keyWord">Confidential Container</strong> projects<a id="_idIndexMarker034"/> come into play. These technologies provide enhanced security and isolation for containerized applications. We’ll delve deeper into these container isolation concepts later in this book.</p>
    <p class="normal">We will learn about how containers help with isolation in the next section. </p>
    <h2 class="heading-2" id="_idParaDest-29">Understanding the benefits of container isolation</h2>
    <p class="normal">Container engines<a id="_idIndexMarker035"/> such as Docker<a id="_idIndexMarker036"/> and Podman<a id="_idIndexMarker037"/> play a crucial role in managing microservices. Unlike <strong class="keyWord">virtual machines</strong> (<strong class="keyWord">VMs</strong>) that require a full guest operating system, containers are lightweight units that share the host machine’s Linux kernel. This makes them much faster to start and stop than VMs.</p>
    <p class="normal">Container engines provide a user-friendly API to build, deploy, and manage containers. Container engines don’t introduce an additional layer of virtualization. Instead, they use the built-in capabilities of the Linux kernel for process isolation, security, and resource allocation. This efficient approach makes containerization a compelling solution for deploying microservices.</p>
    <p class="normal">The following <a id="_idIndexMarker038"/>diagram <a id="_idIndexMarker039"/>shows how containers are different from virtual machines:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_01_06.png"/></figure>
    <p class="packt_figref">Figure 1.6: The difference between virtual machines and containers</p>
    <p class="normal">Your<a id="_idIndexMarker040"/> microservices are going to be launched on top of this <a id="_idIndexMarker041"/>layer, not directly on the host system whose sole role will be to run your containers.</p>
    <p class="normal">Since containers are isolated, you can run as many containers as you want and have them run applications written in different languages without any conflicts. Microservice relocation becomes as easy as stopping a running container and launching another one from the same image on another machine.</p>
    <p class="normal">The usage of containers with microservices<a id="_idIndexMarker042"/> provides three main benefits:</p>
    <ul>
      <li class="bulletList">It reduces the footprint on the host system.</li>
      <li class="bulletList">It mutualizes the host system without conflicts between different microservices.</li>
      <li class="bulletList">It removes the coupling between the microservice and the host system.</li>
    </ul>
    <p class="normal">Once a microservice has been containerized, you can eliminate its coupling with the host operating system. The microservice will only depend on the container in which it will operate. Since a container is much lighter than a real full-featured Linux operating system, it will be easy to share and deploy on many different machines. Therefore, the container and your microservice will work on any machine that is running a container engine.</p>
    <p class="normal">The following diagram shows a microservice architecture where each microservice is wrapped by a container:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_01_07.png"/></figure>
    <p class="packt_figref">Figure 1.7: A microservice application where all microservices are wrapped by a container; the life cycle of the app becomes tied to the container, and it is easy to deploy it on any machine that is running a container engine</p>
    <p class="normal">Containers fit well with the DevOps methodology too. By developing locally in a container, which would later be built and deployed in production, you ensure you develop in the same environment as the one that will eventually run the application.</p>
    <p class="normal">Container engines<a id="_idIndexMarker043"/> are not only capable of managing the life cycle of a container but also an entire ecosystem around containers. They can manage networks, and the intercommunication between different containers, and all these features respond particularly well to the properties of the microservice architecture that we mentioned earlier.</p>
    <p class="normal">By using the cloud and containers together, you can build a very strong infrastructure to host your microservice. The cloud will give you as many machines as you want. You simply need to install a container engine on each of them, and you’ll be able to deploy multiple containerized microservices on each of these machines.</p>
    <p class="normal">Container engines such as Docker or Podman are very nice tools on their own. However, you’ll discover that it’s hard to run them in production alone, just as they are.</p>
    <p class="normal">Container engines<a id="_idIndexMarker044"/> excel in development environments because of their:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Simplicity</strong>: Container engines are easy to install and use, allowing developers to quickly build, test, and run containerized applications.</li>
      <li class="bulletList"><strong class="keyWord">Flexibility</strong>: Developers can use container engines to experiment with different container configurations and explore the world of containerization.</li>
      <li class="bulletList"><strong class="keyWord">Isolation</strong>: Container engines ensure isolation between applications, preventing conflicts and simplifying debugging.</li>
    </ul>
    <p class="normal">However, production environments have strict requirements. Container engines alone cannot address all of these needs:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Scaling</strong>: Container <a id="_idIndexMarker045"/>engines (such as Docker or Podman) don’t provide built-in auto-scaling features to dynamically adapt container deployments based on resource utilization.</li>
      <li class="bulletList"><strong class="keyWord">Disaster Recovery</strong>: Container engines don’t provide comprehensive disaster recovery capabilities to ensure service availability in case of outages.</li>
      <li class="bulletList"><strong class="keyWord">Security</strong>: While container engines provide basic isolation, managing security policies for large-scale containerized deployments across multiple machines can be challenging.</li>
      <li class="bulletList"><strong class="keyWord">Standardization</strong>: Container engines require custom scripting or integrations for interacting with external systems, such as CI/CD pipelines or monitoring tools.</li>
    </ul>
    <p class="normal">While container engines excel in development environments, production deployments demand a more robust approach. Kubernetes, a powerful container orchestration platform, tackles this challenge by providing a comprehensive suite of functionalities. It manages the entire container lifecycle, from scheduling them to run on available resources to scaling deployments up or down based on demand and distributing traffic for optimal performance (load balancing). Unlike custom scripting with container engines, Kubernetes provides a well-defined API for interacting with containerized applications, simplifying integration with other tools used in production environments. Beyond basic isolation, Kubernetes provides advanced security features such as role-based access control and network policies. This allows the efficient management of containerized workloads from multiple teams or projects on the same infrastructure, optimizing resource utilization and simplifying complex deployments.</p>
    <p class="normal">Before we dive into the Kubernetes topics, let’s discuss the basics of containers and container engines in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-30">Container engines</h2>
    <p class="normal">A <strong class="keyWord">container engine</strong> acts <a id="_idIndexMarker046"/>as the interface for end-users and REST clients, managing user inputs, downloading container images from container registries, extracting downloaded images onto the disk, transforming user or REST client data for interaction with container engines, preparing container mount points, and facilitating communication with container engines. In essence, container engines serve as the user-facing layer, streamlining image and container management, while the underlying container runtimes handle the intricate low-level details of container and image management.</p>
    <p class="normal">Docker<a id="_idIndexMarker047"/> stands out as one of the most widely adopted container engines, but it’s important to note that various <a id="_idIndexMarker048"/>alternatives exist in the containerization<a id="_idIndexMarker049"/> landscape. Some <a id="_idIndexMarker050"/>notable ones<a id="_idIndexMarker051"/> are <strong class="keyWord">LXD</strong>, <strong class="keyWord">Rkt</strong>, <strong class="keyWord">CRI-O</strong>, and <strong class="keyWord">Podman</strong>.</p>
    <p class="normal">At its core, Docker relies on the <code class="inlineCode">containerd</code> container runtime, which oversees critical aspects of container management, including the container life cycle, image transfer and storage, execution, and supervision, as well as storage and network attachments. <code class="inlineCode">containerd</code>, in turn, relies on components such as <code class="inlineCode">runc </code>and <code class="inlineCode">hcsshim</code>. Runc is a command-line tool that facilitates creating and running containers in Linux, while <code class="inlineCode">hcsshim</code> plays a crucial role in the creation and management of Windows containers. </p>
    <p class="normal">It’s worth noting that <code class="inlineCode">containerd</code> is typically not meant for direct end-user interaction. Instead, container engines, such as Docker, interact with the container runtime to facilitate the creation and management of containers. The essential role of <code class="inlineCode">runc</code> is evident, serving not only <code class="inlineCode">containerd</code> but also being used by Podman, CRI-O, and indirectly by Docker itself.</p>
    <h2 class="heading-2" id="_idParaDest-31">The basics of containers </h2>
    <p class="normal">As we learned in the previous section, Docker is a well-known and widely used container engine. Let’s learn the basic terminology related to containers in general.</p>
    <h3 class="heading-3" id="_idParaDest-32">Container image</h3>
    <p class="normal">A container image<a id="_idIndexMarker052"/> is a kind of template used by container engines to launch containers. A container image is a self-contained, executable package that encapsulates an application and its dependencies. It includes everything needed to run the software, such as code, runtime, libraries, and system tools. Container images are created from a <code class="inlineCode">Dockerfile</code> or <code class="inlineCode">Containerfile</code>, which specify the build steps. Container images are stored in image repositories and shared through container registries such as Docker Hub, making them a fundamental component of containerization.</p>
    <h3 class="heading-3" id="_idParaDest-33">Container</h3>
    <p class="normal">A container<a id="_idIndexMarker053"/> can be considered a running instance of a container image. Containers are like modular shipping containers for applications. They bundle an application’s code, dependencies, and runtime environment into a single, lightweight package. Containers run consistently across different environments because they include everything needed. Each container runs independently, preventing conflicts with other applications on the same system. Containers share the host operating system’s kernel, making them faster to start and stop than virtual machines.</p>
    <h3 class="heading-3" id="_idParaDest-34">Container registry</h3>
    <p class="normal">A container registry<a id="_idIndexMarker054"/> is a centralized repository for storing and sharing container images. It acts as a distribution mechanism, allowing users to push and pull images to and from the registry. Popular public registries include Docker Hub, Red Hat Quayi, Amazon’s <strong class="keyWord">Elastic Container Registry</strong> (<strong class="keyWord">ECR</strong>), Azure Container Registry, Google Container Registry, and GitHub Container Registry. Organizations often use private registries to securely store and share custom images. Registries play a crucial role in the Docker ecosystem, facilitating collaboration and efficient management of containerized applications.</p>
    <h3 class="heading-3" id="_idParaDest-35">Dockerfile or Containerfile</h3>
    <p class="normal">A Dockerfile<a id="_idIndexMarker055"/> or Containerfile<a id="_idIndexMarker056"/> is a text document that contains a set of instructions for building a container image. It defines the base image, sets up the environment, copies the application code, installs the dependencies, and configures the runtime settings. Dockerfiles or Containerfiles provide a reproducible and automated way to create consistent images, enabling developers to version and share their application configurations.</p>
    <p class="normal">A sample Dockerfile can be seen in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># syntax=docker/dockerfile:1</span>
 
<span class="hljs-keyword">FROM</span> node:<span class="hljs-number">18</span>-alpine
<span class="hljs-keyword">WORKDIR</span> /app
<span class="hljs-keyword">COPY</span> . .
<span class="hljs-keyword">RUN</span> yarn install --production
<span class="hljs-keyword">CMD</span> ["node", "src/index.js"]
<span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">3000</span>
</code></pre>
    <p class="normal">And, here’s a line-by-line explanation of the provided Dockerfile:</p>
    <ol>
      <li class="numberedList" value="1"><code class="inlineCode"># syntax=docker/dockerfile:1</code>: This line defines the Dockerfile syntax version used to build the image. In this case, it specifies version 1 of the standard Dockerfile syntax.</li>
      <li class="numberedList"><code class="inlineCode">FROM node:18-alpine</code>: This line defines the base image for your container. It instructs the container engine to use the official Node.js 18 image with the Alpine Linux base. This provides a lightweight and efficient foundation for your application.</li>
      <li class="numberedList"><code class="inlineCode">WORKDIR /app</code>: This line sets the working directory within the container. Here, it specifies /app as the working directory. This is where subsequent commands in the Dockerfile will be executed relative to.</li>
      <li class="numberedList"><code class="inlineCode">COPY . .</code>: This line copies all files and directories from the current context (the directory where you have your Dockerfile) into the working directory (<code class="inlineCode">/app</code>) defined in the previous step. This essentially copies your entire application codebase into the container. </li>
      <li class="numberedList"><code class="inlineCode">RUN yarn install --production</code>: This line instructs the container engine to execute a command within the container. In this case, it runs <code class="inlineCode">yarn install --production</code>. This command uses the <code class="inlineCode">yarn</code> package manager to install all production dependencies listed in your <code class="inlineCode">package.json</code> file. The <code class="inlineCode">--production</code> flag ensures that only production dependencies are installed, excluding development dependencies. </li>
      <li class="numberedList"><code class="inlineCode">CMD ["node", "src/index.js"]</code>: This line defines the default command to be executed when the container starts. Here, it specifies an array with two elements: <code class="inlineCode">“node”</code> and <code class="inlineCode">“src/index.js”</code>. This tells Docker to run the Node.js interpreter (node) and execute the application’s entry point script (<code class="inlineCode">src/index.js</code>) when the container starts up.</li>
      <li class="numberedList"><code class="inlineCode">EXPOSE 3000</code>: This line exposes a port on the container. Here, it exposes port <code class="inlineCode">3000</code> within the container. This doesn’t map the port to the host machine by default, but it allows you to do so later when running the container with the <code class="inlineCode">-p</code> flag (e.g., <code class="inlineCode">docker run -p 3000:3000 my-image</code>). Exposing port <code class="inlineCode">3000</code> suggests your application might <a id="_idIndexMarker057"/>be listening on this port for incoming connections.<div class="note">
          <p class="normal"><strong class="keyWord">IMPORTANT NOTE</strong></p>
          <p class="normal">To build the container image, you can use a supported container engine (such as Docker or Podman) or a container build tool, such as Buildah or kaniko.</p>
        </div>
      </li>
    </ol>
    <h3 class="heading-3" id="_idParaDest-36">Docker Compose or Podman Compose</h3>
    <p class="normal">Docker Compose<a id="_idIndexMarker058"/> is a tool for defining and running multi-container applications. It uses a YAML file to configure the services, networks, and volumes required for an application, allowing developers to define the entire application stack in a single file. Docker Compose or Podman Compose<a id="_idIndexMarker059"/> simplifies the orchestration of complex applications, making it easy to manage multiple containers as a single application stack.</p>
    <p class="normal">The following <code class="inlineCode">compose.yaml</code> file will spin up two containers for a WordPress application stack using a single <code class="inlineCode">docker compose</code> or <code class="inlineCode">podman compose</code> command:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># compose.yaml</span>
<span class="hljs-attr">services:</span>
  <span class="hljs-attr">db:</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">docker.io/library/mariadb</span>
    <span class="hljs-attr">command:</span> <span class="hljs-string">'</span><span class="hljs-string">--default-authentication-plugin=mysql_native_password'</span>
    <span class="hljs-attr">volumes:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">db_data:/var/lib/mysql</span>
    <span class="hljs-attr">restart:</span> <span class="hljs-string">always</span>
    <span class="hljs-attr">environment:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">MYSQL_ROOT_PASSWORD=somewordpress</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">MYSQL_DATABASE=wordpress</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">MYSQL_USER=wordpress</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">MYSQL_PASSWORD=wordpress</span>
    <span class="hljs-attr">expose:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-number">3306</span>
      <span class="hljs-bullet">-</span> <span class="hljs-number">33060</span>
    <span class="hljs-attr">networks:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">wordpress</span>
  <span class="hljs-attr">wordpress:</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">wordpress:latest</span>
    <span class="hljs-attr">ports:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-number">8081</span><span class="hljs-string">:80</span>
    <span class="hljs-attr">restart:</span> <span class="hljs-string">always</span>
    <span class="hljs-attr">environment:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">WORDPRESS_DB_HOST=db</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">WORDPRESS_DB_USER=wordpress</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">WORDPRESS_DB_PASSWORD=wordpress</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">WORDPRESS_DB_NAME=wordpress</span>
    <span class="hljs-attr">networks:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">wordpress</span>
<span class="hljs-attr">volumes:</span>
  <span class="hljs-attr">db_data:</span>
<span class="hljs-attr">networks:</span>
  <span class="hljs-attr">wordpress:</span> {}
</code></pre>
    <p class="normal">In the next section, we will learn how Kubernetes can efficiently orchestrate all these container operations.</p>
    <h1 class="heading-1" id="_idParaDest-37">How can Kubernetes help you to manage your containers?</h1>
    <p class="normal">In this section, we will focus on Kubernetes, which is the purpose of this book.</p>
    <h2 class="heading-2" id="_idParaDest-38">Kubernetes – designed to run workloads in production</h2>
    <p class="normal">If you open the official <a id="_idIndexMarker060"/>Kubernetes website (at <a href="https://kubernetes.io"><span class="url">https://kubernetes.io</span></a>), the title you will see is <strong class="screenText">Production-Grade Container Orchestration</strong>:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_01_08.png"/></figure>
    <p class="packt_figref">Figure 1.8: The Kubernetes home page showing the header and introducing Kubernetes as a production container orchestration platform</p>
    <p class="normal">Those four words perfectly sum up what <a id="_idIndexMarker061"/>Kubernetes is: it is a container orchestration platform for production. Kubernetes does not aim to replace Docker or any of the features of Docker or other container engines; rather, it aims to manage the clusters of machines running container runtimes. When working with Kubernetes, you use both Kubernetes and the full-featured standard installations of container runtimes.</p>
    <p class="normal">The title mentions <strong class="keyWord">production</strong>. Indeed, the concept of production<a id="_idIndexMarker062"/> is central to Kubernetes: it was conceived and designed to answer modern production needs. Managing production workloads is different today compared to what it was in the 2000s. Back in the 2000s, your production workload would consist of just a few bare-metal servers, if not even one on-premises. These servers mostly ran monoliths directly installed on the host Linux system. However, today, thanks to public cloud platforms such <a id="_idIndexMarker063"/>as <strong class="keyWord">Amazon Web Services</strong> (<strong class="keyWord">AWS</strong>) or <strong class="keyWord">Google Cloud Platform</strong> (<strong class="keyWord">GCP</strong>), anyone<a id="_idIndexMarker064"/> can now get hundreds or even thousands of machines in the form of instances or virtual machines with just a few clicks. Even better, we no longer deploy our applications on the host system but as containerized microservices on top of Docker Engine instead, thereby reducing the footprint of the host system.</p>
    <p class="normal">A problem will arise when you must manage Docker installations on each of these virtual machines on the cloud. Let’s imagine that you have 10 (or 100 or 1,000) machines launched on your preferred cloud and you want to achieve a very simple task: deploy a containerized Docker app on each of these machines.</p>
    <p class="normal">You could do this by running the <code class="inlineCode">docker run</code> command on each of your machines. It would work, but of course, there is a better way to do it. And that’s by <a id="_idIndexMarker065"/>using a <strong class="keyWord">container orchestrator</strong> such <a id="_idIndexMarker066"/>as <strong class="keyWord">Kubernetes</strong>. To give you an extremely simplified vision of Kubernetes, it is<a id="_idIndexMarker067"/> a <strong class="keyWord">REST API</strong> that keeps a registry of your machines executing a Docker daemon.</p>
    <p class="normal">Again, this is an extremely simplified definition of Kubernetes. In fact, it’s not made of a single centralized REST API, because as you might have gathered, Kubernetes was built as a suite of microservices.</p>
    <p class="normal">Also note that while Kubernetes excels at managing containerized workloads, it doesn’t replace virtual machines (<strong class="keyWord">VMs</strong>) entirely. VMs can still be valuable for specific use cases, such as running legacy applications or software with complex dependencies that are difficult to containerize. However, Kubernetes is evolving to bridge the gap between containers and VMs.</p>
    <h2 class="heading-2" id="_idParaDest-39">KubeVirt – a bridge between containers and VMs</h2>
    <p class="normal"><strong class="keyWord">KubeVirt </strong>is a <a id="_idIndexMarker068"/>project that extends Kubernetes’ ability to manage virtual machines using the familiar Kubernetes API. This allows users to leverage the power and flexibility of Kubernetes for VM deployments alongside containerized applications. KubeVirt embraces <strong class="keyWord">Infrastructure as Code</strong> (<strong class="keyWord">IaC</strong>) principles, enabling users to define and manage VMs declaratively within their Kubernetes manifests. This simplifies VM management and integrates it seamlessly into existing Kubernetes workflows.</p>
    <p class="normal">By incorporating VMs under the Kubernetes umbrella, KubeVirt provides a compelling approach for organizations that require a hybrid environment with both containers and VMs. It demonstrates the ongoing evolution of Kubernetes as a platform for managing diverse workloads, potentially leading to a more unified approach to application deployment and management.</p>
    <p class="normal">We have learned about containers and the complications of managing and orchestrating containers at a large scale. In the next section, we will learn about the history and evolution of Kubernetes.</p>
    <h1 class="heading-1" id="_idParaDest-40">Understanding the history of Kubernetes</h1>
    <p class="normal">Now, let’s discuss the history of the Kubernetes project. It will be useful for you to understand the context in which the Kubernetes project started and the people who are keeping this project alive.</p>
    <h2 class="heading-2" id="_idParaDest-41">Understanding how and where Kubernetes started</h2>
    <p class="normal">Since its founding in 1998, Google has gained huge experience in managing high-demanding workloads at scale, especially container-based workloads. Since the mid-2000s, Google has been at the forefront of developing its applications as Linux containers. Well before Docker simplified container usage for the general public, Google recognized the advantages of containerization, giving rise to an internal project known as Borg. To enhance the architecture of Borg, making it more extensible and robust, Google initiated another container orchestrator project called Omega. Subsequently, several improvements introduced by Omega found their way into the Borg project.</p>
    <p class="normal">Kubernetes was born as <a id="_idIndexMarker069"/>an internal project at Google, and the first commit of Kubernetes was in 2014 by Brendan Burns, Joe Beda, and Craig McLendon, among others. However, Google didn’t open source Kubernetes on its own. It was the efforts of individuals like Clayton Coleman, who was working at Red Hat at the time, and who played a crucial role in championing the idea of open-sourcing Kubernetes and ensuring its success as a community-driven project. Kelsey Hightower, an early Kubernetes champion at CoreOS, became a prominent voice advocating for the technology. Through his work as a speaker, writer, and co-founder of KubeCon, he significantly boosted Kubernetes’ adoption and community growth.</p>
    <p class="normal">Today, in addition to Google, Red Hat, Amazon, Microsoft, and other companies are also contributing to the Kubernetes project actively.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">IMPORTANT NOTE</strong></p>
      <p class="normal">Borg is not the ancestor of Kubernetes because the project is not dead and is still in use at Google. It would be more appropriate to say that a lot of ideas from Borg were reused to make Kubernetes. Bear in mind that Kubernetes is not Borg or Omega. Borg was built in C++ and Kubernetes in Go. In fact, they are two entirely different projects, but one is heavily inspired by the other. This is important to understand: Borg and Omega are two internal Google projects. They were not built for the public.</p>
    </div>
    <p class="normal">Kubernetes was developed with the experience gained by Google to manage containers in production. Most importantly, it inherited Borg’s and Omega’s ideas, concepts, and architectures. Here is a brief list of ideas and concepts taken from Borg and Omega, which have now been implemented in Kubernetes:</p>
    <ul>
      <li class="bulletList">The concept of Pods<a id="_idIndexMarker070"/> to manage your containers: Kubernetes uses a logical object, called a pod, to create, update, and delete your containers.</li>
      <li class="bulletList">Each pod has its own IP address in the cluster.</li>
      <li class="bulletList">There are distributed components that all watch the central Kubernetes API to retrieve the cluster state.</li>
      <li class="bulletList">There is internal load balancing between Pods and Services.</li>
      <li class="bulletList">Labels<a id="_idIndexMarker071"/> and selectors<a id="_idIndexMarker072"/> are metadata that are used together to manage and orchestrate resources in Kubernetes.</li>
    </ul>
    <p class="normal">That’s why Kubernetes is so powerful when it comes to managing containers in production at scale. In fact, the concepts you’ll learn from Kubernetes are older than Kubernetes itself. Although Kubernetes is a young project, it was built on solid foundations.</p>
    <h2 class="heading-2" id="_idParaDest-42">Who manages Kubernetes today?</h2>
    <p class="normal">Kubernetes is no longer maintained by Google because Google handed over operational control of the Kubernetes project to the <strong class="keyWord">Cloud Native Computing Foundation</strong> (<strong class="keyWord">CNCF</strong>)<a id="_idIndexMarker073"/> on August 29, 2018. CNCF is a non-profit organization that aims to foster and sustain an open ecosystem of cloud-native technologies.</p>
    <p class="normal">Google is a founding member of CNCF, along with companies such as Cisco, Red Hat, and Intel. The Kubernetes source code is hosted on GitHub and is an extremely active project on the platform. The Kubernetes code is under Apache License version 2.0, which is a permissive open source license. You won’t have to pay to use Kubernetes, and if you are good at coding with Go, you can even contribute to the code.</p>
    <h2 class="heading-2" id="_idParaDest-43">Where is Kubernetes today?</h2>
    <p class="normal">In the realm <a id="_idIndexMarker074"/>of container orchestration, Kubernetes faces competition from various alternatives, including both open-source solutions and platform-specific offerings. Some notable contenders include:</p>
    <ul>
      <li class="bulletList">Apache Mesos</li>
      <li class="bulletList">HashiCorp Nomad</li>
      <li class="bulletList">Docker Swarm</li>
      <li class="bulletList">Amazon ECS</li>
    </ul>
    <p class="normal">While each of these orchestrators comes with its own set of advantages and drawbacks, Kubernetes stands out as the most widely adopted and popular choice in the field.</p>
    <p class="normal">Kubernetes<a id="_idIndexMarker075"/> has won the fight for popularity and adoption and has become the standard way of deploying container-based workloads in production. As its immense growth has made it one of the hottest topics in the IT industry, it has become crucial for cloud providers to come up with a Kubernetes offering as part of their services. Therefore, Kubernetes is supported almost everywhere now.</p>
    <p class="normal">The following Kubernetes-based services can help you get a Kubernetes cluster up and running with just a few clicks:</p>
    <ul>
      <li class="bulletList">Google Kubernetes Engine (GKE) on Google Cloud Platform </li>
      <li class="bulletList">Elastic Kubernetes Service (Amazon EKS)</li>
      <li class="bulletList">Azure Kubernetes Service on Microsoft Azure</li>
      <li class="bulletList">Alibaba Cloud Container Service for Kubernetes (ACK)</li>
    </ul>
    <p class="normal">It’s not just about the cloud offerings. It’s also about the Platform-as-a-Service market. <strong class="keyWord">Red Hat</strong> started <a id="_idIndexMarker076"/>incorporating Kubernetes into its OpenShift container platform with the release of OpenShift version 3 in 2015. This marked a significant shift in OpenShift’s architecture, moving from its original design to a Kubernetes-based container orchestration system, providing users with enhanced container management capabilities and offering a complete set of enterprise tools to build, deploy, and manage containers entirely on top of Kubernetes. In addition to this, other projects, such as Rancher, were built as <strong class="keyWord">Kubernetes distributions</strong> to <a id="_idIndexMarker077"/>offer a complete set of tools around the Kubernetes orchestrator, whereas projects such as <strong class="keyWord">Knative </strong>manage<a id="_idIndexMarker078"/> serverless workloads with the Kubernetes orchestrator.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">IMPORTANT NOTE</strong></p>
      <p class="normal">AWS is an exception because it has two container orchestrator services. The first one is Amazon ECS, which is entirely made by AWS. The second one is Amazon EKS, which was released later than ECS and is a complete Kubernetes offering on AWS. These services are not the same, so do not be misguided by their similar names.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-44">Where is Kubernetes going?</h2>
    <p class="normal">Kubernetes isn’t stopping at containers! It’s evolving to manage a wider range of workloads. KubeVirt<a id="_idIndexMarker079"/> extends its reach to virtual machines, while integration with AI/ML frameworks such as TensorFlow could allow Kubernetes to orchestrate even machine learning tasks. The future<a id="_idIndexMarker080"/> of Kubernetes is one of flexibility, potentially becoming a one-stop platform for managing diverse applications across containers, VMs, and even AI/ML workflows.</p>
    <p class="normal">Learning Kubernetes today is one of the smartest decisions you can take if you are into managing cloud-native applications in production. Kubernetes is evolving rapidly, and there is no reason to wonder why its growth would stop.</p>
    <p class="normal">By mastering this wonderful tool, you’ll get one of the hottest skills being searched for in the IT industry today. We hope you are now convinced!</p>
    <p class="normal">In the next section, we will learn how Kubernetes can simplify operations.</p>
    <h1 class="heading-1" id="_idParaDest-45">Exploring the problems that Kubernetes solves</h1>
    <p class="normal">Now, why is Kubernetes such a good fit for DevOps teams? Here’s the connection: Kubernetes shines as a container orchestration platform, managing the deployment, scaling, and networking of containerized applications. Containers are lightweight packages that bundle an application with its dependencies, allowing faster and more reliable deployments across different environments. Users leverage Kubernetes<a id="_idIndexMarker081"/> for several reasons:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Automation</strong>: Kubernetes automates many manual tasks associated with deploying and managing containerized applications, freeing up time for developers to focus on innovation.</li>
      <li class="bulletList"><strong class="keyWord">Scalability</strong>: Kubernetes facilitates easy scaling of applications up or down based on demand, ensuring optimal resource utilization.</li>
      <li class="bulletList"><strong class="keyWord">Consistency</strong>: Kubernetes ensures consistent deployments across different environments, from development to production, minimizing configuration errors and streamlining the delivery process.</li>
      <li class="bulletList"><strong class="keyWord">Flexibility</strong>: Kubernetes is compatible with various tools and technologies commonly used by DevOps teams, simplifying integration into existing workflows.</li>
    </ul>
    <p class="normal">You can imagine that launching containers on your local machine or a development environment is not going to require the same level of planning as launching these same containers on remote machines, which could face millions of users. Problems specific to production will arise, and <a id="_idIndexMarker082"/>Kubernetes is a great way to address these problems when using containers in production:</p>
    <ul>
      <li class="bulletList">Ensuring high availability</li>
      <li class="bulletList">Handling release management and container deployments</li>
      <li class="bulletList">Autoscaling containers</li>
      <li class="bulletList">Network isolation</li>
      <li class="bulletList">Role-Based Access Control (RBAC)</li>
      <li class="bulletList">Stateful workloads</li>
      <li class="bulletList">Resource management</li>
    </ul>
    <h2 class="heading-2" id="_idParaDest-46">Ensuring high availability</h2>
    <p class="normal">High availability is the central<a id="_idIndexMarker083"/> principle of production. This means that your application should always remain accessible and should never be down. Of course, it’s utopian. Even the biggest companies experience service outages. However, you should always bear in mind that this is your goal. Kubernetes includes a whole battery of functionality to make your containers highly available by replicating them on several host machines and monitoring their health on a regular and frequent basis.</p>
    <p class="normal">When you deploy containers, the accessibility of your application will directly depend on the health of your containers. Let’s imagine that for some reason, a container containing one of your microservices becomes inaccessible; with Docker alone, you cannot automatically guarantee that the container is terminated and recreated to ensure the service restoration. With Kubernetes, it becomes possible as Kubernetes will help you design applications that can automatically repair themselves by performing automated tasks such as health checking and container replacement.</p>
    <p class="normal">If one machine in your cluster were to fail, all the containers running on it would disappear. Kubernetes would immediately notice that and reschedule all the containers on another machine. In this way, your applications will become highly available and fault tolerant as well.</p>
    <h2 class="heading-2" id="_idParaDest-47">Release management and container deployment</h2>
    <p class="normal">Deployment management<a id="_idIndexMarker084"/> is <a id="_idIndexMarker085"/>another of these production-specific problems that Kubernetes solves. The process of deployment consists of updating your application in production<a id="_idIndexMarker086"/> to replace an old version of a given microservice with a new version.</p>
    <p class="normal">Deployments<a id="_idIndexMarker087"/> in production are always complex because you have to update the containers that are responding to requests from end users. If you miss them, the consequences could be severe for your application because it could become unstable or inaccessible, which is why you should always be able to quickly revert to the previous version of your application by running a rollback. The challenge of deployment is that it needs to be performed in the least visible way to the end user, with as little friction as possible.</p>
    <p class="normal">Whenever you release a new version of the application, there are multiple processes involved, as follows:</p>
    <ol>
      <li class="numberedList" value="1">Update the <code class="inlineCode">Dockerfile</code> or <code class="inlineCode">Containerfile</code> with the latest application info (if any).</li>
      <li class="numberedList">Build a new Docker container image with the latest version of the application.</li>
      <li class="numberedList">Push the new container image to the container registry.</li>
      <li class="numberedList">Pull the new container image from the container registry to the staging/UAT/production system (Docker host).</li>
      <li class="numberedList">Stop and delete the existing (old version) of the application container running on the system.</li>
      <li class="numberedList">Launch the new container image with the new version of the application container image in the staging/UAT/production system.</li>
    </ol>
    <p class="normal">Refer to the following image to understand the high-level flow in a typical scenario (please note that this is an ideal scenario because, in an actual environment, you might be using different and isolated container registries for development, staging, and production environments).</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_01_09.png"/></figure>
    <p class="packt_figref">Figure 1.9: High-level workflow of container management</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">IMPORTANT NOTE</strong></p>
      <p class="normal">The container build process has absolutely nothing to do with Kubernetes: it’s purely a container image management part. Kubernetes will come into play later when you have to deploy new containers based on a newly built image.</p>
    </div>
    <p class="normal">Without <a id="_idIndexMarker088"/>Kubernetes, you’ll have to run all these operations including <code class="inlineCode">docker pull</code>, <code class="inlineCode">docker stop</code>, <code class="inlineCode">docker delete</code>, and <code class="inlineCode">docker run</code> on the machine where you want to deploy a<a id="_idIndexMarker089"/> new version of the container. Then, you will have to repeat this operation on each server that runs a copy of the container. It should work, but it is extremely tedious since it is not automated. And guess what? Kubernetes can automate this for you.</p>
    <p class="normal">Kubernetes has features that allow it to manage deployments and rollbacks of Docker containers, and this will make your life a lot easier when responding to this problem. With a single command, you can ask Kubernetes to update your containers on all of your machines as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">$ kubectl <span class="hljs-built_in">set</span> image deploy/myapp <span class="hljs-attribute">myapp_container</span>=myapp:1.0.0
</code></pre>
    <p class="normal">On a real Kubernetes cluster, this command will update the container called <code class="inlineCode">myapp_container</code>, which is running as part of the application deployment called <code class="inlineCode">myapp</code>, on every single machine where <code class="inlineCode">myapp_container</code> runs to the <code class="inlineCode">1.0.0</code> tag.</p>
    <p class="normal">Whether it must update one container running on one machine or millions over multiple datacenters, this command works the same. Even better, it ensures high availability.</p>
    <p class="normal">Remember that the goal is always to meet the requirement of high availability; a deployment should not cause your application to crash or cause a service disruption. Kubernetes is natively capable of managing deployment strategies such as rolling updates, which aim to prevent service interruptions.</p>
    <p class="normal">Additionally, Kubernetes <a id="_idIndexMarker090"/>keeps in memory all the revisions of a specific deployment<a id="_idIndexMarker091"/> and allows you to revert to a previous version with just one command. It’s an incredibly powerful tool that allows you to update a cluster of Docker containers with just one command.</p>
    <h2 class="heading-2" id="_idParaDest-48">Autoscaling containers</h2>
    <p class="normal">Scaling<a id="_idIndexMarker092"/> is another<a id="_idIndexMarker093"/> production-specific problem that has been widely democratized using public clouds such as <strong class="keyWord">Amazon Web Services</strong> (<strong class="keyWord">AWS</strong>) and <strong class="keyWord">Google Cloud Platform</strong> (<strong class="keyWord">GCP</strong>). Scaling is the ability to adapt your <a id="_idIndexMarker094"/>computing power to the load you are facing, again to <a id="_idIndexMarker095"/>meet the requirement of high availability and load balancing. Never forget that the goal is to prevent outages and downtime.</p>
    <p class="normal">When your production machines are facing a traffic spike and one of your containers is no longer able to cope with the load, you need to find a way to scale the container workloads efficiently. There are two scaling methods:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Vertical scaling</strong>: This allows <a id="_idIndexMarker096"/>your container to use more computing<a id="_idIndexMarker097"/> power offered by the host machine.</li>
      <li class="bulletList"><strong class="keyWord">Horizontal scaling</strong>: You can<a id="_idIndexMarker098"/> duplicate your container in the <a id="_idIndexMarker099"/>same or another machine, and you can load-balance the traffic between the multiple containers.</li>
    </ul>
    <p class="normal">Docker is not able to respond to this problem alone; however, when you manage Docker with Kubernetes, it becomes possible.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_01_10.png"/></figure>
    <p class="packt_figref">Figure 1.10: Vertical scaling versus horizontal scaling for pods</p>
    <p class="normal">Kubernetes can manage both vertical<a id="_idIndexMarker100"/> and horizontal scaling<a id="_idIndexMarker101"/> automatically. It does this by letting your containers consume more computing power from the host or by creating additional containers that can be deployed on the same or <a id="_idIndexMarker102"/>another node in the cluster. And if your Kubernetes cluster is<a id="_idIndexMarker103"/> not capable of handling more containers because all your nodes are full, Kubernetes will even be able to launch new virtual machines by interfacing with your cloud provider in a fully automated and transparent manner by using a component called <a id="_idIndexMarker104"/>a <strong class="keyWord">cluster autoscaler</strong>.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">IMPORTANT NOTE</strong></p>
      <p class="normal">The cluster autoscaler only works if the Kubernetes cluster is deployed on a supported cloud provider (a private or public cloud).</p>
    </div>
    <p class="normal">These goals cannot be achieved without using a container orchestrator. The reason for this is simple. You can’t afford to do these tasks; you need to think about DevOps’ culture and agility and seek to automate these tasks so that your applications can repair themselves, be fault-tolerant, and be highly available.</p>
    <p class="normal">Contrary to scaling out your containers or cluster, you must also be able to decrease the number of containers if the load starts to decrease to adapt your resources to the load, whether it is rising or falling. Again, Kubernetes can do this, too.</p>
    <h2 class="heading-2" id="_idParaDest-49">Network isolation</h2>
    <p class="normal">In a world of millions of users, ensuring<a id="_idIndexMarker105"/> secure communication between containers is paramount. Traditional approaches can involve complex manual configuration. This is where Kubernetes shines:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pod networking</strong>: Kubernetes<a id="_idIndexMarker106"/> creates a virtual network overlay for your pods. By default, containers within the same Pod can communicate directly, while containers in different Pods are isolated by default. This prevents unintended communication between containers and enhances security.</li>
      <li class="bulletList"><strong class="keyWord">Network policies</strong>: Kubernetes allows you to define granular network policies<a id="_idIndexMarker107"/> that further restrict how pods can communicate. You can specify allowed ingress (incoming traffic) and egress (outgoing traffic) for pods, ensuring they only access the resources they need. This approach simplifies network configuration and strengthens security in production environments.</li>
    </ul>
    <h2 class="heading-2" id="_idParaDest-50">Role-Based Access Control (RBAC)</h2>
    <p class="normal">Managing access to container resources in a production environment with multiple users is crucial. Here’s how Kubernetes <a id="_idIndexMarker108"/>empowers secure access control: </p>
    <ul>
      <li class="bulletList"><strong class="keyWord">User roles</strong>: Kubernetes defines user roles<a id="_idIndexMarker109"/> that specify permissions for accessing and managing container resources. These roles can be assigned to individual users or groups, allowing granular control over who can perform specific actions (such as viewing pod logs and deploying new containers).</li>
      <li class="bulletList"><strong class="keyWord">Service accounts</strong>: Kubernetes utilizes service accounts<a id="_idIndexMarker110"/> to provide identities for pods running within the cluster. These service accounts can be assigned roles, ensuring pods only have the access they require to function correctly. </li>
    </ul>
    <p class="normal">This multi-layered approach of using user roles and service accounts strengthens security and governance in production deployments.</p>
    <h2 class="heading-2" id="_idParaDest-51">Stateful workloads </h2>
    <p class="normal">While containers are typically<a id="_idIndexMarker111"/> stateless (their data doesn’t persist after they stop), some applications require persistent storage. Kubernetes provides solutions to manage stateful <a id="_idIndexMarker112"/>workloads: <strong class="keyWord">Persistent Volumes (PVs)</strong> and <strong class="keyWord">Persistent Volume Claims (PVCs)</strong>. Kubernetes introduces the concept of PVs, which are persistent <a id="_idIndexMarker113"/>storage resources provisioned by the administrator (e.g., host directory, cloud storage). Applications can then request storage using PVCs. This abstraction decouples storage management from the application, allowing containers to leverage persistent storage without worrying about the underlying details.</p>
    <h2 class="heading-2" id="_idParaDest-52">Resource management</h2>
    <p class="normal">Efficiently allocating resources to<a id="_idIndexMarker114"/> containers becomes critical in production to optimize performance and avoid resource bottlenecks. Kubernetes provides functionalities for managing resources: </p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Resource quotas</strong>: Kubernetes allows you to set<a id="_idIndexMarker115"/> resource quotas (limits and requests) for CPU, memory, and other resources for namespaces or pods. This ensures fair resource allocation and prevents individual pods from consuming excessive resources that could starve other applications.</li>
      <li class="bulletList"><strong class="keyWord">Resource limits and requests</strong>: When <a id="_idIndexMarker116"/>defining deployments, you can specify resource requests (minimum guaranteed resources) and<a id="_idIndexMarker117"/> resource limits (maximum allowed resources) for containers. These ensure your application has the resources it needs to function properly while preventing uncontrolled resource usage.</li>
    </ul>
    <p class="normal">We will learn about all of these features in the upcoming chapters.</p>
    <p class="normal">Should we use Kubernetes everywhere? Let’s discuss that in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-53">When and where is Kubernetes not the solution?</h2>
    <p class="normal">Kubernetes has undeniable benefits; however, it is not always advisable to use it as a solution. Here, we have listed several cases where another solution might be more appropriate:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Container-less architecture</strong>: If you do not use a container at all, Kubernetes won’t be of any use to you.</li>
      <li class="bulletList"><strong class="keyWord">A very small number of microservices or applications</strong>: Kubernetes stands out when it must manage many containers. If your app consists of two to three microservices, a simpler orchestrator might be a better fit.</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-54">Summary</h1>
    <p class="normal">This first chapter gave us room for a big introduction. We covered a lot of subjects, such as monoliths, microservices, Docker containers, cloud computing, and Kubernetes. We also discussed how this project came to life. You should now have a global vision of how Kubernetes can be used to manage your containers in production. You have also learned why Kubernetes was introduced and how it became a well-known container orchestration tool.</p>
    <p class="normal">In the next chapter, we will discuss the process Kubernetes follows to launch a Docker container. You will discover that you can issue commands to Kubernetes, and these commands will be interpreted by Kubernetes as instructions to run containers. We will list and explain each component of Kubernetes and its role in the whole cluster. There are a lot of components that make up a Kubernetes cluster, and we will discover all of them. We will explain how Kubernetes was built with a focus on the distinction between master nodes, worker nodes, and control plane components.</p>
    <h1 class="heading-1" id="_idParaDest-55">Further reading</h1>
    <ul>
      <li class="bulletList">Kubernetes documentation: <a href="https://kubernetes.io/docs/home/"><span class="url">https://kubernetes.io/docs/home/</span></a></li>
      <li class="bulletList">Podman documentation: <a href="https://docs.podman.io/en/latest/"><span class="url">https://docs.podman.io/en/latest/</span></a></li>
      <li class="bulletList">Docker docs: <a href="https://docs.docker.com/"><span class="url">https://docs.docker.com/</span></a></li>
      <li class="bulletList">Kata containers: <a href="https://katacontainers.io/"><span class="url">https://katacontainers.io/</span></a></li>
      <li class="bulletList">kaniko: <a href="https://github.com/GoogleContainerTools/kaniko"><span class="url">https://github.com/GoogleContainerTools/kaniko</span></a></li>
      <li class="bulletList">Buildah: <a href="https://buildah.io"><span class="url">https://buildah.io</span></a></li>
      <li class="bulletList">KubeVirt: <a href="https://kubevirt.io"><span class="url">https://kubevirt.io</span></a></li>
      <li class="bulletList">Knative: <a href="https://knative.dev/docs/ "><span class="url">https://knative.dev/docs/</span></a></li>
      <li class="bulletList">Kubernetes: The Documentary [PART 1]: <a href="https://www.youtube.com/watch?v=BE77h7dmoQU"><span class="url">https://www.youtube.com/watch?v=BE77h7dmoQU</span></a></li>
      <li class="bulletList">Kubernetes: The Documentary [PART 2]: <a href="https://www.youtube.com/watch?v=318elIq37PE"><span class="url">https://www.youtube.com/watch?v=318elIq37PE</span></a></li>
      <li class="bulletList">Technically Speaking: Clayton Coleman on the History of Kubernetes: <a href="https://www.youtube.com/watch?v=zUJTGqWZtq0"><span class="url">https://www.youtube.com/watch?v=zUJTGqWZtq0</span></a></li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-56">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img alt="" src="image/QR_Code119001106479081656.png"/></p>
  </div>
</body></html>