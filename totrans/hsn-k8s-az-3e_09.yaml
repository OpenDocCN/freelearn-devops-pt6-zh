- en: 5\. Handling common failures in AKS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is a distributed system with many working parts. AKS abstracts most
    of it for you, but it is still your responsibility to know where to look and how
    to respond when bad things happen. Much of the failure handling is done automatically
    by Kubernetes; however, you will encounter situations where manual intervention
    is required.
  prefs: []
  type: TYPE_NORMAL
- en: There are two areas where things can go wrong in an application that is deployed
    on top of AKS. Either the cluster itself has issues, or the application deployed
    on top of the cluster has issues. This chapter focuses specifically on cluster
    issues. There are several things that can go wrong with a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that can go wrong is a node in the cluster can become unavailable.
    This can happen either due to an Azure infrastructure outage or due to an issue
    with the virtual machine itself, such as an operating system crash. Either way,
    Kubernetes monitors the cluster for node failures and will recover automatically.
    You will see this process in action in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A second common issue in a Kubernetes cluster is out-of-resource failures. This
    means that the workload you are trying to deploy requires more resources than
    are available on your cluster. You will learn how to monitor these signals and
    how you can solve them.
  prefs: []
  type: TYPE_NORMAL
- en: Another common issue is problems with mounting storage, which happens when a
    node becomes unavailable. When a node in Kubernetes becomes unavailable, Kubernetes
    will not detach the disks attached to this failed node. This means that those
    disks cannot be used by workloads on other nodes. You will see a practical example
    of this and learn how to recover from this failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look into the following topics in depth in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Handling node failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving out-of-resource failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling storage mount issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you will learn about common failure scenarios, as well as solutions
    to those scenarios. To start, we will introduce node failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Refer to Kubernetes the Hard Way ([https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way)),
    an excellent tutorial, to get an idea about the blocks on which Kubernetes is
    built. For the Azure version, refer to Kubernetes the Hard Way – Azure Translation
    ([https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure](https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure)).
  prefs: []
  type: TYPE_NORMAL
- en: Handling node failures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Intentionally (to save costs) or unintentionally, nodes can go down. When that
    happens, you don''t want to get the proverbial 3 a.m. call that your system is
    down. Kubernetes can handle moving workloads on failed nodes automatically for
    you instead. In this exercise, you are going to deploy the guestbook application
    and bring a node down in your cluster to see what Kubernetes does in response:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure that your cluster has at least two nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should generate an output as shown in *Figure 5.1*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![List of nodes in the created cluster](img/B17338_05_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.1: List of nodes in the cluster'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you don''t have two nodes in your cluster, look for your cluster in the
    Azure portal, navigate to Node pools, select the pool you wish to scale, and click
    on Scale. You can then scale Node count to 2 nodes as shown in *Figure 5.2*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Scaling the cluster size to two nodes using the Azure portal](img/B17338_05_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.2: Scaling the cluster'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As an example application in this section, deploy the guestbook application.
    The YAML file to deploy this has been provided in the source code for this chapter
    (`guestbook-all-in-one.yaml`). To deploy the guestbook application, use the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Watch the `service` object until the public IP becomes available. To do this,
    type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can also get services in Kubernetes by using `kubectl get svc` rather than
    the full `kubectl get service`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This will take a couple of seconds to show you the updated external IP. *Figure
    5.3* shows the service's public IP. Once you see the public IP appear (20.72.244.113
    in this case), you can exit the watch command by hitting *Ctrl* + *C*:![Fetching
    the external IP of the Service object](img/B17338_05_03.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.3: The external IP of the frontend service changes from <pending>
    to an actual IP address'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Go to `http://<EXTERNAL-IP>` (`http://20.72.244.113` in this case) as shown
    in *Figure 5.4*:![Browsing to the guestbook application using the external IP](img/B17338_05_04.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.4: Browsing to the guestbook application'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s see where the pods are currently running using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will generate an output as shown in *Figure 5.5*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![List of pods running on nodes 0 and 2](img/B17338_05_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.5: The pods are spread between node 0 and node 2'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This shows you that you should have the workload spread between node 0 and node
    2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: In the example shown in *Figure 5.5*, the workload is spread between nodes 0
    and 2\. You might notice that node 1 is missing here. If you followed the example
    in *Chapter 4, Building scalable applications*, your cluster should be in a similar
    state. The reason for this is that as Azure removes old nodes and adds new nodes
    to a cluster (as you did in *Chapter 4, Building scalable applications*), it keeps
    incrementing the node counter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before introducing the node failures, there are two optional steps you can
    take to verify whether your application can continue to run. You can run the following
    command to hit the guestbook front end every 5 seconds and get the HTML. It''s
    recommended to open this in a new Cloud Shell window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The preceding command will keep calling your application till you press *Ctrl* + *C*.
    There might be intermittent times where you don't get a reply, which is to be
    expected as Kubernetes takes a couple of minutes to rebalance the system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can also add some guestbook entries to see what happens to them when you
    cause the node to shut down. This will display an output as shown in *Figure 5.6*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Adding a couple of entries in the guestbook application](img/B17338_05_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.6: Writing a couple of messages in the guestbook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this example, you are exploring how Kubernetes handles a node failure. To
    demonstrate this, shut down a node in the cluster. You can shut down either node,
    although for maximum impact it is recommended you shut down the node from *step
    6* that hosted the most pods. In the case of the example shown, node 2 will be
    shut down.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To shut down this node, look for `kubectl get nodes`, you would see node 2 is
    in a NotReady state. There is a configuration in Kubernetes called `pod-eviction-timeout`
    that defines how long the system will wait to reschedule pods on a healthy host.
    The default is 5 minutes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you recorded a number of messages in the guestbook during *step 7*, browse
    back to the guestbook application on its public IP. What you can see is that all
    your precious messages are gone! This shows the importance of having **PersistentVolumeClaims**
    (**PVCs**) for any data that you want to survive in the case of a node failure,
    which is not the case in our application here. You will see an example of this
    in the last section of this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, you learned how Kubernetes automatically handles node failures
    by recreating pods on healthy nodes. In the next section, you will learn how you
    can diagnose and solve out-of-resource issues.
  prefs: []
  type: TYPE_NORMAL
- en: Solving out-of-resource failures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another common issue that can come up with Kubernetes clusters is the cluster
    running out of resources. When the cluster doesn't have enough CPU power or memory
    to schedule additional pods, pods will become stuck in a `Pending` state. You
    have seen this behavior in *Chapter 4, Building scalable applications*, as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes uses requests to calculate how much CPU power or memory a certain
    pod requires. The guestbook application has requests defined for all the deployments.
    If you open the `guestbook-all-in-one.yaml` file in the folder `Chapter05`, you''ll
    see the following for the `redis-replica` deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This section explains that every pod for the `redis-replica` deployment requires
    `200m` of a CPU core (`200` milli or `20%`) and `100MiB` (Mebibyte) of memory.
    In your 2 CPU clusters (with node 1 shut down), scaling this to 10 pods will cause
    issues with the available resources. Let''s look into this:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Kubernetes, you can use either the binary prefix notation or the base 10
    notation to specify memory and storage. Binary prefix notation means using KiB
    (kibibyte) to represent 1,024 bytes, MiB (mebibyte) to represent 1,024 KiB, and
    Gib (gibibyte) to represent 1,024 MiB. Base 10 notation means using kB (kilobyte)
    to represent 1,000 bytes, MB (megabyte) to represent 1,000 kB, and GB (gigabyte)
    represents 1,000 MB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by scaling the `redis-replica` deployment to 10 pods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will cause a couple of new pods to be created. We can check our pods using
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will generate an output as shown in *Figure 5.10*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![The Redis replica pod in a Pending state due to a shortage of resources](img/B17338_05_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.10: Some pods are in the Pending state'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Highlighted here is one of the pods that are in the Pending state. This occurs
    if the cluster is out of resources.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can get more information about these pending pods using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will show you more details. At the bottom of the `describe` command, you
    should see something like what''s shown in *Figure 5.11*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Fetching more details about the pending pod using the kubectl describe pod
    command](img/B17338_05_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.11: Kubernetes is unable to schedule this pod'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It explains two things:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: One of the nodes is out of CPU resources.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the nodes has a taint (node.kubernetes.io/unreachable) that the pod didn't
    tolerate. This means that the node that is `NotReady` can't accept pods.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We can solve this capacity issue by starting up node 2 as shown in *Figure 5.12*.
    This can be done in a way similar to the shutdown process:![Starting node 2 again
    from the Instances pane of the selected VMSS](img/B17338_05_12.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.12: Start node 2 again'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It will take a couple of minutes for the other node to become available again
    in Kubernetes. You can monitor the progress on the pods by executing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will show you an output after a couple of minutes similar to *Figure 5.13*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Monitoring the transition of the pods from the Pending state to the Running
    state](img/B17338_05_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.13: Pods move from a Pending state to ContainerCreating to Running'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here again, you see the container status change from Pending, to ContainerCreating,
    to finally Running.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you re-execute the `describe` command on the previous pod, you''ll see an
    output like what''s shown in *Figure 5.14*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Output showing that the Kubernetes scheduler assigned the redis replica pod
    to node 2](img/B17338_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: When the node is available again, the Pending pods are assigned
    to that node'
  prefs: []
  type: TYPE_NORMAL
- en: This shows that after node 2 became available, Kubernetes scheduled the pod
    on that node, and then started the container.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to diagnose out-of-resource errors. You were
    able to solve the error by adding another node to the cluster. Before moving on
    to the final failure mode, clean up the guestbook deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In *Chapter 4, Building scalable applications*, the **cluster autoscaler** was
    introduced. The cluster autoscaler will monitor out-of-resource errors and add
    new nodes to the cluster automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s clean up the guestbook deployment by running the following `delete`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It is now also safe to close the other Cloud Shell window you opened earlier.
  prefs: []
  type: TYPE_NORMAL
- en: So far, you have learned how to recover from two failure modes for nodes in
    a Kubernetes cluster. First, you saw how Kubernetes handles a node going offline
    and how the system reschedules pods to a working node. After that, you saw how
    Kubernetes uses requests to manage the scheduling of pods on a node, and what
    happens when a cluster is out of resources. In the next section, you'll learn
    about another failure mode in Kubernetes, namely what happens when Kubernetes
    encounters storage mounting issues.
  prefs: []
  type: TYPE_NORMAL
- en: Fixing storage mount issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier in this chapter, you noticed how the guestbook application lost data
    when the Redis master was moved to another node. This happened because that sample
    application didn't use any persistent storage. In this section, you'll see an
    example of how PVCs can be used to prevent data loss when Kubernetes moves a pod
    to another node. You will see a common error that occurs when Kubernetes moves
    pods with PVCs attached, and you'll learn how to fix this.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, you will reuse the WordPress example from the previous chapter. Before
    starting, let''s make sure that the cluster is in a clean state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This should show you just the one Kubernetes service, as in *Figure 5.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Checking the status of the cluster using the kubectl get all command](img/B17338_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.15: You should only have the one Kubernetes service running for now'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also ensure that both nodes are running and Ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This should show us both nodes in a Ready state, as in *Figure 5.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Checking the status of both nodes using the kubectl get nodes command](img/B17338_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.16: You should have two nodes available in your cluster'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, under the *Handling node failures* section, you saw
    that the messages stored in `redis-master` are lost if the pod gets restarted.
    The reason for this is that `redis-master` stores all data in its container, and
    whenever it is restarted, it uses the clean image without the data. In order to
    survive reboots, the data has to be stored outside. Kubernetes uses PVCs to abstract
    the underlying storage provider to provide this external storage.
  prefs: []
  type: TYPE_NORMAL
- en: To start this example, set up the WordPress installation.
  prefs: []
  type: TYPE_NORMAL
- en: Starting the WordPress installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's start by installing WordPress. We will demonstrate how it works and then
    verify that storage is still present after a reboot.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have not done so yet in a previous chapter, add the Helm repository
    for Bitnami:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Begin reinstallation by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will take a couple of minutes to process. You can follow the status of
    this installation by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After a couple of minutes, this should show you two pods with a status of Running
    and with a ready status of 1/1 for both pods, as shown in *Figure 5.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using kubectl get pods -w to follow the progress of WordPress installation](img/B17338_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.17: All pods will have the status of Running after a couple of minutes'
  prefs: []
  type: TYPE_NORMAL
- en: You might notice that the `wp-wordpress` pod went through an Error status and
    was restarted afterward. This is because the `wp-mariadb` pod was not ready in
    time, and `wp-wordpress` went through a restart. You will learn more about readiness
    and how this can influence pod restarts in *Chapter 7, Monitoring the AKS cluster
    and the application*.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you saw how to install WordPress. Now, you will see how to
    avoid data loss using persistent volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Using persistent volumes to avoid data loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A **persistent volume** (**PV**) is the way to store persistent data in the
    cluster with Kubernetes. PVs were discussed in more detail in *Chapter 3, Application
    deployment on AKS*. Let''s explore the PVs created for the WordPress deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get the PersistentVolumeClaims using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will generate an output as shown in *Figure 5.18*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Fetching the details of the PersistentVolumeClaims using the kubectl get
    pvc command](img/B17338_05_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will show you the two PersistentVolumes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using the kubectl get pv command to check the created PersistentVolumes](img/B17338_05_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will show you the details of that volume, as in *Figure 5.20*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using the kubectl describe pv<pv name> command to get details of specific
    PersistentVolumes](img/B17338_05_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.20: The details of one of the PVs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, you can see which PVC has claimed this volume and what the DiskName is
    in Azure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Verify that your site is working:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will show us the public IP of our WordPress site, as seen in *Figure 5.21*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Obtaining the public IP of our WordPress site](img/B17338_05_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.21: Public IP of the WordPress site'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you remember from *Chapter 3, Application deployment of AKS*, Helm showed
    you the commands you need to get the admin credentials for our WordPress site.
    Let''s grab those commands and execute them to log on to the site as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will show you the username and password, as displayed in *Figure 5.22*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using Helm commands to obtain a username and password to login to the WordPress
    site](img/B17338_05_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.22: Getting the username and password for the WordPress application'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can log in to our site via the following address: `http://<external-ip>/admin`.
    Log in here with the credentials from the previous step. Then you can go ahead
    and add a post to your website. Click the Write your first blog post button, and
    then create a short post, as shown in *Figure 5.23*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Writing your first blog post on the WordPress website](img/B17338_05_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.23: Writing your first blog post'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type some text now and hit the Publish button, as shown in *Figure 5.24*. The
    text itself isn''t important; you are writing this to verify that data is indeed
    persisted to disk:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the Publish button to publish a post with random text on the WordPress
    website](img/B17338_05_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.24: Publishing a post with random text'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you now head over to the main page of your website at `http://<external-ip>`,
    you''ll see your test post as shown in *Figure 5.25*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the website’s external IP to navigate to the WordPress website and
    verify the published post](img/B17338_05_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.25: The published blog post appears on the home page'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you deployed a WordPress site, you logged in to your WordPress
    site, and you created a post. You will verify whether this post survives a node
    failure in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Handling pod failure with PVC involvement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first test you''ll do with the PVCs is to kill the pods and verify whether
    the data has indeed persisted. To do this, let''s do two things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watch the pods in your application**: To do this, use the current Cloud Shell
    and execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`watch` command that you executed earlier. You should see an output like what''s
    shown in *Figure 5.27*:![Kubernetes creates new pods to recover from the pod outage
    caused due to the deletion of pods](img/B17338_05_27.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.27: After deleting the pods, Kubernetes will automatically recreate
    both pods'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, the two original pods went into a Terminating state. Kubernetes
    quickly started creating new pods to recover from the pod outage. The pods went
    through a similar life cycle as the original ones, going from Pending to ContainerCreating
    to Running.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you head on over to your website, you should see that your demo post has
    been persisted. This is how PVCs can help you prevent data loss, as they persist
    data that would not have been persisted in the pod itself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, you've learned how PVCs can help when pods get recreated on
    the same node. In the next section, you'll see how PVCs are used when a node has
    a failure.
  prefs: []
  type: TYPE_NORMAL
- en: Handling node failure with PVC involvement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous example, you saw how Kubernetes can handle pod failures when
    those pods have a PV attached. In this example, you''ll learn how Kubernetes handles
    node failures when a volume is attached:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first check which node is hosting your application, using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the example shown in *Figure 5.28*, node 2 was hosting MariaDB, and node
    0 was hosting the WordPress site:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Checking the node that is hosting your application](img/B17338_05_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.28: Check which node hosts the WordPress site'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Introduce a failure and stop the node that is hosting the WordPress pod using
    the Azure portal. You can do this in the same way as in the earlier example. First,
    look for the scale set backing your cluster, as shown in *Figure 5.29*:![Searching
    for vmss in the azure search bar, and selecting the scale set used by your cluster](img/B17338_05_29.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.29: Looking for the scale set hosting your cluster'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then shut down the node, by clicking on Instances in the left-hand menu, then
    selecting the node you need to shut down and clicking the Stop button, as shown
    in *Figure 5.30*:![Shutting down the desired node through the Instances pane of
    the scale set used by your cluster](img/B17338_05_30.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.30: Shutting down the node'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After this action, once again, watch the pods to see what is happening in the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As in the previous example, it is going to take 5 minutes before Kubernetes
    will start taking action against the failed node. You can see that happening in
    *Figure 5.31*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![The status of the pod indicates that it is stuck in a ContainerCreating state](img/B17338_05_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.31: A pod in a ContainerCreating state'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You are seeing a new issue here. The new pod is stuck in a ContainerCreating
    state. Let''s figure out what is happening here. First, describe that pod:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get an output as shown in *Figure 5.32*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using the kubectl describe command to understand the issue with the pod stuck
    in theContainerCreating state](img/B17338_05_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.32: Output explaining why the pod is in a ContainerCreating state'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This tells you that there is a problem with the volume. You see two errors
    related to that volume: the `FailedAttachVolume` error explains that the volume
    is already used by another pod, and `FailedMount` explains that the current pod
    cannot mount the volume. You can solve this by manually forcefully removing the
    old pod stuck in the `Terminating` state.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The behavior of the pod stuck in the `Terminating` state is not a bug. This
    is default Kubernetes behavior. The Kubernetes documentation states the following:
    *"Kubernetes (versions 1.5 or newer) will not delete pods just because a Node
    is unreachable. The pods running on an unreachable Node enter the Terminating
    or Unknown state after a timeout. Pods may also enter these states when the user
    attempts the graceful deletion of a pod on an unreachable Node."* You can read
    more at [https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/](https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To forcefully remove the terminating pod from the cluster, get the full pod
    name using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will show you an output similar to *Figure 5.33*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Fetching the name of the pod stuck in the Terminating state](img/B17338_05_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.33: Getting the name of the pod stuck in the Terminating state'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the pod''s name to force the deletion of this pod:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the pod has been deleted, it will take a couple of minutes for the other
    pod to enter a Running state. You can monitor the state of the pod using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will return an output similar to *Figure 5.34*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![The new WordPress pod returning to a Running state](img/B17338_05_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.34: The new WordPress pod returning to a Running state'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As you can see, this brought the new pod to a healthy state. It did take a
    couple of minutes for the system to pick up the changes and then mount the volume
    to the new pod. Let''s get the details of the pod again using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will generate an output as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![The new pod is now attaching the volume and pulling the container image](img/B17338_05_35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.35: The new pod is now attaching the volume and pulling the container
    image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This shows you that the new pod successfully got the volume attached and that
    the container image got pulled. This also made your WordPress website available
    again, which you can verify by browsing to the public IP. Before continuing to
    the next chapter, clean up the application using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s also start the node that was shut down: go back to the scale set pane
    in the Azure portal, click Instances in the left-hand menu, select the node you
    need to start, and click on the Start button, as shown in *Figure 5.36*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Using the Instances pane of the selected VMSS to start the node that was
    shut down](img/B17338_05_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.36: Starting node 0 again'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how you can recover from a node failure when PVCs
    aren't mounting to new pods. All you needed to do was forcefully delete the pod
    that was stuck in the `Terminating` state.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, you learned about common Kubernetes failure modes and how you
    can recover from them. This chapter started with an example of how Kubernetes
    automatically detects node failures and how it will start new pods to recover
    the workload. After that, you scaled out your workload and had your cluster run
    out of resources. You recovered from that situation by starting the failed node
    again to add new resources to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you saw how PVs are useful to store data outside of a pod. You deleted
    all pods on the cluster and saw how the PV ensured that no data was lost in your
    application. In the final example in this chapter, you saw how you can recover
    from a node failure when PVs are attached. You were able to recover the workload
    by forcefully deleting the terminating pod. This brought your workload back to
    a healthy state.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has explained common failure modes in Kubernetes. In the next chapter,
    we will introduce HTTPS support to our services and introduce authentication with
    Azure Active Directory.
  prefs: []
  type: TYPE_NORMAL
