- en: 5\. Handling common failures in AKS
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. 处理 AKS 中的常见故障
- en: Kubernetes is a distributed system with many working parts. AKS abstracts most
    of it for you, but it is still your responsibility to know where to look and how
    to respond when bad things happen. Much of the failure handling is done automatically
    by Kubernetes; however, you will encounter situations where manual intervention
    is required.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是一个分布式系统，包含许多工作组件。AKS 为你抽象了大部分内容，但了解如何在故障发生时找到问题所在并作出响应，仍然是你的责任。大部分故障处理是由
    Kubernetes 自动完成的；然而，你仍会遇到需要手动干预的情况。
- en: There are two areas where things can go wrong in an application that is deployed
    on top of AKS. Either the cluster itself has issues, or the application deployed
    on top of the cluster has issues. This chapter focuses specifically on cluster
    issues. There are several things that can go wrong with a cluster.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署在 AKS 上的应用中，有两个方面可能会出现问题。要么是集群本身出现问题，要么是部署在集群上的应用出现问题。本章专注于集群问题。集群可能会出现几种故障。
- en: The first thing that can go wrong is a node in the cluster can become unavailable.
    This can happen either due to an Azure infrastructure outage or due to an issue
    with the virtual machine itself, such as an operating system crash. Either way,
    Kubernetes monitors the cluster for node failures and will recover automatically.
    You will see this process in action in this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个可能出错的情况是集群中的一个节点变得不可用。这可能是由于 Azure 基础设施故障或虚拟机本身的问题，比如操作系统崩溃。无论是哪种情况，Kubernetes
    都会监控集群的节点故障并自动恢复。你将在本章中看到这一过程的实际操作。
- en: A second common issue in a Kubernetes cluster is out-of-resource failures. This
    means that the workload you are trying to deploy requires more resources than
    are available on your cluster. You will learn how to monitor these signals and
    how you can solve them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 集群中的第二个常见问题是资源不足故障。这意味着你尝试部署的工作负载需要的资源超过了集群中可用的资源。你将学习如何监控这些信号，以及如何解决它们。
- en: Another common issue is problems with mounting storage, which happens when a
    node becomes unavailable. When a node in Kubernetes becomes unavailable, Kubernetes
    will not detach the disks attached to this failed node. This means that those
    disks cannot be used by workloads on other nodes. You will see a practical example
    of this and learn how to recover from this failure.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见问题是存储挂载问题，这通常发生在节点不可用时。当 Kubernetes 中的节点变得不可用时，Kubernetes 不会卸载与该故障节点附加的磁盘。这意味着这些磁盘无法被其他节点上的工作负载使用。你将看到一个实际示例，并学习如何从这种故障中恢复。
- en: 'We will look into the following topics in depth in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将深入探讨以下主题：
- en: Handling node failures
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理节点故障
- en: Solving out-of-resource failures
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决资源不足故障
- en: Handling storage mount issues
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理存储挂载问题
- en: In this chapter, you will learn about common failure scenarios, as well as solutions
    to those scenarios. To start, we will introduce node failures.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解常见的故障场景以及这些场景的解决方案。首先，我们将介绍节点故障。
- en: 'Note:'
  id: totrans-11
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意：
- en: Refer to Kubernetes the Hard Way ([https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way)),
    an excellent tutorial, to get an idea about the blocks on which Kubernetes is
    built. For the Azure version, refer to Kubernetes the Hard Way – Azure Translation
    ([https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure](https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考 Kubernetes the Hard Way（[https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way)），这是一个很好的教程，可以帮助你了解
    Kubernetes 构建的基础。对于 Azure 版本，请参考 Kubernetes the Hard Way – Azure 翻译版（[https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure](https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure)）。
- en: Handling node failures
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理节点故障
- en: 'Intentionally (to save costs) or unintentionally, nodes can go down. When that
    happens, you don''t want to get the proverbial 3 a.m. call that your system is
    down. Kubernetes can handle moving workloads on failed nodes automatically for
    you instead. In this exercise, you are going to deploy the guestbook application
    and bring a node down in your cluster to see what Kubernetes does in response:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是有意（为了节省成本）还是无意中，节点可能会宕机。当这种情况发生时，你不希望接到凌晨三点的电话，告诉你系统宕机了。Kubernetes 可以自动处理在故障节点上迁移工作负载。在本练习中，你将部署一个访客留言簿应用，并将集群中的一个节点关闭，以观察
    Kubernetes 如何响应：
- en: 'Ensure that your cluster has at least two nodes:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你的集群至少有两个节点：
- en: '[PRE0]'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This should generate an output as shown in *Figure 5.1*:'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应该会生成如*图 5.1*所示的输出：
- en: '![List of nodes in the created cluster](img/B17338_05_01.jpg)'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![创建集群中节点列表](img/B17338_05_01.jpg)'
- en: 'Figure 5.1: List of nodes in the cluster'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 5.1: 集群中的节点列表'
- en: 'If you don''t have two nodes in your cluster, look for your cluster in the
    Azure portal, navigate to Node pools, select the pool you wish to scale, and click
    on Scale. You can then scale Node count to 2 nodes as shown in *Figure 5.2*:'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你的集群中没有两个节点，请在 Azure 门户中查找你的集群，导航到节点池，选择要扩展的池，并点击“缩放”。然后你可以将节点计数器缩放到 2 节点，如*图
    5.2*所示：
- en: '![Scaling the cluster size to two nodes using the Azure portal](img/B17338_05_02.jpg)'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用 Azure 门户将集群大小扩展到两个节点](img/B17338_05_02.jpg)'
- en: 'Figure 5.2: Scaling the cluster'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 5.2: 扩展集群'
- en: 'As an example application in this section, deploy the guestbook application.
    The YAML file to deploy this has been provided in the source code for this chapter
    (`guestbook-all-in-one.yaml`). To deploy the guestbook application, use the following
    command:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为本节的示例应用程序，请部署访客留言应用程序。部署此应用程序的 YAML 文件已在本章的源代码中提供（`guestbook-all-in-one.yaml`）。要部署访客留言应用程序，请使用以下命令：
- en: '[PRE1]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Watch the `service` object until the public IP becomes available. To do this,
    type the following:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监视 `service` 对象，直到公共 IP 可用为止。为此，请输入以下命令：
- en: '[PRE2]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can also get services in Kubernetes by using `kubectl get svc` rather than
    the full `kubectl get service`.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以通过使用 `kubectl get svc` 而不是完整的 `kubectl get service` 来获取 Kubernetes 中的服务。
- en: This will take a couple of seconds to show you the updated external IP. *Figure
    5.3* shows the service's public IP. Once you see the public IP appear (20.72.244.113
    in this case), you can exit the watch command by hitting *Ctrl* + *C*:![Fetching
    the external IP of the Service object](img/B17338_05_03.jpg)
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将花费几秒钟时间来显示更新后的外部 IP。*图 5.3*显示了服务的公共 IP。一旦你看到公共 IP 出现（在本例中为 20.72.244.113），你可以通过按下*Ctrl*
    + *C*退出 watch 命令：![获取服务对象的外部 IP](img/B17338_05_03.jpg)
- en: 'Figure 5.3: The external IP of the frontend service changes from <pending>
    to an actual IP address'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 5.3: 前端服务的外部 IP 从 <pending> 变为实际 IP 地址'
- en: Go to `http://<EXTERNAL-IP>` (`http://20.72.244.113` in this case) as shown
    in *Figure 5.4*:![Browsing to the guestbook application using the external IP](img/B17338_05_04.jpg)
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 `http://<EXTERNAL-IP>`（在本例中为 `http://20.72.244.113`），如*图 5.4*所示：![使用外部 IP
    浏览访客留言应用程序](img/B17338_05_04.jpg)
- en: 'Figure 5.4: Browsing to the guestbook application'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 5.4: 浏览访客留言应用程序'
- en: 'Let''s see where the pods are currently running using the following command:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看当前运行的 Pod 使用以下命令：
- en: '[PRE3]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will generate an output as shown in *Figure 5.5*:'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成如*图 5.5*所示的输出：
- en: '![List of pods running on nodes 0 and 2](img/B17338_05_05.jpg)'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![运行在节点 0 和节点 2 上的 Pod 列表](img/B17338_05_05.jpg)'
- en: 'Figure 5.5: The pods are spread between node 0 and node 2'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 5.5: Pod 分布在节点 0 和节点 2 之间'
- en: This shows you that you should have the workload spread between node 0 and node
    2.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这显示你应该将工作负载分布在节点 0 和节点 2 之间。
- en: Note
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In the example shown in *Figure 5.5*, the workload is spread between nodes 0
    and 2\. You might notice that node 1 is missing here. If you followed the example
    in *Chapter 4, Building scalable applications*, your cluster should be in a similar
    state. The reason for this is that as Azure removes old nodes and adds new nodes
    to a cluster (as you did in *Chapter 4, Building scalable applications*), it keeps
    incrementing the node counter.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在*图 5.5*中的示例中，工作负载分布在节点 0 和节点 2 之间。你可能注意到这里缺少节点 1。如果你按照*第 4 章，构建可扩展应用程序*的示例操作，你的集群应该处于类似状态。这是因为
    Azure 在从集群中移除旧节点并添加新节点时（正如你在*第 4 章，构建可扩展应用程序*中所做的），它会不断增加节点计数器。
- en: 'Before introducing the node failures, there are two optional steps you can
    take to verify whether your application can continue to run. You can run the following
    command to hit the guestbook front end every 5 seconds and get the HTML. It''s
    recommended to open this in a new Cloud Shell window:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在引入节点故障之前，你可以执行两个可选步骤来验证你的应用程序是否能够继续运行。你可以每 5 秒运行以下命令，以访问访客留言前端并获取 HTML。建议在新的
    Cloud Shell 窗口中打开此命令：
- en: '[PRE4]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding command will keep calling your application till you press *Ctrl* + *C*.
    There might be intermittent times where you don't get a reply, which is to be
    expected as Kubernetes takes a couple of minutes to rebalance the system.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前述命令将持续调用你的应用程序，直到你按下*Ctrl* + *C*。可能会有间歇性时间，你将不会收到回复，这在 Kubernetes 重新平衡系统时是正常的。
- en: 'You can also add some guestbook entries to see what happens to them when you
    cause the node to shut down. This will display an output as shown in *Figure 5.6*:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你还可以添加一些留言本条目，以查看当你使节点关闭时它们会发生什么。这将显示如下所示的输出，见*图5.6*：
- en: '![Adding a couple of entries in the guestbook application](img/B17338_05_06.jpg)'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![在留言本应用中添加几条条目](img/B17338_05_06.jpg)'
- en: 'Figure 5.6: Writing a couple of messages in the guestbook'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.6：在留言本中写入几条信息
- en: In this example, you are exploring how Kubernetes handles a node failure. To
    demonstrate this, shut down a node in the cluster. You can shut down either node,
    although for maximum impact it is recommended you shut down the node from *step
    6* that hosted the most pods. In the case of the example shown, node 2 will be
    shut down.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个示例中，你正在探索Kubernetes如何处理节点故障。为了演示这一点，关闭集群中的一个节点。你可以关闭任何一个节点，虽然为了达到最大影响，建议关闭*第6步*中托管了最多pods的节点。在这个示例中，节点2将被关闭。
- en: To shut down this node, look for `kubectl get nodes`, you would see node 2 is
    in a NotReady state. There is a configuration in Kubernetes called `pod-eviction-timeout`
    that defines how long the system will wait to reschedule pods on a healthy host.
    The default is 5 minutes.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要关闭这个节点，查找`kubectl get nodes`，你会看到节点2处于NotReady状态。在Kubernetes中有一个配置项叫做`pod-eviction-timeout`，它定义了系统等待多长时间以便在健康的主机上重新调度pods。默认值为5分钟。
- en: If you recorded a number of messages in the guestbook during *step 7*, browse
    back to the guestbook application on its public IP. What you can see is that all
    your precious messages are gone! This shows the importance of having **PersistentVolumeClaims**
    (**PVCs**) for any data that you want to survive in the case of a node failure,
    which is not the case in our application here. You will see an example of this
    in the last section of this chapter.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你在*第7步*中记录了多个留言本条目，回到留言本应用的公共IP地址时，你会发现所有的留言都不见了！这表明在节点故障时，任何希望能持久保存的数据都应该使用**持久卷声明**（**PVCs**），而我们的应用程序中并没有这样做。你将在本章的最后一节看到一个关于此的示例。
- en: In this section, you learned how Kubernetes automatically handles node failures
    by recreating pods on healthy nodes. In the next section, you will learn how you
    can diagnose and solve out-of-resource issues.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你学习了Kubernetes如何通过在健康节点上重新创建pods自动处理节点故障。在接下来的章节中，你将学习如何诊断和解决资源耗尽的问题。
- en: Solving out-of-resource failures
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决资源耗尽故障
- en: Another common issue that can come up with Kubernetes clusters is the cluster
    running out of resources. When the cluster doesn't have enough CPU power or memory
    to schedule additional pods, pods will become stuck in a `Pending` state. You
    have seen this behavior in *Chapter 4, Building scalable applications*, as well.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群中另一个常见的问题是集群资源耗尽。当集群没有足够的CPU或内存来调度额外的pods时，pods将会进入`Pending`状态。你在*第4章，构建可扩展应用程序*中也看到了这种行为。
- en: 'Kubernetes uses requests to calculate how much CPU power or memory a certain
    pod requires. The guestbook application has requests defined for all the deployments.
    If you open the `guestbook-all-in-one.yaml` file in the folder `Chapter05`, you''ll
    see the following for the `redis-replica` deployment:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes使用请求来计算某个pod所需的CPU或内存量。留言本应用对所有的部署都定义了请求。如果你打开文件夹`Chapter05`中的`guestbook-all-in-one.yaml`文件，你会看到如下关于`redis-replica`部署的内容：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This section explains that every pod for the `redis-replica` deployment requires
    `200m` of a CPU core (`200` milli or `20%`) and `100MiB` (Mebibyte) of memory.
    In your 2 CPU clusters (with node 1 shut down), scaling this to 10 pods will cause
    issues with the available resources. Let''s look into this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了`redis-replica`部署中的每个pod需要`200m`的CPU核心（`200`毫或`20%`）和`100MiB`（兆字节）的内存。在你有2个CPU的集群（且节点1已关闭）的情况下，将其扩展到10个pods会导致可用资源的问题。我们来看看这个情况：
- en: Note
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In Kubernetes, you can use either the binary prefix notation or the base 10
    notation to specify memory and storage. Binary prefix notation means using KiB
    (kibibyte) to represent 1,024 bytes, MiB (mebibyte) to represent 1,024 KiB, and
    Gib (gibibyte) to represent 1,024 MiB. Base 10 notation means using kB (kilobyte)
    to represent 1,000 bytes, MB (megabyte) to represent 1,000 kB, and GB (gigabyte)
    represents 1,000 MB.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，你可以使用二进制前缀表示法或十进制表示法来指定内存和存储。二进制前缀表示法意味着使用KiB（千字节）表示1,024字节，MiB（兆字节）表示1,024
    KiB，Gib（吉字节）表示1,024 MiB。十进制表示法意味着使用kB（千字节）表示1,000字节，MB（兆字节）表示1,000 kB，GB（千兆字节）表示1,000
    MB。
- en: 'Let''s start by scaling the `redis-replica` deployment to 10 pods:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先将 `redis-replica` 部署扩展到 10 个 pods：
- en: '[PRE6]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will cause a couple of new pods to be created. We can check our pods using
    the following:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将导致创建几个新的 pods。我们可以使用以下命令检查我们的 pods：
- en: '[PRE7]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will generate an output as shown in *Figure 5.10*:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成如下所示的输出，类似于 *图 5.10*：
- en: '![The Redis replica pod in a Pending state due to a shortage of resources](img/B17338_05_10.jpg)'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![由于资源不足，Redis 副本 pod 处于 Pending 状态](img/B17338_05_10.jpg)'
- en: 'Figure 5.10: Some pods are in the Pending state'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.10：一些 pods 处于 Pending 状态
- en: Highlighted here is one of the pods that are in the Pending state. This occurs
    if the cluster is out of resources.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里突出显示的是一个处于 Pending 状态的 pod。这通常发生在集群资源不足时。
- en: 'We can get more information about these pending pods using the following command:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令获取有关这些待处理 pod 的更多信息：
- en: '[PRE8]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will show you more details. At the bottom of the `describe` command, you
    should see something like what''s shown in *Figure 5.11*:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示更多详细信息。在 `describe` 命令的底部，你应该看到类似于 *图 5.11* 所示的内容：
- en: '![Fetching more details about the pending pod using the kubectl describe pod
    command](img/B17338_05_11.jpg)'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用 kubectl describe pod 命令获取有关待处理 pod 的更多详情](img/B17338_05_11.jpg)'
- en: 'Figure 5.11: Kubernetes is unable to schedule this pod'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.11：Kubernetes 无法调度此 pod
- en: 'It explains two things:'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它解释了两件事：
- en: One of the nodes is out of CPU resources.
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中一个节点的 CPU 资源不足。
- en: One of the nodes has a taint (node.kubernetes.io/unreachable) that the pod didn't
    tolerate. This means that the node that is `NotReady` can't accept pods.
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中一个节点有一个污点（node.kubernetes.io/unreachable），该 pod 没有容忍这个污点。这意味着处于 `NotReady`
    状态的节点无法接受 pods。
- en: We can solve this capacity issue by starting up node 2 as shown in *Figure 5.12*.
    This can be done in a way similar to the shutdown process:![Starting node 2 again
    from the Instances pane of the selected VMSS](img/B17338_05_12.jpg)
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过启动节点 2 来解决这个容量问题，如 *图 5.12* 所示。这可以通过类似于关闭过程的方式来完成：![从选定的 VMSS 的实例窗格重新启动节点
    2](img/B17338_05_12.jpg)
- en: 'Figure 5.12: Start node 2 again'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.12：重新启动节点 2
- en: 'It will take a couple of minutes for the other node to become available again
    in Kubernetes. You can monitor the progress on the pods by executing the following
    command:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，另一个节点重新变为可用需要几分钟时间。你可以通过执行以下命令来监控 pods 的进度：
- en: '[PRE9]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This will show you an output after a couple of minutes similar to *Figure 5.13*:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将在几分钟后显示类似于 *图 5.13* 的输出：
- en: '![Monitoring the transition of the pods from the Pending state to the Running
    state](img/B17338_05_13.jpg)'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![监控 pods 从 Pending 状态到 Running 状态的过渡](img/B17338_05_13.jpg)'
- en: 'Figure 5.13: Pods move from a Pending state to ContainerCreating to Running'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.13：Pods 从 Pending 状态转变为 ContainerCreating 再到 Running
- en: Here again, you see the container status change from Pending, to ContainerCreating,
    to finally Running.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，你再次看到容器状态从 Pending 变为 ContainerCreating，再到最后的 Running。
- en: 'If you re-execute the `describe` command on the previous pod, you''ll see an
    output like what''s shown in *Figure 5.14*:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你重新执行前一个 pod 的 `describe` 命令，你将看到类似于 *图 5.14* 所示的输出：
- en: '![Output showing that the Kubernetes scheduler assigned the redis replica pod
    to node 2](img/B17338_05_14.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![显示 Kubernetes 调度器将 redis replica pod 分配到节点 2 的输出](img/B17338_05_14.jpg)'
- en: 'Figure 5.14: When the node is available again, the Pending pods are assigned
    to that node'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14：当节点再次可用时，待处理的 pods 会被分配到该节点
- en: This shows that after node 2 became available, Kubernetes scheduled the pod
    on that node, and then started the container.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明在节点 2 可用后，Kubernetes 将该 pod 调度到该节点，并启动了容器。
- en: In this section, you learned how to diagnose out-of-resource errors. You were
    able to solve the error by adding another node to the cluster. Before moving on
    to the final failure mode, clean up the guestbook deployment.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你学习了如何诊断资源不足错误。你通过向集群中添加另一个节点解决了该错误。在进入最终的故障模式之前，请清理 guestbook 部署。
- en: Note
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In *Chapter 4, Building scalable applications*, the **cluster autoscaler** was
    introduced. The cluster autoscaler will monitor out-of-resource errors and add
    new nodes to the cluster automatically.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 4 章，构建可扩展应用程序* 中，介绍了 **集群自动扩缩器**。集群自动扩缩器会监控资源不足错误，并自动向集群添加新节点。
- en: 'Let''s clean up the guestbook deployment by running the following `delete`
    command:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行以下 `delete` 命令清理 guestbook 部署：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It is now also safe to close the other Cloud Shell window you opened earlier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在也可以安全地关闭你之前打开的另一个 Cloud Shell 窗口。
- en: So far, you have learned how to recover from two failure modes for nodes in
    a Kubernetes cluster. First, you saw how Kubernetes handles a node going offline
    and how the system reschedules pods to a working node. After that, you saw how
    Kubernetes uses requests to manage the scheduling of pods on a node, and what
    happens when a cluster is out of resources. In the next section, you'll learn
    about another failure mode in Kubernetes, namely what happens when Kubernetes
    encounters storage mounting issues.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经学习了如何从Kubernetes集群中的两种节点故障模式中恢复。首先，您看到Kubernetes如何处理节点离线，并将Pod重新调度到工作节点。之后，您看到了Kubernetes如何使用请求来管理Pod在节点上的调度，并且当集群资源不足时会发生什么。在接下来的部分中，您将了解Kubernetes遇到存储挂载问题时会发生什么。
- en: Fixing storage mount issues
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修复存储挂载问题
- en: Earlier in this chapter, you noticed how the guestbook application lost data
    when the Redis master was moved to another node. This happened because that sample
    application didn't use any persistent storage. In this section, you'll see an
    example of how PVCs can be used to prevent data loss when Kubernetes moves a pod
    to another node. You will see a common error that occurs when Kubernetes moves
    pods with PVCs attached, and you'll learn how to fix this.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面，您注意到当Redis主节点移动到另一个节点时，访客留言应用程序丢失了数据。这是因为该示例应用程序没有使用任何持久存储。在本节中，您将看到一个示例，说明当Kubernetes将附有PVC的Pod移动到另一个节点时会发生的常见错误，并学习如何修复这些错误。
- en: 'For this, you will reuse the WordPress example from the previous chapter. Before
    starting, let''s make sure that the cluster is in a clean state:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，您将重复使用上一章中的WordPress示例。在开始之前，让我们确保集群处于干净的状态：
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This should show you just the one Kubernetes service, as in *Figure 5.15*:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该只显示一个Kubernetes服务，如图 *5.15* 所示：
- en: '![Checking the status of the cluster using the kubectl get all command](img/B17338_05_15.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![使用kubectl get all命令检查集群的状态](img/B17338_05_15.jpg)'
- en: 'Figure 5.15: You should only have the one Kubernetes service running for now'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15：目前您只应该有一个运行中的Kubernetes服务
- en: 'Let''s also ensure that both nodes are running and Ready:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确保两个节点都在运行并处于Ready状态：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This should show us both nodes in a Ready state, as in *Figure 5.16*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该显示我们两个节点都处于Ready状态，如图 *5.16* 所示：
- en: '![Checking the status of both nodes using the kubectl get nodes command](img/B17338_05_16.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![使用kubectl get nodes命令检查两个节点的状态](img/B17338_05_16.jpg)'
- en: 'Figure 5.16: You should have two nodes available in your cluster'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16：您应该在集群中有两个可用的节点
- en: In the previous example, under the *Handling node failures* section, you saw
    that the messages stored in `redis-master` are lost if the pod gets restarted.
    The reason for this is that `redis-master` stores all data in its container, and
    whenever it is restarted, it uses the clean image without the data. In order to
    survive reboots, the data has to be stored outside. Kubernetes uses PVCs to abstract
    the underlying storage provider to provide this external storage.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，在 *处理节点故障* 部分下，您看到如果Pod重新启动，存储在 `redis-master` 中的消息将丢失。原因是 `redis-master`
    将所有数据存储在其容器中，每次重新启动时都使用不带数据的清洁镜像。为了在重新启动时保留数据，数据必须存储在外部。Kubernetes使用PVC来抽象底层存储提供程序，以提供这种外部存储。
- en: To start this example, set up the WordPress installation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动此示例，请设置WordPress安装。
- en: Starting the WordPress installation
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启动WordPress安装
- en: Let's start by installing WordPress. We will demonstrate how it works and then
    verify that storage is still present after a reboot.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始安装WordPress。我们将演示其工作原理，然后验证在重新启动后存储是否仍然存在。
- en: 'If you have not done so yet in a previous chapter, add the Helm repository
    for Bitnami:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在之前的章节中还没有执行此操作，请添加Bitnami的Helm仓库：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Begin reinstallation by using the following command:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令开始重新安装：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will take a couple of minutes to process. You can follow the status of
    this installation by executing the following command:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要几分钟的时间来处理。您可以通过执行以下命令来跟踪安装的状态：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After a couple of minutes, this should show you two pods with a status of Running
    and with a ready status of 1/1 for both pods, as shown in *Figure 5.17*:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，这应该显示两个Pod都处于Running状态，并且两个Pod的Ready状态为1/1，如图 *5.17* 所示：
- en: '![Using kubectl get pods -w to follow the progress of WordPress installation](img/B17338_05_17.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![使用kubectl get pods -w跟踪WordPress安装的进度](img/B17338_05_17.jpg)'
- en: 'Figure 5.17: All pods will have the status of Running after a couple of minutes'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17：几分钟后，所有的Pod都将处于运行状态
- en: You might notice that the `wp-wordpress` pod went through an Error status and
    was restarted afterward. This is because the `wp-mariadb` pod was not ready in
    time, and `wp-wordpress` went through a restart. You will learn more about readiness
    and how this can influence pod restarts in *Chapter 7, Monitoring the AKS cluster
    and the application*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，`wp-wordpress` pod 曾经历过错误状态，并在之后重启。这是因为 `wp-mariadb` pod 没有及时准备好，导致 `wp-wordpress`
    pod 进行了重启。你将在*第 7 章，监控 AKS 集群和应用程序*中学到更多关于就绪性的信息，以及它如何影响 pod 重启。
- en: In this section, you saw how to install WordPress. Now, you will see how to
    avoid data loss using persistent volumes.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你学习了如何安装 WordPress。接下来，你将了解如何使用持久化卷避免数据丢失。
- en: Using persistent volumes to avoid data loss
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用持久化卷避免数据丢失
- en: 'A **persistent volume** (**PV**) is the way to store persistent data in the
    cluster with Kubernetes. PVs were discussed in more detail in *Chapter 3, Application
    deployment on AKS*. Let''s explore the PVs created for the WordPress deployment:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**持久卷**（**PV**）是在 Kubernetes 集群中存储持久数据的方式。有关 PV 的详细讨论，请参考*第 3 章，AKS 上的应用部署*。让我们一起探讨为
    WordPress 部署创建的 PV：'
- en: 'You can get the PersistentVolumeClaims using the following command:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用以下命令获取 PersistentVolumeClaims：
- en: '[PRE16]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will generate an output as shown in *Figure 5.18*:'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成如*图 5.18*所示的输出：
- en: '![Fetching the details of the PersistentVolumeClaims using the kubectl get
    pvc command](img/B17338_05_18.jpg)'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用 kubectl get pvc 命令获取 PersistentVolumeClaims 的详细信息](img/B17338_05_18.jpg)'
- en: '[PRE17]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will show you the two PersistentVolumes:'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示你两个 PersistentVolumes：
- en: '![Using the kubectl get pv command to check the created PersistentVolumes](img/B17338_05_19.jpg)'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用 kubectl get pv 命令检查已创建的 PersistentVolumes](img/B17338_05_19.jpg)'
- en: '[PRE18]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will show you the details of that volume, as in *Figure 5.20*:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示该卷的详细信息，如*图 5.20*所示：
- en: '![Using the kubectl describe pv<pv name> command to get details of specific
    PersistentVolumes](img/B17338_05_20.jpg)'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用 kubectl describe pv<pv 名称> 命令获取特定 PersistentVolumes 的详细信息](img/B17338_05_20.jpg)'
- en: 'Figure 5.20: The details of one of the PVs'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.20：某个 PV 的详细信息
- en: Here, you can see which PVC has claimed this volume and what the DiskName is
    in Azure.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，你可以看到哪个 PVC 声明了这个卷以及 Azure 中的磁盘名称是什么。
- en: 'Verify that your site is working:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证你的网站是否正常工作：
- en: '[PRE19]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will show us the public IP of our WordPress site, as seen in *Figure 5.21*:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示我们 WordPress 网站的公网 IP，如*图 5.21*所示：
- en: '![Obtaining the public IP of our WordPress site](img/B17338_05_21.jpg)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![获取我们 WordPress 网站的公网 IP](img/B17338_05_21.jpg)'
- en: 'Figure 5.21: Public IP of the WordPress site'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.21：WordPress 网站的公网 IP
- en: 'If you remember from *Chapter 3, Application deployment of AKS*, Helm showed
    you the commands you need to get the admin credentials for our WordPress site.
    Let''s grab those commands and execute them to log on to the site as follows:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你还记得*第 3 章，AKS 应用部署*中的内容，Helm 向你展示了获取我们 WordPress 网站管理员凭证所需的命令。让我们获取这些命令并执行它们，以便登录网站，操作如下：
- en: '[PRE20]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will show you the username and password, as displayed in *Figure 5.22*:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示用户名和密码，如*图 5.22*所示：
- en: '![Using Helm commands to obtain a username and password to login to the WordPress
    site](img/B17338_05_22.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Helm 命令获取用户名和密码，以便登录 WordPress 网站](img/B17338_05_22.jpg)'
- en: 'Figure 5.22: Getting the username and password for the WordPress application'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.22：获取 WordPress 应用程序的用户名和密码
- en: 'You can log in to our site via the following address: `http://<external-ip>/admin`.
    Log in here with the credentials from the previous step. Then you can go ahead
    and add a post to your website. Click the Write your first blog post button, and
    then create a short post, as shown in *Figure 5.23*:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下地址登录我们的网站：`http://<external-ip>/admin`。使用上一步获取的凭证登录。然后，你可以开始为你的网站添加一篇帖子。点击“写下你的第一篇博客文章”按钮，然后创建一个简短的帖子，如*图
    5.23*所示：
- en: '![Writing your first blog post on the WordPress website](img/B17338_05_23.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![在 WordPress 网站上写下你的第一篇博客文章](img/B17338_05_23.jpg)'
- en: 'Figure 5.23: Writing your first blog post'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.23：写下你的第一篇博客文章
- en: 'Type some text now and hit the Publish button, as shown in *Figure 5.24*. The
    text itself isn''t important; you are writing this to verify that data is indeed
    persisted to disk:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在输入一些文本并点击发布按钮，如*图 5.24*所示。文本本身并不重要；你是写这篇文章来验证数据是否确实持久化到磁盘：
- en: '![Using the Publish button to publish a post with random text on the WordPress
    website](img/B17338_05_24.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![使用发布按钮将带有随机文本的帖子发布到 WordPress 网站](img/B17338_05_24.jpg)'
- en: 'Figure 5.24: Publishing a post with random text'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.24：发布一篇带有随机文本的帖子
- en: 'If you now head over to the main page of your website at `http://<external-ip>`,
    you''ll see your test post as shown in *Figure 5.25*:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在访问网站的主页 `http://<external-ip>`，你会看到你的测试文章，如*图 5.25*所示：
- en: '![Using the website’s external IP to navigate to the WordPress website and
    verify the published post](img/B17338_05_25.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![使用网站的外部 IP 导航到 WordPress 网站并验证已发布的文章](img/B17338_05_25.jpg)'
- en: 'Figure 5.25: The published blog post appears on the home page'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.25：已发布的博客文章出现在主页上
- en: In this section, you deployed a WordPress site, you logged in to your WordPress
    site, and you created a post. You will verify whether this post survives a node
    failure in the next section.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你部署了一个 WordPress 网站，登录了你的 WordPress 网站，并创建了一个文章。你将在下一节中验证该文章是否能在节点故障时存活。
- en: Handling pod failure with PVC involvement
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理涉及 PVC 的 Pod 故障
- en: 'The first test you''ll do with the PVCs is to kill the pods and verify whether
    the data has indeed persisted. To do this, let''s do two things:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你对 PVC 进行的第一个测试是杀死 Pod，并验证数据是否确实已持久化。为此，让我们做两件事：
- en: '**Watch the pods in your application**: To do this, use the current Cloud Shell
    and execute the following command:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**监控你应用程序中的 Pod**：为此，请使用当前的 Cloud Shell 并执行以下命令：'
- en: '[PRE21]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`watch` command that you executed earlier. You should see an output like what''s
    shown in *Figure 5.27*:![Kubernetes creates new pods to recover from the pod outage
    caused due to the deletion of pods](img/B17338_05_27.jpg)'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你之前执行的 `watch` 命令。你应该看到类似*图 5.27*所示的输出：![Kubernetes 创建新 Pod 以从 Pod 故障中恢复，该故障是由于删除
    Pod 引起的](img/B17338_05_27.jpg)
- en: 'Figure 5.27: After deleting the pods, Kubernetes will automatically recreate
    both pods'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.27：删除 Pod 后，Kubernetes 会自动重新创建这两个 Pod
- en: As you can see, the two original pods went into a Terminating state. Kubernetes
    quickly started creating new pods to recover from the pod outage. The pods went
    through a similar life cycle as the original ones, going from Pending to ContainerCreating
    to Running.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，两个原始 Pod 进入了 Terminating 状态。Kubernetes 很快开始创建新的 Pod 以恢复 Pod 故障。Pod 经历了与原始
    Pod 类似的生命周期，从 Pending 到 ContainerCreating，再到 Running。
- en: If you head on over to your website, you should see that your demo post has
    been persisted. This is how PVCs can help you prevent data loss, as they persist
    data that would not have been persisted in the pod itself.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你访问你的网站，你应该看到你的演示文章已经持久化。这就是 PVC 如何帮助你防止数据丢失的方式，因为它们持久化了本应不会在 Pod 中持久化的数据。
- en: In this section, you've learned how PVCs can help when pods get recreated on
    the same node. In the next section, you'll see how PVCs are used when a node has
    a failure.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你了解了 PVC 如何帮助当 Pod 在同一节点上重新创建时。在下一节中，你将看到当节点发生故障时，PVC 是如何使用的。
- en: Handling node failure with PVC involvement
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理涉及 PVC 的节点故障
- en: 'In the previous example, you saw how Kubernetes can handle pod failures when
    those pods have a PV attached. In this example, you''ll learn how Kubernetes handles
    node failures when a volume is attached:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，你看到 Kubernetes 如何处理具有 PV 附加的 Pod 故障。在本示例中，你将了解 Kubernetes 如何处理附加卷的节点故障。
- en: 'Let''s first check which node is hosting your application, using the following
    command:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先让我们检查哪个节点托管了你的应用程序，使用以下命令：
- en: '[PRE22]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the example shown in *Figure 5.28*, node 2 was hosting MariaDB, and node
    0 was hosting the WordPress site:'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在*图 5.28*中所示的示例中，节点 2 托管了 MariaDB，节点 0 托管了 WordPress 网站：
- en: '![Checking the node that is hosting your application](img/B17338_05_28.jpg)'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![检查托管你应用程序的节点](img/B17338_05_28.jpg)'
- en: 'Figure 5.28: Check which node hosts the WordPress site'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.28：检查托管 WordPress 网站的节点
- en: Introduce a failure and stop the node that is hosting the WordPress pod using
    the Azure portal. You can do this in the same way as in the earlier example. First,
    look for the scale set backing your cluster, as shown in *Figure 5.29*:![Searching
    for vmss in the azure search bar, and selecting the scale set used by your cluster](img/B17338_05_29.jpg)
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 引入故障并通过 Azure 门户停止托管 WordPress Pod 的节点。你可以按照之前的示例中的相同方式进行操作。首先，查找支持你集群的扩展集，如*图
    5.29*所示：![在 Azure 搜索栏中搜索 vmss，并选择集群使用的扩展集](img/B17338_05_29.jpg)
- en: 'Figure 5.29: Looking for the scale set hosting your cluster'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.29：查找托管你集群的扩展集
- en: Then shut down the node, by clicking on Instances in the left-hand menu, then
    selecting the node you need to shut down and clicking the Stop button, as shown
    in *Figure 5.30*:![Shutting down the desired node through the Instances pane of
    the scale set used by your cluster](img/B17338_05_30.jpg)
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，通过点击左侧菜单中的“Instances”，选择需要关闭的节点并点击停止按钮，关闭节点，如 *图 5.30* 所示：![通过集群使用的缩放集的实例面板关闭目标节点](img/B17338_05_30.jpg)
- en: 'Figure 5.30: Shutting down the node'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.30：关闭节点
- en: 'After this action, once again, watch the pods to see what is happening in the
    cluster:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行此操作后，再次观察 pod，查看集群中发生了什么：
- en: '[PRE23]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As in the previous example, it is going to take 5 minutes before Kubernetes
    will start taking action against the failed node. You can see that happening in
    *Figure 5.31*:'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前例所示，Kubernetes 将需要 5 分钟的时间才会开始对失败的节点采取行动。你可以在 *图 5.31* 中看到这一过程：
- en: '![The status of the pod indicates that it is stuck in a ContainerCreating state](img/B17338_05_31.jpg)'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![pod 状态显示其处于 ContainerCreating 状态](img/B17338_05_31.jpg)'
- en: 'Figure 5.31: A pod in a ContainerCreating state'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.31：处于 ContainerCreating 状态的 pod
- en: 'You are seeing a new issue here. The new pod is stuck in a ContainerCreating
    state. Let''s figure out what is happening here. First, describe that pod:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里出现了一个新的问题。新的 pod 被卡在了 ContainerCreating 状态。让我们来弄清楚发生了什么。首先，描述这个 pod：
- en: '[PRE24]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You will get an output as shown in *Figure 5.32*:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将看到如下所示的输出，类似于 *图 5.32*：
- en: '![Using the kubectl describe command to understand the issue with the pod stuck
    in theContainerCreating state](img/B17338_05_32.jpg)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用 kubectl describe 命令查看处于 ContainerCreating 状态的 pod 问题](img/B17338_05_32.jpg)'
- en: 'Figure 5.32: Output explaining why the pod is in a ContainerCreating state'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.32：解释 pod 处于 ContainerCreating 状态的输出
- en: 'This tells you that there is a problem with the volume. You see two errors
    related to that volume: the `FailedAttachVolume` error explains that the volume
    is already used by another pod, and `FailedMount` explains that the current pod
    cannot mount the volume. You can solve this by manually forcefully removing the
    old pod stuck in the `Terminating` state.'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这表明卷存在问题。你看到与该卷相关的两个错误：`FailedAttachVolume` 错误说明该卷已经被另一个 pod 使用，而 `FailedMount`
    错误说明当前 pod 无法挂载该卷。你可以通过手动强制删除卡在 `Terminating` 状态的旧 pod 来解决此问题。
- en: Note
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The behavior of the pod stuck in the `Terminating` state is not a bug. This
    is default Kubernetes behavior. The Kubernetes documentation states the following:
    *"Kubernetes (versions 1.5 or newer) will not delete pods just because a Node
    is unreachable. The pods running on an unreachable Node enter the Terminating
    or Unknown state after a timeout. Pods may also enter these states when the user
    attempts the graceful deletion of a pod on an unreachable Node."* You can read
    more at [https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/](https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/).'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: pod 处于 `Terminating` 状态的行为并不是一个 bug。这是 Kubernetes 的默认行为。Kubernetes 文档中说明如下：*“Kubernetes（版本
    1.5 或更高）不会仅仅因为节点不可达而删除 pod。运行在不可达节点上的 pod 会在超时后进入 Terminating 或 Unknown 状态。当用户尝试在不可达节点上优雅删除
    pod 时，pod 也可能进入这些状态。”* 你可以在 [https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/](https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/)
    阅读更多内容。
- en: 'To forcefully remove the terminating pod from the cluster, get the full pod
    name using the following command:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要强制从集群中移除终止中的 pod，请使用以下命令获取完整的 pod 名称：
- en: '[PRE25]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will show you an output similar to *Figure 5.33*:'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示类似于 *图 5.33* 的输出：
- en: '![Fetching the name of the pod stuck in the Terminating state](img/B17338_05_33.jpg)'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![获取处于 Terminating 状态的 pod 名称](img/B17338_05_33.jpg)'
- en: 'Figure 5.33: Getting the name of the pod stuck in the Terminating state'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.33：获取处于 Terminating 状态的 pod 名称
- en: 'Use the pod''s name to force the deletion of this pod:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pod 的名称强制删除此 pod：
- en: '[PRE26]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After the pod has been deleted, it will take a couple of minutes for the other
    pod to enter a Running state. You can monitor the state of the pod using the following
    command:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 pod 被删除后，其他 pod 进入 Running 状态大约需要几分钟时间。你可以使用以下命令监控 pod 的状态：
- en: '[PRE27]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This will return an output similar to *Figure 5.34*:'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将返回类似于 *图 5.34* 的输出：
- en: '![The new WordPress pod returning to a Running state](img/B17338_05_34.jpg)'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![新的 WordPress pod 返回到 Running 状态](img/B17338_05_34.jpg)'
- en: 'Figure 5.34: The new WordPress pod returning to a Running state'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.34：新的 WordPress pod 返回到 Running 状态
- en: 'As you can see, this brought the new pod to a healthy state. It did take a
    couple of minutes for the system to pick up the changes and then mount the volume
    to the new pod. Let''s get the details of the pod again using the following command:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，这使得新Pod恢复到了健康状态。系统确实花了几分钟时间来识别更改并将卷挂载到新Pod。让我们使用以下命令再次查看Pod的详细信息：
- en: '[PRE28]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This will generate an output as follows:'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成如下输出：
- en: '![The new pod is now attaching the volume and pulling the container image](img/B17338_05_35.jpg)'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![新Pod现在正在挂载卷并拉取容器镜像](img/B17338_05_35.jpg)'
- en: 'Figure 5.35: The new pod is now attaching the volume and pulling the container
    image'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.35：新Pod现在正在挂载卷并拉取容器镜像
- en: 'This shows you that the new pod successfully got the volume attached and that
    the container image got pulled. This also made your WordPress website available
    again, which you can verify by browsing to the public IP. Before continuing to
    the next chapter, clean up the application using the following command:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这表明新Pod成功地挂载了卷，并且容器镜像已经被拉取。这也使您的WordPress网站重新可用，您可以通过访问公共IP来验证这一点。在继续下一章之前，请使用以下命令清理应用程序：
- en: '[PRE29]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s also start the node that was shut down: go back to the scale set pane
    in the Azure portal, click Instances in the left-hand menu, select the node you
    need to start, and click on the Start button, as shown in *Figure 5.36*:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也启动已关闭的节点：返回Azure门户中的规模集面板，点击左侧菜单中的实例，选择需要启动的节点，然后点击“启动”按钮，如*图5.36*所示：
- en: '![Using the Instances pane of the selected VMSS to start the node that was
    shut down](img/B17338_05_36.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![使用所选VMSS的实例面板启动已关闭的节点](img/B17338_05_36.jpg)'
- en: 'Figure 5.36: Starting node 0 again'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.36：重新启动节点0
- en: In this section, you learned how you can recover from a node failure when PVCs
    aren't mounting to new pods. All you needed to do was forcefully delete the pod
    that was stuck in the `Terminating` state.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您学习了如何在PVC未能挂载到新Pod时从节点故障中恢复。您所需要做的就是强制删除卡在`Terminating`状态的Pod。
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about common Kubernetes failure modes and how you
    can recover from them. This chapter started with an example of how Kubernetes
    automatically detects node failures and how it will start new pods to recover
    the workload. After that, you scaled out your workload and had your cluster run
    out of resources. You recovered from that situation by starting the failed node
    again to add new resources to the cluster.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了Kubernetes中的常见故障模式以及如何从中恢复。本章从Kubernetes如何自动检测节点故障并启动新Pod以恢复工作负载的示例开始。接着，您扩展了工作负载，并让集群资源耗尽。您通过重新启动故障节点并为集群添加新资源来恢复了这种情况。
- en: Next, you saw how PVs are useful to store data outside of a pod. You deleted
    all pods on the cluster and saw how the PV ensured that no data was lost in your
    application. In the final example in this chapter, you saw how you can recover
    from a node failure when PVs are attached. You were able to recover the workload
    by forcefully deleting the terminating pod. This brought your workload back to
    a healthy state.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您看到如何使用PVs将数据存储在Pod外部。您删除了集群中的所有Pod，并查看了PVs如何确保您的应用程序中的数据没有丢失。在本章的最后一个示例中，您看到了如何在PVs被挂载时从节点故障中恢复。您通过强制删除终止中的Pod成功恢复了工作负载。这使得您的工作负载恢复到了健康状态。
- en: This chapter has explained common failure modes in Kubernetes. In the next chapter,
    we will introduce HTTPS support to our services and introduce authentication with
    Azure Active Directory.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 本章已经解释了Kubernetes中的常见故障模式。在下一章中，我们将为我们的服务引入HTTPS支持，并介绍与Azure Active Directory的认证。
