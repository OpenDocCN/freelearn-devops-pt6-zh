["```\n$ helm repo add kyverno https://kyverno.github.io/kyverno/\n\"kyverno\" has been added to your repositories\n$ helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"kyverno\" chart repository\nUpdate Complete. ![](img/B18998_09_001.png)Happy Helming!![](img/B18998_09_001.png) \n```", "```\n$ helm install kyverno kyverno/kyverno -n kyverno --create-namespace\nNAME: kyverno\nLAST DEPLOYED: Sat Dec 31 15:34:11 2022\nNAMESPACE: kyverno\nSTATUS: deployed\nREVISION: 1\nNOTES:\nChart version: 2.6.5\nKyverno version: v1.8.5\nThank you for installing kyverno! Your release is named kyverno.\n![](img/B18998_16_001.png) WARNING: Setting replicas count below 3 means Kyverno is not running in high availability mode.\n![](img/B18998_16_002.png) Note: There is a trade-off when deciding which approach to take regarding Namespace exclusions. Please see the documentation at https://kyverno.io/docs/installation/#security-vs-operability to understand the risks. \n```", "```\n$ k get-all -n kyverno\nNAME                                                          NAMESPACE  AGE\nconfigmap/kube-root-ca.crt                                    kyverno    2m27s\nconfigmap/kyverno                                             kyverno    2m26s\nconfigmap/kyverno-metrics                                     kyverno    2m26s\nendpoints/kyverno-svc                                         kyverno    2m26s\nendpoints/kyverno-svc-metrics                                 kyverno    2m26s\npod/kyverno-7c444878f7-gfht8                                  kyverno    2m26s\nsecret/kyverno-svc.kyverno.svc.kyverno-tls-ca                 kyverno    2m22s\nsecret/kyverno-svc.kyverno.svc.kyverno-tls-pair               kyverno    2m21s\nsecret/sh.helm.release.v1.kyverno.v1                          kyverno    2m26s\nserviceaccount/default                                        kyverno    2m27s\nserviceaccount/kyverno                                        kyverno    2m26s\nservice/kyverno-svc                                           kyverno    2m26s\nservice/kyverno-svc-metrics                                   kyverno    2m26s\ndeployment.apps/kyverno                                       kyverno    2m26s\nreplicaset.apps/kyverno-7c444878f7                            kyverno    2m26s\nlease.coordination.k8s.io/kyverno                             kyverno    2m23s\nlease.coordination.k8s.io/kyverno-health                      kyverno    2m13s\nlease.coordination.k8s.io/kyvernopre                          kyverno    2m25s\nlease.coordination.k8s.io/kyvernopre-lock                     kyverno    2m24s\nendpointslice.discovery.k8s.io/kyverno-svc-7ghzl              kyverno    2m26s\nendpointslice.discovery.k8s.io/kyverno-svc-metrics-qflr5      kyverno    2m26s\nrolebinding.rbac.authorization.k8s.io/kyverno:leaderelection  kyverno    2m26s\nrole.rbac.authorization.k8s.io/kyverno:leaderelection         kyverno    2m26s \n```", "```\n$ k get crd\nNAME                                      CREATED AT\nadmissionreports.kyverno.io               2022-12-31T23:34:12Z\nbackgroundscanreports.kyverno.io          2022-12-31T23:34:12Z\nclusteradmissionreports.kyverno.io        2022-12-31T23:34:12Z\nclusterbackgroundscanreports.kyverno.io   2022-12-31T23:34:12Z\nclusterpolicies.kyverno.io                2022-12-31T23:34:12Z\nclusterpolicyreports.wgpolicyk8s.io       2022-12-31T23:34:12Z\ngeneraterequests.kyverno.io               2022-12-31T23:34:12Z\npolicies.kyverno.io                       2022-12-31T23:34:12Z\npolicyreports.wgpolicyk8s.io              2022-12-31T23:34:12Z\nupdaterequests.kyverno.io                 2022-12-31T23:34:12Z \n```", "```\n$ k get validatingwebhookconfigurations\nNAME                                      WEBHOOKS   AGE\nkyverno-policy-validating-webhook-cfg     1          40m\nkyverno-resource-validating-webhook-cfg   1          40m\n$ k get mutatingwebhookconfigurations\nNAME                                    WEBHOOKS   AGE\nkyverno-policy-mutating-webhook-cfg     1          40m\nkyverno-resource-mutating-webhook-cfg   0          40m\nkyverno-verify-mutating-webhook-cfg     1          40m \n```", "```\n$ helm install kyverno-policies kyverno/kyverno-policies -n kyverno-policies --create-namespace\nNAME: kyverno-policies\nLAST DEPLOYED: Sat Dec 31 15:48:26 2022\nNAMESPACE: kyverno-policies\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThank you for installing kyverno-policies 2.6.5 ![](img/B18998_16_003.png)\nWe have installed the \"baseline\" profile of Pod Security Standards and set them in audit mode.\nVisit https://kyverno.io/policies/ to find more sample policies. \n```", "```\n$ k get clusterpolicies.kyverno.io\nNAME                             BACKGROUND   VALIDATE ACTION   READY\ndisallow-capabilities            true         audit             true\ndisallow-host-namespaces         true         audit             true\ndisallow-host-path               true         audit             true\ndisallow-host-ports              true         audit             true\ndisallow-host-process            true         audit             true\ndisallow-privileged-containers   true         audit             true\ndisallow-proc-mount              true         audit             true\ndisallow-selinux                 true         audit             true\nrestrict-apparmor-profiles       true         audit             true\nrestrict-seccomp                 true         audit             true\nrestrict-sysctls                 true         audit             true \n```", "```\n$ k get cm kyverno -o yaml -n kyverno | yq .data\nresourceFilters: '[*,kyverno,*][Event,*,*][*,kube-system,*][*,kube-public,*][*,kube-node-lease,*][Node,*,*][APIService,*,*][TokenReview,*,*][SubjectAccessReview,*,*][SelfSubjectAccessReview,*,*][Binding,*,*][ReplicaSet,*,*][AdmissionReport,*,*][ClusterAdmissionReport,*,*][BackgroundScanReport,*,*][ClusterBackgroundScanReport,*,*][ClusterRole,*,kyverno:*][ClusterRoleBinding,*,kyverno:*][ServiceAccount,kyverno,kyverno][ConfigMap,kyverno,kyverno][ConfigMap,kyverno,kyverno-metrics][Deployment,kyverno,kyverno][Job,kyverno,kyverno-hook-pre-delete][NetworkPolicy,kyverno,kyverno][PodDisruptionBudget,kyverno,kyverno][Role,kyverno,kyverno:*][RoleBinding,kyverno,kyverno:*][Secret,kyverno,kyverno-svc.kyverno.svc.*][Service,kyverno,kyverno-svc][Service,kyverno,kyverno-svc-metrics][ServiceMonitor,kyverno,kyverno-svc-service-monitor][Pod,kyverno,kyverno-test]'\nwebhooks: '[{\"namespaceSelector\": {\"matchExpressions\": \n[{\"key\":\"kubernetes.io/metadata.name\",\"operator\":\"NotIn\",\"values\":[\"kyverno\"]}]}}]' \n```", "```\n$ k get deploy kyverno -n kyverno -o yaml | yq '.spec.template.spec.containers[0].args'\n- --autogenInternals=true\n- --loggingFormat=text \n```", "```\n$ k get clusterpolicies.kyverno.io disallow-capabilities\nNAME                    BACKGROUND   VALIDATE ACTION   READY\ndisallow-capabilities   true         audit             true \n```", "```\n$ cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: some-pod\nspec:\n  containers:\n  - name: some-container\n    command: [ \"sleep\", \"999999\" ]\n    image: g1g1/py-kube:0.3\n    securityContext:\n      capabilities:\n        add: [\"NET_ADMIN\"]\nEOF    \npod/some-pod created \n```", "```\n$ docker exec -it kind-control-plane sh\n# \n```", "```\n# ps aux | grep 'PID\\|sleep' | grep -v grep\nUSER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot        4549  0.0  0.0 148276  6408 ?        Ssl  02:54   0:00 /usr/bin/qemu-x86_64 /bin/sleep 999999 \n```", "```\n# getpcaps 4549\n4549: cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_admin,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap=ep \n```", "```\n$ k get clusterpolicies.kyverno.io disallow-capabilities -o yaml | yq .spec.validationFailureAction\naudit \n```", "```\n$ k delete po some-pod\npod \"some-pod\" deleted\n$ k patch clusterpolicies.kyverno.io disallow-capabilities --type merge -p '{\"spec\": {\"validationFailureAction\": \"enforce\"}}'\nclusterpolicy.kyverno.io/disallow-capabilities patched \n```", "```\n$ cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: some-pod\nspec:\n  containers:\n  - name: some-container\n    command: [ \"sleep\", \"999999\" ]\n    image: g1g1/py-kube:0.3\n    securityContext:\n      capabilities:\n        add: [\"NET_ADMIN\"]\nEOF\nError from server: error when creating \"STDIN\": admission webhook \"validate.kyverno.svc-fail\" denied the request:\npolicy Pod/kyverno-policies/some-pod for resource violation:\ndisallow-capabilities:\n  adding-capabilities: Any capabilities added beyond the allowed list (AUDIT_WRITE,\n    CHOWN, DAC_OVERRIDE, FOWNER, FSETID, KILL, MKNOD, NET_BIND_SERVICE, SETFCAP, SETGID,\n    SETPCAP, SETUID, SYS_CHROOT) are disallowed. \n```", "```\nspec:\n  rules:\n  - name: some-rule\n    match:\n      any:\n      - resources:\n          kinds: \n          - Service\n          names: \n          - \"service-1\" \n          - \"service-2\"\n      - resources:\n          kinds: \n          - Service\n          namespaces:\n          - \"ns-1\" \n```", "```\n rules:\n    - name: some-rule\n      match:\n        all:\n          - resources:\n              kinds:\n                - Service\n              names:\n                - \"service-1\"\n            clusterRoles:\n              - some-cluster-role \n```", "```\nrules:\n- name: some-rule\n  match:\n    any:\n      - resources:\n          kinds:\n            - Service\n  exclude:\n    any:\n      - resources:\n          namespaces:\n            - \"ns-1\" \n```", "```\n rules:\n  - name: memory-limit\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n    preconditions:\n      any:\n      - key: \"{{request.object.spec.containers[*].resources.requests.memory}}\"\n        operator: LessThan\n        value: 1Gi \n```", "```\n validate:\n      message: \"The resource must have a label named `app`.\"\n      pattern:\n        metadata:\n          labels:\n            some-label: \"app\" \n```", "```\n rules:\n    - name: validate-replica-count\n      match:\n        any:\n        - resources:\n            kinds:\n            - Deployment\n      validate:\n        message: \"Replica count for a Deployment must be at least 3.\"\n        pattern:\n          spec:\n            replicas: \">=3\" \n```", "```\nrules:\n  - name: block-deletes-of-deployments-and-statefulsets\n    match:\n      any:\n      - resources:\n          kinds:\n            - Deployment\n            - Statefulset\n    validate:\n      message: \"Deleting {{request.oldObject.kind}}/{{request.oldObject.metadata.name}} is not allowed\"\n      deny:\n        conditions:\n          any:\n          - key: \"{{request.operation}}\"\n            operator: Equals\n            value: DELETE \n```", "```\nmutate:\n  patchStrategicMerge:\n    spec:\n      containers:\n        # match images which end with :latest\n        - (image): \"*:latest\"\n          # set the imagePullPolicy to \"IfNotPresent\"\n          imagePullPolicy: \"IfNotPresent\"```", "```\n\nThe other flavor of mutation is JSON Patch ([http://jsonpatch.com](http://jsonpatch.com)), which is specified in RFC 6902 ([https://datatracker.ietf.org/doc/html/rfc6902](https://datatracker.ietf.org/doc/html/rfc6902)). JSON Patch has similar semantics to preconditions and deny rules. The patch has an operation, path, and value. It applies the operation to the patch with the value. The operation can be one of:\n\n*   `add`\n*   `remove`\n*   `replace`\n*   `copy`\n*   `move`\n*   `test`\n\nHere is an example of adding some data to a config map using JSON Patch. It adds multiple fields to the `/data/properties` path and a single value to the `/data/key` path:\n\n```", "```\n\n### Generating resources\n\nGenerating resources is an interesting use case. Whenever a request comes in, Kyverno may create new resources instead of mutating or validating the request (other policies may validate or mutate the original request).\n\nA policy with a `generate` rule has the same `match` and/or `exclude` statements as other policies. This means it can be triggered by any resource request as well as existing resources. However, instead of validating or mutating, it generates a new resource when the origin resource is created. A `generate` rule has an important property called `synchronize`. When `synchronize` is true, the generated resource is always in sync with the origin resource (when the origin resource is deleted, the generated resource is deleted as well). Users can’t modify or delete a generated resource. When `synchronize` is false, Kyverno doesn’t keep track of the generated resource, and users can modify or delete it at will.\n\nHere is a `generate` rule that creates a `NetworkPolicy` that prevents any traffic when a new `Namespace` is created. Note the `data` field, which defines the generated resource:\n\n```", "```\n\nWhen generating resources for an existing origin resource instead of a `data` field, a `clone` field is used. For example, if we have a config map called `config-template` in the `default` namespace, the following `generate` rule will clone that config map into every new namespace:\n\n```", "```\n\nIt’s also possible to clone multiple resources by using a `cloneList` field instead of a `clone` field.\n\n### Advanced policy rules\n\nKyverno has some additional advanced capabilities, such as external data sources and autogen rules for pod controllers.\n\n#### External data sources\n\nSo far we’ve seen how Kyverno uses information from an admission review object to perform validation, mutation, and generation. However, sometimes additional data is needed. This is done by defining a context field with variables that can be populated from an external config map, the Kubernetes API server, or an image registry.\n\nHere is an example of defining a variable called `dictionary` and using it to mutate a pod and add a label called `environment`, where the value comes from the config map variable:\n\n```", "```\n\nThe way it works is that the context named “dictionary” points to a config map. Inside the config map there is a section called “data” with a key called “env”.\n\n#### Autogen rules for pod controllers\n\nPods are one of the most common resources to apply policies to. However, pods can be created indirectly by many types of resources: Pods (directly), Deployments, StatefulSets, DaemonSets, and Jobs. If we want to verify that every pod has a label called “app” then we will be forced to write complex match rules with an `any` statement that covers all the various resources that create pods. Kyverno provides a very elegant solution in the form of autogen rules for pod controllers.\n\nThe auto-generated rules can be observed in the status of the policy object. We will see an example in the next section.\n\nWe covered in detail a lot of the powerful capabilities Kyverno brings to the table. Let’s write some policies and see them in action.\n\n## Writing and testing Kyverno policies\n\nIn this section, we will actually write some Kyverno policies and see them in action. We will use some of the rules we explored in the previous section and embed them in full-fledged policies, apply the policies, create resources that comply with the policies as well as resources that violate the policies (in the case of validating policies), and see the outcome.\n\n### Writing validating policies\n\nLet’s start with a validating policy that disallows services in the namespace `ns-1` as well as services named `service-1` or `service-2` in any namespace:\n\n```", "```\n\nNow that the policy is in place, let’s try to create a service named “service-1” in the default namespace that violates the policy. Note that there is no need to actually create resources to check the outcome of admission control. It is sufficient to run in dry-run mode as long as the dry-run happens on the server side:\n\n```", "```\n\nAs you can see, the request was rejected, with a nice message from the policy that explains why.\n\nIf we try to do the dry-run on the client side, it succeeds (but doesn’t actually create any service), as the admission control check happens only on the server:\n\n```", "```\n\nNow that we have proved the point, we will use only a server-side dry-run.\n\nLet’s try to create a service called `service-3` in the default namespace, which should be allowed:\n\n```", "```\n\nLet’s try to create `service-3` in the forbidden `ns-1` namespace:\n\n```", "```\n\nYep. That failed as expected. Let’s see what happens if we change the `validationFailureAction` from `Enforce` to `Audit`:\n\n```", "```\n\nHowever, it generated a report of validation failure:\n\n```", "```\n\nNow, the service passes the admission control, but a record of the violation was captured in the policy report. We will look at reports in more detail later in the chapter.\n\nFor now, let’s look at mutating policies.\n\n### Writing mutating policies\n\nMutating policies are a lot of fun. They quietly modify incoming requests to comply with the policy. They don’t cause failures like validating policies in “enforce” mode, and they don’t generate reports you need to scour through like validating policies in “audit” mode. If an invalid or incomplete request comes in, you just change it until it’s valid.\n\nHere is a policy that sets the `imagePullPolicy` to `IfNotPresent` when the tag is `latest` (by default it is `Always`).\n\n```", "```\n\nLet’s see it in action. Note that for a mutating policy, we can’t use dry-run because the whole point is to actually mutate a resource.\n\nThe following pod matches our policy and doesn’t have `imagePullPolicy` set:\n\n```", "```\n\nLet’s verify that the mutation worked and check the container’s `imagePullPolicy`:\n\n```", "```\n\nYes. It was set correctly. Let’s confirm that Kyverno was responsible for setting the `imagePullPolicy` by deleting the policy and then creating another pod:\n\n```", "```\n\nThe Kyverno policy was deleted, and another pod called `another-pod` with the same image `g1g1/py-kube:latest` was created. Let’s see if its `imagePullPolicy` is the expected `Always` (the default for images with the `latest` image tag):\n\n```", "```\n\nYes, it works how it should! Let’s move on to another type of exciting Kyverno policy – a generating policy, which can create new resources out of thin air.\n\n### Writing generating policies\n\nGenerating policies create new resources in addition to the requested resource when a new resource is created. Let’s take our previous example of creating an automatic network policy for new namespaces that prevents any network traffic from coming in and out. This is a cluster policy that applies to any new namespace except the excluded namespaces:\n\n```", "```\n\nThe `deny-all-traffic` Kyverno policy was created successfully. Let’s create a new namespace, `ns-2`, and see if the expected NetworkPolicy is generated:\n\n```", "```\n\nYes, it worked! Kyverno lets you easily generate additional resources.\n\nNow that we have some hands-on experience in creating Kyverno policies, let’s learn about how to test them and why.\n\n## Testing policies\n\nTesting Kyverno policies before deploying them to production is very important because Kyverno policies are very powerful, and they could easily cause outages and incidents if misconfigured by blocking valid requests, allowing invalid requests, improperly mutating resources, and generating resources in the wrong namespaces.\n\nKyverno offers tooling as well as guidance about testing its policies.\n\n### The Kyverno CLI\n\nThe Kyverno CLI is a versatile command-line program that lets you apply policies on the client side and see the results, run tests, and evaluate JMESPath expressions.\n\nFollow these instructions to install the Kyverno CLI: [https://kyverno.io/docs/kyverno-cli/#building-and-installing-the-cli](https://kyverno.io/docs/kyverno-cli/#building-and-installing-the-cli).\n\nVerify that it was installed correctly by checking the version:\n\n```", "```\n\nHere is the help screen if you just type kyverno with no additional command:\n\n```", "```\n\nEarlier in the chapter we saw how to evaluate the results of a validating Kyverno policy without actually creating resources, using a dry-run. This is not possible for mutating or generating policies. With `kyverno apply` we can achieve the same effect for all policy types.\n\nLet’s see how to apply a mutating policy to a resource and examine the results. We will apply the `set-image-pull-policy` to a pod stored in the file `some-pod.yaml`. The policy was defined earlier, and is available in the attached code as the file `mutate-image-pull-policy.yaml`.\n\nFirst, let’s see what the result would be if we just created the pod without applying the Kyverno policy:\n\n```", "```\n\nIt is `Always`. Now, we will apply the Kyverno policy to this pod resource and check the outcome:\n\n```", "```\n\nAs you can see, after the mutating policy is applied to `some-pod`, the `imagePullPolicy` is `IfNotPresent` as expected.\n\nLet’s play with the `kyverno jp` sub-command. It accepts standard input or can take a file.\n\nHere is an example that checks how many arguments the command of the first container in a pod has. We will use this pod manifest as input:\n\n```", "```\n\nNote that it has a command called `sleep` with a single argument, “9999”. We expect the answer to be 1\\. The following command does the trick:\n\n```", "```\n\nHow does it work? First it pipes the content of `some-pod.yaml` to the `kyverno jp` command with the JMESPath expression that takes the length of the command of the first container (an array with two elements, “sleep” and “9000”), and then it pipes it to the `subtract()` function, which subtracts 1 and, hence, ends up with the expected result of 1.\n\nThe Kyverno CLI commands `apply` and `jp` are great for ad hoc exploration and the quick prototyping of complex JMESPath expressions. However, if you use Kyverno policies at scale (and you should), then I recommend a more rigorous testing practice. Luckily Kyverno has good support for testing via the `kyverno test` command. Let’s see how to write and run Kyverno tests.\n\n### Understanding Kyverno tests\n\nThe `kyverno test` command operates on a set of resources and policies governed by a file called `kyverno-test.yaml`, which defines what policy rules should be applied to which resources and what the expected outcome is. It then returns the results.\n\nThe result of applying a policy rule to a resource can be one of the following four:\n\n*   `pass` – the resource matches the policy and doesn’t trigger the `deny` statement (only for validating policies)\n*   `fail` – the resource matches the policy and triggers the deny statement (only for validating policies)\n*   `skip` – the resource doesn’t match the policy definition and the policy wasn’t applied\n*   `warn` – the resource doesn’t comply with the policy but has an annotation: `policies.kyverno.io/scored: \"false\"`\n\nIf the expected outcome of the test doesn’t match the result of applying the policy to the resource, then the test will be considered a failure.\n\nFor mutating and generating policies, the test will include `patchedResource` and `generatedResource` respectively.\n\nLet’s see what the `kyverno-test.yaml` file looks like:\n\n```", "```\n\nMany different test cases can be defined in a single `kyverno-test.yaml` file. The file has five sections:\n\n*   `policies`\n*   `resources`\n*   `variables`\n*   `userInfo`\n*   `results`\n\nThe `policies` and `resources` sections specify paths to all the policies and resources that participate in the tests. The `variables` and `userInfo` optional sections can define additional information that will be used by the test cases.\n\nThe `results` section is where the various test cases are specified. Each test case tests the application of a single policy rule to a single resource. If it’s a validating rule, then the `result` field should contain the expected outcome.\n\nIf it’s a mutating or generating rule, then the corresponding `patchedResource` or `generatedResource` should contain the expected outcome.\n\nLet’s write some Kyverno tests for our policies.\n\n### Writing Kyverno tests\n\nAll the files mentioned here are available in the `tests` sub-directory of the code attached to the chapter.\n\nLet’s start by writing our `kyverno-test.yaml` file:\n\n```", "```\n\nThe `policies` section contains the `disallow-some-services-policy.yaml` file. This policy rejects services named `service-1` or `service-2` and any service in the `ns-1` namespace.\n\nThe `resources` section contains three different files that all contain a Service resource:\n\n*   `test-service-ok.yaml`\n*   `test-service-bad-name.yaml`\n*   `test-service-bad-namespace.yaml`\n\nThe `test-service-ok.yaml` file contains a service that doesn’t match any of the rules of the policy. The `test-service-bad-name.yaml` file contains a service named `service-1`, which is not allowed. Finally, the `test-service-bad-namespace.yaml` file contains a resource named `service-in-ns-1`, which is allowed. However, it has the `ns-1` namespace, which is not allowed.\n\nLet’s look at the `results` section. There are three different test cases here. They all test the same rule in our policy, but each test case uses a different resource name. This comprehensively covers the behavior of the policy.\n\nThe first test case verifies that a service that doesn’t match the rule is skipped. It specifies the policy, the rule name, the resources the test case should be applied to, and most importantly, the expected result, which is `skip` in this case:\n\n```", "```\n\nThe second test case is similar except that the resource name is different and the expected result is `fail`:\n\n```", "```\n\nThere is one more slight difference. In this test case, the kind of the target resource is explicitly specified (`kind: Service`). This may seem redundant at first glance because the `service-1` resource defined in `test-service-bad-name.yaml` already has the `kind` listed:\n\n```", "```\n\nThe reason the `kind` field is needed is to disambiguate which resource is targeted, in case the resource file contains multiple resources with the same name.\n\nThe third test case is the same as the second test case, except it targets a different resource and, as a consequence, a different part of the rule (disallowing services in the `ns-1` namespace):\n\n```", "```\n\nOK. We have our test cases. Let’s see how to run these tests.\n\n### Running Kyverno tests\n\nRunning Kyverno tests is very simple. You just type `kyverno test` and the path to the folder containing a `kyverno-test.yaml` file or a Git repository and a branch.\n\nLet’s run our tests:\n\n```", "```\n\nWe get a nice output that lists each test case and then a one-line summary. All three tests passed, so that’s great.\n\nWhen you have test files that contain a lot of test cases and you try to tweak one specific rule, you may want to run a specific test case only. Here is the syntax:\n\n```", "```\n\nThe `kyverno test` command has very good documentation with a lot of examples. Just type `kyverno test -h`.\n\nSo far, we have written policies, rules, and policy tests and executed them. The last piece of the puzzle is viewing reports when Kyverno is running.\n\n## Viewing Kyverno reports\n\nKyverno generates reports for policies with `validate` or `verifyImages` rules. Only policies in `audit` mode or that have `spec.background: true` will generate reports.\n\nAs you recall Kyverno can generate two types of reports in the form of custom resources. `PolicyReports` are generated for namespace-scoped resources (like services) in the namespace the resource was applied. `ClusterPolicyReports` are generated for cluster-scoped resources (like namespaces).\n\nOur `disallow-some-services` policy has a `validate` rule and operates in `audit` mode, which means that if we create a service that violates the rule, the service will be created, but a report will be generated. Here we go:\n\n```", "```\n\nWe created a service in the forbidden `ns-1` namespace. Kyverno didn’t block the creation of the service because of audit mode. Let’s review the report (that `polr` is shorthand for `policyreports`):\n\n```", "```\n\nA report named `cpol-disallow-some-services` was created. We can see that it counted one failure. What happens if we create another service?\n\n```", "```\n\nYep. Another failure is reported. The meaning of these failures is that the resource failed to pass the `validate` rule. Let’s peek inside. The report has a `metadata` field, which includes an annotation for the policy it represents. Then there is a `results` section where each failed resource is listed. The info for each result includes the resource that caused the failure and the rule it violated. Finally, the `summary` contains aggregate information about the results:\n\n```", "```\n\nThis is pretty nice, but it might not be the best option to keep track of your cluster if you have a lot of namespaces. It’s considered best practice to collect all the reports and periodically export them to a central location. Check out the Policy reporter project: [https://github.com/kyverno/policy-reporter](https://github.com/kyverno/policy-reporter). It also comes with a web-based policy reporter UI.\n\nLet’s install it:\n\n```", "```\n\nThe policy reporter has been installed successfully in the `policy-reporter` namespace, and we enabled the UI.\n\nThe next step is to do port-forwarding to access the UI:\n\n```"]