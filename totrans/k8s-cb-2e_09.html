<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Logging and Monitoring</h1>
                </header>
            
            <article>
                
<p><span>This chapter will cover the following recipes:</span></p>
<ul>
<li>Working with EFK</li>
<li>Working with Google Stackdriver</li>
<li>Monitoring master and node</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>Logging and monitoring are two of the most important tasks in Kubernetes. However, there are many ways to achieve logging and monitoring in Kubernetes, because there are a lot of logging and monitoring open source applications, as well as many public cloud services.</p>
<p>Kubernetes has a best practice for setting up a logging and monitoring infrastructure that most Kubernetes provisioning tools support as an add-on. In addition, managed Kubernetes services, such as Google Kubernetes Engine, integrate GCP log and a monitoring service out of the box.</p>
<p>Let's set up a logging and monitoring service on your Kubernetes cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with EFK</h1>
                </header>
            
            <article>
                
<p>In the Container world, log management always faces a technical difficulty, because Container has its own filesystem, and when Container is dead or evicted, the log files are gone. In addition, Kubernetes can easily scale out and scale down the Pods, so we need to care about a centralized log persistent mechanism.</p>
<p>Kubernetes has an add-on for setting up centralized log management, which is called EFK. EFK stands for <strong>Elasticsearch</strong>, <strong>Fluentd</strong>, and <strong>Kibana</strong>. These applications' stack bring you a full function of log collection, indexing, and UI.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In <a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml" target="_blank">Chapter 1</a>, <em>Building Your Own Kubernetes <span>Cluster</span></em>, we set up our Kubernetes cluster with several different provisioning tools. Based on your Kubernetes provisioning tool, there is an easy way to set up EFK stack. Note that Elasticsearch and Kibana are heavy-duty Java applications. They require at least 2 GB of memory each.</p>
<p>Therefore, if you use minikube, your machine should have at least 8 GB of memory (16 GB is recommended). If you use kubespray or kops to set up Kubernetes cluster, Kubernetes node should have at least 4 core CPUs and 16 GB of memory in total (in other words, if you have 2 nodes, each node should have a minimum of 2 core CPUs and 8GB of memory).</p>
<p>In addition, in order to demonstrate how to gather the application logs efficiently, we create one additional namespace. It will help you to search your application log easily:</p>
<pre>$ kubectl create namespace chap9<br/>namespace "chap9" created</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will use the following Kubernetes provisioning tools to set up EFK stack. Based on your Kubernetes cluster, please read the appropriate section of this recipe:</p>
<ul>
<li>minikube</li>
<li>kubespray (ansible)</li>
<li>kops</li>
</ul>
<div class="packt_infobox">Note that for GKE on the Google Cloud Platform, we will introduce another way to set up logging infrastructure in the next recipe.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up EFK with minikube</h1>
                </header>
            
            <article>
                
<p>minikube provides an add-on feature for EFK out of the box, but it is disabled by default. So, you need to enable EFK on your minikube manually. EFK consumes a large amount of heap memory but minikube allocates only 2 GB by default, which is absolutely not sufficient to run the EFK stack in minikube. Therefore, we'll need to enlarge minikube's memory size explicitly.</p>
<p>In addition, you should use the latest version of minikube, due to several bug fixes made for EFK while writing this cookbook. So, we are using minikube version 0.25.2. Let's configure minikube to enable EFK using the following steps:</p>
<ol>
<li>If you are already running <kbd>minikube</kbd>, stop <kbd>minikube</kbd> first:</li>
</ol>
<pre style="padding-left: 90px">$ minikube stop</pre>
<ol start="2">
<li>Update to the latest version of minikube:</li>
</ol>
<pre style="padding-left: 90px">//if you are using macOS <br/>$ brew update<br/>$ brew cask upgrade<br/><br/><br/>//if you are using Windows, download a latest binary from<br/>https://github.com/kubernetes/minikube/releases </pre>
<ol start="3">
<li>Since EFK consumes a large amount of heap memory, start <kbd>minikube</kbd> with 5 GB of memory:</li>
</ol>
<pre style="padding-left: 90px">$ minikube start --vm-driver=hyperkit <strong>--memory 5120</strong></pre>
<ol start="4">
<li>Make sure, all Pods in the kube-system Namespace are up, because EFK relies on <kbd>kube-addon-manager-minikube</kbd>:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl get pods --all-namespaces<br/>NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE<br/>kube-system   <strong>kube-addon-manager-minikube</strong>             1/1       <strong>Running</strong>   0          1m<br/>kube-system   kube-dns-54cccfbdf8-rc7gf               3/3       <strong>Running</strong>   0          1m<br/>kube-system   kubernetes-dashboard-77d8b98585-hkjrr   1/1       <strong>Running</strong>   0          1m<br/>kube-system   storage-provisioner                     1/1       <strong>Running</strong>   0          1m</pre>
<ol start="5">
<li>Enable the <kbd>efk</kbd> add-on:</li>
</ol>
<pre style="padding-left: 90px">$ minikube addons enable efk<br/>efk was successfully enabled</pre>
<ol start="6">
<li>Wait for a while; Elasticsearch, fluentd and kibana Pod have been deployed in the kube-system namespace automatically. Wait for the <kbd>STATUS </kbd>to become <kbd>Running</kbd>. It should take at least 10 minutes to complete:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl get pods --namespace=kube-system<br/>NAME                                    READY     STATUS              RESTARTS   AGE<br/>$ kubectl get pods --all-namespaces<br/>NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE<br/>kube-system   <strong>elasticsearch-logging-t5tq7</strong>             1/1       <strong>Running</strong>   0          9m<br/>kube-system   <strong>fluentd-es-8z2tr</strong>                        1/1       <strong>Running</strong>   0          9m<br/>kube-system   <strong>kibana-logging-dgql7</strong>                    1/1       <strong>Running</strong>   0          9m<br/>kube-system   kube-addon-manager-minikube             1/1       Running   1          34m<br/>…</pre>
<ol start="7">
<li>Use <kbd>kubectl logs</kbd> to watch a kibana that waits for the state to become <kbd>green</kbd>. This also takes an additional five minutes:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl logs -f kibana-logging-dgql7  --namespace=kube-system<br/>{"type":"log","@timestamp":"2018-03-25T18:53:54Z","tags":["info","optimize"],"pid":1,"message":"Optimizing and caching bundles for graph, ml, kibana, stateSessionStorageRedirect, timelion and status_page. This may take a few minutes"}<br/><br/><em>(wait for around 5min)</em><br/><br/>{"type":"log","@timestamp":"2018-03-25T19:03:10Z","tags":["status","plugin:elasticsearch@5.6.2","info"],"pid":1,"state":"yellow","message":"Status changed from yellow to yellow - No existing Kibana index found","prevState":"yellow","prevMsg":"Waiting for Elasticsearch"}<br/>{"type":"log","@timestamp":"2018-03-25T19:03:15Z","tags":["status","plugin:elasticsearch@5.6.2","info"],"pid":1,"state":"green","message":"<strong>Status changed from yellow to green</strong> - Kibana index ready","prevState":"yellow","prevMsg":"No existing Kibana index found"}</pre>
<ol start="8">
<li>Access the kibana service using the <kbd>minikube service</kbd> command:</li>
</ol>
<pre style="padding-left: 90px">$ minikube service kibana-logging --namespace=kube-system<br/>Opening kubernetes service kube-system/kibana-logging in default browser...</pre>
<p>Now, you have access to the Kibana UI from your machine. You just need to set up an index. Since Fluentd keeps sending a log with the index name as <kbd>logstash-yyyy.mm.dd</kbd>, the index pattern is <kbd>logstash-*</kbd> by default. Click the <span class="packt_screen">Create</span> button:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6221d481-761c-45c3-a8ad-82c7f7249adb.png" style="width:48.58em;height:29.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up EFK with kubespray</h1>
                </header>
            
            <article>
                
<p>kubespray has a configuration concerning whether or not to enable EFK. By default, it is disabled, so you need to enable it with the following steps:</p>
<ol>
<li>Open <kbd>&lt;kubespray dir&gt;/inventory/mycluster/group_vars/k8s-cluster.yaml</kbd>.</li>
</ol>
<p> </p>
<ol start="2">
<li>Around line number 152 in the <kbd>k8s-cluster.yml</kbd> file, change the value of  <kbd>efk_enabled</kbd> to <kbd>true</kbd>:</li>
</ol>
<pre style="padding-left: 90px"># Monitoring apps for k8s<br/>efk_enabled: <strong>true</strong></pre>
<ol start="3">
<li>Run the <kbd>ansible-playbook</kbd> command to update your Kubernetes cluster:</li>
</ol>
<pre style="padding-left: 90px">$ ansible-playbook -b -i inventory/mycluster/hosts.ini cluster.yml</pre>
<ol start="4">
<li>Check to see if Elasticsearch, Fluentd, and Kibana Pod's <span class="packt_screen">STATUS</span> became <span class="packt_screen">Running</span> or not; if you see the <span class="packt_screen">Pending</span> state for more than 10 minutes, check <kbd>kubectl describe pod &lt;Pod name&gt;</kbd> to see the status. In most cases, you will get an error saying insufficient memory. If so, you need to add more Nodes or increase the available RAM:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl get pods --all-namespaces<br/>NAMESPACE     NAME                                        READY     STATUS    RESTARTS   AGE<br/>kube-system   calico-node-9wnwn                           1/1       Running   0          2m<br/>kube-system   calico-node-jg67p                           1/1       Running   0          2m<br/><strong>kube-system   elasticsearch-logging-v1-776b8b856c-97qrq   1/1       Running   0          1m<br/></strong><strong>kube-system   elasticsearch-logging-v1-776b8b856c-z7jhm   1/1       Running   0          1m<br/></strong><strong>kube-system   fluentd-es-v1.22-gtvzg                      1/1       Running   0          49s<br/></strong><strong>kube-system   fluentd-es-v1.22-h8r4h                      1/1       Running   0          49s<br/></strong><strong>kube-system   kibana-logging-57d98b74f9-x8nz5             1/1       Running   0          44s<br/></strong>kube-system   kube-apiserver-master-1                     1/1       Running   0          3m<br/>kube-system   kube-controller-manager-master-1            1/1       Running   0          3m<br/>…</pre>
<ol start="5">
<li>Check the kibana log to see if the status has become <kbd>green</kbd>:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl logs -f kibana-logging-57d98b74f9-x8nz5 --namespace=kube-system<br/>ELASTICSEARCH_URL=http://elasticsearch-logging:9200<br/>server.basePath: /api/v1/proxy/namespaces/kube-system/services/kibana-logging<br/>{"type":"log","@timestamp":"2018-03-25T05:11:10Z","tags":["info","optimize"],"pid":5,"message":"Optimizing and caching bundles for kibana and statusPage. This may take a few minutes"}<br/><br/><em>(wait for around 5min)</em><br/><br/>{"type":"log","@timestamp":"2018-03-25T05:17:55Z","tags":["status","plugin:elasticsearch@1.0.0","info"],"pid":5,"state":"yellow","message":"Status changed from yellow to yellow - No existing Kibana index found","prevState":"yellow","prevMsg":"Waiting for Elasticsearch"}<br/>{"type":"log","@timestamp":"2018-03-25T05:17:58Z","tags":["status","plugin:elasticsearch@1.0.0","info"],"pid":5,"state":"green","message":"Status changed from <strong>yellow to green</strong> - Kibana index ready","prevState":"yellow","prevMsg":"No existing Kibana index found"}</pre>
<ol start="6">
<li>Run <kbd>kubectl cluster-info</kbd>, confirm Kibana is running, and capture the URL of Kibana:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl cluster-info<br/>Kubernetes master is running at http://localhost:8080<br/>Elasticsearch is running at http://localhost:8080/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy<br/><strong>Kibana is running at http://localhost:8080/api/v1/namespaces/kube-system/services/kibana-logging/proxy<br/></strong>KubeDNS is running at http://localhost:8080/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</pre>
<ol start="7">
<li>In order to access the Kibana WebUI from your machine remotely, <span>it is easier to use ssh port forwarding from your machine to the Kubernetes master:</span></li>
</ol>
<pre style="padding-left: 90px">$ ssh -L 8080:127.0.0.1:8080 &lt;Kubernetes master IP address&gt;</pre>
<ol start="8">
<li>Access the Kibana WebUI from your machine using the following URL: <kbd>http://localhost:8080/api/v1/namespaces/kube-system/services/kibana-logging/proxy</kbd>.</li>
</ol>
<p>Now you can access Kibana from your machine. You also need to configure the index. Just make sure the index name has <kbd>logstash-*</kbd> as the default value. Then, click the <span class="packt_screen">Create</span> button:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/280efe0d-f605-4867-9a01-356809c8b4d0.png" style="width:47.42em;height:31.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up EFK with kops</h1>
                </header>
            
            <article>
                
<p>kops also has an add-on for setting up the EFK stack on your Kubernetes cluster easily. Proceed through the following steps to run EFK stack on your Kubernetes:</p>
<ol>
<li>Run <kbd>kubectl create</kbd> to specify the kops EFK add-on:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl create -f <strong>https://raw.githubusercontent.com/kubernetes/kops/master/addons/logging-elasticsearch/v1.6.0.yaml</strong><br/>serviceaccount "elasticsearch-logging" created<br/>clusterrole "elasticsearch-logging" created<br/>clusterrolebinding "elasticsearch-logging" created<br/>serviceaccount "fluentd-es" created<br/>clusterrole "fluentd-es" created<br/>clusterrolebinding "fluentd-es" created<br/>daemonset "fluentd-es" created<br/>service "elasticsearch-logging" created<br/>statefulset "elasticsearch-logging" created<br/>deployment "kibana-logging" created<br/>service "kibana-logging" created</pre>
<ol start="2">
<li>Wait for the <kbd>STATUS </kbd>of all Pods to become <kbd>Running</kbd>:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl get pods --all-namespaces<br/>NAMESPACE     NAME                                                  READY     STATUS    RESTARTS   AGE<br/>kube-system   dns-controller-dc46485d8-pql7r                        1/1       Running   0          5m<br/><strong>kube-system   elasticsearch-logging-0                               1/1       Running   0          1m<br/></strong><strong>kube-system   elasticsearch-logging-1                               1/1       Running   0          53s<br/></strong>kube-system   etcd-server-events-ip-10-0-48-239.ec2.internal        1/1       Running   0          5m<br/>kube-system   etcd-server-ip-10-0-48-239.ec2.internal               1/1       Running   0          5m<br/><strong>kube-system   fluentd-es-29xh9                                      1/1       Running   0          1m<br/></strong><strong>kube-system   fluentd-es-xfbd6                                      1/1       Running   0          1m<br/></strong><strong>kube-system   kibana-logging-649d7dcc87-mrtzc                       1/1       Running   0          1m<br/></strong>kube-system   kube-apiserver-ip-10-0-48-239.ec2.internal            1/1       Running   0          5m<br/>...</pre>
<ol start="3">
<li>Check Kibana's log and wait until the state becomes <kbd>green</kbd>:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl logs -f kibana-logging-649d7dcc87-mrtzc --namespace=kube-system<br/>ELASTICSEARCH_URL=http://elasticsearch-logging:9200<br/>server.basePath: /api/v1/proxy/namespaces/kube-system/services/kibana-logging<br/>{"type":"log","@timestamp":"2018-03-26T01:02:04Z","tags":["info","optimize"],"pid":6,"message":"Optimizing and caching bundles for kibana and statusPage. This may take a few minutes"}<br/><br/><em>(wait for around 5min)</em><br/><br/>{"type":"log","@timestamp":"2018-03-26T01:08:00Z","tags":["status","plugin:elasticsearch@1.0.0","info"],"pid":6,"state":"yellow","message":"Status changed from yellow to yellow - No existing Kibana index found","prevState":"yellow","prevMsg":"Waiting for Elasticsearch"}<br/>{"type":"log","@timestamp":"2018-03-26T01:08:03Z","tags":["status","plugin:elasticsearch@1.0.0","info"],"pid":6,"state":"green","message":"<strong>Status changed from yellow to green</strong> - Kibana index ready","prevState":"yellow","prevMsg":"No existing Kibana index found"}</pre>
<ol start="4">
<li>Run <kbd>kubetl cluster-info</kbd> to capture the Kibana URL:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl cluster-info<br/>Kubernetes master is running at https://api.chap9.k8s-devops.net<br/>Elasticsearch is running at https://api.chap9.k8s-devops.net/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy<br/>Kibana is running at https://api.chap9.k8s-devops.net<strong>/api/v1/namespaces/kube-system/services/kibana-logging/proxy<br/></strong>KubeDNS is running at https://api.chap9.k8s-devops.net/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</pre>
<ol start="5">
<li>Use <kbd>kubectl proxy</kbd> to forward your machine to the Kubernetes API server:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl proxy --port=8080<br/>Starting to serve on 127.0.0.1:8080</pre>
<ol start="6">
<li><span>Access the Kibana WebUI from your machine using the following URL:</span> <span><kbd>http://127.0.0.1:8080/api/v1/namespaces/kube-system/services/kibana-logging/proxy. Note that the IP address is 127.0.0.1, which is correct because we are using a kubectl proxy</kbd>.</span></li>
</ol>
<p>Now, you can start to use Kibana. Configure an index as described in the preceding minikube and kubespray sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>As you can see, the installed Kibana versions are different based on the Kubernetes provisioning tool. But this cookbook explores the basic functions of Kibana. Therefore, there are no version-specific operations to worry about.</p>
<p>Let's launch a sample application and then learn how to monitor your application log using Kibana:</p>
<ol>
<li>Prepare a sample application that keeps printing a <kbd>DateTime</kbd> and hello message to the <kbd>stdout</kbd>:</li>
</ol>
<pre style="padding-left: 90px">$ cat myapp.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: myapp<br/>spec:<br/>  containers:<br/>  - image: busybox<br/>    name: application<br/>    args:<br/>      - /bin/sh<br/>      - -c<br/>      - &gt;<br/>        while true; do<br/>        echo "<strong>$(date) INFO hello</strong>";<br/>        sleep 1;<br/>        done</pre>
<ol start="2">
<li>Create a sample application in the <kbd>chap9</kbd> namespace:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl create -f myapp.yaml --namespace=chap9<br/>pod "myapp" created</pre>
<ol start="4">
<li>Access the Kibana WebUI, then click the <span class="packt_screen">Discover</span> tab:</li>
</ol>
<ol start="5">
<li>Make sure the time range is <kbd>Last 15 minutes</kbd>, then type <kbd>kubernetes.namespace_name: chap9</kbd> in the search box and hit the <em>Enter</em> key:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-994 image-border" src="assets/605953dd-4b60-4118-b5e1-8b43fd12b726.png" style="width:166.58em;height:24.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Searching the chap9 namespace log in 15 minutes</div>
<ol start="6">
<li>You can see all of the logs in the <kbd>chap9</kbd> namespaces as follows. The screenshot shows much more information than you might have expected. <span>By clicking the <span class="packt_screen">add</span> button for</span> <kbd>kubernetes.host</kbd>, <kbd>kubernetes.pod_name</kbd>, <span>and</span> <kbd>log </kbd>will display only the fields necessary for this purpose:</li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-995 image-border" src="assets/43d5f71f-eda4-47ca-a253-91abc698340c.png" style="width:166.58em;height:107.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Choosing log columns</div>
<ol start="7">
<li>Now you can see a more simple log view for this application:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-996 image-border" src="assets/640154f0-7bbe-4370-9d5c-617ca4a18e9a.png" style="width:166.58em;height:107.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Showing the final state of a customized Kibana view</div>
<p>Congratulations! You now have have a centralized log management system in your Kubernetes cluster. You can observe the deployment of some Pods to see how you can see the application log.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The preceding EFK stack collects Pods' logs only, because Fluentd is monitoring <kbd>/var/log/containers/*</kbd> in the Kubernetes node host. It is good enough to monitor an application's behavior, but, as a Kubernetes administrator, you also need some Kubernetes system logs such as master and node logs.</p>
<p>There is an easy way to achieve Kubernetes system log management that integrates with the EFK stack; add a Kubernetes Event Exporter, which keeps monitoring a Kubernetes event. When the new event has occurred, send a log to Elasticsearch. So, you can monitor a Kubernetes event with Kibana as well.</p>
<p>We have prepared an Eventer (Event Exporter) add-on <span>(<a href="https://raw.githubusercontent.com/kubernetes-cookbook/second-edition/master/chapter9/9-1/eventer.yml">https://raw.githubusercontent.com/kubernetes-cookbook/second-edition/master/chapter9/9-1/eventer.yml</a>). It</span> is Heapster (<a href="https://github.com/kubernetes/heapster">https://github.com/kubernetes/heapster</a>), based and expected to run on top of EFK add-ons. We can use this Eventer to monitor Kubernetes events through EFK:</p>
<div class="packt_infobox">Details of Heapster will be described in the next section—<em>Monitoring master and nodes</em>.</div>
<ol>
<li>Add eventer to your existing Kubernetes cluster:</li>
</ol>
<pre style="padding-left: 90px"><span>$ kubectl create -f https://raw.githubusercontent.com/kubernetes-cookbook/second-edition/master/chapter9/9-1/eventer.yml<br/></span><span>deployment "eventer-v1.5.2" created<br/></span><span>serviceaccount "heapster" created<br/></span><span>clusterrolebinding "heapster" created</span></pre>
<ol start="2">
<li>Make sure Eventer Pod's <kbd>STATUS</kbd> is <kbd>Running</kbd>:</li>
</ol>
<pre style="padding-left: 90px"><span>$ kubectl get pods --all-namespaces<br/></span><span>NAMESPACE     NAME                                        READY     STATUS    RESTARTS   AGE<br/></span><span>kube-system   elasticsearch-logging-v1-776b8b856c-9vvfl   1/1       Running   0          9m<br/></span><span>kube-system   elasticsearch-logging-v1-776b8b856c-gg5gx   1/1       Running   0          9m<br/></span><span><strong>kube-system   eventer-v1.5.2-857bcc76d9-9gwn8             1/1       Running   0          29s</strong><br/></span><span>kube-system   fluentd-es-v1.22-8prkn                      1/1       Running   0          9m<br/>...</span></pre>
<ol start="3">
<li>Use <kbd>kubectl logs</kbd> to keep observing Heapster and whether it can capture the event:</li>
</ol>
<pre style="padding-left: 90px"><span>$ kubectl logs -f eventer-v1.5.2-857bcc76d9-9gwn8 --namespace=kube-system<br/></span><span>I0327 03:49:53.988961       1 eventer.go:68] /eventer --source=kubernetes:'' --sink=elasticsearch:http://elasticsearch-logging:9200?sniff=false<br/></span><span>I0327 03:49:53.989025       1 eventer.go:69] Eventer version v1.5.2<br/></span><span>I0327 03:49:54.087982       1 eventer.go:95] Starting with ElasticSearch Sink sink<br/></span><span>I0327 03:49:54.088040       1 eventer.go:109] Starting eventer<br/></span><span>I0327 03:49:54.088048       1 eventer.go:117] Starting eventer http service<br/></span><span>I0327 03:50:00.000199       1 manager.go:100] <strong>Exporting 0 events</strong></span></pre>
<ol start="4">
<li>For testing purposes, open another terminal, and then create a <kbd>nginx</kbd> Pod:</li>
</ol>
<pre style="padding-left: 90px"><span>$ kubectl run my-nginx --image=nginx<br/></span><span>deployment "my-nginx" created</span></pre>
<ol start="5">
<li>Observe Heapster's log; some new events have been captured:</li>
</ol>
<pre style="padding-left: 90px"><span>I0327 03:52:00.000235       1 manager.go:100] Exporting 0 events<br/></span><span>I0327 03:52:30.000166       1 manager.go:100] Exporting <strong>8 events</strong><br/></span><span>I0327 03:53:00.000241       1 manager.go:100] Exporting 0 events</span></pre>
<ol start="6">
<li>Open Kibana and navigate to <span class="packt_screen">Settings</span> | <span class="packt_screen">Indices</span>| <span class="packt_screen">Add New</span>. This will add a new index. </li>
</ol>
<p> </p>
<ol start="7">
<li>Put the Index name as <kbd>heapster-*</kbd>, set the time-field name as <kbd>Metadata.creationTimestamp</kbd>, and then click <span class="packt_screen">Create:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bcc5beb4-06a2-4c0a-82e5-ba0fb8277743.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Configurring a Heapster index</div>
<ol start="8">
<li>Go back to the <span class="packt_screen">Discover</span> page, and then choose the <kbd>heapster-*</kbd> index from the left-hand panel.</li>
</ol>
<ol start="9">
<li>Select (click the <span class="packt_screen">Add</span> button) <span class="packt_screen">Message</span>, <span class="packt_screen">Source.component</span>, and <span class="packt_screen">Source.host</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/339ef52f-785f-4797-a212-6dd019737602.png" style="width:42.25em;height:15.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Choosing the necessary columns</div>
<ol start="10">
<li>Now you can see the Kubernetes system log, which shows the <kbd>nginx</kbd> Pod creation event as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4de710de-182a-4d4f-bf5f-8a0a58b488ec.png" style="width:41.83em;height:26.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Showing the final state of the system log view in Kibana</div>
<p>Now you can monitor not only the application log, but also the Kubernetes system log in the EFK stack. Through switching indexes between either <kbd>logstash-*</kbd> (application log) or <kbd>heapster-*</kbd> (system log), you have a flexible log management environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>In this cookbook, we learned how to enable the EFK stack for your Kubernetes cluster. Kibana is a powerful tool that you can use to create your own dashboard and keep checking the logs more efficiently. Please visit Kibana's online documentation to understand how to use it:</p>
<ul>
<li>Kibana User Guide Reference: <a href="https://www.elastic.co/guide/en/kibana/current/index.html">https://www.elastic.co/guide/en/kibana/index.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with Google Stackdriver</h1>
                </header>
            
            <article>
                
<p>In <a href="dfc46490-f109-4f07-ba76-1a381b006d76.xhtml" target="_blank">Chapter 7</a>, <em>Building Kubernetes on GCP</em>, we introduced GKE. It has an integrated logging mechanism, which is called Google Stackdriver. In this section, we will explore the integration between GKE and Stackdriver.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>To use a Stackdriver, you just need a GCP account. If you have never used GCP, please go back and read <a href="dfc46490-f109-4f07-ba76-1a381b006d76.xhtml" target="_blank">Chapter 7</a>, <em>Building Kubernetes on GCP</em>, to set up a GCP account and the <kbd>gcloud</kbd> command-line interface.</p>
<p>To use Stackdriver on GKE, no action is needed, because GKE uses Stackdriver as a logging platform by default. But if you want to explicitly enable Stackdriver, specify <kbd>--enable-cloud-logging</kbd> while launching your Kubernetes by using the <kbd>gcloud</kbd> command, as follows:</p>
<pre>$ gcloud container clusters create my-gke --cluster-version 1.9.4-gke.1 <strong>--enable-cloud-logging</strong> --network default --zone us-west1-b</pre>
<p>If, for some reason, you have a GKE that doesn't enable Stackdriver, you can use the <kbd>gcloud</kbd> command to enable it afterwards: </p>
<pre>$ gcloud container clusters <strong>update</strong> my-gke <strong>--logging-service logging.googleapis.com</strong> --zone us-west1-b</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p><span>In order to demonstrate Stackdriver with GKE, we will create one namespace on Kubernetes, then launch a sample Pod to see some logs on the Stackdriver, as shown in the following steps:</span></p>
<ol>
<li>Create the <kbd>chap9</kbd> namespace:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl create namespace chap9<br/>namespace "chap9" created</pre>
<ol start="2">
<li>Prepare a sample application Pod:</li>
</ol>
<pre style="padding-left: 90px"><span>$ cat myapp.yaml <br/></span><span>apiVersion: v1<br/></span><span>kind: Pod<br/></span><span>metadata:<br/></span><span>  name: myapp<br/></span><span>spec:<br/></span><span>  containers:<br/></span><span>  - image: busybox<br/></span><span>    name: application<br/></span><span>    args:<br/></span><span>      - /bin/sh<br/></span><span>      - -c<br/></span><span>      - &gt;<br/></span><span>        while true; do<br/></span><span>        echo "$(date) INFO hello";<br/></span><span>        sleep 1;<br/></span><span>        done</span></pre>
<ol start="3">
<li>Create the Pod on the <kbd>chap9</kbd> namespace:</li>
</ol>
<pre style="padding-left: 90px"><span>$ kubectl create -f myapp.yaml --namespace=chap9<br/></span><span>pod "myapp" created</span></pre>
<ol start="4">
<li>Access the GCP Web Console and navigate to <span class="packt_screen">Logging</span> | <span class="packt_screen">Logs</span>.</li>
<li>Select <span class="packt_screen">Audited </span><span class="packt_screen">Resources</span> | <span class="packt_screen">GKE Container</span> | Your GKE cluster name (ex: <span class="packt_screen">my-gke</span>) | <span class="packt_screen">chap9</span> namespace:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-999 image-border" src="assets/56cce8ab-6c3c-4e16-a08e-064c7457fe23.png" style="width:166.58em;height:106.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Selecting the chap9 namespace Pod log</div>
<ol start="6">
<li>As an alternative way of accessing the <kbd>chap9</kbd> namespace log, you can select an advanced filter. Then, type the following criteria to the text field and click the <span class="packt_screen">Submit Filter</span> button:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4e824b79-9f07-4735-8829-7a16e421fa31.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Using an advanced filter</div>
<pre style="padding-left: 90px">resource.type="container" <br/>resource.labels.cluster_name="&lt;Your GKE name&gt;"<br/><span>resource.labels.namespace_id="chap9"</span></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6443a266-ce69-4ce2-b263-14d382b44278.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Input an advanced filter criterion</div>
<ol start="7">
<li>Now, you can see the <kbd>myapp</kbd> log on Stackdriver:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1000 image-border" src="assets/d7882e0f-0411-4fa5-ba69-a38cce0bc95f.png" style="width:166.58em;height:106.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Showing the chap9 Pod log in Stackdriver</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Stackdriver has a basic functionality for narrowing down a date, severity, and keyword search. It helps to monitor an application's behavior. How about system-level behavior, such as master and node activities? Stackdriver also supports searching of the system-level log. Actually, <kbd>fluentd</kbd> captures not only the application log, but the system log as well. By performing the following steps, you can see the system log in Stackdriver:</p>
<ol>
<li>Select <span class="packt_screen">GKE Cluster Operation</span>s | Your GKE name (for example, <span class="packt_screen">my-gke</span>) | <span class="packt_screen">All locatio</span><span class="packt_screen">n</span>:</li>
</ol>
<div class="packt_tip">You should select <span class="packt_screen">All location</span> instead of a particular location, because some Kubernetes operation logs do not contain location values. </div>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1001 image-border" src="assets/9c02d2bc-23bf-4707-be44-f29c62780a8c.png" style="width:166.58em;height:76.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Choosing a GKE system log in Stackdriver</div>
<ol start="2">
<li>As an alternative, input an advanced filter as follows:</li>
</ol>
<pre style="padding-left: 90px">resource.type="gke_cluster"<br/>resource.labels.cluster_name="&lt;Your GKE name&gt;"</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2807ea60-99cc-4078-accf-f993e14c1c46.png" style="width:60.58em;height:27.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Showing a GKE system log in Stackdriver</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>In this recipe, we introduced Google Stackdriver. It is a built-in function of Google Kubernetes Engine. Stackdriver is a simple but powerful log management tool. In addition, Stackdriver is capable of monitoring the system status. You can make built-in or custom metrics to monitor and provide alerts regarding events as well. This will be described in the next recipe.</p>
<p>In addition, please read the following chapter to recap the basics of GCP and GKE:</p>
<ul>
<li><a href="dfc46490-f109-4f07-ba76-1a381b006d76.xhtml" target="_blank">Chapter 7</a>, <em>Building Kubernetes on GCP</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring master and node</h1>
                </header>
            
            <article>
                
<p>During the journey of the previous recipes, you learned how to build your own cluster, run various resources, enjoy different usage scenarios, and even enhance cluster administration. Now, here comes a new level of perspective for your Kubernetes cluster. In this recipe, we are going to talk about monitoring. Through the monitoring tool, users will not only learn about the resource consumption of nodes, but also the Pods. This will help us to have greater efficiency as regards resource utilization.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>As with earlier recipes, all you have to prepare is a healthy Kubernetes cluster. The following command, along with <kbd>kubectl</kbd>, will help you to verify the status of your Kubernetes system:</p>
<pre>// check the components<br/>$ kubectl get cs</pre>
<p>For demonstration later, we will deploy the monitoring system on a <kbd>minikube-booted</kbd> cluster. However, it works for any kind of well-installed clusters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this section, we will work on installing a monitoring system and introducing its dashboard. This monitoring system is based on <em>Heapster</em> (<a href="https://github.com/kubernetes/heapster">https://github.com/kubernetes/heapster</a>), a resource usage collecting and analyzing tool. Heapster communicates with kubelet to get the resource usage of both machine and container. Along with Heapster, we have influxDB (<a href="https://influxdata.com">https://influxdata.com</a>) for storage and Grafana (<a href="http://grafana.org">http://grafana.org</a>) as the frontend dashboard, which visualizes the status of resources in several user-friendly plots:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7e703755-eaf8-442f-a2b2-881dcecd6c4c.png" style="width:28.42em;height:19.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The interaction of monitoring components</div>
<p>Heapster gathers information from <strong>kubelet</strong> on each node and provides data for other platforms. In our case, an influxDB is a sink for saving historical data. It is available for users to do further data analysis, such as the prediction of peak workload, and then make corresponding resource adjustments. We have Grafana working as an affable web console; users can manage monitoring status through the browser. Moreover, <kbd>kubectl</kbd> has the subcommand <kbd>top</kbd>, which provides the ability to grep cluster-wide information through Heapster:</p>
<pre>// try kubectl top before installing Heapster<br/>$ kubectl top node<br/>Error from server (NotFound): the server could not find the requested resource (get services http:heapster:)</pre>
<p>This command turns out an error message.</p>
<p>Installing a monitoring system could be much easier than anticipated. By applying configuration files from the open-source communities and companies, we can set up component monitoring on Kubernetes simply with the aid of a few commands:</p>
<pre>// installing influxDB<br/>$ kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml<br/>deployment "monitoring-influxdb" created<br/>service "monitoring-influxdb" created<br/>// installing Heapster<br/>$ kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml<br/>serviceaccount "heapster" created<br/>deployment "heapster" created<br/>//installing Grafana<br/>$kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml<br/>deployment "monitoring-grafana" created<br/>service "monitoring-grafana" created</pre>
<p>You could find that applying an online source is also feasible for creating Kubernetes applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>After you have installed influxDB, Heapster, and Grafana, let's learn how to get the status of the resource. First, you may use <kbd>kubectl top</kbd> now. Check the utilization of nodes and Pods, as well as verifying the functionality of monitoring:</p>
<pre>// check the status of nodes<br/>$ kubectl top node<br/>NAME       CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%<br/>minikube   236m         11%       672Mi           35%<br/>// check the status of Pods in Namespace kube-system<br/>$ kubectl top pod -n kube-system<br/>NAME                                    CPU(cores)   MEMORY(bytes)<br/>heapster-5c448886d-k9wt7                1m           18Mi<br/>kube-addon-manager-minikube             36m          32Mi<br/>kube-dns-54cccfbdf8-j65x6               3m           22Mi<br/>kubernetes-dashboard-77d8b98585-z8hch   0m           12Mi<br/>monitoring-grafana-65757b9656-8cl6d     0m           13Mi<br/>monitoring-influxdb-66946c9f58-hwv8g    1m           26Mi</pre>
<p>Currently, <kbd>kubectl top</kbd> only covers nodes and Pods, and just shows their CPU and RAM usage.</p>
<div class="packt_infobox">
<p>According to the output of <kbd>kubectl top</kbd>, what does the <strong>m</strong> mean in terms of the quantity of CPU usage? It means "milli", as in millisecond and millimeter. Millicpu is regarded as 10<sup>-3</sup> CPU. For example, if the Heapster Pod uses 1 m CPU, it only takes 0.1% CPU computation power at this moment.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing the Grafana dashboard</h1>
                </header>
            
            <article>
                
<p>Now, let's take a look at the Grafana dashboard. In our case, for the minikube-booted cluster, we should open a proxy to enable accessibility from the localhost to the Kubernetes cluster:</p>
<pre>$ kubectl proxy<br/>Starting to serve on 127.0.0.1:8001</pre>
<p>You may access Grafana through this URL: <kbd>http://localhost:8001/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy/</kbd>. The magic that enables us to see the web page is made by the Kubernetes DNS server and proxy:</p>
<div class="packt_tip">
<p><strong>Accessing the Grafana dashboard in an anti-minikube Kubernetes</strong></p>
<p><span>To access Grafana through a browser, it depends on the network configuration of nodes and the Kubernetes service of Grafana. Follow these points for forwarding the web page to your client:</span></p>
<ul>
<li><strong>Upgrade Grafana's service type</strong>: <span>The configuration file we applied creates Grafana with a ClusterIP service. You should change it to <kbd>NodePort</kbd> or <kbd>LoadBalancer</kbd> for exposing Grafana to the outside world.</span></li>
<li><strong>Check firewalls</strong>: <span>Make sure your clients or load balancer are able to access your node of the cluster.</span></li>
<li><strong>Dashboard access through the target port</strong>: <span>Instead of using a detailed URL, like we did on the minikube cluster, you can access Grafana with simple ones such as <kbd>NODE_ENTRYPOINT:3000</kbd> (Grafana requests port 3000 in the configuration file by default) or the entry point of the load balancer.</span></li>
</ul>
</div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9b62104d-22b6-43df-839f-f3dcf61859f6.png" style="width:51.08em;height:23.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The home page of Grafana</div>
<p>In the default settings of Grafana, we have two dashboards, <strong>Cluster</strong> and <strong>Pods</strong>. The <strong>Cluster</strong> board covers the nodes' resource utilization, such as CPU, memory, network transaction, and storage. The <strong>Pods</strong> dashboard has similar plots for each Pod and you can check each container in a Pod:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/e1bda91b-4fb6-4da5-804f-a586ce2eaf50.png" style="width:53.67em;height:26.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Viewing the Pod kube-dns by Pod dashboard</div>
<p>As the preceding screenshot show, for example, we can observe the CPU utilization of individual containers in the Pod <kbd>kube-dns</kbd> in the namespace <kbd>kube-system</kbd>, which is the cluster of the DNS server. You can find that there are three containers in this Pod, <kbd>kubedns</kbd>, <kbd>dnsmasq</kbd>, and <kbd>sidecar</kbd>, and the lines in the plot express the limit, request, and real usage of CPU for containers respectively.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a new metric to monitor Pod</h1>
                </header>
            
            <article>
                
<p>For a running application, metrics are the data we can collect and use to analyze its behavior and performance. Metrics can come from the system side, such as the usage of CPU, or be based on the functionality of an application, such as the request frequency of certain functions. There are several metrics for monitoring offered by Heapster (<a href="https://github.com/kubernetes/heapster/blob/master/docs/storage-schema.md">https://github.com/kubernetes/heapster/blob/master/docs/storage-schema.md</a>). We are going to show you how to create a customized panel by yourself. Please take the following steps as a reference:</p>
<ol>
<li>Go to the dashboard of Pod, and drag the web page to the bottom. There is a button called <span class="packt_screen">ADD RO</span>W; click it to add a metric. Then, choose the first category <span class="packt_screen">Graph</span> as a new panel for expressing this metric:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/988cdb9a-61da-4fb4-993c-1dfbbd44ff19.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Adding a new metric with graph expression</div>
<ol start="2">
<li>An empty panel block appears. Go ahead and click on it for further configuration. Just choose <span class="packt_screen">Edit</span> when the editing block shows up right after you pick the panel:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/261dae87-d0cc-437e-822d-5dc45d76c76b.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Starting to edit a panel</div>
<ol start="3">
<li>First, give your panel a name. For example, <kbd>CPU Rate</kbd>. We would like to create one showing the rate of CPU utilization:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5d8b175c-f7cb-4d2f-b640-6e133b8f8ddd.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Giving the panel a title on the "General" page</div>
<ol start="4">
<li>Set up the following parameters for specific data querying. Take the following screenshot as reference:
<ul>
<li><span class="packt_screen">FROM</span>: <kbd>cpu/usage_rate</kbd></li>
<li><span class="packt_screen">WHERE</span>: <kbd>type = pod_container</kbd></li>
<li><span class="packt_screen">AND</span>: <kbd>namespace_name=$namespace, pod_name=$podname value</kbd></li>
<li><span class="packt_screen">GROUP BY</span>: <kbd>tag(container_name)</kbd></li>
<li><span class="packt_screen">ALIAS BY</span>: <kbd>$tag_container_name</kbd></li>
</ul>
</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fef0d5dc-9c4c-4942-b895-c4a5a663f7fa.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Parameters of data querying for CPU-rate metric</div>
<ol start="5">
<li>Does any line of status show up? If not, modifying the configuration in the display page will help you build the best looking graph for you. Make the <span class="packt_screen">Null value</span> <span class="packt_screen">connected</span> and you will find lines showing out:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f3a38ac6-cf33-48ea-b24d-63f864e5b62a.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Editing the look of your metric. Checking the null value to be "connected" for showing the lines</div>
<ol start="6">
<li>Here you go! Feel free to close the edit mode. You now have a new metric for every Pod.</li>
</ol>
<p><span>Just try to discover more functionality of the Grafana dashboard and the Heapster monitoring tool. You will obtain further details about your system, services, and containers through the information from the monitoring system.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>We built up a monitoring system based on Heapster, which is maintained by the Kubernetes group. Yet, several tools and platforms focusing on container cluster have sprung up to support the community, such as Prometheus (<a href="https://prometheus.io">https://prometheus.io</a>). On the other hand, public clouds may have run daemons on VM for grabbing the metrics by default, and provided services for corresponding actions. We don't have to build one within the cluster. Next, we are going to introduce the monitoring method on AWS and GCP. You may wish to check <a href="b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml" target="_blank">C<em>hapter 6</em></a>, <em>Building Kubernetes on AWS</em>, and <a href="dfc46490-f109-4f07-ba76-1a381b006d76.xhtml" target="_blank">C<em>hapter 7</em></a>, <em>Building Kubernetes on GCP</em>, to build a cluster and learn more concepts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring your Kubernetes cluster on AWS</h1>
                </header>
            
            <article>
                
<p>While working on AWS, we usually rely on AWS CloudWatch<em> </em>(<a href="https://aws.amazon.com/cloudwatch/">https://aws.amazon.com/cloudwatch/</a>) for monitoring. You can create a dashboard, and pick up any basic metrics you want. CloudWatch already collects a bunch of metrics for you:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/74f9230e-9818-4dc3-95e8-453cf4ea2bad.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Create a new metric with AWS CloudWatch</div>
<p>But, for the resource of Kubernetes, such as Pods, customized metrics for them need to be sent out to CloudWatch with manual configuration. With a kops installation, it is recommended that you build your monitoring system with Heapster or Prometheus.</p>
<p>AWS has its own container cluster service, Amazon ECS. This may be the reason why AWS didn't support Kubernetes deeply and we have to build clusters through kops and terraform, along with other add-on services. Nevertheless, according to recent news, there will be a new service called <strong>Amazon Elastic Container Service for Kubernetes</strong> (<strong>Amazon EKS</strong>). We can look forward to the integration of Kubernetes and other AWS services.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring your Kubernetes cluster on GCP</h1>
                </header>
            
            <article>
                
<p>Before we look at the monitoring platform of GCP, the nodes of GKE cluster should admit being scanned for any applied status:</p>
<pre>// add nodes with monitoring scope<br/>$ gcloud container clusters create my-k8s-cluster --cluster-version 1.9.7-gke.0 - -machine-type f1-micro --num-nodes 3 --network k8s-network --zone us-central1-a - -tags private --scopes=storage-rw,compute-ro, <strong>https://www.googleapis.com/auth/monitoring</strong></pre>
<p><em>Google Stackdriver</em> provides system monitoring in a hybrid cloud environment. Besides its own GCP, it can also cover your computing resources on AWS. To access its web console, you can find its section under the menu on the left-hand side. There are multiple service categories in Stackdriver. Select <span class="packt_screen">Monitoring</span> to check related functionality.</p>
<p>As a new user, you will get a 30-day free trial. The initial configuration is simple; just enable an account and bind your project. You may avoid the agent installation and AWS account setup since we simply want to check the GKE cluster. Once you log in to Stackdriver successfully, click <span class="packt_screen">Resources</span> on the left-side panel and choose <span class="packt_screen">Kubernetes Engine</span> under infrastructure type:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4f113195-bb4d-4420-976a-93ac659c547a.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Main page for GKE on Strackdriver monitoring</div>
<p>There are several metrics set up already for computing resources from node to container. Take your time exploring and check the official introduction for more features: <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/monitoring">https://cloud.google.com/kubernetes-engine/docs/how-to/monitoring</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>This recipe showed you how to monitor your machines in the Kubernetes system. However, it is wise to study the recipes of the main components and daemons. You can get more ideas about working processes and resource usage. Moreover, since we have worked with several services to build our monitoring system, reviewing recipes about the Kubernetes services again will give you a clear idea about how you can build up this monitoring system:</p>
<ul>
<li>The <em>Exploring Architecture</em> recipe in <a href="1a0d884d-59d3-4f67-adee-2d2e37030132.xhtml" target="_blank">Chapter 1</a>, <em>Building Your Own Kubernetes <span>Cluster</span></em></li>
<li>The <em>Working with Services</em> recipe in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml" target="_blank">Chapter 2</a>, <em>Walking through Kubernetes Concepts</em></li>
</ul>
<p>Kubernetes is a project that keeps moving forward and upgrading apace. The recommended way to catch up is to check out new features on its official website: <a href="http://kubernetes.io">http://kubernetes.io</a>. Also, you can always get new Kubernetes versions on GitHub at <a href="https://github.com/kubernetes/kubernetes/releases">https://github.com/kubernetes/kubernetes/releases</a>. Keeping your Kubernetes system up to date, and learning new features practically, is the best method for accessing Kubernetes technology continuously.</p>


            </article>

            
        </section>
    </body></html>