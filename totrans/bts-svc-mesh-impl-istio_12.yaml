- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Summarizing What We Have Learned and the Next Steps
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结我们学到的内容及下一步
- en: Throughout the book, you learned about and practiced various concepts of Service
    Mesh and how to apply them using Istio. It is strongly recommended that you practice
    the hands-on examples in each chapter. Don’t just limit yourself to the scenarios
    presented in this book but rather explore, tweak, and extend the examples and
    apply them to real-world problems you are facing in your organizations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在全书中，你学习并实践了服务网格的各种概念，并了解如何使用Istio应用它们。强烈建议你在每一章中练习动手示例。不要仅限于本书中呈现的场景，而是要探索、调整、扩展这些示例，并将它们应用到你在组织中面临的实际问题上。
- en: 'In this chapter, we will revise the concepts discussed in this book by implementing
    Istio for an Online Boutique application. It will be a good idea to look at scenarios
    presented in this chapter and try to implement them yourself before looking at
    code examples. I hope reading this last chapter provides you with more confidence
    in using Istio. We will go through the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过为在线精品应用程序实现Istio，回顾本书中讨论的概念。建议在查看代码示例之前，先查看本章中呈现的场景并尝试自己实现。我希望阅读本章能给你使用Istio带来更多信心。我们将在本章中讲解以下主题：
- en: Enforcing best practices using OPA Gatekeeper
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用OPA Gatekeeper执行最佳实践
- en: Applying the learnings of this book to a sample Online Boutique application
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将本书中的学习应用到示例的在线精品应用程序
- en: Istio roadmap, vision, and documentation, and how to engage with the community
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio的路线图、愿景和文档，以及如何与社区互动
- en: Certification, learning resources, and various pathways to learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认证、学习资源以及各种学习路径
- en: The Extended Berkeley Packet Filter
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展伯克利数据包过滤器
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The technical requirements in this chapter are similar to *Chapter 4*. We will
    be using AWS EKS to deploy a website for an online boutique store, which is an
    open source application available under Apache License 2.0 at [https://github.com/GoogleCloudPlatform/microservices-demo](https://github.com/GoogleCloudPlatform/microservices-demo).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的技术要求类似于*第4章*。我们将使用AWS EKS部署一个在线精品商店的网站，这是一个可通过Apache许可证2.0获得的开源应用程序，地址是[https://github.com/GoogleCloudPlatform/microservices-demo](https://github.com/GoogleCloudPlatform/microservices-demo)。
- en: Please check *Chapter 4*’s *Technical requirements* section to set up the infrastructure
    in AWS using Terraform, set up kubectl, and install Istio including observability
    add-ons. To deploy the Online Boutique store application, please use the deployment
    artifacts in the `Chapter12/online-boutique-orig` file on GitHub.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 请检查*第4章*的*技术要求*部分，使用Terraform在AWS中设置基础设施，设置kubectl，并安装包括可观察性附加组件在内的Istio。要部署在线精品商店应用程序，请使用GitHub上`Chapter12/online-boutique-orig`文件中的部署工件。
- en: 'You can deploy the Online Boutique store application using the following commands:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令部署在线精品商店应用程序：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The last command should deploy the Online Boutique application. After some
    time, you should be able to see all the Pods running:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一条命令应该部署在线精品应用程序。过一段时间后，你应该能够看到所有的Pods正在运行：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The name of the workloads also reflects their role in the Online Boutique application,
    but you can find more about this freely available open source application at [https://github.com/GoogleCloudPlatform/microservices-demo](https://github.com/GoogleCloudPlatform/microservices-demo).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 工作负载的名称也反映了它们在在线精品应用程序中的角色，但你可以在[https://github.com/GoogleCloudPlatform/microservices-demo](https://github.com/GoogleCloudPlatform/microservices-demo)上找到更多关于这个免费开源应用程序的信息。
- en: 'For now, you can access the application via the following command:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以通过以下命令访问应用程序：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can then open it on the browser using `http://localhost:8080`. You should
    see something like the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以通过`http://localhost:8080`在浏览器中打开它。你应该能看到类似以下内容：
- en: '![Figure 12.1 – Online Boutique application by Google](img/Figure_12.01_B17989.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.1 – Google 的在线精品应用程序](img/Figure_12.01_B17989.jpg)'
- en: Figure 12.1 – Online Boutique application by Google
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 – Google 的在线精品应用程序
- en: This completes the technical setup required for code examples in this chapter.
    Let’s get into the main topics of the chapter. We will begin with setting up the
    OPA Gatekeeper to enforce Istio deployment best practices.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了本章中代码示例所需的技术设置。现在，让我们进入本章的主要内容。我们将从设置OPA Gatekeeper开始，以执行Istio部署最佳实践。
- en: Enforcing workload deployment best practices using OPA Gatekeeper
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OPA Gatekeeper执行工作负载部署最佳实践
- en: 'In this section, we will deploy OPA Gatekeeper using our knowledge from *Chapter
    11*. We will then configure OPA policies to enforce that every deployment has
    `app` and `version` as labels, and all port names have protocol names as a prefix:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用*第11章*的知识部署 OPA Gatekeeper。然后，我们将配置 OPA 策略，强制每个部署都有 `app` 和 `version`
    作为标签，并且所有端口名称以协议名称作为前缀：
- en: 'Install OPA Gatekeeper. Deploy it by following the instructions in *Chapter
    11*, in the *Automating best practices using OPA* *Gatekeeper* section:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 OPA Gatekeeper。按照*第11章*中*使用 OPA 自动化最佳实践*的*Gatekeeper*部分的说明进行部署：
- en: '[PRE3]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After deploying OPA Gatekeeper, you need to configure it to sync namespaces,
    Pods, services and Istio CRD gateways, virtual services, destination rules, and
    policy and service role bindings into its cache. We will make use of the configuration
    file we created in *Chapter 11*:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署 OPA Gatekeeper 后，您需要配置它来同步命名空间、Pod、服务以及 Istio CRD 网关、虚拟服务、目标规则和策略与服务角色绑定到其缓存中。我们将使用在*第11章*中创建的配置文件：
- en: '[PRE4]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Configure OPA Gatekeeper to apply the constraints. In *Chapter 11*, we configured
    constraints to enforce that Pods should have `app` and `version` numbers as labels
    (defined in `Chapter11/gatekeeper/01-istiopodlabelconstraint_template.yaml` and
    `Chapter11/gatekeeper/01-istiopodlabelconstraint.yaml`), and all port names should
    have a protocol name as a prefix (defined in `Chapter11/gatekeeper/02-istioportconstraints_template.yaml`
    and `Chapter11/gatekeeper/02-istioportconstraints.yaml`). Apply the constraints
    using the following commands:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置 OPA Gatekeeper 以应用约束。在*第11章*中，我们配置了约束，强制 Pod 应有 `app` 和 `version` 作为标签（定义在
    `Chapter11/gatekeeper/01-istiopodlabelconstraint_template.yaml` 和 `Chapter11/gatekeeper/01-istiopodlabelconstraint.yaml`
    中），并且所有端口名称应该以协议名称作为前缀（定义在 `Chapter11/gatekeeper/02-istioportconstraints_template.yaml`
    和 `Chapter11/gatekeeper/02-istioportconstraints.yaml` 中）。使用以下命令应用约束：
- en: '[PRE5]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This completes the deployment and configuration of OPA Gatekeeper. You should
    extend the constraints with anything else you might like to be included to ensure
    good hygiene of deployment descriptors of the workloads.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了 OPA Gatekeeper 的部署和配置。您应该根据需要扩展约束，确保工作负载的部署描述符保持良好的规范。
- en: In the next section, we will redeploy the Online Boutique application and enable
    istio sidecar injection and then discover the configurations that are in violation
    of OPA constraints and resolve them one by one.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将重新部署在线精品应用，并启用 istio sidecar 注入，然后发现违反 OPA 约束的配置，并逐一解决它们。
- en: Applying our learnings to a sample application
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将我们的学习应用到示例应用中
- en: In this section, we will apply the learnings of the book – specifically, the
    knowledge from *Chapters 4* to *6* – to our Online Boutique application. Let’s
    dive right in!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将把书中的学习成果——特别是*第4章*到*第6章*的知识——应用到我们的在线精品应用中。让我们直接开始吧！
- en: Enabling Service Mesh for the sample application
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为示例应用启用服务网格
- en: Now that OPA Gatekeeper is in place with all the constraints we want it to enforce
    on deployments, it’s time to deploy a sample application. We will first start
    with un-deploying the `online-boutique` application and redeploying with istio-injection
    enabled at the namespace level.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，OPA Gatekeeper 已经配置好并强制执行我们希望它在部署中应用的所有约束，是时候部署示例应用了。我们将首先从撤销 `online-boutique`
    应用的部署开始，并在命名空间级别启用 istio-injection 后重新部署。
- en: 'Undeploy the Online Boutique application by deleting the `online-boutique`
    namespace:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过删除 `online-boutique` 命名空间来撤销在线精品应用的部署：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once undeployed, let’s modify the namespace and add an `istio-injection:enabled`
    label and redeploy the application. The updated namespace configuration will be
    as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦撤销部署，让我们修改命名空间并添加一个 `istio-injection:enabled` 标签，然后重新部署应用。更新后的命名空间配置如下：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The sample file is available at `Chapter12/OPAGatekeeper/automaticsidecarinjection/00-online-boutique-shop-ns.yaml`
    on GitHub.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 示例文件可在 GitHub 上的 `Chapter12/OPAGatekeeper/automaticsidecarinjection/00-online-boutique-shop-ns.yaml`
    中找到。
- en: 'With automatic sidecar injection enabled, let’s try to deploy the application
    using the following commands:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 启用自动 sidecar 注入后，让我们尝试使用以下命令部署应用：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There will be errors caused by constraint violations imposed by OPA Gatekeeper.
    The output in the preceding example is truncated to avoid repetitions but from
    the output in your terminal, you must notice that all deployments are in violation
    and hence no resource is deployed to the online-boutique namespace.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 OPA Gatekeeper 强制的约束违反，将会发生错误。前面的示例输出已被截断以避免重复，但从您的终端输出中，您必须注意到所有部署都违反了约束，因此没有任何资源部署到
    online-boutique 命名空间。
- en: Try to fix the constraint violation by applying the correct labels and naming
    ports correctly as suggested by Istio best practices.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试通过应用正确的标签并按照 Istio 最佳实践正确命名端口来修复约束违规问题。
- en: 'You need to apply `app` and `version` labels to all deployments. The following
    is an example for a `frontend` deployment:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要为所有部署应用`app`和`version`标签。以下是一个`frontend`部署的示例：
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Similarly, you need to add `name` to all port definitions in the service declaration.
    The following is an example of a `carts` service:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你需要为服务声明中的所有端口定义添加`name`。以下是一个`carts`服务的示例：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'For your convenience, the updated files are available in `Chapter12/OPAGatekeeper/automaticsidecarinjection`.
    Deploy the Online Boutique application using the following command:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，更新后的文件可在`Chapter12/OPAGatekeeper/automaticsidecarinjection`中找到。使用以下命令部署
    Online Boutique 应用：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: With that, we have practiced the deployment of the Online Boutique application
    in the Service Mesh. You should have the Online Boutique application along with
    automatic sidecar injection deployed in your cluster. The Online Boutique application
    is part of the Service Mesh but not yet completely ready for it. In the next section,
    we will apply the learning from *Chapter 5* on managing application traffic.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个，我们已经练习了在 Service Mesh 中部署 Online Boutique 应用。你应该在集群中部署了 Online Boutique
    应用及其自动侧车注入功能。Online Boutique 应用是 Service Mesh 的一部分，但尚未完全准备好。在下一节中，我们将应用*第5章*关于管理应用流量的学习内容。
- en: Configuring Istio to manage application traffic
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置 Istio 以管理应用流量
- en: In this section, using the learnings from *Chapter 4*, we will configure the
    Service Mesh to manage application traffic for the Online Boutique application.
    We will first start with configuring the Istio Ingress gateway to allow traffic
    inside the mesh.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，基于*第4章*的学习内容，我们将配置 Service Mesh 来管理 Online Boutique 应用的流量。我们将首先从配置 Istio
    Ingress 网关开始，允许流量进入服务网格。
- en: Configuring Istio Ingress Gateway
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置 Istio Ingress 网关
- en: In *Chapter 4*, we read that a gateway is like a load balancer on the edge of
    the mesh that accepts incoming traffic that is then routed to underlying workloads.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4章*中，我们了解到网关就像是服务网格边缘的负载均衡器，它接收传入的流量，然后将流量路由到底层的工作负载。
- en: 'In the following source code block, we have defined the gateway configuration:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下源代码块中，我们已经定义了网关配置：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The file is also available in `Chapter12/trafficmanagement/01-gateway.yaml`
    on GitHub. Apply the configuration using the following command:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件也可以在 GitHub 上的`Chapter12/trafficmanagement/01-gateway.yaml`找到。使用以下命令应用配置：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Next, we need to configure `VirtualService` to route traffic for the `onlineboutique.com`
    host to the corresponding `frontend` service.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要配置`VirtualService`，将`onlineboutique.com`主机的流量路由到相应的`frontend`服务。
- en: Configuring VirtualService
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置 VirtualService
- en: '`VirtualService` is used to define route rules for every host as specified
    in the gateway configuration. `VirtualService` is associated with the gateway
    and the hostname is managed by that gateway. In `VirtualService`, you can define
    rules on how a traffic/route can be matched and, if matched, then where it should
    be routed to.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`VirtualService`用于定义网关配置中指定的每个主机的路由规则。`VirtualService`与网关相关联，主机名由该网关管理。在`VirtualService`中，你可以定义流量/路由的匹配规则，并在匹配时指定流量应该被路由到哪里。'
- en: 'The following source code block defines `VirtualService` that matches any traffic
    handled by `online-boutique-ingress-gateway` with a hostname of `onlineboutique.com`.
    If matched, the traffic is routed to subset `v1` of the destination service named
    `frontend`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下源代码块定义了`VirtualService`，它匹配由`online-boutique-ingress-gateway`处理的任何流量，并且主机名为`onlineboutique.com`。如果匹配，流量将被路由到名为`frontend`的目标服务的`v1`子集：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The configuration is available in `Chapter12/trafficmanagement/02-virtualservice-frontend.yaml`
    on GitHub.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件可以在 GitHub 上的`Chapter12/trafficmanagement/02-virtualservice-frontend.yaml`找到。
- en: Next, we will configure `DestinationRule`, which defines how the request will
    be handled by the destination.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将配置`DestinationRule`，它定义了请求将如何由目标处理。
- en: Configuring DestinationRule
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置 DestinationRule
- en: 'Though they might appear unnecessary, when you have more than one version of
    the workload, then `DestinationRule` is used for defining traffic policies such
    as a load balancing policy, connection pool policy, outlier detection policy,
    and so on. The following code block configures `DestinationRule` for the `frontend`
    service:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它们看起来可能不必要，但当您的工作负载有多个版本时，`DestinationRule` 用于定义流量策略，例如负载均衡策略、连接池策略、异常检测策略等。以下代码块为
    `frontend` 服务配置了 `DestinationRule`：
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The configuration is available along with the `VirtualService` configuration
    in `Chapter12/trafficmanagement/02-virtualservice-frontend.yaml` on GitHub.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 配置已与 `VirtualService` 配置一起提供，在 GitHub 上的 `Chapter12/trafficmanagement/02-virtualservice-frontend.yaml`
    中。
- en: 'Next, let’s create `VirtualService` and `DestinationRule` by using the following
    commands:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下命令创建 `VirtualService` 和 `DestinationRule`：
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You should now be able to access the Online Boutique store site from the web
    browser. You need to find the public IP of the AWS load balancer exposing the
    Ingress gateway service – do not forget to add a **Host** header using the **ModHeader**
    extension to Chrome, as discussed in *Chapter 4* and as seen in the following
    screenshot:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该能够通过 Web 浏览器访问在线精品店网站。您需要找到暴露 Ingress 网关服务的 AWS 负载均衡器的公共 IP – 别忘了按照*第
    4 章*中的讨论，并通过 **ModHeader** 扩展在 Chrome 中添加 **Host** 头部，正如以下截图所示：
- en: '![Figure 12.2 – ModHeader extension with Host header](img/Figure_12.02_B17989.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.2 – 带 Host 头部的 ModHeader 扩展](img/Figure_12.02_B17989.jpg)'
- en: Figure 12.2 – ModHeader extension with Host header
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 – 带 Host 头部的 ModHeader 扩展
- en: 'Once the correct Host header is added, you can access the Online Boutique from
    Chrome using the AWS load balancer public DNS:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦添加了正确的 Host 头部，您就可以通过 AWS 负载均衡器的公共 DNS 使用 Chrome 访问在线精品店：
- en: '![Figure 12.3 – Online Boutique landing page](img/Figure_12.03_B17989.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.3 – 在线精品店登录页面](img/Figure_12.03_B17989.jpg)'
- en: Figure 12.3 – Online Boutique landing page
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3 – 在线精品店登录页面
- en: So far, we have created only one virtual service to route traffic from the Ingress
    gateway to the `frontend` service in the mesh. By default, Istio will send traffic
    to all respective microservices in the mesh, but as we discussed In the previous
    chapter, the best practice is to define routes via `VirtualService` and how the
    request should be routed via destination rules. Following the best practice, we
    need to define `VirtualService` and `DestinationRule` for the remaining microservices.
    Having `VirtualService` and `DestinationRule` helps you manage traffic when there
    is more than one version of underlying workloads.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只创建了一个虚拟服务，将流量从 Ingress 网关路由到网格中的 `frontend` 服务。默认情况下，Istio 会将流量发送到网格中的所有相关微服务，但正如我们在前一章讨论的那样，最佳实践是通过
    `VirtualService` 定义路由，并通过目标规则定义请求如何路由。遵循最佳实践，我们需要为剩余的微服务定义 `VirtualService` 和
    `DestinationRule`。拥有 `VirtualService` 和 `DestinationRule` 有助于在底层工作负载有多个版本时管理流量。
- en: 'For your convenience, `VirtualService` and `DestinationRule` are already defined
    in the `Chapter12/trafficmanagement/03-virtualservicesanddr-otherservices.yaml`
    file on GitHub. You can apply the configuration using the following command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，`VirtualService` 和 `DestinationRule` 已在 GitHub 上的 `Chapter12/trafficmanagement/03-virtualservicesanddr-otherservices.yaml`
    文件中定义。您可以使用以下命令应用配置：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After applying the configuration and generating some traffic, check out the
    Kiali dashboard:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用配置并生成一些流量后，查看 Kiali 仪表板：
- en: '![Figure 12.4 – Versioned app graph for the Online Boutique shop](img/Figure_12.04_B17989.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.4 – 在线精品店的版本化应用图](img/Figure_12.04_B17989.jpg)'
- en: Figure 12.4 – Versioned app graph for the Online Boutique shop
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4 – 在线精品店的版本化应用图
- en: In the Kiali dashboard, you can observe the Ingress gateway, all virtual services,
    and underlying workloads.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kiali 仪表板中，您可以观察到 Ingress 网关、所有虚拟服务以及底层工作负载。
- en: Configuring access to external services
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置访问外部服务
- en: 'Next, we quickly revise concepts on routing traffic to destinations outside
    the mesh. In *Chapter 4*, we learned about `ServiceEntry`, which enables us to
    add additional entries to Istio’s internal service registry so that service in
    the mesh can route traffic to these endpoints that are not part of the Istio service
    registry. The following is an example of `ServiceRegistry` adding `xyz.com` to
    the Istio service registry:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们快速回顾一下将流量路由到网格外目标的概念。在*第 4 章*中，我们学习了 `ServiceEntry`，它使我们能够将额外的条目添加到 Istio
    的内部服务注册表中，以便网格中的服务可以将流量路由到这些不属于 Istio 服务注册表的端点。以下是将 `xyz.com` 添加到 Istio 服务注册表的
    `ServiceRegistry` 示例：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This concludes the section on managing application traffic, in which we exposed
    `onlineboutique.com` via Istio Ingress Gateway and defined `VirtualService` and
    `DestinationRule` for routing and handling traffic in the mesh.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容讲解了如何管理应用程序流量，我们通过Istio Ingress Gateway暴露了`onlineboutique.com`，并为流量路由和处理定义了`VirtualService`和`DestinationRule`。
- en: Configuring Istio to manage application resiliency
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Istio以管理应用程序的韧性
- en: Istio provides various capabilities to manage application resiliency, and we
    discussed them in great detail in *Chapter 5*. We will apply some of the concepts
    from that chapter to the Online Boutique application.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Istio提供了多种功能来管理应用程序的韧性，我们在*第5章*中详细讨论了这些功能。我们将应用该章节中的一些概念到在线商店应用程序中。
- en: Let’s start with timeouts and retries!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从超时和重试开始！
- en: Configuring timeouts and retries
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置超时和重试
- en: Let’s assume that the email service suffers from intermittent failures, and
    it is prudent to timeout after 5 seconds if a response is not received from the
    email service, and then retry sending the email a few times rather than aborting
    it. We will configure retries and timeout for the email service to revise application
    resiliency concepts.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 假设电子邮件服务存在间歇性故障，如果在5秒内没有收到电子邮件服务的响应，那么最好设置超时，并在失败后重试发送邮件几次，而不是直接放弃。我们将配置电子邮件服务的重试和超时，以修改应用程序的韧性概念。
- en: 'Istio provides a provision to configure timeouts, which is the amount of time
    that an Istio-proxy sidecar should wait for replies from a given service. In the
    following configuration, we have applied a timeout of 5 seconds for the email
    service:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Istio提供了配置超时的功能，即Istio代理边车应该等待来自给定服务的回复的时间。在以下配置中，我们为电子邮件服务应用了5秒的超时：
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Istio also provides provision for automated retries that are implemented as
    part of the `VirtualService` configuration. In the following source code block,
    we have configured Istio to retry the request to the email service twice, with
    each retry to timeout after 2 seconds and a retry to happen only if `5xx,gateway-error,reset,connect-failure,refused-stream,retriable-4xx`
    errors are returned from downstream:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Istio还提供了自动重试的功能，这一功能作为`VirtualService`配置的一部分实现。在以下源代码块中，我们已将Istio配置为重试两次发送请求给电子邮件服务，每次重试的超时时间为2秒，且仅在下游返回`5xx,gateway-error,reset,connect-failure,refused-stream,retriable-4xx`错误时才会重试：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We have configured `timeout` and `retries` via the `VirtualService` configuration.
    With the assumption that the email service is fragile and suffers interim failure,
    let’s try to alleviate this issue by mitigating any potential issue caused by
    a traffic surge or spike.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经通过`VirtualService`配置设置了`timeout`和`retries`。假设电子邮件服务较为脆弱，并且会出现临时故障，我们将尝试通过缓解流量激增或尖峰可能导致的问题来解决这一问题。
- en: Configuring rate limiting
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置速率限制
- en: Istio provides controls to handle a surge of traffic from consumers, as well
    as to control the traffic to match consumers’ capability to handle the traffic.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Istio提供了控制来自消费者的流量激增的功能，并能够根据消费者的处理能力来控制流量。
- en: 'In the following destination rule, we are defining rate-limiting controls for
    the email service. We have defined that the number of active requests to the email
    service will be `1` (as per `http2MaxRequests`), there will be only 1 request
    per connection (as defined in `maxRequestsPerConnection`), and there will be 0
    requests queued while waiting for connection from the connection pool (as defined
    in `http1MaxPendingRequests`):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下目标规则中，我们定义了电子邮件服务的速率限制控制。我们定义了电子邮件服务的活跃请求数为`1`（根据`http2MaxRequests`），每个连接只有1个请求（在`maxRequestsPerConnection`中定义），而在等待连接池连接时，排队请求数为0（根据`http1MaxPendingRequests`定义）：
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s make some more assumptions and assume that there are two versions of
    the email service, with `v1` being more rogue than the other, `v2`. In such scenarios,
    we need to apply outlier detection policies to perform circuit breakers. Istio
    provides good control for outlier detection. The following code block describes
    the config you need to add to `trafficPolicy` in the corresponding destination
    rule for the email service:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做更多假设，假设电子邮件服务有两个版本，其中`v1`比`v2`更不稳定。在这种情况下，我们需要应用异常检测策略来执行熔断器。Istio提供了良好的异常检测控制。以下代码块描述了您需要在电子邮件服务的目标规则中的`trafficPolicy`中添加的配置：
- en: '[PRE22]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the outlier detection, we have defined `baseEjectionTime` with a value of
    `5` minutes, which is the minimum duration per ejection. It is then also multiplied
    by the number of times an email service is found to be unhealthy. For example,
    if the `v1` email service is found to be an outlier 5 times, then it will be ejected
    from the connection pool for `baseEjectionTime*5`. Next, we have defined `consecutive5xxErrors`
    with a value of `1`, which is the number of `5x` errors that need to occur to
    qualify the upstream to be an outlier. Then, we have defined `interval` with a
    value of `90s`, which is the time between the checks when Istio scans the upstream
    for the health status. Finally, we have defined `maxEjectionPercent` with a value
    of `50%`, which is the maximum number of hosts in the connection pool that can
    be ejected.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在异常值检测中，我们定义了`baseEjectionTime`，其值为`5`分钟，这是每次驱逐的最短持续时间。然后，这个值会乘以电子邮件服务被发现不健康的次数。例如，如果`v1`电子邮件服务被发现异常5次，那么它将从连接池中被驱逐`baseEjectionTime*5`分钟。接下来，我们定义了`consecutive5xxErrors`，其值为`1`，即需要发生的`5x`错误次数，才能将上游服务标记为异常值。然后，我们定义了`interval`，其值为`90s`，即Istio扫描上游服务健康状态的检查间隔时间。最后，我们定义了`maxEjectionPercent`，其值为`50%`，即连接池中可以被驱逐的最大主机数。
- en: With that, we revised and applied various controls for managing application
    resiliency for the Online Boutique application. Istio provides various controls
    for managing application resiliency without needing to modify or build anything
    specific in your application. In the next section, we will apply the learning
    of *Chapter 6* to our Online Boutique application.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就对在线精品应用程序的应用韧性管理进行了修订和应用了各种控制。Istio提供了各种控制，用于管理应用韧性，而无需修改或构建任何特定于应用程序的内容。在下一部分，我们将把*第6章*的学习应用到我们的在线精品应用程序中。
- en: Configuring Istio to manage application security
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Istio来管理应用安全性
- en: 'Now that we have created Ingress via Istio Gateway, routing rules via Istio
    `VirtualService`, and `DestinationRules` to handle how traffic will be routed
    to the end destination, we can move on to the next step of securing the traffic
    in the mesh. The following policy enforces that all traffic in the mesh should
    strictly happen over mTLS:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经通过Istio Gateway创建了Ingress，通过Istio `VirtualService`和`DestinationRules`配置了路由规则，以处理流量如何被路由到最终目的地，我们可以进入下一步——在网格中保护流量。以下策略强制要求所有流量在网格中严格通过mTLS进行：
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The configuration is available in the `Chapter12/security/strictMTLS.yaml`
    file on GitHub. Without this configuration, all the traffic in the mesh is happening
    in *PERMISSIVE* mode, which means that the traffic can happen over mTLS as well
    as plain text. You can validate that by deploying a `curl` Pod and making an HTTP
    call to any of the microservices in the mesh. But once you apply the policy, Istio
    will enforce *STRICT* mode, which means mTLS will be strictly enforced for all
    traffic. Apply the configuration using the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件可在GitHub上的`Chapter12/security/strictMTLS.yaml`文件中找到。如果没有此配置，网格中的所有流量都将处于*PERMISSIVE*模式，这意味着流量可以通过mTLS或明文传输进行。你可以通过部署`curl`
    Pod并向网格中的任何微服务发起HTTP请求来验证这一点。但一旦应用此策略，Istio将强制执行*STRICT*模式，这意味着所有流量将严格使用mTLS进行。使用以下命令应用配置：
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can check in Kiali that all traffic in the mesh is happening over mTLS:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Kiali中检查网格中的所有流量是否都通过mTLS进行：
- en: '![Figure 12.5 – App graph showing mTLS communication between services](img/Figure_12.05_B17989.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.5 – 显示服务之间mTLS通信的应用图](img/Figure_12.05_B17989.jpg)'
- en: Figure 12.5 – App graph showing mTLS communication between services
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5 – 显示服务之间mTLS通信的应用图
- en: Next, we will be securing Ingress traffic using `https`. This step is important
    to revise but the outcome of it creates a problem in accessing the application,
    so we will perform the steps to revise the concepts and then revert them back
    so that we can continue accessing the application.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`https`来保护Ingress流量。这个步骤很重要，需要修订，但它的结果会导致无法访问应用程序的问题，因此我们将执行修订步骤后再将其恢复，以便继续访问应用程序。
- en: 'We will use the learning from *Chapter 4*’s *Exposing Ingress over HTTPS* section.
    The steps are much easier if you have a `onlineboutique.com` domain:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用*第4章*中*通过HTTPS暴露入口*部分的内容进行学习。如果你拥有一个`onlineboutique.com`域名，步骤会简单得多：
- en: 'Create a CA. Here, we are creating a CA with a `onlineboutique.inc`:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个CA。这里，我们正在使用`onlineboutique.inc`来创建CA：
- en: '[PRE25]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Generate a `onlineboutique.com`, which also generates a private key:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个`onlineboutique.com`，同时生成一个私钥：
- en: '[PRE26]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Sign the CSR using the CA using the following command:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令通过 CA 签署 CSR：
- en: '[PRE27]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Load the certificate and private key as a Kubernetes Secret:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将证书和私钥作为 Kubernetes Secret 加载：
- en: '[PRE28]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We have created the certificate and stored them as Kubernetes Secret. In the
    next steps, we will modify the Istio Gateway configuration to expose the traffic
    over HTTPS using the certificates.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了证书并将其存储为 Kubernetes Secret。在接下来的步骤中，我们将修改 Istio Gateway 配置，通过 HTTPS 暴露流量，并使用这些证书。
- en: 'Create the Gateway configuration as described in the following command:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下命令创建 Gateway 配置：
- en: '[PRE29]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Apply the following configuration:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 应用以下配置：
- en: '[PRE30]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You can access and check the certificate using the following commands. Please
    note that the output is truncated to highlight relevant sections only:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令访问并检查证书。请注意，输出已被截断，仅突出显示相关部分：
- en: '[PRE31]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The configuration will secure the Ingress traffic to the `online-boutique` store,
    but it also means that you will not be able to access it from the browser because
    of a mismatch of the FQDN being used in the browser and the CN configured in the
    certificates. You can alternatively register DNS names against the AWS load balancer
    but for now, you might find it easier to remove the HTTPS configuration and revert
    to using the `Chapter12/trafficmanagement/01-gateway.yaml` file on GitHub.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 该配置将保护 `online-boutique` 商店的 Ingress 流量，但这也意味着你将无法通过浏览器访问它，因为浏览器中使用的 FQDN 与证书中配置的
    CN 不匹配。你可以选择在 AWS 负载均衡器上注册 DNS 名称，但现在，你可能会发现删除 HTTPS 配置并恢复使用 GitHub 上的 `Chapter12/trafficmanagement/01-gateway.yaml`
    文件会更容易。
- en: Let’s dive deeper into security and perform `RequestAuthentication` and authorization
    for the Online Boutique store. In *Chapter 6*, we did an elaborate exercise of
    building authentication and authorization using Auth0\. Along the same lines,
    we will be building an authentication and authorization policy for the `frontend`
    service but this time, we will use a dummy JWKS endpoint, which is shipped in
    with Istio.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨安全性，并为 Online Boutique 商店执行 `RequestAuthentication` 和授权。在 *第6章* 中，我们详细讲解了使用
    Auth0 构建认证和授权的过程。按照相同的思路，我们将为 `frontend` 服务构建认证和授权策略，但这一次，我们将使用 Istio 附带的虚拟 JWKS
    端点。
- en: 'We will start with creating a `RequestAuthentication` policy to define the
    authentication method supported by the `frontend` service:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从创建一个 `RequestAuthentication` 策略开始，用来定义 `frontend` 服务支持的认证方法：
- en: '[PRE32]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We are making use of dummy `jwksUri`, which comes along with Istio for testing
    purposes. Apply the `RequestAuthentication` policy using the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用 Istio 附带的虚拟 `jwksUri` 进行测试。使用以下命令应用 `RequestAuthentication` 策略：
- en: '[PRE33]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'After applying the `RequestAuthentication` policy, you can test that by providing
    a dummy token to the `frontend` service:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 应用 `RequestAuthentication` 策略后，你可以通过向 `frontend` 服务提供虚拟令牌来进行测试：
- en: 'Fetch the dummy token and set it as an environment variable to be used in requests
    later:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取虚拟令牌并将其设置为环境变量，稍后在请求中使用：
- en: '[PRE34]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Test using `curl`:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `curl` 进行测试：
- en: '[PRE35]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Notice that you received a `200` response.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到你收到了一个 `200` 响应。
- en: 'Now try testing with an invalid token:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在尝试使用无效令牌进行测试：
- en: '[PRE36]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The `RequestAuthentication` policy plunged into action and denied the request.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`RequestAuthentication` 策略已生效并拒绝了请求。'
- en: 'Test without any token:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试没有令牌的情况：
- en: '[PRE37]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The outcome of the request is not desired but is expected because the `RequestAuthentication`
    policy is only responsible for validating a token if a token is passed. If there
    is no `Authorization` header in the request, then the `RequestAuthentication`
    policy will not be invoked. We can solve this problem using `AuthorizationPolicy`,
    which enforces an access control policy for workloads in the mesh.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 请求的结果不是我们期望的，但这是预期的，因为 `RequestAuthentication` 策略仅负责在传递令牌时验证该令牌。如果请求中没有 `Authorization`
    头部，`RequestAuthentication` 策略将不会被调用。我们可以通过使用 `AuthorizationPolicy` 来解决这个问题，后者为服务网格中的工作负载强制执行访问控制策略。
- en: 'Let’s build `AuthorizationPolicy`, which enforces that a principal must be
    present in the request:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建 `AuthorizationPolicy`，强制要求请求中必须存在一个主体：
- en: '[PRE38]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The configuration is available in the `Chapter12/security/requestAuthorizationPolicy.yaml`
    file in GitHub. Apply the configuration using the following command:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件可以在 GitHub 上的 `Chapter12/security/requestAuthorizationPolicy.yaml` 文件中找到。使用以下命令应用配置：
- en: '[PRE39]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'After applying the configuration test using *Steps 1* to *4*, which we performed
    after applying the `RequestAuthentication` policy, you will notice that all steps
    work as expected, but for *Step 4*, we are getting the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用了`RequestAuthentication`策略后，按照*步骤1*到*步骤4*执行配置测试时，您会发现所有步骤都按预期工作，但在*步骤4*中，我们遇到了以下问题：
- en: '[PRE40]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: That is because the authorization policy enforces the required presence of a
    JWT with the `["``testing@secure.istio.io/testing@secure.istio.io"]` principal.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为授权策略强制要求JWT的存在，且该JWT具有`["``testing@secure.istio.io/testing@secure.istio.io"]`主体。
- en: This concludes the security configuration for our Online Boutique application.
    In the next section, we will read about various resources that will help you become
    an expert and certified in using and operating Istio.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们为在线商店应用程序配置安全性设置的总结。在下一节中，我们将阅读有关各种资源的内容，这些资源将帮助您成为使用和操作Istio的专家，并获得认证。
- en: Certification and learning resources for Istio
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Istio的认证和学习资源
- en: The primary resource for learning Istio is the Istio website ([https://istio.io/latest/](https://istio.io/latest/)).
    There is elaborative documentation on performing basic to multi-cluster setups.
    There are resources for beginners and advanced users, and various exercises on
    performing traffic management, security, observability, extensibility, and policy
    enforcement. Outside of the Istio documentation, the other organization providing
    lots of supportive content on Istio is Tetrate ([https://tetrate.io/](https://tetrate.io/)),
    which also provides labs and certification courses. One such certification provided
    by Tetrate Academy is **Certified Istio Administrator**. Details about the course
    and exam are available at [https://academy.tetrate.io/courses/certified-istio-administrator](https://academy.tetrate.io/courses/certified-istio-administrator).
    Tetrate Academy also provides a free course to learn about Istio fundamentals.
    You can find the details of the course at [https://academy.tetrate.io/courses/istio-fundamentals](https://academy.tetrate.io/courses/istio-fundamentals).
    Similarly, there is a course from Solo.io named **Get Started with Istio**; you
    can find details of the course at [https://academy.solo.io/get-started-with-istio](https://academy.solo.io/get-started-with-istio).
    Another good course from The Linux Foundation is named **Introduction to Istio**,
    and you can find the details of the course at [https://training.linuxfoundation.org/training/introduction-to-istio-lfs144x/](https://training.linuxfoundation.org/training/introduction-to-istio-lfs144x/).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 学习Istio的主要资源是Istio官方网站([https://istio.io/latest/](https://istio.io/latest/))。网站上有关于执行从基础到多集群设置的详细文档。资源涵盖了初学者和高级用户，并提供了各种关于流量管理、安全性、可观察性、可扩展性和策略执行的练习。除了Istio文档之外，提供大量Istio支持内容的另一个组织是Tetrate([https://tetrate.io/](https://tetrate.io/))，它还提供实验室和认证课程。Tetrate
    Academy提供的一项认证是**认证Istio管理员**。有关课程和考试的详细信息，请访问[https://academy.tetrate.io/courses/certified-istio-administrator](https://academy.tetrate.io/courses/certified-istio-administrator)。Tetrate
    Academy还提供了一门免费的Istio基础课程。您可以在[https://academy.tetrate.io/courses/istio-fundamentals](https://academy.tetrate.io/courses/istio-fundamentals)找到课程的详细信息。同样，Solo.io也提供了一门名为**开始使用Istio**的课程，您可以在[https://academy.solo.io/get-started-with-istio](https://academy.solo.io/get-started-with-istio)查看课程的详细信息。来自Linux基金会的另一门好课程叫做**Istio简介**，您可以在[https://training.linuxfoundation.org/training/introduction-to-istio-lfs144x/](https://training.linuxfoundation.org/training/introduction-to-istio-lfs144x/)找到该课程的详细信息。
- en: I personally enjoy the learning resources available at [https://istiobyexample.dev/](https://istiobyexample.dev/);
    the site explains various use cases of Istio (such as canary deployment, managing
    Ingress, managing gRPC traffic, and so on) in great detail, along with configuration
    examples. For any technical questions, you can always head to StackOverflow at
    [https://stackoverflow.com/questions/tagged/istio](https://stackoverflow.com/questions/tagged/istio).
    There is an energetic and enthusiastic community of Istio users and builders who
    are discussing various topics about Istio at [https://discuss.istio.io/](https://discuss.istio.io/);
    feel free to sign up for the discussion board.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我个人喜欢在[https://istiobyexample.dev/](https://istiobyexample.dev/)上使用的学习资源；该网站详细解释了Istio的各种使用场景（例如金丝雀发布、管理Ingress、管理gRPC流量等），并提供了配置示例。如有任何技术问题，您可以随时访问StackOverflow，[https://stackoverflow.com/questions/tagged/istio](https://stackoverflow.com/questions/tagged/istio)。这里有一个充满活力和热情的Istio用户与开发者社区，他们在[https://discuss.istio.io/](https://discuss.istio.io/)讨论各种与Istio相关的话题；欢迎注册讨论论坛。
- en: Tetrate Academy also provides a free course on Envoy fundamentals; the course
    is very helpful to understand the fundamentals of Envoy and, in turn, the Istio
    data plane. You can find the details of this course at [https://academy.tetrate.io/courses/envoy-fundamentals](https://academy.tetrate.io/courses/envoy-fundamentals).
    The course is full of practical labs and quizzes that are very helpful in mastering
    your Envoy skills.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Tetrate Academy 还提供了一个免费的 Envoy 基础课程；该课程非常有助于理解 Envoy 的基础知识，进而理解 Istio 数据平面。你可以在
    [https://academy.tetrate.io/courses/envoy-fundamentals](https://academy.tetrate.io/courses/envoy-fundamentals)
    找到该课程的详细信息。课程中包含了许多实用的实验和小测验，非常有助于你掌握 Envoy 技能。
- en: The Istio website has compiled a list of helpful resources to keep you updated
    with Istio and engage with the Istio community; you can find the list at [https://istio.io/latest/get-involved/](https://istio.io/latest/get-involved/).
    The list also provides you with details on how to report bugs and issues.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 网站已经整理了一份有用的资源清单，帮助你了解 Istio 的最新动态并与 Istio 社区互动；你可以在 [https://istio.io/latest/get-involved/](https://istio.io/latest/get-involved/)
    找到这份清单。该清单还提供了如何报告 bugs 和问题的详细信息。
- en: To summarize, there are not many resources except a few books and websites,
    but you will find most of the answers to your questions at [https://istio.io/latest/docs/](https://istio.io/latest/docs/).
    It is also a great idea to follow **IstioCon**, which is the Istio Community conference
    and happens on a yearly cadence. You can find a session of IstioCon 2022 at [https://events.istio.io/istiocon-2022/sessions/](https://events.istio.io/istiocon-2022/sessions/)
    and 2021 at [https://events.istio.io/istiocon-2021/sessions/](https://events.istio.io/istiocon-2021/sessions/).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，除了少数几本书和网站外，相关资源并不多，但你大多数问题的答案可以在 [https://istio.io/latest/docs/](https://istio.io/latest/docs/)
    找到。关注 **IstioCon** 也是一个不错的主意，IstioCon 是 Istio 社区的年会，每年举办一次。你可以在 [https://events.istio.io/istiocon-2022/sessions/](https://events.istio.io/istiocon-2022/sessions/)
    找到 2022 年的会议内容，在 [https://events.istio.io/istiocon-2021/sessions/](https://events.istio.io/istiocon-2021/sessions/)
    找到 2021 年的会议内容。
- en: Understanding eBPF
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 eBPF
- en: As we are at the end of this book, it is important to also look at other technologies
    that are relevant to Service Mesh. One such technology is the **Extended Berkeley
    Packet Filter** (**eBPF**). In this section, we will read about eBPF and its role
    in Service Mesh evolution.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书接近尾声时，了解与服务网格相关的其他技术也非常重要。其中一个技术就是 **扩展伯克利数据包过滤器**（**eBPF**）。在这一部分，我们将了解
    eBPF 及其在服务网格演变中的作用。
- en: eBPF is a framework that allows users to run custom programs within the kernel
    of the operating system without needing to change kernel source code or load kernel
    modules. The custom programs are called **eBPF programs** and are used to add
    additional capabilities to the operating system at runtime. The eBPF programs
    are safe and efficient and, like the kernel modules, they are like lightweight
    sandbox virtual machines run in a privileged context by the operating system.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: eBPF 是一个框架，它允许用户在操作系统内核中运行自定义程序，而无需修改内核源代码或加载内核模块。自定义程序被称为 **eBPF 程序**，用于在运行时向操作系统添加额外的功能。eBPF
    程序既安全又高效，像内核模块一样，它们是轻量级的沙盒虚拟机，由操作系统在特权上下文中运行。
- en: eBPF programs are triggered based on events happening at the kernel level, which
    is achieved by associating them to hook points. Hooks are predefined at kernel
    levels and include system calls, network events, function entry and exit, and
    so on. In scenarios where an appropriate hook doesn’t exist, then users can make
    use of kernel probes, also called kprobes. The kprobes are inserted into the kernel
    routine; ebPF programs are defined as a handler to kprobes and are executed whenever
    a particular breakpoint is hit in the kernel. Like hooks and kprobes, eBPF programs
    can also be attached to uprobes, which are probes at user space levels and are
    tied to an event at the user application level, thus eBPF programs can be executed
    at any level from the kernel to the user application. When executing programs
    at the kernel level, the biggest concern is the security of the program. In eBPF,
    that is assured by BPF libraries. The BPF libraries handle the system call to
    load the eBPF programs in two steps. The first step is the verification step,
    during which the eBPF program is validated to ensure that it will run to completion
    and will not lock up the kernel, the process loading the eBPF program has correct
    privileges, and the eBPF program will not harm the kernel in any way. The second
    step is a **Just-In-Time** (**JIT**) compilation step, which translates the generic
    bytecode of the program into the machine-specific instruction and optimizes it
    to get the maximum execution speed of the program. This makes eBPF programs run
    as efficiently as natively compiled kernel code as if it was loaded as a kernel
    module. Once the two steps are complete, the eBPF program is loaded and compiled
    into the kernel waiting for the hook or kprobes to trigger the execution.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: eBPF 程序是基于内核级事件触发的，这通过将它们与钩子点关联来实现。钩子是预定义的内核级事件，包括系统调用、网络事件、函数入口和退出等。在没有合适钩子的场景下，用户可以使用内核探针，也叫做
    kprobes。kprobes 被插入到内核例程中；eBPF 程序被定义为 kprobes 的处理程序，并且每当在内核中触发特定断点时，eBPF 程序就会执行。像钩子和
    kprobes 一样，eBPF 程序也可以附加到 uprobes 上，uprobes 是用户空间级别的探针，绑定到用户应用程序级别的事件，因此 eBPF 程序可以在从内核到用户应用程序的任何级别执行。执行内核级程序时，最大的关注点是程序的安全性。在
    eBPF 中，这是通过 BPF 库来保证的。BPF 库处理加载 eBPF 程序的系统调用，分为两个步骤。第一个步骤是验证步骤，在此过程中验证 eBPF 程序，确保它能够完成执行并且不会锁死内核，加载
    eBPF 程序的进程具有正确的权限，并且 eBPF 程序不会对内核造成任何危害。第二个步骤是**即时编译**（**JIT**）步骤，它将程序的通用字节码转换为特定机器的指令，并对其进行优化，以获得程序的最大执行速度。这使得
    eBPF 程序的运行效率与本地编译的内核代码相当，就像它作为内核模块加载一样。一旦这两个步骤完成，eBPF 程序就会被加载并编译进内核，等待钩子或 kprobes
    触发执行。
- en: BPF has been widely used as an add-on to the kernel. Most of the applications
    have been at the network level and mostly in observability space. eBPF has been
    used to provide visibility into system calls at packet and socket levels, which
    are then used for building security solution systems that can operate with low-level
    context from the kernel. eBPF programs are also used for introspection of user
    applications along with the part of the kernel running the application, which
    provides a consolidated insight to troubleshoot application performance issues.
    You might be wondering why we are discussing eBPF in the context of Service Mesh.
    The programmability and plugin model of eBPF is particularly useful in networking.
    eBPF can be used to perform IP routing, packet filtering, monitoring, and so on
    at native speeds of kernel modules. One of the drawbacks of the Istio architecture
    is its model of deploying a sidecar with every workload, as we discussed in *Chapter
    2* – the sidecar basically works by intercepting network traffic, making use of
    iptables to configure the kernel’s netfilter packet filter functionality. The
    drawback of this approach is less optimal performance, as the data path created
    for service traffic is much longer than what it would have been if the workload
    was just by itself without any sidecar traffic interception. With eBPF socket-related
    program types, you can filter socket data, redirect socket data, and monitor socket
    events. These programs have the potential for replacing the iptables-based traffic
    interception; using eBPF, there are options to intercept and manage network traffic
    without incurring any negative impacts on the data path performance.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: BPF已经广泛作为内核的附加组件使用。大多数应用程序都集中在网络层，主要用于可观察性领域。eBPF被用来提供对数据包和套接字级别的系统调用的可视化，进而用于构建可以从内核低级上下文中操作的安全解决方案系统。eBPF程序还用于对用户应用程序以及运行该应用程序的内核部分进行自省，这提供了一个统一的视角，用于排查应用程序性能问题。你可能会想知道为什么我们在服务网格的上下文中讨论eBPF。eBPF的可编程性和插件模型在网络领域特别有用。eBPF可以用于执行IP路由、数据包过滤、监控等功能，且速度与内核模块的本地速度相同。Istio架构的一个缺点是它为每个工作负载都部署一个边车（sidecar），正如我们在*第2章*中讨论的那样——边车的基本工作原理是拦截网络流量，利用iptables配置内核的netfilter数据包过滤功能。这种方法的缺点是性能不太理想，因为为服务流量创建的数据路径比没有边车流量拦截时的路径要长得多。通过eBPF的套接字相关程序类型，你可以过滤套接字数据、重定向套接字数据，并监控套接字事件。这些程序有可能替代基于iptables的流量拦截；使用eBPF，有多种方式可以拦截和管理网络流量，而不会对数据路径性能产生负面影响。
- en: '**Isovalent** (at [https://isovalent.com/](https://isovalent.com/)) is one
    such organization that is revolutionizing the architecture of API Gateway and
    Service Mesh. **Cilium** is a product from Isovalent, and it provides a variety
    of functionality, including API Gateway function, Service Mesh, observability,
    and networking. Cilium is built with eBPF as its core technology where it injects
    eBPF programs at various points in the Linux kernel to achieve application networking,
    security, and observability functions. Cilium is getting adopted in Kubernetes
    networking to solve performance degradation caused by packets needing to traverse
    the same network stack multiple times between the host and the Pod. Cilium is
    solving this problem by bypassing iptables in the networking stacking, avoiding
    net filters and other overheads caused by iptables, which has led to significant
    gains in network performance. You can read more about the Cilium product stack
    at [https://isovalent.com/blog/post/cilium-release-113/](https://isovalent.com/blog/post/cilium-release-113/);
    you will be amazed to see how eBPF is revolutionizing the application networking
    space.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**Isovalent**（访问 [https://isovalent.com/](https://isovalent.com/)）就是这样一个正在革新API网关和服务网格架构的组织。**Cilium**是Isovalent的一个产品，它提供多种功能，包括API网关功能、服务网格、可观察性和网络功能。Cilium是以eBPF为核心技术构建的，它在Linux内核的多个位置注入eBPF程序，从而实现应用程序网络、安全和可观察性功能。Cilium正在被Kubernetes网络中采用，以解决由于数据包需要在主机和Pod之间多次穿越相同的网络栈而导致的性能下降问题。Cilium通过绕过iptables的网络堆栈，避免了iptables带来的网络过滤器和其他开销，从而大幅提高了网络性能。你可以在[https://isovalent.com/blog/post/cilium-release-113/](https://isovalent.com/blog/post/cilium-release-113/)阅读更多关于Cilium产品栈的信息；你将会惊讶地发现，eBPF正在如何革新应用程序网络领域。'
- en: Istio has also created an open source project called Merbridge, which replaces
    iptables with eBPF programs to allow the transporting of data directly between
    inbound and outbound sockets of sidecar containers and application containers
    to shorten the overall data path. Merbridge is in its early days but has produced
    some promising results; you can find the open source project at [https://github.com/merbridge/merbridge](https://github.com/merbridge/merbridge).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Istio还创建了一个名为Merbridge的开源项目，它用eBPF程序替代了iptables，允许数据直接在边车容器和应用容器的进出套接字之间传输，从而缩短整体数据路径。Merbridge虽然还处于初期阶段，但已经取得了一些有前景的成果；你可以在[https://github.com/merbridge/merbridge](https://github.com/merbridge/merbridge)找到这个开源项目。
- en: With eBPF and products like Cilium, it is highly likely that there will be an
    advancement in how network proxy-based products will be designed and operated
    in the future. eBPF is being actively explored by various Service Mesh technologies,
    including Istio, on how it can be used to overcome drawbacks and improve the overall
    performance and experience of using Istio. eBPF is a very promising technology
    and is already being used for doing awesome things with products such as Cilium
    and Calico.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 随着eBPF和像Cilium这样的产品的出现，未来网络代理产品的设计和操作方式可能会有显著的进展。eBPF正在被包括Istio在内的各种服务网格技术积极探索，研究如何利用它来克服缺点并提升使用Istio的整体性能和体验。eBPF是一项非常有前景的技术，已经在像Cilium和Calico这样的产品中得到了广泛应用，做出了很多令人惊叹的成果。
- en: Summary
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: I hope this book has provided you with a good insight into Istio. *Chapters
    1* to *3* set the context on why Service Mesh is needed and how Istio the control
    and data planes operate. The information in these three chapters is important
    to appreciate Istio and to build an understanding of Istio architecture. *Chapters
    4* to *6* then provided details on how to use Istio for building the application
    network that we discussed in the earlier chapters.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望本书能为你提供对Istio的深入了解。*第1章*到*第3章*为你设定了服务网格为何需要以及Istio的控制平面和数据平面如何运作的背景。这三章中的信息对理解Istio及其架构非常重要。接着，*第4章*到*第6章*提供了如何使用Istio构建我们在前几章中讨论的应用网络的详细信息。
- en: Then, in *Chapter 7*, you learned about observability and how Istio provides
    integration into various observation tools, as the next steps you should explore
    integration with other observability and monitoring tools such as Datadog. Following
    that, *Chapter 8* showed practices on how to deploy Istio across multiple Kubernetes
    clusters, which should have given you confidence on how to install Istio in production
    environments. *Chapter 9* then provided details on how Istio can be extended using
    WebAssembly and its applications, while *Chapter 10* discussed how Istio helps
    bridge the old world of virtual machines with the new world of Kubernetes by discussing
    how the Service Mesh can be extended to include workloads deployed on virtual
    machines. Lastly, *Chapter 11* covered the best practices for operating Istio
    and how tools such as OPA Gatekeeper can be used to automate some of the best
    practices.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在*第7章*中，你学习了可观察性以及Istio如何与各种观察工具进行集成，接下来的步骤是你应该探索与其他可观察性和监控工具的集成，例如Datadog。接着，*第8章*展示了如何在多个Kubernetes集群中部署Istio的实践，这应该让你对如何在生产环境中安装Istio充满信心。*第9章*接着提供了如何使用WebAssembly扩展Istio及其应用的详细信息，而*第10章*讨论了Istio如何帮助将旧有的虚拟机世界与新兴的Kubernetes世界连接，通过讨论如何扩展服务网格以包含部署在虚拟机上的工作负载。最后，*第11章*涵盖了操作Istio的最佳实践，以及如何使用OPA
    Gatekeeper等工具自动化一些最佳实践。
- en: In this chapter, we managed to revise the concepts of *Chapters 4* to *6* by
    deploying and configuring another open source demo application, which should have
    provided you with the confidence and experience to take on the learnings from
    the book to real-life applications and to take advantage of application networking
    and security provided by Istio.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过部署和配置另一个开源示范应用，复习了*第4章*到*第6章*的概念，这应该让你充满信心并获得经验，将书中的学习应用到实际应用中，并利用Istio提供的应用网络和安全功能。
- en: 'You also read about eBPF and what a game-changing technology it is, making
    it possible to write code at the kernel level without needing to understand or
    experience the horrors of the kernel. eBPF will possibly bring lots of changes
    to how Service Mesh, API Gateway, and networking solutions in general operate.
    In the Appendix of this book, you will find information about other Service Mesh
    technologies: Consul Connect, Kuma Mesh, Gloo Mesh, and Linkerd. The Appendix
    provides a good overview of these technologies and helps you appreciate their
    strength and limitations.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会了解 eBPF 以及它作为一种颠覆性技术的重要性，它使得在不需要理解或经历内核恐怖的情况下，也可以在内核级别编写代码。eBPF 可能会给服务网格、API
    网关和一般的网络解决方案带来许多变化。在本书的附录中，你将找到关于其他服务网格技术的信息：Consul Connect、Kuma Mesh、Gloo Mesh
    和 Linkerd。附录提供了这些技术的概述，并帮助你理解它们的优势和局限性。
- en: I hope you enjoyed learning about Istio. To establish your knowledge of Istio,
    you can also explore taking the Certified Istio Administrator exam provided by
    Tetrate. You can also explore the other learning avenues provided in this chapter.
    I hope reading this book was an endeavor that will take you to the next level
    in your career and experience of building scalable, resilient, and secure applications
    using Istio.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢学习 Istio。为了巩固你对 Istio 的知识，你还可以探索参加 Tetrate 提供的认证 Istio 管理员考试。你还可以探索本章中提供的其他学习途径。希望你阅读本书的过程能帮助你提升职业生涯，并在使用
    Istio 构建可扩展、弹性和安全的应用程序方面获得更多经验。
- en: BEST OF LUCK!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你好运！
- en: Appendix – Other Service Mesh Technologies
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 – 其他服务网格技术
- en: 'In this appendix, we will learn about the following Service Mesh implementations:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中，我们将学习以下服务网格实现：
- en: Consul Connect
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Consul Connect
- en: Gloo Mesh
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gloo Mesh
- en: Kuma
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuma
- en: Linkerd
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linkerd
- en: These Service Mesh technologies are popular and are gaining recognition and
    adoption by organizations. The information provided in this *Appendix* about these
    Service Mesh technologies is not exhaustive; rather, the goal here is to make
    you familiar with and aware of the alternatives to Istio. I hope reading this
    *Appendix* will provide some basic awareness of these alternative technologies
    and help you understand how these technologies fare in comparison to Istio. Let’s
    dive in!
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这些服务网格技术很受欢迎，并且正在被越来越多的组织认可和采用。本 *附录* 中提供的关于这些服务网格技术的信息并不详尽；这里的目标是让你熟悉并了解 Istio
    的替代方案。我希望阅读本 *附录* 能为你提供这些替代技术的基本认知，并帮助你理解这些技术与 Istio 相比的表现。让我们开始深入了解吧！
- en: Consul Connect
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Consul Connect
- en: Consul Connect is a Service Mesh solution offered by HashiCorp. It is also known
    as Consul Service Mesh. On the HashiCorp website, you will find that the terms
    Consul Connect and Consul Service Mesh are used interchangeably. It is built upon
    Consul, which is a service discovery solution and a key-value store. Consul is
    a very popular and long-established service discovery solution; it provides and
    manages service identities for every type of workload, which are then used by
    Service Mesh to manage traffic between Services in Kubernetes. It also supports
    using ACLs to implement zero-trust networking and provides granular control over
    traffic flow in the mesh.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Consul Connect 是由 HashiCorp 提供的服务网格解决方案，也称为 Consul 服务网格。在 HashiCorp 的官网上，你会发现
    Consul Connect 和 Consul 服务网格这两个术语是可以互换使用的。它是基于 Consul 构建的，而 Consul 是一个服务发现解决方案和键值存储。Consul
    是一个非常流行且历史悠久的服务发现解决方案；它为各种工作负载提供和管理服务身份，然后这些身份被服务网格用来管理 Kubernetes 中服务之间的流量。它还支持使用
    ACL 来实现零信任网络，并提供对网格中流量流动的精细控制。
- en: Consul uses Envoy as its data plane and injects it into workload Pods as sidecars.
    The injection can be based on annotations as well as global configurations to
    automatically inject sidecar proxies into all workloads in specified namespaces.
    We will start by installing Consul Service Mesh on your workstation, followed
    by some exercises to practice the basics of using Consul Service Mesh.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Consul 使用 Envoy 作为其数据平面，并将其注入到工作负载 Pod 中作为 sidecar。注入可以基于注解和全局配置来自动将 sidecar
    代理注入到指定命名空间中的所有工作负载。我们将从在你的工作站上安装 Consul 服务网格开始，然后进行一些练习，帮助你掌握使用 Consul 服务网格的基础知识。
- en: 'Let’s begin by installing Consul:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从安装 Consul 开始：
- en: 'Clone the Consul repository:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆 Consul 仓库：
- en: '[PRE41]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Install the Consul CLI:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 Consul CLI：
- en: 'For MacOS, follow these steps:'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 MacOS，请按照以下步骤操作：
- en: 'Install the HashiCorp tap:'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 HashiCorp tap：
- en: '[PRE42]'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Install the Consul Kubernetes CLI:'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 Consul Kubernetes CLI：
- en: '[PRE43]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'For Linux Ubuntu/Debian, follow these steps:'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Linux Ubuntu/Debian，请按照以下步骤操作：
- en: 'Add the HashiCorp GPG key:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加 HashiCorp GPG 密钥：
- en: '[PRE45]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Add the HashiCorp apt repository:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加HashiCorp的apt仓库：
- en: '[PRE46]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '% sudo apt-get update && sudo apt-get install consul-k8s'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '% sudo apt-get update && sudo apt-get install consul-k8s'
- en: '[PRE47]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '% sudo yum install -y yum-utils'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '% sudo yum install -y yum-utils'
- en: '[PRE48]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'consul-k8s CLI:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: consul-k8s CLI：
- en: '[PRE49]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Start minikube:'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 启动minikube：
- en: '[PRE50]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Install Consul on minikube using the Consul CLI.
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用Consul CLI在minikube上安装Consul。
- en: 'Run the following in learn-consul-get-started-kubernetes/local:'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在learn-consul-get-started-kubernetes/local中运行以下命令：
- en: '[PRE51]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Check the Consul Pods in the namespace:'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查命名空间中的Consul Pods：
- en: '[PRE52]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Configure the Consul CLI to be able to communicate with Consul.
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置Consul CLI以便能够与Consul通信。
- en: We will set environment variables so that the Consul CLI can communicate with
    your Consul cluster.
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将设置环境变量，以便Consul CLI能够与您的Consul集群通信。
- en: 'Set `CONSUL_HTTP_TOKEN` from `secrets/consul-bootstrap-acl-token` and set it
    as an environment variable:'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从`secrets/consul-bootstrap-acl-token`设置`CONSUL_HTTP_TOKEN`并将其设置为环境变量：
- en: '[PRE53]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Set the Consul destination address. By default, Consul runs on port 8500 for
    HTTP and 8501 for HTTPS:'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置Consul目标地址。默认情况下，Consul在端口8500上运行HTTP，在端口8501上运行HTTPS：
- en: '[PRE54]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Remove SSL verification checks to simplify communication with your Consul cluster:'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 删除SSL验证检查，以简化与您的Consul集群的通信：
- en: '[PRE55]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Access the Consul dashboard using the following command:'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令访问Consul仪表盘：
- en: '[PRE56]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Open `localhost:8501` in your browser to access the Consul dashboard, as shown
    in the following screenshot:'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在浏览器中打开`localhost:8501`以访问Consul仪表盘，如下图所示：
- en: '![Figure A.1 – Consul Dashboard](img/Figure_13.01_B17989.jpg)'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 A.1 – Consul仪表盘](img/Figure_13.01_B17989.jpg)'
- en: Figure A.1 – Consul Dashboard
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图A.1 – Consul仪表盘
- en: Now that we have installed Consul Service Mesh on minikube, let’s deploy an
    example application and go through the fundamentals of Consul Service Mesh.
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已在minikube上安装了Consul服务网格，让我们部署一个示例应用程序，并了解Consul服务网格的基本原理。
- en: Deploying an example application
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部署示例应用程序
- en: In this section, we will deploy envoydummy along with a curl application. The
    `sampleconfiguration` file is available in `AppendixA/envoy-proxy-01.yaml`.
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这一节中，我们将部署envoydummy及一个curl应用程序。`sampleconfiguration`文件位于`AppendixA/envoy-proxy-01.yaml`。
- en: 'In the configuration file, you will notice the following annotation:'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在配置文件中，您将看到以下注释：
- en: '[PRE57]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: This annotation allows Consul to automatically inject a proxy for each service.
    The proxies create a data plane to handle requests between services based on the
    configuration from Consul.
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该注释允许Consul自动为每个服务注入一个代理。代理创建了一个数据平面，根据Consul的配置处理服务之间的请求。
- en: 'Apply the configuration to create `envoydummy` and the `curl` Pods:'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应用配置以创建`envoydummy`和`curl` Pods：
- en: '[PRE58]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'In a few seconds, you will notice that Consul automatically injects a sidecar
    into the Pods:'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 几秒钟后，您将看到Consul自动将sidecar注入到Pods中：
- en: '[PRE59]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'To find out more about the sidecar, please inspect the `envoydummy` Pod using
    the following commands:'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要了解更多关于sidecar的信息，请使用以下命令检查`envoydummy` Pod：
- en: '[PRE60]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: In the output, you can see a container named `consul-dataplane` created from
    an image called `hashicorp/consul-dataplane:1.0.0`. You can inspect the image
    at [https://hub.docker.com/layers/hashicorp/consul-dataplane/1.0.0-beta1/images/sha256-f933183f235d12cc526099ce90933cdf43c7281298b3cd34a4ab7d4ebeeabf84?context=explore](https://hub.docker.com/layers/hashicorp/consul-dataplane/1.0.0-beta1/images/sha256-f933183f235d12cc526099ce90933cdf43c7281298b3cd34a4ab7d4ebeeabf84?context=explore)
    and you will notice that it is made up of envoy proxy.
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在输出中，您会看到一个名为`consul-dataplane`的容器，它是从名为`hashicorp/consul-dataplane:1.0.0`的镜像创建的。您可以在[https://hub.docker.com/layers/hashicorp/consul-dataplane/1.0.0-beta1/images/sha256-f933183f235d12cc526099ce90933cdf43c7281298b3cd34a4ab7d4ebeeabf84?context=explore](https://hub.docker.com/layers/hashicorp/consul-dataplane/1.0.0-beta1/images/sha256-f933183f235d12cc526099ce90933cdf43c7281298b3cd34a4ab7d4ebeeabf84?context=explore)查看该镜像，您会发现它是由envoy代理组成的。
- en: 'Let’s try to access `envoydummy` from the `curl` Pod:'
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们尝试从`curl` Pod访问`envoydummy`：
- en: '[PRE61]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: So far, we have successfully deployed the `envoydummy` Pod along with `consul-dataplane`
    as a sidecar. We have observed Consul Service Mesh security in action by seeing
    that the `curl` Pod, while deployed in the same namespace, is unable to access
    the `envoydummy` Pod. In the next section, we will understand this behavior and
    learn how to configure Consul to perform zero-trust networking.
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到目前为止，我们已成功部署`envoydummy` Pod，并将`consul-dataplane`作为sidecar一同部署。我们通过观察`curl`
    Pod发现，尽管它与`envoydummy` Pod部署在同一个命名空间，但无法访问该Pod，从而验证了Consul服务网格安全的实际效果。在下一节中，我们将了解这一行为，并学习如何配置Consul以实现零信任网络。
- en: Zero-trust networking
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 零信任网络
- en: Consul manages inter-service authorization with Consul constructs called intentions.
    Using Consul CRDs, you need to define intentions that prescribe what services
    are allowed to communicate with each other. Intentions are the cornerstones of
    zero-trust networking in Consul.
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Consul通过名为意图的Consul构造来管理服务间授权。使用Consul CRD，你需要定义意图，规定哪些服务可以互相通信。意图是Consul中零信任网络的基石。
- en: Intentions are enforced by the sidecar proxy on inbound connections. The sidecar
    proxy identifies the inbound service using its TLS client certificate. After identifying
    the inbound service, the sidecar proxy then checks if an intention exists to allow
    the client to communicate with the destination service.
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 意图由边车代理在入站连接上执行。边车代理通过其TLS客户端证书识别入站服务。在识别入站服务后，边车代理检查是否存在允许客户端与目标服务通信的意图。
- en: 'In the following code block, we are defining an intention to allow traffic
    from the `curl` service to the `envoydummy` service:'
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们定义了一个意图，允许`curl`服务到`envoydummy`服务的流量：
- en: '[PRE62]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: In the configuration, we have specified the names of the destination service
    and the source service. In `action`, we have specified `allow` to allow traffic
    from source to destination. Another possible value of `action` is `deny`, which
    denies traffic from source to destination. If you do not want to specify the name
    of a service, you will need to use `*`. For example, if the service name in `sources`
    is `*` then it will allow traffic from all services to `envoydummy`.
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在配置中，我们指定了目标服务和源服务的名称。在`action`中，我们指定了`allow`，允许源到目标的流量。`action`的另一个可能值是`deny`，即拒绝源到目标的流量。如果你不想指定服务名称，你需要使用`*`。例如，如果`sources`中的服务名称是`*`，则允许所有服务到`envoydummy`的流量。
- en: 'Let’s apply intentions using the following command:'
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们使用以下命令应用意图：
- en: '[PRE63]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'You can verify the created intentions in the Consul dashboard:'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在Consul仪表板中验证已创建的意图：
- en: '![Figure A.2 – Consul intentions](img/Figure_13.02_B17989.jpg)'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 A.2 – Consul 意图](img/Figure_13.02_B17989.jpg)'
- en: Figure A.2 – Consul intentions
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 A.2 – Consul 意图
- en: 'Considering that we have created the intention to allow traffic from the `curl`
    service to the `envoydummy` service, let’s test that the `curl` Pod is able to
    communicate with the `envoydummy` Pod using the following command:'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鉴于我们已经创建了允许`curl`服务到`envoydummy`服务的流量意图，接下来让我们测试`curl` Pod是否能够通过以下命令与`envoydummy`
    Pod进行通信：
- en: '[PRE64]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Using intentions, we were able to define rules to control traffic between services
    without needing to configure a firewall or any changes in the cluster. Intentions
    are key building blocks of Consul for creating zero-trust networks.
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过使用意图，我们能够定义控制服务间流量的规则，而无需配置防火墙或进行集群中的任何更改。意图是Consul构建零信任网络的关键组成部分。
- en: Traffic management and routing
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 流量管理与路由
- en: 'Consul provides a comprehensive set of service discovery and traffic management
    features. The service discovery comprises three stages: routing, splitting, and
    resolution. These three stages are also referred to as the service discovery chain,
    and it can be used to implement traffic controls based on HTTP headers, path,
    query strings, and workload version.'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Consul提供了全面的服务发现和流量管理功能。服务发现包括三个阶段：路由、拆分和解析。这三个阶段也被称为服务发现链，它可以用于基于HTTP头、路径、查询字符串和工作负载版本实现流量控制。
- en: Let’s go through each stage of the service discovery chain.
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们逐步分析服务发现链的每个阶段。
- en: Routing
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 路由
- en: 'This is the first stage of the service discovery chain, and it is used to intercept
    traffic using Layer 7 constructs such as HTTP header and path. This is achieved
    via service-router config entry through which you can control the traffic routing
    using various criteria. For example, for `envoydummy`, let’s say we want to enforce
    that any request send to `envoydummy` version v1 with `/latest` in the URI should
    be routed to `envoydummy` version v2 instead, and any request to version v2 of
    the `envoydummy` app but with `/old` in the path should be routed to version v1
    of the `envoydummy` app. This can be achieved using the following `ServiceRouter`
    configuration:'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是服务发现链的第一个阶段，用于使用Layer 7构造（如HTTP头和路径）拦截流量。这是通过`service-router`配置条目实现的，你可以使用各种标准控制流量路由。例如，对于`envoydummy`，假设我们希望强制任何发送到`envoydummy`版本v1并且URI中包含`/latest`的请求应该路由到`envoydummy`版本v2，而任何发送到`envoydummy`应用版本v2但路径中包含`/old`的请求应该路由到`envoydummy`应用版本v1。这可以通过以下`ServiceRouter`配置实现：
- en: '[PRE65]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'In the configuration, we are specifying that any request destined for the `envoydummy`
    service but with `pathPrefix` set to `''/latest''` will be routed to `envoydummy2`.
    And in the following configuration, we are specifying that any request destined
    for the `envoydummy2` service but with `pathPrefix` set to `''/old''` will be
    routed to `envoydummy`:'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在配置中，我们指定了任何目标为 `envoydummy` 服务的请求，如果 `pathPrefix` 设置为 `'/latest'`，将会路由到 `envoydummy2`。而在接下来的配置中，我们指定了任何目标为
    `envoydummy2` 服务的请求，如果 `pathPrefix` 设置为 `'/old'`，将会路由到 `envoydummy`：
- en: '[PRE66]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Both `ServiceRouter` configurations are saved in `AppendixA/Consul/routing-to-envoy-dummy.yaml`.
    A deployment descriptor for `envoydummy` version v2 and the intentions that allow
    traffic from the `curl` Pod are also available in `AppendixA/Consul/envoy-proxy-02.yaml`
    on GitHub.
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两个 `ServiceRouter` 配置保存在 `AppendixA/Consul/routing-to-envoy-dummy.yaml` 中。`envoydummy`
    版本 v2 的部署描述符以及允许从 `curl` Pod 进行流量的意图也可以在 GitHub 上的 `AppendixA/Consul/envoy-proxy-02.yaml`
    找到。
- en: 'Go ahead and deploy version v2 of `envoydummy` along with the ServiceRouter
    configuration using the following commands:'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用以下命令部署 `envoydummy` 版本 v2 以及 `ServiceRouter` 配置：
- en: '[PRE67]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'You can check the configuration using the Consul dashboard. The following two
    screenshots show the two `ServiceRouter` configurations we have applied:'
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以使用 Consul 仪表板检查配置。以下两张截图展示了我们应用的两个 `ServiceRouter` 配置：
- en: '`ServiceRouter` configuration to send traffic with the prefix `/latest` to
    `envoydummy2`:'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ServiceRouter` 配置，将带有 `/latest` 前缀的流量发送到 `envoydummy2`：'
- en: '![Figure A.3](img/Figure_13.03_B17989.jpg)'
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 A.3](img/Figure_13.03_B17989.jpg)'
- en: Figure A.3
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 A.3
- en: '`ServiceRouter` configuration to send traffic with the prefix `/old` to `envoydummy`:'
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ServiceRouter` 配置，将带有 `/old` 前缀的流量发送到 `envoydummy`：'
- en: '![Figure A.4](img/Figure_13.04_B17989.jpg)'
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 A.4](img/Figure_13.04_B17989.jpg)'
- en: Figure A.4
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 A.4
- en: 'Now that we have configured the service routes, let’s test the routing behavior:'
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经配置了服务路由，让我们来测试一下路由行为：
- en: 'Make any request to version v1 of `envoydummy` with a URI that is not `/latest`:'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向 `envoydummy` 版本 v1 发起请求，URI 不是 `/latest`：
- en: '[PRE68]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The output is as expected: the request should be routed to version v1 of `envoydummy`.'
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出符合预期：请求应该被路由到 `envoydummy` 版本 v1。
- en: 'Make a request to version v1 of `envoydummy` with a URI that is `/latest`:'
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向 `envoydummy` 版本 v1 发起请求，URI 为 `/latest`：
- en: '[PRE69]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The output is as expected: the request, although addressed to version v1 of
    `envoydummy`, is routed to version v2 of `envoydummy`.'
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出符合预期：请求虽然是发送给 `envoydummy` 版本 v1，但实际上被路由到了 `envoydummy` 版本 v2。
- en: 'Make any request to version v2 of `envoydummy` with a URI that is not `/old`:'
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向 `envoydummy` 版本 v2 发起请求，URI 不是 `/old`：
- en: '[PRE70]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The output is as expected: the request should be routed to version v2 of `envoydummy`.'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出符合预期：请求应该被路由到 `envoydummy` 版本 v2。
- en: 'Make a request to version v2 of `envoydummy` with a URI that is `/old`:'
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向 `envoydummy` 版本 v2 发起请求，URI 为 `/old`：
- en: '[PRE71]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The output is as expected: the request although addressed to version v2 of
    `envoydummy` is routed to version v1 of `envoydummy`.'
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出符合预期：请求虽然是发送给 `envoydummy` 版本 v2，但实际上被路由到了 `envoydummy` 版本 v1。
- en: 'In these examples, we made use of path prefixes as criteria for routing. The
    other options are query parameters and HTTP headers. `ServiceRouter` also supports
    retry logic, which can be added to the destination configuration. Here is an example
    of retry logic added to the `ServiceRouter` config:'
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这些示例中，我们利用路径前缀作为路由标准。其他选项还包括查询参数和 HTTP 头。`ServiceRouter` 还支持重试逻辑，这可以添加到目标配置中。以下是添加到
    `ServiceRouter` 配置中的重试逻辑示例：
- en: '[PRE72]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'You can read more about `ServiceRouter` configuration on the HashiCorp website:
    https://developer.hashicorp.com/consul/docs/connect/config-entries/service-router.'
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在 HashiCorp 网站上阅读更多关于 `ServiceRouter` 配置的内容：[https://developer.hashicorp.com/consul/docs/connect/config-entries/service-router](https://developer.hashicorp.com/consul/docs/connect/config-entries/service-router)。
- en: Next in the service discovery chain is splitting, which we will learn about
    in the following section.
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来是服务发现链中的拆分，我们将在以下部分学习相关内容。
- en: Splitting
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 拆分
- en: 'Service splitting is the second stage in the Consul service discovery chain
    and is configured via the `ServiceSplitter` configuration. `ServiceSplitter` allows
    you to split a request to a service to multiple subset workloads. Using this configuration,
    you can also perform canary deployments. Here is an example where traffic for
    the `envoydummy` service is routed in a 20:80 ratio to version v1 and v2 of the
    `envoydummy` application:'
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 服务分流是Consul服务发现链中的第二个阶段，通过`ServiceSplitter`配置进行配置。`ServiceSplitter`允许你将请求分流到多个子集工作负载。通过此配置，你还可以进行金丝雀发布。以下是一个示例，展示了`envoydummy`服务的流量以20:80的比例路由到`envoydummy`应用的v1和v2版本：
- en: '[PRE73]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'In the `ServiceSplitter` configuration, we have configured 80% of the traffic
    to `envoydummy` to be routed to the `envoydummy2` service and the remaining 20%
    of the traffic to be routed to the `envoydummy` service. The configuration is
    available in `AppendixA/Consul/splitter.yaml`. Apply the configuration using the
    following command:'
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在`ServiceSplitter`配置中，我们将80%的流量路由到`envoydummy2`服务，剩下的20%流量路由到`envoydummy`服务。该配置文件位于`AppendixA/Consul/splitter.yaml`中。你可以使用以下命令应用该配置：
- en: '[PRE74]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'After applying the configuration, you can check out the routing config on the
    Consul dashboard. In the following screenshot, we can see that all traffic to
    `envoydummy` is routed to `envoydummy` and `envoydummy2`. The following screenshot
    doesn’t show the percentage, but you can hover the mouse over the arrows connecting
    the splitters and resolvers and you should be able to see the percentage:'
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应用配置后，你可以在Consul仪表盘上查看路由配置。在下面的截图中，我们可以看到所有流量都被路由到`envoydummy`和`envoydummy2`。截图中没有显示百分比，但你可以将鼠标悬停在连接分流器和解析器的箭头上，应该能看到百分比：
- en: '![Figure A.5 – Split of traffic to envoydummy2](img/Figure_13.05_B17989.jpg)'
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![图A.5 – envoydummy2服务的流量分配](img/Figure_13.05_B17989.jpg)'
- en: Figure A.5 – Split of traffic to envoydummy2
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图A.5 – envoydummy2服务的流量分配
- en: 'The following screenshot shows the split of traffic for `envoydummy`:'
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下截图显示了`envoydummy`的流量分配：
- en: '![Figure A.6 – Split of traffic to envoydummy service](img/Figure_13.06_B17989.jpg)'
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![图A.6 – envoydummy服务的流量分配](img/Figure_13.06_B17989.jpg)'
- en: Figure A.6 – Split of traffic to envoydummy service
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图A.6 – envoydummy服务的流量分配
- en: 'Now that the `ServiceSplitter` configuration is in place, test that traffic
    to our services is routed in the ratio specified in the config file:'
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在`ServiceSplitter`配置已经到位，测试我们服务的流量是否按照配置文件中指定的比例进行路由：
- en: '[PRE75]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: You will observe that the traffic is routed in a 20:80 ratio between the two
    services. `ServiceSplitter` is a powerful feature that can be used for A/B testing,
    as well as canary and blue/green deployments. Using `ServiceSplitter`, you can
    also perform weight-based routing between subsets of the same service. It also
    allows you to add HTTP headers while routing the service. You can read more about
    `ServiceSplitter` at [https://developer.hashicorp.com/consul/docs/connect/config-entries/service-splitter](https://developer.hashicorp.com/consul/docs/connect/config-entries/service-splitter).
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将观察到，流量在两个服务之间按20:80的比例进行路由。`ServiceSplitter`是一个强大的功能，可以用于A/B测试、金丝雀发布以及蓝绿部署。通过使用`ServiceSplitter`，你还可以在同一服务的不同子集之间进行基于权重的路由。它还允许你在路由服务时添加HTTP头。你可以在[https://developer.hashicorp.com/consul/docs/connect/config-entries/service-splitter](https://developer.hashicorp.com/consul/docs/connect/config-entries/service-splitter)了解更多关于`ServiceSplitter`的信息。
- en: We have looked at two of the three steps in Consul’s service discovery chain.
    The final stage is resolution, which we will cover in the next section.
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经了解了Consul服务发现链中的三个步骤中的两个，最后一个步骤是解析，我们将在下一节中讲解。
- en: Resolution
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解析
- en: Consul has another config type called `ServiceResolver`, which is used to define
    which instances of a service map to the service name requested by the client.
    They control the service discovery and decide where the request is finally routed
    to. Using `ServiceResolver`, you can control the resilience of your system by
    routing the request to healthy upstreams. `ServiceResolver` distributes load to
    services when they are spread across more than one data center and provides failover
    when the services are suffering from outages. More details about `ServiceResolver`
    can be found at [https://developer.hashicorp.com/consul/docs/connect/config-entries/service-resolver](https://developer.hashicorp.com/consul/docs/connect/config-entries/service-resolver).
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Consul还有另一种配置类型叫做`ServiceResolver`，用于定义哪些服务实例映射到客户端请求的服务名称。它们控制服务发现，并决定请求最终路由到哪里。通过使用`ServiceResolver`，你可以通过将请求路由到健康的上游来控制系统的弹性。`ServiceResolver`可以在服务跨多个数据中心时分发负载，并在服务出现故障时提供故障转移。关于`ServiceResolver`的更多细节可以在[https://developer.hashicorp.com/consul/docs/connect/config-entries/service-resolver](https://developer.hashicorp.com/consul/docs/connect/config-entries/service-resolver)找到。
- en: 'Consul Service Mesh also has provision for gateways to manage traffic from
    outside the mesh. It supports three kinds of gateway:'
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Consul服务网格还为网关提供了管理来自网格外部流量的功能。它支持三种类型的网关：
- en: '**Mesh gateways** are used to enable and secure communication between data
    centers. It acts as a proxy providing Ingress to Service Mesh while at the same
    time securing the traffic using mTLS. Mesh gateways are used to communicate between
    Consul Service Mesh instances deployed in different data centers and/or Kubernetes
    clusters. A good hands-on exercise on mesh gateways is available at [https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-mesh-gateways](https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-mesh-gateways).'
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网格网关**用于启用并保护数据中心之间的通信。它充当代理，提供对服务网格的入口，同时使用mTLS来保护流量。网格网关用于在不同数据中心和/或Kubernetes集群中部署的Consul服务网格实例之间进行通信。关于网格网关的一个很好的实践练习可以在[https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-mesh-gateways](https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-mesh-gateways)找到。'
- en: '**Ingress gateways** are used to provide access to services in the mesh to
    clients outside the mesh. The client can be outside the mesh but in the same Kubernetes
    cluster, or completely outside the cluster but within or beyond the network perimeter
    of the organization. You can read more about Ingress Gateway at [https://developer.hashicorp.com/consul/docs/k8s/connect/ingress-gateways](https://developer.hashicorp.com/consul/docs/k8s/connect/ingress-gateways).'
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**入口网关**用于为网格内的服务提供对外部客户端的访问。客户端可以位于同一个Kubernetes集群内，也可以完全位于集群外部，但在组织的网络边界内或边界外。你可以在[https://developer.hashicorp.com/consul/docs/k8s/connect/ingress-gateways](https://developer.hashicorp.com/consul/docs/k8s/connect/ingress-gateways)了解更多关于入口网关的内容。'
- en: '`ServiceDefault`. This is where the details about the external service are
    defined, and it is referred to by the terminating gateway. You can read more about
    terminating gateways at [https://developer.hashicorp.com/consul/docs/k8s/connect/terminating-gateways](https://developer.hashicorp.com/consul/docs/k8s/connect/terminating-gateways).'
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ServiceDefault`。这里定义了外部服务的详细信息，终结网关会引用这些信息。你可以在[https://developer.hashicorp.com/consul/docs/k8s/connect/terminating-gateways](https://developer.hashicorp.com/consul/docs/k8s/connect/terminating-gateways)了解更多关于终结网关的内容。'
- en: Finally, Consul Service Mesh also provides comprehensive observability of the
    mesh. The sidecar proxies collect and expose data about the traffic traversing
    the mesh. The metrics data exposed by the sidecar proxies are then scraped by
    Prometheus. The data includes Layer 7 metrics such as HTTP status code, request
    latency, and throughput. The Consul control plane also provides some metrics such
    as config synchronization status, exceptions, and errors, like the Istio control
    plane. The tech stack for observability is also like Istio; like Istio, Consul
    also supports integration with various other observability tools, such as Datadog,
    to get insight into Consul Service Mesh health and performance. You can read more
    about Consul Service Mesh observability at [https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-layer7-observability](https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-layer7-observability).
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，Consul 服务网格还提供了对网格的全面可观察性。Sidecar 代理收集并暴露通过网格流量的数据。Sidecar 代理暴露的度量数据随后由 Prometheus
    抓取。数据包括第七层度量，如 HTTP 状态码、请求延迟和吞吐量。Consul 控制平面还提供一些度量数据，如配置同步状态、异常和错误，类似于 Istio
    控制平面。可观察性技术栈也类似于 Istio；像 Istio 一样，Consul 也支持与各种其他可观察性工具的集成，如 Datadog，以便深入了解 Consul
    服务网格的健康状况和性能。你可以在 [https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-layer7-observability](https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-layer7-observability)
    上了解更多关于 Consul 服务网格可观察性的内容。
- en: 'I hope this section provided a brief but informative rundown of how Consul
    Service Mesh operates, what the various constructs in Consul Service Mesh are,
    and how they operate. I am sure you must have noticed the similarities between
    Consul Service Mesh and Istio: they both use Envoy as a sidecar proxy, and the
    Consul service discovery chain closely resembles Istio virtual services and destination
    rules; Consul Service Mesh gateways are very similar to Istio gateways. The main
    difference is how the control plane is implemented and the use of an agent on
    each node of the cluster. Consul Service Mesh can run on VMs to provide benefits
    of Service Mesh on legacy workloads. Consul Service Mesh is backed by HashiCorp
    and is tightly integrated with HashiCorp’s other products, including HashiCorp
    Vault. It is also offered as a freemium product. There is also an Enterprise version
    for organizations needing enterprise support and a SaaS offering called HCP Consul
    that provides a fully managed cloud service to customers who want one-click mesh
    deployments.'
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我希望这一部分简要但信息丰富地介绍了 Consul 服务网格是如何工作的，Consul 服务网格中的各种构件是什么，以及它们是如何操作的。我相信你一定注意到
    Consul 服务网格和 Istio 之间的相似之处：它们都使用 Envoy 作为 sidecar 代理，Consul 服务发现链与 Istio 的虚拟服务和目标规则非常相似；Consul
    服务网格网关与 Istio 网关非常相似。主要区别在于控制平面的实现方式以及每个集群节点上使用代理的方式。Consul 服务网格可以在虚拟机上运行，为传统工作负载提供服务网格的优势。Consul
    服务网格由 HashiCorp 提供支持，并与 HashiCorp 的其他产品（包括 HashiCorp Vault）紧密集成。它也作为一种免费增值产品提供。对于需要企业支持的组织，还有一个企业版本，并且有一个名为
    HCP Consul 的 SaaS 提供服务，为希望一键式网格部署的客户提供完全托管的云服务。
- en: Uninstalling Consul Service Mesh
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 卸载 Consul 服务网格
- en: 'You can use consul-k8s to uninstall Consul Service Mesh using the following
    commands:'
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以使用 consul-k8s 卸载 Consul 服务网格，使用以下命令：
- en: '`% consul-k8s uninstall -auto-approve=true -wipe-data=true`'
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`% consul-k8s uninstall -auto-approve=true -wipe-data=true`'
- en: '`..`'
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`..`'
- en: '`Deleting data for installation:`'
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`删除安装数据：`'
- en: '`Name: consul`'
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`名称：consul`'
- en: '`Namespace consul`'
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`命名空间 consul`'
- en: '`✓` `Deleted PVC => data-consul-consul-server-0`'
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`✓` `已删除 PVC => data-consul-consul-server-0`'
- en: '`✓` `PVCs deleted.`'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`✓` `PVCs已删除。`'
- en: '`✓` `Deleted Secret => consul-bootstrap-acl-token`'
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`✓` `已删除密钥 => consul-bootstrap-acl-token`'
- en: '`✓` `Consul secrets deleted.`'
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`✓` `Consul 密钥已删除。`'
- en: '`✓` `Deleted Service Account => consul-tls-init`'
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`✓` `已删除服务帐户 => consul-tls-init`'
- en: '`✓` `Consul service accounts deleted.`'
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`✓` `Consul 服务帐户已删除。`'
- en: 'You can uninstall consul-k8s CLI using Brew on macOS:'
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以通过 macOS 上的 Brew 卸载 consul-k8s CLI：
- en: '`% brew` `uninstall consul-k8s`'
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`% brew` `uninstall consul-k8s`'
- en: Gloo Mesh
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Gloo Mesh
- en: Gloo Mesh is a Service Mesh offering from Solo.io. There is an open source version
    called Gloo Mesh and an enterprise offering called Gloo Mesh Enterprise. Both
    are based on Istio Service Mesh and claim to have a better control plane and added
    functionality on top of open source Istio. Solo.io provides a feature comparison
    on its website outlining the differences between Gloo Mesh Enterprise, Gloo Mesh
    Open Source, and Istio, which you can access at [https://www.solo.io/products/gloo-mesh/](https://www.solo.io/products/gloo-mesh/).
    Gloo Mesh is primarily focused on providing a Kubernetes-native management plane
    through which users can configure and operate multiple heterogeneous Service Mesh
    instances across multiple clusters. It comes with an API that abstracts the complexity
    of managing and operating multiple meshes without the user needing to know the
    complexity under the hood caused by multiple Service Meshes. You can find details
    about Gloo Mesh at [https://docs.solo.io/gloo-mesh-open-source/latest/getting_started/](https://docs.solo.io/gloo-mesh-open-source/latest/getting_started/).
    This is a comprehensive resource on how to install and try Gloo Mesh. Solo.io
    has another product called Gloo Edge, which acts as a Kubernetes Ingress controller
    as well as an API gateway. Gloo Mesh Enterprise is deployed along with Gloo Edge,
    which provides many comprehensive API management and Ingress capabilities. Gateway
    Gloo Mesh Enterprise adds support for external authentication using OIDC, OAuth,
    API key, LDAP, and OPA. These policies are implemented via a custom CRD called
    ExtAuthPolicy, which can apply these authentications when routes and destinations
    match certain criteria.
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Gloo Mesh 是 Solo.io 提供的服务网格产品。它有一个开源版本，叫做 Gloo Mesh，还有一个企业版，叫做 Gloo Mesh Enterprise。两者都基于
    Istio 服务网格，并声称在开源 Istio 的基础上提供了更好的控制平面和附加功能。Solo.io 在其官网提供了一个功能对比，概述了 Gloo Mesh
    Enterprise、Gloo Mesh Open Source 和 Istio 之间的差异，您可以在 [https://www.solo.io/products/gloo-mesh/](https://www.solo.io/products/gloo-mesh/)
    访问该对比内容。Gloo Mesh 主要专注于提供一个 Kubernetes 原生管理平面，用户可以通过它配置和操作多个集群中的多个异构服务网格实例。它提供一个
    API，抽象了管理和操作多个网格的复杂性，用户无需了解多个服务网格下的复杂实现。您可以在 [https://docs.solo.io/gloo-mesh-open-source/latest/getting_started/](https://docs.solo.io/gloo-mesh-open-source/latest/getting_started/)
    获取有关 Gloo Mesh 的详细信息，这是一个关于如何安装和试用 Gloo Mesh 的全面资源。Solo.io 还提供另一款产品，叫做 Gloo Edge，它既是一个
    Kubernetes Ingress 控制器，又是一个 API 网关。Gloo Mesh Enterprise 与 Gloo Edge 一起部署，后者提供了许多全面的
    API 管理和 Ingress 功能。Gateway Gloo Mesh Enterprise 增加了使用 OIDC、OAuth、API 密钥、LDAP 和
    OPA 的外部身份验证支持。这些策略通过一个自定义 CRD，称为 ExtAuthPolicy 来实现，当路由和目标匹配某些条件时，可以应用这些身份验证。
- en: Gloo Mesh Enterprise provides WAF policies to monitor, filter, and block any
    harmful HTTP traffic. It also provides support for data loss prevention by doing
    a series of regex replacements on the response body and content that is logged
    by Envoy. This is a very important feature from a security point of view and stops
    sensitive data from being logged into the log files. DLP filters can be configured
    on listeners, virtual services, and routes. Gloo Mesh also provides support for
    connecting to legacy applications via the SOAP message format. There are options
    for building data transformation policies to apply XSLT transformation to modernize
    SOAP/XML endpoints. The data transformation policies can be applied to transform
    request or response payloads. It also supports special transformations such as
    via Inja templates. With Inja, you can write loops, conditional logic, and other
    functions to transform requests and responses.
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Gloo Mesh Enterprise 提供 WAF 策略，用于监控、过滤和阻止任何有害的 HTTP 流量。它还通过对 Envoy 记录的响应体和内容执行一系列正则替换，提供数据丢失防护（DLP）支持。这是一个从安全角度来看非常重要的功能，可以防止敏感数据被记录到日志文件中。DLP
    过滤器可以在监听器、虚拟服务和路由上进行配置。Gloo Mesh 还提供支持，通过 SOAP 消息格式连接到传统应用程序。提供了构建数据转换策略的选项，应用
    XSLT 转换来现代化 SOAP/XML 端点。这些数据转换策略可以用于转换请求或响应负载。它还支持通过 Inja 模板进行特殊转换。使用 Inja，你可以编写循环、条件逻辑以及其他函数来转换请求和响应。
- en: There is also extensive support for WASM filters. Solo.io provides custom tooling
    that speeds up the development and deployment of web assemblies. To store WASM
    files, solo.io provides WebAssembly Hub, available at [https://webassemblyhub.io/](https://webassemblyhub.io/),
    and an open source CLI tool called wasme. You can read more about how to use Web
    Assembly Hub and the wasme CLI at [https://docs.solo.io/web-assembly-hub/latest/tutorial_code/getting_started/](https://docs.solo.io/web-assembly-hub/latest/tutorial_code/getting_started/).
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 还广泛支持WASM过滤器。Solo.io提供了定制的工具，加速WebAssembly的开发和部署。为了存储WASM文件，Solo.io提供了WebAssembly
    Hub，网址是[https://webassemblyhub.io/](https://webassemblyhub.io/)，还有一个名为wasme的开源CLI工具。你可以在[https://docs.solo.io/web-assembly-hub/latest/tutorial_code/getting_started/](https://docs.solo.io/web-assembly-hub/latest/tutorial_code/getting_started/)了解更多关于如何使用WebAssembly
    Hub和wasme CLI的信息。
- en: As Gloo Mesh and other products from Solo.io are closely integrated with the
    Enterprise Service Mesh offering, you get a plethora of other features, and one
    such feature is a global API portal. The API portal is a self-discovery portal
    for publishing, sharing, and monitoring API usage for internal and external monetization.
    When using a multi-heterogeneous mesh, users don’t need to worry about managing
    observability tools for every mesh; instead, Gloo Mesh Enterprise provides aggregated
    metrics across every mesh, providing a seamless experience of managing and observing
    multiple meshes.
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于Gloo Mesh和Solo.io的其他产品与企业服务网格（Enterprise Service Mesh）紧密集成，您可以获得许多其他功能，其中之一就是全球API门户。API门户是一个自我发现门户，用于发布、共享和监控API的使用情况，支持内部和外部的盈利化。在使用多异构网格时，用户无需担心为每个网格管理可观察性工具；相反，Gloo
    Mesh Enterprise通过每个网格提供汇总的度量数据，提供了管理和观察多个网格的无缝体验。
- en: In enterprise environments, it is important that multiple teams and users can
    access and deploy services in the mesh without stepping on each others’ toes.
    Users need to know what services are available to consume and what services they
    have published. Users should be able to confidently perform mesh operations without
    impacting the services of other teams. Gloo Mesh uses the concept of workspaces,
    which are logical boundaries for a team, limiting team Service Mesh operations
    within the confines of the workspace so that multiple teams can concurrently use
    the mesh. Workspaces provide security isolation between configurations published
    by every team. Through workspaces, Gloo Mesh addresses the complexity of muti-tenancy
    in Enterprise environments, making it simpler for multiple teams to adopt Service
    Mesh with config isolation from each other and strict access control for safe
    muti-tenant usage of the mesh.
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在企业环境中，多个团队和用户能够访问和部署网格中的服务，而不会互相干扰是非常重要的。用户需要知道可以消费哪些服务以及他们已经发布了哪些服务。用户应该能够自信地执行网格操作，而不影响其他团队的服务。Gloo
    Mesh使用工作空间的概念，工作空间是为团队设定的逻辑边界，限制团队在工作空间内执行服务网格操作，这样多个团队可以同时使用网格。工作空间提供了每个团队发布的配置之间的安全隔离。通过工作空间，Gloo
    Mesh解决了企业环境中多租户的复杂性，使多个团队可以在彼此配置隔离的情况下，安全地使用网格，并实现严格的访问控制。
- en: Gloo Mesh is also integrated with another Service Mesh based on a different
    architecture than Istio. The mesh is called Istio Ambient Mesh, which, rather
    than adding a sidecar proxy per workload, adds a proxy at the per-node level.
    Istio Ambient Mesh is integrated with Gloo Mesh, and users can run their sidecar
    proxy-based mesh along with their per-node proxy Istio Ambient Mesh.
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Gloo Mesh还与基于不同架构的另一个服务网格（与Istio不同）进行了集成。这个网格叫做Istio Ambient Mesh，它不是为每个工作负载添加侧车代理，而是在每个节点级别添加代理。Istio
    Ambient Mesh与Gloo Mesh集成，用户可以同时运行基于侧车代理的网格和每个节点代理的Istio Ambient Mesh。
- en: Gloo Enterprise Mesh, with integration with Solo.io products such as Gloo Edge,
    makes it a strong contender among Service Mesh offerings. The ability to support
    multi-cluster and multi-mesh deployments, multi-tenancy via workspaces, strong
    support for authentication, zero-trust networking, and mature Ingress management
    via Gloo Edge makes it a comprehensive Service Mesh offering.
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Gloo Enterprise Mesh与Solo.io产品（如Gloo Edge）的集成，使其成为服务网格解决方案中的强有力竞争者。它支持多集群和多网格部署，通过工作空间实现多租户，强大的身份验证支持，零信任网络，以及通过Gloo
    Edge成熟的Ingress管理，使其成为一个全面的服务网格解决方案。
- en: Kuma
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Kuma
- en: Kuma is an open source CNCF sandbox project donated to CNCF by Kong Inc. Like
    Istio, Kuma also uses Envoy as the data plane. It supports multi-cluster and multi-mesh
    deployments, providing one global control plane to manage them all. At the time
    of writing this book, Kuma is one single executable written in GoLang. It can
    be deployed on Kubernetes as well as on VMs. When deployed in non-Kubernetes environments,
    it requires a PostgreSQL database to store its configurations.
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Kuma是一个开源的CNCF沙箱项目，由Kong Inc.捐赠给CNCF。像Istio一样，Kuma也使用Envoy作为数据平面。它支持多集群和多网格部署，提供一个全球控制平面来管理所有部署。在撰写本书时，Kuma是一个用GoLang编写的单一可执行文件。它可以在Kubernetes上部署，也可以在虚拟机上部署。当在非Kubernetes环境中部署时，它需要一个PostgreSQL数据库来存储其配置。
- en: 'Let’s start by downloading and installing Kuma, followed by hands-on exercises
    on this topic:'
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们先下载并安装Kuma，然后进行相关的动手操作：
- en: 'Download Kuma for your operating system:'
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载适用于你操作系统的Kuma：
- en: '[PRE76]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Install Kuma on minikube. Unzip the download file and, in the unzipped folder’s
    `bin` directory, run the following commands to install Kuma on Kubernetes:'
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在minikube上安装Kuma。解压下载的文件，在解压文件夹的`bin`目录中运行以下命令，将Kuma安装到Kubernetes中：
- en: '[PRE77]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: This will create a namespace called `kuma-system` and install the Kuma control
    plane in that namespace, along with configuring various CRDs and admission controllers.
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将创建一个名为`kuma-system`的命名空间，并在该命名空间中安装Kuma控制平面，同时配置各种CRD和准入控制器。
- en: 'At this point, we can access Kuma’s GUI using the following command:'
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，我们可以使用以下命令访问Kuma的图形用户界面（GUI）：
- en: '[PRE78]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Open `localhost:5681/gui` in your browser and you will see the following dashboard:'
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在浏览器中打开`localhost:5681/gui`，你将看到以下仪表板：
- en: '![Figure A.7 – Kuma dashboard](img/Figure_13.07_B17989.jpg)'
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![图A.7 – Kuma仪表板](img/Figure_13.07_B17989.jpg)'
- en: Figure A.7 – Kuma dashboard
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图A.7 – Kuma仪表板
- en: The Kuma GUI provides comprehensive details about the mesh. We will use this
    to check the configuration as we build policies and add applications to the mesh.
    On the home page of the GUI, you will notice that it shows one mesh called **default**.
    A mesh in Kuma is a Service Mesh that is logically isolated from other Service
    Meshes in Kuma. You can have one Kuma installation in a Kubernetes cluster, which
    can then manage multiple Service Meshes perhaps for each team or department deploying
    their apps in that Kubernetes cluster. This is a very important concept and a
    key differentiator of Kuma from other Service Mesh technologies.
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Kuma GUI提供了关于网格的全面详细信息。我们将使用它来检查配置，当我们构建策略并将应用程序添加到网格中时。在GUI的主页上，你会注意到显示了一个名为**default**的网格。Kuma中的网格是一个与其他Kuma网格逻辑隔离的服务网格。你可以在一个Kubernetes集群中安装Kuma，然后管理多个服务网格，也许为每个团队或部门部署其应用程序。这个概念非常重要，并且是Kuma与其他服务网格技术的一个关键区分点。
- en: Deploying envoydemo and curl in Kuma mesh
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在Kuma网格中部署envoydemo和curl
- en: 'The deployment file is available at `AppendixA/Kuma/envoy-proxy-01.yaml`. The
    noticeable difference compared to Istio in the deployment file is the addition
    of the following label, which instructs Kuma to inject its sidecar proxies into
    `envoydummy`:'
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部署文件可以在`AppendixA/Kuma/envoy-proxy-01.yaml`找到。与Istio的部署文件相比，明显的区别是添加了以下标签，它指示Kuma将其sidecar代理注入到`envoydummy`中：
- en: '[PRE79]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The following commands will deploy the `envoydummy` and `curl` applications:'
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下命令将部署`envoydummy`和`curl`应用程序：
- en: '[PRE80]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'After a few seconds, check whether the Pods have been deployed and the sidecars
    injected using the following commands:'
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 几秒钟后，使用以下命令检查Pods是否已部署并且sidecar已注入：
- en: '[PRE81]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: The sidecars are also called **data plane proxies** (**DPPs**), and they run
    along with every workload in the mesh. DPP comprises a data plane entity that
    defines the configuration of the DPP and a kuma-dp binary. During startup, kuma-dp
    retrieves the startup configuration for Envoy from the Kuma control plane (kuma-cp)
    and uses that to spawn the Envoy process. Once Envoy starts, it connects to kuma-cp
    using XDS. kuma-dp also spawns a core-dns process at startup.
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些sidecar也叫做**数据平面代理**（**DPPs**），它们与网格中的每个工作负载一起运行。DPP由定义DPP配置的数据平面实体和一个kuma-dp二进制文件组成。在启动过程中，kuma-dp从Kuma控制平面（kuma-cp）检索Envoy的启动配置，并使用该配置启动Envoy进程。一旦Envoy启动，它将通过XDS连接到kuma-cp。kuma-dp在启动时还会启动一个core-dns进程。
- en: It is worth noticing that installing Kuma and deploying an application has been
    a breeze. It is very simple, and the GUI is very intuitive, even for beginners.
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 值得注意的是，安装Kuma并部署应用程序非常简单。它非常直观，甚至对于初学者来说也很容易上手。
- en: Using the GUI, let’s check the overall status of the mesh.
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用GUI，让我们检查网格的整体状态。
- en: 'From **MESH** | **Overview**, you can see newly added DPPs:'
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从**MESH** | **概览**中，你可以看到新添加的DPP：
- en: '![Figure A.8 – Mesh overview in the Kuma GUI](img/Figure_13.08_B17989.jpg)'
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 A.8 – Kuma GUI中的网格概览](img/Figure_13.08_B17989.jpg)'
- en: Figure A.8 – Mesh overview in the Kuma GUI
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 A.8 – Kuma GUI中的网格概览
- en: 'From **MESH** | **Data Plane Proxies**, you can find details about the workloads:'
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从**MESH** | **数据平面代理**，你可以找到关于工作负载的详细信息：
- en: '![Figure A.9 – Data plane proxies](img/Figure_13.09_B17989.jpg)'
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 A.9 – 数据平面代理](img/Figure_13.09_B17989.jpg)'
- en: Figure A.9 – Data plane proxies
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 A.9 – 数据平面代理
- en: Now that we have installed apps, we will perform some hands-on exercises with
    Kuma policies to get experience with Kuma.
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经安装了应用程序，我们将进行一些关于Kuma策略的动手操作，来体验Kuma的使用。
- en: 'We will start by accessing the `envoydummy` service from the `curl` Pod:'
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将首先从`curl` Pod访问`envoydummy`服务：
- en: '[PRE82]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The output is as expected. By default, Kuma allows traffic in and out of the
    mesh. By default, all traffic is unencrypted in the mesh. We will enable mTLS
    and deny all traffic in the mesh to establish zero-trust networking. First, we
    will delete the policy that allows all traffic within the mesh using the following
    command:'
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果符合预期。默认情况下，Kuma允许网格内外的流量。默认情况下，所有流量在网格中都是未加密的。我们将启用mTLS并拒绝网格内的所有流量，以建立零信任网络。首先，我们将删除允许网格内所有流量的策略，使用以下命令：
- en: '[PRE83]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '`allow-all-traffic` is a traffic permission policy that allows all traffic
    within the mesh.  The previous command deletes the policy, thereby restricting
    all traffic in the mesh.'
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`allow-all-traffic`是一个允许网格内所有流量的流量权限策略。前面的命令删除了该策略，从而限制了网格内的所有流量。'
- en: Next, we will enable mTLS within the mesh to enable secure communication and
    let `kong-dp` correctly identify a service by comparing the service identity with
    the DPP certificate. Without enabling mTLS, Kuma cannot enforce traffic permissions.
    The following policy enables mTLS in the default mesh. It makes use of an inbuilt
    CA, but in case you want to use an external CA then there are also provisions
    to provide externally generated root CA and key. Kuma automatically generates
    certificates for every workload with SAN in SPIFEE format.
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将在网格内启用mTLS以启用安全通信，并让`kong-dp`通过比较服务身份与DPP证书来正确识别服务。如果没有启用mTLS，Kuma无法强制执行流量权限。以下策略启用默认网格中的mTLS。它使用内建的CA，但如果你希望使用外部CA，也可以提供外部生成的根CA和密钥。Kuma会为每个工作负载自动生成证书，并以SPIFEE格式的SAN进行配置。
- en: '[PRE84]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'In the config file, we have defined that this policy applies to the `default`
    mesh. We have declared a CA named `ca-1` of the `builtin` type and have configured
    it to be used as the root CA for mTLS by defining `enabledBackend`. The configuration
    file is available at `AppendixA/Kuma/enablemutualTLS.yaml`. You can apply the
    configuration using the following commands:'
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在配置文件中，我们定义了该策略适用于`default`网格。我们声明了一个名为`ca-1`的CA，它是`builtin`类型，并将其配置为作为mTLS的根CA，定义`enabledBackend`。配置文件位于`AppendixA/Kuma/enablemutualTLS.yaml`。你可以使用以下命令应用配置：
- en: '[PRE85]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'After enabling mTLS, let’s try to access `envoydummy` from `curl`:'
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 启用mTLS后，我们来尝试从`curl`访问`envoydummy`：
- en: '[PRE86]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: The output is as expected because mTLS is enabled and there is no `TrafficPermission`
    policy allowing the traffic between `curl` and `envoydummy`.
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果符合预期，因为启用了mTLS，且没有`TrafficPermission`策略允许`curl`与`envoydummy`之间的流量。
- en: 'To allow traffic, we need to create the following `TrafficPermission` policy:'
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了允许流量，我们需要创建以下`TrafficPermission`策略：
- en: '[PRE87]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Note that the `kuma.io/service` fields contain the values of the corresponding
    tags. Tags are sets of key-value pairs that contain details of the service that
    the DPP is part of and metadata about the exposed service. The following are tags
    applied to DPP for `envoydummy`:'
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，`kuma.io/service`字段包含对应标签的值。标签是一组键值对，包含DPP所属服务的详细信息和该服务的元数据。以下是应用于`envoydummy`的DPP标签：
- en: '[PRE88]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Similarly, you can fetch the value of the `curl` DPP. The configuration file
    is available at `AppendixA/Kuma/allow-traffic-curl-to-envoyv1.yaml`. Apply the
    configuration using the following command:'
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类似地，你可以获取`curl` DPP的值。配置文件位于`AppendixA/Kuma/allow-traffic-curl-to-envoyv1.yaml`。使用以下命令应用配置：
- en: '[PRE89]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'After applying the configuration, test that you can access `envoydummy` from
    `curl`:'
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应用配置后，测试你是否可以从`curl`访问`envoydummy`：
- en: '[PRE90]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: We have just experienced how you can control traffic between workloads in the
    mesh. You will find this very similar to `ServiceIntentions` in Consul Service
    Mesh.
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们刚刚体验了如何在网格中控制工作负载之间的流量。你会发现这与Consul服务网格中的`ServiceIntentions`非常相似。
- en: Traffic management and routing
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 流量管理与路由
- en: Now we will explore traffic routing in Kuma. We will deploy version v2 of the
    `envoydummy` service and route certain requests between version v1 and v2.
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们将探讨Kuma中的流量路由。我们将部署`envoydummy`服务的v2版本，并在版本v1和v2之间路由特定的请求。
- en: 'The first step is to deploy version v2 of `envoydummy`, followed by defining
    traffic permission to allow traffic between the `curl` Pod and the `envoydummy`
    v2 Pod. The files are at `AppendixA/Kuma/envoy-proxy-02.yaml` and `AppendixA/Kuma/allow-traffic-curl-to-envoyv2.yaml`.
    Apply the configuration, and once you have applied both files, test that `curl`
    is able to reach both v1 and v2 of the `envoydummy` Pod:'
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一步是部署`envoydummy`的v2版本，然后定义流量权限以允许`curl` Pod与`envoydummy` v2 Pod之间的流量。文件位于`AppendixA/Kuma/envoy-proxy-02.yaml`和`AppendixA/Kuma/allow-traffic-curl-to-envoyv2.yaml`。应用配置后，测试`curl`是否能够访问`envoydummy`的v1和v2
    Pod：
- en: '[PRE91]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Next, we will configure the routing by using a Kuma policy called `TrafficRoute`.
    This policy allows us to configure rules for traffic routing in the mesh.
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将通过使用名为`TrafficRoute`的Kuma策略来配置路由。此策略允许我们为网格中的流量路由配置规则。
- en: 'The policy can be split into four parts to make it easier to understand:'
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该策略可以分为四个部分，便于理解：
- en: 'In the first part, we are declaring the `TrafficRoute` policy. The basic usage
    of the policy is documented at [https://kuma.io/docs/2.0.x/policies/traffic-route/](https://kuma.io/docs/2.0.x/policies/traffic-route/).
    Here, we are declaring that the policy applies to the default mesh and  to any
    request in the mesh originating from `curl_appendix-kuma_svc` with a destination
    of `envoydummy_appendix-kuma_svc_80`:'
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一部分，我们声明了`TrafficRoute`策略。该策略的基本用法已在[https://kuma.io/docs/2.0.x/policies/traffic-route/](https://kuma.io/docs/2.0.x/policies/traffic-route/)文档中记录。在此，我们声明该策略适用于默认网格，以及来自`curl_appendix-kuma_svc`并以`envoydummy_appendix-kuma_svc_80`为目标的任何请求：
- en: '[PRE92]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Next, we are configuring any request with a prefix of `''/latest''` to be routed
    to the DPP with the tags that are highlighted under `destination`:'
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将配置任何带有`'/latest'`前缀的请求，按照`destination`下标出的标签路由到DPP：
- en: '[PRE93]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Then, we are configuring request with a prefix of `''/old''` to be routed to
    the data plane with the tags that are highlighted under `destination`:'
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将配置带有`'/old'`前缀的请求，按照`destination`下标出的标签路由到数据平面：
- en: '[PRE94]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Finally, we are declaring the default destination for requests that do not
    match any of the paths defined in previous parts of the config. The default destination
    will be the DPP with the tags highlighted in the following code:'
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们声明了未匹配前面配置部分中任何路径的请求的默认目标。默认目标将是带有以下代码中高亮标签的DPP：
- en: '[PRE95]'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'The configuration file is available at `AppendixA/Kuma/trafficRouting01.yaml`.
    Apply the configuration and test the following scenarios:'
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 配置文件位于`AppendixA/Kuma/trafficRouting01.yaml`。应用配置后，测试以下场景：
- en: 'All requests with `''/latest''` should be routed to version v2:'
  id: totrans-410
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有带有`'/latest'`前缀的请求应该路由到版本v2：
- en: '[PRE96]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'All request with `''/old''` should be routed to version v1:'
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有带有`'/old'`前缀的请求应路由到版本v1：
- en: '[PRE97]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'All other requests should follow the default behavior:'
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有其他请求应遵循默认行为：
- en: '[PRE98]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The request routing works as expected, and it is similar to how you would configure
    the same behavior using Istio. Now, let’s look at the load balancing properties
    of Kuma Mesh. We will build another traffic routing policy to do weighted routing
    between version v1 and v2 of `envoydummy`. Here is a snippet of the configuration
    available at `AppendixA/Kuma/trafficRouting02.yaml`:'
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请求路由按预期工作，类似于您使用Istio配置相同行为的方式。现在，让我们看看Kuma Mesh的负载均衡特性。我们将构建另一个流量路由策略，在`envoydummy`的v1和v2版本之间进行加权路由。以下是`AppendixA/Kuma/trafficRouting02.yaml`配置的一个片段：
- en: '[PRE99]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'After applying the configuration, you can test the traffic distribution using
    the following command:'
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应用配置后，您可以使用以下命令测试流量分配：
- en: '[PRE100]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'The traffic should be distributed between the two versions in approximately
    a 1:9 ratio. You can perform traffic routing, traffic modification, traffic splitting,
    load balancing, canary deployments, and locality-aware load balancing using a
    `TrafficRoute` policy. To read more about `TrafficRoute`, please use the comprehensive
    documentation available here: https://kuma.io/docs/2.0.x/policies/traffic-route.'
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 流量应以大约1:9的比例在两个版本之间分配。您可以使用`TrafficRoute`策略进行流量路由、流量修改、流量拆分、负载均衡、金丝雀发布和感知区域的负载均衡。要了解更多关于`TrafficRoute`的信息，请查阅此处的完整文档：https://kuma.io/docs/2.0.x/policies/traffic-route。
- en: 'Kuma also provides policies for circuit breaking, fault injection, timeout,
    rate limiting, and many more things. A comprehensive list of Kuma policies is
    available here: [https://kuma.io/docs/2.0.x/policies/introduction/](https://kuma.io/docs/2.0.x/policies/introduction/).
    These out-of-the-box policies make Kuma very easy to use with a very shallow learning
    curve.'
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Kuma 还提供了用于断路器、故障注入、超时、速率限制等的策略。Kuma 策略的完整列表可以在这里查看：[https://kuma.io/docs/2.0.x/policies/introduction/](https://kuma.io/docs/2.0.x/policies/introduction/)。这些开箱即用的策略使得
    Kuma 非常易于使用，学习曲线也非常浅。
- en: 'In the hands-on example so far, we have been deploying all workloads in the
    default mesh. We discussed earlier that Kuma allows you to create different isolated
    meshes, allowing teams to have isolated mesh environments within the same Kuma
    cluster. You can create a new mesh using the following configuration:'
  id: totrans-422
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在到目前为止的实践示例中，我们一直在默认网格中部署所有工作负载。我们之前讨论过，Kuma 允许你创建不同的隔离网格，使得团队可以在同一个 Kuma 集群中拥有隔离的网格环境。你可以使用以下配置创建一个新网格：
- en: '[PRE101]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'The configuration is available in `AppendixA/Kuma/team-digital-mesh.yaml`.
    Apply the configuration using the following command:'
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 配置可以在 `AppendixA/Kuma/team-digital-mesh.yaml` 中找到。使用以下命令应用该配置：
- en: '[PRE102]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Once you have created the mesh, you can create all the resources within the
    mesh by adding the following annotations to the workload deployment configurations:'
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦你创建了网格，你可以通过在工作负载部署配置中添加以下注释来创建网格中的所有资源：
- en: '[PRE103]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'And add the following to the Kuma policies:'
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 并将以下内容添加到 Kuma 策略中：
- en: '[PRE104]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: The ability to create a mesh is a very useful feature for enterprise environments
    and a key differentiator of Kuma compared to Istio.
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建网格的能力是企业环境中非常有用的功能，也是 Kuma 与 Istio 区别的一个关键点。
- en: Kuma also provides built-in Ingress capabilities to handle north-south traffic
    as well as east-west traffic. The Ingress is managed as a Kuma resource called
    a gateway, which in turn is an instance of kuma-dp. You have the flexibility to
    deploy as many Kuma gateways as you want, but ideally, one gateway per mesh is
    recommended. Kuma also supports integration with non-Kuma gateways, also called
    delegated gateways. For now, we will talk about built-in Kuma gateways and, later,
    briefly discuss delegated gateways.
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Kuma 还提供了内置的 Ingress 能力来处理南北向流量以及东西向流量。Ingress 被管理为一个名为网关的 Kuma 资源，网关本身是 kuma-dp
    的一个实例。你可以灵活地部署任意数量的 Kuma 网关，但理想情况下，建议每个网格部署一个网关。Kuma 还支持与非 Kuma 网关集成，也称为委托网关。现在，我们将讨论内置的
    Kuma 网关，稍后会简要讨论委托网关。
- en: 'To create a built-in gateway, you first need to define `MeshGatewayInstance`
    along with a matching `MeshGateway`. `MeshGatewayInstance` provides the details
    of how a gateway instance should be instantiated. Here is an example configuration
    of `MeshGatewayInstance`, which is also available at `AppendixA/Kuma/envoydummyGatewayInstance01.yaml`:'
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要创建内置网关，首先需要定义 `MeshGatewayInstance` 和匹配的 `MeshGateway`。`MeshGatewayInstance`
    提供了网关实例如何实例化的详细信息。以下是 `MeshGatewayInstance` 的示例配置，该配置也可以在 `AppendixA/Kuma/envoydummyGatewayInstance01.yaml`
    中找到：
- en: '[PRE105]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'In the config, we are setting that there will be `1 replica` and a `serviceType`
    of `LoadBalancer`, and we have applied a tag, `kuma.io/service: envoydummy-edge-gateway`,
    which will be used to build the association with `MeshGateway`.'
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '在配置中，我们设置了将有 `1 个副本` 和 `LoadBalancer` 类型的服务，并且我们应用了一个标签 `kuma.io/service: envoydummy-edge-gateway`，该标签将用于与
    `MeshGateway` 构建关联。'
- en: 'In the following configuration, we are creating a `MeshGateway` named `envoydummy-edge-gateway`.
    The configuration is available in `AppendixA/Kuma/envoydummyGateway01.yaml`:'
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在以下配置中，我们正在创建一个名为 `envoydummy-edge-gateway` 的 `MeshGateway`。该配置可以在 `AppendixA/Kuma/envoydummyGateway01.yaml`
    中找到：
- en: '[PRE106]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: The `MeshGateway` resource specifies the listeners, which are endpoints that
    accept network traffic. In the configuration, you specify ports, protocols, and
    an optional hostname. Under `selectors`, we are also specifying the `MeshGatewayInstance`
    tags with which the `MeshGateway` configuration is associated. Notice that we
    are specifying the same tags we defined in the `MeshGatewayInstance` configuration.
  id: totrans-437
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`MeshGateway` 资源指定了监听器，它们是接收网络流量的端点。在配置中，你需要指定端口、协议和一个可选的主机名。在 `selectors`
    下，我们还指定了与 `MeshGateway` 配置相关联的 `MeshGatewayInstance` 标签。请注意，我们指定的是在 `MeshGatewayInstance`
    配置中定义的相同标签。'
- en: 'Next, we will define `MeshGatewayRoute`, which describes how a request is routed
    from `MeshGatewayInstance` to the workload service. An example configuration is
    available at `AppendixA/Kuma/envoydummyGatewayRoute01.yaml`. Here are some snippets
    from the file:'
  id: totrans-438
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将定义`MeshGatewayRoute`，它描述了请求如何从`MeshGatewayInstance`路由到工作负载服务。配置示例可在`AppendixA/Kuma/envoydummyGatewayRoute01.yaml`中找到。以下是文件中的一些片段：
- en: 'Under `selectors`, we are specifying the details of the gateway and the listener
    to which this route should be attached. The details are specified by providing
    the tags of the corresponding gateway and listeners:'
  id: totrans-439
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`selectors`下，我们指定了此路由应附加到的网关和监听器的详细信息。通过提供相应网关和监听器的标签来指定详细信息：
- en: '[PRE107]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'In the `conf` part, we provide Layer 7 matching criteria for the request, such
    as the path and HTTP headers, and the destination details:'
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`conf`部分，我们提供了请求的第7层匹配标准，例如路径和HTTP头信息，以及目标详细信息：
- en: '[PRE108]'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'And last but not least, we allow traffic between the edge gateway and the envoy
    dummy service by configuring `TrafficPermission` as described in following snippet.
    You can find the configuration at `AppendixA/Kuma/allow-traffic-edgegateway-to-envoy.yaml`:'
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们通过配置`TrafficPermission`，允许边缘网关与envoy假服务之间的流量，如以下代码片段所述。你可以在`AppendixA/Kuma/allow-traffic-edgegateway-to-envoy.yaml`中找到该配置：
- en: '[PRE109]'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'With traffic permission in place, we are now ready to apply the configuration
    using the following set of commands:'
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在配置了流量权限之后，我们现在准备通过以下一组命令应用配置：
- en: 'Create `MeshGatewayInstance`:'
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`MeshGatewayInstance`：
- en: '[PRE110]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Create `MeshGateway`:'
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`MeshGateway`：
- en: '[PRE111]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Create `MeshGatewayRoute`:'
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`MeshGatewayRoute`：
- en: '[PRE112]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Create `TrafficPermissions`:'
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`TrafficPermissions`：
- en: '[PRE113]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'You can verify that Kuma has created a gateway instance using the following
    commands:'
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以通过以下命令验证Kuma是否已创建网关实例：
- en: '[PRE114]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'You can also check the corresponding service using the following command:'
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以使用以下命令检查相应的服务：
- en: '[PRE115]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'We are now all set to access `envoydummy` using the built-in Kuma gateway.
    But first, we need to find an IP address through which we can access the Ingress
    gateway service on minikube. Use the following command to find the IP address:'
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经准备好使用内置的Kuma网关访问`envoydummy`。但首先，我们需要找到一个IP地址，通过该地址我们可以访问minikube上的Ingress网关服务。使用以下命令来查找IP地址：
- en: '[PRE116]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Now, using http://127.0.0.1:52346, you can access the `envoydummy` service
    by performing `curl` from your terminal:'
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，通过使用http://127.0.0.1:52346，你可以通过在终端执行`curl`访问`envoydummy`服务：
- en: '[PRE117]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: You have learned how to create a `MeshGatewayInstance`, which is then associated
    with `MeshGateway`. After the association, kuma-cp created a gateway instance
    of the built-in Kuma gateway. We then created a `MeshGatewayRoute` that specifies
    how the request will be routed from the gateway to the workload service. Later,
    we created a `TrafficPermission` resource to allow traffic flow from `MeshGateway`
    to the `EnvoyDummy` workload.
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你已经学习了如何创建一个`MeshGatewayInstance`，它随后与`MeshGateway`关联。关联后，kuma-cp创建了一个内置Kuma网关的网关实例。接着，我们创建了一个`MeshGatewayRoute`，指定了如何将请求从网关路由到工作负载服务。随后，我们创建了一个`TrafficPermission`资源，允许从`MeshGateway`到`EnvoyDummy`工作负载的流量。
- en: Kuma also provides options for using an external gateway as Ingress, also called
    a delegated gateway. In a delegated gateway, Kuma supports integrations with various
    API gateways, but Kong Gateway is the preferred and most well-documented option.
    You can read more about delegated gateways at [https://kuma.io/docs/2.0.x/explore/gateway/#delegated](https://kuma.io/docs/2.0.x/explore/gateway/#delegated).
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Kuma还提供了将外部网关用作Ingress的选项，也叫做委托网关。在委托网关中，Kuma支持与各种API网关的集成，但Kong Gateway是首选且文档最完善的选项。你可以在[https://kuma.io/docs/2.0.x/explore/gateway/#delegated](https://kuma.io/docs/2.0.x/explore/gateway/#delegated)阅读有关委托网关的更多内容。
- en: Like Istio, Kuma also provides native support for both Kubernetes and VM-based
    workloads. Kuma provides extensive support for running advanced configurations
    of Service Mesh spanning multiple Kubernetes clusters, data centers, and cloud
    providers. Kuma has a concept of zones, which are logical aggregations of DPPs
    that can communicate with each other. Kuma supports running Service Mesh in multiple
    zones and the separation of control planes in a multi-zone deployment. Each zone
    is allocated its own horizontally scalable control plane providing complete isolation
    between every zone. All zones are then also managed by a centralized global control
    plane, which manages the creation of and changes to policies that are applied
    to DPPs and the transmission of zone-specific policies and configurations to respective
    control planes of underlying zones. The global control plane is a single pane
    of glass providing an inventory of all DPPs across all zones.
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与 Istio 类似，Kuma 也原生支持 Kubernetes 和基于虚拟机的工作负载。Kuma 为在多个 Kubernetes 集群、数据中心和云提供商之间运行复杂配置的服务网格提供了广泛的支持。Kuma
    有一个区域的概念，区域是可以相互通信的 DPP（数据平面代理）的逻辑聚合。Kuma 支持在多个区域中运行服务网格，并支持多区域部署中控制平面的分离。每个区域都会分配一个水平可扩展的控制平面，从而实现区域之间的完全隔离。所有区域还由一个集中式的全球控制平面进行管理，后者负责管理应用于
    DPP 的策略的创建和更改，并将特定区域的策略和配置传输到各自区域的控制平面。全球控制平面提供了一个单一视窗，展示所有区域中的所有 DPP 的清单。
- en: As mentioned earlier, Kuma is an open source project that was donated by Kong
    to CNCF. Kong also provides Kong Mesh, which is an enterprise version of Kuma
    built on top of Kuma, extending it to include capabilities required for running
    critical functionality for enterprise workloads. Kong Mesh provides a turnkey
    Service Mesh solution with capabilities such as integration with OPA, FIPS 140-2
    compliance, and role-based access control. Coupled with Kong Gateway as an Ingress
    gateway, a Service Mesh based on Kuma, additional enterprise-grade add-ons and
    reliable enterprise support makes Kong Mesh a turnkey Service Mesh technology.
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前所述，Kuma 是一个开源项目，由 Kong 捐赠给 CNCF。Kong 还提供了 Kong Mesh，这是一个基于 Kuma 的企业版，扩展了 Kuma，包含了运行企业级工作负载所需的功能。Kong
    Mesh 提供了一个即插即用的服务网格解决方案，具有与 OPA 集成、FIPS 140-2 合规性和基于角色的访问控制等功能。结合 Kong Gateway
    作为入口网关，基于 Kuma 的服务网格、额外的企业级插件以及可靠的企业支持，使得 Kong Mesh 成为一项即插即用的服务网格技术。
- en: Uninstalling Kuma
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 卸载 Kuma
- en: 'You can uninstall Kuma Mesh using the following command:'
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以使用以下命令卸载 Kuma Mesh：
- en: '`% kumactl install control-plane | kubectl delete -``f -`'
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`% kumactl install control-plane | kubectl delete -``f -`'
- en: Linkerd
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Linkerd
- en: Linkerd is a CNCF graduated project licensed under Apache v2\. Buoyant ([https://buoyant.io/](https://buoyant.io/))
    is the major contributor to Linkerd. Out of all Service Mesh technologies, Linkerd
    is probably one of the earliest, if not the oldest. It was initially made public
    in 2017 by Buoyant. It had initial success, but then it was criticized for being
    very resource hungry. The proxy used in Linkerd was written using the Scala and
    Java networking ecosystem, which uses the **Java Virtual Machine** (**JVM**) at
    runtime, causing significant resource consumption. In 2018, Buoyant released a
    new version of Linkerd called Conduit. Conduit was later renamed Linkerd v2\.
    The Linkerd v2 data plane is made up of Linkerd2-proxy, which is written in Rust
    and has a small resource consumption footprint.  Linkerd2- proxy is purpose built
    for proxying as a sidecar in Kubernetes Pods. While Linkerd2-proxy is written
    in Rust, the Linkerd control plane is developed in Golang.
  id: totrans-470
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Linkerd 是一个 CNCF 毕业项目，采用 Apache v2 许可证。Buoyant ([https://buoyant.io/](https://buoyant.io/))
    是 Linkerd 的主要贡献者。在所有服务网格技术中，Linkerd 可能是最早的之一，甚至可能是最老的。它最初由 Buoyant 在 2017 年公开发布。虽然它一开始取得了一定的成功，但后来因为资源消耗过大而受到批评。Linkerd
    中使用的代理是用 Scala 和 Java 网络生态系统编写的，运行时使用 **Java 虚拟机**（**JVM**），导致显著的资源消耗。2018 年，Buoyant
    发布了 Linkerd 的新版本，名为 Conduit。Conduit 后来被重新命名为 Linkerd v2。Linkerd v2 数据平面由 Linkerd2-proxy
    组成，它是用 Rust 编写的，资源消耗较小。Linkerd2-proxy 专门为 Kubernetes Pods 中的 sidecar 代理而构建。虽然
    Linkerd2-proxy 是用 Rust 编写的，但 Linkerd 控制平面是用 Golang 开发的。
- en: 'Like other open source Service Mesh technologies discussed in this *Appendix*,
    we will discover Linkerd by playing around with it and observing how it is similar
    to or different to Istio. Let’s start by installing Linkerd on minikube:'
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与本文 *附录* 中讨论的其他开源服务网格技术一样，我们将通过实际操作 Linkerd，观察它与 Istio 的相似与不同之处。让我们从在 minikube
    上安装 Linkerd 开始：
- en: 'Install Linkerd on minikube using the following command:'
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在 minikube 上安装 Linkerd：
- en: '[PRE118]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Follow the suggestion to include linkerd2 in your path:'
  id: totrans-474
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照建议将 linkerd2 添加到你的路径中：
- en: '[PRE119]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'Linkerd provides an option to check and validate that the Kubernetes cluster
    meets all the prerequisites required to install Linkerd:'
  id: totrans-476
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Linkerd 提供了一项检查和验证 Kubernetes 集群是否满足安装 Linkerd 所需所有先决条件的选项：
- en: '[PRE120]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'If the output contains the following, then you are good to go with the installation:'
  id: totrans-478
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果输出包含以下内容，那么安装就成功了：
- en: '[PRE121]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: If not, then you need to resolve the issues by going through suggestions at
    [https://linkerd.io/2.12/tasks/troubleshooting/#pre-k8s-cluster-k8s%20for%20hints](https://linkerd.io/2.12/tasks/troubleshooting/#pre-k8s-cluster-k8s%20for%20hints).
  id: totrans-480
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果没有，您需要通过以下链接中的建议解决问题：[https://linkerd.io/2.12/tasks/troubleshooting/#pre-k8s-cluster-k8s%20for%20hints](https://linkerd.io/2.12/tasks/troubleshooting/#pre-k8s-cluster-k8s%20for%20hints)。
- en: 'Next, we will Install Linkerd in two steps:'
  id: totrans-481
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将分两步安装 Linkerd：
- en: 'First, we install the CRDs:'
  id: totrans-482
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们安装 CRDs：
- en: '[PRE122]'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'After installing the control plane, check that Linkerd is fully installed using
    the following commands:'
  id: totrans-485
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安装控制平面后，请使用以下命令检查 Linkerd 是否完全安装：
- en: '[PRE124]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'If Linkerd is successfully installed, then you should see the following message:'
  id: totrans-487
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 Linkerd 安装成功，你应该会看到以下消息：
- en: '[PRE125]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'That complete the setup of Linkerd! Let’s now analyze what has been installed:'
  id: totrans-489
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这完成了 Linkerd 的安装！现在让我们分析一下已安装的内容：
- en: '[PRE126]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: It’s worth noticing here that the control plane comprises many Pods and Services.
    The `linkerd-identity` service is a CA for generating signed certificates for
    Linkerd proxies. `linkerd-proxy-injector` is the Kubernetes admission controller
    responsible for modifying Kubernetes Pod specifications to add linkerd-proxy and
    proxy-init containers. The `destination` service is the brains of the Linkerd
    control plane and maintains service discovery and identity information about the
    services, along with policies for securing and managing the traffic in the mesh.
  id: totrans-491
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里需要注意的是，控制平面由多个 Pod 和服务组成。`linkerd-identity` 服务是用于生成 Linkerd 代理签名证书的 CA。`linkerd-proxy-injector`
    是负责修改 Kubernetes Pod 规格以添加 linkerd-proxy 和 proxy-init 容器的 Kubernetes 审核控制器。`destination`
    服务是 Linkerd 控制平面的“大脑”，它维护服务发现和关于服务的身份信息，以及管理和安全保障网格中流量的策略。
- en: Deploying envoydemo and curl in Linkerd
  id: totrans-492
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 Linkerd 中部署 envoydemo 和 curl
- en: 'Now let’s deploy envoydummy and curl apps and check how Linkerd performs Service
    Mesh functions. Follow these steps to install the application:'
  id: totrans-493
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在让我们部署 envoydummy 和 curl 应用，并检查 Linkerd 如何执行服务网格功能。按照以下步骤安装应用：
- en: 'Like most Service Mesh solutions, we need to annotate the deployment descriptors
    with the following annotations:'
  id: totrans-494
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和大多数服务网格解决方案一样，我们需要在部署描述符中添加以下注解：
- en: '[PRE127]'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: The configuration file for the `envoydummy` and `curl` apps along with annotations
    is available in `AppendixA/Linkerd/envoy-proxy-01.yaml`.
  id: totrans-496
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`envoydummy` 和 `curl` 应用的配置文件和注解位于 `AppendixA/Linkerd/envoy-proxy-01.yaml`。'
- en: 'After preparing the deployment descriptors, you can apply the configurations:'
  id: totrans-497
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备好部署描述符后，你可以应用配置：
- en: '[PRE128]'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'That should deploy the Pod. Once the Pod is deployed, you can check what has
    been injected into the Pods via the following commands:'
  id: totrans-499
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这应该会部署 Pod。一旦 Pod 部署完成，你可以通过以下命令检查 Pod 中注入了什么内容：
- en: '[PRE129]'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: From the preceding output, observe that Pod initialization was performed by
    a container named `linkerd-init` of the `cr.l5d.io/linkerd/proxy-init:v2.0.0`
    type, and the Pod has two running containers, `curl` and `linkerd-proxy`, of the
    `cr.l5d.io/linkerd/proxy:stable-2.12.3` type. The `linkerd-init` container runs
    during the initialization phase of the Pod and modifies iptables rules to route
    all network traffic from `curl` to `linkerd-proxy`. As you may recall, in Istio
    we have `istio-init` and `istio-proxy` containers, which are similar to Linkerd
    containers. `linkerd-proxy` is ultra-light and ultra-fast in comparison to Envoy.
    Being written in Rust makes its performance predictable and it doesn’t need garbage
    collection, which often causes high latency during garbage collection passes.
    Rust is arguably much more memory safe than C++ and C, which makes it less susceptible
    to memory safety bugs. You can read more about why `linkerd-proxy` is better than
    envoy at [https://linkerd.io/2020/12/03/why-linkerd-doesnt-use-envoy/](https://linkerd.io/2020/12/03/why-linkerd-doesnt-use-envoy/).
  id: totrans-501
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Verify that `curl` is able to communicate with the `envoydummy` Pod as follows:'
  id: totrans-502
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE130]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: Now that we have installed the `curl` and `envoydummy` Pods, let’s explore Linkerd
    Service Mesh functions. Let’s start by exploring how we can restrict traffic within
    the mesh using Linkerd.
  id: totrans-504
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Zero-trust networking
  id: totrans-505
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Linkerd provides comprehensive policies to restrict traffic in the mesh. Linkerd
    provides a set of CRDs through which policies can be defined to control the traffic
    in the mesh. Let’s explore these policies by implementing policies to control
    traffic to the `envoydummy` Pod:'
  id: totrans-506
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will first lock down all traffic in the cluster using following:'
  id: totrans-507
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: We used the `linkerd upgrade` command to apply a `default-inbound-policy` of
    `deny`, which prohibits all traffic to ports exposed by workloads in the mesh
    unless there is a server resource attached to the port.
  id: totrans-509
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After applying the policy, all access to the `envoydummy` service is denied:'
  id: totrans-510
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE132]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'Next, we create a server resource to describe the `envoydummy` port. A Server
    resource is a means of instructing Linkerd that only authorized clients can access
    the resource. We do that by declaring the following Linkerd policy:'
  id: totrans-512
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'The configuration file is available at `AppendixA/Linkerd/envoydummy-server.yaml`.
    The server resource is defined in the same namespace as the workload. In the configuration
    file, we also define the following:'
  id: totrans-514
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`podSelector`: Criteria for selecting the workload'
  id: totrans-515
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`port`: Name or number of the port for which this server configuration is being
    declared'
  id: totrans-516
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxyProtocol`: Configures protocol discovery for inbound connections and
    must be one of the following: unknown, `HTTP/1`, `HTTP/2`, `gRPC`, `opaque`, or
    `TLS`'
  id: totrans-517
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apply the server resource using the following command:'
  id: totrans-518
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE134]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: Although we have applied the server resource, the `curl` Pod still can’t access
    the `envoydummy` service unless we authorize it.
  id: totrans-520
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this step, we will create an authorization policy that authorizes `curl`
    to access `envoydummy`. The authorization policy is configured by providing server
    details of the target destination and service account details being used to run
    the originating service. We created a server resource named `envoydummy` in the
    previous step and, as per `AppendixA/Linkerd/envoy-proxy-01.yaml`, we are using
    a service account named `curl` to run the `curl` Pod. The policy is defined as
    follows and is also available at `AppendixA/Linkerd/authorize-curl-access-to-envoydummy.yaml`:'
  id: totrans-521
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'Apply the configuration as follows:'
  id: totrans-523
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: Once the `AuthorizationPolicy` is in place, it will authorize all traffic to
    the Envoy server from any workload running using a `curl` service account.
  id: totrans-525
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can verify the access between the `curl` and `envoydummy` Pods using the
    following command:'
  id: totrans-526
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: Using `AuthorizationPolicy`, we have controlled access to ports presented as
    servers from other clients in the mesh. Granular access control, such as controlling
    access to an HTTP resource, can be managed by another policy called `s`.
  id: totrans-528
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can understand this concept better via an example, so let’s make a requirement
    that only requests whose URI start with `/dummy` can be accessible from `curl`;
    requests to any other URI must be denied. Let’s get started:'
  id: totrans-529
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We need to first define an `HTTPRoute` policy as described in the following
    code snippet:'
  id: totrans-530
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: The configuration is also available at `AppendixA/Linkerd/HTTPRoute.yaml`. This
    will create an HTTP route targeting the `envoydummy` server resource. In the `rules`
    section, we define the criteria for identifying requests that will be used to
    identify the HTTP request for this route. Here, we have defined to rule to match
    any request with the `dummy` prefix and the `GET` method. `HTTPRoute` also supports
    route matching using headers and query parameters. You can also apply other filters
    in `HTTPRoute` to specify how the request should be processed during the request
    or response cycle; you can modify inbound request headers, redirect requests,
    modify request paths, and so on.
  id: totrans-532
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once we have defined `HTTPRoute`, we can modify the `AuthorizationPolicy` to
    associate with `HTTPRoute` instead of the server, as listed in the following code
    snippet and also available at `AppendixA/Linkerd/HttpRouteAuthorization.yaml`:'
  id: totrans-533
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: The configuration updates `AuthorizationPolicy` and, instead of referencing
    the server (`envoydummy` configured in `AppendixA/Linkerd/authorize-curl-access-to-envoydummy.yaml`)
    as the target, the policy is now referencing `HTTPRoute` (named `envoydummy-dummy-route`).
  id: totrans-535
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply both configurations and test that you are able to make requests with the
    `/dummy` prefix in the URI. Any other request will be denied by Linkerd.
  id: totrans-536
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'So far in `AuthorizationPolicy` we have used `ServiceAccount` authentication.
    `AuthorizationPolicy` also supports `MeshTLSAuthentication` and `NetworkAuthentication`.
    Here is a brief overview of these authentication types:'
  id: totrans-537
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`MeshTLSAuthentication` is used to identify a client based on its mesh identity.
    For example, the `curl` Pod will be represented as `curl.appendix-linkerd.serviceaccount.identity.linkerd.local`.'
  id: totrans-538
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NetworkAuthentication` is used to identify a client based on its network location
    using **Classless Inter-Domain Routing** (**CIDR**) blocks.'
  id: totrans-539
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Linkerd also provides retries and timeouts to provide application resilience
    when systems are under stress or suffering partial failures. Apart from support
    for usual retry strategies, there is also a provision for specifying retry budgets
    so that retries do not end up amplifying resilience problems. Linkerd provides
    automated load balancing of requests to all destination endpoints using the **exponentially
    weighted moving average** (**EWMA**) algorithm. Linkerd supports weight-based
    traffic splitting, which is useful for performing canary and blue/green deployments.
    Traffic splitting in Linkerd uses the **Service Mesh Interface** (**SMI**) Traffic
    Split API, allowing users to incrementally shift traffic between blue and green
    services. You can read about the Traffic Split API at [https://github.com/servicemeshinterface/smi-spec/blob/main/apis/traffic-split/v1alpha4/traffic-split.md](https://github.com/servicemeshinterface/smi-spec/blob/main/apis/traffic-split/v1alpha4/traffic-split.md)
    and SMI at [https://smi-spec.io](https://smi-spec.io). Linkerd provides a well-defined
    and documented integration with Flagger to perform automatic traffic shifting
    when performing canary and blue/green deployments.
  id: totrans-540
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: There is a lot more to learn and digest about Linkerd. You can read about it
    at [https://linkerd.io/2.12](https://linkerd.io/2.12). Linkerd is ultra-performant
    because of its ultra-light service proxy build using Rust. It is carefully designed
    to solve application networking problems. The ultra-light proxy performs most
    Service Mesh functions but lacks in features such as circuit breaking and rate
    limiting. Let’s hope that the Linkerd creators bridge the gap with Envoy.
  id: totrans-541
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Hopefully, you are now familiar with the various alternatives to Istio and how
    they implement Service Mesh. Consul, Linkerd, Kuma, and Gloo Mesh have lots of
    similarities, and all of them are powerful, but Istio is the one that has the
    biggest community behind it and the support of various well-known organizations.
    Also, there are various organizations that provide enterprise support for Istio,
    which is a very important consideration when deploying Istio to production.
  id: totrans-542
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE140]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
