- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Summarizing What We Have Learned and the Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the book, you learned about and practiced various concepts of Service
    Mesh and how to apply them using Istio. It is strongly recommended that you practice
    the hands-on examples in each chapter. Don’t just limit yourself to the scenarios
    presented in this book but rather explore, tweak, and extend the examples and
    apply them to real-world problems you are facing in your organizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will revise the concepts discussed in this book by implementing
    Istio for an Online Boutique application. It will be a good idea to look at scenarios
    presented in this chapter and try to implement them yourself before looking at
    code examples. I hope reading this last chapter provides you with more confidence
    in using Istio. We will go through the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Enforcing best practices using OPA Gatekeeper
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the learnings of this book to a sample Online Boutique application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Istio roadmap, vision, and documentation, and how to engage with the community
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certification, learning resources, and various pathways to learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Extended Berkeley Packet Filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The technical requirements in this chapter are similar to *Chapter 4*. We will
    be using AWS EKS to deploy a website for an online boutique store, which is an
    open source application available under Apache License 2.0 at [https://github.com/GoogleCloudPlatform/microservices-demo](https://github.com/GoogleCloudPlatform/microservices-demo).
  prefs: []
  type: TYPE_NORMAL
- en: Please check *Chapter 4*’s *Technical requirements* section to set up the infrastructure
    in AWS using Terraform, set up kubectl, and install Istio including observability
    add-ons. To deploy the Online Boutique store application, please use the deployment
    artifacts in the `Chapter12/online-boutique-orig` file on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can deploy the Online Boutique store application using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The last command should deploy the Online Boutique application. After some
    time, you should be able to see all the Pods running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The name of the workloads also reflects their role in the Online Boutique application,
    but you can find more about this freely available open source application at [https://github.com/GoogleCloudPlatform/microservices-demo](https://github.com/GoogleCloudPlatform/microservices-demo).
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, you can access the application via the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then open it on the browser using `http://localhost:8080`. You should
    see something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Online Boutique application by Google](img/Figure_12.01_B17989.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – Online Boutique application by Google
  prefs: []
  type: TYPE_NORMAL
- en: This completes the technical setup required for code examples in this chapter.
    Let’s get into the main topics of the chapter. We will begin with setting up the
    OPA Gatekeeper to enforce Istio deployment best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Enforcing workload deployment best practices using OPA Gatekeeper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will deploy OPA Gatekeeper using our knowledge from *Chapter
    11*. We will then configure OPA policies to enforce that every deployment has
    `app` and `version` as labels, and all port names have protocol names as a prefix:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install OPA Gatekeeper. Deploy it by following the instructions in *Chapter
    11*, in the *Automating best practices using OPA* *Gatekeeper* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After deploying OPA Gatekeeper, you need to configure it to sync namespaces,
    Pods, services and Istio CRD gateways, virtual services, destination rules, and
    policy and service role bindings into its cache. We will make use of the configuration
    file we created in *Chapter 11*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Configure OPA Gatekeeper to apply the constraints. In *Chapter 11*, we configured
    constraints to enforce that Pods should have `app` and `version` numbers as labels
    (defined in `Chapter11/gatekeeper/01-istiopodlabelconstraint_template.yaml` and
    `Chapter11/gatekeeper/01-istiopodlabelconstraint.yaml`), and all port names should
    have a protocol name as a prefix (defined in `Chapter11/gatekeeper/02-istioportconstraints_template.yaml`
    and `Chapter11/gatekeeper/02-istioportconstraints.yaml`). Apply the constraints
    using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This completes the deployment and configuration of OPA Gatekeeper. You should
    extend the constraints with anything else you might like to be included to ensure
    good hygiene of deployment descriptors of the workloads.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will redeploy the Online Boutique application and enable
    istio sidecar injection and then discover the configurations that are in violation
    of OPA constraints and resolve them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Applying our learnings to a sample application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will apply the learnings of the book – specifically, the
    knowledge from *Chapters 4* to *6* – to our Online Boutique application. Let’s
    dive right in!
  prefs: []
  type: TYPE_NORMAL
- en: Enabling Service Mesh for the sample application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that OPA Gatekeeper is in place with all the constraints we want it to enforce
    on deployments, it’s time to deploy a sample application. We will first start
    with un-deploying the `online-boutique` application and redeploying with istio-injection
    enabled at the namespace level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Undeploy the Online Boutique application by deleting the `online-boutique`
    namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once undeployed, let’s modify the namespace and add an `istio-injection:enabled`
    label and redeploy the application. The updated namespace configuration will be
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The sample file is available at `Chapter12/OPAGatekeeper/automaticsidecarinjection/00-online-boutique-shop-ns.yaml`
    on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'With automatic sidecar injection enabled, let’s try to deploy the application
    using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: There will be errors caused by constraint violations imposed by OPA Gatekeeper.
    The output in the preceding example is truncated to avoid repetitions but from
    the output in your terminal, you must notice that all deployments are in violation
    and hence no resource is deployed to the online-boutique namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Try to fix the constraint violation by applying the correct labels and naming
    ports correctly as suggested by Istio best practices.
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to apply `app` and `version` labels to all deployments. The following
    is an example for a `frontend` deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, you need to add `name` to all port definitions in the service declaration.
    The following is an example of a `carts` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'For your convenience, the updated files are available in `Chapter12/OPAGatekeeper/automaticsidecarinjection`.
    Deploy the Online Boutique application using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: With that, we have practiced the deployment of the Online Boutique application
    in the Service Mesh. You should have the Online Boutique application along with
    automatic sidecar injection deployed in your cluster. The Online Boutique application
    is part of the Service Mesh but not yet completely ready for it. In the next section,
    we will apply the learning from *Chapter 5* on managing application traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Istio to manage application traffic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, using the learnings from *Chapter 4*, we will configure the
    Service Mesh to manage application traffic for the Online Boutique application.
    We will first start with configuring the Istio Ingress gateway to allow traffic
    inside the mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Istio Ingress Gateway
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Chapter 4*, we read that a gateway is like a load balancer on the edge of
    the mesh that accepts incoming traffic that is then routed to underlying workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following source code block, we have defined the gateway configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The file is also available in `Chapter12/trafficmanagement/01-gateway.yaml`
    on GitHub. Apply the configuration using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to configure `VirtualService` to route traffic for the `onlineboutique.com`
    host to the corresponding `frontend` service.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring VirtualService
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`VirtualService` is used to define route rules for every host as specified
    in the gateway configuration. `VirtualService` is associated with the gateway
    and the hostname is managed by that gateway. In `VirtualService`, you can define
    rules on how a traffic/route can be matched and, if matched, then where it should
    be routed to.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following source code block defines `VirtualService` that matches any traffic
    handled by `online-boutique-ingress-gateway` with a hostname of `onlineboutique.com`.
    If matched, the traffic is routed to subset `v1` of the destination service named
    `frontend`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The configuration is available in `Chapter12/trafficmanagement/02-virtualservice-frontend.yaml`
    on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will configure `DestinationRule`, which defines how the request will
    be handled by the destination.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring DestinationRule
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Though they might appear unnecessary, when you have more than one version of
    the workload, then `DestinationRule` is used for defining traffic policies such
    as a load balancing policy, connection pool policy, outlier detection policy,
    and so on. The following code block configures `DestinationRule` for the `frontend`
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The configuration is available along with the `VirtualService` configuration
    in `Chapter12/trafficmanagement/02-virtualservice-frontend.yaml` on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s create `VirtualService` and `DestinationRule` by using the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You should now be able to access the Online Boutique store site from the web
    browser. You need to find the public IP of the AWS load balancer exposing the
    Ingress gateway service – do not forget to add a **Host** header using the **ModHeader**
    extension to Chrome, as discussed in *Chapter 4* and as seen in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – ModHeader extension with Host header](img/Figure_12.02_B17989.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – ModHeader extension with Host header
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the correct Host header is added, you can access the Online Boutique from
    Chrome using the AWS load balancer public DNS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Online Boutique landing page](img/Figure_12.03_B17989.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – Online Boutique landing page
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have created only one virtual service to route traffic from the Ingress
    gateway to the `frontend` service in the mesh. By default, Istio will send traffic
    to all respective microservices in the mesh, but as we discussed In the previous
    chapter, the best practice is to define routes via `VirtualService` and how the
    request should be routed via destination rules. Following the best practice, we
    need to define `VirtualService` and `DestinationRule` for the remaining microservices.
    Having `VirtualService` and `DestinationRule` helps you manage traffic when there
    is more than one version of underlying workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'For your convenience, `VirtualService` and `DestinationRule` are already defined
    in the `Chapter12/trafficmanagement/03-virtualservicesanddr-otherservices.yaml`
    file on GitHub. You can apply the configuration using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying the configuration and generating some traffic, check out the
    Kiali dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Versioned app graph for the Online Boutique shop](img/Figure_12.04_B17989.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – Versioned app graph for the Online Boutique shop
  prefs: []
  type: TYPE_NORMAL
- en: In the Kiali dashboard, you can observe the Ingress gateway, all virtual services,
    and underlying workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring access to external services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we quickly revise concepts on routing traffic to destinations outside
    the mesh. In *Chapter 4*, we learned about `ServiceEntry`, which enables us to
    add additional entries to Istio’s internal service registry so that service in
    the mesh can route traffic to these endpoints that are not part of the Istio service
    registry. The following is an example of `ServiceRegistry` adding `xyz.com` to
    the Istio service registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This concludes the section on managing application traffic, in which we exposed
    `onlineboutique.com` via Istio Ingress Gateway and defined `VirtualService` and
    `DestinationRule` for routing and handling traffic in the mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Istio to manage application resiliency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Istio provides various capabilities to manage application resiliency, and we
    discussed them in great detail in *Chapter 5*. We will apply some of the concepts
    from that chapter to the Online Boutique application.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with timeouts and retries!
  prefs: []
  type: TYPE_NORMAL
- en: Configuring timeouts and retries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s assume that the email service suffers from intermittent failures, and
    it is prudent to timeout after 5 seconds if a response is not received from the
    email service, and then retry sending the email a few times rather than aborting
    it. We will configure retries and timeout for the email service to revise application
    resiliency concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Istio provides a provision to configure timeouts, which is the amount of time
    that an Istio-proxy sidecar should wait for replies from a given service. In the
    following configuration, we have applied a timeout of 5 seconds for the email
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Istio also provides provision for automated retries that are implemented as
    part of the `VirtualService` configuration. In the following source code block,
    we have configured Istio to retry the request to the email service twice, with
    each retry to timeout after 2 seconds and a retry to happen only if `5xx,gateway-error,reset,connect-failure,refused-stream,retriable-4xx`
    errors are returned from downstream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We have configured `timeout` and `retries` via the `VirtualService` configuration.
    With the assumption that the email service is fragile and suffers interim failure,
    let’s try to alleviate this issue by mitigating any potential issue caused by
    a traffic surge or spike.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring rate limiting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Istio provides controls to handle a surge of traffic from consumers, as well
    as to control the traffic to match consumers’ capability to handle the traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following destination rule, we are defining rate-limiting controls for
    the email service. We have defined that the number of active requests to the email
    service will be `1` (as per `http2MaxRequests`), there will be only 1 request
    per connection (as defined in `maxRequestsPerConnection`), and there will be 0
    requests queued while waiting for connection from the connection pool (as defined
    in `http1MaxPendingRequests`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s make some more assumptions and assume that there are two versions of
    the email service, with `v1` being more rogue than the other, `v2`. In such scenarios,
    we need to apply outlier detection policies to perform circuit breakers. Istio
    provides good control for outlier detection. The following code block describes
    the config you need to add to `trafficPolicy` in the corresponding destination
    rule for the email service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In the outlier detection, we have defined `baseEjectionTime` with a value of
    `5` minutes, which is the minimum duration per ejection. It is then also multiplied
    by the number of times an email service is found to be unhealthy. For example,
    if the `v1` email service is found to be an outlier 5 times, then it will be ejected
    from the connection pool for `baseEjectionTime*5`. Next, we have defined `consecutive5xxErrors`
    with a value of `1`, which is the number of `5x` errors that need to occur to
    qualify the upstream to be an outlier. Then, we have defined `interval` with a
    value of `90s`, which is the time between the checks when Istio scans the upstream
    for the health status. Finally, we have defined `maxEjectionPercent` with a value
    of `50%`, which is the maximum number of hosts in the connection pool that can
    be ejected.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we revised and applied various controls for managing application
    resiliency for the Online Boutique application. Istio provides various controls
    for managing application resiliency without needing to modify or build anything
    specific in your application. In the next section, we will apply the learning
    of *Chapter 6* to our Online Boutique application.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Istio to manage application security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have created Ingress via Istio Gateway, routing rules via Istio
    `VirtualService`, and `DestinationRules` to handle how traffic will be routed
    to the end destination, we can move on to the next step of securing the traffic
    in the mesh. The following policy enforces that all traffic in the mesh should
    strictly happen over mTLS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration is available in the `Chapter12/security/strictMTLS.yaml`
    file on GitHub. Without this configuration, all the traffic in the mesh is happening
    in *PERMISSIVE* mode, which means that the traffic can happen over mTLS as well
    as plain text. You can validate that by deploying a `curl` Pod and making an HTTP
    call to any of the microservices in the mesh. But once you apply the policy, Istio
    will enforce *STRICT* mode, which means mTLS will be strictly enforced for all
    traffic. Apply the configuration using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check in Kiali that all traffic in the mesh is happening over mTLS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – App graph showing mTLS communication between services](img/Figure_12.05_B17989.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 – App graph showing mTLS communication between services
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will be securing Ingress traffic using `https`. This step is important
    to revise but the outcome of it creates a problem in accessing the application,
    so we will perform the steps to revise the concepts and then revert them back
    so that we can continue accessing the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the learning from *Chapter 4*’s *Exposing Ingress over HTTPS* section.
    The steps are much easier if you have a `onlineboutique.com` domain:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a CA. Here, we are creating a CA with a `onlineboutique.inc`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a `onlineboutique.com`, which also generates a private key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sign the CSR using the CA using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the certificate and private key as a Kubernetes Secret:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have created the certificate and stored them as Kubernetes Secret. In the
    next steps, we will modify the Istio Gateway configuration to expose the traffic
    over HTTPS using the certificates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the Gateway configuration as described in the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You can access and check the certificate using the following commands. Please
    note that the output is truncated to highlight relevant sections only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The configuration will secure the Ingress traffic to the `online-boutique` store,
    but it also means that you will not be able to access it from the browser because
    of a mismatch of the FQDN being used in the browser and the CN configured in the
    certificates. You can alternatively register DNS names against the AWS load balancer
    but for now, you might find it easier to remove the HTTPS configuration and revert
    to using the `Chapter12/trafficmanagement/01-gateway.yaml` file on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive deeper into security and perform `RequestAuthentication` and authorization
    for the Online Boutique store. In *Chapter 6*, we did an elaborate exercise of
    building authentication and authorization using Auth0\. Along the same lines,
    we will be building an authentication and authorization policy for the `frontend`
    service but this time, we will use a dummy JWKS endpoint, which is shipped in
    with Istio.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with creating a `RequestAuthentication` policy to define the
    authentication method supported by the `frontend` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We are making use of dummy `jwksUri`, which comes along with Istio for testing
    purposes. Apply the `RequestAuthentication` policy using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying the `RequestAuthentication` policy, you can test that by providing
    a dummy token to the `frontend` service:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fetch the dummy token and set it as an environment variable to be used in requests
    later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test using `curl`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that you received a `200` response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now try testing with an invalid token:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `RequestAuthentication` policy plunged into action and denied the request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Test without any token:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The outcome of the request is not desired but is expected because the `RequestAuthentication`
    policy is only responsible for validating a token if a token is passed. If there
    is no `Authorization` header in the request, then the `RequestAuthentication`
    policy will not be invoked. We can solve this problem using `AuthorizationPolicy`,
    which enforces an access control policy for workloads in the mesh.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build `AuthorizationPolicy`, which enforces that a principal must be
    present in the request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration is available in the `Chapter12/security/requestAuthorizationPolicy.yaml`
    file in GitHub. Apply the configuration using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying the configuration test using *Steps 1* to *4*, which we performed
    after applying the `RequestAuthentication` policy, you will notice that all steps
    work as expected, but for *Step 4*, we are getting the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: That is because the authorization policy enforces the required presence of a
    JWT with the `["``testing@secure.istio.io/testing@secure.istio.io"]` principal.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the security configuration for our Online Boutique application.
    In the next section, we will read about various resources that will help you become
    an expert and certified in using and operating Istio.
  prefs: []
  type: TYPE_NORMAL
- en: Certification and learning resources for Istio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The primary resource for learning Istio is the Istio website ([https://istio.io/latest/](https://istio.io/latest/)).
    There is elaborative documentation on performing basic to multi-cluster setups.
    There are resources for beginners and advanced users, and various exercises on
    performing traffic management, security, observability, extensibility, and policy
    enforcement. Outside of the Istio documentation, the other organization providing
    lots of supportive content on Istio is Tetrate ([https://tetrate.io/](https://tetrate.io/)),
    which also provides labs and certification courses. One such certification provided
    by Tetrate Academy is **Certified Istio Administrator**. Details about the course
    and exam are available at [https://academy.tetrate.io/courses/certified-istio-administrator](https://academy.tetrate.io/courses/certified-istio-administrator).
    Tetrate Academy also provides a free course to learn about Istio fundamentals.
    You can find the details of the course at [https://academy.tetrate.io/courses/istio-fundamentals](https://academy.tetrate.io/courses/istio-fundamentals).
    Similarly, there is a course from Solo.io named **Get Started with Istio**; you
    can find details of the course at [https://academy.solo.io/get-started-with-istio](https://academy.solo.io/get-started-with-istio).
    Another good course from The Linux Foundation is named **Introduction to Istio**,
    and you can find the details of the course at [https://training.linuxfoundation.org/training/introduction-to-istio-lfs144x/](https://training.linuxfoundation.org/training/introduction-to-istio-lfs144x/).
  prefs: []
  type: TYPE_NORMAL
- en: I personally enjoy the learning resources available at [https://istiobyexample.dev/](https://istiobyexample.dev/);
    the site explains various use cases of Istio (such as canary deployment, managing
    Ingress, managing gRPC traffic, and so on) in great detail, along with configuration
    examples. For any technical questions, you can always head to StackOverflow at
    [https://stackoverflow.com/questions/tagged/istio](https://stackoverflow.com/questions/tagged/istio).
    There is an energetic and enthusiastic community of Istio users and builders who
    are discussing various topics about Istio at [https://discuss.istio.io/](https://discuss.istio.io/);
    feel free to sign up for the discussion board.
  prefs: []
  type: TYPE_NORMAL
- en: Tetrate Academy also provides a free course on Envoy fundamentals; the course
    is very helpful to understand the fundamentals of Envoy and, in turn, the Istio
    data plane. You can find the details of this course at [https://academy.tetrate.io/courses/envoy-fundamentals](https://academy.tetrate.io/courses/envoy-fundamentals).
    The course is full of practical labs and quizzes that are very helpful in mastering
    your Envoy skills.
  prefs: []
  type: TYPE_NORMAL
- en: The Istio website has compiled a list of helpful resources to keep you updated
    with Istio and engage with the Istio community; you can find the list at [https://istio.io/latest/get-involved/](https://istio.io/latest/get-involved/).
    The list also provides you with details on how to report bugs and issues.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, there are not many resources except a few books and websites,
    but you will find most of the answers to your questions at [https://istio.io/latest/docs/](https://istio.io/latest/docs/).
    It is also a great idea to follow **IstioCon**, which is the Istio Community conference
    and happens on a yearly cadence. You can find a session of IstioCon 2022 at [https://events.istio.io/istiocon-2022/sessions/](https://events.istio.io/istiocon-2022/sessions/)
    and 2021 at [https://events.istio.io/istiocon-2021/sessions/](https://events.istio.io/istiocon-2021/sessions/).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding eBPF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we are at the end of this book, it is important to also look at other technologies
    that are relevant to Service Mesh. One such technology is the **Extended Berkeley
    Packet Filter** (**eBPF**). In this section, we will read about eBPF and its role
    in Service Mesh evolution.
  prefs: []
  type: TYPE_NORMAL
- en: eBPF is a framework that allows users to run custom programs within the kernel
    of the operating system without needing to change kernel source code or load kernel
    modules. The custom programs are called **eBPF programs** and are used to add
    additional capabilities to the operating system at runtime. The eBPF programs
    are safe and efficient and, like the kernel modules, they are like lightweight
    sandbox virtual machines run in a privileged context by the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: eBPF programs are triggered based on events happening at the kernel level, which
    is achieved by associating them to hook points. Hooks are predefined at kernel
    levels and include system calls, network events, function entry and exit, and
    so on. In scenarios where an appropriate hook doesn’t exist, then users can make
    use of kernel probes, also called kprobes. The kprobes are inserted into the kernel
    routine; ebPF programs are defined as a handler to kprobes and are executed whenever
    a particular breakpoint is hit in the kernel. Like hooks and kprobes, eBPF programs
    can also be attached to uprobes, which are probes at user space levels and are
    tied to an event at the user application level, thus eBPF programs can be executed
    at any level from the kernel to the user application. When executing programs
    at the kernel level, the biggest concern is the security of the program. In eBPF,
    that is assured by BPF libraries. The BPF libraries handle the system call to
    load the eBPF programs in two steps. The first step is the verification step,
    during which the eBPF program is validated to ensure that it will run to completion
    and will not lock up the kernel, the process loading the eBPF program has correct
    privileges, and the eBPF program will not harm the kernel in any way. The second
    step is a **Just-In-Time** (**JIT**) compilation step, which translates the generic
    bytecode of the program into the machine-specific instruction and optimizes it
    to get the maximum execution speed of the program. This makes eBPF programs run
    as efficiently as natively compiled kernel code as if it was loaded as a kernel
    module. Once the two steps are complete, the eBPF program is loaded and compiled
    into the kernel waiting for the hook or kprobes to trigger the execution.
  prefs: []
  type: TYPE_NORMAL
- en: BPF has been widely used as an add-on to the kernel. Most of the applications
    have been at the network level and mostly in observability space. eBPF has been
    used to provide visibility into system calls at packet and socket levels, which
    are then used for building security solution systems that can operate with low-level
    context from the kernel. eBPF programs are also used for introspection of user
    applications along with the part of the kernel running the application, which
    provides a consolidated insight to troubleshoot application performance issues.
    You might be wondering why we are discussing eBPF in the context of Service Mesh.
    The programmability and plugin model of eBPF is particularly useful in networking.
    eBPF can be used to perform IP routing, packet filtering, monitoring, and so on
    at native speeds of kernel modules. One of the drawbacks of the Istio architecture
    is its model of deploying a sidecar with every workload, as we discussed in *Chapter
    2* – the sidecar basically works by intercepting network traffic, making use of
    iptables to configure the kernel’s netfilter packet filter functionality. The
    drawback of this approach is less optimal performance, as the data path created
    for service traffic is much longer than what it would have been if the workload
    was just by itself without any sidecar traffic interception. With eBPF socket-related
    program types, you can filter socket data, redirect socket data, and monitor socket
    events. These programs have the potential for replacing the iptables-based traffic
    interception; using eBPF, there are options to intercept and manage network traffic
    without incurring any negative impacts on the data path performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Isovalent** (at [https://isovalent.com/](https://isovalent.com/)) is one
    such organization that is revolutionizing the architecture of API Gateway and
    Service Mesh. **Cilium** is a product from Isovalent, and it provides a variety
    of functionality, including API Gateway function, Service Mesh, observability,
    and networking. Cilium is built with eBPF as its core technology where it injects
    eBPF programs at various points in the Linux kernel to achieve application networking,
    security, and observability functions. Cilium is getting adopted in Kubernetes
    networking to solve performance degradation caused by packets needing to traverse
    the same network stack multiple times between the host and the Pod. Cilium is
    solving this problem by bypassing iptables in the networking stacking, avoiding
    net filters and other overheads caused by iptables, which has led to significant
    gains in network performance. You can read more about the Cilium product stack
    at [https://isovalent.com/blog/post/cilium-release-113/](https://isovalent.com/blog/post/cilium-release-113/);
    you will be amazed to see how eBPF is revolutionizing the application networking
    space.'
  prefs: []
  type: TYPE_NORMAL
- en: Istio has also created an open source project called Merbridge, which replaces
    iptables with eBPF programs to allow the transporting of data directly between
    inbound and outbound sockets of sidecar containers and application containers
    to shorten the overall data path. Merbridge is in its early days but has produced
    some promising results; you can find the open source project at [https://github.com/merbridge/merbridge](https://github.com/merbridge/merbridge).
  prefs: []
  type: TYPE_NORMAL
- en: With eBPF and products like Cilium, it is highly likely that there will be an
    advancement in how network proxy-based products will be designed and operated
    in the future. eBPF is being actively explored by various Service Mesh technologies,
    including Istio, on how it can be used to overcome drawbacks and improve the overall
    performance and experience of using Istio. eBPF is a very promising technology
    and is already being used for doing awesome things with products such as Cilium
    and Calico.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope this book has provided you with a good insight into Istio. *Chapters
    1* to *3* set the context on why Service Mesh is needed and how Istio the control
    and data planes operate. The information in these three chapters is important
    to appreciate Istio and to build an understanding of Istio architecture. *Chapters
    4* to *6* then provided details on how to use Istio for building the application
    network that we discussed in the earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in *Chapter 7*, you learned about observability and how Istio provides
    integration into various observation tools, as the next steps you should explore
    integration with other observability and monitoring tools such as Datadog. Following
    that, *Chapter 8* showed practices on how to deploy Istio across multiple Kubernetes
    clusters, which should have given you confidence on how to install Istio in production
    environments. *Chapter 9* then provided details on how Istio can be extended using
    WebAssembly and its applications, while *Chapter 10* discussed how Istio helps
    bridge the old world of virtual machines with the new world of Kubernetes by discussing
    how the Service Mesh can be extended to include workloads deployed on virtual
    machines. Lastly, *Chapter 11* covered the best practices for operating Istio
    and how tools such as OPA Gatekeeper can be used to automate some of the best
    practices.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we managed to revise the concepts of *Chapters 4* to *6* by
    deploying and configuring another open source demo application, which should have
    provided you with the confidence and experience to take on the learnings from
    the book to real-life applications and to take advantage of application networking
    and security provided by Istio.
  prefs: []
  type: TYPE_NORMAL
- en: 'You also read about eBPF and what a game-changing technology it is, making
    it possible to write code at the kernel level without needing to understand or
    experience the horrors of the kernel. eBPF will possibly bring lots of changes
    to how Service Mesh, API Gateway, and networking solutions in general operate.
    In the Appendix of this book, you will find information about other Service Mesh
    technologies: Consul Connect, Kuma Mesh, Gloo Mesh, and Linkerd. The Appendix
    provides a good overview of these technologies and helps you appreciate their
    strength and limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoyed learning about Istio. To establish your knowledge of Istio,
    you can also explore taking the Certified Istio Administrator exam provided by
    Tetrate. You can also explore the other learning avenues provided in this chapter.
    I hope reading this book was an endeavor that will take you to the next level
    in your career and experience of building scalable, resilient, and secure applications
    using Istio.
  prefs: []
  type: TYPE_NORMAL
- en: BEST OF LUCK!
  prefs: []
  type: TYPE_NORMAL
- en: Appendix – Other Service Mesh Technologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this appendix, we will learn about the following Service Mesh implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: Consul Connect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gloo Mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kuma
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linkerd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These Service Mesh technologies are popular and are gaining recognition and
    adoption by organizations. The information provided in this *Appendix* about these
    Service Mesh technologies is not exhaustive; rather, the goal here is to make
    you familiar with and aware of the alternatives to Istio. I hope reading this
    *Appendix* will provide some basic awareness of these alternative technologies
    and help you understand how these technologies fare in comparison to Istio. Let’s
    dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Consul Connect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consul Connect is a Service Mesh solution offered by HashiCorp. It is also known
    as Consul Service Mesh. On the HashiCorp website, you will find that the terms
    Consul Connect and Consul Service Mesh are used interchangeably. It is built upon
    Consul, which is a service discovery solution and a key-value store. Consul is
    a very popular and long-established service discovery solution; it provides and
    manages service identities for every type of workload, which are then used by
    Service Mesh to manage traffic between Services in Kubernetes. It also supports
    using ACLs to implement zero-trust networking and provides granular control over
    traffic flow in the mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Consul uses Envoy as its data plane and injects it into workload Pods as sidecars.
    The injection can be based on annotations as well as global configurations to
    automatically inject sidecar proxies into all workloads in specified namespaces.
    We will start by installing Consul Service Mesh on your workstation, followed
    by some exercises to practice the basics of using Consul Service Mesh.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by installing Consul:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the Consul repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the Consul CLI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For MacOS, follow these steps:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Install the HashiCorp tap:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the Consul Kubernetes CLI:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'For Linux Ubuntu/Debian, follow these steps:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add the HashiCorp GPG key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the HashiCorp apt repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '% sudo apt-get update && sudo apt-get install consul-k8s'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '% sudo yum install -y yum-utils'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'consul-k8s CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Start minikube:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Install Consul on minikube using the Consul CLI.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following in learn-consul-get-started-kubernetes/local:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the Consul Pods in the namespace:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Configure the Consul CLI to be able to communicate with Consul.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We will set environment variables so that the Consul CLI can communicate with
    your Consul cluster.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set `CONSUL_HTTP_TOKEN` from `secrets/consul-bootstrap-acl-token` and set it
    as an environment variable:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the Consul destination address. By default, Consul runs on port 8500 for
    HTTP and 8501 for HTTPS:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove SSL verification checks to simplify communication with your Consul cluster:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Access the Consul dashboard using the following command:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open `localhost:8501` in your browser to access the Consul dashboard, as shown
    in the following screenshot:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure A.1 – Consul Dashboard](img/Figure_13.01_B17989.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.1 – Consul Dashboard
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have installed Consul Service Mesh on minikube, let’s deploy an
    example application and go through the fundamentals of Consul Service Mesh.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Deploying an example application
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: In this section, we will deploy envoydummy along with a curl application. The
    `sampleconfiguration` file is available in `AppendixA/envoy-proxy-01.yaml`.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the configuration file, you will notice the following annotation:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: This annotation allows Consul to automatically inject a proxy for each service.
    The proxies create a data plane to handle requests between services based on the
    configuration from Consul.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Apply the configuration to create `envoydummy` and the `curl` Pods:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'In a few seconds, you will notice that Consul automatically injects a sidecar
    into the Pods:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'To find out more about the sidecar, please inspect the `envoydummy` Pod using
    the following commands:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: In the output, you can see a container named `consul-dataplane` created from
    an image called `hashicorp/consul-dataplane:1.0.0`. You can inspect the image
    at [https://hub.docker.com/layers/hashicorp/consul-dataplane/1.0.0-beta1/images/sha256-f933183f235d12cc526099ce90933cdf43c7281298b3cd34a4ab7d4ebeeabf84?context=explore](https://hub.docker.com/layers/hashicorp/consul-dataplane/1.0.0-beta1/images/sha256-f933183f235d12cc526099ce90933cdf43c7281298b3cd34a4ab7d4ebeeabf84?context=explore)
    and you will notice that it is made up of envoy proxy.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s try to access `envoydummy` from the `curl` Pod:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: So far, we have successfully deployed the `envoydummy` Pod along with `consul-dataplane`
    as a sidecar. We have observed Consul Service Mesh security in action by seeing
    that the `curl` Pod, while deployed in the same namespace, is unable to access
    the `envoydummy` Pod. In the next section, we will understand this behavior and
    learn how to configure Consul to perform zero-trust networking.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Zero-trust networking
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Consul manages inter-service authorization with Consul constructs called intentions.
    Using Consul CRDs, you need to define intentions that prescribe what services
    are allowed to communicate with each other. Intentions are the cornerstones of
    zero-trust networking in Consul.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Intentions are enforced by the sidecar proxy on inbound connections. The sidecar
    proxy identifies the inbound service using its TLS client certificate. After identifying
    the inbound service, the sidecar proxy then checks if an intention exists to allow
    the client to communicate with the destination service.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the following code block, we are defining an intention to allow traffic
    from the `curl` service to the `envoydummy` service:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: In the configuration, we have specified the names of the destination service
    and the source service. In `action`, we have specified `allow` to allow traffic
    from source to destination. Another possible value of `action` is `deny`, which
    denies traffic from source to destination. If you do not want to specify the name
    of a service, you will need to use `*`. For example, if the service name in `sources`
    is `*` then it will allow traffic from all services to `envoydummy`.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s apply intentions using the following command:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify the created intentions in the Consul dashboard:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure A.2 – Consul intentions](img/Figure_13.02_B17989.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.2 – Consul intentions
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Considering that we have created the intention to allow traffic from the `curl`
    service to the `envoydummy` service, let’s test that the `curl` Pod is able to
    communicate with the `envoydummy` Pod using the following command:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Using intentions, we were able to define rules to control traffic between services
    without needing to configure a firewall or any changes in the cluster. Intentions
    are key building blocks of Consul for creating zero-trust networks.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Traffic management and routing
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Consul provides a comprehensive set of service discovery and traffic management
    features. The service discovery comprises three stages: routing, splitting, and
    resolution. These three stages are also referred to as the service discovery chain,
    and it can be used to implement traffic controls based on HTTP headers, path,
    query strings, and workload version.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Let’s go through each stage of the service discovery chain.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Routing
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This is the first stage of the service discovery chain, and it is used to intercept
    traffic using Layer 7 constructs such as HTTP header and path. This is achieved
    via service-router config entry through which you can control the traffic routing
    using various criteria. For example, for `envoydummy`, let’s say we want to enforce
    that any request send to `envoydummy` version v1 with `/latest` in the URI should
    be routed to `envoydummy` version v2 instead, and any request to version v2 of
    the `envoydummy` app but with `/old` in the path should be routed to version v1
    of the `envoydummy` app. This can be achieved using the following `ServiceRouter`
    configuration:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'In the configuration, we are specifying that any request destined for the `envoydummy`
    service but with `pathPrefix` set to `''/latest''` will be routed to `envoydummy2`.
    And in the following configuration, we are specifying that any request destined
    for the `envoydummy2` service but with `pathPrefix` set to `''/old''` will be
    routed to `envoydummy`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Both `ServiceRouter` configurations are saved in `AppendixA/Consul/routing-to-envoy-dummy.yaml`.
    A deployment descriptor for `envoydummy` version v2 and the intentions that allow
    traffic from the `curl` Pod are also available in `AppendixA/Consul/envoy-proxy-02.yaml`
    on GitHub.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Go ahead and deploy version v2 of `envoydummy` along with the ServiceRouter
    configuration using the following commands:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check the configuration using the Consul dashboard. The following two
    screenshots show the two `ServiceRouter` configurations we have applied:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`ServiceRouter` configuration to send traffic with the prefix `/latest` to
    `envoydummy2`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure A.3](img/Figure_13.03_B17989.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.3
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`ServiceRouter` configuration to send traffic with the prefix `/old` to `envoydummy`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure A.4](img/Figure_13.04_B17989.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.4
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have configured the service routes, let’s test the routing behavior:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make any request to version v1 of `envoydummy` with a URI that is not `/latest`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as expected: the request should be routed to version v1 of `envoydummy`.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make a request to version v1 of `envoydummy` with a URI that is `/latest`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as expected: the request, although addressed to version v1 of
    `envoydummy`, is routed to version v2 of `envoydummy`.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make any request to version v2 of `envoydummy` with a URI that is not `/old`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as expected: the request should be routed to version v2 of `envoydummy`.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make a request to version v2 of `envoydummy` with a URI that is `/old`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as expected: the request although addressed to version v2 of
    `envoydummy` is routed to version v1 of `envoydummy`.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In these examples, we made use of path prefixes as criteria for routing. The
    other options are query parameters and HTTP headers. `ServiceRouter` also supports
    retry logic, which can be added to the destination configuration. Here is an example
    of retry logic added to the `ServiceRouter` config:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'You can read more about `ServiceRouter` configuration on the HashiCorp website:
    https://developer.hashicorp.com/consul/docs/connect/config-entries/service-router.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Next in the service discovery chain is splitting, which we will learn about
    in the following section.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Splitting
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Service splitting is the second stage in the Consul service discovery chain
    and is configured via the `ServiceSplitter` configuration. `ServiceSplitter` allows
    you to split a request to a service to multiple subset workloads. Using this configuration,
    you can also perform canary deployments. Here is an example where traffic for
    the `envoydummy` service is routed in a 20:80 ratio to version v1 and v2 of the
    `envoydummy` application:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `ServiceSplitter` configuration, we have configured 80% of the traffic
    to `envoydummy` to be routed to the `envoydummy2` service and the remaining 20%
    of the traffic to be routed to the `envoydummy` service. The configuration is
    available in `AppendixA/Consul/splitter.yaml`. Apply the configuration using the
    following command:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying the configuration, you can check out the routing config on the
    Consul dashboard. In the following screenshot, we can see that all traffic to
    `envoydummy` is routed to `envoydummy` and `envoydummy2`. The following screenshot
    doesn’t show the percentage, but you can hover the mouse over the arrows connecting
    the splitters and resolvers and you should be able to see the percentage:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure A.5 – Split of traffic to envoydummy2](img/Figure_13.05_B17989.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.5 – Split of traffic to envoydummy2
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following screenshot shows the split of traffic for `envoydummy`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure A.6 – Split of traffic to envoydummy service](img/Figure_13.06_B17989.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.6 – Split of traffic to envoydummy service
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that the `ServiceSplitter` configuration is in place, test that traffic
    to our services is routed in the ratio specified in the config file:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: You will observe that the traffic is routed in a 20:80 ratio between the two
    services. `ServiceSplitter` is a powerful feature that can be used for A/B testing,
    as well as canary and blue/green deployments. Using `ServiceSplitter`, you can
    also perform weight-based routing between subsets of the same service. It also
    allows you to add HTTP headers while routing the service. You can read more about
    `ServiceSplitter` at [https://developer.hashicorp.com/consul/docs/connect/config-entries/service-splitter](https://developer.hashicorp.com/consul/docs/connect/config-entries/service-splitter).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: We have looked at two of the three steps in Consul’s service discovery chain.
    The final stage is resolution, which we will cover in the next section.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Resolution
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Consul has another config type called `ServiceResolver`, which is used to define
    which instances of a service map to the service name requested by the client.
    They control the service discovery and decide where the request is finally routed
    to. Using `ServiceResolver`, you can control the resilience of your system by
    routing the request to healthy upstreams. `ServiceResolver` distributes load to
    services when they are spread across more than one data center and provides failover
    when the services are suffering from outages. More details about `ServiceResolver`
    can be found at [https://developer.hashicorp.com/consul/docs/connect/config-entries/service-resolver](https://developer.hashicorp.com/consul/docs/connect/config-entries/service-resolver).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Consul Service Mesh also has provision for gateways to manage traffic from
    outside the mesh. It supports three kinds of gateway:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Mesh gateways** are used to enable and secure communication between data
    centers. It acts as a proxy providing Ingress to Service Mesh while at the same
    time securing the traffic using mTLS. Mesh gateways are used to communicate between
    Consul Service Mesh instances deployed in different data centers and/or Kubernetes
    clusters. A good hands-on exercise on mesh gateways is available at [https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-mesh-gateways](https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-mesh-gateways).'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ingress gateways** are used to provide access to services in the mesh to
    clients outside the mesh. The client can be outside the mesh but in the same Kubernetes
    cluster, or completely outside the cluster but within or beyond the network perimeter
    of the organization. You can read more about Ingress Gateway at [https://developer.hashicorp.com/consul/docs/k8s/connect/ingress-gateways](https://developer.hashicorp.com/consul/docs/k8s/connect/ingress-gateways).'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ServiceDefault`. This is where the details about the external service are
    defined, and it is referred to by the terminating gateway. You can read more about
    terminating gateways at [https://developer.hashicorp.com/consul/docs/k8s/connect/terminating-gateways](https://developer.hashicorp.com/consul/docs/k8s/connect/terminating-gateways).'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, Consul Service Mesh also provides comprehensive observability of the
    mesh. The sidecar proxies collect and expose data about the traffic traversing
    the mesh. The metrics data exposed by the sidecar proxies are then scraped by
    Prometheus. The data includes Layer 7 metrics such as HTTP status code, request
    latency, and throughput. The Consul control plane also provides some metrics such
    as config synchronization status, exceptions, and errors, like the Istio control
    plane. The tech stack for observability is also like Istio; like Istio, Consul
    also supports integration with various other observability tools, such as Datadog,
    to get insight into Consul Service Mesh health and performance. You can read more
    about Consul Service Mesh observability at [https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-layer7-observability](https://developer.hashicorp.com/consul/tutorials/kubernetes/kubernetes-layer7-observability).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'I hope this section provided a brief but informative rundown of how Consul
    Service Mesh operates, what the various constructs in Consul Service Mesh are,
    and how they operate. I am sure you must have noticed the similarities between
    Consul Service Mesh and Istio: they both use Envoy as a sidecar proxy, and the
    Consul service discovery chain closely resembles Istio virtual services and destination
    rules; Consul Service Mesh gateways are very similar to Istio gateways. The main
    difference is how the control plane is implemented and the use of an agent on
    each node of the cluster. Consul Service Mesh can run on VMs to provide benefits
    of Service Mesh on legacy workloads. Consul Service Mesh is backed by HashiCorp
    and is tightly integrated with HashiCorp’s other products, including HashiCorp
    Vault. It is also offered as a freemium product. There is also an Enterprise version
    for organizations needing enterprise support and a SaaS offering called HCP Consul
    that provides a fully managed cloud service to customers who want one-click mesh
    deployments.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Uninstalling Consul Service Mesh
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can use consul-k8s to uninstall Consul Service Mesh using the following
    commands:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`% consul-k8s uninstall -auto-approve=true -wipe-data=true`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`..`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Deleting data for installation:`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Name: consul`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Namespace consul`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`✓` `Deleted PVC => data-consul-consul-server-0`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`✓` `PVCs deleted.`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`✓` `Deleted Secret => consul-bootstrap-acl-token`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`✓` `Consul secrets deleted.`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`✓` `Deleted Service Account => consul-tls-init`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`✓` `Consul service accounts deleted.`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can uninstall consul-k8s CLI using Brew on macOS:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`% brew` `uninstall consul-k8s`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Gloo Mesh
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Gloo Mesh is a Service Mesh offering from Solo.io. There is an open source version
    called Gloo Mesh and an enterprise offering called Gloo Mesh Enterprise. Both
    are based on Istio Service Mesh and claim to have a better control plane and added
    functionality on top of open source Istio. Solo.io provides a feature comparison
    on its website outlining the differences between Gloo Mesh Enterprise, Gloo Mesh
    Open Source, and Istio, which you can access at [https://www.solo.io/products/gloo-mesh/](https://www.solo.io/products/gloo-mesh/).
    Gloo Mesh is primarily focused on providing a Kubernetes-native management plane
    through which users can configure and operate multiple heterogeneous Service Mesh
    instances across multiple clusters. It comes with an API that abstracts the complexity
    of managing and operating multiple meshes without the user needing to know the
    complexity under the hood caused by multiple Service Meshes. You can find details
    about Gloo Mesh at [https://docs.solo.io/gloo-mesh-open-source/latest/getting_started/](https://docs.solo.io/gloo-mesh-open-source/latest/getting_started/).
    This is a comprehensive resource on how to install and try Gloo Mesh. Solo.io
    has another product called Gloo Edge, which acts as a Kubernetes Ingress controller
    as well as an API gateway. Gloo Mesh Enterprise is deployed along with Gloo Edge,
    which provides many comprehensive API management and Ingress capabilities. Gateway
    Gloo Mesh Enterprise adds support for external authentication using OIDC, OAuth,
    API key, LDAP, and OPA. These policies are implemented via a custom CRD called
    ExtAuthPolicy, which can apply these authentications when routes and destinations
    match certain criteria.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Gloo Mesh Enterprise provides WAF policies to monitor, filter, and block any
    harmful HTTP traffic. It also provides support for data loss prevention by doing
    a series of regex replacements on the response body and content that is logged
    by Envoy. This is a very important feature from a security point of view and stops
    sensitive data from being logged into the log files. DLP filters can be configured
    on listeners, virtual services, and routes. Gloo Mesh also provides support for
    connecting to legacy applications via the SOAP message format. There are options
    for building data transformation policies to apply XSLT transformation to modernize
    SOAP/XML endpoints. The data transformation policies can be applied to transform
    request or response payloads. It also supports special transformations such as
    via Inja templates. With Inja, you can write loops, conditional logic, and other
    functions to transform requests and responses.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: There is also extensive support for WASM filters. Solo.io provides custom tooling
    that speeds up the development and deployment of web assemblies. To store WASM
    files, solo.io provides WebAssembly Hub, available at [https://webassemblyhub.io/](https://webassemblyhub.io/),
    and an open source CLI tool called wasme. You can read more about how to use Web
    Assembly Hub and the wasme CLI at [https://docs.solo.io/web-assembly-hub/latest/tutorial_code/getting_started/](https://docs.solo.io/web-assembly-hub/latest/tutorial_code/getting_started/).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: As Gloo Mesh and other products from Solo.io are closely integrated with the
    Enterprise Service Mesh offering, you get a plethora of other features, and one
    such feature is a global API portal. The API portal is a self-discovery portal
    for publishing, sharing, and monitoring API usage for internal and external monetization.
    When using a multi-heterogeneous mesh, users don’t need to worry about managing
    observability tools for every mesh; instead, Gloo Mesh Enterprise provides aggregated
    metrics across every mesh, providing a seamless experience of managing and observing
    multiple meshes.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: In enterprise environments, it is important that multiple teams and users can
    access and deploy services in the mesh without stepping on each others’ toes.
    Users need to know what services are available to consume and what services they
    have published. Users should be able to confidently perform mesh operations without
    impacting the services of other teams. Gloo Mesh uses the concept of workspaces,
    which are logical boundaries for a team, limiting team Service Mesh operations
    within the confines of the workspace so that multiple teams can concurrently use
    the mesh. Workspaces provide security isolation between configurations published
    by every team. Through workspaces, Gloo Mesh addresses the complexity of muti-tenancy
    in Enterprise environments, making it simpler for multiple teams to adopt Service
    Mesh with config isolation from each other and strict access control for safe
    muti-tenant usage of the mesh.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Gloo Mesh is also integrated with another Service Mesh based on a different
    architecture than Istio. The mesh is called Istio Ambient Mesh, which, rather
    than adding a sidecar proxy per workload, adds a proxy at the per-node level.
    Istio Ambient Mesh is integrated with Gloo Mesh, and users can run their sidecar
    proxy-based mesh along with their per-node proxy Istio Ambient Mesh.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Gloo Enterprise Mesh, with integration with Solo.io products such as Gloo Edge,
    makes it a strong contender among Service Mesh offerings. The ability to support
    multi-cluster and multi-mesh deployments, multi-tenancy via workspaces, strong
    support for authentication, zero-trust networking, and mature Ingress management
    via Gloo Edge makes it a comprehensive Service Mesh offering.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Kuma
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Kuma is an open source CNCF sandbox project donated to CNCF by Kong Inc. Like
    Istio, Kuma also uses Envoy as the data plane. It supports multi-cluster and multi-mesh
    deployments, providing one global control plane to manage them all. At the time
    of writing this book, Kuma is one single executable written in GoLang. It can
    be deployed on Kubernetes as well as on VMs. When deployed in non-Kubernetes environments,
    it requires a PostgreSQL database to store its configurations.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s start by downloading and installing Kuma, followed by hands-on exercises
    on this topic:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Download Kuma for your operating system:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install Kuma on minikube. Unzip the download file and, in the unzipped folder’s
    `bin` directory, run the following commands to install Kuma on Kubernetes:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will create a namespace called `kuma-system` and install the Kuma control
    plane in that namespace, along with configuring various CRDs and admission controllers.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'At this point, we can access Kuma’s GUI using the following command:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open `localhost:5681/gui` in your browser and you will see the following dashboard:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure A.7 – Kuma dashboard](img/Figure_13.07_B17989.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.7 – Kuma dashboard
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: The Kuma GUI provides comprehensive details about the mesh. We will use this
    to check the configuration as we build policies and add applications to the mesh.
    On the home page of the GUI, you will notice that it shows one mesh called **default**.
    A mesh in Kuma is a Service Mesh that is logically isolated from other Service
    Meshes in Kuma. You can have one Kuma installation in a Kubernetes cluster, which
    can then manage multiple Service Meshes perhaps for each team or department deploying
    their apps in that Kubernetes cluster. This is a very important concept and a
    key differentiator of Kuma from other Service Mesh technologies.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Deploying envoydemo and curl in Kuma mesh
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The deployment file is available at `AppendixA/Kuma/envoy-proxy-01.yaml`. The
    noticeable difference compared to Istio in the deployment file is the addition
    of the following label, which instructs Kuma to inject its sidecar proxies into
    `envoydummy`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The following commands will deploy the `envoydummy` and `curl` applications:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few seconds, check whether the Pods have been deployed and the sidecars
    injected using the following commands:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: The sidecars are also called **data plane proxies** (**DPPs**), and they run
    along with every workload in the mesh. DPP comprises a data plane entity that
    defines the configuration of the DPP and a kuma-dp binary. During startup, kuma-dp
    retrieves the startup configuration for Envoy from the Kuma control plane (kuma-cp)
    and uses that to spawn the Envoy process. Once Envoy starts, it connects to kuma-cp
    using XDS. kuma-dp also spawns a core-dns process at startup.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: It is worth noticing that installing Kuma and deploying an application has been
    a breeze. It is very simple, and the GUI is very intuitive, even for beginners.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Using the GUI, let’s check the overall status of the mesh.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From **MESH** | **Overview**, you can see newly added DPPs:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure A.8 – Mesh overview in the Kuma GUI](img/Figure_13.08_B17989.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.8 – Mesh overview in the Kuma GUI
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From **MESH** | **Data Plane Proxies**, you can find details about the workloads:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure A.9 – Data plane proxies](img/Figure_13.09_B17989.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: Figure A.9 – Data plane proxies
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have installed apps, we will perform some hands-on exercises with
    Kuma policies to get experience with Kuma.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will start by accessing the `envoydummy` service from the `curl` Pod:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as expected. By default, Kuma allows traffic in and out of the
    mesh. By default, all traffic is unencrypted in the mesh. We will enable mTLS
    and deny all traffic in the mesh to establish zero-trust networking. First, we
    will delete the policy that allows all traffic within the mesh using the following
    command:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '`allow-all-traffic` is a traffic permission policy that allows all traffic
    within the mesh.  The previous command deletes the policy, thereby restricting
    all traffic in the mesh.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we will enable mTLS within the mesh to enable secure communication and
    let `kong-dp` correctly identify a service by comparing the service identity with
    the DPP certificate. Without enabling mTLS, Kuma cannot enforce traffic permissions.
    The following policy enables mTLS in the default mesh. It makes use of an inbuilt
    CA, but in case you want to use an external CA then there are also provisions
    to provide externally generated root CA and key. Kuma automatically generates
    certificates for every workload with SAN in SPIFEE format.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'In the config file, we have defined that this policy applies to the `default`
    mesh. We have declared a CA named `ca-1` of the `builtin` type and have configured
    it to be used as the root CA for mTLS by defining `enabledBackend`. The configuration
    file is available at `AppendixA/Kuma/enablemutualTLS.yaml`. You can apply the
    configuration using the following commands:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'After enabling mTLS, let’s try to access `envoydummy` from `curl`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: The output is as expected because mTLS is enabled and there is no `TrafficPermission`
    policy allowing the traffic between `curl` and `envoydummy`.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To allow traffic, we need to create the following `TrafficPermission` policy:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `kuma.io/service` fields contain the values of the corresponding
    tags. Tags are sets of key-value pairs that contain details of the service that
    the DPP is part of and metadata about the exposed service. The following are tags
    applied to DPP for `envoydummy`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, you can fetch the value of the `curl` DPP. The configuration file
    is available at `AppendixA/Kuma/allow-traffic-curl-to-envoyv1.yaml`. Apply the
    configuration using the following command:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying the configuration, test that you can access `envoydummy` from
    `curl`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: We have just experienced how you can control traffic between workloads in the
    mesh. You will find this very similar to `ServiceIntentions` in Consul Service
    Mesh.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Traffic management and routing
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Now we will explore traffic routing in Kuma. We will deploy version v2 of the
    `envoydummy` service and route certain requests between version v1 and v2.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The first step is to deploy version v2 of `envoydummy`, followed by defining
    traffic permission to allow traffic between the `curl` Pod and the `envoydummy`
    v2 Pod. The files are at `AppendixA/Kuma/envoy-proxy-02.yaml` and `AppendixA/Kuma/allow-traffic-curl-to-envoyv2.yaml`.
    Apply the configuration, and once you have applied both files, test that `curl`
    is able to reach both v1 and v2 of the `envoydummy` Pod:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will configure the routing by using a Kuma policy called `TrafficRoute`.
    This policy allows us to configure rules for traffic routing in the mesh.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The policy can be split into four parts to make it easier to understand:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the first part, we are declaring the `TrafficRoute` policy. The basic usage
    of the policy is documented at [https://kuma.io/docs/2.0.x/policies/traffic-route/](https://kuma.io/docs/2.0.x/policies/traffic-route/).
    Here, we are declaring that the policy applies to the default mesh and  to any
    request in the mesh originating from `curl_appendix-kuma_svc` with a destination
    of `envoydummy_appendix-kuma_svc_80`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we are configuring any request with a prefix of `''/latest''` to be routed
    to the DPP with the tags that are highlighted under `destination`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we are configuring request with a prefix of `''/old''` to be routed to
    the data plane with the tags that are highlighted under `destination`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we are declaring the default destination for requests that do not
    match any of the paths defined in previous parts of the config. The default destination
    will be the DPP with the tags highlighted in the following code:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The configuration file is available at `AppendixA/Kuma/trafficRouting01.yaml`.
    Apply the configuration and test the following scenarios:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'All requests with `''/latest''` should be routed to version v2:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'All request with `''/old''` should be routed to version v1:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'All other requests should follow the default behavior:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The request routing works as expected, and it is similar to how you would configure
    the same behavior using Istio. Now, let’s look at the load balancing properties
    of Kuma Mesh. We will build another traffic routing policy to do weighted routing
    between version v1 and v2 of `envoydummy`. Here is a snippet of the configuration
    available at `AppendixA/Kuma/trafficRouting02.yaml`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying the configuration, you can test the traffic distribution using
    the following command:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'The traffic should be distributed between the two versions in approximately
    a 1:9 ratio. You can perform traffic routing, traffic modification, traffic splitting,
    load balancing, canary deployments, and locality-aware load balancing using a
    `TrafficRoute` policy. To read more about `TrafficRoute`, please use the comprehensive
    documentation available here: https://kuma.io/docs/2.0.x/policies/traffic-route.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Kuma also provides policies for circuit breaking, fault injection, timeout,
    rate limiting, and many more things. A comprehensive list of Kuma policies is
    available here: [https://kuma.io/docs/2.0.x/policies/introduction/](https://kuma.io/docs/2.0.x/policies/introduction/).
    These out-of-the-box policies make Kuma very easy to use with a very shallow learning
    curve.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the hands-on example so far, we have been deploying all workloads in the
    default mesh. We discussed earlier that Kuma allows you to create different isolated
    meshes, allowing teams to have isolated mesh environments within the same Kuma
    cluster. You can create a new mesh using the following configuration:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration is available in `AppendixA/Kuma/team-digital-mesh.yaml`.
    Apply the configuration using the following command:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have created the mesh, you can create all the resources within the
    mesh by adding the following annotations to the workload deployment configurations:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'And add the following to the Kuma policies:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: The ability to create a mesh is a very useful feature for enterprise environments
    and a key differentiator of Kuma compared to Istio.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Kuma also provides built-in Ingress capabilities to handle north-south traffic
    as well as east-west traffic. The Ingress is managed as a Kuma resource called
    a gateway, which in turn is an instance of kuma-dp. You have the flexibility to
    deploy as many Kuma gateways as you want, but ideally, one gateway per mesh is
    recommended. Kuma also supports integration with non-Kuma gateways, also called
    delegated gateways. For now, we will talk about built-in Kuma gateways and, later,
    briefly discuss delegated gateways.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To create a built-in gateway, you first need to define `MeshGatewayInstance`
    along with a matching `MeshGateway`. `MeshGatewayInstance` provides the details
    of how a gateway instance should be instantiated. Here is an example configuration
    of `MeshGatewayInstance`, which is also available at `AppendixA/Kuma/envoydummyGatewayInstance01.yaml`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'In the config, we are setting that there will be `1 replica` and a `serviceType`
    of `LoadBalancer`, and we have applied a tag, `kuma.io/service: envoydummy-edge-gateway`,
    which will be used to build the association with `MeshGateway`.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the following configuration, we are creating a `MeshGateway` named `envoydummy-edge-gateway`.
    The configuration is available in `AppendixA/Kuma/envoydummyGateway01.yaml`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: The `MeshGateway` resource specifies the listeners, which are endpoints that
    accept network traffic. In the configuration, you specify ports, protocols, and
    an optional hostname. Under `selectors`, we are also specifying the `MeshGatewayInstance`
    tags with which the `MeshGateway` configuration is associated. Notice that we
    are specifying the same tags we defined in the `MeshGatewayInstance` configuration.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will define `MeshGatewayRoute`, which describes how a request is routed
    from `MeshGatewayInstance` to the workload service. An example configuration is
    available at `AppendixA/Kuma/envoydummyGatewayRoute01.yaml`. Here are some snippets
    from the file:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Under `selectors`, we are specifying the details of the gateway and the listener
    to which this route should be attached. The details are specified by providing
    the tags of the corresponding gateway and listeners:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the `conf` part, we provide Layer 7 matching criteria for the request, such
    as the path and HTTP headers, and the destination details:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And last but not least, we allow traffic between the edge gateway and the envoy
    dummy service by configuring `TrafficPermission` as described in following snippet.
    You can find the configuration at `AppendixA/Kuma/allow-traffic-edgegateway-to-envoy.yaml`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With traffic permission in place, we are now ready to apply the configuration
    using the following set of commands:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create `MeshGatewayInstance`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create `MeshGateway`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create `MeshGatewayRoute`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create `TrafficPermissions`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can verify that Kuma has created a gateway instance using the following
    commands:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also check the corresponding service using the following command:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now all set to access `envoydummy` using the built-in Kuma gateway.
    But first, we need to find an IP address through which we can access the Ingress
    gateway service on minikube. Use the following command to find the IP address:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, using http://127.0.0.1:52346, you can access the `envoydummy` service
    by performing `curl` from your terminal:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: You have learned how to create a `MeshGatewayInstance`, which is then associated
    with `MeshGateway`. After the association, kuma-cp created a gateway instance
    of the built-in Kuma gateway. We then created a `MeshGatewayRoute` that specifies
    how the request will be routed from the gateway to the workload service. Later,
    we created a `TrafficPermission` resource to allow traffic flow from `MeshGateway`
    to the `EnvoyDummy` workload.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Kuma also provides options for using an external gateway as Ingress, also called
    a delegated gateway. In a delegated gateway, Kuma supports integrations with various
    API gateways, but Kong Gateway is the preferred and most well-documented option.
    You can read more about delegated gateways at [https://kuma.io/docs/2.0.x/explore/gateway/#delegated](https://kuma.io/docs/2.0.x/explore/gateway/#delegated).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Like Istio, Kuma also provides native support for both Kubernetes and VM-based
    workloads. Kuma provides extensive support for running advanced configurations
    of Service Mesh spanning multiple Kubernetes clusters, data centers, and cloud
    providers. Kuma has a concept of zones, which are logical aggregations of DPPs
    that can communicate with each other. Kuma supports running Service Mesh in multiple
    zones and the separation of control planes in a multi-zone deployment. Each zone
    is allocated its own horizontally scalable control plane providing complete isolation
    between every zone. All zones are then also managed by a centralized global control
    plane, which manages the creation of and changes to policies that are applied
    to DPPs and the transmission of zone-specific policies and configurations to respective
    control planes of underlying zones. The global control plane is a single pane
    of glass providing an inventory of all DPPs across all zones.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: As mentioned earlier, Kuma is an open source project that was donated by Kong
    to CNCF. Kong also provides Kong Mesh, which is an enterprise version of Kuma
    built on top of Kuma, extending it to include capabilities required for running
    critical functionality for enterprise workloads. Kong Mesh provides a turnkey
    Service Mesh solution with capabilities such as integration with OPA, FIPS 140-2
    compliance, and role-based access control. Coupled with Kong Gateway as an Ingress
    gateway, a Service Mesh based on Kuma, additional enterprise-grade add-ons and
    reliable enterprise support makes Kong Mesh a turnkey Service Mesh technology.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Uninstalling Kuma
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can uninstall Kuma Mesh using the following command:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`% kumactl install control-plane | kubectl delete -``f -`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Linkerd
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Linkerd is a CNCF graduated project licensed under Apache v2\. Buoyant ([https://buoyant.io/](https://buoyant.io/))
    is the major contributor to Linkerd. Out of all Service Mesh technologies, Linkerd
    is probably one of the earliest, if not the oldest. It was initially made public
    in 2017 by Buoyant. It had initial success, but then it was criticized for being
    very resource hungry. The proxy used in Linkerd was written using the Scala and
    Java networking ecosystem, which uses the **Java Virtual Machine** (**JVM**) at
    runtime, causing significant resource consumption. In 2018, Buoyant released a
    new version of Linkerd called Conduit. Conduit was later renamed Linkerd v2\.
    The Linkerd v2 data plane is made up of Linkerd2-proxy, which is written in Rust
    and has a small resource consumption footprint.  Linkerd2- proxy is purpose built
    for proxying as a sidecar in Kubernetes Pods. While Linkerd2-proxy is written
    in Rust, the Linkerd control plane is developed in Golang.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Like other open source Service Mesh technologies discussed in this *Appendix*,
    we will discover Linkerd by playing around with it and observing how it is similar
    to or different to Istio. Let’s start by installing Linkerd on minikube:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Install Linkerd on minikube using the following command:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Follow the suggestion to include linkerd2 in your path:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Linkerd provides an option to check and validate that the Kubernetes cluster
    meets all the prerequisites required to install Linkerd:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the output contains the following, then you are good to go with the installation:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If not, then you need to resolve the issues by going through suggestions at
    [https://linkerd.io/2.12/tasks/troubleshooting/#pre-k8s-cluster-k8s%20for%20hints](https://linkerd.io/2.12/tasks/troubleshooting/#pre-k8s-cluster-k8s%20for%20hints).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will Install Linkerd in two steps:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we install the CRDs:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'After installing the control plane, check that Linkerd is fully installed using
    the following commands:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'If Linkerd is successfully installed, then you should see the following message:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'That complete the setup of Linkerd! Let’s now analyze what has been installed:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: It’s worth noticing here that the control plane comprises many Pods and Services.
    The `linkerd-identity` service is a CA for generating signed certificates for
    Linkerd proxies. `linkerd-proxy-injector` is the Kubernetes admission controller
    responsible for modifying Kubernetes Pod specifications to add linkerd-proxy and
    proxy-init containers. The `destination` service is the brains of the Linkerd
    control plane and maintains service discovery and identity information about the
    services, along with policies for securing and managing the traffic in the mesh.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Deploying envoydemo and curl in Linkerd
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now let’s deploy envoydummy and curl apps and check how Linkerd performs Service
    Mesh functions. Follow these steps to install the application:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Like most Service Mesh solutions, we need to annotate the deployment descriptors
    with the following annotations:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The configuration file for the `envoydummy` and `curl` apps along with annotations
    is available in `AppendixA/Linkerd/envoy-proxy-01.yaml`.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After preparing the deployment descriptors, you can apply the configurations:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That should deploy the Pod. Once the Pod is deployed, you can check what has
    been injected into the Pods via the following commands:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From the preceding output, observe that Pod initialization was performed by
    a container named `linkerd-init` of the `cr.l5d.io/linkerd/proxy-init:v2.0.0`
    type, and the Pod has two running containers, `curl` and `linkerd-proxy`, of the
    `cr.l5d.io/linkerd/proxy:stable-2.12.3` type. The `linkerd-init` container runs
    during the initialization phase of the Pod and modifies iptables rules to route
    all network traffic from `curl` to `linkerd-proxy`. As you may recall, in Istio
    we have `istio-init` and `istio-proxy` containers, which are similar to Linkerd
    containers. `linkerd-proxy` is ultra-light and ultra-fast in comparison to Envoy.
    Being written in Rust makes its performance predictable and it doesn’t need garbage
    collection, which often causes high latency during garbage collection passes.
    Rust is arguably much more memory safe than C++ and C, which makes it less susceptible
    to memory safety bugs. You can read more about why `linkerd-proxy` is better than
    envoy at [https://linkerd.io/2020/12/03/why-linkerd-doesnt-use-envoy/](https://linkerd.io/2020/12/03/why-linkerd-doesnt-use-envoy/).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Verify that `curl` is able to communicate with the `envoydummy` Pod as follows:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have installed the `curl` and `envoydummy` Pods, let’s explore Linkerd
    Service Mesh functions. Let’s start by exploring how we can restrict traffic within
    the mesh using Linkerd.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Zero-trust networking
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Linkerd provides comprehensive policies to restrict traffic in the mesh. Linkerd
    provides a set of CRDs through which policies can be defined to control the traffic
    in the mesh. Let’s explore these policies by implementing policies to control
    traffic to the `envoydummy` Pod:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will first lock down all traffic in the cluster using following:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We used the `linkerd upgrade` command to apply a `default-inbound-policy` of
    `deny`, which prohibits all traffic to ports exposed by workloads in the mesh
    unless there is a server resource attached to the port.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After applying the policy, all access to the `envoydummy` service is denied:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a server resource to describe the `envoydummy` port. A Server
    resource is a means of instructing Linkerd that only authorized clients can access
    the resource. We do that by declaring the following Linkerd policy:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The configuration file is available at `AppendixA/Linkerd/envoydummy-server.yaml`.
    The server resource is defined in the same namespace as the workload. In the configuration
    file, we also define the following:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`podSelector`: Criteria for selecting the workload'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`port`: Name or number of the port for which this server configuration is being
    declared'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxyProtocol`: Configures protocol discovery for inbound connections and
    must be one of the following: unknown, `HTTP/1`, `HTTP/2`, `gRPC`, `opaque`, or
    `TLS`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apply the server resource using the following command:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: Although we have applied the server resource, the `curl` Pod still can’t access
    the `envoydummy` service unless we authorize it.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this step, we will create an authorization policy that authorizes `curl`
    to access `envoydummy`. The authorization policy is configured by providing server
    details of the target destination and service account details being used to run
    the originating service. We created a server resource named `envoydummy` in the
    previous step and, as per `AppendixA/Linkerd/envoy-proxy-01.yaml`, we are using
    a service account named `curl` to run the `curl` Pod. The policy is defined as
    follows and is also available at `AppendixA/Linkerd/authorize-curl-access-to-envoydummy.yaml`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the configuration as follows:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the `AuthorizationPolicy` is in place, it will authorize all traffic to
    the Envoy server from any workload running using a `curl` service account.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can verify the access between the `curl` and `envoydummy` Pods using the
    following command:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using `AuthorizationPolicy`, we have controlled access to ports presented as
    servers from other clients in the mesh. Granular access control, such as controlling
    access to an HTTP resource, can be managed by another policy called `s`.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can understand this concept better via an example, so let’s make a requirement
    that only requests whose URI start with `/dummy` can be accessible from `curl`;
    requests to any other URI must be denied. Let’s get started:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We need to first define an `HTTPRoute` policy as described in the following
    code snippet:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The configuration is also available at `AppendixA/Linkerd/HTTPRoute.yaml`. This
    will create an HTTP route targeting the `envoydummy` server resource. In the `rules`
    section, we define the criteria for identifying requests that will be used to
    identify the HTTP request for this route. Here, we have defined to rule to match
    any request with the `dummy` prefix and the `GET` method. `HTTPRoute` also supports
    route matching using headers and query parameters. You can also apply other filters
    in `HTTPRoute` to specify how the request should be processed during the request
    or response cycle; you can modify inbound request headers, redirect requests,
    modify request paths, and so on.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once we have defined `HTTPRoute`, we can modify the `AuthorizationPolicy` to
    associate with `HTTPRoute` instead of the server, as listed in the following code
    snippet and also available at `AppendixA/Linkerd/HttpRouteAuthorization.yaml`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The configuration updates `AuthorizationPolicy` and, instead of referencing
    the server (`envoydummy` configured in `AppendixA/Linkerd/authorize-curl-access-to-envoydummy.yaml`)
    as the target, the policy is now referencing `HTTPRoute` (named `envoydummy-dummy-route`).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Apply both configurations and test that you are able to make requests with the
    `/dummy` prefix in the URI. Any other request will be denied by Linkerd.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'So far in `AuthorizationPolicy` we have used `ServiceAccount` authentication.
    `AuthorizationPolicy` also supports `MeshTLSAuthentication` and `NetworkAuthentication`.
    Here is a brief overview of these authentication types:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`MeshTLSAuthentication` is used to identify a client based on its mesh identity.
    For example, the `curl` Pod will be represented as `curl.appendix-linkerd.serviceaccount.identity.linkerd.local`.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NetworkAuthentication` is used to identify a client based on its network location
    using **Classless Inter-Domain Routing** (**CIDR**) blocks.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Linkerd also provides retries and timeouts to provide application resilience
    when systems are under stress or suffering partial failures. Apart from support
    for usual retry strategies, there is also a provision for specifying retry budgets
    so that retries do not end up amplifying resilience problems. Linkerd provides
    automated load balancing of requests to all destination endpoints using the **exponentially
    weighted moving average** (**EWMA**) algorithm. Linkerd supports weight-based
    traffic splitting, which is useful for performing canary and blue/green deployments.
    Traffic splitting in Linkerd uses the **Service Mesh Interface** (**SMI**) Traffic
    Split API, allowing users to incrementally shift traffic between blue and green
    services. You can read about the Traffic Split API at [https://github.com/servicemeshinterface/smi-spec/blob/main/apis/traffic-split/v1alpha4/traffic-split.md](https://github.com/servicemeshinterface/smi-spec/blob/main/apis/traffic-split/v1alpha4/traffic-split.md)
    and SMI at [https://smi-spec.io](https://smi-spec.io). Linkerd provides a well-defined
    and documented integration with Flagger to perform automatic traffic shifting
    when performing canary and blue/green deployments.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: There is a lot more to learn and digest about Linkerd. You can read about it
    at [https://linkerd.io/2.12](https://linkerd.io/2.12). Linkerd is ultra-performant
    because of its ultra-light service proxy build using Rust. It is carefully designed
    to solve application networking problems. The ultra-light proxy performs most
    Service Mesh functions but lacks in features such as circuit breaking and rate
    limiting. Let’s hope that the Linkerd creators bridge the gap with Envoy.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Hopefully, you are now familiar with the various alternatives to Istio and how
    they implement Service Mesh. Consul, Linkerd, Kuma, and Gloo Mesh have lots of
    similarities, and all of them are powerful, but Istio is the one that has the
    biggest community behind it and the support of various well-known organizations.
    Also, there are various organizations that provide enterprise support for Istio,
    which is a very important consideration when deploying Istio to production.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
