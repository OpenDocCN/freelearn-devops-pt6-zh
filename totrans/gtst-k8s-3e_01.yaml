- en: Introduction to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this book, we will help you build, scale, and manage production-ready Kubernetes
    clusters. Each section of this book will empower you with the core container concepts
    and the operational context of running modern web services that need to be available
    24 hours of the day, 7 days a week, 365 days of the year. As we progress, you''ll
    be given concrete, code-based examples that you can deploy into running clusters
    in order to get real-world feedback on Kubernetes'' many abstractions. By the
    end of this book, you will have mastered the core conceptual building blocks of
    Kubernetes, and will have a firm understanding of how to handle the following
    paradigms:'
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identity and authentication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter will set the stage for *why Kubernetes?* and give an overview of
    modern container history, diving into how containers work, as well as why it's
    important to schedule, orchestrate, and manage a container platform well. We'll
    tie this back to concrete objectives and goals for your business and product.
    This chapter will also give a brief overview of how Kubernetes orchestration can
    enhance our container management strategy and how we can get a basic Kubernetes
    cluster up, running, and ready for container deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing container operations and management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The importance of container management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The advantages of Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading the latest Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and starting up a new Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The components of a Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You''ll need to have the following tools installed:'
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS CLI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud CLI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minikube
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll go into the specifics of these tools' installation and configuration as
    we go through this chapter. If you already know how to do this, you can go ahead
    and set them up now.
  prefs: []
  type: TYPE_NORMAL
- en: A brief overview of containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Believe it or not, containers and their precursors have been around for over
    15 years in the Linux and Unix operating systems. If you look deeper into the
    fundamentals of how containers operate, you can see their roots in the chroot
    technology that was invented all the way back in 1970\. Since the early 2000s,
    FreeBSD, Linux, Solaris, Open VZ, Warden, and finally Docker all made significant
    attempts at encapsulating containerization technology for the end user.
  prefs: []
  type: TYPE_NORMAL
- en: While the VServer's project and first commit (*running several general purpose
    Linux server on a single box with a high degree of independence and security*
    ([http://ieeexplore.ieee.org/document/1430092/?reload=true](http://ieeexplore.ieee.org/document/1430092/?reload=true)))
    may have been one of the most interesting historical junctures in container history,
    it's clear that Docker set the container ecosystem on fire back in late 2013 when
    they went full in on the container ecosystem and decided to rebrand from dotCloud
    to Docker. Their mass marketing of container appeal set the stage for the broad
    market adoption we see today and is a direct precursor of the massive container
    orchestration and scheduling platforms we're writing about here.
  prefs: []
  type: TYPE_NORMAL
- en: Over the past five years, containers have grown in popularity like wildfire.
    Where containers were once relegated to developer laptops, testing, or development
    environments, you'll now see them as the building blocks of powerful production
    systems. They're running highly secure banking workloads and trading systems,
    powering IoT, keeping our on-demand economy humming, and scaling up to millions
    of containers to keep the products of the 21st century running at peak efficiency
    in both the cloud and private data centers. Furthermore, containerization technology
    permeates our technological zeitgest, with every technology conference in the
    world devoting a significant portion of their talks and sessions devoted to building,
    running, or developing in containers.
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of this compelling story lies Docker and their compelling suite
    of developer-friendly tools. Docker for macOS and Windows, Compose, Swarm, and
    Registry have been incredibly powerful tools that have shaped workflows and changed
    how companies develop software. They've built a bridge for containers to exist
    at the very heart of the **Software Delivery Life Cycle** (**SDLC**), and a remarkable
    ecosystem has sprung up around those containers. As Malcom McLean revolutionized
    the physical shipping world in the 1950s by creating a standardized shipping container,
    which is used today for everything from ice cube trays to automobiles, Linux containers
    are revolutionizing the software development world by making application environments
    portable and consistent across the infrastructure landscape.
  prefs: []
  type: TYPE_NORMAL
- en: We'll pick this story up as containers go mainstream, go to production, and
    go big within organizations. We'll look at what makes a container next.
  prefs: []
  type: TYPE_NORMAL
- en: What is a container?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containers are a type of operating system virtualization, much like the virtual
    machines that preceded them. There's also lesser known types of virtualization
    such as Application Virtualization, Network Virtualization, and Storage Virtualization.
    While these technologies have been around since the 1960s, Docker's encapsulation
    of the container paradigm represents a modern implementation of resource isolation
    that utilizes built-in Linux kernel features such as chroot, **control groups**
    (**cgroups**), UnionFS, and namespaces to fully isolated resource control at the
    process level.
  prefs: []
  type: TYPE_NORMAL
- en: Containers use these technologies to create lightweight images that act as a
    standalone, fully encapsulated piece of software that carries everything it needs
    inside the box. This can include application binaries, any system tools or libraries,
    environment-based configuration, and runtime. This special property of isolation
    is very important, as it allows developers and operators to leverage the all-in-one
    nature of a container to run without issue, regardless of the environment it's
    run on. This includes developer laptops and any kind of pre-production or production
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: This decoupling of application packaging mechanism from the environment on which
    it runs is a powerful concept that provides a clear separation of concerns between
    engineering teams. This allows developers to focus on building the core business
    capabilities into their application code and managing their own dependencies,
    while operators can streamline the continuous integration, promotion, and deployment
    of said applications without having to worry about their configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the core of container technology are three key concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: cgroups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Namespaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Union filesystems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cgroups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: cgroups work by allowing the host to share and also limit the resources each
    process or container can consume. This is important for both resource utilization
    and security, as it prevents **denial-of-service** (**DoS**) attacks on the host's
    hardware resources. Several containers can share CPU and memory while staying
    within the predefined constraints. cgroups allow containers to provision access
    to memory, disk I/O, network, and CPU. You can also access devices (for example,
    `/dev/foo`). cgroups also power the soft and hard limits of container constraints
    that we'll discuss in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are seven major cgroups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory cgroup**: This keeps track of page access by the group, and can define
    limits for physical, kernel, and total memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blkio cgroup**: This tracks the I/O usage per group, across the read and
    write activity per block device. You can throttle by group per device, on operations
    versus bytes, and for reads versus writes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU cgroup**: This keeps track of user and system CPU time and usage per
    CPU. This allows you to set weights, but not limits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Freezer cgroup**: This is useful in batch management systems that are often
    stopping and starting tasks in order to schedule resources efficiently. The SIGSTOP
    signal is used to suspend a process, and the process is generally unaware that
    it is being suspended (or resumed, for that matter.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPUset cgroup**: This allows you to pin a group to a specific CPU within
    a multi-core CPU architecture. You can pin by application, which will prevent
    it from moving between CPUs. This can improve the performance of your code by
    increasing the amount of local memory access or minimizing thread switching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Net_cls/net_prio cgroup**: This keeps tabs on the egress traffic class (`net_cls`)
     or priority (`net_prio`) that is generated by the processes within the cgroup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Devices cgroup**: This controls what read/write permissions the group has
    on device nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Namespaces offer another form of isolation for process interaction within operating
    systems, creating the workspace we call a container. Linux namespaces are created
    via a syscall named `unshare`, while `clone` and `setns` allow you to manipulate
    namespaces in other manners.
  prefs: []
  type: TYPE_NORMAL
- en: '`unshare()` allows a process (or thread) to disassociate parts of its execution
    context that are currently being shared with other processes (or threads). Part
    of the execution context, such as the mount namespace, is shared implicitly when
    a new process is created using FORK(2) (for more information visit [http://man7.org/linux/man-pages/man2/fork.2.html](http://man7.org/linux/man-pages/man2/fork.2.html))
    or VFORK(2) (for more information visit [http://man7.org/linux/man-pages/man2/vfork.2.html](http://man7.org/linux/man-pages/man2/vfork.2.html)),
    while other parts, such as virtual memory, may be shared by explicit request when
    creating a process or thread using CLONE(2) (for more information visit [http://man7.org/linux/man-pages/man2/clone.2.html](http://man7.org/linux/man-pages/man2/clone.2.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: Namespaces limit the visibility a process has on other processes, networking,
    filesystems, and user ID components. Container processes are limited to seeing
    only what is in the same namespace. Processes from containers or the host processes
    are not directly accessible from within this container process. Additionally,
    Docker gives each container its own networking stack that protects the sockets
    and interfaces in a similar fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'If cgroups limit how much of a thing you can use, namespaces limit what things
    you can see. The following diagram shows the composition of a container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cce4fa1-a187-490d-90b0-ee6b67e68600.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the case of the Docker engine, the following namespaces are used:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pid`: Provides process isolation via an independent set of process IDs from
    other namespaces. These are nested.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net`: Manages network interfaces by virtualizing the network stack through
    providing a loopback interface, and can create physical and virtual network interfaces
    that exist in a single namespace at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ipc`: Manages access to interprocess communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mnt`: Controls filesystem mount points. These were the first kind of namespaces
    created in the Linux kernel, and can be private or shared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`uts`: The Unix time-sharing system isolates version IDs and kernel by allowing
    a single system to provide different host and domain naming schemes to different
    processes. The processes `gethostname` and `sethostname` use this namespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`user`: This namespace allows you to map UID/GID from container to host, and
    prevents the need for extra configuration in the container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Union filesystems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Union filesystems are also a key advantage of using Docker containers. Containers
    run from an image. Much like an image in the VM or cloud world, it represents
    state at a particular point in time. Container images snapshot the filesystem,
    but tend to be much smaller than a VM. The container shares the host kernel and
    generally runs a much smaller set of processes, so the filesystem and bootstrap
    period tend to be much smaller—though those constraints are not strictly enforced.
    Second, the union filesystem allows for the efficient storage, download, and execution
    of these images. Containers use the idea of *copy-on-write storage*, which is
    able to create a brand new container immediately, without having to wait on copying
    out a whole new filesystem. This is similar to thin provisioning in other systems,
    where storage is allocated as needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e23753d0-1323-4c21-a729-5e298550813e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Copy-on-write storage keeps track of what''s changed, and in this way is similar
    to **distributed version control systems** (**DVCS**) such as Git. There are a
    number of options available to the end user that leverage copy-on-write storage:'
  prefs: []
  type: TYPE_NORMAL
- en: AUFS and overlay at the file level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Device mapper at the block level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BTRFS and ZFS and the filesystem level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The easiest way to understand union filesystems is to think of them like a layer
    cake with each layer baked independently. The Linux kernel is our base layer;
    then, we might add an OS such as Red Hat Linux or Ubuntu.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we might add an application such as nginx or Apache. Every change creates
    a new layer. Finally, as you make changes and new layers are added, you'll always
    have a top layer (think frosting) that is a writable layer. Union filesystems
    leverage this strategy to make each layer lightweight and speedy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Docker''s case, the storage driver is responsible for stacking these layers
    on top of each other and providing a single pane of glass to view these systems.
    The thin writable layer on the top of this stack of layers is where you''ll do
    your work: the writable container layer. We can consider each layer below to be
    container image layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d71ff188-e5be-44a7-99e7-38e1e04ccd6a.png)'
  prefs: []
  type: TYPE_IMG
- en: What makes this truly efficient is that Docker caches the layers the first time
    we build them. So, let's say that we have an image with Ubuntu and then add Apache
    and build the image. Next, we build MySQL with Ubuntu as the base. The second
    build will be much faster because the Ubuntu layer is already cached. Essentially,
    our chocolate and vanilla layers, from the preceding diagram, are already baked.
    We simply need to bake the pistachio (MySQL) layer, assemble, and add the icing
    (the writable layer).
  prefs: []
  type: TYPE_NORMAL
- en: Why are containers so cool?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What's also really exciting is that not only has the open source community embraced
    containers and Kubernetes, but the cloud providers have also deeply embraced the
    container ecosystem, and invested millions of dollars in supporting tooling, ecosystem,
    and management planes that can help manage containers. This means you have more
    options to run container workloads, and you'll have more tools to manage the scheduling
    and orchestration of the applications running on your clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll explore some specific opportunities available to Kubernetes users, but
    at the time of this book''s publishing, all of the major **cloud service providers**
    (**CSPs**) are offering some form of hosted or managed Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon Web Services**: AWS offers **Elastic Container Service for Kubernetes**
    (**EKS**) (for more information visit [https://aws.amazon.com/eks/](https://aws.amazon.com/eks/)),
    a managed service that simplifies running Kubernetes clusters in their cloud.
    You can also roll your own clusters with kops (for information visit [https://kubernetes.io/docs/setup/custom-cloud/kops/](https://kubernetes.io/docs/setup/custom-cloud/kops/)).
    This product is still in active development:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/02bfdebe-ea92-4aeb-8f55-40461f10787e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Google Cloud Platform**: GCP offers the **Google Kubernetes Engine** (**GKE**)
    (for more information visit [https://cloud.google.com/kubernetes-engine/](https://cloud.google.com/kubernetes-engine/)),
    a powerful cluster manager that can deploy, manage, and scale containerized applications
    in the cloud. Google has been running containerized workloads for over 15 years,
    and this platform is an excellent choice for sophisticated workload management:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/ff002177-ab39-4bb5-9c8f-569c4f5b4c98.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Microsoft Azure**: Azure offers the **Azure Container Service** (**AKS**)
    (for more information visit [https://azure.microsoft.com/en-us/services/kubernetes-service/](https://azure.microsoft.com/en-us/services/kubernetes-service/)),
    which aims to simplify the deployment, management, and operations of a full-scale
    Kubernetes cluster. This product is still in active development:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f45bd510-b702-4f04-b815-6bcc2081699d.png)'
  prefs: []
  type: TYPE_IMG
- en: When you take advantage of one of these systems, you get built-in management
    of your Kubernetes cluster, which allows you to focus on the optimization, configuration,
    and deployment of your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The advantages of Continuous Integration/Continuous Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ThoughtWorks defines Continuous Integration as a development practice that requires
    developers to integrate code into a shared repository several times a day. By
    having a continuous process of building and deploying code, organizations are
    able to instill quality control and testing as part of the everyday work cycle.
    The result is that updates and bug fixes happen much faster and the overall quality
    improves.
  prefs: []
  type: TYPE_NORMAL
- en: However, there has always been a challenge in creating development environments
    that match those of testing and production. Often, inconsistencies in these environments
    make it difficult to gain the full advantage of Continuous Delivery. Continuous
    Integration is the first step in speeding up your organization's software delivery
    life cycle, which helps you get your software features in front of customer quickly
    and reliably.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of Continuous Delivery/Deployment uses Continuous Integration to
    enables developers to have truly portable deployments. Containers that are deployed
    on a developer's laptop are easily deployed on an in-house staging server. They
    are then easily transferred to the production server running in the cloud. This
    is facilitated due to the nature of containers, which build files that specify
    parent layers, as we discussed previously. One advantage of this is that it becomes
    very easy to ensure OS, package, and application versions are the same across
    development, staging, and production environments. Because all the dependencies
    are packaged into the layer, the same host server can have multiple containers
    running a variety of OS or package versions. Furthermore, we can have various
    languages and frameworks on the same host server without the typical dependency
    clashes we would get in a VM with a single operating system.
  prefs: []
  type: TYPE_NORMAL
- en: This sets the stage for Continuous Delivery/Deployment of the application, as
    the operations teams or the developers themselves can focus on getting deployments
    and application rollouts correct, without having to worry about the intricacies
    of dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Delivery is the embodiment and process wherein all code changes are
    automatically built, tested (Continuous Integration), and then released into production
    (Continuous Delivery). If this process captures the correct quality gates, security
    guarantees, and unit/integration/system tests, the development teams will constantly
    release production-ready and deployable artifacts that have moved through an automated
    and standardized process.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that CD requires the engineering teams to automate more
    than just unit tests. In order to utilize CD in sophisticated scheduling and orchestration
    systems such as Kubernetes, teams need to verify application functionality across
    many dimensions before they're deployed to customers. We'll explore deployment
    strategies that Kubernetes has to offer in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, it''s important to keep in mind that utilizing Kubernetes with CI/CD
    reduces the risk of the many common problems that technology firms face:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Long release cycles**: If it takes a long time to release code to your users,
    then it''s a potential functionality that they''re missing out on, and this results
    in lost revenue. If you have a manual testing or release process, it''s going
    to slow down getting changes to production, and therefore in front of your customers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fixing code is hard**: When you shorten the release cycle, you''re able to
    discover and remediate bugs closer to the point of creation. This lowers the fixed
    cost, as there''s a correlation between bug introduction and bug discovery times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Release better**: The more you release, the better you get at releasing.
    Challenging your developers and operators to build automation, monitoring, and
    logging around the processes of CI/CD will make your pipeline more robust. As
    you release more often, the amount of difference between releases also decreases.
    A smaller difference allows teams to troubleshoot potential breaking changes more
    quickly, which in turn gives them more time to refine the release process further.
    It''s a virtuous cycle!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because all the dependencies are packaged into the layer, the same host server
    can have multiple containers running a variety of OS or package versions. Furthermore,
    we can have various languages and frameworks on the same host server without the
    typical dependency clashes we would get in a VM with a single operating system.
  prefs: []
  type: TYPE_NORMAL
- en: Resource utilization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The well-defined isolation and layer filesystem also makes containers ideal
    for running systems with a very small footprint and domain-specific purpose. A
    streamlined deployment and release process means we can deploy quickly and often.
    As such, many companies have reduced their deployment time from weeks or months
    to days and hours in some cases. This development life cycle lends itself extremely
    well to small, targeted teams working on small chunks of a larger application.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices and orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we break down an application into very specific domains, we need a uniform
    way to communicate between all the various pieces and domains. Web services have
    served this purpose for years, but the added isolation and granular focus that
    containers bring have paved the way for microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'A definition for microservices can be a bit nebulous, but a definition from
    Martin Fowler, a respected author and speaker on software development, says this:'
  prefs: []
  type: TYPE_NORMAL
- en: In short, the microservice architectural style is an approach to developing
    a single application as a suite of small services, each running in its own process
    and communicating with lightweight mechanisms, often an HTTP resource API. These
    services are built around business capabilities and independently deployable by
    fully automated deployment machinery. There is a bare minimum of centralized management
    of these services, which may be written in different programming languages and
    use different data storage technologies.
  prefs: []
  type: TYPE_NORMAL
- en: As the pivot to containerization and as microservices evolve in an organization,
    they will soon need a strategy to maintain many containers and microservices.
    Some organizations will have hundreds or even thousands of containers running
    in the years ahead.
  prefs: []
  type: TYPE_NORMAL
- en: Future challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Life cycle processes alone are an important piece of operation and management.
    How will we automatically recover when a container fails? Which upstream services
    are affected by such an outage? How will we patch our applications with minimal
    downtime? How will we scale up our containers and services as our traffic grows?
  prefs: []
  type: TYPE_NORMAL
- en: Networking and processing are also important concerns. Some processes are part
    of the same service and may benefit from proximity to the network. Databases,
    for example, may send large amounts of data to a particular microservice for processing.
    How will we place containers near each other in our cluster? Is there common data
    that needs to be accessed? How will new services be discovered and made available
    to other systems?
  prefs: []
  type: TYPE_NORMAL
- en: Resource utilization is also key. The small footprint of containers means that
    we can optimize our infrastructure for greater utilization. Extending the savings
    started in the Elastic cloud will take us even further toward minimizing wasted
    hardware. How will we schedule workloads most efficiently? How will we ensure
    that our important applications always have the right resources? How can we run
    less important workloads on spare capacity?
  prefs: []
  type: TYPE_NORMAL
- en: Finally, portability is a key factor in moving many organizations to containerization.
    Docker makes it very easy to deploy a standard container across various operating
    systems, cloud providers, and on-premise hardware or even developer laptops. However,
    we still need tooling to move containers around. How will we move containers between
    different nodes on our cluster? How will we roll out updates with minimal disruption?
    What process do we use to perform blue-green deployments or canary releases?
  prefs: []
  type: TYPE_NORMAL
- en: Whether you are starting to build out individual microservices and separating
    concerns into isolated containers or you simply want to take full advantage of
    the portability and immutability in your application development, the need for
    management and orchestration becomes clear. This is where orchestration tools
    such as Kubernetes offer the biggest value.
  prefs: []
  type: TYPE_NORMAL
- en: Our first clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is supported on a variety of platforms and OSes. For the examples
    in this book, I used an Ubuntu 16.04 Linux VirtualBox ([https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads))
    for my client and **Google Compute Engine** (**GCE**) with Debian for the cluster
    itself. We will also take a brief look at a cluster running on **Amazon Web Services**
    (**AWS**) with Ubuntu.
  prefs: []
  type: TYPE_NORMAL
- en: To save some money, both GCP ([https://cloud.google.com/free/](https://cloud.google.com/free/))
    and AWS ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)) offer free
    tiers and trial offers for their cloud infrastructure. It's worth using these
    free trials for learning Kubernetes, if possible.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the concepts and examples in this book should work on any installation
    of a Kubernetes cluster. To get more information on other platform setups, refer
    to the Kubernetes getting started page, which will help you pick the right solution
    for your cluster: [http://kubernetes.io/docs/getting-started-guides/](http://kubernetes.io/docs/getting-started-guides/).
  prefs: []
  type: TYPE_NORMAL
- en: Running Kubernetes on GCE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have a few options for setting up the prerequisites for our development environment.
    While we'll use a Linux client on our local machine in this example, you can also
    use the Google Cloud Shell to simplify your dependencies and setup. You can check
    out that documentation at [https://cloud.google.com/shell/docs/](https://cloud.google.com/shell/docs/),
    and then jump down to the `gcloud auth login` portion of the tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting back to the local installation, let''s make sure that our environment
    is properly set up before we install Kubernetes. Start by updating the packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something similar to the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Install Python and `curl` if they are not present:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the `gcloud` SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We will need to start a new shell before `gcloud` is on our path.
  prefs: []
  type: TYPE_NORMAL
- en: 'Configure your GCP account information. This should automatically open a browser,
    from where we can log in to our Google Cloud account and authorize the SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you have problems with login or want to use another browser, you can optionally
    use the `--no-launch-browser` command. Copy and paste the URL to the machine and/or
    browser of your choice. Log in with your Google Cloud credentials and click Allow
    on the permissions page. Finally, you should receive an authorization code that
    you can copy and paste back into the shell, where the prompt will be waiting.
  prefs: []
  type: TYPE_NORMAL
- en: 'A default project should be set, but we can verify this with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can modify this and set a new default project with the following command.
    Make sure to use project ID and not project name, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can find our project ID in the console at the following URL: [https://console.developers.google.com/project](https://console.developers.google.com/project).
    Alternatively, we can list the active projects with `$ gcloud alpha projects list`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can turn on API access to your project at this point in the GCP dashboard, [https://console.developers.google.com/project](https://console.developers.google.com/project),
    or the Kubernetes script will prompt you to do so in the next section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bca0be7b-7eeb-4967-9f75-6e95d2d658c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, you want to change to a directory when you can install the Kubernetes
    binaries. We''ll set that up and then download the software:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Installing the latest Kubernetes version is done in a single step, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'It may take a minute or two to download Kubernetes depending on your connection
    speed. Earlier versions would automatically call the `kube-up.sh` script and start
    building our cluster. In version 1.5, we will need to call the `kube-up.sh` script
    ourselves to launch the cluster. By default, it will use the Google Cloud and
    GCE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you get an error at this point due to missing components, you''ll need to
    add a few pieces to your local Linux box. If you''re running the Google Cloud
    Shell, or are utilizing a VM in GCP, you probably won''t see this error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that these components are missing and are required for leveraging
    the `kube-up.sh script`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can update the components by adding them to your shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After you run the `kube-up.sh` script, you will see quite a few lines roll
    past. Let''s take a look at them one section at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9bce1223-359c-4702-b07b-f3725376f242.png)'
  prefs: []
  type: TYPE_IMG
- en: If your `gcloud` components are not up to date, you may be prompted to update
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshot shows the checks for prerequisites, as well as making
    sure that all components are up to date. This is specific to each provider. In
    the case of GCE, it will verify that the SDK is installed and that all components
    are up to date. If not, you will see a prompt at this point to install or update:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c277bd3e-da43-4520-803f-b125efcf6e5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, the script is turning up the cluster. Again, this is specific to the provider.
    For GCE, it first checks to make sure that the SDK is configured for a default
    project and zone. If they are set, you''ll see those in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: You may see an output that the bucket for storage hasn't been created. That's
    normal! The creation script will go ahead and create it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, it uploads the server binaries to Google Cloud storage, as seen in the
    Creating gs:... lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc10e65f-1d28-48dc-832f-000a511b0f70.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It then checks for any pieces of a cluster already running. Then, we finally
    start creating the cluster. In the output in the preceding screenshot, we can
    see it creating the master server, IP address, and appropriate firewall configurations
    for the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86efdb69-7538-4f21-80db-9c14d13bd6dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, it creates the minions or nodes for our cluster. This is where our
    container workloads will actually run. It will continually loop and wait while
    all the minions start up. By default, the cluster will have four nodes (minions),
    but K8s supports having more than 1,000 (and soon beyond). We will come back to
    scaling the nodes later on in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that everything is created, the cluster is initialized and started. Assuming
    that everything goes well, we will get an IP address for the master:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Also, note that configuration along with the cluster management credentials
    are stored in `home/<Username>/.kube/config`.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the script will validate the cluster. At this point, we are no longer
    running provider-specific code. The validation script will query the cluster via
    the `kubectl.sh` script. This is the central script for managing our cluster.
    In this case, it checks the number of minions found, registered, and in a ready
    state. It loops through, giving the cluster up to 10 minutes to finish initialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a successful startup, a summary of the minions and the cluster component
    health is printed on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Finally, a `kubectl cluster-info` command is run, which outputs the URL for
    the master services, including DNS, UI, and monitoring. Let's take a look at some
    of these components.
  prefs: []
  type: TYPE_NORMAL
- en: If you'd like to get further debugging and/or diagnose cluster problems, you
    can use `kubectl cluster-info dump` to see what's going on with your cluster.
    Additionally, if you need to pause and take a break and want to conserve your
    free hours, you can log into the GUI and set the `kubernetes-minion-group` instance
    group to zero, which will remove all of the instances. The pencil will edit the
    group for you; set it to zero. Don't forget to set it back to three if you want
    to pick up again!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7a95be3-d56c-4796-be5a-d8174fcb7028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can simply stop the manager as well. You''ll need to click the stop button
    to shut it down:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f1c5f60-568c-4c18-aedf-532a60dddd42.png)'
  prefs: []
  type: TYPE_IMG
- en: If you'd like to start the cluster up again, start the servers again to keep
    going. They'll need some time to start up and connect to each other.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to work on more than one cluster at a time or you want to use a
    different name than the default, see the `<kubernetes>/cluster/gce/config-default.sh`
    file for more fine-grained configuration of your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since Kubernetes v1.3.x, you can no longer authenticate through public IP addresses
    to the GUI. To get around this, we''ll use the `kubectl proxy` command. First,
    grab the token from the configuration command, and then we''ll use it to launch
    a local proxy version of the UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Open a browser and enter the following URL: `https://localhost/ui/`.
  prefs: []
  type: TYPE_NORMAL
- en: You can also type these commands to open a browser window automatically if you're
    on macOS: `$ open https://localhost/ui/` or `$ xdg-open https://localhost/ui` if
    you're on Linux.
  prefs: []
  type: TYPE_NORMAL
- en: 'The certificate is self-signed by default, so you''ll need to ignore the warnings
    in your browser before proceeding. After this, we will see a login dialog:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c4da402-e2f2-4f0a-b72f-101788aae39f.png)'
  prefs: []
  type: TYPE_IMG
- en: At this login dialog, you'll need to input the token that you grabbed in the
    aforementioned command.
  prefs: []
  type: TYPE_NORMAL
- en: This is where we use the credentials listed during the K8s installation. We
    can find them at any time by simply using the `config` command `$ kubectl config
    view`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the **Token** option and log in to your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/080257e6-6645-4639-9997-14b8833b5c09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have entered our token, you should see a dashboard like the one
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf003d6e-6874-4688-9547-f145566721f5.png)'
  prefs: []
  type: TYPE_IMG
- en: The main dashboard takes us to a page with not much display at first. There
    is a link to deploy a containerized app that will take you to a GUI for deployment.
    This GUI can be a very easy way to get started deploying apps without worrying
    about the YAML syntax for Kubernetes. However, as your use of containers matures,
    it's a good practice to use the YAML definitions that are checked in to source
    control.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click on the **Nodes** link on the left-hand side menu, you will see
    some metrics on the current cluster nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6012dc3f-9574-4e15-b021-880e9fe99bed.png)'
  prefs: []
  type: TYPE_IMG
- en: At the top, we can see an aggregate of the CPU and memory use followed by a
    listing of our cluster nodes. Clicking on one of the nodes will take us to a page
    with detailed information about that node, its health, and various metrics.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes UI has a lot of other views that will become more useful as we
    start launching real applications and adding configurations to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another service installed by default is Grafana. This tool will give us a dashboard
    to view metrics on the cluster nodes. We can access it using the following syntax
    in a browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The Grafana dashboard should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b4c0b74-8e2e-4589-91b1-d950b7f7b873.png)'
  prefs: []
  type: TYPE_IMG
- en: From the main page, click on the Home drop-down and select Cluster. Here, Kubernetes
    is actually running a number of services. Heapster is used to collect the resource
    usage on the pods and nodes, and stores the information in InfluxDB. The results,
    such as CPU and memory usage, are what we see in the Grafana UI. We will explore
    this in depth in [Chapter 8](da3af5da-0ef5-4aa7-9652-2aed6b8c709e.xhtml), *Monitoring
    and Logging*.
  prefs: []
  type: TYPE_NORMAL
- en: Command line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `kubectl` script has commands for exploring our cluster and the workloads
    running on it. You can find it in the `/kubernetes/client/bin` folder. We will
    be using this command throughout the book, so let''s take a second to set up our
    environment. We can do so by putting the binaries folder on our `PATH`, in the
    following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You may choose to download the `kubernetes` folder outside your home folder,
    so modify the preceding command as appropriate. It is also a good idea to make
    the changes permanent by adding the `export` command to the end of your `.bashrc`
    file in your home directory.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have `kubectl` on our path, we can start working with it. It has
    quite a few commands. Since we have not spun up any applications yet, most of
    these commands will not be very interesting. However, we can explore two commands
    right away.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have already seen the `cluster-info` command during initialization,
    but we can run it again at any time with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Another useful command is `get`. It can be used to see currently running services,
    pods, replication controllers, and a lot more. Here are the three examples that
    are useful right out of the gate:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lists the nodes in our cluster:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Lists cluster events:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can see any services that are running in the cluster, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: To start with, we will only see one service, named `kubernetes`. This service
    is the core API server for the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: For any of the preceding commands, you can always add a `-h` flag on the end
    to understand the intended usage.
  prefs: []
  type: TYPE_NORMAL
- en: Services running on the master
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s dig a little bit deeper into our new cluster and its core services.
    By default, machines are named with the `kubernetes-` prefix. We can modify this
    using `$KUBE_GCE_INSTANCE_PREFIX` before a cluster is spun up. For the cluster
    we just started, the master should be named `kubernetes-master`. We can use the
    `gcloud` command-line utility to SSH into the machine. The following command will
    start an SSH session with the master node. Be sure to substitute your project
    ID and zone to match your environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If you have trouble with SSH via the Google Cloud CLI, you can use the console,
    which has a built-in SSH client. Simply go to the VM instances details page and
    you'll see an SSH option as a column in the `kubernetes-master` listing. Alternatively,
    the VM instance details page has the SSH option at the top.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15ca2d54-f6e8-4982-981a-3e6b5c05ae31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we are logged in, we should get a standard shell prompt. Let''s run the
    `docker` command that filters for `Image` and `Status`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/5c649932-5aca-49e5-8f55-1db402052a08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Even though we have not deployed any applications on Kubernetes yet, we can
    note that there are several containers already running. The following is a brief
    description of each container:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fluentd-gcp`: This container collects and sends the cluster logs file to the
    Google Cloud Logging service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node-problem-detector`: This container is a daemon that runs on every node
    and currently detects issues at the hardware and kernel layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rescheduler`: This is another add-on container that makes sure critical components
    are always running. In cases of low resource availability, it may even remove
    less critical pods to make room.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glbc`: This is another Kubernetes add-on container that provides Google Cloud
    Layer 7 load balancing using the new Ingress capability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-addon-manager`: This component is core to the extension of Kubernetes
    through various add-ons. It also periodically applies any changes to the `/etc/kubernetes/addons`
    directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd-empty-dir-cleanup`: A utility to clean up empty keys in `etcd`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-controller-manager`: This is a controller manager that controls a variety
    of cluster functions, ensuring accurate and up-to-date replication is one of its
    vital roles. Additionally, it monitors, manages, and discovers new nodes. Finally,
    it manages and updates service endpoints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-apiserver`: This container runs the API server. As we explored in the
    Swagger interface, this RESTful API allows us to create, query, update, and remove
    various components of our Kubernetes cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-scheduler`: This scheduler takes unscheduled pods and binds them to nodes
    based on the current scheduling algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd`: This runs the `etcd` software built by CoreOS, and it is a distributed
    and consistent key-value store. This is where the Kubernetes cluster state is
    stored, updated, and retrieved by various components of K8s.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pause`: This container is often referred to as the pod infrastructure container
    and is used to set up and hold the networking namespace and resource limits for
    each pod.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I omitted the `amd64` for many of these names to make this more generic. The
    purpose of the pods remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: To exit the SSH session, simply type `exit` at the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will also show how a few of these services work together
    in the first image, *Kubernetes core architecture*.
  prefs: []
  type: TYPE_NORMAL
- en: Services running on the minions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We could SSH to one of the minions, but since Kubernetes schedules workloads
    across the cluster, we would not see all the containers on a single minion. However,
    we can look at the pods running on all the minions using the `kubectl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we have not started any applications on the cluster yet, we don''t see
    any pods. However, there are actually several system pods running pieces of the
    Kubernetes infrastructure. We can see these pods by specifying the `kube-system`
    namespace. We will explore namespaces and their significance later, but for now,
    the `--namespace=kube-system` command can be used to look at these K8s system
    resources, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The first six lines should look familiar. Some of these are the services we
    saw running on the master, and we will see pieces of these on the nodes. There
    are a few additional services we have not seen yet. The `kube-dns` option provides
    the DNS and service discovery plumbing, `kubernetes-dashboard-xxxx` is the user
    interface for Kubernetes, `l7-default-backend-xxxx` provides the default load
    balancing backend for the new layer-7 load balancing capability, and `heapster-v1.2.0-xxxx` and `monitoring-influx-grafana`
    provide the Heapster database and user interface to monitor resource usage across
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `kube-proxy-kubernetes-minion-group-xxxx`  is the proxy, which directs
    traffic to the proper backing services and pods running on our cluster. The `kube-apiserver`
    validates and configures data for the API objects, which include services, replication
    controllers, pods, and other Kubernetes objects. The `rescheduler` guarantees
    the scheduling of critical system add-ons, given that the cluster has enough available
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we did SSH into a random minion, we would see several containers that run
    across a few of these pods. A sample might look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec7f9677-b44e-4b66-b529-68b35753f39c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Again, we saw a similar lineup of services on the master. The services we did
    not see on the master include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubedns`: This container monitors the service and endpoint resources in Kubernetes
    and synchronizes any changes to DNS lookups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-dnsmasq`: This is another container that provides DNS caching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dnsmasq-metrics`: This provides metric reporting for DNS services in cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l7-defaultbackend`: This is the default backend for handling the GCE L7 load
    balancer and Ingress.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-proxy`: This is the network and service proxy for your cluster. This
    component makes sure that service traffic is directed to wherever your workloads
    are running on the cluster. We will explore this in more depth later in this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`heapster`: This container is for monitoring and analytics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`addon-resizer`: This cluster utility is for scaling containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`heapster_grafana`: This tracks resource usage and monitoring.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`heapster_influxdb`: This time series database is for Heapster data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster-proportional-autoscaler`: This cluster utility is for scaling containers
    in proportion to the cluster size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exechealthz`: This performs health checks on the pods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, I have omitted the `amd64` for many of these names to make this more
    generic. The purpose of the pods remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: Tearing down a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Alright, this is our first cluster on GCE, but let''s explore some other providers.
    To keep things simple, we need to remove the one we just created on GCE. We can
    tear down the cluster with one simple command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Working with other providers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By default, Kubernetes uses the GCE provider for Google Cloud. In order to
    use other cloud providers, we can explore a rapidly expanding tool set of different
    options. Let''s use AWS for this example, where we have two main options: kops
    ([https://github.com/kubernetes/kops](https://github.com/kubernetes/kops)) and
    kube-aws ([https://github.com/kubernetes-incubator/kube-aws](https://github.com/kubernetes-incubator/kube-aws)).
    For reference, the following `KUBERNETES_PROVIDER` are listed in this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Provider** | **KUBERNETES_PROVIDER value** | **Type** |'
  prefs: []
  type: TYPE_TB
- en: '| Google Compute Engine | `gce` | Public cloud |'
  prefs: []
  type: TYPE_TB
- en: '| Google Container Engine | `gke` | Public cloud |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon Web Services | `aws` | Public cloud |'
  prefs: []
  type: TYPE_TB
- en: '| Microsoft Azure | `azure` | Public cloud |'
  prefs: []
  type: TYPE_TB
- en: '| Hashicorp vagrant | `vagrant` | Virtual development environment |'
  prefs: []
  type: TYPE_TB
- en: '| VMware vSphere | `vsphere` | Private cloud/on-premise virtualization |'
  prefs: []
  type: TYPE_TB
- en: '| `libvirt` running CoreOS | `libvirt-coreos` | Virtualization management tool
    |'
  prefs: []
  type: TYPE_TB
- en: '| Canonical Juju (folks behind Ubuntu) | `juju` | OS service orchestration
    tool |'
  prefs: []
  type: TYPE_TB
- en: CLI setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s try setting up the cluster on AWS. As a prerequisite, we need to have
    the AWS CLI installed and configured for our account. The AWS CLI installation
    and configuration documentation can be found at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Installation documentation: [http://docs.aws.amazon.com/cli/latest/userguide/installing.html#install-bundle-other-os](http://docs.aws.amazon.com/cli/latest/userguide/installing.html#install-bundle-other-os)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Configuration documentation: [http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You''ll also need to configure your credentials as recommended by AWS (refer
    to [https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/configuring-sdk.html#specifying-credentials](https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/configuring-sdk.html#specifying-credentials))
    in order to use kops. To get started, you''ll need to first install the CLI tool
    (refer to [https://github.com/kubernetes/kops/blob/master/docs/install.md](https://github.com/kubernetes/kops/blob/master/docs/install.md)).
    If you''re running on Linux, you can install the tools as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: If you're installing this for macOS, you can use `brew update && brew install
    kops` from the command-line Terminal. As a reminder, you'll need `kubectl` installed
    if you haven't already! Check the instructions in the preceding links to confirm
    the installation.
  prefs: []
  type: TYPE_NORMAL
- en: IAM setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order for us to use kops, we''ll need an IAM role created in AWS with the
    following permissions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you''ve created those pieces manually in the AWS GUI, you can run the
    following commands from your PC to set up permissions with the correct access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to use this newly created kops user to interact with the kops tool,
    you need to copy down the `SecretAccessKey` and `AccessKeyID` from the output
    JSON, and then configure the AWS CLI as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We're going to use a gossip-based cluster to bypass a kops configuration requirement
    of public DNS zones. This requires kops 1.6.2 or later, and allows you to create
    a locally registered cluster that requires a name ending in `.k8s.local`. More
    on that in a bit.
  prefs: []
  type: TYPE_NORMAL
- en: If you'd like to explore how to purchase and set up publicly routable DNS  through
    a provider, you can review the available scenarios in the kops documentation here: [https://github.com/kubernetes/kops/blob/master/docs/aws.md#configure-dns](https://github.com/kubernetes/kops/blob/master/docs/aws.md#configure-dns).
  prefs: []
  type: TYPE_NORMAL
- en: Cluster state storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we're building resources in the cloud using configuration management,
    we're going to need to store the representation of our cluster in a dedicated
    S3 bucket. This source of truth will allow us to maintain a single location for
    the configuration and state of our Kubernetes cluster. Please prepend your bucket
    name with a unique value.
  prefs: []
  type: TYPE_NORMAL
- en: You'll need to have `kubectl`, `kops`, the `aws cli`, and IAM credentials set
    up for yourself at this point!
  prefs: []
  type: TYPE_NORMAL
- en: 'Be sure to create your bucket in the `us-east-1` region for now, as kops is
    currently opinionated as to where the bucket belongs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Let's go ahead and set up versioning as well, so you can roll your cluster back
    to previous states in case anything goes wrong. Behold the power of Infrastructure
    as Code!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Creating your cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll go ahead and use the `.k8s.local` settings mentioned previously to simplify
    the DNS setup of the cluster. If you''d prefer, you can also use the name and
    state flags available within kops to avoid using environment variables. Let''s
    prepare the local environment first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s spin up our cluster in Ohio, and verify that we can see that region
    first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! Let''s make some Kubernetes. We''re going to use the most basic kops
    cluster command available, though there are much more complex examples available
    in the documentation ([https://github.com/kubernetes/kops/blob/master/docs/high_availability.md](https://github.com/kubernetes/kops/blob/master/docs/high_availability.md)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: With kops and generally with Kubernetes, everything is going to be created within
    **Auto Scaling groups** (**ASGs**).
  prefs: []
  type: TYPE_NORMAL
- en: Read more about AWS autoscaling groups here—they're essential: [https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html](https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html).
  prefs: []
  type: TYPE_NORMAL
- en: Once you run this command, you'll get a whole lot of configuration output in
    what we call a dry run format. This is similar to the Terraform idea of a Terraform
    plan, which lets you see what you're about to build in AWS and lets you edit the
    output accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the output, you''ll see the following text, which gives you some
    basic suggestions on the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: If you don't have an SSH keypair in your `~/.ssh` directory, you'll need to
    create one. This article will lead you through the steps: [https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/](https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''ve confirmed that you like the look of the output, you can create
    the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give you a lot of output about cluster creation that you can follow
    along with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'As with GCE, the setup activity will take a few minutes. It will stage files
    in **S3** and create the appropriate instances, **Virtual Private Cloud** (**VPC**),
    security groups, and so on in our AWS account. Then, the Kubernetes cluster will
    be set up and started. Once everything is finished and started, we should see
    some options on what comes next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll be able to see instances and security groups, and a VPC will be created
    for your cluster. The `kubectl` context will also be pointed at your new AWS cluster
    so that you can interact with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72363a73-9f3d-4f9e-b610-d159caafa5f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once again, we will SSH into master. This time, we can use the native SSH client
    and the admin user as the AMI for Kubernetes in kops is Debian. We''ll find the
    key files in `/home/<username>/.ssh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have trouble with your SSH key, you can set it manually on the cluster
    by creating a secret, adding it to the cluster, and checking if the cluster requires
    a rolling update:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you''ve gotten into the cluster master, we can look at the containers.
    We''ll use `sudo docker ps --format ''table {{.Image}}t{{.Status}}''` to explore
    the running containers. We should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We can see some of the same containers as our GCE cluster had. However, there
    are several missing. We can see the core Kubernetes components, but the `fluentd-gcp`
    service is missing, as well as some of the newer utilities such as `node-problem-detector`, `rescheduler`, `glbc`, `kube-addon-manager`,
    and `etcd-empty-dir-cleanup`. This reflects some of the subtle differences in
    the `kube-up` script between various public cloud providers. This is ultimately
    decided by the efforts of the large Kubernetes open-source community, but GCP
    often has many of the latest features first.
  prefs: []
  type: TYPE_NORMAL
- en: You also have a command that allows you to check on the state of the cluster
    in `kops validate cluster`, which allows you to make sure that the cluster is
    working as expected. There's also a lot of handy modes that kops provides that
    allow you to do various things with the output, provisioners, and configuration
    of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Other modes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are various other modes to take into consideration, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Build a terraform model**: `--target=terraform`. The terraform model will
    be built in `out/terraform`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Build a cloudformation model**: `--target=cloudformation`. The Cloudformation
    JSON file will be built in `out/cloudformation`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specify the K8s build to run**: `--kubernetes-version=1.2.2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Run nodes in multiple zones**: `--zones=us-east-1b,us-east-1c,us-east-1d`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Run with a HA master**: `--master-zones=us-east-1b,us-east-1c,us-east-1d`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specify the number of nodes**: `--node-count=4`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specify the node size**: `--node-size=m4.large`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specify the master size**: `--master-size=m4.large`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Override the default DNS zone**: `--dns-zone=<my.hosted.zone>`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full list of CLI documentation can be found here: [https://github.com/kubernetes/kops/tree/master/docs/cli](https://github.com/kubernetes/kops/tree/master/docs/cli).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another tool for diagnosing the cluster status is the `componentstatuses` command,
    which will inform you of state of the major Kubernetes moving pieces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Resetting the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You just had a little taste of running the cluster on AWS. For the remainder
    of this book, I will be basing my examples on a GCE cluster. For the best experience
    following along, you can get back to a GCE cluster easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simply tear down the AWS cluster, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'If you omit the `--yes` flag, you''ll get a similar dry run output that you
    can confirm. Then, create a GCE cluster again using the following, and in doing
    so making sure that you''re back in the directory where you installed the Kubernetes
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Investigating other deployment automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you'd like to learn more about other tools for cluster automation, we recommend
    that you visit the kube-deploy repository, which has references to community maintained
    Kubernetes cluster deployment tools.
  prefs: []
  type: TYPE_NORMAL
- en: Visit [https://github.com/kubernetes/kube-deploy](https://github.com/kubernetes/kube-deploy) to
    learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Local alternatives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `kube-up.sh` script and `kops` are pretty handy ways to get started using
    Kubernetes on your platform of choice. However, they're not without flaws and
    can sometimes run aground when conditions are not just so.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, since K8's inception, a number of alternative methods for creating
    clusters have emerged. We'd recommend checking out Minikube in particular, as
    it's an extremely simple and local development environment that you can use to
    test out your Kubernetes configuration.
  prefs: []
  type: TYPE_NORMAL
- en: This project can be found here: [https://github.com/kubernetes/minikube](https://github.com/kubernetes/minikube).
  prefs: []
  type: TYPE_NORMAL
- en: It's important to mention that you're going to need a hypervisor on your machine
    to run Minikube. For Linux, you can use kvm/kvm2, or VirtualBox, and on macOS
    you can run native xhyve or VirtualBox. For Windows, Hyper-V is the default hypervisor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main limitation for this project is that it only runs a single node, which
    limits our exploration of certain advanced topics that require multiple machines.
    Minikube is a great resource for simple or local development however, and can
    be installed very simply on your Linux VM with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Or install it on macOS with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll cover how to get started with Minikube with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'You can create a sample deployment quite simply:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Once you have your cluster and service up and running, you can interact with
    it simply by using the `kubectl` tool and the `context` command. You can get to
    the Minikube dashboard with `minikube dashboard`.
  prefs: []
  type: TYPE_NORMAL
- en: Minikube is powered by localkube ([https://github.com/kubernetes/minikube/tree/master/pkg/localkube](https://github.com/kubernetes/minikube/tree/master/pkg/localkube))
    and libmachine ([https://github.com/docker/machine/tree/master/libmachine](https://github.com/docker/machine/tree/master/libmachine)).
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we've already referenced a number of managed services, including
    GKE, EKS, and Microsoft **Azure Container Service** (**ACS**), which provide an
    automated installation and some managed cluster operations. We will look at a
    demos of these in [Chapter 14](f805c680-0926-43dc-86db-662abeeccdb2.xhtml), *Hardening
    Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, there is the option to start from scratch. Luckily, starting in 1.4,
    the Kubernetes team has put a major focus on simplifying  the cluster setup process.
    To that end, they have introduced kubeadm for Ubuntu 16.04, CentOS 7, and HypriotOS
    v1.0.1+.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a quick look at spinning up a cluster on AWS from scratch using the
    kubeadm tool.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will need to provision our cluster master and nodes beforehand. For the moment,
    we are limited to the operating systems and version listed earlier. Additionally,
    it is recommended that you have at least 1 GB of RAM. All the nodes must have
    network connectivity to one another.
  prefs: []
  type: TYPE_NORMAL
- en: For this walkthrough, we will need one t2.medium (master node) and three t2.mirco
    (nodes) sized instances on AWS. These instance have burstable CPU and come with
    the minimum 1 GB of RAM that's required. We will need to create one master and
    three worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also need to create some security groups for the cluster. The following
    ports are needed for the master:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Protocol** | **Port range** | **Source** |'
  prefs: []
  type: TYPE_TB
- en: '| All traffic | All | All | {This SG ID (Master SG)} |'
  prefs: []
  type: TYPE_TB
- en: '| All traffic | All | All | {Node SG ID} |'
  prefs: []
  type: TYPE_TB
- en: '| SSH | TCP | `22` | {Your Local Machine''s IP} |'
  prefs: []
  type: TYPE_TB
- en: '| HTTPS | TCP | `443` | {Range allowed to access K8s API and UI} |'
  prefs: []
  type: TYPE_TB
- en: 'The following table shows the port''s node security groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Protocol** | **Port range** | **Source** |'
  prefs: []
  type: TYPE_TB
- en: '| All traffic | All | All | {Master SG ID} |'
  prefs: []
  type: TYPE_TB
- en: '| All traffic | All | All | {This SG ID (Node SG)} |'
  prefs: []
  type: TYPE_TB
- en: '| SSH | TCP | `22` | {Your Local Machine''s IP} |'
  prefs: []
  type: TYPE_TB
- en: Once you have these SGs, go ahead and spin up four instances (one t2.medium
    and three t2.mircos) using Ubuntu 16.04\. If you are new to AWS, refer to the
    documentation on spinning up EC2 instances at the following URL: **[http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/LaunchingAndUsingInstances.html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/LaunchingAndUsingInstances.html).**
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to identify the t2.medium instance as the master and associate the master
    security group. Name the other three as nodes and associate the node security
    group with those.
  prefs: []
  type: TYPE_NORMAL
- en: These steps are adapted from the walk-through in the manual. For more information
    or to work with an alternative to Ubuntu, refer to [https://kubernetes.io/docs/getting-started-guides/kubeadm/](https://kubernetes.io/docs/getting-started-guides/kubeadm/).
  prefs: []
  type: TYPE_NORMAL
- en: Installing Kubernetes components (kubelet and kubeadm)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we will need to SSH into all four of the instances and install the Kubernetes
    components.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the root user, perform the following steps on all four instances:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the packages and install the `apt-transport-https` package so that we
    can download from sources that use HTTPS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the Google Cloud public key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s set up the repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll need to make sure that the `cgroup` driver used by the `kubelet` on
    the master node is configured correctly to work with Docker. Make sure you''re
    on the master node, then run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'If these items don''t match, you''re going to need to change the kubelet configuration
    to match the Docker driver. Running `sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g"
    /etc/systemd/system/kubelet.service.d/10-kubeadm.conf ` should fix the settings,
    or you can manually open the `systemd` file and add the correct flag to the appropriate
    environment. After that''s complete, restart the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Setting up a master
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On the instance you have previously chosen as master, we will run master initialization.
    Again, as the root, run the following command, and you should see the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'To start using your cluster, you need to run as a regular user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: You should now deploy a pod network to the cluster. Run `kubectl apply -f [podnetwork].yaml` with
    one of the options listed at [https://kubernetes.io/docs/concepts/cluster-administration/addons/](https://kubernetes.io/docs/concepts/cluster-administration/addons/).
  prefs: []
  type: TYPE_NORMAL
- en: You can now join any number of machines by running the following on each node
  prefs: []
  type: TYPE_NORMAL
- en: 'as root:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Note that initialization can only be run once, so if you run into problems,
    you'll need to use `kubeadm reset`.
  prefs: []
  type: TYPE_NORMAL
- en: Joining nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After a successful initialization, you will get a `join` command that can be
    used by the nodes. Copy this down for the join process later on. It should look
    similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The token is used to authenticate cluster nodes, so make sure to store it somewhere
    securely for future use.
  prefs: []
  type: TYPE_NORMAL
- en: Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our cluster will need a networking layer for the pods to communicate on. Note
    that kubeadm requires a CNI compatible network fabric. The list of plugins currently
    available can be found here: **[http://kubernetes.io/docs/admin/addons/](http://kubernetes.io/docs/admin/addons/)**.
  prefs: []
  type: TYPE_NORMAL
- en: For our example, we will use calico. We will need to create the calico components
    on our cluster using the following `yaml`. For convenience, you can download it
    here: **[http://docs.projectcalico.org/v1.6/getting-started/kubernetes/installation/hosted/kubeadm/calico.yaml](http://docs.projectcalico.org/v1.6/getting-started/kubernetes/installation/hosted/kubeadm/calico.yaml)**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have this file on your master, create the components with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Give this a minute to run setup and then list the `kube-system` nodes in order
    to check this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get a listing similar to the following one with three new calico
    pods and one completed job that is not shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00c12709-5b01-475e-98f8-23e9b5a11b1e.png)'
  prefs: []
  type: TYPE_IMG
- en: Calico setup
  prefs: []
  type: TYPE_NORMAL
- en: Joining the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we need to run the `join` command we copied earlier, on each of our node
    instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you''ve finished that, you should be able to see all nodes from the master
    by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'If all went well, this will show three nodes and one master, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cff5aa3d-ca69-4360-9d4b-7338aae22b1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We took a very brief look at how containers work and how they lend themselves
    to the new architecture patterns in microservices. You should now have a better
    understanding of how these two forces will require a variety of operations and
    management tasks, and how Kubernetes offers strong features to address these challenges.
    We created two different clusters on both GCE and AWS, and explored the startup
    script as well as some of the built-in features of Kubernetes. Finally, we looked
    at the alternatives to the `kube-up` script in kops, and tried our hand at manual
    cluster configuration with the kubeadm tool on AWS with Ubuntu 16.04.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the core concept and abstractions K8s provides
    to manage containers and full application stacks. We will also look at basic scheduling,
    service discovery, and health checking.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Name three places where you can easily deploy a Kubernetes cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are other types of pre-existing virtualization technologies that predate
    containers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name as many cgroup controls as you can!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the reasons why enabling CI/CD with containers is so important
    to organizations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What prerequisites are required to get a Kubernetes cluster up and running on
    AWS or GCE?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Name four services running on the Kubernetes master nodes. Hint: these are
    containers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some alternatives to the `kube-up.sh` script?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the tool used for building a Kubernetes cluster from scratch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Want more information on DevOps practices on Kubernetes? Check out *DevOps with
    Kubernetes*: [https://www.packtpub.com/virtualization-and-cloud/devops-kubernetes](https://www.packtpub.com/virtualization-and-cloud/devops-kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: You can also read about different applications and automation approaches with
    the *Kubernetes Cookbook*: [https://www.packtpub.com/virtualization-and-cloud/kubernetes-cookbook](https://www.packtpub.com/virtualization-and-cloud/kubernetes-cookbook).
  prefs: []
  type: TYPE_NORMAL
