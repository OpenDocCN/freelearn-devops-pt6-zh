<html><head></head><body>
		<div>
			<div id="_idContainer015" class="Content">
			</div>
		</div>
		<div id="_idContainer016" class="Content">
			<h1 id="_idParaDest-14">1. <a id="_idTextAnchor015"/>Introduction to containers and Kubernetes</h1>
		</div>
		<div id="_idContainer024" class="Content">
			<p>Kubernetes has become the leading standard in container orchestration. Since its inception in 2014, Kubernetes has gained tremendous popularity. It has been adopted by start-ups as well as major enterprises, with all major public cloud vendors offering a managed Kubernetes service.</p>
			<p>Kubernetes builds upon the success of the Docker container revolution. Docker is both a company and the name of a technology. Docker as a technology is the most common way of creating and running software containers, called Docker containers. A container is a way of packaging software that makes it easy to run that software on any platform, ranging from your laptop to a server in a datacenter to a cluster running in the public cloud.</p>
			<p>Although the core technology is open source, the Docker company focuses on reducing complexity for developers through a number of commercial offerings.</p>
			<p>Kubernetes takes containers to the next level. Kubernetes is a container orchestrator. A container orchestrator is a software platform that makes it easy to run many thousands of containers on top of thousands of machines. It automates a lot of the manual tasks required to deploy, run, and scale applications. The orchestrator takes care of scheduling the right container to run on the right machine. It also takes care of health monitoring and failover, as well as scaling your deployed application.</p>
			<p>The container technology Docker uses and Kubernetes are both open-source software projects. Open-source software allows developers from many companies to collaborate on a single piece of software. Kubernetes itself has contributors from companies such as Microsoft, Google, Red Hat, VMware, and many others.</p>
			<p>The three major public cloud platforms—Azure, <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), and <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>)—all offer a managed Kubernetes service. They attract a lot of interest in the market since the virtually unlimited compute power and the ease of use of these managed services make it easy to build and deploy large-scale applications.</p>
			<p><strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>) is Azure's managed service for Kubernetes. It reduces the complexity of building and managing Kubernetes clusters. In this book, you will learn how to use AKS to run your applications. Each chapter will introduce new concepts, which you will apply through the many examples in this book.</p>
			<p>As a user, however, it is still very useful to understand the technologies that underpin AKS. We will explore these foundations in this chapter. You will learn about Linux processes and how they are related to Docker and containers. You will see how various processes fit nicely into containers and how containers fit nicely into Kubernetes. </p>
			<p>This chapter introduces fundamental Docker concepts so that you can begin your Kubernetes journey. This chapter also briefly introduces the basics that will help you build containers, implement clusters, perform container orchestration, and troubleshoot applications on AKS. Having cursory knowledge of what's in this chapter will demystify much of the work needed to build your authenticated, encrypted, and highly scalable applications on AKS. Over the next few chapters, you will gradually build scalable and secure applications.</p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>The software evolution that brought us here</li>
				<li>The fundamentals of containers</li>
				<li>The fundamentals of Kubernetes</li>
				<li>The fundamentals of AKS</li>
			</ul>
			<p>The aim of this chapter is to introduce the essentials rather than to provide a thorough information source describing Docker and Kubernetes. To begin with, we'll first take a look at how software has evolved to get us to where we are now.</p>
			<h2 id="_idParaDest-15"><a id="_idTextAnchor016"/>The software evolution that brought us here</h2>
			<p>There are two major software development evolutions that enabled the popularity of containers and Kubernetes. One is the adoption of a microservices architectural style. Microservices allow an application to be built from a collection of small services that each serve a specific function. The other evolution that enabled containers and Kubernetes is DevOps. DevOps is a set of cultural practices that allows people, processes, and tools to build and release software faster, more frequently, and more reliably.</p>
			<p>Although you can use both containers and Kubernetes without using either microservices or DevOps, the technologies are most widely adopted for deploying microservices using DevOps methodologies.</p>
			<p>In this section, we'll discuss both evolutions, starting with microservices.</p>
			<h3 id="_idParaDest-16"><a id="_idTextAnchor017"/>Microservices</h3>
			<p>Software development has drastically evolved over time. Initially, software was developed and run on a single system, typically a mainframe. A client could connect to the mainframe through a terminal, and only through that terminal. This changed when computer networks became common when the client-server programming model emerged. A client could connect remotely to a server and even run part of the application on their own system while connecting to the server to retrieve the data the application required.</p>
			<p>The client-server programming model has evolved toward distributed systems. Distributed systems are different from the traditional client-server model as they have multiple different applications running on multiple different systems, all interconnected.</p>
			<p>Nowadays, a microservices architecture is common when developing distributed systems. A microservices-based application consists of a group of services that work together to form the application, while the individual services themselves can be built, tested, deployed, and scaled independently of each other. The style has many benefits but also has several disadvantages.</p>
			<p>A key part of a microservices architecture is the fact that each individual service serves one and only one core function. Each service serves a single-bound business function. Different services work together to form the complete application. Those services work together over network communication, commonly using HTTP REST APIs or gRPC:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B17338_01_01.jpg" alt="Evolution from mainframe to client-server, to distributed systems to microservices"/>
				</div>
			</div>
			<p class="figure">Figure 1.1: A standard microservices architecture</p>
			<p>This architectural approach is commonly adopted by applications that run using containers and Kubernetes. Containers are used as the packaging format for the individual services, while Kubernetes is the orchestrator that deploys and manages the different services running together.</p>
			<p>Before we dive into container and Kubernetes specifics, let's first explore the benefits and downsides of adopting microservices.</p>
			<h3 id="_idParaDest-17"><a id="_idTextAnchor018"/>Advantages of running microservices</h3>
			<p>There are several advantages to running a microservices-based application. The first is the fact that each service is independent of the other services. The services are designed to be small enough (hence micro) to handle the needs of a business domain. As they are small, they can be made self-contained and independently testable, and so are independently releasable.</p>
			<p>This leads to the benefit that each microservice is independently scalable as well. If a certain part of the application is getting more demand, that part of the application can be scaled independently from the rest of the application.</p>
			<p>The fact that services are independently scalable also means that they are independently deployable. There are multiple deployment strategies when it comes to microservices. The most popular are rolling deployments and blue/green deployments.</p>
			<p>With a rolling upgrade, a new version of the service is deployed only to a part of the application. This new version is carefully monitored and gradually gets more traffic if the service remains healthy. If something goes wrong, the previous version is still running, and traffic can easily be cut over.</p>
			<p>With a blue/green deployment, you deploy the new version of the service in isolation. Once the new version of the service is deployed and tested, you cut over 100% of the production traffic to the new version. This allows for a clean transition between service versions.</p>
			<p>Another benefit of the microservices architecture is that each service can be written in a different programming language. This is described as polyglot—the ability to understand and use multiple languages. For example, the front-end service can be developed in a popular JavaScript framework, the back end can be developed in C#, and the machine learning algorithm can be developed in Python. This allows you to select the right language for the right service and allows developers to use the languages they are most familiar with.</p>
			<h3 id="_idParaDest-18"><a id="_idTextAnchor019"/>Disadvantages of running microservices</h3>
			<p>There's a flip side to every coin, and the same is true for microservices. While there are multiple advantages to a microservices-based architecture, this architecture has its downsides as well.</p>
			<p>Microservices designs and architectures require a high degree of software development maturity in order to be implemented correctly. Architects who understand the domain very well must ensure that each service is bounded and that different services are cohesive. Since services are independent of each other and versioned independently, the software contract between these different services is important to get right.</p>
			<p>Another common issue with a microservices design is the added complexity when it comes to monitoring and troubleshooting such an application. Since different services make up a single application, and those different services run on multiple servers, both logging and tracing such an application is a complicated endeavor.</p>
			<p>Linked to the disadvantages mentioned before is that, typically, in microservices, you need to build more fault tolerance into your application. Due to the dynamic nature of the different services in an application, faults are more likely to happen. In order to guarantee application availability, it is important to build fault tolerance into the different microservices that make up an application. Implementing patterns such as retry logic or circuit breakers is critical to avoid a single fault causing application downtime.</p>
			<p>In this section, you learned about microservices, their benefits, and their disadvantages. Often linked to microservices, but a separate topic, is the DevOps movement. We will explore what DevOps means in the next section.</p>
			<h3 id="_idParaDest-19"><a id="_idTextAnchor020"/>DevOps</h3>
			<p>DevOps literally means the combination of development and operations. More specifically, DevOps is the union of people, processes, and tools to deliver software faster, more frequently, and more reliably. DevOps is more about a set of cultural practices than about any specific tools or implementations. Typically, DevOps spans four areas of software development: planning, developing, releasing, and operating software.</p>
			<h4>Note</h4>
			<p class="callout">Many definitions of DevOps exist. The authors have adopted this definition, but you as a reader are encouraged to explore different definitions in the literature around DevOps.</p>
			<p>The DevOps culture starts with planning. In the planning phase of a DevOps project, the goals of a project are outlined. These goals are outlined both at a high level (called an epic) and at a lower level (as features and tasks). The different work items in a DevOps project are captured in the feature backlog. Typically, DevOps teams use an agile planning methodology working in programming sprints. Kanban boards are often used to represent project status and to track work. As a task changes status from <em class="italics">to do</em> to <em class="italics">doing</em> to <em class="italics">done</em>, it moves from left to right on a Kanban board.</p>
			<p>When work is planned, actual development can be done. Development in a DevOps culture isn't only about writing code but also about testing, reviewing, and integrating code with team members. A version control system such as Git is used for different team members to share code with each other. An automated <strong class="bold">continuous integration</strong> (<strong class="bold">CI</strong>) tool is used to automate most manual tasks such as testing and building code.</p>
			<p>When a feature is code-complete, tested, and built, it is ready to be delivered. The next phase in a DevOps project can start delivery. A <strong class="bold">continuous delivery</strong> (<strong class="bold">CD</strong>) tool is used to automate the deployment of software. Typically, software is deployed to different environments, such as testing, quality assurance, and production. A combination of automated and manual gates is used to ensure quality before moving to the next environment.</p>
			<p>Finally, when a piece of software is running in production, the operations phase can start. This phase involves the maintaining, monitoring, and supporting of an application in production. The end goal is to operate an application reliably with as little downtime as possible. Any issues are to be identified as proactively as possible. Bugs in the software will be tracked in the backlog.</p>
			<p>The DevOps process is an iterative process. A single team is never in a single phase of the process. The whole team is continuously planning, developing, delivering, and operating software.</p>
			<p>Multiple tools exist to implement DevOps practices. There are point solutions for a single phase, such as Jira for planning or Jenkins for CI and CD, as well as complete DevOps platforms, such as GitLab. Microsoft operates two solutions that enable customers to adopt DevOps practices: Azure DevOps and GitHub. Azure DevOps is a suite of services to support all phases of the DevOps process. GitHub is a separate platform that enables DevOps software development. GitHub is known as the leading open-source software development platform, hosting over 40 million open-source projects.</p>
			<p>Both microservices and DevOps are commonly used in combination with containers and Kubernetes. Now that we've had this introduction to microservices and DevOps, we'll continue this first chapter with the fundamentals of containers and then the fundamentals of Kubernetes.</p>
			<h3 id="_idParaDest-20"><a id="_idTextAnchor021"/>Fundamentals of containers</h3>
			<p>A form of container technology has existed in the Linux kernel since the 1970s. The technology powering today's containers, called <strong class="bold">cgroups</strong> (abbreviated from <strong class="bold">control groups</strong>), was introduced into the Linux kernel in 2006 by Google. The Docker company popularized the technology in 2013 by introducing an easy developer workflow. Although the name Docker can refer to both the company as well as the technology, most commonly, though, we use Docker to refer to the technology.</p>
			<h4>Note</h4>
			<p class="callout">Although the Docker technology is a popular way to build and run containers, it is not the only way to build and run them. Many alternatives exist for either building or running containers. One of those alternatives is containerd, which is a container runtime also used by Kubernetes.</p>
			<p>Docker as a technology is both a packaging format and a container runtime. Packaging is a process that allows an application to be packaged together with its dependencies, such as binaries and runtime. The runtime points at the actual process of running the container images.</p>
			<p>There are three important pieces in Docker's architecture: the client, the daemon, and the registry:</p>
			<ul>
				<li>The Docker client is a client-side tool that you use to interact with the Docker daemon, running locally or remotely. </li>
				<li>The Docker daemon is a long-running process that is responsible for building container images and running containers. The Docker daemon can run on either your local machine or a remote machine. </li>
				<li>A Docker registry is a place to store Docker images. There are public registries such as Docker Hub that contain public images, and there are private registries such as <strong class="bold">Azure Container Registry</strong> (<strong class="bold">ACR</strong>) that you can use to store your own private images. The Docker daemon can pull images from a registry if images are not available locally:</li>
			</ul>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B17338_01_02.jpg" alt="Three important componentsin Docker architecture: the client, the daemon, and the registry"/>
				</div>
			</div>
			<p class="figure">Figure 1.2: Fundamentals of Docker architecture</p>
			<p>You can experiment with Docker by creating a free Docker account at Docker Hub (<a href="https://hub.docker.com/">https://hub.docker.com/</a>) and using that login to open Docker Labs (<a href="https://labs.play-with-docker.com/">https://labs.play-with-docker.com/</a>). This will give you access to an environment with Docker pre-installed that is valid for 4 hours. We will be using Docker Labs in this section as we build our own container and image.</p>
			<h4>Note</h4>
			<p class="callout">Although we are using the browser-based Docker Labs in this chapter to introduce Docker, you can also install Docker on your local desktop or server. For workstations, Docker has a product called Docker Desktop (<a href="https://azure.microsoft.com/resources/designing-distributed-systems/">https://www.docker.com/products/docker-desktop</a>) that is available for Windows and Mac to create Docker containers locally. On servers—both Windows and Linux—Docker is also available as a runtime for containers.</p>
			<h3 id="_idParaDest-21"><a id="_idTextAnchor022"/>Container images</h3>
			<p>To start a new container, you need an image. An image contains all the software you need to run within your container. Container images can be stored locally on your machine, as well as in a container registry. There are public registries, such as the public Docker Hub (<a href="https://hub.docker.com/">https://hub.docker.com/</a>), or private registries, such as ACR. When you, as a user, don't have an image locally on your PC, you can pull an image from a registry using the <strong class="inline">docker pull</strong> command.</p>
			<p>In the following example, we will pull an image from the public Docker Hub repository and run the actual container. You can run this example in Docker Labs, which we introduced in the previous section, by following these instructions:</p>
			<p class="snippet">#First, we will pull an image</p>
			<p class="snippet">docker pull docker/whalesay</p>
			<p class="snippet">#We can then look at which images are stored locally</p>
			<p class="snippet">docker images</p>
			<p class="snippet">#Then we will run our container</p>
			<p class="snippet">docker run docker/whalesay cowsay boo</p>
			<p>The output of these commands will look similar to <em class="italics">Figure 1.3</em>:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B17338_01_03.jpg" alt="Pulling an image from the public Docker Hub repository and running the actual container"/>
				</div>
			</div>
			<p class="figure">Figure 1.3: An example of running containers in Docker Labs</p>
			<p>What happened here is that Docker first pulled your image in multiple parts and stored it locally on the machine it was running on. When you ran the actual application, it used that local image to start a container. If we look at the commands in detail, you will see that <strong class="inline">docker pull</strong> took in a single parameter, <strong class="inline">docker/whalesay</strong>. If you don't provide a private container registry, Docker will look in the public Docker Hub for images, which is where Docker pulled this image from. The <strong class="inline">docker run</strong> command took in a couple of arguments. The first argument was <strong class="inline">docker/whalesay</strong>, which is the reference to the image. The next two arguments, <strong class="inline">cowsay boo</strong>, are commands that were passed to the running container to execute.</p>
			<p>In the previous example, you learned that it is possible to run a container without building an image first. It is, however, very common that you will want to build your own images. To do this, you use a Dockerfile. A Dockerfile contains steps that Docker will follow to start from a base image and build your image. These instructions can range from adding files to installing software or setting up networking. </p>
			<p>In the next example, you will build a custom Docker image. This custom image will display inspirational quotes in the whale output. The following Dockerfile will be used to generate this custom image. You will create it in your Docker playground:</p>
			<p class="snippet">FROM docker/whalesay:latest</p>
			<p class="snippet">RUN apt-get -y -qq update</p>
			<p class="snippet">RUN apt-get install -qq -y fortunes</p>
			<p class="snippet">CMD /usr/games/fortune -a | cowsay</p>
			<p>There are four lines in this Dockerfile. The first one will instruct Docker on which image to use as a source image for this new image. The next two steps are commands that are run to add new functionality to our image, in this case, updating your <strong class="inline">apt</strong> repository and installing an application called <strong class="inline">fortunes</strong>. The <strong class="inline">fortunes</strong> application is a small command-line tool that generates inspirational quotes. We will use that to include quotes in the output rather than user input. Finally, the <strong class="inline">CMD</strong> command tells Docker which command to execute when a container based on this image is run.</p>
			<p>You typically save a Dockerfile in a file called <strong class="inline">Dockerfile</strong>, without an extension. To build an image, you need to execute the <strong class="inline">docker build</strong> command and point it to the Dockerfile you created. In building the Docker image, the Docker daemon will read the Dockerfile and execute the different steps in the Dockerfile. This command will also output the steps it took to run a container and build your image. Let's walk through a demo of building an image.</p>
			<p>In order to create this Dockerfile, open up a text editor via the <strong class="inline">vi Dockerfile</strong> command. <strong class="bold">vi</strong> is an advanced text editor on the Linux command line. If you are not familiar with it, let's walk through how you would enter the text in there:</p>
			<ol>
				<li>After you've opened vi, hit the <em class="italics">I</em> key to enter insert mode.</li>
				<li>Then, either copy and paste or type the four code lines.</li>
				<li>Afterward, hit the <em class="italics">Esc</em> key, and type <strong class="inline">:wq!</strong> to write <strong class="inline">(w)</strong> your file and quit <strong class="inline">(q)</strong> the text editor.</li>
			</ol>
			<p>The next step is to execute <strong class="inline">docker build</strong> to build the image. We will add a final bit to that command, namely adding a tag to our image so we can call it by a meaningful name. To build the image, you will use the <strong class="inline">docker build -t smartwhale.</strong> command (don't forget to add the final period here).</p>
			<p>You will now see Docker execute a number of steps—four in this case—to build the image. After the image is built, you can run your application. To run your container, you run <strong class="inline">docker run smartwhale</strong>, and you should see an output similar to <em class="italics">Figure 1.4</em>. However, you will probably see a different smart quote. This is due to the <strong class="inline">fortunes</strong> application generating different quotes. If you run the container multiple times, you will see different quotes appear, as shown in <em class="italics">Figure 1.4</em>:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B17338_01_04.jpg" alt="Running a custom container using the docker run command"/>
				</div>
			</div>
			<p class="figure">Figure 1.4: Running a custom container</p>
			<p>That concludes our overview and demo of containers. In this section, you started with an existing container image and launched it on Docker Labs. Afterward, you took that a step further and built your own container image, then started containers using that image. You have now learned what it takes to build and run a container. In the next section, we will cover Kubernetes. Kubernetes allows you to run multiple containers at scale.</p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor023"/>Kubernetes as a container orchestration platform</h2>
			<p>Building and running a single container seems easy enough. However, things can get complicated when you need to run multiple containers across multiple servers. This is where a container orchestrator can help. A container orchestrator takes care of scheduling containers to be run on servers, restarting containers when they fail, moving containers to a new host when a host becomes unhealthy, and much more.</p>
			<p>The current leading orchestration platform is Kubernetes (<a href="https://kubernetes.io/">https://kubernetes.io/</a>). Kubernetes was inspired by Google's Borg project, which, by itself, was running millions of containers in production.</p>
			<p>Kubernetes takes a declarative approach to orchestration; that is, you specify what you need, and Kubernetes takes care of deploying the workload you specified. You don't need to start these containers manually yourself anymore, as Kubernetes will launch the containers you specified.</p>
			<h4>Note</h4>
			<p class="callout">Although Kubernetes used to support Docker as the container runtime, that support has been deprecated in Kubernetes version 1.20. In AKS, <strong class="bold">containerd</strong> has become the default container runtime starting with Kubernetes 1.19. </p>
			<p>Throughout the book, you will build multiple examples that run containers in Kubernetes, and you will learn more about the different objects in Kubernetes. In this introductory chapter, you will learn three elementary objects in Kubernetes that you will likely see in every application: a pod, a deployment, and a service.</p>
			<h3 id="_idParaDest-23"><a id="_idTextAnchor024"/>Pods in Kubernetes</h3>
			<p>A <strong class="bold">pod</strong> in Kubernetes is the essential scheduling element. A pod is a group of one or more containers. This means a pod can contain either a single container or multiple containers. When creating a pod with a single container, you can use the terms container and pod interchangeably. However, the term pod is still preferred and is the term used throughout this book.</p>
			<p>When a pod contains multiple containers, these containers share the same file system and the same network namespace. This means that when a container that is part of a pod writes a file, other containers in that same pod can read that file as well. This also means that all containers in a pod can communicate with each other using localhost networking.</p>
			<p>In terms of design, you should only put containers that need to be tightly integrated in the same pod. Imagine the following situation: you have an old web application that does not support HTTPS. You want to upgrade that application to support HTTPS. You could create a pod that contains your old web application and includes another container that would do <strong class="bold">Transport Layer Security</strong> (<strong class="bold">TLS</strong>) offloading for that application, as described in <em class="italics">Figure 1.5</em>. Users would connect to your application using HTTPS, while the container in the middle converts HTTPS traffic to HTTP:</p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B17338_01_05.jpg" alt="Using two containers in a single pod to support HTTPS traffic"/>
				</div>
			</div>
			<p class="figure">Figure 1.5: An example of a multi-container pod that does HTTPS offloading</p>
			<h4>Note</h4>
			<p class="callout">This design principle is known as a sidecar. Microsoft has a free e-book available that describes multiple multi-container pod designs and designing distributed systems (<a href="https://azure.microsoft.com/resources/designing-distributed-systems/">https://azure.microsoft.com/resources/designing-distributed-systems/</a>).</p>
			<p>A pod, whether it be a single- or multi-container pod, is an ephemeral resource. This means that a pod can be terminated at any point and restarted on another node. When this happens, the state that was stored in that pod will be lost. If you need to store state in your application, you either need to store that state in external storage, such as an external disk or a file share, or store the state outside of Kubernetes in an external database.</p>
			<h3 id="_idParaDest-24"><a id="_idTextAnchor025"/>Deployments in Kubernetes</h3>
			<p>A <strong class="bold">deployment</strong> in Kubernetes provides a layer of functionality around pods. It allows you to create multiple pods from the same definition and to easily perform updates to your deployed pods. A deployment also helps with scaling your application, and potentially even autoscaling your application.</p>
			<p>Under the hood, a deployment creates a <strong class="bold">ReplicaSet</strong>, which in turn will create the replica pods you requested. A ReplicaSet is another object in Kubernetes. The purpose of a ReplicaSet is to maintain a stable set of replica pods running at any given time. If you perform updates on your deployment, Kubernetes will create a new ReplicaSet that will contain the updated pods. By default, Kubernetes will do a rolling upgrade to the new version. This means that it will start a few new pods, verify those are running correctly, and if so, then Kubernetes will terminate the old pods and continue this loop until only new pods are running:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B17338_01_06.jpg" alt="Relationship showing that a deployment creates a replicaset, which in turn creates multiple pods"/>
				</div>
			</div>
			<p class="figure">Figure1.6: The relationship between deployments, ReplicaSets, and pods</p>
			<h3 id="_idParaDest-25"><a id="_idTextAnchor026"/>Services in Kubernetes</h3>
			<p>A <strong class="bold">service</strong> in Kubernetes is a network-level abstraction. This allows you to expose multiple pods under a single IP address and a single DNS name.</p>
			<p>Each pod in Kubernetes has its own private IP address. You could theoretically connect to your applications using this private IP address. However, as mentioned before, Kubernetes pods are ephemeral, meaning they can be terminated and moved, which would change their IP address. By using a service, you can connect to your applications using a single IP address. When a pod moves from one node to another, the service ensures that traffic is routed to the correct endpoint. If there are multiple pods serving traffic behind one service, that traffic will be load balanced between the different pods.</p>
			<p>In this section, we have introduced Kubernetes and three essential objects with Kubernetes. In the next section, we'll introduce AKS.</p>
			<h3 id="_idParaDest-26"><a id="_idTextAnchor027"/>Azure Kubernetes Service</h3>
			<p>AKS makes creating and managing Kubernetes clusters easier.</p>
			<p>A typical Kubernetes cluster consists of a number of master nodes and a number of worker nodes. A node within Kubernetes is equivalent to a server or a <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>). The master nodes contain the Kubernetes API and a database that contains the cluster state. The worker nodes are the machines that run your actual workload.</p>
			<p>AKS makes it easier to create a cluster. When you create an AKS cluster, AKS sets up the Kubernetes master for you. AKS will then create one or more <strong class="bold">virtual machine scale sets</strong> (<strong class="bold">VMSS</strong>) in your subscription and turn the VMs in these VMSSs into worker nodes of your Kubernetes cluster in your network. In AKS, you have the option to either use a free Kubernetes control plane or pay for a control plane that comes with a financially backed SLA. In either case, you also need to pay for the VMs hosting your worker nodes:</p>
			<p class="figure"> </p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B17338_01_07.jpg" alt="Scheduling pods on nodes in AKS"/>
				</div>
			</div>
			<p class="figure">Figure 1.7: Scheduling of pods in AKS</p>
			<p>Within AKS, services running on Kubernetes are integrated with Azure Load Balancer and Kubernetes Ingresses can be integrated with Azure Application Gateway. The Azure Load Balancer is a layer-4 network load balancer service; Application Gateway is a layer-7 HTTP-based load balancer. The integration between Kubernetes and both services means that when you create a service or Ingress in Kubernetes, Kubernetes will create a rule in an Azure Load Balancer or Azure Application Gateway respectively. Azure Load Balancer or Application Gateway will then route the traffic to the right node in your cluster that hosts your pod.</p>
			<p>Additionally, AKS adds a number of functionalities that make it easier to manage a cluster. AKS contains logic to upgrade clusters to newer Kubernetes versions. It also can easily scale your clusters, by either adding or removing nodes to the cluster.</p>
			<p>AKS also comes with integration options that make operations easier. AKS clusters can be configured with integration with <strong class="bold">Azure Active Directory</strong> (<strong class="bold">Azure AD</strong>) to make managing identities and <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>) straightforward. RBAC is the configuration process that defines which users get access to resources and which actions they can take against those resources. AKS can also easily be integrated into Azure Monitor for containers, which makes monitoring and troubleshooting your applications simpler. You will learn about all these capabilities throughout this book.</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor028"/>Summary</h2>
			<p>In this chapter, you learned about the concepts of containers and Kubernetes. You ran a number of containers, starting with an existing image and then using an image you built yourself. After that demo, you were introduced to three essential Kubernetes objects: the pod, the deployment, and the service.</p>
			<p>This provides the context for the remaining chapters, where you will deploy containerized applications using Microsoft AKS. You will see how the AKS offering from Microsoft streamlines deployment by handling many of the management and operational tasks that you would have to do yourself if you managed and operated your own Kubernetes infrastructure.</p>
			<p>In the next chapter, you will use the Azure portal to create your first AKS cluster.</p>
		</div>
	</body></html>