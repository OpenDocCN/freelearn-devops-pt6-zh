<html><head></head><body>
		<div id="_idContainer015">
			<h1 class="chapter-number" id="_idParaDest-31"><a id="_idTextAnchor031"/>2</h1>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor032"/>Kubernetes Architecture</h1>
			<p>Understanding Kubernetes architecture is crucial to properly leverage its capabilities. In this chapter, we will go over the main components and concepts that make up a Kubernetes cluster. Getting familiar with these building blocks will allow you to understand how Kubernetes works under <span class="No-Break">the hood.</span></p>
			<p>We will start by looking at the different components that make up a Kubernetes cluster – the control plane and the worker nodes. The control plane—made up of components such as the API server, controller manager, and etcd—is responsible for managing and maintaining the desired state of the cluster. The worker nodes run your containerized applications <span class="No-Break">in pods.</span></p>
			<p>After covering the cluster architecture, we will dive into the main Kubernetes abstractions and API resources such as pods, deployments, StatefulSets, services, ingress, and persistent volumes. These resources allow you to declare the desired state of your applications and have Kubernetes reconcile the actual state to match it. Understanding these concepts is key to being able to deploy and manage applications <span class="No-Break">on Kubernetes.</span></p>
			<p>We will also look at supporting resources such as ConfigMaps and Secrets, which allow you to separate configuration from code. Jobs provide support for <span class="No-Break">batch workloads.</span></p>
			<p>By the end of this chapter, you will have a solid understanding of how a Kubernetes cluster is put together and how you can leverage its capabilities by utilizing its API resources. This will enable you to start deploying your own applications and managing <span class="No-Break">them efficiently.</span></p>
			<p>We’ll be covering these concepts under the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li><span class="No-Break">Cluster architecture</span></li>
				<li><span class="No-Break">Pods</span></li>
				<li><span class="No-Break">Deployments</span></li>
				<li><span class="No-Break">StatefulSets</span></li>
				<li><span class="No-Break">Jobs</span></li>
				<li><span class="No-Break">Services</span></li>
				<li>Ingress and <span class="No-Break">ingress controllers</span></li>
				<li><span class="No-Break">Gateway</span></li>
				<li><span class="No-Break">Persistent volumes</span></li>
				<li>ConfigMaps <span class="No-Break">and Secrets</span></li>
			</ul>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor033"/>Technical requirements</h1>
			<p>There are no technical requirements for this chapter. All code presented here is generic and practical executable examples will be given in <a href="B21927_03.xhtml#_idTextAnchor053"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor034"/>Kubernetes architecture</h1>
			<p>Kubernetes is a<a id="_idIndexMarker054"/> cluster architecture. This means that in a full production environment, you will usually have several machines running your workloads simultaneously to create a reliable and scalable architecture. (Note that Kubernetes can run on one machine also, which is great for testing but misses the whole point <span class="No-Break">for production.)</span></p>
			<p>To coordinate cluster functionalities, Kubernetes has two main feature groups: the <strong class="bold">control plane</strong> responsible<a id="_idIndexMarker055"/> for cluster management and the <strong class="bold">node components</strong> that <a id="_idIndexMarker056"/>communicate with the control plane and execute tasks in the worker machines. <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em> shows a representation of the <span class="No-Break">whole system.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer014">
					<img alt="Figure 2.1 – Kubernetes architecture" src="image/B21927_02_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Kubernetes architecture</p>
			<p>Let’s take a <a id="_idIndexMarker057"/>closer look at each group and <span class="No-Break">its components.</span></p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor035"/>Control plane</h2>
			<p>The main components of the <a id="_idIndexMarker058"/>control plane<a id="_idIndexMarker059"/> are <span class="No-Break">the following:</span></p>
			<ul>
				<li><span class="No-Break">kube-apiserver</span></li>
				<li><span class="No-Break">etcd</span></li>
				<li><span class="No-Break">kube-scheduler</span></li>
				<li><span class="No-Break">kube-controller-manager</span></li>
			</ul>
			<p>When running a cloud-based Kubernetes cluster, we also have another component <span class="No-Break">called cloud-controller-manager.</span></p>
			<h3>kube-apiserver</h3>
			<p>The <a id="_idIndexMarker060"/>Kubernetes API is exposed to the <a id="_idIndexMarker061"/>administrator with <strong class="bold">kube-apiserver</strong>. It can be considered the “frontend” of the control plane. It is through the API server that we will interact with Kubernetes, sending instructions to the cluster or getting data from it. It is highly scalable and scales horizontally (deploying more worker nodes to <span class="No-Break">the cluster).</span></p>
			<h3>etcd</h3>
			<p>Kubernetes <a id="_idIndexMarker062"/>utilizes etcd, a <a id="_idIndexMarker063"/>distributed key-value database, to persistently store all cluster data and state. Etcd serves as the backing store for the Kubernetes API server, providing a secure and resilient foundation for the orchestration of containers across nodes in a cluster. By leveraging etcd’s capabilities for consistency, high availability, and distribution, Kubernetes can reliably manage the desired state of applications and infrastructure. etcd is fault tolerant even if the failure happens in a leader node of <span class="No-Break">the cluster.</span></p>
			<h3>kube-scheduler</h3>
			<p><strong class="bold">kube-scheduler</strong> is responsible <a id="_idIndexMarker064"/>for distributing work or containers across<a id="_idIndexMarker065"/> multiple nodes. It watches for newly created pods that are not assigned to any node and selects a node for them to run on. To make scheduling decisions, kube-scheduler analyzes individual and collective available resources, hardware/software/policy constraints, affinity and anti-affinity instructions, deadlines, data locality and eventual interferences <span class="No-Break">between workloads.</span></p>
			<h3>kube-controller-manager</h3>
			<p><strong class="bold">kube-controller-manager</strong> runs <a id="_idIndexMarker066"/>controllers that regulate behavior in the <a id="_idIndexMarker067"/>cluster, such as node controllers, job controllers, EndpointSlice controllers, and ServiceAccount controllers. The controllers reconcile the desired state with the <span class="No-Break">current state.</span></p>
			<h3>cloud-controller-manager</h3>
			<p><strong class="bold">cloud-controller-manager</strong> interacts with the underlying cloud providers and sets up cloud-based networking <a id="_idIndexMarker068"/>services (such as networking routes and load balancers). It <a id="_idIndexMarker069"/>separates components that interact with the cloud provider from the components that run only inside the cluster. This controller manager only runs controllers that are specific to the cloud provider in use, thus, if you are running a test local Kubernetes instance, cloud-controller-manager will not be<a id="_idIndexMarker070"/> used, since it only deals with <span class="No-Break">cloud-based </span><span class="No-Break"><a id="_idIndexMarker071"/></span><span class="No-Break">services.</span></p>
			<p>Next, let’s have a look at the <span class="No-Break">node components.</span></p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor036"/>Node components</h2>
			<p>Node components are<a id="_idIndexMarker072"/> present in every single worker node of the<a id="_idIndexMarker073"/> cluster and are responsible for communicating with the control plane, running and maintaining workloads, and providing a runtime environment. The main components are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><span class="No-Break">Container runtime</span></li>
				<li><span class="No-Break">kubelet</span></li>
				<li><span class="No-Break">kube-proxy</span></li>
			</ul>
			<h3>Container runtime</h3>
			<p><strong class="bold">A container runtime</strong> is the <a id="_idIndexMarker074"/>underlying software that is responsible for running containers. Kubernetes supports several container runtimes, but the most common ones are <a id="_idIndexMarker075"/>Docker and containerd. The container runtime is responsible for pulling the images from the registries, running the containers, and managing <span class="No-Break">containers’ lifecycles.</span></p>
			<h3>kubelet</h3>
			<p><strong class="bold">kubelet</strong> is the <a id="_idIndexMarker076"/>primary node agent that watches the assigned pods and ensures containers are running and <a id="_idIndexMarker077"/>healthy. It interacts with the container runtime to pull images and <span class="No-Break">run containers.</span></p>
			<h3>kube-proxy</h3>
			<p><strong class="bold">kube-proxy</strong> is a <a id="_idIndexMarker078"/>network proxy and load balancer that implements Kubernetes networking<a id="_idIndexMarker079"/> services on each node by maintaining network rules and performing <span class="No-Break">connection forwarding.</span></p>
			<p>Now, let’s move our attention to the Kubernetes components we will use to build <span class="No-Break">our workloads.</span></p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor037"/>Pods</h1>
			<p><strong class="bold">Pods</strong> are<a id="_idIndexMarker080"/> the smallest deployable units in Kubernetes and represent a single instance of an application. Pods contain one or more containers (although the most common case is to have just one container inside a pod). When multiple containers live inside a pod, they are guaranteed to be co-located on the same node and can <span class="No-Break">share resources.</span></p>
			<p>Pods provide two <span class="No-Break">main benefits:</span></p>
			<ul>
				<li><strong class="bold">Resource sharing and communication</strong>: Containers within the same pod can communicate <a id="_idIndexMarker081"/>over <strong class="source-inline">localhost</strong> and share resources such as volumes. This facilitates easy communication between related containers. It is important to notice, though, that this is an advanced use case and should be used only when your containers are tightly coupled. We regularly use pods for <span class="No-Break">single-container deployments.</span></li>
				<li><strong class="bold">Management and deployment</strong>: Pods are the units that get deployed, scaled, and managed in Kubernetes. You don’t directly create or manage the containers within pods. This entire process is <span class="No-Break">fully automated.</span></li>
			</ul>
			<p>Usually, you do not define pods directly. Pods can be created and managed though other resources such as deployments, jobs, and <strong class="source-inline">StatefulSets</strong>. Nevertheless, you can define a pod with a proper YAML <span class="No-Break">file manifest.</span></p>
			<p><strong class="bold">Manifests</strong> are<a id="_idIndexMarker082"/> the basic <a id="_idIndexMarker083"/>way of telling Kubernetes about the desired state of any object or giving it instructions about how to act or deploy anything. It is written as a <strong class="source-inline">.yaml</strong> file. <strong class="bold">YAML files</strong> are <a id="_idIndexMarker084"/>often used for configuration. They are very close to JSON files, but they are more readable since they rely on indentation for code hierarchy structure rather than brackets and braces. The following is an example of a manifest to deploy a <span class="No-Break">single pod:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod.yaml</p>
			<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
    name: myapp-pod
    labels:
    app: myapp
spec:
    containers:
    - name: myapp-container
    image: myimage:latest
    ports:
    - containerPort: 80</pre>			<p> Let’s have a <a id="_idIndexMarker085"/>closer look at<a id="_idIndexMarker086"/> each part of <span class="No-Break">this manifest:</span></p>
			<ul>
				<li><strong class="source-inline">apiVersion</strong>: The Kubernetes API version for the objects in this manifest. For pods this <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">v1</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">kind</strong>: The type of object being created, which is <strong class="source-inline">Pod</strong> for <span class="No-Break">this manifest.</span></li>
				<li><strong class="source-inline">metadata</strong>: This section contains metadata for the pod, such as the name and labels. The name is a unique identifier. Labels will be particularly important in the future as they serve as identifiers for other Kubernetes resources such as deployments <span class="No-Break">and services.</span></li>
				<li><strong class="source-inline">spec</strong>: This section defines the desired state of the pod including <span class="No-Break">its containers.</span></li>
				<li><strong class="source-inline">containers</strong>: Specifies the container(s) that run inside the pod. Includes the image, <span class="No-Break">ports, etc.</span></li>
				<li><strong class="source-inline">image</strong>: The Docker image to use for the container. It can be a public or <span class="No-Break">private image.</span></li>
				<li><strong class="source-inline">ports</strong>: Defines the ports exposed by <span class="No-Break">the container.</span></li>
			</ul>
			<p>This covers the basic structure of a pod manifest. Pods provide a simple way to deploy and manage containers in Kubernetes. Now that we have discussed pods and how to define them, let’s discuss a way of automating more complex pod structures <span class="No-Break">with deployments.</span></p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor038"/>Deployments</h1>
			<p><strong class="bold">Deployments</strong> are <a id="_idIndexMarker087"/>one of the most important resources in Kubernetes for running applications. They provide a declarative way to manage pods <span class="No-Break">and replicas.</span></p>
			<p>A deployment defines the desired state for your application, including the container image, number of replicas, resource limits, and more. The Kubernetes control plane works to match the actual state of your cluster to the desired state in <span class="No-Break">the deployment.</span></p>
			<p>For example, here is a simple <span class="No-Break">deployment manifest:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">deployment.yaml</p>
			<pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
    name: myapp-deployment
spec:
    replicas: 3
    selector:
    matchLabels:
      app: myapp
    template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: nginx:1.16
        ports:
        - containerPort: 80</pre>			<p>Let’s break this <a id="_idIndexMarker088"/>down section <span class="No-Break">by section:</span></p>
			<ul>
				<li><strong class="source-inline">apiVersion</strong>: This specifies the Kubernetes API version for the Deployment resource. We want the <strong class="source-inline">apps/v1</strong> version which <span class="No-Break">includes Deployments.</span></li>
				<li><strong class="source-inline">kind: Deployment</strong>: The type of resource we are creating, in this case, <span class="No-Break">a Deployment.</span></li>
				<li><strong class="source-inline">metadata</strong>: Standard metadata for the resource like a <span class="No-Break">unique name.</span></li>
				<li><strong class="source-inline">spec</strong>: The specification for the Deployment. This defines the <span class="No-Break">desired state.</span></li>
				<li><strong class="source-inline">replicas: 3</strong>: We want three pod replicas to be running. Kubernetes will maintain this number <span class="No-Break">of pods.</span></li>
				<li><strong class="source-inline">selector</strong>: Used to match pods managed by this Deployment. Pods will be selected based on the <span class="No-Break">label selector.</span></li>
				<li><strong class="source-inline">template</strong>: The template for the pods that will be created. It defines the pod specifications. Note that the deployment will relate to the <span class="No-Break">label specified.</span></li>
				<li><strong class="source-inline">spec: containers</strong>: Pod spec including the container(s) <span class="No-Break">to run.</span></li>
				<li><strong class="source-inline">image: nginx:1.16</strong>: The container image <span class="No-Break">to use.</span></li>
				<li><strong class="source-inline">ports</strong>: Ports exposed by <span class="No-Break">the container.</span></li>
			</ul>
			<p>When this Deployment is applied, Kubernetes will launch three pods matching the template, each running an Nginx container. The Deployment controller will monitor the pods and ensure the desired state matches the actual state, restarting pods <span class="No-Break">if needed.</span></p>
			<p>Deployments provide powerful capabilities for running scalable and resilient applications on Kubernetes. Using declarative configuration makes deployments easy. Next, we will discuss a different approach for managing pods and <span class="No-Break">replicas: StatefulSets</span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor039"/>StatefulSets</h1>
			<p>StatefulSets <a id="_idIndexMarker089"/>are a Kubernetes resource used to manage stateful applications such as databases. They are similar to Deployments but are designed to handle stateful workloads that require persistent storage and unique <span class="No-Break">network identifiers.</span></p>
			<p>A StatefulSet manages Pods that contain stateful applications (applications that must keep track of data for other applications or other user sessions). The Pods in a StatefulSet have a sticky, unique identity that persists across rescheduling. This allows each Pod to maintain its state when restarted or rescheduled onto a new node. This makes StatefulSets ideal for stateful apps such as databases that require data persistence. Deployments, on the other hand, are designed for stateless workloads and provide identical Pods with no persistent storage. Thus, they are better for stateless <span class="No-Break">web apps.</span></p>
			<p>StatefulSets operate by creating PersistentVolumes (which will be covered later in this chapter) for each Pod to mount. This ensures data persists across Pod restarts. StatefulSets also provide a unique hostname and stable network ID per Pod using a predictable <span class="No-Break">naming convention.</span></p>
			<p>Here is an example of a statefulset manifest for deploying a <span class="No-Break">MySQL database:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">statefulset.yaml</p>
			<pre class="source-code">
apiVersion: apps/v1
kind: StatefulSet
metadata:
    name: mysql
spec:
    serviceName: "mysql"
    replicas: 1
    selector:
    matchLabels:
      app: mysql
    template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
    volumeClaimTemplates:
    - metadata:
      name: mysql-persistent-storage
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi</pre>			<p>This manifest creates a StatefulSet for MySQL with one replica. It uses a <strong class="source-inline">volumeClaimTemplate</strong> to dynamically provision a PersistentVolume for each Pod. The MySQL data will be persisted in the <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">var/lib/mysql</strong></span><span class="No-Break"> path.</span></p>
			<p>Each Pod gets a unique name, such as <strong class="source-inline">mysql-0</strong>, and a stable hostname. If the Pod gets rescheduled, it will remount its PersistentVolume to continue <span class="No-Break">running statefully.</span></p>
			<p>In this way, StatefulSets <a id="_idIndexMarker090"/>provide powerful stateful management for databases and other stateful apps in Kubernetes. They ensure persistence, stable networking, ordered deployment, and graceful scaling. Next, we will continue with a discussion about <span class="No-Break">Kubernetes jobs.</span></p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor040"/>Jobs</h1>
			<p><strong class="bold">Jobs</strong> are a fundamental <a id="_idIndexMarker091"/>resource type in Kubernetes used to run batch processes that run to completion. Unlike long-running services such as web servers, jobs are intended to terminate when the batch <span class="No-Break">process finishes.</span></p>
			<p>A job creates one or more pods that run a defined workload and then terminates when the workload is complete. This is useful for tasks such as data processing, machine learning training, or any <span class="No-Break">finite computation.</span></p>
			<p>To create a job, you define a Job resource in a YAML manifest <span class="No-Break">like this:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">job.yaml</p>
			<pre class="source-code">
apiVersion: batch/v1
kind: Job
metadata:
    name: myjob
spec:
    template:
    spec:
      containers:
      - name: myjob
        image: busybox
        command: ['sh', '-c', 'echo Hello Kubernetes! &amp;&amp; sleep 30']
      restartPolicy: Never
    backoffLimit: 4</pre>			<p>Let’s have a closer <a id="_idIndexMarker092"/>look at this code, part <span class="No-Break">by part:</span></p>
			<ul>
				<li><strong class="source-inline">apiVersion</strong> <strong class="source-inline">'</strong> <strong class="source-inline">batch/v1</strong> <span class="No-Break">for jobs</span></li>
				<li><strong class="source-inline">kind</strong> <strong class="source-inline">'</strong> <span class="No-Break"><strong class="source-inline">Job</strong></span></li>
				<li><strong class="source-inline">metadata.name</strong> <strong class="source-inline">'</strong> Name of <span class="No-Break">the job</span></li>
				<li><strong class="source-inline">spec.template</strong> <strong class="source-inline">'</strong> Pod template defining the container(s) to run the same way we saw in the previous <span class="No-Break">resource definitions</span></li>
				<li><strong class="source-inline">spec.template.spec.restartPolicy</strong> <strong class="source-inline">'</strong> Set to <strong class="source-inline">Never</strong> since jobs <span class="No-Break">shouldn’t restart</span></li>
				<li><strong class="source-inline">spec.backoffLimit</strong> <strong class="source-inline">'</strong> Optional limit on failed <span class="No-Break">job retries</span></li>
			</ul>
			<p>The pod template under <strong class="source-inline">spec.template</strong> defines the container(s) to run just like a pod manifest. You can specify the image, commands, environment variables, and so on. An important setting is the <strong class="source-inline">restartPolicy</strong>, which should be <strong class="source-inline">Never</strong> for jobs. This ensures pods are not restarted if they fail or exit. The <strong class="source-inline">backoffLimit</strong> is optional and specifies the number of times a failed job pod can be retried. The default is <strong class="bold">6</strong>. Set this lower if jobs should not retry too many times <span class="No-Break">on failure.</span></p>
			<p>When you create the job, Kubernetes will schedule one or more pods matching the template to run your workload. As the pods finish, Kubernetes will track their status and know when the job is completed. You can view job status and pod logs to monitor progress. Jobs make it easy to run batch computational workloads on Kubernetes. In the next section, we will take a look at <span class="No-Break">Kubernetes services.</span></p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor041"/>Services</h1>
			<p><strong class="bold">Services</strong> provide <a id="_idIndexMarker093"/>stable endpoints to access pods running in a cluster, thus exposing our applications to users online. They allow pods to die and replicate without interrupting access to the applications running in those pods. There are several types of services in Kubernetes. We will discuss three of them in detail: ClusterIP, NodePort, and <span class="No-Break">load balancer.</span></p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor042"/>ClusterIP Service</h2>
			<p>A ClusterIP service<a id="_idIndexMarker094"/> provides an IP address that is only accessible inside the<a id="_idIndexMarker095"/> cluster. This IP does not change for the lifetime of the service, providing a stable endpoint to access the pods. Here is an example ClusterIP <span class="No-Break">service manifest:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">service_clusterip.yaml</p>
			<pre class="source-code">
apiVersion: v1
kind: Service
metadata:
    name: my-service
spec:
    type: ClusterIP
    selector:
    app: my-app
    ports:
    - protocol: TCP
     port: 80
     targetPort: 9376</pre>			<p>This manifest creates a service called <strong class="source-inline">my-service</strong> that will forward requests to pods with the label <strong class="source-inline">app: my-app</strong> on <strong class="source-inline">port 80</strong>. Requests will be forwarded to <strong class="source-inline">port 9376</strong> on the target pods. The ClusterIP will not change while this <span class="No-Break">service exists.</span></p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor043"/>NodePort Service</h2>
			<p>A NodePort service<a id="_idIndexMarker096"/> makes an internal ClusterIP service accessible externally<a id="_idIndexMarker097"/> through a port allocated on each node. The NodePort is allocated from a configured range (by default, <strong class="source-inline">30000</strong>-<strong class="source-inline">32767</strong>) and will be the same on every node. Traffic to <strong class="source-inline">&lt;NodeIP&gt;:&lt;NodePort&gt;</strong> will be forwarded to the ClusterIP service. Here is <span class="No-Break">an example:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">service_nodeport.yaml</p>
			<pre class="source-code">
apiVersion: v1
kind: Service
metadata:
    name: my-service
spec:
    type: NodePort
    selector:
    app: my-app
    ports:
    - port: 80
      targetPort: 9376
      nodePort: 30007</pre>			<p>This exposes the internal ClusterIP on port <strong class="source-inline">30007</strong> on every node. Requests to <strong class="source-inline">&lt;NodeIP&gt;:30007</strong> will be forwarded to <span class="No-Break">the service.</span></p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor044"/>LoadBalancer Service</h2>
			<p>A<a id="_idIndexMarker098"/> load balancer service provisions an external load balancer to expose the service to<a id="_idIndexMarker099"/> external traffic. A ClusterIP exposes the service on an internal IP address within the Kubernetes cluster. This makes the service only reachable within the cluster. Load balancer, on the other hand, exposes the service externally using the cloud provider’s load balancer implementation. This makes the service reachable from outside the <span class="No-Break">Kubernetes cluster.</span></p>
			<p>The<a id="_idIndexMarker100"/> load balancer implementation depends on the environment. For example, on <a id="_idIndexMarker101"/>AWS, this would create an <strong class="bold">Elastic Load Balancer</strong> (<strong class="bold">ELB</strong>), an <a id="_idIndexMarker102"/>AWS service to provide a managed load balancer. Here is <span class="No-Break">an example:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">service_loadbalancer.yaml</p>
			<pre class="source-code">
apiVersion: v1
kind: Service
metadata:
    name: my-service
spec:
    selector:
    app: my-app
    ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
    type: LoadBalancer</pre>			<p>This creates a load balancer and assigns an external IP address. Traffic to the external IP is forwarded to the internal ClusterIP service. Next, we will discuss a different way of defining services with Ingress and <span class="No-Break">Ingress Controller.</span></p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor045"/>Ingress and Ingress Controller</h1>
			<p>An <strong class="bold">Ingress</strong> resource<a id="_idIndexMarker103"/> defines rules for external connectivity to Kubernetes services. It enables inbound HTTP and HTTPS connections to reach services running within the cluster. Traffic routing is controlled by rules defined on the Ingress resource. For an ingress to be able to run, you need to have a running ingress controller <span class="No-Break">on Kubernetes.</span></p>
			<p>An <strong class="bold">Ingress controller</strong> is <a id="_idIndexMarker104"/>responsible for fulfilling the Ingress, usually with a load balancer. It watches for Ingress resources and configures the load balancer accordingly. Different load balancers require different Ingress <span class="No-Break">controller implementations.</span></p>
			<p>Some examples<a id="_idIndexMarker105"/> of Ingress controllers include <span class="No-Break">the following:</span></p>
			<ul>
				<li>NGINX Ingress Controller: Uses<a id="_idIndexMarker106"/> NGINX as a load balancer and reverse proxy. It is one of the most common and fully <span class="No-Break">featured controllers.</span></li>
				<li>HAProxy Ingress Controller: Uses <a id="_idIndexMarker107"/>HAProxy for load balancing. Provides high performance <span class="No-Break">and reliability.</span></li>
				<li>Traefik Ingress Controller: A cloud-native controller that integrates with Let’s Encrypt for automatic <a id="_idIndexMarker108"/>HTTPS <span class="No-Break">certificate generation.</span></li>
				<li>AWS ALB Ingress Controller: Uses<a id="_idIndexMarker109"/> the AWS <strong class="bold">Application Load Balancer</strong> (<strong class="bold">ALB</strong>). Integrates<a id="_idIndexMarker110"/> natively with other <span class="No-Break">AWS services.</span></li>
			</ul>
			<p>The Ingress resource<a id="_idIndexMarker111"/> contains two main parts – a backend and rules. The backend specifies the default service to route unmatched requests. The rules contain a set of paths and the services to route <span class="No-Break">them to:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ingress.yaml</p>
			<pre class="source-code">
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
    name: example-ingress
spec:
    backend:
    service:
      name: example-service
      port:
        number: 80
    rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: example-service
                port:
                  number: 80</pre>			<p>In this example, requests to the root path <strong class="source-inline">/</strong> will be routed to the <strong class="source-inline">example-service</strong> on port <strong class="source-inline">80</strong>. The <strong class="source-inline">pathType:</strong> prefix indicates that any subpath should also be routed to <span class="No-Break">the service.</span></p>
			<p>Multiple rules can be <a id="_idIndexMarker112"/>defined to route different paths to <span class="No-Break">different services:</span></p>
			<pre class="source-code">
spec:
    rules:
    - http:
        paths:
          - path: /foo
            backend:
              service:
                name: foo-service
                port:
                  number: 80
    - http:
        paths:
          - path: /bar
            backend:
              service:
                name: bar-service
                port:
                  number: 80</pre>			<p>With the preceding code, requests to <strong class="source-inline">/foo</strong> will go to <strong class="source-inline">foo-service</strong> and requests to <strong class="source-inline">/bar</strong> will go <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">bar-service</strong></span><span class="No-Break">.</span></p>
			<p>In some cases, we have to <a id="_idIndexMarker113"/>configure secure connections with in-transit encryption. When this is the case, we can configure advanced encryption options in Ingress controllers <span class="No-Break">using annotations:</span></p>
			<pre class="source-code">
metadata:
    annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"</pre>			<p>Host-based routing can also be configured by specifying <span class="No-Break">host names:</span></p>
			<pre class="source-code">
spec:
    rules:
    - host: foo.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: foo-service
          servicePort: 80
    - host: bar.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: bar-service
          servicePort: 80</pre>			<p>Now <strong class="source-inline">foo.example.com</strong> will route to <strong class="source-inline">foo-service</strong> and <strong class="source-inline">bar.example.com</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">bar-service</strong></span><span class="No-Break">.</span></p>
			<p>In summary, Ingress provides a<a id="_idIndexMarker114"/> way to intelligently route HTTP and HTTPS traffic to services in a Kubernetes cluster. Ingress controllers handle the actual load balancing and reverse proxy functionality. Common use cases for Ingress include exposing services to external users and handling TLS/SSL. Careful Ingress configuration is crucial for production-grade <span class="No-Break">Kubernetes deployments.</span></p>
			<p><strong class="bold">It is important to note that the ingress API is frozen.</strong> That means that this API will not be receiving any more updates. It is replaced by the Gateway API. Nevertheless, it is important to know it since a lot of the big data tools that we will use in this book are still deployed using Ingress instructions. Now, let’s move to the Gateway API and understand how <span class="No-Break">it works.</span></p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor046"/>Gateway</h1>
			<p>The Gateway API<a id="_idIndexMarker115"/> is a Kubernetes API that provides a way to dynamically configure load balancing and service mesh capabilities on Kubernetes. The Gateway API allows defining routes and policies to manage external traffic to Kubernetes services in a centralized, <span class="No-Break">declarative way.</span></p>
			<p>The main resources in Gateway API are <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">GatewayClass</strong> <strong class="source-inline">'</strong> Defines<a id="_idIndexMarker116"/> a set of gateways with a common configuration and behavior. It is like the concept of StorageClass for <span class="No-Break">persistent volumes.</span></li>
				<li><strong class="source-inline">Gateway</strong> <strong class="source-inline">'</strong> Defines a set of routes for a given hostname. This binds GatewayClass, TLS certificate, and other configurations to a set <span class="No-Break">of routes.</span></li>
				<li><strong class="source-inline">HTTPRoute/TCPRoute</strong> <strong class="source-inline">'</strong> Defines the actual routes to Kubernetes services and their policies, such as timeouts, retries, and <span class="No-Break">so on.</span></li>
			</ul>
			<p>Here is an example <span class="No-Break">GatewayClass resource:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">gateway_class.yaml</p>
			<pre class="source-code">
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
    name: external-lb
spec:
    controllerName: lb.acme.io/gateway-controller</pre>			<p>This defines a GatewayClass named <strong class="source-inline">external-lb</strong> that will be handled by a <span class="No-Break"><strong class="source-inline">lb.acme.io/gateway-controller</strong></span><span class="No-Break"> controller.</span></p>
			<p>A Gateway resource binds a hostname and TLS certificate to the GatewayClass as we can see in the <span class="No-Break">following code:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">gateway.yaml</p>
			<pre class="source-code">
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
    name: my-gateway
spec:
    gatewayClassName: external-lb
    listeners:
    - name: http
    port: 80
    protocol: HTTP</pre>			<p>This Gateway named <strong class="source-inline">my-gateway</strong> uses the <strong class="source-inline">external-lb</strong> GatewayClass defined earlier. It handles HTTP traffic on port <strong class="source-inline">80</strong>. Note that the <strong class="source-inline">addresses</strong> field is not specified, so an address or hostname will be assigned to the gateway by <span class="No-Break">its controller.</span></p>
			<p>Finally, HTTPRoute <a id="_idIndexMarker117"/>and TCPRoute resources define the actual routes to backend services. Here is <span class="No-Break">an example:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">http_route.yaml</p>
			<pre class="source-code">
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
    name: http-route
spec:
    parentRefs:
    - name: my-gateway
    rules:
    - matches:
    - path:
    type: PathPrefix
    value: /foo
    backendRefs:
    - name: my-foo-service
      port: 80
    - matches:
    - path:
        type: PathPrefix
        value: /bar
    backendRefs:
    - name: my-bar-service
      port: 80</pre>			<p>This HTTPRoute is a child of the <strong class="source-inline">my-gateway</strong> Gateway defined earlier. It routes requests to the <strong class="source-inline">/foo</strong> path to the <strong class="source-inline">my-foo-service</strong> service on port <strong class="source-inline">80</strong> and requests to <strong class="source-inline">/bar</strong> are routed to <strong class="source-inline">my-bar-service</strong> on port <strong class="source-inline">80</strong>. Also, additional features such as request timeouts, retries, and traffic splitting can be configured on the <span class="No-Break">Route resources.</span></p>
			<p>Gateways are a <a id="_idIndexMarker118"/>new and great way of configuring networking and routing in Kubernetes. The centralized configuration for ingress traffic management acts as a single source of truth. While the Ingress resource presents a simple, declarative interface focused specifically on exposing HTTP applications, the Gateway API resource offers a more generalized abstraction for proxying diverse protocols beyond HTTP. Also, they decouple the data plane from the control plane. Any gateway controller can be used, including NGINX, HAProxy, and Istio. Gateways provide improved security with TLS handling and authentication and fine-grained traffic control using advanced routing rules and policies. Finally, they have easier management and operation for complex <span class="No-Break">ingress configurations.</span></p>
			<p>Next, we will approach <span class="No-Break">persistent volumes.</span></p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor047"/>Persistent Volumes</h1>
			<p>Kubernetes was originally designed for stateless applications. So, one of the key challenges when running stateful applications on Kubernetes is managing storage. Kubernetes provides abstractions that allow storage to be provisioned and consumed in a portable manner across different environments. When designing storage infrastructure on Kubernetes, there are two main <a id="_idIndexMarker119"/>resources to understand: <strong class="bold">PersistentVolumes</strong> (<strong class="bold">PVs</strong>) and <strong class="bold">PersistentVolumeClaims</strong> (<strong class="bold">PVCs</strong>). A PV represents a networked storage<a id="_idIndexMarker120"/> unit provisioned by the cluster administrator. Much like compute nodes, PVs become a pool of cluster resources. In contrast, PVCs allow end users to request abstract storage with defined capacity and access modes. The PVC functions similarly to a pod resource request, but instead of CPU and memory, users can specify their desired volume size and read/write permissions. The Kubernetes control plane handles binding matching <a id="_idIndexMarker121"/>PV and PVC resources to provision storage for pods as declared. With this separation of roles, the underlying storage layer gains lifecycle independence from <span class="No-Break">individual pods.</span></p>
			<p>Here is an example <a id="_idIndexMarker122"/>PersistentVolume <span class="No-Break">YAML manifest:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">persistent_volume.yaml</p>
			<pre class="source-code">
apiVersion: v1
kind: PersistentVolume
metadata:
    name: pv0003
spec:
    capacity:
    storage: 5Gi
    volumeMode: Filesystem
    accessModes:
    - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    storageClassName: slow
    mountOptions:
    - hard
    - nfsvers=4.1
    nfs:
    path: /tmp
    server: 172.17.0.2</pre>			<p>This defines an <strong class="bold">Network File System </strong>(<strong class="bold">NFS</strong>) PV that supports the ReadWriteOnce access mode, has a capacity<a id="_idIndexMarker123"/> of 5GB, and is mounted at <strong class="source-inline">/tmp</strong> on the NFS server at <strong class="source-inline">172.17.0.2</strong>. The reclaim policy is set to <strong class="source-inline">Recycle</strong>, which means the volume will be recycled rather than deleted when released. <strong class="source-inline">storageClassName</strong> is set to <strong class="source-inline">slow</strong>, which can be used to match this PV to PVCs requesting specific <span class="No-Break">storage </span><span class="No-Break"><a id="_idIndexMarker124"/></span><span class="No-Break">classes.</span></p>
			<p>Here is an example PersistentVolumeClaim <span class="No-Break">YAML manifest:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pvc.yaml</p>
			<pre class="source-code">
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
    name: myclaim
spec:
    accessModes:
    - ReadWriteOnce
    volumeMode: Filesystem
    resources:
    requests:
      storage: 8Gi
    storageClassName: slow
    selector:
    matchLabels:
      release: "stable"</pre>			<p>This PVC requests 8GB of storage with <strong class="source-inline">ReadWriteOnce</strong> access. It specifies the <strong class="source-inline">slow</strong> <strong class="source-inline">storageClassName</strong>, which will match it to the preceding PV with the same storage class. There is also a selector that will match PVs with a <strong class="source-inline">stable</strong> <span class="No-Break">release label.</span></p>
			<p>When a PVC is created, the Kubernetes control plane looks for a matching PV to bind to the PVC. This matching takes into account access modes, storage capacity, and <strong class="source-inline">storageClassName</strong> among other factors. Once bound, that storage is then available to be mounted <span class="No-Break">by pods.</span></p>
			<p>Here is a pod YAML <a id="_idIndexMarker125"/>that mounts the <span class="No-Break">preceding PVC:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod_with_pvc.yaml</p>
			<pre class="source-code">
kind: Pod
apiVersion: v1
metadata:
    name: mypod
spec:
    containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
    volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim</pre>			<p>This pod mounts the PVC into the container at <strong class="source-inline">/var/www/html</strong>. The PVC provides durable storage for the pod that persists even if the pod is deleted or moved to a <span class="No-Break">different node.</span></p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor048"/>StorageClasses</h2>
			<p>The <a id="_idIndexMarker126"/>StorageClass objects define different <em class="italic">classes</em> of storage that can be requested. This allows administrators to offer different tiers of storage within the same cluster. The following code defines a standard hard disk StorageClass and a fast SSD StorageClass <span class="No-Break">on GCE:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">storage_classes.yaml</p>
			<pre class="source-code">
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
    name: standard
provisioner: kubernetes.io/gce-pd
parameters:
    type: pd-standard
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
    name: fast
provisioner: kubernetes.io/gce-pd
parameters:
    type: pd-ssd</pre>			<p>The <strong class="source-inline">---</strong> line tells Kubernetes that we aggregated two YAML manifests in just one file. After defining a StorageClass, PVCs can then request a <span class="No-Break">particular class:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pvc2.yaml</p>
			<pre class="source-code">
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
    name: myclaim
spec:
    accessModes:
    - ReadWriteOnce
    storageClassName: fast
    resources:
    requests:
      storage: 30Gi</pre>			<p>This allows a<a id="_idIndexMarker127"/> cluster to provide different types of storage without requiring users to understand how the details are implemented. Next, we will discuss the final subject in this chapter and one that is very important for security in Kubernetes: ConfigMaps <span class="No-Break">and Secrets.</span></p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor049"/>ConfigMaps and Secrets</h1>
			<p>ConfigMaps <a id="_idIndexMarker128"/>and Secrets<a id="_idIndexMarker129"/> are two important Kubernetes objects that allow you to separate configuration data from your application code. This makes your applications more portable, manageable, <span class="No-Break">and secure.</span></p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/>ConfigMaps</h2>
			<p>ConfigMaps <a id="_idIndexMarker130"/>provide a convenient way to pass configuration data into pods in a declarative manner. They allow you to store configuration information without putting them directly in a pod definition or container image. Pods can access the data stored in a ConfigMap through environment variables, command-line arguments, or by mounting the ConfigMap as a volume. Using ConfigMaps enables you to separate your configuration data from your <span class="No-Break">application code.</span></p>
			<p>With the following manifest, you can create a ConfigMap to store <span class="No-Break">configuration files:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">config_map.yaml</p>
			<pre class="source-code">
apiVersion: v1
kind: ConfigMap
metadata:
    name: app-config
data:
    config.properties: |
    app.color=blue
    app.mode=prod</pre>			<p>This ConfigMap <a id="_idIndexMarker131"/>contains a <strong class="source-inline">config.properties</strong> file that pods can mount <span class="No-Break">and consume.</span></p>
			<p>You can also create ConfigMaps from directories, files, or literal values. The following commands show an example of each ConfigMap definition. Those commands are run in a shell using the <strong class="source-inline">kubectl</strong> executable (we will take a deeper look at it in the <span class="No-Break">next chapter):</span></p>
			<pre class="source-code">
kubectl create configmap app-config --from-file=path/to/dir
kubectl create configmap app-config --from-file=config.properties
kubectl create configmap app-config --from-literal=app.color=blue</pre>			<p>To consume a ConfigMap in a pod, you can do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Set environment variables from <span class="No-Break">ConfigMap data</span></li>
				<li>Set command-line arguments from <span class="No-Break">ConfigMap data</span></li>
				<li>Consume ConfigMap values <span class="No-Break">in volumes</span></li>
			</ul>
			<p>This following YAML file defines a Kubernetes Pod that consumes configuration data from a ConfigMap<a id="_idIndexMarker132"/> using environment variables and consuming it as a volume. Let’s see how that <span class="No-Break">is done:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod_with_configmap.yaml</p>
			<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
    name: configmap-demo
spec:
    containers:
    - name: demo
      image: alpine
      env:
        - name: APP_COLOR
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: app.color
      args:
        - $(APP_MODE)
        valueFrom:
          configMapKeyRef:
            name: app-config
            key: app.mode
      volumeMounts:
        - name: config-volume
          mountPath: /etc/config
    volumes:
    - name: config-volume
      configMap:
        name: app-config</pre>			<p>Let’s take a closer look <a id="_idIndexMarker133"/>at this code and understand what <span class="No-Break">it’s doing:</span></p>
			<ul>
				<li>It defines a Pod with the <span class="No-Break">name </span><span class="No-Break"><strong class="source-inline">configmap-demo</strong></span><span class="No-Break">.</span></li>
				<li>The Pod has one container called <strong class="source-inline">demo</strong> that uses the <span class="No-Break"><strong class="source-inline">alpine</strong></span><span class="No-Break"> image.</span></li>
				<li>The container has two environment <span class="No-Break">variables set:</span><ul><li><strong class="source-inline">APP_COLOR</strong> is set from the <strong class="source-inline">app.color</strong> key in the <span class="No-Break"><strong class="source-inline">app-config</strong></span><span class="No-Break"> ConfigMap</span></li><li><strong class="source-inline">APP_MODE</strong> is set from the <strong class="source-inline">app.mode</strong> key in the <strong class="source-inline">app-config</strong> ConfigMap (this is defined as an argument to the <span class="No-Break">run command)</span></li></ul></li>
				<li>The container has one volume mount called <strong class="source-inline">config-volume</strong> that mounts the <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">etc/config</strong></span><span class="No-Break"> path.</span></li>
				<li>The Pod defines a volume called <strong class="source-inline">config-volume</strong> that uses the <strong class="source-inline">app-config</strong> ConfigMap as a data source. This makes the data from the ConfigMap available to the container on the <span class="No-Break">mount path.</span></li>
			</ul>
			<p>Although ConfigMaps are really useful, they don’t provide secrecy for confidential data. For that, Kubernetes <span class="No-Break">provides Secrets.</span></p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/>Secrets</h2>
			<p>A Secret<a id="_idIndexMarker134"/> is an object that contains a small amount of sensitive data such as passwords, tokens, or keys. Secrets allow you to store and manage this sensitive data without exposing it in your <span class="No-Break">application code.</span></p>
			<p>For example, you can create a Secret from literal values <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">kubectl</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
kubectl create secret generic db-secret \
--from-literal=DB_HOST=mysql \
--from-literal=DB_USER=root \
--from-literal=DB_PASSWORD=password123</pre>			<p>The preceding code would create a Secret containing confidential database credentials. You can also create Secrets from files <span class="No-Break">or directories:</span></p>
			<pre class="source-code">
kubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/path/to/key
kubectl create secret generic app-secret --from-file=/path/to/dir</pre>			<p>Secrets <a id="_idIndexMarker135"/>store data encoded in <strong class="source-inline">base64</strong> format. This prevents the values from being exposed as <strong class="source-inline">plaintext</strong> in <strong class="source-inline">etcd</strong>. However, the data is not encrypted. You can consume your secret data from pods <span class="No-Break">like this:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod_with_secrets.yaml</p>
			<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
    name: secret-demo
spec:
    containers:
    - name: demo
    image: nginx
    env:
      - name: DB_HOST
        valueFrom:
          secretKeyRef:
            name: db-secret
            key: DB_HOST
      - name: DB_USER
        valueFrom:
          secretKeyRef:
            name: db-secret
            key: DB_USER
      - name: DB_PASSWORD
        valueFrom:
          secretKeyRef:
            name: db-secret
            key: DB_PASSWORD
    volumeMounts:
      - name: secrets-volume
        mountPath: /etc/secrets
        readOnly: true
    volumes:
    - name: secrets-volume
      secret:
        secretName: app-secret</pre>			<p>The preceding YAML file defines a Kubernetes Pod that consumes secrets from the Kubernetes API. Let’s go through <span class="No-Break">the code:</span></p>
			<ul>
				<li>It defines a Pod <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">secret-demo</strong></span><span class="No-Break">.</span></li>
				<li>The Pod has one container named <strong class="source-inline">demo</strong> based on the <span class="No-Break">NGINX image.</span></li>
				<li>The container has three environment variables that get their values <span class="No-Break">from secrets:</span><ul><li><strong class="source-inline">DB_HOST</strong> gets its value from the <strong class="source-inline">DB_HOST</strong> key in the <span class="No-Break"><strong class="source-inline">db-secret</strong></span><span class="No-Break"> secret</span></li><li><strong class="source-inline">DB_USER</strong> gets its value from the <strong class="source-inline">DB_USER</strong> key in the <span class="No-Break"><strong class="source-inline">db-secret</strong></span><span class="No-Break"> secret</span></li><li><strong class="source-inline">DB_PASSWORD</strong> gets its value from the <strong class="source-inline">DB_PASSWORD</strong> key in the <span class="No-Break"><strong class="source-inline">db-secret</strong></span><span class="No-Break"> secret</span></li></ul></li>
				<li>The container mounts a volume called <strong class="source-inline">secrets-volume</strong> at the <strong class="source-inline">/etc/secrets</strong> path. This volume <span class="No-Break">is read-only.</span></li>
				<li>The <strong class="source-inline">secrets-volume</strong> volume uses the <strong class="source-inline">app-secret</strong> secret as its backing store. So, any<a id="_idIndexMarker136"/> keys/values in <strong class="source-inline">app-secret</strong> will be exposed as files in <strong class="source-inline">/etc/secrets</strong> in <span class="No-Break">the container.</span></li>
			</ul>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor052"/>Summary</h1>
			<p>In this chapter, we covered the fundamental architecture and components that make up a Kubernetes cluster. Understanding the control plane, nodes, and their components is crucial for operating <span class="No-Break">Kubernetes effectively.</span></p>
			<p>We looked at how the API server, etcd, controller manager, and schedulers in the control plane manage and maintain desired cluster state. kubelet and kube-proxy run on nodes to communicate with the control plane and manage containers. Getting familiar with these building blocks provides a mental model for how Kubernetes <span class="No-Break">functions internally.</span></p>
			<p>We also explored the main API resources used to deploy and manage applications, including Pods, Deployments, StatefulSets, Jobs, and Services. Pods encapsulate containers and provide networking and storage for closely related containers. Deployments and StatefulSets allow declarative management of pod replicas and provide self-healing capabilities. Jobs enable batch workloads to run to completion. Services enable loose coupling between pods and provide <span class="No-Break">stable networking.</span></p>
			<p>We discussed how ingress resources and ingress controllers configure external access to cluster services through routing rules. The new Gateway API provides a centralized way to manage ingress configuration. PersistentVolumes and PersistentVolumeClaims allow portable, network-attached storage to be provisioned and consumed efficiently. StorageClasses enable different classes of storage to <span class="No-Break">be offered.</span></p>
			<p>Finally, we looked at how ConfigMaps and Secrets allow configuration data and sensitive data to be injected into pods in a decoupled manner. Overall, these API resources provide powerful abstractions for deploying and managing <span class="No-Break">applications robustly.</span></p>
			<p>Learning these fundamental concepts equips you to use Kubernetes effectively. You now understand how a cluster is put together, how applications can be deployed and managed in line with the desired state, and how the supporting resources including storage, configuration, and secrets work. This critical foundation enables you to start deploying applications on Kubernetes and leverage its automation capabilities for self-healing, scaling, and management. The knowledge gained in this chapter will be indispensable as we <span class="No-Break">move forward.</span></p>
			<p>In the next chapter, we will do some hands-on exercises with Kubernetes to apply all the concepts that we <span class="No-Break">studied here.</span></p>
		</div>
	</body></html>