- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing Container Images with Harbor
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we covered the tools in the Tanzu portfolio that help
    us build cloud-native applications. We started our first segment with an overview
    of the evolution of building, running, and managing modern cloud-native applications
    and their platforms. Then, we saw how we can start application development using
    templates, how to build secure container images, how to quickly provision backing
    services for the applications, and how to manage APIs using various Tanzu products.
    After learning about building cloud-native applications, in this chapter, we will
    take a deep dive into various aspects of running them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: As the title of this chapter indicates, we will learn how to manage our container
    images and securely make them accessible using Harbor to deploy our applications
    on Kubernetes. Harbor is an open source container registry project under the **Cloud
    Native Computing Foundation** (**CNCF**) umbrella. Despite Harbor being a fully
    open source tool, we have included it in this book for three main reasons. Firstly,
    Harbor was incubated by VMware and donated to CNCF in mid-2018\. VMware is also
    one of the major contributors to the project and has actively invested in Harbor
    since then. Secondly, Harbor has also been recognized as a graduate project under
    the CNCF umbrella, which is a state that is tagged as a very popular, mature,
    and stable project within the CNCF ecosystem. Finally, the main reason to include
    Harbor in this book is that VMware, being a significant stakeholder in this project,
    also provides commercial enterprise support for Harbor as a part of its Tanzu
    portfolio.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Sidenote
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Henceforth, in this chapter, we will refer to a *container image* as an *image*
    only for brevity.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover Harbor in detail by covering the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '**Why Harbor?**: A walkthrough of the features and capabilities of Harbor'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unboxing Harbor**: A detailed overview of the anatomy of Harbor'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Getting started with Harbor**: Learn how to install and configure Harbor'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Common day-2 operations with Harbor**: Learn how to perform various configuration
    and usage-related activities on Harbor'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by learning about the background of Harbor.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Why Harbor?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will review the various features, capabilities, and reasons
    to consider using Harbor as a container registry. These reasons will be explained
    using the security, control, and extensibility features of Harbor as described
    henceforth.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Using Harbor for security
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are some strong security reasons and features that make Harbor a good
    choice for a container registry, which shifts security to a proactive measure
    rather than reactive in the applications’ journey toward production. Let’s review
    these security benefits:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Harbor comes with the capability to scan each image for the presence of **critical
    vulnerability exposures** (**CVEs**) as a result of certain software libraries
    and operating system versions used in the image. Such scanning provides a detailed
    report of the CVEs found in the corresponding image, along with their severity
    level, details of the exposure, and the version of the software in which that
    CVE is remediated. We can get such scanning results using either the web portal
    or using the REST APIs provided by Harbor. Harbor also allows you to use an external
    image scanning tool in place of or in addition to the default one. Such visibility
    of the possible security loopholes in the images could provide a preventative
    security posture well in advance in the application deployment process.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harbor 具备扫描每个镜像是否存在**关键漏洞暴露**（**CVEs**）的功能，这些漏洞可能源自镜像中使用的某些软件库和操作系统版本。此类扫描提供了详细的
    CVE 报告，包括发现的 CVE 及其严重性等级、漏洞的详细信息以及该 CVE 已被修复的软版本。我们可以通过 web 门户或使用 Harbor 提供的 REST
    API 获取这些扫描结果。Harbor 还允许您使用外部镜像扫描工具，替代或附加默认工具。通过这种方式，您可以在应用部署过程中提前了解镜像中可能存在的安全漏洞，从而提供预防性的安全防护。
- en: Depending on the application environment and the preferred tolerance level,
    Harbor also provides a way to prevent its clients from pulling such images that
    are scanned for CVEs and contain CVEs higher than the allowed severity level.
    For example, we can configure a policy in Harbor that any image that has CVEs
    found with categories more than medium severity in a project named `Production
    Repo` may not be pulled to deploy containers. This capability provides required
    guardrails to prevent damage at the front gate itself. It ensures that the flagged
    images are never allowed to be pulled to run workloads and allow bad actors to
    exploit them later.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据应用环境和偏好的容忍度，Harbor 还提供了一种方法，防止客户端拉取那些已扫描 CVE 且包含高于允许严重性等级的 CVE 的镜像。例如，我们可以在
    Harbor 中配置一个策略，要求任何在名为 `Production Repo` 的项目中发现的 CVE 严重性高于中等的镜像，无法被拉取来部署容器。此功能提供了所需的保护措施，防止在前门就发生损害，确保被标记的镜像永远无法被拉取来运行工作负载，避免不法分子稍后利用它们。
- en: Harbor also supports integrations with Notary ([https://github.com/notaryproject/notary](https://github.com/notaryproject/notary)),
    which is an open source project that can digitally sign the images for authenticity.
    You can create a container deployment pipeline using such an image signing utility
    to allow only signed and hence authorized images to be deployed in your production
    environment. Such an arrangement can greatly enhance your security posture as
    no unverified, unscanned, or potentially dangerous images can be deployed in your
    environment.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harbor 还支持与 Notary（[https://github.com/notaryproject/notary](https://github.com/notaryproject/notary)）的集成，Notary
    是一个开源项目，可以为镜像进行数字签名以保证其真实性。您可以使用这种镜像签名工具创建一个容器部署管道，从而仅允许签名且经过授权的镜像在生产环境中进行部署。这样的安排可以大大增强您的安全防护，因为在您的环境中无法部署任何未验证、未扫描或潜在危险的镜像。
- en: Harbor has robust **role-based access control** (**RBAC**) capabilities. It
    allows you to configure users with two levels, mainly at the project level and
    at the system level, to provide the required control and flexibility for a multi-tenant
    environment.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harbor 拥有强大的**基于角色的访问控制**（**RBAC**）功能。它允许您在项目级别和系统级别配置用户，提供在多租户环境中所需的控制和灵活性。
- en: Moreover, Harbor also allows you to separate user accounts from system accounts
    (known as **r****obot accounts** in Harbor) that can be used for **continuous
    integration** (**CI**) and **continuous deployment** (**CD**) automation processes.
    We may specify required permissions to such robot accounts to perform only allowed
    operations using the automation processes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Harbor 还允许您将用户帐户与系统帐户（在 Harbor 中称为**机器人帐户**）分开，后者可以用于**持续集成**（**CI**）和**持续部署**（**CD**）自动化流程。我们可以为这些机器人帐户指定所需的权限，使其仅能通过自动化流程执行允许的操作。
- en: We may create a hub-and-spoke architecture while using Harbor as the hub that
    replicates images to and from either external or other internal Harbor container
    registries. An example of such a deployment is shown in *Figure 6**.1*. Such an
    arrangement may allow organizations to prevent their internal users from pulling
    arbitrary and insecure images from unauthorized sources. But at the same time,
    it allows them to pull those images from only the internally deployed Harbor,
    which would have replicated authorized images from an external image repository.
    This feature provides the required control to ensure security without affecting
    developers’ freedom and productivity.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以创建一个中心-辐射架构，在这种架构中，Harbor 作为中心节点，负责将镜像从外部或其他内部 Harbor 容器注册表之间进行复制。这样的部署示例如
    *图 6.1* 所示。通过这种安排，组织可以防止内部用户从未经授权的来源拉取任意和不安全的镜像。与此同时，它允许他们只从内部部署的 Harbor 拉取镜像，而这些镜像已经从外部镜像库复制了授权的镜像。这个功能提供了所需的控制机制，以确保安全，同时不影响开发人员的自由和生产力。
- en: As we will see later in this chapter, Harbor has several components and supports
    many external integrations for various capabilities. To ensure the safety of such
    data transfers, all these inter-component communication channels use **Transport
    Layer Security** (**TLS**) encryption.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如我们在本章稍后将看到的，Harbor 拥有多个组件，并支持许多外部集成，以实现各种功能。为了确保这些数据传输的安全性，所有这些组件间的通信通道都使用
    **传输层安全性**（**TLS**）加密。
- en: After reviewing the key features of Harbor around security, let’s check what
    its benefits are from an operational control point of view.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了 Harbor 在安全方面的关键特性之后，我们来看看从运营控制的角度来看，它的优势是什么。
- en: Using Harbor for operational control
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Harbor 进行运营控制
- en: There are several popular container registries available in the market as online
    **Software-as-a-Service** (**SaaS**) offerings, including Docker Hub, **Google
    Container Registry** (**GCR**), and offerings from many other cloud providers.
    The point where Harbor differs from these online options is the fact that it can
    be deployed in an air-gapped environment. When there is a need to keep the application
    images private and on-premise, we need an offering like Harbor. With such on-premises
    deployments, as discussed in the previous section about security-specific reasons,
    Harbor provides a control mechanism to expose only authorized images that are
    replicated in Harbor from external sources for internal consumption. This way,
    the operators can prevent internal image users from downloading potentially vulnerable
    images from unauthorized sources.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 市场上有多个流行的容器注册表，作为在线 **软件即服务**（**SaaS**）提供，包括 Docker Hub、**Google 容器注册表**（**GCR**）以及许多其他云服务提供商的产品。Harbor
    与这些在线选项的不同之处在于，它可以部署在隔离的环境中。当需要保持应用镜像私密且本地存储时，我们需要像 Harbor 这样的解决方案。通过此类本地部署，如前一节关于安全性的讨论，Harbor
    提供了一个控制机制，确保仅暴露从外部源复制到 Harbor 中的授权镜像供内部使用。这样，运营人员可以防止内部镜像用户从未经授权的源下载可能存在漏洞的镜像。
- en: Additionally, Harbor is also an open source community-driven project that is
    at the **Graduated** maturity level in CNCF, like Kubernetes. CNCF only graduates
    an open source project when there is a significant community contribution and
    adoption. Since VMware is one of the major contributors to the project, it also
    provides commercial support for Harbor.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Harbor 还是一个由社区驱动的开源项目，处于 CNCF 的 **毕业级**成熟度阶段，类似于 Kubernetes。CNCF 只有在开源项目有显著的社区贡献和采用时才会将其毕业。由于
    VMware 是该项目的主要贡献者之一，它也为 Harbor 提供商业支持。
- en: Along with the point of being a CNCF-mature and commercially supported open
    source project, Harbor has an array of multi-tenancy features. We will visit some
    of these features later in this chapter. But at a high level, Harbor admins can
    configure team-wise storage quotas for images and choose from different image
    vulnerability scanners, image retention periods, team-wise webhook configurations
    to trigger a CD pipeline, CVE whitelisting, and a few others. Having these configurations
    separate for different teams using the same deployment of Harbor provides the
    required operational control to Harbor admins, along with the required flexibility
    to the user groups.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, under the operational control area, Harbor provides various general
    administrative configurations that are common for the deployment and all user
    groups. Such configurations include the following administrative controls:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up all untagged artifacts using a garbage collection routine that can
    be triggered on-demand or scheduled
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing user groups and their permissions
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring external or internal authentication providers, including **Light-weight
    Directory Access Protocol** (**LDAP**) and **Open ID Connect** (**OIDC**) systems
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring custom **Open Container Initiative** (**OCI**) artifacts to store
    binary objects other than images in Harbor
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring image proxy caching to allow externally hosted images to be stored
    in an offline mode
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing key performance metrics to check on Harbor’s health
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling the distributed tracing telemetry data for enhanced troubleshooting
    capabilities for Harbor
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '**Open Container Initiative** (**OCI**) is an open governance structure for
    creating open industry standards around container formats and runtimes. Source:
    [https://opencontainers.org/](https://opencontainers.org/).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: After seeing how Harbor can help to obtain control over various types of configurations,
    let’s see one more category of reasons to use Harbor – its extensibility.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Using Harbor for its extensibility
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Harbor is a solution that is comprised of a few different microservices that
    work together to serve the purpose of being a purpose-built container registry.
    It has several components that can be replaced with other available options providing
    similar functionalities. Moreover, we can extend some functionalities to provide
    more choices for the end users to pick from. Such areas of extensibility include
    integration with an external container registry, CVE scanners, authentication
    providers, and OCI-compliant objects that can be hosted on Harbor. The following
    sections describe them in detail.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Extending image sources and destinations through replication
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Harbor allows you to create image replication rules to extend the image library
    using an external image repository. That way, the clients of Harbor can the pull
    required images from an external repository such as Docker Hub without accessing
    Docker Hub. Such extensions are helpful for an air-gapped deployment where open
    internet access and open image downloading from a public repository are not desirable
    from a security point of view. Additionally, Harbor allows you to create replication
    rules for push and pull operations for a bidirectional flow of artifacts. *Figure
    6**.1* shows how *the central Harbor repository* pulls (replicates) images from
    *Docker Hub* and *GCR* and then pushes those images to the *remote Harbor repositories*
    for a better network co-location for the nearby Kubernetes clusters. The arrows
    in the figure indicate the flow of images:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Harbor deployment topology to take advantage of the replication
    feature](img/B18145_06_01.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Harbor deployment topology to take advantage of the replication
    feature
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Such extensions of image repository locations for different sources and destinations
    can be very useful to provide controlled access to the replicated images from
    security and governance. Additionally, it can also help reduce network latency
    and bandwidth requirements.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: In addition to using Harbor for replication, we can also configure Harbor as
    a proxy to cache externally located images. This caching arrangement can help
    save network latency in transferring frequently used images and save the network
    bandwidth required for internet traffic. Additionally, using Harbor for caching
    may cache only used images for a given timeframe. And if the image is not actively
    pulled, then it is removed. However, such a proxy configuration allows more freedom
    to access any available images versus only replicated ones. Both replication and
    caching have their use cases, pros, and cons.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Adding or replacing vulnerability scanners
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By default, Harbor comes with Trivy ([https://github.com/aquasecurity/trivy](https://github.com/aquasecurity/trivy))
    for CVE scanning of images. However, you can incorporate your own instance of
    a Trivy implementation if you have one. You may also integrate a different CVE
    scanner with Harbor in place of or in addition to Trivy. This extension allows
    different teams to use their preferred scanner from the list of supported ones
    by Harbor. In the present scenario, Harbor supports Clair, Anchore, Aqua, DoSec,
    Sysdig Secure, and Tensor Security in addition to Trivy.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Extending authentication providers
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Harbor provides database-level authentication where user accounts can be directly
    configured in Harbor as the default and primitive approach. However, the administrator
    may configure Harbor to use either an LDAP/Active Directory service or an OIDC
    provider. In that case, such external authentication providers will be used to
    create and manage user accounts. Harbor will redirect authentication requests
    to these external authentication providers and based on the identity provided
    by the authentication provider, Harbor grants the required access to the user.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Harbor 提供数据库级别的身份验证，用户帐户可以直接在 Harbor 中配置，作为默认和原始方法。然而，管理员可以配置 Harbor 使用 LDAP/Active
    Directory 服务或 OIDC 提供者。在这种情况下，这些外部身份验证提供者将用于创建和管理用户帐户。Harbor 将把身份验证请求重定向到这些外部身份验证提供者，并根据身份验证提供者提供的身份信息，Harbor
    授予用户所需的访问权限。
- en: Extending user-defined OCI artifacts hosting
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展用户定义的 OCI 制品托管
- en: Along with images, Harbor can also store Helm charts and other user-defined
    OCI artifacts. Such artifacts can be **Kubeflow** data models, which are used
    for machine learning on Kubernetes. For such extensions, the objects must follow
    Harbor-specific configuration using a manifest file. The use cases of such user-defined
    extensions are rare but possible.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 除了镜像外，Harbor 还可以存储 Helm 图表和其他用户定义的 OCI 制品。这些制品可以是**Kubeflow** 数据模型，用于 Kubernetes
    上的机器学习。对于这种扩展，对象必须遵循 Harbor 特定的配置，并使用清单文件。这些用户定义扩展的使用场景较少，但也是可能的。
- en: So far in this chapter, we have seen different security, operational, and extensibility
    reasons explaining the *Why* behind using Harbor as a container repository. It
    is open source but supported by VMware and a lightweight, flexible, and purpose-built
    container registry that also helps enhance the overall container security posture
    via image scanning, replication, and signing features. In the next section of
    this chapter, we will discuss the *What* part of Harbor to see what is under the
    hood.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们已经看到了一些不同的安全、运营和可扩展性的原因，解释了使用 Harbor 作为容器注册表的*原因*。它是开源的，但由 VMware
    支持，是一个轻量级、灵活、专为容器注册设计的注册表，还通过镜像扫描、复制和签名功能帮助增强整体容器安全性。在本章的下一节中，我们将讨论 Harbor 的*功能*部分，了解它的内部工作原理。
- en: Unboxing Harbor
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开箱 Harbor
- en: After seeing some good reasons to consider using Harbor as a container artifact
    repository around business, security, operational control, and extensibility,
    let’s learn what Harbor is made up of. In this section, we will learn about the
    internal components and functions of Harbor. Being a container registry to serve
    the cloud-native community, Harbor itself is a cloud-native application comprised
    of multiple smaller microservices performing different activities. Let’s understand
    how they work together by providing an architectural overview of Harbor.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到一些关于使用 Harbor 作为容器制品库的商业、安全、运营控制和可扩展性等方面的好理由后，让我们了解一下 Harbor 的组成部分。在本节中，我们将学习
    Harbor 的内部组件和功能。作为一个服务于云原生社区的容器注册表，Harbor 本身是一个由多个较小的微服务组成的云原生应用，这些微服务执行不同的任务。让我们通过提供
    Harbor 的架构概述来理解它们如何协同工作。
- en: Architecture overview
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构概述
- en: 'Harbor has several internal and external components. As shown in *Figure 6**.2*,
    we can distribute these components into the following categories:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Harbor 有多个内部和外部组件。如 *图 6.2* 所示，我们可以将这些组件分为以下几类：
- en: '**Consumers**: Consist of all clients and client interfaces'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者**：由所有客户端和客户端接口组成'
- en: '**Fundamental Services**: Consist of all core functionalities that are part
    of the Harbor project and other key third-party projects that are essential components
    of the overall package'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础服务**：由 Harbor 项目中的所有核心功能和其他对整体包至关重要的第三方关键项目组成'
- en: '**Data Access Layer**: Consists of all the different data stores'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据访问层**：由所有不同的数据存储组成'
- en: '**Identity Providers**: Consist of all external authentication provider extensions'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**身份提供者**：由所有外部身份验证提供者扩展组成'
- en: '**Scan Providers**: Consist of all external image CVE scanner extensions'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扫描提供者**：由所有外部镜像 CVE 扫描扩展组成'
- en: '**Replicated Registry Providers**: Consist of all external image replication
    extensions:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复制的注册表提供者**：由所有外部镜像复制扩展组成：'
- en: '![Figure 6.2 – Harbor 2.0 architecture (https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor)](img/B18145_06_02.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – Harbor 2.0 架构（https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor)](img/B18145_06_02.jpg)'
- en: Figure 6.2 – Harbor 2.0 architecture ([https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor](https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor))
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – Harbor 2.0 架构 ([https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor](https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor))
- en: Let’s review some of the key components covered in *Figure 6**.2*. You will
    see these components deployed in your Kubernetes environment when we install and
    configure Harbor later in this chapter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下*图6.2*中涵盖的一些关键组件。你将在稍后安装和配置 Harbor 时，看到这些组件在你的 Kubernetes 环境中被部署。
- en: Harbor Chart Museum
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Harbor 图表博物馆
- en: As we discussed previously, along with images, Harbor also supports storing
    Helm charts. To support this feature, Harbor internally uses `my-harbor-chartmuseum`
    with a Kubernetes service running with the same name once you have a running instance
    of Harbor in your Kubernetes cluster.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，除了镜像，Harbor 还支持存储 Helm 图表。为了支持这一功能，Harbor 内部使用 `my-harbor-chartmuseum`，并且当你在
    Kubernetes 集群中运行一个 Harbor 实例时，会有一个名称相同的 Kubernetes 服务在后台运行。
- en: Harbor Core
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Harbor 核心
- en: As described in *Figure 6**.2*, Harbor Core is a collection of several modules
    that include key capabilities of Harbor being a container registry. These capabilities
    include concerns such as API management, authentication and authorization, interfacing
    glues, including pluggable image replication providers, image scanners, and image
    signature providers, and other foundational functionalities such as multitenancy
    capabilities, configuration management, artifact manager, and others. In our Kubernetes-based
    Harbor deployment, all the modules displayed in *Figure 6**.2* under `my-harbor-core`,
    and this is exposed as a Kubernetes service resource with the same name.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图6.2*所示，Harbor 核心是由多个模块组成的集合，这些模块包括 Harbor 作为容器注册中心的关键功能。这些功能包括诸如 API 管理、身份验证和授权、接口连接（包括可插拔的镜像复制提供者、镜像扫描器、镜像签名提供者）以及其他基础功能，如多租户能力、配置管理、工件管理器等。在我们的基于
    Kubernetes 的 Harbor 部署中，所有在*图6.2*中显示的模块都位于`my-harbor-core`下，并作为具有相同名称的 Kubernetes
    服务资源对外暴露。
- en: Harbor job service
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Harbor 作业服务
- en: This is Harbor’s asynchronous task execution engine that exposes the required
    REST APIs for other components to submit their job requests. One such example
    is a job to scan an image. You will see this microservice also getting deployed
    as its own Kubernetes deployment and a service named `my-harbor-jobservice`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Harbor 的异步任务执行引擎，暴露了所需的 REST API 供其他组件提交其作业请求。例如，一个作业是扫描一个镜像。你将看到这个微服务也会作为独立的
    Kubernetes 部署和名为`my-harbor-jobservice`的服务进行部署。
- en: Harbor Notary
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Harbor 公证服务
- en: Notary ([https://github.com/notaryproject/notary](https://github.com/notaryproject/notary))
    is a third-party open source project under the CNCF umbrella. It is used to provide
    content trust establishment capabilities, which are achieved through an image
    signing procedure. As reviewed under the security-related reasons to use Harbor,
    such an image signing capability could be a great way to ensure that only verified
    images are deployed in a Kubernetes environment. It allows the image publisher
    to digitally sign an image using a private key authenticating the signer. Then,
    the consumers of that image can verify the publisher/signer of the image and take
    an informed decision to either trust or distrust the image based on the digital
    signature and the associated metadata. In secured and fully automated Kubernetes
    platforms, such operations of image signing and their verification are the steps
    of a CI/CD pipeline. Notary provides this functionality using its two main components
    – the server and the signer. The Notary server is responsible to store content
    metadata, ensuring the validity of the uploaded content, attaching the timestamps,
    and serving this content to the clients when requested. On the other side, the
    Notary signer is responsible for storing the private signing keys in a separate
    database from the Notary server database and performing the signing operations
    using these keys as and when requested by the Notary server. You will see these
    two components deployed as Kubernetes deployment resources named `my-harbor-notary-server`
    and `my-harbor-notary-signer`, along with their corresponding service resources,
    in a Kubernetes-based Harbor deployment that we will cover later in this chapter.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Harbor portal
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name suggests, it is the `my-harbor-portal`, along with its corresponding
    service with the same name, later in this chapter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Harbor registry
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is based on the open source project named Distribution ([https://github.com/distribution/distribution](https://github.com/distribution/distribution)),
    which wraps functionalities to pack, ship, store, and deliver content. It implements
    the standards defined by the OCI Distribution Specification. It is the core library
    used for image registry operations and used by many open source and commercial
    registries, including Docker Hub, GitLab Container Registry, and DigitalOcean
    Container Registry, including Harbor. You will see this component deployed as
    a Kubernetes deployment resource named `my-harbor-registry`, along with its exposed
    service with the same name, later in this chapter.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: PostgreSQL database
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the main database of Harbor and is used to store all required configurations
    and metadata used in Harbor. It stores all Harbor constructs, including but not
    limited to the data related to projects, users, policies, scanners, charts, and
    images. It is deployed as a stateful set on a Kubernetes cluster called `my-harbor-postgresql`,
    along with its service resource exposed with the same name.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Redis cache
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is also deployed as a stateful set on Kubernetes and it is called `my-harbor-redis-master`.
    It is used as a key-value store to cache the required metadata used by the job
    service.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Trivy Scanner
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is an open source project by Aqua Security ([https://github.com/aquasecurity/trivy](https://github.com/aquasecurity/trivy))
    and the default image CVE scanner that is deployed with Harbor 2.x. It can scan
    operating system layers and language-specific packages that are used in the image
    to find known vulnerability exposures present in those artifacts. Harbor uses
    such scanners to provide a comprehensive scanning capability. Such scanners can
    scan images and produce detailed reports, including CVE metadata. Such metadata
    includes a list of CVE numbers, vulnerability areas, severity levels, fixed versions
    if available, and other details. You will see this component getting deployed
    as `my-harbor-trivy` as a Kubernetes deployment post our installation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: What is my-harbor?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: The prefix, `my-harbor`, that you have seen in the names of different components
    that will be deployed in your Kubernetes cluster is an arbitrary name given to
    the Helm chart instance of Harbor at the time of deployment. It can be replaced
    with any other name.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several other internal and external components described in *Figure
    6**.2* other than what we have covered here. The components we have covered are
    based on what is deployed on our Kubernetes cluster under Harbor’s namespace.
    To learn more details about Harbor’s architecture, visit this link: [https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor](https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve learned about the different modules of Harbor, let’s learn how
    to install and configure it on a Kubernetes cluster.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Harbor
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to install and configure a Harbor registry
    instance on an existing Kubernetes cluster. But before we do that, we need to
    ensure that the following prerequisites are met.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the prerequisites for the Harbor installation instructions
    given in this section:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes cluster with version 1.10+
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open internet connectivity from the Kubernetes cluster
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The operator machine should have the following tools:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker` CLI: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`helm` CLI version 2.8.0+: [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/)'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl` CLI version 1.10+: [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: There should be a default **StorageClass** configured in your Kubernetes cluster
    that Harbor can use to create required storage volumes. By default, Harbor will
    need several **PersistentVolumeClaim** resources that are used by Redis cache,
    a PostgreSQL database, the registry storage, and more.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The infrastructure running the Kubernetes cluster should be able to expose an
    externally accessible IP address upon the creation of a `LoadBalancer` type Kubernetes
    service, making it accessible outside the Kubernetes cluster. We have used `LoadBalancer`
    type service deployed in the GKE cluster.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The operator machine should have a browser to access the Harbor GUI.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional learning
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: To keep the deployment of Harbor on Kubernetes simpler, we could also deploy
    it as a `NodePort` service and access it externally using the Kubernetes node
    IP address and the port associated with the Harbor service. However, we cannot
    access this deployment of Harbor from a Docker client to push and pull images
    using the node port. This is because the Docker client can only connect to a registry
    using port `443` (HTTPS) or port `80` (HTTP).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a load balancer machine or a service instance for each `LoadBalancer`
    type service running on a Kubernetes cluster is not an efficient approach when
    several `LoadBalancer` type services are running on a Kubernetes cluster. Because,
    in this way, we may need several external load balancer instances for each externally
    facing service in the Kubernetes cluster. It is especially inefficient in a public
    cloud environment such as GKE, where such load balancer instances are charged
    separately. In a more sophisticated way, we can expose such externally facing
    services outside of a Kubernetes cluster using an **Ingress Controller** service
    running in the Kubernetes cluster. **Contour** ([https://projectcontour.io/](https://projectcontour.io/))
    is one such open source project under CNCF to be an Ingress Controller used for
    this reason that is supported by VMware and supplied with **Tanzu Kubernetes Grid**,
    which we will cover in the next chapter.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: To keep things simple for learning, we have used GKE to expose Harbor externally
    for this chapter. However, AWS Elastic Kubernetes Service and Azure Kubernetes
    Service can also provision the load balancers, similar to GKE. If your Kubernetes
    cluster is running on an infrastructure that cannot automatically expose a `LoadBalancer`
    service using an external endpoint, you can also do that manually. For that, you
    need to create a reverse-proxy server such as Nginx and deploy Harbor as a NodePort
    service rather than a `LoadBalancer` service using the `--set service.type=NodePort`
    option for the `helm install` command for Harbor deployment, which will be covered
    later in the installation steps.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s start installing Harbor.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Installing Harbor
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While there are various ways to install and configure the Harbor repository,
    we will use a simple and easy-to-follow Bitnami-provided Helm chart approach to
    get a Harbor instance up and running in a Kubernetes cluster. It is required that
    all the steps in this section are performed using the same workstation. Let’s
    get started:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Add Bitnami’s Helm repository to your workstation:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create a namespace on the cluster to deploy all Harbor components within it:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Install the Helm chart to get Harbor components deployed in the `harbor` namespace.
    It should deploy all Harbor components to expose a `LoadBalancer` type Kubernetes
    service to expose the portal:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Within just a few seconds, you will see the following output if the command
    was successfully executed. Here, `my-harbor` is just a name given to this Helm
    deployment that can be used to upgrade or delete the installation with that name
    reference:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding code snippet shows the output of the successful execution of the
    `helm` `install` command.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the status of all the pods deployed in the `harbor` namespace to ensure
    everything is running fine:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should see all the pods running and all the containers in a ready state,
    as shown in the following code snippet:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Retrieve the admin user password generated by the installation. Note down this
    password as we will use it later to access the Harbor GUI:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Retrieve the external IP address on which Harbor service is exposed by running
    the following command:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Access the Harbor portal from a browser using `https://<external-ip>/`. You
    may need to ignore the browser security prompts as we are not using a certificate
    signed by a valid certificate authority for the portal. You should see the following
    screen:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Harbor login page](img/B18145_06_03.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Harbor login page
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the username as `admin` and the password that you retrieved in *step
    5* previously. You should be able to log in successfully and see the following
    home page of the Harbor GUI:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Harbor landing page](img/B18145_06_04.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Harbor landing page
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: If you can log in using the aforementioned credentials and see the previous
    screen, you are done with the required setup to have a Harbor instance on your
    Kubernetes cluster running. As the next step, we will perform a small smoke test
    by pushing an image to this registry to validate our setup.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Validating the setup
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to validate the installation:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Retrieve the CA certificate used by Harbor by following these steps. We need
    to add this certificate in the trust store used by the Docker client to connect
    to the Harbor deployment:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go into the **library** project by clicking on the highlighted link:'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Clicking the library project](img/B18145_06_05.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Clicking the library project
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the certificate into your workstation using the **REGISTRY CERTIFICATE**
    link shown in the following screenshot:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Downloading the registry certificate](img/B18145_06_06.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Downloading the registry certificate
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the Harbor portal certificate in the trust store used by the Docker
    client running in your workstation as explained here, depending on your operating
    system: [https://docs.docker.com/registry/insecure/#use-self-signed-certificates](https://docs.docker.com/registry/insecure/#use-self-signed-certificates).'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For macOS, run the following command:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, replace `<downloaded-harbor-certificate>` with the certificate path that
    was downloaded in *step 1*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Restart the Docker daemon in your workstation if it is running; otherwise, start
    it to make the certificate visible to the Docker client.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a local DNS entry in the `/etc/hosts` file to link the default domain
    name, `core.harbor.domain`, with the external load balancer IP address that was
    used to access the portal in the previous steps.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Log in to the Harbor registry using the `docker` CLI to enable push/pull operations:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Push an image into your newly setup Harbor registry using the Docker client:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the `busybox:latest` image using the `docker` CLI. The following command
    will download the image from the Docker Hub repository into your local workstation:'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Verify the presence of the `busybox:latest` image in your local image repository
    by running the following command:'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You should see a record indicating the `busybox` image with a `latest` tag as
    a result of the previous command.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tag the `busybox:latest` image to prepare it to push to our Harbor registry
    instance:'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Push the newly tagged image to your Harbor instance:'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Upon this successful push operation, you should be able to see the image listed
    in your **library** project in the portal, as shown in the following screenshot:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Verifying the new image presence](img/B18145_06_07.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Verifying the new image presence
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: If you see the `busybox` image in the previous screen, your Harbor installation
    is complete. You may also choose to perform a pull test either from the same workstation
    by removing the existing image from the local repository, or a different workstation
    that has a Docker client. If you prefer to use a different workstation, you may
    need to configure the Harbor certificate there and authenticate against the Harbor
    repository using the `docker` CLI.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover some of the crucial day-2 operations for
    Harbor using this installation.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Common day-2 operations with Harbor
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a working setup of Harbor, let’s look into some important
    day-2 operations that we may need to perform on it. We will cover the following
    activities in this section:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Creating and configuring a project in Harbor, which is the multi-tenancy enabling
    construct on Harbor that allows different teams to have separate configurations
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring automated image scanning and working with the scan results
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preventing insecure images from being used to deploy containers using them
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring image replication to allow selective access to the external images
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing a cleanup of unused image tags to free up the storage quota of a
    project
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see here, we have lots of ground to cover. So, let’s get started.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a project in Harbor
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create a new project called **project-1** and configure it using the
    admin user we used previously to verify the installation:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **NEW PROJECT** button on the home screen of the Harbor portal
    after logging in as **admin**:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Creating a new project](img/B18145_06_08.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Creating a new project
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Configure the project’s name and quota, as shown in the following screenshot,
    and click **OK**. We will keep this project private, which means that you need
    to get authenticated to pull images. We will also not configure this project to
    be used as a pull-through cache for an external registry such as Docker Hub:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Entering New Project details](img/B18145_06_09.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Entering New Project details
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'You should be able to see a new project listed on the screen, as shown in the
    following screenshot:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Verifying the new project’s presence](img/B18145_06_10.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Verifying the new project’s presence
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have this project created, the users of this project may push their
    images with the `core.harbor.domain/project-1/` prefix for the images. The project
    will not accept new images after it reaches its 2 GB storage quota, which we configured
    while creating it in *step 2*. Now, let’s learn about some of the important project-level
    configurations.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Configuring image scanning for a project in Harbor
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scanning images for the presence of CVEs is an important security feature of
    Harbor. Now, let’s configure **project-1** to enable automated CVE scanning as
    soon as an image is pushed in this project using the default scanner, **Trivy**:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the **project-1** detail page by clicking on the highlighted link:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Selecting the newly created project](img/B18145_06_11.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Selecting the newly created project
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the **Configuration** tab and select the highlighted option to scan every
    image upon push. Finally, click **SAVE**:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Enabling image auto-scanning for the project](img/B18145_06_12.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Enabling image auto-scanning for the project
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s verify whether this configuration works by pushing the `busybox:latest`
    image that we pulled from Docker Hub previously. This should be present in your
    local workstation’s Docker repository.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the `busybox:latest` image to be pushed into the **project-1** repository
    of Harbor by applying the appropriate tag:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Push the newly tagged `busybox:latest` image to the **project-1** repository:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As you can see in the result of the previous command, Harbor used the same layer
    of the image that we had pushed under the **library** project during the verification
    process we performed earlier.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'erify the image presence under **project-1** and click on the **project/busybox**
    link that is highlighted in the following screenshot:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Clicking the image repository](img/B18145_06_13.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Clicking the image repository
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following screenshot, the scanning of the `busybox:latest`
    image has already been completed with no vulnerabilities found in it. This scanning
    was triggered automatically upon pushing the new image into the repository:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Verifying the scanning results](img/B18145_06_14.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Verifying the scanning results
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s create a policy based on such scan results to prevent pulling an
    image that has CVEs of more than medium severity to prevent running a container
    with such vulnerabilities.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Preventing insecure images from being used in Harbor
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To stop insecure images from being pulled, navigate to the project landing
    page of **project-1**. You may see an option named **Prevent vulnerable images
    from running** under the **Configuration** tab, as shown in the following screenshot.
    Check this option, select **High** from the dropdown menu, and save the configuration
    changes:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Preventing image pulling with high and critical CVEs](img/B18145_06_15.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Preventing image pulling with high and critical CVEs
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s test this configuration change. For that, we need to push an insecure
    image into **project-1**. You may follow these steps to perform this test:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Pull the `nginx:1.9.5` image from Docker Hub, which is very old and full of
    CVEs:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Tag `nginx:1.9.5` for the Harbor **project-1** repository:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Push `nginx:1.9.5` to the Harbor **project-1** repository:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Verify that the image on Harbor under **project-1** has been scanned and showing
    CVEs, as shown in the following screenshot:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Verifying the CVE scan results](img/B18145_06_16.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Verifying the CVE scan results
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Delete the `nginx:1.9.5` image from the local Docker repository so that we
    can attempt to pull it from our Harbor **project-1** repository:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Try to pull `nginx:1.9.5` from the Harbor **project-1** repository:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As you can see, Harbor denied sending the image because of the CVEs present
    in the image above the configured tolerance threshold.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: We took this testing approach to keep it simple. But alternatively, you may
    also attempt to create a pod using this image on a Kubernetes cluster to mimic
    a practical scenario. The result would be the same and you may not create a pod
    using the `core.harbor.domain/project-1/nginx:1.9.5` image. However, to test creating
    a pod using this Harbor setup, you may need to add the DNS entries to all your
    Kubernetes cluster nodes’ `/etc/hosts` file. Alternatively, you might need to
    create a more production-like Harbor setup with a proper domain name that can
    be resolved using an external DNS record from within the Kubernetes cluster.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our configuration and its test to prevent insecure container
    images from being pulled from a Harbor repository. In the next section, we will
    learn how to configure a remote repository sync to allow internal developers to
    pull the required externally available allowed images via Harbor.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Replicating images in Harbor
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to configure image replication in Harbor.
    There are two types of replications in Harbor, as described here, along with their
    practical applications:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '**Push-based**: To configure a rule to push certain images from Harbor to another
    repository, which could be another remote Harbor deployment or even a public repository.
    This type of replication is very useful to implement a hub-and-spoke type of deployment
    where we need to make certain images available in the Kubernetes clusters running
    at edge locations. Having the required images on the same Kubernetes clusters
    available at the edge location could be a great help to reduce network latency
    and dependency (hence application availability) when the image pulls are required
    to deploy containers on the edge Kubernetes clusters. This scenario is depicted
    in *Figure 6**.1*.'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pull-based**: To configure a rule to pull certain images from a remote public
    or private repository. Such a replication policy allows access to developers of
    certain allowed container images from an external repository such as Docker Hub
    or GCR. This feature of Harbor not only allows freedom for developers to use approved
    externally hosted images but also allows operators to prevent the wild-wild-west
    situation where anyone may pull any image from an external repository. *Figure
    6**.1* shows how the central Harbor repository pulls required images from Docker
    Hub and GCR using this feature.'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we understand the types and their uses, let’s see how we can configure
    these replication rules in Harbor. Here, we will configure a pull-based policy
    to allow developers to access MySQL images from Docker Hub. We will configure
    a remote repository location and the replication rule in Harbor, followed by quickly
    verifying that these configurations are working as expected:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Add Docker Hub as an external registry endpoint:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Registries** option under the **Administration** menu and click
    on the **NEW ENDPOINT** button, as shown in the following screenshot:'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Adding a new external registry endpoint](img/B18145_06_17.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Adding a new external registry endpoint
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Select **Docker Hub** from the dropdown, enter a name and description, and
    click on the **TEST CONNECTION** button. You may leave the authentication details
    empty for this test. However, for a production-grade deployment, you should supply
    these credentials to prevent Docker Hub from applying an image pull rate limit.
    Docker Hub throttles unauthenticated pull requests with lower rate limits:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Adding Docker Hub details and testing the connection](img/B18145_06_18.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Adding Docker Hub details and testing the connection
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Click the **OK** button to save and exit.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verify the newly created entry under the **Registries** page, as shown in the
    following screenshot:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Verifying the presence of the Docker Hub endpoint](img/B18145_06_19.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – Verifying the presence of the Docker Hub endpoint
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a replication rule that allows us to pull MySQL images from Docker Hub
    using the registry endpoint we just created:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Replications** option under the **Administration** menu and
    click on the **NEW REPLICATION RULE** button, as shown in the following screenshot:'
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Creating a new replication rule](img/B18145_06_20.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Creating a new replication rule
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the name and the description of the replication rule, select the **Pull-based**
    replication option, select the Docker Hub registry endpoint from the dropdown
    that we created in *step 1*, provide image filter criteria, as shown in the following
    screenshot, and click the **SAVE** button:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.21 – Submitting the replication rule’s details](img/B18145_06_21.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: Figure 6.21 – Submitting the replication rule’s details
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: We will leave the other options as-is. These other options include the image
    destination details to specify if we want a different image folder name and the
    directory structure, which is different from the source. It also has the option
    to select when we want to trigger pulling the images matching the filter criteria.
    The default value of the same is to pull manually when required but we can also
    create a schedule-based pull. Then, we have the option to restrict the bandwidth
    requirements to prevent the network from being overwhelmed with a flood of pull
    requests being executed for large filter criteria. And finally, there is the option
    to enable/disable the images from being overwritten when there is an image with
    a different SHA but the same name and tag available on the source registry endpoint.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Customized filter patterns
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous configuration, we used very basic filter criteria to pick all
    MySQL images that have tags starting with `8.` characters. However, you may apply
    complex patterns here to allow/disallow images based on your requirements. You
    may learn more about such patterns here: [https://goharbor.io/docs/2.4.0/administration/configuring-replication/create-replication-rules/](https://goharbor.io/docs/2.4.0/administration/configuring-replication/create-replication-rules/).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Trigger the replication manually by selecting the newly created rule and clicking
    on the **REPLICATE** button:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Triggering image replication](img/B18145_06_22.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: Figure 6.22 – Triggering image replication
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'Confirm the replication by pressing the **REPLICATE** button:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.23 – Confirming image replication](img/B18145_06_23.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: Figure 6.23 – Confirming image replication
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify whether the replication execution was successful from the **Executions**
    section on the **Replications** page, as shown in the following screenshot. Depending
    on the network connection, it may take a few minutes before all the images are
    successfully replicated:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.24 – Verifying image replication execution](img/B18145_06_24.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: Figure 6.24 – Verifying image replication execution
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon successful completion of this replication, go to the **Projects** screen
    and click on the **library** project:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Opening the library project](img/B18145_06_25.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: Figure 6.25 – Opening the library project
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see a new namespace under the **Repositories** tab named **library/mysql**.
    Click on that link:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.26 – Verifying the newly replicated repository](img/B18145_06_26.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: Figure 6.26 – Verifying the newly replicated repository
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'You may see several images listed under **library/mysql** as a result of the
    replication operation:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.27 – Verifying the newly replicated repository’s content](img/B18145_06_27.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
- en: Figure 6.27 – Verifying the newly replicated repository’s content
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Pull one of these images into the local workstation’s Docker repository to
    verify it is working as expected:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This command should be able to pull the image successfully from our Harbor repository.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our replication configuration step. Now, the developers may pull
    a required MySQL image directly from the Harbor repository rather than getting
    them from Docker Hub.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: As the next day-2 activity in this list, we will learn how to configure a rule-based
    tag retention policy to clean up stale images and free up the storage.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Configuring rule-based tag retention policies in Harbor
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you saw in the previous configuration, we replicated over 25 different images
    for MySQL. In production-grade implementations, we often encounter situations
    where there are several stale images not being used but occupying the project’s
    allocated space quota. In our previous MySQL replication, we may see only a few
    image tags that are used, and we can remove the rest. For that, we will learn
    how to configure such automated tag-based retention policies to clean up old and
    useless content. Let’s get started:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the **Projects** screen and click on the **library** project:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.28 – Opening the library project](img/B18145_06_28.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
- en: Figure 6.28 – Opening the library project
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the **library** project, go to the **Policy** tab and click on the **ADD**
    **RULE** button:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.29 – Adding a new image retention rule](img/B18145_06_29.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: Figure 6.29 – Adding a new image retention rule
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter `mysql` as the matching repository, select `1` as the count to retain,
    and press the **ADD** button:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.30 – Entering details about the image retention rule](img/B18145_06_30.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
- en: Figure 6.30 – Entering details about the image retention rule
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: This policy will remove all MySQL images except for `mysql:8.0.27` because we
    pulled that one in the verification step earlier.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the new tag retention policy is in place. While we can also
    schedule this activity at a regular frequency, for now, we will run it manually
    using the **RUN** **NOW** button:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.31 – Verifying the creation of the image retention rule](img/B18145_06_31.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
- en: Figure 6.31 – Verifying the creation of the image retention rule
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Move on by pressing the **RUN** button by accepting the warning of a mass deletion
    operation:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.32 – Executing the image retention rule](img/B18145_06_32.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: Figure 6.32 – Executing the image retention rule
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify whether the clean-up activity was completed successfully by inspecting
    the activity log record, which indicates there was only 1 tag retained out of
    24 total. This was the expected outcome:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.33 – Verifying the image retention rule’s execution](img/B18145_06_33.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
- en: Figure 6.33 – Verifying the image retention rule’s execution
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Repositories** tab to list the existing repositories. On that
    screen, you should be able to see only 1 tag available for **mysql** instead of
    the 24 from earlier:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.34 – Verifying the remaining image count post-retention rule’s execution](img/B18145_06_34.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
- en: Figure 6.34 – Verifying the remaining image count post-retention rule’s execution
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the `mysql` repository to verify that the tags other than `8.0.27`
    have been deleted:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.35 – Verifying the remaining image’s post-retention rule’s execution](img/B18145_06_35.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: Figure 6.35 – Verifying the remaining image’s post-retention rule’s execution
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our tag retention policy configuration. Like tag retention, we
    may also configure tag immutability policies to configure certain critical image
    tags from being overwritten with a new version of the image. Without such a policy
    in place, you may push new images with changes but with the same tag value. Hence,
    an application that is tested against one tagged version of an image may not be
    able to fully ensure that the content of the same tag would be the same in the
    next pull of the image. This could potentially break applications from working
    in case of any unexpected changes in the newer version of the same tagged images
    are pushed. Ideally, this should not be the case as it is a poor development practice.
    But there should be some controls in place to prevent it from happening. Hence,
    Harbor helps in this case by allowing you to create policies where the Harbor
    users may not push images with the same tag with different content determined
    by the SHA-256 algorithm.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Important information
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: To safeguard our containerized applications from failing because of changed
    image content for the same tag value, it is always a good practice to pull images
    using their SHA values rather than their tags. Pulling an image using its SHA
    value (also known as the digest) ensures you always get the same image with the
    same content and there is no fear of it getting accidentally overwritten with
    the same tag value. For example, the image content pulled with the `docker pull
    ubuntu:20.04` command can be theoretically different for multiple executions,
    but the image content will always be the same when it is pulled with its digest
    using the `docker pull` `ubuntu@sha256:82becede498899ec668628e7cb0ad87b6e1c371cb8a1e597d83a47fac21d6af3`
    command.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: There are several more administrative and user day-2 activities we may need
    to perform on Harbor, including user account management, robot accounts for automation,
    image signing, webhook-based automation triggers, and many more. In this chapter,
    we covered some of the most common activities that we usually perform on a container
    registry such as Harbor. While we can add more details for other operations, the
    goal of this book is not to be a Harbor guide alone. But if you want, you may
    find more details in the official documentation for Harbor at [https://goharbor.io/docs/2.4.0/](https://goharbor.io/docs/2.4.0/).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s review what we have covered in this chapter. First, we discussed several
    benefits and the use cases of Harbor while explaining the *Why* behind using it.
    We looked at the different security-related benefits of Harbor, including image
    scanning, robust RBAC capabilities, and the ability to restrict public repository
    access requirements using image replications. For the operational control aspect,
    we discussed the benefits, such as on-premises and air-gapped deployment, a popular
    open source project under CNCF, comprehensive multi-tenancy, and administrative
    configurations. For the extensibility aspect, we saw how Harbor can be used with
    its replication feature of extending image library contents. Harbor’s pluggable
    model for vulnerability scanners and authentication providers was also discussed
    in this category.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: After that, we covered details of Harbor’s architecture and learned about the
    different components that make up Harbor in detail. Following this, we learned
    how to quickly get started with Harbor using the Bitnami Helm chart and verified
    the installation. Finally, we walked through some of the important day-2 operations
    around Harbor, including creating a project, performing image scanning, preventing
    risky images from being pulled, image replication from Docker Hub, and cleaning
    stale images to free up the storage quota for a project.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to run these images using Tanzu Kubernetes
    Grid, a multi-cloud Kubernetes offering from VMware.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
