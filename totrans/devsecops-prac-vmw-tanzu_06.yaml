- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing Container Images with Harbor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we covered the tools in the Tanzu portfolio that help
    us build cloud-native applications. We started our first segment with an overview
    of the evolution of building, running, and managing modern cloud-native applications
    and their platforms. Then, we saw how we can start application development using
    templates, how to build secure container images, how to quickly provision backing
    services for the applications, and how to manage APIs using various Tanzu products.
    After learning about building cloud-native applications, in this chapter, we will
    take a deep dive into various aspects of running them.
  prefs: []
  type: TYPE_NORMAL
- en: As the title of this chapter indicates, we will learn how to manage our container
    images and securely make them accessible using Harbor to deploy our applications
    on Kubernetes. Harbor is an open source container registry project under the **Cloud
    Native Computing Foundation** (**CNCF**) umbrella. Despite Harbor being a fully
    open source tool, we have included it in this book for three main reasons. Firstly,
    Harbor was incubated by VMware and donated to CNCF in mid-2018\. VMware is also
    one of the major contributors to the project and has actively invested in Harbor
    since then. Secondly, Harbor has also been recognized as a graduate project under
    the CNCF umbrella, which is a state that is tagged as a very popular, mature,
    and stable project within the CNCF ecosystem. Finally, the main reason to include
    Harbor in this book is that VMware, being a significant stakeholder in this project,
    also provides commercial enterprise support for Harbor as a part of its Tanzu
    portfolio.
  prefs: []
  type: TYPE_NORMAL
- en: Sidenote
  prefs: []
  type: TYPE_NORMAL
- en: Henceforth, in this chapter, we will refer to a *container image* as an *image*
    only for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover Harbor in detail by covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Why Harbor?**: A walkthrough of the features and capabilities of Harbor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unboxing Harbor**: A detailed overview of the anatomy of Harbor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Getting started with Harbor**: Learn how to install and configure Harbor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Common day-2 operations with Harbor**: Learn how to perform various configuration
    and usage-related activities on Harbor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by learning about the background of Harbor.
  prefs: []
  type: TYPE_NORMAL
- en: Why Harbor?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will review the various features, capabilities, and reasons
    to consider using Harbor as a container registry. These reasons will be explained
    using the security, control, and extensibility features of Harbor as described
    henceforth.
  prefs: []
  type: TYPE_NORMAL
- en: Using Harbor for security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are some strong security reasons and features that make Harbor a good
    choice for a container registry, which shifts security to a proactive measure
    rather than reactive in the applications’ journey toward production. Let’s review
    these security benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Harbor comes with the capability to scan each image for the presence of **critical
    vulnerability exposures** (**CVEs**) as a result of certain software libraries
    and operating system versions used in the image. Such scanning provides a detailed
    report of the CVEs found in the corresponding image, along with their severity
    level, details of the exposure, and the version of the software in which that
    CVE is remediated. We can get such scanning results using either the web portal
    or using the REST APIs provided by Harbor. Harbor also allows you to use an external
    image scanning tool in place of or in addition to the default one. Such visibility
    of the possible security loopholes in the images could provide a preventative
    security posture well in advance in the application deployment process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the application environment and the preferred tolerance level,
    Harbor also provides a way to prevent its clients from pulling such images that
    are scanned for CVEs and contain CVEs higher than the allowed severity level.
    For example, we can configure a policy in Harbor that any image that has CVEs
    found with categories more than medium severity in a project named `Production
    Repo` may not be pulled to deploy containers. This capability provides required
    guardrails to prevent damage at the front gate itself. It ensures that the flagged
    images are never allowed to be pulled to run workloads and allow bad actors to
    exploit them later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harbor also supports integrations with Notary ([https://github.com/notaryproject/notary](https://github.com/notaryproject/notary)),
    which is an open source project that can digitally sign the images for authenticity.
    You can create a container deployment pipeline using such an image signing utility
    to allow only signed and hence authorized images to be deployed in your production
    environment. Such an arrangement can greatly enhance your security posture as
    no unverified, unscanned, or potentially dangerous images can be deployed in your
    environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harbor has robust **role-based access control** (**RBAC**) capabilities. It
    allows you to configure users with two levels, mainly at the project level and
    at the system level, to provide the required control and flexibility for a multi-tenant
    environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, Harbor also allows you to separate user accounts from system accounts
    (known as **r****obot accounts** in Harbor) that can be used for **continuous
    integration** (**CI**) and **continuous deployment** (**CD**) automation processes.
    We may specify required permissions to such robot accounts to perform only allowed
    operations using the automation processes.
  prefs: []
  type: TYPE_NORMAL
- en: We may create a hub-and-spoke architecture while using Harbor as the hub that
    replicates images to and from either external or other internal Harbor container
    registries. An example of such a deployment is shown in *Figure 6**.1*. Such an
    arrangement may allow organizations to prevent their internal users from pulling
    arbitrary and insecure images from unauthorized sources. But at the same time,
    it allows them to pull those images from only the internally deployed Harbor,
    which would have replicated authorized images from an external image repository.
    This feature provides the required control to ensure security without affecting
    developers’ freedom and productivity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we will see later in this chapter, Harbor has several components and supports
    many external integrations for various capabilities. To ensure the safety of such
    data transfers, all these inter-component communication channels use **Transport
    Layer Security** (**TLS**) encryption.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After reviewing the key features of Harbor around security, let’s check what
    its benefits are from an operational control point of view.
  prefs: []
  type: TYPE_NORMAL
- en: Using Harbor for operational control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several popular container registries available in the market as online
    **Software-as-a-Service** (**SaaS**) offerings, including Docker Hub, **Google
    Container Registry** (**GCR**), and offerings from many other cloud providers.
    The point where Harbor differs from these online options is the fact that it can
    be deployed in an air-gapped environment. When there is a need to keep the application
    images private and on-premise, we need an offering like Harbor. With such on-premises
    deployments, as discussed in the previous section about security-specific reasons,
    Harbor provides a control mechanism to expose only authorized images that are
    replicated in Harbor from external sources for internal consumption. This way,
    the operators can prevent internal image users from downloading potentially vulnerable
    images from unauthorized sources.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Harbor is also an open source community-driven project that is
    at the **Graduated** maturity level in CNCF, like Kubernetes. CNCF only graduates
    an open source project when there is a significant community contribution and
    adoption. Since VMware is one of the major contributors to the project, it also
    provides commercial support for Harbor.
  prefs: []
  type: TYPE_NORMAL
- en: Along with the point of being a CNCF-mature and commercially supported open
    source project, Harbor has an array of multi-tenancy features. We will visit some
    of these features later in this chapter. But at a high level, Harbor admins can
    configure team-wise storage quotas for images and choose from different image
    vulnerability scanners, image retention periods, team-wise webhook configurations
    to trigger a CD pipeline, CVE whitelisting, and a few others. Having these configurations
    separate for different teams using the same deployment of Harbor provides the
    required operational control to Harbor admins, along with the required flexibility
    to the user groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, under the operational control area, Harbor provides various general
    administrative configurations that are common for the deployment and all user
    groups. Such configurations include the following administrative controls:'
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up all untagged artifacts using a garbage collection routine that can
    be triggered on-demand or scheduled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing user groups and their permissions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring external or internal authentication providers, including **Light-weight
    Directory Access Protocol** (**LDAP**) and **Open ID Connect** (**OIDC**) systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring custom **Open Container Initiative** (**OCI**) artifacts to store
    binary objects other than images in Harbor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring image proxy caching to allow externally hosted images to be stored
    in an offline mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing key performance metrics to check on Harbor’s health
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling the distributed tracing telemetry data for enhanced troubleshooting
    capabilities for Harbor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: '**Open Container Initiative** (**OCI**) is an open governance structure for
    creating open industry standards around container formats and runtimes. Source:
    [https://opencontainers.org/](https://opencontainers.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: After seeing how Harbor can help to obtain control over various types of configurations,
    let’s see one more category of reasons to use Harbor – its extensibility.
  prefs: []
  type: TYPE_NORMAL
- en: Using Harbor for its extensibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Harbor is a solution that is comprised of a few different microservices that
    work together to serve the purpose of being a purpose-built container registry.
    It has several components that can be replaced with other available options providing
    similar functionalities. Moreover, we can extend some functionalities to provide
    more choices for the end users to pick from. Such areas of extensibility include
    integration with an external container registry, CVE scanners, authentication
    providers, and OCI-compliant objects that can be hosted on Harbor. The following
    sections describe them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Extending image sources and destinations through replication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Harbor allows you to create image replication rules to extend the image library
    using an external image repository. That way, the clients of Harbor can the pull
    required images from an external repository such as Docker Hub without accessing
    Docker Hub. Such extensions are helpful for an air-gapped deployment where open
    internet access and open image downloading from a public repository are not desirable
    from a security point of view. Additionally, Harbor allows you to create replication
    rules for push and pull operations for a bidirectional flow of artifacts. *Figure
    6**.1* shows how *the central Harbor repository* pulls (replicates) images from
    *Docker Hub* and *GCR* and then pushes those images to the *remote Harbor repositories*
    for a better network co-location for the nearby Kubernetes clusters. The arrows
    in the figure indicate the flow of images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Harbor deployment topology to take advantage of the replication
    feature](img/B18145_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Harbor deployment topology to take advantage of the replication
    feature
  prefs: []
  type: TYPE_NORMAL
- en: Such extensions of image repository locations for different sources and destinations
    can be very useful to provide controlled access to the replicated images from
    security and governance. Additionally, it can also help reduce network latency
    and bandwidth requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to using Harbor for replication, we can also configure Harbor as
    a proxy to cache externally located images. This caching arrangement can help
    save network latency in transferring frequently used images and save the network
    bandwidth required for internet traffic. Additionally, using Harbor for caching
    may cache only used images for a given timeframe. And if the image is not actively
    pulled, then it is removed. However, such a proxy configuration allows more freedom
    to access any available images versus only replicated ones. Both replication and
    caching have their use cases, pros, and cons.
  prefs: []
  type: TYPE_NORMAL
- en: Adding or replacing vulnerability scanners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By default, Harbor comes with Trivy ([https://github.com/aquasecurity/trivy](https://github.com/aquasecurity/trivy))
    for CVE scanning of images. However, you can incorporate your own instance of
    a Trivy implementation if you have one. You may also integrate a different CVE
    scanner with Harbor in place of or in addition to Trivy. This extension allows
    different teams to use their preferred scanner from the list of supported ones
    by Harbor. In the present scenario, Harbor supports Clair, Anchore, Aqua, DoSec,
    Sysdig Secure, and Tensor Security in addition to Trivy.
  prefs: []
  type: TYPE_NORMAL
- en: Extending authentication providers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Harbor provides database-level authentication where user accounts can be directly
    configured in Harbor as the default and primitive approach. However, the administrator
    may configure Harbor to use either an LDAP/Active Directory service or an OIDC
    provider. In that case, such external authentication providers will be used to
    create and manage user accounts. Harbor will redirect authentication requests
    to these external authentication providers and based on the identity provided
    by the authentication provider, Harbor grants the required access to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Extending user-defined OCI artifacts hosting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Along with images, Harbor can also store Helm charts and other user-defined
    OCI artifacts. Such artifacts can be **Kubeflow** data models, which are used
    for machine learning on Kubernetes. For such extensions, the objects must follow
    Harbor-specific configuration using a manifest file. The use cases of such user-defined
    extensions are rare but possible.
  prefs: []
  type: TYPE_NORMAL
- en: So far in this chapter, we have seen different security, operational, and extensibility
    reasons explaining the *Why* behind using Harbor as a container repository. It
    is open source but supported by VMware and a lightweight, flexible, and purpose-built
    container registry that also helps enhance the overall container security posture
    via image scanning, replication, and signing features. In the next section of
    this chapter, we will discuss the *What* part of Harbor to see what is under the
    hood.
  prefs: []
  type: TYPE_NORMAL
- en: Unboxing Harbor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After seeing some good reasons to consider using Harbor as a container artifact
    repository around business, security, operational control, and extensibility,
    let’s learn what Harbor is made up of. In this section, we will learn about the
    internal components and functions of Harbor. Being a container registry to serve
    the cloud-native community, Harbor itself is a cloud-native application comprised
    of multiple smaller microservices performing different activities. Let’s understand
    how they work together by providing an architectural overview of Harbor.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Harbor has several internal and external components. As shown in *Figure 6**.2*,
    we can distribute these components into the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consumers**: Consist of all clients and client interfaces'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fundamental Services**: Consist of all core functionalities that are part
    of the Harbor project and other key third-party projects that are essential components
    of the overall package'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Access Layer**: Consists of all the different data stores'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identity Providers**: Consist of all external authentication provider extensions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scan Providers**: Consist of all external image CVE scanner extensions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replicated Registry Providers**: Consist of all external image replication
    extensions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Harbor 2.0 architecture (https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor)](img/B18145_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Harbor 2.0 architecture ([https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor](https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor))
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review some of the key components covered in *Figure 6**.2*. You will
    see these components deployed in your Kubernetes environment when we install and
    configure Harbor later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Harbor Chart Museum
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed previously, along with images, Harbor also supports storing
    Helm charts. To support this feature, Harbor internally uses `my-harbor-chartmuseum`
    with a Kubernetes service running with the same name once you have a running instance
    of Harbor in your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Harbor Core
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As described in *Figure 6**.2*, Harbor Core is a collection of several modules
    that include key capabilities of Harbor being a container registry. These capabilities
    include concerns such as API management, authentication and authorization, interfacing
    glues, including pluggable image replication providers, image scanners, and image
    signature providers, and other foundational functionalities such as multitenancy
    capabilities, configuration management, artifact manager, and others. In our Kubernetes-based
    Harbor deployment, all the modules displayed in *Figure 6**.2* under `my-harbor-core`,
    and this is exposed as a Kubernetes service resource with the same name.
  prefs: []
  type: TYPE_NORMAL
- en: Harbor job service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is Harbor’s asynchronous task execution engine that exposes the required
    REST APIs for other components to submit their job requests. One such example
    is a job to scan an image. You will see this microservice also getting deployed
    as its own Kubernetes deployment and a service named `my-harbor-jobservice`.
  prefs: []
  type: TYPE_NORMAL
- en: Harbor Notary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Notary ([https://github.com/notaryproject/notary](https://github.com/notaryproject/notary))
    is a third-party open source project under the CNCF umbrella. It is used to provide
    content trust establishment capabilities, which are achieved through an image
    signing procedure. As reviewed under the security-related reasons to use Harbor,
    such an image signing capability could be a great way to ensure that only verified
    images are deployed in a Kubernetes environment. It allows the image publisher
    to digitally sign an image using a private key authenticating the signer. Then,
    the consumers of that image can verify the publisher/signer of the image and take
    an informed decision to either trust or distrust the image based on the digital
    signature and the associated metadata. In secured and fully automated Kubernetes
    platforms, such operations of image signing and their verification are the steps
    of a CI/CD pipeline. Notary provides this functionality using its two main components
    – the server and the signer. The Notary server is responsible to store content
    metadata, ensuring the validity of the uploaded content, attaching the timestamps,
    and serving this content to the clients when requested. On the other side, the
    Notary signer is responsible for storing the private signing keys in a separate
    database from the Notary server database and performing the signing operations
    using these keys as and when requested by the Notary server. You will see these
    two components deployed as Kubernetes deployment resources named `my-harbor-notary-server`
    and `my-harbor-notary-signer`, along with their corresponding service resources,
    in a Kubernetes-based Harbor deployment that we will cover later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Harbor portal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name suggests, it is the `my-harbor-portal`, along with its corresponding
    service with the same name, later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Harbor registry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is based on the open source project named Distribution ([https://github.com/distribution/distribution](https://github.com/distribution/distribution)),
    which wraps functionalities to pack, ship, store, and deliver content. It implements
    the standards defined by the OCI Distribution Specification. It is the core library
    used for image registry operations and used by many open source and commercial
    registries, including Docker Hub, GitLab Container Registry, and DigitalOcean
    Container Registry, including Harbor. You will see this component deployed as
    a Kubernetes deployment resource named `my-harbor-registry`, along with its exposed
    service with the same name, later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: PostgreSQL database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the main database of Harbor and is used to store all required configurations
    and metadata used in Harbor. It stores all Harbor constructs, including but not
    limited to the data related to projects, users, policies, scanners, charts, and
    images. It is deployed as a stateful set on a Kubernetes cluster called `my-harbor-postgresql`,
    along with its service resource exposed with the same name.
  prefs: []
  type: TYPE_NORMAL
- en: Redis cache
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is also deployed as a stateful set on Kubernetes and it is called `my-harbor-redis-master`.
    It is used as a key-value store to cache the required metadata used by the job
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Trivy Scanner
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is an open source project by Aqua Security ([https://github.com/aquasecurity/trivy](https://github.com/aquasecurity/trivy))
    and the default image CVE scanner that is deployed with Harbor 2.x. It can scan
    operating system layers and language-specific packages that are used in the image
    to find known vulnerability exposures present in those artifacts. Harbor uses
    such scanners to provide a comprehensive scanning capability. Such scanners can
    scan images and produce detailed reports, including CVE metadata. Such metadata
    includes a list of CVE numbers, vulnerability areas, severity levels, fixed versions
    if available, and other details. You will see this component getting deployed
    as `my-harbor-trivy` as a Kubernetes deployment post our installation.
  prefs: []
  type: TYPE_NORMAL
- en: What is my-harbor?
  prefs: []
  type: TYPE_NORMAL
- en: The prefix, `my-harbor`, that you have seen in the names of different components
    that will be deployed in your Kubernetes cluster is an arbitrary name given to
    the Helm chart instance of Harbor at the time of deployment. It can be replaced
    with any other name.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several other internal and external components described in *Figure
    6**.2* other than what we have covered here. The components we have covered are
    based on what is deployed on our Kubernetes cluster under Harbor’s namespace.
    To learn more details about Harbor’s architecture, visit this link: [https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor](https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve learned about the different modules of Harbor, let’s learn how
    to install and configure it on a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Harbor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to install and configure a Harbor registry
    instance on an existing Kubernetes cluster. But before we do that, we need to
    ensure that the following prerequisites are met.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the prerequisites for the Harbor installation instructions
    given in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes cluster with version 1.10+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open internet connectivity from the Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The operator machine should have the following tools:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker` CLI: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`helm` CLI version 2.8.0+: [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl` CLI version 1.10+: [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: There should be a default **StorageClass** configured in your Kubernetes cluster
    that Harbor can use to create required storage volumes. By default, Harbor will
    need several **PersistentVolumeClaim** resources that are used by Redis cache,
    a PostgreSQL database, the registry storage, and more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The infrastructure running the Kubernetes cluster should be able to expose an
    externally accessible IP address upon the creation of a `LoadBalancer` type Kubernetes
    service, making it accessible outside the Kubernetes cluster. We have used `LoadBalancer`
    type service deployed in the GKE cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The operator machine should have a browser to access the Harbor GUI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional learning
  prefs: []
  type: TYPE_NORMAL
- en: To keep the deployment of Harbor on Kubernetes simpler, we could also deploy
    it as a `NodePort` service and access it externally using the Kubernetes node
    IP address and the port associated with the Harbor service. However, we cannot
    access this deployment of Harbor from a Docker client to push and pull images
    using the node port. This is because the Docker client can only connect to a registry
    using port `443` (HTTPS) or port `80` (HTTP).
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a load balancer machine or a service instance for each `LoadBalancer`
    type service running on a Kubernetes cluster is not an efficient approach when
    several `LoadBalancer` type services are running on a Kubernetes cluster. Because,
    in this way, we may need several external load balancer instances for each externally
    facing service in the Kubernetes cluster. It is especially inefficient in a public
    cloud environment such as GKE, where such load balancer instances are charged
    separately. In a more sophisticated way, we can expose such externally facing
    services outside of a Kubernetes cluster using an **Ingress Controller** service
    running in the Kubernetes cluster. **Contour** ([https://projectcontour.io/](https://projectcontour.io/))
    is one such open source project under CNCF to be an Ingress Controller used for
    this reason that is supported by VMware and supplied with **Tanzu Kubernetes Grid**,
    which we will cover in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: To keep things simple for learning, we have used GKE to expose Harbor externally
    for this chapter. However, AWS Elastic Kubernetes Service and Azure Kubernetes
    Service can also provision the load balancers, similar to GKE. If your Kubernetes
    cluster is running on an infrastructure that cannot automatically expose a `LoadBalancer`
    service using an external endpoint, you can also do that manually. For that, you
    need to create a reverse-proxy server such as Nginx and deploy Harbor as a NodePort
    service rather than a `LoadBalancer` service using the `--set service.type=NodePort`
    option for the `helm install` command for Harbor deployment, which will be covered
    later in the installation steps.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s start installing Harbor.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Harbor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While there are various ways to install and configure the Harbor repository,
    we will use a simple and easy-to-follow Bitnami-provided Helm chart approach to
    get a Harbor instance up and running in a Kubernetes cluster. It is required that
    all the steps in this section are performed using the same workstation. Let’s
    get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add Bitnami’s Helm repository to your workstation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a namespace on the cluster to deploy all Harbor components within it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the Helm chart to get Harbor components deployed in the `harbor` namespace.
    It should deploy all Harbor components to expose a `LoadBalancer` type Kubernetes
    service to expose the portal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Within just a few seconds, you will see the following output if the command
    was successfully executed. Here, `my-harbor` is just a name given to this Helm
    deployment that can be used to upgrade or delete the installation with that name
    reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet shows the output of the successful execution of the
    `helm` `install` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the status of all the pods deployed in the `harbor` namespace to ensure
    everything is running fine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see all the pods running and all the containers in a ready state,
    as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Retrieve the admin user password generated by the installation. Note down this
    password as we will use it later to access the Harbor GUI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Retrieve the external IP address on which Harbor service is exposed by running
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Access the Harbor portal from a browser using `https://<external-ip>/`. You
    may need to ignore the browser security prompts as we are not using a certificate
    signed by a valid certificate authority for the portal. You should see the following
    screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Harbor login page](img/B18145_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Harbor login page
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the username as `admin` and the password that you retrieved in *step
    5* previously. You should be able to log in successfully and see the following
    home page of the Harbor GUI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Harbor landing page](img/B18145_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Harbor landing page
  prefs: []
  type: TYPE_NORMAL
- en: If you can log in using the aforementioned credentials and see the previous
    screen, you are done with the required setup to have a Harbor instance on your
    Kubernetes cluster running. As the next step, we will perform a small smoke test
    by pushing an image to this registry to validate our setup.
  prefs: []
  type: TYPE_NORMAL
- en: Validating the setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to validate the installation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Retrieve the CA certificate used by Harbor by following these steps. We need
    to add this certificate in the trust store used by the Docker client to connect
    to the Harbor deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go into the **library** project by clicking on the highlighted link:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Clicking the library project](img/B18145_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Clicking the library project
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the certificate into your workstation using the **REGISTRY CERTIFICATE**
    link shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Downloading the registry certificate](img/B18145_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Downloading the registry certificate
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the Harbor portal certificate in the trust store used by the Docker
    client running in your workstation as explained here, depending on your operating
    system: [https://docs.docker.com/registry/insecure/#use-self-signed-certificates](https://docs.docker.com/registry/insecure/#use-self-signed-certificates).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For macOS, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, replace `<downloaded-harbor-certificate>` with the certificate path that
    was downloaded in *step 1*.
  prefs: []
  type: TYPE_NORMAL
- en: Restart the Docker daemon in your workstation if it is running; otherwise, start
    it to make the certificate visible to the Docker client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a local DNS entry in the `/etc/hosts` file to link the default domain
    name, `core.harbor.domain`, with the external load balancer IP address that was
    used to access the portal in the previous steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Log in to the Harbor registry using the `docker` CLI to enable push/pull operations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Push an image into your newly setup Harbor registry using the Docker client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the `busybox:latest` image using the `docker` CLI. The following command
    will download the image from the Docker Hub repository into your local workstation:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify the presence of the `busybox:latest` image in your local image repository
    by running the following command:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see a record indicating the `busybox` image with a `latest` tag as
    a result of the previous command.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tag the `busybox:latest` image to prepare it to push to our Harbor registry
    instance:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Push the newly tagged image to your Harbor instance:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upon this successful push operation, you should be able to see the image listed
    in your **library** project in the portal, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Verifying the new image presence](img/B18145_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Verifying the new image presence
  prefs: []
  type: TYPE_NORMAL
- en: If you see the `busybox` image in the previous screen, your Harbor installation
    is complete. You may also choose to perform a pull test either from the same workstation
    by removing the existing image from the local repository, or a different workstation
    that has a Docker client. If you prefer to use a different workstation, you may
    need to configure the Harbor certificate there and authenticate against the Harbor
    repository using the `docker` CLI.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover some of the crucial day-2 operations for
    Harbor using this installation.
  prefs: []
  type: TYPE_NORMAL
- en: Common day-2 operations with Harbor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a working setup of Harbor, let’s look into some important
    day-2 operations that we may need to perform on it. We will cover the following
    activities in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating and configuring a project in Harbor, which is the multi-tenancy enabling
    construct on Harbor that allows different teams to have separate configurations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring automated image scanning and working with the scan results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preventing insecure images from being used to deploy containers using them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring image replication to allow selective access to the external images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing a cleanup of unused image tags to free up the storage quota of a
    project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see here, we have lots of ground to cover. So, let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a project in Harbor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create a new project called **project-1** and configure it using the
    admin user we used previously to verify the installation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **NEW PROJECT** button on the home screen of the Harbor portal
    after logging in as **admin**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Creating a new project](img/B18145_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Creating a new project
  prefs: []
  type: TYPE_NORMAL
- en: 'Configure the project’s name and quota, as shown in the following screenshot,
    and click **OK**. We will keep this project private, which means that you need
    to get authenticated to pull images. We will also not configure this project to
    be used as a pull-through cache for an external registry such as Docker Hub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Entering New Project details](img/B18145_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Entering New Project details
  prefs: []
  type: TYPE_NORMAL
- en: 'You should be able to see a new project listed on the screen, as shown in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Verifying the new project’s presence](img/B18145_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Verifying the new project’s presence
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have this project created, the users of this project may push their
    images with the `core.harbor.domain/project-1/` prefix for the images. The project
    will not accept new images after it reaches its 2 GB storage quota, which we configured
    while creating it in *step 2*. Now, let’s learn about some of the important project-level
    configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring image scanning for a project in Harbor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scanning images for the presence of CVEs is an important security feature of
    Harbor. Now, let’s configure **project-1** to enable automated CVE scanning as
    soon as an image is pushed in this project using the default scanner, **Trivy**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the **project-1** detail page by clicking on the highlighted link:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Selecting the newly created project](img/B18145_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Selecting the newly created project
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the **Configuration** tab and select the highlighted option to scan every
    image upon push. Finally, click **SAVE**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Enabling image auto-scanning for the project](img/B18145_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Enabling image auto-scanning for the project
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s verify whether this configuration works by pushing the `busybox:latest`
    image that we pulled from Docker Hub previously. This should be present in your
    local workstation’s Docker repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the `busybox:latest` image to be pushed into the **project-1** repository
    of Harbor by applying the appropriate tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Push the newly tagged `busybox:latest` image to the **project-1** repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see in the result of the previous command, Harbor used the same layer
    of the image that we had pushed under the **library** project during the verification
    process we performed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'erify the image presence under **project-1** and click on the **project/busybox**
    link that is highlighted in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Clicking the image repository](img/B18145_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Clicking the image repository
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following screenshot, the scanning of the `busybox:latest`
    image has already been completed with no vulnerabilities found in it. This scanning
    was triggered automatically upon pushing the new image into the repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Verifying the scanning results](img/B18145_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Verifying the scanning results
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s create a policy based on such scan results to prevent pulling an
    image that has CVEs of more than medium severity to prevent running a container
    with such vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Preventing insecure images from being used in Harbor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To stop insecure images from being pulled, navigate to the project landing
    page of **project-1**. You may see an option named **Prevent vulnerable images
    from running** under the **Configuration** tab, as shown in the following screenshot.
    Check this option, select **High** from the dropdown menu, and save the configuration
    changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Preventing image pulling with high and critical CVEs](img/B18145_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Preventing image pulling with high and critical CVEs
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s test this configuration change. For that, we need to push an insecure
    image into **project-1**. You may follow these steps to perform this test:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pull the `nginx:1.9.5` image from Docker Hub, which is very old and full of
    CVEs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tag `nginx:1.9.5` for the Harbor **project-1** repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Push `nginx:1.9.5` to the Harbor **project-1** repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that the image on Harbor under **project-1** has been scanned and showing
    CVEs, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Verifying the CVE scan results](img/B18145_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Verifying the CVE scan results
  prefs: []
  type: TYPE_NORMAL
- en: 'Delete the `nginx:1.9.5` image from the local Docker repository so that we
    can attempt to pull it from our Harbor **project-1** repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Try to pull `nginx:1.9.5` from the Harbor **project-1** repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, Harbor denied sending the image because of the CVEs present
    in the image above the configured tolerance threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We took this testing approach to keep it simple. But alternatively, you may
    also attempt to create a pod using this image on a Kubernetes cluster to mimic
    a practical scenario. The result would be the same and you may not create a pod
    using the `core.harbor.domain/project-1/nginx:1.9.5` image. However, to test creating
    a pod using this Harbor setup, you may need to add the DNS entries to all your
    Kubernetes cluster nodes’ `/etc/hosts` file. Alternatively, you might need to
    create a more production-like Harbor setup with a proper domain name that can
    be resolved using an external DNS record from within the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our configuration and its test to prevent insecure container
    images from being pulled from a Harbor repository. In the next section, we will
    learn how to configure a remote repository sync to allow internal developers to
    pull the required externally available allowed images via Harbor.
  prefs: []
  type: TYPE_NORMAL
- en: Replicating images in Harbor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to configure image replication in Harbor.
    There are two types of replications in Harbor, as described here, along with their
    practical applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Push-based**: To configure a rule to push certain images from Harbor to another
    repository, which could be another remote Harbor deployment or even a public repository.
    This type of replication is very useful to implement a hub-and-spoke type of deployment
    where we need to make certain images available in the Kubernetes clusters running
    at edge locations. Having the required images on the same Kubernetes clusters
    available at the edge location could be a great help to reduce network latency
    and dependency (hence application availability) when the image pulls are required
    to deploy containers on the edge Kubernetes clusters. This scenario is depicted
    in *Figure 6**.1*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pull-based**: To configure a rule to pull certain images from a remote public
    or private repository. Such a replication policy allows access to developers of
    certain allowed container images from an external repository such as Docker Hub
    or GCR. This feature of Harbor not only allows freedom for developers to use approved
    externally hosted images but also allows operators to prevent the wild-wild-west
    situation where anyone may pull any image from an external repository. *Figure
    6**.1* shows how the central Harbor repository pulls required images from Docker
    Hub and GCR using this feature.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we understand the types and their uses, let’s see how we can configure
    these replication rules in Harbor. Here, we will configure a pull-based policy
    to allow developers to access MySQL images from Docker Hub. We will configure
    a remote repository location and the replication rule in Harbor, followed by quickly
    verifying that these configurations are working as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add Docker Hub as an external registry endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Registries** option under the **Administration** menu and click
    on the **NEW ENDPOINT** button, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Adding a new external registry endpoint](img/B18145_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Adding a new external registry endpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'Select **Docker Hub** from the dropdown, enter a name and description, and
    click on the **TEST CONNECTION** button. You may leave the authentication details
    empty for this test. However, for a production-grade deployment, you should supply
    these credentials to prevent Docker Hub from applying an image pull rate limit.
    Docker Hub throttles unauthenticated pull requests with lower rate limits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Adding Docker Hub details and testing the connection](img/B18145_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Adding Docker Hub details and testing the connection
  prefs: []
  type: TYPE_NORMAL
- en: Click the **OK** button to save and exit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verify the newly created entry under the **Registries** page, as shown in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Verifying the presence of the Docker Hub endpoint](img/B18145_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – Verifying the presence of the Docker Hub endpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a replication rule that allows us to pull MySQL images from Docker Hub
    using the registry endpoint we just created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Replications** option under the **Administration** menu and
    click on the **NEW REPLICATION RULE** button, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Creating a new replication rule](img/B18145_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Creating a new replication rule
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the name and the description of the replication rule, select the **Pull-based**
    replication option, select the Docker Hub registry endpoint from the dropdown
    that we created in *step 1*, provide image filter criteria, as shown in the following
    screenshot, and click the **SAVE** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.21 – Submitting the replication rule’s details](img/B18145_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.21 – Submitting the replication rule’s details
  prefs: []
  type: TYPE_NORMAL
- en: We will leave the other options as-is. These other options include the image
    destination details to specify if we want a different image folder name and the
    directory structure, which is different from the source. It also has the option
    to select when we want to trigger pulling the images matching the filter criteria.
    The default value of the same is to pull manually when required but we can also
    create a schedule-based pull. Then, we have the option to restrict the bandwidth
    requirements to prevent the network from being overwhelmed with a flood of pull
    requests being executed for large filter criteria. And finally, there is the option
    to enable/disable the images from being overwritten when there is an image with
    a different SHA but the same name and tag available on the source registry endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Customized filter patterns
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous configuration, we used very basic filter criteria to pick all
    MySQL images that have tags starting with `8.` characters. However, you may apply
    complex patterns here to allow/disallow images based on your requirements. You
    may learn more about such patterns here: [https://goharbor.io/docs/2.4.0/administration/configuring-replication/create-replication-rules/](https://goharbor.io/docs/2.4.0/administration/configuring-replication/create-replication-rules/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Trigger the replication manually by selecting the newly created rule and clicking
    on the **REPLICATE** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Triggering image replication](img/B18145_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.22 – Triggering image replication
  prefs: []
  type: TYPE_NORMAL
- en: 'Confirm the replication by pressing the **REPLICATE** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.23 – Confirming image replication](img/B18145_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.23 – Confirming image replication
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify whether the replication execution was successful from the **Executions**
    section on the **Replications** page, as shown in the following screenshot. Depending
    on the network connection, it may take a few minutes before all the images are
    successfully replicated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.24 – Verifying image replication execution](img/B18145_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.24 – Verifying image replication execution
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon successful completion of this replication, go to the **Projects** screen
    and click on the **library** project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Opening the library project](img/B18145_06_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.25 – Opening the library project
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see a new namespace under the **Repositories** tab named **library/mysql**.
    Click on that link:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.26 – Verifying the newly replicated repository](img/B18145_06_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.26 – Verifying the newly replicated repository
  prefs: []
  type: TYPE_NORMAL
- en: 'You may see several images listed under **library/mysql** as a result of the
    replication operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.27 – Verifying the newly replicated repository’s content](img/B18145_06_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.27 – Verifying the newly replicated repository’s content
  prefs: []
  type: TYPE_NORMAL
- en: 'Pull one of these images into the local workstation’s Docker repository to
    verify it is working as expected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This command should be able to pull the image successfully from our Harbor repository.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our replication configuration step. Now, the developers may pull
    a required MySQL image directly from the Harbor repository rather than getting
    them from Docker Hub.
  prefs: []
  type: TYPE_NORMAL
- en: As the next day-2 activity in this list, we will learn how to configure a rule-based
    tag retention policy to clean up stale images and free up the storage.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring rule-based tag retention policies in Harbor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you saw in the previous configuration, we replicated over 25 different images
    for MySQL. In production-grade implementations, we often encounter situations
    where there are several stale images not being used but occupying the project’s
    allocated space quota. In our previous MySQL replication, we may see only a few
    image tags that are used, and we can remove the rest. For that, we will learn
    how to configure such automated tag-based retention policies to clean up old and
    useless content. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the **Projects** screen and click on the **library** project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.28 – Opening the library project](img/B18145_06_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.28 – Opening the library project
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the **library** project, go to the **Policy** tab and click on the **ADD**
    **RULE** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.29 – Adding a new image retention rule](img/B18145_06_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.29 – Adding a new image retention rule
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter `mysql` as the matching repository, select `1` as the count to retain,
    and press the **ADD** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.30 – Entering details about the image retention rule](img/B18145_06_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.30 – Entering details about the image retention rule
  prefs: []
  type: TYPE_NORMAL
- en: This policy will remove all MySQL images except for `mysql:8.0.27` because we
    pulled that one in the verification step earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the new tag retention policy is in place. While we can also
    schedule this activity at a regular frequency, for now, we will run it manually
    using the **RUN** **NOW** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.31 – Verifying the creation of the image retention rule](img/B18145_06_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.31 – Verifying the creation of the image retention rule
  prefs: []
  type: TYPE_NORMAL
- en: 'Move on by pressing the **RUN** button by accepting the warning of a mass deletion
    operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.32 – Executing the image retention rule](img/B18145_06_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.32 – Executing the image retention rule
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify whether the clean-up activity was completed successfully by inspecting
    the activity log record, which indicates there was only 1 tag retained out of
    24 total. This was the expected outcome:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.33 – Verifying the image retention rule’s execution](img/B18145_06_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.33 – Verifying the image retention rule’s execution
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Repositories** tab to list the existing repositories. On that
    screen, you should be able to see only 1 tag available for **mysql** instead of
    the 24 from earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.34 – Verifying the remaining image count post-retention rule’s execution](img/B18145_06_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.34 – Verifying the remaining image count post-retention rule’s execution
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the `mysql` repository to verify that the tags other than `8.0.27`
    have been deleted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.35 – Verifying the remaining image’s post-retention rule’s execution](img/B18145_06_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.35 – Verifying the remaining image’s post-retention rule’s execution
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our tag retention policy configuration. Like tag retention, we
    may also configure tag immutability policies to configure certain critical image
    tags from being overwritten with a new version of the image. Without such a policy
    in place, you may push new images with changes but with the same tag value. Hence,
    an application that is tested against one tagged version of an image may not be
    able to fully ensure that the content of the same tag would be the same in the
    next pull of the image. This could potentially break applications from working
    in case of any unexpected changes in the newer version of the same tagged images
    are pushed. Ideally, this should not be the case as it is a poor development practice.
    But there should be some controls in place to prevent it from happening. Hence,
    Harbor helps in this case by allowing you to create policies where the Harbor
    users may not push images with the same tag with different content determined
    by the SHA-256 algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Important information
  prefs: []
  type: TYPE_NORMAL
- en: To safeguard our containerized applications from failing because of changed
    image content for the same tag value, it is always a good practice to pull images
    using their SHA values rather than their tags. Pulling an image using its SHA
    value (also known as the digest) ensures you always get the same image with the
    same content and there is no fear of it getting accidentally overwritten with
    the same tag value. For example, the image content pulled with the `docker pull
    ubuntu:20.04` command can be theoretically different for multiple executions,
    but the image content will always be the same when it is pulled with its digest
    using the `docker pull` `ubuntu@sha256:82becede498899ec668628e7cb0ad87b6e1c371cb8a1e597d83a47fac21d6af3`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: There are several more administrative and user day-2 activities we may need
    to perform on Harbor, including user account management, robot accounts for automation,
    image signing, webhook-based automation triggers, and many more. In this chapter,
    we covered some of the most common activities that we usually perform on a container
    registry such as Harbor. While we can add more details for other operations, the
    goal of this book is not to be a Harbor guide alone. But if you want, you may
    find more details in the official documentation for Harbor at [https://goharbor.io/docs/2.4.0/](https://goharbor.io/docs/2.4.0/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s review what we have covered in this chapter. First, we discussed several
    benefits and the use cases of Harbor while explaining the *Why* behind using it.
    We looked at the different security-related benefits of Harbor, including image
    scanning, robust RBAC capabilities, and the ability to restrict public repository
    access requirements using image replications. For the operational control aspect,
    we discussed the benefits, such as on-premises and air-gapped deployment, a popular
    open source project under CNCF, comprehensive multi-tenancy, and administrative
    configurations. For the extensibility aspect, we saw how Harbor can be used with
    its replication feature of extending image library contents. Harbor’s pluggable
    model for vulnerability scanners and authentication providers was also discussed
    in this category.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we covered details of Harbor’s architecture and learned about the
    different components that make up Harbor in detail. Following this, we learned
    how to quickly get started with Harbor using the Bitnami Helm chart and verified
    the installation. Finally, we walked through some of the important day-2 operations
    around Harbor, including creating a project, performing image scanning, preventing
    risky images from being pulled, image replication from Docker Hub, and cleaning
    stale images to free up the storage quota for a project.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to run these images using Tanzu Kubernetes
    Grid, a multi-cloud Kubernetes offering from VMware.
  prefs: []
  type: TYPE_NORMAL
