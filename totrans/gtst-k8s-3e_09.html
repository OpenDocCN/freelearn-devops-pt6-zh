<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Operating Systems, Platforms, and Cloud and Local Providers</h1>
                </header>
            
            <article>
                
<p>The first half of this chapter will cover how open standards encourage a diverse ecosystem of container implementations. We'll look at the <strong>Open Container Initiative</strong> (<strong>OCI</strong>) and its mission to provide an open container specification as well. The second half of this chapter will cover the various operating systems available for running containerized workloads, such as CoreOS. We'll also look at its advantages as a host OS, including performance and support for various container implementations. Additionally, we'll take a brief look at the Tectonic Enterprise offering from CoreOS. We'll look at the various hosted platforms offered by the major <strong>cloud service providers</strong> (<strong>CSPs</strong>) and see how they stack up.</p>
<p>This chapter will discuss the following topics:</p>
<ul>
<li>Why do standards matter?</li>
<li>The OCI and the <strong>Cloud Native Computing Foundation</strong> (<strong>CNCF</strong>)</li>
<li>Container specifications versus implementations</li>
<li>Various container-oriented operating systems</li>
<li>Tectonic</li>
<li>The CSP platforms available that can run Kubernetes workloads</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You'll need to have your Google Cloud Platform account enabled and logged in, or you can use a local Minikube instance of Kubernetes. You can also use Play with Kubernetes online at <a href="https://labs.play-with-k8s.com/">https://labs.play-with-k8s.com/</a>.</p>
<p>You'll also need GitHub credentials, which we'll go over setting up later in the chapter.</p>
<p class="mce-root"/>
<p>The GitHub repository for this chapter can be found at<span> <a href="https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter09">https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter09</a></span>.<a href="https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/ch09"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The importance of standards</h1>
                </header>
            
            <article>
                
<p>Over the past two years, containerization technology has had a tremendous growth in popularity. While Docker has been at the center of this ecosystem, there is an increasing number of players in the container space. There are already a number of alternatives to the containerization and Docker implementation itself (rkt, Garden, and so on). In addition, there is a rich ecosystem of third-party tools that enhance and complement your container infrastructure. While Kubernetes is designed to manage the state of a container and the orchestration, scheduling, and networking side of this ecosystem, the bottom line is that all of these tools form the basis to build cloud-native applications.</p>
<p>As we mentioned at the very beginning of this book, one of the most attractive things about containers is their ability to package our application for deployment across various environment tiers (that is, development, testing, and production) and various infrastructure providers (GCP, AWS, on-premises, and so on).</p>
<p>To truly support this type of deployment agility, we need not only the containers themselves to have a common platform, but also the underlying specifications to follow a common set of ground rules. This will allow for implementations that are both flexible and highly specialized. For example, some workloads may need to be run on a highly secure implementation. To provide this, the implementation will have to make more intentional decisions about some aspects of the implementation. In either case, we will have more agility and freedom if our containers are built on some common structures that all implementations agree on and support.</p>
<p>In the following pages, we'll explore the building blocks of the many competing standards in the Kubernetes ecosystem. We'll explain how they're changing and developing and what part they may play in the future.</p>
<p>One of the examples that we'll explore more deeply in this third edition is the CRI-O project, which came to be after the creation of the OCI Charter. Let's make sure we understand the importance of that mission.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The OCI Charter</h1>
                </header>
            
            <article>
                
<p>The mission of the OCI Charter is to ensure that the open source community has a stable platform from which industry participants can contribute the portable, open, and vendor-neutral runtimes required to build container-powered applications. The Linux Foundation is the holder of the charter, which is a sister organization to the CNCF. We'll look more into the implications of a foundation in <a href="d7bfc2e3-8a59-4df7-94ee-67828421848d.xhtml">Chapter 11</a>, <em>Kubernetes SIGs, Incubation Projects, and the CNCF</em>.</p>
<div class="packt_tip">If you'd like to read more about these foundations, you can check out their websites here: <a href="https://www.linuxfoundation.org/">https://www.linuxfoundation.org/</a> and <a href="https://www.linuxfoundation.org/"/><strong><a href="https://www.cncf.io/">https://www.cncf.io/</a>.</strong></div>
<p>While the OCI Charter tries to standardize the building blocks of the ecosystem, it does not attempt to define the system at the macroscopic level, nor does it market a particular pathway or solution. There's also a process defined that helps technology mature in a responsible way through these foundations, to ensure that the best possible technology is reaching the end user. These are defined as the following stages:</p>
<ol>
<li style="font-weight: 400">Sandbox</li>
<li style="font-weight: 400">Incubating</li>
<li style="font-weight: 400">Graduated</li>
</ol>
<p>For the specifics of this chapter as regards the OCI, let's look at what else they're trying to accomplish. Firstly, we're attempting to create a format specification. This specification will call out a few important dimensions in order to create a consensus:</p>
<ul>
<li style="font-weight: 400"><strong>Provide a format</strong>: In order to ensure a specification that can be used across multiple runtimes, you need a standard container format and runtime specification. The container format is represented by the root filesystem that sits on the disk, with the necessary additional configuration that allows a given container to be run on the system. There is a push to categorize the standardization into the following layers: base, optional, and out of scope.</li>
<li><strong>Provide a runtime</strong>: This is more straightforward, as it's designed to provide an executable that can directly run a container via consumption of the aforementioned container format and runtime specification.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The Charter also incentivizes a number of projects, the first two of which are the runc projects, and the third of which involves the definition of its own specifications in the OCI Specification project. New projects are added by members through a review process that needs two-thirds approval from the current <strong>Technical Oversight Board</strong> (<strong>TOB</strong>). If we look deeper into the principles that govern the OCI, the website names six guiding principles:</p>
<ul>
<li style="font-weight: 400">Technology leadership</li>
<li style="font-weight: 400">Influence through contribution</li>
<li style="font-weight: 400">Limited scope, limited politics</li>
<li style="font-weight: 400">Minimalist structure</li>
<li style="font-weight: 400">Representative leadership</li>
<li style="font-weight: 400">Adherence to anti-trust regulations</li>
</ul>
<p>These items are a blend of philosophical and logical frameworks that encourage competition, collaboration, meritocracy, and the continuous improvement cycles that many Agile and DevOps practitioners have long utilized.</p>
<p>Let's dig more into the initiative itself now.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The OCI</h1>
                </header>
            
            <article>
                
<p>One of the first initiatives to gain widespread industry engagement is the OCI. Among the 36 industry collaborators are Docker, Red Hat, VMware, IBM, Google, and AWS, as listed on the OCI website at <a href="https://www.opencontainers.org/"><span class="URLPACKT">https://www.opencontainers.org/</span></a>.</p>
<p>The purpose of the OCI is to split implementations, such as Docker and rkt, from a standard specification for the format and runtime of containerized workloads. According to their own terms, the goal of the OCI specifications has three basic tenets (you can refer to more details about this in the <em>Further reading</em> section at the end of the chapter):</p>
<ul>
<li>Creating a formal specification for container image formats and runtime, which will allow a compliant container to be portable across all major, compliant operating systems and platforms without artificial technical barriers.</li>
<li>Accepting, maintaining, and advancing the projects associated with these standards. It will look to agree on a standard set of container actions (start, exec, pause, and so on), as well as a runtime environment associated with a container runtime.</li>
<li>Harmonizing the previously referenced standard with other proposed standards, including the appc specification.</li>
</ul>
<p class="mce-root"/>
<p>By following these principals, the OCI hopes to bolster a collaborative and inclusive ecosystem that provides a rich and evolving toolset to meet the needs of today's complex application workloads, be they <span>cloud-native or traditional.</span></p>
<p>There are additionally some guiding principles for the development of standards in this space. These principles were integrated from the founding beliefs of the folks who created appc, and are as follows:</p>
<ul>
<li style="font-weight: 400"><strong>Security</strong>: Isolate containers via pluggable interfaces using secure cryptographic principles, and a chain of custody for both images and application code.</li>
<li style="font-weight: 400"><strong>Portability</strong>: Ensure that containers continue to be portable across a wide variety software, clouds, and hardware.</li>
<li style="font-weight: 400"><strong>Decentralized</strong>: Container images should be straightforward and should take advantage of federation and namespacing.</li>
<li style="font-weight: 400"><strong>Open</strong>: The runtime and formats should be community-built, with multiple interchangeable parts.</li>
<li style="font-weight: 400"><strong>Backward compatible</strong>: Given the popularity of Docker and containers with nearly 9 billion downloads, backward compatibility should be given high priority.</li>
<li style="font-weight: 400"><strong>Composable</strong>: Tools for the operation of containers should be well integrated, but modular.</li>
<li style="font-weight: 400"><strong>Code</strong>: Consensus should be built from running, working code that follows principles of minimalism that adhere to domain-driven design. It should be stable and extensible.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Container Runtime Interface</h1>
                </header>
            
            <article>
                
<p>Let's look at one of the newer and Kubernetes-specific OCI-based initiatives, CRI-O. CRI-O is currently part of the Kubernetes incubator, but it may move out to its own project as it matures. One of the compelling parts of the CRI-O design is that it never breaks Kubernetes. This is different because other runtimes are designed to do many things, such as building images, managing security, orchestration, and inspecting images. CRI-O is only designed to help Kubernetes orchestrate and schedule containers.</p>
<div class="packt_tip">You can get the code for the CRI-O project and read the documentation at <a href="https://github.com/kubernetes-incubator/cri-o">https://github.com/kubernetes-incubator/cri-o/</a>.<a href="https://github.com/kubernetes-incubator/cri-o"/></div>
<p class="mce-root"/>
<p>To this end, CRI-O is developed congruently with the CRI itself, and aligns itself with upstream releases of the Kubernetes system. The following diagram shows how the CRI-O works with the OCI:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/e46506c8-1ed5-4515-bda4-f1d4b4f43506.png" style="width:53.75em;height:53.25em;" width="734" height="728"/></p>
<p>In order to achieve this workflow, the following happens:</p>
<ol>
<li style="font-weight: 400">The operator decides to start a pod, which causes Kubernetes to use the <kbd>kubelet</kbd> to start a pod. That <kbd>kubelet</kbd> talks through the CRI to the CRI-O daemon.</li>
<li style="font-weight: 400">CRI-O then uses several libraries, built with the OCI standard, to pull and unpack the given container image from a registry. From these operations, CRI-O generates a JSON blob that is used in the next step to run the container.</li>
<li style="font-weight: 400">CRI-O kicks off an OCI-compatible runtime, which then runs the container process. This could be runc or the new Kata Container runtime (which has absorbed Intel's clear containers initiative).</li>
</ol>
<p>You'll notice here that the CRI-O is acting as an interleaving layer between the libraries and runtimes, such that it's using standard formats to accomplish most its goals. This ensures the goal is making Kubernetes work at all times. Here's a diagram showing the system of the flow that was described in this section:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/dc16cc9a-9ac5-4faa-bc78-94cc4ca29d37.png" width="1950" height="1148"/></p>
<p>For networking, CRI-O would leverage the <strong>Container Networking Interface</strong> (<strong>CNI</strong>), which is similar to the CRI, but deals with the networking stack. You should begin to see a pattern emerge here.</p>
<p>CRI-O is an implementation that helps to implement the OCI specification. This allows users to take for granted the container runtime being used as an implementation detail, and to focus instead on how the application is interacting with the objects and abstractions of the Kubernetes system.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Trying out CRI-O</h1>
                </header>
            
            <article>
                
<p>Let's look at some installation methods so you can give CRI-O a try on your own. In order to get started, you'll need a few things, including runc or another OCI compatible runtime, as well as socat, iproute, and iptables. There's a few options for running CRI-O in Kubernetes:</p>
<ul>
<li style="font-weight: 400">In a full-scale cluster, using <kbd>kube-adm</kbd> and <kbd>systemd</kbd> to leverage the CRI-O socket with <kbd>--container-runtime-endpoint /var/run/crio/crio.sock</kbd></li>
<li style="font-weight: 400">With Minikube, by starting it up with specific command-line options</li>
<li style="font-weight: 400">On atomic with atomic install <kbd>--system-package=no -n cri-o --storage ostree registry.centos.org/projectatomic/cri-o:latest</kbd></li>
</ul>
<p>If <span>you'd like to build CRI-O from source, you can run the following on your laptop. You need some dependencies installed in order to make this build phase work. First, run the following commands to get your dependencies installed.</span></p>
<p>The following commands are for Fedora, CentOS, and RHEL distributions:</p>
<pre><strong>yum install -y \</strong><br/><strong>  btrfs-progs-devel \</strong><br/><strong>  device-mapper-devel \</strong><br/><strong>  git \</strong><br/><strong>  glib2-devel \</strong><br/><strong>  glibc-devel \</strong><br/><strong>  glibc-static \</strong><br/><strong>  go \</strong><br/><strong>  golang-github-cpuguy83-go-md2man \</strong><br/><strong>  gpgme-devel \</strong><br/><strong>  libassuan-devel \</strong><br/><strong>  libgpg-error-devel \</strong><br/><strong>  libseccomp-devel \</strong><br/><strong>  libselinux-devel \</strong><br/><strong>  ostree-devel \</strong><br/><strong>  pkgconfig \</strong><br/><strong>  runc \</strong><br/><strong>  skopeo-containers</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>These commands are to be used for Debian, Ubuntu, and related distributions:</p>
<pre><strong>apt-get install -y \</strong><br/><strong>  btrfs-tools \</strong><br/><strong>  git \</strong><br/><strong>  golang-go \</strong><br/><strong>  libassuan-dev \</strong><br/><strong>  libdevmapper-dev \</strong><br/><strong>  libglib2.0-dev \</strong><br/><strong>  libc6-dev \</strong><br/><strong>  libgpgme11-dev \</strong><br/><strong>  libgpg-error-dev \</strong><br/><strong>  libseccomp-dev \</strong><br/><strong>  libselinux1-dev \</strong><br/><strong>  pkg-config \</strong><br/><strong>  go-md2man \</strong><br/><strong>  runc \</strong><br/><strong>  skopeo-containers</strong></pre>
<p>Secondly, you'll need to grab the source code like so:</p>
<pre><strong>git clone https://github.com/kubernetes-incubator/cri-o # or your fork</strong><br/><strong>cd cri-o</strong></pre>
<p>Once you have the code, go ahead and build it:</p>
<pre><strong>make install.tools</strong><br/><strong>make</strong><br/><strong><span>sudo make install</span></strong></pre>
<p>You can use additional build flags to add thing such as <kbd>seccomp</kbd>, SELinux, and <kbd>apparmor</kbd> with this format: <kbd>make BUILDTAGS='seccomp apparmor'</kbd>.</p>
<p>You can run Kubernetes locally with the <kbd>local-up-cluster.sh</kbd> script in Kubernetes. I'll also show you how to run this on Minikube.</p>
<p>First, clone the Kubernetes repository:</p>
<pre><strong>git clone https://github.com/kubernetes/kubernetes.git</strong></pre>
<p>Next, you'll need to start the CRI-O daemon and run the following command to get spin up your cluster using CRI-O:</p>
<pre><strong>CGROUP_DRIVER=systemd \</strong><br/><strong> CONTAINER_RUNTIME=remote \</strong><br/><strong> CONTAINER_RUNTIME_ENDPOINT='unix:///var/run/crio/crio.sock  --runtime-request-timeout=15m' \</strong><br/><strong> ./hack/local-up-cluster.sh</strong></pre>
<div class="packt_tip">If you have a running cluster, you can also use the instructions, available at the following URL, to switch the runtime from Docker to CRI-O: <a href="https://github.com/kubernetes-incubator/cri-o/blob/master/kubernetes.md">https://github.com/kubernetes-incubator/cri-o/blob/master/kubernetes.md/</a>.</div>
<p>Let's also check how to use CRI-O on Minikube, which is one of the easiest ways to get experimenting:</p>
<pre><strong>minikube start \</strong><br/><strong> --network-plugin=cni \</strong><br/><strong> --extra-config=kubelet.container-runtime=remote \</strong><br/><strong> --extra-config=kubelet.container-runtime-endpoint=/var/run/crio/crio.sock \</strong><br/><strong> --extra-config=kubelet.image-service-endpoint=/var/run/crio/crio.sock \</strong><br/><strong> --bootstrapper=kubeadm</strong></pre>
<p>Lastly, we can use our GCP platform to spin up a cluster with CRI-O and start experimenting:</p>
<pre><strong>gcloud compute instances create cri-o \</strong><br/><strong>  --machine-type n1-standard-2 \</strong><br/><strong>  --image-family ubuntu-1610 \</strong><br/><strong>  --image-project ubuntu-os-cloud</strong></pre>
<p>Let's use these machines to run through a quick tutorial. SSH into the machine using <kbd>gcloud compute ssh cri-o</kbd>.</p>
<p>Once you're on the server, we'll need to install the <kbd>cri-o</kbd>, <kbd>crioctl</kbd>, <kbd>cni</kbd>, and <kbd>runc</kbd> programs. Grab the <kbd>runc</kbd> binary first:</p>
<pre><strong>wget https://github.com/opencontainers/runc/releases/download/v1.0.0-rc4/runc.amd64</strong></pre>
<p>Set it executable and move it to your path as follows:</p>
<pre><strong>chmod +x runc.amd64</strong><br/><strong>sudo mv runc.amd64 /usr/bin/runc</strong></pre>
<p>You can see it's working by checking the version:</p>
<pre><strong>$ runc -version</strong><br/><strong>runc version 1.0.0-rc4</strong><br/><strong>commit: 2e7cfe036e2c6dc51ccca6eb7fa3ee6b63976dcd</strong><br/><strong>spec: 1.0.0</strong></pre>
<p>You'll need to install the CRI-O binary from source, as it's not currently shipping any binaries.</p>
<p>First, download the latest binary release and install Go:</p>
<pre><strong>wget https://storage.googleapis.com/golang/go1.8.5.linux-amd64.tar.gz</strong><br/><strong>sudo tar -xvf go1.8.5.linux-amd64.tar.gz -C /usr/local/</strong><br/><strong>mkdir -p $HOME/go/src</strong><br/><strong>export GOPATH=$HOME/go</strong><br/><strong>export PATH=$PATH:/usr/local/go/bin:$GOPATH/bin</strong></pre>
<p>This should feel familiar, as you would install Go the same way for any other project. Check your version:</p>
<pre><strong>go version</strong><br/><strong>go version go1.8.5 linux/amd64</strong></pre>
<p>Next up, get <kbd>crictl</kbd> using the following commands:</p>
<pre><strong>go get github.com/kubernetes-incubator/cri-tools/cmd/crictl</strong><br/><strong>cd $GOPATH/src/github.com/kubernetes-incubator/cri-tools</strong><br/><strong>make</strong><br/><strong>make install</strong></pre>
<p>After that's downloaded, you'll need to build CRI-O from source:</p>
<pre><strong>sudo apt-get update &amp;&amp; apt-get install -y libglib2.0-dev \</strong><br/><strong> libseccomp-dev \</strong><br/><strong> libgpgme11-dev \</strong><br/><strong> libdevmapper-dev \</strong><br/><strong> make \</strong><br/><strong> git</strong></pre>
<p>Now, get CRI-O and install it:</p>
<pre><strong>go get -d github.com/kubernetes-incubator/cri-o</strong><br/><strong>cd $GOPATH/src/github.com/kubernetes-incubator/cri-o</strong><br/><strong>make install.tools</strong><br/><strong>Make</strong><br/><strong>sudo make install</strong></pre>
<p>After this is complete, you'll need to create configuration files with <kbd>sudo make install.config</kbd>. You need to ensure that you're using a valid registry option in the <kbd>/etc/crio/cirio.conf</kbd> file. An example of this looks like the following:</p>
<pre>registries = ['registry.access..com', 'registry.fedoraproject.org', 'docker.io']</pre>
<p class="mce-root"/>
<p>At this point, we're ready to start the CRI-O system daemon, which we can do by leveraging <kbd>systemctl</kbd>. Let's create a <kbd>crio.service</kbd>:</p>
<pre><strong>$ vim /etc/systemd/system/crio.service</strong></pre>
<p>Add the following text:</p>
<pre>[Unit]<br/>Description=OCI-based implementation of Kubernetes Container Runtime Interface<br/>Documentation=https://github.com/kubernetes-incubator/cri-o<br/> <br/>[Service]<br/>ExecStart=/usr/local/bin/crio<br/>Restart=on-failure<br/>RestartSec=5<br/> <br/>[Install]<br/>WantedBy=multi-user.target</pre>
<p>Once that's complete, we can reload <kbd>systemctl</kbd> and enable CRI-O:</p>
<pre><strong>$ sudo systemctl daemon-reload &amp;&amp; \</strong><br/><strong> sudo systemctl enable crio &amp;&amp; \</strong><br/><strong> sudo systemctl start crio</strong></pre>
<p>After this is complete, we can validate whether or not we have a working install of CRI-O by checking the version of the endpoint as follows:</p>
<pre><strong>$ sudo crictl --runtime-endpoint unix:///var/run/crio/crio.sock version</strong><br/><strong>Version:  0.1.0</strong><br/><strong>RuntimeName:  cri-o</strong><br/><strong>RuntimeVersion:  1.10.0-dev</strong><br/><strong>RuntimeApiVersion:  v1alpha1</strong></pre>
<p>Next up, we'll need to grab the latest version of the CNI plugin, so we can build and use it from source. Let's use Go to grab our source code:</p>
<pre><strong>go get -d github.com/containernetworking/plugins</strong><br/><strong>cd $GOPATH/src/github.com/containernetworking/plugins</strong><br/><strong>./build.sh</strong></pre>
<p>Next, install the CNI plugins into your cluster:</p>
<pre><strong>sudo mkdir -p /opt/cni/bin</strong><br/><strong>sudo cp bin/* /opt/cni/bin/</strong></pre>
<p class="mce-root"/>
<p>Now, we can configure the CNI so that CRI-O can use it.  First, make a directory to store the configuration, then we'll set two configuration files as follows:</p>
<pre><strong>sudo mkdir -p /etc/cni/net.d</strong></pre>
<p>Next, you'll want to create and compose <kbd>10-mynet.conf</kbd>:</p>
<pre>sudo sh -c 'cat &gt;/etc/cni/net.d/10-mynet.conf &lt;&lt;-EOF<br/>{<br/>"cniVersion": "0.2.0",<br/>   "name": "mynet",<br/>   "type": "bridge",<br/>   "bridge": "cni0",<br/>   "isGateway": true,<br/>   "ipMasq": true,<br/>   "ipam": {<br/>       "type": "host-local",<br/>       "subnet": "10.88.0.0/16",<br/>       "routes": [<br/>           { "dst": "0.0.0.0/0"  }<br/>       ]<br/>   }<br/>}<br/>EOF'</pre>
<p>And then, compose the <kbd>loopback</kbd> interface as follows:</p>
<pre><strong>sudo sh -c 'cat &gt;/etc/cni/net.d/99-loopback.conf &lt;&lt;-EOF</strong><br/><strong>{</strong><br/><strong>   "cniVersion": "0.2.0",</strong><br/><strong>   "type": "loopback"</strong><br/><strong>}</strong><br/><strong>EOF'</strong></pre>
<p>Next up, we'll need some special containers from Project Atomic to get this working. <kbd>skopeo</kbd> is a command-line utility that is OCI-compliant and can perform various operations on container images and image repositories. Install the containers as follows:</p>
<pre><strong>sudo add-apt-repository ppa:projectatomic/ppa</strong><br/><strong>sudo apt-get update</strong><br/><strong>sudo apt-get install skopeo-containers -y</strong></pre>
<p>Restart CRI-O to pick up the CNI configuration with <kbd>sudo systemctl restart crio</kbd>. Great! Now that we have these components installed, let's build something!</p>
<p>First off, we'll create a sandbox using a template policy from the Kubernetes incubator.</p>
<div class="packt_infobox packt_tip CDPAlignLeft CDPAlign">This template is NOT production ready!</div>
<p>Change first to the CRI-O source tree with the template, as follows:</p>
<pre><strong>cd $GOPATH/src/github.com/kubernetes-incubator/cri-o</strong></pre>
<p>Next, you'll need to create and capture the pod ID:</p>
<pre><strong>sudo mkdir /etc/containers/</strong><br/><strong>sudo cp test/policy.json /etc/containers</strong></pre>
<p>You can use <kbd>critcl</kbd> to get the status of the pod as follows:</p>
<pre><strong>sudo crictl inspectp --output table $POD_ID</strong><br/><strong>ID: cd6c0883663c6f4f99697aaa15af8219e351e03696bd866bc3ac055ef289702a</strong><br/><strong>Name: podsandbox1</strong><br/><strong>UID: redhat-test-crio</strong><br/><strong>Namespace: redhat.test.crio</strong><br/><strong>Attempt: 1</strong><br/><strong>Status: SANDBOX_READY</strong><br/><strong>Created: 2016-12-14 15:59:04.373680832 +0000 UTC</strong><br/><strong>Network namespace: /var/run/netns/cni-bc37b858-fb4d-41e6-58b0-9905d0ba23f8</strong><br/><strong>IP Address: 10.88.0.2</strong><br/><strong>Labels:</strong><br/><strong>group -&gt; test</strong><br/><strong>Annotations:</strong><br/><strong>owner -&gt; jwhite</strong><br/><strong>security.alpha.kubernetes.io/seccomp/pod -&gt; unconfined</strong><br/><strong>security.alpha.kubernetes.io/sysctls -&gt;</strong><br/><strong>kernel.shm_rmid_forced=1,net.ipv4.ip_local_port_range=1024 65000</strong><br/><strong>security.alpha.kubernetes.io/unsafe-sysctls -&gt; kernel.msgmax=8192</strong></pre>
<p>We'll use the <kbd>crictl</kbd> tool again to pull a container image for a Redis server:</p>
<pre><strong>sudo crictl pull quay.io/crio/redis:alpine</strong><br/><strong>CONTAINER_ID=$(sudo crictl create $POD_ID test/testdata/container_redis.json test/testdata/sandbox_config.json)</strong></pre>
<p>Next, we'll start and check the status of the Redis container as follows:</p>
<pre><strong>sudo crictl start $CONTAINER_ID</strong><br/><strong>sudo crictl inspect $CONTAINER_ID</strong></pre>
<p>At this point, you should be able to <kbd>telnet</kbd> into the Redis container to test its functionality:</p>
<pre><strong>telnet 10.88.0.2 6379</strong><br/><strong>Trying 10.88.0.2…</strong><br/><strong>Connected to 10.88.0.2.</strong><br/><strong>Escape character is '^]'.</strong></pre>
<p>Nicely done—you've now created a pod and container manually, using some of the core abstractions of the Kubernetes system! You can stop the container and shut down the pod with the following commands:</p>
<pre><strong>sudo crictl stop $CONTAINER_ID</strong><br/><strong>sudo crictl rm $CONTAINER_ID</strong><br/><strong>sudo crictl stopp $POD_ID</strong><br/><strong>sudo crictl rmp $POD_ID</strong><br/><strong>sudo crictl pods</strong><br/><strong>sudo crictl ps</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">More on container runtimes</h1>
                </header>
            
            <article>
                
<p>There's a number of container- and VM-based options for OCI-compliant implementations. We know of runc, which is the standard reference implementation of the OCI runtime. This is what the container uses. There's also the following available:</p>
<ul>
<li style="font-weight: 400"><kbd>projectatomic/bwrap-oci</kbd> (<a href="https://github.com/projectatomic/bwrap-oci">https://github.com/projectatomic/bwrap-oci</a>): Converts the OCI spec file to a command line for <kbd>projectatomic/bubblewrap</kbd> (<a href="https://github.com/projectatomic/bubblewrap">https://github.com/projectatomic/bubblewrap</a>)</li>
<li style="font-weight: 400"><kbd>giuseppe/crun</kbd> (<a href="https://github.com/giuseppe/crun">https://github.com/giuseppe/crun</a>): Runtime implementation in C</li>
</ul>
<p>There are also VM-based implementations that take a different path towards security:</p>
<ul>
<li style="font-weight: 400"><kbd>hyperhq/runv</kbd> (<a href="https://github.com/hyperhq/runv">https://github.com/hyperhq/runv</a>)—hypervisor-based runtime for OCI</li>
<li style="font-weight: 400"><kbd>clearcontainers/runtime</kbd> (<a href="https://github.com/clearcontainers/runtime">https://github.com/clearcontainers/runtime</a>)—hypervisor-based OCI runtime utilizing <kbd>containers/virtcontainers</kbd> (<a href="https://github.com/containers/virtcontainers">https://github.com/containers/virtcontainers</a>) by Intel</li>
<li style="font-weight: 400"><kbd>google/gvisor</kbd> (<a href="https://github.com/google/gvisor">https://github.com/google/gvisor</a>)—gVisor is a user-space kernel, which contains runsc to run sandboxed containers</li>
</ul>
<ul>
<li style="font-weight: 400"><kbd>kata-containers/runtime</kbd> (<a href="https://github.com/kata-containers/runtime">https://github.com/kata-containers/runtime</a>)—hypervisor-based OCI runtime combining technology from <kbd>clearcontainers/runtime</kbd> (<a href="https://github.com/clearcontainers/runtime">https://github.com/clearcontainers/runtime</a>) and <kbd>hyperhq/runv</kbd> (<a href="https://github.com/hyperhq/runv">https://github.com/hyperhq/runv</a>)</li>
</ul>
<p>The most interesting project of these is the last in the list, Kata containers, which combines clear container and runV into a cohesive package. These foundational pieces are already in production use at scale in the enterprises, and Kata is looking to provide a secure, lightweight VM for containerized environments. By utilizing runV, Kata containers can run inside of any KVM-compatible VM, such as Xen, KVM, and vSphere, while still remaining compatible with CRI-O, which is important! Kata hopes to offer the speed of a container with the security surface of a VM.</p>
<p>Here's a diagram from Kata's site, explaining the architecture in visual detail:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/a0daf1d8-69b9-4a3b-b027-efa0db055a5c.png" style="width:44.83em;height:25.83em;" width="1632" height="940"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">CNCF</h1>
                </header>
            
            <article>
                
<p>A second initiative that also has widespread industry acceptance is the CNCF. While still focused on containerized workloads, the CNCF operates a bit higher up the stack, at the application design level.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Its purpose is to provide a standard set of tools and technologies to build, operate, and orchestrate cloud-native application stacks. Cloud has given us access to a variety of new technologies and practices that can improve and evolve our classic software designs. The CNCF is also particularly focused on the new paradigm of microservice-oriented development.</p>
<p>As a founding participant in the CNCF, Google has donated the Kubernetes open source project. The goal will be to increase interoperability in the ecosystem and support better integration with projects. The CNCF already hosts a variety of projects on orchestration, logging, monitoring, tracing, and application resilience.</p>
<div class="packt_infobox">For more information on CNCF, refer to <a href="https://cncf.io/">https://cncf.io/</a>.<a href="https://cncf.io/"/></div>
<p class="mce-root">We'll talk more about the CNCF, <strong>Special Interest Groups</strong> (<strong>SIGs</strong>), and the landscape therein in the following chapters.</p>
<div class="packt_figref CDPAlignLeft CDPAlign packt_infobox"><span>For now, here's a landscape and trail map to consider</span>: <a href="https://www.cncf.io/blog/2018/03/08/introducing-the-cloud-native-landscape-2-0-interactive-edition/">https://www.cncf.io/blog/2018/03/08/introducing-the-cloud-native-landscape-2-0-interactive-edition/</a>.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Standard container specification</h1>
                </header>
            
            <article>
                
<p>A core result of the OCI effort is the creation and development of the overarching container specification. The specification has five core principles that all containers should follow, which I will briefly paraphrase:</p>
<ul>
<li>The container must have <em>standard operations</em> to create, start, and stop containers across all implementations.</li>
<li>The container must be <em>content-agnostic</em>, which means that type of application inside the container does not alter the standard operations or publishing of the container itself.</li>
<li>The container must be <em>infrastructure-agnostic</em> as well. Portability is paramount; therefore, the container must be able to operate just as easily in GCE as in your company's data center or on a developer's laptop.</li>
</ul>
<ul>
<li>A container must also be <em>designed for automation</em>, which allows us to automate across the build, as well as for updates and the deployment pipelines. While this rule is a bit vague, the container implementation should not require onerous manual steps for creation and release.</li>
<li>Finally, the implementation must support <em>industrial-grade delivery</em>. Once again, this means speaking to the build and deployment pipelines and requiring streamlined efficiency in the portability and transit of the containers between infrastructure and deployment tiers.</li>
</ul>
<p>The specification also defines core principles for container formats and runtimes. You can read more about the specifications on the open containers GitHub page at <span class="URLPACKT"><a href="https://github.com/opencontainers/specs">https://github.com/opencontainers/specs</a>.</span><a href="https://github.com/opencontainers/specs"><br/></a></p>
<p>While the core specification can be a bit abstract, the runc implementation is a concrete example of the OCI specs, in the form of a container runtime and image format. Again, you can read more of the technical details on GitHub at <a href="https://github.com/opencontainers/runc"><span class="URLPACKT">https://github.com/opencontainers/runc</span></a>.</p>
<p>The backing format and runtime for a variety of popular container tools is runc. It was donated to OCI by Docker and was created from the same plumbing work used in the Docker platform. Since its release, it has received a welcome uptake by numerous projects.</p>
<p>Even the popular open source PaaS Cloud Foundry announced that it will use runc in Garden. Garden provides the containerization plumbing for Diego, which acts as an orchestration layer similar to Kubernetes.</p>
<p>The rkt implementation was originally based on the appc specification. The appc specification was actually an earlier attempt by the folks at CoreOS to form a common specification around containerization. Now that CoreOS is participating in OCI, they are working to help merge the appc specification into OCI; this should result in a higher level of compatibility across the container ecosystem.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">CoreOS</h1>
                </header>
            
            <article>
                
<p>While the specifications provide us with a common ground, there are also some trends evolving around the choice of OS for our containers. There are several tailored-fit OSes that are being developed specifically to run container workloads. Although implementations vary, they all have similar characteristics. The focus is on a slim installation base, atomic OS updating, and signed applications for efficient and secure operations.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>One OS that is gaining popularity is CoreOS. CoreOS offers major benefits for both security and resource utilization. It provides resource utilization by <span>completely</span><span> </span><span>removing package dependencies from the picture. Instead, CoreOS runs all applications and services in containers. By providing only a small set of services required to support running containers and bypassing the need for hypervisor usage, CoreOS lets us use a larger portion of the resource pool to run our containerized applications. This allows users to gain higher performance from their infrastructure and better container-to-node (server) usage ratios.</span></p>
<p>Recently, CoreOS was purchased by Red Hat, which means that the current version of container Linux will evolve against Red Hat's container OS offering, Project Atomic. These two products will eventually turn into Red Hat CoreOS. If you consider the upstream community approach that Fedora takes to Red Hat Enterprise Linux, it seems likely that there will be something similar for Red Hat CoreOS.</p>
<p>This also means that Red Hat will be integration Tectonic, which we'll explore later in the chapter, and the Quay, the enterprise container registry that CoreOS acquired. It's important to note that the rkt container standard will not be part of the acquisition, and will instead become a community supported project.</p>
<div class="packt_tip">If you'd like to see the relevant official announcements for the news discussed in the preceding section, you can check out these posts:<br/>
<ul>
<li><strong>Press release</strong>: <a href="https://www.redhat.com/en/about/press-releases/red-hat-acquire-coreos-expanding-its-kubernetes-and-containers-leadership">https://www.redhat.com/en/about/press-releases/red-hat-acquire-coreos-expanding-its-kubernetes-and-containers-leadership</a></li>
<li><strong>Red Hat blog</strong>:<strong> </strong><a href="https://www.redhat.com/en/blog/coreos-bet">https://www.redhat.com/en/blog/coreos-bet</a></li>
<li><strong>CoreOS blog</strong>:<strong> </strong><a href="https://coreos.com/blog/coreos-agrees-to-join-red-hat/">https://coreos.com/blog/coreos-agrees-to-join-red-hat/</a></li>
</ul>
</div>
<p>Here's a brief overview of the various container OSes. There are several other container-optimized OSes that have emerged recently:</p>
<ul>
<li><em>Red Hat Enterprise Linux Atomic Host</em> focuses on security with SELinux enabled by default and atomic updates to the OS similar to what we saw with CoreOS. Refer to the following link: <a href="https://access.redhat.com/articles/rhel-atomic-getting-started">https://access.redhat.com/articles/rhel-atomic-getting-started</a>.</li>
<li><em>Ubuntu Snappy</em> also capitalizes on the efficiency and security gains of separating the OS components from the frameworks and applications. Using application images and verification signatures, we get an efficient Ubuntu-based OS for our container workloads at <a href="http://www.ubuntu.com/cloud/tools/snappy">http://www.ubuntu.com/cloud/tools/snappy</a>.</li>
</ul>
<ul>
<li><em>Ubuntu LXD</em> runs a container hypervisor and provides a path for migrating Linux-based VMs to containers with ease: <a href="https://www.ubuntu.com/cloud/lxd">https://www.ubuntu.com/cloud/lxd</a>.</li>
<li><em>VMware Photon</em> is another lightweight container OS that is optimized specifically for vSphere and the VMware platform. It runs Docker, rkt, and Garden and also has some images that you can run on the popular public cloud providers. Refer to the following link: <a href="https://vmware.github.io/photon/">https://vmware.github.io/photon/</a>.</li>
</ul>
<p>Using the isolated nature of containers, we increase reliability and decrease the complexity of updates for each application. Now, applications can be updated along with supporting libraries whenever a new container release is ready, as shown in the following diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref"><img class="image-border" src="Images/c42dff26-cbee-4e99-adc6-41680ffc95df.png" style="width:23.75em;height:28.50em;" width="400" height="481"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">CoreOS update procedure</div>
<p>Finally, CoreOS has some added advantages in the realm of security. For starters, the OS can be updated as one whole unit, instead of via individual packages (refer to the preceding diagram). This avoids many issues that arise from partial updates. To achieve this, CoreOS uses two partitions: one as the active OS partition, and a secondary one to receive a full update. Once updates are completed successfully, a reboot promotes the secondary partition. If anything goes wrong, the original partition is available as a fallback.</p>
<p>The system owners can also control when those updates are applied. This gives us the flexibility to prioritize critical updates, while working with real-world scheduling for the more common updates. In addition, the entire update is signed and transmitted via SSL for added security across the entire process.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">rkt</h1>
                </header>
            
            <article>
                
<p>As mentioned previously, rkt will be continuing on as a community driven project. rkt is another implementation with a specific focus on security. The main advantage of rkt is that it runs the engine without a daemon as root, the way Docker does today. Initially, rkt also had an advantage in the establishment of trust for container images. However, recent updates to Docker have made great strides, especially the new content trust feature.</p>
<p>The bottom line is that rkt is still an implementation, with a focus on security, for running containers in production. rkt uses an image format named ACI, but it also supports Docker-based images. Over the past year, rkt has undergone significant updates and is now at version 1.24.0. It has gained much momentum as a means to run Docker images securely in production. </p>
<p>Here's a diagram showing how the rkt execution chain works:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/66dd571b-1ae2-4020-b788-29621df494f5.png" style="width:22.50em;height:25.50em;" width="342" height="387"/></p>
<p>In addition, CoreOS is working with Intel® to integrate the new Intel® Virtualization Technology, which allows containers to run in higher levels of isolation. This hardware-enhanced security allows the containers to be run inside a <strong>Kernel-based Virtual Machine</strong> (<strong>KVM</strong>) process, providing isolation from the kernel in a similar fashion to what we see with hypervisors today.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">etcd</h1>
                </header>
            
            <article>
                
<p>Another central piece in the CoreOS ecosystem worth mentioning is their open source etcd project. etcd is a distributed and consistent key-value store. A RESTful API is used to interface with etcd, so it's easy to integrate with your project.</p>
<p>If it sounds familiar, it's because we saw this process running in <a href="446f901d-70fa-4ebe-be8a-0de14248f99c.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Introduction to Kubernetes</em>, in the section entitled <em>Services running on the master</em>. Kubernetes actually utilizes etcd to keep track of cluster configuration and current state. K8s uses it for its service discovery capabilities as well. For more details, refer to <a href="https://github.com/coreos/etcd">https://github.com/coreos/etcd</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Kubernetes with CoreOS</h1>
                </header>
            
            <article>
                
<p>Now that we understand the benefits, let's take a look at a Kubernetes cluster using CoreOS. The documentation supports a number of platforms, but one of the easiest to spin up is AWS with the CoreOS CloudFormation and CLI scripts.</p>
<div class="packt_tip packt_infobox">If you are interested in running Kubernetes with CoreOS on other platforms, you can find more details in the CoreOS documentation at <a href="https://coreos.com/kubernetes/docs/latest/">https://coreos.com/kubernetes/docs/latest/</a>. <a href="https://coreos.com/kubernetes/docs/latest/">You can find the latest instructions for AWS at</a> <a href="https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html">https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html</a><a href="https://coreos.com/kubernetes/docs/latest/">.</a></div>
<p>You can follow the instructions covered previously in this chapter to spin up Kubernetes on CoreOS. You'll need to create a key pair on AWS, and also specify a region, cluster name, cluster size, and DNS to proceed.</p>
<p>In addition, we will need to create a DNS entry, and will require a service such as Route 53 or a production DNS service. When following the instructions, you'll want to set the DNS to a domain or sub-domain on which you have permission to set up a record. We will need to update the record after the cluster is up and running and has a dynamic endpoint defined. </p>
<p>There you have it! We now have a cluster running CoreOS. The script creates all the necessary AWS resources, such as <strong>Virtual Private Clouds</strong> (<strong>VPCs</strong>), security groups, and IAM roles. Now that the cluster is up and running, we can get the endpoint with the <kbd>status</kbd> command and update our DNS record as follows:</p>
<pre><strong>$ kube-aws status</strong></pre>
<p>Copy the entry listed next to <kbd>Controller DNS Name</kbd> in the output from the preceding command, and then edit your DNS records to get the domain or sub-domain you specified earlier to point to this load balancer.</p>
<p>If you forget which domain you specified or need to check on the configuration, you can look in the generated <kbd>kubeconfig</kbd> file with your favorite editor. It will look something like this:</p>
<pre>apiVersion: v1<br/>kind: Config<br/>clusters:<br/>- cluster:<br/>    certificate-authority: credentials/ca.pem<br/>    server: <strong>https://coreos.mydomain.com</strong><br/>  name: kube-aws-my-coreos-cluster-cluster<br/>contexts:<br/>- context:<br/>    cluster: kube-aws-my-coreos-cluster-cluster<br/>    namespace: default<br/>    user: kube-aws-my-coreos-cluster-admin<br/>  name: kube-aws-my-coreos-cluster-context<br/>users:<br/>- name: kube-aws-my-coreos-cluster-admin<br/>  user:<br/>    client-certificate: credentials/admin.pem<br/>    client-key: credentials/admin-key.pem<br/>current-context: kube-aws-my-coreos-cluster-context</pre>
<p>In this case, the <kbd>server</kbd> line will have your domain name.</p>
<div class="packt_tip">If this is a fresh box, you will need to download <kbd>kubectl</kbd> separately, as it is not bundled with <kbd>kube-aws</kbd>:<br/>
<kbd><strong>$ wget https://storage.googleapis.com/kubernetes-release/release/v1.0.6/bin/linux/amd64/kubectl</strong></kbd></div>
<p>We can now use <kbd>kubectl</kbd> to see our new cluster:</p>
<pre><strong>$ ./kubectl --kubeconfig=kubeconfig get nodes</strong></pre>
<p>We should see a single node listed with the EC2 internal DNS as the name. Note <kbd>kubeconfig</kbd>, this tells Kubernetes the path to use the configuration file for the cluster that was just created instead. This is also useful if we want to manage multiple clusters from the same machine.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Tectonic</h1>
                </header>
            
            <article>
                
<p>Running Kubernetes on CoreOS is a great start, but you may find that you want a higher level of support. Enter Tectonic, the CoreOS enterprise offering for running Kubernetes with CoreOS. Tectonic uses many of the components we already discussed. Both Docker and rkt runtimes are supported. In addition, Kubernetes, etcd, and flannel are packaged together to give a full stack of cluster orchestration. We discussed flannel briefly in <a href="9b6aa290-fc68-4111-98a9-697c94f0e6a2.xhtml"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Working with Networking, Load Balancers, and Ingress</em>. It is an overlay network that uses a model similar to the native Kubernetes model, and uses etcd as a backend.</p>
<p>Offering a support package similar to Red Hat, CoreOS also provides 24/7 support for the open source software that Tectonic is built on. Tectonic also provides regular cluster updates and a nice dashboard with views for all of the components of Kubernetes. <strong>CoreUpdate</strong> allows users to have more control of the automatic update process. In addition, it ships with modules for monitoring, SSO, and other security features.</p>
<p>As CoreOS is integrated into Red Hat, this offering will be replaced over time with a Red Hat approach.</p>
<div class="packt_infobox">You can find more information and the latest instructions to install at <a href="https://coreos.com/tectonic/docs/latest/install/aws/index.html">https://coreos.com/tectonic/docs/latest/install/aws/index.html</a>.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dashboard highlights</h1>
                </header>
            
            <article>
                
<p>Some highlights of the Tectonic dashboard are shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="Images/148e826b-de2b-46d9-899b-fcbd580bdd25.png" style="width:37.67em;height:38.33em;" width="635" height="646"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">The Tectonic main dashboard</div>
<p>Tectonic is now generally available and the dashboard already has some nice features. As you can see in the following screenshot, we can see a lot of detail about our replication controller, and can even use the GUI to scale up and down with the click of a button:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/73c9308e-df74-46aa-86df-8b79da9190c4.png" style="width:38.33em;height:46.17em;" width="634" height="765"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Tectonic replication controller detail</div>
<p class="packt_figure CDPAlignLeft CDPAlign">This graphic is quite large, so it's broken across two pages. The following screenshot continues from the preceding screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/6f6c2827-552a-400f-91fc-2da18b0c24b0.png" style="width:45.58em;height:49.67em;" width="634" height="692"/></div>
<p class="packt_figure CDPAlignLeft CDPAlign">Another nice feature is the <span class="packt_screen">E</span><span class="packt_screen">vents</span> page. Here, we can watch the events live, pause them, and filter them based on event severity and resource type:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="Images/5a11a984-804c-4fa6-8423-b99f62bcafe4.png" style="width:27.58em;height:41.92em;" width="640" height="974"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Events stream</div>
<p>A useful feature to browse anywhere in the dashboard system is the <span class="packt_screen">Namespace:</span> filtering option. Simply click on the drop-down menu next to the word <span class="packt_screen">N</span><span class="packt_screen">amespace:</span> at the top of any page that shows resources, and we can filter our views by namespace. This can be helpful if we want to filter out the Kubernetes system pods, or just look at a particular collection of resources:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="Images/c8b94900-44de-4112-9234-4ed5b897e16d.png" style="width:37.83em;height:17.50em;" width="633" height="293"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Namespace filtering</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Hosted platforms</h1>
                </header>
            
            <article>
                
<p>There are several options available for hosted Kubernetes in the cloud. These <strong>Platforms as a service</strong> (<strong>PaaS</strong>) can provide a stable operating model as you push towards production. Here's an overview of the major PaaSes provided by Amazon, Microsoft, and Google.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Amazon Web Services</h1>
                </header>
            
            <article>
                
<p><strong>Elastic Container Service</strong> (<strong>ECS</strong>) has just been launched as of the time of this chapter's writing. AWS is preparing a networking plugin to differentiate itself from other offerings, called the vpc-cni. This allows for pod networking in Kubernetes to use <strong>Elastic Network Interfaces</strong> (<strong>ENIs</strong>) on AWS. With ECS, you do have to pay for manager nodes, which is a different path to that taken by Microsoft and Google. ECS' startup procedure is also currently more complex and doesn't have single-command creation via the CLI.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Microsoft Azure</h1>
                </header>
            
            <article>
                
<p>The Azure Container Service is the second longest running hosted Kubernetes service in the cloud after the Google Kubernetes Engine. You can use Azure templates and the Resource Manager to spin up clusters with Terraform. Microsoft offers advanced networking features, integration with Azure Active Directory, and monitoring as its standout features.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Google Kubernetes Engine</h1>
                </header>
            
            <article>
                
<p>The Google Kubernetes Engine is another excellent option for running your containerized workloads. At the time of writing, it's considered to be one of the most robust offerings.  GKE is able to autoscale the cluster size, while AWS and Azure offer manual scaling. GKE offers a one-command start, and is the fastest to provision a Kubernetes cluster. It also offers an <em>Alpha Mode</em> where you can try bleeding edge features in the alpha channel releases. GKE provides high availability in zones and regions, the latter of which spreads out master node zones to provide best-in-class high availability.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at the emerging standards bodies in the container community and how they are using open specifications to shape the technology for the better. We looked at various container frameworks and runtimes. We dipped our toes into the CNCF, and tried out CRI-O.</p>
<p>We also took a closer look at CoreOS, a key player in both the container and Kubernetes community. We explored the technology that CoreOS is developing in order to enhance and complement container orchestration, and saw first-hand how to use some of it with Kubernetes. Finally, we looked at the supported enterprise offering of Tectonic and some of the features that are available now.</p>
<p>We also looked at some of the major PaaS offered by cloud service providers.</p>
<p>In the next chapter, we will explore the broader Kubernetes ecosystem and the tools available to move your cluster from development and testing into full-blown production.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><a href="https://www.opencontainers.org/faq/"><span class="URLPACKT">https://www.opencontainers.org/faq/</span></a> <span class="URLPACKT">(under</span> <span class="URLPACKT"><span class="packt_screen">How broad is the mission of the OCI?</span></span>)</li>
<li><a href="https://github.com/opencontainers/specs/blob/master/principles.md"><span class="URLPACKT">https://github.com/opencontainers/specs/blob/master/principles.md</span></a></li>
</ul>


            </article>

            
        </section>
    </div>



  </body></html>