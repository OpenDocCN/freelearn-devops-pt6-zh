- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: High Availability and Reliability
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用性与可靠性
- en: In *Chapter 2*, *Creating Kubernetes Clusters*, we learned how to create Kubernetes
    clusters in different environments, experimented with different tools, and created
    a couple of clusters. Creating a Kubernetes cluster is just the beginning of the
    story. Once the cluster is up and running, you need to make sure it stays operational.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第二章*，*创建 Kubernetes 集群*中，我们学习了如何在不同环境中创建 Kubernetes 集群，尝试了不同的工具，并创建了几个集群。创建
    Kubernetes 集群只是故事的开始。一旦集群启动并运行，你需要确保它保持正常运作。
- en: In this chapter, we will dive into the topic of highly available clusters. This
    is a complicated topic. The Kubernetes project and the community haven’t settled
    on one true way to achieve high availability nirvana. There are many aspects to
    highly available Kubernetes clusters, such as ensuring that the control plane
    can keep functioning in the face of failures, protecting the cluster state in
    etcd, protecting the system’s data, and recovering capacity and/or performance
    quickly. Different systems will have different reliability and availability requirements.
    How to design and implement a highly available Kubernetes cluster will depend
    on those requirements.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨高可用集群的话题。这是一个复杂的话题。Kubernetes 项目和社区还没有确定一种真正的方式来实现高可用性。高可用 Kubernetes
    集群有许多方面，例如确保控制平面在故障面前能够持续运行、保护 etcd 中的集群状态、保护系统数据以及快速恢复容量和/或性能。不同的系统将有不同的可靠性和可用性要求。如何设计和实现一个高度可用的
    Kubernetes 集群将取决于这些要求。
- en: 'This chapter will explore the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨以下主要主题：
- en: High availability concepts
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高可用性概念
- en: High availability best practices
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高可用性最佳实践
- en: High availability, scalability, and capacity planning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高可用性、可扩展性和容量规划
- en: Large cluster performance, cost, and design trade-offs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型集群的性能、成本和设计权衡
- en: Choosing and managing the cluster capacity
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择和管理集群容量
- en: Pushing the envelope with Kubernetes
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推动 Kubernetes 的极限
- en: Testing Kubernetes at scale
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大规模上测试 Kubernetes
- en: At the end of this chapter, you will understand the various concepts associated
    with high availability and be familiar with Kubernetes’ high availability best
    practices and when to employ them. You will be able to upgrade live clusters using
    different strategies and techniques, and you will be able to choose between multiple
    possible solutions based on trade-offs between performance, cost, and availability.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将理解与高可用性相关的各种概念，并熟悉 Kubernetes 的高可用性最佳实践以及何时使用它们。你将能够使用不同的策略和技术升级实时集群，并能够根据性能、成本和可用性之间的权衡，选择多种可能的解决方案。
- en: High availability concepts
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用性概念
- en: In this section, we will start our journey into high availability by exploring
    the concepts and building blocks of reliable and highly available systems. The
    million (trillion?) dollar question is, how do we build reliable and highly available
    systems from unreliable components? Components will fail; you can take that to
    the bank. Hardware will fail, networks will fail, configuration will be wrong,
    software will have bugs, and people will make mistakes. Accepting that, we need
    to design a system that can be reliable and highly available even when components
    fail. The idea is to start with redundancy, detect component failure, and replace
    bad components quickly.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将通过探索可靠且高度可用的系统的概念和构建模块，开始我们的高可用性之旅。百万（万亿？）美元的问题是，我们如何从不可靠的组件中构建可靠且高度可用的系统？组件会失败，这一点你可以放心。硬件会故障，网络会中断，配置会出错，软件会有
    bug，人们会犯错。接受这一点后，我们需要设计一个即使在组件失败时也能保持可靠和高度可用的系统。我们的思路是，从冗余开始，检测组件故障，并快速替换坏掉的组件。
- en: Redundancy
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 冗余
- en: Redundancy is the foundation of reliable and highly available systems at the
    hardware and software levels. If a critical component fails and you want the system
    to keep running, you must have another identical component ready to go. Kubernetes
    itself takes care of your stateless pods via replication controllers and replica
    sets. But, your cluster state in etcd and the control plane components themselves
    need redundancy to function when some components fail. In practice, this means
    running etcd and the API server on 3 or more nodes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 冗余是硬件和软件层面上可靠且高度可用系统的基础。如果一个关键组件发生故障，并且你希望系统继续运行，你必须准备另一个相同的组件。Kubernetes 本身通过复制控制器和副本集来管理你的无状态
    Pod。但你的 etcd 集群状态和控制平面组件本身需要冗余，以便在某些组件故障时继续运行。实际操作中，这意味着需要在 3 个或更多节点上运行 etcd 和
    API 服务器。
- en: In addition, if your system’s stateful components are not already backed up
    by redundant persistent storage (for example, on a cloud platform), then you need
    to add redundancy to prevent data loss.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你的系统的有状态组件还没有通过冗余的持久化存储进行备份（例如，在云平台上），你需要添加冗余以防止数据丢失。
- en: Hot swapping
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 热插拔
- en: 'Hot swapping is the concept of replacing a failed component on the fly without
    taking the system down, with minimal (ideally, zero) interruption for users. If
    the component is stateless (or its state is stored in separate redundant storage),
    then hot swapping a new component to replace it is easy and just involves redirecting
    all clients to the new component. But, if it stores local state, including in
    memory, then hot swapping is not trivial. There are two main options:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 热插拔是指在不中断系统的情况下，快速更换一个失败的组件，并尽量减少（理想情况下为零）对用户的影响。如果该组件是无状态的（或者它的状态存储在单独的冗余存储中），那么热插拔一个新组件来替换它很容易，只需将所有客户端重定向到新组件即可。但如果它存储了本地状态，包括内存中的状态，那么热插拔就不是一件简单的事。主要有两种选择：
- en: Give up on in-flight transactions (clients will retry)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 放弃正在进行的事务（客户端将重试）
- en: Keep a hot replica in sync (active-active)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持热备副本同步（主动-主动）
- en: The first solution is much simpler. Most systems are resilient enough to cope
    with failures. Clients can retry failed requests and the hot-swapped component
    will service them.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个解决方案要简单得多。大多数系统足够具备弹性来应对故障。客户端可以重试失败的请求，热插拔的组件将为其提供服务。
- en: The second solution is more complicated and fragile and will incur a performance
    overhead because every interaction must be replicated to both copies (and acknowledged).
    It may be necessary for some critical parts of the system.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个解决方案更复杂且更脆弱，并且会带来性能开销，因为每个交互都必须复制到两个副本（并且需要确认）。它可能是系统某些关键部分所必需的。
- en: Leader election
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 领导者选举
- en: Leader election is a common pattern in distributed systems. You often have multiple
    identical components that collaborate and share the load, but one component is
    elected as the leader and certain operations are serialized through the leader.
    You can think of distributed systems with leader election as a combination of
    redundancy and hot swapping. The components are all redundant and when the current
    leader fails or becomes unavailable, a new leader is elected and hot-swapped in.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 领导者选举是分布式系统中常见的模式。你通常有多个相同的组件，它们协作并分担负载，但会选举出一个组件作为领导者，某些操作会通过领导者进行序列化。你可以把带有领导者选举的分布式系统看作是冗余和热插拔的结合体。所有组件都是冗余的，当当前领导者失败或不可用时，会选举出一个新的领导者并进行热插拔。
- en: Smart load balancing
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 智能负载均衡
- en: Load balancing is about distributing the workload across multiple replicas that
    service incoming requests. This is useful for scaling up and down under a heavy
    load by adjusting the number of replicas. When some replicas fail, the load balancer
    will stop sending requests to failed or unreachable components. Kubernetes will
    provision new replicas, restore capacity, and update the load balancer. Kubernetes
    provides great facilities to support this via services, endpoints, replica sets,
    labels, and ingress controllers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡是将工作负载分配到多个副本上，以处理传入的请求。这对于在负载较重时通过调整副本数量进行扩展和收缩非常有用。当一些副本失败时，负载均衡器会停止向失败或无法访问的组件发送请求。Kubernetes
    会配置新的副本，恢复容量，并更新负载均衡器。Kubernetes 提供了出色的设施来支持这一点，通过服务、端点、副本集、标签和入口控制器。
- en: Idempotency
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幂等性
- en: Many types of failure can be temporary. This is most common with networking
    issues or with too-stringent timeouts. A component that doesn’t respond to a health
    check will be considered unreachable and another component will take its place.
    Work that was scheduled for the presumably failed component may be sent to another
    component. But the original component may still be working and complete the same
    work. The end result is that the same work may be performed twice. It is very
    difficult to avoid this situation. To support exactly-once semantics, you need
    to pay a heavy price in overhead, performance, latency, and complexity. Thus,
    most systems opt to support at-least-once semantics, which means it is OK for
    the same work to be performed multiple times without violating the system’s data
    integrity. This property is called idempotency. Idempotent systems maintain their
    state even if an operation is performed multiple times.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 许多类型的故障可能是暂时性的。这在网络问题或超时过于严格时最为常见。未响应健康检查的组件将被视为不可达，另一个组件将取而代之。原本为假定失败的组件安排的工作可能会发送到另一个组件。但是，原始组件可能仍在工作并完成相同的任务。最终的结果是相同的工作可能会执行两次。要避免这种情况非常困难。为了支持精确一次语义，你需要在开销、性能、延迟和复杂性方面付出很大的代价。因此，大多数系统选择支持至少一次语义，这意味着相同的工作可以多次执行，而不会破坏系统的数据完整性。这个特性称为幂等性。幂等系统即使执行操作多次，也能保持其状态。
- en: Self-healing
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自愈
- en: When component failures occur in dynamic systems, you usually want the system
    to be able to heal itself. Kubernetes replica sets are great examples of self-healing
    systems. But failure can extend well beyond pods. Self-healing starts with the
    automated detection of problems followed by an automated resolution. Quotas and
    limits help create checks and balances to ensure automated self-healing doesn’t
    run amok due to bugs or circumstances such as DDoS attacks. Self-healing systems
    deal very well with transient failures by retrying failed operations and escalating
    failures only when it’s clear there is no other option. Some self-healing systems
    have fallback paths including serving cached content if up-to-date content is
    unavailable. Self-healing systems attempt to degrade gracefully and keep working
    until the core issue can be fixed.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当动态系统中的组件发生故障时，通常希望系统能够自我修复。Kubernetes 的副本集就是自愈系统的典型例子。但是，故障可能远远超出 pod 的范围。自愈从自动检测问题开始，接着进行自动修复。配额和限制帮助创建检查与平衡，以确保自动化自愈不会因程序错误或诸如
    DDoS 攻击等特殊情况而失控。自愈系统通过重试失败的操作来很好地处理暂时性故障，并且只有在没有其他选项时才会升级故障。一些自愈系统具有回退路径，包括在无法获取最新内容时提供缓存内容。自愈系统尽力优雅降级，并在核心问题修复之前继续工作。
- en: In this section, we considered various concepts involved in creating reliable
    and highly available systems. In the next section, we will apply them and demonstrate
    best practices for systems deployed on Kubernetes clusters.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了构建可靠和高度可用系统所涉及的各种概念。在下一节中，我们将应用这些概念，并演示在 Kubernetes 集群上部署系统的最佳实践。
- en: High availability best practices
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用性最佳实践
- en: Building reliable and highly available distributed systems is a non-trivial
    endeavor. In this section, we will check some of the best practices that enable
    a Kubernetes-based system to function reliably and be available in the face of
    various failure categories. We will also dive deep and see how to go about constructing
    your own highly available clusters. However, due to the complexity and the large
    number of factors that impact HA clusters, we will just provide guidance. We will
    not provide here step by step instructions for building a HA cluster.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 构建可靠且高度可用的分布式系统是一项复杂的工作。在本节中，我们将检查一些最佳实践，这些最佳实践使基于 Kubernetes 的系统能够在面对各种故障类别时可靠运行并保持可用性。我们还将深入探讨如何构建你自己的高可用集群。然而，由于高可用集群的复杂性以及影响因素的众多，我们这里只提供指导，而不是逐步的构建高可用集群的教程。
- en: Note that you should roll your own highly available Kubernetes cluster only
    in very special cases. There are multiple robust tools (usually built on top of
    kubeadm) that provide battle-tested ways to create highly available Kubernetes
    clusters at the control plane level. You should take advantage of all the work
    and effort that went into these tools. In particular, the cloud providers offer
    managed Kubernetes clusters that are highly available.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你应该仅在非常特殊的情况下自己构建高度可用的 Kubernetes 集群。有多种强大的工具（通常是建立在 kubeadm 基础上）提供经过实战验证的方法，在控制平面级别创建高度可用的
    Kubernetes 集群。你应该充分利用这些工具所投入的所有工作和努力。特别是，云服务提供商提供的托管 Kubernetes 集群具有高度可用性。
- en: Creating highly available clusters
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建高度可用的集群
- en: 'To create a highly available Kubernetes cluster, the control plane components
    must be redundant. That means etcd must be deployed as a cluster (typically across
    three or five nodes) and the Kubernetes API server must be redundant. Auxiliary
    cluster-management services such as the observability stack storage should be
    deployed redundantly too. The following diagram depicts a typical reliable and
    highly available Kubernetes cluster in a stacked etcd topology. There are several
    load-balanced control plane nodes, each one containing all the control plane components
    as well as an etcd component:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个高度可用的 Kubernetes 集群，控制平面组件必须具备冗余性。这意味着 etcd 必须作为集群部署（通常跨三个或五个节点），并且 Kubernetes
    API 服务器必须具有冗余。辅助的集群管理服务，如可观察性堆栈存储，也应当冗余部署。以下图示描绘了一个典型的可靠且高度可用的 Kubernetes 集群，其采用堆叠的
    etcd 拓扑结构。这里有多个负载均衡的控制平面节点，每个节点都包含所有控制平面组件以及一个 etcd 组件：
- en: '![Figure 3.1: A highly available cluster configuration](img/B18998_03_01.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1：高度可用的集群配置](img/B18998_03_01.png)'
- en: 'Figure 3.1: A highly available cluster configuration'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：高度可用的集群配置
- en: This is not the only way to configure highly available clusters. You may prefer,
    for example, to deploy a standalone etcd cluster to optimize the machines to their
    workload or if you require more redundancy for your etcd cluster than the rest
    of the control plane nodes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是配置高度可用集群的唯一方法。例如，你可能更倾向于部署一个独立的 etcd 集群，以优化机器的工作负载，或者如果你需要比其他控制平面节点更高的冗余性来保护你的
    etcd 集群。
- en: 'The following diagram shows a Kubernetes cluster where etcd is deployed as
    an external cluster:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个 Kubernetes 集群，其中 etcd 被部署为外部集群：
- en: '![Figure 3.2: etcd used as an external cluster](img/B18998_03_02.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2：etcd 用作外部集群](img/B18998_03_02.png)'
- en: 'Figure 3.2: etcd used as an external cluster'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：etcd 用作外部集群
- en: Self-hosted Kubernetes clusters, where control plane components are deployed
    as pods and stateful sets in the cluster, are a great approach to simplify the
    robustness, disaster recovery, and self-healing of the control plane components
    by applying Kubernetes to Kubernetes. This means that some of the components that
    manage Kubernetes are themselves managed by Kubernetes. For example, if one of
    the Kubernetes API server nodes goes down, the other API server pods will notice
    and provision a new API server.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 自托管的 Kubernetes 集群，其中控制平面组件作为 pods 和 stateful sets 部署在集群中，是一种很好的方法，通过将 Kubernetes
    应用到 Kubernetes 来简化控制平面组件的可靠性、灾难恢复和自我修复。这意味着一些管理 Kubernetes 的组件本身也由 Kubernetes
    来管理。例如，如果某个 Kubernetes API 服务器节点发生故障，其他 API 服务器 pods 会察觉到并启动一个新的 API 服务器。
- en: Making your nodes reliable
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高节点的可靠性
- en: Nodes will fail, or some components will fail, but many failures are transient.
    The basic guarantee is to make sure that the runtime engine (Docker daemon, Containerd,
    or whatever the CRI implementation is) and the kubelet restart automatically in
    the event of a failure.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 节点可能会失败，或者某些组件可能会失败，但许多故障是暂时的。基本的保证是确保运行时引擎（Docker 守护进程、Containerd 或任何 CRI 实现）和
    kubelet 在故障发生时能够自动重启。
- en: 'If you run CoreOS, a modern Debian-based OS (including Ubuntu >= 16.04), or
    any other OS that uses systemd as its init mechanism, then it’s easy to deploy
    Docker and the kubelet as self-starting daemons:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行的是 CoreOS、基于现代 Debian 的操作系统（包括 Ubuntu >= 16.04），或者任何其他使用 systemd 作为初始化机制的操作系统，那么部署
    Docker 和 kubelet 作为自启动守护进程是非常简单的：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For other operating systems, the Kubernetes project selected the **monit** process
    monitor for their high availability example, but you can use any process monitor
    you prefer. The main requirement is to make sure that those two critical components
    will restart in the event of failure, without external intervention.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他操作系统，Kubernetes项目选择了**monit**进程监视器作为高可用性示例，但你可以使用任何你喜欢的进程监视器。主要要求是确保这两个关键组件在发生故障时会自动重启，无需外部干预。
- en: See [https://monit-docs.web.cern.ch/base/kubernetes/](https://monit-docs.web.cern.ch/base/kubernetes/).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 查看此链接：[https://monit-docs.web.cern.ch/base/kubernetes/](https://monit-docs.web.cern.ch/base/kubernetes/)。
- en: Protecting your cluster state
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保护你的集群状态
- en: The Kubernetes cluster state is typically stored in etcd (some Kubernetes implementations
    like k3s use alternative storage engines like SQLite). The etcd cluster was designed
    to be super reliable and distributed across multiple nodes. It’s important to
    take advantage of these capabilities for a reliable and highly available Kubernetes
    cluster by making sure to have multiple copies of the cluster state in case one
    of the copies is lost and unreachable.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群状态通常存储在etcd中（某些Kubernetes实现，如k3s，使用像SQLite这样的替代存储引擎）。etcd集群设计时考虑了超高的可靠性，并且分布在多个节点上。为了确保Kubernetes集群的可靠性和高可用性，利用这些功能是至关重要的，确保有多个副本存储集群状态，以防某个副本丢失或不可达。
- en: Clustering etcd
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群化etcd
- en: You should have at least three nodes in your etcd cluster. If you need more
    reliability and redundancy, you can have five, seven, or any other odd number
    of nodes. The number of nodes must be odd to have a clear majority in case of
    a network split.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你的etcd集群应该至少有三个节点。如果需要更高的可靠性和冗余，可以选择五个、七个或任何其他奇数节点。节点数必须是奇数，以便在发生网络分裂时能有明确的多数节点。
- en: 'In order to create a cluster, the etcd nodes should be able to discover each
    other. There are several methods to accomplish that such as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个集群，etcd节点应该能够互相发现。实现这一点有多种方法，比如：
- en: static
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态
- en: etcd discovery
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd发现
- en: DNS discovery
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS发现
- en: 'The etcd-operator project from CoreOS used to be the go-to solution for deploying
    etcd clusters. Unfortunately, the project has been archived and is not developed
    actively anymore. Kubeadm uses the static method for provisioning etcd clusters
    for Kubernetes, so if you use any tool based on kubeadm, you’re all set. If you
    want to deploy a HA etcd cluster, I recommend following the official documentation:
    [https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md](https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS的etcd-operator项目曾是部署etcd集群的首选解决方案。不幸的是，该项目已被归档，目前不再积极开发。Kubeadm使用静态方法来配置Kubernetes的etcd集群，因此如果你使用任何基于kubeadm的工具，你就可以顺利完成。如果你想部署一个高可用的etcd集群，建议你遵循官方文档：[https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md](https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md)。
- en: Protecting your data
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保护你的数据
- en: Protecting the cluster state and configuration is great, but even more important
    is protecting your own data. If somehow the cluster state gets corrupted, you
    can always rebuild the cluster from scratch (although the cluster will not be
    available during the rebuild). But if your own data is corrupted or lost, you’re
    in deep trouble. The same rules apply; redundancy is king. But while the Kubernetes
    cluster state is very dynamic, much of your data may be less dynamic. For example,
    a lot of historic data is often important and can be backed up and restored. Live
    data might be lost, but the overall system may be restored to an earlier snapshot
    and suffer only temporary damage.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 保护集群状态和配置固然重要，但更重要的是保护你自己的数据。如果集群状态 somehow 受损，你总是可以从头重建集群（尽管在重建期间集群不可用）。但如果你自己的数据受损或丢失，你就会陷入深深的麻烦。相同的规则依然适用，冗余是王道。虽然Kubernetes集群状态是高度动态的，但你的数据可能不那么动态。例如，很多历史数据通常很重要，可以备份并恢复。实时数据可能会丢失，但整个系统可以恢复到较早的快照，并且仅遭受暂时性损坏。
- en: You should consider Velero as a solution for backing up your entire cluster
    including your own data. Heptio (now part of VMWare) developed Velero, which is
    open source and may be a lifesaver for critical systems.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该考虑使用Velero作为备份整个集群（包括你的数据）的解决方案。Heptio（现在是VMWare的一部分）开发了Velero，它是开源的，并且对于关键系统来说可能是救命稻草。
- en: 'Check it out here: [https://velero.io/](https://velero.io/).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里查看：[https://velero.io/](https://velero.io/)。
- en: Running redundant API servers
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行冗余的API服务器
- en: The API servers are stateless, fetching all the necessary data on the fly from
    the etcd cluster. This means that you can easily run multiple API servers without
    needing to coordinate between them. Once you have multiple API servers running,
    you can put a load balancer in front of them to make it transparent to clients.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: API 服务器是无状态的，所有必要的数据都实时从 etcd 集群中获取。这意味着你可以轻松运行多个 API 服务器，而无需在它们之间进行协调。一旦你运行了多个
    API 服务器，可以在它们前面放置一个负载均衡器，使客户端对其透明。
- en: Running leader election with Kubernetes
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 运行领导选举
- en: Some control plane components, such as the scheduler and the controller manager,
    can’t have multiple instances active at the same time. This will be chaos, as
    multiple schedulers try to schedule the same pod into multiple nodes or multiple
    times into the same node. It is possible to run multiple schedulers that are configured
    to manage different pods. The correct way to have a highly scalable Kubernetes
    cluster is to have these components run in leader election mode. This means that
    multiple instances are running but only one is active at a time, and if it fails,
    another one is elected as leader and takes its place.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一些控制平面组件，例如调度器和控制器管理器，不能同时有多个实例处于活动状态。这将导致混乱，因为多个调度器会试图将同一个 Pod 调度到多个节点，或者将多个调度请求调度到同一个节点。可以运行多个调度器，并配置它们管理不同的
    Pods。实现高可扩展 Kubernetes 集群的正确方法是让这些组件在领导选举模式下运行。这意味着多个实例同时运行，但只有一个处于活动状态，如果它失败，另一个将被选举为领导并接替其位置。
- en: Kubernetes supports this mode via the `–leader-elect` flag (the default is `True`).
    The scheduler and the controller manager can be deployed as pods by copying their
    respective manifests to `/etc/kubernetes/manifests`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 通过 `–leader-elect` 标志支持此模式（默认值为 `True`）。调度器和控制器管理器可以通过将它们各自的清单复制到
    `/etc/kubernetes/manifests` 目录中，作为 Pods 部署。
- en: 'Here is a snippet from a scheduler manifest that shows the use of the flag:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个来自调度器清单的代码片段，展示了如何使用该标志：
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is a snippet from a controller manager manifest that shows the use of
    the flag:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个来自控制器管理器清单的代码片段，展示了如何使用该标志：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There are several other flags to control leader election. All of them have
    reasonable defaults:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 还有几个标志可用来控制领导选举。它们都有合理的默认值：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that it is not possible to have these components restarted automatically
    by Kubernetes like other pods because these are exactly the Kubernetes components
    responsible for restarting failed pods, so they can’t restart themselves if they
    fail. There must be a ready-to-go replacement already running.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Kubernetes 不可能像其他 Pods 那样自动重启这些组件，因为它们恰恰是负责重启失败 Pods 的 Kubernetes 组件，因此它们无法在失败时自行重启。必须有一个已准备好的替代实例在运行。
- en: Making your staging environment highly available
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使你的暂存环境具备高可用性
- en: High availability is not trivial to set up. If you go to the trouble of setting
    up high availability, it means there is a business case for a highly available
    system. It follows that you want to test your reliable and highly available cluster
    before you deploy it to production (unless you’re Netflix, where you test in production).
    Also, any change to the cluster may, in theory, break your high availability without
    disrupting other cluster functions. The essential point is that, just like anything
    else, if you don’t test it, assume it doesn’t work.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用性并不是一个简单的配置。如果你花费时间去设置高可用性，那就意味着你的系统有高可用性的业务需求。因此，在将其部署到生产环境之前，你需要测试你的可靠性和高可用性集群（除非你是
    Netflix，那里是在生产环境中测试的）。另外，集群中的任何更改在理论上可能会破坏高可用性，而不会影响其他集群功能。关键点是，就像其他任何事情一样，如果你不测试它，就假设它无法正常工作。
- en: 'We’ve established that you need to test reliability and high availability.
    The best way to do it is to create a staging environment that replicates your
    production environment as closely as possible. This can get expensive. There are
    several ways to manage the cost:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确认你需要测试可靠性和高可用性。最好的方法是创建一个尽可能接近生产环境的暂存环境。这可能会变得很昂贵。这里有几种方法来管理成本：
- en: '**Ad hoc HA staging environment**: Create a large HA cluster only for the duration
    of HA testing.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**临时高可用性暂存环境**：只为高可用性测试的期间创建一个大型高可用性集群。'
- en: '**Compress time**: Create interesting event streams and scenarios ahead of
    time, feed the input, and simulate the situations in rapid succession.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**压缩时间**：提前创建有趣的事件流和场景，输入这些数据，并快速模拟各种情况。'
- en: '**Combine HA testing with performance and stress testing**: At the end of your
    performance and stress tests, overload the system and see how the reliability
    and high availability configuration handles the load.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将HA测试与性能和压力测试结合起来**：在你的性能和压力测试结束时，对系统进行超负荷测试，看看可靠性和高可用性配置如何处理负载。'
- en: It’s also important to practice chaos engineering and intentionally instigate
    failure at different levels to verify the system can handle those failure modes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，实践混沌工程并故意在不同层次上引发故障也非常重要，以验证系统是否能够处理这些故障模式。
- en: Testing high availability
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试高可用性
- en: Testing high availability takes planning and a deep understanding of your system.
    The goal of every test is to reveal flaws in the system’s design and/or implementation
    and to provide good enough coverage that, if the tests pass, you’ll be confident
    that the system behaves as expected.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 测试高可用性需要规划和对系统的深入理解。每个测试的目标是揭示系统设计和/或实现中的缺陷，并提供足够的覆盖范围，以便如果测试通过，你可以确信系统按照预期行为运行。
- en: In the realm of reliability, self-healing, and high availability, it means you
    need to figure out ways to break the system and watch it put itself back together.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在可靠性、自愈和高可用性的领域，这意味着你需要找到方法来破坏系统并观察它如何自我修复。
- en: 'That requires several pieces, as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要几个部分，如下所示：
- en: Comprehensive list of possible failures (including reasonable combinations)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能故障的全面清单（包括合理的组合情况）
- en: For each possible failure, it should be clear how the system should respond
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每一种可能的故障，应该清楚地知道系统应该如何响应。
- en: A way to induce the failure
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诱发故障的方法
- en: A way to observe how the system reacts
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种观察系统反应的方法。
- en: None of the pieces are trivial. The best approach in my experience is to do
    it incrementally and try to come up with a relatively small number of generic
    failure categories and generic responses, rather than an exhaustive, ever-changing
    list of low-level failures.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 没有哪一部分是微不足道的。根据我的经验，最好的方法是逐步进行，尝试提出相对较少的通用故障类别和通用响应，而不是列出一个详尽的、不断变化的低级故障清单。
- en: For example, a generic failure category is node-unresponsive. The generic response
    could be rebooting the node, the way to induce the failure can be stopping the
    VM of the node (if it’s a VM), and the observation should be that, while the node
    is down, the system still functions properly based on standard acceptance tests;
    the node is eventually up, and the system gets back to normal. There may be many
    other things you want to test, such as whether the problem was logged, whether
    relevant alerts went out to the right people, and whether various stats and reports
    were updated.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个通用的故障类别是节点无响应。通用的响应可以是重启节点，诱发故障的方法可以是停止节点的虚拟机（如果它是虚拟机），观察应当是，在节点停机期间，系统仍然能够通过标准验收测试正常运行；节点最终恢复，系统恢复正常。你可能还想测试其他很多事情，比如问题是否被记录，是否有相关的警报发送给了正确的人，是否更新了各种统计信息和报告。
- en: But, beware of over-generalizing. In the case of the generic unresponsive node
    failure mode, a key component is detecting that the node is unresponsive. If your
    method of detection is faulty, then your system will not react properly. Use best
    practices like health checks and readiness checks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，要小心过度泛化。在通用的无响应节点故障模式下，一个关键的组成部分是检测节点是否无响应。如果你的检测方法存在缺陷，那么系统将无法做出正确反应。请使用健康检查和就绪检查等最佳实践。
- en: Note that sometimes, a failure can’t be resolved in a single response. For example,
    in our unresponsive node case, if it’s a hardware failure, then a reboot will
    not help. In this case, the second line of response gets into play and maybe a
    new node is provisioned to replace the failed node. In this case, you can’t be
    too generic and you may need to create tests for specific types of pods/roles
    that were on the node (etcd, master, worker, database, and monitoring).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，有时一个故障无法通过单一响应来解决。例如，在我们节点无响应的案例中，如果是硬件故障，那么重启不会有所帮助。在这种情况下，第二线响应就会发挥作用，可能会通过提供新的节点来替代故障的节点。在这种情况下，你不能过于泛化，可能需要为节点上特定类型的Pods/角色（例如etcd、master、worker、数据库和监控）创建测试。
- en: If you have high-quality requirements, be prepared to spend much more time setting
    up the proper testing environments and tests than even the production environment.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有高质量的需求，请准备花费比生产环境更多的时间来设置适当的测试环境和测试。
- en: One last important point is to try to be as non-intrusive as possible. That
    means that, ideally, your production system will not have testing features that
    allow shutting down parts of it or cause it to be configured to run at reduced
    capacity for testing. The reason is that it increases the attack surface of your
    system and it can be triggered by accident by mistakes in configuration. Ideally,
    you can control your testing environment without resorting to modifying the code
    or configuration that will be deployed in production. With Kubernetes, it is usually
    easy to inject pods and containers with custom test functionality that can interact
    with system components in the staging environment but will never be deployed in
    production.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个重要的观点是尽量保持非侵入性。这意味着理想情况下，您的生产系统不应具有允许关闭其部分或使其配置为在测试中以降低容量运行的测试功能。原因是这会增加系统的攻击面，并且可能会因配置错误而意外触发。理想情况下，您可以在不修改将部署在生产中的代码或配置的情况下控制测试环境。使用
    Kubernetes，通常可以轻松地注入具有自定义测试功能的 Pod 和容器，这些功能可以与演示环境中的系统组件交互，但永远不会部署到生产环境中。
- en: 'The Chaos Mesh CNCF incubating project is a good starting point: [https://chaos-mesh.org](https://chaos-mesh.org).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Chaos Mesh CNCF 孵化项目是一个很好的起点：[https://chaos-mesh.org](https://chaos-mesh.org)。
- en: In this section, we looked at what it takes to actually have a reliable and
    highly available cluster, including etcd, the API server, the scheduler, and the
    controller manager. We considered best practices for protecting the cluster itself,
    as well as your data, and paid special attention to the issue of starting environments
    and testing.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们看了如何确保一个可靠且高可用的集群，包括 etcd、API 服务器、调度器和控制器管理器。我们考虑了保护集群本身和您的数据的最佳实践，并特别关注了启动环境和测试的问题。
- en: High availability, scalability, and capacity planning
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用性、可扩展性和容量规划
- en: Highly available systems must also be scalable. The load on most complicated
    distributed systems can vary dramatically based on time of day, weekday vs weekend,
    seasonal effects, marketing campaigns, and many other factors. Successful systems
    will have more users over time and accumulate more and more data. That means that
    the physical resources of the clusters - mostly nodes and storage - will have
    to grow over time too. If your cluster is under-provisioned, it will not be able
    to satisfy all demands and it will not be available because requests will time
    out or be queued up and not processed fast enough.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用系统还必须具备可扩展性。大多数复杂的分布式系统的负载可能会根据一天中的时间、工作日与周末、季节性影响、营销活动以及许多其他因素而显著变化。成功的系统会随着时间推移拥有更多用户并积累更多数据。这意味着集群的物理资源
    - 主要是节点和存储 - 也必须随着时间增长。如果您的集群配置不足，它将无法满足所有需求，并且由于请求超时或排队等待而无法提供服务。
- en: 'This is the realm of capacity planning. One simple approach is to over-provision
    your cluster. Anticipate the demand and make sure you have enough of a buffer
    for spikes of activity. But, this approach suffers from several deficiencies:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是容量规划的领域。一种简单的方法是过度配置您的集群。预测需求并确保您有足够的缓冲区以处理活动高峰。但是，这种方法存在几个不足之处：
- en: For highly dynamic and complicated distributed systems, it’s difficult to predict
    the demand even approximately.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于高度动态和复杂的分布式系统，即使是大致预测需求也很困难。
- en: Over-provisioning is expensive. You spend a lot of money on resources that are
    rarely or never used.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过度配置是昂贵的。您会在很少或从不使用的资源上花费大量资金。
- en: You have to periodically redo the whole process because the average and peak
    load on the system changes over time.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您必须定期重新执行整个过程，因为系统的平均和峰值负载随时间而变化。
- en: You have to do the entire process for multiple groups of workloads that use
    specific resources (e.g. workloads that use high-memory nodes and workloads that
    require GPUs).
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您必须为使用特定资源的多组工作负载执行整个过程（例如使用高内存节点的工作负载和需要 GPU 的工作负载）。
- en: A much better approach is to use intent-based capacity planning where high-level
    abstraction is used and the system adjusts itself accordingly. In the context
    of Kubernetes, there is the **Horizontal Pod Autoscaler** (**HPA**), which can
    grow and shrink the number of pods needed to handle requests for a particular
    workload. But, that works only to change the ratio of resources allocated to different
    workloads. When the entire cluster (or node pool) approaches saturation, you simply
    need more resources. This is where the cluster autoscaler comes into play. It
    is a Kubernetes project that became available with Kubernetes 1.8\. It works particularly
    well in cloud environments where additional resources can be provisioned via programmatic
    APIs.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的方法是使用基于意图的容量规划，其中使用高级抽象，系统会相应地调整自己。在 Kubernetes 的环境中，有**水平 Pod 自动伸缩器**（**HPA**），它可以根据工作负载请求的需要增加或减少所需的
    Pod 数量。但是，这仅适用于更改分配给不同工作负载的资源比例。当整个集群（或节点池）接近饱和时，您只需增加更多资源。这就是集群自动缩放器发挥作用的地方。它是一个
    Kubernetes 项目，在 Kubernetes 1.8 版本中推出。它在云环境中特别有效，可以通过编程 API 提供额外的资源。
- en: When the **cluster autoscaler** (**CAS**) determines that pods can’t be scheduled
    (are in a pending state) it provisions a new node for the cluster. It can also
    remove nodes from the cluster (downscaling) if it determines that the cluster
    has more nodes than necessary to handle the load. The CAS will check for pending
    pods every 30 seconds by default. It will remove nodes only after 10 minutes of
    low usage to avoid thrashing.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当**集群自动缩放器**（**CAS**）确定无法为 Pod 安排节点（处于挂起状态）时，它会为集群提供一个新节点。如果确定集群的节点多于处理负载所需的节点，它还可以从集群中删除节点（缩减）。CAS
    默认每30秒检查一次挂起的 Pod。仅在低使用率持续10分钟后才会删除节点，以避免频繁变动。
- en: The CAS makes its scale-down decision based on CPU and memory usage. If the
    sum of CPU and memory requests of all pods running on a node is smaller than 50%
    (by default, it is configurable) of the node’s allocatable resources, then the
    node will be considered for removal. All pods (except DaemonSet pods) must be
    movable (some pods can’t be moved due to factors like scheduling constraints or
    local storage) and the node must not have scale-down disabled.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: CAS 根据 CPU 和内存使用情况做出缩减决策。如果所有运行在节点上的 Pod 的 CPU 和内存请求总和小于节点可分配资源的50%（默认可配置），则会考虑删除该节点。所有
    Pod（除了 DaemonSet Pod）必须是可移动的（某些 Pod 由于调度约束或本地存储等因素无法移动），并且节点不能禁用缩减。
- en: 'Here are some issues to consider:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是需要考虑的一些问题：
- en: A cluster may require more nodes even if the total CPU or memory utilization
    is low due to control mechanisms like affinity, anti-affinity, taints, tolerations,
    pod priorities, max pods per node, max persistent volumes per node, and pod disruption
    budgets.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使总 CPU 或内存利用率较低，由于诸如亲和性、反亲和性、污点、容忍度、Pod 优先级、每节点最大 Pod 数量、每节点最大持久卷数量和 Pod 中断预算等控制机制，集群可能仍需要更多节点。
- en: In addition to the built-in delays in triggering scale up or scale down of nodes,
    there is an additional delay of several minutes when provisioning a new node from
    the cloud provider.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了触发节点扩展或缩减的内置延迟外，从云服务提供商预配新节点还会增加几分钟的额外延迟。
- en: Some nodes (e.g with local storage) can’t be removed by default (require special
    annotation).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有些节点（例如带有本地存储的节点）默认情况下无法删除（需要特殊标注）。
- en: The interactions between HPA and the CAS can be subtle.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HPA 和 CAS 之间的交互可能会有些微妙。
- en: Installing the cluster autoscaler
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装集群自动缩放器
- en: 'Note that you can’t test the CAS locally. You must have a Kubernetes cluster
    running on one of the supported cloud providers:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您无法在本地测试 CAS。您必须在支持的云服务提供商之一上运行 Kubernetes 集群：
- en: AWS
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS
- en: BaiduCloud
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BaiduCloud
- en: Brightbox
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brightbox
- en: CherryServers
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CherryServers
- en: CloudStack
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CloudStack
- en: HuaweiCloud
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HuaweiCloud
- en: External gRPC
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部 gRPC
- en: Hetzner
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hetzner
- en: Equinix Metal
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Equinix Metal
- en: IonosCloud
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IonosCloud
- en: OVHcloud
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OVHcloud
- en: Linode
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linode
- en: OracleCloud
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OracleCloud
- en: ClusterAPI
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ClusterAPI
- en: BizflyCloud
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BizflyCloud
- en: Vultr
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vultr
- en: TencentCloud
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TencentCloud
- en: 'I have installed it successfully on GKE, EKS, and AKS. There are two reasons
    to use a CAS in a cloud environment:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我已成功在 GKE、EKS 和 AKS 上安装了它。在云环境中使用 CAS 的两个原因如下：
- en: You installed non-managed Kubernetes yourself and you want to benefit from the
    CAS.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您自行安装了非托管 Kubernetes，并希望从 CAS 中受益。
- en: You use a managed Kubernetes, but you want to modify some of its settings (e.g
    higher CPU utilization threshold). In this case, you will need to disable the
    cloud provider’s CAS to avoid conflicts.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您正在使用托管的 Kubernetes，但希望修改一些设置（例如更高的 CPU 利用率阈值）。在这种情况下，您需要禁用云服务提供商的 CAS 以避免冲突。
- en: 'Let’s look at the manifests for installing CAS on AWS. There are several ways
    to do it. I chose the multi-ASG (auto-scaling groups option), which is the most
    production-ready. It supports multiple node groups with different configurations.
    The file contains all the Kubernetes resources needed to install the cluster autoscaler.
    It involves creating a service account, and giving it various RBAC permissions
    because it needs to monitor node usage across the cluster and be able to act on
    it. Finally, there is a Deployment that actually deploys the cluster autoscaler
    image itself with a command line that includes the range of nodes (minimum and
    maximum number) it should maintain, and in the case of EKS, node groups are needed
    too. The maximum number is important to prevent a situation where an attack or
    error causes the cluster autoscaler to just add more and more nodes uncontrollably
    and rack up a huge bill. The full file is here: [https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-multi-asg.yaml](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-multi-asg.yaml).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下在 AWS 上安装 CAS 的清单。有几种方法可以做到这一点。我选择了多 ASG（自动扩缩组选项），这是最适合生产的。它支持具有不同配置的多个节点组。该文件包含安装集群自动扩缩器所需的所有
    Kubernetes 资源。它涉及创建一个服务账户，并授予它各种 RBAC 权限，因为它需要监控整个集群的节点使用情况，并能够对其进行操作。最后，还有一个
    Deployment，实际部署集群自动扩缩器镜像，并附带一个命令行，指定应维护的节点范围（最小和最大数量），在 EKS 的情况下，还需要节点组。最大数量非常重要，以防攻击或错误导致集群自动扩缩器不断添加节点，从而失控并造成巨大的账单。完整文件请见：[https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-multi-asg.yaml](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-multi-asg.yaml)。
- en: 'Here is a snippet from the pod template of the Deployment:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 Deployment 的 pod 模板片段：
- en: '[PRE4]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: See [https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-multi-asg.yaml#L120](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-multi-asg.yaml#L120).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看 [https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-multi-asg.yaml#L120](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-multi-asg.yaml#L120)。
- en: The combination of the HPA and CAS provides a truly elastic cluster where the
    HPA ensures that services use the proper amount of pods to handle the load per
    service and the CAS makes sure that the number of nodes matches the overall load
    on the cluster.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: HPA 和 CAS 的结合提供了一个真正弹性的集群，其中 HPA 确保服务使用适当数量的 pods 来处理每个服务的负载，而 CAS 确保节点的数量与集群的整体负载匹配。
- en: Considering the vertical pod autoscaler
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考虑垂直 pod 自动缩放器
- en: 'The vertical pod autoscaler is another autoscaler that operates on pods. Its
    job is to adjust the CPU and memory requests and limits of pods based on actual
    usage. It is configured using a CRD for each workload and has three components:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直 pod 自动缩放器是另一个作用于 pods 的自动缩放器。它的作用是根据实际使用情况调整 pods 的 CPU 和内存请求与限制。它通过 CRD
    为每个工作负载配置，并具有三个组件：
- en: Recommender - Watches CPU and memory usage and provides recommendations for
    new values for CPU and memory requests
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐器 - 监控 CPU 和内存使用情况，并为 CPU 和内存请求提供新值的推荐
- en: Updater - Kills managed pods whose CPU and memory requests don’t match the recommendations
    made by the recommender
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新器 - 终止那些其 CPU 和内存请求不符合推荐值的受管 pods
- en: Admission control webhook - Sets the CPU and memory requests for new or recreated
    pods based on recommendations
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Admission control webhook - 根据推荐值设置新建或重建 pods 的 CPU 和内存请求
- en: The VPA can run in recommendation mode only or actively resizing pods. When
    the VPA decides to resize a pod, it evicts the pod. When the pod is rescheduled,
    it modifies the requests and limits based on the latest recommendation.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: VPA 可以仅在推荐模式下运行，也可以主动调整 pods 的大小。当 VPA 决定调整 pod 大小时，它会驱逐该 pod。重新调度 pod 时，它会根据最新的推荐修改请求和限制。
- en: 'Here is an example that defines a VPA custom resource for a Deployment called
    `awesome-deployment` in recommendation mode:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个定义在推荐模式下的 VPA 自定义资源的示例，针对名为 `awesome-deployment` 的 Deployment：
- en: '[PRE5]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here are some of the main requirements and limitations of using the VPA:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 VPA 时的一些主要要求和限制：
- en: Requires the metrics server
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要 metrics server
- en: Can’t set memory to less than 250Mi
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不能将内存设置为低于 250Mi
- en: Unable to update running pod (hence the updater kills pods to get them restarted
    with the correct requests)
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can’t evict pods that aren’t managed by a controller
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not recommended to run the VPA alongside the HPA
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoscaling based on custom metrics
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The HPA operates by default on CPU and memory metrics. But, it can be configured
    to operate on arbitrary custom metrics like a queue depth (e.g. AWS SQS queue)
    or a number of threads, which may become the bottleneck due to concurrency even
    if there is still available CPU and memory. The Keda project ([https://keda.sh/](https://keda.sh/))
    provides a strong solution for custom metrics instead of starting from scratch.
    They use the concept of event-based autoscaling as a generalization.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: This section covered the interactions between auto-scalability and high availability
    and looked at different approaches for scaling Kubernetes clusters and the applications
    running on these clusters.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Large cluster performance, cost, and design trade-offs
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we looked at various ways to provision and plan for
    capacity and autoscale clusters and workloads. In this section, we will consider
    the various options and configurations of large clusters with different reliability
    and high availability properties. When you design your cluster, you need to understand
    your options and choose wisely based on the needs of your organization.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The topics we will cover include various availability requirements, from best
    effort all the way to the holy grail of zero downtime. Finally, we will settle
    down on the practical site reliability engineering approach. For each category
    of availability, we will consider what it means from the perspectives of performance
    and cost.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '**Availability requirements**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Different systems have very different requirements for reliability and availability.
    Moreover, different sub-systems have very different requirements. For example,
    billing systems are always a high priority because if the billing system is down,
    you can’t make money. But, even within the billing system, if the ability to dispute
    charges is sometimes unavailable, it may be OK from the business point of view.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Best effort
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Best effort means, counter-intuitively, no guarantee whatsoever. If it works,
    great! If it doesn’t work – oh well, what are you going to do? This level of reliability
    and availability may be appropriate for internal components that change often
    and the effort to make them robust is not worth it. As long the services or clients
    that invoke the unreliable services are able to handle the occasional errors or
    outages, then all is well. It may also be appropriate for services released in
    the wild as beta.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Best effort is great for developers. Developers can move fast and break things.
    They are not worried about the consequences and they don’t have to go through
    a gauntlet of rigorous tests and approvals. The performance of best effort services
    may be better than more robust services because the best effort service can often
    skip expensive steps such as verifying requests, persisting intermediate results,
    and replicating data. But, on the other hand, more robust services are often heavily
    optimized and their supporting hardware is fine-tuned to their workload. The cost
    of best effort services is usually lower because they don’t need to employ redundancy
    unless the operators neglect to do basic capacity planning and just over-provision
    needlessly.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳努力模式对开发人员来说是很有利的。开发人员可以快速行动并打破常规。他们不必担心后果，也不需要经过严格的测试和批准过程。最佳努力服务的性能可能优于更稳健的服务，因为最佳努力服务通常可以跳过一些昂贵的步骤，例如验证请求、持久化中间结果和数据复制。但另一方面，更稳健的服务通常会经过大量优化，且其支持硬件已针对工作负载进行了精细调校。最佳努力服务的成本通常较低，因为它们不需要采用冗余，除非运营商忽视了基本的容量规划，导致过度配置。
- en: In the context of Kubernetes, the big question is whether all the services provided
    by the cluster are best effort. If this is the case, then the cluster itself doesn’t
    have to be highly available. You can probably have a single master node with a
    single instance of etcd, and a monitoring solution may not even need to be deployed.
    This is typically appropriate for local development clusters only. Even a shared
    development cluster that multiple developers use should have a decent level of
    reliability and robustness or else all the developers will be twiddling their
    thumbs whenever the cluster goes down unexpectedly.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes的背景下，关键问题是集群提供的所有服务是否都是最佳努力模式。如果是这样，那么集群本身就不需要高度可用。您可能只需要一个主节点和一个etcd实例，甚至可能不需要部署监控解决方案。这通常仅适用于本地开发集群。即使是多个开发人员共享的开发集群，也应该具有合理的可靠性和稳健性，否则每当集群意外宕机时，所有开发人员都会无所事事。
- en: Maintenance windows
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维护窗口
- en: In a system with maintenance windows, special times are dedicated to performing
    various maintenance activities, such as applying security patches, upgrading software,
    pruning log files, and database cleanups. With a maintenance window, the system
    (or a sub-system) becomes unavailable. This is typically planned for off-hours
    and often, users are notified. The benefit of maintenance windows is that you
    don’t have to worry about how your maintenance actions are going to interact with
    live requests coming into the system. It can drastically simplify operations.
    System administrators and operators love maintenance windows just as much as developers
    love best effort systems.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在有维护窗口的系统中，特定时间段被专门用于执行各种维护活动，如应用安全补丁、升级软件、修剪日志文件和数据库清理。有了维护窗口，系统（或子系统）会暂时不可用。通常会计划在非工作时间进行，并且用户通常会提前收到通知。维护窗口的好处在于，您不必担心维护操作会如何与进入系统的实时请求发生冲突。它可以大大简化操作。系统管理员和运营人员喜欢维护窗口，就像开发人员喜欢最佳努力系统一样。
- en: The downside, of course, is that the system is down during maintenance. It may
    only be acceptable for systems where user activity is limited to certain times
    (e.g. US office hours or weekdays only).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，缺点是系统在维护期间会宕机。它可能仅适用于用户活动在特定时间内的系统（例如美国办公时间或仅限工作日）。
- en: With Kubernetes, you can do maintenance windows by redirecting all incoming
    requests via the load balancer to a web page (or JSON response) that notifies
    users about the maintenance window.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kubernetes，您可以通过将所有传入请求通过负载均衡器重定向到一个网页（或JSON响应）来通知用户维护窗口，从而实现维护窗口。
- en: But in most cases, the flexibility of Kubernetes should allow you to do live
    maintenance. In extreme cases, such as upgrading the Kubernetes version, or the
    switch from etcd v2 to etcd v3, you may want to resort to a maintenance window.
    Blue-green deployment is another alternative. But the larger the cluster, the
    more expansive the blue-green alternative because you must duplicate your entire
    production cluster, which is both costly and can cause you to run into problems
    like an insufficient quota.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 但在大多数情况下，Kubernetes的灵活性应该允许您进行在线维护。在极端情况下，例如升级Kubernetes版本，或者从etcd v2切换到etcd
    v3，您可能希望依赖维护窗口。蓝绿部署是另一种替代方案。但集群越大，蓝绿部署的替代方案就越庞大，因为您必须复制整个生产集群，这既昂贵又可能导致配额不足等问题。
- en: Quick recovery
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速恢复
- en: Quick recovery is another important aspect of highly available clusters. Something
    will go wrong at some point. Your unavailability clock starts running. How quickly
    can you get back to normal? **Mean time to recovery** (**MTTR**) is an important
    measure to track and ensure your system can deal adequately with disasters.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 快速恢复是高可用集群的另一个重要方面。总会有某个时刻出现问题。你的不可用时钟开始运行。你能多快恢复正常？**平均恢复时间**（**MTTR**）是一个重要的度量指标，用来跟踪并确保你的系统能够妥善应对灾难。
- en: Sometimes it’s not up to you. For example, if your cloud provider has an outage
    (and you didn’t implement a federated cluster, as we will discuss later in *Chapter
    11*, *Running Kubernetes on Multiple Clusters*), then you just have to sit and
    wait until they sort it out. But the most likely culprit is a problem with a recent
    deployment. There are, of course, time-related issues, and even calendar-related
    issues. Do you remember the leap-year bug that took down Microsoft Azure on February
    29, 2012?
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，这并不是由你决定的。例如，如果你的云服务提供商发生了故障（而你没有实现联合集群，正如我们在*第11章*《在多个集群上运行 Kubernetes》中将讨论的那样），那么你只能坐等他们解决问题。但最可能的罪魁祸首是最近的部署问题。当然，也存在与时间或日历相关的问题。你还记得
    2012 年 2 月 29 日导致微软 Azure 崩溃的闰年 bug 吗？
- en: The poster boy of quick recovery is, of course, the blue-green deployment–if
    you keep the previous version running when the problem is discovered. But, that’s
    usually good for problems that happen during deployment or shortly after. If a
    sneaky bug lays dormant and is discovered only hours after the deployment, then
    you will have torn down your blue deployment already and you will not be able
    to revert to it.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 快速恢复的典型代表当然是蓝绿部署——如果在发现问题时，你仍然保留着前一个版本的运行。但这通常适用于在部署过程中或刚部署后发生的问题。如果一个潜伏的 bug
    在部署后几个小时才被发现，那么你可能已经拆掉了蓝色部署，而无法恢复。
- en: On the other hand, rolling updates mean that if the problem is discovered early,
    then most of your pods still run the previous version.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，滚动更新意味着，如果问题早期被发现，那么大部分的 pods 仍会运行之前的版本。
- en: Data-related problems can take a long time to reverse, even if your backups
    are up to date and your restore procedure actually works (definitely test this
    regularly).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据相关的问题可能需要很长时间才能恢复，即使你的备份是最新的，而且恢复程序确实有效（一定要定期测试这一点）。
- en: Tools like Velero can help in some scenarios by creating a snapshot backup of
    your cluster that you can just restore, in case something goes wrong and you’re
    not sure how to fix it.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Velero 这样的工具可以帮助你在某些场景中通过创建集群的快照备份来恢复系统，以防出现问题而你不知道如何修复时。
- en: Zero downtime
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零停机
- en: Finally, we arrive at the zero-downtime system. There is no such thing as a
    system that truly has zero downtime. All systems fail and all software systems
    definitely fail. The reliability of a system is often measured in the “number
    of nines.” See [https://en.wikipedia.org/wiki/High_availability#%22Nines%22](https://en.wikipedia.org/wiki/High_availability#%22Nines%22).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来到了零停机系统。没有任何系统能够真正实现零停机。所有系统都会失败，所有软件系统也一定会失败。系统的可靠性通常通过“九个数”来衡量。请参阅[https://en.wikipedia.org/wiki/High_availability#%22Nines%22](https://en.wikipedia.org/wiki/High_availability#%22Nines%22)。
- en: Sometimes the failure is serious enough that the system or some of its services
    will be down. Think about zero downtime as a best-effort distributed system design.
    You design for zero downtime in the sense that you provide a lot of redundancy
    and mechanisms to address expected failures without bringing the system down.
    As always, remember that, even if there is a business case for zero downtime,
    it doesn’t mean that every component must be zero downtime. Reliable (within reason)
    systems can be constructed from highly unreliable components.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 有时故障严重到足以使系统或其中一些服务停机。将零停机视为一种最佳努力的分布式系统设计。你通过提供大量冗余和机制来应对预期的故障，避免系统停机，这样设计就是为了零停机。和往常一样，请记住，即使有零停机的商业需求，也并不意味着每个组件都必须实现零停机。可靠的（在合理范围内）系统可以由高度不可靠的组件构建而成。
- en: 'The plan for zero downtime is as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 零停机的计划如下：
- en: '**Redundancy at every level**: This is a required condition. You can’t have
    a single point of failure in your design because when it fails, your system is
    down.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个层级的冗余**：这是一个必要的条件。你不能在设计中有单点故障，因为当它失败时，整个系统会崩溃。'
- en: '**Automated hot-swapping of failed components**: Redundancy is only as good
    as the ability of the redundant components to kick into action as soon as the
    original component has failed. Some components can share the load (for example,
    stateless web servers), so there is no need for explicit action. In other cases,
    such as the Kubernetes scheduler and controller manager, you need a leader election
    in place to make sure the cluster keeps humming along.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动热交换故障组件**：冗余系统的有效性取决于冗余组件在原始组件故障后能否迅速投入使用。一些组件可以分担负载（例如，无状态的 Web 服务器），因此无需显式操作。其他情况下，如
    Kubernetes 调度器和控制器管理器，你需要进行领导者选举，以确保集群能够持续运行。'
- en: '**Tons of metrics, monitoring, and alerts to detect problems early**: Even
    with careful design, you may miss something or some implicit assumption might
    invalidate your design. Often, such subtle issues creep up on you and with enough
    attention, you may discover them before they become an all-out system failure.
    For example, suppose there is a mechanism in place to clean up old log files when
    disk space is over 90% full, but for some reason, it doesn’t work. If you set
    an alert for when disk space is over 95% full, then you’ll catch it and be able
    to prevent the system failure.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大量的指标、监控和警报来早期发现问题**：即使经过仔细设计，你也可能漏掉一些东西，或者某些隐性假设可能会使你的设计失效。通常，这些微妙的问题会悄悄出现，经过足够关注后，你可能会在它们变成系统崩溃之前发现它们。例如，假设有一个机制可以在磁盘空间超过
    90% 时清理旧的日志文件，但由于某种原因，它没有工作。如果你设置了一个磁盘空间超过 95% 时的警报，那么你就能及时发现并防止系统故障。'
- en: '**Tenacious testing before deployment to production**: Comprehensive tests
    have proven themselves as a reliable way to improve quality. It is hard work to
    have comprehensive tests for something as complicated as a large Kubernetes cluster
    running a massive distributed system, but you need it. What should you test? Everything.
    That’s right. For zero downtime, you need to test both the application and the
    infrastructure together. Your 100% passing unit tests are a good start, but they
    don’t provide much confidence that when you deploy your application on your production
    Kubernetes cluster, it will still run as expected. The best tests are, of course,
    on your production cluster after a blue-green deployment or identical cluster.
    In lieu of a full-fledged identical cluster, consider a staging environment with
    as much fidelity as possible to your production environment. Here is a list of
    tests you should run. Each of these tests should be comprehensive because if you
    leave something untested, it might be broken:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署到生产环境前的彻底测试**：全面的测试已被证明是提高质量的可靠方法。为复杂的大型 Kubernetes 集群和庞大的分布式系统进行全面测试是一项艰巨的工作，但你必须进行。你应该测试什么？所有的东西。没错。为了零停机，你需要同时测试应用程序和基础设施。你通过的
    100% 单元测试是一个不错的起点，但它们不能让你完全放心地知道，当你在生产 Kubernetes 集群上部署应用时，它仍然会按预期运行。当然，最好的测试是在你生产集群上，进行蓝绿部署或在相同集群上进行的测试。若无法提供一个完全相同的集群，可以考虑建立一个尽可能忠实于生产环境的预发布环境。以下是你应该运行的测试列表。每个测试都应当全面，因为如果你漏掉了某些测试，可能会导致系统故障：'
- en: Unit tests
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元测试
- en: Acceptance tests
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验收测试
- en: Performance tests
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能测试
- en: Stress tests
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压力测试
- en: Rollback tests
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回滚测试
- en: Data restore tests
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据恢复测试
- en: Penetration tests
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 渗透测试
- en: '**Keep the raw data**: For many systems, the data is the most critical asset.
    If you keep the raw data, you can recover from any data corruption and processed
    data loss that happens later. This will not really help you with zero downtime
    because it can take a while to reprocess the raw data, but it will help with zero-data
    loss, which is often more important. The downside to this approach is that the
    raw data is often huge compared to the processed data. A good option may be to
    store the raw data in cheaper storage compared to the processed data.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保留原始数据**：对于许多系统来说，数据是最关键的资产。如果你保留原始数据，就可以在数据损坏或后期处理数据丢失的情况下恢复。这对于实现零停机没有直接帮助，因为重新处理原始数据可能需要一些时间，但它有助于实现零数据丢失，这通常更为重要。这个方法的缺点是原始数据通常比处理后的数据要大得多。一个好的选择是将原始数据存储在比处理数据更便宜的存储介质上。'
- en: '**Perceived uptime as a last resort**: OK. Some part of the system is down.
    You may still be able to maintain some level of service. In many situations, you
    may have access to a slightly stale version of the data or can let the user access
    some other part of the system. It is not a great user experience, but technically
    the system is still available.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将感知的正常运行时间作为最后的手段**：好的，系统的某个部分可能已经宕机。你仍然可能能够维持某种程度的服务。在许多情况下，你可能能访问稍微过时的数据，或者允许用户访问系统的其他部分。这不是一个理想的用户体验，但从技术角度来看，系统仍然是可用的。'
- en: Does that sound crazy? Good. Zero-downtime large-scale systems are hard (actually
    impossible). There is a reason why Microsoft, Google, Amazon, Facebook, and other
    big companies have tens of thousands of software engineers (combined) just working
    on infrastructure, operations, and making sure things are up and running.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来疯狂吗？很好。零停机的大规模系统确实很难（实际上是不可能的）。微软、谷歌、亚马逊、Facebook 和其他大公司有成千上万的工程师（合计）专门从事基础设施、运营，并确保系统正常运行，这背后是有原因的。
- en: Site reliability engineering
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网站可靠性工程
- en: SRE is a real-world approach for operating reliable distributed systems. SRE
    embraces failures and works with **service-level indicators** (**SLIs**), **service-level
    objectives** (**SLOs**), and **service-level agreements** (**SLAs**). Each service
    has objectives such as latency below 50 milliseconds for 95% of requests. If a
    service violates its objectives, then the team focuses on fixing the issue before
    going back to work on new features and capabilities.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: SRE 是一种在现实世界中操作可靠分布式系统的方法。SRE 接纳故障，并与**服务水平指标**（**SLIs**）、**服务水平目标**（**SLOs**）和**服务水平协议**（**SLAs**）配合使用。每个服务都有目标，比如
    95% 的请求延迟低于 50 毫秒。如果某个服务未能达到其目标，那么团队会集中精力修复问题，然后再回到新功能和能力的开发工作中。
- en: The beauty of SRE is that you get to play with the knobs for cost and performance.
    If you want to invest more in reliability, then be ready to pay for it in resources
    and development time.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: SRE 的魅力在于你可以调整成本和性能的“旋钮”。如果你想在可靠性上投入更多，那么要准备好为此支付更多的资源和开发时间。
- en: Performance and data consistency
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能和数据一致性
- en: 'When you develop or operate distributed systems, the CAP theorem should always
    be in the back of your mind. CAP stands for consistency, availability, and partition
    tolerance:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开发或运营分布式系统时，CAP 定理应该始终在你脑海中。CAP 代表一致性、可用性和分区容错：
- en: Consistency means that every read receives the most recent write or an error
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一致性意味着每次读取都能得到最新的写入结果或错误
- en: Availability means that every request receives a non-error response (but the
    response may be stale)
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用性意味着每个请求都会收到一个非错误响应（但响应可能是陈旧的）
- en: Partition tolerance means the system continues to operate even when an arbitrary
    number of messages between nodes are dropped or delayed by the network
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区容错意味着即使节点之间的任意数量的消息被丢失或延迟，系统仍然能够继续运行
- en: The theorem says that you can have at most two out of the three. Since any distributed
    system can suffer from a network partition, in practice you can choose between
    CP or AP. CP means that in order to remain consistent, the system will not be
    available in the event of a network partition. AP means that the system will always
    be available but might not be consistent. For example, reads from different partitions
    might return different results because one of the partitions didn’t receive a
    write.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 该定理表明，你最多只能在三者中选择两个。由于任何分布式系统都可能遭遇网络分区，因此在实际操作中，你可以选择 CP 或 AP。CP 意味着为了保持一致性，系统在发生网络分区时将无法提供服务。AP
    意味着系统将始终保持可用，但可能并不一致。例如，来自不同分区的读取可能返回不同的结果，因为某个分区没有收到写入操作。
- en: In this section, we have focused on highly available systems, which means AP.
    To achieve high availability, we must sacrifice consistency. But that doesn’t
    mean that our system will have corrupt or arbitrary data. The key concept is eventual
    consistency. Our system may be a little bit behind and provide access to somewhat
    stale data, but eventually, you’ll get what you expect.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们重点讨论了高可用系统，即 AP。为了实现高可用性，我们必须牺牲一致性。但这并不意味着我们的系统会出现损坏或任意的数据。关键概念是最终一致性。我们的系统可能会稍微滞后，提供一些过时的数据，但最终你会得到预期的结果。
- en: When you start thinking in terms of eventual consistency, it opens the door
    to potentially significant performance improvements. For example, if some important
    value is updated frequently (let’s say, every second), but you send its value
    only every minute, you have reduced your network traffic by a factor of 60 and
    you’re on average only 30 seconds behind real-time updates. This is very significant.
    This is huge. You have just scaled your system to handle 60 times more users or
    requests with the same amount of resources.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始从最终一致性的角度思考时，它为潜在的显著性能提升打开了大门。例如，如果某个重要的值频繁更新（假设每秒一次），但是你只每分钟发送一次其值，那么你将网络流量减少了
    60 倍，并且你平均只有 30 秒的延迟。这是非常显著的，这意味着你可以在相同的资源下，让你的系统处理更多的用户或请求——最多能应付 60 倍的负载。
- en: As we discussed earlier, redundancy is key to highly available systems. However,
    there is tension between redundancy and cost. In the next section, we will discuss
    choosing and managing your cluster capacity.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，冗余是高可用系统的关键。然而，冗余和成本之间存在张力。在下一节中，我们将讨论如何选择和管理集群容量。
- en: Choosing and managing the cluster capacity
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择和管理集群容量
- en: With Kubernetes’ horizontal pod autoscaling, DaemonSets, StatefulSets, and quotas,
    we can scale and control our pods, storage, and other objects. However, in the
    end, we’re limited by the physical (virtual) resources available to our Kubernetes
    cluster. If all your nodes are running at 100% capacity, you need to add more
    nodes to your cluster. There is no way around it. Kubernetes will just fail to
    scale. On the other hand, if you have very dynamic workloads, then Kubernetes
    can scale down your pods, but if you don’t scale down your nodes correspondingly,
    you will still pay for the excess capacity. In the cloud, you can stop and start
    instances on demand. Combining it with the cluster autoscaler can solve the compute
    capacity problem automatically. That’s the theory. In practice, there are always
    nuances.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Kubernetes 的水平 pod 自动扩展、DaemonSets、StatefulSets 和配额，我们可以扩展和控制我们的 pods、存储和其他对象。然而，最终我们受到
    Kubernetes 集群中可用的物理（虚拟）资源的限制。如果所有节点都在 100% 的容量下运行，你需要为集群添加更多的节点。没有捷径可走，Kubernetes
    将无法扩展。另一方面，如果你有非常动态的工作负载，Kubernetes 可以缩小你的 pods，但如果你没有相应地缩小节点，你仍然会为多余的容量付费。在云中，你可以按需停止和启动实例。将它与集群自动扩展器结合使用，可以自动解决计算容量问题。这是理论上的解决方案，但在实践中，总是会有一些细微差别。
- en: Choosing your node types
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择节点类型
- en: The simplest solution is to choose a single node type with a known quantity
    of CPU, memory, and local storage. But that is typically not the most efficient
    and cost-effective solution. It makes capacity planning simple because the only
    question is how many nodes are needed. Whenever you add a node, you add a known
    quantity of CPU and memory to your cluster, but most Kubernetes clusters and components
    within the cluster handle different workloads. We may have a stream processing
    pipeline where many pods receive some data and process it in one place.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的解决方案是选择单一节点类型，该节点类型具有已知的 CPU、内存和本地存储量。但这通常不是最有效且最具成本效益的解决方案。它使得容量规划变得简单，因为唯一的问题是需要多少个节点。每当你添加一个节点时，你就会为集群添加已知数量的
    CPU 和内存，但大多数 Kubernetes 集群及其内部组件处理的是不同的工作负载。我们可能有一个流处理管道，其中许多 pods 接收一些数据并在一个地方进行处理。
- en: This workload is CPU-heavy and may or may not need a lot of memory. Some components,
    such as a distributed memory cache, need a lot of memory, but very little CPU.
    Other components, such as a Cassandra cluster, need multiple SSD disks attached
    to each node. Machine learning workloads can benefit from GPUs. In addition, cloud
    providers offer spot instances – nodes that are cheaper but may be snatched away
    from you if another customer is willing to pay the regular price.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这种工作负载是 CPU 密集型的，可能需要大量内存，也可能不需要。一些组件，如分布式内存缓存，可能需要大量内存，但 CPU 占用非常少。其他组件，如 Cassandra
    集群，需要每个节点附加多个 SSD 硬盘。机器学习工作负载可能会受益于 GPU。此外，云服务提供商提供了抢占实例——这些节点价格更低，但如果其他客户愿意支付正常价格，它们可能会被抢走。
- en: At scale, costs start to add up and you should try to align your workloads with
    the configuration of the nodes they run on, which means multiple node pools with
    different node (instance) types.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模部署中，成本开始累积，你应该尝试将工作负载与节点配置对齐，这意味着使用多个具有不同节点（实例）类型的节点池。
- en: For each type of node, you should assign proper labels and make sure that Kubernetes
    schedules the pods that are designed to run on that node type.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每种类型的节点，您应分配适当的标签，并确保 Kubernetes 将设计在该节点类型上运行的 pods 调度到该节点。
- en: Choosing your storage solutions
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择你的存储解决方案
- en: 'Storage is a huge factor in scaling a cluster. There are three categories of
    scalable storage solutions:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在扩展集群时是一个重要因素。有三种可扩展存储解决方案：
- en: Roll your own
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自己动手
- en: Use your cloud platform storage solution
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用你的云平台存储解决方案
- en: Use an out-of-cluster solution
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集群外的解决方案
- en: When you roll your own, you install some type of storage solution in your Kubernetes
    cluster. The benefits are flexibility and full control, but you have to manage
    and scale it yourself.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当你自己动手时，你会在你的Kubernetes集群中安装某种类型的存储解决方案。它的好处是灵活性和完全控制，但你必须自己管理和扩展它。
- en: When you use your cloud platform storage solution, you get a lot out of the
    box, but you lose control, you typically pay more, and, depending on the service,
    you may be locked into that provider.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用云平台存储解决方案时，你可以获得很多现成的功能，但你会失去控制权，通常会支付更多费用，并且根据服务的不同，你可能会被锁定在那个提供商那里。
- en: When you use an out-of-cluster solution, the performance and cost of data transfer
    may be much greater. You typically use this option if you need to integrate with
    an existing system.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用集群外的解决方案时，数据传输的性能和成本可能会大大增加。如果你需要与现有系统集成，通常会使用这个选项。
- en: Of course, large clusters may have multiple data stores from all categories.
    This is one of the most critical decisions you have to make, and your storage
    needs may change and evolve over time.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，大型集群可能会有来自所有类别的多个数据存储。这是你必须做出的最关键的决策之一，而且你的存储需求可能会随着时间的推移而发生变化和演变。
- en: Trading off cost and response time
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权衡成本和响应时间
- en: If money was not an issue, you could just over-provision your cluster. Every
    node would have the best hardware configuration available, you would have way
    more nodes than are needed to process your workloads, and you would have copious
    amounts of available storage. But guess what? Money is always an issue!
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果钱不是问题，你可以直接为你的集群进行过度配置。每个节点都会拥有最好的硬件配置，你会拥有比处理工作负载所需更多的节点，并且有大量可用存储。但猜猜看？钱永远是个问题！
- en: You may get by with over-provisioning when you’re just starting and your cluster
    doesn’t handle a lot of traffic. You may just run five nodes, even if two nodes
    are enough most of the time. However, multiply everything by 1,000, and someone
    from the finance department will come asking questions if you have thousands of
    idle machines and petabytes of empty storage.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当你刚开始时，集群的流量不大，你可能会通过过度配置来应对。你可能只运行五个节点，即使大部分时间两个节点就足够了。然而，将一切都乘以1,000，财务部门的人就会来问你，为什么你有成千上万台闲置的机器和几PB的空闲存储。
- en: OK. So, you measure and optimize carefully and you get 99.99999% utilization
    of every resource. Congratulations, you just created a system that can’t handle
    an iota of extra load or the failure of a single node without dropping requests
    on the floor or delaying responses.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧。所以，你仔细地衡量并优化，每个资源的利用率都达到了99.99999%。恭喜你，你刚刚创建了一个无法处理任何额外负载或单个节点故障的系统，否则请求就会丢失或响应会延迟。
- en: You need to find the middle ground. Understand the typical fluctuations of your
    workloads and consider the cost/benefit ratio of having excess capacity versus
    having reduced response time or processing ability.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要找到一个折衷方案。了解你的工作负载的典型波动，并考虑拥有过剩容量与减少响应时间或处理能力的成本/收益比。
- en: Sometimes, if you have strict availability and reliability requirements, you
    can build redundancy into the system, and then you over-provision by design. For
    example, you want to be able to hot-swap a failed component with no downtime and
    no noticeable effects. Maybe you can’t lose even a single transaction. In this
    case, you’ll have a live backup for all critical components, and that extra capacity
    can be used to mitigate temporary fluctuations without any special actions.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，如果你有严格的可用性和可靠性要求，你可以在系统中建立冗余，然后通过设计过度配置。例如，你希望能够在没有停机时间和没有明显影响的情况下热插拔一个故障的组件。也许你甚至不能失去一个交易。在这种情况下，你会为所有关键组件准备一个实时备份，这额外的容量可以用来在没有特殊操作的情况下缓解临时波动。
- en: Using multiple node configurations effectively
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有效使用多节点配置
- en: Effective capacity planning requires you to understand the usage patterns of
    your system and the load each component can handle. That may include a lot of
    data streams generated inside the system. When you have a solid understanding
    of the typical workloads via metrics, you can look at workflows and which components
    handle which parts of the load. Then you can compute the number of pods and their
    resource requirements. In my experience, there are some relatively fixed workloads,
    some workloads that vary predictably (such as office hours versus non-office hours),
    and then you have your completely crazy workloads that behave erratically. You
    have to plan accordingly for each workload, and you can design several families
    of node configurations that can be used to schedule pods that match a particular
    workload.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的容量规划要求你理解系统的使用模式和每个组件可以承受的负载。这可能包括系统内部生成的大量数据流。当你通过指标对典型工作负载有了深入了解后，可以查看工作流以及哪些组件处理哪些部分的负载。然后你可以计算Pod的数量及其资源需求。根据我的经验，有些工作负载相对固定，有些工作负载是可以预测变化的（如办公时间与非办公时间的区别），而有些则是完全不可预测的疯狂工作负载。你必须根据每种工作负载进行规划，并设计几种节点配置的方案，以便调度适应特定工作负载的Pod。
- en: Benefiting from elastic cloud resources
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 受益于弹性云资源
- en: Most cloud providers let you scale instances automatically, which is a perfect
    complement to Kubernetes’ horizontal pod autoscaling. If you use cloud storage,
    it also grows magically without you having to do anything. However, there are
    some gotchas that you need to be aware of.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数云服务提供商允许你自动扩展实例，这与Kubernetes的水平Pod自动扩展是完美的互补。如果你使用云存储，它也会在你什么都不做的情况下神奇地增长。然而，你需要注意一些潜在的问题。
- en: Autoscaling instances
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动扩展实例
- en: All the big cloud providers have instance autoscaling in place. There are some
    differences, but scaling up and down based on CPU utilization is always available,
    and sometimes, custom metrics are available too. Sometimes, load balancing is
    offered as well. As you can see, there is some overlap with Kubernetes here.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 所有大型云服务提供商都已提供实例自动扩展功能。虽然有些细节差异，但基于CPU利用率的扩展总是可用的，有时也提供自定义指标。有时，还会提供负载均衡。正如你所看到的，这里与Kubernetes有一些重叠。
- en: 'If your cloud provider doesn’t have adequate autoscaling with proper control
    and is not supported by the cluster autoscaler, it is relatively easy to roll
    your own, where you monitor your cluster resource usage and invoke cloud APIs
    to add or remove instances. You can extract the metrics from Kubernetes. Here
    is a diagram that shows how two new instances are added based on a CPU load monitor:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的云服务提供商没有提供足够的自动扩展控制，并且不支持集群自动扩展器，那么相对容易实现自定义扩展，你可以监控集群的资源使用情况，并调用云API来添加或移除实例。你可以从Kubernetes中提取指标。下面是一个示意图，展示了如何基于CPU负载监控来添加两个新实例：
- en: '![Figure 3.3: etcd used as an external cluster](img/B18998_03_03.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3：etcd 用作外部集群](img/B18998_03_03.png)'
- en: 'Figure 3.3: CPU-based autoscaling'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：基于CPU的自动扩展
- en: Mind your cloud quotas
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 留意你的云配额
- en: When working with cloud providers, some of the most annoying things are quotas.
    I’ve worked with four different cloud providers (AWS, GCP, Azure, and Alibaba
    Cloud) and I was always bitten by quotas at some point. The quotas exist to let
    the cloud providers do their own capacity planning (and also to protect you from
    inadvertently starting 1,000,000 instances that you won’t be able to pay for),
    but from your point of view, it is yet one more thing that can trip you up. Imagine
    that you set up a beautiful autoscaling system that works like magic, and suddenly
    the system doesn’t scale when you hit 100 nodes. You quickly discover that you
    are limited to 100 nodes and you open a support request to increase the quota.
    However, a human must approve quota requests, and that can take a day or two.
    In the meantime, your system is unable to handle the load.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在与云服务提供商合作时，最令人烦恼的事情之一就是配额。我曾与四个不同的云服务提供商（AWS、GCP、Azure 和阿里云）合作过，且在某些时刻都曾被配额所困扰。配额存在的目的是让云服务提供商进行自己的容量规划（同时保护你免于无意中启动100万个你无法支付的实例），但从你的角度来看，它是一个可能绊倒你的因素。试想，你设置了一个像魔法一样运作的完美自动扩展系统，结果当节点数达到100时，系统突然无法扩展。你很快发现自己被限制为100个节点，于是你提交了支持请求以增加配额。然而，配额请求必须由人工审批，这可能需要一两天时间。在此期间，你的系统无法处理负载。
- en: Manage regions carefully
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小心管理区域
- en: Cloud platforms are organized in regions and availability zones. The cost difference
    between regions can be up to 20% on cloud providers like GCP and Azure. On AWS,
    it may be even more extreme (30%-70%). Some services and machine configurations
    are available only in some regions.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 云平台按照区域和可用区进行组织。在像GCP和Azure这样的云服务提供商中，区域之间的成本差异可以高达20%。在AWS上，可能会更为极端（30%-70%）。一些服务和机器配置仅在某些区域提供。
- en: Cloud quotas are also managed at the regional level. Performance and cost of
    data transfers within regions are much lower (often free) than across regions.
    When planning your clusters, you should carefully consider your geo-distribution
    strategy. If you need to run your workloads across multiple regions, you may have
    some tough decisions to make regarding redundancy, availability, performance,
    and cost.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 云配额也在区域级别进行管理。区域内的数据传输的性能和成本要比跨区域的低得多（通常是免费的）。在规划集群时，您应仔细考虑您的地理分布策略。如果您需要在多个区域运行工作负载，您可能需要就冗余性、可用性、性能和成本做出一些艰难的决策。
- en: Considering container-native solutions
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 考虑容器原生解决方案
- en: A container-native solution is when your cloud provider offers a way to deploy
    containers directly into their infrastructure. You don’t need to provision instances
    and then install a container runtime (like the Docker daemon) and only then deploy
    your containers. Instead, you just provide your containers and the platform is
    responsible for finding a machine to run your container. You are totally separated
    from the actual machines your containers are running on.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 容器原生解决方案是指云服务提供商提供了一种直接将容器部署到其基础设施中的方式。您无需先配置实例，然后安装容器运行时（如Docker守护进程），再部署容器。相反，您只需提供容器，平台负责寻找运行容器的机器。您与容器运行的实际机器完全隔离。
- en: 'All the major cloud providers now provide solutions that abstract instances
    completely:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 所有主要的云服务提供商现在都提供完全抽象化的实例解决方案：
- en: AWS Fargate
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Fargate
- en: Azure Container Instances
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Container Instances
- en: Google Cloud Run
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud Run
- en: These solutions are not Kubernetes-specific, but they can work great with Kubernetes.
    The cloud providers already provide a managed Kubernetes control plane with Google’s
    **Google Kubernetes Engine** (**GKE**), Microsoft’s **Azure Kubernetes Service**
    (**AKS**), and Amazon Web Services’ **Elastic Kubernetes Service** (**EKS**).
    But managing the data plane (the nodes) was left to the cluster administrator.
    The container-native solution allows the cloud provider to do that on your behalf.
    Google Run for GKE, AKS with ACI, and AWS EKS with Fargate can manage both the
    control plane and the data plane.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这些解决方案并非专门针对Kubernetes，但它们可以与Kubernetes协同工作。云服务提供商已经提供了托管的Kubernetes控制平面，如Google的**Google
    Kubernetes Engine**（**GKE**）、Microsoft的**Azure Kubernetes Service**（**AKS**）和Amazon
    Web Services的**Elastic Kubernetes Service**（**EKS**）。但是，数据平面（即节点）仍然由集群管理员管理。容器原生解决方案使云服务提供商可以代为管理这些工作。Google
    Run for GKE、AKS与ACI以及AWS EKS与Fargate可以管理控制平面和数据平面。
- en: For example, in AKS, you can provision virtual nodes. A virtual node is not
    backed up by an actual VM. Instead, it utilizes ACI to deploy containers when
    necessary. You pay for it only when the cluster needs to scale beyond the capacity
    of the regular nodes. It is faster to scale than using the cluster autoscaler
    that needs to provision an actual VM-backed node.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在AKS中，您可以配置虚拟节点。虚拟节点并不是由实际的虚拟机支持的。相反，它利用ACI在必要时部署容器。只有当集群需要超出常规节点的容量时，您才需要为此付费。与需要配置实际虚拟机支持的节点的集群自动扩展器相比，虚拟节点的扩展速度更快。
- en: 'The following diagram illustrates this burst to the ACI approach:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了这一突发到ACI的方式：
- en: '![Figure 3.4: AKS and ACI](img/B18998_03_04.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4：AKS 和 ACI](img/B18998_03_04.png)'
- en: 'Figure 3.4: AKS and ACI'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4：AKS 和 ACI
- en: In this section, we looked at various factors that impact your decision regarding
    cluster capacity as well as cloud-provider solutions that do the heavy lifting
    on your behalf. In the next section, we will see how far you can stress a single
    Kubernetes cluster.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了影响您关于集群容量决策的各种因素，以及云服务提供商为您完成繁重工作的解决方案。在接下来的章节中，我们将看看单一Kubernetes集群的负载极限。
- en: Pushing the envelope with Kubernetes
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推动Kubernetes的极限
- en: 'In this section, we will see how the Kubernetes team pushes Kubernetes to its
    limit. The numbers are quite telling, but some of the tools and techniques, such
    as Kubemark, are ingenious, and you may even use them to test your clusters. Kubernetes
    is designed to support clusters with the following properties:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到 Kubernetes 团队如何将 Kubernetes 推向极限。这些数字非常具有参考价值，但一些工具和技术，如 Kubemark，极为巧妙，你甚至可以用它们来测试你的集群。Kubernetes
    设计支持具有以下特性的集群：
- en: Up to 110 pods per node
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节点最多支持 110 个 Pods
- en: Up to 5,000 nodes
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最多 5,000 个节点
- en: Up to 150,000 pods
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节点最多支持 150,000 个 Pods
- en: Up to 300,000 total containers
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最多 300,000 个总容器
- en: Those numbers are just guidelines and not hard limits. Clusters that host specialized
    workloads with specialized deployment and runtime patterns regarding new pods
    coming and going may support very different numbers.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字只是指导性建议，并不是硬性限制。承载专门工作负载的集群，其部署和运行时模式可能支持不同数量的新 Pods 的进出。
- en: 'At CERN, the OpenStack team achieved 2 million requests per second: [http://superuser.openstack.org/articles/scaling-magnum-and-kubernetes-2-million-requests-per-second](http://superuser.openstack.org/articles/scaling-magnum-and-kubernetes-2-million-requests-per-second).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CERN，OpenStack 团队实现了每秒 200 万个请求：[http://superuser.openstack.org/articles/scaling-magnum-and-kubernetes-2-million-requests-per-second](http://superuser.openstack.org/articles/scaling-magnum-and-kubernetes-2-million-requests-per-second)。
- en: Mirantis conducted a performance and scaling test in their scaling lab where
    they deployed 5,000 Kubernetes nodes (in VMs) on 500 physical servers.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Mirantis 在其扩展实验室中进行了一次性能和扩展性测试，他们在 500 台物理服务器上部署了 5,000 个 Kubernetes 节点（在虚拟机中）。
- en: 'OpenAI scaled their machine learning Kubernetes cluster to 2,500 nodes on Azure
    and learned some valuable lessons such as minding the query load of logging agents
    and storing events in a separate etcd cluster: [https://openai.com/research/scaling-kubernetes-to-2500-nodes](https://openai.com/research/scaling-kubernetes-to-2500-nodes).'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 将其机器学习 Kubernetes 集群扩展到 Azure 上的 2,500 个节点，并从中获得了一些宝贵的经验教训，例如注意日志代理的查询负载并将事件存储在单独的
    etcd 集群中：[https://openai.com/research/scaling-kubernetes-to-2500-nodes](https://openai.com/research/scaling-kubernetes-to-2500-nodes)。
- en: 'There are many more interesting use cases here: [https://www.cncf.io/projects/case-studies](https://www.cncf.io/projects/case-studies).'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有许多有趣的使用案例：[https://www.cncf.io/projects/case-studies](https://www.cncf.io/projects/case-studies)。
- en: By the end of this section, you’ll appreciate the effort and creativeness that
    goes into improving Kubernetes on a large scale, you will know how far you can
    push a single Kubernetes cluster and what performance to expect, and you’ll get
    an inside look at some tools and techniques that can help you evaluate the performance
    of your own Kubernetes clusters.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 本节结束时，你将欣赏到为了在大规模上改进 Kubernetes 所付出的努力和创造力，你将了解单个 Kubernetes 集群能够达到的极限以及预期的性能，并且你将深入了解一些工具和技术，帮助你评估自己
    Kubernetes 集群的性能。
- en: Improving the performance and scalability of Kubernetes
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升 Kubernetes 性能和可扩展性
- en: The Kubernetes team focused heavily on performance and scalability in Kubernetes
    1.6\. When Kubernetes 1.2 was released, it supported clusters of up to 1,000 nodes
    within the Kubernetes service-level objectives. Kubernetes 1.3 doubled the number
    to 2,000 nodes, and Kubernetes 1.6 brought it to a staggering 5,000 nodes per
    cluster. 5,000 nodes can carry you very far, especially if you use large nodes.
    But, when you run large nodes, you need to pay attention to pods per node guideline
    too. Note that cloud providers still recommend up to 1,000 nodes per cluster.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 团队在 Kubernetes 1.6 中非常重视性能和可扩展性。当 Kubernetes 1.2 发布时，它支持 Kubernetes
    服务级目标内最多 1,000 个节点的集群。Kubernetes 1.3 将这一数字增加到 2,000 个节点，而 Kubernetes 1.6 将其提升到惊人的
    5,000 个节点每个集群。5,000 个节点可以支持非常大的规模，尤其是当你使用大型节点时。但是，当你使用大型节点时，也需要注意每个节点的 Pods 数量指南。需要注意的是，云服务商仍然推荐每个集群最多支持
    1,000 个节点。
- en: We will get into the numbers later, but first, let’s look under the hood and
    see how Kubernetes achieved these impressive improvements.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会讨论这些数字，但首先，让我们深入了解 Kubernetes 是如何实现这些令人印象深刻的改进的。
- en: Caching reads in the API server
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: API 服务器中的读取缓存
- en: Kubernetes keeps the state of the system in etcd, which is very reliable, though
    not super-fast (although etcd 3 delivered massive improvement specifically to
    enable larger Kubernetes clusters). The various Kubernetes components operate
    on snapshots of that state and don’t rely on real-time updates. That fact allows
    the trading of some latency for throughput. All the snapshots used to be updated
    by etcd watches. Now, the API server has an in-memory read cache that is used
    for updating state snapshots. The in-memory read cache is updated by etcd watches.
    These schemes significantly reduce the load on etcd and increase the overall throughput
    of the API server.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 将系统的状态保存在 etcd 中，etcd 非常可靠，尽管速度并不是特别快（尽管 etcd 3 在特定方面提供了巨大的改进，特别是为了支持更大的
    Kubernetes 集群）。Kubernetes 的各个组件基于这些状态的快照进行操作，而不依赖于实时更新。这个事实使得可以通过牺牲一些延迟来换取吞吐量。所有的快照以前是通过
    etcd 观察进行更新的。现在，API 服务器有一个内存中的读取缓存，用于更新状态快照。这个内存读取缓存是通过 etcd 观察更新的。这些方案显著减少了 etcd
    的负载，并提高了 API 服务器的整体吞吐量。
- en: The pod lifecycle event generator
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pod 生命周期事件生成器
- en: Increasing the number of nodes in a cluster is key for horizontal scalability,
    but pod density is crucial too. Pod density is the number of pods that the kubelet
    can manage efficiently on one node. If pod density is low, then you can’t run
    too many pods on one node. That means that you might not benefit from more powerful
    nodes (more CPU and memory per node) because the kubelet will not be able to manage
    more pods. The other alternative is to force the developers to compromise their
    design and create coarse-grained pods that do more work per pod. Ideally, Kubernetes
    should not force your hand when it comes to pod granularity. The Kubernetes team
    understands this very well and invested a lot of work in improving pod density.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 增加集群中的节点数量是水平扩展的关键，但 Pod 密度也至关重要。Pod 密度是 kubelet 在一个节点上可以高效管理的 Pod 数量。如果 Pod
    密度较低，那么你就无法在一个节点上运行太多 Pod。这意味着你可能无法从更强大的节点（每个节点的 CPU 和内存更多）中获益，因为 kubelet 无法管理更多的
    Pod。另一种选择是迫使开发者妥协设计，创建粒度较大的 Pod，每个 Pod 执行更多的工作。理想情况下，Kubernetes 不应强迫你在 Pod 粒度上做出妥协。Kubernetes
    团队对此有着深刻的理解，并投入了大量工作来提高 Pod 密度。
- en: In Kubernetes 1.1, the official (tested and advertised) number was 30 pods per
    node. I actually ran 40 pods per node on Kubernetes 1.1, but I paid for it in
    excessive kubelet overhead that stole CPU from the worker pods. In Kubernetes
    1.2, the number jumped to 100 pods per node. The kubelet used to poll the container
    runtime constantly for each pod in its own goroutine. That put a lot of pressure
    on the container runtime that, during peaks to performance, has reliability issues,
    in particular CPU utilization. The solution was the **Pod Lifecycle Event Generator**
    (**PLEG**). The way the PLEG works is that it lists the state of all the pods
    and containers and compares it to the previous state. This is done once for all
    the pods and containers. Then, by comparing the state to the previous state, the
    PLEG knows which pods need to sync again and invokes only those pods. That change
    resulted in a significant four-times-lower CPU usage by the kubelet and the container
    runtime. It also reduced the polling period, which improves responsiveness.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 1.1 中，官方（经过测试和宣传的）每个节点支持的 Pod 数量为 30 个。实际上，我在 Kubernetes 1.1 上运行了每个节点
    40 个 Pod，但我为此付出了过多的 kubelet 开销，这些开销抢占了 worker pod 的 CPU。在 Kubernetes 1.2 中，这个数字跃升至每个节点
    100 个 Pod。以前，kubelet 会在每个 Pod 的自己的 goroutine 中持续轮询容器运行时。这给容器运行时带来了很大的压力，尤其是在性能峰值时，容器运行时会出现可靠性问题，特别是
    CPU 利用率方面。解决方案是 **Pod 生命周期事件生成器**（**PLEG**）。PLEG 的工作方式是，它列出所有 Pod 和容器的状态，并将其与先前的状态进行比较。这是针对所有
    Pod 和容器进行一次性操作。然后，通过比较当前状态与先前的状态，PLEG 能够知道哪些 Pod 需要再次同步，并仅调用这些 Pod。这个变化导致 kubelet
    和容器运行时的 CPU 使用率降低了四倍。它还减少了轮询周期，从而提高了响应能力。
- en: 'The following diagram shows the CPU utilization for 120 pods on Kubernetes
    1.1 versus Kubernetes 1.2\. You can see the 4X factor very clearly:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了在 Kubernetes 1.1 和 Kubernetes 1.2 下，120 个 Pod 的 CPU 利用率对比。你可以非常清晰地看到
    4 倍的差异：
- en: '![Figure 3.5: CPU utilization for 120 pods with Kube 1.1 and Kube 1.2](img/B18998_03_05.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5：Kube 1.1 和 Kube 1.2 下 120 个 Pod 的 CPU 利用率](img/B18998_03_05.png)'
- en: 'Figure 3.5: CPU utilization for 120 pods with Kube 1.1 and Kube 1.2'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5：Kube 1.1 和 Kube 1.2 下 120 个 Pod 的 CPU 利用率
- en: Serializing API objects with protocol buffers
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用协议缓冲区序列化 API 对象
- en: The API server has a REST API. REST APIs typically use JSON as their serialization
    format, and the Kubernetes API server was no different. However, JSON serialization
    implies marshaling and unmarshaling JSON to native data structures. This is an
    expensive operation. In a large-scale Kubernetes cluster, a lot of components
    need to query or update the API server frequently. The cost of all that JSON parsing
    and composition adds up quickly. In Kubernetes 1.3, the Kubernetes team added
    an efficient protocol buffers serialization format. The JSON format is still there,
    but all internal communication between Kubernetes components uses the protocol
    buffers serialization format.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: API 服务器提供了一个 REST API。REST API 通常使用 JSON 作为其序列化格式，Kubernetes API 服务器也不例外。然而，JSON
    序列化意味着需要将 JSON 序列化和反序列化为本地数据结构。这是一个开销较大的操作。在大规模的 Kubernetes 集群中，很多组件需要频繁地查询或更新
    API 服务器。所有这些 JSON 解析和组装的成本迅速累积。在 Kubernetes 1.3 中，Kubernetes 团队增加了一种高效的协议缓冲区序列化格式。JSON
    格式依然存在，但 Kubernetes 各组件之间的所有内部通信都使用协议缓冲区序列化格式。
- en: etcd3
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: etcd3
- en: Kubernetes switched from etcd2 to etcd3 in Kubernetes 1.6\. This was a big deal.
    Scaling Kubernetes to 5,000 nodes wasn’t possible due to limitations of etcd2,
    especially related to the watch implementation. The scalability needs of Kubernetes
    drove many of the improvements of etcd3, as CoreOS used Kubernetes as a measuring
    stick. Some of the big ticket items are talked about here.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 在 1.6 版本中将 etcd2 替换为 etcd3。这是一个重大变化。由于 etcd2 的限制，尤其是在 watch 实现方面，Kubernetes
    扩展到 5000 个节点变得不可行。Kubernetes 的可扩展性需求推动了 etcd3 的许多改进，因为 CoreOS 使用 Kubernetes 作为衡量标准。这里讨论了一些重要的改进内容。
- en: gRPC instead of REST
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 gRPC 代替 REST
- en: etcd2 has a REST API, and etcd3 has a gRPC API (and a REST API via gRPC gateway).
    The HTTP/2 protocol at the base of gRPC can use a single TCP connection for multiple
    streams of requests and responses.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: etcd2 提供了一个 REST API，而 etcd3 提供了一个 gRPC API（并通过 gRPC 网关提供 REST API）。gRPC 底层的
    HTTP/2 协议可以使用单个 TCP 连接处理多个请求和响应流。
- en: Leases instead of TTLs
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用租约（Leases）代替 TTL
- en: etcd2 uses **Time to Live** (**TTL**) per key as the mechanism to expire keys,
    while etcd3 uses leases with TTLs where multiple keys can share the same key.
    This significantly reduces keep-alive traffic.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: etcd2 使用**生存时间**（**TTL**）作为每个键的过期机制，而 etcd3 使用带 TTL 的租约（Leases），多个键可以共享同一个租约。这大大减少了保持连接的流量。
- en: Watch implementation
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Watch 实现
- en: The watch implementation of etcd3 takes advantage of gRPC bidirectional streams
    and maintains a single TCP connection to send multiple events, which reduced the
    memory footprint by at least an order of magnitude.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: etcd3 的 watch 实现利用了 gRPC 双向流，并保持一个单一的 TCP 连接来发送多个事件，从而将内存占用减少了至少一个数量级。
- en: State storage
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 状态存储
- en: With etcd3, Kubernetes started storing all the state as protocol buffers, which
    eliminated a lot of wasteful JSON serialization overhead.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 etcd3 后，Kubernetes 开始将所有状态存储为协议缓冲区，这消除了大量浪费的 JSON 序列化开销。
- en: Other optimizations
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他优化
- en: 'The Kubernetes team made many other optimizations such as:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 团队还进行了许多其他优化，例如：
- en: Optimizing the scheduler (which resulted in 5-10x higher scheduling throughput)
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化调度器（这使得调度吞吐量提高了 5 到 10 倍）
- en: Switching all controllers to a new recommended design using shared informers,
    which reduced resource consumption of controller-manager
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有控制器切换到使用共享信息器的新推荐设计，从而减少了控制器管理器的资源消耗
- en: Optimizing individual operations in the API server (conversions, deep copies,
    and patch)
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化 API 服务器中的个别操作（转换、深拷贝和补丁）
- en: Reducing memory allocation in the API server (which significantly impacts the
    latency of API calls)
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少 API 服务器中的内存分配（这对 API 调用的延迟有显著影响）
- en: Measuring the performance and scalability of Kubernetes
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量 Kubernetes 的性能和可扩展性
- en: In order to improve performance and scalability, you need a sound idea of what
    you want to improve and how you’re going to measure the improvements. You must
    also make sure that you don’t violate basic properties and guarantees in the quest
    for improved performance and scalability. What I love about performance improvements
    is that they often buy you scalability improvements for free. For example, if
    a pod needs 50% of the CPU of a node to do its job and you improve performance
    so that the pod can do the same work using 33% CPU, then you can suddenly run
    three pods instead of two on that node, and you’ve improved the scalability of
    your cluster by 50% overall (or reduced your cost by 33%).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高性能和可伸缩性，您需要清楚地了解您想要改进的内容以及如何测量这些改进。您还必须确保在追求性能和可伸缩性的过程中不违反基本的属性和保证。我喜欢性能改进的原因在于它们通常可以免费为您提供可伸缩性改进。例如，如果一个
    pod 需要一个节点的 50% CPU 来完成其工作，而您通过提高性能使得该 pod 可以使用 33% CPU 完成相同的工作，那么您突然可以在该节点上运行三个
    pod 而不是两个，从而将您的集群总体的可伸缩性提高了 50%（或者将成本降低了 33%）。
- en: The Kubernetes SLOs
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubernetes SLOs
- en: Kubernetes has **service level objectives** (**SLOs**). Those guarantees must
    be respected when trying to improve performance and scalability. Kubernetes has
    a one-second response time for API calls (99 percentile). That’s 1,000 milliseconds.
    It actually achieves an order of magnitude faster response times most of the time.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 具有**服务级别目标**（**SLOs**）。在尝试提高性能和可伸缩性时，必须遵守这些保证。Kubernetes 对 API 调用的响应时间有一个秒级（99
    百分位数）的 SLO。实际上，大多数情况下，它实现了比这个响应时间快一个数量级的更快响应时间。
- en: Measuring API responsiveness
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量 API 响应性
- en: The API has many different endpoints. There is no simple API responsiveness
    number. Each call has to be measured separately. In addition, due to the complexity
    and the distributed nature of the system, not to mention networking issues, there
    can be a lot of volatility in the results. A solid methodology is to break the
    API measurements into separate endpoints and then run a lot of tests over time
    and look at percentiles (which is standard practice).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: API 具有许多不同的端点。没有简单的 API 响应性数字。每个调用必须单独测量。此外，由于系统的复杂性和分布式特性，更不用说网络问题，结果可能会有很大的波动性。一种可靠的方法是将
    API 测量分成单独的端点，然后随时间运行大量测试，并查看百分位数（这是标准做法）。
- en: It’s also important to use enough hardware to manage a large number of objects.
    The Kubernetes team used a 32-core VM with 120 GB for the master in this test.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是要使用足够的硬件来管理大量对象。在此测试中，Kubernetes 团队使用了一个带有 32 核心和 120 GB 内存的虚拟机作为主节点。
- en: 'The following diagram describes the 50th, 90th, and 99th percentile of various
    important API call latencies for Kubernetes 1.3\. You can see that the 90th percentile
    is very low, below 20 milliseconds. Even the 99th percentile is less than 125
    milliseconds for the `DELETE` pods operation and less than 100 milliseconds for
    all other operations:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了 Kubernetes 1.3 版本中各个重要 API 调用延迟的 50th、90th 和 99th 百分位数。您可以看到，90th 百分位数非常低，低于
    20 毫秒。甚至对于`DELETE` pods 操作，99th 百分位数也低于 125 毫秒，对于所有其他操作也低于 100 毫秒：
- en: '![Figure 3.6: API call latencies](img/B18998_03_06.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6: API 调用延迟](img/B18998_03_06.png)'
- en: 'Figure 3.6: API call latencies'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.6: API 调用延迟'
- en: 'Another category of API calls is `LIST` operations. Those calls are more expansive
    because they need to collect a lot of information in a large cluster, compose
    the response, and send a potentially large response. This is where performance
    improvements such as the in-memory read cache and the protocol buffers serialization
    really shine. The response time is understandably greater than the single API
    calls, but it is still way below the SLO of one second (1,000 milliseconds):'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类 API 调用是`LIST`操作。这些调用更为昂贵，因为它们需要在大型集群中收集大量信息，组成响应，并发送可能很大的响应。这就是性能改进（例如内存读取缓存和协议缓冲区序列化）的亮点所在。响应时间理所当然地比单个
    API 调用要长，但仍远低于一秒（1,000 毫秒）的 SLO：
- en: '![Figure 3.7: LIST API call latencies](img/B18998_03_07.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7: LIST API 调用延迟](img/B18998_03_07.png)'
- en: 'Figure 3.7: LIST API call latencies'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.7: LIST API 调用延迟'
- en: Measuring end-to-end pod startup time
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量端到端 pod 启动时间
- en: One of the most important performance characteristics of a large dynamic cluster
    is end-to-end pod startup time. Kubernetes creates, destroys, and shuffles pods
    around all the time. You could say that the primary function of Kubernetes is
    to schedule pods. In the following diagram, you can see that pod startup time
    is less volatile than API calls. This makes sense since there is a lot of work
    that needs to be done, such as launching a new instance of a runtime, that doesn’t
    depend on cluster size. With Kubernetes 1.2 on a 1,000-node cluster, the 99th
    percentile end-to-end time to launch a pod was less than 3 seconds. With Kubernetes
    1.3, the 99th percentile end-to-end time to launch a pod was a little over 2.5
    seconds.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s remarkable that the time is very close, but a little better with Kubernetes
    1.3, on a 2,000-node cluster versus a 1,000-node cluster:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8: Pod startup latencies 1](img/B18998_03_08.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Pod startup latencies 1'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, bar chart  Description automatically generated](img/B18998_03_09.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: Pod startup latencies 2'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at how far we can push Kubernetes and how clusters
    with a large number of nodes perform. In the next section, we will examine some
    of the creative ways the Kubernetes developers test Kubernetes at scale.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Testing Kubernetes at scale
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clusters with thousands of nodes are expensive. Even a project such as Kubernetes
    that enjoys the support of Google and other industry giants still needs to come
    up with reasonable ways to test without breaking the bank.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes team runs a full-fledged test on a real cluster at least once
    per release to collect real-world performance and scalability data. However, there
    is also a need for a lightweight and cheaper way to experiment with potential
    improvements and detect regressions. Enter Kubemark.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Kubemark tool
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubemark is a Kubernetes cluster that runs mock nodes called hollow nodes used
    for running lightweight benchmarks against large-scale (hollow) clusters. Some
    of the Kubernetes components that are available on a real node such as the kubelet
    are replaced with a hollow kubelet. The hollow kubelet fakes a lot of the functionality
    of a real kubelet. A hollow kubelet doesn’t actually start any containers, and
    it doesn’t mount any volumes. But from the Kubernetes point of view – the state
    stored in etcd – all those objects exist and you can query the API server. The
    hollow kubelet is actually the real kubelet with an injected mock Docker client
    that doesn’t do anything.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Another important hollow component is the hollow proxy, which mocks the kube-proxy
    component. It again uses the real kube-proxy code with a mock proxier interface
    that does nothing and avoids touching iptables.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Kubemark cluster
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A Kubemark cluster uses the power of Kubernetes. To set up a Kubemark cluster,
    perform the following steps:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Create a regular Kubernetes cluster where we can run *N* hollow nodes.
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a dedicated VM to start all master components for the Kubemark cluster.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Schedule *N* hollow node pods on the base Kubernetes cluster. Those hollow nodes
    are configured to talk to the Kubemark API server running on the dedicated VM.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create add-on pods by scheduling them on the base cluster and configuring them
    to talk to the Kubemark API server.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A full-fledged guide is available here:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scalability/kubemark-guide.md](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scalability/kubemark-guide.md)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Comparing a Kubemark cluster to a real-world cluster
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The performance of Kubemark clusters is mostly similar to the performance of
    real clusters. For the pod startup end-to-end latency, the difference is negligible.
    For the API responsiveness, the differences are greater, though generally less
    than a factor of two. However, trends are exactly the same: an improvement/regression
    on a real cluster is visible as a similar percentage drop/increase in metrics
    on Kubemark.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at reliable and highly available large-scale Kubernetes
    clusters. This is arguably the sweet spot for Kubernetes. While it is useful to
    be able to orchestrate a small cluster running a few containers, it is not necessary,
    but at scale, you must have an orchestration solution in place that you can trust
    to scale with your system and provide the tools and the best practices to do that.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: You now have a solid understanding of the concepts of reliability and high availability
    in distributed systems. You delved into the best practices for running reliable
    and highly available Kubernetes clusters. You explored the complex issues surrounding
    scaling Kubernetes clusters and measuring their performance. You can now make
    wise design choices regarding levels of reliability and availability, as well
    as their performance and cost.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will address the important topic of security in Kubernetes.
    We will also discuss the challenges of securing Kubernetes and the risks involved.
    We will learn all about namespaces, service accounts, admission control, authentication,
    authorization, and encryption.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Join us on Discord!
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read this book alongside other users, cloud experts, authors, and like-minded
    professionals.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Ask questions, provide solutions to other readers, chat with the authors via.
    Ask Me Anything sessions and much more.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Scan the QR code or visit the link to join the community now.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code844810820358034203.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
