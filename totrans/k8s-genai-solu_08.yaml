- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Networking Best Practices for Deploying GenAI on K8s
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore best practices for cloud networking when deploying
    GenAI applications on **Kubernetes** (**K8s**). Effective networking is essential
    for ensuring seamless communication between Pods, optimizing performance, and
    enhancing security. The chapter will start with the K8s networking foundations,
    such as **Container Network Interface** (**CNI**) ([https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/))
    to set up Pod networking and network policies to enforce security and access controls
    within the K8s cluster, and we will also dive into using optimized cloud networking
    interfaces such as **Elastic Fabric Adapter** (**EFA**) ([https://aws.amazon.com/hpc/efa/](https://aws.amazon.com/hpc/efa/))
    for better network performance. By defining granular rules for communication between
    Pods and services, organizations can mitigate potential security threats to safeguard
    their intellectual property and GenAI models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Kubernetes networking model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced traffic management with a service mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Securing GenAI workloads with Kubernetes network policies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing network performance for GenAI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Kubernetes networking model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K8s networking has evolved from **Docker’s networking model** ([https://docs.docker.com/engine/network/](https://docs.docker.com/engine/network/))
    to better address the complexities of managing large clusters of containers across
    distributed environments. Docker’s initial networking used a *single-host, bridge-based
    networking* model where containers on the same host could communicate via a local
    bridge network. However, containers on the different hosts required additional
    configuration to explicitly create links between containers or to map container
    ports to host ports to make them reachable by containers on other hosts. K8s simplified
    this networking model by ensuring *seamless inter-Pod communication* across hosts,
    *automatic service discovery*, and *load balancing*.
  prefs: []
  type: TYPE_NORMAL
- en: 'K8s’ networking model ([https://kubernetes.io/docs/concepts/services-networking/#the-kubernetes-network-model](https://kubernetes.io/docs/concepts/services-networking/#the-kubernetes-network-model))
    has the following key tenets:'
  prefs: []
  type: TYPE_NORMAL
- en: Each Pod in a K8s cluster has its own unique IP address, and all containers
    within a Pod share a private network namespace. Containers within the same Pod
    can communicate with each other using localhost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods can communicate with each other directly across the cluster, without the
    need for proxies or address translation, such as **Network Address** **Translation**
    (**NAT**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Service API provides an IP address or hostname for services, even as the
    Pods making up those services change. K8s manages **EndpointSlice** ([https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/))
    objects to keep track of these Pods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NetworkPolicy** ([https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/))
    is a built-in K8s API that allows the control of traffic between Pods and external
    sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will cover Service APIs and network policies in detail in the later part
    of this chapter. Some key components of the K8s networking model are **Kubelet**
    ([https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)),
    **Container Runtime Interface** (**CRI**) ([https://kubernetes.io/docs/concepts/architecture/cri/](https://kubernetes.io/docs/concepts/architecture/cri/)),
    and **Container Network Interface** (**CNI**) ([https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)),
    which handle the lifecycle and networking of containers within a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubelet**: The kubelet is an agent running on each worker node in a K8s cluster
    that ensures that the containers described by the K8s API are running properly
    on the node. The kubelet interacts with the CRI to start, stop, and monitor containers
    based on the configurations set in Pod specifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CRI**: The CRI is an API that allows the kubelet to communicate with different
    container runtimes in a standardized way by abstracting the underlying container
    runtime such as Containerd or CRI-O.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CNI**: The CNI is an open source API specification designed with simplicity
    and modularity in mind. It allows K8s to handle container networking in a unified,
    plug-and-play manner by using any CNI-compatible plugin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a Pod is scheduled to a worker node, Kubelet instructs the CRI to create
    containers for the Pod. Once the containers are ready, Kubelet invokes the CNI
    plugin to set up the Pod network – attaching network interfaces, assigning IP
    addresses, configuring routing, and ensuring network policies are enforced. This
    allows Pods to seamlessly communicate with each other and with external networks,
    adhering to the K8s network model.
  prefs: []
  type: TYPE_NORMAL
- en: There are several CNI plugins available for K8s, each with a unique set of features
    and strengths. Some of the popular CNI plugins are **Calico** ([https://www.tigera.io/project-calico/](https://www.tigera.io/project-calico/)),
    **Cilium** ([https://github.com/cilium/cilium](https://github.com/cilium/cilium)),
    **Weave Net** ([https://github.com/weaveworks/weave](https://github.com/weaveworks/weave)),
    **Antrea** ([https://antrea.io/](https://antrea.io/)), and **Amazon VPC CNI**
    ([https://github.com/aws/amazon-vpc-cni-k8s](https://github.com/aws/amazon-vpc-cni-k8s)).
    Refer to the K8s documentation at [https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy](https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy)
    for a detailed list.
  prefs: []
  type: TYPE_NORMAL
- en: Other important K8s networking components are **IP Address Management** (**IPAM**),
    which is used by the CNI plugin to assign and manage IP addresses for Pods within
    a K8s cluster, and **IPTables** ([https://man7.org/linux/man-pages/man8/iptables.8.html](https://man7.org/linux/man-pages/man8/iptables.8.html)),
    which is responsible for packet filtering and is part of the Linux kernel. In
    K8s, components like kube-proxy and certain CNI network plugins use **IPTables**
    to manage network rules and direct traffic within worker nodes. These IPTables
    rules enable Pods to communicate with each other within the cluster, manage external
    traffic flows, and, depending on the network plugin, help implement network policies.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.1* shows how the kubelet, the CNI, IPAM, and IPTables work together
    within a K8s worker node to set up and manage networking for a Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: In step 1, Kubelet communicates with the CNI plugin to request the creation
    and configuration of a Pod’s network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In step 2, the CNI plugin creates a network namespace and calls the IPAM module
    to reserve an IP address for the Pod.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In step 3, the CNI plugin configures IPTables to manage network traffic rules
    for the Pod. This step ensures that the Pod can communicate with other Pods and
    external networks according to the cluster’s networking policies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the last step, the allocated IP address (e.g., 10.0.0.12) is assigned to
    the Pod. This makes the Pod reachable within the cluster using the assigned IP
    address.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.1 – IP allocation flow in a worker node](img/B31108_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – IP allocation flow in a worker node
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the CNI networking mode for GenAI applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When choosing a CNI plugin, it is important to understand the differences between
    **overlay networks** and **native networking**.
  prefs: []
  type: TYPE_NORMAL
- en: CNI plugins that use an overlay network create an additional layer of abstraction
    over the existing network, encapsulating traffic in tunnels to isolate Pod networks
    and simplify routing across nodes. While this provides flexibility and network
    segmentation, it often comes at the cost of higher latency and lower throughput
    due to the encapsulation overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Native networking plugins, such as the Amazon VPC CNI and Cilium, integrate
    directly with the underlying infrastructure’s routing, allowing Pods to communicate
    using real network interfaces and IP addresses without encapsulation. This integration
    can result in higher throughput and lower latency, making native networking an
    optimal choice for applications that require high performance. For GenAI workloads,
    where rapid data transfer and minimal network latency are critical for effective
    model training and inference, native networking CNI plugins are usually recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Service implementation in K8s
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A **Service** is a K8s resource that provides network endpoint and load balancing
    across a set of Pods, making it easy to expose an application to other services
    or external clients. Here are the key features of K8s Service implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: K8s Services provide a consistent IP and DNS name for a set of Pods, so even
    as Pods get created or destroyed, service performance isn’t affected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services automatically distribute incoming traffic to the available Pods based
    on their labels. This helps with load balancing and ensures availability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'K8s supports the following four different types of services:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aws-load-balancer-controller` ([https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/](https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/))
    addon to expose our GenAI models using the AWS **Network Load Balance**r (**NLB**)
    in [*Chapter 3*](B31108_03.xhtml#_idTextAnchor039).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ExternalName** maps the service to an external DNS name, allowing K8s services
    to connect to an external service outside the cluster.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: GenAI applications often require scalable, low latency, and efficient networking
    to serve the customers effectively. K8s’ **LoadBalancer** Service can be used
    to expose models outside the cluster. It provisions an external load balancer
    through the underlying cloud provider (such as AWS, Azure, or GCP). This setup
    enables seamless distribution of incoming traffic across multiple Pods, ensuring
    high availability and scalability. On the other hand, **ClusterIP** service can
    be utilized when the GenAI models are accessed only within the cluster by other
    K8s Pods. It assigns a friendly service lookup name and an IP address for each
    service, facilitating reliable service discovery and communication between Pods
    without exposing them externally.
  prefs: []
  type: TYPE_NORMAL
- en: By default, when using the K8s `aws-load-balancer-controller`, a **NodePort**
    Service is created to forward traffic from the AWS NLB to the K8s worker nodes.
    From there, kube-proxy routes the traffic to the individual Pods, introducing
    an additional network hop that can increase the latency and throughput. To reduce
    this overhead, you can configure the LoadBalancer service to route traffic directly
    to K8s Pods by registering the Pods as NLB targets.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be achieved by adding the `service.beta.kubernetes.io/aws-load-balancer-nlb-target-type:
    ip` annotation to the K8s service, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Service health checks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another important consideration in ensuring optimal performance and reliability
    is configuring **healthcheck** settings on the LoadBalancer service type. GenAI
    models are typically resource-intensive and can have varying startup and response
    times based on input complexity and server load. Without proper health checks,
    NLB may flag the targets as unhealthy, which will lead to replacing the K8s Pods,
    degraded performance, or potential downtime.
  prefs: []
  type: TYPE_NORMAL
- en: LoadBalancer health checks ensure that traffic is routed only to healthy Pods
    capable of handling requests. For instance, in a scenario where a GenAI model
    serves inference through a K8s Service of type LoadBalancer, the health check
    can monitor an endpoint such as `/healthz` on each backend Pod. This endpoint
    could return an *HTTP 200 OK* status if the model is fully loaded, the required
    resources (e.g., GPU memory) are available, and the inference service is operational.
    If a Pod fails the health check due to issues such as resource exhaustion or process
    crashes, the LoadBalancer will automatically exclude it from the pool of available
    endpoints, directing traffic to healthy Pods instead. This mechanism prevents
    request failures and ensures a seamless experience for end users while maintaining
    the overall stability and scalability of the GenAI workload.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the `aws-load-balancer-controller` documentation at [https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/guide/service/annotations/#health-check](https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/guide/service/annotations/#health-check)
    for various health check settings.
  prefs: []
  type: TYPE_NORMAL
- en: While K8s LoadBalancer Services provide a simple and straightforward way to
    expose K8s workloads externally, they can be limited in more complex routing scenarios.
    Ingress and Gateway APIs provide greater flexibility and control over traffic
    flow, enabling features such as path or host-based routing or TLS termination.
    In the next section, we will cover the Ingress and Gateway APIs, which handle
    the input traffic for K8s workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress controller
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In K8s, **Ingress** API ([https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/))
    is used to expose applications outside the cluster using HTTP/S protocols. It
    acts as a routing layer to direct incoming requests to K8s applications based
    on HTTP URL paths, hostnames, headers, and so on. An Ingress controller is responsible
    for provisioning required infrastructure resources such as Application Load Balancers
    (in AWS), configuring routing rules, and terminating SSL connections, to fulfill
    the Ingress resources. Some of the popular **Ingress controllers** ([https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/))
    include **ingress-nginx**, **aws-lb-controller**, **HAProxy Ingress**, and **Istio
    Ingress**. Refer to the K8s documentation at [https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers)
    for a detailed list. When deploying GenAI workloads in a K8s cluster, you should
    consider Ingress when exposing multiple applications/models under one entry point
    (domain name) as shown in *Figure 8**.2*, which typically requires centralized
    traffic routing. Additionally, Ingress supports advanced routing features such
    as path-based and host-based routing, enabling you to direct specific requests
    to different model versions or applications based on URLs and HTTP headers, which
    is useful for **A/B testing** ([https://aws.amazon.com/developer/application-security-performance/articles/a-b-testing/](https://aws.amazon.com/developer/application-security-performance/articles/a-b-testing/))
    or **canary releases**, where new application changes are gradually rolled out
    to a small set of users, allowing teams to monitor performance and catch issues
    before full-scale deployment. Apart from routing, Ingress controllers also integrate
    with monitoring tools such as **Prometheus** to provide detailed metrics such
    as latencies, request rates, and error rates.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Overview of Ingress](img/B31108_08_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Overview of Ingress
  prefs: []
  type: TYPE_NORMAL
- en: The **Gateway API** ([https://gateway-api.sigs.k8s.io/](https://gateway-api.sigs.k8s.io/))
    was created to address the limitations of Ingress, offering a more flexible, extensible,
    and standardized approach to managing traffic in K8s clusters, especially for
    complex networking needs. It is the official K8s project focused on L4 and L7
    routing, representing the next generation of Ingress, Load Balancing, and Service
    Mesh APIs. Like Ingress, you need to install **Gateway Controller** to provision
    necessary infrastructure resources along with advanced routing features. There
    are many implementations of this API, refer to K8s documentation at [https://gateway-api.sigs.k8s.io/implementations/](https://gateway-api.sigs.k8s.io/implementations/)
    for a complete list.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about foundational concepts of K8s networking, core
    tenets, and how it is different from other orchestrators. We discussed the role
    of the CNI plugin to configure the Pod networking, IPAM to manage the IP addressing,
    and considerations when choosing a CNI networking mode. We explored various K8s
    services, Ingress, and the Gateway API to expose GenAI models outside of the cluster.
    In the next section, we will discuss advanced application networking constructs
    such as service mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced traffic management with a service mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As GenAI applications grow in complexity, the number of services involved such
    as model inferencing, data ingestion, data processing, fine-tuning, and training
    also increases the complexity of managing service-to-service communication. These
    typically include traffic management (load balancing, retries, rate limiting),
    security (authentication, authorization, encryption), and observability (logs,
    metrics, traces).
  prefs: []
  type: TYPE_NORMAL
- en: A **service mesh** is an infrastructure layer that enables reliable, secure,
    and observable communication between microservices. It abstracts the complexity
    of service-to-service communication, including traffic management, load balancing,
    security, and observability, without requiring changes in application code.
  prefs: []
  type: TYPE_NORMAL
- en: Service mesh typically provides these features by deploying a sidecar proxy
    such as **Envoy** ([https://www.envoyproxy.io/](https://www.envoyproxy.io/)) alongside
    the application, which intercepts all network traffic in and out of the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The service mesh has two key components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Control plane**, which manages the sidecar proxies, distributing configuration
    and policies across the side car proxies and gathering telemetry data for centralized
    control and monitoring. Different kinds of policies can be implemented in service
    mesh, such as the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traffic management policies**, which define rules for how traffic should
    be routed, circuit-breaking rules, load balancing, and retry options'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security policies**, which can establish rules for encryption (mTLS), authentication
    (e.g., JWT tokens), and authorization (RBAC)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resilience and fault tolerance policies**, which can define retry mechanisms,
    timeouts, and failover options'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data plane**, which combines sidecar proxies and is responsible for the actual
    network traffic between services. Each sidecar can apply policies independently,
    ensuring that each service instance respects the network rules and policies set
    at the control plane. **Sidecar containers** are secondary containers that run
    alongside the main application container within the same Pod. They complement
    the primary container by offering additional capabilities, such as logging, monitoring,
    security, or data synchronization, enhancing the application’s functionality without
    requiring changes to its code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some popular service mesh implementations in K8s are **Istio**, **Linkerd**,
    and **Ambient Mesh**. While traditional meshes such as Istio and Linkerd rely
    on sidecars, Ambient Mesh recently introduced by Istio offers a sidecarless architecture
    that can leverage eBPF for efficient traffic management and security. *Figure
    8**.3* shows how a service mesh implementation can route the traffic in and out
    of the mesh, intercept requests, and implement end-to-end TLS encryption or mTLS
    using sidecar proxies.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Service mesh implementation](img/B31108_08_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Service mesh implementation
  prefs: []
  type: TYPE_NORMAL
- en: This figure shows a service mesh architecture, where traffic enters through
    the Ingress Gateway, flows between Pods via sidecar proxies, and exits through
    the Egress Gateway. The Service Mesh Control Plane centrally configures and manages
    the sidecar proxies to enforce traffic policies and security. This setup enables
    efficient and secure microservices communication within the cluster. Refer to
    the *Getting Started with Istio on Amazon EKS* article at [https://aws.amazon.com/blogs/opensource/getting-started-with-istio-on-amazon-eks/](https://aws.amazon.com/blogs/opensource/getting-started-with-istio-on-amazon-eks/)
    for step-by-step instructions to deploy Istio service mesh on Amazon EKS.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explored the advanced traffic management features of a service
    mesh such as load balancing, security, observability, and a high-level architectural
    overview. In the next section, we will dive deeper into securing GenAI workloads
    using K8s native network policies.
  prefs: []
  type: TYPE_NORMAL
- en: Securing GenAI workloads with Kubernetes’ network policies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Network policies** are a native K8s feature that controls ingress (incoming)
    and egress (outgoing) traffic between Pods within a cluster. They are implemented
    through the K8s **NetworkPolicy API** and allow administrators to define rules
    that determine which Pods or IP addresses can communicate with each other.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike a service mesh, which provides advanced traffic management and security
    features, network policies focus on traffic isolation and network segmentation
    for security purposes, such as namespace isolation.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, all Pods in a K8s cluster can communicate with each other; however,
    network policies can restrict this and allow fine-grained control over network
    traffic. This is especially useful in multi-tenant clusters, where different teams
    or applications require isolation for security or compliance. Some key features
    of network policies are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ingress and egress rules**: *Ingress rules* define which sources are allowed
    to communicate with a specific Pod or set of Pods. Similarly, *egress rules* define
    which destinations a Pod or set of Pods can connect to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`app: backend` can be configured to allow ingress only from Pods with the label
    `app: frontend`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deny-by-default model**: By default, if there are no network policies applied,
    all traffic is allowed. However, once a network policy is applied to a Pod, only
    the traffic that matches the specified policy rules is allowed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of a network policy that allows incoming traffic to Pods
    with the label `app: backend` only from other Pods with the label `app: frontend`
    in the same namespace. All other ingress traffic is denied by default for these
    Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s explore how to implement K8s network policies to secure traffic
    flows among different components of our e-commerce chatbot application.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing network policies in a chatbot application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B31108_05.xhtml#_idTextAnchor062), we deployed a chatbot application
    in the EKS cluster that comprises four components (chatbot-ui, vector database,
    RAG application, and fine-tuned Llama3 model), as depicted in *Figure 8**.4*.
    By default, all components can talk to each other on any ports/protocols, which
    is not a security best practice. Our goal is to implement network segmentation
    so that only trusted components can communicate with each other on approved ports
    and protocols.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – E-commerce chatbot application architecture](img/B31108_08_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – E-commerce chatbot application architecture
  prefs: []
  type: TYPE_NORMAL
- en: In this setup, the Chatbot UI application communicates with the RAG application
    and the fine-tuned Llama 3 model over HTTP on port 80\. So, let’s create an Ingress
    network policy on both RAG and Llama 3 applications to allow only HTTP/80 ingress
    traffic from the chatbot UI app to secure the network traffic flow. We can use
    the labels applied to respective K8s deployments to create the network policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following network policy selects the RAG application Pods using the `app.kubernetes.io/name:
    rag-app` label and applies an ingress rule to allow HTTP/80 traffic only from
    Pods identified by the `app.kubernetes.io/name=chatbot-ui` label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also create another network policy for a fine-tuned Llama 3 application
    to allow HTTP/80 traffic only from the chatbot UI app by using the `app.kubernetes.io/name:
    my-llama-finetuned`, `app.kubernetes.io/name:` `chatbot-ui` labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the RAG application communicates with the vector database over HTTP
    on port 6333\. To restrict inbound traffic, we can create a network policy that
    applies to the Pods labeled `app.kubernetes.io/name: qdrant`, allowing HTTP/6333
    traffic only from Pods labeled `app.kubernetes.io/name: rag-app`.'
  prefs: []
  type: TYPE_NORMAL
- en: You can find all the network policies in the GitHub repository at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch8](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch8).
    You can download and apply them in the EKS cluster using the `kubectl apply` command.
    In this walkthrough, we focused on configuring Ingress rules to restrict the inbound
    traffic. To further tighten security, you can extend these policies to include
    egress rules for the outbound traffic. However, if your application performs DNS
    lookups on K8s Services, be sure to allow DNS traffic to the `kube-system` namespace.
  prefs: []
  type: TYPE_NORMAL
- en: By default, upstream K8s network policies support only a limited set of rules
    to define traffic flows (primarily IP address, port, protocol, podSelector, and
    namespaceSelector) and do not support domain-based rules or cluster or global
    policies. This can be insufficient for GenAI applications when using external
    APIs such as OpenAI or Claude. To address these gaps, **Cilium** ([https://docs.cilium.io/en/stable/security/](https://docs.cilium.io/en/stable/security/)),
    **Calico** ([https://docs.tigera.io/calico/latest/network-policy/](https://docs.tigera.io/calico/latest/network-policy/))
    and others offer advanced capabilities such as DNS-based policies, which allow
    specifying fully qualified domain names for dynamic policy enforcement, and global
    (cluster-wide) policies, which ensure a uniform security posture across all namespaces.
    These features simplify policy management, strengthen cluster-wide data governance,
    and maintain consistent traffic controls.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, K8s network policies help define traffic rules and segmentation
    at the network and transport layers of the OSI model but lack advanced features
    for application layer traffic management and observability. In the next section,
    we will compare network policies with service mesh technology, highlighting how
    these solutions differ in their key features.
  prefs: []
  type: TYPE_NORMAL
- en: Service mesh versus K8s network policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While both service mesh and K8s network policies help to secure and manage
    K8s networking, they serve different purposes and often complement each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Feature** | **Service Mesh** | **K8s** **Network Policies** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Key Focus** | Observability, traffic management (such as load balancing),
    retries, and security (mTLS support) | Security and traffic isolation using namespaces
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Traffic Routing** | Advanced routing and load balancing | Basic |'
  prefs: []
  type: TYPE_TB
- en: '| **OSI Layer** | Primarily at Layer 7 (application layer) | Primarily Layer
    3 and 4 (network and transport layer) |'
  prefs: []
  type: TYPE_TB
- en: '| **Mutual TLS** | Supported | Not supported |'
  prefs: []
  type: TYPE_TB
- en: '| **Complexity** | Requires sidecar proxies | Simpler, native to Kubernetes
    |'
  prefs: []
  type: TYPE_TB
- en: In many production environments, both service mesh and K8s network policies
    are deployed together to enhance security and traffic management. For instance,
    you might use K8s network policies to enforce namespace-based access restrictions,
    then use a service mesh to handle routing, retries, load balancing, and mTLS within
    the defined traffic boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing network performance for GenAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover some important network optimizations when deploying
    GenAI workloads on K8s, such as Kube-Proxy, IP exhaustion issues, and advanced
    networking capabilities such as **Single Root Input/Output Virtualization** (**SR-IOV**)
    and **extended Berkeley Packet** **Filter** (**eBPF**).
  prefs: []
  type: TYPE_NORMAL
- en: Kube-Proxy – IPTables versus IPVS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kube-Proxy** ([https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/))
    is a core component of K8s that is responsible for managing networking within
    the cluster. It ensures seamless communication between services and Pods by setting
    up network rules and configuration on each node. Kube-Proxy maintains network
    rules to direct traffic to the appropriate backend Pods that serve each service,
    allowing internal cluster traffic to reach the correct destinations. By default,
    Kube-Proxy uses **IPTables** mode, which efficiently intercepts and redirects
    network requests based on IP rules, making it suitable for small and medium-sized
    clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: However, for large-scale K8s clusters, particularly those running data-intensive
    workloads such as GenAI, IPTables mode might become a performance bottleneck.
    As clusters scale up to include hundreds or thousands of services and endpoints,
    maintaining and updating these rules in IPTables can lead to increased latency
    and reduced network performance, impacting the overall efficiency of AI models.
  prefs: []
  type: TYPE_NORMAL
- en: To address these limitations, K8s offers the option to run Kube-Proxy in **IP
    Virtual Server** (**IPVS**) mode. IPVS provides advanced load-balancing capabilities
    by leveraging the Linux kernel’s IPVS module, which is more scalable and efficient
    than IPTables for handling high traffic. IPVS maintains an in-memory hash table
    for Service-to-Pod routing, enabling faster packet processing with minimal CPU
    overhead.
  prefs: []
  type: TYPE_NORMAL
- en: IPVS mode offers benefits such as more sophisticated load balancing algorithms
    (e.g., round-robin, least connections, source hashing), better handling of dynamic
    and large service environments, and reduced latency in packet forwarding.
  prefs: []
  type: TYPE_NORMAL
- en: 'IPVS can be enabled by updating the `kube-proxy-config` `ConfigMap` in the
    `kube -``system` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To enable IPVS in Amazon EKS cluster setup, take a look at the EKS documentation
    at [https://docs.aws.amazon.com/eks/latest/best-practices/ipvs.html](https://docs.aws.amazon.com/eks/latest/best-practices/ipvs.html)
    for step-by-step instructions.
  prefs: []
  type: TYPE_NORMAL
- en: While advanced networking configurations such as IPVS help enhance traffic management
    and scalability in K8s, another critical challenge for large-scale clusters lies
    in efficiently managing IP address allocation, which we will cover in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: IP address exhaustion issues and custom networking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using CNI plugins such as Amazon VPC CNI in native networking mode, each
    K8s Pod receives an IP address directly from the VPC CIDR block. This approach
    allows each Pod to be fully addressable within the VPC, providing visibility of
    the Pod IP addresses with tools such as **VPC flow logs** and other monitoring
    solutions. However, in large-scale clusters running intensive workloads such as
    GenAI, this model can lead to IP address exhaustion as each Pod consumes an IP
    from a finite VPC CIDR pool.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate this, one effective solution is to use **IPv6 addressing**. IPv6
    provides an exponentially larger address space than IPv4, reducing the risk of
    IP exhaustion and allowing clusters to scale without worrying about running out
    of IP addresses. But not all organizations are ready to adopt IPV6 yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to address IP exhaustion is through VPC CNI custom networking.
    This involves enhancing the VPC design by associating additional, non-routable
    secondary CIDR blocks with the VPC and creating new subnets from these CIDRs.
    These subnets are designated specifically for Pod IP allocation, while the primary,
    routable CIDR is preserved for node IPs and other resources. We then configure
    the VPC CNI to allocate Pod IPs from these non-routable subnets, freeing up the
    primary CIDR for other networking needs and reducing the risk of IP exhaustion.
    The following link explains how custom networking can be implemented in Amazon
    EKS: [https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network-tutorial.html](https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network-tutorial.html).'
  prefs: []
  type: TYPE_NORMAL
- en: To address networking challenges in K8s, including IP address exhaustion and
    performance optimization, it’s worth exploring advanced networking solutions,
    such as VPC CNI custom networking or IPV6\. Emerging technologies such as eBPF
    and SR-IOV offer innovative methods to improve network efficiency and scalability,
    which we will discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: eBPF and SR-IOV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**extended Berkeley Packet Filter** (**eBPF**) ([https://ebpf.io/what-is-ebpf/](https://ebpf.io/what-is-ebpf/))
    allows advanced programmability within the Linux kernel without requiring changes
    to kernel code. This can be used to create powerful, lightweight networking, observability,
    and security solutions directly at the kernel level. For example, the Cilium CNI
    plugin, which leverages eBPF, provides fine-grained network security, load balancing,
    and observability capabilities. For GenAI workloads, where performance and data
    transfer speed are crucial, eBPF’s minimal overhead and kernel-level processing
    can make it a good choice for lower latency and higher throughput data transfer.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single Root Input/Output Virtualization** (**SR-IOV**) enables a single physical
    **Network Interface Card** (**NIC**) to be partitioned into multiple **virtual
    functions**, providing direct hardware access to VMs or containers. Each virtual
    function acts as an independent interface, offering high throughput and low latency
    network throughput, which is ideal for GenAI workloads that involve large data
    transfers. SR-IOV reduces CPU overhead by offloading packet processing to NIC,
    ensuring better resource utilization for compute-intensive tasks. It also provides
    dedicated network paths, ensuring network isolation and predictable performance,
    which is crucial for consistent AI model inference. This technology enhances scalability
    by optimizing resource allocation and supports multi-tenant clusters with isolated,
    high-performance networking.'
  prefs: []
  type: TYPE_NORMAL
- en: Technologies such as eBPF and SR-IOV optimize networking performance and resource
    efficiency in K8s clusters, enabling high-speed and reliable data processing.
    Complementing these advancements is CoreDNS autoscaling, which ensures seamless
    service discovery and efficient DNS resolution, critical for inter-service communication.
  prefs: []
  type: TYPE_NORMAL
- en: CoreDNS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**CoreDNS** ([https://github.com/coredns/coredns](https://github.com/coredns/coredns))
    is a crucial component in K8s for providing DNS services that facilitate internal
    service discovery and networking within the K8s cluster. It acts as the cluster’s
    DNS server, allowing Pods to find and communicate with each other using simple
    service names. For optimal performance and management, using the **CoreDNS managed
    add-on** ([https://docs.aws.amazon.com/eks/latest/userguide/coredns-add-on-create.html](https://docs.aws.amazon.com/eks/latest/userguide/coredns-add-on-create.html))
    in EKS is recommended. Monitoring CoreDNS is also essential for maintaining efficient
    network performance, as issues with DNS resolution can lead to service disruptions
    or latency, particularly in workloads that require extensive inter-service communication,
    such as GenAI.'
  prefs: []
  type: TYPE_NORMAL
- en: To keep up with the growing needs of large-scale K8s clusters, it is recommended
    to implement CoreDNS autoscaling to scale the number of CoreDNS Pods proportional
    to the size of the cluster. You can utilize the `cluster-proportional-autoscaler`
    ([https://github.com/kubernetes-sigs/cluster-proportional-autoscaler](https://github.com/kubernetes-sigs/cluster-proportional-autoscaler))
    addon, which watches over the number of schedulable nodes and CPU cores in the
    cluster and resizes the number of replicas for critical resources such as CoreDNS.
    When using CoreDNS managed addon in EKS, this functionality is provided natively
    and can be enabled using the addon configuration described at [https://docs.aws.amazon.com/eks/latest/userguide/coredns-autoscaling.html](https://docs.aws.amazon.com/eks/latest/userguide/coredns-autoscaling.html).
  prefs: []
  type: TYPE_NORMAL
- en: Another tool for improving DNS performance in the K8s cluster is the `cluster.local`
    suffix), the local caching agent forwards the query to the CoreDNS service.
  prefs: []
  type: TYPE_NORMAL
- en: CoreDNS is a vital component in K8s that enables internal service discovery
    and networking by acting as the cluster’s DNS server. To optimize performance
    in large-scale clusters, use CoreDNS autoscaling with `cluster-proportional-autoscaler`
    and leverage NodeLocal DNSCache to reduce latency. In the next section, we will
    cover a few other options to optimize network latency and throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Network latency and throughput enhancements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GenAI workloads, such as large-scale training and inference of machine learning
    models, require extensive communication between multiple compute nodes. In distributed
    training scenarios where model parameters need to be synchronized across nodes,
    achieving high bandwidth and low latency is critical to achieve high performance
    and reduce the training/inference costs.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss two cloud-specific techniques to achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon EC2 placement groups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Amazon EC2 placement groups** ([https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html))
    provide a way to organize nodes for specific networking or resilience objectives.
    In AWS, the following three placement groups are supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster**, which places instances close together within an Availability Zone
    to enable low-latency network performance for high-performance computing applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition**, which distributes instances across separate logical partitions
    so that each partition’s group of instances does not share underlying hardware
    with those in other partitions. This strategy is typically used by large distributed
    and replicated workloads, such as Hadoop, Cassandra, and Kafka.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spread**, which distributes a small group of instances across distinct underlying
    hardware to minimize the risk of correlated failures and improves resilience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cluster placement group, which places instances physically close together
    within a single data center or Availability Zone, provides low-latency and high-throughput
    networking between nodes. It could improve performance for distributed GenAI applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command creates a placement group called `custom-placement-group`
    in AWS with a cluster or proximity placement objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, EKS worker nodes can be launched within this placement group by creating
    an Auto Scaling group with the following launch template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In brief, EC2 placement groups offer strategies for organizing EC2 instances
    to optimize networking and throughput optimization. In the next section, we will
    cover the **Elastic Fabric Adapter** (**EFA**), a specialized network interface
    that delivers ultra-low latency and high throughput for inter-node communication.
  prefs: []
  type: TYPE_NORMAL
- en: EFA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: EFA ([https://aws.amazon.com/hpc/efa/](https://aws.amazon.com/hpc/efa/)) is
    a network interface designed by AWS to provide ultra-low latency and a high-throughput
    network interface for internode communications. This is critical for GenAI workloads
    as it ensures that data transfer between nodes is fast and efficient. EFA supports
    **Remote Direct Memory Access** (**RDMA**), which reduces the overhead of data
    transfer between nodes, providing a low-latency, high-throughput path. Through
    RDMA, data can be transferred directly between the memory of two compute nodes
    across a network without involving the operating system or CPUs. EFA also supports
    **NVIDIA Collective Communications Library** (**NCCL**) ([https://developer.nvidia.com/nccl](https://developer.nvidia.com/nccl))
    for AI and ML applications to enable high-performance, scalable distributed training
    by accelerating communication between GPUs across multiple nodes. This integration
    reduces latency and improves bandwidth for inter-GPU communication, allowing faster
    model training times in GenAI applications that require synchronized, collective
    operations, such as data parallelism and model parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 'To integrate EFA with K8s Pods in the EKS cluster, you can create worker nodes
    with EFA-compatible instance types and deploy `aws-efa-k8s-device-plugin` ([https://github.com/aws/eks-charts/tree/master/stable/aws-efa-k8s-device-plugin](https://github.com/aws/eks-charts/tree/master/stable/aws-efa-k8s-device-plugin)),
    which detects and advertises EFA interfaces as allocatable resources to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Refer to the EKS documentation at [https://docs.aws.amazon.com/eks/latest/userguide/node-efa.html](https://docs.aws.amazon.com/eks/latest/userguide/node-efa.html)
    for step-by-step instructions and an example walkthrough.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed various network optimization techniques such as
    picking Kube-proxy options, scaling CoreDNS Pods, strategies to solve IP exhaustion
    issues, emerging trends in K8s networking space, and other cloud-provider-specific
    optimizations including EFA and EC2 placement groups for managing large-scale
    GenAI workloads in the K8s clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on optimizing cloud networking for deploying GenAI
    applications on K8s, highlighting best practices for efficient, secure, and high-performance
    networking. We started with K8s networking fundamentals, covering key components
    such as the **Container Network Interface** (**CNI**), the kubelet, and the **Container
    Runtime Interface** (**CRI**), which manage Pod networking and ensure connectivity
    across the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The K8s networking model is supported by CNI plugins such as Calico, Cilium,
    and Amazon VPC CNI, each with specific benefits. CNI plugins operate in two modes:
    overlay networks and native networking. Overlay networks, such as Flannel, add
    flexibility with network abstraction but may increase latency. On the other hand,
    native networking (e.g., Amazon VPC CNI) integrates with the underlying cloud
    infrastructure, offering lower latency, and is recommended for GenAI workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: Service management within K8s provides stable IPs and DNS names, ensuring reliability
    even as Pods are added or removed. Service mesh tools, such as Istio or Linkerd,
    could be highly effective in enhancing traffic management, security, and observability
    by intercepting all traffic through sidecar proxies and implementing policies
    for load balancing, retry mechanisms, and TLS encryption.
  prefs: []
  type: TYPE_NORMAL
- en: NetworkPolicy, a native feature, further strengthens security by controlling
    ingress and egress traffic within K8s clusters, allowing isolation between teams
    or applications in multi-tenant environments. To address GenAI-specific needs,
    K8s supports advanced networking options such as Kube-Proxy with IPVS mode, offering
    scalable load balancing for high-demand clusters. Additionally, as K8s clusters
    scale, IP address exhaustion can become a challenge when using the CNI’s native
    networking mode; solutions such as IPv6 addressing and custom networking configuration
    can be employed to mitigate IP limitations in large-scale deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Other modern technologies support high-performance networking, such as **extended
    Berkeley Packet Filter** (**eBPF**) and **Single Root Input/Output Virtualization**
    (**SR-IOV**) provide minimal-overhead, kernel-level networking ideal for low-latency,
    high-throughput data processing. Finally, K8s networking benefits from specific
    enhancements in cloud environments, such as AWS’s placement groups and Elastic
    Fabric Adapter (EFA). In the next chapter, we will build on these concepts and
    discuss how to secure GenAI applications running in K8s.
  prefs: []
  type: TYPE_NORMAL
