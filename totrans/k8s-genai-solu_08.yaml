- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Networking Best Practices for Deploying GenAI on K8s
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在K8s上部署GenAI的网络最佳实践
- en: In this chapter, we will explore best practices for cloud networking when deploying
    GenAI applications on **Kubernetes** (**K8s**). Effective networking is essential
    for ensuring seamless communication between Pods, optimizing performance, and
    enhancing security. The chapter will start with the K8s networking foundations,
    such as **Container Network Interface** (**CNI**) ([https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/))
    to set up Pod networking and network policies to enforce security and access controls
    within the K8s cluster, and we will also dive into using optimized cloud networking
    interfaces such as **Elastic Fabric Adapter** (**EFA**) ([https://aws.amazon.com/hpc/efa/](https://aws.amazon.com/hpc/efa/))
    for better network performance. By defining granular rules for communication between
    Pods and services, organizations can mitigate potential security threats to safeguard
    their intellectual property and GenAI models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨在**Kubernetes**（**K8s**）上部署GenAI应用时云网络的最佳实践。有效的网络连接对于确保Pod之间的无缝通信、优化性能和增强安全性至关重要。本章将首先介绍K8s网络的基础知识，例如**容器网络接口**（**CNI**）（[https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)）以设置Pod网络和网络策略，从而在K8s集群内实施安全和访问控制，此外，我们还将深入探讨使用优化的云网络接口，如**弹性网络适配器**（**EFA**）（[https://aws.amazon.com/hpc/efa/](https://aws.amazon.com/hpc/efa/)），以提升网络性能。通过为Pod和服务之间的通信定义细粒度的规则，组织可以减少潜在的安全威胁，从而保护其知识产权和GenAI模型。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主要主题：
- en: Understanding the Kubernetes networking model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Kubernetes网络模型
- en: Advanced traffic management with a service mesh
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用服务网格进行高级流量管理
- en: Securing GenAI workloads with Kubernetes network policies
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kubernetes网络策略保护GenAI工作负载
- en: Optimizing network performance for GenAI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化GenAI的网络性能
- en: Understanding the Kubernetes networking model
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Kubernetes网络模型
- en: K8s networking has evolved from **Docker’s networking model** ([https://docs.docker.com/engine/network/](https://docs.docker.com/engine/network/))
    to better address the complexities of managing large clusters of containers across
    distributed environments. Docker’s initial networking used a *single-host, bridge-based
    networking* model where containers on the same host could communicate via a local
    bridge network. However, containers on the different hosts required additional
    configuration to explicitly create links between containers or to map container
    ports to host ports to make them reachable by containers on other hosts. K8s simplified
    this networking model by ensuring *seamless inter-Pod communication* across hosts,
    *automatic service discovery*, and *load balancing*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: K8s网络已从**Docker的网络模型**（[https://docs.docker.com/engine/network/](https://docs.docker.com/engine/network/)）演变而来，以更好地解决在分布式环境中管理大型容器集群的复杂性。Docker最初的网络使用的是*单主机、基于桥接的网络*模型，其中同一主机上的容器可以通过本地桥接网络进行通信。然而，不同主机上的容器需要额外的配置，显式地创建容器之间的链接，或将容器端口映射到主机端口，以使其他主机上的容器能够访问。K8s通过确保*无缝的跨主机Pod通信*、*自动服务发现*和*负载均衡*简化了这一网络模型。
- en: 'K8s’ networking model ([https://kubernetes.io/docs/concepts/services-networking/#the-kubernetes-network-model](https://kubernetes.io/docs/concepts/services-networking/#the-kubernetes-network-model))
    has the following key tenets:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: K8s的网络模型（[https://kubernetes.io/docs/concepts/services-networking/#the-kubernetes-network-model](https://kubernetes.io/docs/concepts/services-networking/#the-kubernetes-network-model)）具有以下关键原则：
- en: Each Pod in a K8s cluster has its own unique IP address, and all containers
    within a Pod share a private network namespace. Containers within the same Pod
    can communicate with each other using localhost.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K8s集群中的每个Pod都有其独特的IP地址，Pod内的所有容器共享一个私有网络命名空间。位于同一Pod内的容器可以通过localhost相互通信。
- en: Pods can communicate with each other directly across the cluster, without the
    need for proxies or address translation, such as **Network Address** **Translation**
    (**NAT**).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods可以直接跨集群相互通信，无需代理或地址转换，例如**网络地址转换**（**NAT**）。
- en: The Service API provides an IP address or hostname for services, even as the
    Pods making up those services change. K8s manages **EndpointSlice** ([https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/))
    objects to keep track of these Pods.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Service API 提供了一个 IP 地址或主机名来表示服务，即使构成这些服务的 Pods 发生变化。K8s 管理 **EndpointSlice**
    ([https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/))
    对象来跟踪这些 Pods。
- en: '**NetworkPolicy** ([https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/))
    is a built-in K8s API that allows the control of traffic between Pods and external
    sources.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NetworkPolicy** ([https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/))
    是一个内建的 K8s API，允许控制 Pod 与外部源之间的流量。'
- en: We will cover Service APIs and network policies in detail in the later part
    of this chapter. Some key components of the K8s networking model are **Kubelet**
    ([https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)),
    **Container Runtime Interface** (**CRI**) ([https://kubernetes.io/docs/concepts/architecture/cri/](https://kubernetes.io/docs/concepts/architecture/cri/)),
    and **Container Network Interface** (**CNI**) ([https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)),
    which handle the lifecycle and networking of containers within a cluster.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面详细介绍 Service API 和网络策略。K8s 网络模型的一些关键组件包括 **Kubelet** ([https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/))、**容器运行时接口**
    (**CRI**) ([https://kubernetes.io/docs/concepts/architecture/cri/](https://kubernetes.io/docs/concepts/architecture/cri/))
    和 **容器网络接口** (**CNI**) ([https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/))，它们负责集群中容器的生命周期管理和网络连接。
- en: '**Kubelet**: The kubelet is an agent running on each worker node in a K8s cluster
    that ensures that the containers described by the K8s API are running properly
    on the node. The kubelet interacts with the CRI to start, stop, and monitor containers
    based on the configurations set in Pod specifications.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubelet**：Kubelet 是在 K8s 集群中每个工作节点上运行的一个代理，它确保 K8s API 描述的容器在节点上正确运行。Kubelet
    与 CRI 交互，根据 Pod 规格中设置的配置启动、停止和监控容器。'
- en: '**CRI**: The CRI is an API that allows the kubelet to communicate with different
    container runtimes in a standardized way by abstracting the underlying container
    runtime such as Containerd or CRI-O.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CRI**：CRI 是一个 API，允许 Kubelet 通过抽象底层容器运行时（如 Containerd 或 CRI-O），以标准化的方式与不同的容器运行时进行通信。'
- en: '**CNI**: The CNI is an open source API specification designed with simplicity
    and modularity in mind. It allows K8s to handle container networking in a unified,
    plug-and-play manner by using any CNI-compatible plugin.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CNI**：CNI 是一个开源的 API 规范，设计时考虑了简洁性和模块化。它允许 K8s 通过使用任何兼容 CNI 的插件，以统一的即插即用方式处理容器网络。'
- en: When a Pod is scheduled to a worker node, Kubelet instructs the CRI to create
    containers for the Pod. Once the containers are ready, Kubelet invokes the CNI
    plugin to set up the Pod network – attaching network interfaces, assigning IP
    addresses, configuring routing, and ensuring network policies are enforced. This
    allows Pods to seamlessly communicate with each other and with external networks,
    adhering to the K8s network model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个 Pod 被调度到工作节点时，Kubelet 会指示 CRI 为该 Pod 创建容器。一旦容器准备好，Kubelet 调用 CNI 插件来设置 Pod
    网络——附加网络接口、分配 IP 地址、配置路由并确保网络策略得到执行。这使得 Pod 之间以及与外部网络的通信无缝进行，遵循 K8s 网络模型。
- en: There are several CNI plugins available for K8s, each with a unique set of features
    and strengths. Some of the popular CNI plugins are **Calico** ([https://www.tigera.io/project-calico/](https://www.tigera.io/project-calico/)),
    **Cilium** ([https://github.com/cilium/cilium](https://github.com/cilium/cilium)),
    **Weave Net** ([https://github.com/weaveworks/weave](https://github.com/weaveworks/weave)),
    **Antrea** ([https://antrea.io/](https://antrea.io/)), and **Amazon VPC CNI**
    ([https://github.com/aws/amazon-vpc-cni-k8s](https://github.com/aws/amazon-vpc-cni-k8s)).
    Refer to the K8s documentation at [https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy](https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy)
    for a detailed list.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: K8s 有多个可用的 CNI 插件，每个插件都有独特的功能和优势。一些流行的 CNI 插件包括**Calico**（[https://www.tigera.io/project-calico/](https://www.tigera.io/project-calico/)）、**Cilium**（[https://github.com/cilium/cilium](https://github.com/cilium/cilium)）、**Weave
    Net**（[https://github.com/weaveworks/weave](https://github.com/weaveworks/weave)）、**Antrea**（[https://antrea.io/](https://antrea.io/)）和**Amazon
    VPC CNI**（[https://github.com/aws/amazon-vpc-cni-k8s](https://github.com/aws/amazon-vpc-cni-k8s)）。有关详细列表，请参阅
    K8s 文档：[https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy](https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy)。
- en: Other important K8s networking components are **IP Address Management** (**IPAM**),
    which is used by the CNI plugin to assign and manage IP addresses for Pods within
    a K8s cluster, and **IPTables** ([https://man7.org/linux/man-pages/man8/iptables.8.html](https://man7.org/linux/man-pages/man8/iptables.8.html)),
    which is responsible for packet filtering and is part of the Linux kernel. In
    K8s, components like kube-proxy and certain CNI network plugins use **IPTables**
    to manage network rules and direct traffic within worker nodes. These IPTables
    rules enable Pods to communicate with each other within the cluster, manage external
    traffic flows, and, depending on the network plugin, help implement network policies.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其他重要的 K8s 网络组件包括**IP 地址管理**（**IPAM**），CNI 插件使用它为 K8s 集群中的 Pod 分配和管理 IP 地址，以及**IPTables**（[https://man7.org/linux/man-pages/man8/iptables.8.html](https://man7.org/linux/man-pages/man8/iptables.8.html)），它负责数据包过滤，并是
    Linux 内核的一部分。在 K8s 中，像 kube-proxy 和某些 CNI 网络插件这样的组件使用**IPTables**来管理网络规则并在工作节点内部进行流量引导。这些
    IPTables 规则使 Pod 能够在集群内相互通信、管理外部流量，并根据网络插件的不同帮助实现网络策略。
- en: '*Figure 8**.1* shows how the kubelet, the CNI, IPAM, and IPTables work together
    within a K8s worker node to set up and manage networking for a Pod:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.1* 显示了 kubelet、CNI、IPAM 和 IPTables 如何在 K8s 工作节点内协同工作，以设置和管理 Pod 的网络：'
- en: In step 1, Kubelet communicates with the CNI plugin to request the creation
    and configuration of a Pod’s network.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第 1 步中，Kubelet 与 CNI 插件进行通信，请求创建和配置 Pod 的网络。
- en: In step 2, the CNI plugin creates a network namespace and calls the IPAM module
    to reserve an IP address for the Pod.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第 2 步中，CNI 插件创建一个网络命名空间，并调用 IPAM 模块为 Pod 保留一个 IP 地址。
- en: In step 3, the CNI plugin configures IPTables to manage network traffic rules
    for the Pod. This step ensures that the Pod can communicate with other Pods and
    external networks according to the cluster’s networking policies.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第 3 步中，CNI 插件配置 IPTables 以管理 Pod 的网络流量规则。此步骤确保 Pod 能够根据集群的网络策略与其他 Pod 和外部网络进行通信。
- en: In the last step, the allocated IP address (e.g., 10.0.0.12) is assigned to
    the Pod. This makes the Pod reachable within the cluster using the assigned IP
    address.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最后一步，分配的 IP 地址（例如，10.0.0.12）被分配给 Pod。这使得 Pod 可以通过分配的 IP 地址在集群内进行访问。
- en: '![Figure 8.1 – IP allocation flow in a worker node](img/B31108_08_1.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – 工作节点中的 IP 分配流程](img/B31108_08_1.jpg)'
- en: Figure 8.1 – IP allocation flow in a worker node
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 工作节点中的 IP 分配流程
- en: Selecting the CNI networking mode for GenAI applications
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择 GenAI 应用程序的 CNI 网络模式
- en: When choosing a CNI plugin, it is important to understand the differences between
    **overlay networks** and **native networking**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择 CNI 插件时，了解**覆盖网络**和**本地网络**之间的区别是非常重要的。
- en: CNI plugins that use an overlay network create an additional layer of abstraction
    over the existing network, encapsulating traffic in tunnels to isolate Pod networks
    and simplify routing across nodes. While this provides flexibility and network
    segmentation, it often comes at the cost of higher latency and lower throughput
    due to the encapsulation overhead.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用覆盖网络的 CNI 插件在现有网络上创建了一个额外的抽象层，通过隧道封装流量，以隔离 Pod 网络并简化节点之间的路由。虽然这提供了灵活性和网络分段，但通常会因为封装开销而导致更高的延迟和较低的吞吐量。
- en: Native networking plugins, such as the Amazon VPC CNI and Cilium, integrate
    directly with the underlying infrastructure’s routing, allowing Pods to communicate
    using real network interfaces and IP addresses without encapsulation. This integration
    can result in higher throughput and lower latency, making native networking an
    optimal choice for applications that require high performance. For GenAI workloads,
    where rapid data transfer and minimal network latency are critical for effective
    model training and inference, native networking CNI plugins are usually recommended.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 原生网络插件，如 Amazon VPC CNI 和 Cilium，直接与底层基础设施的路由进行集成，允许 Pod 使用真实的网络接口和 IP 地址进行通信，而无需封装。这种集成可以带来更高的吞吐量和更低的延迟，使得原生网络成为需要高性能的应用的最佳选择。对于
    GenAI 工作负载，快速的数据传输和最小的网络延迟对于有效的模型训练和推理至关重要，因此通常建议使用原生网络 CNI 插件。
- en: Service implementation in K8s
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K8s 中的服务实现
- en: 'A **Service** is a K8s resource that provides network endpoint and load balancing
    across a set of Pods, making it easy to expose an application to other services
    or external clients. Here are the key features of K8s Service implementation:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**服务**是 K8s 资源，提供网络端点和负载均衡功能，能够跨一组 Pod 实现负载均衡，简化将应用暴露给其他服务或外部客户端的过程。以下是 K8s
    服务实现的主要特点：'
- en: K8s Services provide a consistent IP and DNS name for a set of Pods, so even
    as Pods get created or destroyed, service performance isn’t affected.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K8s 服务为一组 Pod 提供一致的 IP 地址和 DNS 名称，因此即使 Pod 被创建或销毁，服务性能也不会受到影响。
- en: Services automatically distribute incoming traffic to the available Pods based
    on their labels. This helps with load balancing and ensures availability.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务会根据 Pod 的标签自动分配传入流量到可用的 Pod，从而实现负载均衡并确保可用性。
- en: 'K8s supports the following four different types of services:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K8s 支持以下四种不同类型的服务：
- en: '`aws-load-balancer-controller` ([https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/](https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/))
    addon to expose our GenAI models using the AWS **Network Load Balance**r (**NLB**)
    in [*Chapter 3*](B31108_03.xhtml#_idTextAnchor039).'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aws-load-balancer-controller` ([https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/](https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/))
    插件，用于通过 AWS **网络负载均衡器**（**NLB**）在 [*第 3 章*](B31108_03.xhtml#_idTextAnchor039)中公开我们的
    GenAI 模型。'
- en: '**ExternalName** maps the service to an external DNS name, allowing K8s services
    to connect to an external service outside the cluster.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ExternalName** 将服务映射到外部 DNS 名称，允许 K8s 服务连接到集群外部的外部服务。'
- en: GenAI applications often require scalable, low latency, and efficient networking
    to serve the customers effectively. K8s’ **LoadBalancer** Service can be used
    to expose models outside the cluster. It provisions an external load balancer
    through the underlying cloud provider (such as AWS, Azure, or GCP). This setup
    enables seamless distribution of incoming traffic across multiple Pods, ensuring
    high availability and scalability. On the other hand, **ClusterIP** service can
    be utilized when the GenAI models are accessed only within the cluster by other
    K8s Pods. It assigns a friendly service lookup name and an IP address for each
    service, facilitating reliable service discovery and communication between Pods
    without exposing them externally.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: GenAI 应用通常需要可扩展、低延迟和高效的网络，以有效地为客户提供服务。K8s 的 **LoadBalancer** 服务可以用于将模型暴露到集群外部。它通过底层云提供商（如
    AWS、Azure 或 GCP）配置外部负载均衡器。这种设置可以实现将传入流量无缝分配到多个 Pod，确保高可用性和可扩展性。另一方面，**ClusterIP**
    服务可以在仅限集群内的其他 K8s Pod 访问 GenAI 模型时使用。它为每个服务分配一个友好的服务查找名称和 IP 地址，从而促进 Pod 之间的可靠服务发现和通信，而无需将它们暴露给外部。
- en: By default, when using the K8s `aws-load-balancer-controller`, a **NodePort**
    Service is created to forward traffic from the AWS NLB to the K8s worker nodes.
    From there, kube-proxy routes the traffic to the individual Pods, introducing
    an additional network hop that can increase the latency and throughput. To reduce
    this overhead, you can configure the LoadBalancer service to route traffic directly
    to K8s Pods by registering the Pods as NLB targets.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，使用 K8s 的 `aws-load-balancer-controller` 时，会创建一个 **NodePort** 服务，将来自 AWS
    NLB 的流量转发到 K8s 工作节点。从那里，kube-proxy 将流量路由到各个 Pod，引入了额外的网络跳跃，这可能会增加延迟和吞吐量。为了减少这种开销，可以配置
    LoadBalancer 服务，将流量直接路由到 K8s Pod，通过将 Pod 注册为 NLB 目标。
- en: 'This can be achieved by adding the `service.beta.kubernetes.io/aws-load-balancer-nlb-target-type:
    ip` annotation to the K8s service, as shown here:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '这可以通过向 K8s 服务添加 `service.beta.kubernetes.io/aws-load-balancer-nlb-target-type:
    ip` 注解来实现，如下所示：'
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Service health checks
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务健康检查
- en: Another important consideration in ensuring optimal performance and reliability
    is configuring **healthcheck** settings on the LoadBalancer service type. GenAI
    models are typically resource-intensive and can have varying startup and response
    times based on input complexity and server load. Without proper health checks,
    NLB may flag the targets as unhealthy, which will lead to replacing the K8s Pods,
    degraded performance, or potential downtime.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 确保最佳性能和可靠性的另一个重要考虑因素是配置 **healthcheck** 设置在 LoadBalancer 服务类型上。GenAI 模型通常资源密集型，并且根据输入复杂性和服务器负载可能具有不同的启动和响应时间。如果没有适当的健康检查，NLB
    可能会将目标标记为不健康，这将导致替换 K8s Pods、性能下降或可能的停机。
- en: LoadBalancer health checks ensure that traffic is routed only to healthy Pods
    capable of handling requests. For instance, in a scenario where a GenAI model
    serves inference through a K8s Service of type LoadBalancer, the health check
    can monitor an endpoint such as `/healthz` on each backend Pod. This endpoint
    could return an *HTTP 200 OK* status if the model is fully loaded, the required
    resources (e.g., GPU memory) are available, and the inference service is operational.
    If a Pod fails the health check due to issues such as resource exhaustion or process
    crashes, the LoadBalancer will automatically exclude it from the pool of available
    endpoints, directing traffic to healthy Pods instead. This mechanism prevents
    request failures and ensures a seamless experience for end users while maintaining
    the overall stability and scalability of the GenAI workload.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LoadBalancer 健康检查确保流量仅路由到能够处理请求的健康 Pod。例如，在一个 GenAI 模型通过 K8s 类型为 LoadBalancer
    的服务提供推理服务的场景中，健康检查可以监控每个后端 Pod 上的 `/healthz` 端点。如果模型已完全加载、所需的资源（例如 GPU 内存）可用且推理服务正常运行，该端点可能会返回
    *HTTP 200 OK* 状态。如果由于资源耗尽或进程崩溃等问题导致某个 Pod 未通过健康检查，LoadBalancer 会自动将其从可用端点池中排除，将流量引导到健康的
    Pod 上。这种机制可以防止请求失败，确保终端用户的无缝体验，同时保持 GenAI 工作负载的整体稳定性和可扩展性。
- en: Refer to the `aws-load-balancer-controller` documentation at [https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/guide/service/annotations/#health-check](https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/guide/service/annotations/#health-check)
    for various health check settings.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅 `aws-load-balancer-controller` 文档，网址为 [https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/guide/service/annotations/#health-check](https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/guide/service/annotations/#health-check)，了解各种健康检查设置。
- en: While K8s LoadBalancer Services provide a simple and straightforward way to
    expose K8s workloads externally, they can be limited in more complex routing scenarios.
    Ingress and Gateway APIs provide greater flexibility and control over traffic
    flow, enabling features such as path or host-based routing or TLS termination.
    In the next section, we will cover the Ingress and Gateway APIs, which handle
    the input traffic for K8s workloads.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 K8s LoadBalancer 服务提供了一种简单直接的方式来将 K8s 工作负载暴露到外部，但在更复杂的路由场景中，它们可能会受到限制。Ingress
    和 Gateway API 提供了更大的灵活性和流量控制，支持路径或基于主机的路由、TLS 终止等功能。在下一节中，我们将介绍 Ingress 和 Gateway
    API，它们负责处理 K8s 工作负载的输入流量。
- en: Ingress controller
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Ingress 控制器
- en: In K8s, **Ingress** API ([https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/))
    is used to expose applications outside the cluster using HTTP/S protocols. It
    acts as a routing layer to direct incoming requests to K8s applications based
    on HTTP URL paths, hostnames, headers, and so on. An Ingress controller is responsible
    for provisioning required infrastructure resources such as Application Load Balancers
    (in AWS), configuring routing rules, and terminating SSL connections, to fulfill
    the Ingress resources. Some of the popular **Ingress controllers** ([https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/))
    include **ingress-nginx**, **aws-lb-controller**, **HAProxy Ingress**, and **Istio
    Ingress**. Refer to the K8s documentation at [https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers)
    for a detailed list. When deploying GenAI workloads in a K8s cluster, you should
    consider Ingress when exposing multiple applications/models under one entry point
    (domain name) as shown in *Figure 8**.2*, which typically requires centralized
    traffic routing. Additionally, Ingress supports advanced routing features such
    as path-based and host-based routing, enabling you to direct specific requests
    to different model versions or applications based on URLs and HTTP headers, which
    is useful for **A/B testing** ([https://aws.amazon.com/developer/application-security-performance/articles/a-b-testing/](https://aws.amazon.com/developer/application-security-performance/articles/a-b-testing/))
    or **canary releases**, where new application changes are gradually rolled out
    to a small set of users, allowing teams to monitor performance and catch issues
    before full-scale deployment. Apart from routing, Ingress controllers also integrate
    with monitoring tools such as **Prometheus** to provide detailed metrics such
    as latencies, request rates, and error rates.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在K8s中，**Ingress** API（[https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/)）用于通过HTTP/S协议将应用程序暴露到集群外部。它作为一个路由层，依据HTTP
    URL路径、主机名、头信息等，将传入请求定向到K8s应用程序。Ingress控制器负责配置所需的基础设施资源，如应用负载均衡器（在AWS中），配置路由规则，并终止SSL连接，以满足Ingress资源的需求。一些流行的**Ingress控制器**（[https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)）包括**ingress-nginx**、**aws-lb-controller**、**HAProxy
    Ingress**和**Istio Ingress**。有关详细的控制器列表，请参阅K8s文档：[https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers)。在K8s集群中部署GenAI工作负载时，您应该考虑使用Ingress来将多个应用程序/模型暴露在一个入口点（域名）下，如*图8.2*所示，这通常需要集中式流量路由。此外，Ingress支持高级路由功能，如基于路径和基于主机的路由，使您能够根据URL和HTTP头将特定请求定向到不同的模型版本或应用程序，这对于**A/B测试**（[https://aws.amazon.com/developer/application-security-performance/articles/a-b-testing/](https://aws.amazon.com/developer/application-security-performance/articles/a-b-testing/)）或**金丝雀发布**非常有用，后者是将新的应用程序变更逐步推出给少数用户，允许团队在全面部署之前监控性能并解决问题。除了路由外，Ingress控制器还与监控工具如**Prometheus**集成，以提供详细的指标，如延迟、请求率和错误率。
- en: '![Figure 8.2 – Overview of Ingress](img/B31108_08_2.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – Ingress概述](img/B31108_08_2.jpg)'
- en: Figure 8.2 – Overview of Ingress
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – Ingress概述
- en: The **Gateway API** ([https://gateway-api.sigs.k8s.io/](https://gateway-api.sigs.k8s.io/))
    was created to address the limitations of Ingress, offering a more flexible, extensible,
    and standardized approach to managing traffic in K8s clusters, especially for
    complex networking needs. It is the official K8s project focused on L4 and L7
    routing, representing the next generation of Ingress, Load Balancing, and Service
    Mesh APIs. Like Ingress, you need to install **Gateway Controller** to provision
    necessary infrastructure resources along with advanced routing features. There
    are many implementations of this API, refer to K8s documentation at [https://gateway-api.sigs.k8s.io/implementations/](https://gateway-api.sigs.k8s.io/implementations/)
    for a complete list.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**网关API**（[https://gateway-api.sigs.k8s.io/](https://gateway-api.sigs.k8s.io/)）旨在解决Ingress的局限性，提供一种更灵活、可扩展和标准化的方式来管理K8s集群中的流量，特别是对于复杂的网络需求。它是K8s的官方项目，专注于L4和L7路由，代表了Ingress、负载均衡和服务网格API的下一代。像Ingress一样，您需要安装**网关控制器**来提供必要的基础设施资源，并支持高级路由功能。此API有许多实现，完整的实现列表请参考K8s文档：[https://gateway-api.sigs.k8s.io/implementations/](https://gateway-api.sigs.k8s.io/implementations/)。'
- en: In this section, we learned about foundational concepts of K8s networking, core
    tenets, and how it is different from other orchestrators. We discussed the role
    of the CNI plugin to configure the Pod networking, IPAM to manage the IP addressing,
    and considerations when choosing a CNI networking mode. We explored various K8s
    services, Ingress, and the Gateway API to expose GenAI models outside of the cluster.
    In the next section, we will discuss advanced application networking constructs
    such as service mesh.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了K8s网络的基础概念、核心原则，以及它与其他编排工具的不同之处。我们讨论了CNI插件在配置Pod网络中的作用，IPAM用于管理IP地址分配，以及选择CNI网络模式时需要考虑的因素。我们还探讨了各种K8s服务、Ingress和网关API，以便将GenAI模型暴露到集群外部。在下一节中，我们将讨论高级应用程序网络构造，如服务网格。
- en: Advanced traffic management with a service mesh
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级流量管理与服务网格
- en: As GenAI applications grow in complexity, the number of services involved such
    as model inferencing, data ingestion, data processing, fine-tuning, and training
    also increases the complexity of managing service-to-service communication. These
    typically include traffic management (load balancing, retries, rate limiting),
    security (authentication, authorization, encryption), and observability (logs,
    metrics, traces).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 随着GenAI应用程序复杂度的增长，涉及的服务数量也在增加，例如模型推理、数据摄取、数据处理、微调和训练，这也增加了管理服务间通信的复杂性。通常包括流量管理（负载均衡、重试、速率限制）、安全性（身份验证、授权、加密）和可观察性（日志、度量、跟踪）。
- en: A **service mesh** is an infrastructure layer that enables reliable, secure,
    and observable communication between microservices. It abstracts the complexity
    of service-to-service communication, including traffic management, load balancing,
    security, and observability, without requiring changes in application code.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**服务网格**是一个基础设施层，它使微服务之间的通信变得可靠、安全和可观察。它抽象了服务间通信的复杂性，包括流量管理、负载均衡、安全性和可观察性，无需修改应用程序代码。'
- en: Service mesh typically provides these features by deploying a sidecar proxy
    such as **Envoy** ([https://www.envoyproxy.io/](https://www.envoyproxy.io/)) alongside
    the application, which intercepts all network traffic in and out of the application.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格通常通过部署边车代理，如**Envoy**（[https://www.envoyproxy.io/](https://www.envoyproxy.io/)），与应用程序一起使用，拦截所有进出应用程序的网络流量。
- en: 'The service mesh has two key components:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格有两个关键组件：
- en: '**Control plane**, which manages the sidecar proxies, distributing configuration
    and policies across the side car proxies and gathering telemetry data for centralized
    control and monitoring. Different kinds of policies can be implemented in service
    mesh, such as the following:'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制平面**，负责管理边车代理，分发配置和策略到各个边车代理，并收集遥测数据以便集中控制和监控。服务网格中可以实现不同类型的策略，如下所示：'
- en: '**Traffic management policies**, which define rules for how traffic should
    be routed, circuit-breaking rules, load balancing, and retry options'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流量管理策略**，定义了流量应如何路由、断路规则、负载均衡和重试选项'
- en: '**Security policies**, which can establish rules for encryption (mTLS), authentication
    (e.g., JWT tokens), and authorization (RBAC)'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全策略**，可以建立加密（如mTLS）、身份验证（例如JWT令牌）和授权（RBAC）等规则'
- en: '**Resilience and fault tolerance policies**, which can define retry mechanisms,
    timeouts, and failover options'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性和容错策略**，可以定义重试机制、超时和故障切换选项。'
- en: '**Data plane**, which combines sidecar proxies and is responsible for the actual
    network traffic between services. Each sidecar can apply policies independently,
    ensuring that each service instance respects the network rules and policies set
    at the control plane. **Sidecar containers** are secondary containers that run
    alongside the main application container within the same Pod. They complement
    the primary container by offering additional capabilities, such as logging, monitoring,
    security, or data synchronization, enhancing the application’s functionality without
    requiring changes to its code.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据平面**，它结合了边车代理，负责服务之间的实际网络流量。每个边车可以独立应用策略，确保每个服务实例遵守在控制平面设置的网络规则和策略。**边车容器**是与主应用容器一起在同一Pod内运行的辅助容器。它们通过提供额外的功能，如日志记录、监控、安全性或数据同步，来补充主容器的功能，无需更改应用程序代码。'
- en: Some popular service mesh implementations in K8s are **Istio**, **Linkerd**,
    and **Ambient Mesh**. While traditional meshes such as Istio and Linkerd rely
    on sidecars, Ambient Mesh recently introduced by Istio offers a sidecarless architecture
    that can leverage eBPF for efficient traffic management and security. *Figure
    8**.3* shows how a service mesh implementation can route the traffic in and out
    of the mesh, intercept requests, and implement end-to-end TLS encryption or mTLS
    using sidecar proxies.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一些在K8s中流行的服务网格实现包括**Istio**、**Linkerd**和**Ambient Mesh**。传统的网格如Istio和Linkerd依赖于边车，而Istio最近推出的Ambient
    Mesh则提供了一种无边车的架构，可以利用eBPF进行高效的流量管理和安全性。*图8.3*展示了服务网格实现如何在网格内外路由流量、拦截请求，并使用边车代理实现端到端的TLS加密或mTLS。
- en: '![Figure 8.3 – Service mesh implementation](img/B31108_08_3.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – 服务网格实现](img/B31108_08_3.jpg)'
- en: Figure 8.3 – Service mesh implementation
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 服务网格实现
- en: This figure shows a service mesh architecture, where traffic enters through
    the Ingress Gateway, flows between Pods via sidecar proxies, and exits through
    the Egress Gateway. The Service Mesh Control Plane centrally configures and manages
    the sidecar proxies to enforce traffic policies and security. This setup enables
    efficient and secure microservices communication within the cluster. Refer to
    the *Getting Started with Istio on Amazon EKS* article at [https://aws.amazon.com/blogs/opensource/getting-started-with-istio-on-amazon-eks/](https://aws.amazon.com/blogs/opensource/getting-started-with-istio-on-amazon-eks/)
    for step-by-step instructions to deploy Istio service mesh on Amazon EKS.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了一个服务网格架构，流量通过入口网关进入，经过边车代理在Pods之间流动，然后通过出口网关离开。服务网格控制平面集中配置和管理边车代理，以执行流量策略和安全性。这个设置使得集群内微服务的通信更加高效和安全。参考[https://aws.amazon.com/blogs/opensource/getting-started-with-istio-on-amazon-eks/](https://aws.amazon.com/blogs/opensource/getting-started-with-istio-on-amazon-eks/)中的*在Amazon
    EKS上使用Istio入门*文章，按步骤说明如何在Amazon EKS上部署Istio服务网格。
- en: In this section, we explored the advanced traffic management features of a service
    mesh such as load balancing, security, observability, and a high-level architectural
    overview. In the next section, we will dive deeper into securing GenAI workloads
    using K8s native network policies.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们探讨了服务网格的高级流量管理特性，如负载均衡、安全性、可观察性以及高层次的架构概览。在下一节中，我们将深入探讨如何使用K8s原生网络策略保护GenAI工作负载。
- en: Securing GenAI workloads with Kubernetes’ network policies
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kubernetes网络策略保护GenAI工作负载
- en: '**Network policies** are a native K8s feature that controls ingress (incoming)
    and egress (outgoing) traffic between Pods within a cluster. They are implemented
    through the K8s **NetworkPolicy API** and allow administrators to define rules
    that determine which Pods or IP addresses can communicate with each other.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络策略**是K8s的原生功能，用于控制集群内Pods之间的流入（入站）和流出（出站）流量。它们通过K8s的**NetworkPolicy API**实现，允许管理员定义规则来决定哪些Pods或IP地址可以相互通信。'
- en: Unlike a service mesh, which provides advanced traffic management and security
    features, network policies focus on traffic isolation and network segmentation
    for security purposes, such as namespace isolation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与提供高级流量管理和安全功能的服务网格不同，网络策略侧重于流量隔离和网络分段，目的是出于安全考虑，例如命名空间隔离。
- en: 'By default, all Pods in a K8s cluster can communicate with each other; however,
    network policies can restrict this and allow fine-grained control over network
    traffic. This is especially useful in multi-tenant clusters, where different teams
    or applications require isolation for security or compliance. Some key features
    of network policies are as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，K8s集群中的所有Pod都可以互相通信；然而，网络策略可以限制这一点，并允许对网络流量进行细粒度控制。这在多租户集群中尤其有用，其中不同的团队或应用需要出于安全或合规性的原因进行隔离。网络策略的一些关键特性如下：
- en: '**Ingress and egress rules**: *Ingress rules* define which sources are allowed
    to communicate with a specific Pod or set of Pods. Similarly, *egress rules* define
    which destinations a Pod or set of Pods can connect to.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**入口和出口规则**：*入口规则*定义了哪些源可以与特定Pod或一组Pod进行通信。类似地，*出口规则*定义了Pod或一组Pod可以连接到哪些目的地。'
- en: '`app: backend` can be configured to allow ingress only from Pods with the label
    `app: frontend`.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`app: backend`可以配置为仅允许来自具有标签`app: frontend`的Pod的入口流量。'
- en: '**Deny-by-default model**: By default, if there are no network policies applied,
    all traffic is allowed. However, once a network policy is applied to a Pod, only
    the traffic that matches the specified policy rules is allowed.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**默认拒绝模型**：默认情况下，如果没有应用网络策略，则允许所有流量。然而，一旦网络策略应用于Pod，只有符合指定策略规则的流量才被允许。'
- en: 'Here is an example of a network policy that allows incoming traffic to Pods
    with the label `app: backend` only from other Pods with the label `app: frontend`
    in the same namespace. All other ingress traffic is denied by default for these
    Pods:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '这是一个网络策略示例，它仅允许来自同一命名空间中具有标签`app: frontend`的其他Pod的流量进入带有标签`app: backend`的Pod。所有其他入口流量默认被拒绝：'
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, let’s explore how to implement K8s network policies to secure traffic
    flows among different components of our e-commerce chatbot application.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探讨如何实现K8s网络策略，以确保电子商务聊天机器人应用不同组件之间的流量安全。
- en: Implementing network policies in a chatbot application
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在聊天机器人应用中实现网络策略
- en: In [*Chapter 5*](B31108_05.xhtml#_idTextAnchor062), we deployed a chatbot application
    in the EKS cluster that comprises four components (chatbot-ui, vector database,
    RAG application, and fine-tuned Llama3 model), as depicted in *Figure 8**.4*.
    By default, all components can talk to each other on any ports/protocols, which
    is not a security best practice. Our goal is to implement network segmentation
    so that only trusted components can communicate with each other on approved ports
    and protocols.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第5章*](B31108_05.xhtml#_idTextAnchor062)中，我们在EKS集群中部署了一个包含四个组件（chatbot-ui、向量数据库、RAG应用和微调的Llama3模型）的聊天机器人应用，如*图8.4*所示。默认情况下，所有组件都可以在任何端口/协议上相互通信，这不是一个安全的最佳实践。我们的目标是实现网络分段，以便只有受信任的组件才能在批准的端口和协议上相互通信。
- en: '![Figure 8.4 – E-commerce chatbot application architecture](img/B31108_08_4.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – 电子商务聊天机器人应用架构](img/B31108_08_4.jpg)'
- en: Figure 8.4 – E-commerce chatbot application architecture
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 电子商务聊天机器人应用架构
- en: In this setup, the Chatbot UI application communicates with the RAG application
    and the fine-tuned Llama 3 model over HTTP on port 80\. So, let’s create an Ingress
    network policy on both RAG and Llama 3 applications to allow only HTTP/80 ingress
    traffic from the chatbot UI app to secure the network traffic flow. We can use
    the labels applied to respective K8s deployments to create the network policy.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在此设置中，聊天机器人UI应用通过HTTP协议在端口80上与RAG应用和微调的Llama 3模型进行通信。因此，我们在RAG和Llama 3应用上创建一个入口网络策略，仅允许来自聊天机器人UI应用的HTTP/80入口流量，以确保网络流量的安全。我们可以使用分别应用于K8s部署的标签来创建网络策略。
- en: 'The following network policy selects the RAG application Pods using the `app.kubernetes.io/name:
    rag-app` label and applies an ingress rule to allow HTTP/80 traffic only from
    Pods identified by the `app.kubernetes.io/name=chatbot-ui` label:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '以下网络策略选择了带有`app.kubernetes.io/name: rag-app`标签的RAG应用Pod，并应用入口规则，仅允许来自由`app.kubernetes.io/name=chatbot-ui`标签标识的Pod的HTTP/80流量：'
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also create another network policy for a fine-tuned Llama 3 application
    to allow HTTP/80 traffic only from the chatbot UI app by using the `app.kubernetes.io/name:
    my-llama-finetuned`, `app.kubernetes.io/name:` `chatbot-ui` labels.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还可以为微调的Llama 3应用创建另一个网络策略，仅允许来自聊天机器人UI应用的HTTP/80流量，通过使用`app.kubernetes.io/name:
    my-llama-finetuned`、`app.kubernetes.io/name: chatbot-ui`标签。'
- en: 'Similarly, the RAG application communicates with the vector database over HTTP
    on port 6333\. To restrict inbound traffic, we can create a network policy that
    applies to the Pods labeled `app.kubernetes.io/name: qdrant`, allowing HTTP/6333
    traffic only from Pods labeled `app.kubernetes.io/name: rag-app`.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '类似地，RAG 应用程序通过 HTTP 在端口 6333 上与向量数据库通信。为了限制入站流量，我们可以创建一个网络策略，适用于标记为 `app.kubernetes.io/name:
    qdrant` 的 Pod，仅允许来自标记为 `app.kubernetes.io/name: rag-app` 的 Pod 的 HTTP/6333 流量。'
- en: You can find all the network policies in the GitHub repository at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch8](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch8).
    You can download and apply them in the EKS cluster using the `kubectl apply` command.
    In this walkthrough, we focused on configuring Ingress rules to restrict the inbound
    traffic. To further tighten security, you can extend these policies to include
    egress rules for the outbound traffic. However, if your application performs DNS
    lookups on K8s Services, be sure to allow DNS traffic to the `kube-system` namespace.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 GitHub 代码库中找到所有网络策略，网址为 [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch8](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch8)。您可以使用
    `kubectl apply` 命令在 EKS 集群中下载并应用这些策略。在本操作指南中，我们重点配置入站流量的 Ingress 规则。为了进一步加强安全性，您可以扩展这些策略以包括对出站流量的
    egress 规则。然而，如果您的应用程序在 K8s 服务上执行 DNS 查询，请务必允许 DNS 流量进入 `kube-system` 命名空间。
- en: By default, upstream K8s network policies support only a limited set of rules
    to define traffic flows (primarily IP address, port, protocol, podSelector, and
    namespaceSelector) and do not support domain-based rules or cluster or global
    policies. This can be insufficient for GenAI applications when using external
    APIs such as OpenAI or Claude. To address these gaps, **Cilium** ([https://docs.cilium.io/en/stable/security/](https://docs.cilium.io/en/stable/security/)),
    **Calico** ([https://docs.tigera.io/calico/latest/network-policy/](https://docs.tigera.io/calico/latest/network-policy/))
    and others offer advanced capabilities such as DNS-based policies, which allow
    specifying fully qualified domain names for dynamic policy enforcement, and global
    (cluster-wide) policies, which ensure a uniform security posture across all namespaces.
    These features simplify policy management, strengthen cluster-wide data governance,
    and maintain consistent traffic controls.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，上游的 K8s 网络策略仅支持有限的规则来定义流量流向（主要包括 IP 地址、端口、协议、podSelector 和 namespaceSelector），不支持基于域的规则或集群或全局策略。当使用外部
    API（如 OpenAI 或 Claude）时，这可能不足够支持 GenAI 应用程序。为了弥补这些差距，**Cilium** ([https://docs.cilium.io/en/stable/security/](https://docs.cilium.io/en/stable/security/))、**Calico**
    ([https://docs.tigera.io/calico/latest/network-policy/](https://docs.tigera.io/calico/latest/network-policy/))
    等提供了高级功能，如基于 DNS 的策略，允许指定动态策略强制执行的完全限定域名，以及全局（集群范围）策略，确保所有命名空间间的统一安全姿态。这些特性简化了策略管理，增强了集群范围的数据治理，并保持了一致的流量控制。
- en: To summarize, K8s network policies help define traffic rules and segmentation
    at the network and transport layers of the OSI model but lack advanced features
    for application layer traffic management and observability. In the next section,
    we will compare network policies with service mesh technology, highlighting how
    these solutions differ in their key features.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，K8s 网络策略有助于定义 OSI 模型的网络和传输层上的流量规则和分段，但在应用层流量管理和可观察性方面缺乏高级功能。在接下来的部分中，我们将比较网络策略与服务网格技术，突出这些解决方案在关键特性上的不同。
- en: Service mesh versus K8s network policies
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务网格与 K8s 网络策略
- en: 'While both service mesh and K8s network policies help to secure and manage
    K8s networking, they serve different purposes and often complement each other:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然服务网格和 K8s 网络策略都有助于保护和管理 K8s 网络，但它们的用途不同，通常互补：
- en: '| **Feature** | **Service Mesh** | **K8s** **Network Policies** |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **特性** | **服务网格** | **K8s 网络策略** |'
- en: '| --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Key Focus** | Observability, traffic management (such as load balancing),
    retries, and security (mTLS support) | Security and traffic isolation using namespaces
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| **主要焦点** | 可观察性、流量管理（如负载均衡）、重试和安全性（支持 mTLS） | 使用命名空间进行安全性和流量隔离 |'
- en: '| **Traffic Routing** | Advanced routing and load balancing | Basic |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| **流量路由** | 高级路由和负载均衡 | 基本 |'
- en: '| **OSI Layer** | Primarily at Layer 7 (application layer) | Primarily Layer
    3 and 4 (network and transport layer) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| **OSI 层** | 主要在第 7 层（应用层） | 主要在第 3 和第 4 层（网络和传输层） |'
- en: '| **Mutual TLS** | Supported | Not supported |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| **双向 TLS** | 支持 | 不支持 |'
- en: '| **Complexity** | Requires sidecar proxies | Simpler, native to Kubernetes
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **复杂性** | 需要 sidecar 代理 | 更简单，原生支持 Kubernetes |'
- en: In many production environments, both service mesh and K8s network policies
    are deployed together to enhance security and traffic management. For instance,
    you might use K8s network policies to enforce namespace-based access restrictions,
    then use a service mesh to handle routing, retries, load balancing, and mTLS within
    the defined traffic boundaries.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多生产环境中，通常会同时部署服务网格和 K8s 网络策略，以增强安全性和流量管理。例如，您可以使用 K8s 网络策略来强制执行基于命名空间的访问限制，然后使用服务网格处理路由、重试、负载均衡和
    mTLS。
- en: Optimizing network performance for GenAI
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化 GenAI 的网络性能
- en: In this section, we will cover some important network optimizations when deploying
    GenAI workloads on K8s, such as Kube-Proxy, IP exhaustion issues, and advanced
    networking capabilities such as **Single Root Input/Output Virtualization** (**SR-IOV**)
    and **extended Berkeley Packet** **Filter** (**eBPF**).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍在 K8s 上部署 GenAI 工作负载时的一些重要网络优化，包括 Kube-Proxy、IP 地址耗尽问题，以及一些高级网络功能，如**单根输入/输出虚拟化**（**SR-IOV**）和**扩展贝尔克利数据包过滤器**（**eBPF**）。
- en: Kube-Proxy – IPTables versus IPVS
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kube-Proxy – IPTables 与 IPVS
- en: '**Kube-Proxy** ([https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/))
    is a core component of K8s that is responsible for managing networking within
    the cluster. It ensures seamless communication between services and Pods by setting
    up network rules and configuration on each node. Kube-Proxy maintains network
    rules to direct traffic to the appropriate backend Pods that serve each service,
    allowing internal cluster traffic to reach the correct destinations. By default,
    Kube-Proxy uses **IPTables** mode, which efficiently intercepts and redirects
    network requests based on IP rules, making it suitable for small and medium-sized
    clusters.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kube-Proxy** ([https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/))
    是 K8s 的核心组件，负责管理集群内的网络。它通过在每个节点上设置网络规则和配置，确保服务和 Pod 之间的无缝通信。Kube-Proxy 维护网络规则，将流量导向为每个服务提供服务的后端
    Pods，使得集群内部流量能够到达正确的目的地。默认情况下，Kube-Proxy 使用 **IPTables** 模式，它通过基于 IP 规则高效地拦截并重定向网络请求，适用于中小型集群。'
- en: However, for large-scale K8s clusters, particularly those running data-intensive
    workloads such as GenAI, IPTables mode might become a performance bottleneck.
    As clusters scale up to include hundreds or thousands of services and endpoints,
    maintaining and updating these rules in IPTables can lead to increased latency
    and reduced network performance, impacting the overall efficiency of AI models.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于大规模的 K8s 集群，特别是那些运行数据密集型工作负载（如 GenAI）的集群，IPTables 模式可能成为性能瓶颈。随着集群规模的扩大，服务和端点达到数百或数千个时，维护和更新这些
    IPTables 规则可能导致延迟增加和网络性能下降，进而影响 AI 模型的整体效率。
- en: To address these limitations, K8s offers the option to run Kube-Proxy in **IP
    Virtual Server** (**IPVS**) mode. IPVS provides advanced load-balancing capabilities
    by leveraging the Linux kernel’s IPVS module, which is more scalable and efficient
    than IPTables for handling high traffic. IPVS maintains an in-memory hash table
    for Service-to-Pod routing, enabling faster packet processing with minimal CPU
    overhead.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些限制，K8s 提供了在 **IP 虚拟服务器**（**IPVS**）模式下运行 Kube-Proxy 的选项。IPVS 通过利用 Linux
    内核的 IPVS 模块提供了先进的负载均衡功能，它比 IPTables 更具可扩展性和高效性，能够处理高流量。IPVS 维护一个内存中的哈希表用于服务到 Pod
    的路由，使得数据包处理速度更快且 CPU 开销最小。
- en: IPVS mode offers benefits such as more sophisticated load balancing algorithms
    (e.g., round-robin, least connections, source hashing), better handling of dynamic
    and large service environments, and reduced latency in packet forwarding.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: IPVS 模式提供了诸多优势，如更精细的负载均衡算法（例如轮询、最少连接数、源地址哈希）、更好的动态和大规模服务环境处理能力，以及减少数据包转发的延迟。
- en: 'IPVS can be enabled by updating the `kube-proxy-config` `ConfigMap` in the
    `kube -``system` namespace:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过更新 `kube-proxy-config` `ConfigMap` 在 `kube-system` 命名空间中启用 IPVS：
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To enable IPVS in Amazon EKS cluster setup, take a look at the EKS documentation
    at [https://docs.aws.amazon.com/eks/latest/best-practices/ipvs.html](https://docs.aws.amazon.com/eks/latest/best-practices/ipvs.html)
    for step-by-step instructions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Amazon EKS 集群设置中启用 IPVS，请参阅 EKS 文档：[https://docs.aws.amazon.com/eks/latest/best-practices/ipvs.html](https://docs.aws.amazon.com/eks/latest/best-practices/ipvs.html)，查看逐步操作说明。
- en: While advanced networking configurations such as IPVS help enhance traffic management
    and scalability in K8s, another critical challenge for large-scale clusters lies
    in efficiently managing IP address allocation, which we will cover in the next
    section.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像 IPVS 这样的高级网络配置有助于提高 K8s 中流量管理和可扩展性，但大规模集群的另一个关键挑战在于有效管理 IP 地址分配，我们将在下一节中讨论这一问题。
- en: IP address exhaustion issues and custom networking
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IP 地址耗尽问题和自定义网络
- en: When using CNI plugins such as Amazon VPC CNI in native networking mode, each
    K8s Pod receives an IP address directly from the VPC CIDR block. This approach
    allows each Pod to be fully addressable within the VPC, providing visibility of
    the Pod IP addresses with tools such as **VPC flow logs** and other monitoring
    solutions. However, in large-scale clusters running intensive workloads such as
    GenAI, this model can lead to IP address exhaustion as each Pod consumes an IP
    from a finite VPC CIDR pool.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用像 Amazon VPC CNI 这样的 CNI 插件时，在原生网络模式下，每个 K8s Pod 直接从 VPC CIDR 块中获取 IP 地址。这种方法使每个
    Pod 在 VPC 内都能完全寻址，可以使用 **VPC 流日志** 和其他监控工具查看 Pod 的 IP 地址。然而，在运行密集型工作负载如 GenAI
    的大规模集群中，这种模式可能导致 IP 地址耗尽，因为每个 Pod 都会从有限的 VPC CIDR 池中消耗一个 IP 地址。
- en: To mitigate this, one effective solution is to use **IPv6 addressing**. IPv6
    provides an exponentially larger address space than IPv4, reducing the risk of
    IP exhaustion and allowing clusters to scale without worrying about running out
    of IP addresses. But not all organizations are ready to adopt IPV6 yet.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这一问题，一个有效的解决方案是使用 **IPv6 地址**。IPv6 提供了比 IPv4 更大的地址空间，减少了 IP 耗尽的风险，并允许集群在不担心地址用尽的情况下扩展。但并不是所有组织都准备好采用
    IPV6。
- en: 'Another way to address IP exhaustion is through VPC CNI custom networking.
    This involves enhancing the VPC design by associating additional, non-routable
    secondary CIDR blocks with the VPC and creating new subnets from these CIDRs.
    These subnets are designated specifically for Pod IP allocation, while the primary,
    routable CIDR is preserved for node IPs and other resources. We then configure
    the VPC CNI to allocate Pod IPs from these non-routable subnets, freeing up the
    primary CIDR for other networking needs and reducing the risk of IP exhaustion.
    The following link explains how custom networking can be implemented in Amazon
    EKS: [https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network-tutorial.html](https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network-tutorial.html).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 解决 IP 耗尽的另一种方法是通过 VPC CNI 自定义网络。这涉及通过将额外的、不可路由的次级 CIDR 块与 VPC 关联，并从这些 CIDR 创建新的子网来增强
    VPC 设计。这些子网专门用于 Pod IP 分配，而主要的可路由 CIDR 用于节点 IP 和其他资源。然后，我们配置 VPC CNI 从这些不可路由的子网中分配
    Pod IP，从而释放主要的 CIDR 用于其他网络需求，并减少 IP 耗尽的风险。以下链接解释了如何在 Amazon EKS 中实现自定义网络：[https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network-tutorial.html](https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network-tutorial.html)。
- en: To address networking challenges in K8s, including IP address exhaustion and
    performance optimization, it’s worth exploring advanced networking solutions,
    such as VPC CNI custom networking or IPV6\. Emerging technologies such as eBPF
    and SR-IOV offer innovative methods to improve network efficiency and scalability,
    which we will discuss in the next section.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决 K8s 中的网络挑战，包括 IP 地址耗尽和性能优化，值得探索一些高级网络解决方案，如 VPC CNI 自定义网络或 IPV6。新兴技术如 eBPF
    和 SR-IOV 提供了创新的方法来提高网络效率和可扩展性，我们将在下一节讨论这些技术。
- en: eBPF and SR-IOV
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: eBPF 和 SR-IOV
- en: '**extended Berkeley Packet Filter** (**eBPF**) ([https://ebpf.io/what-is-ebpf/](https://ebpf.io/what-is-ebpf/))
    allows advanced programmability within the Linux kernel without requiring changes
    to kernel code. This can be used to create powerful, lightweight networking, observability,
    and security solutions directly at the kernel level. For example, the Cilium CNI
    plugin, which leverages eBPF, provides fine-grained network security, load balancing,
    and observability capabilities. For GenAI workloads, where performance and data
    transfer speed are crucial, eBPF’s minimal overhead and kernel-level processing
    can make it a good choice for lower latency and higher throughput data transfer.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩展伯克利数据包过滤器**（**eBPF**）([https://ebpf.io/what-is-ebpf/](https://ebpf.io/what-is-ebpf/))
    允许在不修改内核代码的情况下进行 Linux 内核的高级可编程性。它可以用来在内核级别直接创建强大、轻量级的网络、安全和可观察性解决方案。例如，利用 eBPF
    的 Cilium CNI 插件提供了细粒度的网络安全、负载均衡和可观察性功能。对于 GenAI 工作负载，其中性能和数据传输速度至关重要，eBPF 的最小开销和内核级处理使其成为低延迟和高吞吐量数据传输的理想选择。'
- en: '**Single Root Input/Output Virtualization** (**SR-IOV**) enables a single physical
    **Network Interface Card** (**NIC**) to be partitioned into multiple **virtual
    functions**, providing direct hardware access to VMs or containers. Each virtual
    function acts as an independent interface, offering high throughput and low latency
    network throughput, which is ideal for GenAI workloads that involve large data
    transfers. SR-IOV reduces CPU overhead by offloading packet processing to NIC,
    ensuring better resource utilization for compute-intensive tasks. It also provides
    dedicated network paths, ensuring network isolation and predictable performance,
    which is crucial for consistent AI model inference. This technology enhances scalability
    by optimizing resource allocation and supports multi-tenant clusters with isolated,
    high-performance networking.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**单根输入/输出虚拟化** (**SR-IOV**)使单个物理**网络接口卡** (**NIC**)能够划分为多个**虚拟功能**，为虚拟机或容器提供直接的硬件访问。每个虚拟功能充当独立的接口，提供高吞吐量和低延迟的网络吞吐量，尤其适用于涉及大量数据传输的GenAI工作负载。SR-IOV通过将数据包处理卸载到NIC，从而减少了CPU开销，确保计算密集型任务的资源利用率更高。它还提供专用的网络路径，确保网络隔离和可预测的性能，这对于一致的AI模型推理至关重要。该技术通过优化资源分配提升了可扩展性，并支持具有隔离的高性能网络的多租户集群。'
- en: Technologies such as eBPF and SR-IOV optimize networking performance and resource
    efficiency in K8s clusters, enabling high-speed and reliable data processing.
    Complementing these advancements is CoreDNS autoscaling, which ensures seamless
    service discovery and efficient DNS resolution, critical for inter-service communication.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: eBPF和SR-IOV等技术优化了K8s集群中的网络性能和资源效率，支持高速且可靠的数据处理。这些进展的补充是CoreDNS自动扩展，它确保无缝的服务发现和高效的DNS解析，这对于服务间通信至关重要。
- en: CoreDNS
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CoreDNS
- en: '**CoreDNS** ([https://github.com/coredns/coredns](https://github.com/coredns/coredns))
    is a crucial component in K8s for providing DNS services that facilitate internal
    service discovery and networking within the K8s cluster. It acts as the cluster’s
    DNS server, allowing Pods to find and communicate with each other using simple
    service names. For optimal performance and management, using the **CoreDNS managed
    add-on** ([https://docs.aws.amazon.com/eks/latest/userguide/coredns-add-on-create.html](https://docs.aws.amazon.com/eks/latest/userguide/coredns-add-on-create.html))
    in EKS is recommended. Monitoring CoreDNS is also essential for maintaining efficient
    network performance, as issues with DNS resolution can lead to service disruptions
    or latency, particularly in workloads that require extensive inter-service communication,
    such as GenAI.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**CoreDNS** ([https://github.com/coredns/coredns](https://github.com/coredns/coredns))是K8s中的一个关键组件，提供DNS服务，促进K8s集群内部的服务发现和网络通信。它充当集群的DNS服务器，使Pods能够使用简单的服务名称找到并相互通信。为了实现最佳的性能和管理，建议在EKS中使用**CoreDNS托管插件**
    ([https://docs.aws.amazon.com/eks/latest/userguide/coredns-add-on-create.html](https://docs.aws.amazon.com/eks/latest/userguide/coredns-add-on-create.html))。监控CoreDNS对于维持高效的网络性能也至关重要，因为DNS解析问题可能会导致服务中断或延迟，特别是在需要大量服务间通信的工作负载中，如GenAI。'
- en: To keep up with the growing needs of large-scale K8s clusters, it is recommended
    to implement CoreDNS autoscaling to scale the number of CoreDNS Pods proportional
    to the size of the cluster. You can utilize the `cluster-proportional-autoscaler`
    ([https://github.com/kubernetes-sigs/cluster-proportional-autoscaler](https://github.com/kubernetes-sigs/cluster-proportional-autoscaler))
    addon, which watches over the number of schedulable nodes and CPU cores in the
    cluster and resizes the number of replicas for critical resources such as CoreDNS.
    When using CoreDNS managed addon in EKS, this functionality is provided natively
    and can be enabled using the addon configuration described at [https://docs.aws.amazon.com/eks/latest/userguide/coredns-autoscaling.html](https://docs.aws.amazon.com/eks/latest/userguide/coredns-autoscaling.html).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足大规模K8s集群日益增长的需求，建议实现CoreDNS自动扩展，以使CoreDNS Pods的数量与集群的大小成比例。你可以利用`cluster-proportional-autoscaler`
    ([https://github.com/kubernetes-sigs/cluster-proportional-autoscaler](https://github.com/kubernetes-sigs/cluster-proportional-autoscaler))插件，它监控集群中可调度节点和CPU核心的数量，并根据需要调整核心资源（如CoreDNS）的副本数。在EKS中使用CoreDNS托管插件时，该功能是原生提供的，可以通过[https://docs.aws.amazon.com/eks/latest/userguide/coredns-autoscaling.html](https://docs.aws.amazon.com/eks/latest/userguide/coredns-autoscaling.html)中描述的插件配置启用。
- en: Another tool for improving DNS performance in the K8s cluster is the `cluster.local`
    suffix), the local caching agent forwards the query to the CoreDNS service.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个用于提高 K8s 集群中 DNS 性能的工具是 `cluster.local` 后缀)，本地缓存代理会将查询转发给 CoreDNS 服务。
- en: CoreDNS is a vital component in K8s that enables internal service discovery
    and networking by acting as the cluster’s DNS server. To optimize performance
    in large-scale clusters, use CoreDNS autoscaling with `cluster-proportional-autoscaler`
    and leverage NodeLocal DNSCache to reduce latency. In the next section, we will
    cover a few other options to optimize network latency and throughput.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: CoreDNS 是 K8s 中的一个重要组件，通过充当集群的 DNS 服务器来实现内部服务发现和网络连接。为了优化大规模集群的性能，可以使用 CoreDNS
    自动扩展与 `cluster-proportional-autoscaler`，并利用 NodeLocal DNSCache 来减少延迟。在接下来的章节中，我们将介绍一些其他优化网络延迟和吞吐量的选项。
- en: Network latency and throughput enhancements
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络延迟和吞吐量增强
- en: GenAI workloads, such as large-scale training and inference of machine learning
    models, require extensive communication between multiple compute nodes. In distributed
    training scenarios where model parameters need to be synchronized across nodes,
    achieving high bandwidth and low latency is critical to achieve high performance
    and reduce the training/inference costs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: GenAI 工作负载，例如大规模机器学习模型的训练和推理，需要在多个计算节点之间进行大量通信。在分布式训练场景中，模型参数需要在节点之间进行同步，获取高带宽和低延迟对提高性能、降低训练/推理成本至关重要。
- en: In this section, we will discuss two cloud-specific techniques to achieve this.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论两种云特定的技术来实现这一目标。
- en: Amazon EC2 placement groups
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Amazon EC2 放置组
- en: '**Amazon EC2 placement groups** ([https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html))
    provide a way to organize nodes for specific networking or resilience objectives.
    In AWS, the following three placement groups are supported:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon EC2 放置组** ([https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html))
    提供了一种组织节点的方式，旨在实现特定的网络或弹性目标。在 AWS 中，支持以下三种放置组：'
- en: '**Cluster**, which places instances close together within an Availability Zone
    to enable low-latency network performance for high-performance computing applications.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cluster**，将实例放置在同一个可用区内，提供低延迟网络性能，适用于高性能计算应用。'
- en: '**Partition**, which distributes instances across separate logical partitions
    so that each partition’s group of instances does not share underlying hardware
    with those in other partitions. This strategy is typically used by large distributed
    and replicated workloads, such as Hadoop, Cassandra, and Kafka.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Partition**，将实例分布在不同的逻辑分区中，使每个分区中的实例组与其他分区中的实例不共享底层硬件。这种策略通常用于大规模分布式和复制工作负载，例如
    Hadoop、Cassandra 和 Kafka。'
- en: '**Spread**, which distributes a small group of instances across distinct underlying
    hardware to minimize the risk of correlated failures and improves resilience.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spread**，将小组实例分布在不同的底层硬件上，以最小化相关故障的风险，并提高弹性。'
- en: The cluster placement group, which places instances physically close together
    within a single data center or Availability Zone, provides low-latency and high-throughput
    networking between nodes. It could improve performance for distributed GenAI applications.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 集群放置组将实例物理上靠近地放置在单一数据中心或可用区内，提供节点间低延迟和高吞吐量的网络连接。它可能会提升分布式 GenAI 应用程序的性能。
- en: 'The following command creates a placement group called `custom-placement-group`
    in AWS with a cluster or proximity placement objective:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令在 AWS 中创建一个名为 `custom-placement-group` 的放置组，并设置集群或邻近放置目标：
- en: '[PRE4]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, EKS worker nodes can be launched within this placement group by creating
    an Auto Scaling group with the following launch template:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可以通过创建一个自动扩展组，并使用以下启动模板，在此放置组中启动 EKS 工作节点：
- en: '[PRE5]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In brief, EC2 placement groups offer strategies for organizing EC2 instances
    to optimize networking and throughput optimization. In the next section, we will
    cover the **Elastic Fabric Adapter** (**EFA**), a specialized network interface
    that delivers ultra-low latency and high throughput for inter-node communication.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，EC2 放置组提供了组织 EC2 实例的策略，以优化网络连接和吞吐量。在接下来的章节中，我们将介绍 **Elastic Fabric Adapter**
    (**EFA**)，这是一种专用网络接口，提供超低延迟和高吞吐量的节点间通信。
- en: EFA
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EFA
- en: EFA ([https://aws.amazon.com/hpc/efa/](https://aws.amazon.com/hpc/efa/)) is
    a network interface designed by AWS to provide ultra-low latency and a high-throughput
    network interface for internode communications. This is critical for GenAI workloads
    as it ensures that data transfer between nodes is fast and efficient. EFA supports
    **Remote Direct Memory Access** (**RDMA**), which reduces the overhead of data
    transfer between nodes, providing a low-latency, high-throughput path. Through
    RDMA, data can be transferred directly between the memory of two compute nodes
    across a network without involving the operating system or CPUs. EFA also supports
    **NVIDIA Collective Communications Library** (**NCCL**) ([https://developer.nvidia.com/nccl](https://developer.nvidia.com/nccl))
    for AI and ML applications to enable high-performance, scalable distributed training
    by accelerating communication between GPUs across multiple nodes. This integration
    reduces latency and improves bandwidth for inter-GPU communication, allowing faster
    model training times in GenAI applications that require synchronized, collective
    operations, such as data parallelism and model parallelism.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: EFA ([https://aws.amazon.com/hpc/efa/](https://aws.amazon.com/hpc/efa/)) 是 AWS
    设计的一种网络接口，旨在为节点间通信提供超低延迟和高吞吐量的网络接口。这对于 GenAI 工作负载至关重要，因为它确保了节点之间的数据传输快速且高效。EFA
    支持 **远程直接内存访问** (**RDMA**)，可以减少节点间数据传输的开销，提供低延迟、高吞吐量的传输路径。通过 RDMA，数据可以在两个计算节点的内存之间直接传输，而无需操作系统或
    CPU 的参与。EFA 还支持 **NVIDIA 集体通信库** (**NCCL**) ([https://developer.nvidia.com/nccl](https://developer.nvidia.com/nccl))，为
    AI 和 ML 应用提供高性能、可扩展的分布式训练，通过加速多个节点间 GPU 之间的通信。这一集成减少了延迟并提高了 GPU 之间的带宽，使得需要同步集体操作（如数据并行和模型并行）的
    GenAI 应用能够更快地进行模型训练。
- en: 'To integrate EFA with K8s Pods in the EKS cluster, you can create worker nodes
    with EFA-compatible instance types and deploy `aws-efa-k8s-device-plugin` ([https://github.com/aws/eks-charts/tree/master/stable/aws-efa-k8s-device-plugin](https://github.com/aws/eks-charts/tree/master/stable/aws-efa-k8s-device-plugin)),
    which detects and advertises EFA interfaces as allocatable resources to the cluster:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 EKS 集群中将 EFA 集成到 K8s Pods 中，可以创建支持 EFA 的实例类型的工作节点，并部署 `aws-efa-k8s-device-plugin`
    ([https://github.com/aws/eks-charts/tree/master/stable/aws-efa-k8s-device-plugin](https://github.com/aws/eks-charts/tree/master/stable/aws-efa-k8s-device-plugin))，该插件可以检测并将
    EFA 接口作为可分配资源向集群广播：
- en: '[PRE6]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Refer to the EKS documentation at [https://docs.aws.amazon.com/eks/latest/userguide/node-efa.html](https://docs.aws.amazon.com/eks/latest/userguide/node-efa.html)
    for step-by-step instructions and an example walkthrough.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考 EKS 文档 [https://docs.aws.amazon.com/eks/latest/userguide/node-efa.html](https://docs.aws.amazon.com/eks/latest/userguide/node-efa.html)，获取逐步操作指南和示例演练。
- en: In this section, we discussed various network optimization techniques such as
    picking Kube-proxy options, scaling CoreDNS Pods, strategies to solve IP exhaustion
    issues, emerging trends in K8s networking space, and other cloud-provider-specific
    optimizations including EFA and EC2 placement groups for managing large-scale
    GenAI workloads in the K8s clusters.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了各种网络优化技术，如选择 Kube-proxy 选项、扩展 CoreDNS Pods、解决 IP 耗尽问题的策略、K8s 网络领域的新兴趋势以及其他云提供商特定的优化措施，包括
    EFA 和 EC2 部署组，用于在 K8s 集群中管理大规模 GenAI 工作负载。
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we focused on optimizing cloud networking for deploying GenAI
    applications on K8s, highlighting best practices for efficient, secure, and high-performance
    networking. We started with K8s networking fundamentals, covering key components
    such as the **Container Network Interface** (**CNI**), the kubelet, and the **Container
    Runtime Interface** (**CRI**), which manage Pod networking and ensure connectivity
    across the cluster.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们重点讨论了优化云网络以便在 K8s 上部署 GenAI 应用，强调了高效、安全和高性能网络的最佳实践。我们从 K8s 网络基础知识开始，介绍了
    **容器网络接口** (**CNI**)、kubelet 和 **容器运行时接口** (**CRI**) 等关键组件，这些组件负责管理 Pod 网络并确保集群间的连接性。
- en: 'The K8s networking model is supported by CNI plugins such as Calico, Cilium,
    and Amazon VPC CNI, each with specific benefits. CNI plugins operate in two modes:
    overlay networks and native networking. Overlay networks, such as Flannel, add
    flexibility with network abstraction but may increase latency. On the other hand,
    native networking (e.g., Amazon VPC CNI) integrates with the underlying cloud
    infrastructure, offering lower latency, and is recommended for GenAI workloads.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: K8s网络模型由CNI插件支持，例如Calico、Cilium和Amazon VPC CNI，每种插件都有其特定的优势。CNI插件有两种工作模式：覆盖网络和本地网络。覆盖网络，如Flannel，增加了网络抽象的灵活性，但可能会增加延迟。另一方面，本地网络（例如Amazon
    VPC CNI）与底层云基础设施集成，提供更低的延迟，推荐用于GenAI工作负载。
- en: Service management within K8s provides stable IPs and DNS names, ensuring reliability
    even as Pods are added or removed. Service mesh tools, such as Istio or Linkerd,
    could be highly effective in enhancing traffic management, security, and observability
    by intercepting all traffic through sidecar proxies and implementing policies
    for load balancing, retry mechanisms, and TLS encryption.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: K8s中的服务管理提供稳定的IP和DNS名称，确保即使Pods被添加或删除，服务也能保持可靠性。服务网格工具，如Istio或Linkerd，通过拦截所有流量并通过边车代理实施负载均衡、重试机制和TLS加密策略，在提升流量管理、安全性和可观察性方面可能非常有效。
- en: NetworkPolicy, a native feature, further strengthens security by controlling
    ingress and egress traffic within K8s clusters, allowing isolation between teams
    or applications in multi-tenant environments. To address GenAI-specific needs,
    K8s supports advanced networking options such as Kube-Proxy with IPVS mode, offering
    scalable load balancing for high-demand clusters. Additionally, as K8s clusters
    scale, IP address exhaustion can become a challenge when using the CNI’s native
    networking mode; solutions such as IPv6 addressing and custom networking configuration
    can be employed to mitigate IP limitations in large-scale deployments.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: NetworkPolicy是一个本地功能，通过控制K8s集群内的流入和流出流量，进一步加强安全性，允许在多租户环境中实现团队或应用之间的隔离。为了满足GenAI的特定需求，K8s支持高级网络选项，如带有IPVS模式的Kube-Proxy，提供可扩展的负载均衡，适用于高需求集群。此外，随着K8s集群的扩展，当使用CNI的本地网络模式时，IP地址耗尽可能成为一个挑战；可以采用IPv6地址分配和自定义网络配置等解决方案，以缓解大规模部署中的IP限制问题。
- en: Other modern technologies support high-performance networking, such as **extended
    Berkeley Packet Filter** (**eBPF**) and **Single Root Input/Output Virtualization**
    (**SR-IOV**) provide minimal-overhead, kernel-level networking ideal for low-latency,
    high-throughput data processing. Finally, K8s networking benefits from specific
    enhancements in cloud environments, such as AWS’s placement groups and Elastic
    Fabric Adapter (EFA). In the next chapter, we will build on these concepts and
    discuss how to secure GenAI applications running in K8s.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 其他现代技术支持高性能网络，例如**扩展伯克利数据包过滤器**（**eBPF**）和**单根输入/输出虚拟化**（**SR-IOV**），它们提供最小开销的内核级网络，非常适合低延迟、高吞吐量的数据处理。最后，K8s网络受益于云环境中特定的增强功能，如AWS的放置组和弹性网络适配器（EFA）。在下一章中，我们将基于这些概念，讨论如何保护在K8s中运行的GenAI应用。
