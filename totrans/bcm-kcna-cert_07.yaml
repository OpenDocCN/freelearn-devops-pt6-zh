- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Application Placement and Debugging with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll see how we can control the placement of workloads on
    Kubernetes, how its scheduler works, and how we can debug applications running
    on K8s when something goes wrong. This chapter covers aspects from the *Kubernetes
    Fundamentals* as well as *Cloud Native Observability* domains of the **Kubernetes
    and Cloud Native Associate** (**KCNA**) exam at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, we will perform a few exercises with our minikube Kubernetes, so
    keep your setup handy. We’re going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource requests and limits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging applications in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s continue our Kubernetes journey!
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve already touched the surface of what Kubernetes scheduler (`kube-scheduler`)
    does in [*Chapter 5*](B18970_05.xhtml#_idTextAnchor059). The scheduler is the
    component of the K8s control plane that decides on which node a pod will run.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling is the process of assigning Pods to Kubernetes nodes for the kubelet
    to run them. The scheduler watches for newly created Pods that have no *node*
    assigned in an infinite loop, and for every Pod it discovers, it will be responsible
    for finding the optimal node to run it on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default `kube-scheduler` scheduler selects a node for a pod in two stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Filtering**: The first stage is where the scheduler determines the set of
    nodes where it is feasible to run the pod. This includes checks for nodes to have
    sufficient capacity and other requirements for a particular pod. This list might
    be empty if there are no suitable nodes in the cluster, and in such a case, the
    pod will hang in an unscheduled state until either the requirements or cluster
    state is changed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Scoring**: The second stage is where the scheduler ranks the nodes filtered
    in the first stage to choose the most suitable pod placement. Each node in the
    list will be ranked and gets a score, and at the end, the node with the highest
    score is picked.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s have a real-world example. Imagine we have an application that requires
    nodes with certain hardware. For example, you run **machine learning** (**ML**)
    workloads that can utilize GPUs for faster processing, or an application requires
    a particular CPU generation that is not available on every node in the cluster.
    In all such cases, we need to instruct Kubernetes to restrict the list of suitable
    nodes for our pods. There are multiple ways to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: Specifying a `nodeSelector` field in the pod spec and labeling nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying an exact `nodeName` field in the pod spec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **affinity** and **anti-affinity** rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using pod **topology** **spread constraints**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let’s get back to our minikube setup and extend the cluster by adding one
    more node with the `minikube node add` command (*this operation might take* *some
    time*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We should have a two-node minikube cluster at this point! Let’s check the list
    of nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If your system does not have sufficient resources to run another Kubernetes
    node, you can also continue as before with just one node. In such cases, however,
    you will need to adjust the example commands you encounter in this chapter to
    label the first node.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now create a modified version of the nginx deployment from before,
    which will require the node to have `purpose: web-server` appended to it. The
    respective `Deployment` spec could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you have not yet deleted the resources from the previous chapter’s exercises,
    do so now by executing `kubectl delete deployment`, `kubectl delete sts`, or `kubectl
    delete service` in the `kcna` namespace respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go ahead and create the aforementioned nginx deployment spec:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check what happened; for example, query pods in the `kcna` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'There it goes – the created Nginx pod is stuck in a `Pending` state. Let’s
    check more details with the `kubectl describe` command (*your pod naming* *will
    differ*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The message is clear – we have requested a node with a certain label for our
    nginx deployment pod and there are no nodes with such a label available.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check which labels our nodes have by adding the `--``show-labels` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Default labels include some useful information about the roles of the nodes,
    CPU architecture, OS, and more. Let’s now label the newly added node with the
    same label our nginx deployment is looking for (*the node name might be similar
    in your case, so* *adjust accordingly*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And just a moment later, we can see the nginx pod is being created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'By adding the `-o wide` option, we can see which node the pod was assigned
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: That was a demonstration of perhaps the most common way to provide placement
    instructions to a Kubernetes scheduler with `nodeSelector`.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to discuss the other scheduling controls Kubernetes offers. `nodeName`
    should be obvious – it allows us to specify exactly which node we want the workload
    to be scheduled to. Affinity and anti-affinity rules are more interesting. Conceptually,
    affinity is similar to `nodeSelector` but has more customization options.
  prefs: []
  type: TYPE_NORMAL
- en: nodeAffinity and podAffinity
  prefs: []
  type: TYPE_NORMAL
- en: These allow you to schedule Pods either on certain nodes (`nodeAffinity`) in
    a cluster or to nodes that are already running specified Pods (`podAffinity`).
  prefs: []
  type: TYPE_NORMAL
- en: nodeAntiAffinity and podAntiAffinity
  prefs: []
  type: TYPE_NORMAL
- en: The opposite of affinity. These allow you to either schedule Pods to different
    nodes than the ones specified (`nodeAntiAffinity`) or to schedule to different
    nodes where the specified Pods run (`podAntiAffinity`).
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, affinity rules are used to attract Pods to certain nodes or
    other Pods, and anti-affinity for the opposite – to push back from certain nodes
    or other Pods. Affinity can also be of two types – *hard* and *soft*:'
  prefs: []
  type: TYPE_NORMAL
- en: '`requiredDuringSchedulingIgnoredDuringExecution` – Hard requirement, meaning
    the pod won’t be scheduled unless the rule is met'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`preferredDuringSchedulingIgnoredDuringExecution` – Soft requirement, meaning
    the scheduler will try to find the node that satisfies the requirement, but if
    not available, the pod will still be scheduled on any other node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`IgnoredDuringExecution` means that if the node labels change after Kubernetes
    already scheduled the pod, the pod continues to run on the same node.'
  prefs: []
  type: TYPE_NORMAL
- en: Last on our list is pod **topology** **spread constraints**.
  prefs: []
  type: TYPE_NORMAL
- en: Topology spread constraints
  prefs: []
  type: TYPE_NORMAL
- en: These allow us to control how Pods are spread across the cluster among failure
    domains such as regions, **availability zones** (**AZs**), nodes, or other user-defined
    topologies.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, these allow us to control where Pods run, taking into account the
    physical topology of the cluster. In today’s cloud environments, we typically
    have multiple AZs in each region where the cloud provider operates.
  prefs: []
  type: TYPE_NORMAL
- en: AZ
  prefs: []
  type: TYPE_NORMAL
- en: This is one or multiple discrete data centers with redundant power, networking,
    and internet connectivity.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a good practice to run both the control plane and worker nodes of Kubernetes
    across multiple AZs. For example, in the `eu-central-1` region, **Amazon Web Services**
    (**AWS**) currently has three AZs, so we can run one control plane node in each
    AZ and multiple worker nodes per AZ. In this case, to achieve **high availability**
    (**HA**) as well as efficient resource utilization, we can apply topology spread
    constraints to our workloads to control the spread of Pods by nodes and zones,
    as shown in *Figure 7**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Example of Pods spread across a cluster](img/B18970_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Example of Pods spread across a cluster
  prefs: []
  type: TYPE_NORMAL
- en: This way, we can protect our workloads against individual node outages as well
    as wider cloud provider outages that might affect a whole AZ. Besides, it is possible
    to combine different methods and rules for more precise and granular control of
    where the Pods will be placed – for example, we can have topology spread constraints
    together with `nodeAffinity` and `podAntiAffinity` rules in one deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to combine multiple rules per pod - for example, `nodeSelector`
    together with hard `nodeAffinity` rules (`requiredDuringSchedulingIgnoredDuringExecution`).
    Both rules must be satisfied for the pod to be scheduled. In cases where at least
    one rule is not satisfied, the pod will be in a `Pending` state.
  prefs: []
  type: TYPE_NORMAL
- en: Altogether, scheduling in Kubernetes might seem to be a bit complicated at first,
    but as you start getting more experience, you’ll see that its extensive features
    are great and let us handle complex scenarios as well as very large and diverse
    K8s installations. For the scope of the KCNA exam, you are not required to know
    in-depth details, but if you have some time, you are encouraged to check the *Further
    reading* section at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Resource requests and limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we were exploring the features of the K8s scheduler previously, have you
    wondered how Kubernetes knows *what is the best node in the cluster for a particular
    pod*? If we create a Deployment with no affinity settings, topology constraints,
    or node selectors, how can Kubernetes decide what is the best location in the
    cluster for the application we want to run?
  prefs: []
  type: TYPE_NORMAL
- en: By default, K8s is not aware of how many resources (CPU, memory, and other)
    each container in a scheduled pod requires to run. Therefore, for Kubernetes to
    make the best scheduling decisions, we need to make K8s aware of what each container
    requires for normal operation.
  prefs: []
  type: TYPE_NORMAL
- en: Resource requests
  prefs: []
  type: TYPE_NORMAL
- en: A resource request is an optional specification of how many resources each container
    in a pod needs. Containers can use more resources than requested if the node where
    the Pod runs has available resources. The specified request amounts will be reserved
    on the node where the pod is scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes also allows us to impose hard limits on resources that the container
    can consume.
  prefs: []
  type: TYPE_NORMAL
- en: Resource limits
  prefs: []
  type: TYPE_NORMAL
- en: A resource limit is an optional specification of the maximum resources a running
    container can consume that are enforced by the kubelet and container runtime.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can set that the nginx container requires `250 MiB`. If the
    pod with this container gets scheduled on a node with `8 GiB` total memory with
    few other running Pods, our nginx container could possibly use `1 GiB` or even
    more. However, if we additionally set a limit of `1 GiB`, the runtime will prevent
    nginx from going beyond that limit. If a process tries to allocate more memory,
    the node kernel will forcefully terminate that process with an **Out Of Memory**
    (**OOM**) error, and the container gets restarted.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the CPU, limits and requests are measured by absolute units,
    where 1 CPU unit is an equivalent of 1 `0.5` CPU is the same as `500m` units,
    where `m` stands for *milliCPU*, and—as you probably guessed—it allows us to specify
    a fraction of CPU this way. Unlike with memory, if a process tries to consume
    more CPU time than allowed by the limit, it won’t be killed; instead, it simply
    gets throttled.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When a resource limit is specified but not the request, and no default request
    is set (for example, the default might be inherited from the namespace settings),
    Kubernetes will copy the specified limit and use it as the request too. For example,
    a `500 MiB` limit will cause the request to be `500 MiB` as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time to see these in action! Let’s get back to our minikube setup and try creating
    the following example pod with a single container in the `kcna` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The application inside is a simple `stress` test tool that generates configurable
    load and memory consumption. With the arguments specified in the preceding spec,
    it consumes exactly `150 Mi` of memory. Because `150 Mi` is less than the limit
    set (`200 Mi`), everything is working fine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s modify the `stress` arguments in the spec to use `250M` instead
    of `150M`. The respective changes are highlighted in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Delete the old pod and apply the updated spec, assuming that the file is now
    called `memory-request-over-limit.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If you’re typing quickly enough, you should be able to see the `OOMKilled`
    status and, eventually, `CrashLoopBackOff`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, you can invoke `minikube kubectl -- describe po memory-demo -n kcna`
    to see more details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Because the process allocates `250 MiB` with a limit of `150 MiB` set on the
    container, it gets killed. Remember that if you run multiple containers in a pod,
    the whole pod will stop accepting requests if at least one container of that pod
    is not running.
  prefs: []
  type: TYPE_NORMAL
- en: To sum all this up, requests and limits are very important, and **the best practice
    is to configure both for all workloads running in Kubernetes** because Kubernetes
    does not know how many resources your applications need, and you might end up
    with overloaded or underutilized worker nodes in your cluster, impacting stability
    when resource requests are undefined. Resource limits, on the other hand, help
    protect from rogue pods and applications with bugs that might be leaking memory
    or trying to use all available CPU time, affecting the neighbor workloads.
  prefs: []
  type: TYPE_NORMAL
- en: After you’re done, feel free to delete the pod and other resources in the `kcna`
    namespace, if any. Next, we will continue exploring Kubernetes and learn about
    the ways to debug applications running on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging applications in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you start using Kubernetes to run various applications, you’ll eventually
    face the need to debug at least some of them. Sometimes, an application might
    have a bug that causes it to crash – maybe it was misconfigured or misbehaving
    under certain scenarios. Kubernetes provides multiple mechanisms that help us
    figure out what is wrong with the containerized payload and individual pod containers,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Fetching logs from all containers in a pod, and also logs from the previous
    pod run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying events that happened in a cluster recently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Port forwarding from a pod to a local environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running arbitrary commands inside containers of a pod
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs play a crucial role and are very helpful to understand what is not working
    as intended. Applications often support multiple log levels categorized by the
    severity and verbosity of information that gets recorded, such as received requests
    and their payload; interrupted connections; failures to connect to a database
    or other services; and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Common log levels are `INFO`, `WARNING`, `ERROR`, `DEBUG`, and `CRITICAL`, where
    the `ERROR` and `CRITICAL` settings will only record events that are considered
    errors and major problems as such. `INFO` and `WARNING` levels might provide generic
    information about what is happening or what might indicate an application problem,
    and `DEBUG` usually gives the most details by recording everything that happened
    with the application. As the name suggests, it might be wise to enable maximum
    verbosity by enabling the `DEBUG` log level to help debug problems. While this
    level of categorization is pretty standard across the industry, some software
    might have its own ways and definitions of log verbosity, so refer to the respective
    documentation and configuration samples. A standard log representation format
    today is JSON, and it is widely supported by development libraries in any language
    and all sorts of applications.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to logging architecture, the best way is to use a separate backend
    to store, analyze, and query logs that will persist the log records independent
    from the lifecycle of Kubernetes nodes, Pods, and containers. This approach is
    known as **cluster-level logging**. Kubernetes does not provide a native log storage
    solution and only keeps the most recent logs on each node. However, there are
    plenty of logging solutions that offer seamless integration with Kubernetes, such
    as **Grafana Loki** or **Elastic Stack** (**ELK**), to name a few. The opposite
    of cluster-level logging is when logging is configured individually for each pod
    (application) running in the cluster. This is not a recommended way to go with
    Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to collect logs from each Kubernetes node for aggregation and longer
    storage, it is common to use **node logging agents**. Those are small, containerized
    agents that run on every node and push all collected logs to a logging backend
    server. Because they need to run on each node in the cluster, it is common to
    define them as a **DaemonSet**. A schematic of such a setup is shown in *Figure
    7**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Using a node logging agent for log collection and aggregation](img/B18970_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Using a node logging agent for log collection and aggregation
  prefs: []
  type: TYPE_NORMAL
- en: 'For a moment, let’s get back to our minikube setup and see in action how to
    fetch application logs. Let’s start with a basic pod that simply writes the current
    date and time into the **standard** **output** (**stdout**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute `kubectl logs` to get the container logs. If a pod has multiple containers,
    you’ll have to additionally specify which particular container you want to get
    the logs from with the `--``container` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to fetch logs from the previous container execution (before
    it was restarted) by adding the `--previous` argument to the `kubectl` `logs`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: What we can see here is actually what the application inside the container writes
    to `stdout` and `stderr` (`kubectl logs`.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if an application does not have a configuration to log to `stdout`
    and `stderr`, it is possible to add a logging sidecar – a separate container running
    in the same pod that captures the logs from the main container and forwards those
    to either a logging server or its own `stdout` and `stderr` output streams. Such
    a setup is depicted in *Figure 7**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Log steaming with a sidecar container](img/B18970_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Log steaming with a sidecar container
  prefs: []
  type: TYPE_NORMAL
- en: Next on our list are events that can provide valuable insights into what is
    happening in your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes event
  prefs: []
  type: TYPE_NORMAL
- en: This is a record documenting a change in the state of Kubernetes resources—for
    example, changes to nodes, pods, and any other resources when a node becomes `NotReady`
    or when a **PersistentVolume** (**PV**) fails to be mounted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Events are namespaced, and the `kubectl get events` command can be used to
    query recent events. For example, if we recently scaled an nginx deployment, we
    could see something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: By default, only events that happened in the last hour are kept. The duration
    can be increased in the `kube-apiserver` configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming next is another useful feature that can help during debugging or development
    with Kubernetes—`kubectl` command, you can configure to forward connections made
    on the local port on your system to the specified remote port of any pod running
    in your K8s cluster. The syntax of the command is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This is very helpful if you need to access a Kubernetes payload as though it
    is running locally on your workstation. For example, you have a web application
    listening on port `80` in the pod, and you forward your local port `8080` toward
    remote port `80` of the respective pod. While port forwarding is running, you
    can reach the application locally under `localhost:8080`.
  prefs: []
  type: TYPE_NORMAL
- en: Port forwarding is not a part of the KCNA exam; however, if you have time, feel
    free to check the *Further reading* section at the end of the chapter for examples
    of how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: The last point on our list is about starting processes inside already running
    containers of a pod. We’ve already done that previously, in [*Chapter 6*](B18970_06.xhtml#_idTextAnchor068),
    *Deploying and Scaling Applications* *with Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The respective `kubectl exec` command allows us to start transient, arbitrary
    processes in containers, and most of the time, you’d probably use it to start
    a shell (`bash` or `sh`) in interactive mode. This *feels* much like logging inside
    a container with a **Secure Shell** (**SSH**) protocol. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Kubernetes also allows us to copy files between your local system and remote
    containers. The respective command is `kubectl cp`, which works very similarly
    to the Linux `scp` tool, but in the Kubernetes context.
  prefs: []
  type: TYPE_NORMAL
- en: Both `exec` and `cp` are very practical for understanding what is going on inside
    the container with an application we’re debugging. It allows us to quickly verify
    configuration settings, perform an HTTP request, or fetch logs that are not written
    to `stdout` or `stderr`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All in all, in this chapter, we’ve learned a few important aspects of running
    and operating workloads with K8s. We’ve seen how pod scheduling works with Kubernetes,
    its stages (`nodeSelector`, `nodeName`, and **affinity** and **anti-affinity**
    settings, as well as **topology spread constraints**. Extensive features of the
    Kubernetes scheduler allow us to cover all imaginable scenarios of controlling
    how a certain workload will be placed within the nodes of a cluster. With those
    controls, we can spread Pods of one application between nodes in multiple AZs
    for HA; schedule Pods that require specialized hardware (for example, a GPU) only
    to nodes that have it available; magnet multiple applications together to run
    on the same nodes with affinity; and much more.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ve seen that **resource requests** help Kubernetes make better scheduling
    decisions, and **resource limits** are there to protect the cluster and other
    Pods from misbehaving apps or simply restrict resource utilization. With the container
    reaching the specified memory limit, its process gets killed (and the container
    restarted), and when reaching allocated CPU units, the process gets throttled.
  prefs: []
  type: TYPE_NORMAL
- en: After, we explored ways to debug applications on K8s. Logs are one of the basic
    and most important tools in any problem analysis. Kubernetes follows a `stdout`
    and `stderr`, the solution is to run a **logging sidecar container** in the same
    pod as the application that will stream the logs.
  prefs: []
  type: TYPE_NORMAL
- en: Other practical ways to debug applications and get insights about what is happening
    in a cluster include Kubernetes events, port forwarding, and execution of arbitrary
    processes such as a `kubectl` `exec` command.
  prefs: []
  type: TYPE_NORMAL
- en: Coming next is the final chapter from the Kubernetes part, where we will learn
    some of the best practices and explore other operational aspects of K8s. If you
    would like to go deeper into the content described in this section, check out
    the *Further* *reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we conclude, here is a list of questions for you to test your knowledge
    regarding this chapter’s material. You will find the answers in the *Assessments*
    section of the *Appendix*:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following stages are part of scheduling in Kubernetes (pick multiple)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spreading
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Launching
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Filtering
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Scoring
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens if the Kubernetes scheduler cannot assign a pod to a node?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It will be stuck in a `CrashLoopBackOff` state
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It will be stuck in a `Pending` state
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It will be stuck in a `NotScheduled` state
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It will be forcefully run on one of the control plane nodes
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following scheduler instructions will not prevent a pod from being
    scheduled if a condition cannot be satisfied (soft affinity or soft anti-affinity)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`requiredDuringSchedulingIgnoredDuringExecution`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`preferredDuringSchedulingIgnoredDuringExecution`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`neededDuringSchedulingIgnoredDuringExecution`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`softAffinity`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which Kubernetes scheduler feature should be used to control how Pods are spread
    across different failure domains (such as AZs, nodes, and so on)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pod failure domain constraints
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Pod topology spread constraints
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`podAntiAffinity`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`nodeName`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of `podAffinity` in Kubernetes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To schedule Pods to certain nodes in the cluster
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To group two or more Pods together for better performance
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To schedule Pods to nodes where other Pods already running
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To schedule Pods to different nodes where other Pods already running
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of resource requests in Kubernetes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They help to plan ahead the cluster extension
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: They define more important workloads in the cluster
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: They are needed to pick the right hardware in the cluster for the workload
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: They are needed for optimal pod placement in the cluster
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens if a container has a memory limit of `500Mi` set but tries to allocate
    `550Mi`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`550Mi` is within a 10% margin, so the container will allocate memory normally'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Pods have higher limits than containers, so memory allocation will work
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The container process will be killed with an OOM error
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The container process will be stuck when it gets over `500Mi`
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What will be the value of the container CPU request if the limit is set to `1.5`
    and there are no defaults for that namespace?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`0.0`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`0.75`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`1.5`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`1.0`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens with a pod if its containers request a total of `10.0` CPU units,
    but the largest node in the cluster only has `8.0` CPUs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Requests are not hard requirements; the pod gets scheduled
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Requests are hard requirements; the pod will be stuck in a `Pending` state
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Because of the `preferredDuringScheduling` option, the pod gets scheduled anyway
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The pod will be in a `CrashLoopBackOff` state due to a lack of resources in
    the cluster
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which logging level typically provides maximum verbosity for debugging purposes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`INFO`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ERROR`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`MAXIMUM`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DEBUG`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What does cluster-level logging mean for log storage in Kubernetes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K8s aggregates all cluster logs on control plane nodes
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: K8s needs separate log collection and aggregation systems
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: K8s provides a complete log storage and aggregation solution
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: K8s provides storage only for the most important cluster health logs
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What do node logging agents in Kubernetes do (pick multiple)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect logs only from worker nodes
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect logs from all nodes
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Send logs from worker nodes to control plane nodes
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Send logs for aggregation and storage to a logging backend
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of logging sidecar agents in Kubernetes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To collect and stream logs from applications that cannot log to `stdout` and
    `stderr`
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To provide a backup copy of logs in case of a node failure
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To allow logging on verbosity levels such as `ERROR` and `DEBUG`
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To enable the `kubectl logs` command to work
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following `kubectl` commands allows us to run an arbitrary process
    in a container?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl run`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl start`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl exec`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl proc`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following commands will return logs from a pod container that has
    failed and restarted?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl logs` `POD_NAME --previous`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl` `logs POD_NAME`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl previous` `logs POD_NAME`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl logs`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which Kubernetes scheduler feature provides a simple way to constrain Pods to
    nodes with specific labels?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl local`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`nodeConstrain`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`nodeSelector`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`nodeName`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scheduling and assigning Pods to nodes: [https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pod topology spread constraints: [https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Resource management in Kubernetes: [https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using port forwarding to access applications in a K8s cluster: [https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/](https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Getting a shell to a running container: [https://kubernetes.io/docs/tasks/debug/debug-application/get-shell-running-container/](https://kubernetes.io/docs/tasks/debug/debug-application/get-shell-running-container/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
