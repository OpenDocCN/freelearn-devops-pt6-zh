<html><head></head><body>
		<div id="_idContainer121">
			<h1 class="chapter-number" id="_idParaDest-167"><a id="_idTextAnchor167"/>11</h1>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor168"/>Generative AI on Kubernetes</h1>
			<p><strong class="bold">Generative</strong> <strong class="bold">artificial intelligence</strong> (<strong class="bold">GenAI</strong>) has emerged as a transformative technology, revolutionizing how we interact with and leverage AI. In this chapter, we will explore the exciting world of generative AI and learn how to harness its power on Kubernetes. We will dive into the fundamentals of generative AI and understand its main differences from <span class="No-Break">traditional AI.</span></p>
			<p>Our focus will be on leveraging <strong class="bold">Amazon Bedrock</strong>, a comprehensive suite of services designed to simplify the development and deployment of generative AI applications. Through hands-on examples, you will gain practical experience in building a generative AI application on Kubernetes using <strong class="bold">Streamlit</strong>, a powerful Python library for creating interactive data applications. We will cover the entire process, from the development to deploying the application on a <span class="No-Break">Kubernetes cluster.</span></p>
			<p>Moreover, we will explore the concept of <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>), which combines the power of generative AI with external <span class="No-Break">knowledge bases.</span></p>
			<p>Finally, we will introduce <strong class="bold">Agents for Amazon Bedrock</strong>, a powerful feature that allows you to automate tasks and create intelligent assistants. You will learn how to build an agent, define its capabilities through an OpenAPI schema, and create the underlying Lambda function that serves as the backend for <span class="No-Break">your agent.</span></p>
			<p>By the end of this chapter, you will have a solid understanding of generative AI, its applications, and the tools and techniques required to build and deploy generative AI applications <span class="No-Break">on Kubernetes.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>What generative AI is and what it <span class="No-Break">is not</span></li>
				<li>Using Amazon Bedrock to work with <span class="No-Break">foundational models</span></li>
				<li>Building a generative AI application <span class="No-Break">on Kubernetes</span></li>
				<li>Building RAG with <strong class="bold">Knowledge Bases for </strong><span class="No-Break"><strong class="bold">Amazon Bedrock</strong></span></li>
				<li>Building action models <span class="No-Break">with agents</span></li>
			</ul>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor169"/>Technical requirements</h1>
			<p>For this chapter, you will need an AWS account and a running Kubernetes cluster. We will also be using <strong class="bold">LangChain</strong> and Streamlit libraries. Although it is not necessary to have them installed for application deployment in Kubernetes, the installation is advised if you want to test the code locally and modify it to your <span class="No-Break">own experiments.</span></p>
			<p>Also, it will be necessary to install the <strong class="bold">Beautiful Soup</strong> library to get data for the RAG exercise (<span class="No-Break">fourth section).</span></p>
			<p>All the code for this chapter is available at <a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes">https://github.com/PacktPublishing/Bigdata-on-Kubernetes</a> under the <span class="No-Break"><strong class="source-inline">Chapter11</strong></span><span class="No-Break"> folder.</span></p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor170"/>What generative AI is and what it is not</h1>
			<p>At its core, generative AI<a id="_idIndexMarker724"/> refers to AI systems capable of generating new, original content, such as text, images, audio, or code, based on the training data they have been exposed to. Generative AI models are trained on large datasets of existing content, and they learn the patterns and relationships within that data. When prompted, these models can then generate new, original content that resembles the training data but is not an exact copy of any <span class="No-Break">specific example.</span></p>
			<p>This contrasts with traditional machine learning models, which are focused on making predictions or classifications based on <span class="No-Break">existing data.</span></p>
			<p>Traditional machine learning models, such<a id="_idIndexMarker725"/> as those used for image recognition, natural language processing, or predictive analytics, are designed to take in input data and make predictions or classifications based on that data. Machine learning models excel at tasks such as classification (e.g., identifying objects in images or topics in texts), regression (e.g., predicting house prices based on features such as square footage and location), and clustering (e.g., grouping customers based on similar <span class="No-Break">behavior patterns).</span></p>
			<p>For example, an image recognition model might be trained on a large dataset of labeled images to learn to recognize and classify objects in new, unseen images. Similarly, a natural language processing model might be trained on a corpus of text data to perform tasks such as sentiment analysis, named entity recognition, or <span class="No-Break">language translation.</span></p>
			<p>In a credit risk assessment scenario, a machine learning model<a id="_idIndexMarker726"/> would be trained on a dataset containing information about past loan applicants, such as their income, credit history, and other relevant features, along with labels indicating whether they defaulted on their loans or not. The model would learn the patterns and relationships between these features and the loan default outcomes. When presented with a new loan application, the trained model can then predict the likelihood of the applicant defaulting on <span class="No-Break">the loan.</span></p>
			<p>In these cases, the machine learning model is not generating new content; instead, it is using the patterns and relationships it has learned from the training data to make informed predictions or decisions about new, <span class="No-Break">unseen data.</span></p>
			<p>In contrast, for instance, a generative AI model trained on a vast corpus of text can generate human-like writing on any given topic or in any desired style. Similarly, models trained on images can create entirely new, realistic-looking images based on textual descriptions or other <span class="No-Break">input data.</span></p>
			<p>While the end result of generative AI<a id="_idIndexMarker727"/> is the creation of new content, the underlying mechanism is still based on the same principles of machine learning: making predictions. However, instead of predicting a single output (such as a classification or a numerical value), generative AI models are trained to predict the next element in a sequence, whether that sequence is a sequence of words, pixels, or any other type <span class="No-Break">of data.</span></p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor171"/>The power of large neural networks</h2>
			<p>While the concept of predicting<a id="_idIndexMarker728"/> the next element in a sequence is relatively simple, the ability of generative AI models to generate coherent, high-quality content lies in the sheer scale and complexity of the neural networks used to power <span class="No-Break">these models.</span></p>
			<p>Generative AI models typically employ large, deep neural networks with billions or even trillions of parameters. These neural networks are trained on vast amounts of data, often spanning millions or billions of examples, allowing them to capture incredibly nuanced patterns and relationships within <span class="No-Break">the data.</span></p>
			<p>For example, the Anthropic models, such as Claude, are trained on an enormous corpus of text data, spanning a wide range of topics and domains. This allows the models to develop a deep understanding of language, context, and domain-specific knowledge, enabling them to generate text that is not only grammatically correct but also semantically coherent and relevant to the <span class="No-Break">given context.</span></p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor172"/>Challenges and limitations</h2>
			<p>While generative AI has demonstrated<a id="_idIndexMarker729"/> remarkable capabilities, it is not without its challenges and limitations. One of the primary concerns is the potential for these models to generate biased, harmful, or misleading content, especially when trained on datasets that reflect societal biases or contain <span class="No-Break">inaccurate information.</span></p>
			<p>Additionally, generative AI models can sometimes produce outputs that are nonsensical, inconsistent, or factually incorrect, even though they may appear coherent and plausible on the surface. This<a id="_idIndexMarker730"/> is known as the “hallucination” problem, where the model generates content that is not grounded in factual knowledge or the provided context. Here are two well-known real-life cases. Air Canada’s AI-powered chatbot provided misleading information to a passenger regarding the airline’s bereavement fare policy. The chatbot incorrectly stated that passengers could apply for reduced bereavement fares retroactively, even after travel had already occurred, which contradicted Air Canada’s actual policy. The passenger relied on this hallucinated response from the chatbot and subsequently filed a successful small claims case against Air Canada when the airline refused to honor the chatbot’s advice (<a href="https://www.forbes.com/sites/marisagarcia/2024/02/19/what-air-canada-lost-in-remarkable-lying-ai-chatbot-case/">https://www.forbes.com/sites/marisagarcia/2024/02/19/what-air-canada-lost-in-remarkable-lying-ai-chatbot-case/</a>). Also, a federal judge in Brazil used the ChatGPT AI system to research legal precedents for a ruling he was writing. However, the AI provided fabricated information, citing non-existent rulings from the Superior Court of Justice as the basis for the judge’s <span class="No-Break">decision (</span><a href="https://g1.globo.com/politica/blog/daniela-lima/post/2023/11/13/juiz-usa-inteligencia-artificial-para-fazer-decisao-e-cita-jurisprudencia-falsa-cnj-investiga-caso.ghtml"><span class="No-Break">https://g1.globo.com/politica/blog/daniela-lima/post/2023/11/13/juiz-usa-inteligencia-artificial-para-fazer-decisao-e-cita-jurisprudencia-falsa-cnj-investiga-caso.ghtml</span></a><span class="No-Break">).</span></p>
			<p>Despite these challenges, generative AI is a rapidly evolving field, and researchers and developers are actively working on addressing these issues. Techniques such as fine-tuning, prompt engineering, and the use of external knowledge sources (e.g., knowledge bases or RAG) are being explored to improve the reliability, safety, and factual accuracy of generative <span class="No-Break">AI models.</span></p>
			<p>In the following sections, we will dive deeper into the practical aspects of building and deploying generative AI applications using Amazon Bedrock and its foundational models, knowledge base, and <span class="No-Break">agent-based architectures.</span></p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor173"/>Using Amazon Bedrock to work with foundational models</h1>
			<p>Amazon Bedrock provides a suite<a id="_idIndexMarker731"/> of foundational models<a id="_idIndexMarker732"/> that can be used as building blocks for your generative AI applications. It’s important to understand the capabilities and intended use cases of each model to choose the right one for <span class="No-Break">your application.</span></p>
			<p>The available models in Amazon Bedrock include language models, computer vision models, and multimodal models. Language models excel at understanding and generating human-like text. They can be employed for tasks such as text summarization, question answering, and content generation. Computer vision models, on the other hand, are adept at analyzing and understanding visual data, making them ideal for applications such as image recognition, object detection, and <span class="No-Break">scene understanding.</span></p>
			<p>Multimodal models, as the name suggests, can handle multiple modalities simultaneously. This makes it suitable for tasks such as image captioning, visual question answering, and data <span class="No-Break">chart analysis.</span></p>
			<p>It’s important to note that each model has its own strengths and limitations, and the choice of model should be guided by the specific requirements of your application. For example, if your application primarily deals with text-based tasks, a language model such as Llama might be the most appropriate choice. However, if you need to process both text and images, a multimodal model such as Claude would be a <span class="No-Break">better fit.</span></p>
			<p>To effectively integrate Amazon Bedrock’s foundational models into our generative AI applications, follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>To use Amazon Bedrock’s available foundational models, first, we need to activate them. Go to the AWS console and search for the Amazon Bedrock page. Then, click on <strong class="bold">Modify model access</strong> (<span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">).</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer099">
					<img alt="Figure 11.1 – Modifying model access on Amazon Bedrock" src="image/B21927_11_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Modifying model access on Amazon Bedrock</p>
			<ol>
				<li value="2">On the next page, select<a id="_idIndexMarker733"/> the <strong class="bold">Claude 3 Sonnet</strong> and <strong class="bold">Claude 3 Haiku</strong> Anthropic models. Those are the foundational models<a id="_idIndexMarker734"/> we will use for our generative AI applications. You can select all the available models if you wish to play and experiment with different models (<span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">).</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer100">
					<img alt="Figure 11.2 – Requesting access for Anthropic’s Claude 3 models" src="image/B21927_11_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Requesting access for Anthropic’s Claude 3 models</p>
			<ol>
				<li value="3">Click <strong class="bold">Next</strong> and, on the next page, review the changes and click <strong class="bold">Submit</strong>. Those models can take<a id="_idIndexMarker735"/> a few minutes<a id="_idIndexMarker736"/> to get <span class="No-Break">access granted.</span></li>
			</ol>
			<p>Once access has been granted, we have all we need to develop a generative AI application. Let’s get <span class="No-Break">to it.</span></p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor174"/>Building a generative AI application on Kubernetes</h1>
			<p>In this section, we will build<a id="_idIndexMarker737"/> a generative AI application<a id="_idIndexMarker738"/> with Streamlit. A diagram representing the architecture for this application is shown in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.3</em>. In this application, the user will be able to choose which foundational model they are going to <span class="No-Break">talk to.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer101">
					<img alt="Figure 11.3 – Foundational models’ application architecture" src="image/B21927_11_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Foundational models’ application architecture</p>
			<p>Let’s start with the Python code<a id="_idIndexMarker739"/> for the application. The complete code<a id="_idIndexMarker740"/> is available under the <a href="B21927_11.xhtml#_idTextAnchor167"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><strong class="source-inline">/streamlit-claude/app</strong> folders on GitHub. We will walk through the code, block <span class="No-Break">by block:</span></p>
			<ol>
				<li>Create a folder named <strong class="source-inline">app</strong> and inside it, create a <strong class="source-inline">main.py</strong> code file. First, we import the necessary files and create a client to access Amazon Bedrock <span class="No-Break">runtime APIs:</span><pre class="source-code">
import boto3
from langchain_community.chat_models import BedrockChat
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
bedrock = boto3.client(service_name='bedrock-runtime', region_name="us-east-1")</pre></li>				<li>Next, we define a dictionary of parameters that are important for working <span class="No-Break">with Claude:</span><pre class="source-code">
inference_modifier = {
    "max_tokens": 4096,
    "temperature": 0.5,
    "top_k": 250,
    "top_p": 1,
    "stop_sequences": ["\n\nHuman:"],
}</pre></li>				<li>Next, we will configure a function<a id="_idIndexMarker741"/> to allow the choosing <a id="_idIndexMarker742"/>of the preferred foundational model. With the choice, we will return a model object that can access Bedrock <span class="No-Break">through Langchain:</span><pre class="source-code">
def choose_model(option):
    modelId = ""
    if option == "Claude 3 Haiku":
        modelId = "anthropic.claude-3-haiku-20240307-v1:0"
    elif option == "Claude 3 Sonnet":
        modelId = "anthropic.claude-3-sonnet-20240229-v1:0"
    model = BedrockChat(
        model_id=modelId,
        client=bedrock,
        model_kwargs=inference_modifier,
        streaming=True,
        callbacks=[StreamingStdOutCallbackHandler()],
    )
    return model</pre></li>				<li>Now, we will add a small function to reset the <span class="No-Break">conversation history:</span><pre class="source-code">
def reset_conversation():
    st.session_state.messages = []</pre></li>				<li>Next, we will begin the development<a id="_idIndexMarker743"/> of the <strong class="source-inline">main</strong> function<a id="_idIndexMarker744"/> and add some widgets to the application interface. The following code creates a sidebar. In it, we add a selection box with Claude 3 Haiku and Claude 3 Sonnet as options, we write a confirmation message to tell the user which model they are talking to, and we add a <strong class="bold">Reset Chat</strong> button. After that, we run the <strong class="source-inline">choose_model</strong> function to return the class that connects to Bedrock and write the title of the application, <em class="italic">Chat with </em><span class="No-Break"><em class="italic">Claude 3</em></span><span class="No-Break">:</span><pre class="source-code">
def main():
    with st.sidebar:
        option = st.selectbox(
            "What model do you want to talk to?",
            ("Claude 3 Haiku", "Claude 3 Sonnet")
        )
        st.write(f"You are talking to **{option}**")
        st.button('Reset Chat', on_click=reset_conversation)
    model = choose_model(option)
    st.title("Chat with Claude 3")</pre></li>				<li>Next, we will initialize the chat history as an empty list if it doesn’t already exist in <strong class="source-inline">st.session_state</strong>. <strong class="source-inline">st.session_state</strong> is a Streamlit object that persists data across app reruns. Then, we iterate over the <strong class="source-inline">messages</strong> list in <strong class="source-inline">st.session_state</strong> and display each message in a chat message container. The <strong class="source-inline">st.chat_message</strong> function creates a chat message container with the specified role (e.g., <strong class="source-inline">user</strong> or <strong class="source-inline">assistant</strong>). The <strong class="source-inline">st.markdown</strong> function displays the message content inside <span class="No-Break">the container:</span><pre class="source-code">
if "messages" not in st.session_state:
        st.session_state.messages = []
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])</pre></li>				<li>Next, we handle user input<a id="_idIndexMarker745"/> and display<a id="_idIndexMarker746"/> the conversation. The <strong class="source-inline">st.chat_input</strong> function creates an input field where the user can enter their prompt. If the user enters a prompt, the following steps are executed: (1) the user’s prompt is added to the <strong class="source-inline">messages</strong> list in <strong class="source-inline">st.session_state</strong> with the <strong class="source-inline">user</strong> role; (2) the user’s prompt is displayed in a chat message container with the <strong class="source-inline">user</strong> role; (3) the <strong class="source-inline">model.stream(prompt)</strong> function is called, which sends the user’s prompt to the Bedrock model and streams the response back. The <strong class="source-inline">st.write_stream</strong> function displays the streamed response in real-time; (4) the assistant’s response is added to the <strong class="source-inline">messages</strong> list in <strong class="source-inline">st.session_state</strong> with the <span class="No-Break"><strong class="source-inline">assistant</strong></span><span class="No-Break"> role:</span><pre class="source-code">
    if prompt := st.chat_input("Enter your prompt here"):
        st.session_state.messages.append(
            {"role": "user", "content": prompt}
        )
        with st.chat_message("user"):
            st.markdown(prompt)
        with st.chat_message("assistant"):
            response = st.write_stream(
                model.stream(prompt)
            )
        st.session_state.messages.append(
            {"role": "assistant", "content": response}
        )</pre></li>				<li>Finally, we call the main function<a id="_idIndexMarker747"/> to start the <span class="No-Break">Streamlit</span><span class="No-Break"><a id="_idIndexMarker748"/></span><span class="No-Break"> application:</span><pre class="source-code">
if __name__ == "__main__":
    main()</pre><p class="list-inset">If you want to run this application locally, here is a <span class="No-Break"><strong class="source-inline">requirements.txt</strong></span><span class="No-Break"> file:</span></p><pre class="source-code">boto3==1.34.22
langchain-community==0.0.33
langchain==0.1.16
streamlit==1.34.0</pre><p class="list-inset">Install the necessary libraries with <span class="No-Break">the following:</span></p><pre class="source-code">pip install -r requirements.txt</pre><p class="list-inset">If you have the libraries already installed, authenticate your AWS CLI with the <strong class="source-inline">aws configure</strong> command and start the application locally with <span class="No-Break">the following:</span></p><pre class="source-code">streamlit run main.py</pre><p class="list-inset">This is an awesome<a id="_idIndexMarker749"/> way of testing the application<a id="_idIndexMarker750"/> before building a container image for deployment. You can test and modify the application as <span class="No-Break">you wish.</span></p><p class="list-inset">When it is ready, now, let’s build a container image <span class="No-Break">for deployment.</span></p></li>				<li>The following<a id="_idIndexMarker751"/> is a simple <strong class="bold">Dockerfile</strong> to build <span class="No-Break">the image:</span><p class="list-inset"><span class="No-Break"><strong class="bold">Dockerfile</strong></span></p><pre class="source-code">
FROM python:3.9-slim
WORKDIR /app
RUN apt-get update &amp;&amp; apt-get install -y \
    build-essential \
    curl \
    software-properties-common \
    git \
    &amp;&amp; rm -rf /var/lib/apt/lists/*
COPY app /app/
EXPOSE 8501
HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health
RUN pip3 install -r requirements.txt
ENTRYPOINT ["streamlit", "run", "main.py", "--server.port=8501", "--server.address=0.0.0.0"]</pre><p class="list-inset">This Dockerfile starts with the Python 3.9 slim base image and sets the working directory to <strong class="source-inline">/app</strong>. It then installs various system packages required for the application, such as <strong class="source-inline">build-essential</strong>, <strong class="source-inline">curl</strong>, <strong class="source-inline">software-properties-common</strong>, and <strong class="source-inline">git</strong>. The application code is copied into the <strong class="source-inline">/app</strong> directory, and the container exposes port <strong class="source-inline">8501</strong>. A health check is set up to check whether the Streamlit application is running correctly on <a href="http://localhost:8501/_stcore/health">http://localhost:8501/_stcore/health</a>. The required Python packages are installed using <strong class="source-inline">pip3</strong> based on the <strong class="source-inline">requirements.txt</strong> file. Finally, the <strong class="source-inline">ENTRYPOINT</strong> command starts the Streamlit application by running <strong class="source-inline">streamlit run main.py</strong> and specifying the server port <span class="No-Break">and address.</span></p></li>				<li>To build the image locally, type <span class="No-Break">the following:</span><pre class="source-code">
docker build --platform linux/amd64 -t &lt;YOUR_USERNAME&gt;/chat-with-claude:v1 .</pre><p class="list-inset">Remember to change <strong class="source-inline">&lt;YOUR_USERNAME&gt;</strong> to your actual Docker Hub username. Then, push the image with <span class="No-Break">the following:</span></p><pre class="source-code">docker push &lt;YOUR_USERNAME&gt;/chat-with-claude:v1</pre></li>			</ol>
			<p>Remember that this image <a id="_idIndexMarker752"/>is going to be publicly available on Docker Hub. Don’t put any authentication credentials or sensitive data in the code<a id="_idIndexMarker753"/> or as <span class="No-Break">environment variables!</span></p>
			<p>Now, let’s deploy our application <span class="No-Break">on Kubernetes.</span></p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor175"/>Deploying the Streamlit app</h2>
			<p>As we have seen before, to deploy<a id="_idIndexMarker754"/> our app on Kubernetes, we need a <strong class="source-inline">Deployment</strong> and a <strong class="source-inline">Service</strong> <strong class="source-inline">.yaml</strong> definition. We can provide both in a <span class="No-Break">single file:</span></p>
			<ol>
				<li>First, create a <strong class="source-inline">deploy_chat_with_claude.yaml</strong> file with <span class="No-Break">this code:</span><p class="list-inset"><span class="No-Break"><strong class="bold">deploy_chat_with_claude.yaml</strong></span></p><pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chat-with-claude
spec:
  replicas: 1
  selector:
    matchLabels:
      app: chat-with-claude
  template:
    metadata:
      labels:
        app: chat-with-claude
    spec:
      containers:
      - name: chat-with-claude
        image: docker.io/neylsoncrepalde/chat-with-claude:v1
        ports:
        - containerPort: 8501
        env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: aws_access_key_id
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: aws_secret_access_key</pre><p class="list-inset">The first part of the code defines a <strong class="source-inline">Deployment</strong> resource named <strong class="source-inline">chat-with-claude</strong>. It takes a previously built image (which you can change to your own new image) and opens port <strong class="source-inline">8501</strong> in the container to be accessed from outside the pod. The <strong class="source-inline">spec.template.spec.containers.env</strong> block mounts AWS credentials as environment variables in the container from a secret <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">aws-credentials</strong></span><span class="No-Break">.</span></p></li>				<li>The second part of the code<a id="_idIndexMarker755"/> defines a <strong class="source-inline">LoadBalancer</strong> service for the pods defined in <strong class="source-inline">Deployment</strong>, which listens on port <strong class="source-inline">8501</strong> and directs traffic to port <strong class="source-inline">8501</strong> in the container. Don’t forget <strong class="source-inline">---</strong>, which is necessary to separate several resources in a <span class="No-Break">single file:</span><pre class="source-code">
---
apiVersion: v1
kind: Service
metadata:
  name: chat-with-claude
spec:
  type: LoadBalancer
  ports:
  - port: 8501
    targetPort: 8501
  selector:
    app: chat-with-claude</pre></li>				<li>Now, we are going to create the namespace and the secret and deploy the application with <span class="No-Break">the following:</span><pre class="source-code">
kubectl create namespace genai
kubectl create secret generic aws-credentials --from-literal=aws_access_key_id=&lt;YOUR_ACCESS_KEY_ID&gt; --from-literal=aws_secret_access_key="&lt;YOUR_SECRET_ACCESS_KEY&gt;" -n genai
kubectl apply -f deploy_chat_with_claude.yaml -n genai</pre></li>				<li>That’s it. Wait a few minutes<a id="_idIndexMarker756"/> for <strong class="source-inline">LoadBalancer</strong> to be up and running and check its URL with <span class="No-Break">the following:</span><pre class="source-code">
kubectl get svc -n genai</pre></li>				<li>Now, paste the URL with <strong class="source-inline">:8501</strong> at the end to define the correct port <em class="italic">et voilà!</em> (<span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">).</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer102">
					<img alt="Figure 11.4 – The Chat with Claude 3 app UI" src="image/B21927_11_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – The Chat with Claude 3 app UI</p>
			<p>Now, play a little bit with the assistant. Try Haiku and Sonnet and note their differences in speed and quality of answer. After a few shots, you will notice that asking specific questions to foundational models leads to a hallucination. Ask the model, for instance, who are you. You are going to have a nice surprise (and some laughs). This model<a id="_idIndexMarker757"/> <span class="No-Break">needs context.</span></p>
			<p>In the next section, we will provide some context <span class="No-Break">using RAG.</span></p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor176"/>Building RAG with Knowledge Bases for Amazon Bedrock</h1>
			<p>RAG is a technique used<a id="_idIndexMarker758"/> in generative AI models<a id="_idIndexMarker759"/> to provide additional context and knowledge to foundational models during the generation process. It works by first retrieving relevant information from a knowledge base or corpus of documents, and then using this retrieved information to augment the input to the <span class="No-Break">generative model.</span></p>
			<p>RAG is a good choice for giving context to generative AI models because it allows the model to access and utilize external knowledge sources, which can significantly improve the quality, accuracy, and relevance of the generated output. Without RAG, the model would be limited to the knowledge and patterns it learned during training, which may not always be sufficient or up to date, especially for domain-specific or rapidly <span class="No-Break">evolving topics.</span></p>
			<p>One of the key advantages<a id="_idIndexMarker760"/> of RAG is that it enables the model<a id="_idIndexMarker761"/> to leverage large knowledge bases or document collections, which would be impractical or impossible to include in the model’s training data. This allows the model to generate more informed and knowledgeable outputs, as it can draw upon a vast amount of relevant information. Additionally, RAG can help mitigate issues such as hallucination and bias, as the model has access to authoritative and <span class="No-Break">factual sources.</span></p>
			<p>However, RAG also has some limitations. The quality of the generated output heavily depends on the relevance and accuracy of the retrieved information, which can be influenced by the quality of the knowledge base, the effectiveness of the retrieval mechanism, and the ability of the model to properly integrate the retrieved information. Additionally, RAG can introduce computational overhead and latency, as it requires an additional retrieval step before the <span class="No-Break">generation process.</span></p>
			<p>To build an AI assistant with RAG, we will use the Knowledge Bases for Amazon Bedrock service, a feature in Bedrock that allows you to create and manage a knowledge base seamlessly. Let’s get <span class="No-Break">to it.</span></p>
			<p>For our exercise, we will build an AI assistant capable of giving information about the AWS Competency Program. A visual representation of this assistant’s architecture is shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer103">
					<img alt="Figure 11.5 – Knowledge Bases for Amazon Bedrock application architecture" src="image/B21927_11_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Knowledge Bases for Amazon Bedrock application architecture</p>
			<p>The AWS Competency Program<a id="_idIndexMarker762"/> is a validation program<a id="_idIndexMarker763"/> offered by AWS that recognizes partners who have demonstrated technical proficiency and proven customer success in specialized solution areas. AWS Competencies are awarded to <strong class="bold">AWS Partner Network</strong> (<strong class="bold">APN</strong>) members who have undergone technical validation<a id="_idIndexMarker764"/> related to specific AWS services or workloads, ensuring they have the expertise needed to deliver consistent, high-quality solutions on AWS. These competencies span various areas such as DevOps, migration, data and analytics, machine learning, and security. Each competency has its own rules document and can be quite challenging <span class="No-Break">to understand.</span></p>
			<ol>
				<li>First, we will gather some context information about the program. On GitHub, under the <a href="B21927_11.xhtml#_idTextAnchor167"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><strong class="source-inline">/claude-kb/knowledge-base/</strong> folder, you will find a Python code that will gather information on conversational AI, data and analytics, DevOps, education, energy, financial services, machine learning, and security programs. After saving this code locally, install the Beautiful Soup library with <span class="No-Break">the following:</span><pre class="source-code">
pip install "beautifulsoup4==4.12.2"</pre><p class="list-inset">Then, run the code with <span class="No-Break">the following:</span></p><pre class="source-code">python get_competency_data.py</pre><p class="list-inset">After a few seconds, data should be saved locally on <span class="No-Break">your machine.</span></p></li>				<li>Next, create an S3 bucket<a id="_idIndexMarker765"/> and upload these files. This will<a id="_idIndexMarker766"/> be the base for our <span class="No-Break">RAG layer.</span></li>
				<li>Next, go to the <strong class="bold">Bedrock</strong> page in the AWS console. In the side menu, click on <strong class="bold">Knowledge Bases</strong> and then, click on <strong class="bold">Create knowledge base</strong> (<span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">).</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer104">
					<img alt="Figure 11.6 – The Knowledge bases landing page" src="image/B21927_11_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – The Knowledge bases landing page</p>
			<ol>
				<li value="4">On the next page, choose a name for your knowledge base and select <strong class="bold">Create and use a new service role</strong> under the <strong class="bold">IAM permissions</strong> section. Then, <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>Next, you will configure the data source. Choose a data source name as you wish. For <strong class="bold">Data source location</strong>, make sure the <strong class="bold">This AWS account</strong> box option is checked. Then, in the <strong class="bold">S3 URI</strong> section, click on <strong class="bold">Browse S3</strong> to search for your S3 bucket that contains the AWS Competency datasets (the bucket we created in <em class="italic">Step 2</em>). An example<a id="_idIndexMarker767"/> of that configuration<a id="_idIndexMarker768"/> is shown in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.7</em>. After selecting the S3 bucket, <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer105">
					<img alt="Figure 11.7 – Choosing a data source for the knowledge base" src="image/B21927_11_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – Choosing a data source for the knowledge base</p>
			<p class="list-inset">Next, we are going to choose the embeddings model. This embeddings model is responsible for transforming text or image files into vector<a id="_idIndexMarker769"/> representations called <strong class="bold">embeddings</strong>. These embeddings capture the semantic and contextual information of the input data, allowing for efficient similarity comparisons and retrieval operations. One of Bedrock’s embeddings models, Amazon Titan, should be available by default. If it is not, do the same process of asking for access in <span class="No-Break">the console.</span></p>
			<ol>
				<li value="6">On the next page, in the <strong class="bold">Embeddings model</strong> section, choose <strong class="bold">Titan Embeddings G1 - Text</strong>. In the <strong class="bold">Vector database</strong> section, make sure<a id="_idIndexMarker770"/> the <strong class="bold">Quick create a new vector store</strong> option is checked. This quick creation option<a id="_idIndexMarker771"/> creates a vector database based on OpenSearch Serverless. Leave the other options unmarked and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">OpenSearch is an open-source distributed<a id="_idIndexMarker772"/> search and analytics engine based on Apache Lucene and derived from Elasticsearch. It is a great option for a RAG vector database because it provides efficient full-text search and nearest-neighbor search capabilities for vector embeddings. OpenSearch supports dense vector indexing and retrieval, making it suitable for storing and querying large collections of vector embeddings, which are essential for the retrieval component of <span class="No-Break">RAG models.</span></p>
			<ol>
				<li value="7">Next, review the information to see whether it was correctly provided. If everything looks good, click on <strong class="bold">Create knowledge base</strong>. Be patient. This creation will take several minutes <span class="No-Break">to complete.</span></li>
				<li>After the knowledge base is up and running, go back to the <strong class="bold">Knowledge base</strong> page in Bedrock and click on the knowledge base you just created. On the next page, scroll until you find the <strong class="bold">Data source</strong> section (as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.8</em>). Select the data source and click <strong class="bold">Sync</strong> to start the embedding of the text content. This will also take a <span class="No-Break">few minutes.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer106">
					<img alt="Figure 11.8 – Syncing the knowledge base with its data source" src="image/B21927_11_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – Syncing the knowledge base with its data source</p>
			<p>After the “sync” is ready, we have everything<a id="_idIndexMarker773"/> we need to run<a id="_idIndexMarker774"/> our generative AI assistant with RAG. Now, it is time to adjust the code to let Claude work with the <span class="No-Break">knowledge base.</span></p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor177"/>Adjusting the code for RAG retrieval</h2>
			<p>We will start from the code<a id="_idIndexMarker775"/> we developed earlier to work<a id="_idIndexMarker776"/> with the pure Claude model. As we just need some small modifications, we won’t go through the entire code again. We will take a closer look at the necessary modifications. The complete code for the RAG application is available at <a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/claude-kb/app">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/claude-kb/app</a> folder. If you don’t want to customize your code, you can use the ready-to-go docker image I provided for <span class="No-Break">this example.</span></p>
			<ol>
				<li>First, we need <span class="No-Break">extra imports:</span><pre class="source-code">
import os
from botocore.client import Config
from langchain.prompts import PromptTemplate
from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever
from langchain.chains import RetrievalQA</pre><p class="list-inset">Here, we import the <strong class="source-inline">os</strong> library to get environment variables. The <strong class="source-inline">Config</strong> class will help build a configuration object to access the <strong class="source-inline">bedrock-agent</strong> API. All the other imports relate to accessing the knowledge base and merging the retrieved documents with <span class="No-Break">AI responses.</span></p></li>				<li>Next, we will get the ID<a id="_idIndexMarker777"/> for the Knowledge Bases for Amazon Bedrock service<a id="_idIndexMarker778"/> from an environment variable. This can be a very helpful approach. If we need to change the knowledge base in the future, there is no need to rebuild the image. We just change the environment variable. Then, we set some configurations and create a client for the <strong class="source-inline">bedrock-agent-runtime</strong> API (needed for the <span class="No-Break">knowledge base):</span><pre class="source-code">
kb_id = os.getenv("KB_ID")
bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})
bedrock_agent_client = boto3.client(
    "bedrock-agent-runtime", config=bedrock_config, region_name = "us-east-1"
)</pre></li>				<li>Next, we will configure a prompt template that will help us chain the retrieved documents from the knowledge base and the user questions. At the end, we instantiate an object that will hold the template and receive<a id="_idIndexMarker779"/> the documents and the user questions<a id="_idIndexMarker780"/> <span class="No-Break">as inputs:</span><pre class="source-code">
PROMPT_TEMPLATE = """
Human: You are a friendly AI assistant and provide answers to questions about AWS competency program for partners.
Use the following pieces of information to provide a concise answer to the question enclosed in &lt;question&gt; tags.
Don't use tags when you generate an answer. Answer in plain text, use bullets or lists if needed.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
&lt;context&gt;
{context}
&lt;/context&gt;
&lt;question&gt;
{question}
&lt;/question&gt;
The response should be specific and use statistics or numbers when possible.
Assistant:"""
claude_prompt = PromptTemplate(template=PROMPT_TEMPLATE,
                               input_variables=["context","question"])</pre></li>				<li>After setting the <strong class="source-inline">choose_model()</strong> function, we need<a id="_idIndexMarker781"/> to instantiate a <strong class="source-inline">retriever</strong> class that will pull documents<a id="_idIndexMarker782"/> from the <span class="No-Break">knowledge base:</span><pre class="source-code">
retriever = AmazonKnowledgeBasesRetriever(
        knowledge_base_id=kb_id,
        retrieval_config={
            "vectorSearchConfiguration": {
                "numberOfResults": 4
            }
        },
        client=bedrock_agent_client
    )</pre></li>				<li>Now, inside the <strong class="source-inline">main</strong> function, we will add <strong class="source-inline">RetrievalQA</strong>. This class is used for building question-answering systems that can retrieve relevant information from the <span class="No-Break">knowledge base:</span><pre class="source-code">
qa = RetrievalQA.from_chain_type(
        llm=model,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=False,
        chain_type_kwargs={"prompt": claude_prompt}
    )</pre></li>				<li>Finally, we will modify the response to give the <span class="No-Break">entire answer:</span><pre class="source-code">
with st.chat_message("assistant"):
    response = qa.invoke(prompt)['result']
    st.write(response)</pre><p class="list-inset">That’s it. The code is ready to be built in a new image. You can rebuild it by creating a new Dockerfile with the same code we used before. When running the <strong class="source-inline">docker build</strong> command, remember to choose a different image name (or, at least, a <span class="No-Break">different version).</span></p></li>				<li>Next, we will start<a id="_idIndexMarker783"/> the deployment. The <strong class="source-inline">.yaml</strong> file is also very similar<a id="_idIndexMarker784"/> to the one we did in the last section (but remember to change all the names for the deployment, services, container, and label to <strong class="source-inline">rag-with-claude</strong>). A full version of this code is available in the GitHub repository). We only need to declare the environment variable for the knowledge base ID. As this is not a sensitive credential, we don’t need to use a Kubernetes secret for that. We will use <strong class="source-inline">ConfigMap</strong>. The <strong class="source-inline">spec.template.spec.container.env</strong> section of your <strong class="source-inline">.yaml</strong> file should look <span class="No-Break">like this:</span><pre class="source-code">
env:
  - name: AWS_ACCESS_KEY_ID
    valueFrom:
      secretKeyRef:
        name: aws-credentials
        key: aws_access_key_id
  - name: AWS_SECRET_ACCESS_KEY
    valueFrom:
      secretKeyRef:
        name: aws-credentials
        key: aws_secret_access_key
  - name: KB_ID
    valueFrom:
      configMapKeyRef:
        name: kb-config
        key: kb_id</pre><p class="list-inset">Note that we added a new environment variable called <strong class="source-inline">KB_ID</strong> that will be imported <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">ConfigMap</strong></span><span class="No-Break">.</span></p></li>				<li>To deploy the new application, we run <span class="No-Break">the following:</span><pre class="source-code">
kubectl create configmap kb-config --from-literal=kb_id=&lt;YOUR_KB_ID&gt; -n genai</pre><p class="list-inset">We run the preceding<a id="_idIndexMarker785"/> to create <strong class="source-inline">ConfigMap</strong> with the knowledge<a id="_idIndexMarker786"/> base ID, and then <span class="No-Break">we run:</span></p><pre class="source-code">kubectl apply -f deploy_chat_with_claude.yaml -n genai</pre><p class="list-inset">We run the preceding to deploy the application. Wait a few minutes for <strong class="source-inline">LoadBalancer</strong> to be up and use the <span class="No-Break">following command:</span></p><pre class="source-code">kubectl get svc -n genai</pre><p class="list-inset">Use the preceding command to get the URL for <strong class="source-inline">LoadBalancer</strong>. Copy and paste the service named <strong class="source-inline">rag-with-claude</strong> in a browser and add <strong class="source-inline">:8501</strong> to connect to the exposed port. <em class="italic">Et voilà!</em> You should see your new application running as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">.</span></p></li>			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer107">
					<img alt="Figure 11.9 – RAG application UI" src="image/B21927_11_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – RAG application UI</p>
			<p>Try to play a little bit<a id="_idIndexMarker787"/> with this application. You will see that if you ask questions<a id="_idIndexMarker788"/> not related to its scope (AWS Competency program), the assistant will say it <span class="No-Break">cannot answer.</span></p>
			<p>Now, we will move to the final part of this chapter and learn how to make generative AI models execute actions <span class="No-Break">with agents.</span></p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor178"/>Building action models with agents</h1>
			<p>Agents are the newest feature<a id="_idIndexMarker789"/> in the generative AI world. They are powerful tools that enable the automation of tasks by allowing generative AI models to take actions on our behalf. They act as intermediaries between the generative AI models and external systems or services, facilitating the execution of tasks in the <span class="No-Break">real world.</span></p>
			<p>Under the hood, an agent “understands” what the user wants and calls a backend function that performs the action. The scope within which the agent can act is defined by an OpenAPI schema that it will use both to “understand” what it does and how to properly call the <span class="No-Break">backend function.</span></p>
			<p>So, in summary, to build an agent we need an OpenAPI schema, a backend function, and a knowledge base. The knowledge base is optional, but it can greatly improve a user’s experience with the <span class="No-Break">AI assistant.</span></p>
			<p>For this section’s exercise, we will build an agent<a id="_idIndexMarker790"/> that “knows” the available information about the AWS Competency program. A visual representation of the agent’s application architecture is shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer108">
					<img alt="Figure 11.10 – Agent application architecture" src="image/B21927_11_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – Agent application architecture</p>
			<p>This agent is going to build a simple worksheet with a use case’s information, save the worksheet to Amazon S3, and register the information on a DynamoDB table for consultation. Let’s get <span class="No-Break">to it:</span></p>
			<ol>
				<li>First, we need an OpenAPI schema defining the methods available for our agent. In this case, we will define two methods. The first one, <strong class="source-inline">generateCaseSheet</strong>, registers the use case information and builds the worksheet. The second, <strong class="source-inline">checkCase</strong>, takes the use case ID and returns information about it. As this is a long JSON file, we will not display it here. The complete code is available at <a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent</a> folder. Copy this code and save it in an <span class="No-Break">S3 bucket.</span></li>
				<li>Next, we will define a Lambda function<a id="_idIndexMarker791"/> that will serve as a backend for the agent. The complete Python code for the function is available in the book’s GitHub repository under the <a href="B21927_11.xhtml#_idTextAnchor167"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><strong class="source-inline">/agent/function</strong> folder. In your machine, create a folder named <strong class="source-inline">function</strong> and save this code as <strong class="source-inline">lambda_function.py</strong> in the <strong class="source-inline">function</strong> folder. This code defines a Lambda function that serves as the backend for a Bedrock agent. The function handles two different API paths: <strong class="source-inline">/generateCaseSheet</strong> and <strong class="source-inline">/checkCase</strong>. Let’s go through the code block by block. After importing the necessary folders, we define two helper functions to extract parameter values from the event object (<strong class="source-inline">get_named_parameter</strong> and <strong class="source-inline">get_named_property</strong>).  The <strong class="source-inline">generateCaseSheet</strong> function is responsible for creating a new case sheet based on the provided information. It extracts the required parameters from the event object, generates a unique ID, creates a new Excel workbook using the <strong class="source-inline">CaseTemplate</strong> class, fills in the template with the provided parameters, saves the workbook to a temporary file, uploads it to an S3 bucket, and stores the case sheet information in a DynamoDB table. Finally, it returns a response object containing the case details. The <strong class="source-inline">checkCase</strong> function retrieves the case sheet information from the DynamoDB table based on the provided <strong class="source-inline">caseSheetId</strong> parameter and returns a response object containing the case details. The <strong class="source-inline">lambda_handler</strong> function is the entry point for the Lambda function. It determines the appropriate action based on the <strong class="source-inline">apiPath</strong> value in the event object. The function constructs the appropriate response object based on the action and <span class="No-Break">returns it.</span></li>
				<li>Next, inside the <strong class="source-inline">function</strong> folder, create a new file called <strong class="source-inline">lambda_requirements.txt</strong> where we will list the dependencies for the Lambda function code. In the <strong class="source-inline">lambda_requirements.txt</strong> file, type <strong class="source-inline">openpyxl==3.0.10</strong> and <span class="No-Break">save it.</span></li>
				<li>Now, before deploying<a id="_idIndexMarker792"/> the function, we need to create an IAM role that will give Lambda the necessary permissions. On the AWS console, go to the IAM page, choose <strong class="bold">Roles</strong> in the side menu, and click on <strong class="bold">Create a </strong><span class="No-Break"><strong class="bold">new role</strong></span><span class="No-Break">.</span></li>
				<li>On the next page, select <strong class="bold">AWS service</strong> for the <strong class="bold">Trusted entity type</strong> and <strong class="bold">Lambda</strong> for the <strong class="bold">Use case</strong> (as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.11</em>). <span class="No-Break">Click </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer109">
					<img alt="Figure 11.11 – Selecting trusted entity and AWS service" src="image/B21927_11_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11 – Selecting trusted entity and AWS service</p>
			<ol>
				<li value="6">Now, we will select a permission policy. Choose <strong class="bold">Administrator Access</strong> and click <strong class="bold">Next</strong>. Remember that having such open permissions is <em class="italic">not</em> a good practice for production environments. You should set permissions only for the actions and <span class="No-Break">resources needed.</span></li>
				<li>Then, choose a name for your IAM role (<strong class="source-inline">BDOK-Lambda-service-role</strong>, for instance) and click on <span class="No-Break"><strong class="bold">Create role</strong></span><span class="No-Break">.</span></li>
				<li>Then, you will see the IAM <strong class="bold">Roles</strong> page again. Search<a id="_idIndexMarker793"/> for your created role and click on it (<span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">).</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer110">
					<img alt="Figure 11.12 – Selecting your created IAM role" src="image/B21927_11_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.12 – Selecting your created IAM role</p>
			<ol>
				<li value="9">On the role’s page, you will see the <strong class="bold">Amazon Resource Name</strong> (<strong class="bold">ARN</strong>) of the role. Copy it and save<a id="_idIndexMarker794"/> it for later. We will need that name to deploy the <span class="No-Break">Lambda function.</span></li>
				<li>Next, inside the <strong class="source-inline">function</strong> folder you created, create a new folder called <strong class="source-inline">worksheet</strong>. Copy two files from <a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent/function/worksheet">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent/function/worksheet</a>, the first named <strong class="source-inline">__init__.py</strong> and the second named <strong class="source-inline">template.py</strong> and place those code files inside the <strong class="source-inline">worksheet</strong> folder. This code contains a class named <strong class="source-inline">CaseTemplate</strong> that builds an Excel worksheet with the <strong class="source-inline">openpyxl</strong> <span class="No-Break">Python library.</span></li>
				<li>Next, copy another two files in <a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent/scripts">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent/scripts</a> folder named <strong class="source-inline">build_lambda_package.sh</strong> and <strong class="source-inline">create_lambda_function.sh</strong>. Those files contain bash code that will install the dependencies for the Lambda function and deploy it <span class="No-Break">to AWS.</span></li>
				<li>Now, we will deploy our Lambda function. This is a good time to check whether your project structure is correct. The folder and file structure should look <span class="No-Break">like this:</span><pre class="source-code">
<strong class="bold">├── app</strong>
<strong class="bold">│   └── main.py</strong>
<strong class="bold">├── function</strong>
<strong class="bold">│   ├── lambda_function.py</strong>
<strong class="bold">│   ├── lambda_requirements.txt</strong>
<strong class="bold">│   ├── test_event.json</strong>
<strong class="bold">│   └── worksheet</strong>
<strong class="bold">│       ├── __init__.py</strong>
<strong class="bold">│       └── template.py</strong>
<strong class="bold">├── openapi_schema.json</strong>
<strong class="bold">└── scripts</strong>
<strong class="bold">    ├── build_lambda_package.sh</strong>
<strong class="bold">    └── create_lambda_function.sh</strong></pre><p class="list-inset">If your project structure is different, correct it to get this exact structure or the bash code will not <span class="No-Break">work properly.</span></p></li>				<li>Now, go to the <strong class="source-inline">scripts</strong> folder and run the <span class="No-Break">following commands:</span><pre class="source-code">
sh build_lambda_package.sh
sh create_lambda_function.sh "&lt;YOUR_ROLE_ARN&gt;"</pre></li>			</ol>
			<p>Remember to change <strong class="source-inline">&lt;YOUR_ROLE_ARN&gt;</strong> to the actual ARN<a id="_idIndexMarker795"/> of your Lambda IAM role. Now, we have some more work to do. Next, we will create the DynamoDB table to store information about the <span class="No-Break">use cases.</span></p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor179"/>Creating a DynamoDB table</h2>
			<p>DynamoDB is a fully managed<a id="_idIndexMarker796"/> NoSQL database service. It is a key-value and document database that can deliver single-digit millisecond performance at any scale. DynamoDB is optimized for running serverless applications and is designed to scale up or down automatically to meet demand, without having to provision or manage servers. It is particularly well suited for applications that need low-latency read and write access to data at any scale. Its extremely low latency makes it a very good choice for an AI assistant application. Let’s get <span class="No-Break">to it:</span></p>
			<ol>
				<li>In the AWS console, navigate to the <strong class="bold">DynamoDB</strong> page. In the side menu, click on <strong class="bold">Tables</strong> and then, click on <span class="No-Break"><strong class="bold">Create table</strong></span><span class="No-Break">.</span></li>
				<li>On the next page, fill in <strong class="bold">Table name</strong> with <strong class="source-inline">case-sheets</strong> and the <strong class="bold">Partition key</strong> field with <strong class="source-inline">caseSheetId</strong>. Remember to select <strong class="bold">Number</strong> to indicate that this entry is a number, as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.13</em>. Leave all the other configurations to default and click <span class="No-Break"><strong class="bold">Create table</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer111">
					<img alt="Figure 11.13 – Creating a DynamoDB table" src="image/B21927_11_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.13 – Creating a DynamoDB table</p>
			<p>In a few seconds, you should<a id="_idIndexMarker797"/> have your DynamoDB table ready for use. Now, we will configure the <span class="No-Break">Bedrock agent.</span></p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor180"/>Configuring the agent</h2>
			<p>Now, in the last part of this section, we will configure<a id="_idIndexMarker798"/> a Bedrock agent and link it to its backend Lambda function and the knowledge base database. Let’s get <span class="No-Break">to it:</span></p>
			<ol>
				<li>First, in the AWS console, search for <strong class="source-inline">Bedrock</strong> and, in the side menu, click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Agents</strong></span><span class="No-Break">.</span></li>
				<li>In the pop-up box, enter the name of your agent (<strong class="source-inline">aws-competency-agent</strong>) and click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Create</strong></span><span class="No-Break">.</span></li>
				<li>Next, you will see the agent configuration page. Scroll down to <strong class="bold">Select model</strong> and choose the Anthropic model <strong class="bold">Claude 3 Haiku</strong> (you can also play with the other available models as <span class="No-Break">you like).</span></li>
				<li>In the <strong class="bold">Instructions for the agent</strong> field, set the initial instructions<a id="_idIndexMarker799"/> that will guide the foundational model in its behavior. You can type, for instance: <strong class="source-inline">You are a friendly AI assistant. Your main goal is to help AWS partner companies build case sheets for the AWS Competency program, register those cases, and tell the user the information about the registered cases. When you generate a case sheet, always show back to the user the ID of the case sheet (id), the client's name (client), and the name of the case (casename) and confirm that the case was successfully created. Also, answer the questions of the user about what you can do and how you can help.</strong> This is a very important part of the agent configuration. Play with these instructions as much as you like. An example of this screen is shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer112">
					<img alt="Figure 11.14 – Configuring the agent’s instructions" src="image/B21927_11_14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.14 – Configuring the agent’s instructions</p>
			<ol>
				<li value="5">After that, click on the <strong class="bold">Save</strong> button at the top of the page<a id="_idIndexMarker800"/> to make AWS create the necessary <span class="No-Break">permission policy.</span></li>
				<li>Next, scroll down to the <strong class="bold">Action groups</strong> section and click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Add</strong></span><span class="No-Break">.</span></li>
				<li>On the next page, select a name for your action group. For <strong class="bold">Action group type</strong>, select <strong class="bold">Define with API schemas</strong>. In <strong class="bold">Action group invocation</strong>, select <strong class="bold">Select an existing Lambda function</strong> and select the Lambda function we just created (<span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.15</em></span><span class="No-Break">).</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer113">
					<img alt="Figure 11.15 – Selecting a Lambda function for the agent’s action group" src="image/B21927_11_15.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.15 – Selecting a Lambda function for the agent’s action group</p>
			<ol>
				<li value="8">Now, in the <strong class="bold">Action group schema</strong> section, choose <strong class="bold">Select an existing API schema</strong> and then, click on <strong class="bold">Browse S3</strong> to search for the OpenAPI schema<a id="_idIndexMarker801"/> we have saved on S3 (<span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.16</em>). Then, click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Create</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer114">
					<img alt="Figure 11.16 – Selecting the OpenAPI schema" src="image/B21927_11_16.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.16 – Selecting the OpenAPI schema</p>
			<ol>
				<li value="9">Next, in the <strong class="bold">Knowledge base</strong> section, click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Add</strong></span><span class="No-Break">.</span></li>
				<li>Select the knowledge base we have created before and type some instructions for the agent on how to use it. For instance: <strong class="source-inline">This knowledge base contains information on the following AWS Competency programs: conversational AI, data and analytics, DevOps, education, energy, financial services, machine learning, and security</strong>. Make sure <strong class="bold">Knowledge base status</strong> is set to <strong class="bold">Enabled</strong> (<span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.17</em>). Click <strong class="bold">Save </strong><span class="No-Break"><strong class="bold">and exit</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer115">
					<img alt="Figure 11.17 – Attaching a knowledge base to the agent" src="image/B21927_11_17.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.17 – Attaching a knowledge base to the agent</p>
			<ol>
				<li value="11">Now, you are back to the agent’s editing<a id="_idIndexMarker802"/> page. Nothing else is needed here, so you can click on <strong class="bold">Prepare</strong> at the top to get your agent ready to run, and then click on <strong class="bold">Save </strong><span class="No-Break"><strong class="bold">and exit</strong></span><span class="No-Break">.</span></li>
				<li>Now, you will be led back to the agent’s main page. Scroll down to the <strong class="bold">Aliases</strong> section and click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Create</strong></span><span class="No-Break">.</span></li>
				<li>Type in an alias name (<strong class="source-inline">aws-competency</strong>, for instance) and click on <span class="No-Break"><strong class="bold">Create Alias</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer116">
					<img alt="Figure 11.18 – Creating an alias" src="image/B21927_11_18.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.18 – Creating an alias</p>
			<ol>
				<li value="14">Now, the last thing to do is register a permission<a id="_idIndexMarker803"/> on Lambda for this agent to trigger the function execution. On the agent’s main page, copy the <span class="No-Break">agent ARN.</span></li>
				<li>Next, go to the <strong class="bold">Lambda</strong> page and click on the function we created for this exercise. On the function’s main page, scroll down, click on <strong class="bold">Configuration</strong>, and then click on <strong class="bold">Permissions</strong> in the side menu (<span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.19</em></span><span class="No-Break">).</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer117">
					<img alt="Figure 11.19 – Lambda permissions" src="image/B21927_11_19.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.19 – Lambda permissions</p>
			<ol>
				<li value="16">Scroll down again<a id="_idIndexMarker804"/> to the <strong class="bold">Resource-based policy statements</strong> section and click on <span class="No-Break"><strong class="bold">Add permissions</strong></span><span class="No-Break">.</span></li>
				<li>On the next page, fill in the <strong class="bold">Statement ID</strong> box. For <strong class="bold">Principal</strong>, paste the agent ARN you copied from the Bedrock agent. For <strong class="bold">Action</strong>, choose <strong class="source-inline">lambda:InvokeFunction</strong> (<span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.20</em>). Then, click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Save</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer118">
					<img alt="Figure 11.20 – Configuring the Lambda permission for the Bedrock agent" src="image/B21927_11_20.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.20 – Configuring the Lambda permission for the Bedrock agent</p>
			<p>That’s all the configuration<a id="_idIndexMarker805"/> we need to get the agent running. Now, it’s time for deployment. Let’s get our Streamlit application <span class="No-Break">to Kubernetes.</span></p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor181"/>Deploying the application on Kubernetes</h2>
			<p>Deploying the agent Streamlit application<a id="_idIndexMarker806"/> on Kubernetes follows the same path<a id="_idIndexMarker807"/> we did for the other two applications deployed before. The only thing different is that we must create a new <strong class="source-inline">configmap</strong> with the agent’s ID and its <span class="No-Break">alias ID:</span></p>
			<ol>
				<li>Go to the <strong class="bold">Agent</strong> page in the AWS console and copy the agent’s ID (in the top section) and the alias ID (in the <span class="No-Break">bottom section).</span></li>
				<li>Now, create <strong class="source-inline">configmap</strong> with those parameters with the <span class="No-Break">following command:</span><pre class="source-code">
kubectl create configmap agent-config --from-literal=agent_alias_id=&lt;YOUR_ALIAS_ID&gt; --from-literal=agent_id=&lt;YOUR_AGENT_ID&gt; -n genai</pre><p class="list-inset">Remember to replace the <strong class="source-inline">&lt;YOUR_ALIAS_ID&gt;</strong> and <strong class="source-inline">&lt;YOUR_AGENT_ID&gt;</strong> placeholders with the <span class="No-Break">actual values.</span></p></li>				<li>Build a custom image<a id="_idIndexMarker808"/> if you want<a id="_idIndexMarker809"/> to customize the application. If you don’t, use the ready image from <span class="No-Break">DockerHub (</span><a href="https://hub.docker.com/r/neylsoncrepalde/chat-with-claude-agent"><span class="No-Break">https://hub.docker.com/r/neylsoncrepalde/chat-with-claude-agent</span></a><span class="No-Break">).</span></li>
				<li>Next, we will define a <strong class="source-inline">deploy_agent.yaml</strong> file for the application and service deployment. The content for this file is available at <a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent"><span class="No-Break">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter11/agent</span></a><span class="No-Break"> folder.</span></li>
				<li>With this file copied locally, now run <span class="No-Break">the following:</span><pre class="source-code">
kubectl apply -f deploy_agent.yaml -n genai</pre></li>				<li>Wait for a few seconds for <strong class="source-inline">LoadBalancer</strong> to get started. Then, run the following to get the URL <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">LoadBalancer</strong></span><span class="No-Break">:</span><pre class="source-code">
kubectl get svc -n genai</pre><p class="list-inset">Paste it in a browser adding the correct port (<strong class="source-inline">:8501</strong>) to see the magic happening (<span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.21</em></span><span class="No-Break">).</span></p></li>			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer119">
					<img alt="Figure 11.21 – AWS Competency agent application UI" src="image/B21927_11_21.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.21 – AWS Competency agent application UI</p>
			<p>Try inserting a prompt<a id="_idIndexMarker810"/> for the creation of a new use case<a id="_idIndexMarker811"/> as in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.18</em>. Also, you can check specific information about this case passing the case’s ID (<span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.22</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer120">
					<img alt="Figure 11.22 – Checking use case information with the agent" src="image/B21927_11_22.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.22 – Checking use case information with the agent</p>
			<p>Play a little bit. Ask questions about the Competency program and try registering different cases. Also, you can check<a id="_idIndexMarker812"/> AWS DynamoDB<a id="_idIndexMarker813"/> and see the information ingested in our created table and check S3 to see the Excel files the <span class="No-Break">agent created.</span></p>
			<p>That is it! Congratulations! You have just deployed a full generative AI agent application that can perform tasks for you using natural language <span class="No-Break">on Kubernetes.</span></p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor182"/>Summary</h1>
			<p>In this chapter, we explored the exciting world of generative AI and learned how to harness its power on Kubernetes. We started by understanding the fundamental concepts of generative AI, its underlying mechanisms, and how it differs from traditional machine <span class="No-Break">learning approaches.</span></p>
			<p>We then leveraged Amazon Bedrock, a comprehensive suite of services, to build and deploy generative AI applications. We learned how to work with Bedrock’s foundational models, such as Claude 3 Haiku and Claude 3 Sonnet, and how to integrate them into a Streamlit application for interactive <span class="No-Break">user experiences.</span></p>
			<p>Next, we delved into the concept of RAG, which combines the power of generative AI with external knowledge bases. We built a RAG system using Knowledge Bases for Amazon Bedrock, enabling our application to access and leverage vast amounts of structured data, improving the accuracy and relevance of the <span class="No-Break">generated output.</span></p>
			<p>Finally, we explored Agents for Amazon Bedrock, a powerful feature that allows generative AI models to automate tasks and take actions on our behalf. We learned how to build an agent, define its capabilities through an OpenAPI schema, and create the underlying Lambda function that serves as the backend for <span class="No-Break">our agent.</span></p>
			<p>Throughout this chapter, we gained hands-on experience in building and deploying generative AI applications on Kubernetes. The skills and knowledge acquired in this chapter are invaluable in today’s rapidly evolving technological landscape. Generative AI is transforming industries and revolutionizing how we interact with and leverage AI. By mastering the tools and techniques presented in this chapter, you will be well equipped to build innovative and intelligent applications that can generate human-like content, leverage external knowledge sources, and <span class="No-Break">automate tasks.</span></p>
			<p>In the next chapter, we will discuss some important points needed for a production-ready Kubernetes environment, which we did not have space to discuss throughout <span class="No-Break">the book.</span></p>
		</div>
	</body></html>