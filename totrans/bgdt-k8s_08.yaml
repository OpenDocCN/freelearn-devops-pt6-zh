- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying the Big Data Stack on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover the deployment of key big data technologies –
    Spark, Airflow, and Kafka – on Kubernetes. As container orchestration and management
    have become critical for running data workloads efficiently, Kubernetes has emerged
    as the de facto standard. By the end of this chapter, you will be able to successfully
    deploy and manage big data stacks on Kubernetes for building robust data pipelines
    and applications.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by deploying Apache Spark on Kubernetes using the Spark operator.
    You will learn how to configure and monitor Spark jobs running as Spark applications
    on your Kubernetes cluster. Being able to run Spark workloads on Kubernetes brings
    important benefits such as dynamic scaling, versioning, and unified resource management.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will deploy Apache Airflow on Kubernetes. You will configure Airflow
    on Kubernetes, link its logs to S3 for easier debugging and monitoring, and set
    it up to orchestrate data pipelines built using tools such as Spark. Running Airflow
    on Kubernetes improves reliability, scaling, and resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will deploy Apache Kafka on Kubernetes, which is critical for streaming
    data pipelines. Running Kafka on Kubernetes simplifies operations, scaling, and
    cluster management.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have hands-on experience with deploying
    and managing big data stacks on Kubernetes. This will enable you to build robust,
    reliable data applications leveraging Kubernetes as your container orchestration
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Spark on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Airflow on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Kafka on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the activities in this chapter, you should have an AWS account and `kubectl`,
    `eksctl`, and `helm` installed. For instructions on how to set up an AWS account
    and `kubectl` and `eksctl` installation, refer to [*Chapter 1*](B21927_01.xhtml#_idTextAnchor015).
    For `helm` installation instructions, access [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/).
  prefs: []
  type: TYPE_NORMAL
- en: We will also be using the Titanic dataset for our exercises. You can find the
    version we will use at [https://github.com/neylsoncrepalde/titanic_data_with_semicolon](https://github.com/neylsoncrepalde/titanic_data_with_semicolon).
  prefs: []
  type: TYPE_NORMAL
- en: All code in this chapter is available in the GitHub repository at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes),
    in the `Chapter08` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Spark on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To help us deploy resources on Kubernetes, we are going to use **Helm**. Helm
    is a package manager for Kubernetes that helps install applications and services.
    Helm uses templates called **Charts**, which package up installation configuration,
    default settings, dependencies, and more, into an easy-to-deploy bundle.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, we have **Operators**. Operators are custom controllers that
    extend the Kubernetes API to manage applications and their components. They provide
    a declarative way to create, configure, and manage complex stateful applications
    on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key benefits of using operators include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplified application deployment and lifecycle management**: Operators abstract
    away low-level details and provide high-level abstractions for deploying applications
    without needing to understand the intricacies of Kubernetes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with monitoring tools**: Operators expose custom metrics and
    logs, enabling integration with monitoring stacks such as Prometheus and Grafana'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes native**: Operators leverage Kubernetes’ extensibility and are
    written specifically for Kubernetes, allowing them to be cloud-agnostic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operators extend Kubernetes by creating **Custom Resource Definitions** (**CRDs**)
    and controllers. A CRD allows you to define a new resource type in Kubernetes.
    For example, the SparkOperator defines a SparkApplication resource.
  prefs: []
  type: TYPE_NORMAL
- en: The operator then creates a controller that watches for these custom resources
    and performs actions based on the resource `spec`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, when a SparkApplication resource is created, the SparkOperator
    controller will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the driver and executor Pods based on the spec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mount storage volumes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor the status of the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform logging and monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s get to it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, let’s create an AWS EKS cluster using `eksctl`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Remember that this line of code takes several minutes to complete. Now, there
    are some important configurations to give our Kubernetes cluster permission to
    create volumes on our behalf. For this, we need to install the AWS EBS CSI driver.
    This is not required for deploying Spark applications, but it will be very important
    for Airflow deployment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, we need to associate the IAM OIDC provider with the EKS cluster, which
    allows IAM roles and users to authenticate to the Kubernetes API. To do that,
    in the terminal, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will create an IAM service account called `ebs-csi-controller-sa`
    in the `kube-system` namespace, with the specified IAM role and policy attached.
    This service account will be used by the EBS CSI driver:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will enable the EBS CSI driver in the cluster and link it to the
    service account and role created earlier. Remember to change `<YOUR_ACCOUNT_NUMBER>`
    for the real value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s start the actual Spark operator deployment. We will create a namespace
    to organize our resources next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will use the SparkOperator Helm chart available online to deploy the
    operator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check whether the operator was correctly deployed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see output like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to register our AWS credentials as a Kubernetes Secret to make
    them available for Spark. This will allow our Spark applications to access resources
    in AWS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, it’s time to develop our Spark code. By now, you should have the Titanic
    dataset stored on Amazon S3\. At [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.py),
    you will find simple code that reads the Titanic dataset from the S3 bucket and
    writes it into another bucket (this second S3 bucket must have been created previously
    – you can do it in the AWS console).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Save this file as `spark_job.py` and upload it to a different S3 bucket. This
    is where the SparkOperator is going to look for the code to run the application.
    Note that this PySpark code is slightly different from what we saw earlier, in
    [*Chapter 5*](B21927_05.xhtml#_idTextAnchor092). Here, we are setting Spark configurations
    separately from the Spark session. We will go through those configurations in
    detail:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.set("spark.cores.max", "2")`: This limits the maximum number of cores this
    Spark application will use to two. This prevents the overallocation of resources.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.set("spark.executor.extraJavaOptions", "-Dcom.amazonaws.services.s3.enableV4=true")`
    and `.set("spark.driver.extraJavaOptions", "-Dcom.amazonaws.services.s3.enableV4=true")`:
    These enable support for reading and writing to S3 using Signature Version 4 authentication,
    which is more secure.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.set("spark.hadoop.fs.s3a.fast.upload", True)`: This property enables the
    fast upload feature of the S3A connector which improves performance when saving
    data to S3.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.set("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")`:
    This configuration sets the S3 FileSystem implementation to use the newer, optimized
    `s3a` instead of the older `s3` connector.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.set("spark.hadoop.fs.s3a.aws.crendentials.provider", "com.amazonaws.auth.EnvironmentVariablesCredentials")`:
    This configures Spark to obtain AWS credentials from the environment variables,
    rather than needing to specify them directly in code.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.set("spark.jars.packages", "org.apache.hadoop:hadoop-aws:2.7.3")`: This adds
    a dependency on the Hadoop AWS module so Spark can access S3.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Also note that, by default, Spark uses log-level `INFO`. In this code, we set
    it to `WARN` to reduce logging and improve logs’ readability. Remember to change
    `<YOUR_BUCKET>` for your own S3 buckets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After uploading this code to S3, it’s time to create a YAML file with the SparkApplication
    definitions. The content for the code is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.yaml](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.yaml).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code defines a new SparkApplication resource. This is only possible because
    the SparkOperator created the SparkApplication custom resource. Let’s take a closer
    look at what this YAML definition is doing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The first block of the YAML file specifies the apiVersion and the kind of resource
    as Spark application. It also sets a name and namespace for the application.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The second block defines a volume mount called “ivy” that will be used to cache
    dependencies and avoid fetching them for each job run. It mounts to `/tmp` in
    the driver and executors.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The third block configures Spark properties, enabling the Ivy cache directory
    and setting the resource allocation batch size for Kubernetes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The fourth block configures Hadoop properties to use the S3A file system implementation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The fifth block sets this Spark application as a Python one, the Python version
    to use, running in cluster mode, and the Docker image to use – in this case, a
    previously prepared Spark image that integrates with AWS and Kafka. It also defines
    that the image will always be pulled from Docker Hub, even if it is already present
    in the cluster.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The sixth block specifies the main Python application file location in S3 and
    the Spark version – in this case, 3.1.1.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The seventh block sets `restartPolicy` to `Never`, so the application runs only
    once.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining blocks set configuration for the driver and executor Pods. Here,
    we set up AWS access key secrets for accessing S3, we request one core and 1 GB
    memory for the driver and the same resources for the executors, we mount an `emptyDir`
    volume called “ivy” for caching dependencies, and we set Spark and driver Pod
    labels for tracking.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once this file is saved on your computer and you already have the `.py` file
    on S3, it’s time to run the Spark Application. In the terminal, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can get a few more details on the application using the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To see the logs of our Spark Application, type this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And that’s it! You just ran your first Spark application on Kubernetes! Kubernetes
    won’t let you deploy another job with the same name, so, to test again, you should
    delete the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s see how to deploy Airflow on Kubernetes using the official Helm chart.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Airflow on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Airflow deployment on Kubernetes is very straightforward. Nevertheless, there
    are some important details in the Helm chart configuration that we need to pay
    attention to.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will download the most recent Helm chart to our local environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to configure a `custom_values.yaml` file to change the default
    deployment configurations for the chart. An example of this YAML file is available
    at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/custom_values.yaml](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/custom_values.yaml).
    We will not go through the entire file but just the most important configurations
    that are needed for this deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: In the `defaultAirflowTag` and `airflowVersion` parameters, make sure to set
    `2.8.3`. This is the latest Airflow version available for the 1.13.1 Helm chart
    version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `executor` parameter should be set to `KubernetesExecutor`. This guarantees
    that Airflow will use Kubernetes infrastructure to launch tasks dynamically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `env` section, we will configure “remote logging” to allow Airflow to
    save logs in S3\. This is best practice for auditing and saving Kubernetes storage
    resources. Here, we configure three environment variables for Airflow. The first
    one sets remote logging to `"True"`; the second one defines in which S3 bucket
    and folder Airflow will write logs, and the last one defines a “connection” that
    Airflow will use to authenticate in AWS. We will have to set this in the Airflow
    UI later. This is an example of what this block should look like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the webserver block, we must configure the first user credentials and the
    type of service. The `service` parameter should be set to “LoadBalancer” so we
    can access the Airflow UI from a browser. The `defaultUser` block should look
    like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is important to have a simple password in the values file and change it in
    the UI as soon as the deployment is ready. This way, your credentials do not get
    stored in plain text. This would be a major security incident.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `redis.enabled` parameter should be set to `false`. As we are using the
    Kubernetes executor, Airlfow will not need Redis to manage tasks. If we don’t
    set this parameter to `false`, Helm will deploy a Redis Pod anyway.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Lastly, in the `dags` block, we will configure `gitSync`. This is the easiest
    way to send our DAG files to Airflow and keep them updated in GitHub (or any other
    Git repository you prefer). First, you should create a GitHub repository with
    a folder named `"dags"` to store Python DAG files. Then, you should configure
    the `gitSync` block as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that we omitted several comments in the original file for readability.
    The `custom_values.yaml` file is ready for deployment. We can now proceed with
    this command in the terminal:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This deployment can take a few minutes to finish because Airflow will do a database
    migration job before making the UI available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to get the URL for the UI’s LoadBalancer. In the terminal, type
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the columns `EXTERNAL-IP`, you will notice one `not empty` value for the
    `airflow-webserver` service. Copy this URL and paste it into your browser, adding
    `":8080"` to access Airflow’s correct port.
  prefs: []
  type: TYPE_NORMAL
- en: After logging in to the UI, click on `aws_conn` (as we stated in the values
    file), choose **Amazon Web Services** for the connection type, and enter your
    access key ID and the secret access key. (At this point, you should have your
    AWS credentials stored locally – if you don’t, in the AWS console, go to IAM and
    generate new credentials for your user. You will not be able to see the old credentials
    onscreen.)
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will use DAG code adapted from [*Chapter 5*](B21927_05.xhtml#_idTextAnchor092)
    that will run smoothly on Kubernetes. This DAG will download the Titanic dataset
    automatically from the internet, perform simple calculations, and print the results,
    which will be accessed on the “logs” page. The content for the code is available
    at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/dags/titanic_dag.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/dags/titanic_dag.py).
  prefs: []
  type: TYPE_NORMAL
- en: Upload a copy of this Python code to your GitHub repository and, in a few seconds,
    it will show on the Airflow UI. Now, activate your DAG (click the `switch` button)
    and follow the execution (*Figure 8**.1*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – DAG successfully executed](img/B21927_08.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – DAG successfully executed
  prefs: []
  type: TYPE_NORMAL
- en: Then click on any task to select it and click on **Logs**. You will see Airflow
    logs being read directly from S3 (*Figure 8**.2*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Airflow logs in S3](img/B21927_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Airflow logs in S3
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You now have Airflow up and running on Kubernetes. In [*Chapter
    10*](B21927_10.xhtml#_idTextAnchor154), we will put all these pieces together
    to build a fully automated data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will deploy a Kafka cluster on Kubernetes using the Strimzi operator.
    Let’s get to it.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Kafka on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Strimzi is an open source operator that facilitates the deployment and management
    of Kafka clusters on Kubernetes, creating new CRDs. It is developed and maintained
    by the Strimzi project, which is part of the **Cloud Native Computing Foundation**
    (**CNCF**). The Strimzi operator provides a declarative approach to managing Kafka
    clusters on Kubernetes. Instead of manually creating and configuring Kafka components,
    you define the desired state of your Kafka cluster using Kubernetes custom resources.
    The operator then takes care of deploying and managing the Kafka components according
    to the specified configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy Strimzi in Kubernetes, first, we need to install its Helm chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we install the operator with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can check whether the deployment was successful with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it’s time to configure the deployment of our Kafka cluster. Here is a
    YAML file to configure a Kafka cluster using the new CRDs. Let’s break it into
    pieces for better understanding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**kafka_jbod.yaml**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first part of the code specifies the API version and the kind of resource
    being defined. In this case, it’s a Kafka resource managed by the Strimzi operator.
    Then, we define metadata for the Kafka resource, specifically its name, which
    is set to `kafka-cluster`. The next block specifies the configuration for the
    Kafka brokers. We’re setting the Kafka version to 3.7.0 and specifying that we
    want three replicas (Kafka broker instances) in the cluster:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define the listeners for the Kafka brokers. We’re configuring three
    listeners:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`plain`: An internal listener on port `9092` without TLS encryption'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tls`: An internal listener on port `9093` with TLS encryption enabled'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`external`: An external listener on port `9094` exposed as a LoadBalancer service,
    without TLS encryption'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'The next block configures the readiness and liveness probes for the Kafka brokers.
    The readiness probe checks whether the broker is ready to accept traffic, while
    the liveness probe checks whether the broker is still running. The `initialDelaySeconds`
    parameter specifies the number of seconds to wait before performing the first
    probe, and `timeoutSeconds` specifies the number of seconds after which the probe
    is considered failed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This `kafka.config` block specifies various configuration options for the Kafka
    brokers, such as the default replication factor, the number of partitions for
    new topics, the replication factor for the offsets and transaction state log topics,
    the log message format version, and the log retention period (in hours). The default
    log retention for Kafka is 7 days (168 hours), but we can change this parameter
    according to our needs. It is important to remember that longer retention periods
    imply more disk storage usage, so be careful:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `kafka.storage` block configures the storage for the Kafka brokers. We’re
    using the `deleteClaim` set to `false` to prevent the persistent volume claims
    from being deleted when the Kafka cluster is deleted:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, the `kafka.resources` block specifies the resource requests and limits
    for the Kafka brokers. We’re requesting 512 MiB of memory and 500 millicpu, and
    setting the memory limit to 1 GiB and the CPU limit to 1 `cpu`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finally, we have a `zookeeper` block that configures the ZooKeeper ensemble
    used by the Kafka cluster. We’re specifying three replicas for ZooKeeper, using
    a 10 GiB persistent volume claim for storage, and setting resource requests and
    limits similar to the Kafka brokers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the configuration file is ready on your machine, type the following command
    to deploy the cluster to Kubernetes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This yields the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, check the Pods:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output shows the three Kafka brokers and the ZooKeeper instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Congrats! You have a fully operational Kafka cluster running on Kubernetes.
    Now, the next step is to deploy a Kafka Connect cluster and make everything ready
    for a real-time data pipeline. We will not do that right now for cloud cost efficiency,
    but we will get back to this configuration in [*Chapter 10*](B21927_10.xhtml#_idTextAnchor154).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to deploy and manage key big data technologies
    such as Apache Spark, Apache Airflow, and Apache Kafka on Kubernetes. Deploying
    these tools on Kubernetes provides several benefits, including simplified operations,
    better resource utilization, scaling, high availability, and unified cluster management.
  prefs: []
  type: TYPE_NORMAL
- en: You started by deploying the Spark operator on Kubernetes and running a Spark
    application to process data from Amazon S3\. This allows you to leverage Kubernetes
    for running Spark jobs in a cloud-native way, taking advantage of dynamic resource
    allocation and scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you deployed Apache Airflow on Kubernetes using the official Helm chart.
    You configured Airflow to run with the Kubernetes executor, enabling it to dynamically
    launch tasks on Kubernetes. You also set up remote logging to Amazon S3 for easier
    monitoring and debugging. Running Airflow on Kubernetes improves reliability,
    scalability, and resource utilization for orchestrating data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you deployed Apache Kafka on Kubernetes using the Strimzi operator.
    You configured a Kafka cluster with brokers, a ZooKeeper ensemble, persistent
    storage volumes, and internal/external listeners. Deploying Kafka on Kubernetes
    simplifies operations, scaling, high availability, and cluster management for
    building streaming data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, you now have hands-on experience with deploying and managing the key
    components of a big data stack on Kubernetes. This will enable you to build robust,
    scalable data applications and pipelines leveraging the power of container orchestration
    with Kubernetes. The skills learned in this chapter are crucial for running big
    data workloads efficiently in cloud-native environments.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to build a data consumption layer on top
    of Kubernetes and how to connect those layers with tools to visualize and use
    data.
  prefs: []
  type: TYPE_NORMAL
