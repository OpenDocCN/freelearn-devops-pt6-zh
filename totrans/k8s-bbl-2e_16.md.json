["```\n$ brew install awscli \n```", "```\n$ aws –version \n```", "```\n$ curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n$ unzip awscliv2.zip\n$ sudo ./aws/install \n```", "```\n$ choco install awscli \n```", "```\n$ aws --version \n```", "```\n$ aws configure \n```", "```\n$ aws ec2 describe-regions \n```", "```\n$ brew install eksctl \n```", "```\n$ choco install eksctl \n```", "```\n$ PLATFORM=$(uname -s)_$(uname -m)\n$ curl -sLO \"https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz\"\n$ tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz\n$ sudo mv /tmp/eksctl /usr/local/bin \n```", "```\n$ eksctl version \n```", "```\n$ eksctl create cluster \n```", "```\n[i]  eksctl version 0.180.0-dev+763027060.2024-05-29T21:36:10Z\n[i]  using region us-west-2 \n```", "```\n[i]  setting availability zones to [us-west-2d us-west-2b us-west-2c]\n[i]  subnets for us-west-2d - public:192.168.0.0/19 private:192.168.96.0/19\n[i]  subnets for us-west-2b - public:192.168.32.0/19 private:192.168.128.0/19\n[i]  subnets for us-west-2c - public:192.168.64.0/19 private:192.168.160.0/19 \n```", "```\n[i]  nodegroup \"ng-11c87ff4\" will use \"[AmazonLinux2/1.29]\"\n[i]  using Kubernetes version 1.29 \n```", "```\n[i]  creating EKS cluster \"hilarious-wardrobe-1717847351\" in \"us-west-2\" region with managed nodes\n[i]  will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup\n[i]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-west-2 --cluster=hilarious-wardrobe-1717847351'\n[i]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"hilarious-wardrobe-1717847351\" in \"us-west-2\" \n```", "```\n[i]  CloudWatch logging will not be enabled for cluster \"hilarious-wardrobe-1717847351\" in \"us-west-2\"\n[i]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-west-2 --cluster=hilarious-wardrobe-1717847351' \n```", "```\n[i]2 sequential tasks: { create cluster control plane \"hilarious-wardrobe-1717847351\",2 sequential sub-tasks: {wait for control plane to become ready, create managed nodegroup \"ng-11c87ff4\",}}\n[i]  building cluster stack \"eksctl-hilarious-wardrobe-1717847351-cluster\"\n[i]  waiting for CloudFormation stack \"eksctl-hilarious-wardrobe-1717847351-cluster\"\n[i]  building managed nodegroup stack \"eksctl-hilarious-wardrobe-1717847351-nodegroup-ng-11c87ff4\"\n[i]  deploying stack \"eksctl-hilarious-wardrobe-1717847351-nodegroup-ng-11c87ff4\"\n[i]  waiting for CloudFormation stack \"eksctl-hilarious-wardrobe-1717847351-nodegroup-ng-11c87ff4\"\n[i]  waiting for the control plane to become ready \n```", "```\n[✔]  saved kubeconfig as \"/Users/russ.mckendrick/.kube/config\"\n[i]  no tasks\n[✔]  all EKS cluster resources for \"hilarious-wardrobe-1717847351\" have been created \n```", "```\n2024-06-08 13:03:34 [✔]  created 0 nodegroup(s) in cluster \"hilarious-wardrobe-1717847351\"\n2024-06-08 13:03:35 [i]  node \"ip-192-168-34-120.us-west-2.compute.internal\" is ready\n2024-06-08 13:03:35 [i]  node \"ip-192-168-67-233.us-west-2.compute.internal\" is ready\n2024-06-08 13:03:35 [i]  waiting for at least 2 node(s) to become ready in \"ng-11c87ff4\"\n2024-06-08 13:03:35 [i]  nodegroup \"ng-11c87ff4\" has 2 node(s)\n2024-06-08 13:03:35 [i]  node \"ip-192-168-34-120.us-west-2.compute.internal\" is ready\n2024-06-08 13:03:35 [i]  node \"ip-192-168-67-233.us-west-2.compute.internal\" is ready\n2024-06-08 13:03:35 [✔]  created 1 managed nodegroup(s) in cluster \"hilarious-wardrobe-1717847351\" \n```", "```\n2024-06-08 13:03:36 [i]  kubectl command should work with \"/Users/russ.mckendrick/.kube/config\", try 'kubectl get nodes'\n2024-06-08 13:03:36 [✔]  EKS cluster \"hilarious-wardrobe-1717847351\" in \"us-west-2\" region is ready \n```", "```\n    $ kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes-engine-samples/main/quickstarts/guestbook/redis-leader-deployment.yaml\n    $ kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes-engine-samples/main/quickstarts/guestbook/redis-leader-service.yaml \n    ```", "```\n    $ kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes-engine-samples/main/quickstarts/guestbook/redis-follower-deployment.yaml\n    $ kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes-engine-samples/main/quickstarts/guestbook/redis-follower-service.yaml \n    ```", "```\n    $ kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes-engine-samples/main/quickstarts/guestbook/frontend-deployment.yaml\n    $ kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes-engine-samples/main/quickstarts/guestbook/frontend-service.yaml \n    ```", "```\n    $ kubectl get service frontend \n    ```", "```\n$ eksctl delete cluster --name hilarious-wardrobe-1717847351 \n```", "```\n[i]  deleting EKS cluster \"hilarious-wardrobe-1717847351\"\n[i]  will drain 0 unmanaged nodegroup(s) in cluster \"hilarious-wardrobe-1717847351\"\n[i]  starting parallel draining, max in-flight of 1\n[i]  deleted 0 Fargate profile(s) \n```", "```\n[✔]  kubeconfig has been updated \n```", "```\n[i]  cleaning up AWS load balancers created by Kubernetes objects of Kind Service or Ingress \n```", "```\n[i]  2 sequential tasks: { delete nodegroup \"ng-11c87ff4\", delete cluster control plane \"hilarious-wardrobe-1717847351\" [async] }\n[i]  will delete stack \"eksctl-hilarious-wardrobe-1717847351-nodegroup-ng-11c87ff4\"\n[i]  waiting for stack \"eksctl-hilarious-wardrobe-1717847351-nodegroup-ng-11c87ff4\" to get deleted\n[i]  waiting for CloudFormation stack \"eksctl-hilarious-wardrobe-1717847351-nodegroup-ng-11c87ff4\"\n[i]  will delete stack \"eksctl-hilarious-wardrobe-1717847351-cluster\"\n[✔]  all cluster resources were deleted \n```"]