["```\n---\nversion: '2'\nservices:\n    zookeeper-1:\n      image: confluentinc/cp-zookeeper:7.6.0\n      environment:\n        ZOOKEEPER_SERVER_ID: 1\n        ZOOKEEPER_CLIENT_PORT: 22181\n        ZOOKEEPER_TICK_TIME: 2000\n        ZOOKEEPER_INIT_LIMIT: 5\n        ZOOKEEPER_SYNC_LIMIT: 2\n        ZOOKEEPER_SERVERS: localhost:22888:23888;localhost:32888:33888;localhost:42888:43888\n    network_mode: host\n    extra_hosts:\n      - \"mynet:127.0.0.1\"\n    kafka-1:\n      image: confluentinc/cp-kafka:7.6.0\n      network_mode: host\n      depends_on:\n        - zookeeper-1\n        - zookeeper-2\n        - zookeeper-3\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: localhost:22181,localhost:32181,localhost:42181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:19092\n    extra_hosts:\n      - \"mynet:127.0.0.1\"\n```", "```\ndocker-compose up –d\n```", "```\ndocker logs multinode-kafka-1-1\n```", "```\n    docker-compose ps\n    ```", "```\n    CONTAINER_NAME=multinode-kafka-1-1\n    docker exec -it $CONTAINER_NAME bash\n    ```", "```\n    BOOTSTRAP_SERVER=localhost:19092\n    TOPIC=mytopic\n    GROUP=mygroup\n    ```", "```\n    kafka-topics --create --bootstrap-server $BOOTSTRAP_SERVER --replication-factor 3 --partitions 3 --topic $TOPIC\n    ```", "```\n    kafka-topics --list --bootstrap-server $BOOTSTRAP_SERVER\n    ```", "```\n    kafka-topics --bootstrap-server $BOOTSTRAP_SERVER --describe --topic $TOPIC\n    ```", "```\n    Topic: mytopic\n    TopicId: UFt3FOyVRZyYU7TYT1TrsQ\n    PartitionCount: 3\n    ReplicationFactor: 3\n    Configs:\n    Topic:mytopic Partition:0 Leader:2 Replicas:2,3,1\n    Topic:mytopic Partition:1 Leader:3 Replicas:3,1,2\n    Topic:mytopic Partition:2 Leader:1 Replicas:1,2,3\n    ```", "```\n    kafka-console-producer --broker-list $BOOTSTRAP_SERVER --topic $TOPIC\n    ```", "```\n    abc\n    def\n    ghi\n    jkl\n    mno\n    pqr\n    stu\n    vwx\n    yza\n    ```", "```\n    CONTAINER_NAME=multinode-kafka-1-1\n    docker exec -it $CONTAINER_NAME bash\n    ```", "```\n    BOOTSTRAP_SERVER=localhost:19092\n    TOPIC=mytopic\n    ```", "```\n    kafka-console-consumer --bootstrap-server $BOOTSTRAP_SERVER --topic $TOPIC --from-beginning\n    ```", "```\ndocker-compose down\n```", "```\ndocker ps -a\n```", "```\n    FROM confluentinc/cp-kafka-connect-base:7.6.0\n    RUN confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:10.7.5 \\\n    && confluent-hub install --no-prompt confluentinc/kafka-connect-s3:10.5.8\n    ```", "```\n    cd connect\n    cd kafka-connect-custom-image\n    docker build -t connect-custom:1.0.0 .\n    cd ..\n    ```", "```\n    AWS_DEFAULT_REGION='us-east-1'\n    AWS_ACCESS_KEY_ID='xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n    AWS_SECRET_ACCESS_KEY='xxxxxxxxxxxx'\n    ```", "```\n    docker-compose up -d\n    ```", "```\n    python make_fake_data.py\n    ```", "```\n    {\n        \"name\": \"pg-connector-json\",\n        \"config\": {\n            \"connector.class\": \"io.confluent.connect.jdbc.JdbcSourceConnector\",\n            \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n            \"value.converter.schemas.enable\": \"true\",\n            \"tasks.max\": 1,\n            \"connection.url\": \"jdbc:postgresql://postgres:5432/postgres\",\n            «connection.user»: «postgres»,\n            «connection.password»: «postgres»,\n            «mode»: «timestamp»,\n            \"timestamp.column.name\": \"dt_update\",\n            \"table.whitelist\": \"public.customers\",\n            \"topic.prefix\": \"json-\",\n            \"validate.non.null\": \"false\",\n            \"poll.interval.ms\": 500\n        }\n    }\n    ```", "```\n    docker-compose exec broker kafka-topics --create --bootstrap-server localhost:9092 --partitions 2 --replication-factor 1 --topic json-customers\n    ```", "```\n    curl -X POST -H \"Content-Type: application/json\" --data @connectors/connect_jdbc_pg_json.config localhost:8083/connectors\n    curl localhost:8083/connectors\n    ```", "```\n    docker logs connect\n    ```", "```\n    docker exec -it broker bash\n    kafka-console-consumer --bootstrap-server localhost:9092 --topic json-customers --from-beginning\n    ```", "```\n    {\n        \"name\": \"customers-s3-sink\",\n        \"config\": {\n            \"connector.class\": \"io.confluent.connect.s3.S3SinkConnector\",\n            \"format.class\": \"io.confluent.connect.s3.format.json.JsonFormat\",\n            \"keys.format.class\": \"io.confluent.connect.s3.format.json.JsonFormat\",\n            \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n            \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n            \"key.converter.schemas.enable\": false,\n            \"value.converter.schemas.enable\": false,\n            \"flush.size\": 1,\n            \"schema.compatibility\": \"FULL\",\n            \"s3.bucket.name\": \"<YOUR_BUCKET_NAME>\",\n            \"s3.region\": \"us-east-1\",\n            \"s3.object.tagging\": true,\n            \"s3.ssea.name\": \"AES256\",\n            \"topics.dir\": \"raw-data/kafka\",\n            \"storage.class\": \"io.confluent.connect.s3.storage.S3Storage\",\n            \"tasks.max\": 1,\n            \"topics\": \"json-customers\"\n        }\n    }\n    ```", "```\n    curl -X POST -H \"Content-Type: application/json\" --data @connectors/connect_s3_sink.config localhost:8083/connectors\n    ```", "```\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as f\nfrom pyspark.sql.types import *\nspark = (\n    SparkSession.builder\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\n    .appName(\"ConsumeFromKafka\")\n    .getOrCreate()\n)\nspark.sparkContext.setLogLevel('ERROR')\ndf = (\n    spark.readStream\n    .format('kafka')\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n    .option(\"subscribe\", \"json-customers\")\n    .option(\"startingOffsets\", \"earliest\")\n    .load()\n)\n```", "```\nschema1 = StructType([\n    StructField(\"schema\", StringType(), False),\n    StructField(\"payload\", StringType(), False)\n])\nschema2 = StructType([\n    StructField(\"name\", StringType(), False),\n    StructField(\"gender\", StringType(), False),\n    StructField(\"phone\", StringType(), False),\n    StructField(\"email\", StringType(), False),\n    StructField(\"photo\", StringType(), False),\n    StructField(\"birthdate\", StringType(), False),\n    StructField(\"profession\", StringType(), False),\n    StructField(\"dt_update\", LongType(), False)\n])\no = df.selectExpr(\"CAST(value AS STRING)\")\no2 = o.select(f.from_json(f.col(\"value\"), schema1).alias(\"data\")).selectExpr(\"data.payload\")\no2 = o2.selectExpr(\"CAST(payload AS STRING)\")\nnewdf = o2.select(f.from_json(f.col(\"payload\"), schema2).alias(\"data\")).selectExpr(\"data.*\")\n```", "```\nquery = (\n    newdf\n    .withColumn(\"dt_birthdate\", f.col(\"birthdate\"))\n    .withColumn(\"today\", f.to_date(f.current_timestamp()))\n    .withColumn(\"age\", f.round(\nf.datediff(f.col(\"today\"), f.col(\"dt_birthdate\"))/365.25, 0)\n    )\n    .groupBy(\"gender\")\n    .agg(\n      f.count(f.lit(1)).alias(\"count\"),\n      f.first(\"dt_birthdate\").alias(\"first_birthdate\"),\n      f.first(\"today\").alias(\"first_now\"),\n      f.round(f.avg(\"age\"), 2).alias(\"avg_age\")\n    )\n)\n```", "```\n(\n    query\n    .writeStream\n    .format(\"console\")\n    .outputMode(\"complete\")\n    .start()\n    .awaitTermination()\n)\n```", "```\nspark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 processing/consume_from_kafka.py\n```", "```\npython simulations/make_fake_data.py\n```"]