<html><head></head><body>
		<div id="_idContainer161">
			<h1 class="chapter-number" id="_idParaDest-205"><a id="_idTextAnchor209"/>11</h1>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor210"/>Deploying Real-World Projects with GitOps on Kubernetes</h1>
			<p>In this chapter, you will embark on a practical journey that bridges the gap between theoretical knowledge and real-world knowledge application. As you delve into the intricate process of setting up a GitOps and Kubernetes-based development environment, you will gain firsthand experience in designing, developing, and deploying an application within this innovative framework. Through detailed guidance on architectural design, <strong class="bold">Continuous Integration and Continuous Delivery</strong> (<strong class="bold">CI/CD</strong>) processes, application scaling, and security, this chapter aims to equip you with the essential skills and insights needed to implement these cutting-edge technologies effectively in your projects. Whether you’re looking to enhance your organizational capabilities or to refine your personal technical expertise, the comprehensive real-life example provided here will serve as an invaluable resource for anyone aspiring to master GitOps and Kubernetes in <span class="No-Break">practical settings.</span></p>
			<p>In this chapter, our focus will be on the following <span class="No-Break">key areas:</span></p>
			<ul>
				<li>Establishing a GitOps and Kubernetes <span class="No-Break">development environment</span></li>
				<li>Implementing CI/CD <span class="No-Break">with GitOps</span></li>
				<li>Designing for scalability <span class="No-Break">and efficiency</span></li>
				<li>Resource management <span class="No-Break">and scalability</span></li>
				<li>Monitoring and securing <span class="No-Break">your application</span></li>
			</ul>
			<h1 id="_idParaDest-207"><a id="_idTextAnchor211"/>Technical requirements</h1>
			<p>This chapter builds on your existing knowledge of Git, Kubernetes, and GitOps tools such as <strong class="bold">Argo CD</strong> and Flux CD, which you acquired in earlier chapters. We will use an <strong class="bold">Azure AKS</strong> cluster deployed by Terraform using a GitHub workflow. Ensure that you have access to a Kubernetes setup and are familiar with CI/CD principles to fully benefit from <span class="No-Break">the exercises.</span></p>
			<p>All necessary code and resources are provided in the <a href="B22100_11.xhtml#_idTextAnchor209"><span class="No-Break"><em class="italic">Chapter 11</em></span></a> folder of our dedicated <span class="No-Break">GitHub repository:</span></p>
			<p><a href="https://github.com/PacktPublishing/Implementing-GitOps-with-Kubernetes"><span class="No-Break">https://github.com/PacktPublishing/Implementing-GitOps-with-Kubernetes</span></a></p>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor212"/>Establishing a GitOps and Kubernetes development environment</h1>
			<p>Establishing a proper development environment is crucial for the successful implementation of <a id="_idIndexMarker947"/>GitOps practices. This environment serves as the backbone for both development and operations teams, enabling seamless integration and continuous delivery of applications. A well-configured development environment ensures that all changes to applications and infrastructure are version-controlled, traceable, and aligned with the declarative configurations stored in Git. This consistency between the development environment and production setups reduces the likelihood of errors and deployment failures, fostering a more reliable and robust delivery pipeline. By emphasizing the importance of a correct setup from the outset, teams can leverage GitOps to its fullest potential, ensuring that automated processes govern deployments and infrastructure management efficiently <span class="No-Break">and effectively.</span></p>
			<p>Installing and configuring Kubernetes<a id="_idIndexMarker948"/> for GitOps involves setting up your Kubernetes cluster in a way that integrates seamlessly with GitOps tools such as Flux CD (see the <em class="italic">Flux integration with Kubernetes</em> section in <a href="B22100_04.xhtml#_idTextAnchor065"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>) or Argo CD (see the <em class="italic">Argo CD integration with Kubernetes</em> section in <a href="B22100_04.xhtml#_idTextAnchor065"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>). What follows is a step-by-step guide that covers the setup process, ensuring that your<a id="_idIndexMarker949"/> Kubernetes environment is ready for a <span class="No-Break">GitOps workflow:</span></p>
			<ol>
				<li>Install a Kubernetes<a id="_idIndexMarker950"/> cluster and choose your environment. For learning and development, consider using <strong class="bold">K3s</strong> (refer to the <em class="italic">Exploring K3s as a lightweight Kubernetes distribution</em> section in <a href="B22100_02.xhtml#_idTextAnchor027"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>) or <strong class="bold">minikube</strong>. Both are suitable for running Kubernetes locally on <a id="_idIndexMarker951"/>your machine. For production or more scalable environments, consider cloud solutions such as <strong class="bold">Amazon EKS</strong>, Azure AKS, or <strong class="bold">Google GKE</strong>. To install minikube, follow the official <a id="_idIndexMarker952"/>minikube <a id="_idIndexMarker953"/>installation<a id="_idIndexMarker954"/> guide at <a href="https://minikube.sigs.k8s.io">https://minikube.sigs.k8s.io</a>. For deploying Kubernetes on cloud platforms, refer to the specific setup guides provided by the respective cloud providers.  For the real-world scenario described in this chapter, we will use an <span class="No-Break">AKS cluster.</span></li>
				<li>Verify installation. Ensure that <strong class="source-inline">kubectl</strong>, the Kubernetes command-line tool, is installed and configured to communicate with your cluster. You can verify this by running <span class="No-Break">the following:</span><pre class="source-code">
<strong class="bold">$ kubectl cluster-info</strong></pre><p class="list-inset">This command should return the cluster details confirming that Kubernetes is up <span class="No-Break">and running:</span></p><pre class="source-code">Kubernetes control plane is running at https://127.0.0.1:32769
CoreDNS is running at https://127.0.0.1:32769/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</pre></li>				<li>Set up your namespace. It’s good practice to create a dedicated namespace for your <span class="No-Break">GitOps tools:</span><pre class="source-code">
<strong class="bold">$ kubectl create namespace gitops</strong></pre></li>				<li>    Set up <a id="_idIndexMarker955"/>permissions. Set up <strong class="bold">Role-Based Access Control</strong> (<strong class="bold">RBAC</strong>) rules to ensure that your GitOps tools have the necessary permissions to manage resources. Most GitOps tools have specific RBAC <a id="_idIndexMarker956"/>configurations outlined in their setup guides. We will see a concrete example of how to set up RBAC in the <em class="italic">Configuring Kubernetes RBAC for user and role management</em> section of <span class="No-Break">this chapter.</span></li>
				<li>Install your GitOps tool. Depending on your preference, select from tools such as Flux CD, Argo CD, Helm, or Kustomize. Each tool has unique strengths and supportive community backing. For more details about the mentioned GitOps tools, refer to the <em class="italic">Overview of popular GitOps tools</em> section in <a href="B22100_04.xhtml#_idTextAnchor065"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>. Additionally, you can explore the <em class="italic">A deep dive into Helm and Kustomize</em>, <em class="italic">Argo CD integration with Kubernetes</em>, and <em class="italic">Flux CD integration with Kubernetes</em> sections of the <span class="No-Break">same chapter.</span></li>
				<li>Set up a Git repository. Configure the GitOps tool to track your Git repository where your Kubernetes manifests are stored. For guidance, refer to the <em class="italic">Kubernetes deployment with Azure DevOps</em> or the <em class="italic">Kubernetes deployment with AWS CodePipeline</em> section, both in <a href="B22100_04.xhtml#_idTextAnchor065"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>. This setup process involves pointing the tool to your repository and specifying which branch and path to monitor <span class="No-Break">for changes.</span></li>
				<li>Validate and test. Start by deploying a simple application using your GitOps tool to confirm that changes in your Git repository automatically trigger deployments in your Kubernetes cluster. Monitor the deployment using the GitOps tool’s dashboard or CLI to ensure that the application is deployed and running as expected. Test updates and rollbacks by modifying the application’s manifest in <a id="_idIndexMarker957"/>your Git repository and noting whether the changes are <span class="No-Break">automatically implemented.</span></li>
			</ol>
			<p>Most of the pipeline points have already been covered in more detail in previous chapters. They will be revisited in the next section, where will see how to implement a real-world scenario for CI/CD <span class="No-Break">with GitOps.</span></p>
			<h1 id="_idParaDest-209"><a id="_idTextAnchor213"/>Implementing CI/CD with GitOps</h1>
			<p>To implement a <a id="_idIndexMarker958"/>real-world CI/CD GitOps scenario, we need an application that no longer uses mocked data but instead utilizes <span class="No-Break">concrete data.</span></p>
			<p>In this section, we will expose a backend service for a weather application that fetches data from a real weather service, such<a id="_idIndexMarker959"/> as <strong class="bold">OpenWeatherMap </strong>(<a href="https://openweathermap.org/">https://openweathermap.org/</a><span class="P---URL">)</span>, to the <span class="No-Break">public internet.</span></p>
			<p>Given that the requirements for setting up our GitOps environment (installing a Kubernetes cluster and choosing your environment, verifying installation, and installing your GitOps tool) have already been met in the previous section of this chapter, the next step is to create a new GitHub repository. For example, you might create <strong class="source-inline">gitops-for-real-world</strong>, with a directory named <strong class="source-inline">Step-01</strong>. This directory will be used to add the code and files for <span class="No-Break">subsequent steps.</span></p>
			<p>Before proceeding, you need to create a free account with the <strong class="source-inline">OpenWeatherMap</strong> service or another similar service of your choice. Services like these typically require a token to query their API, which is used for authentication and billing purposes. It’s crucial to keep this token <em class="italic">confidential</em> and not share it. Please refer to the <strong class="source-inline">OpenWeatherMap</strong> documentation to create a new token. Soon, we will add this token as a <strong class="bold">secret</strong> in the<a id="_idIndexMarker960"/> <span class="No-Break">Kubernetes cluster.</span></p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor214"/>Final objective and implementation</h2>
			<p>To achieve our final objective, this section and the ones that follow will demonstrate the use of <a id="_idIndexMarker961"/>a <strong class="bold">Python Flask application</strong>, packaged as a <strong class="bold">Docker image</strong>. This <a id="_idIndexMarker962"/>image is built with a new tag at each commit and deployed on an Azure AKS cluster, which is provisioned automatically by the pipeline using Terraform for <a id="_idIndexMarker963"/>the <strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>) component. Initially, the entire deployment chain—both IaC and the application—will be managed entirely by our GitHub CI/CD pipeline. Later, we will transition to using Argo CD for the deployment while keeping the CI processes within the <span class="No-Break">GitHub workflow.</span></p>
			<p>Ultimately, to test our service after it has been exposed to the public internet, we will perform <strong class="bold">weather</strong> requests for a specified city via the query string in a URL, such as <a href="http://public-ip/weather?city=zurich">http://public-ip/weather?city=zurich</a>. The response will be in JSON format, which can be rendered directly in the browser or with <a id="_idIndexMarker964"/>tools such <span class="No-Break">as </span><span class="No-Break"><strong class="bold">curl</strong></span><span class="No-Break">.</span></p>
			<p>Our pipeline will be <a id="_idIndexMarker965"/>developed as a GitHub workflow and will be composed as illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer148">
					<img alt="Figure 11.1 – A GitHub workflow pipeline" src="image/B22100_11_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – A GitHub workflow pipeline</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor215"/>CI/CD pipeline using GitHub Actions and Terraform</h2>
			<p>The<a id="_idIndexMarker966"/> pipeline in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.1</em> leverages Terraform for infrastructure management and deploys a Dockerized application to an AKS cluster, providing a practical example of modern DevOps practices. The code of the pipeline is too long to be explicitly added as content in this chapter. What follows are some important aspects that should be considered for a better understanding of that. The workflow description is contained in the <strong class="source-inline">gitops-for-real-ci-cd-pipeline.yml</strong> file in the <strong class="source-inline">.github/workflows/</strong> directory of the repository accompanying <span class="No-Break">this chapter.</span></p>
			<h3>Workflow trigger conditions</h3>
			<p>The <a id="_idIndexMarker967"/>pipeline is configured to trigger on any push or pull request to the main branch with one exception: changes exclusively in the <strong class="source-inline">Step-01/deployment</strong> directory do not initiate the workflow. This precaution prevents redundant runs when only Kubernetes manifest files are updated, ensuring efficient use of resources and avoiding potential conflicts in continuous <span class="No-Break">deployment scenarios.</span></p>
			<h3>Terraform plan and apply</h3>
			<p>The <a id="_idIndexMarker968"/>workflow begins with the <strong class="source-inline">terraform-plan</strong> job. This job executes several <span class="No-Break">critical steps:</span></p>
			<ol>
				<li><strong class="bold">Environment setup</strong>: The job initializes by checking out the repository and setting up the Azure CLI with credentials stored securely as GitHub secrets. This step ensures that the workflow has access to manage resources <span class="No-Break">in Azure.</span><p class="list-inset">All the passwords, tokens, and other sensitive information used in the pipeline need to be configured as GitHub Actions Secrets, as illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer149">
					<img alt="Figure 11.2 – GitHub secrets on the Actions secrets and variables page" src="image/B22100_11_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – GitHub secrets on the Actions secrets and variables page</p>
			<ol>
				<li value="2"><strong class="bold">Terraform initialization</strong>: In this <a id="_idIndexMarker969"/>step, the job runs <strong class="source-inline">terraform init</strong> to prepare the Terraform environment, configuring backend storage for Terraform state files in Azure Blob Storage. The following code is extracted  from the <span class="No-Break">main pipeline:</span><pre class="source-code">
- name: Terraform Init
  run: |
    terraform init \
    --backend-config=»resource_group_name=${{ secrets.BACKEND_RESOURCE_GROUP_NAME }}» \
    --backend-config=»storage_account_name=${{ secrets.BACKEND_STORAGE_ACCOUNT_NAME }}» \
    --backend-config=»container_name=${{ secrets.BACKEND_CONTAINER_NAME }}» \
    --backend-config=»key=${{ secrets.BACKEND_KEY }}» \
    --reconfigure
    working-directory: ./Step-01/terraform
    env:
    ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
    ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
    ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
    ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}</pre></li>				<li><strong class="bold">Execution plan</strong>: Terraform <a id="_idIndexMarker970"/>then generates an execution plan (<strong class="source-inline">terraform plan</strong>), which is reviewed automatically to determine whether there are changes to apply. The plan is saved as an <strong class="bold">artifact</strong> for review and used in the subsequent <span class="No-Break"><strong class="source-inline">terraform-apply</strong></span><span class="No-Break"> job.</span></li>
			</ol>
			<p class="callout-heading">GitHub artifacts</p>
			<p class="callout">An artifact<a id="_idIndexMarker971"/> in GitHub Actions is a file or a collection of files produced during a workflow run. Artifacts can include binary files, logs, test results, or any other type of data that needs to be stored after a job is completed. These artifacts are typically used for storing build and test outputs to be used for debugging, deployment, or further processing in subsequent steps or future runs. GitHub stores these artifacts for a specified period, allowing them to be downloaded or shared across different jobs within the same workflow. This feature facilitates effective CI/CD practices by ensuring that outputs from one part of a workflow can easily be accessed and utilized in other parts, enhancing automation and continuity throughout the software development <span class="No-Break">life cycle.</span></p>
			<p>Following <a id="_idIndexMarker972"/>planning, the <strong class="source-inline">terraform-apply</strong> job applies the approved changes to the infrastructure, ensuring that the actual state matches the expected state defined in the Terraform configurations. This part can take a few minutes due to the provisioning of the resources to Azure. Opening the Azure portal, the final provisioning should be similar to what is illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer150">
					<img alt="Figure 11.3 – Azure resources automatically provisioned by the GitHub workflow" src="image/B22100_11_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Azure resources automatically provisioned by the GitHub workflow</p>
			<h3>Docker image build and push</h3>
			<p>Parallel to<a id="_idIndexMarker973"/> infrastructure management, the <strong class="source-inline">docker-build-and-push</strong> job handles the <span class="No-Break">application side:</span></p>
			<ol>
				<li><strong class="bold">Docker preparation</strong>: The job sets up Docker environments using QEMU and Buildx, tools that enhance Docker’s capabilities on <span class="No-Break">CI environments.</span></li>
				<li><strong class="bold">Building and Pushing Docker Images</strong>: It then builds the Docker image from a <strong class="bold">Dockerfile</strong> located in the <strong class="source-inline">Step-01</strong> directory and pushes it to Docker Hub, tagging it <a id="_idIndexMarker974"/>with the <strong class="bold">commit SHA</strong> for immutability and traceability, as<a id="_idIndexMarker975"/> illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer151">
					<img alt="Figure 11.4 – A Docker repository containing the built images with tags corresponding to the SHA number" src="image/B22100_11_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – A Docker repository containing the built images with tags corresponding to the SHA number</p>
			<h3>Kubernetes deployment</h3>
			<p>After the <a id="_idIndexMarker976"/>Docker image is pushed and the infrastructure is ready, the <strong class="source-inline">deploy-to-kubernetes</strong> <span class="No-Break">job proceeds:</span></p>
			<ol>
				<li><strong class="bold">Cluster configuration</strong>: The job configures <strong class="source-inline">kubectl</strong> with the credentials for the <a id="_idIndexMarker977"/>Kubernetes cluster managed in Azure, ensuring that commands are executed against the correct cluster, as reported in the <span class="No-Break">following code:</span><pre class="source-code">
- name: Update Kubeconfig
  run: az aks get-credentials --resource-group gitops-dev-rg --name gitops-dev-aks --overwrite-existing</pre></li>				<li><strong class="bold">Secrets and configurations</strong>: It then deploys necessary Kubernetes secrets and configurations, such as API keys needed by the application, using best practices for secret management. The following code is extracted from <span class="No-Break">the pipeline:</span><pre class="source-code">
- name: Deploy to Kubernetes
  run: |
    cd ./Step-01/deployment
    kubectl create namespace weather-app-for-real \
       --dry-run=client -o yaml | kubectl apply -f -
    kubectl create <strong class="bold">secret</strong> generic <strong class="bold">weather-api-key</strong> \
       --from-literal=<strong class="bold">WEATHER_API_KEY=${{ secrets.WEATHER_API_TOKEN }}</strong> \
       --namespace weather-app-for-real \
       <strong class="bold">--dry-run=client -o yaml | kubectl apply -f -</strong></pre><p class="list-inset">The use of the <strong class="source-inline">--dry-run=client -o yaml | kubectl apply -f -</strong> command sequence can play a significant role in ensuring that a Kubernetes deployment <span class="No-Break">is </span><span class="No-Break"><strong class="bold">idempotent</strong></span><span class="No-Break">.</span></p></li>			</ol>
			<p class="callout-heading">Idempotency</p>
			<p class="callout">Idempotency, in the context <a id="_idIndexMarker978"/>of deploying resources, means that running the same deployment command multiple times will result in the same state without causing unintended changes or side effects after the <span class="No-Break">initial application.</span></p>
			<ol>
				<li value="3"><strong class="bold">Application deployment</strong>: Applies <a id="_idIndexMarker979"/>the Kubernetes deployment manifest (<strong class="source-inline">Step-01/deployment/backend-api-deployment.yaml</strong>), which references the newly built Docker image, ensuring that the latest version of the application <span class="No-Break">is deployed:</span><pre class="source-code">
<strong class="bold">$ kubectl apply -f backend-api-deployment.yaml</strong></pre></li>			</ol>
			<p class="callout-heading">Beware!</p>
			<p class="callout">If you want to access the remote AKS cluster from your local development, you need to login Azure and execute the following command: <strong class="source-inline">az aks get-credentials --resource-group gitops-real-rg --</strong><span class="No-Break"><strong class="source-inline">name gitops-real-aks</strong></span></p>
			<ol>
				<li value="4"><strong class="bold">Testing</strong>: Unlike local development, for this real-world example, we specified a <strong class="bold">LoadBalancer</strong> port<a id="_idIndexMarker980"/> in the deployment file, so AKS is automatically using a public IP address to expose our service to the public internet, as illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer152">
					<img alt="Figure 11.5 – A public IP address used to expose the backand-api-service to the public internet" src="image/B22100_11_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – A public IP address used to expose the backand-api-service to the public internet</p>
			<ol>
				<li value="5">At this point, we<a id="_idIndexMarker981"/> can query our service using a URL like the one shown in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.6</em> to obtain <span class="No-Break">a response:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer153">
					<img alt="Figure 11.6 – An example of querying the service for Zurich city using real weather data" src="image/B22100_11_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – An example of querying the service for Zurich city using real weather data</p>
			<p>What we have obtained so far is a fully working CI/CD pipeline that exposes a service to the real world. We want to take it a step further by separating the CI pipeline from the CD pipeline using ArgoCD, as described in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor216"/>Using Argo CD for the continuous deployment</h2>
			<p>In the world of modern software delivery, it’s crucial to ensure that our deployment practices are as reliable and scalable as possible. Argo CD, a declarative GitOps continuous delivery tool for <a id="_idIndexMarker982"/>Kubernetes, significantly enhances these aspects by automating deployment processes and syncing the desired application state defined in a Git repository with the <span class="No-Break">production environment.</span></p>
			<h3>Transitioning to Argo CD</h3>
			<p>In this section, we will evolve our <a id="_idIndexMarker983"/>GitHub Actions workflow by transitioning the <strong class="source-inline">deploy-app-to-kubernetes</strong> stage to an <strong class="source-inline">argo-cd-deployment</strong> stage. The <strong class="source-inline">argo-cd-deployment</strong> stage in our GitHub Actions workflow encapsulates the following <span class="No-Break">key operations:</span></p>
			<ol>
				<li><strong class="bold">Argo CD setup</strong>: First, the workflow initializes Argo CD in the Kubernetes cluster if it’s not already installed. This includes setting up the necessary namespaces and applying the Argo CD installation manifests directly from the <span class="No-Break">official sources.</span></li>
				<li><strong class="bold">Repository configuration</strong>: The workflow then adds the Git repository containing the Kubernetes manifests to Argo CD. This step involves configuring Argo CD to monitor changes in the repository, which hosts the deployment definitions for <span class="No-Break">the application.</span></li>
				<li><strong class="bold">Application deployment via Argo CD</strong>: It then ensures that the specific namespace for the application is created and ready <span class="No-Break">for deployment.</span></li>
				<li><strong class="bold">Application sync</strong>: Next, it applies the <strong class="source-inline">argocd_deployment.yaml</strong> file, which defines the Argo CD application. This manifest specifies the path to the Kubernetes deployment manifests within the Git repository, the revision target (e.g., <strong class="source-inline">branch</strong>), and <span class="No-Break">synchronization policies.</span></li>
				<li><strong class="bold">Sync trigger</strong>: Optionally, this <a id="_idIndexMarker984"/>step triggers a manual sync if immediate deployment is required, though typically Argo CD would automatically detect changes based on its <span class="No-Break">polling strategy.</span></li>
			</ol>
			<p class="callout-heading">Managing downtime and ensuring continuity with Argo CD</p>
			<p class="callout">When Argo CD experiences temporary downtime, the primary impact is on the synchronization and automated reconciliation of deployments in Kubernetes environments. During this period, any changes committed to the Git repository will not be synchronized with Kubernetes clusters, which means that updates, fixes, and new feature deployments are postponed. The automated reconciliation process, which ensures that the actual state of the Kubernetes environment matches the desired state specified in the Git repository, is also interrupted. This means that any discrepancies or configuration drifts that occur during the downtime will not be addressed until Argo CD is back online. Upon restoring Argo CD, it will automatically begin to process and apply all changes made during its downtime. The system will fetch the latest configurations from Git and proceed with the necessary reconciliations to align the Kubernetes clusters with the desired states from the repository. It’s important to note that the running applications themselves are not directly affected by Argo CD’s downtime; they will continue to operate as configured prior to the outage. However, to manage critical updates during such downtimes, teams might need to perform manual interventions, which should be handled carefully to avoid further complications once Argo CD resumes normal operation. Robust monitoring and alert systems are recommended to quickly detect any issues with Argo CD and to minimize the impact of <span class="No-Break">such downtimes.</span></p>
			<ol>
				<li value="6">To see the new workflow in action, we need to replace the contents of the <strong class="source-inline">gitops-for-real-ci-cd-pipeline.yml</strong> file in the <strong class="source-inline">Step-02-ArgoCD-Deployment</strong> folder with the contents of the file named in the same manner located in the <strong class="source-inline">.github/workflows</strong> subdirectory. We must then commit and push the updated code to trigger a workflow run, as illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer154">
					<img alt="Figure 11.7 – A new workflow run is triggered after the commit and push of the new workflow definition" src="image/B22100_11_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – A new workflow run is triggered after the commit and push of the new workflow definition</p>
			<p class="list-inset">The external <a id="_idIndexMarker985"/>IP and the admin password that were automatically generated during the setup process can be found in the log of the <strong class="source-inline">Setup ArgoCD on AKS</strong> task, as illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.8</em>. Be careful: this kind of information should not be exposed in real production environments. It is just a shortcut for the scope of <span class="No-Break">this example.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer155">
					<img alt="Figure 11.8 – The Setup ArgoCD on AKS task log containing sensitive information" src="image/B22100_11_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – The Setup ArgoCD on AKS task log containing sensitive information</p>
			<p>At this point, we can log in to the<a id="_idIndexMarker986"/> admin UI of the deployed instance of Argo CD by typing <a href="https://4.226.41.44/">https://4.226.41.44/</a> into your preferred browser (see <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">):</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer156">
					<img alt="Figure 11.9 – The Argo CD home page after logging in, showing the deployment of the backend-api-weather-app pod" src="image/B22100_11_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – The Argo CD home page after logging in, showing the deployment of the backend-api-weather-app pod</p>
			<p>Voilà! For this<a id="_idIndexMarker987"/> example, we didn’t activate <strong class="bold">auto-sync</strong>, so we just need to click on the <strong class="bold">Sync Apps</strong> button to synchronize our weather app application, as illustrated in <span class="No-Break"><em class="italic">figure 11.10</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer157">
					<img alt="Figure 11.10 – The Argo CD application is correctly synchronized with the GitHub repository" src="image/B22100_11_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – The Argo CD application is correctly synchronized with the GitHub repository</p>
			<p>To test the<a id="_idIndexMarker988"/> Argo CD synchronization process, try changing the number of replicas from <strong class="source-inline">1</strong> to <strong class="source-inline">5</strong> (for instance) in the <strong class="source-inline">backend-api-deployment.yaml</strong> file and pushing the change to GitHub. The workflow will not be triggered because we specified the <span class="No-Break">following value:</span></p>
			<pre class="source-code">
paths-ignore:
- 'Step-01/deployment/**'</pre>			<p>However, Argo CD will notice the out-of-sync state and a new synchronization will be needed. Now that we have our CI/CD pipeline in place and working perfectly, it is time to introduce the topics of scalability and efficiency in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor217"/>Designing for scalability and efficiency</h1>
			<p>In this section, we will <a id="_idIndexMarker989"/>delve into designing for scalability and efficiency. These traits are essential in the architecture of modern applications, exemplified by our weather app. Scalability ensures that the application can handle growth, whether it’s an increasing number of users, data volume, or transaction frequency, without compromising performance. Efficiency involves optimizing resource use, which is crucial for minimizing costs and enhancing response times. We will explore architectural principles that support scalability, such as <strong class="bold">microservices</strong> and<a id="_idIndexMarker990"/> load balancing, and discuss how to manage compute, storage, and networking resources effectively. Additionally, we will look at tools and strategies to test scalability to ensure that the architecture can withstand real-world demands. By mastering these elements, you’ll learn how to design a scalable and efficient architecture that is well-suited for deployment on Kubernetes, enhancing the overall performance and reliability of applications such as our <span class="No-Break">weather app.</span></p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor218"/>Architectural principles</h2>
			<p>Architectural principles<a id="_idIndexMarker991"/> for designing scalable and<a id="_idIndexMarker992"/> efficient systems are critical in modern application development, especially as demands for performance and reliability increase. Key strategies include decoupling components to minimize dependencies, which facilitates easier maintenance and scaling. Emphasizing statelessness allows for the replication and distribution of components, enhancing the application’s resilience <span class="No-Break">and responsiveness.</span></p>
			<p>Load balancing is essential to distribute incoming network loads evenly across multiple systems, preventing any single server from becoming overwhelmed and increasing the application’s availability. Horizontal scaling, or scaling out by adding more instances rather than adding resources to a single instance, is more cost-effective and increases <span class="No-Break">fault tolerance.</span></p>
			<p>Database sharding partitions data into smaller, more manageable segments, which is particularly beneficial for large datasets or high throughput demands. Sharding is great for improving performance. Caching frequently accessed data reduces latency and backend load by serving common requests without redundant <span class="No-Break">data processing.</span></p>
			<p>Asynchronous processing of tasks enhances throughput and user experience by handling operations in a non-blocking manner. Adopting a microservices architecture allows for independent deployment, scaling, and management of each service. This modular approach not only boosts performance but also simplifies management as applications evolve, making it ideal for cloud-native environments managed by platforms such as Kubernetes. Although microservices architecture has been mentioned, it will not be part of the example discussed in <span class="No-Break">this chapter.</span></p>
			<p class="callout-heading">Microservices architecture</p>
			<p class="callout">Microservices<a id="_idIndexMarker993"/> architecture is an architectural style that structures an application as a collection of loosely coupled services, each of which implements a specific business capability. This approach enables developers to build and deploy services independently, which enhances flexibility and accelerates development cycles. By breaking down an application into small, manageable components, microservices allow for more granular scaling and efficient resource utilization. Each service can be developed, deployed, and scaled independently, often using different programming languages and technologies that best suit the task at hand. This modularity improves fault isolation, making it easier to identify and fix issues without affecting the entire system. Moreover, microservices facilitate CI/CD practices, promoting a more agile and resilient development process. Overall, microservice architecture fosters a more robust and scalable application environment that is capable of adapting to evolving business needs and <span class="No-Break">technological advancements.</span></p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor219"/>Resource management</h2>
			<p>Effective resource management<a id="_idIndexMarker994"/> is crucial in application design and <a id="_idIndexMarker995"/>operation, especially in environments that aim to maximize efficiency and performance while minimizing costs. Managing compute, storage, and networking resources involves careful planning and orchestration to ensure that each component of an application has the necessary resources to perform optimally <span class="No-Break">without wastage:</span></p>
			<ul>
				<li><strong class="bold">Compute management</strong>: This involves<a id="_idIndexMarker996"/> provisioning the right amount of CPU and memory resources to meet the application’s requirements. Techniques such as <strong class="bold">auto-scaling</strong> and<a id="_idIndexMarker997"/> load balancing help distribute compute workloads evenly across the <span class="No-Break">available infrastructure.</span></li>
				<li><strong class="bold">Storage management</strong>: This ensures that data storage resources are allocated efficiently, keeping data <a id="_idIndexMarker998"/>accessibility and redundancy in mind. This includes choosing appropriate storage types and implementing data partitioning strategies to enhance performance <span class="No-Break">and scalability.</span></li>
				<li><strong class="bold">Networking management</strong>: This focuses<a id="_idIndexMarker999"/> on efficiently configuring network resources to ensure fast and secure data transfer between application components. Proper network configuration reduces latency and prevents bottlenecks, making it essential for real-time data processing <span class="No-Break">and delivery.</span></li>
			</ul>
			<p>Together, these<a id="_idIndexMarker1000"/> resource management practices ensure that applications can scale <a id="_idIndexMarker1001"/>effectively and remain robust under varying operational conditions. Implementing resource management strategies also involves monitoring and analyzing resource usage to make informed decisions about adjustments and improvements, ensuring that resources are utilized in the most efficient <span class="No-Break">way possible.</span></p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor220"/>Testing for scalability</h2>
			<p>Testing for scalability is crucial for ensuring that applications perform well under expected loads and can handle growth in users, transactions, and data efficiently. Scalability testing involves a variety of techniques to simulate different environments and stresses on the system to uncover potential issues before they <span class="No-Break">impact users:</span></p>
			<ul>
				<li><strong class="bold">Load testing</strong>: Simulates a<a id="_idIndexMarker1002"/> specific expected number of concurrent users or transactions to assess how the application behaves under <span class="No-Break">normal conditions</span></li>
				<li><strong class="bold">Stress testing</strong>: Pushes the application beyond its normal operational limits to discover its maximum capacity <a id="_idIndexMarker1003"/>and understand its behavior under <span class="No-Break">extreme conditions</span></li>
				<li><strong class="bold">Soak testing</strong>: Runs the <a id="_idIndexMarker1004"/>application under a heavy load for a prolonged period to identify issues such as memory leaks or slow degradation <span class="No-Break">of performance</span></li>
				<li><strong class="bold">Spike testing</strong>: Checks the<a id="_idIndexMarker1005"/> application’s ability to handle sudden and large spikes <span class="No-Break">in traffic</span></li>
				<li><strong class="bold">Scalability testing</strong>: Tests whether the application can scale up or down based on demand by gradually <a id="_idIndexMarker1006"/>increasing the load and observing how additional resources affect the <span class="No-Break">application’s capacity</span></li>
			</ul>
			<p>These tests often utilize automated testing tools and are conducted in staged environments that <a id="_idIndexMarker1007"/>closely mimic <a id="_idIndexMarker1008"/>real-world traffic patterns. Tools such as <strong class="bold">Apache JMeter</strong>, <strong class="bold">LoadRunner</strong>, and <strong class="bold">Gatling</strong>, along with cloud services such <a id="_idIndexMarker1009"/>as <strong class="bold">AWS CloudWatch</strong> and <strong class="bold">Google Cloud Monitoring</strong>, are commonly employed to <a id="_idIndexMarker1010"/>facilitate these tests. Through regular scalability <a id="_idIndexMarker1011"/>testing across development and deployment phases, teams can ensure that their applications are robust, scalable, and ready to handle real-world <span class="No-Break">operational demands.</span></p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor221"/>Resources management and scalability</h1>
			<p>In this section, we will continue to use the weather app to see how resource management, horizontal scaling, and scalability testing work in a real-world scenario. We can start with the optimization of <span class="No-Break">resource usage.</span></p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor222"/>Optimizing resource usage</h2>
			<p>To optimize resource usage in <a id="_idIndexMarker1012"/>Kubernetes, setting up resource requests and limits is crucial. These settings ensure that pods receive the right amount of CPU and memory resources to function properly while also preventing any single application from consuming excessive cluster resources, which could affect <span class="No-Break">other applications:</span></p>
			<ul>
				<li><strong class="bold">Requests</strong>: These are the amount of resources Kubernetes guarantees for a container. If a container requires more resources than its request<a id="_idIndexMarker1013"/> and they are available on the node, it can <span class="No-Break">consume more.</span></li>
				<li><strong class="bold">Limits</strong>:  This is the maximum amount of resources a container can use. If a container tries to exceed this<a id="_idIndexMarker1014"/> limit, the system will throttle its CPU usage. If the container exceeds its memory limit, Kubernetes might terminate it, depending on <span class="No-Break">the situation.</span></li>
			</ul>
			<p>To test the use of requests and limits, we can try to update the <strong class="source-inline">Step-01/deployment/backend-api-deployment.yaml</strong> file by adding the following code block immediately after <strong class="source-inline">key: </strong><span class="No-Break"><strong class="source-inline">WEATHER_API_KEY row</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
…         <strong class="bold">key: WEATHER_API_KEY</strong>
resources:
          <strong class="bold">requests</strong>:
            cpu: «100m»
            memory: "100Mi"
          <strong class="bold">limits</strong>:
            cpu: «150m»
            memory: "150Mi"</pre>			<p>The previously mentioned code block specifies the resource requests and limits for a container. Here’s what each <span class="No-Break">line means:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">requests</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">cpu: "100m"</strong>: This requests 100 millicores (where 1,000 m equals 1 CPU core) for <span class="No-Break">the container</span></li><li><strong class="source-inline">memory: "100Mi"</strong>: This requests 100 mebibytes <span class="No-Break">of memory</span></li></ul></li>
				<li><span class="No-Break"><strong class="source-inline">limits</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">cpu: "150m"</strong>: This sets a limit of 150 millicores for CPU usage by <span class="No-Break">the container</span></li><li><strong class="source-inline">memory: "150Mi"</strong>: This sets a memory limit of <a id="_idIndexMarker1015"/><span class="No-Break">150 mebibytes</span></li></ul></li>
			</ul>
			<p>Now that we have defined requests and limits, in the next section, we will see how to implement the <strong class="bold">Horizontal Pod Autoscaler</strong> (<strong class="bold">HPA</strong>) in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor223"/>Implementing the HPA</h2>
			<p>Implementing an HPA in <a id="_idIndexMarker1016"/>Kubernetes is an effective way to automatically scale the number of pod replicas in a deployment, replication controller, or replica set based on observed CPU utilization or other select metrics such as memory usage or custom metrics. Here’s a step-by-step guide to setting up <span class="No-Break">an HPA:</span></p>
			<ol>
				<li>Ensure that the <strong class="bold">Metrics Server</strong>, which collects resource metrics from Kubelets and exposes them in Kubernetes through the Metrics API, is installed in the cluster. This is crucial for the HPA to make scaling decisions. We can install it with the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ az aks update --enable-azure-monitor-metrics --name gitops-real-aks --resource-group gitops-for-real-rg</strong></pre></li>				<li>Setup will take a few minutes. At completion, verify the correct installation with the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ kubectl get deployment metrics-server -n kube-system</strong></pre><p class="list-inset">This command should return the deployment details confirming that the Metrics Server is up <span class="No-Break">and running:</span></p><pre class="source-code">NAME            READY   UP-TO-DATE   AVAILABLE   AGE
metrics-server  2/2     2            2           3h37m</pre></li>				<li>    Create an <a id="_idIndexMarker1017"/>HPA that scales based on <span class="No-Break">CPU utilization:</span><pre class="source-code">
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: weather-app-backend-api-hpa
  namespace: weather-app-for-real
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: backend-api-weather-app
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 5</pre><p class="list-inset">This HPA is configured to maintain between <strong class="source-inline">1</strong> and <strong class="source-inline">5</strong> replicas of the pod, scaling up or down when the CPU utilization <span class="No-Break">reaches 50%.</span></p></li>				<li>Create a new <strong class="source-inline">hpa.yaml</strong> file in the <strong class="source-inline">Step-01/deployment</strong> folder with the content that we have described. Commit and push the code, then wait for the Argo CD application<a id="_idIndexMarker1018"/> synchronization or force it if auto-sync <span class="No-Break">is off.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer158">
					<img alt="Figure 11.11– The Argo CD application synchronized with the HPA configuration in place" src="image/B22100_11_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11– The Argo CD application synchronized with the HPA configuration in place</p>
			<ol>
				<li value="5">Use the following command to monitor the status and effectiveness of <span class="No-Break">your HPA:</span><pre class="source-code">
<strong class="bold">$ kubectl get hpa -w --namespace weather-app-for-real</strong></pre><p class="list-inset">This will show the current number of replicas and whether the HPA is in the process of scaling up or down based on the current CPU utilization against the <span class="No-Break">target set.</span></p></li>			</ol>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor224"/>Testing for scalability – an example</h2>
			<p>Now that we<a id="_idIndexMarker1019"/> have implemented the HPA and set up monitoring for it, our next step is to observe how Kubernetes dynamically scales the number of pods in response to changes in CPU utilization. By simulating varying loads, we can watch the HPA adjust the pod count to maintain optimal performance. This process ensures that our application scales efficiently, handling increases or decreases in demand without manual intervention. Understanding this behavior is crucial for optimizing resource management and cost-effectiveness within our Kubernetes environment. What follows is a guided step-by-step <span class="No-Break">testing scenario:</span></p>
			<ol>
				<li>Create a Bash script for testing HPA. Use the <strong class="source-inline">curl</strong> command to make requests to the exposed weather service using various cities and include a random value in the query string to avoid caching. You can use the <strong class="source-inline">hpa-testing.sh</strong> script present in the repository accompanying this chapter as reference. Before executing the script, update <strong class="source-inline">$baseUrl</strong> to match your weather service’s URL. This might look <span class="No-Break">as follows:</span><pre class="source-code">
# Base URL of your weather service
baseUrl="http://20.250.198.208/weather"</pre></li>				<li>Run this <span class="No-Break">Bash script:</span><ol><li class="upper-roman">Open a new terminal and make the script executable with <span class="No-Break">the following:</span></li></ol><pre class="source-code">
<strong class="bold">$ chmod +x test_weather_app.sh</strong></pre><ol><li class="upper-roman" value="2">Execute the script by running <span class="No-Break">the following:</span></li></ol><pre class="source-code"><strong class="bold"> ./hpa_testing.sh.</strong></pre><p class="list-inset">After a while, the percentages described by the <strong class="source-inline">TARGET</strong> column should increase. To speed up the testing scenario and see the number of pods increase faster, execute another instance of the same script in <span class="No-Break">another terminal.</span></p></li>				<li>Check the <a id="_idIndexMarker1020"/>HPA status (see the sixth point in the <em class="italic">Implementing the HPA</em> section of this chapter). Use the following command to check the <span class="No-Break">HPA status:</span><pre class="source-code">
<strong class="bold">$ kubectl get hpa -w --namespace weather-app-for-real</strong></pre><p class="list-inset">You should see the current number of replicas and their scaling activities based on the <span class="No-Break">CPU utilization:</span></p><pre class="source-code">NAME                TARGETS     MINPODS      MAXPODS       REPLICAS
weather-app-…       cpu: &lt;unk&gt;%/5%  1     5             1
weather-app-…    cpu: 5%/5%  1            5             1
weather-app-…    cpu: 20%/5% 1            5             4
…
weather-app-…    cpu: 3%/5%  1         5             4
weather-app-…    cpu: 1%/5%    1             5             4</pre></li>			</ol>
			<p>The HPA monitors the CPU utilization of the deployment and adjusts the number of pods accordingly to ensure <span class="No-Break">optimal performance.</span></p>
			<p>Initially, the CPU utilization is marked as <strong class="source-inline">&lt;unknown&gt;</strong>, likely due to metrics not being available or still being initialized. When the utilization stabilizes at 5%, which matches the target set in the HPA, there’s no change in the number of replicas and they remain at one. As CPU usage increases to 20%—well above the 5% target—the HPA reacts by scaling up the number of replicas from <strong class="source-inline">1</strong> to <strong class="source-inline">4</strong> to handle the <span class="No-Break">increased load.</span></p>
			<p>This elevated level of resource use persists briefly, keeping the replicas at <strong class="source-inline">4</strong>. However, when the utilization drops significantly to 3% and further down to 1%, the HPA initially doesn’t scale down immediately, possibly due to stabilization settings that prevent oscillations in pod count. Ultimately, as the low utilization continues, the HPA scales the number of pods back down <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">.</span></p>
			<p>This sequence <a id="_idIndexMarker1021"/>demonstrates the HPA’s capability to dynamically scale application resources based on real-time data, thus ensuring that the deployment scales efficiently in response to workload changes. This dynamic adjustment helps manage resources effectively, maintaining application responsiveness and optimizing operational costs. The responsiveness of the HPA to changes in CPU utilization exemplifies how Kubernetes can automate scaling to maintain performance and resource efficiency without <span class="No-Break">manual intervention.</span></p>
			<p>As we ensure efficient resource management and scalability, it is equally important to turn our attention to monitoring and securing your application. In the next section, we will explore these crucial aspects of <span class="No-Break">operational excellence.</span></p>
			<h1 id="_idParaDest-221"><a id="_idTextAnchor225"/>Monitoring and securing your application</h1>
			<p>Operational excellence in software deployment and management is a crucial factor for the success of any technology-driven organization. The keys to achieving this excellence are monitoring, scaling, and security, each serving as foundational pillars that ensure the smooth and efficient operation of applications in <span class="No-Break">production environments.</span></p>
			<p>Monitoring<a id="_idIndexMarker1022"/> is vital as it provides the visibility needed to understand the behavior of applications and systems in real time. Effective monitoring strategies help in identifying performance bottlenecks, predicting system failures, and gathering valuable data to aid in decision-making processes. This continuous oversight allows teams to respond proactively to issues before they affect the user experience or lead to more <span class="No-Break">significant disruptions.</span></p>
			<p>Security practices<a id="_idIndexMarker1023"/> are critical to safeguard sensitive data and protect infrastructures from breaches and attacks. In an era where cyber threats are evolving rapidly, ensuring that robust security measures are in place is non-negotiable. Security protocols help in maintaining trust with customers, complying with regulatory requirements, and avoiding the financial and reputational damage associated with <span class="No-Break">data breaches.</span></p>
			<p>Together, monitoring, scaling, and security form the backbone of operational excellence, supporting a stable, efficient, and secure environment for deploying and managing applications. Organizations that master these aspects are better positioned to leverage technology for business success, ensuring that they can deliver continuous value to users while adapting to the ever-changing <span class="No-Break">digital landscape.</span></p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor226"/>Monitoring</h2>
			<p>Grafana and Prometheus are powerful tools that are widely used in the monitoring and observability landscape. They are particularly valuable for managing cloud-native applications deployed in dynamic environments such <span class="No-Break">as Kubernetes:</span></p>
			<ul>
				<li><strong class="bold">Prometheus</strong>: Prometheus<a id="_idIndexMarker1024"/> is an open source monitoring system with a robust query language. It collects and stores its metrics as time-series data, meaning that each metric is stored with its exact time of recording. Prometheus is highly effective for recording real-time metrics in a high-availability environment. It supports a pull model for fetching data from monitored services, allowing it to actively scrape data from registered targets at specified intervals. This data can then be queried and analyzed to monitor the health and performance <span class="No-Break">of applications.</span></li>
				<li><strong class="bold">Grafana</strong>: Grafana<a id="_idIndexMarker1025"/> is an open source analytics and visualization platform that integrates seamlessly with a multitude of data sources, including Prometheus. Grafana is used to create comprehensive dashboards that provide visualizations of metrics data. These dashboards allow developers and operations teams to visually interpret complex data to understand application behavior and resource usage, making it easier to spot trends, patterns, and <span class="No-Break">potential problems.</span></li>
			</ul>
			<p>Together, Prometheus<a id="_idIndexMarker1026"/> and <a id="_idIndexMarker1027"/>Grafana offer a powerful combination for data gathering, storage, and visualization, enhancing the ability to observe system behaviors, troubleshoot issues, and ensure that system performance aligns with user expectations and business objectives. This duo is particularly effective in a DevOps context, where continuous monitoring and feedback loops are critical to the software development and deployment <span class="No-Break">life cycle.</span></p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor227"/>Setting up Prometheus and Grafana</h2>
			<p>The <a id="_idIndexMarker1028"/>following are the necessary steps to set up Prometheus and Grafana on the <span class="No-Break">AKS </span><span class="No-Break"><a id="_idIndexMarker1029"/></span><span class="No-Break">cluster:</span></p>
			<ol>
				<li>Add the Prometheus and Grafana Helm <span class="No-Break">chart repository:</span><pre class="source-code">
<strong class="bold">$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts</strong></pre></li>				<li>Update the Helm repository. Ensure that we are using the most <span class="No-Break">up-to-date version:</span><pre class="source-code">
<strong class="bold">$ helm repo update</strong></pre></li>				<li>Install the Helm Chart in a namespace <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">gitops-real-monitoring</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">$ helm install prometheus \</strong>
<strong class="bold">  prometheus-community/kube-prometheus-stack \</strong>
<strong class="bold">  --namespace gitops-real-monitoring \</strong>
<strong class="bold">  --create-namespace</strong></pre></li>				<li>Check the entire deployment by typing the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ kubectl get all -n gitops-real-monitoring</strong></pre></li>				<li>Since we are not using a <strong class="source-inline">LoadBalancer</strong> service’s type, to log in to Grafana, we need to execute <span class="No-Break">a port-forward:</span><pre class="source-code">
<strong class="bold">$ kubectl port-forward svc/prometheus-grafana -n gitops-real-monitoring 9000:80</strong></pre></li>				<li> Log in with the <strong class="source-inline">admin</strong> username the password defined for <span class="No-Break">the prom-operator.</span></li>
				<li> Expose <a id="_idIndexMarker1030"/>Prometheus using the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ kubectl port-forward svc/prometheus-kube-prometheus-prometheus -n gitops-real-monitoring 9001:9090</strong></pre></li>				<li>After logging in, you should be able to see the <a id="_idIndexMarker1031"/>Grafana homepage. Click on the <strong class="bold">Dashboards</strong> menu item as illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer159">
					<img alt="Figure 11.12 – The Grafana home page, with the Dashboards menu item highlighted" src="image/B22100_11_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.12 – The Grafana home page, with the Dashboards menu item highlighted</p>
			<ol>
				<li value="9">Click on <strong class="bold">Kubernetes</strong> | <strong class="bold">Compute Resources</strong> | <strong class="bold">Namespace (Pods)</strong> and change the namespace to <strong class="source-inline">weather-app-for-real</strong>. You will see some interesting metrics on the Pods that are running there, as illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer160">
					<img alt="Figure 11.13 – CPU, memory, and other metrics for the backend-api-weather pod" src="image/B22100_11_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.13 – CPU, memory, and other metrics for the backend-api-weather pod</p>
			<p>At this point, you have correctly and successfully set up Grafana and Prometheus. Now, you can see interesting statistics about the usage of <strong class="source-inline">backend-api-weather-app</strong>, which can be used to fine-tune the resource limits and requests, as discussed in the <em class="italic">Optimize resource usage</em> section of this chapter. In the next section, we will introduce another important aspect of Kubernetes management in the real world: <span class="No-Break">Kubernetes security.</span></p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor228"/>Understanding Kubernetes security</h2>
			<p>Kubernetes, while <a id="_idIndexMarker1032"/>robust and scalable, presents a unique set of security challenges that stem from its dynamic and distributed nature. Securing a Kubernetes cluster involves safeguarding the infrastructure, the applications running on it, and the data that it processes. Given the complexity of Kubernetes environments, security must be integrated into every layer of the cluster. Key aspects of Kubernetes security include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Authentication and authorization</strong>: This ensures that only verified users can access the cluster <a id="_idIndexMarker1033"/>with methods such as certificates and tokens. It also <a id="_idIndexMarker1034"/>controls user actions using mechanisms such as <a id="_idIndexMarker1035"/>RBAC and <strong class="bold">Attributed-Based Access </strong><span class="No-Break"><strong class="bold">Control</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ABAC</strong></span><span class="No-Break">).</span></li>
				<li><strong class="bold">API security</strong>: Protecting the Kubernetes API<a id="_idIndexMarker1036"/> server, which acts as the central control unit for the cluster, is crucial. Securing access to the API involves using SSL/TLS encryption, API request auditing, and limiting IP access through <span class="No-Break">network policies.</span></li>
				<li><strong class="bold">Network security</strong>: Enforcing <a id="_idIndexMarker1037"/>policies that control the flow of traffic between pods and external networks helps prevent unauthorized access and limits the potential for lateral movement within <span class="No-Break">the cluster.</span></li>
				<li><strong class="bold">Pod security admission</strong>: This is a <a id="_idIndexMarker1038"/>Kubernetes admission controller that enforces security settings on pods at creation time, using predefined security profiles (<strong class="bold">privileged</strong>, <strong class="bold">baseline</strong>, and <strong class="bold">restricted</strong>) to ensure compliance with best security practices and prevent <span class="No-Break">privilege escalations.</span></li>
				<li><strong class="bold">Secrets management</strong>: Kubernetes <a id="_idIndexMarker1039"/>manages sensitive data (such as passwords and tokens) using secrets. Proper handling and security of secrets, including encryption at rest and in transit, is vital to protect <span class="No-Break">sensitive information.</span></li>
			</ul>
			<p class="callout-heading">The importance of a layered security approach</p>
			<p class="callout">Given the complexities of Kubernetes, a single security measure is often not enough. A layered security approach that includes network segmentation, threat detection, secure access controls, and ongoing vulnerability management is crucial for protecting Kubernetes environments <span class="No-Break">from threats.</span></p>
			<p>In the next section, we will see a practical example of how to manage access to the weather app’s resources within a specific namespace <span class="No-Break">using RBAC.</span></p>
			<h3>Configuring Kubernetes RBAC for user and role management</h3>
			<p>Here is a<a id="_idIndexMarker1040"/> step-by-step guide to configuring RBAC for the <span class="No-Break">weather app:</span></p>
			<ol>
				<li>Define a role that specifies the permissions for managing specific resources related to the weather app, such as deployments, services, and pods within a designated namespace. The following are the definitions for the <span class="No-Break"><strong class="source-inline">weather-app-manager</strong></span><span class="No-Break"> role:</span><pre class="source-code">
<strong class="bold">apiVersion: rbac.authorization.k8s.io/v1</strong>
<strong class="bold">kind: Role</strong>
<strong class="bold">metadata:</strong>
<strong class="bold">  namespace: weather-app-for-real</strong>
<strong class="bold">  name: weather-app-manager</strong>
<strong class="bold">rules:</strong>
<strong class="bold">- apiGroups: ["", "apps"]</strong>
<strong class="bold">  resources: ["deployments", "replicasets", "pods", "services"]</strong>
<strong class="bold">  verbs: ["get", "list", "watch", "create", "update", "delete"]</strong></pre><p class="list-inset">Here are the definitions for the <span class="No-Break"><strong class="source-inline">weather-app-user</strong></span><span class="No-Break"> role:</span></p><pre class="source-code">apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: weather-app-for-real
  name: <strong class="bold">weather-app-user</strong>
rules:
- apiGroups: ["", "apps"]
  resources: [<strong class="bold">"pods", "services"</strong>]
  verbs: [<strong class="bold">"get", "list", "watch"</strong>]</pre><p class="list-inset">In a production environment, user management might be handled outside Kubernetes through<a id="_idIndexMarker1041"/> OIDC, LDAP, or other authentication services. For demonstration purposes, this example assumes that user credentials and certificates are managed by your Kubernetes administrator or through a cloud provider’s <span class="No-Break">IAM system.</span></p></li>				<li>Create a <strong class="source-inline">RoleBinding</strong> resource to grant the specified role to a user. This binding will apply the <strong class="source-inline">weather-app-manager</strong> role to a user <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">weather-app-user</strong></span><span class="No-Break">:</span><pre class="source-code">
apiVersion: rbac.authorization.k8s.io/v1
kind: <strong class="bold">RoleBinding</strong>
metadata:
  name: <strong class="bold">weather-app-manager-binding</strong>
  namespace: weather-app-for-real
subjects:
- kind: <strong class="bold">User</strong>
  name: <strong class="bold">weather-app-user</strong>
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: weather-app-manager
  apiGroup: rbac.authorization.k8s.io</pre></li>				<li>Create another <strong class="source-inline">RoleBinding</strong> resource to grant the <strong class="source-inline">weather-app-user</strong> role to a <a id="_idIndexMarker1042"/>user <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">weather-app-operator</strong></span><span class="No-Break">:</span><pre class="source-code">
apiVersion: rbac.authorization.k8s.io/v1
kind: <strong class="bold">RoleBinding</strong>
metadata:
  name: <strong class="bold">weather-app-operator-binding</strong>
  namespace: weather-app-for-real
subjects:
- kind: User
  name: <strong class="bold">weather-app-operator</strong>
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: <strong class="bold">weather-app-operator</strong>
  apiGroup: rbac.authorization.k8s.io</pre></li>				<li>Move the YAML files to the <strong class="source-inline">Step-01/deployment</strong> folder, push the changes to GitHub, and synchronize the Argo <span class="No-Break">CD app.</span></li>
				<li>Verify that the <strong class="source-inline">weather-app-user</strong> and <strong class="source-inline">weather-app-operator</strong> users have the necessary permissions using the <strong class="source-inline">kubectl auth </strong><span class="No-Break"><strong class="source-inline">can-i</strong></span><span class="No-Break"> command:</span><pre class="source-code">
<strong class="bold">$ kubectl auth can-i delete pods --as weather-app-operator -n weather-app-for-real</strong>
<strong class="bold">$ kubectl auth can-i delete pods --as weather-app-manager -n weather-app-for-real</strong></pre><p class="list-inset">The expected output should be <span class="No-Break">as follows:</span></p><pre class="source-code">no
yes</pre></li>				<li>We can list all roles and role bindings in the namespace, or cluster roles affecting the user, with commands such as <span class="No-Break">the following:</span><pre class="source-code">
<strong class="bold">$ kubectl get rolebindings,roles,clusterrolebindings,clusterroles --all-namespaces -o yaml</strong></pre></li>			</ol>
			<p>For <a id="_idIndexMarker1043"/>reference, the <strong class="source-inline">Step-03-Security</strong> directory in the repository accompanying this chapter contains the YAML files with the role and role-binding definitions described <span class="No-Break">so far.</span></p>
			<p class="callout-heading">Beware!</p>
			<p class="callout">To avoid incurring unexpected expenses due to Azure resources, please remember to destroy any undesired Azure <span class="No-Break">provisioned resources.</span></p>
			<p>This section concludes our journey through a real-world GitOps pipeline and deployment. While an entire book might not be enough to delve deeply into every aspect of GitOps with Kubernetes, security, and deployments, we believe that the sections covered so far provide a comprehensive overview. They offer valuable insights into setting up an effective GitOps pipeline for your future <span class="No-Break">Kubernetes projects.</span></p>
			<h1 id="_idParaDest-225"><a id="_idTextAnchor229"/>Summary</h1>
			<p>This chapter provided a comprehensive guide to deploying real-world projects on Kubernetes with GitOps. By following the detailed instructions and examples, you learned how to set up a GitOps and Kubernetes development environment, implement CI/CD processes, design for scalability and efficiency, manage resources, and secure your application. This practical knowledge equipped you with the skills needed to effectively implement these cutting-edge technologies in your projects, enhancing your organizational capabilities and personal <span class="No-Break">technical expertise.</span></p>
			<p>As you now have a solid foundation in deploying and managing applications with GitOps on Kubernetes, the next chapter will delve into observability with GitOps, providing essential strategies to monitor and gain insights into your applications’ performance <span class="No-Break">and health.</span></p>
		</div>
	

		<div class="Content" id="_idContainer162">
			<h1 id="_idParaDest-226" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor230"/>Part 4: Operational Excellence Through GitOps Best Practices</h1>
			<p>In this part, you will focus on achieving operational excellence through best practices in GitOps. You will learn about integrating observability, enhancing security, managing financial operations, and preparing for future trends in GitOps. This section aims to provide a comprehensive guide to maintaining high standards of operational efficiency and security, while also addressing sustainability and financial considerations, thus ensuring that your GitOps practices are both cutting edge <span class="No-Break">and sustainable.</span></p>
			<p>This part includes the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B22100_12.xhtml#_idTextAnchor231"><em class="italic">Chapter 12</em></a>, Observability with GitOps</li>
				<li><a href="B22100_13.xhtml#_idTextAnchor257"><em class="italic">Chapter 13</em></a>, Security with GitOps</li>
				<li><a href="B22100_13.xhtml#_idTextAnchor257"><em class="italic">Chapter 13</em></a>, FinOps, Sustainability, AI, and Future Trends for GitOps</li>
			</ul>
		</div>
		<div>
			<div id="_idContainer163">
			</div>
		</div>
		<div>
			<div class="Basic-Graphics-Frame" id="_idContainer164">
			</div>
		</div>
	</body></html>