["```\n    $ kubectl get node <replace-node-name> -o jsonpath='{.status.allocatable}' | jq .\n    {\n      \"nvidia.com/gpu\": \"1\",\n    ...\n    ```", "```\n    module \"eks_data_addons\" {\n      source = \"aws-ia/eks-data-addons/aws\"\n      ...\n      enable_nvidia_device_plugin = true\n    ```", "```\n    $ helm list -n nvidia-device-plugin\n    NAME   NAMESPACE     CHART\n    nvidia-device-plugin\n    nvidia-device-plugin\n    nvidia-device-plugin\n    $ kubectl get ds -n nvidia-device-plugin --no-headers\n    nvidia-device-plugin\n    nvidia-device-plugin-gpu-feature-discovery\n    nvidia-device-plugin-node-feature-discovery-worker\n    ```", "```\n    module \"eks\" {\n      source = \"terraform-aws-modules/eks/aws\"\n      ...\n      eks_managed_node_groups = {\n        eks-gpu-mng = {\n          instance_types = [\"g6.2xlarge\"]\n          taints = {\n            gpu = {\n              key = \"nvidia.com/gpu\"\n              value = \"true\"\n              effect = \"NO_SCHEDULE\"\n    ...\n    ```", "```\n    $ kubectl label node <node-name> hardware-type=gpu\n    ```", "```\n    ...\n    eks-gpu-mng = {\n          labels = {\n            \"hardware-type\" = \"gpu\"\n          }\n    ...\n    ```", "```\n        apiVersion: v1\n        kind: Pod\n        metadata:\n          name: gpu-demo-pod\n        spec:\n          tolerations:\n          - key: \"nvidia.com/gpu\"\n            operator: \"Exists\"\n            effect: \"NoSchedule\"\n          nodeSelector:\n            hardware-type: gpu\n          containers:\n          - name: gpu-container\n            image: nvidia/cuda\n            resources:\n              limits:\n                nvidia.com/gpu: 1\n        ```", "```\nresource \"helm_release\" \"dcgm_exporter\" {\n  name       = \"dcgm-exporter\"\n  repository = \"https://nvidia.github.io/dcgm-exporter/\"\n  chart      = \"dcgm-exporter\"\n  namespace = \"dcgm-exporter\"\n...\n```", "```\n$ terraform init\n$ terraform plan\n$ terraform apply -auto-approve\n$ kubectl get ds,pods -n dcgm-exporter\nNAME                           DESIRED   CURRENT   READY\ndaemonset.apps/dcgm-exporter   2         2         2\nNAME                      READY   STATUS    RESTARTS   AGE\npod/dcgm-exporter-729qb   1/1     Running   0          84s\npod/dcgm-exporter-rtmtw   1/1     Running   0          84s\n```", "```\n$ kubectl port-forward svc/dcgm-exporter -n dcgm-exporter 9400:9400\nForwarding from 127.0.0.1:9400 -> 9400\nForwarding from [::1]:9400 -> 9400\n$ curl http://localhost:9400/metrics\n# HELP DCGM_FI_DEV_GPU_UTIL GPU utilization (in %).\n# TYPE DCGM_FI_DEV_GPU_UTIL gauge\nDCGM_FI_DEV_GPU_UTIL{gpu=\"0\",UUID=\"GPU-173ced1d-4c1d-072d-6819-86522b018187\",pci_bus_id=\"00000000:31:00.0\",device=\"nvidia0\",modelName=\"NVIDIA L4\",Hostname=\"ip-10-0-34-196.us-west-2.compute.internal\"} 0\n# HELP DCGM_FI_DEV_MEM_COPY_UTIL Memory utilization (in %).\n# TYPE DCGM_FI_DEV_MEM_COPY_UTIL gauge\nDCGM_FI_DEV_MEM_COPY_UTIL{gpu=\"0\",UUID=\"GPU-173ced1d-4c1d-072d-6819-86522b018187\",pci_bus_id=\"00000000:31:00.0\",device=\"nvidia0\",modelName=\"NVIDIA L4\",Hostname=\"ip-10-0-34-196.us-west-2.compute.internal\"} 0\n...\n```", "```\nresources:\n  limits:\n    nvidia.com/mig-1g.18gb: 1\n```", "```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: time-slicing-config\n  namespace: nvidia-device-plugin\ndata:\n  any: |-\n    version: v1\n    flags:\n      migStrategy: none\n    sharing:\n      timeSlicing:\n        resources:\n        - name: nvidia.com/gpu\n          time-slicing-config ConfigMap:\n\n```", "```\n\n\t\t\tNow, update the NVIDIA K8s device plugin configuration so that it can use this ConfigMap in the Terraform code. Download the `aiml-addons.tf` file from [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/aiml-addons.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/aiml-addons.tf). This leverages the `time-slicing-config` ConfigMap to initialize the NVIDIA device plugin, as shown in the following code snippet:\n\n```", "```\n\n\t\t\tExecute the following command to apply the Terraform configuration:\n\n```", "```\n\n\t\t\tThe NVIDIA device plugin automatically reconciles the time-slicing configuration and updates the K8s node details accordingly. We can verify this by running the following command, which displays the GPU count as 10 on a g6.2xlarge EC2 instance. This indicates that one physical NVIDIA L4 GPU on the node has been virtualized into 10 replicas, making them available for the scheduler to allocate:\n\n```", "```\n\n\t\t\tTo demonstrate the use of NVIDIA time-slicing, let’s say we are deploying a small LLM such as **Meta’s Llama-3.2-1B** ([https://huggingface.co/meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B)) parameter model in our application. This model requires 1.8 GB of GPU memory to load and perform inference. In a traditional setup, we would assign a single L4 GPU with 24 GB of memory to one K8s Pod, resulting in 92% of the GPU memory being underutilized. However, by creating 10 time-sliced replicas, we can deploy multiple instances of the Llama-3.2-1B parameter model on a single GPU.\n\t\t\tWe’ve already containerized the Llama-3.2-1B model into a Python FastAPI application and published it on Docker Hub. Using this image, we can deploy multiple copies of the model’s endpoint on the K8s node. Download the K8s deployment manifest from [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/llama32-inf/llama32-deploy.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/llama32-inf/llama32-deploy.yaml). Then, update the manifest with your Hugging Face token and execute the following commands to create a K8s deployment with five replicas of this model. You can verify that all replicas will run on the same node while sharing one physical GPU efficiently:\n\n```", "```\n\n\t\t\tIn this walkthrough, we applied the same time-slicing configuration across all nodes in the K8s cluster, irrespective of the GPU type. If you are using multiple GPU types (e.g., A100, L4, L40S, H100, etc.) in the same cluster, you can consider using a **multiple node-specific configuration** setup. In this approach, you define different time-slicing configurations for each GPU type in the ConfigMap and use node labels to specify which configuration should be applied to each GPU node. Please refer to the NVIDIA documentation at [https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html#applying-multiple-node-specific-configurations](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html#applying-multiple-node-specific-configurations) for a detailed walkthrough.\n\t\t\tIn this section, we learned about various NVIDIA GPU partitioning techniques (MIG, MPS, and time-slicing) and the differences among them. MIG provides strong isolation with minimal overhead and predictable performance, making it ideal for multi-tenant environments and inference workloads. On the other hand, MPS optimizes resource utilization by enabling multiple processes to share a single GPU concurrently. Finally, time-slicing allows multiple workloads to interleave on a single GPU by allocating compute resources in a round-robin fashion. It is compatible with all CUDA-capable GPUs, including older architectures, making it a versatile option. However, time-slicing introduces higher context-switching overhead and lacks both isolation and the efficiency improvements of MPS. In the next section, we will explore additional scaling and optimization considerations when using GPUs in K8s clusters.\n\t\t\tScaling and optimization considerations\n\t\t\tWhen scaling K8s using GPU metrics, you can integrate NVIDIA DCGM to dynamically adjust workloads based on GPU health, utilization, and performance metrics. DCGM collects telemetry data, such as GPU utilization, memory usage, power consumption, and error rates, that is exposed through DCGM-Exporter for integration with Prometheus. Prometheus metrics are then consumed by autoscaling components such as **Horizontal Pod Autoscaler** (**HPA**) or **Vertical Pod Autoscaler** (**VPA**) to scale the K8s workloads.\n\t\t\tFor instance, in an ML training job that’s running on multiple nodes, DCGM-Exporter can track metrics such as `nvidia_gpu_utilization`, feeding them to Prometheus for analysis. In this instance, we can create an HPA policy that tracks GPU utilization and automatically increases the number of replicas (up to 10) when utilization exceeds 80%, thereby distributing processing across more GPUs:\n\n```", "```\n\n\t\t\tSimilarly, in an image recognition service, if the GPU utilization stays below 20% for 10 minutes, the autoscaler can reduce the number of Pods to save resources. DCGM also supports defining policies for *error handling* and *health monitoring*, ensuring K8s can reschedule Pods or drain nodes with unhealthy GPUs. For example, if DCGM detects memory errors or overheating, K8s can remove that node and migrate workloads to healthier nodes. These use cases demonstrate how GPU metrics enable fine-grained control over scaling to optimize resource usage.\n\t\t\tHowever, GPU-based scaling also presents challenges. Metrics such as utilization or memory usage may not directly correlate with application performance, thereby requiring careful threshold tuning. Additionally, in environments using GPU time-slicing or MIG, workloads share resources, and DCGM metrics must account for such configurations to avoid over-scaling.\n\t\t\tContinuously monitoring GPU metrics in large clusters can also add overhead, necessitating efficient integration with Prometheus and optimized alert rules. Scaling must also incorporate GPU health data, such as error rates or temperature, to avoid scheduling workloads on faulty hardware.\n\t\t\tBest practices for GPU scaling include leveraging DCGM’s built-in policies for error detection and recovery, using custom Prometheus metrics that combine GPU and application-specific data, integrating the NVIDIA GPU Operator for seamless management, and regularly testing and fine-tuning scaling thresholds. By combining NVIDIA DCGM with K8s’s autoscaling capabilities, you can optimize GPU utilization, reduce costs, and enhance the performance of GPU-accelerated applications, ensuring scalability and reliability in resource-intensive environments.Bottom of Form\n\t\t\tNVIDIA NIM\n\t\t\t**NVIDIA Inference Microservices** (**NIM**) ([https://developer.nvidia.com/nim](https://developer.nvidia.com/nim)) is a component of NVIDIA AI Enterprise that provides developers with GPU-accelerated inference microservices for deploying pretrained and customized AI models either on the cloud or in on-premises environments. These microservices are built on optimized inference engines from NVIDIA and the community, providing latency and throughput optimization for the specific combination of the foundation model and GPU system that’s detected at runtime. Additionally, NIM containers offer standard observability data feeds and built-in support for autoscaling on K8s with GPUs. Refer to the following AWS blog for a detailed walkthrough of deploying GenAI applications with NVIDIA NIM on Amazon EKS: [https://aws.amazon.com/blogs/hpc/deploying-generative-ai-applications-with-nvidia-nims-on-amazon-eks/](https://aws.amazon.com/blogs/hpc/deploying-generative-ai-applications-with-nvidia-nims-on-amazon-eks/).\n\t\t\tEach NIM container encapsulates a model, its runtime dependencies, and the inference engine. These containers are pre-configured for easy deployment and include APIs for interaction with applications. NIM microservices are deployed in a K8s cluster, enabling orchestration, scaling, and fault-tolerant management. Developers can use NIM to create custom AI workflows tailored to specific applications, such as RAG for chat-based question-answering or simulation pipelines for scientific research.\n\t\t\tNIM’s architecture is designed to streamline the deployment of AI applications by offering prebuilt microservices that can be customized and scaled according to specific use cases. For instance, developers can deploy **RAG pipelines** for chat-based question-answering using NIM-hosted models available in the NVIDIA API catalog. These microservices are regularly updated so that they incorporate the latest advancements in AI models across various domains, including speech AI, data retrieval, digital biology, digital humans, simulation, and LLMs.\n\t\t\tDevelopers interested in utilizing NIM can access it through the NVIDIA developer program, which offers free access for research, development, and testing purposes on up to 16 GPUs across any infrastructure – be it the cloud, a data center, or a personal workstation. For production deployments, NVIDIA AI Enterprise provides a comprehensive suite of tools, including NIM, that come with enterprise-grade security, support, and API stability ([https://www.nvidia.com/en-us/data-center/products/ai-enterprise/](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/)).\n\t\t\tGPU availability in the cloud\n\t\t\tLike CPU instances, GPU instances can be requested in the cloud as on-demand instances, reserved instances, or spot instances. However, the growing demand for GPUs has led to challenges in availability. In response, cloud providers are introducing innovative strategies to improve GPU accessibility and meet the needs of users more effectively.\n\t\t\tFor example, AWS has introduced **Amazon EC2 Capacity Blocks for ML** ([https://aws.amazon.com/ec2/capacityblocks/](https://aws.amazon.com/ec2/capacityblocks/)), which allows you to reserve GPU instances in **Amazon EC2 UltraClusters** ([https://aws.amazon.com/ec2/ultraclusters/](https://aws.amazon.com/ec2/ultraclusters/)) for future dates. At the time of writing, you can reserve capacity for durations ranging from 1 to 14 days, with the option to extend up to 6 months, and schedule start times up to 8 weeks in advance using **Capacity Blocks**. These blocks can be used for a wide range of ML workloads, from small-scale experiments to large-scale distributed training sessions, and are available for various instance types, including P5, P5e, P5en, P4d, Trn1, and Trn2, all powered by NVIDIA GPUs and AWS Trainium chips.\n\t\t\tSummary\n\t\t\tIn this chapter, we covered options for optimizing GPU resources in K8s for GenAI applications. First, we described custom AI/ML accelerators, such as AWS Inferentia, Trainium, and Google TPUs. These specialized devices offer high performance and cost-efficiency for GenAI workloads, such as training LLMs or low-latency inference use cases. K8s supports these accelerators through device plugins, allowing them to be integrated into existing clusters seamlessly.\n\t\t\tWe also covered options to optimize GPU utilization in K8s. This is a critical step due to the high costs of GPU instances and their underutilization. This chapter highlighted various techniques you can implement to address the inefficient use of resources, such as MIG, MPS, and time-slicing.\n\t\t\tMIG allows a single GPU to be partitioned into multiple isolated instances, providing a more granular and efficient allocation of resources. MPS, on the other hand, allows multiple processes to share GPU compute resources concurrently. Time-slicing further enables sequential access to the GPU by dividing execution time across different processes, a technique that’s beneficial for older GPUs that lack MIG support.\n\t\t\tFinally, we covered GPU scaling practices within K8s and emphasized the role of NVIDIA DCGM, which can collect GPU telemetry data such as GPU utilization, memory usage, and power consumption. By integrating DCGM with Prometheus and K8s auto-scalers, you can dynamically scale workloads based on real-time GPU performance metrics.\n\t\t\tNIM simplifies the deployment of AI models by providing pre-configured inference microservices optimized for specific GPU architectures. Cloud GPU availability remains a challenge, with demand often exceeding supply. In the next chapter, we will dive deeper into observability best practices for K8s.\n\n```"]