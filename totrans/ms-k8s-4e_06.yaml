- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Managing Storage
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储管理
- en: In this chapter, we’ll look at how Kubernetes manages storage. Storage is very
    different from compute, but at a high level they are both resources. Kubernetes
    as a generic platform takes the approach of abstracting storage behind a programming
    model and a set of plugins for storage providers. First, we’ll go into detail
    about the conceptual storage model and how storage is made available to containers
    in the cluster. Then, we’ll cover the common cloud platform storage providers,
    such as **Amazon Web Services** (**AWS**), **Google Compute Engine** (**GCE**),
    and Azure. Then we’ll look at a prominent open source storage provider, GlusterFS
    from Red Hat, which provides a distributed filesystem. We’ll also look into another
    solution – Ceph – that manages your data in containers as part of the Kubernetes
    cluster using the Rook operator. We’ll see how Kubernetes supports the integration
    of existing enterprise storage solutions. Finally, we will explore the **Constrainer
    Storage Interface** (**CSI**) and all the advanced capabilities it brings to the
    table.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨Kubernetes如何管理存储。存储与计算有很大的不同，但从高层次来看，它们都是资源。作为一个通用平台，Kubernetes采取了将存储抽象为编程模型和一组存储提供商插件的方式。首先，我们将详细了解概念存储模型，以及如何将存储提供给集群中的容器。接着，我们将介绍一些常见的云平台存储提供商，如**亚马逊网络服务**（**AWS**）、**谷歌计算引擎**（**GCE**）和Azure。然后，我们将介绍一个著名的开源存储提供商，Red
    Hat的GlusterFS，它提供了一个分布式文件系统。我们还将探讨另一个解决方案——Ceph，它通过Rook操作符将数据管理到容器中，成为Kubernetes集群的一部分。我们还将看到Kubernetes如何支持现有企业存储解决方案的集成。最后，我们将深入探讨**容器存储接口**（**CSI**）及其所带来的所有高级功能。
- en: 'This chapter will cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: Persistent volumes walk-through
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久卷演示
- en: Demonstrating persistent volume storage end to end
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示持久卷存储的端到端实现
- en: Public cloud storage volume types – GCE, AWS, and Azure
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公有云存储卷类型 – GCE、AWS 和 Azure
- en: GlusterFS and Ceph volumes in Kubernetes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes中的GlusterFS和Ceph卷
- en: Integrating enterprise storage into Kubernetes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将企业存储集成到Kubernetes中
- en: The Container Storage Interface
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器存储接口
- en: At the end of this chapter, you’ll have a solid understanding of how storage
    is represented in Kubernetes, the various storage options in each deployment environment
    (local testing, public cloud, and enterprise), and how to choose the best option
    for your use case.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将对Kubernetes中存储的表示方式、每种部署环境中的各种存储选项（本地测试、公有云和企业存储）有一个清晰的理解，并且能够根据你的使用场景选择最佳的存储选项。
- en: You should try the code samples in this chapter on minikube, or another cluster
    that supports storage adequately. The KinD cluster has some problems related to
    labeling nodes, which is necessary for some storage solutions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在minikube或其他支持存储的集群上尝试本章中的代码示例。KinD集群在节点标签化方面存在一些问题，而节点标签化是某些存储解决方案所必需的。
- en: Persistent volumes walk-through
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久卷演示
- en: In this section, we will understand the Kubernetes storage conceptual model
    and see how to map persistent storage into containers, so they can read and write.
    Let’s start by understanding the problem of storage.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解Kubernetes存储的概念模型，并了解如何将持久存储映射到容器中，以便它们能够读取和写入数据。让我们先从理解存储问题开始。
- en: Containers and pods are ephemeral. Anything a container writes to its own filesystem
    gets wiped out when the container dies. Containers can also mount directories
    from their host node and read or write to them. These will survive container restarts,
    but the nodes themselves are not immortal. Also, if the pod itself is evicted
    and scheduled to a different node, the pod’s containers will not have access to
    the old node host’s filesystem.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 容器和Pod是短暂的。容器写入其文件系统的任何内容都会在容器停止时被清除。容器还可以挂载主机节点的目录，并读取或写入它们。这些数据在容器重启时会继续存在，但节点本身并不是永生的。此外，如果Pod本身被驱逐并调度到不同的节点，Pod中的容器将无法访问旧节点主机的文件系统。
- en: There are other problems, such as ownership of mounted hosted directories when
    the container dies. Just imagine a bunch of containers writing important data
    to various data directories on their host and then going away, leaving all that
    data all over the nodes with no direct way to tell what container wrote what data.
    You can try to record this information, but where would you record it? It’s pretty
    clear that for a large-scale system, you need persistent storage accessible from
    any node to reliably manage the data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他问题，例如容器死亡时挂载的主机目录的所有权问题。试想一下，一堆容器将重要数据写入它们主机上的各种数据目录，然后消失，留下所有数据分布在节点上，无法直接知道哪个容器写了哪个数据。你可以尝试记录这些信息，但你会把它记录在哪里呢？很明显，对于一个大规模系统，你需要一个从任何节点都能访问的持久存储来可靠地管理数据。
- en: Understanding volumes
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解卷
- en: The basic Kubernetes storage abstraction is the volume. Containers mount volumes
    that are bound to their pod, and they access the storage wherever it may be as
    if it’s in their local filesystem. This is nothing new, and it is great, because
    as a developer who writes applications that need access to data, you don’t have
    to worry about where and how the data is stored. Kubernetes supports many types
    of volumes with their own distinctive features. Let’s review some of the main
    volume types.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的Kubernetes存储抽象是卷。容器挂载绑定到其Pod的卷，并像访问本地文件系统一样访问存储，无论它在哪里。这并不新鲜，而且非常好，因为作为一个开发者，在编写需要访问数据的应用程序时，你不必担心数据存储在哪里以及如何存储。Kubernetes支持许多类型的卷，它们具有各自独特的特点。让我们回顾一些主要的卷类型。
- en: Using emptyDir for intra-pod communication
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用emptyDir进行Pod间通信
- en: It is very simple to share data between containers in the same pod using a shared
    volume. Container 1 and container 2 simply mount the same volume and can communicate
    by reading and writing to this shared space. The most basic volume is the `emptyDir`.
    An `emptyDir` volume is an empty directory on the host. Note that it is not persistent
    because when the pod is evicted or deleted, the contents are erased. If a container
    just crashes, the pod will stick around, and the restarted container can access
    the data in the volume. Another very interesting option is to use a RAM disk,
    by specifying the medium as `Memory`. Now, your containers communicate through
    shared memory, which is much faster, but more volatile of course. If the node
    is restarted, the `emptyDir`'s volume contents are lost.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一Pod内使用共享卷在容器之间共享数据非常简单。容器1和容器2只需挂载相同的卷，并通过读取和写入该共享空间进行通信。最基本的卷是`emptyDir`。`emptyDir`卷是主机上的一个空目录。请注意，它是非持久性的，因为当Pod被驱逐或删除时，内容会被清除。如果容器崩溃，Pod会保留下来，重启的容器可以访问卷中的数据。另一个非常有趣的选项是使用RAM磁盘，通过将介质指定为`Memory`。现在，您的容器通过共享内存进行通信，这样的速度更快，但当然更容易丢失。如果节点被重启，`emptyDir`的卷内容会丢失。
- en: 'Here is a pod configuration file that has two containers that mount the same
    volume, called `shared-volume`. The containers mount it in different paths, but
    when the `hue-global-listener` container is writing a file to `/notifications`,
    the `hue-job-scheduler` will see that file under `/incoming`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个Pod配置文件，包含两个挂载相同卷的容器，名为`shared-volume`。容器在不同的路径上挂载它，但当`hue-global-listener`容器将文件写入`/notifications`时，`hue-job-scheduler`会在`/incoming`下看到该文件：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To use the shared memory option, we just need to add `medium: Memory` to the
    `emptyDir` section:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '要使用共享内存选项，我们只需在`emptyDir`部分添加`medium: Memory`：'
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that memory-based `emptyDir` counts toward the container’s memory limit.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，基于内存的`emptyDir`会计入容器的内存限制。
- en: 'To verify it worked, let’s create the pod and then write a file using one container
    and read it using the other container:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证是否有效，让我们创建Pod，然后使用一个容器写入文件，并使用另一个容器读取它：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that the pod has two containers:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Pod有两个容器：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we can create a file in the `/notifications` directory of the `hue-global-listener`
    container and list it in the `/incoming` directory of the `hue-job-scheduler`
    container:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在`hue-global-listener`容器的`/notifications`目录中创建文件，并在`hue-job-scheduler`容器的`/incoming`目录中列出它：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, we are able to see a file that was created in one container
    in the file system of another container; thereby, the containers can communicate
    via the shared file system.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们可以在一个容器的文件系统中看到另一个容器创建的文件；因此，容器可以通过共享的文件系统进行通信。
- en: Using HostPath for intra-node communication
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用HostPath进行节点间通信
- en: 'Sometimes, you want your pods to get access to some host information (for example,
    the Docker daemon) or you want pods on the same node to communicate with each
    other. This is useful if the pods know they are on the same host. Since Kubernetes
    schedules pods based on available resources, pods usually don’t know what other
    pods they share the node with. There are several cases where a pod can rely on
    other pods being scheduled with it on the same node:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你希望Pod能够访问一些主机信息（例如，Docker守护进程），或者你希望同一节点上的Pod能够相互通信。如果Pod知道它们位于同一主机上，这会很有用。由于Kubernetes根据可用资源调度Pod，Pod通常不知道它们与其他Pod共享同一节点。有几种情况，Pod可以依赖其他Pod与它们一起调度到同一节点：
- en: In a single-node cluster, all pods obviously share the same node
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单节点集群中，所有Pod显然共享同一个节点
- en: '`DaemonSet` pods always share a node with any other pod that matches their
    selector'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DaemonSet` Pod总是与任何匹配其选择器的Pod共享一个节点'
- en: Pods with required pod affinity are always scheduled together
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有所需Pod亲和性的Pod总是一起调度
- en: For example, in *Chapter 5*, *Using Kubernetes Resources in Practice*, we discussed
    a `DaemonSet` pod that serves as an aggregating proxy to other pods. Another way
    to implement this behavior is for the pods to simply write their data to a mounted
    volume that is bound to a host directory, and the `DaemonSet` pod can directly
    read it and act on it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在*第5章*，*实践中使用Kubernetes资源*中，我们讨论了一个`DaemonSet` Pod，它充当聚合代理，连接到其他Pod。实现这种行为的另一种方式是，Pod只需将其数据写入一个挂载的卷，该卷绑定到主机目录，`DaemonSet`
    Pod可以直接读取并进行操作。
- en: 'A `HostPath` volume is a host file or directory that is mounted into a pod.
    Before you decide to use the `HostPath` volume, make sure you understand the consequences:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`HostPath`卷是一个挂载到Pod中的主机文件或目录。在你决定使用`HostPath`卷之前，确保你理解其后果：'
- en: It is a security risk since access to the host filesystem can expose sensitive
    data (e.g. kubelet keys)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一种安全风险，因为访问主机文件系统可能暴露敏感数据（例如，kubelet密钥）
- en: The behavior of pods with the same configuration might be different if they
    are data-driven and the files on their host are different
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果Pod是数据驱动的，并且它们主机上的文件不同，那么具有相同配置的Pod的行为可能会有所不同
- en: It can violate resource-based scheduling because Kubernetes can’t monitor `HostPath`
    resources
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可能违反基于资源的调度，因为Kubernetes无法监控`HostPath`资源
- en: The containers that access host directories must have a security context with
    `privileged` set to `true` or, on the host side, you need to change the permissions
    to allow writing
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问主机目录的容器必须具有一个安全上下文，且`privileged`设置为`true`，或者在主机端，你需要更改权限以允许写入
- en: It’s difficult to coordinate disk usage across multiple pods on the same node.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在同一节点上协调多个Pod的磁盘使用是很困难的。
- en: You can easily run out of disk space
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你很容易用完磁盘空间
- en: 'Here is a configuration file that mounts the `/coupons` directory into the
    `hue-coupon-hunter` container, which is mapped to the host’s `/etc/hue/data/coupons`
    directory:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置文件，将`/coupons`目录挂载到`hue-coupon-hunter`容器中，该目录映射到主机的`/etc/hue/data/coupons`目录：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Since the pod doesn’t have a privileged security context, it will not be able
    to write to the host directory. Let’s change the container spec to enable it by
    adding a security context:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Pod没有特权安全上下文，它将无法写入主机目录。让我们通过添加安全上下文来更改容器规格，从而启用它：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the following diagram, you can see that each container has its own local
    storage area inaccessible to other containers or pods, and the host’s `/data`
    directory is mounted as a volume into both container 1 and container 2:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，你可以看到每个容器都有自己的本地存储区域，其他容器或Pod无法访问，而主机的`/data`目录作为卷挂载到容器1和容器2中：
- en: '![](img/B18998_06_01.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_06_01.png)'
- en: 'Figure 6.1: Container local storage'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：容器本地存储
- en: Using local volumes for durable node storage
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用本地卷进行持久化节点存储
- en: Local volumes are similar to `HostPath`, but they persist across pod restarts
    and node restarts. In that sense they are considered persistent volumes. They
    were added in Kubernetes 1.7\. As of Kubernetes 1.14 they are considered stable.
    The purpose of local volumes is to support Stateful Sets where specific pods need
    to be scheduled on nodes that contain specific storage volumes. Local volumes
    have node affinity annotations that simplify the binding of pods to the storage
    they need to access.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本地卷类似于`HostPath`，但它们在Pod重启和节点重启后依然存在。从这个意义上讲，它们被视为持久卷。它们是在Kubernetes 1.7中引入的。从Kubernetes
    1.14开始，它们被认为是稳定的。本地卷的目的是支持有状态集，其中特定的Pod需要调度到包含特定存储卷的节点上。当地卷有节点亲和性注解，可以简化Pod与它们需要访问的存储之间的绑定。
- en: 'We need to define a storage class for using local volumes. We will cover storage
    classes in depth later in this chapter. In one sentence, storage classes use a
    provisioner to allocate storage to pods. Let’s define the storage class in a file
    called `local-storage-class.yaml` and create it:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为使用本地卷定义一个存储类。我们将在本章稍后详细讨论存储类。简而言之，存储类使用配置器为 Pod 分配存储。让我们在一个名为`local-storage-class.yaml`的文件中定义该存储类并创建它：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we can create a persistent volume using the storage class that will persist
    even after the pod that’s using it is terminated:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用存储类创建一个持久化存储卷，即使使用它的 Pod 被终止后，存储卷仍会保持：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Provisioning persistent volumes
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提供持久化存储卷
- en: While `emptyDir` volumes can be mounted and used by containers, they are not
    persistent and don’t require any special provisioning because they use existing
    storage on the node. `HostPath` volumes persist on the original node, but if a
    pod is restarted on a different node, it can’t access the `HostPath` volume from
    its previous node. Local volumes are real persistent volumes that use storage
    provisioned ahead of time by administrators or dynamic provisioning via storage
    classes. They persist on the node and can survive pod restarts and rescheduling
    and even node restarts. Some persistent volumes use external storage (not a disk
    physically attached to the node) provisioned ahead of time by administrators.
    In cloud environments, the provisioning may be very streamlined, but it is still
    required, and as a Kubernetes cluster administrator you have to at least make
    sure your storage quota is adequate and monitor usage versus quota diligently.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`emptyDir`卷可以被容器挂载并使用，但它们不是持久的，不需要特殊配置，因为它们使用节点上现有的存储。`HostPath`卷在原节点上持久存在，但如果
    Pod 在不同节点上重新启动，它将无法访问先前节点上的`HostPath`卷。本地卷是实际的持久化存储卷，使用管理员预先配置的存储或通过存储类的动态配置来提供存储。它们在节点上持久存在，并且可以在
    Pod 重新启动、重新调度甚至节点重新启动时存活。一些持久化存储卷使用提前由管理员配置的外部存储（而非物理附加到节点的磁盘）。在云环境中，配置可能非常简化，但仍然是必需的，作为
    Kubernetes 集群管理员，你必须至少确保存储配额充足，并且认真监控配额的使用情况。
- en: Remember that persistent volumes are resources that the Kubernetes cluster is
    using, similar to nodes. As such they are not managed by the Kubernetes API server.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，持久化存储卷是 Kubernetes 集群正在使用的资源，类似于节点。因此，它们不由 Kubernetes API 服务器管理。
- en: You can provision resources statically or dynamically.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以静态或动态配置资源。
- en: Provisioning persistent volumes statically
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 静态配置持久化存储卷
- en: Static provisioning is straightforward. The cluster administrator creates persistent
    volumes backed up by some storage media ahead of time, and these persistent volumes
    can be claimed by containers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 静态配置非常简单。集群管理员预先创建由某些存储介质支持的持久化存储卷，这些存储卷可以被容器声明。
- en: Provisioning persistent volumes dynamically
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动态配置持久化存储卷
- en: Dynamic provisioning may happen when a persistent volume claim doesn’t match
    any of the statically provisioned persistent volumes. If the claim specified a
    storage class and the administrator configured that class for dynamic provisioning,
    then a persistent volume may be provisioned on the fly. We will see examples later
    when we discuss persistent volume claims and storage classes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当持久化存储卷声明与任何静态配置的持久化存储卷不匹配时，可能会发生动态配置。如果声明指定了存储类，并且管理员已为该存储类配置了动态配置，则可以实时配置持久化存储卷。稍后我们将在讨论持久化存储卷声明和存储类时看到示例。
- en: Provisioning persistent volumes externally
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 外部配置持久化存储卷
- en: 'Kubernetes originally contained a lot of code for storage provisioning “in-tree”
    as part of the main Kubernetes code base. With the introduction of CSI, storage
    provisioners started to migrate out of Kubernetes core into volume plugins (AKA
    out-of-tree). External provisioners work just like in-tree dynamic provisioners
    but can be deployed and updated independently. Most in-tree storage provisioners
    have been migrated out-of-tree. Check out this project for a library and guidelines
    for writing external storage provisioners: [https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner](https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 最初包含了大量用于存储配置的“内置”代码，作为 Kubernetes 主代码库的一部分。随着 CSI 的引入，存储提供者开始从 Kubernetes
    核心迁移到卷插件（即“外部”）。外部存储提供者的工作方式与内置动态存储提供者相同，但可以独立部署和更新。大多数内置存储提供者已迁移到外部。查看这个项目，获取编写外部存储提供者的库和指南：[https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner](https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner)。
- en: Creating persistent volumes
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建持久卷
- en: 'Here is the configuration file for an NFS persistent volume:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个 NFS 持久卷的配置文件：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'A persistent volume has a spec and metadata that possibly includes a storage
    class name. Let’s focus on the spec here. There are six sections: capacity, volume
    mode, access modes, reclaim policy, storage class, and the volume type (`nfs`
    in the example).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 持久卷有一个规范和元数据，可能包括存储类名称。我们在这里重点关注规范部分。它包含六个部分：容量、卷模式、访问模式、回收策略、存储类和卷类型（示例中为 `nfs`）。
- en: Capacity
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容量
- en: Each volume has a designated amount of storage. Storage claims may be satisfied
    by persistent volumes that have at least that amount of storage. In the example,
    the persistent volume has a capacity of 10 gibibytes (a single gibibyte is 2 to
    the power of 30 bytes).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 每个卷有指定的存储量。存储声明可以由至少具有该存储量的持久卷满足。在这个示例中，持久卷的容量为 10 吉比字节（单个吉比字节是 2 的 30 次方字节）。
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It is important when allocating static persistent volumes to understand the
    storage request patterns. For example, if you provision 20 persistent volumes
    with 100 GiB capacity and a container claims a persistent volume with 150 GiB,
    then this claim will not be satisfied even though there is enough capacity overall
    in the cluster.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在分配静态持久卷时，理解存储请求模式非常重要。例如，如果你配置了 20 个 100 GiB 容量的持久卷，并且某个容器声明了一个 150 GiB 的持久卷，则即使集群中总体容量足够，该声明也不会得到满足。
- en: Volume mode
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷模式
- en: The optional volume mode was added in Kubernetes 1.9 as an Alpha feature (moved
    to Beta in Kubernetes 1.13) for static provisioning. It lets you specify if you
    want a file system (`Filesystem`) or raw storage (`Block`). If you don’t specify
    volume mode, then the default is `Filesystem`, just like it was pre-1.9.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 可选的卷模式是在 Kubernetes 1.9 中作为 Alpha 功能添加的（在 Kubernetes 1.13 中移至 Beta），用于静态配置。它允许你指定是否需要文件系统（`Filesystem`）或原始存储（`Block`）。如果不指定卷模式，则默认使用
    `Filesystem`，就像 1.9 之前的版本一样。
- en: Access modes
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 访问模式
- en: 'There are three access modes:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种访问模式：
- en: '`ReadOnlyMany`: Can be mounted read-only by many nodes'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadOnlyMany`：可以由多个节点以只读方式挂载'
- en: '`ReadWriteOnce`: Can be mounted as read-write by a single node'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadWriteOnce`：只能由单个节点以读写方式挂载'
- en: '`ReadWriteMany`: Can be mounted as read-write by many nodes'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadWriteMany`：可以由多个节点以读写方式挂载'
- en: The storage is mounted to nodes, so even with `ReadWriteOnce`, multiple containers
    on the same node can mount the volume and write to it. If that causes a problem,
    you need to handle it through some other mechanism (for example, claim the volume
    only in DaemonSet pods that you know will have just one per node).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 存储被挂载到节点上，因此即使是`ReadWriteOnce`，同一节点上的多个容器也可以挂载该卷并写入数据。如果这导致问题，需要通过其他机制来处理（例如，仅在你知道每个节点只有一个的
    DaemonSet pod 中声明该卷）。
- en: 'Different storage providers support some subset of these modes. When you provision
    a persistent volume, you can specify which modes it will support. For example,
    NFS supports all modes, but in the example, only these modes were enabled:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的存储提供商支持这些模式的某些子集。当你配置持久卷时，可以指定它将支持哪些模式。例如，NFS 支持所有模式，但在这个示例中，仅启用了以下这些模式：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Reclaim policy
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回收策略
- en: 'The reclaim policy determines what happens when a persistent volume claim is
    deleted. There are three different policies:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 回收策略决定了当持久卷声明被删除时会发生什么。共有三种不同的策略：
- en: '`Retain` – the volume will need to be reclaimed manually'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Retain` – 卷需要手动回收'
- en: '`Delete` – the content, the volume, and the backing storage are removed'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Delete` – 内容、卷和后端存储将被删除'
- en: '`Recycle` – delete content only (`rm -rf /volume/*`)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Recycle` – 仅删除内容（`rm -rf /volume/*`）'
- en: The `Retain` and `Delete` policies mean the persistent volume is not available
    anymore for future claims. The `Recycle` policy allows the volume to be claimed
    again.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`Retain` 和 `Delete` 策略意味着持久卷将不再可用于未来的声明。`Recycle` 策略允许卷再次被声明。'
- en: At the moment, NFS and `HostPath` support the recycle policy, while AWS EBS,
    GCE PD, Azure disk, and Cinder volumes support the delete policy. Note that dynamically
    provisioned volumes are always deleted.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，NFS 和 `HostPath` 支持回收策略，而 AWS EBS、GCE PD、Azure 磁盘和 Cinder 卷支持删除策略。请注意，动态供应的卷始终会被删除。
- en: Storage class
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储类
- en: You can specify a storage class using the optional `storageClassName` field
    of the spec. If you do then only persistent volume claims that specify the same
    storage class can be bound to the persistent volume. If you don’t specify a storage
    class, then only PV claims that don’t specify a storage class can be bound to
    it.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过规格中的可选字段 `storageClassName` 来指定存储类。如果你指定了，那么只有指定相同存储类的持久卷声明才能与持久卷绑定。如果没有指定存储类，则只能绑定那些未指定存储类的
    PV 声明。
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Volume type
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷类型
- en: 'The volume type is specified by name in the spec. There is no `volumeType`
    stanza in the spec. In the preceding example, `nfs` is the volume type:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 卷类型在规格中通过名称指定。规格中没有 `volumeType` 部分。在上面的例子中，`nfs` 是卷类型：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Each volume type may have its own set of parameters. In this case, it’s a path
    and server.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 每种卷类型可能有自己的一组参数。在这种情况下，它是路径和服务器。
- en: We will go over various volume types later.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后将介绍各种卷类型。
- en: Mount options
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 挂载选项
- en: 'Some persistent volume types have additional mount options you can specify.
    The mount options are not validated. If you provide an invalid mount option, the
    volume provisioning will fail. For example, NFS supports additional mount options:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一些持久卷类型有额外的挂载选项，你可以指定。挂载选项不会进行验证。如果提供了无效的挂载选项，卷配置将失败。例如，NFS 支持额外的挂载选项：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now that we have looked at provisioning a single persistent volume, let’s look
    at projected volumes, which add more flexibility and abstraction of storage.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过了如何配置单个持久卷，让我们来看看投影卷，它提供了更多灵活性和存储抽象。
- en: Projected volumes
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投影卷
- en: Projected volumes allow you to mount multiple persistent volumes into the same
    directory. You need to be careful of naming conflicts of course.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 投影卷允许你将多个持久卷挂载到同一目录中。你当然需要小心命名冲突。
- en: 'The following volume types support projected volumes:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下卷类型支持投影卷：
- en: '`ConfigMap`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ConfigMap`'
- en: '`Secret`'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Secret`'
- en: '`SownwardAPI`'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SownwardAPI`'
- en: '`ServiceAccountToken`'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ServiceAccountToken`'
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The parameters for projected volumes are very similar to regular volumes. The
    exceptions are:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 投影卷的参数与常规卷非常相似。例外情况是：
- en: To maintain consistency with `ConfigMap` naming, the field `secretName` has
    been updated to `name` for secrets.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了与 `ConfigMap` 命名保持一致，`secretName` 字段已更新为 `name`，用于存储秘密。
- en: The `defaultMode` can only be set at the projected level and cannot be specified
    individually for each volume source (but you can specify the mode explicitly for
    each projection).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`defaultMode` 只能在投影级别设置，不能单独为每个卷源指定（但你可以为每个投影显式指定模式）。'
- en: Let’s look at a special kind of projected volume – the `serviceAccountToken`
    exceptions.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一种特殊的投影卷——`serviceAccountToken` 例外。
- en: serviceAccountToken projected volumes
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: serviceAccountToken 投影卷
- en: Kubernetes pods can access the Kubernetes API server using the permissions of
    the service account associated with the pod. `serviceAccountToken` projected volumes
    give you more granularity and control from a security standpoint. The token can
    have an expiration and a specific audience.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes pods 可以使用与 pod 关联的服务账户的权限访问 Kubernetes API 服务器。`serviceAccountToken`
    投影卷从安全角度为你提供更多粒度和控制。该令牌可以有过期时间和特定的受众。
- en: 'More details are available here: [https://kubernetes.io/docs/concepts/storage/projected-volumes/#serviceaccounttoken](https://kubernetes.io/docs/concepts/storage/projected-volumes/#serviceaccounttoken).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 更多详细信息请参见：[https://kubernetes.io/docs/concepts/storage/projected-volumes/#serviceaccounttoken](https://kubernetes.io/docs/concepts/storage/projected-volumes/#serviceaccounttoken)。
- en: Creating a local volume
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建本地卷
- en: Local volumes are static persistent disks that are allocated on a specific node.
    They are similar to `HostPath` volumes, but Kubernetes knows which node a local
    volume belongs to and will schedule pods that bind to that local volume always
    to that node. This means the pod will not be evicted and scheduled to another
    node where the data is not available.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本地卷是静态持久磁盘，分配到特定的节点上。它们类似于`HostPath`卷，但Kubernetes知道本地卷属于哪个节点，并且会将绑定到该本地卷的Pod调度到该节点上。这意味着Pod不会被驱逐，也不会调度到另一个数据不可用的节点上。
- en: 'Let’s create a local volume. First, we need to create a backing directory.
    For KinD and k3d clusters you can access the node through Docker:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个本地卷。首先，我们需要创建一个支持目录。对于KinD和k3d集群，你可以通过Docker访问节点：
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: For minikube you need to use `minikube ssh`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于minikube，你需要使用`minikube ssh`。
- en: 'Now, we can create a local volume backed by the `/mnt/disks/disk1` directory:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个由`/mnt/disks/disk1`目录支持的本地卷：
- en: '[PRE18]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is the `create` command:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`create`命令：
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Making persistent volume claims
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建持久卷声明
- en: 'When containers want access to some persistent storage they make a claim (or
    rather, the developer and cluster administrator coordinate on necessary storage
    resources to claim). Here is a sample claim that matches the persistent volume
    from the previous section - *Creating a local volume*:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当容器需要访问持久存储时，它们会发出声明（或者更确切地说，开发人员和集群管理员协调所需的存储资源来发出声明）。这里是一个匹配前面部分的持久卷的声明示例——*创建本地卷*：
- en: '[PRE20]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let’s create the claim and then explain what the different pieces do:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先创建声明，然后再解释各个部分的作用：
- en: '[PRE21]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The name `local-storage-claim` will be important later when mounting the claim
    into a container.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 名称`local-storage-claim`在稍后将会在挂载声明到容器时变得重要。
- en: The access mode in the spec is `ReadWriteOnce`, which means if the claim is
    satisfied no other claim with the `ReadWriteOnce` access mode can be satisfied,
    but claims for `ReadOnlyMany` can still be satisfied.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 规格中的访问模式是`ReadWriteOnce`，这意味着如果声明被满足，则不能满足其他任何具有`ReadWriteOnce`访问模式的声明，但可以满足`ReadOnlyMany`的声明。
- en: The resources section requests 8 GiB. This can be satisfied by our persistent
    volume, which has a capacity of 10 Gi. But, this is a little wasteful because
    2 Gi will not be used by definition.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 资源部分请求8 GiB。这可以由我们容量为10 Gi的持久卷满足。但这有些浪费，因为2 Gi会被定义为未使用。
- en: The storage class name is `local-storage`. As mentioned earlier it must match
    the class name of the persistent volume. However, with PVC there is a difference
    between an empty class name (`""`) and no class name at all. The former (an empty
    class name) matches persistent volumes with no storage class name. The latter
    (no class name) will be able to bind to persistent volumes only if the `DefaultStorageClass`
    admission plugin is turned on and the default storage class is used.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 存储类名称是`local-storage`。如前所述，它必须与持久卷的存储类名称匹配。然而，对于PVC，空的类名（`""`）和没有类名完全不同。前者（空类名）匹配没有存储类名称的持久卷。后者（没有类名）仅在启用了`DefaultStorageClass`入站插件并使用默认存储类时，才会绑定到持久卷。
- en: The selector section allows you to filter available volumes further. For example,
    here the volume must match the label `release:stable` and also have a label with
    either `capacity:8Gi` or `capacity:10Gi`. Imagine that we have several other volumes
    provisioned with capacities of 20 Gi and 50 Gi. We don’t want to claim a 50 Gi
    volume when we only need 8 Gi.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器部分允许你进一步筛选可用的卷。例如，这里卷必须匹配标签`release:stable`，并且还必须具有`capacity:8Gi`或`capacity:10Gi`标签。假设我们有一些其他卷，它们的容量分别为20
    Gi和50 Gi。我们不希望当只需要8 Gi时去声明一个50 Gi的卷。
- en: Kubernetes always tries to match the smallest volume that can satisfy a claim,
    but if there are no 8 Gi or 10 Gi volumes then the labels will prevent assigning
    a 20 Gi or 50 Gi volume and use dynamic provisioning instead.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes始终会尝试匹配能够满足声明的最小卷，但如果没有8 Gi或10 Gi的卷，标签将防止分配20 Gi或50 Gi的卷，并转而使用动态配置。
- en: It’s important to realize that claims don’t mention volumes by name. You can’t
    claim a specific volume. The matching is done by Kubernetes based on storage class,
    capacity, and labels.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要意识到声明不会通过名称提到卷。你不能声明一个特定的卷。匹配是由Kubernetes基于存储类、容量和标签来完成的。
- en: Finally, persistent volume claims belong to a namespace. Binding a persistent
    volume to a claim is exclusive. That means that a persistent volume will be bound
    to a namespace. Even if the access mode is `ReadOnlyMany` or `ReadWriteMany`,
    all the pods that mount the persistent volume claim must be from that claim’s
    namespace.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，持久卷声明属于一个命名空间。将持久卷绑定到声明是独占的。这意味着持久卷将绑定到一个命名空间。即使访问模式是`ReadOnlyMany`或`ReadWriteMany`，所有挂载该持久卷声明的
    Pods 必须来自该声明的命名空间。
- en: Mounting claims as volumes
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将声明挂载为卷
- en: 'OK. We have provisioned a volume and claimed it. It’s time to use the claimed
    storage in a container. This turns out to be pretty simple. First, the persistent
    volume claim must be used as a volume in the pod and then the containers in the
    pod can mount it, just like any other volume. Here is a pod manifest that specifies
    the persistent volume claim we created earlier (bound to the local persistent
    volume we provisioned):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们已经配置了一个卷并声明了它。现在是时候在容器中使用已声明的存储了。这实际上很简单。首先，持久卷声明必须作为 Pod 中的卷使用，然后 Pod
    中的容器可以像任何其他卷一样挂载它。以下是一个 Pod 清单，指定了我们之前创建的持久卷声明（绑定到我们提供的本地持久卷）：
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The key is in the `persistentVolumeClaim` section under volumes. The claim name
    (`local-storage-claim` here) uniquely identifies within the current namespace
    the specific claim and makes it available as a volume (named `persistent-volume`
    here). Then, the container can refer to it by its name and mount it to `"/mnt/data"`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于`persistentVolumeClaim`部分中的卷。声明名称（此处为`local-storage-claim`）在当前命名空间内唯一标识特定声明，并将其作为卷（此处命名为`persistent-volume`）提供。然后，容器可以通过名称引用它并将其挂载到`"/mnt/data"`。
- en: 'Before we create the pod it’s important to note that the persistent volume
    claim didn’t actually claim any storage yet and wasn’t bound to our local volume.
    The claim is pending until some container actually attempts to mount a volume
    using the claim:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建 Pod 之前，重要的是要注意持久卷声明实际上尚未声明任何存储，也未与我们的本地卷绑定。声明处于挂起状态，直到某个容器尝试使用该声明挂载卷：
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, the claim will be bound when creating the pod:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在创建 Pod 时，声明将会被绑定：
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Raw block volumes
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始块卷
- en: Kubernetes 1.9 added this capability as an Alpha feature. Kubernetes 1.13 moved
    it to Beta. Since Kubernetes 1.18 it is GA.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 1.9 将此功能作为 Alpha 特性加入。Kubernetes 1.13 将其移至 Beta。从 Kubernetes 1.18
    开始，它已成为 GA。
- en: 'Raw block volumes provide direct access to the underlying storage, which is
    not mediated via a file system abstraction. This is very useful for applications
    that require high-performance storage like databases or when consistent I/O performance
    and low latency are needed. The following storage providers support raw block
    volumes:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 原始块卷提供对底层存储的直接访问，绕过了文件系统抽象。这对于需要高性能存储的应用程序（如数据库）非常有用，或者在需要一致的 I/O 性能和低延迟时尤为重要。以下存储提供者支持原始块卷：
- en: AWSElasticBlockStore
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWSElasticBlockStore
- en: AzureDisk
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AzureDisk
- en: '**FC** (**Fibre Channel**)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FC** (**光纤通道**)'
- en: GCE Persistent Disk
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCE 持久磁盘
- en: iSCSI
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iSCSI
- en: Local volume
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地卷
- en: OpenStack Cinder
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack Cinder
- en: RBD (Ceph Block Device)
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBD（Ceph 块设备）
- en: VsphereVolume
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VsphereVolume
- en: 'In addition many CSI storage providers also offer raw block volume. For the
    full list check out: [https://kubernetes-csi.github.io/docs/drivers.html](https://kubernetes-csi.github.io/docs/drivers.html).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多 CSI 存储提供商也提供原始块卷。完整的列表请查看：[https://kubernetes-csi.github.io/docs/drivers.html](https://kubernetes-csi.github.io/docs/drivers.html)。
- en: 'Here is how to define a raw block volume using the FireChannel provider:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用 FireChannel 提供程序定义原始块卷：
- en: '[PRE25]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'A matching **Persistent Volume Claim** (**PVC**) MUST specify `volumeMode:
    Block` as well. Here is what it looks like:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '匹配的**持久卷声明** (**PVC**) 也必须指定`volumeMode: Block`。以下是其格式：'
- en: '[PRE26]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Pods consume raw block volumes as devices under `/dev` and NOT as mounted filesystems.
    Containers can then access these devices and read/write to them. In practice this
    means that I/O requests to block storage go directly to the underlying block storage
    and don’t pass through the file system drivers. This is in theory faster, but
    in practice it can actually decrease performance if your application benefits
    from file system buffering.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 将原始块卷作为`/dev`下的设备使用，而不是作为挂载的文件系统。容器可以访问这些设备并进行读写。实际上，这意味着对块存储的 I/O 请求会直接传递到底层的块存储，而不是经过文件系统驱动程序。这理论上更快，但实际上，如果你的应用程序依赖于文件系统缓冲，可能会导致性能下降。
- en: 'Here is a pod with a container that binds the `block-pvc` with the raw block
    storage as a device named `/dev/xdva`:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含容器的 Pod，它将`block-pvc`与名为`/dev/xdva`的原始块存储绑定：
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: CSI ephemeral volumes
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will cover the **Container Storage Interface** (**CSI**) in detail later
    in the chapter in the section *The Container Storage Interface*. CSI ephemeral
    volumes are backed by local storage on the node. These volumes’ lifecycles are
    tied to the pod’s lifecycle. In addition, they can only be mounted by containers
    of that pod, which is useful for populating secrets and certificates directly
    into a pod, without going through a Kubernetes secret object.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a pod with a CSI ephemeral volume:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'CSI ephemeral volumes have been GA since Kubernetes 1.25\. However, they may
    not be supported by all CSI drivers. As usual check the list: [https://kubernetes-csi.github.io/docs/drivers.html](https://kubernetes-csi.github.io/docs/drivers.html).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Generic ephemeral volumes
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generic ephemeral volumes are yet another volume type that is tied to the pod
    lifecycle. When the pod is gone the generic ephemeral volume is gone.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'This volume type actually creates a full-fledged persistent volume claim. This
    provides several capabilities:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The storage for the volume can be either local or network-attached.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The volume has the option to be provisioned with a fixed size.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the driver and specified parameters, the volume may contain initial
    data.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If supported by the driver, typical operations such as snapshotting, cloning,
    resizing, and storage capacity tracking can be performed on the volumes.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of a pod with a generic ephemeral volume:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note that from a security point of view users that have permission to create
    pods, but not PVCs, can now create PVCs via generic ephemeral volumes. To prevent
    that it is possible to use admission control.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Storage classes
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve run into storage classes already. What are they exactly? Storage classes
    let an administrator configure a cluster with custom persistent storage (as long
    as there is a proper plugin to support it). A storage class has a name in the
    metadata (it must be specified in the `storageClassName` file of the claim), a
    provisioner, a reclaim policy, and parameters.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'We declared a storage class for local storage earlier. Here is a sample storage
    class that uses AWS EBS as a provisioner (so, it works only on AWS):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: You may create multiple storage classes for the same provisioner with different
    parameters. Each provisioner has its own parameters.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'The currently supported provisioners are:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: AWSElasticBlockStore
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AzureFile
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AzureDisk
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CephFS
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cinder
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FC
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FlexVolume
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flocker
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GCE Persistent Disk
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GlusterFS
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: iSCSI
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quobyte
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NFS
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RBD
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VsphereVolume
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PortworxVolume
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ScaleIO
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StorageOS
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This list doesn’t contain provisioners for other volume types, such as `configMap`
    or `secret`, that are not backed by your typical network storage. Those volume
    types don’t require a storage class. Utilizing volume types intelligently is a
    major part of architecting and managing your cluster.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Default storage class
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The cluster administrator can also assign a default storage class. When a default
    storage class is assigned and the `DefaultStorageClass` admission plugin is turned
    on, then claims with no storage class will be dynamically provisioned using the
    default storage class. If the default storage class is not defined or the admission
    plugin is not turned on, then claims with no storage class can only match volumes
    with no storage class.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理员还可以分配默认存储类。当分配了默认存储类并且启用了 `DefaultStorageClass` 入场插件时，未指定存储类的声明将使用默认存储类动态配置。如果没有定义默认存储类或未启用入场插件，则未指定存储类的声明只能匹配没有存储类的卷。
- en: We covered a lot of ground and a lot of options for provisioning storage and
    using it in different ways. Let’s put everything together and show the whole process
    from start to finish.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了很多内容，并探讨了多种配置存储和以不同方式使用存储的选项。现在，我们将把所有内容整合起来，展示从头到尾的整个过程。
- en: Demonstrating persistent volume storage end to end
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演示持久卷存储的端到端过程
- en: To illustrate all the concepts, let’s do a mini demonstration where we create
    a `HostPath` volume, claim it, mount it, and have containers write to it. We will
    use k3d for this part.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明所有概念，让我们做一个小型演示，创建一个 `HostPath` 卷，声明它，挂载它，并让容器写入其中。我们将使用 k3d 来完成这部分。
- en: 'Let’s start by creating a `hostPath` volume using the `dir` storage class.
    Save the following in `dir-persistent-volume.yaml`:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过使用 `dir` 存储类创建一个 `hostPath` 卷。将以下内容保存为 `dir-persistent-volume.yaml`：
- en: '[PRE31]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, let’s create it:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们来创建它：
- en: '[PRE32]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'To check out the available volumes, you can use the resource type `persistentvolumes`
    or `pv` for short:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看可用的卷，可以使用资源类型 `persistentvolumes`，简写为 `pv`：
- en: '[PRE33]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The capacity is 1 GiB as requested. The reclaim policy is `Retain` because
    host path volumes are retained (not destroyed). The status is `Available` because
    the volume has not been claimed yet. The access mode is specified as `RWX`, which
    means `ReadWriteMany`. All of the access modes have a shorthand version:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 容量为请求的 1 GiB。回收策略是 `Retain`，因为主机路径卷会被保留（不会销毁）。状态为 `Available`，因为该卷尚未被声明。访问模式指定为
    `RWX`，即 `ReadWriteMany`。所有访问模式都有一个简写版本：
- en: '`RWO` – `ReadWriteOnce`'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RWO` – `ReadWriteOnce`'
- en: '`ROX` – `ReadOnlyMany`'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ROX` – `ReadOnlyMany`'
- en: '`RWX` – `ReadWriteMany`'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RWX` – `ReadWriteMany`'
- en: 'We have a persistent volume. Let’s create a claim. Save the following to `dir-persistent-volume-claim.yaml`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个持久卷。我们来创建一个声明。将以下内容保存为 `dir-persistent-volume-claim.yaml`：
- en: '[PRE34]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then, run the following command:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行以下命令：
- en: '[PRE35]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s check the claim and the volume:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来检查一下声明和卷：
- en: '[PRE36]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'As you can see, the claim and the volume are bound to each other and reference
    each other. The reason the binding works is that the same storage class is used
    by the volume and the claim. But, what happens if they don’t match? Let’s remove
    the storage class from the persistent volume claim and see what happens. Save
    the following persistent volume claim to `some-persistent-volume-claim.yaml`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，声明和卷已经绑定在一起并互相引用。绑定之所以有效，是因为卷和声明使用了相同的存储类。但是，如果它们不匹配会怎样呢？我们来删除持久卷声明中的存储类，看看会发生什么。将以下持久卷声明保存为
    `some-persistent-volume-claim.yaml`：
- en: '[PRE37]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then, create it:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，创建它：
- en: '[PRE38]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Ok. It was created. Let’s check it out:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，已经创建了。我们来看一下：
- en: '[PRE39]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Very interesting. The `some-pvc` claim was associated with the `local-path`
    storage class that we never specified, but it is still pending. Let’s understand
    why.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 非常有趣。`some-pvc` 声明与我们从未指定的 `local-path` 存储类关联，但它仍然是待处理状态。我们来了解一下原因。
- en: 'Here is the `local-path` storage class:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `local-path` 存储类：
- en: '[PRE40]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: It is a storage class that comes with k3d (k3s).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个附带 k3d（k3s）的存储类。
- en: 'Note the annotation: `storageclass.kubernetes.io/is-default-class: "true"`.
    It tells Kubernetes that this is the default storage class. Since our PVC had
    no storage class name it was associated with the default storage class. But, why
    is the claim still pending? The reason is that `volumeBindingMode` is `WaitForFirstConsumer`.
    This means that the volume for the claim will be provisioned dynamically only
    when a container attempts to mount the volume via the claim.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '注意注释：`storageclass.kubernetes.io/is-default-class: "true"`。它告诉 Kubernetes 这是默认的存储类。由于我们的
    PVC 没有指定存储类名称，因此它与默认存储类关联。但是，为什么声明仍然是待处理状态？原因是 `volumeBindingMode` 为 `WaitForFirstConsumer`。这意味着只有当容器尝试通过声明挂载卷时，声明的卷才会动态配置。'
- en: 'Back to our `dir-pvc`. The final step is to create a pod with two containers
    and assign the claim as a volume to both of them. Save the following to `shell-pod.yaml`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的 `dir-pvc`。最后一步是创建一个具有两个容器的 Pod，并将声明作为卷分配给它们两个。将以下内容保存到 `shell-pod.yaml`：
- en: '[PRE41]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This pod has two containers that use the `g1g1/py-kube:0.3` image and both just
    sleep for a long time. The idea is that the containers will keep running, so we
    can connect to them later and check their file system. The pod mounts our persistent
    volume claim with a volume name of `pv`. Note that the volume specification is
    done at the pod level just once and multiple containers can mount it into different
    directories.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Pod 有两个容器，使用 `g1g1/py-kube:0.3` 镜像，两个容器都只是长时间休眠。目的是让容器保持运行状态，这样我们可以稍后连接它们并检查它们的文件系统。该
    Pod 挂载了我们的持久卷声明，卷的名称为 `pv`。请注意，卷的规范仅在 Pod 层面定义一次，多个容器可以将其挂载到不同的目录中。
- en: 'Let’s create the pod and verify that both containers are running:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建 Pod 并验证两个容器是否都在运行：
- en: '[PRE42]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then, connect to the node (`k3d-k3s-default-agent-1`). This is the host whose
    `/tmp/data` is the pod’s volume that is mounted as `/data` and `/another-data`
    into each of the running containers:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，连接到节点（`k3d-k3s-default-agent-1`）。这是主机，其 `/tmp/data` 是 Pod 的卷，并挂载为 `/data`
    和 `/another-data` 进入每个运行中的容器：
- en: '[PRE43]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Then, let’s create a file in the `/tmp/data` directory on the host. It should
    be visible by both containers via the mounted volume:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在主机的 `/tmp/data` 目录下创建一个文件。该文件应该通过挂载卷在两个容器之间都能看到：
- en: '[PRE44]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let’s verify from the outside that the file `cool.txt` is indeed available:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从外部验证文件 `cool.txt` 是否确实可用：
- en: '[PRE45]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, let’s verify the file is available in the containers (in their mapped
    directories):'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们验证文件在容器中是否可用（在它们映射的目录中）：
- en: '[PRE46]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We can even create a new file, `yo.txt`, in one of the containers and see that
    it’s available to the other container or to the node itself:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以在其中一个容器中创建一个新文件 `yo.txt`，并看到它在另一个容器或节点本身上也可用：
- en: '[PRE47]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Yes. Everything works as expected and both containers share the same storage.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，一切如预期般工作，两个容器共享相同的存储。
- en: Public cloud storage volume types – GCE, AWS, and Azure
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公有云存储卷类型 – GCE、AWS 和 Azure
- en: In this section, we’ll look at some of the common volume types available in
    the leading public cloud platforms. Managing storage at scale is a difficult task
    that eventually involves physical resources, similar to nodes. If you choose to
    run your Kubernetes cluster on a public cloud platform, you can let your cloud
    provider deal with all these challenges and focus on your system. But it’s important
    to understand the various options, constraints, and limitations of each volume
    type.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些领先公有云平台中常见的卷类型。在大规模管理存储是一个复杂的任务，最终涉及到物理资源，类似于节点。如果您选择在公有云平台上运行 Kubernetes
    集群，您可以让云提供商处理所有这些挑战，专注于您的系统。但理解每种卷类型的不同选项、约束和限制是非常重要的。
- en: Many of the volume types we will go over used to be handled by in-tree plugins
    (part of core Kubernetes), but have now migrated to out-of-tree CSI plugins.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讲解的许多卷类型以前是由内置插件（Kubernetes 核心部分）处理的，但现在已经迁移到外部 CSI 插件。
- en: The CSI migration feature allows in-tree plugins that have corresponding out-of-tree
    CSI plugins to direct operations toward the out-of-tree plugins as a transitioning
    measure.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: CSI 迁移功能允许将具有相应外部 CSI 插件的内置插件指向外部插件，以作为过渡措施。
- en: We will cover the CSI itself later.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会讲解 CSI 本身。
- en: AWS Elastic Block Store (EBS)
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS 弹性块存储（EBS）
- en: 'AWS provides the **Elastic Block Store** (**EBS**) as persistent storage for
    EC2 instances. An AWS Kubernetes cluster can use AWS EBS as persistent storage
    with the following limitations:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供了 **弹性块存储**（**EBS**）作为 EC2 实例的持久存储。AWS Kubernetes 集群可以使用 AWS EBS 作为持久存储，但有以下限制：
- en: The pods must run on AWS EC2 instances as nodes
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 必须在 AWS EC2 实例上运行作为节点
- en: Pods can only access EBS volumes provisioned in their availability zone
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 只能访问在其可用区内配置的 EBS 卷
- en: An EBS volume can be mounted on a single EC2 instance
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 EBS 卷可以挂载到单个 EC2 实例上
- en: Those are severe limitations. The restriction for a single availability zone,
    while great for performance, eliminates the ability to share storage at scale
    or across a geographically distributed system without custom replication and synchronization.
    The limit of a single EBS volume to a single EC2 instance means even within the
    same availability zone, pods can’t share storage (even for reading) unless you
    make sure they run on the same node.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是严重的限制。单一可用区的限制虽然对性能有好处，但却剥夺了在大规模或跨地理分布系统中共享存储的能力，除非进行自定义复制和同步。单个 EBS 卷只能绑定一个
    EC2 实例的限制意味着，即使在同一个可用区内，pod 也不能共享存储（即使是只读），除非确保它们运行在同一个节点上。
- en: This is an example of an in-tree plugin that also has a CSI driver and supports
    CSIMigration. That means that if the CSI driver for AWS EBS (`ebs.csi.aws.com`)
    is installed, then the in-tree plugin will redirect all plugin operations to the
    out-of-tree plugin.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个内置插件的示例，它也有 CSI 驱动并支持 CSIMigration。这意味着，如果安装了 AWS EBS 的 CSI 驱动（`ebs.csi.aws.com`），则内置插件会将所有插件操作重定向到外部插件。
- en: It is also possible to disable loading the in-tree `awsElasticBlockStore` storage
    plugin from being loaded by setting the `InTreePluginAWSUnregister` feature gate
    to `true` (the default is `false`).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过将 `InTreePluginAWSUnregister` 功能开关设置为 `true`（默认为 `false`）来禁用加载内置的 `awsElasticBlockStore`
    存储插件。
- en: 'Check out all the feature gates here: [https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 查看所有功能开关：[https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/)。
- en: 'Let’s see how to define an AWS EBS persistent volume (static provisioning):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何定义一个 AWS EBS 持久卷（静态配置）：
- en: '[PRE48]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Then you need to define a PVC:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你需要定义一个 PVC：
- en: '[PRE49]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Finally, a pod can mount the PVC:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，pod 可以挂载 PVC：
- en: '[PRE50]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: AWS Elastic File System (EFS)
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS 弹性文件系统（EFS）
- en: 'AWS has a service called the **Elastic File System** (**EFS**). This is really
    a managed NFS service. It uses the NFS 4.1 protocol and has many benefits over
    EBS:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供了一项名为 **Elastic File System** (**EFS**) 的服务。这实际上是一个托管的 NFS 服务。它使用 NFS
    4.1 协议，并且相较于 EBS 具有许多优点：
- en: Multiple EC2 instances can access the same files across multiple availability
    zones (but within the same region)
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个 EC2 实例可以在多个可用区之间访问相同的文件（但必须在同一区域内）
- en: Capacity is automatically scaled up and down based on actual usage
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容量会根据实际使用情况自动扩展和缩减
- en: You pay only for what you use
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你只需为你使用的部分付费
- en: You can connect on-premise servers to EFS over VPN
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过 VPN 将本地服务器连接到 EFS
- en: EFS runs off SSD drives that are automatically replicated across availability
    zones
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EFS 运行在 SSD 硬盘上，这些硬盘会自动在可用区之间进行复制
- en: 'That said, EFS is more expansive than EBS even when you consider the automatic
    replication to multiple AZs (assuming you fully utilize your EBS volumes). The
    recommended way to use EFS via its dedicated CSI driver: [https://github.com/kubernetes-sigs/aws-efs-csi-driver](https://github.com/kubernetes-sigs/aws-efs-csi-driver).'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，EFS 比 EBS 更为广泛，即使考虑到自动复制到多个可用区（假设你充分利用了 EBS 卷）。推荐的使用 EFS 的方式是通过它专用的 CSI
    驱动：[https://github.com/kubernetes-sigs/aws-efs-csi-driver](https://github.com/kubernetes-sigs/aws-efs-csi-driver)。
- en: 'Here is an example of static provisioning. First, define the persistent volume:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这是静态配置的示例。首先，定义持久卷：
- en: '[PRE51]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You can find the `Filesystem Id` using the AWS CLI:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 AWS CLI 查找 `Filesystem Id`：
- en: '[PRE52]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Then define a PVC:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 然后定义一个 PVC：
- en: '[PRE53]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Here is a pod that consumes it:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用 EFS 的 pod：
- en: '[PRE54]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'You can also use dynamic provisioning by defining a proper storage class instead
    of creating a static volume:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过定义一个合适的存储类来进行动态配置，而不是创建静态卷：
- en: '[PRE55]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The PVC is similar, but now uses the storage class name:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: PVC 类似，但现在使用了存储类名称：
- en: '[PRE56]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The pod consumes the PVC just like before:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: pod 像之前一样使用 PVC：
- en: '[PRE57]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: GCE persistent disk
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GCE 持久化磁盘
- en: The `gcePersistentDisk` volume type is very similar to `awsElasticBlockStore`.
    You must provision the disk ahead of time. It can only be used by GCE instances
    in the same project and zone. But the same volume can be used as read-only on
    multiple instances. This means it supports `ReadWriteOnce` and `ReadOnlyMany`.
    You can use a GCE persistent disk to share data as read-only between multiple
    pods in the same zone.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '`gcePersistentDisk` 卷类型与 `awsElasticBlockStore` 非常相似。你必须提前配置磁盘。它只能被同一项目和可用区内的
    GCE 实例使用。但同一个磁盘可以在多个实例上作为只读使用。这意味着它支持 `ReadWriteOnce` 和 `ReadOnlyMany`。你可以使用 GCE
    持久化磁盘在同一可用区内的多个 pod 之间以只读方式共享数据。'
- en: It also has a CSI driver called `pd.csi.storage.gke.io` and supports CSIMigration.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 它还具有一个名为 `pd.csi.storage.gke.io` 的 CSI 驱动程序，并支持 CSIMigration。
- en: If the pod that’s using a persistent disk in `ReadWriteOnce` mode is controlled
    by a replication controller, a replica set, or a deployment, the replica count
    must be 0 or 1\. Trying to scale beyond 1 will fail for obvious reasons.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用 `ReadWriteOnce` 模式的持久磁盘所在的 Pod 由复制控制器、复制集或部署控制，副本数量必须为 0 或 1。尝试扩展到超过 1
    将因显而易见的原因而失败。
- en: 'Here is a storage class for GCE persistent disk using the CSI driver:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用 CSI 驱动程序的 GCE 持久磁盘存储类：
- en: '[PRE58]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Here is the PVC:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 PVC 示例：
- en: '[PRE59]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'A Pod can consume it for dynamic provisioning:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 可以使用它进行动态供给：
- en: '[PRE60]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The GCE persistent disk has supported a regional disk option since Kubernetes
    1.10 (in Beta). Regional persistent disks automatically sync between two zones.
    Here is what the storage class looks like for a regional persistent disk:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: GCE 持久磁盘自 Kubernetes 1.10（Beta 版本）起支持区域磁盘选项。区域持久磁盘在两个区域之间自动同步。以下是区域持久磁盘的存储类示例：
- en: '[PRE61]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Google Cloud Filestore
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google Cloud Filestore
- en: Google Cloud Filestore is the managed NFS file service of GCP. Kubernetes doesn’t
    have an in-tree plugin for it and there is no general-purpose supported CSI driver.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Filestore 是 GCP 的托管 NFS 文件服务。Kubernetes 没有内置的插件来支持它，也没有通用的 CSI
    驱动程序。
- en: However, there is a CSI driver used on GKE and if you are adventurous, you may
    want to try it even if you’re installing Kubernetes yourself on GCP and want to
    use Google Cloud Storage as a storage option.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GKE 上使用了一个 CSI 驱动程序，如果你愿意尝试，即使你自己在 GCP 上安装 Kubernetes 并希望将 Google Cloud Storage
    作为存储选项，也可以尝试使用它。
- en: 'See: [https://github.com/kubernetes-sigs/gcp-filestore-csi-driver](https://github.com/kubernetes-sigs/gcp-filestore-csi-driver).'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见：[https://github.com/kubernetes-sigs/gcp-filestore-csi-driver](https://github.com/kubernetes-sigs/gcp-filestore-csi-driver)。
- en: Azure data disk
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure 数据磁盘
- en: The Azure data disk is a virtual hard disk stored in Azure storage. It’s similar
    in capabilities to AWS EBS or a GCE persistent disk.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 数据磁盘是存储在 Azure 存储中的虚拟硬盘。它的功能与 AWS EBS 或 GCE 持久磁盘类似。
- en: 'It also has a CSI driver called `disk.csi.azure.com` and supports CSIMigration.
    See: [https://github.com/kubernetes-sigs/azuredisk-csi-driver](https://github.com/kubernetes-sigs/azuredisk-csi-driver).'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 它还有一个名为 `disk.csi.azure.com` 的 CSI 驱动程序，并支持 CSIMigration。请参见：[https://github.com/kubernetes-sigs/azuredisk-csi-driver](https://github.com/kubernetes-sigs/azuredisk-csi-driver)。
- en: 'Here is an example of defining an Azure disk persistent volume:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是定义 Azure 磁盘持久化卷的示例：
- en: '[PRE62]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'In addition to the mandatory `diskName` and `diskURI` parameters, it also has
    a few optional parameters:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 除了必需的 `diskName` 和 `diskURI` 参数外，它还具有一些可选参数：
- en: '`kind`: The available options for disk storage configurations are `Shared`
    (allowing multiple disks per storage account), `Dedicated` (providing a single
    blob disk per storage account), or `Managed` (offering an Azure-managed data disk).
    The default is `Shared`.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kind`：磁盘存储配置的可用选项有 `Shared`（允许每个存储帐户多个磁盘）、`Dedicated`（每个存储帐户提供一个独立的 Blob 磁盘）或
    `Managed`（提供 Azure 管理的数据磁盘）。默认值是 `Shared`。'
- en: '`cachingMode`: The disk caching mode. This must be one of `None`, `ReadOnly`,
    or `ReadWrite`. The default is `None`.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cachingMode`：磁盘缓存模式。必须是 `None`、`ReadOnly` 或 `ReadWrite` 之一。默认值是 `None`。'
- en: '`fsType`: The filesystem type set to mount. The default is `ext4`.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsType`：设置挂载的文件系统类型。默认值是 `ext4`。'
- en: '`readOnly`: Whether the filesystem is used as `readOnly`. The default is `false`.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readOnly`：文件系统是否作为 `readOnly` 使用。默认值是 `false`。'
- en: Azure data disks are limited to 32 GiB. Each Azure VM can have up to 32 data
    disks. Larger VM sizes can have more disks attached. You can attach an Azure data
    disk to a single Azure VM.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 数据磁盘的最大容量为 32 GiB。每个 Azure 虚拟机可以有最多 32 个数据磁盘。更大的虚拟机规格可以连接更多的磁盘。你可以将 Azure
    数据磁盘附加到单个 Azure 虚拟机上。
- en: As usual you should create a PVC and consume it in a pod (or a pod controller).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你应该创建一个 PVC 并在 Pod（或 Pod 控制器）中使用它。
- en: Azure file
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure 文件
- en: In addition to the data disk, Azure has also a shared filesystem similar to
    AWS EFS. However, Azure file storage uses the SMB/CIFS protocol (it supports SMB
    2.1 and SMB 3.0). It is based on the Azure storage platform and has the same availability,
    durability, scalability, and geo-redundancy capabilities as Azure Blob, Table,
    or Queue storage.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据磁盘外，Azure 还提供了一个类似于 AWS EFS 的共享文件系统。不过，Azure 文件存储使用 SMB/CIFS 协议（它支持 SMB
    2.1 和 SMB 3.0）。它基于 Azure 存储平台，并具备与 Azure Blob、Table 或 Queue 存储相同的可用性、耐久性、可扩展性和地理冗余能力。
- en: 'In order to use Azure file storage, you need to install on each client VM the
    `cifs-utils` package. You also need to create a secret, which is a required parameter:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 Azure 文件存储，您需要在每个客户端虚拟机上安装`cifs-utils`包。您还需要创建一个机密，这是一个必需的参数：
- en: '[PRE63]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Here is a pod that uses Azure file storage:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用 Azure 文件存储的 Pod：
- en: '[PRE64]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Azure file storage supports sharing within the same region as well as connecting
    on-premise clients.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 文件存储支持在同一区域内共享，并连接本地客户端。
- en: This covers the public cloud storage volume types. Let’s look at some distributed
    storage volumes you can install on your own in your cluster.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容涵盖了公共云存储卷类型。接下来，我们来看一些您可以在集群中自己安装的分布式存储卷。
- en: GlusterFS and Ceph volumes in Kubernetes
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 中的 GlusterFS 和 Ceph 卷
- en: 'GlusterFS and Ceph are two distributed persistent storage systems. GlusterFS
    is, at its core, a network filesystem. Ceph is, at its core, an object store.
    Both expose block, object, and filesystem interfaces. Both use the `xfs` filesystem
    under the hood to store the data and metadata as `xattr` attributes. There are
    several reasons why you may want to use GlusterFS or Ceph as persistent volumes
    in your Kubernetes cluster:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS 和 Ceph 是两种分布式持久存储系统。GlusterFS 本质上是一个网络文件系统，而 Ceph 本质上是一个对象存储。两者都暴露块、对象和文件系统接口。两者在底层都使用
    `xfs` 文件系统来存储数据和元数据作为 `xattr` 属性。您可能希望在 Kubernetes 集群中使用 GlusterFS 或 Ceph 作为持久卷的几个原因包括：
- en: You run on-premises and cloud storage is not available
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您运行的是本地存储，云存储不可用
- en: You may have a lot of data and applications that access the data in GlusterFS
    or Ceph
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能有大量数据和应用程序需要访问 GlusterFS 或 Ceph 中的数据
- en: You have operational expertise managing GlusterFS or Ceph
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您具备管理 GlusterFS 或 Ceph 的操作经验
- en: You run in the cloud, but the limitations of the cloud platform persistent storage
    are a non-starter
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您运行在云端，但云平台持久存储的限制使其不可行
- en: Let’s take a closer look at GlusterFS.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解一下 GlusterFS。
- en: Using GlusterFS
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 GlusterFS
- en: 'GlusterFS is intentionally simple, exposing the underlying directories as they
    are and leaving it to clients (or middleware) to handle high availability, replication,
    and distribution. GlusterFS organizes the data into logical volumes, which encompass
    multiple nodes (machines) that contain bricks, which store files. Files are allocated
    to bricks according to DHT (distributed hash table). If files are renamed or the
    GlusterFS cluster is expanded or rebalanced, files may be moved between bricks.
    The following diagram shows the GlusterFS building blocks:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS 故意保持简单，将底层目录暴露出来，交由客户端（或中间件）处理高可用性、复制和分发。GlusterFS 将数据组织成逻辑卷，这些逻辑卷涵盖了包含砖块的多个节点（机器），砖块用于存储文件。文件根据
    DHT（分布式哈希表）分配到砖块。如果文件被重命名，或者 GlusterFS 集群被扩展或重新平衡，文件可能会在砖块之间移动。以下图展示了 GlusterFS
    的构建块：
- en: '![](img/B18998_06_02.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_06_02.png)'
- en: 'Figure 6.2: GlusterFS building blocks'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：GlusterFS 构建块
- en: To use a GlusterFS cluster as persistent storage for Kubernetes (assuming you
    have an up-and-running GlusterFS cluster), you need to follow several steps. In
    particular, the GlusterFS nodes are managed by the plugin as a Kubernetes service.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 GlusterFS 集群用作 Kubernetes 的持久存储（假设您有一个运行中的 GlusterFS 集群），您需要按照几个步骤进行操作。特别是，GlusterFS
    节点由插件作为 Kubernetes 服务管理。
- en: Creating endpoints
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建端点
- en: 'Here is an example of an endpoints resource that you can create as a normal
    Kubernetes resource using `kubectl create`:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您可以通过 `kubectl create` 创建的端点资源示例，作为普通的 Kubernetes 资源：
- en: '[PRE65]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Adding a GlusterFS Kubernetes service
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加一个 GlusterFS Kubernetes 服务
- en: 'To make the endpoints persistent, you use a Kubernetes service with no selector
    to indicate the endpoints are managed manually:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使端点保持持久性，您可以使用一个没有选择器的 Kubernetes 服务，表示这些端点是手动管理的：
- en: '[PRE66]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Creating pods
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 Pods
- en: 'Finally, in the pod spec’s `volumes` section, provide the following information:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在 Pod 规格的`volumes`部分，提供以下信息：
- en: '[PRE67]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The containers can then mount `glusterfsvol` by name.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 容器随后可以按名称挂载`glusterfsvol`。
- en: The endpoints tell the GlusterFS volume plugin how to find the storage nodes
    of the GlusterFS cluster.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 端点告诉 GlusterFS 卷插件如何找到 GlusterFS 集群的存储节点。
- en: 'There was an effort to create a CSI driver for GlusterFS, but it was abandoned:
    [https://github.com/gluster/gluster-csi-driver](https://github.com/gluster/gluster-csi-driver).'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 曾有尝试为 GlusterFS 创建 CSI 驱动程序，但该项目已被放弃：[https://github.com/gluster/gluster-csi-driver](https://github.com/gluster/gluster-csi-driver)。
- en: After covering GlusterFS let’s look at CephFS.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍完 GlusterFS 后，我们来看看 CephFS。
- en: Using Ceph
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Ceph
- en: Ceph’s object store can be accessed using multiple interfaces. Unlike GlusterFS,
    Ceph does a lot of work automatically. It does distribution, replication, and
    self-healing all on its own. The following diagram shows how RADOS – the underlying
    object store – can be accessed in multiple ways.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph 的对象存储可以通过多种接口访问。与 GlusterFS 不同，Ceph 会自动完成很多工作。它自行进行分布式存储、复制和自我修复。下图展示了
    RADOS——底层对象存储——可以通过多种方式访问。
- en: '![](img/B18998_06_03.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_06_03.png)'
- en: 'Figure 6.3: Accessing RADOS'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3：访问 RADOS
- en: Kubernetes supports Ceph via the **Rados Block Device** (**RBD**) interface.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 通过 **Rados Block Device**（**RBD**）接口支持 Ceph。
- en: Connecting to Ceph using RBD
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 RBD 连接到 Ceph
- en: 'You must install `ceph-common` on each node of the Kubernetes cluster. Once
    you have your Ceph cluster up and running, you need to provide some information
    required by the Ceph RBD volume plugin in the pod configuration file:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 必须在 Kubernetes 集群的每个节点上安装 `ceph-common`。一旦你的 Ceph 集群启动并运行，你需要在 Pod 配置文件中提供 Ceph
    RBD 卷插件所需的一些信息：
- en: '`monitors`: Ceph monitors.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`monitors`: Ceph 监视器。'
- en: '`pool`: The name of the RADOS pool. If not provided, the default RBD pool is
    used.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pool`: RADOS 池的名称。如果未提供，则使用默认的 RBD 池。'
- en: '`image`: The image name that RBD has created.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`: RBD 创建的镜像名称。'
- en: '`user`: The RADOS username. If not provided, the default admin is used.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`user`: RADOS 用户名。如果未提供，则使用默认的管理员。'
- en: '`keyring`: The path to the keyring file. If not provided, the default `/etc/ceph/keyring`
    is used.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keyring`: 密钥环文件的路径。如果未提供，则使用默认路径 `/etc/ceph/keyring`。'
- en: '`secretName`: The name of the authentication secrets. If provided, `secretName`
    overrides `keyring`. Note: see the following paragraph about how to create a secret.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`secretName`: 认证秘密的名称。如果提供了，`secretName` 将覆盖 `keyring`。注意：请参见下面关于如何创建秘密的段落。'
- en: '`fsType`: The filesystem type (`ext4`, `xfs`, and so on) that is formatted
    on the device.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsType`: 设备上格式化的文件系统类型（如 `ext4`、`xfs` 等）。'
- en: '`readOnly`: Whether the filesystem is used as `readOnly`.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readOnly`: 是否将文件系统用作 `readOnly`。'
- en: 'If the Ceph authentication secret is used, you need to create a secret object:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用 Ceph 认证秘密，则需要创建一个秘密对象：
- en: '[PRE68]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The secret type is `kubernetes.io/rbd`.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 秘密类型为 `kubernetes.io/rbd`。
- en: 'Here is a sample pod that uses Ceph through RBD with a secret using the in-tree
    provider:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例 Pod，它通过 RBD 使用内置提供程序与 Ceph 进行连接，并使用了一个秘密：
- en: '[PRE69]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Ceph RBD supports `ReadWriteOnce` and `ReadOnlyMany` access modes. But, these
    days it is best to work with Ceph via Rook.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph RBD 支持 `ReadWriteOnce` 和 `ReadOnlyMany` 访问模式。但如今，最好通过 Rook 来使用 Ceph。
- en: Rook
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Rook
- en: 'Rook is an open source cloud native storage orchestrator. It is currently a
    graduated CNCF project. It used to provide a consistent experience on top of multiple
    storage solutions like Ceph, edgeFS, Cassandra, Minio, NFS, CockroachDB, and YugabyteDB.
    But, eventually it laser-focused on supporting only Ceph. Here are the features
    Rook provides:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: Rook 是一个开源的云原生存储编排器。目前它是一个已毕业的 CNCF 项目。以前它提供了一个一致的体验，支持多种存储解决方案，如 Ceph、edgeFS、Cassandra、Minio、NFS、CockroachDB
    和 YugabyteDB。但最终，它聚焦于只支持 Ceph。以下是 Rook 提供的功能：
- en: Automating deployment
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化部署
- en: Bootstrapping
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动
- en: Configuration
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置
- en: Provisioning
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置存储
- en: Scaling
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展
- en: Upgrading
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 升级
- en: Migration
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移
- en: Scheduling
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度
- en: Lifecycle management
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生命周期管理
- en: Resource management
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源管理
- en: Monitoring
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控
- en: Disaster recovery
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灾难恢复
- en: Rook takes advantage of modern Kubernetes best practices like CRDs and operators.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: Rook 利用现代 Kubernetes 最佳实践，如 CRD 和操作符。
- en: 'Here is the Rook architecture:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Rook 架构：
- en: '![](img/B18998_06_04.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_06_04.png)'
- en: 'Figure 6.4: Rook architecture'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：Rook 架构
- en: 'Once you install the Rook operator you can create a Ceph cluster using a Rook
    CRD such as: [https://github.com/rook/rook/blob/release-1.10/deploy/examples/cluster.yaml](https://github.com/rook/rook/blob/release-1.10/deploy/examples/cluster.yaml).'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了 Rook 操作符，你可以使用 Rook CRD 创建 Ceph 集群，例如：[https://github.com/rook/rook/blob/release-1.10/deploy/examples/cluster.yaml](https://github.com/rook/rook/blob/release-1.10/deploy/examples/cluster.yaml)。
- en: 'Here is a shortened version (without the comments):'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简化版本（没有注释）：
- en: '[PRE70]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Here is a storage class for CephFS:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 CephFS 的存储类：
- en: '[PRE71]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The full code is available here: [https://github.com/rook/rook/blob/release-1.10/deploy/examples/storageclass-bucket-retain.yaml](https://github.com/rook/rook/blob/release-1.10/deploy/examples/storageclass-bucket-retain.yaml).'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码可以在这里找到：[https://github.com/rook/rook/blob/release-1.10/deploy/examples/storageclass-bucket-retain.yaml](https://github.com/rook/rook/blob/release-1.10/deploy/examples/storageclass-bucket-retain.yaml)。
- en: Now that we’ve covered using distributed storage using GlusterFS, Ceph, and
    Rook, let’s look at enterprise storage options.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了使用GlusterFS、Ceph和Rook的分布式存储，让我们来看看企业级存储选项。
- en: Integrating enterprise storage into Kubernetes
  id: totrans-409
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将企业存储集成到Kubernetes中
- en: 'If you have an existing **Storage Area Network** (**SAN**) exposed over the
    iSCSI interface, Kubernetes has a volume plugin for you. It follows the same model
    as other shared persistent storage plugins we’ve seen earlier. It supports the
    following features:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个通过iSCSI接口暴露的现有**存储区域网络**（**SAN**），Kubernetes为你提供了一个卷插件。它遵循与我们之前看到的其他共享持久存储插件相同的模型。它支持以下功能：
- en: Connecting to one portal
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到一个门户
- en: Mounting a device directly or via `multipathd`
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接挂载设备或通过`multipathd`
- en: Formatting and partitioning any new device
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式化和分区任何新设备
- en: Authenticating via CHAP
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过CHAP进行认证
- en: 'You must configure the iSCSI initiator, but you don’t have to provide any initiator
    information. All you need to provide is the following:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须配置iSCSI发起程序，但无需提供任何发起程序信息。你只需提供以下内容：
- en: IP address of the iSCSI target and port (if not the default `3260`)
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iSCSI目标的IP地址和端口（如果不是默认的`3260`）
- en: Target’s **IQN** (**iSCSI Qualified Name**) – typically the reversed domain
    name
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标的**IQN**（**iSCSI合格名称**）——通常是反转的域名
- en: '**LUN** (**Logical Unit Number**)'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LUN**（**逻辑单元号**）'
- en: Filesystem type
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件系统类型
- en: '`Readonly` Boolean flag'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Readonly` 布尔标志'
- en: 'The iSCSI plugin supports `ReadWriteOnce` and `ReadonlyMany`. Note that you
    can’t partition your device at this time. Here is an example pod with an iSCSI
    volume spec:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: iSCSI插件支持`ReadWriteOnce`和`ReadonlyMany`。请注意，此时你不能对设备进行分区。下面是一个带有iSCSI卷规格的示例Pod：
- en: '[PRE72]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Other storage providers
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他存储提供商
- en: 'The Kubernetes storage scene keeps innovating. A lot of companies adapt their
    products to Kubernetes and some companies and organizations build Kubernetes-dedicated
    storage solutions. Here are some of the more popular and mature solutions:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的存储领域不断创新。许多公司将其产品适配到Kubernetes，一些公司和组织则构建了专门的Kubernetes存储解决方案。以下是一些更受欢迎和成熟的解决方案：
- en: OpenEBS
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenEBS
- en: Longhorn
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Longhorn
- en: Portworx
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Portworx
- en: The Container Storage Interface
  id: totrans-428
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器存储接口
- en: The **Container Storage Interface** (**CSI**) is a standard interface for the
    interaction between container orchestrators and storage providers. It was developed
    by Kubernetes, Docker, Mesos, and Cloud Foundry. The idea is that storage providers
    implement just one CSI driver and all container orchestrators need to support
    only the CSI. It is the equivalent of CNI for storage.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器存储接口**（**CSI**）是容器编排器与存储提供商之间交互的标准接口。它由Kubernetes、Docker、Mesos和Cloud Foundry开发。其目的是让存储提供商只实现一个CSI驱动程序，而所有容器编排器只需要支持CSI。这相当于存储的CNI。'
- en: A CSI volume plugin was added in Kubernetes 1.9 as an Alpha feature and has
    been generally available since Kubernetes 1.13\. The older FlexVolume approach
    (which you may have come across) is deprecated now.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes 1.9中添加了一个CSI卷插件作为Alpha功能，并自Kubernetes 1.13起正式提供。以前的FlexVolume方法（你可能遇到过）现在已经被弃用。
- en: 'Here is a diagram that demonstrates how CSI works within Kubernetes:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个展示CSI如何在Kubernetes内工作的示意图：
- en: '![](img/B18998_06_05.png)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_06_05.png)'
- en: 'Figure 6.5: CSI architecture'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：CSI架构
- en: The migration effort to port all in-tree plugins to out-of-tree CSI drivers
    is well underway. See [https://kubernetes-csi.github.io](https://kubernetes-csi.github.io)
    for more details.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内建插件迁移到外部CSI驱动程序的工作正在顺利进行。更多详情请参见[https://kubernetes-csi.github.io](https://kubernetes-csi.github.io)。
- en: Advanced storage features
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级存储功能
- en: These features are available only to CSI drivers. They represent the benefits
    of a uniform storage model that allows adding optional advanced functionality
    across all storage providers with a uniform interface.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 这些功能仅对CSI驱动程序可用。它们代表了一种统一的存储模型的优势，允许通过统一的接口在所有存储提供商中添加可选的高级功能。
- en: Volume snapshots
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷快照
- en: 'Volume snapshots are generally available as of Kubernetes 1.20\. They are exactly
    what they sound like – a snapshot of a volume at a certain point in time. You
    can create and later restore volumes from a snapshot. It’s interesting that the
    API objects associated with snapshots are CRDs and not part of the core Kubernetes
    API. The objects are:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 卷快照从Kubernetes 1.20开始正式提供。它们就是字面意思——在某个时间点对卷的快照。你可以从快照创建卷，并在之后恢复。值得注意的是，与快照相关的API对象是CRD，而不是Kubernetes核心API的一部分。这些对象包括：
- en: '`VolumeSnapshotClass`'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VolumeSnapshotClass`'
- en: '`VolumeSnapshotContents`'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VolumeSnapshotContents`'
- en: '`VolumeSnapshot`'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VolumeSnapshot`'
- en: Volume snapshots work using an `external-prosnapshotter` sidecar container that
    the Kubernetes team developed. It watches for snapshot CRDs to be created and
    interacts with the snapshot controller, which can invoke the `CreateSnapshot`
    and `DeleteSnapshot` operations of CSI drivers that implement snapshot support.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 卷快照使用一个`external-prosnapshotter`侧车容器，该容器由Kubernetes团队开发。它监视快照CRD的创建，并与快照控制器交互，后者可以调用实现快照支持的CSI驱动程序的`CreateSnapshot`和`DeleteSnapshot`操作。
- en: 'Here is how to declare a volume snapshot:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是声明卷快照的方式：
- en: '[PRE73]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: You can also provision volumes from a snapshot.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以从快照中提供卷。
- en: 'Here is a persistent volume claim bound to a snapshot:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个绑定到快照的持久化卷声明：
- en: '[PRE74]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: See [https://github.com/kubernetes-csi/external-snapshotter#design](https://github.com/kubernetes-csi/external-snapshotter#design)
    for more details.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 详情请参见[https://github.com/kubernetes-csi/external-snapshotter#design](https://github.com/kubernetes-csi/external-snapshotter#design)。
- en: CSI volume cloning
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CSI卷克隆
- en: Volume cloning is available in GA as of Kubernetes 1.18\. Volume clones are
    new volumes that are populated with the content of an existing volume. Once the
    volume cloning is complete there is no relation between the original and the clone.
    Their content will diverge over time. You can perform a clone manually by creating
    a snapshot and then create a new volume from the snapshot. But, volume cloning
    is more streamlined and efficient.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 卷克隆从Kubernetes 1.18版本起已进入GA。卷克隆是新创建的卷，内容来自现有卷。一旦卷克隆完成，原始卷和克隆卷之间没有关联。它们的内容将随着时间的推移而分歧。你可以通过手动创建快照并从该快照创建新卷来执行克隆。但卷克隆更加简化和高效。
- en: 'It only works for dynamic provisioning and uses the storage class of the source
    volume for the clone as well. You initiate a volume clone by specifying an existing
    persistent volume claim as a data source of a new persistent volume claim. That
    triggers the dynamic provisioning of a new volume that clones the source claim’s
    volume:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 它仅适用于动态供应，并且克隆使用源卷的存储类。你可以通过指定一个现有的持久化卷声明作为新持久化卷声明的数据源来启动卷克隆。这将触发动态供应新卷，并克隆源声明的卷：
- en: '[PRE75]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: See [https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/)
    for more details.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 详情请参见[https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/)。
- en: Storage capacity tracking
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储容量跟踪
- en: Storage capacity tracking (GA as of Kubernetes 1.24) allows the scheduler to
    better schedule pods that require storage into nodes that can provide that storage.
    This requires a CSI driver that supports storage capacity tracking.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 存储容量跟踪（Kubernetes 1.24版本起GA）允许调度器将需要存储的Pod调度到能够提供该存储的节点。这需要支持存储容量跟踪的CSI驱动程序。
- en: The CSI driver will create a `CSIStorageCapacity` object for each storage class
    and determine which nodes have access to this storage. In addition the `CSIDriverSpec`'s
    field `StorageCapacity` must be set to true.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: CSI驱动程序将为每个存储类创建一个`CSIStorageCapacity`对象，并确定哪些节点可以访问该存储。此外，`CSIDriverSpec`字段`StorageCapacity`必须设置为true。
- en: When a pod specifies a storage class name in `WaitForFirstConsumer` mode and
    the CSI driver has `StorageCapacity` set to true the Kubernetes scheduler will
    consider the CSIStorageCapacity object associated with the storage class and schedule
    the pod only to nodes that have sufficient storage.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 当Pod在`WaitForFirstConsumer`模式下指定存储类名称，并且CSI驱动程序将`StorageCapacity`设置为true时，Kubernetes调度器将考虑与存储类关联的CSIStorageCapacity对象，并将Pod调度到有足够存储的节点。
- en: 'Check out: [https://kubernetes.io/docs/concepts/storage/storage-capacity](https://kubernetes.io/docs/concepts/storage/storage-capacity)
    for more details.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多信息，请参见：[https://kubernetes.io/docs/concepts/storage/storage-capacity](https://kubernetes.io/docs/concepts/storage/storage-capacity)。
- en: Volume health monitoring
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷健康监控
- en: 'Volume health monitoring is a recent addition to the storage APIs. It has been
    in Alpha since Kubernetes 1.21\. It involves two components:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 卷健康监控是存储API的最近添加功能，自Kubernetes 1.21起处于Alpha阶段。它涉及两个组件：
- en: An external health monitor
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部健康监控器
- en: The kubelet
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubelet
- en: CSI drivers that support volume health monitoring will update PVCs with events
    on abnormal conditions of associated storage volumes. The external health monitor
    also watches nodes for failures and will report events on PVCs bound to these
    nodes.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 支持卷健康监控的CSI驱动程序将更新PVC，报告与关联存储卷的异常情况相关的事件。外部健康监控器还会监视节点故障，并将报告与这些节点绑定的PVC的事件。
- en: In the case where a CSI driver enables volume health monitoring from the node
    side, any abnormal condition detected will result in an event being reported for
    every pod that utilizes a PVC with the corresponding issue.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在启用了 CSI 驱动程序的节点端卷健康监控的情况下，任何检测到的异常情况都会导致为每个使用具有相关问题的 PVC 的 pod 报告事件。
- en: 'There is also a new metric associated with volume health: `kubelet_volume_stats_health_status_abnormal`.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个与卷健康相关的新指标：`kubelet_volume_stats_health_status_abnormal`。
- en: 'It has two labels: `namespace` and `persistentvolumeclaim`. The values are
    0 or 1.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 它有两个标签：`namespace` 和 `persistentvolumeclaim`。其值为 0 或 1。
- en: 'More details are available here: [https://kubernetes.io/docs/concepts/storage/volume-health-monitoring/](https://kubernetes.io/docs/concepts/storage/volume-health-monitoring/).'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节请见这里：[https://kubernetes.io/docs/concepts/storage/volume-health-monitoring/](https://kubernetes.io/docs/concepts/storage/volume-health-monitoring/)。
- en: CSI is an exciting initiative that simplified the Kubernetes code base itself
    by externalizing storage drivers. It simplified the life of storage solutions
    that can develop out-of-tree drivers and added a lot of advanced capabilities
    to the Kubernetes storage story.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: CSI 是一个令人兴奋的计划，通过外部化存储驱动程序简化了 Kubernetes 代码库本身。它简化了存储解决方案的工作，使其能够开发树外驱动程序，并为
    Kubernetes 存储系统增添了许多先进功能。
- en: Summary
  id: totrans-469
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we took a deep look into storage in Kubernetes. We’ve looked
    at the generic conceptual model based on volumes, claims, and storage classes,
    as well as the implementation of volume plugins. Kubernetes eventually maps all
    storage systems into mounted filesystems in containers or devices of raw block
    storage. This straightforward model allows administrators to configure and hook
    up any storage system from local host directories, through cloud-based shared
    storage, all the way to enterprise storage systems. The transition of storage
    provisioners from in-tree to CSI-based out-of-tree drivers bodes well for the
    storage ecosystem. You should now have a clear understanding of how storage is
    modeled and implemented in Kubernetes and be able to make intelligent choices
    on how to implement storage in your Kubernetes cluster.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了 Kubernetes 中的存储。我们研究了基于卷、声明和存储类的通用概念模型，以及卷插件的实现。Kubernetes 最终将所有存储系统映射为容器中挂载的文件系统或原始块存储设备。这种简单的模型使管理员能够配置并连接任何存储系统，从本地主机目录到基于云的共享存储，再到企业级存储系统。存储提供者从树内驱动程序转变为基于
    CSI 的树外驱动程序，这对存储生态系统来说是个好兆头。你现在应该清楚了解 Kubernetes 中存储的建模和实现方式，并能够在你的 Kubernetes
    集群中做出智能的存储实现选择。
- en: In *Chapter 7*, *Running Stateful Applications with Kubernetes*, we’ll see how
    Kubernetes can raise the level of abstraction and, on top of storage, help to
    develop, deploy, and operate stateful applications using concepts such as stateful
    sets.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 7 章*，*使用 Kubernetes 运行有状态应用程序*中，我们将看到 Kubernetes 如何提升抽象级别，并在存储之上，帮助开发、部署和运行有状态应用程序，使用像有状态集这样的概念。
- en: Join us on Discord!
  id: totrans-472
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们在 Discord 的社区！
- en: Read this book alongside other users, cloud experts, authors, and like-minded
    professionals.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他用户、云专家、作者和志同道合的专业人士一起阅读本书。
- en: Ask questions, provide solutions to other readers, chat with the authors via.
    Ask Me Anything sessions and much more.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 提出问题，向其他读者提供解决方案，通过问我任何问题的环节与作者聊天，还有更多内容。
- en: Scan the QR code or visit the link to join the community now.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描二维码或访问链接立即加入社区。
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
- en: '![](img/QR_Code844810820358034203.png)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code844810820358034203.png)'
