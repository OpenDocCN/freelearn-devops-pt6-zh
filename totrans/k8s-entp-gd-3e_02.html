<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer088">
<h1 class="chapterNumber">2</h1>
<h1 class="chapterTitle" id="_idParaDest-47">Deploying Kubernetes Using KinD</h1>
<p class="normal">Like many IT professionals, having a Kubernetes cluster on our laptops is highly beneficial for showcasing and testing products. In certain cases, you may require running a cluster with multiple nodes or clusters for intricate demonstrations or testing, such as a multi-cluster service mesh. These scenarios require multiple servers to create the required clusters, which, in turn, call<a id="_idIndexMarker069"/> for substantial RAM and a <strong class="keyWord">hypervisor</strong> to run virtual machines.</p>
<p class="normal">To do full testing on a multiple-cluster scenario, you would need to create multiple nodes for each cluster. If you created the clusters using virtual machines, you would need to have enough resources to run multiple virtual machines. Each of the machines would have an <strong class="keyWord">overhead</strong>, including disk space, memory, and CPU utilization. </p>
<p class="normal">Imagine if it were possible to establish a cluster solely using containers. By utilizing containers instead of full virtual machines, you gain the advantage of running additional nodes due to the reduced system requirements. This approach enables you to quickly create and delete clusters within minutes using a single command. Additionally, you can employ scripts to facilitate cluster creation and even run multiple clusters on a single host.</p>
<p class="normal">Using containers to run a Kubernetes cluster provides you with an environment that would be difficult for most people to deploy using virtual machines or physical hardware, due to resource constraints. Lucky for us, there is a tool<a id="_idIndexMarker070"/> to accomplish this called <strong class="keyWord">KinD</strong> (<strong class="keyWord">Kubernetes in Docker</strong>), which allows us to run Kubernetes cluster(s) on a single machine. KinD is a tool that provides a simple, quick, and easy method to deploy a development Kubernetes cluster. It is smaller when compared to other alternatives like Minikube, and even K3s, making it ideal for most users to run on their own systems. </p>
<p class="normal">We will use KinD to deploy a multi-node cluster that you will use in future chapters to test and deploy components, such as Ingress controllers, authentication, <strong class="keyWord">RBAC</strong> (<strong class="keyWord">Role-Based Access Control</strong>), security policies, and more.</p>
<p class="normal">In this chapter, we will cover the following main topics:</p>
<ul>
<li class="bulletList">Introducing Kubernetes components and objects</li>
<li class="bulletList">Using development clusters</li>
<li class="bulletList">Installing KinD</li>
<li class="bulletList">Creating a KinD cluster</li>
<li class="bulletList">Reviewing your KinD cluster</li>
<li class="bulletList">Adding a custom load balancer for Ingress</li>
</ul>
<p class="normal">Let’s get started!</p>
<h1 class="heading-1" id="_idParaDest-48">Technical requirements</h1>
<p class="normal">This chapter has the following technical requirements:</p>
<ul>
<li class="bulletList">An Ubuntu 22.04+ server running Docker with a minimum of 4 GB of RAM, although 8 GB is recommended</li>
<li class="bulletList">Scripts from the <code class="inlineCode">chapter2</code> folder from the GitHub repo, which you can access by going to this book’s GitHub repository: <a href="https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition"><span class="url">https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition</span></a></li>
</ul>
<p class="normal">We consider it essential to highlight that this chapter will mention various Kubernetes objects, some of which may lack extensive context. However, in <em class="chapterRef">Chapter 3</em>, <em class="italic">Kubernetes Bootcamp</em>, we will dive into Kubernetes objects in depth, providing numerous example commands to enhance your understanding. To ensure a practical learning experience, we recommend having a cluster while reading the bootcamp chapter.</p>
<p class="normal">Most of the basic Kubernetes topics covered in this chapter will be discussed in future chapters, so if some topics become a bit foggy after you’ve read this chapter, never fear! They will be discussed in detail in later chapters.</p>
<h1 class="heading-1" id="_idParaDest-49">Introducing Kubernetes components and objects</h1>
<p class="normal">Since this chapter will discuss various common Kubernetes objects and components, we’ve included a table with brief definitions for each term. This will provide you with the necessary context and help ensure you understand the terminology as you read through the chapter.</p>
<p class="normal">In <em class="chapterRef">Chapter 3</em>, <em class="italic">Kubernetes Bootcamp</em>, we will go over the components of Kubernetes and the basic set of objects that are included in a cluster. Since we will have to use some basic objects in this module, we have provided some common Kubernetes components and resources in the table below. </p>
<figure class="mediaobject"><img alt="Table 4.1 – Kubernetes components and objects " height="1051" src="../Images/B21165_02_01.png" width="853"/></figure>
<p class="packt_figref">Figure 2.1: Kubernetes components and objects</p>
<p class="normal">While these are only a few of the objects that are available in a Kubernetes cluster, they are the objects we will discuss in this chapter. Knowing what each resource is and having basic knowledge<a id="_idIndexMarker071"/> of its functionality will help you to understand this chapter.</p>
<h2 class="heading-2" id="_idParaDest-50">Interacting with a cluster</h2>
<p class="normal">To interact with a cluster, you use<a id="_idIndexMarker072"/> the <code class="inlineCode">kubectl</code> executable. We will go over <code class="inlineCode">kubectl</code> in <em class="chapterRef">Chapter 3</em>, <em class="italic">Kubernetes Bootcamp</em>, but since we will be using a few commands in this chapter, we wanted to provide the basic commands we will use in a table with an explanation of what the options provide:</p>
<figure class="mediaobject"><img alt="Table 4.2 – Basic kubectl commands " height="454" src="../Images/B21165_02_02.png" width="878"/></figure>
<p class="packt_figref">Figure 2.2: Basic kubectl commands</p>
<p class="normal">In this chapter, you will use these basic commands<a id="_idIndexMarker073"/> to deploy parts of the cluster that we will use throughout this book.</p>
<p class="normal">Next, we will introduce the concept of development clusters and then focus on one of the most popular tools used to create the development clusters, KinD.</p>
<h1 class="heading-1" id="_idParaDest-51">Using development clusters</h1>
<p class="normal">Over time, several solutions<a id="_idIndexMarker074"/> have been developed to facilitate the installation of development Kubernetes clusters, enabling administrators and developers to conduct testing on local systems. While these tools have proven effective for basic Kubernetes testing, they often possess certain limitations that render them suboptimal for more advanced scenarios.</p>
<p class="normal">Some of the most common solutions available are as follows:</p>
<ul>
<li class="bulletList">Docker Desktop</li>
<li class="bulletList">K3s</li>
<li class="bulletList">KinD</li>
<li class="bulletList">kubeadm</li>
<li class="bulletList">minikube</li>
<li class="bulletList">Rancher Desktop</li>
</ul>
<p class="normal">Each solution has benefits, limitations, and use cases. Some solutions limit you to a single node that runs both the control plane and worker nodes. Others offer multi-node support but require additional resources to create multiple virtual machines. Depending on your development or testing requirements, these solutions may not meet your needs completely.</p>
<p class="normal">To truly get into Kubernetes, you need<a id="_idIndexMarker075"/> to have a cluster that has at least a control plane and a single worker node. You may want to test scenarios where you drop a worker node suddenly to see how a workload reacts; in this case, you would need to create a cluster that has a control plane node and three worker nodes. To create these various cluster types, we can use<a id="_idIndexMarker076"/> a project from the Kubernetes <strong class="keyWord">Special Interest Group</strong> (<strong class="keyWord">SIG</strong>), called <strong class="keyWord">KinD</strong>.</p>
<p class="normal"><strong class="keyWord">KinD</strong> (<strong class="keyWord">Kubernetes in Docker</strong>) offers the capability to create<a id="_idIndexMarker077"/> multiple clusters on a single host, where each cluster can have multiple control planes and worker nodes. This feature facilitates advanced testing scenarios that would have otherwise required additional resource allocation, using alternative solutions. The community response to KinD has been highly positive, as shown by its active GitHub community at <a href="https://github.com/kubernetes-sigs/kind"><span class="url">https://github.com/kubernetes-sigs/kind</span></a> and the availability of a dedicated Slack channel (#kind).</p>
<div class="note">
<p class="normal">While KinD is a great tool to create development clusters, do not use KinD as a production cluster or expose a KinD cluster to the Internet. Although KinD clusters offer most of the same features you would want in a production cluster, it has <strong class="keyWord">not</strong> been designed for production environments. </p>
</div>
<h2 class="heading-2" id="_idParaDest-52">Why did we select KinD for this book?</h2>
<p class="normal">When we started this book, our objective<a id="_idIndexMarker078"/> was to combine theoretical knowledge with practical hands-on experience. KinD emerged as an important tool in achieving this goal by enabling us to include scripts to quickly set up and tear down clusters. While alternative solutions may offer comparable functionality, KinD stands out by its ability to establish multi-node clusters within a matter of minutes. We intended to provide a cluster that has both a control plane and worker nodes to emulate a more “real-world” cluster environment. In order to reduce hardware demands and streamline Ingress configuration, we have chosen to limit most of the exercise scenarios in this book to a single control plane node and a single worker node.</p>
<p class="normal">Some of you may be asking yourselves why we didn’t use kubeadm or some other tool to deploy a cluster that has multiple nodes for both the control plane and worker nodes. As we have said, while KinD is not meant to be used in production, it requires fewer resources to simulate a multi-node cluster, allowing most of the readers to work on a cluster that will act like a standard enterprise-ready Kubernetes cluster.</p>
<p class="normal">A multi-node cluster can be created in a few minutes, and once testing has been completed, clusters can be torn down in a few seconds. The ability to spin up and down clusters makes KinD the perfect platform for our exercises. KinD’s requirements are simple: you only need a running Docker daemon to create a cluster. This means that it is compatible with most operating systems, including the following:</p>
<ul>
<li class="bulletList">Linux</li>
<li class="bulletList">macOS running Docker Desktop</li>
<li class="bulletList">Windows running Docker Desktop</li>
<li class="bulletList">Windows running WSL2</li>
</ul>
<p class="normal">At the time of writing, KinD does not offer official support for Chrome OS. There are a number of posts in the KinD Git repository on the required steps to make KinD work on a system running Chrome OS; however, it’s not officially supported by the team.</p>
<p class="normal">While KinD supports<a id="_idIndexMarker079"/> most operating <a id="_idIndexMarker080"/>systems, we have selected <strong class="keyWord">Ubuntu 22.04</strong> as our host system. Some of the exercises in this book require files to be in specific directories and commands; selecting a single Linux version helps us make sure the exercises work as designed. If you do not have access to a Ubuntu server at home, you can create<a id="_idIndexMarker081"/> a compute instance in a cloud provider such as <strong class="keyWord">Google Cloud Platform</strong> (<strong class="keyWord">GCP</strong>). Google offers $300 in credit, which is more than enough to run a single Ubuntu server for a few weeks. You can view GCP’s free options at <a href="https://cloud.google.com/free/"><span class="url">https://cloud.google.com/free/</span></a>.</p>
<p class="normal">Finally, the scripts used to deploy the exercises are all pinned to specific versions of KinD and other dependencies. Kubernetes and the cloud-native world move very quickly, and we can’t guarantee that everything will work as expected with the latest versions of the systems that exist<a id="_idIndexMarker082"/> when you read our book.</p>
<p class="normal">Now, let’s explain how KinD works and what a basic KinD Kubernetes cluster looks like. Before we move on to creating the cluster, we will use it for the book exercises.</p>
<h2 class="heading-2" id="_idParaDest-53">Working with a basic KinD Kubernetes cluster</h2>
<p class="normal">From a broader perspective, a KinD cluster can be seen<a id="_idIndexMarker083"/> as comprising a single Docker container, responsible for running both a control plane node and a worker node, creating a Kubernetes cluster. To ensure a straightforward and resilient deployment, KinD packages all Kubernetes objects into a unified image, referred to as a node image. This node image<a id="_idIndexMarker084"/> includes all the necessary Kubernetes components required to create either a single-node or multi-node cluster(s).</p>
<p class="normal">To show what’s running in a KinD container, we can utilize Docker to execute commands within a control plane node container and examine the process list. Within the process list, you will observe the standard Kubernetes components that are active on the control plane nodes, If we were to execute the following command: <code class="inlineCode">docker exec cluster01-worker ps -ef</code>.</p>
<figure class="mediaobject"><img alt="A screenshot of a computer program  Description automatically generated with medium confidence" height="512" src="../Images/B21165_02_03.png" width="876"/></figure>
<p class="packt_figref">Figure 2.3: Host process list showing control plane components</p>
<p class="normal">If you were to exec into a worker node to check the components, you would see all the standard worker node components:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer program  Description automatically generated with medium confidence" height="280" src="../Images/B21165_02_04.png" width="878"/></figure>
<p class="packt_figref">Figure 2.4: Host process list showing worker components</p>
<p class="normal">We will cover the standard Kubernetes components in <em class="chapterRef">Chapter 3</em>, <em class="italic">Kubernetes Bootcamp</em>, including <code class="inlineCode">kube-apiserver</code>, <code class="inlineCode">kubelets</code>, <code class="inlineCode">kube-proxy</code>, <code class="inlineCode">kube-scheduler</code>, and <code class="inlineCode">kube-controller-manager</code>.</p>
<p class="normal">In addition to standard Kubernetes components, both KinD nodes (the control plane node and worker node) have an additional component that is not part of most standard Kubernetes<a id="_idIndexMarker085"/> installations, referred to as <strong class="keyWord">Kindnet</strong>. Kindnet is a <strong class="keyWord">Container Network Interface</strong> (<strong class="keyWord">CNI</strong>) solution that is included<a id="_idIndexMarker086"/> in a default KinD deployment and provides networking to a Kubernetes cluster.</p>
<p class="normal">The Kubernetes CNI is a specification that allows Kubernetes to utilize a large list of network software solutions, including <strong class="keyWord">Calico</strong>, <strong class="keyWord">Flannel</strong>, <strong class="keyWord">Cilium</strong>, <strong class="keyWord">Kindnet</strong>, and more.</p>
<p class="normal">Although Kindnet serves as the default CNI, it is possible to deactivate it and opt for an alternative, such as Calico, which we will utilize for our KinD cluster. While Kindnet would work for most tasks we need to run, it isn’t a CNI that you will see in the real world running a Kubernetes cluster. Since this book is meant to help you along your Kubernetes enterprise journey, we wanted to replace the CNI with a more commonly used CNI like Calico.</p>
<p class="normal">Now that you have discussed<a id="_idIndexMarker087"/> each of the nodes and the Kubernetes components, let’s take a look at what’s included with a base KinD cluster. To show the complete cluster and all the components that are running, we can run the <code class="inlineCode">kubectl get pods --all</code> command. This will list all the running components on the cluster, including the base components, which we will discuss in <em class="chapterRef">Chapter 3</em>, <em class="italic">Kubernetes Bootcamp</em>. In addition to the base cluster components, you may notice a running pod in a namespace called <code class="inlineCode">local-path-storage</code>, along with a pod named <code class="inlineCode">local-path-provisioner</code>. This pod runs one of the add-ons included with KinD, providing the cluster with the ability to auto-provision <code class="inlineCode">PersistentVolumeClaims</code>:</p>
<figure class="mediaobject"><img alt="A screen shot of a computer  Description automatically generated with medium confidence" height="231" src="../Images/B21165_02_05.png" width="877"/></figure>
<p class="packt_figref">Figure 2.5: kubectl get pods showing local-path-provisioner</p>
<p class="normal">Each development cluster option<a id="_idIndexMarker088"/> typically provides similar functionalities essential for testing deployments. These options typically include a Kubernetes control plane, worker nodes, and a default <strong class="keyWord">Container Networking Interface</strong> (<strong class="keyWord">CNI</strong>) for networking requirements. While most offerings meet these fundamental needs, some go beyond and offer additional capabilities. As your Kubernetes workloads progress, you may find the need for supplementary add-ons like the local-path-provisioner. In this book, we heavily depend on this component for various exercises, as it plays a pivotal role in deploying many of the examples featured throughout the book. Without it, completing the exercises would become considerably more challenging.</p>
<p class="normal">Why should the use of persistent volumes in your development cluster be significant? It’s all about knowledge that you will run into when using most enterprise Kubernetes clusters. As Kubernetes matures, numerous organizations have transitioned stateful workloads to containers, requiring persistent storage for their data. Being equipped with the capability to interact with storage resources within a KinD cluster offers an opportunity to acquire knowledge about working with storage, all accomplished without the need for additional resources.</p>
<p class="normal">The local provisioner is great for development and testing, but it should not be used in a production environment. Most production clusters running Kubernetes will provide persistent storage to developers. Usually, the storage will be backed by storage systems<a id="_idIndexMarker089"/> based<a id="_idIndexMarker090"/> on block storage, <strong class="keyWord">S3</strong> (<strong class="keyWord">Simple Storage Service</strong>), or <strong class="keyWord">NFS</strong> (<strong class="keyWord">Network File System</strong>).</p>
<p class="normal">Aside from NFS, most home labs rarely have the resources to run a full-featured storage system. <code class="inlineCode">local-path-provisioner</code> removes this limitation from users by providing all the functions to your KinD cluster that an expensive storage solution would provide using local disk resources.</p>
<p class="normal">In <em class="chapterRef">Chapter 3</em>, <em class="italic">Kubernetes Bootcamp</em>, we will discuss a few API objects that are part of Kubernetes storage. We will discuss the <code class="inlineCode">CSIdrivers</code>, <code class="inlineCode">CSInodes</code>, and <code class="inlineCode">StorageClass</code> objects. These objects are used by the cluster to provide access to the backend storage system. Once installed and configured, pods consume the storage using the <code class="inlineCode">PersistentVolumes</code> and <code class="inlineCode">PersistentVolumeClaims</code> objects. Storage objects are important to understand, but when they were first released, they were difficult for most people to test, since they weren’t included in most Kubernetes development offerings.</p>
<p class="normal">KinD recognized this limitation<a id="_idIndexMarker091"/> and chose to bundle a project from Rancher Labs, now part of SUSE, called <code class="inlineCode">local-path-provisioner</code>, which is built upon the Kubernetes local persistent<a id="_idIndexMarker092"/> volumes framework, initially introduced in <strong class="keyWord">Kubernetes 1.10</strong>.</p>
<p class="normal">You may be wondering why anyone would need an add-on, since Kubernetes has native support for local host persistent volumes. While support may have been added for local persistent storage, Kubernetes has not added auto-provisioning capabilities. While the <strong class="keyWord">CNCF</strong> (<strong class="keyWord">Cloud Native Computing Foundation</strong>) does offer an auto-provisioner, it must be installed<a id="_idIndexMarker093"/> and configured as a separate Kubernetes component. KinD’s provisioner removes this configuration, so you can use persistent volumes easily on development clusters. Rancher’s project provides the following to KinD:</p>
<ul>
<li class="bulletList">Auto-creation of <code class="inlineCode">PersistentVolumes</code> when a <code class="inlineCode">PersistentVolumeClaim</code> request is created.</li>
<li class="bulletList">A default <code class="inlineCode">StorageClass</code> named standard.</li>
</ul>
<p class="normal">When the auto-provisioner sees a <code class="inlineCode">PersistentVolumeClaim</code> (<strong class="keyWord">PVC</strong>) request hit the API server, a <code class="inlineCode">PersistentVolume</code> will be created, and the pod’s PVC will be bound to the newly created <code class="inlineCode">PersistentVolume</code>. The PVC can then be used by a pod that requires persistent storage.</p>
<p class="normal">The <code class="inlineCode">local-path-provisioner</code> adds a feature to KinD clusters that greatly expands the potential test scenarios that you can run. Without the ability to auto-provision persistent disks, it would be a challenge to test deployments that require persistent disks.</p>
<p class="normal">With the help of Rancher, KinD provides you with a solution so that you can experiment with dynamic volumes, storage classes, and other storage tests that would otherwise be impossible to run outside of an expensive home lab or a data center.</p>
<p class="normal">We will use the provisioner in multiple chapters to provide volumes to different deployments. Knowing how to use persistent storage in a Kubernetes<a id="_idIndexMarker094"/> cluster is a great skill to have, and in future chapters, you will see the provisioner in action.</p>
<p class="normal">Now, let’s move on to explain the KinD node image, which is used to deploy both the control plane and the worker nodes.</p>
<h2 class="heading-2" id="_idParaDest-54">Understanding the node image</h2>
<p class="normal">The node image is what gives KinD the magic<a id="_idIndexMarker095"/> to run Kubernetes inside a Docker container. This is an impressive accomplishment, since Docker relies on a <code class="inlineCode">systemd</code> running system and other components that are not included in most container images.</p>
<p class="normal">KinD starts with a base image, which is an image the team has developed that contains everything required for Docker, Kubernetes, and <code class="inlineCode">systemd</code>. Since the node image is based on a base Ubuntu image, the team removes services that are not required and configures <code class="inlineCode">systemd</code> for Docker.</p>
<p class="normal">If you want to know the details of how the base image is created, you can look at the Docker file in the KinD team’s GitHub repository at <a href="https://github.com/kubernetes-sigs/kind/blob/main/images/base/Dockerfile"><span class="url">https://github.com/kubernetes-sigs/kind/blob/main/images/base/Dockerfile</span></a>.</p>
<h2 class="heading-2" id="_idParaDest-55">KinD and Docker networking</h2>
<p class="normal">When employing KinD, which relies<a id="_idIndexMarker096"/> on Docker<a id="_idIndexMarker097"/> or Red Hat’s <strong class="keyWord">Podman</strong> as the container engine<a id="_idIndexMarker098"/> to run cluster nodes, it’s important to note that the clusters have the same network constraints typically associated with standard Docker containers. While these limitations don’t hinder testing the KinD Kubernetes cluster from the local host, they may introduce complications when attempting to test containers from other machines on your network.</p>
<div class="note">
<p class="normal"><strong class="keyWord">Podman</strong> is outside of the scope<a id="_idIndexMarker099"/> of this book; it is mentioned as an alternative that KinD now supports. At a high level, it’s an open source offering from Red Hat that is meant to replace Docker as a runtime engine. It offers advantages over Docker for many Enterprise use cases, such as enhanced security, not requiring a system daemon, and more. While it has advantages, it can also add complexity for people who are new<a id="_idIndexMarker100"/> to the container world</p>
</div>
<p class="normal">When you install KinD on a Docker host, a new Docker bridge network will be created, called <code class="inlineCode">kind</code>. This network configuration was introduced in <code class="inlineCode">KinD v0.8.0</code>, which resolved multiple issues from previous versions that used the default Docker bridge network. Most users will not notice this change, but it’s important to know this; as you start to create more advanced KinD clusters with additional containers, you may need to run on the same network as KinD. If you have the requirement to run additional containers on the KinD network, you will need to add <code class="inlineCode">--net=kind</code> to your <code class="inlineCode">docker run</code> command.</p>
<p class="normal">Along with the Docker networking <a id="_idIndexMarker101"/>considerations, we must consider<a id="_idIndexMarker102"/> the Kubernetes CNI as well. KinD supports multiple supports different CNIs, including Kindnet, Calico, Cilium, and others. Officially, Kindnet is the only CNI they will support, but you do have the option to disable the default Kindnet installation, which will create a cluster without a CNI installed. After the cluster has been deployed, you need to deploy a CNI such as Calico. Since many Kubernetes installations for both small development clusters and enterprise clusters use Tigera’s Calico for the CNI, we have elected to use it as our CNI for the exercises in this book.</p>
<h3 class="heading-3" id="_idParaDest-56">Keeping track of the nesting dolls</h3>
<p class="normal">The deployment of a solution<a id="_idIndexMarker103"/> like KinD, which involves a container-in-a-container approach, can become perplexing. We compare this to the concept of Russian nesting dolls, where one doll fits inside another, and so on. As you use KinD for your own cluster, it’s possible to lose track of the communication paths between your host, Docker, and the Kubernetes nodes. To maintain clarity and sanity, it is crucial to have a thorough understanding of the location of each container and how you can interact with them.</p>
<p class="normal"><em class="italic">Figure 2.6</em> shows the three tiers and the network flow for a KinD cluster. It is crucial to recognize that each tier can solely interact with the layer immediately above it so the KinD container within the third layer can solely communicate with the Docker image running within the second layer, while the Docker image can only access the Linux host operating in the first layer. If you wish to establish direct communication between the host and a container operational within your KinD cluster, you will be required to traverse through the Docker layer before reaching the Kubernetes container in the third layer.</p>
<p class="normal">This is important to understand so that you can use KinD effectively as a testing environment:</p>
<figure class="mediaobject"><img alt="Figure 4.4 – Host cannot communicate with KinD directly " height="528" src="../Images/B21165_02_06.png" width="645"/></figure>
<p class="packt_figref">Figure 2.6: KinD network flow</p>
<p class="normal">Suppose you intend to deploy <a id="_idIndexMarker104"/>a web server to your Kubernetes cluster as an example. After successfully deploying an Ingress controller within the KinD cluster, you want to test the website using Chrome on your Docker host or another workstation on the network. However, when you attempt to access the host on port <code class="inlineCode">80</code>, the browser fails to establish a connection. Why does this issue arise?</p>
<p class="normal">The reason behind this failure is that the web server’s pod operates at layer 3 and cannot directly receive traffic from the host or network machines. To access the web server from your host, you must forward the traffic from the Docker layer to the KinD layer. Specifically, you need to enable port forwarding for port <code class="inlineCode">80</code> and port <code class="inlineCode">443</code>. When a container is initiated with port specifications, the Docker daemon assumes the responsibility of routing incoming traffic from the host to the running Docker container.</p>
<figure class="mediaobject"><img alt="Figure 4.5 – Host communicates with KinD via an Ingress controller " height="537" src="../Images/B21165_02_07.png" width="648"/></figure>
<p class="packt_figref">Figure 2.7: Host communicates with KinD via an Ingress controller</p>
<p class="normal">With ports <code class="inlineCode">80</code> and <code class="inlineCode">443</code> exposed<a id="_idIndexMarker105"/> on the Docker container, the Docker daemon will now accept incoming requests for <code class="inlineCode">80</code> and <code class="inlineCode">443</code>, allowing the NGINX Ingress controller to receive the traffic. This works because we have exposed ports <code class="inlineCode">80</code> and <code class="inlineCode">443</code> in two places, first on the Docker layer and then on the Kubernetes layer by running our NGINX controller on the host, using ports <code class="inlineCode">80</code> and <code class="inlineCode">443</code>.</p>
<p class="normal">Now, let’s look at the traffic flow<a id="_idIndexMarker106"/> for this example.</p>
<p class="normal">On the host, you make a request for a web server that has an Ingress rule in your Kubernetes cluster:</p>
<ol>
<li class="numberedList" value="1">The request looks at the IP address that was requested (in this case, the local IP address) and the traffic is sent to the Docker container running on the host.</li>
<li class="numberedList">The NGINX web server on the Docker container running our Kubernetes node listens on the IP address for ports <code class="inlineCode">80</code> and <code class="inlineCode">443</code>, so the request is accepted and sent to the running container.</li>
<li class="numberedList">The NGINX pod in your Kubernetes cluster has been configured to use the host ports <code class="inlineCode">80</code> and <code class="inlineCode">443</code>, so the traffic is forwarded to the pod.</li>
<li class="numberedList">The user receives the requested web page from the web server via the NGINX Ingress<a id="_idIndexMarker107"/> controller.</li>
</ol>
<p class="normal">This is a little confusing, but the more you use KinD and interact with it, the easier it becomes.</p>
<p class="normal">In order to utilize a KinD cluster to meet your development needs, it’s important to have an understanding of how KinD operates. So far, you have acquired knowledge about the node image and its role in cluster creation. You have also familiarized yourself with the flow of network traffic between the Docker host and the containers that run the cluster within KinD. With this foundational knowledge, we will now proceed to the first step in creating our Kubernetes cluster, installing KinD.</p>
<h1 class="heading-1" id="_idParaDest-57">Installing KinD</h1>
<p class="normal">At the time of writing, the current<a id="_idIndexMarker108"/> version of KinD is <code class="inlineCode">0.22.0</code>, supporting Kubernetes clusters up to <code class="inlineCode">1.30.x</code>.</p>
<p class="normal">The files required to deploy KinD and all of the components for the cluster that we will use for the chapters are located in the repository, under the <code class="inlineCode">chapter2</code> folder. The script, create-cluster.sh, located in the root of the <code class="inlineCode">chapter2</code> directory, will execute all of the steps discussed in the remainder of the chapter. You do not need to execute the commands as you read the chapter; you are welcome to follow the steps, but before executing the install script in the repo, you must delete any KinD clusters that may have been deployed.</p>
<p class="normal">The deployment script contains in-line remarks to explain each step; however, we will explain each step of the installation process in the next section. </p>
<h2 class="heading-2" id="_idParaDest-58">Installing KinD – prerequisites</h2>
<p class="normal">There are multiple methods<a id="_idIndexMarker109"/> available for installing KinD, but the simplest and fastest approach to begin building KinD clusters is to download the KinD binary along with the standard Kubernetes kubectl executable, which enables interaction with the cluster.</p>
<h3 class="heading-3" id="_idParaDest-59">Installing kubectl</h3>
<p class="normal">Since KinD is a single <a id="_idIndexMarker110"/>executable, it does not install <code class="inlineCode">kubectl</code>. If you do not have <code class="inlineCode">kubectl</code> installed and you use an Ubuntu 22.04 system, you can install it by running <code class="inlineCode">snap install</code>, or you can download it directly from Google.</p>
<p class="normal">To install <code class="inlineCode">kubectl</code> using <code class="inlineCode">snap</code>, you only need to run a single command:</p>
<pre class="programlisting con"><code class="hljs-con">sudo snap install kubectl --classic
</code></pre>
<p class="normal">To install <code class="inlineCode">kubectl</code> from Google directly, you need to download the binary, give it the execute permission, and move it to a location in your system’s path. This can be completed using the steps outlined below:</p>
<pre class="programlisting con"><code class="hljs-con">curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl
</code></pre>
<p class="normal">Looking at the <code class="inlineCode">curl</code> command above, you can see that the initial URL is used to find the current release, which at the time of writing was v1.30.0. Using the value returned from the <code class="inlineCode">curl</code> command, we download that release from Google storage.</p>
<p class="normal">Now that you have <code class="inlineCode">kubectl</code>, we can move on to downloading the KinD executable so that we can start to create clusters.</p>
<h2 class="heading-2" id="_idParaDest-60">Installing the KinD binary</h2>
<p class="normal">Now that we have <code class="inlineCode">kubectl</code>, we need<a id="_idIndexMarker111"/> to download the KinD binary, which is a single executable that we use to create and delete clusters. The binary for KinD v0.22.0 can be downloaded directly using the following URL: <a href="https://github.com/kubernetes-sigs/kind/releases/download/v0.22.0/kind-linux-amd64"><span class="url">https://github.com/kubernetes-sigs/kind/releases/download/v0.22.0/kind-linux-amd64</span></a>. The <code class="inlineCode">create-cluster.sh</code> script will download the binary, rename it <code class="inlineCode">kind</code>, mark it as executable, and then move it to <code class="inlineCode">/usr/bin</code>. To manually download KinD and move it to <code class="inlineCode">/usr/bin</code>, as the script does for you, you would execute the commands below:</p>
<pre class="programlisting con"><code class="hljs-con">curl -Lo ./kind https://github.com/kubernetes-sigs/kind/releases/download/v0.22.0/kind-linux-amd64
chmod +x ./kind
sudo mv ./kind /usr/bin
</code></pre>
<p class="normal">The KinD executable provides all of the options you need to maintain a cluster’s life cycle. Of course, the KinD executable can create and delete clusters, but it also provides the following capabilities:</p>
<ul>
<li class="bulletList">Can create custom a build base and node images</li>
<li class="bulletList">Can export <code class="inlineCode">kubeconfig</code> or log files</li>
<li class="bulletList">Can retrieve clusters, nodes, or <code class="inlineCode">kubeconfig</code> files</li>
<li class="bulletList">Can load images into nodes</li>
</ul>
<p class="normal">With the KinD binary successfully installed, you are now on the verge of creating your first KinD cluster.</p>
<p class="normal">Since we need a few other executables for some of the exercises in the book, the script also downloads Helm and jq. To manually download these utilities, you would execute the commands below:</p>
<pre class="programlisting con"><code class="hljs-con">curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
sudo snap install jq --classic
</code></pre>
<p class="normal">If you are new to these tools, <code class="inlineCode">Helm</code> is a Kubernetes package manager designed to streamline the deployment and administration of applications and services. It simplifies the process of creating, installing, and managing applications within a cluster. Alternatively, <code class="inlineCode">jq</code> allows you to extract, filter, transform, and format JSON data sourced from files, command outputs, and APIs. It provides a set of functionalities to work with JSON data, enabling streamlined data manipulation and analysis.</p>
<p class="normal">Now that we have the required<a id="_idIndexMarker112"/> prerequisites, we can move on to creating clusters. However, before we create our first cluster, it is important to understand the various creation options offered by KinD. Knowing the options will ensure a smooth cluster creation process.</p>
<h1 class="heading-1" id="_idParaDest-61">Creating a KinD cluster</h1>
<p class="normal">The KinD utility offers the flexibility<a id="_idIndexMarker113"/> to create both single-node clusters and more intricate setups with multiple control plane nodes and worker nodes. In this section, we will dive into the various options provided by the KinD executable. By the conclusion of this chapter, you will have a fully operational two-node cluster comprising a control plane node and a worker node.</p>
<div class="note">
<p class="normal"> <strong class="keyWord">Note</strong></p>
<p class="normal">Kubernetes cluster concepts, including the control plane and worker nodes will be covered in detail in the next chapter, <em class="chapterRef">Chapter 3:</em> <em class="italic">Kubernetes Bootcamp</em>.</p>
</div>
<p class="normal">For the exercises covered in this book, we will be setting up a multi-node cluster. The simple cluster configuration provided in the next section serves as an introductory example and should not be employed for the exercises presented in the book.</p>
<h2 class="heading-2" id="_idParaDest-62">Creating a simple cluster</h2>
<p class="normal">We will create a cluster later in the chapter, but before we do that, let’s explain how we can use KinD to create different cluster types.</p>
<p class="normal">To create a simple cluster<a id="_idIndexMarker114"/> that runs the control plane and a worker node in a single container, you only need to execute the KinD executable with the <code class="inlineCode">create cluster</code> option.</p>
<p class="normal">By executing this command, a cluster named <code class="inlineCode">kind</code> will be created, encompassing all the necessary Kubernetes components within a single Docker container. The Docker container itself will be assigned the name <code class="inlineCode">kind-control-plane</code>. If you prefer to assign a custom cluster name instead of using the default name, you can include the <code class="inlineCode">--name &lt;cluster name&gt;</code> option within the <code class="inlineCode">create cluster</code> command, for example, <code class="inlineCode">kind create cluster --name custom-cluster</code>:</p>
<pre class="programlisting con"><code class="hljs-con">Creating cluster "kind" ...
 <img alt="" height="10" src="../Images/tick.png" width="9"/> Ensuring node image (kindest/node:v1.30.0) <img alt="" height="16" src="../Images/icon_1.png" width="15"/>
 <img alt="" height="10" src="../Images/tick.png" width="9"/> Preparing nodes <img alt="" height="15" src="../Images/icon_2.png" width="15"/>
 <img alt="" height="10" src="../Images/tick.png" width="9"/> Writing configuration <img alt="" height="15" src="../Images/icon_3.png" width="15"/>
 <img alt="" height="10" src="../Images/tick.png" width="9"/> Starting control-plane <img alt="" height="15" src="../Images/icon_4.png" width="15"/>
 <img alt="" height="10" src="../Images/tick.png" width="9"/> Installing CNI <img alt="" height="15" src="../Images/icon_5.png" width="14"/>
 <img alt="" height="10" src="../Images/tick.png" width="9"/> Installing StorageClass <img alt="" height="15" src="../Images/icon_6.png" width="15"/>
Set kubectl context to "kind-kind"
You can now use your cluster with:
kubectl cluster-info --context kind-kind
Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community <img alt="" height="15" src="../Images/icon_7.png" width="15"/>
</code></pre>
<p class="normal">The <code class="inlineCode">create</code> command will create the cluster and modify the kubectl <code class="inlineCode">config</code> file. KinD will add the new cluster to your current kubectl <code class="inlineCode">config</code> file, and it will set the new cluster as the default context. If you are new to Kubernetes and the concept of context, it is the configuration that will be used to access a cluster and namespace with a set of credentials.</p>
<p class="normal">Once the cluster has been deployed, you can verify that the cluster was created successfully by listing the nodes using the <code class="inlineCode">kubectl get nodes</code> command. The command will return the running nodes in the cluster, which, for a basic KinD cluster, is a single node:</p>
<pre class="programlisting con"><code class="hljs-con">NAME                STATUS   ROLES                  AGE  VERSION
kind-control-plane  Ready    control-plane,master   32m  v1.30.0
</code></pre>
<p class="normal">The main point of deploying this single-node cluster was to show you how quickly KinD can create a cluster that you can use for testing. For our exercises, we want to split up the control plane<a id="_idIndexMarker115"/> and worker node, so we can delete this cluster using the steps in the next section.</p>
<h2 class="heading-2" id="_idParaDest-63">Deleting a cluster</h2>
<p class="normal">When you no longer need the cluster, you can delete it using the KinD <code class="inlineCode">delete</code> cluster command. The <code class="inlineCode">delete</code> command will quickly delete the cluster, including any entries related to the KinD cluster in your <code class="inlineCode">kubeconfig</code> file.</p>
<p class="normal">If you execute the delete command without providing a cluster name, it will only attempt to delete a cluster called <code class="inlineCode">kind</code>. In our previous example, we did not provide a cluster name when we created the cluster, so the default name of <code class="inlineCode">kind</code> was used. If you did name the cluster when you created it, the <code class="inlineCode">delete</code> command would require the <code class="inlineCode">--name</code> option to delete the correct cluster. For example, if we created a cluster named <code class="inlineCode">cluster01</code>, we would need to execute <code class="inlineCode">kind delete cluster</code> <code class="inlineCode">--name cluster01</code>, to delete it.</p>
<p class="normal">While a quick, single-node cluster, is useful for many use cases, you may want to create a multi-node cluster for various testing scenarios. Creating a more complex cluster, with multiple nodes, requires that you create a config file.</p>
<h2 class="heading-2" id="_idParaDest-64">Creating a cluster config file</h2>
<p class="normal">When creating a multi-node <a id="_idIndexMarker116"/>cluster, such as a two-node cluster with custom options, we need to create a cluster config file. The config file is a YAML file and the format should look familiar. Setting values in this file allows you to customize the KinD cluster, including the number of nodes, API options, and more. The config file we’ll use to create the cluster for the book is shown here – it is included in this book’s repository at <code class="inlineCode">/chapter2/cluster01-kind.yaml</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kind.x-k8s.io/v1alpha4</span>
<span class="hljs-attr">runtimeConfig:</span>
  <span class="hljs-attr">"</span><span class="hljs-attr">authentication.k8s.io/v1beta1":</span> <span class="hljs-string">"true"</span>
  <span class="hljs-attr">"admissionregistration.k8s.io/v1beta1":</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">featureGates:</span>
  <span class="hljs-attr">"ValidatingAdmissionPolicy":</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">networking:</span>
  <span class="hljs-attr">apiServerAddress:</span> <span class="hljs-string">"0.0.0.0"</span>
  <span class="hljs-attr">disableDefaultCNI:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">apiServerPort:</span> <span class="hljs-number">6443</span>
  <span class="hljs-attr">podSubnet:</span> <span class="hljs-string">"10.240.0.0/16"</span>
  <span class="hljs-attr">serviceSubnet:</span> <span class="hljs-string">"10.96.0.0/16"</span>
<span class="hljs-attr">nodes:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span>
  <span class="hljs-attr">extraPortMappings:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">2379</span>
    <span class="hljs-attr">hostPort:</span> <span class="hljs-number">2379</span>
  <span class="hljs-attr">extraMounts:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">hostPath:</span> <span class="hljs-string">/sys/kernel/security</span>
    <span class="hljs-attr">containerPath:</span> <span class="hljs-string">/sys/kernel/security</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span>
  <span class="hljs-attr">extraPortMappings:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span>
    <span class="hljs-attr">hostPort:</span> <span class="hljs-number">80</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">443</span>
    <span class="hljs-attr">hostPort:</span> <span class="hljs-number">443</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">2222</span>
    <span class="hljs-attr">hostPort:</span> <span class="hljs-number">2222</span>
  <span class="hljs-attr">extraMounts:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">hostPath:</span> <span class="hljs-string">/sys/kernel/security</span>
    <span class="hljs-attr">containerPath:</span> <span class="hljs-string">/sys/kernel/security</span>
</code></pre>
<p class="normal">Details about each of the custom<a id="_idIndexMarker117"/> options in the file are provided in the following table:</p>
<table class="table-container" id="table001-2">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Config Options</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Option Details</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">apiServerAddress</code></p>
</td>
<td class="table-cell">
<p class="normal">This configuration option tells the installation what IP address the API server will listen on. By default, it will use <code class="inlineCode">127.0.0.1</code>, but since we plan to use the cluster from other networked machines, we have selected to listen on all IP addresses.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">disableDefaultCNI</code></p>
</td>
<td class="table-cell">
<p class="normal">This setting is used to enable or disable the Kindnet installation. The default value is <code class="inlineCode">false</code>, but since we want to use Calico as our CNI, we need to set it to <code class="inlineCode">true</code>.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">podSubnet</code></p>
</td>
<td class="table-cell">
<p class="normal">Sets the CIDR range that will be used by pods.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">serviceSubnet</code></p>
</td>
<td class="table-cell">
<p class="normal">Sets the CIDR range that will be used by services.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">Nodes</code></p>
</td>
<td class="table-cell">
<p class="normal">This section is where you define the nodes for the cluster. For our cluster, we will create a single control plane node and a single worker node.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">- role: control-plane</code></p>
</td>
<td class="table-cell">
<p class="normal">The role section allows you to set options for nodes. The first role section is for the <code class="inlineCode">control-plane</code>. </p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">- role: worker</code></p>
</td>
<td class="table-cell">
<p class="normal">This is the second node section, which allows you to configure options that the worker nodes will use. Since we will deploy an Ingress controller, we have also added additional ports that will be used by the NGINX pod.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">extraPortMappings</code></p>
</td>
<td class="table-cell">
<p class="normal">To expose ports to your KinD nodes, you need to add them to the <code class="inlineCode">extraPortMappings</code> section of the configuration. Each mapping has two values, the container port and the host port. The host port is the port you would use to target the cluster, while the container port is the port that the container listens on.</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 2.1: KinD configuration options</p>
<p class="normal">Understanding the available options allows you to customize a KinD cluster according to your specific requirements. This includes incorporating advanced components like ingress controllers, which facilitate efficient routing of external traffic to services within the cluster. It also provides the ability to deploy multiple nodes within the cluster, allowing you to conduct testing and failure/recovery procedures, ensuring the resilience and stability of your cluster. By leveraging these capabilities, you can fine-tune your cluster to meet the exact demands of your applications<a id="_idIndexMarker118"/> and infrastructure.</p>
<p class="normal">Now that you know how to create a simple all-in-one container to run a cluster and create a multi-node cluster using a config file, let’s discuss a more complex cluster example.</p>
<h2 class="heading-2" id="_idParaDest-65">Multi-node cluster configuration</h2>
<p class="normal">If you only wanted a multi-node cluster<a id="_idIndexMarker119"/> without any extra<a id="_idIndexMarker120"/> options, you could create a simple configuration file that lists the number and node types you want in the cluster. The following example <code class="inlineCode">config</code> file will create a cluster with three control plane nodes and three worker nodes:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kind.x-k8s.io/v1alpha4</span>
<span class="hljs-attr">nodes:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span>
</code></pre>
<p class="normal">Incorporating multiple control plane servers introduces added complexities, since our <code class="inlineCode">kubectl</code> config file can only target a single host or IP. To make this solution work across all three control plane nodes, it is necessary to deploy a load balancer in front of our cluster. This load balancer will facilitate the distribution of control plane traffic among the control plane servers. It’s important to note that, by default, <code class="inlineCode">HAProxy</code> will not load balance any traffic between the worker nodes. To load balance traffic to worker nodes is more complex, and we will discuss it later in the chapter.</p>
<p class="normal">KinD has considered this, and if you deploy<a id="_idIndexMarker121"/> multiple control plane<a id="_idIndexMarker122"/> nodes, the installation will create an additional container running a HAProxy load balancer. During the creation of a multi-node cluster, you will see a few additional lines regarding configuring an extra load balancer, joining additional control-plane nodes and extra worker nodes – as shown in the example below, we created a cluster using the example cluster config, with three control plane and worker nodes:</p>
<pre class="programlisting con"><code class="hljs-con">Creating cluster "multinode" ...
 <img alt="" height="10" src="../Images/tick.png" width="9"/> Ensuring node image (kindest/node:v1.30.0) <img alt="" height="15" src="../Images/icon_8.png" width="15"/>
 <img alt="" height="10" src="../Images/tick.png" width="9"/> Preparing nodes <img alt="" height="15" src="../Images/icon_9.png" width="107"/>
 <img alt="" height="10" src="../Images/tick.png" width="9"/> Configuring the external load balancer <img alt="" height="15" src="../Images/icon_10.png" width="15"/>
 <img alt="" height="10" src="../Images/tick.png" width="9"/> Writing configuration <img alt="" height="15" src="../Images/icon_3.png" width="15"/>
 <img alt="" height="10" src="../Images/tick.png" width="9"/> Starting control-plane <img alt="" height="15" src="../Images/icon_12.png" width="15"/>
 <img alt="" height="10" src="../Images/tick.png" width="9"/> Installing StorageClass <img alt="" height="15" src="../Images/icon_6.png" width="15"/>
 <img alt="" height="10" src="../Images/tick.png" width="9"/> Joining more control-plane nodes <img alt="" height="11" src="../Images/icon_14.png" width="15"/>
 <img alt="" height="10" src="../Images/tick.png" width="9"/> Joining worker nodes <img alt="" height="13" src="../Images/icon_15.png" width="15"/>
Set kubectl context to "kind-multinode"
You can now use your cluster with:
kubectl cluster-info --context kind-multinode
Thanks for using kind! <img alt="" height="15" src="../Images/icon_16.png" width="15"/>
</code></pre>
<p class="normal">In the output above, you will see a line that says <code class="inlineCode">Configuring the external load balancer</code>. This step deploys a load balancer to route the incoming traffic to the API server to the three control plane nodes.</p>
<p class="normal">If we look at the running containers from a multi-node control plane config, we will see six node containers running and a HAProxy container:</p>
<table class="table-container" id="table002-1">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Container ID</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Image</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Port</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Names</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">d9107c31eedb</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kindest/haproxy:</code> <code class="inlineCode">haproxy:v20230606-42a2262b</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">0.0.0.0:6443</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">multinode-external-load-balancer</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">03a113144845</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kindest/node:v1.30.0</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">127.0.0.1:44445-&gt;6443/tcp</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">multinode-control-plane3</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">9b078ecd69b7</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kindest/node:v1.30.0</code></p>
</td>
<td class="table-cell"/>
<td class="table-cell">
<p class="normal"><code class="inlineCode">multinode-worker2</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">b779fa15206a</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kindest/node:v1.30.0</code></p>
</td>
<td class="table-cell"/>
<td class="table-cell">
<p class="normal"><code class="inlineCode">multinode-worker</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">8171baafac56</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kindest/node:v1.30.0</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">127.0.0.1:42673-&gt;6443/tcp</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">multinode-control-plane</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">3ede5e163eb0</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kindest/node:v1.30.0</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">127.0.0.1:43547-&gt;6443/tcp</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">multinode-control-plane2</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">6a85afc27cfe</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kindest/node:v1.30.0</code></p>
</td>
<td class="table-cell"/>
<td class="table-cell">
<p class="normal"><code class="inlineCode">multinode-worker3</code></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 2.2: KinD configuration options</p>
<p class="normal">Since we have a single host for KinD, each control plane node and the HAProxy container must operate on distinct ports. To enable incoming requests, it is necessary to expose each container on a unique port, since only a single process can be bound to a network port. In this scenario, you can see that port <code class="inlineCode">6443</code> is the assigned port of the HAProxy container. If you were to examine your Kubernetes configuration file, you would observe that it points to <code class="inlineCode">https://0.0.0.0:6443</code>, representing the port assigned to the HAProxy container.</p>
<p class="normal">When a command is executed using <code class="inlineCode">kubectl</code>, it is sent directly to the HAProxy server on port <code class="inlineCode">6443</code>. Using a configuration file that was created by KinD during the cluster’s creation, the HAProxy container knows how to route traffic among the three control plane nodes, providing a highly available control plane for testing.</p>
<p class="normal">The included HAProxy image<a id="_idIndexMarker123"/> is not configurable. It is only provided<a id="_idIndexMarker124"/> to handle the control plane and to load balance the control plane API traffic. Due to this limitation, if you want to use a load balancer for the worker nodes, you will need to provide your own load balancer. We will explain how to deploy a second HAproxy instance that you can use to load balance incoming traffic among multiple worker nodes later in the chapter.</p>
<p class="normal">An example where this would be typically used is when there is a need to utilize an ingress controller across multiple worker nodes. In this scenario, a load balancer would be required to accept incoming requests on ports <code class="inlineCode">80</code> and <code class="inlineCode">443</code> and distribute the traffic among the worker nodes, each hosting an instance of the ingress controller. Later in this chapter, we will show a configuration that uses a customized HAProxy setup to load balance traffic across the worker nodes</p>
<p class="normal">You will often find yourself creating clusters<a id="_idIndexMarker125"/> that may require additional API<a id="_idIndexMarker126"/> settings for your testing. In the next section, we will show you how to add extra options to your cluster, including adding options like <strong class="keyWord">OIDC values</strong> and enabling <strong class="keyWord">feature gates</strong>.</p>
<h2 class="heading-2" id="_idParaDest-66">Customizing the control plane and Kubelet options</h2>
<p class="normal">You may want to go beyond <a id="_idIndexMarker127"/>simple clusters<a id="_idIndexMarker128"/> to test features such as <strong class="keyWord">OIDC</strong> integration or Kubernetes <strong class="keyWord">feature gates</strong>.</p>
<div class="note">
<p class="normal"><strong class="keyWord">OIDC</strong> provides Kubernetes<a id="_idIndexMarker129"/> with authentication and authorization through the OpenID Connect protocol, enabling secure access to the Kubernetes cluster based on user identities.</p>
<p class="normal">A <strong class="keyWord">feature gate</strong> in Kubernetes serves<a id="_idIndexMarker130"/> as a tool to enable access to experimental or alpha-level features. It functions like a toggle switch, allowing administrators to activate or deactivate specific functionalities within Kubernetes as required.</p>
</div>
<p class="normal">This requires you to modify the startup options of components, like the API server. KinD uses the same configuration that you would use for a <code class="inlineCode">kubeadm</code> installation, allowing you to add any optional parameter that you require. As an example, if you wanted to integrate a cluster with an OIDC provider, you could add the required options to the configuration patch section:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kind.x-k8s.io/v1alpha4</span>
<span class="hljs-attr">kubeadmConfigPatches:</span>
<span class="hljs-bullet">-</span> <span class="hljs-string">|</span>
  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterConfiguration</span>
  <span class="hljs-attr">metadata:</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">config</span>
  <span class="hljs-attr">apiServer:</span>
    <span class="hljs-attr">extraArgs:</span>
      <span class="hljs-attr">oidc-issuer-url:</span> <span class="hljs-string">"https://oidc.testdomain.com/auth/idp/k8sIdp"</span>
      <span class="hljs-attr">oidc-client-id:</span> <span class="hljs-string">"kubernetes"</span>
      <span class="hljs-attr">oidc-username-claim:</span> <span class="hljs-string">sub</span>
      <span class="hljs-attr">oidc-client-id:</span> <span class="hljs-string">kubernetes</span>
      <span class="hljs-attr">oidc-ca-file:</span> <span class="hljs-string">/etc/oidc/ca.crt</span>
<span class="hljs-attr">nodes:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span>
</code></pre>
<p class="normal">This is only a small example<a id="_idIndexMarker131"/> of the type of customization you can do when deploying a KinD cluster. For a list of available configuration<a id="_idIndexMarker132"/> options, take a look at the <em class="italic">Customizing control plane configuration with kubeadm</em> page on the<a id="_idIndexMarker133"/> Kubernetes site at <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/"><span class="url">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/</span></a>.</p>
<p class="normal">Now that you have created the cluster file, let’s move on to how you use the configuration file to create your KinD cluster.</p>
<h2 class="heading-2" id="_idParaDest-67">Creating a custom KinD cluster</h2>
<p class="normal">Finally! Now that you are familiar<a id="_idIndexMarker134"/> with KinD, we can move forward and create our cluster.</p>
<p class="normal">We need to create a controlled, known environment, so we will give the cluster a name and provide a cluster config file.</p>
<p class="normal">Before you begin, make sure that you are in your cloned repository in the <code class="inlineCode">chapter2</code> directory. You will create the entire cluster using our supplied script, <code class="inlineCode">create-cluster.sh.</code></p>
<p class="normal">This script will create a cluster using a configuration file called <code class="inlineCode">cluster01-kind.yaml</code>, which will create a cluster called <code class="inlineCode">cluster01</code> with a control plane and worker node, exposing ports <code class="inlineCode">80</code> and <code class="inlineCode">443</code> on the worker node for our <code class="inlineCode">ingress</code> controller.</p>
<p class="normal">Rather than providing each step in the chapter, we have documented the script itself. You can read what each step does when you look at the source code for the script. Below is a high-level list of the steps that are executed by the script:</p>
<ol>
<li class="numberedList" value="1">Downloads the KinD v 0.22.0 binary, makes it executable, and moves it to <code class="inlineCode">/usr/bin</code>.</li>
<li class="numberedList">Downloads <code class="inlineCode">kubectl</code>, make it executable, and moves it to <code class="inlineCode">/usr/bin</code>.</li>
<li class="numberedList">Downloads the <strong class="keyWord">Helm</strong> installation script and executes it.</li>
<li class="numberedList">Installs <code class="inlineCode">jq</code>.</li>
<li class="numberedList">Executes KinD to create our cluster using the config file and declaring the image to use (we do this to avoid any issues with newer releases and our chapter scripts).</li>
<li class="numberedList">Labels the worker node for ingress.</li>
<li class="numberedList">Uses the two manifests, <code class="inlineCode">custom-resources.yaml</code> and <code class="inlineCode">tigera-operator.yaml</code>, in the <code class="inlineCode">chapter2/calico</code> to deploy <strong class="keyWord">Calico</strong>.</li>
<li class="numberedList">Deploys the NGINX Ingress using the <code class="inlineCode">nginx-deploy.yaml</code> manifest in the <code class="inlineCode">chapter2/nginx-ingress</code> directory.</li>
</ol>
<p class="normal">The manifests we used in steps 7 and 8 are the standard deployment manifests from both the Calico and NGINX-Ingress projects. We will store them in the repository to make the deployments quicker, and also to avoid any issues if either deployment is updated with options<a id="_idIndexMarker135"/> that may fail on our KinD cluster.</p>
<p class="normal">Congratulations! You now have a fully functioning, two-node Kubernetes cluster running Calico with an Ingress controller.</p>
<h1 class="heading-1" id="_idParaDest-68">Reviewing your KinD cluster</h1>
<p class="normal">Now that you have a fully functional Kubernetes<a id="_idIndexMarker136"/> cluster, we can dive into the realm of a few key Kubernetes objects, specifically storage objects. In the next chapter, <em class="italic">Kubernetes Bootcamp</em>, we will get deeper into the other objects that are available in a Kubernetes cluster. While that chapter will explore the large list of objects available in a cluster, it is important to introduce the storage-related objects at this point, since KinD provides storage capabilities.</p>
<p class="normal">In this section, we shall acquaint ourselves with the storage objects seamlessly integrated within KinD. These purpose-built storage objects extend persistent storage capabilities to your workloads within the cluster, ensuring data persistence and resilience. By familiarizing ourselves with these storage objects, we lay the foundation for seamless data management within the Kubernetes ecosystem.</p>
<h2 class="heading-2" id="_idParaDest-69">KinD storage objects</h2>
<p class="normal">Remember that KinD includes<a id="_idIndexMarker137"/> Rancher’s auto-provisioner to provide automated persistent disk management for the cluster. Kubernetes has a number of storage objects, but there is one object that the auto-provisioner does not require, since it uses a base Kubernetes feature: a <code class="inlineCode">CSIdriver</code> object. Since the ability to use local host paths as PVCs is part of Kubernetes, we will not see any <code class="inlineCode">CSIdriver</code> objects for local storage in our KinD cluster.</p>
<p class="normal">The first object in our KinD cluster we will discuss is <code class="inlineCode">CSInodes</code>. Any node that can run a workload will have a <code class="inlineCode">CSInode</code> object. On our KinD clusters, both nodes have a <code class="inlineCode">CSInode</code> object, which you can verify by executing <code class="inlineCode">kubectl get csinodes</code>:</p>
<pre class="programlisting con"><code class="hljs-con">NAME                      DRIVERS   AGE
cluster01-control-plane   0         20m
cluster01-worker          0         20m
</code></pre>
<p class="normal">If we were to describe one of the nodes using <code class="inlineCode">kubectl describe csinodes &lt;node name&gt;</code>, you would see the details of the object:</p>
<pre class="programlisting con"><code class="hljs-con">Name:               cluster01-worker
Labels:             &lt;none&gt;
Annotations:        storage.alpha.kubernetes.io/migrated-plugins:
                      kubernetes.io/aws-ebs,kubernetes.io/azure-disk,kubernetes.io/azure-file,kubernetes.io/cinder,kubernetes.io/gce-pd,kubernetes.io/vsphere-vo...
CreationTimestamp:  Tue, 20 Jun 2023 16:45:54 +0000
Spec:
  Drivers:
    csi.tigera.io:
      Node ID:  cluster01-worker
Events:         &lt;none&gt;
</code></pre>
<p class="normal">The main thing to point out is the <code class="inlineCode">Spec</code> section of the output. This lists the details of any drivers that may be installed to support backend storage systems. In the driver section, you will see an entry for a driver called <code class="inlineCode">csi.tigera.io</code>, which was deployed when we installed Calico. This driver is used by Calico to enable secure connections between Calico’s <strong class="keyWord">Felix</strong>, which handles<a id="_idIndexMarker138"/> network policy<a id="_idIndexMarker139"/> enforcement, and <strong class="keyWord">Dikastes</strong>, which manages Kubernetes network policy translation and enforcement pods by mounting a shared volume.</p>
<p class="normal">It is important to note<a id="_idIndexMarker140"/> that this driver is not used by standard Kubernetes deployments for persistent storage.</p>
<p class="normal">Since the local-provisioner does not require a driver, we will not see an additional driver on our cluster for the local storage.</p>
<h2 class="heading-2" id="_idParaDest-70">Storage drivers</h2>
<p class="normal">A storage driver in Kubernetes <a id="_idIndexMarker141"/>plays an important role in handling the communication between containerized applications and the underlying storage infrastructure. Its primary function is to control the provisioning, attachment, and management of storage resources for applications deployed in Kubernetes clusters.</p>
<p class="normal">As we already mentioned, your KinD cluster does not require any additional storage drivers for the local-provisioner, but we do have a driver for Calico’s communication. If you execute <code class="inlineCode">kubectl get csidrivers</code>, you will see<a id="_idIndexMarker142"/> the <code class="inlineCode">csi.tigera.io</code> in the list.</p>
<h2 class="heading-2" id="_idParaDest-71">KinD storage classes</h2>
<p class="normal">To attach to any cluster-provided<a id="_idIndexMarker143"/> storage, the cluster requires a <code class="inlineCode">StorageClass</code> object. Rancher’s provider creates a default storage class called <code class="inlineCode">standard</code>. It also sets the class as the default <code class="inlineCode">StorageClass</code>, so you do not need to provide a <code class="inlineCode">StorageClass</code> name in your PVC requests. If a default <code class="inlineCode">StorageClass</code> is not set, every PVC request would require a <code class="inlineCode">StorageClass</code> name in the request. If a default class is not enabled and a PVC request fails to set a <code class="inlineCode">StorageClass</code> name, the PVC allocation will fail, since the API server won’t be able to link the request to a <code class="inlineCode">StorageClass</code>.</p>
<p class="normal">In a production cluster, it is recommended to avoid setting a default <code class="inlineCode">StorageClass</code>. This approach helps prevent potential issues that can arise when deployments forget to specify a class and the default storage system does not meet the deployment requirements. Such issues may only surface when they become critical in a production environment, impacting business revenue and reputation. By not assigning a default class, developers will encounter a failed PVC request, prompting the identification of the problem before any negative impact on the business. Additionally, this approach encourages developers to explicitly select a <code class="inlineCode">StorageClass</code> that aligns with the desired performance requirement, enabling them to use cost-effective storage for non-critical systems or high-speed storage for critical workloads.</p>
<p class="normal">To list the storage classes on the cluster, execute <code class="inlineCode">kubectl get storageclasses</code>, or use the shortened version with <code class="inlineCode">sc</code> instead of <code class="inlineCode">storageclasses</code>:</p>
<pre class="programlisting con"><code class="hljs-con">NAME            ATTACHREQUIRED   PODINFOOwhoNT   STORAGECAPACITY  
csi.tigera.io   true             true             false            
</code></pre>
<p class="normal">Now that you know about the objects that Kubernetes uses for storage, let’s learn how to use the provisioner.</p>
<h2 class="heading-2" id="_idParaDest-72">Using KinD’s Storage Provisioner</h2>
<p class="normal">Using the included provisioner<a id="_idIndexMarker144"/> is very simple. Since it can auto-provision the storage and is set as the default class, any PVC requests that come in are seen by the provisioning pod, which then creates the <code class="inlineCode">PersistentVolume</code> and <code class="inlineCode">PersistentVolumeClaim</code>.</p>
<p class="normal">To show this process, let’s go through the necessary steps. The following is the output of running <code class="inlineCode">kubectl get pv</code> and <code class="inlineCode">kubectl get pvc</code> on a base KinD cluster:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get pv
No resources found
</code></pre>
<p class="normal"><code class="inlineCode">PVs</code> are not namespaced objects, meaning they are cluster-level resources, so we don’t need to add a namespace option to the command. PVCs are namespaced objects, so when we tell Kubernetes to show the PVs that are available, we need to specify the namespace with the <code class="inlineCode">kubectl get pvc</code> command. Since this is a new cluster and none of the default workloads require a persistent disk, there are currently no PV or PVC objects.</p>
<p class="normal">Without an auto-provisioner, we would need<a id="_idIndexMarker145"/> to create a PV before a PVC could claim the volume. Since we have the Rancher provisioner running in our cluster, we can test the creation process by deploying a pod with a PVC request, like the one listed here, which we will name <code class="inlineCode">pvctest.yaml</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">test-claim</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">accessModes:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>
  <span class="hljs-attr">resources:</span>
    <span class="hljs-attr">requests:</span>
      <span class="hljs-attr">storage:</span> <span class="hljs-string">1Mi</span>
</code></pre>
<p class="normal">This PVC will be named <code class="inlineCode">test-claim</code> in the default namespace, since we didn’t provide a namespace, and its volume is set at 1 MB. Again, we do need to include the <code class="inlineCode">StorageClass</code> option, since KinD has set a default <code class="inlineCode">StorageClass</code> for the cluster.</p>
<p class="normal">To generate the PVC, we can execute a <code class="inlineCode">kubectl</code> command by using the <code class="inlineCode">create</code> command, along with the <code class="inlineCode">pvctest.yaml</code> file, <code class="inlineCode">kubectl create -f pvctest.yaml</code>. Kubernetes will respond by confirming the creation of the PVC. However, it is crucial to understand that this acknowledgment does not guarantee the PVC’s complete functionality. While the PVC object itself was successfully created, it is possible that certain dependencies within the PVC request may be incorrect or missing. In such cases, although the object is created, the PVC request itself will not be fulfilled and may fail.</p>
<p class="normal">After creating a PVC, you can check the real status using one of two options. The first is a simple <code class="inlineCode">get</code> command – that is, <code class="inlineCode">kubectl get pvc</code>. Since our request is in the default namespace, I don’t need to include a namespace value in the <code class="inlineCode">get</code> command (note that we had to shorten the volume’s name so that it fits the page):</p>
<pre class="programlisting con"><code class="hljs-con">NAME       STATUS   VOLUME         CAPACITY   ACCESS MODES  STORAGECLASS   AGE
test-claim Bound    pvc-b6ecf50…   1Mi        RWO           standard       15s
</code></pre>
<p class="normal">We know that we created a PVC object by submitting the PVC manifest, but we did not create a PV request. If we look at the PVs now, we can see that a single PV was created from our PVC request. Again, we shortened the PV name in order to fit the output on a single line:</p>
<pre class="programlisting con"><code class="hljs-con">NAME        CAPACITY  ACCESS MODES RECLAIM POLICY STATUS   CLAIM
pvc-b6ecf…  1Mi       RWO          Delete         Bound    default/test-claim
</code></pre>
<p class="normal">Given the increasing number of workloads that rely on persistent disks, it’s important to have a clear understanding of how Kubernetes workloads integrate with storage systems. In the previous section, you gained insights into how KinD enhances the cluster with the auto-provisioner. In <em class="chapterRef">Chapter 3</em>, <em class="italic">Kubernetes Bootcamp</em>, we will further strengthen<a id="_idIndexMarker146"/> our understanding of these Kubernetes storage objects.</p>
<p class="normal">In the next section, we will discuss the complex topic of using a load balancer with our KinD cluster to enable highly available clusters, using HAproxy. </p>
<h1 class="heading-1" id="_idParaDest-73">Adding a custom load balancer for Ingress</h1>
<p class="normal">We added this section for anybody<a id="_idIndexMarker147"/> who may want to know<a id="_idIndexMarker148"/> more about how to load balance between multiple worker nodes.</p>
<p class="normal">This section disccusses a complex topic that covers adding a custom HAProxy container that you can use to load balance worker nodes in a KinD cluster. You should not deploy this on the KinD cluster that we will use for the remaining chapters.</p>
<p class="normal">Since you will interact with load balancers in most enterprise environments, we wanted to add a section on how to configure your own HAProxy container for worker nodes, in order to load balance between three KinD nodes.</p>
<p class="normal">First, we will not use this configuration for any of the chapters in this book. We want to make the exercises available to everyone, so to limit the required resources, we will always use the two-node cluster that we created earlier in this chapter. If you want to test KinD nodes with a load balancer, we suggest using a different Docker host or waiting until<a id="_idIndexMarker149"/> you have finished this book and deleted<a id="_idIndexMarker150"/> your KinD cluster.</p>
<h2 class="heading-2" id="_idParaDest-74">Creating the KinD cluster configuration</h2>
<p class="normal">We have provided<a id="_idIndexMarker151"/> a script, <code class="inlineCode">create-multinode.sh</code>, located in the <code class="inlineCode">chapter2/HAdemo</code> directory, that will create a cluster with three nodes for both the control plane and worker nodes. The script will create a cluster named <code class="inlineCode">multimode</code>, which means the control plane nodes will be named <code class="inlineCode">multinode-control-plane</code>, <code class="inlineCode">multinode-control-plane2</code>, and <code class="inlineCode">multinode-control-plane3</code>, while the worker nodes will be named <code class="inlineCode">multinode-worker</code>, <code class="inlineCode">multinode-worker2</code>, and <code class="inlineCode">multinode-worker3</code>. </p>
<p class="normal">Since we will use a HAProxy container exposed on ports <code class="inlineCode">80</code> and <code class="inlineCode">443</code> on your Docker host, you do not need to expose any worker node ports in your <code class="inlineCode">KinD</code> config file. The config file we use in the script is shown below:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kind.x-k8s.io/v1alpha4</span>
<span class="hljs-attr">networking:</span>
  <span class="hljs-attr">apiServerAddress:</span> <span class="hljs-string">"0.0.0.0"</span>
  <span class="hljs-attr">disableDefaultCNI:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">apiServerPort:</span> <span class="hljs-number">6443</span>
  <span class="hljs-attr">podSubnet:</span> <span class="hljs-string">"10.240.0.0/16"</span>
  <span class="hljs-attr">serviceSubnet:</span> <span class="hljs-string">"</span><span class="hljs-string">10.96.0.0/16"</span>
<span class="hljs-attr">nodes:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span>
</code></pre>
<p class="normal">Notice that we do not expose the worker nodes on any ports for ingress, so there is no need to expose the ports directly on the nodes. Once we deploy the HAProxy container, it will be exposed on ports <code class="inlineCode">80</code> and <code class="inlineCode">443</code>, and since it’s on the same host as the KinD cluster, the HAProxy container will be able to communicate with the nodes using the Docker network.</p>
<p class="normal">At this point, you should have a working multi-node cluster, with a load balancer for the API server and another load balancer for your worker nodes. One of the worker nodes will run an NGINX ingress controller, but it could be any of the three nodes, so how does the HAProxy server know which node is running NGINX? It does a health check against all nodes, and any node that replies with 200 (a successful connection) is running NGINX and is added<a id="_idIndexMarker152"/> to the backend(s) server list.</p>
<p class="normal">In the next section, we will explain the configuration file that HAProxy uses to control the backend server(s) and the health checks that are executed.</p>
<h2 class="heading-2" id="_idParaDest-75">The HAProxy configuration file</h2>
<p class="normal">HAProxy offers a container<a id="_idIndexMarker153"/> on Docker Hub that is easy to deploy, requiring only a config file to start the container.</p>
<p class="normal">To create the configuration file, you will need to know the IP addresses of each worker node in the cluster. The included script that creates the cluster and deploys HAProxy will find this information for you, create the config file, and start the HAProxy container.</p>
<p class="normal">Since configuring HAProxy may be unfamiliar to many people, we will provide a breakdown of the script, explaining the main sections that we configured. The script creates the file for us by querying the IP address of the worker node containers:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-string">global</span>  <span class="hljs-string">log</span> <span class="hljs-string">/dev/log</span> <span class="hljs-string">local0</span>  <span class="hljs-string">log</span> <span class="hljs-string">/dev/log</span> <span class="hljs-string">local1</span> <span class="hljs-string">notice</span>  <span class="hljs-string">daemon</span>
<span class="hljs-string">defaults</span>  <span class="hljs-string">log</span> <span class="hljs-string">global</span>  <span class="hljs-string">mode</span> <span class="hljs-string">tcp</span>  <span class="hljs-string">timeout</span> <span class="hljs-string">connect</span> <span class="hljs-number">5000</span>  <span class="hljs-string">timeout</span> <span class="hljs-string">client</span> <span class="hljs-number">50000</span>  <span class="hljs-string">timeout</span> <span class="hljs-string">server</span> <span class="hljs-number">50000</span>
<span class="hljs-string">frontend</span> <span class="hljs-string">workers_https</span>  <span class="hljs-string">bind</span> <span class="hljs-string">*:443</span>  <span class="hljs-string">mode</span> <span class="hljs-string">tcp</span>  <span class="hljs-string">use_backend</span> <span class="hljs-string">ingress_https</span> <span class="hljs-string">backend</span> <span class="hljs-string">ingress_https</span>  <span class="hljs-string">option</span> <span class="hljs-string">httpchk</span> <span class="hljs-string">GET</span> <span class="hljs-string">/healthz</span>  <span class="hljs-string">mode</span> <span class="hljs-string">tcp</span>  <span class="hljs-string">server</span> <span class="hljs-string">worker</span> <span class="hljs-number">172.18.0.8</span><span class="hljs-string">:443</span> <span class="hljs-string">check</span> <span class="hljs-string">port</span> <span class="hljs-number">80</span>  <span class="hljs-string">server</span> <span class="hljs-string">worker2</span> <span class="hljs-number">172.18.0.7</span><span class="hljs-string">:443</span> <span class="hljs-string">check</span> <span class="hljs-string">port</span> <span class="hljs-number">80</span>  <span class="hljs-string">server</span> <span class="hljs-string">worker3</span> <span class="hljs-number">172.18.0.4</span><span class="hljs-string">:443</span> <span class="hljs-string">check</span> <span class="hljs-string">port</span> <span class="hljs-number">80</span>
<span class="hljs-string">frontend</span> <span class="hljs-string">stats</span>
  <span class="hljs-string">bind</span> <span class="hljs-string">*:8404</span>
  <span class="hljs-string">mode</span> <span class="hljs-string">http</span>
  <span class="hljs-string">stats</span> <span class="hljs-string">enable</span>
  <span class="hljs-string">stats</span> <span class="hljs-string">uri</span> <span class="hljs-string">/</span>
  <span class="hljs-string">stats</span> <span class="hljs-string">refresh</span> <span class="hljs-string">10s</span>
<span class="hljs-string">frontend</span> <span class="hljs-string">workers_http</span>  <span class="hljs-string">bind</span> <span class="hljs-string">*:80</span>  <span class="hljs-string">use_backend</span> <span class="hljs-string">ingress_http</span> <span class="hljs-string">backend</span> <span class="hljs-string">ingress_http</span>  <span class="hljs-string">mode</span> <span class="hljs-string">http</span>  <span class="hljs-string">option</span> <span class="hljs-string">httpchk</span> <span class="hljs-string">GET</span> <span class="hljs-string">/healthz</span>  <span class="hljs-string">server</span> <span class="hljs-string">worker</span> <span class="hljs-number">172.18.0.8</span><span class="hljs-string">:80</span> <span class="hljs-string">check</span> <span class="hljs-string">port</span> <span class="hljs-number">80</span>  <span class="hljs-string">server</span> <span class="hljs-string">worker2</span> <span class="hljs-number">172.18.0.7</span><span class="hljs-string">:80</span> <span class="hljs-string">check</span> <span class="hljs-string">port</span> <span class="hljs-number">80</span>  <span class="hljs-string">server</span> <span class="hljs-string">worker3</span> <span class="hljs-number">172.18.0.4</span><span class="hljs-string">:80</span> <span class="hljs-string">check</span> <span class="hljs-string">port</span> <span class="hljs-number">80</span>
</code></pre>
<p class="normal">The frontend sections are key to the configuration. This tells HAProxy the port to bind to and the server group to use for the backend traffic. Let’s look at the first frontend entry:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-string">frontend</span> <span class="hljs-string">workers_https</span>
 <span class="hljs-string">bind</span> <span class="hljs-string">*:443</span>
 <span class="hljs-string">mode</span> <span class="hljs-string">tcp</span>
 <span class="hljs-string">use_backend</span> <span class="hljs-string">ingress_https</span>
</code></pre>
<p class="normal">This binds a frontend called <code class="inlineCode">workers_https</code> to TCP port <code class="inlineCode">443</code>. The last line, <code class="inlineCode">use_backend</code>, tells HAProxy which server group will receive traffic on port <code class="inlineCode">443</code>.</p>
<p class="normal">Next, we declare the backend servers, or the collection of nodes that will be running the workloads for the desired port or URL. The first backend section contains the servers that are part of the <code class="inlineCode">workers_https</code> group.</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-string">backend</span> <span class="hljs-string">ingress_https</span>
 <span class="hljs-string">option</span> <span class="hljs-string">httpchk</span> <span class="hljs-string">GET</span> <span class="hljs-string">/healthz</span>
 <span class="hljs-string">mode</span> <span class="hljs-string">tcp</span>
 <span class="hljs-string">server</span> <span class="hljs-string">worker</span> <span class="hljs-number">172.18.0.8</span><span class="hljs-string">:443</span> <span class="hljs-string">check</span> <span class="hljs-string">port</span> <span class="hljs-number">443</span>
 <span class="hljs-string">server</span> <span class="hljs-string">worker2</span> <span class="hljs-number">172.18.0.7</span><span class="hljs-string">:443</span> <span class="hljs-string">check</span> <span class="hljs-string">port</span> <span class="hljs-number">443</span>
 <span class="hljs-string">server</span> <span class="hljs-string">worker3</span> <span class="hljs-number">172.18.0.4</span><span class="hljs-string">:443</span> <span class="hljs-string">check</span> <span class="hljs-string">port</span> <span class="hljs-number">443</span>
</code></pre>
<p class="normal">The first line contains the name<a id="_idIndexMarker154"/> of the rule; in our example, we have called the rule <code class="inlineCode">ingress-https</code>. The option <code class="inlineCode">httpchk</code> tells HAProxy how to health check each of the backend servers. If the check is successful, HAProxy will add it as a healthy backend target. If the check fails, the server will not direct any traffic to the failed node(s). Finally, we provide the list of servers; each endpoint has its own line that starts with the server, followed by the name, IP address, and the port to check – in our example, port <code class="inlineCode">443</code>.</p>
<p class="normal">You can use a similar block for any other port that you want HAProxy to load balance. In our script, we configure HAProxy to listen on TCP ports <code class="inlineCode">80</code> and <code class="inlineCode">443</code>, using the same backend servers.</p>
<p class="normal">We have also added a backend section to expose the HAProxy status page. The status page must be exposed via HTTP, and it runs on port <code class="inlineCode">8404</code>. This doesn’t need to be forwarded to any group of servers, since the status page is part of the Docker container itself. We only need to add it to the configuration file, and when we execute the HAProxy container, we need to add the port mapping for port <code class="inlineCode">8404</code> (you will see that in the <code class="inlineCode">docker run</code> command that is described in the next paragraph). We will show the status page and how to use it in the next section.</p>
<p class="normal">The final step is to start a Docker container that runs HAProxy with our created configuration file containing the three worker nodes, exposed on the Docker host on ports <code class="inlineCode">80</code> and <code class="inlineCode">443</code>, and connected<a id="_idIndexMarker155"/> to the KinD network in Docker:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Start the HAProxy Container for the Worker Nodes</span>
<span class="hljs-string">docker</span> <span class="hljs-string">run</span> <span class="hljs-string">--name</span> <span class="hljs-string">HAProxy-workers-lb</span> <span class="hljs-string">--network</span> <span class="hljs-string">$KIND_NETWORK</span> <span class="hljs-string">-d</span> <span class="hljs-string">-p</span> <span class="hljs-number">8404</span><span class="hljs-string">:8404</span> <span class="hljs-string">-p</span> <span class="hljs-number">80</span><span class="hljs-string">:80</span> <span class="hljs-string">-p</span> <span class="hljs-number">443</span><span class="hljs-string">:443</span> <span class="hljs-string">-v</span> <span class="hljs-string">~/HAProxy:/usr/local/etc/HAProxy:ro</span> <span class="hljs-string">haproxy</span> <span class="hljs-string">-f</span> <span class="hljs-string">/usr/local/etc/HAProxy/HAProxy.cfg</span>
</code></pre>
<p class="normal">Now that you have learned how to create and deploy a custom HAProxy load balancer for your worker nodes, let’s look at how HAProxy communication works.</p>
<h2 class="heading-2" id="_idParaDest-76">Understanding HAProxy traffic flow</h2>
<p class="normal">The cluster will have a total<a id="_idIndexMarker156"/> of eight containers running. Six of these containers will be the standard Kubernetes components – that is, three control plane servers and three worker nodes. The other two containers are KinD’s HAProxy server and your own custom HAProxy container (the <code class="inlineCode">docker ps</code> output has been shortened due to formatting):</p>
<table class="table-container" id="table003-1">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">CONTAINER</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">ID</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">NAMES</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">3d876a9f8f02</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">Haproxy</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">HAProxy-workers-lb</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">183e86be2be3</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kindest/node:v1.30.1</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">multinode-worker3</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">ce2d2174a2ba</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kindest/haproxy:v20230606-42a2262b</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">multinode-external-load-balancer</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">697b2c2bef68</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kindest/node:v1.30.1</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">multinode-control-plane</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">f3938a66a097</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kindest/node:v1.30.1</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">multinode-worker2</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">43372928d2f2</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kindest/node:v1.30.1</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">multinode-control-plane2</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">baa450f8fe56</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kindest/node:v1.30.1</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">multinode-worker</code></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><code class="inlineCode">ee4234ff4333</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">kindest/node:v1.30.1</code></p>
</td>
<td class="table-cell">
<p class="normal"><code class="inlineCode">multinode-control-plane3</code></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 2.3: Cluster having eight containers running</p>
<p class="normal">The container named <code class="inlineCode">HAProxy-workers-lb</code> container is exposed on the host ports <code class="inlineCode">80</code> and <code class="inlineCode">443</code>. This means that any incoming requests to the host on port <code class="inlineCode">80</code> or <code class="inlineCode">443</code> will be directed to the custom HAProxy container, which will then send the traffic to the Ingress controller.</p>
<p class="normal">The default NGINX Ingress deployment only has a single replica, which means that the controller runs on a single node, but it could move to any of the other nodes at any time. Let’s use the HAProxy status page that we mentioned in the previous section to see where the Ingress controller is running. Using a browser, we need to point to our Docker host’s IP address on port <code class="inlineCode">8404</code>. For our example, the host is on <code class="inlineCode">192.168.149.129</code>, so in our browser, we would enter <code class="inlineCode">http://192.168.149.129:8404</code>, which will bring up the HAProxy status page, similar to what is shown in <em class="italic">Figure 2.8</em> below:</p>
<figure class="mediaobject"><img alt="" height="256" src="../Images/B21165_02_08.png" width="878"/></figure>
<p class="packt_figref">Figure 2.8: HAProxy status page</p>
<p class="normal">In the status page details, you<a id="_idIndexMarker157"/> will see the backends that we created in our HAProxy configuration and the status of each service, including which worker node is running the ingress controller. To explain this in more detail, let’s focus on the details of the incoming SSL traffic. On the status page, we will focus on the <strong class="screenText">ingress_https</strong> section, as shown in <em class="italic">Figure 2.9</em>.</p>
<figure class="mediaobject"><img alt="" height="193" src="../Images/B21165_02_09.png" width="878"/></figure>
<p class="packt_figref">Figure 2.9: HAProxy HTTPS Status</p>
<p class="normal">In the HAProxy config, we created a backend called <code class="inlineCode">ingress_https</code> that includes all of the worker nodes in the cluster. Since we only have a single replica running for the controller, only one node will run the ingress controller. In the list of nodes, you will see that two of them are in a DOWN state, while <code class="inlineCode">worker2</code> is in an UP state. The DOWN state is expected, since the health check for HTTPS will fail on any node that isn’t running a replica of the ingress controller.</p>
<p class="normal">While we would run at least three replicas<a id="_idIndexMarker158"/> in production, we only have three nodes, and we want to show how <code class="inlineCode">HAproxy</code> will update the backend services when the ingress controller pod moves from the active node to a new node. So, we will simulate a node failure to prove that HAProxy provides high availability to our NGINX ingress controller.</p>
<h2 class="heading-2" id="_idParaDest-77">Simulating a kubelet failure</h2>
<p class="normal">In our example, we want to prove<a id="_idIndexMarker159"/> that HAProxy provides HA support for NGINX. To simulate a failure, we can stop the kubelet service on a node, which will alert the <code class="inlineCode">kube-apisever</code> so that it doesn’t schedule any additional pods on the node. We know that the running container is on <code class="inlineCode">worker2</code>, so that’s the node we want to take down.</p>
<p class="normal">The easiest way to stop <code class="inlineCode">kubelet</code> is to send a <code class="inlineCode">docker exec</code> command to the container:</p>
<pre class="programlisting con"><code class="hljs-con">docker exec multinode-worker2 systemctl stop kubelet
</code></pre>
<p class="normal">You will not see any output from this command, but if you wait a few minutes for the cluster to receive the updated node status, you can verify the node is down by looking at a list of nodes:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get nodes
</code></pre>
<p class="normal">You will receive the following output:</p>
<pre class="programlisting con"><code class="hljs-con">NAME                       		STATUS     	ROLES           		AGE	VERSION
multinode-control-plane    Ready      	control-plane		29m  	v1.30.0
multinode-control-plane2   Ready      	control-plane		29m  	v1.30.0
multinode-control-plane3   Ready      	control-plane		29m  	v1.30.0
multinode-worker           	Ready      	&lt;none&gt;          		28m   	v1.30.0
multinode-worker2          	NotReady   	&lt;none&gt;          		28m   	v1.30.0
multinode-worker3          	Ready      	&lt;none&gt;          		28m   	v1.30.0
</code></pre>
<p class="normal">This verifies that we just simulated a <code class="inlineCode">kubelet</code> failure and that <code class="inlineCode">worker2</code> is now down, in a <code class="inlineCode">NotReady</code> status.</p>
<p class="normal">Any pods that were running before the <code class="inlineCode">kubelet</code> “failure” will continue to run, but <code class="inlineCode">kube-scheduler</code> will not schedule any workloads on the node until the <code class="inlineCode">kubelet</code> issue is resolved. Since we know the pod will not restart on the node, we can delete the pod so that it can be rescheduled on a different node.</p>
<p class="normal">You need to get the pod name and then delete it to force a restart:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl get pods -n ingress-nginx
</code></pre>
<p class="normal">This will return the pods in the namespace, such as the following:</p>
<pre class="programlisting con"><code class="hljs-con">nginx-ingress-controller-7d6bf88c86-r7ztq
</code></pre>
<p class="normal">Delete the ingress controller pod using <code class="inlineCode">kubectl</code>:</p>
<pre class="programlisting con"><code class="hljs-con">kubectl delete pod nginx-ingress-controller-7d6bf88c86-r7ztq -n ingress-nginx
</code></pre>
<p class="normal">This will force the scheduler to start the container on another worker node. It will also cause the HAProxy container to update the backend list, since the NGINX controller has moved to another worker node.</p>
<p class="normal">To prove this, we can look at the HAproxy status page, and you see that the active node has changed to <strong class="screenText">worker3</strong>. Since the failure that we simulated was for <strong class="screenText">worker2</strong>, when we killed the pod, Kubernetes<a id="_idIndexMarker160"/> rescheduled the pod to start up on another, healthy, node.</p>
<figure class="mediaobject"><img alt="" height="189" src="../Images/B21165_02_10.png" width="536"/></figure>
<p class="packt_figref">Figure 2.10: HAproxy backend node update</p>
<p class="normal">If you plan to use this HA cluster for additional tests, you will want to restart the kubelet on <code class="inlineCode">multinode-worker2</code>.</p>
<p class="normal">If you plan to delete the HA cluster, you can just run a KinD cluster delete, and all the nodes will be deleted. Since we called the cluster <code class="inlineCode">multinode</code>, you would run the following command to delete the KinD cluster:</p>
<pre class="programlisting con"><code class="hljs-con">kind delete cluster –name multinode
</code></pre>
<p class="normal">You will also need to delete the HAProxy container that we deployed for the worker nodes, since we executed that container from Docker and it was not created by the KinD deployment. To clean up the worker nodes’ HAproxy deployment, execute the commands below:</p>
<pre class="programlisting con"><code class="hljs-con">docker stop HAProxy-workers-lb &amp;&amp; docker rm HAProxy-workers-lb
</code></pre>
<p class="normal">That completes this KinD chapter! We mentioned a lot of different <code class="inlineCode">Kubernetes</code> services in this chapter, but we only scratched the surface of the objects included in clusters. In the next section, we will go through a bootcamp on what components make up a cluster and provide<a id="_idIndexMarker161"/> an overview of the base objects in a cluster.</p>
<h1 class="heading-1" id="_idParaDest-78">Summary</h1>
<p class="normal">This chapter provided an overview of the KinD project, a Kubernetes SIG project. We covered the process of installing optional components in a KinD cluster, such as Calico for CNI and NGINX for Ingress control. Additionally, we explored the Kubernetes storage objects that are included with a KinD cluster.</p>
<p class="normal">You should now understand the potential benefits that KinD can bring to you and your organization. It offers a user-friendly and highly customizable Kubernetes cluster deployment, and the number of clusters on a single host is only limited by the available host resources.</p>
<p class="normal">In the next chapter, we will dive into Kubernetes objects. We’ve called the next chapter <em class="italic">Kubernetes Bootcamp</em>, since it will cover the majority of the basic Kubernetes objects and what each one is used for. The next chapter can be considered a “Kubernetes pocket guide.” It contains a quick reference to Kubernetes objects and what they do, as well as when to use them.</p>
<p class="normal">It’s a packed chapter and is designed to be a refresher for those of you who have experience with Kubernetes; alternatively, it’s a crash course for those of you who are new to Kubernetes. Our intention with this book is to go beyond the basic Kubernetes objects, since there are many books on the market today that cover the basics of Kubernetes very well.</p>
<h1 class="heading-1" id="_idParaDest-79">Questions</h1>
<ol>
<li class="numberedList" value="1">What object must be created before you can create a <code class="inlineCode">PersistentVolumeClaim</code>?<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">PVC</li>
<li class="alphabeticList level-2">A disk</li>
<li class="alphabeticList level-2"><code class="inlineCode">PersistentVolume</code></li>
<li class="alphabeticList level-2"><code class="inlineCode">VirtualDisk</code></li>
</ol>
</li>
</ol>
<p class="normal-one">Answer: c</p>
<ol>
<li class="numberedList" value="2">KinD includes a dynamic disk provisioner. Which company created the provisioner?<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">Microsoft</li>
<li class="alphabeticList level-2">CNCF</li>
<li class="alphabeticList level-2">VMware</li>
<li class="alphabeticList level-2">Rancher</li>
</ol>
</li>
</ol>
<p class="normal-one">Answer: d</p>
<ol>
<li class="numberedList" value="3">If you created a KinD cluster with multiple worker nodes, what would you install to direct traffic to each node?<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">A load balancer</li>
<li class="alphabeticList level-2">A proxy server</li>
<li class="alphabeticList level-2">Nothing</li>
<li class="alphabeticList level-2">Replicas set to 3</li>
</ol>
</li>
</ol>
<p class="normal-one">Answer: a</p>
<ol>
<li class="numberedList" value="4">True or false? A Kubernetes cluster can only have one CSIdriver installed.<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">True</li>
<li class="alphabeticList level-2">False</li>
</ol>
</li>
</ol>
<p class="normal-one">Answer: b</p>
</div>
</div></body></html>