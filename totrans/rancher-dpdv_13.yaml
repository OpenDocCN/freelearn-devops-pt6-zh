- en: '*Chapter 9*: Cluster Configuration Backup and Recovery'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第九章*：集群配置备份与恢复'
- en: The previous chapters covered importing externally managed clusters into Rancher.
    This chapter will cover managing RKE1 and RKE2 clusters in Rancher when it comes
    to backup and recovery of the cluster. This includes some of the best practices
    for setting up your backups. Then, we'll walk through an etcd restore, finally
    covering the limitations of an etcd backup.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章讲解了如何将外部管理的集群导入到 Rancher 中。本章将重点介绍如何在 Rancher 中管理 RKE1 和 RKE2 集群，尤其是在集群的备份和恢复方面。我们将包括设置备份的一些最佳实践。然后，我们将演示一个
    etcd 恢复，最后讨论 etcd 备份的限制。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: What is an etcd backup?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 etcd 备份？
- en: Why do I need to back up my etcd?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我需要备份我的 etcd？
- en: How does an etcd backup work?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd 备份是如何工作的？
- en: How does an etcd restore work?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd 恢复是如何工作的？
- en: When do you need an etcd restore?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时需要进行 etcd 恢复？
- en: What does an etcd backup not protect?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd 备份无法保护哪些内容？
- en: How do you configure etcd backups?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何配置 etcd 备份？
- en: How do you take an etcd backup?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何进行 etcd 备份？
- en: How do you restore from an etcd backup?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从 etcd 备份中恢复？
- en: Setting up a lab environment to test common failure scenarios
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置实验环境以测试常见故障场景
- en: What is an etcd backup?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 etcd 备份？
- en: As we covered in [*Chapter 2*](B18053_02_Epub.xhtml#_idTextAnchor025), *Rancher
    and Kubernetes High-Level Architecture*, etcd is the database for Kubernetes where
    the cluster's configuration is stored. Both RKE1 and RKE2 use etcd for this role,
    but other distributions, such as k3s, can use different databases, such as MySQL,
    PostgreSQL, or SQLite. For this chapter, we'll only be focusing on etcd. With
    Kubernetes, all components are designed to be stateless and not store any data
    locally. The significant exemption to that rule is etcd as its only job is to
    store persistent data for the cluster. This includes all the settings for the
    cluster and definitions of all your Deployments, Secrets, and ConfigMap. This
    means that if the etcd cluster is ever lost, you lose the whole cluster, which
    is why it's crucial to protect the etcd cluster from an availability viewpoint,
    which we covered in[*Chapter 2*](B18053_02_Epub.xhtml#_idTextAnchor025), *Rancher
    and Kubernetes High-Level Architecture*, and [*Chapter 4*](B18053_04_Epub.xhtml#_idTextAnchor052),
    *Creating an RKE and RKE2 Cluster*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第二章*](B18053_02_Epub.xhtml#_idTextAnchor025)《Rancher和Kubernetes高级架构》中所讲的，etcd
    是 Kubernetes 的数据库，用于存储集群的配置。RKE1 和 RKE2 都使用 etcd 来完成这一角色，但其他发行版，如 k3s，可能使用不同的数据库，如
    MySQL、PostgreSQL 或 SQLite。对于本章内容，我们将仅关注 etcd。在 Kubernetes 中，所有组件都设计为无状态的，并且不在本地存储任何数据。唯一的例外是
    etcd，它的唯一任务就是为集群存储持久化数据。这包括集群的所有设置，以及所有部署、Secrets 和 ConfigMap 的定义。这意味着如果 etcd
    集群丢失，你将丢失整个集群，这也是为什么从可用性角度来看，保护 etcd 集群至关重要的原因，我们在[*第二章*](B18053_02_Epub.xhtml#_idTextAnchor025)《Rancher和Kubernetes高级架构》和[*第四章*](B18053_04_Epub.xhtml#_idTextAnchor052)《创建RKE和RKE2集群》中都有提到。
- en: Why do I need to back up my etcd?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我需要备份我的 etcd？
- en: One of the questions that always comes up when people start their Kubernetes
    journey is "Why do I need to back up, etcd?" The next question is then "Can't
    I just redeploy the cluster if anything happens?" How I answer that question is
    "Yes, in an ideal world, you should be able to rebuild your cluster from zero
    by just simply deploying everything. But we don't live in a perfect world. It
    is tough to redeploy 100% of our applications in the real world if you lose a
    cluster."
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们开始 Kubernetes 之旅时，常见的一个问题是：“为什么我需要备份 etcd？”接下来常问的问题是：“如果发生任何问题，我不能重新部署集群吗？”我回答这个问题的方法是：“是的，在理想的情况下，你应该能够通过重新部署一切来从零开始重建集群。但我们并不生活在完美的世界里。如果你丢失了集群，在现实世界中重新部署
    100% 的应用程序是非常困难的。”
- en: The scenario I always give is, let's say it's late on a Friday night, you just
    did a Kubernetes upgrade, and now everything is failing. Applications are crashing,
    and you can't find a fix to fail forward with the upgrade. If you have an etcd
    backup from before the upgrade, with Rancher, it's a few clicks to roll the cluster
    back to a state it was in before the upgrade, versus you spending hours spinning
    up a new cluster and then spending hours deploying all your core services, such
    as monitoring, logging, and storage, on the cluster. Then, who knows how fast
    you can redeploy all your applications, assuming the standup process is fully
    documented or still working.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常举的例子是，假设现在是星期五的深夜，你刚刚进行了一次 Kubernetes 升级，但现在一切都失败了。应用程序崩溃，你无法找到解决方法来继续升级。如果你有一次升级前的
    etcd 备份，使用 Rancher 只需要几次点击，就能将集群恢复到升级前的状态，而不是花费数小时启动一个新集群，然后再花几个小时在集群上部署所有核心服务，比如监控、日志和存储。更何况，谁知道你能多快地重新部署所有应用程序，前提是启动过程已经完备记录，或者仍然有效。
- en: It is highly recommended to take etcd backups no matter the environment, including
    development and testing environments, as it simply gives you options. I always
    follow the rule *No one ever got fired for having too many backups*. It is important
    to note that backups are turned on by default with Rancher-deployed clusters.
    This was done for the simple fact that an etcd backup usually only takes up a
    couple hundred megabits of storage and is so valuable during a disaster.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议无论在什么环境下，包括开发和测试环境，都进行 etcd 备份，因为它为你提供了更多选择。我一直遵循的原则是 *没人因为备份太多而被解雇*。值得注意的是，在
    Rancher 部署的集群中，备份默认是开启的。这是因为 etcd 备份通常只占用几百兆存储，在灾难发生时非常有价值。
- en: One of the questions that come up is, "Do I need etcd backups if I have a VM
    snapshot?" While having additional backups is always great, the issue is recovering
    etcd after restoring from a snapshot. The problem is that all nodes must be in
    sync at the time of the snapshot for the restore to be successful. If it is the
    only option you have, you can still recover etcd from a VM snapshot, but you'll
    need to restore one of the etcd nodes, clean the other etcd nodes, and resync
    the etcd data from the restored node. You can find this process and scripts at
    https://github.com/rancherlabs/support-tools/tree/master/etcd-tools. It is essential
    to know this process can be very difficult and time-consuming, and it is not an
    officially supported solution.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的问题之一是：“如果我有虚拟机快照，我还需要 etcd 备份吗？”虽然有额外的备份总是好的，但问题在于从快照恢复后如何恢复 etcd。问题是，所有节点必须在快照时处于同步状态，恢复才会成功。如果这是你唯一的选择，你仍然可以从虚拟机快照中恢复
    etcd，但你需要恢复一个 etcd 节点，清理其他 etcd 节点，并从恢复的节点重新同步 etcd 数据。你可以在 [https://github.com/rancherlabs/support-tools/tree/master/etcd-tools](https://github.com/rancherlabs/support-tools/tree/master/etcd-tools)
    找到此过程和脚本。需要知道的是，这个过程可能非常困难且耗时，并且它不是官方支持的解决方案。
- en: How does an etcd backup work?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: etcd 备份是如何工作的？
- en: In this section, we'll look at how etcd backups work for RKE and RKE2 clusters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解 etcd 备份如何在 RKE 和 RKE2 集群中工作。
- en: RKE clusters
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE 集群
- en: For RKE clusters, the process for one-time snapshots is controlled by the RKE
    binary, with scheduled snapshots being managed by a standalone container that
    is deployed by RKE called `etcd-rolling-snapshots`. Both processes follow the
    same basic steps, with the first step being to go to each etcd node in the cluster
    one at a time and start a container called `etcd-snapshot-once` or `etcd-rolling-snapshots`,
    depending on the type of backup. This container is what is going to do most of
    the heavy lifting in this process. It is important to note that this is a Docker
    container outside Kubernetes, and customization on this container is minimal.
    Once the container is started, it runs a tool called `rke-etcd-backup`, which
    is part of Rancher's rke-tools, which can be found at [https://github.com/rancher/rke-tools/](https://github.com/rancher/rke-tools/).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RKE 集群，单次快照的过程由 RKE 二进制文件控制，定期快照则由一个由 RKE 部署的独立容器 `etcd-rolling-snapshots`
    管理。两个过程遵循相同的基本步骤，第一步是依次进入集群中的每个 etcd 节点，并启动一个名为 `etcd-snapshot-once` 或 `etcd-rolling-snapshots`
    的容器，具体取决于备份类型。这个容器将承担大部分的工作。需要注意的是，这是一个 Kubernetes 外部的 Docker 容器，且对这个容器的定制非常有限。一旦容器启动，它会运行一个名为
    `rke-etcd-backup` 的工具，这是 Rancher 的 rke-tools 之一，可以在 [https://github.com/rancher/rke-tools/](https://github.com/rancher/rke-tools/)
    找到。
- en: This tool is mainly a utility script that handles finding the certificates files,
    at which point it will run the `etcdctl snapshot save` command. This command will
    export the whole etcd database as a single file. It is important to note that
    this is a full backup and not an incremental or differential backup. Also, etcd
    does not have translation logs like other databases, so the snapshot file contains
    the whole database as a single file.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该工具主要是一个实用脚本，用于查找证书文件，并在此时执行 `etcdctl snapshot save` 命令。此命令将整个 etcd 数据库导出为一个单一文件。需要特别注意的是，这是一个完整备份，而不是增量或差异备份。此外，etcd不像其他数据库那样有日志翻译功能，因此快照文件包含了整个数据库作为一个单一文件。
- en: Once the database is backed up, RKE will backup some additional files to make
    cluster restores easier. This includes extracting the `cluster.rkestate` file
    from `configmap full-cluster-state` in the `kube-system` namespace. In versions
    of RKE before v1.0.0, RKE would back up the `/etc/kubernetes/ssl/` certificate
    folder, but this is no longer needed as the `rkestate` file has all the certificates
    and their private keys as part of the JSON. Once all the files have been created,
    rke-tools will zip up all the files into a single backup file stored in `/opt/rke/etcd-snapshots/`
    on the host. Then, if you have configured S3 backups, rke-tools will upload the
    backup file to the S3 bucket.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据库完成备份，RKE 会备份一些额外的文件，以便于集群恢复。这包括从 `kube-system` 命名空间中的 `configmap full-cluster-state`
    提取 `cluster.rkestate` 文件。在 v1.0.0 之前的 RKE 版本中，RKE 会备份 `/etc/kubernetes/ssl/` 证书文件夹，但现在不再需要，因为
    `rkestate` 文件已经将所有证书及其私钥包含在 JSON 文件中。所有文件创建完毕后，rke-tools 会将所有文件压缩成一个备份文件，并存储在主机的
    `/opt/rke/etcd-snapshots/` 目录下。然后，如果已配置 S3 备份，rke-tools 会将备份文件上传到 S3 存储桶。
- en: Important Note
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 重要注意事项
- en: By default, rke-tools will leave behind a local copy of the backup just in case.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，rke-tools 会保留本地备份副本，以防万一。
- en: Finally, rke-tools will purge backups. This is done by counting the total number
    of scheduled backup files. Then, if that count is greater than the retention setting,
    which is 6 by default, it will start deleting the oldest backup until it meets
    the retention settings. It is important to note that any one-time snapshots will
    not be counted and deleted. So, it is common for these backups to stay on the
    nodes until they are manually cleaned up. Once this process is finished, RKE will
    start on the next etcd node in the cluster.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，rke-tools 会清理备份。此操作是通过计算所有计划备份文件的总数来完成的。如果该数量超过了保留设置（默认设置为 6），它将开始删除最旧的备份，直到符合保留设置。需要特别注意的是，任何一次性快照将不会被计入和删除。因此，这些备份通常会保留在节点上，直到手动清理。此过程完成后，RKE
    将开始处理集群中的下一个 etcd 节点。
- en: Note
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: There is a known design quirk that for S3 backups, a backup will be taken on
    all etcd nodes in the cluster, and each node will upload its backup file to the
    S3 bucket with the same name. This means that the file will be overwritten multiple
    times during a backup.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个已知的设计特点，对于 S3 备份，集群中的所有 etcd 节点都会进行备份，并且每个节点将把其备份文件上传到 S3 存储桶，且文件名称相同。这意味着在备份过程中，文件会被多次覆盖。
- en: RKE2/k3s clusters
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE2/k3s 集群
- en: With RKE2 and k3s clusters, they share the code for an etcd backup and restore
    it. The main difference from RKE is that the etcd backup process is directly built
    into the `rke2-server` binary instead of a separate container. With RKE2/k3s,
    etcd backups are turned on by default and are configured with server options,
    which we will cover later in this chapter. The other main difference is with RKE2;
    the only file that is backed up is just an etcd snapshot file as RKE2 doesn't
    need the `rkestate` file as RKE did. With the cluster status for RKE2 being stored
    in the bootstrap key is stored in the etcd database directly. It is important
    to note that the bootstrap key is encrypted using an AES SHA1 cipher using the
    server token as the encryption key, not stored in etcd. You are required to store
    and protect the token outside the backup process. If you lose the token, there
    is no way of recovering the cluster without breaking the encryption.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RKE2 和 k3s 集群，它们共享一个 etcd 备份和恢复的代码。与 RKE 的主要区别在于，etcd 备份过程直接集成在 `rke2-server`
    二进制文件中，而不是作为单独的容器存在。对于 RKE2/k3s，etcd 备份默认启用，并通过服务器选项进行配置，我们将在本章后面讲解。另一个主要区别是，在
    RKE2 中，备份的唯一文件就是 etcd 快照文件，因为 RKE2 不需要像 RKE 那样的 `rkestate` 文件。对于 RKE2，集群状态存储在引导密钥中，并直接存储在
    etcd 数据库中。需要特别注意的是，引导密钥使用 AES SHA1 加密算法，以服务器令牌作为加密密钥进行加密，而不是存储在 etcd 中。您需要在备份过程之外存储并保护令牌。如果丢失了令牌，将无法在不破解加密的情况下恢复集群。
- en: The other difference is how backups are configured because each master node
    is configured independently, meaning that you can set different backup schedules
    on each node. This also includes how the scheduled snapshot is run in the fact
    it uses the cronjob format, which allows you to force the backup to happen at
    set times, for example, nightly at midnight or every hour on the hour. To address
    the S3 overwrite issue that RKE has, RKE2 uses the hostname of the node in the
    backup filename. This means that every node in the cluster will still take an
    etcd backup and upload it to the S3 bucket, but it will not be overwritten. Because
    of this, you will have duplicate backups in your S3 bucket, meaning if you have
    three master nodes in the RKE2 cluster, you will have three copies of the etcd
    backup file stored in S3\. Again, in etcd, backups are usually tiny, so the increased
    storage is usually just background noise.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个区别是备份的配置方式，因为每个主节点都是独立配置的，这意味着你可以为每个节点设置不同的备份计划。这也包括定期快照的运行方式，实际上它使用了 cronjob
    格式，允许你在设定的时间强制执行备份，例如，每晚午夜或每小时的整点。为了解决 RKE 存在的 S3 覆盖问题，RKE2 在备份文件名中使用了节点的主机名。这意味着集群中的每个节点仍会进行
    etcd 备份并将其上传到 S3 存储桶中，但不会被覆盖。因此，你的 S3 存储桶中会有重复的备份文件，这意味着如果你在 RKE2 集群中有三个主节点，你将会在
    S3 中有三个 etcd 备份文件的副本。再次强调，在 etcd 中，备份通常很小，因此增加的存储通常只是背景噪音。
- en: How does an etcd restore work?
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: etcd 恢复是如何工作的？
- en: Next, let's look at how an etcd restore works for the different clusters.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看在不同集群中，etcd 恢复是如何工作的。
- en: RKE clusters
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE 集群
- en: For RKE clusters, the process of restoring etcd is done using the `rke etcd
    snapshot-restore` command, which uses the same standalone container with rke-tools
    as the RKE binary uses for the backups. The main difference is that all etcd nodes
    will need the same backup file for the restore. This means when you give the RKE
    binary the snapshot name, all nodes in the cluster must have a copy of that file.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RKE 集群，恢复 etcd 的过程是使用`rke etcd snapshot-restore`命令完成的，该命令使用与 RKE 二进制文件用于备份的相同独立容器（rke-tools）。主要区别在于，所有
    etcd 节点都需要相同的备份文件来进行恢复。这意味着，当你将快照名称提供给 RKE 二进制文件时，集群中的所有节点必须拥有该文件的副本。
- en: The first step in the restore process is to create an MD5 hash of the file on
    each node and compare the hashes to verify that all nodes are in agreement. If
    this check fails, the restore will stop and require the user to copy the backup
    file between nodes manually. A flag called `--skip-hash-check=true` can be added
    to the RKE `restore` command, but this is a safety feature that shouldn't be disabled
    unless you know what you are doing. If you are using the S3 option, RKE will download
    the backup file from the S3 bucket on each node before running this process, at
    which point the hash verification process is the same.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复过程的第一步是在每个节点上创建文件的 MD5 哈希值，并比较哈希值以验证所有节点是否一致。如果此检查失败，恢复将停止，并要求用户手动在节点之间复制备份文件。可以向
    RKE `restore` 命令添加一个名为`--skip-hash-check=true`的标志，但这是一个安全功能，除非你知道自己在做什么，否则不应禁用。如果你使用的是
    S3 选项，RKE 会在每个节点上从 S3 存储桶下载备份文件，然后才会运行此过程，此时哈希验证过程是相同的。
- en: Once the backup files have been verified, RKE will tear down the etcd cluster,
    meaning that RKE will stop all the etcd and control plane containers on all nodes,
    at which point RKE will start a standalone container called `etcd-restore`, which
    will restore the etcd data directory on each node. This is why all nodes must
    have the same snapshot file. Once the restore container has been completed successfully
    on all nodes, RKE will run a standard RKE up process to build the etcd and control
    plane back up, including creating a new etcd cluster and then starting the control
    plane services.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦备份文件经过验证，RKE 将拆除 etcd 集群，这意味着 RKE 会停止所有节点上的 etcd 和控制平面容器，此时 RKE 将启动一个名为`etcd-restore`的独立容器，该容器将在每个节点上恢复
    etcd 数据目录。这就是为什么所有节点必须拥有相同的快照文件的原因。 一旦所有节点上的恢复容器成功完成，RKE 将运行标准的 RKE up 过程来重新构建
    etcd 和控制平面，包括创建新的 etcd 集群，然后启动控制平面服务。
- en: Finally, it ends the process by updating the worker nodes. During this task,
    the cluster will be offline for about 5 to 10 minutes while the restore process
    is running. Most application Pods should continue to run without impact, assuming
    they do not depend on the kube-api service. For example, the ingress-nginx-controller
    will stay up and running during a restore but will have a stall configuration.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过更新工作节点来结束这个过程。在此任务期间，集群将会停机大约 5 到 10 分钟，直到恢复过程完成。大多数应用程序 Pods 应该会继续运行而不受影响，前提是它们不依赖于
    kube-api 服务。例如，ingress-nginx-controller 在恢复过程中将保持运行，但配置会出现暂停。
- en: RKE2/k3s clusters
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE2/k3s 集群
- en: 'The restore process is very different in RKE2/k3s because in RKE, one master
    server in the cluster will be used as a new bootstrap node to reset the cluster.
    This process stops the `rke2-server` service on all master nodes in the cluster.
    The new bootstrap node, `rke2`, will run the following command:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复过程在 RKE2/k3s 中与 RKE 非常不同，因为在 RKE 中，集群中的一个主服务器将作为新的引导节点来重置集群。这个过程会停止集群中所有主节点上的
    `rke2-server` 服务。新的引导节点 `rke2` 将运行以下命令：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This will create a new etcd cluster ID and restore the etcd snapshot into the
    new single-node etcd cluster. At this point, `rke2-server` will be able to start.
    The rest of the rke2 master nodes need to be cleaned and rejoined to the cluster
    as *new* nodes. Once all the master nodes are back up and healthy, the worker
    nodes should rejoin automatically on their own, but it can be slow and unreliable,
    so it is standard practice to restart the rke2-agents after the restore.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个新的 etcd 集群 ID，并将 etcd 快照恢复到新的单节点 etcd 集群中。此时，`rke2-server` 将能够启动。其余的 rke2
    主节点需要清理并重新加入集群，作为*新的*节点。一旦所有主节点都恢复并健康运行，工作节点应该会自动重新加入，但这可能较慢且不可靠，因此在恢复后重启 rke2-agents
    是标准做法。
- en: Important Note
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is important to note that the whole cluster will be rolled back for both
    restore processes. This includes any Deployments, ConfigMaps, Secrets, and so
    on. So, if you are restoring to resolve an application issue, you will need to
    reapply any changes to any other applications in the cluster.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，恢复过程将使整个集群回滚。这包括任何 Deployments、ConfigMaps、Secrets 等内容。因此，如果你正在恢复以解决应用程序问题，你需要重新应用集群中其他应用程序的任何更改。
- en: When do you need an etcd restore?
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你什么时候需要进行 etcd 恢复？
- en: 'Of course, the following questions always comes up: "When should I do an etcd
    restore?" and "Is an etcd restore only for emergencies?". The general rule of
    thumb is that an etcd restore is mainly for disaster recovery and rolling back
    a Kubernetes upgrade. For example, you accidentally delete most or all the etcd
    nodes in a cluster or have an infrastructure issue such as a power outage or storage
    failure. If the cluster does self-recover on its own, doing an etcd restore from
    the last backup before the event will be the fastest way to restore service to
    the cluster.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，以下问题总是会出现：“我什么时候应该进行 etcd 恢复？”和“etcd 恢复仅仅是应急情况吗？”一般来说，etcd 恢复主要用于灾难恢复和回滚
    Kubernetes 升级。例如，你不小心删除了集群中大部分或所有的 etcd 节点，或遇到基础设施问题，如停电或存储故障。如果集群能够自行恢复，从事件发生前的最后备份恢复等会是恢复集群服务的最快方式。
- en: The other main reason for doing a restore is a failed Kubernetes upgrade. As
    with RKE, there is no way to downgrade a cluster without restoring the cluster
    from an etcd backup before the upgrade. This is the way it is always recommended
    to take a snapshot right before the upgrade. It is important to note that the
    RKE binary will allow you to set an older Kubernetes version and will try to push
    out that version to cluster. This process will generally break the cluster and
    is highly unsupported. In both these cases, the cluster is down or in a failed
    state, and our goal is to restore service as soon as possible.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 进行恢复的另一个主要原因是 Kubernetes 升级失败。和 RKE 一样，没有办法在不从升级前的 etcd 备份恢复集群的情况下回退集群。这就是为什么我们总是建议在升级前拍摄快照。需要注意的是，RKE
    二进制文件允许你设置旧的 Kubernetes 版本，并尝试将该版本推送到集群。这一过程通常会破坏集群，并且没有官方支持。在这两种情况下，集群会停机或处于失败状态，我们的目标是尽快恢复服务。
- en: Of course, the next question is, "When should I not do an etcd restore?" The
    answer is, you shouldn't be doing a restore to roll back a failed application
    change. For example, an application team pushes out a change to their application
    that fails, that is, there is a bug in their code, or they misconfigured something
    in their application. Doing an etcd restore from before the change will work to
    roll back the changes, but you are also impacting all the other applications deployed
    in the cluster and recycling the cluster to roll back a change that really should
    just be fixed by redeploying the application with the older code/settings.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，下一个问题是，“什么时候我不应该执行etcd恢复？”答案是，你不应该通过恢复来回滚失败的应用程序更改。例如，如果一个应用程序团队推送了一个更改，导致应用程序失败，也就是说，他们的代码中有一个bug，或者他们在应用程序中配置错误。执行etcd恢复到更改之前的状态可以回滚这些更改，但你也会影响集群中所有其他部署的应用程序，并且重新启动集群来回滚本应通过重新部署带有旧代码/设置的应用程序来修复的更改。
- en: Note
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Having processes in place for your application teams to roll back a Deployment
    should be required in your environment. Most CI/CD systems usually have a way
    to select an older commit and push it out.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为应用程序团队建立回滚部署的流程应该是你环境中的必需项。大多数CI/CD系统通常都能选择旧的提交并将其推送出去。
- en: The other main reason not to do an etcd restore is to restore an old snapshot.
    For example, if you restore from a backup that is a few weeks old, there is a
    good chance that the tokens will be expired, and because of this, the cluster
    will not come up after the restore. Resolving this will require manual work to
    refresh the tokens for the broken services. Plus, the biggest issue is "What has
    changed in this cluster since that backup?" Who knows what upgrades, deployment,
    code changes, and more have changed in this cluster since that snapshot was taken.
    You could be fixing one team's problem but breaking everyone else's application
    in the process. The rule that I follow is 72 hours. If a snapshot is older than
    72 hours, I need to weigh my options of restoring it, that is, is most of that
    time over the weekend when no changes are being made? Great, I have no problem
    recovering a snapshot from Friday on Monday. But if I know that application teams
    like to deploy on Thursdays and I'm restoring from a Wednesday snapshot, I should
    probably stop and talk to the application teams before moving forward.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不应该执行etcd恢复的主要原因是恢复旧的快照。例如，如果你从几周前的备份恢复，很有可能令牌已经过期，因此恢复后集群无法启动。解决这个问题将需要手动操作来刷新损坏服务的令牌。而且，最大的问题是：“这个集群自从备份以来发生了什么变化？”谁知道自从快照拍摄以来，这个集群发生了哪些升级、部署、代码更改等。你可能在修复一个团队的问题时，破坏了其他团队的应用程序。我遵循的规则是72小时。如果快照超过72小时，我需要权衡是否恢复它，也就是说，是否大部分时间是在没有进行更改的周末？太好了，我对从周五恢复到周一的快照没问题。但如果我知道应用程序团队喜欢在周四部署，而我正在从周三的快照恢复，那么我应该停下来与应用程序团队沟通，然后再继续操作。
- en: Finally, when restoring after a Kubernetes upgrade, my rule is an upgrade is
    a line in the sand for restores that should only be crossed shortly after the
    upgrade. For example, say I upgraded my cluster from v1.19 to v1.20, and within
    minutes, my applications started having issues. Then great, let's restore to the
    snapshot right before the upgrade. But if I did that upgrade on Friday night,
    and on Tuesday, an application team member comes to me and says, "Hey, we are
    seeing some weird errors. Can you roll back that upgrade?" My answer is going
    to be "No." Too much time has passed since the last upgrade and rolling back will
    cause too much impact on the cluster. Of course, my next question to them would
    be, "Why didn't your smoke test catch this after the upgrade?", as it is a standard
    process to smoke test applications after a significant change to the environment.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在Kubernetes升级后恢复时，我的规则是：升级是恢复的“分界线”，应该只在升级后不久进行恢复。例如，假设我将集群从v1.19升级到v1.20，并且在几分钟内，我的应用程序开始出现问题。那么，太好了，我们可以恢复到升级前的快照。但如果我在周五晚上做了这个升级，而在周二，一个应用程序团队的成员找到了我，说：“嘿，我们看到一些奇怪的错误。你能回滚这个升级吗？”我的回答将是“不行”。自上次升级以来已经过去太长时间，回滚将对集群造成过多影响。当然，我接下来会问他们，“为什么你们的冒烟测试在升级后没发现这个问题？”因为在环境发生重大变化后，冒烟测试应用程序是一个标准流程。
- en: What does an etcd backup not protect?
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: etcd备份不能保护什么？
- en: Of course, an etcd backup does not cover all data in a cluster, as we covered
    earlier in this chapter; etcd stores the cluster's configuration. But there is
    additional data in the cluster that is not stored in etcd. The main one is volumes
    and the data stored inside the volume data. Suppose you have a **PersistentVolumeClaim**
    (**PVC**) or **PersistentVolume** (**PV**) with data inside the volume. That data
    is not stored in etcd but is stored in the storage device, that is, **Network
    File System** (**NFS**), local storage, Longhorn, and so on. The only thing stored
    in etcd is the definition of the volume, that is, the name of the volume, size,
    configuration, and more. This means if you restore from an etcd backup of a cluster
    after a volume was deleted, depending on the storage provider and its retain policy,
    the data inside that volume is lost. So, even if you do an etcd restore, the cluster
    will create a new volume to replace the deleted volume, but the volume will be
    empty with no data inside it. If you need to backup volumes or other higher-level
    backup functions, you should look at tools such as Veeam's Kasten or VMware's
    Velero.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，etcd备份并不覆盖集群中的所有数据，正如我们在本章前面讨论过的，etcd只存储集群的配置。但集群中还有其他数据没有存储在etcd中。最主要的就是卷和卷数据中存储的数据。假设你有一个**PersistentVolumeClaim**（**PVC**）或**PersistentVolume**（**PV**），其中包含卷内的数据。该数据不会存储在etcd中，而是存储在存储设备上，即**网络文件系统**（**NFS**）、本地存储、Longhorn等。etcd中唯一存储的内容是卷的定义，即卷的名称、大小、配置等。这意味着如果在删除卷后从etcd备份恢复集群，根据存储提供商及其保留策略，卷内的数据将丢失。因此，即使你执行etcd恢复，集群会创建一个新卷来替代删除的卷，但该卷将是空的，里面没有数据。如果你需要备份卷或其他更高级的备份功能，应该考虑使用像Veeam的Kasten或VMware的Velero这样的工具。
- en: 'The other big item that doesn''t get backed up in an etcd backup is the container
    images, which means a deployment with a custom image. etcd only stores the image
    configuration, that is, the image example: `docker.io/rancherlabs/swiss-army-knife:v1.0`.
    But this does include the image data itself. This typically comes up when someone
    deploys an app with a custom image then loses access to the image down the road.
    A great example is hosting your container images in a repository server such as
    Harbor or JFrog inside the cluster that needs them to start.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在etcd备份中没有备份的大项是容器镜像，这意味着使用自定义镜像的部署。etcd只存储镜像配置，即镜像示例：`docker.io/rancherlabs/swiss-army-knife:v1.0`。但这不包括镜像本身的数据。这通常出现在某人使用自定义镜像部署应用程序后，之后失去对该镜像的访问权限。一个很好的例子是将容器镜像托管在集群中需要的仓库服务器中，例如Harbor或JFrog。
- en: How do you configure etcd backups?
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何配置etcd备份？
- en: Let's look at how to configure etcd backups for RKE and RKE2 clusters.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何为RKE和RKE2集群配置etcd备份。
- en: RKE clusters
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE集群
- en: For RKE clusters, the etcd backup configuration is stored in the `cluster.yml`
    file. There are two main types of etcd backups with RKE. The first is a one-time
    backup that is triggered manually by a user event, such as manually running the
    `rke etcd snapshot-save` command, upgrading the Kubernetes versions, or making
    a change to an etcd node in the cluster. The second type is recurring snapshots
    that are turned on by default with the release of RKE v0.1.12\. The default process
    is to take a backup of everything every 12 hours. It is important to note that
    this schedule is not fixed like a cronjob where it will always run at the same
    time but instead is based on how much time has passed since the last scheduled
    backup.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RKE集群，etcd备份配置存储在`cluster.yml`文件中。RKE有两种主要类型的etcd备份。第一种是一次性备份，由用户事件手动触发，例如手动运行`rke
    etcd snapshot-save`命令、升级Kubernetes版本或更改集群中的etcd节点。第二种是定期快照，这种方式在RKE v0.1.12版本发布时默认开启。默认过程是每12小时备份一次所有内容。需要注意的是，这个时间表不像cronjob那样固定，总是在相同的时间运行，而是基于自上次备份以来经过的时间。
- en: 'Following is a set of example `cluster.yaml` files for both local and S3 etcd
    backups:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一组用于本地和S3等备份的`cluster.yaml`示例文件：
- en: Local backup only – [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/local-backups.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/local-backups.yaml)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅本地备份 – [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/local-backups.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/local-backups.yaml)
- en: Local and S3 backup – [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/s3-backups.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/s3-backups.yaml)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地和 S3 备份 – [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/s3-backups.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/s3-backups.yaml)
- en: For the full list of options and settings, please see the official Rancher documentation
    at [https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#options-for-the-etcd-snapshot-service](https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#options-for-the-etcd-snapshot-service).
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要查看完整的选项和设置列表，请参阅官方 Rancher 文档，网址为[https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#options-for-the-etcd-snapshot-service](https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#options-for-the-etcd-snapshot-service)。
- en: RKE2/k3s clusters
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE2/k3s 集群
- en: As we talked about earlier in this chapter, RKE2 and k3s handle etcd backups
    at a node level instead of a cluster level, which means that you define your etcd
    backup schedule and other settings on each master node in the cluster instead
    of defining it at the cluster level. This allows you to do some cool things, such
    as shifting your backup schedule for each node, for example, the first node backups
    at 12 A.M., 3 A.M., 6 A.M., and so on. The second node backups at 1 A.M., 4 A.M.,
    7 A.M., and so on, with the third node having a schedule of 2 A.M., 5 A.M., 8
    A.M. Note that this is usually only done in large clusters to prevent all etcd
    nodes from being backed up simultaneously as there is a slight dip in performance
    for etcd during the backup. So, we want only to impact one etcd node at a time.
    You can also only configure backups on a first node for lower environments where
    backups are excellent but not required.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章前面提到的，RKE2 和 k3s 在节点级别处理 etcd 备份，而不是在集群级别处理，这意味着您需要在集群中的每个主节点上定义 etcd
    备份计划和其他设置，而不是在集群级别定义。这使您可以做一些有趣的事情，例如为每个节点调整备份计划。例如，第一个节点在 12 A.M.、3 A.M.、6 A.M.
    等时间进行备份。第二个节点在 1 A.M.、4 A.M.、7 A.M. 等时间进行备份，第三个节点则在 2 A.M.、5 A.M.、8 A.M. 等时间进行备份。请注意，这通常仅在大型集群中进行，以防止所有
    etcd 节点同时备份，因为在备份过程中 etcd 的性能会略微下降。因此，我们希望每次只影响一个 etcd 节点。在低环境中，您也可以仅在第一个节点上配置备份，其中备份很优秀但不是必需的。
- en: For the complete list of options and settings, please see the official Rancher
    documentation for RKE2 at https://docs.rke2.io/backup_restore/ or k3s. Please
    see [https://rancher.com/docs/k3s/latest/en/backup-restore/](https://rancher.com/docs/k3s/latest/en/backup-restore/)
    for the official documentation for k3s. It is important to note that embedded
    etcd in k3s is still experimental.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看完整的选项和设置列表，请参阅 RKE2 的官方 Rancher 文档，网址为 https://docs.rke2.io/backup_restore/
    或 k3s 的文档。有关 k3s 的官方文档，请参见[https://rancher.com/docs/k3s/latest/en/backup-restore/](https://rancher.com/docs/k3s/latest/en/backup-restore/)。需要注意的是，k3s
    中的内嵌 etcd 仍处于实验阶段。
- en: Note
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you need an HTTP proxy to access your S3 bucket, please configure the proxy
    setting as stated in the documentation located at [https://docs.rke2.io/advanced/#configuring-an-http-proxy](https://docs.rke2.io/advanced/#configuring-an-http-proxy).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要通过 HTTP 代理访问您的 S3 存储桶，请按照[https://docs.rke2.io/advanced/#configuring-an-http-proxy](https://docs.rke2.io/advanced/#configuring-an-http-proxy)中的文档配置代理设置。
- en: 'Following is a set of example rke2 configuration file for both local and S3
    etcd backups:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本地和 S3 etcd 备份的示例 rke2 配置文件：
- en: Local backup only – [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke2/local-backups.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke2/local-backups.yaml)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅限本地备份 – [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke2/local-backups.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke2/local-backups.yaml)
- en: Local and S3 backup – [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke2/s3-backups.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke2/s3-backups.yaml)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地和 S3 备份 – [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke2/s3-backups.yaml](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke2/s3-backups.yaml)
- en: How do you take an etcd backup?
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何进行 etcd 备份？
- en: In this section, we'll look at taking an etcd backup for RKE and RKE2 clusters.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍如何为 RKE 和 RKE2 集群进行 etcd 备份。
- en: RKE clusters
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE 集群
- en: 'For custom clusters, you can make a one-time backup using the following command:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自定义集群，您可以使用以下命令进行一次性备份：
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first option sets the `cluster.yml` filename. This is only needed if you
    are not using the default filename of `cluster.yml`. The second option specifies
    the name of the backup. This is technically optional, but it is highly recommended
    to set this to something meaningful, such as `pre-k8s-upgrade` and `post-k8s-upgrade`.
    It is also highly recommended to avoid using special characters in the filename.
    If you use S3 backups, the settings will default to whatever is defined in the
    `cluster.yml` file. You can override these settings using the command-line flags,
    which are documented at [https://rancher.com/docs/rke/latest/en/etcd-snapshots/one-time-snapshots/#options-for-rke-etcd-snapshot-save](https://rancher.com/docs/rke/latest/en/etcd-snapshots/one-time-snapshots/#options-for-rke-etcd-snapshot-save).
    If you are using an RKE cluster deployed via Rancher, please see the documentation
    at https://rancher.com/docs/rancher/v2.6/en/cluster-admin/backing-up-etcd/.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个选项设置 `cluster.yml` 文件名。只有在你没有使用默认的 `cluster.yml` 文件名时才需要此选项。第二个选项指定备份的名称。技术上这是可选的，但强烈建议将其设置为具有实际意义的名称，例如
    `pre-k8s-upgrade` 和 `post-k8s-upgrade`。也强烈建议避免在文件名中使用特殊字符。如果你使用 S3 备份，设置将默认为 `cluster.yml`
    文件中定义的内容。你可以使用命令行标志覆盖这些设置，具体文档见 [https://rancher.com/docs/rke/latest/en/etcd-snapshots/one-time-snapshots/#options-for-rke-etcd-snapshot-save](https://rancher.com/docs/rke/latest/en/etcd-snapshots/one-time-snapshots/#options-for-rke-etcd-snapshot-save)。如果你正在使用通过
    Rancher 部署的 RKE 集群，请参阅文档 [https://rancher.com/docs/rancher/v2.6/en/cluster-admin/backing-up-etcd/](https://rancher.com/docs/rancher/v2.6/en/cluster-admin/backing-up-etcd/)。
- en: RKE2/k3s clusters
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE2/k3s 集群
- en: 'Both RKE2 and k3s use the same commands for taking backups by replacing `rke2`
    with `k3s` for k3s clusters. For a one-time backup, you''ll run the following
    command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: RKE2 和 k3s 使用相同的命令来进行备份，只需将 `rke2` 替换为 `k3s`，即可用于 k3s 集群。对于一次性备份，你将运行以下命令：
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `name` flag has the same rules as RKE, but with the main difference being
    you might get an error about `FATA[0000] flag provided but not defined: ...` for
    all options in `config.yaml` that are not related to the S3 settings. To work
    around this issue, it is recommended to copy only the S3 settings to a new file
    called `s3.yaml` in `/etc/rancher/rke2/` and add the `–config /etc/rancher/rke2/s3.yaml`
    flag to the command.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`name` 标志与 RKE 相同，唯一的主要区别是，对于 `config.yaml` 中与 S3 设置无关的所有选项，你可能会遇到 `FATA[0000]
    flag provided but not defined: ...` 错误。为了解决这个问题，建议仅将 S3 设置复制到一个新的文件 `s3.yaml`
    中，路径为 `/etc/rancher/rke2/`，并将 `–config /etc/rancher/rke2/s3.yaml` 标志添加到命令中。'
- en: How do you restore from an etcd backup?
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何从 etcd 备份中恢复？
- en: Let's now look at how to restore data from an etcd backup.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下如何从 etcd 备份中恢复数据。
- en: RKE clusters
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE 集群
- en: For restores in RKE, you'll need to run the `rke etcd snapshot-save --config
    cluster.yml --name snapshot-name` command. It is imperative you set the snapshot
    name to be the filename of the snapshot you want to restore minus the `.zip` file
    extension. Suppose you are restoring from a scheduled snapshot. In that case,
    the filename will have some control characters as part of the timestamp, so it's
    recommended that you wrap the filename in single quotes again, making sure to
    remove the file extension.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RKE 的恢复，你需要运行 `rke etcd snapshot-save --config cluster.yml --name snapshot-name`
    命令。至关重要的是，你必须将快照名称设置为你要恢复的快照的文件名，去掉 `.zip` 文件扩展名。假设你是从一个定期快照中恢复。在这种情况下，文件名将包含时间戳的控制字符，因此建议你再次用单引号括起文件名，并确保去除文件扩展名。
- en: Note
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you are restoring an etcd backup into a new cluster, that is, all new nodes,
    you'll run into some token issues and need to address this issue. You can use
    the script at [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/restore-into-new-cluster.sh](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/restore-into-new-cluster.sh).sh
    to delete the secret and recycle the services. This script was designed for a
    three-node cluster and assumes using the default settings.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在将 etcd 备份恢复到一个新的集群中，即所有节点都是新的，你将遇到一些令牌问题，并需要解决此问题。你可以使用以下脚本 [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/restore-into-new-cluster.sh](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/restore-into-new-cluster.sh)
    删除机密并回收服务。此脚本是为一个三节点集群设计的，并假设使用默认设置。
- en: RKE2/k3s clusters
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE2/k3s 集群
- en: For restores in RKE2, there is a little more work than RKE. The first step is
    to stop `rke2-server` on all master nodes using the `systemctl stop rke2-server`
    command. Then, from one node master, you'll reset the cluster and restore the
    etcd database using the `rke2 server --cluster-reset --cluster-reset-restore-path=<PATH-TO-SNAPSHOT>`
    command. Once the restore is finished, you'll run the `systemctl start rke2-server`
    command to start the *new* etcd cluster. You'll then need to go to other master
    nodes in the cluster and run the `rm -rf /var/lib/rancher/rke2/server/db` command
    to remove the etcd data stored on the node, at which point we can restart `rke2-server`
    using the `systemctl start rke2-server` command to rejoin the cluster. This will
    cause a new etcd member to join the etcd cluster and sync the data from the bootstrap
    node. It is recommended that you only rejoin the nodes one at a time, allowing
    the node to go into a *Ready* status before rejoining the next node. Finally,
    once all the master nodes have rejoined, the worker nodes should recover. But
    after 5 minutes, you might want to restart the rke2-agent using the `systemctl
    restart rke2-agent` command to speed up the recovery process.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RKE2的恢复，工作量比RKE稍大一些。第一步是通过`systemctl stop rke2-server`命令停止所有主节点上的`rke2-server`。然后，从一个主节点上重置集群，并使用`rke2
    server --cluster-reset --cluster-reset-restore-path=<PATH-TO-SNAPSHOT>`命令恢复etcd数据库。恢复完成后，运行`systemctl
    start rke2-server`命令启动*新的*etcd集群。接下来，你需要去集群中的其他主节点，并运行`rm -rf /var/lib/rancher/rke2/server/db`命令来删除节点上存储的etcd数据，然后重新启动`rke2-server`，使用`systemctl
    start rke2-server`命令让节点重新加入集群。这将导致一个新的etcd成员加入etcd集群，并从引导节点同步数据。建议你一次只重新加入一个节点，在该节点进入*Ready*状态后，再重新加入下一个节点。最后，一旦所有主节点重新加入，工作节点应该也能恢复。但5分钟后，你可能需要使用`systemctl
    restart rke2-agent`命令重启rke2-agent，以加快恢复过程。
- en: Setting up a lab environment to test common failure scenarios
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置实验环境以测试常见的故障场景
- en: Finally, we'll end this chapter by practicing some common failure scenarios.
    I created a Kubernetes masterclass on this subject called *Recovering from a disaster
    with Rancher and Kubernetes*, which can be found at https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery,
    with the YouTube video located at https://www.youtube.com/watch?v=qD2kFA8THrY.
    I cover some of the training scenarios that I have created in this class. Each
    scenario has a script for deploying a lab cluster and breaking it. I then dive
    into troubleshooting and restoring/recovering steps for each scenario. Finally,
    it ends with some preventive tasks. I usually recommend new customers go through
    these scenarios at least once before rolling Rancher/RKE into production. It should
    be something you are comfortable with and have a documented process for. This
    includes verifying you have the correct permissions or have a documented process
    for getting them. Typically, you'll need root/sudo permissions on all etcd, control
    plane, and master nodes.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过练习一些常见的故障场景来结束本章。我创建了一个关于此主题的Kubernetes课程，名为*使用Rancher和Kubernetes从灾难中恢复*，可以在
    https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery
    上找到，YouTube视频位于 https://www.youtube.com/watch?v=qD2kFA8THrY。我在这个课程中介绍了一些我创建的训练场景。每个场景都有一个部署实验集群并故障的脚本。然后，我深入讲解了每个场景的故障排除和恢复步骤。最后，课程以一些预防性任务结束。我通常建议新客户至少在将Rancher/RKE投入生产前完成这些场景一次。这应该是你感到舒适并且有文档化流程的内容。这包括验证你是否拥有正确的权限，或者是否有文档化的流程来获取这些权限。通常，你需要在所有etcd、控制平面和主节点上拥有root/sudo权限。
- en: Summary
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about RKE, RKE2, k3s, and etcd backups and recovery.
    This includes how the backup and restore process works. We learned about the limitations
    of etcd backups. We then covered how to configure scheduled backups. We finally
    went into detail about the steps for taking a one-time backup and restoring from
    a snapshot. We ended the chapter by talking about the *Recovering from a disaster
    with Rancher and Kubernetes* masterclass. At this point, you should be comfortable
    backing up and restoring your cluster, including using etcd backups to recover
    from catastrophic failure.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们学习了RKE、RKE2、k3s以及etcd的备份与恢复，包括备份和恢复过程的工作原理。我们了解了etcd备份的局限性。接着，我们介绍了如何配置定期备份。最后，我们详细讲解了一次性备份和从快照恢复的步骤。本章的结尾，我们讨论了*使用Rancher和Kubernetes从灾难中恢复*的课程。到此为止，你应该已经能够轻松地备份和恢复你的集群，包括使用etcd备份从灾难性故障中恢复。
- en: The next chapter will cover monitoring and logging in Rancher.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讲解Rancher中的监控和日志记录。
