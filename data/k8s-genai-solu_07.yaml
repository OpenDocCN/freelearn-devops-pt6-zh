- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Cost Optimization of GenAI Applications on Kubernetes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes上优化GenAI应用程序的成本
- en: In this chapter, we will cover the key cost components for deploying GenAI applications
    in the cloud, covering compute, storage, and networking costs. We will then cover
    options to optimize these costs, such as *right-sizing* resources to prevent over-provisioning,
    thinking through *efficient storage management*, and *networking best practices*.
    This chapter will cover monitoring and optimization tools, such as Kubecost, to
    identify resource utilization patterns and cost-saving opportunities.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖在云端部署GenAI应用程序的关键成本组成部分，包括计算、存储和网络成本。接着，我们将讨论优化这些成本的选项，例如通过*调整资源规模*来防止过度配置，思考如何进行*高效存储管理*，以及*网络最佳实践*。本章还将介绍监控和优化工具，如Kubecost，以识别资源使用模式和节省成本的机会。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要主题：
- en: Understanding the key cost components
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解关键成本组成部分
- en: Cost optimization techniques
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本优化技术
- en: Understanding the key cost components
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解关键成本组成部分
- en: 'Key cost components while deploying an application in the cloud typically involve
    compute, storage, and networking costs:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在云端部署应用程序时，主要的成本组成部分通常包括计算、存储和网络成本：
- en: '**Compute costs**: Compute could be a significant cost driver for GenAI applications
    because of their resource-intensive nature. Compute costs are based on the instance
    size, which includes CPU, GPU, and memory sizes. On AWS, these compute instances
    are billed on a per-second basis, with a minimum of 60 seconds. So, after the
    first minute, these costs are billed in one-second increments. Refer to AWS pricing
    documentation at [https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/)
    for a deeper understanding.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算成本**：计算可能是GenAI应用程序的一个重要成本因素，因为其资源密集型的特性。计算成本基于实例大小，包括CPU、GPU和内存大小。在AWS上，这些计算实例按秒计费，最小为60秒。所以，在第一个分钟后，这些费用将按秒增量计费。欲了解更深层次的内容，请参考AWS定价文档：[https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/)。'
- en: '**Storage costs**: GenAI models often need large amounts of data for training
    and inference, so storage is another critical cost component. Key storage costs
    include object storage and block storage. Object storage, such as Amazon S3, is
    typically used for storing datasets such as image, text, or video files. The object
    storage costs are usually based on the volume of data stored (GB/month) and any
    associated retrieval fees. Block storage, such as **Amazon Elastic Block Storage**
    (**EBS**), offers block-level storage volumes that can be attached to Amazon EC2
    instances. EBS is commonly used for workloads requiring consistent, low-latency
    access in GenAI applications, such as model checkpoints, logs, and other intermediate
    files. Block storage costs are based on storage volume and type, such as **solid
    state disks** (**SSDs**) versus **hard disk** **drives** (**HDDs**).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储成本**：GenAI模型通常需要大量的数据用于训练和推理，因此存储是另一个关键的成本组成部分。主要的存储成本包括对象存储和块存储。对象存储，如Amazon
    S3，通常用于存储数据集，例如图像、文本或视频文件。对象存储的费用通常基于存储的数据量（GB/月）和任何相关的取回费用。块存储，如**Amazon Elastic
    Block Storage**（**EBS**），提供可以附加到Amazon EC2实例的块级存储卷。EBS通常用于GenAI应用程序中需要一致、低延迟访问的工作负载，如模型检查点、日志和其他中间文件。块存储的费用基于存储量和类型，例如**固态硬盘**（**SSDs**）与**硬盘驱动器**（**HDDs**）的区别。'
- en: '**Networking costs**: Networking costs can add up in cloud deployments, especially
    for GenAI applications that involve large-scale data transfers across regions
    or availability zones. Networking cost components include ingress/ egress costs,
    cross-region transfer costs, NAT gateway costs, and **content delivery network**
    (**CDN**) costs.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络成本**：在云部署中，网络成本可能会增加，特别是对于涉及跨区域或可用区的大规模数据传输的GenAI应用程序。网络成本组成部分包括流入/流出费用、跨区域传输费用、NAT网关费用以及**内容分发网络**（**CDN**）费用。'
- en: '**Egress costs** include costs for data transferred out of the cloud. If one
    is serving large AI models to end users or moving data between regions or to on-premises
    environments, these costs could add up. **Ingress costs** for the inbound data
    in the cloud are often lower or free. If an application involves communication
    between multiple cloud regions, availability zones, or different cloud providers,
    such as in hybrid or multi-cloud setups, inter-region or outbound *data transfer
    costs* could also be significant.'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外流成本**包括从云中传输出的数据的费用。如果需要将大型 AI 模型提供给最终用户，或者在区域之间或与本地环境之间移动数据，这些费用可能会累积。云中入站数据的
    **入流成本** 通常较低或免费。如果一个应用程序涉及多个云区域、可用区或不同云提供商之间的通信，例如在混合云或多云设置中，区域间或外部的 *数据传输成本*
    也可能非常高。'
- en: '**NAT gateway costs** include both fixed hourly charges as well as the data
    processing charges based on the amount of data transferred. Data transfer costs
    vary based on the direction of data flow. Inbound data transfer into the cloud
    from the internet is usually free; however, outbound data transfer from the cloud
    to the internet can incur charges based on the amount of data (GB).'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NAT 网关费用**包括固定的每小时费用以及基于数据传输量的处理费用。数据传输成本根据数据流向的不同而有所不同。来自互联网的入站数据传输通常是免费的；然而，从云中到互联网的外部数据传输可能会根据数据量（GB）收取费用。'
- en: We’ve covered various cost components involved in running GenAI applications.
    Now, let’s explore how to gain granular visibility into infrastructure costs in
    **Kubernetes** (**K8s**) clusters. There are several cost allocation tools available
    in K8s and cloud environments. Some examples include **OpenCost** ([https://www.opencost.io/](https://www.opencost.io/)),
    **Kubecost** ([https://www.kubecost.com/](https://www.kubecost.com/)), Spot.io
    ([https://spot.io/](https://spot.io/)), **Cast.ai** ([https://cast.ai/](https://cast.ai/)),
    **PerfectScale** ([https://www.perfectscale.io/](https://www.perfectscale.io/)),
    **IBM Cloudability** ([https://www.apptio.com/products/cloudability/](https://www.apptio.com/products/cloudability/)),
    **Harness** ([https://www.harness.io/](https://www.harness.io/)), cloud-provider-specific
    solutions, and so on. In this chapter, we will explore Kubecost for K8s cluster
    cost analysis.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了运行 GenAI 应用程序中涉及的各种成本组件。现在，让我们探索如何在 **Kubernetes**（**K8s**）集群中获得对基础设施成本的细粒度可视化。K8s
    和云环境中有多个可用的成本分配工具。一些示例包括 **OpenCost** ([https://www.opencost.io/](https://www.opencost.io/))、**Kubecost**
    ([https://www.kubecost.com/](https://www.kubecost.com/))、Spot.io ([https://spot.io/](https://spot.io/))、**Cast.ai**
    ([https://cast.ai/](https://cast.ai/))、**PerfectScale** ([https://www.perfectscale.io/](https://www.perfectscale.io/))、**IBM
    Cloudability** ([https://www.apptio.com/products/cloudability/](https://www.apptio.com/products/cloudability/))、**Harness**
    ([https://www.harness.io/](https://www.harness.io/))、云服务提供商特定的解决方案等。在本章中，我们将探讨
    Kubecost 用于 K8s 集群成本分析。
- en: Kubecost
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubecost
- en: Kubecost is a cost monitoring and optimization tool designed for K8s environments.
    It helps to track, allocate, and optimize costs by providing detailed insights
    into various cost components associated with running workloads in K8s clusters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Kubecost 是一款专为 K8s 环境设计的成本监控与优化工具。它通过提供有关在 K8s 集群中运行工作负载的各类成本组件的详细洞察，帮助跟踪、分配和优化成本。
- en: 'Kubecost can help visualize cost components for K8s deployments by providing
    the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Kubecost 可以通过提供以下功能帮助可视化 K8s 部署的成本组件：
- en: '**Cost breakdown by namespace, Pod, and service**: Kubecost allows us to see
    detailed costs allocated to different K8s objects, such as *namespaces*, to track
    the costs of different teams or applications. It can also highlight the cost attribution
    at the individual Pod level, services level, or deployments level. Kubecost can
    aggregate the costs for specific services or deployments, giving a clear picture
    of how much we’re spending on a microservice or specific deployment. This granular
    breakdown is especially helpful for a multi-tenant K8s cluster, where multiple
    teams or services could be sharing the same cluster.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按命名空间、Pod 和服务的成本细分**：Kubecost 允许我们查看分配给不同 K8s 对象的详细成本，如 *命名空间*，以跟踪不同团队或应用程序的成本。它还可以突出显示在单个
    Pod 层级、服务层级或部署层级的成本归属。Kubecost 可以汇总特定服务或部署的成本，清晰展示我们在微服务或特定部署上花费了多少。这种细粒度的细分对于多租户
    K8s 集群尤为重要，因为多个团队或服务可能共享同一个集群。'
- en: '**Compute costs**: Kubecost can track the EC2 instance costs, which can help
    determine whether we are using the most cost-effective node types. Kubecost can
    also highlight whether any node or compute resource is underutilized.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算成本**：Kubecost 可以跟踪 EC2 实例成本，帮助您判断是否使用了最具成本效益的节点类型。Kubecost 还可以突出显示任何节点或计算资源是否未被充分利用。'
- en: '**Storage costs**: Kubecost can track the costs of storage volumes attached
    to K8s workloads. Kubecost can distinguish between different EBS volume types,
    such as gp2, gp3, and io1, and their costs, helping you optimize based on performance
    requirements and costs. It can also detect unused or underutilized volumes.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储成本**：Kubecost 可以跟踪附加到 K8s 工作负载的存储卷成本。Kubecost 可以区分不同类型的 EBS 卷，如 gp2、gp3
    和 io1，并分析它们的成本，帮助您根据性能需求和成本优化。它还可以检测未使用或低效利用的存储卷。'
- en: '**Network costs**: Kubecost can track networking-related costs, such as data
    transfer costs between different nodes, regions, or even AWS services. This can
    help optimize the network configuration and reduce unnecessary cross-region or
    cross-AZ data transfers.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络成本**：Kubecost 可以跟踪与网络相关的成本，如不同节点、区域之间，甚至 AWS 服务之间的数据传输费用。这有助于优化网络配置，减少不必要的跨区域或跨
    AZ 数据传输。'
- en: '**Ingress/egress**: One can track and visualize the costs of traffic flowing
    into and out of your K8s cluster, which is particularly relevant when serving
    external applications or users.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流量进出**：可以跟踪和可视化进出您的 K8s 集群的流量成本，尤其在为外部应用程序或用户提供服务时，这一点尤为相关。'
- en: '**Load balancers**: Kubecost identifies and breaks down the costs of load balancers
    (such as AWS NLB or ALB) that are associated with K8s services or ingress resources.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载均衡器**：Kubecost 可以识别并分解与 K8s 服务或入口资源相关的负载均衡器（如 AWS NLB 或 ALB）成本。'
- en: '**Integration with AWS Cost and Usage Reports (CUR)**: Kubecost can integrate
    with AWS CUR ([https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html](https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html))
    to provide a comprehensive view of costs, including non-K8s AWS services that
    your EKS workloads rely on, such as Amazon S3 or RDS. If you have multiple EKS
    clusters, Kubecost can also aggregate costs across all your clusters and provide
    insights at a global level or drill down into specific clusters for more detailed
    cost analysis.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与 AWS 成本和使用报告 (CUR) 的集成**：Kubecost 可以与 AWS CUR ([https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html](https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html))
    集成，提供全面的成本视图，包括您的 EKS 工作负载所依赖的非 K8s AWS 服务，如 Amazon S3 或 RDS。如果您有多个 EKS 集群，Kubecost
    还可以汇总所有集群的成本，并提供全球层面的洞察，或深入特定集群进行更详细的成本分析。'
- en: Kubecost supports *cost allocation by labels*, which is especially useful for
    multi-tenant environments where one needs to attribute costs to different teams,
    projects, or environments (e.g., development, QA, staging, or production). Kubecost
    provides historical cost tracking and allows you to visualize cost trends over
    time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Kubecost 支持 *按标签分配成本*，这对于多租户环境尤其有用，在这种环境中，需要将成本归属到不同的团队、项目或环境（例如开发、QA、预发布或生产）。Kubecost
    提供历史成本追踪，并允许您可视化成本趋势。
- en: Kubecost can not only help you to visualize costs, but also provide actionable
    recommendations to optimize spending, such as *right-sizing recommendations* by
    analyzing resource usage (CPU, memory, storage) and recommending resizing your
    workloads to avoid overprovisioning or underutilization of resources. Kubecost
    can suggest where you could switch to **EC2 Spot Instances** to save on compute
    costs, which is especially relevant for non-critical or interruptible workloads.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Kubecost 不仅可以帮助您可视化成本，还能提供可操作的优化建议，例如通过分析资源使用情况（CPU、内存、存储）并建议调整工作负载大小，从而避免资源的过度配置或低效利用，*推荐合适的资源规模*。Kubecost
    还可以建议您在哪些地方切换到 **EC2 Spot 实例**，以节省计算成本，这对于非关键或可中断的工作负载尤其重要。
- en: Setting up Kubecost
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 Kubecost
- en: Kubecost can be installed in our EKS cluster as a Helm chart. Refer to the Amazon
    EKS integration in the Kubecost documentation at [https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=installations-amazon-eks-integration](https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=installations-amazon-eks-integration)
    for various installation options.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Kubecost 可以作为 Helm chart 安装在我们的 EKS 集群中。有关不同安装选项，请参阅 Kubecost 文档中的 Amazon EKS
    集成部分：[https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=installations-amazon-eks-integration](https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=installations-amazon-eks-integration)。
- en: 'In our setup, we will install Kubecost using the Terraform Helm provider. Download
    the `addons.tf` file from [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch7/addons.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch7/addons.tf)
    to the Terraform project folder and run the following commands:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的设置中，我们将使用Terraform Helm提供程序安装Kubecost。从[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch7/addons.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch7/addons.tf)下载`addons.tf`文件到Terraform项目文件夹，并运行以下命令：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can verify the installation using the following command, which displays
    the status, version, and other details of the deployment:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令验证安装情况，该命令显示部署的状态、版本及其他详细信息：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To access the Kubecost UI console, run the following command to enable port
    forwarding:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问Kubecost UI控制台，请运行以下命令以启用端口转发：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can now access the Kubecost UI by visiting http://localhost:9090 in your
    web browser. In the Kubecost UI console, expand the **Monitor** section in the
    left-hand side panel. You will see various dashboards such as **Allocations**,
    **Assets**, **Cloud Costs**, **Network**, **Clusters**, **External Costs**, and
    so on, which provide cost visualization of K8s workloads, savings recommendations,
    and governance tools. For example, select **Allocations** to navigate to the Allocations
    dashboard, as shown in *Figure 7**.1*, which allows you to view the allocated
    spend across all native K8s constructs such as namespaces, services, deployments,
    and K8s labels.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以通过在浏览器中访问http://localhost:9090来访问Kubecost UI。在Kubecost UI控制台中，展开左侧面板中的**Monitor**部分。你将看到各种仪表盘，如**分配**、**资产**、**云费用**、**网络**、**集群**、**外部费用**等，这些都提供了K8s工作负载的成本可视化、节省建议和治理工具。例如，选择**分配**以导航到分配仪表盘，如*图7**.1*所示，它允许你查看所有原生K8s构造（如命名空间、服务、部署和K8s标签）上的分配开支。
- en: '![Figure 7.1 – The Kubecost Allocations dashboard](img/B31108_07_1.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – Kubecost分配仪表盘](img/B31108_07_1.jpg)'
- en: Figure 7.1 – The Kubecost Allocations dashboard
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – Kubecost分配仪表盘
- en: To view the costs of our GenAI applications deployed in [*Chapter 5*](B31108_05.xhtml#_idTextAnchor062),
    change the `Aggregate By` query from *Namespace* to *Deployment* and apply the
    *default* namespace filter. You will see the costs for the fine-tuned Llama 3
    deployment, RAG API, and Chatbot UI applications, as shown in *Figure 7**.2*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看部署在[*第5章*](B31108_05.xhtml#_idTextAnchor062)中的GenAI应用程序的费用，请将`Aggregate By`查询从*命名空间*更改为*部署*，并应用*默认*命名空间筛选器。你将看到经过微调的Llama
    3部署、RAG API和聊天机器人UI应用程序的费用，如*图7**.2*所示。
- en: Important note
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Kubecost only monitors the costs from the time it is installed on the cluster,
    so the costs for the Llama 3 fine-tuning job are not available. You can rerun
    the job to view those costs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Kubecost仅监控自安装在集群上的时间开始的费用，因此Llama 3微调作业的费用不可用。你可以重新运行该作业以查看相关费用。
- en: '![Figure 7.2 – Cost analysis of GenAI applications](img/B31108_07_2.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – GenAI应用程序的成本分析](img/B31108_07_2.jpg)'
- en: Figure 7.2 – Cost analysis of GenAI applications
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – GenAI应用程序的成本分析
- en: Refer to the Kubecost documentation at [https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=navigating-kubecost-ui](https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=navigating-kubecost-ui)
    to learn more about these dashboards.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[Kubecost文档](https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=navigating-kubecost-ui)以了解有关这些仪表盘的更多信息。
- en: In this section, we explored the various cost components involved in running
    GenAI applications on K8s, such as compute, storage, networking, and so on. We
    also looked into various tools for gaining deeper visibility into K8s workload
    costs, deployed Kubecost on our EKS cluster, and used the Allocations dashboard
    to aggregate costs by namespace and deployment.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了运行GenAI应用程序所涉及的各种成本组件，如计算、存储、网络等。我们还了解了多种工具，以深入了解K8s工作负载的成本，部署了Kubecost在我们的EKS集群上，并使用分配仪表盘按命名空间和部署汇总费用。
- en: In the next section, we will dive into various cost optimization techniques.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将深入探讨各种成本优化技术。
- en: Cost optimization techniques
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本优化技术
- en: 'To effectively reduce the costs of running GenAI workloads on K8s, it is important
    to optimize each of the key cost components: compute, storage, and networking.
    In this section, we will discuss various strategies for each of these components
    to lower the costs while maintaining the best performance.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效降低在 K8s 上运行 GenAI 工作负载的成本，重要的是优化每个关键成本组件：计算、存储和网络。在这一部分，我们将讨论每个组件的各种策略，以降低成本同时保持最佳性能。
- en: Compute best practices
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算最佳实践
- en: Compute is often the most significant component of GenAI costs, as these applications
    typically require access to specialized hardware such as GPUs, which are expensive
    and scarce. Let’s look at various techniques to efficiently utilize the compute
    resources and lower the costs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 计算通常是 GenAI 成本中最重要的部分，因为这些应用程序通常需要访问像 GPU 这样昂贵且稀缺的专业硬件。让我们看看各种技术，如何高效利用计算资源并降低成本。
- en: Right-sizing resources
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 资源调整
- en: Right-sizing resources is the most important step in optimizing the cost efficiency
    of GenAI workloads. This involves understanding the nature of the applications
    by profiling them and configuring the appropriate resource requests (CPU, memory,
    GPU) based on the actual utilization of the K8s workloads.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 资源调整是优化 GenAI 工作负载成本效率的最重要步骤。这涉及通过分析应用程序的运行情况并根据 K8s 工作负载的实际利用情况配置适当的资源请求（CPU、内存、GPU），从而理解应用程序的特性。
- en: Right-sizing resources in K8s can minimize waste and maximize efficiency. For
    example, under-provisioning resources can lead to performance degradation and
    poor user experience, whereas over-provisioning can result in unnecessary cloud
    spending. By accurately setting resource requests and limits, the user can strike
    a balance between performance and cost. Selecting the right size instances enhances
    cluster density, allowing more workloads to run on fewer nodes, further optimizing
    costs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 K8s 中调整资源大小可以最大限度地减少浪费并提高效率。例如，资源配置不足可能导致性能下降和用户体验差，而资源配置过多则会导致不必要的云开支。通过准确设置资源请求和限制，用户可以在性能和成本之间找到平衡。选择合适的实例大小可以提高集群密度，使更多工作负载能够在更少的节点上运行，从而进一步优化成本。
- en: There are a number of tools in the K8s community that help us estimate the resource
    requests and limits. Some notable ones are **Goldilocks** ([https://goldilocks.docs.fairwinds.com/](https://goldilocks.docs.fairwinds.com/)),
    **StormForge** ([https://stormforge.io/optimize-live/](https://stormforge.io/optimize-live/)),
    **KRR** ([https://github.com/robusta-dev/krr](https://github.com/robusta-dev/krr)),
    **Kubecost**, and so on. We will delve into a few details about Goldilocks here.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 K8s 社区中，有许多工具可以帮助我们估算资源请求和限制。一些值得注意的工具有 **Goldilocks** ([https://goldilocks.docs.fairwinds.com/](https://goldilocks.docs.fairwinds.com/))、**StormForge**
    ([https://stormforge.io/optimize-live/](https://stormforge.io/optimize-live/))、**KRR**
    ([https://github.com/robusta-dev/krr](https://github.com/robusta-dev/krr))、**Kubecost**
    等等。我们将在这里深入探讨 Goldilocks 的一些细节。
- en: '**Goldilocks** is a tool designed to help K8s users optimize their resource
    requests and limits, which improves the efficiency and cost-effectiveness of K8s
    clusters. Goldilocks uses Vertical Pod Autoscaler (VPA) in *Recommender* mode
    to suggest the optimal resource requests and limits for your K8s Pods. VPA monitors
    the actual CPU and memory usage of running Pods over time. It gathers usage data
    directly from K8s metrics. Goldilocks takes the historical CPU and memory utilization
    data from VPA and provides recommended resource requests and limits based on the
    actual needs of your application. These recommendations help ensure that you’re
    not over-provisioning or under-provisioning resources. Goldilocks provides a dashboard
    or CLI tool to visualize its findings. It displays the current resource requests/limits
    and the recommended values based on the observed usage.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**Goldilocks** 是一款旨在帮助 K8s 用户优化资源请求和限制的工具，它提高了 K8s 集群的效率和成本效益。Goldilocks 使用垂直
    Pod 自动扩展器（VPA）在 *推荐器* 模式下，建议 K8s Pods 的最佳资源请求和限制。VPA 会监控运行中 Pods 的实际 CPU 和内存使用情况，并持续收集来自
    K8s 度量的使用数据。Goldilocks 获取 VPA 的历史 CPU 和内存利用数据，并根据应用程序的实际需求提供推荐的资源请求和限制。这些建议有助于确保你不会过度配置或配置不足资源。Goldilocks
    提供了一个仪表盘或 CLI 工具，用于可视化其结果。它展示了当前的资源请求/限制以及基于观察到的使用情况推荐的值。'
- en: 'Refer to the Goldilocks installation guide at [https://goldilocks.docs.fairwinds.com/installation/](https://goldilocks.docs.fairwinds.com/installation/)
    for detailed instructions on setting up Goldilocks in your EKS cluster. After
    the installation, you can enable monitoring by labeling the target namespace with
    `goldilocks.fairwinds.com/enabled=true`. For example, you can execute the following
    command to enable monitoring on the default namespace:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅Goldilocks安装指南[https://goldilocks.docs.fairwinds.com/installation/](https://goldilocks.docs.fairwinds.com/installation/)以获取有关在EKS集群中设置Goldilocks的详细说明。安装完成后，你可以通过使用`goldilocks.fairwinds.com/enabled=true`标记目标命名空间来启用监控。例如，你可以执行以下命令在默认命名空间上启用监控：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once the target namespaces are labeled, you can view the recommendations in
    the Goldilocks UI dashboard, as shown in *Figure 7**.3*, which displays resource
    usage recommendations for our Chatbot UI application.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦目标命名空间被标记，你可以在Goldilocks UI控制面板中查看推荐项，如*图 7.3*所示，该面板显示了我们Chatbot UI应用程序的资源使用推荐。
- en: '![Figure 7.3 – The Goldilocks dashboard](img/B31108_07_3.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – Goldilocks 控制面板](img/B31108_07_3.jpg)'
- en: Figure 7.3 – The Goldilocks dashboard
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – Goldilocks 控制面板
- en: Kubecost also provides right-sizing recommendations for the K8s workloads. Select
    **Savings** from the left-hand side menu in the Kubecost UI console to view the
    cost savings recommendations. **Savings Insights** provides various recommendations,
    such as right-sizing cluster nodes, containers, remedying abandoned workloads,
    and so on, to lower the K8s and cloud costs, as shown in *Figure 7**.4*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Kubecost还提供了K8s工作负载的右-sizing建议。在Kubecost UI控制台的左侧菜单中选择**Savings**查看节省成本的建议。**节省见解**提供了各种建议，如右-sizing集群节点、容器、修复废弃工作负载等，以降低K8s和云成本，如*图
    7.4*所示。
- en: '![Figure 7.4 – Kubecost savings insights](img/B31108_07_4.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – Kubecost 节省见解](img/B31108_07_4.jpg)'
- en: Figure 7.4 – Kubecost savings insights
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – Kubecost 节省见解
- en: Learn more about these insights in the Kubecost documentation at [https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=ui-savings](https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=ui-savings).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在[https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=ui-savings](https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=ui-savings)的Kubecost文档中了解更多这些见解。
- en: Compute capacity options
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算容量选项
- en: 'When deploying workloads on the cloud, you have several options for managing
    compute capacity. These options vary in terms of cost, performance, and availability.
    The following is a breakdown of the different capacity types available for Amazon
    EKS, including **Reserved Instances** (**RIs**), Spot Instances, and x86 versus
    ARM architecture choices:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在云上部署工作负载时，你有多种选择来管理计算容量。这些选项在成本、性能和可用性方面有所不同。以下是适用于Amazon EKS的不同容量类型的细分，包括**预留实例**（**RIs**）、Spot实例以及x86与ARM架构的选择：
- en: '**EC2 on-demand instances** ([https://aws.amazon.com/ec2/pricing/on-demand/](https://aws.amazon.com/ec2/pricing/on-demand/))
    are the default capacity type when deploying on EKS. They provide a flexible compute
    option without any long-term commitments, and you pay by the minute or second
    for the instances you use. On-demand instances are the most expensive, but they
    offer the highest level of flexibility and availability. This flexibility is especially
    beneficial during development and experimentation phases of GenAI workloads, where
    workload patterns and resource requirements may vary significantly.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EC2按需实例** ([https://aws.amazon.com/ec2/pricing/on-demand/](https://aws.amazon.com/ec2/pricing/on-demand/))
    是在EKS上部署时的默认容量类型。它们提供灵活的计算选项，无需长期承诺，你按使用的实例按分钟或秒计费。按需实例是最贵的，但提供了最高的灵活性和可用性。这种灵活性在GenAI工作负载的开发和实验阶段特别有用，因为这些工作负载的模式和资源需求可能有显著差异。'
- en: '**EC2 RIs** ([https://aws.amazon.com/ec2/pricing/reserved-instances/](https://aws.amazon.com/ec2/pricing/reserved-instances/))
    provide a significant discount (up to 72%) compared to on-demand pricing in exchange
    for a one- or three-year commitment. RIs are well-suited for predictable workloads
    where you expect consistent usage over time. There are two different kinds of
    Ris:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EC2预留实例** ([https://aws.amazon.com/ec2/pricing/reserved-instances/](https://aws.amazon.com/ec2/pricing/reserved-instances/))
    提供了与按需定价相比显著的折扣（最高可达72%），以换取一到三年的承诺。预留实例非常适合可预测的工作负载，在这些工作负载中，你期望随着时间的推移保持一致的使用量。预留实例有两种不同的类型：'
- en: '**Standard RIs**: These provide the largest discounts but require a longer
    commitment for a given instance type. Usually, the larger the commitment period,
    the larger the discount.'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准 RIs**：这些提供最大的折扣，但需要为特定的实例类型做出更长时间的承诺。通常，承诺期越长，折扣越大。'
- en: '**Convertible RIs**: These offer the flexibility to change the instance families,
    operating system, or tenancy during the commitment period. This flexibility comes
    at a slightly smaller discount compared to that of Standard RIs.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可转换 RIs**：这些提供了在承诺期内更改实例系列、操作系统或租用方式的灵活性。与标准 RIs 相比，这种灵活性提供的折扣略小。'
- en: '**EC2 Spot Instances** ([https://aws.amazon.com/ec2/spot/](https://aws.amazon.com/ec2/spot/))
    allow you to use spare EC2 capacity at a significantly lower cost (up to 90%).
    However, Spot Instances can be interrupted by AWS when it needs the capacity back,
    so they are suited for fault-tolerant workloads. You’ll receive a two-minute notice
    before your Spot Instance is reclaimed by AWS. It’s essential to architect your
    workloads to handle interruptions gracefully, using techniques such as checkpointing,
    distributed job management, or backup on-demand instances. Spot Instances can
    be used for batch processing, stateless web servers, CI/CD pipelines, or any other
    workloads that can tolerate occasional interruptions or delays. Tools such as
    **Ray,** **Kubeflow**, and **Horovod** ([https://github.com/horovod/horovod](https://github.com/horovod/horovod))
    can be configured to leverage Spot Instances for running distributed training/fine-tuning
    of GenAI workloads, offering features such as *checkpointing*, *interruption handling*,
    and so on. When combined with a compute autoscaling solution such as **Karpenter**,
    these tools can automatically fall back to on-demand capacity when spot capacity
    is not available, ensuring both cost efficiency and reliability.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EC2 Spot 实例** ([https://aws.amazon.com/ec2/spot/](https://aws.amazon.com/ec2/spot/))
    允许你以显著更低的成本（最多可节省 90%）使用闲置的 EC2 容量。然而，当 AWS 需要恢复容量时，Spot 实例可能会被中断，因此它们适用于容错工作负载。在
    AWS 回收你的 Spot 实例之前，你会收到两分钟的通知。设计你的工作负载时，必须考虑如何优雅地处理中断，使用如检查点、分布式作业管理或按需实例备份等技术。Spot
    实例可用于批处理、无状态 Web 服务器、CI/CD 管道或任何能够容忍偶尔中断或延迟的工作负载。像 **Ray**、**Kubeflow** 和 **Horovod**
    ([https://github.com/horovod/horovod](https://github.com/horovod/horovod)) 这样的工具可以配置为利用
    Spot 实例运行 GenAI 工作负载的分布式训练/微调，提供如 *检查点*、*中断处理* 等功能。当与像 **Karpenter** 这样的计算自动扩展解决方案结合使用时，这些工具可以在没有
    Spot 容量时自动回退到按需容量，确保既具成本效益又可靠。'
- en: '**Savings Plans** ([https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html](https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html))
    offer an alternative to RIs by providing flexibility in instance types and sizes
    while providing a significant discount for committing to a consistent amount of
    usage over a one- or three-year period. You can apply Savings Plan discounts across
    different EC2 instance families, AWS regions, and even compute services such as
    EC2, AWS Fargate, and AWS Lambda.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Savings Plans** ([https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html](https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html))
    通过提供实例类型和大小的灵活性，为 RIs 提供了一种替代方案，同时承诺在一年或三年的时间里维持一定的使用量，从而获得显著的折扣。你可以将 Savings
    Plan 的折扣应用于不同的 EC2 实例系列、AWS 区域，甚至计算服务，如 EC2、AWS Fargate 和 AWS Lambda。'
- en: '**AWS Graviton instances (ARM-based)** ([https://aws.amazon.com/ec2/graviton/](https://aws.amazon.com/ec2/graviton/))
    AWS offers two main processor architectures for EC2 instances: **x86-based** (typically
    Intel or AMD processors) and **ARM-based** (AWS Graviton processors). Choosing
    between these can impact both performance and cost. Graviton-based instances can
    offer up to 40% better price performance for various applications. Refer to the
    AWS documentation at [https://aws.amazon.com/ec2/instance-explorer/](https://aws.amazon.com/ec2/instance-explorer/)
    to learn more about various Graviton instance types and respective use cases.
    These instances provide a cost-effective and efficient compute option for CPU-intensive
    parts of GenAI workloads, such as data preparation, lightweight model inferencing
    (when GPU acceleration is not needed), chatbot UIs, and other microservice-based
    workloads.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Graviton instances (基于ARM架构)** ([https://aws.amazon.com/ec2/graviton/](https://aws.amazon.com/ec2/graviton/))
    AWS提供了两种主要的处理器架构用于EC2实例：**x86架构**（通常是Intel或AMD处理器）和**基于ARM架构**（AWS Graviton处理器）。在这两者之间的选择可能影响性能和成本。基于Graviton的实例可以为各种应用提供高达40%的性价比优势。请参阅AWS文档，了解更多有关各种Graviton实例类型及其用途的信息，网址是[https://aws.amazon.com/ec2/instance-explorer/](https://aws.amazon.com/ec2/instance-explorer/)。这些实例为GenAI工作负载的CPU密集部分提供了一种经济高效的计算选择，例如数据准备、轻量级模型推断（当不需要GPU加速时）、聊天机器人UI和其他基于微服务的工作负载。'
- en: '**AWS Fargate** ([https://aws.amazon.com/fargate/](https://aws.amazon.com/fargate/))
    is a serverless compute engine for containers that works with both Amazon ECS
    and Amazon EKS. It allows the running of K8s Pods without managing the underlying
    EC2 infrastructure. This provides a *serverless* experience, where you only pay
    for the compute resources used by the Pods. With the Fargate capacity type, there
    is no need to manage EC2 instances for OS updates, patching, or node scaling.
    Like Graviton, we can utilize AWS Fargate for data preparation, chatbot interfaces,
    and other microservice-based workloads.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Fargate** ([https://aws.amazon.com/fargate/](https://aws.amazon.com/fargate/))
    是一种面向容器的无服务器计算引擎，可与Amazon ECS和Amazon EKS配合使用。它允许在不管理底层EC2基础设施的情况下运行K8s Pods。这提供了一种*无服务器*体验，您只需支付Pods使用的计算资源费用。使用Fargate容量类型，无需管理EC2实例进行操作系统更新、打补丁或节点扩展。与Graviton一样，我们可以利用AWS
    Fargate进行数据准备、聊天机器人界面和其他基于微服务的工作负载。'
- en: '**EC2 Capacity Blocks for ML** ([https://aws.amazon.com/ec2/capacityblocks/](https://aws.amazon.com/ec2/capacityblocks/))
    provides reserved accelerated compute capacity to run AI/ML workloads in AWS for
    a future start date. EC2 Capacity Blocks supports EC2 P5e ([https://aws.amazon.com/ec2/instance-types/p5/](https://aws.amazon.com/ec2/instance-types/p5/)),
    P5 ([https://aws.amazon.com/ec2/instance-types/p5/](https://aws.amazon.com/ec2/instance-types/p5/)),
    P4d ([https://aws.amazon.com/ec2/instance-types/p4/](https://aws.amazon.com/ec2/instance-types/p4/)),
    and other EC2 instances powered by NVIDIA GPUs, Trn1 and Trn2 instances powered
    by AWS Trainium processor. These help to ensure the guaranteed capacity for model
    experimentation, scheduling large training, and fine-tuning jobs. Capacity Blocks
    are co-located in **Amazon EC2 UltraClusters** ([https://aws.amazon.com/ec2/ultraclusters/](https://aws.amazon.com/ec2/ultraclusters/)),
    designed for high-performance ML workloads and providing *low-latency, high-throughput
    network connectivity* for distributed training.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EC2 Capacity Blocks for ML** ([https://aws.amazon.com/ec2/capacityblocks/](https://aws.amazon.com/ec2/capacityblocks/))
    为将来启动日期提供预留的加速计算能力，用于在AWS上运行AI/ML工作负载。EC2 Capacity Blocks支持EC2 P5e ([https://aws.amazon.com/ec2/instance-types/p5/](https://aws.amazon.com/ec2/instance-types/p5/))、P5
    ([https://aws.amazon.com/ec2/instance-types/p5/](https://aws.amazon.com/ec2/instance-types/p5/))、P4d
    ([https://aws.amazon.com/ec2/instance-types/p4/](https://aws.amazon.com/ec2/instance-types/p4/))，以及由NVIDIA
    GPU提供动力支持的其他EC2实例，Trn1和Trn2实例由AWS Trainium处理器提供支持。这些有助于确保模型实验、调度大规模培训和微调作业的保证容量。容量块位于**Amazon
    EC2 UltraClusters** ([https://aws.amazon.com/ec2/ultraclusters/](https://aws.amazon.com/ec2/ultraclusters/))
    中，专为高性能ML工作负载设计，并为分布式训练提供*低延迟、高吞吐量的网络连接*。'
- en: In this section, we explored best practices for optimizing the compute costs
    associated with running GenAI workloads in K8s clusters. Compute resources, including
    CPU, GPU, and custom accelerator nodes, are often the largest contributors to
    operational expenses, especially for GenAI models. We covered techniques such
    as right-sizing resources and using tools such as Kubecost and Goldilocks to get
    right-sizing recommendations, using different capacity types such as Spot Instances,
    RIs, and Capacity Blocks, and using Savings Plans for different types of workloads.
    By following these practices, you can minimize compute costs while maintaining
    the performance and scalability of GenAI workloads.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们探讨了优化运行 GenAI 工作负载在 K8s 集群中所涉及的计算成本的最佳实践。计算资源，包括 CPU、GPU 和自定义加速器节点，通常是运营费用的最大来源，尤其是对于
    GenAI 模型。我们讨论了如合理配置资源、使用 Kubecost 和 Goldilocks 等工具来获得资源配置建议、使用不同的容量类型（如 Spot 实例、RIs
    和容量块），以及使用不同工作负载的 Savings Plans 等技术。通过遵循这些实践，您可以在保持 GenAI 工作负载性能和可扩展性的同时，最大限度地减少计算成本。
- en: Networking best practices
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络最佳实践
- en: To achieve high availability in Amazon EKS, it is recommended to distribute
    workloads across multiple Availability Zones (AZs). This architecture enhances
    system reliability, especially during outages or infrastructure failures in an
    AZ. However, data transfer, latency between the K8s Pods, nodes, and AZs can quickly
    add up, especially for resource-intensive workloads such as model training and
    data preparation tasks. To control the data transfer costs that arise from communication
    between AZs or regions, effective network management is needed. Let’s look at
    various techniques to minimize the networking costs while maintaining performance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 Amazon EKS 中实现高可用性，建议将工作负载分布在多个可用区（AZ）之间。这样的架构提高了系统的可靠性，尤其是在某个 AZ 出现故障或基础设施故障时。然而，K8s
    Pod、节点和 AZ 之间的数据传输和延迟可能会迅速增加，尤其是在处理资源密集型工作负载（如模型训练和数据准备任务）时。为了控制因 AZ 或区域之间通信产生的数据传输成本，必须进行有效的网络管理。接下来，我们将探讨一些技术，以在保持性能的同时最小化网络成本。
- en: Pod-to-pod communication
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pod 到 Pod 的通信
- en: Inter-pod traffic across AZs can incur significant costs. Limiting cross-zone
    traffic by aligning communication within the same AZ helps reduce these expenses.
    **Topology Aware Routing** ([https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/](https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/))
    ensures that traffic between services is routed to the nearest Pod in the same
    AZ. K8s uses **EndpointSlices** ([https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/))
    with zone-specific hints, ensuring **kube-proxy** directs traffic based on the
    origin zone, minimizing inter-AZ traffic. However, there are many considerations
    to be made for this to work effectively in a K8s cluster. Refer to the AWS blog
    at [https://aws.amazon.com/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/](https://aws.amazon.com/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/)
    for a deeper understanding.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 跨 AZ 的 Pod 间流量可能会产生显著的成本。通过将通信限制在同一 AZ 内来减少跨区流量，有助于降低这些费用。**拓扑感知路由** ([https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/](https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/))
    确保服务之间的流量被路由到同一 AZ 内最近的 Pod。K8s 使用 **EndpointSlices** ([https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/))
    和区特定的提示，确保 **kube-proxy** 根据源区来指引流量，从而最小化跨 AZ 的流量。然而，为了使其在 K8s 集群中有效运作，需要考虑许多因素。请参考
    AWS 博客 [https://aws.amazon.com/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/](https://aws.amazon.com/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/)
    以深入了解。
- en: 'Workloads can also be restricted to specific AZs using `topology.kubernetes.io/zone`,
    as shown in the following example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用 `topology.kubernetes.io/zone` 限制工作负载到特定的 AZ，如下所示：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Load balancer configuration
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载均衡器配置
- en: '**AWS Load Balancer Controller** ([https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/](https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/))
    manages ELB resources, including application and network load balancers. In **IP
    mode** ([https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-register-targets.html#register-ip-addresses](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-register-targets.html#register-ip-addresses)),
    where K8s Pods are registered directly as ELB targets, traffic is routed straight
    to the destination Pods. This reduces extra network hops, lowers network latency,
    and eliminates the inter-AZ data transfer costs. In **Instance mode** ([https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-register-targets.html#register-instances](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-register-targets.html#register-instances)),
    traffic is first routed to the EC2 worker nodes and then forwarded to the appropriate
    K8s Pods. This additional routing step can result in extra network hops and may
    incur inter-AZ data transfer costs, especially when the K8s Pod is in a different
    AZ. Refer to the AWS blog at [https://aws.amazon.com/blogs/networking-and-content-delivery/exploring-data-transfer-costs-for-classic-and-application-load-balancers/](https://aws.amazon.com/blogs/networking-and-content-delivery/exploring-data-transfer-costs-for-classic-and-application-load-balancers/)
    to understand the data transfer costs when using application load balancers.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWS 负载均衡器控制器**（[https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/](https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/)）管理
    ELB 资源，包括应用程序负载均衡器和网络负载均衡器。在**IP 模式**（[https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-register-targets.html#register-ip-addresses](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-register-targets.html#register-ip-addresses)）下，K8s
    Pod 被直接注册为 ELB 目标，流量直接路由到目标 Pod。这减少了额外的网络跳数，降低了网络延迟，并消除了跨可用区（AZ）之间的数据传输成本。在**实例模式**（[https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-register-targets.html#register-instances](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-register-targets.html#register-instances)）下，流量首先路由到
    EC2 工作节点，然后转发到适当的 K8s Pod。这一额外的路由步骤可能导致额外的网络跳数，并可能产生跨 AZ 的数据传输成本，尤其是当 K8s Pod
    位于不同的 AZ 时。请参阅 AWS 博客 [https://aws.amazon.com/blogs/networking-and-content-delivery/exploring-data-transfer-costs-for-classic-and-application-load-balancers/](https://aws.amazon.com/blogs/networking-and-content-delivery/exploring-data-transfer-costs-for-classic-and-application-load-balancers/)
    以了解使用应用负载均衡器时的数据传输成本。'
- en: Data transfer and VPC connectivity
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据传输和 VPC 连接
- en: To reduce data transfer costs between services, VPC endpoints ([https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html](https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html))
    enable direct access to AWS services without routing through the public internet.
    This eliminates the need for deploying an **internet gateway** ([https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html))
    and **network address translation (NAT) gateway** ([https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html))
    to communicate with AWS services. For workloads spread across different VPCs,
    **VPC peering** ([https://docs.aws.amazon.com/vpc/latest/userguide/vpc-peering.html](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-peering.html))
    or an **AWS Transit Gateway** ([https://docs.aws.amazon.com/vpc/latest/userguide/extend-tgw.html](https://docs.aws.amazon.com/vpc/latest/userguide/extend-tgw.html))
    is recommended to enable low-cost, inter-VPC communication.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低服务之间的数据传输成本，VPC 端点（[https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html](https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html)）允许直接访问
    AWS 服务，无需通过公共互联网路由。这消除了部署**互联网网关**（[https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html)）和**网络地址转换（NAT）网关**（[https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html)）与
    AWS 服务通信的需求。对于跨不同 VPC 部署的工作负载，建议使用**VPC 对等连接**（[https://docs.aws.amazon.com/vpc/latest/userguide/vpc-peering.html](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-peering.html)）或**AWS
    传输网关**（[https://docs.aws.amazon.com/vpc/latest/userguide/extend-tgw.html](https://docs.aws.amazon.com/vpc/latest/userguide/extend-tgw.html)）来实现低成本的
    VPC 间通信。
- en: Optimizing image pulls from Amazon ECR
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化从 Amazon ECR 拉取镜像
- en: This strategy can help reduce networking costs and improve K8s Pod startup times.
    In-region image pulls from Amazon ECR are free, but you will be charged a NAT
    gateway data processing fee. So, you can utilize the VPC endpoints of Amazon ECR
    and Amazon S3 to privately access the ECR images. For large GenAI workloads, pre-caching
    container images in custom AMIs can further optimize the image pull times during
    worker node/Pod startup. This approach minimizes data transfer during scaling
    events and speeds up instance readiness, which is particularly beneficial for
    dynamic, auto-scaling environments. Refer to the AWS blog at [https://aws.amazon.com/blogs/containers/start-pods-faster-by-prefetching-images/](https://aws.amazon.com/blogs/containers/start-pods-faster-by-prefetching-images/)
    for a deeper understanding of how this works.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这一策略有助于降低网络成本并提高 K8s Pod 启动速度。来自 Amazon ECR 的区域内镜像拉取是免费的，但你将被收取 NAT 网关的数据处理费用。因此，你可以使用
    Amazon ECR 和 Amazon S3 的 VPC 端点来私密访问 ECR 镜像。对于大型 GenAI 工作负载，将容器镜像预先缓存到自定义 AMI
    中，可以进一步优化在工作节点/Pod 启动期间的镜像拉取时间。这种方法能够在扩展事件中减少数据传输，并加速实例就绪，特别适用于动态自动扩展的环境。欲了解更多详细信息，请参考
    AWS 博客 [https://aws.amazon.com/blogs/containers/start-pods-faster-by-prefetching-images/](https://aws.amazon.com/blogs/containers/start-pods-faster-by-prefetching-images/)。
- en: In this section, we explored best practices for optimizing networking costs
    for running GenAI workloads in K8s clusters. Effective networking strategies can
    help reduce the significant costs associated with data transfer, NAT gateways,
    and load balancing. Using techniques such as Topology Aware Routing and IP targets
    in ALB can help reduce latency and data transfer costs. We also discussed the
    benefits of optimizing image pulls from ECR to speed up the startup times and
    reduce networking costs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了优化 K8s 集群中 GenAI 工作负载网络成本的最佳实践。有效的网络策略有助于减少与数据传输、NAT 网关和负载均衡相关的高额成本。使用拓扑感知路由和
    ALB 中的 IP 目标等技术可以帮助减少延迟和数据传输成本。我们还讨论了优化来自 ECR 的镜像拉取的好处，以加速启动时间并降低网络成本。
- en: In the next section, we will explore storage-related best practices in K8s clusters.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将探讨 K8s 集群中与存储相关的最佳实践。
- en: Storage best practices
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储最佳实践
- en: There are multiple storage options available in a K8s environment, and selecting
    the right one is essential to optimize application performance and cost. Depending
    on the workload, one could use either ephemeral storage or persistent storage
    for their applications.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在 K8s 环境中有多种存储选项可供选择，选择合适的存储方式对于优化应用程序的性能和成本至关重要。根据工作负载的不同，用户可以选择临时存储或持久存储来满足应用程序的需求。
- en: Ephemeral storage
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 临时存储
- en: Ephemeral volumes are temporary storage volumes that do not persist beyond a
    Pod’s life cycle, making them suitable for scratch space or caching. These volumes
    are often backed by the root disk of the host system or RAM, meaning they do not
    persist once the Pod is terminated. For cost efficiency, it’s important to properly
    configure ephemeral storage to avoid over-provisioning, and where possible, leverage
    node-local storage for temporary tasks to reduce reliance on external storage
    systems, thus lowering costs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 临时卷是临时存储卷，它们的生命周期与 Pod 相同，Pod 终止后会消失，因此适合用作临时存储或缓存。这些卷通常由主机系统的根磁盘或内存支持，意味着一旦
    Pod 终止，它们就不会持久化。为了提高成本效益，正确配置临时存储至关重要，以避免过度配置，并且在可能的情况下，可以利用节点本地存储来处理临时任务，从而减少对外部存储系统的依赖，降低成本。
- en: Object storage
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对象存储
- en: While ephemeral and EBS volumes provide low-latency, high-performance storage
    for many workloads, object storage solutions such as Amazon S3 offer a flexible,
    scalable, and cost-effective alternative for storing large datasets, model artifacts,
    and logs. The **Mountpoint for Amazon S3 CSI driver** ([https://github.com/awslabs/mountpoint-s3-csi-driver](https://github.com/awslabs/mountpoint-s3-csi-driver))
    enables you to mount an S3 bucket as a filesystem inside the K8s Pods, allowing
    applications to interact with object storage using familiar filesystem semantics.
    It offers significant performance gains compared to traditional S3 access methods,
    making it ideal for data-intensive workloads and AI/ML training.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然临时卷和 EBS 卷为许多工作负载提供了低延迟、高性能的存储，但像 Amazon S3 这样的对象存储解决方案提供了一种灵活、可扩展且具有成本效益的替代方案，用于存储大型数据集、模型产物和日志。**Amazon
    S3 CSI 驱动程序的挂载点** ([https://github.com/awslabs/mountpoint-s3-csi-driver](https://github.com/awslabs/mountpoint-s3-csi-driver))
    使你能够将 S3 存储桶作为文件系统挂载到 K8s Pods 中，允许应用程序使用熟悉的文件系统语义与对象存储进行交互。与传统的 S3 访问方法相比，它提供了显著的性能提升，特别适合数据密集型工作负载和
    AI/ML 训练。
- en: The Mountpoint for S3 CSI driver supports both Amazon S3 Standard and S3 Express
    One Zone storage classes. S3 Express One Zone is a high-performance storage class
    designed for single-AZ deployments. It offers consistent, single-digit-millisecond
    data access, making it ideal for frequently accessed data and latency-sensitive
    applications. By co-locating storage and compute resources within the same AZ,
    you can optimize performance and potentially reduce networking costs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Mountpoint for S3 CSI 驱动程序支持 Amazon S3 Standard 和 S3 Express One Zone 存储类。S3
    Express One Zone 是一个高性能存储类，专为单一可用区（AZ）部署设计。它提供一致的单数毫秒级数据访问，非常适合频繁访问的数据和对延迟敏感的应用程序。通过将存储和计算资源共置于同一
    AZ 内，您可以优化性能，并可能降低网络成本。
- en: In addition to performance considerations, optimizing object storage costs is
    essential when working with large-scale training datasets. Best practices include
    selecting the appropriate S3 storage class based on the access patterns. For example,
    use *S3 Standard* or *S3 Intelligent-Tiering* for frequently accessed data, and
    transition infrequently accessed data to S3 Infrequent Access or archival classes
    such as Glacier. Implement life cycle policies to automate transitions and data
    expirations, thereby reducing unnecessary storage costs. Refer to the AWS documentation
    at [https://aws.amazon.com/s3/storage-classes/](https://aws.amazon.com/s3/storage-classes/)
    for a deeper understanding of the various S3 storage classes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 除了性能考量，优化对象存储成本在处理大规模训练数据集时至关重要。最佳实践包括根据访问模式选择适当的 S3 存储类。例如，对于频繁访问的数据，使用 *S3
    Standard* 或 *S3 Intelligent-Tiering*，并将不常访问的数据转移到 S3 Infrequent Access 或归档存储类（如
    Glacier）。实施生命周期策略来自动执行数据的过渡和过期，从而降低不必要的存储成本。有关各种 S3 存储类的详细信息，请参阅 AWS 文档：[https://aws.amazon.com/s3/storage-classes/](https://aws.amazon.com/s3/storage-classes/)。
- en: 'When using Mountpoint for S3 CSI driver, you can attach an existing S3 bucket
    to K8s Pods by creating a *PersistentVolume*, as shown in the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Mountpoint for S3 CSI 驱动程序时，您可以通过创建 *PersistentVolume* 将现有的 S3 存储桶附加到 K8s
    Pods，如下所示：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: By leveraging the Mountpoint-S3 and adhering to cost-effective storage best
    practices, you can seamlessly integrate Amazon S3 object storage into your K8s
    workloads. This approach enables your GenAI workloads to access large-scale, cost-effective
    storage with familiar filesystem semantics while optimizing both performance and
    cost.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 Mountpoint-S3 并遵循成本效益存储的最佳实践，您可以将 Amazon S3 对象存储无缝集成到您的 K8s 工作负载中。这种方法使得您的
    GenAI 工作负载能够访问大规模、成本效益高的存储，并具有熟悉的文件系统语义，同时优化性能和成本。
- en: EBS volumes
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EBS 卷
- en: 'Amazon EBS volumes provide block-level persistent storage. EBS volumes are
    managed via the Amazon EBS CSI driver, enabling dynamic provisioning through K8s.
    The **Container Storage Interface (CSI)** is a standardized K8s API that ensures
    interoperability between K8s and external storage systems. K8s applications request
    storage by creating **Persistent Volume Claims (PVCs)** specifying the size and
    access mode. The EBS CSI driver provisions the EBS volume based on the linked
    StorageClass. For example, following YAML code will create a gp3 storage class:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon EBS 卷提供块级持久存储。EBS 卷通过 Amazon EBS CSI 驱动程序进行管理，支持通过 K8s 动态供应。**容器存储接口
    (CSI)** 是一个标准化的 K8s API，确保 K8s 和外部存储系统之间的互操作性。K8s 应用程序通过创建 **Persistent Volume
    Claims (PVCs)** 来请求存储，指定大小和访问模式。EBS CSI 驱动程序根据关联的 StorageClass 配置 EBS 卷。例如，以下
    YAML 代码将创建一个 gp3 存储类：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: A reclaim policy for StorageClass could be either *delete* or *retain*. The
    *delete* policy ensures that the PersistentVolume is automatically deleted when
    the associated Pod is removed. The *retain* policy keeps the PV even after the
    PVC is deleted. The volume stays intact with all its data, but becomes unbound
    and available for manual intervention.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: StorageClass 的回收策略可以是 *delete* 或 *retain*。*delete* 策略确保当关联的 Pod 被删除时，PersistentVolume
    会被自动删除。*retain* 策略即使 PVC 被删除，PV 也会被保留。该卷保持完整，包含所有数据，但会变为未绑定状态，可供手动干预。
- en: 'The following YAML file is now creating a PVC, which is linked to the previous
    storage class for 10 GB of storage:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 YAML 文件正在创建一个 PVC，它链接到先前的存储类，并为 10 GB 的存储分配空间：
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For cost optimization, it is a good idea to ensure that the correct amount of
    storage is claimed based on your application’s needs. Over-provisioning large
    volumes that are not fully utilized leads to unnecessary costs. Similarly, it’s
    common for unused *PVs* and *EBS snapshots* to accumulate over time, so one should
    keep monitoring their storage costs or use *delete* as a reclaim policy. You can
    use Kubecost to get insights into unclaimed volumes, orphaned resources, persistent
    volume right-sizing recommendations, and so on. Open the Kubecost UI console and
    select Savings -> Insights from the left-hand side menu to navigate to the **Savings**
    page, where you can view the cost savings recommendations, as shown in *Figure
    7**.5* and *Figure 7**.6*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行成本优化，确保根据应用的需求分配正确数量的存储是一个好主意。过度配置未充分利用的大容量存储会导致不必要的费用。同样，未使用的*PVs*和*EBS快照*会随着时间的推移积累，因此应持续监控存储成本，或使用*删除*作为回收策略。你可以使用Kubecost查看未分配的卷、孤立资源、持久卷调整建议等内容。打开Kubecost
    UI控制台，在左侧菜单中选择Savings -> Insights，进入**Savings**页面，在此页面可以查看成本节省建议，如*图 7.5*和*图 7.6*所示。
- en: '![Figure 7.5 – An overview of unclaimed volumes in the Kubecost UI console](img/B31108_07_5.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – Kubecost UI控制台中未分配卷的概览](img/B31108_07_5.jpg)'
- en: Figure 7.5 – An overview of unclaimed volumes in the Kubecost UI console
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – Kubecost UI控制台中未分配卷的概览
- en: These insights help you identify cost-saving opportunities by highlighting persistent
    volumes that are allocated but not actively used. By analyzing these patterns,
    Kubecost enables you to take informed actions such as reclaiming unused resources
    or resizing existing volumes to better fit your workload requirements, as shown
    in the Figure 7.6.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些洞察帮助你通过突出显示已分配但未被积极使用的持久卷，识别节省成本的机会。通过分析这些模式，Kubecost使你能够采取有根据的措施，如回收未使用的资源或调整现有卷的大小，更好地适应工作负载需求，如图7.6所示。
- en: '![Figure 7.6 – An overview of PV right-sizing recommendations in the Kubecost
    UI console](img/B31108_07_6.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – Kubecost UI控制台中持久卷调整建议的概览](img/B31108_07_6.jpg)'
- en: Figure 7.6 – An overview of PV right-sizing recommendations in the Kubecost
    UI console
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – Kubecost UI控制台中持久卷调整建议的概览
- en: For optimized costs, gp3 volumes are recommended, as they offer up to 20% lower
    costs compared to gp2 and allow independent scaling of IOPS and throughput without
    increasing volume size. For high-performance needs, **io2 Block Express** volumes
    support up to 256,000 IOPS, but they are more expensive and require specific EC2
    instance types.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化成本，推荐使用gp3卷，因为与gp2相比，gp3卷提供最高20%的成本节省，并允许独立扩展IOPS和吞吐量，而无需增加卷的大小。对于高性能需求，**io2
    Block Express**卷支持高达256,000 IOPS，但它们更昂贵，并且需要特定的EC2实例类型。
- en: For workloads with less frequent access to data, such as logs and backups, one
    could use **Cold HDD (sc1)** or **Throughput Optimized HDD (st1)**. These options
    are cheaper than SSD-backed volumes. Refer to the Amazon EBS pricing page at [https://aws.amazon.com/ebs/pricing/](https://aws.amazon.com/ebs/pricing/)
    for detailed pricing.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于访问数据频率较低的工作负载，如日志和备份，可以使用**Cold HDD (sc1)**或**Throughput Optimized HDD (st1)**。这些选项比SSD支持的卷便宜。有关详细定价，请参考Amazon
    EBS定价页面：[https://aws.amazon.com/ebs/pricing/](https://aws.amazon.com/ebs/pricing/)。
- en: 'A few other considerations to make when optimizing storage costs in K8s applications
    are as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化K8s应用的存储成本时，还需要考虑以下几个因素：
- en: '**Optimize container image storage**: Container images can consume a significant
    amount of storage, especially when large, multi-layer images are used in an Amazon
    EKS cluster. Optimizing it is crucial for reducing both storage costs and the
    time it takes to pull images during startup. To achieve this, it’s best to use
    smaller, lightweight parent images where possible. Additionally, by employing
    multi-stage builds ([https://docs.docker.com/build/building/multi-stage/](https://docs.docker.com/build/building/multi-stage/)),
    only the necessary components are included in the final container image, further
    decreasing image size. These optimizations not only save storage space but also
    lead to faster deployment times and reduced costs for both storing and pulling
    container images.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化容器镜像存储**：容器镜像可能会消耗大量存储，特别是在使用大型多层镜像的Amazon EKS集群中。优化容器镜像对于减少存储成本和启动时拉取镜像的时间至关重要。为此，最好尽可能使用较小、轻量的父镜像。此外，通过使用多阶段构建（[https://docs.docker.com/build/building/multi-stage/](https://docs.docker.com/build/building/multi-stage/)），只有必要的组件被包含在最终的容器镜像中，从而进一步减少镜像大小。这些优化不仅节省了存储空间，还加快了部署时间，并降低了存储和拉取容器镜像的成本。'
- en: '**Use data retention policies**: Implement data retention policies to automatically
    delete old, unnecessary datasets such as logs, metrics, and backups that accumulate
    over time. Tools such as **Elasticsearch** and **AWS CloudWatch Logs** offer controls
    to set appropriate retention policies to delete old logs and reduce storage costs.
    Similarly for backups, set appropriate retention policies, ensuring that only
    necessary backups are retained, while old, redundant backups are deleted.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用数据保留策略**：实施数据保留策略，自动删除随着时间积累的旧的、不必要的数据集，如日志、指标和备份等。像**Elasticsearch**和**AWS
    CloudWatch Logs**这样的工具提供控制选项，可以设置适当的保留策略，以删除旧日志并降低存储成本。对于备份，同样应设置适当的保留策略，确保仅保留必要的备份，同时删除旧的、冗余的备份。'
- en: In this section, we discussed various available storage options and how to use
    CSI drivers to dynamically provision the storage volumes in K8s clusters. We also
    explored the importance of reducing the container image size to not only reduce
    the storage costs but also improve the startup times of K8s Pods.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了各种可用的存储选项，以及如何使用CSI驱动程序在K8s集群中动态地配置存储卷。我们还探讨了减少容器镜像大小的重要性，这不仅可以降低存储成本，还能提高K8s
    Pod的启动时间。
- en: Summary
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we explored the best practices for optimizing the cost of
    deploying GenAI applications in the cloud by focusing on three key components:
    compute, storage, and networking. We also introduced tools such as Kubecost and
    Goldilocks to monitor resource utilization and ensure efficient resource allocation.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了通过专注于计算、存储和网络三大关键组件，优化在云中部署GenAI应用程序成本的最佳实践。我们还介绍了像Kubecost和Goldilocks这样的工具，用于监控资源使用情况，确保资源的高效分配。
- en: For compute costs, selecting the appropriate instance types is essential. It’s
    crucial to monitor resource utilization to ensure workloads run on optimally sized
    instances. For storage, choosing the right storage type is the key to optimizing
    the storage costs of large datasets needed for model training and inference.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算成本，选择合适的实例类型至关重要。监控资源使用情况，以确保工作负载运行在优化大小的实例上也非常重要。对于存储，选择合适的存储类型是优化用于模型训练和推理的大型数据集存储成本的关键。
- en: Kubecost is an effective tool for monitoring and optimizing the cost of K8s
    clusters. It provides detailed cost breakdowns by namespaces, Pods, and services,
    helping attribute expenses to individual teams or applications. Kubecost also
    identifies underutilized nodes, recommends more cost-effective instance types,
    and detects storage and networking inefficiencies, such as unnecessary inter-AZ
    data transfers. Goldilocks leverages the VPA to analyze historical CPU and memory
    usage, providing recommendations for right-sizing resource requests and limits.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Kubecost是一个有效的工具，用于监控和优化K8s集群的成本。它提供按命名空间、Pod和服务的详细成本分解，帮助将费用归因于各个团队或应用程序。Kubecost还可以识别使用率低的节点，推荐更具成本效益的实例类型，并检测存储和网络效率问题，如不必要的跨可用区数据传输。Goldilocks利用VPA分析历史的CPU和内存使用情况，提供资源请求和限制的适当大小建议。
- en: On the networking front, we discussed the importance of aligning Pod communication
    within the same AZ to minimize cross-AZ traffic costs. Using Topology Aware Routing
    ensures that traffic is routed within the AZ, reducing inter-AZ transfer fees.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络方面，我们讨论了确保Pod通信在同一可用区（AZ）内对齐的重要性，以减少跨AZ流量成本。使用拓扑感知路由可以确保流量在同一AZ内路由，从而减少跨AZ的传输费用。
- en: This chapter also highlighted the importance of continuous monitoring, right-sizing
    resources, and making strategic trade-offs across compute, storage, and networking
    to optimize costs effectively.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还强调了持续监控、资源的合理配置，以及在计算、存储和网络之间做出战略性权衡的重要性，以有效优化成本。
- en: In the next chapter, we will dive deeper and cover the networking best practices
    for deploying GenAI applications in K8s.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨并介绍在K8s中部署GenAI应用的网络最佳实践。
- en: Join the CloudPro Newsletter with 44000+ Subscribers
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入CloudPro新闻通讯，拥有超过44,000名订阅者
- en: Want to know what’s happening in cloud computing, DevOps, IT administration,
    networking, and more? Scan the QR code to subscribe to **CloudPro**, our weekly
    newsletter for 44,000+ tech professionals who want to stay informed and ahead
    of the curve.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解云计算、DevOps、IT管理、网络等领域的最新动态吗？扫描二维码订阅**CloudPro**，我们的每周通讯，面向44,000多位希望保持信息领先的技术专业人士。
- en: '![https://packt.link/cloudpro](img/NL_Part1.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![https://packt.link/cloudpro](img/NL_Part1.jpg)'
- en: '[https://packt.link/cloudpro](https://packt.link/cloudpro)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/cloudpro](https://packt.link/cloudpro)'
