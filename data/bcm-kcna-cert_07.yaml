- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Application Placement and Debugging with Kubernetes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 进行应用程序部署和调试
- en: In this chapter, we’ll see how we can control the placement of workloads on
    Kubernetes, how its scheduler works, and how we can debug applications running
    on K8s when something goes wrong. This chapter covers aspects from the *Kubernetes
    Fundamentals* as well as *Cloud Native Observability* domains of the **Kubernetes
    and Cloud Native Associate** (**KCNA**) exam at the same time.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将了解如何控制工作负载在 Kubernetes 上的部署，Kubernetes 调度器是如何工作的，以及当应用程序在 K8s 上运行时出现问题时，如何进行调试。本章同时涵盖了**Kubernetes
    和 Cloud Native Associate**（**KCNA**）认证考试的 *Kubernetes 基础* 和 *Cloud Native Observability*
    域的内容。
- en: 'As before, we will perform a few exercises with our minikube Kubernetes, so
    keep your setup handy. We’re going to cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将继续使用我们的 minikube Kubernetes 进行一些练习，所以保持你的设置准备好。我们将涵盖以下主题：
- en: Scheduling in Kubernetes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes中的调度
- en: Resource requests and limits
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源请求和限制
- en: Debugging applications in Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中调试应用程序
- en: Let’s continue our Kubernetes journey!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续我们的 Kubernetes 之旅！
- en: Scheduling in Kubernetes
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes中的调度
- en: We’ve already touched the surface of what Kubernetes scheduler (`kube-scheduler`)
    does in [*Chapter 5*](B18970_05.xhtml#_idTextAnchor059). The scheduler is the
    component of the K8s control plane that decides on which node a pod will run.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在 [*第 5 章*](B18970_05.xhtml#_idTextAnchor059) 简要介绍了 Kubernetes 调度器（`kube-scheduler`）的工作原理。调度器是
    K8s 控制平面的组件，决定 pod 会在哪个节点上运行。
- en: Scheduling
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 调度
- en: Scheduling is the process of assigning Pods to Kubernetes nodes for the kubelet
    to run them. The scheduler watches for newly created Pods that have no *node*
    assigned in an infinite loop, and for every Pod it discovers, it will be responsible
    for finding the optimal node to run it on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 调度是将 Pods 分配给 Kubernetes 节点以供 kubelet 执行的过程。调度器会在无限循环中监控新创建的 Pods，查看哪些 Pods
    尚未分配 *节点*，并为每个发现的 Pod 找到合适的节点来运行它。
- en: 'The default `kube-scheduler` scheduler selects a node for a pod in two stages:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 `kube-scheduler` 调度器会分两阶段为 pod 选择节点：
- en: '**Filtering**: The first stage is where the scheduler determines the set of
    nodes where it is feasible to run the pod. This includes checks for nodes to have
    sufficient capacity and other requirements for a particular pod. This list might
    be empty if there are no suitable nodes in the cluster, and in such a case, the
    pod will hang in an unscheduled state until either the requirements or cluster
    state is changed.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**过滤**：第一阶段是调度器确定可以运行 pod 的节点集合。这包括检查节点是否具有足够的容量以及其他特定 pod 的需求。如果集群中没有适合的节点，这个列表可能为空，且在这种情况下，pod
    会停留在未调度状态，直到需求或集群状态发生变化。'
- en: '**Scoring**: The second stage is where the scheduler ranks the nodes filtered
    in the first stage to choose the most suitable pod placement. Each node in the
    list will be ranked and gets a score, and at the end, the node with the highest
    score is picked.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评分**：第二阶段是调度器在第一阶段过滤的节点中进行排名，选择最合适的 pod 部署位置。列表中的每个节点都会被评分，最终选择得分最高的节点。'
- en: 'Let’s have a real-world example. Imagine we have an application that requires
    nodes with certain hardware. For example, you run **machine learning** (**ML**)
    workloads that can utilize GPUs for faster processing, or an application requires
    a particular CPU generation that is not available on every node in the cluster.
    In all such cases, we need to instruct Kubernetes to restrict the list of suitable
    nodes for our pods. There are multiple ways to do that:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个实际的例子来说明。假设我们有一个应用程序需要具备特定硬件的节点。例如，你运行**机器学习**（**ML**）工作负载，可以利用 GPU 进行更快的处理，或者一个应用程序要求特定的
    CPU 代际，而这个 CPU 可能并非集群中的每个节点都具备。在所有这种情况下，我们需要指示 Kubernetes 限制可用于 pod 的适合节点列表。方法有多种：
- en: Specifying a `nodeSelector` field in the pod spec and labeling nodes
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 pod 规范中指定 `nodeSelector` 字段，并给节点打标签
- en: Specifying an exact `nodeName` field in the pod spec
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 pod 规范中指定一个确切的 `nodeName` 字段
- en: Using **affinity** and **anti-affinity** rules
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**亲和性**和**反亲和性**规则
- en: Using pod **topology** **spread constraints**
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pod **拓扑** **分布约束**
- en: 'Now let’s get back to our minikube setup and extend the cluster by adding one
    more node with the `minikube node add` command (*this operation might take* *some
    time*):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到 minikube 设置，通过使用 `minikube node add` 命令（*此操作可能需要* *一些时间*）来扩展集群，添加一个新的节点：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We should have a two-node minikube cluster at this point! Let’s check the list
    of nodes:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们应该已经拥有一个包含两个节点的 minikube 集群！让我们查看节点列表：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If your system does not have sufficient resources to run another Kubernetes
    node, you can also continue as before with just one node. In such cases, however,
    you will need to adjust the example commands you encounter in this chapter to
    label the first node.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的系统没有足够的资源来运行另一个Kubernetes节点，你也可以像之前一样仅使用一个节点。但是，在这种情况下，你需要调整本章中遇到的示例命令来标记第一个节点。
- en: 'We will now create a modified version of the nginx deployment from before,
    which will require the node to have `purpose: web-server` appended to it. The
    respective `Deployment` spec could look like this:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们将创建之前的nginx部署的修改版，要求节点上附加`purpose: web-server`标签。相应的`Deployment`规范可能是这样的：'
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you have not yet deleted the resources from the previous chapter’s exercises,
    do so now by executing `kubectl delete deployment`, `kubectl delete sts`, or `kubectl
    delete service` in the `kcna` namespace respectively.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有删除上一章练习中的资源，可以通过分别在`kcna`命名空间中执行`kubectl delete deployment`、`kubectl delete
    sts`或`kubectl delete service`来删除它们。
- en: 'Go ahead and create the aforementioned nginx deployment spec:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建前述的nginx部署规范：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s check what happened; for example, query pods in the `kcna` namespace:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查发生了什么；例如，查询`kcna`命名空间中的Pod：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'There it goes – the created Nginx pod is stuck in a `Pending` state. Let’s
    check more details with the `kubectl describe` command (*your pod naming* *will
    differ*):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 看，它开始了——创建的Nginx Pod卡在了`Pending`状态。让我们通过`kubectl describe`命令检查更多细节（*你的Pod命名*
    *可能不同*）：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The message is clear – we have requested a node with a certain label for our
    nginx deployment pod and there are no nodes with such a label available.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 信息很明确——我们已经请求了一个具有特定标签的节点来运行我们的nginx部署Pod，但没有可用的具有该标签的节点。
- en: 'We can check which labels our nodes have by adding the `--``show-labels` parameter:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加`--show-labels`参数来检查节点上有哪些标签：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Default labels include some useful information about the roles of the nodes,
    CPU architecture, OS, and more. Let’s now label the newly added node with the
    same label our nginx deployment is looking for (*the node name might be similar
    in your case, so* *adjust accordingly*):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 默认标签包括有关节点角色、CPU架构、操作系统等一些有用信息。现在我们为新添加的节点标记与nginx部署所需要的相同标签（*节点名称在你的情况下可能类似，请*
    *相应调整*）：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And just a moment later, we can see the nginx pod is being created:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 过了一会儿，我们可以看到nginx Pod正在被创建：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'By adding the `-o wide` option, we can see which node the pod was assigned
    to:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加`-o wide`选项，我们可以看到Pod被分配到哪个节点：
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: That was a demonstration of perhaps the most common way to provide placement
    instructions to a Kubernetes scheduler with `nodeSelector`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了通过`nodeSelector`向Kubernetes调度器提供位置指令的最常见方式。
- en: Let’s move on to discuss the other scheduling controls Kubernetes offers. `nodeName`
    should be obvious – it allows us to specify exactly which node we want the workload
    to be scheduled to. Affinity and anti-affinity rules are more interesting. Conceptually,
    affinity is similar to `nodeSelector` but has more customization options.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讨论Kubernetes提供的其他调度控制。`nodeName`应该很明显——它允许我们指定希望工作负载被调度到哪个节点。亲和性和反亲和性规则则更有趣。从概念上讲，亲和性类似于`nodeSelector`，但有更多的自定义选项。
- en: nodeAffinity and podAffinity
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: nodeAffinity 和 podAffinity
- en: These allow you to schedule Pods either on certain nodes (`nodeAffinity`) in
    a cluster or to nodes that are already running specified Pods (`podAffinity`).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则允许你将Pod调度到集群中的某些节点（`nodeAffinity`），或者调度到已经运行指定Pod的节点（`podAffinity`）。
- en: nodeAntiAffinity and podAntiAffinity
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: nodeAntiAffinity 和 podAntiAffinity
- en: The opposite of affinity. These allow you to either schedule Pods to different
    nodes than the ones specified (`nodeAntiAffinity`) or to schedule to different
    nodes where the specified Pods run (`podAntiAffinity`).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 亲和性（affinity）的反义词。这些规则允许你将Pod调度到与指定节点不同的节点（`nodeAntiAffinity`），或者将Pod调度到已有指定Pod运行的不同节点（`podAntiAffinity`）。
- en: 'In other words, affinity rules are used to attract Pods to certain nodes or
    other Pods, and anti-affinity for the opposite – to push back from certain nodes
    or other Pods. Affinity can also be of two types – *hard* and *soft*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，亲和性规则用于将Pod吸引到某些节点或其他Pod上，而反亲和性则是相反——将Pod推离某些节点或其他Pod。亲和性也可以分为两种类型——*硬性*和*软性*：
- en: '`requiredDuringSchedulingIgnoredDuringExecution` – Hard requirement, meaning
    the pod won’t be scheduled unless the rule is met'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requiredDuringSchedulingIgnoredDuringExecution`——硬性要求，意味着除非满足规则，否则Pod不会被调度。'
- en: '`preferredDuringSchedulingIgnoredDuringExecution` – Soft requirement, meaning
    the scheduler will try to find the node that satisfies the requirement, but if
    not available, the pod will still be scheduled on any other node'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preferredDuringSchedulingIgnoredDuringExecution` – 软性要求，意味着调度器会尝试找到满足要求的节点，但如果不可用，pod
    仍会被调度到其他任何节点上。'
- en: Note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`IgnoredDuringExecution` means that if the node labels change after Kubernetes
    already scheduled the pod, the pod continues to run on the same node.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`IgnoredDuringExecution` 意味着如果 Kubernetes 已经调度了 pod 后节点标签发生变化，pod 将继续在同一节点上运行。'
- en: Last on our list is pod **topology** **spread constraints**.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们清单上的最后一项是 pod **拓扑** **分布约束**。
- en: Topology spread constraints
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑分布约束
- en: These allow us to control how Pods are spread across the cluster among failure
    domains such as regions, **availability zones** (**AZs**), nodes, or other user-defined
    topologies.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则使我们能够控制 Pods 在集群中的分布，跨越不同的故障域，如区域、**可用区** (**AZs**)、节点或其他用户定义的拓扑。
- en: Essentially, these allow us to control where Pods run, taking into account the
    physical topology of the cluster. In today’s cloud environments, we typically
    have multiple AZs in each region where the cloud provider operates.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这些规则使我们能够控制 Pods 的运行位置，考虑到集群的物理拓扑。在今天的云环境中，我们通常会在每个云提供商运营的区域内拥有多个 AZ。
- en: AZ
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: AZ
- en: This is one or multiple discrete data centers with redundant power, networking,
    and internet connectivity.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这指的是一个或多个离散的数据中心，具有冗余的电力、网络和互联网连接。
- en: 'It is a good practice to run both the control plane and worker nodes of Kubernetes
    across multiple AZs. For example, in the `eu-central-1` region, **Amazon Web Services**
    (**AWS**) currently has three AZs, so we can run one control plane node in each
    AZ and multiple worker nodes per AZ. In this case, to achieve **high availability**
    (**HA**) as well as efficient resource utilization, we can apply topology spread
    constraints to our workloads to control the spread of Pods by nodes and zones,
    as shown in *Figure 7**.1*:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，最佳实践是将控制平面和工作节点运行在多个可用区（AZ）中。例如，在 `eu-central-1` 区域，**Amazon
    Web Services** (**AWS**) 当前有三个 AZ，因此我们可以在每个 AZ 中运行一个控制平面节点，并在每个 AZ 中运行多个工作节点。在这种情况下，为了实现**高可用性**
    (**HA**) 和高效的资源利用，我们可以对工作负载应用拓扑分布约束，以控制 Pods 在节点和可用区之间的分布，如*图 7.1*所示：
- en: '![Figure 7.1 – Example of Pods spread across a cluster](img/B18970_07_01.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – Pods 在集群中分布的示例](img/B18970_07_01.jpg)'
- en: Figure 7.1 – Example of Pods spread across a cluster
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – Pods 在集群中分布的示例
- en: This way, we can protect our workloads against individual node outages as well
    as wider cloud provider outages that might affect a whole AZ. Besides, it is possible
    to combine different methods and rules for more precise and granular control of
    where the Pods will be placed – for example, we can have topology spread constraints
    together with `nodeAffinity` and `podAntiAffinity` rules in one deployment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以保护工作负载免受单个节点故障以及可能影响整个 AZ 的云提供商故障的影响。此外，还可以结合不同的方法和规则，进行更精确和细粒度的
    Pods 安排控制。例如，我们可以将拓扑分布约束与 `nodeAffinity` 和 `podAntiAffinity` 规则结合使用，应用于同一个部署。
- en: Note
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is possible to combine multiple rules per pod - for example, `nodeSelector`
    together with hard `nodeAffinity` rules (`requiredDuringSchedulingIgnoredDuringExecution`).
    Both rules must be satisfied for the pod to be scheduled. In cases where at least
    one rule is not satisfied, the pod will be in a `Pending` state.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 可以为每个 pod 组合多个规则，例如，`nodeSelector` 与硬性 `nodeAffinity` 规则（`requiredDuringSchedulingIgnoredDuringExecution`）一起使用。两个规则都必须满足才能调度
    pod。在至少有一个规则不满足的情况下，pod 将处于 `Pending` 状态。
- en: Altogether, scheduling in Kubernetes might seem to be a bit complicated at first,
    but as you start getting more experience, you’ll see that its extensive features
    are great and let us handle complex scenarios as well as very large and diverse
    K8s installations. For the scope of the KCNA exam, you are not required to know
    in-depth details, but if you have some time, you are encouraged to check the *Further
    reading* section at the end of this chapter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，Kubernetes 的调度可能一开始看起来有些复杂，但随着经验的积累，你会发现它的丰富特性非常强大，可以帮助我们处理复杂的场景以及非常大规模和多样化的
    K8s 安装。就 KCNA 考试的范围而言，你不需要了解深入的细节，但如果你有时间，建议查看本章末尾的*进一步阅读*部分。
- en: Resource requests and limits
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源请求和限制
- en: As we were exploring the features of the K8s scheduler previously, have you
    wondered how Kubernetes knows *what is the best node in the cluster for a particular
    pod*? If we create a Deployment with no affinity settings, topology constraints,
    or node selectors, how can Kubernetes decide what is the best location in the
    cluster for the application we want to run?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前探索 K8s 调度器功能时，你是否曾想过 Kubernetes 是如何知道 *哪个节点是集群中最适合特定 Pod 的节点*？如果我们创建一个没有亲和性设置、拓扑约束或节点选择器的
    Deployment，Kubernetes 如何决定集群中哪个位置最适合运行我们想要的应用？
- en: By default, K8s is not aware of how many resources (CPU, memory, and other)
    each container in a scheduled pod requires to run. Therefore, for Kubernetes to
    make the best scheduling decisions, we need to make K8s aware of what each container
    requires for normal operation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，K8s 不知道每个容器在调度的 Pod 中需要多少资源（CPU、内存等）来运行。因此，为了让 Kubernetes 做出最佳调度决策，我们需要让
    K8s 了解每个容器正常运行所需的资源。
- en: Resource requests
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 资源请求
- en: A resource request is an optional specification of how many resources each container
    in a pod needs. Containers can use more resources than requested if the node where
    the Pod runs has available resources. The specified request amounts will be reserved
    on the node where the pod is scheduled.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 资源请求是可选的指定，表明每个 Pod 中的容器需要多少资源。如果 Pod 所在的节点有可用资源，容器可以使用超过请求的资源。指定的请求量将在 Pod
    被调度到的节点上保留。
- en: Kubernetes also allows us to impose hard limits on resources that the container
    can consume.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 还允许我们对容器可以消耗的资源设置硬性限制。
- en: Resource limits
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 资源限制
- en: A resource limit is an optional specification of the maximum resources a running
    container can consume that are enforced by the kubelet and container runtime.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 资源限制是对正在运行的容器最大可消耗资源的可选指定，这些限制由 kubelet 和容器运行时强制执行。
- en: For example, we can set that the nginx container requires `250 MiB`. If the
    pod with this container gets scheduled on a node with `8 GiB` total memory with
    few other running Pods, our nginx container could possibly use `1 GiB` or even
    more. However, if we additionally set a limit of `1 GiB`, the runtime will prevent
    nginx from going beyond that limit. If a process tries to allocate more memory,
    the node kernel will forcefully terminate that process with an **Out Of Memory**
    (**OOM**) error, and the container gets restarted.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以设置 nginx 容器需要 `250 MiB`。如果这个容器所在的 Pod 被调度到一个总内存为 `8 GiB` 的节点上，且该节点上运行的
    Pod 不多，那么我们的 nginx 容器可能会使用 `1 GiB` 或更多内存。然而，如果我们额外设置了 `1 GiB` 的限制，运行时将防止 nginx
    超过该限制。如果进程试图分配更多内存，节点的内核将强制终止该进程并报出 **Out Of Memory** (**OOM**) 错误，容器将被重启。
- en: In the case of the CPU, limits and requests are measured by absolute units,
    where 1 CPU unit is an equivalent of 1 `0.5` CPU is the same as `500m` units,
    where `m` stands for *milliCPU*, and—as you probably guessed—it allows us to specify
    a fraction of CPU this way. Unlike with memory, if a process tries to consume
    more CPU time than allowed by the limit, it won’t be killed; instead, it simply
    gets throttled.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CPU，限制和请求是通过绝对单位来衡量的，其中 1 CPU 单位等于 1 `0.5` CPU，也就是 `500m` 单位，其中 `m` 代表 *milliCPU*，如你所猜测的，它允许我们以这种方式指定
    CPU 的一部分。与内存不同，如果进程试图消耗超出限制的 CPU 时间，它不会被杀死；而是会被限制使用。
- en: Note
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When a resource limit is specified but not the request, and no default request
    is set (for example, the default might be inherited from the namespace settings),
    Kubernetes will copy the specified limit and use it as the request too. For example,
    a `500 MiB` limit will cause the request to be `500 MiB` as well.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当指定了资源限制但未指定请求，并且未设置默认请求（例如，默认值可能从命名空间设置中继承）时，Kubernetes 将复制指定的限制并将其用作请求。例如，`500
    MiB` 的限制会导致请求也为 `500 MiB`。
- en: 'Time to see these in action! Let’s get back to our minikube setup and try creating
    the following example pod with a single container in the `kcna` namespace:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候看到这些操作了！让我们回到 Minikube 配置，尝试在 `kcna` 命名空间中创建以下带有单个容器的示例 Pod：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Execute the following command:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下命令：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The application inside is a simple `stress` test tool that generates configurable
    load and memory consumption. With the arguments specified in the preceding spec,
    it consumes exactly `150 Mi` of memory. Because `150 Mi` is less than the limit
    set (`200 Mi`), everything is working fine.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中的应用是一个简单的 `stress` 测试工具，能够生成可配置的负载和内存消耗。根据前面规范中指定的参数，它正好消耗 `150 Mi` 内存。由于
    `150 Mi` 小于设置的限制（`200 Mi`），一切正常。
- en: 'Now, let’s modify the `stress` arguments in the spec to use `250M` instead
    of `150M`. The respective changes are highlighted in the following code snippet:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们修改 `stress` 规格中的参数，将 `150M` 改为 `250M`。相应的更改在以下代码片段中突出显示：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Delete the old pod and apply the updated spec, assuming that the file is now
    called `memory-request-over-limit.yaml`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 删除旧的 pod，并应用更新后的规格，假设文件现在名为 `memory-request-over-limit.yaml`：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If you’re typing quickly enough, you should be able to see the `OOMKilled`
    status and, eventually, `CrashLoopBackOff`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你输入足够快，你应该能够看到 `OOMKilled` 状态，最终还会看到 `CrashLoopBackOff`：
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Also, you can invoke `minikube kubectl -- describe po memory-demo -n kcna`
    to see more details:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还可以运行 `minikube kubectl -- describe po memory-demo -n kcna` 查看更多细节：
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Because the process allocates `250 MiB` with a limit of `150 MiB` set on the
    container, it gets killed. Remember that if you run multiple containers in a pod,
    the whole pod will stop accepting requests if at least one container of that pod
    is not running.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因为该进程分配了 `250 MiB` 内存，并设置了 `150 MiB` 的限制，所以它被杀死。请记住，如果你在一个 pod 中运行多个容器，而该 pod
    中至少有一个容器没有运行，则整个 pod 会停止接受请求。
- en: To sum all this up, requests and limits are very important, and **the best practice
    is to configure both for all workloads running in Kubernetes** because Kubernetes
    does not know how many resources your applications need, and you might end up
    with overloaded or underutilized worker nodes in your cluster, impacting stability
    when resource requests are undefined. Resource limits, on the other hand, help
    protect from rogue pods and applications with bugs that might be leaking memory
    or trying to use all available CPU time, affecting the neighbor workloads.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，请求和限制非常重要，**最佳实践是为所有在 Kubernetes 中运行的工作负载配置这两者**，因为 Kubernetes 并不知道你的应用需要多少资源，而你可能会导致集群中的工作节点过载或未充分利用，从而影响稳定性，尤其是在资源请求未定义时。另一方面，资源限制有助于防止出现异常的
    pod 和有 bug 的应用，它们可能会泄漏内存或试图使用所有可用的 CPU 时间，影响邻近的工作负载。
- en: After you’re done, feel free to delete the pod and other resources in the `kcna`
    namespace, if any. Next, we will continue exploring Kubernetes and learn about
    the ways to debug applications running on Kubernetes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，如果需要，可以删除 `kcna` 命名空间中的 pod 和其他资源。接下来，我们将继续探索 Kubernetes，并学习如何调试在 Kubernetes
    上运行的应用。
- en: Debugging applications in Kubernetes
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中调试应用
- en: 'As you start using Kubernetes to run various applications, you’ll eventually
    face the need to debug at least some of them. Sometimes, an application might
    have a bug that causes it to crash – maybe it was misconfigured or misbehaving
    under certain scenarios. Kubernetes provides multiple mechanisms that help us
    figure out what is wrong with the containerized payload and individual pod containers,
    including the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始使用 Kubernetes 运行各种应用时，你最终会遇到需要调试至少部分应用的情况。有时，应用可能会遇到导致崩溃的 bug——可能是配置错误，或者在某些场景下表现异常。Kubernetes
    提供了多种机制来帮助我们找出容器化负载和各个 pod 容器的问题，包括以下内容：
- en: Fetching logs from all containers in a pod, and also logs from the previous
    pod run
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 pod 中所有容器获取日志，并获取之前 pod 运行的日志
- en: Querying events that happened in a cluster recently
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询集群中最近发生的事件
- en: Port forwarding from a pod to a local environment
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 pod 转发端口到本地环境
- en: Running arbitrary commands inside containers of a pod
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 pod 容器内运行任意命令
- en: Logs play a crucial role and are very helpful to understand what is not working
    as intended. Applications often support multiple log levels categorized by the
    severity and verbosity of information that gets recorded, such as received requests
    and their payload; interrupted connections; failures to connect to a database
    or other services; and so on.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 日志在调试过程中起着至关重要的作用，帮助我们理解哪些地方没有按预期工作。应用通常支持多个日志级别，按信息的严重性和详细程度分类，例如接收到的请求及其负载；中断的连接；无法连接到数据库或其他服务等。
- en: Common log levels are `INFO`, `WARNING`, `ERROR`, `DEBUG`, and `CRITICAL`, where
    the `ERROR` and `CRITICAL` settings will only record events that are considered
    errors and major problems as such. `INFO` and `WARNING` levels might provide generic
    information about what is happening or what might indicate an application problem,
    and `DEBUG` usually gives the most details by recording everything that happened
    with the application. As the name suggests, it might be wise to enable maximum
    verbosity by enabling the `DEBUG` log level to help debug problems. While this
    level of categorization is pretty standard across the industry, some software
    might have its own ways and definitions of log verbosity, so refer to the respective
    documentation and configuration samples. A standard log representation format
    today is JSON, and it is widely supported by development libraries in any language
    and all sorts of applications.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的日志级别有`INFO`、`WARNING`、`ERROR`、`DEBUG`和`CRITICAL`，其中`ERROR`和`CRITICAL`级别只会记录被认为是错误或重大问题的事件。`INFO`和`WARNING`级别可能提供关于发生了什么或可能表示应用程序问题的通用信息，而`DEBUG`通常会记录应用程序发生的所有事件，提供最详细的内容。顾名思义，启用`DEBUG`日志级别来帮助调试问题通常是明智的。尽管这种分类方式在业界非常标准，但一些软件可能有自己定义的日志详细程度方式，因此请参考相应的文档和配置示例。目前，标准的日志表示格式是JSON，并且几乎所有编程语言和各种应用程序的开发库都广泛支持这一格式。
- en: When it comes to logging architecture, the best way is to use a separate backend
    to store, analyze, and query logs that will persist the log records independent
    from the lifecycle of Kubernetes nodes, Pods, and containers. This approach is
    known as **cluster-level logging**. Kubernetes does not provide a native log storage
    solution and only keeps the most recent logs on each node. However, there are
    plenty of logging solutions that offer seamless integration with Kubernetes, such
    as **Grafana Loki** or **Elastic Stack** (**ELK**), to name a few. The opposite
    of cluster-level logging is when logging is configured individually for each pod
    (application) running in the cluster. This is not a recommended way to go with
    Kubernetes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在日志架构方面，最佳做法是使用单独的后端来存储、分析和查询日志，以便独立于Kubernetes节点、Pod和容器的生命周期持久化日志记录。这种方法称为**集群级日志**。Kubernetes本身并不提供原生的日志存储解决方案，只会在每个节点上保留最新的日志。然而，有许多日志解决方案可以与Kubernetes无缝集成，例如**Grafana
    Loki**或**Elastic Stack**（**ELK**）等。集群级日志的对立面是为集群中运行的每个Pod（应用程序）单独配置日志。这种方式不推荐在Kubernetes中使用。
- en: 'In order to collect logs from each Kubernetes node for aggregation and longer
    storage, it is common to use **node logging agents**. Those are small, containerized
    agents that run on every node and push all collected logs to a logging backend
    server. Because they need to run on each node in the cluster, it is common to
    define them as a **DaemonSet**. A schematic of such a setup is shown in *Figure
    7**.2*:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了收集每个Kubernetes节点的日志进行聚合和长期存储，通常使用**节点日志代理**。这些是小型的容器化代理，运行在每个节点上，并将所有收集的日志推送到日志后端服务器。由于它们需要在集群中的每个节点上运行，因此通常将它们定义为**DaemonSet**。这种设置的示意图如*图
    7.2*所示：
- en: '![Figure 7.2 – Using a node logging agent for log collection and aggregation](img/B18970_07_02.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 使用节点日志代理进行日志收集与聚合](img/B18970_07_02.jpg)'
- en: Figure 7.2 – Using a node logging agent for log collection and aggregation
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 使用节点日志代理进行日志收集与聚合
- en: 'For a moment, let’s get back to our minikube setup and see in action how to
    fetch application logs. Let’s start with a basic pod that simply writes the current
    date and time into the **standard** **output** (**stdout**):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时回到我们的minikube设置，看看如何获取应用程序日志。我们从一个基本的Pod开始，它只是将当前的日期和时间写入**标准** **输出**（**stdout**）：
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Execute `kubectl logs` to get the container logs. If a pod has multiple containers,
    you’ll have to additionally specify which particular container you want to get
    the logs from with the `--``container` argument:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 执行`kubectl logs`以获取容器日志。如果一个Pod有多个容器，你需要额外指定想要获取日志的特定容器，可以使用`--container`参数：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is also possible to fetch logs from the previous container execution (before
    it was restarted) by adding the `--previous` argument to the `kubectl` `logs`
    command.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过在`kubectl` `logs`命令中添加`--previous`参数，来获取上一个容器执行的日志（在容器被重启之前）。
- en: What we can see here is actually what the application inside the container writes
    to `stdout` and `stderr` (`kubectl logs`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到的实际上是容器内应用程序写入的`stdout`和`stderr`内容（`kubectl logs`）。
- en: 'However, if an application does not have a configuration to log to `stdout`
    and `stderr`, it is possible to add a logging sidecar – a separate container running
    in the same pod that captures the logs from the main container and forwards those
    to either a logging server or its own `stdout` and `stderr` output streams. Such
    a setup is depicted in *Figure 7**.3*:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果一个应用没有配置将日志记录到`stdout`和`stderr`，则可以添加一个日志侧车（sidecar）——一个在同一Pod中运行的独立容器，它捕获主容器的日志并将其转发到日志服务器或自身的`stdout`和`stderr`输出流。这样的设置在*图
    7.3*中有示意图：
- en: '![Figure 7.3 – Log steaming with a sidecar container](img/B18970_07_03.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 使用侧车容器流式传输日志](img/B18970_07_03.jpg)'
- en: Figure 7.3 – Log steaming with a sidecar container
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 使用侧车容器流式传输日志
- en: Next on our list are events that can provide valuable insights into what is
    happening in your Kubernetes cluster.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是可以提供有价值的见解，帮助你了解Kubernetes集群中发生了什么的事件。
- en: Kubernetes event
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes事件
- en: This is a record documenting a change in the state of Kubernetes resources—for
    example, changes to nodes, pods, and any other resources when a node becomes `NotReady`
    or when a **PersistentVolume** (**PV**) fails to be mounted.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个记录Kubernetes资源状态变化的记录——例如，当节点变为`NotReady`或**PersistentVolume**（**PV**）未能挂载时，对节点、Pod及其他资源的变化。
- en: 'Events are namespaced, and the `kubectl get events` command can be used to
    query recent events. For example, if we recently scaled an nginx deployment, we
    could see something similar to this:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 事件是有命名空间的，可以使用`kubectl get events`命令查询最近的事件。例如，如果我们最近扩展了一个nginx部署，我们可能会看到类似这样的内容：
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: By default, only events that happened in the last hour are kept. The duration
    can be increased in the `kube-apiserver` configuration.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，只有过去一小时内发生的事件会被保留。这个时长可以在`kube-apiserver`配置中增加。
- en: 'Coming next is another useful feature that can help during debugging or development
    with Kubernetes—`kubectl` command, you can configure to forward connections made
    on the local port on your system to the specified remote port of any pod running
    in your K8s cluster. The syntax of the command is shown here:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是另一个在调试或开发过程中使用Kubernetes时非常有用的功能——`kubectl`命令，你可以配置它将本地端口上的连接转发到K8s集群中任意Pod的指定远程端口。命令的语法如下所示：
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This is very helpful if you need to access a Kubernetes payload as though it
    is running locally on your workstation. For example, you have a web application
    listening on port `80` in the pod, and you forward your local port `8080` toward
    remote port `80` of the respective pod. While port forwarding is running, you
    can reach the application locally under `localhost:8080`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要像在工作站本地运行一样访问Kubernetes负载，这非常有帮助。例如，你有一个Web应用在Pod中的`80`端口上监听，你将本地端口`8080`转发到该Pod的远程端口`80`。在端口转发运行时，你可以通过`localhost:8080`在本地访问该应用。
- en: Port forwarding is not a part of the KCNA exam; however, if you have time, feel
    free to check the *Further reading* section at the end of the chapter for examples
    of how to use it.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 端口转发不是KCNA考试的一部分；然而，如果你有时间，可以随时查看本章结尾的*进一步阅读*部分，里面有关于如何使用它的示例。
- en: The last point on our list is about starting processes inside already running
    containers of a pod. We’ve already done that previously, in [*Chapter 6*](B18970_06.xhtml#_idTextAnchor068),
    *Deploying and Scaling Applications* *with Kubernetes*.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列表中的最后一点是关于在已经运行的Pod容器中启动进程。我们之前已经做过这件事，在[*第6章*](B18970_06.xhtml#_idTextAnchor068)，*使用Kubernetes部署和扩展应用*中。
- en: 'The respective `kubectl exec` command allows us to start transient, arbitrary
    processes in containers, and most of the time, you’d probably use it to start
    a shell (`bash` or `sh`) in interactive mode. This *feels* much like logging inside
    a container with a **Secure Shell** (**SSH**) protocol. Here’s an example:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的`kubectl exec`命令允许我们在容器中启动临时的、任意的进程，大多数情况下，你可能会用它来启动一个交互式模式的shell（`bash`或`sh`）。这*感觉*就像使用**安全外壳**（**SSH**）协议登录到容器中。以下是一个示例：
- en: '[PRE20]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Kubernetes also allows us to copy files between your local system and remote
    containers. The respective command is `kubectl cp`, which works very similarly
    to the Linux `scp` tool, but in the Kubernetes context.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还允许我们在本地系统和远程容器之间复制文件。相关命令是`kubectl cp`，它的工作方式与Linux的`scp`工具非常相似，但在Kubernetes的上下文中使用。
- en: Both `exec` and `cp` are very practical for understanding what is going on inside
    the container with an application we’re debugging. It allows us to quickly verify
    configuration settings, perform an HTTP request, or fetch logs that are not written
    to `stdout` or `stderr`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`exec` 和 `cp` 都非常实用于理解在调试的应用程序容器内发生的事情。它们允许我们快速验证配置设置、执行 HTTP 请求，或获取未写入 `stdout`
    或 `stderr` 的日志。'
- en: Summary
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: All in all, in this chapter, we’ve learned a few important aspects of running
    and operating workloads with K8s. We’ve seen how pod scheduling works with Kubernetes,
    its stages (`nodeSelector`, `nodeName`, and **affinity** and **anti-affinity**
    settings, as well as **topology spread constraints**. Extensive features of the
    Kubernetes scheduler allow us to cover all imaginable scenarios of controlling
    how a certain workload will be placed within the nodes of a cluster. With those
    controls, we can spread Pods of one application between nodes in multiple AZs
    for HA; schedule Pods that require specialized hardware (for example, a GPU) only
    to nodes that have it available; magnet multiple applications together to run
    on the same nodes with affinity; and much more.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，在本章中，我们学习了在 K8s 中运行和操作工作负载的一些重要方面。我们了解了 pod 调度在 Kubernetes 中是如何工作的，它的各个阶段（`nodeSelector`、`nodeName`、**亲和性**和**反亲和性**设置，以及**拓扑分布约束**）。Kubernetes
    调度器的丰富功能使我们能够涵盖控制工作负载在集群节点上如何分配的所有可能场景。通过这些控制，我们可以将一个应用程序的 Pods 分布到多个可用区（AZ）中的节点上以实现高可用性；将需要特殊硬件（例如
    GPU）的 Pods 仅调度到具有该硬件的节点上；使用亲和性将多个应用程序集中在同一节点上运行；等等。
- en: Next, we’ve seen that **resource requests** help Kubernetes make better scheduling
    decisions, and **resource limits** are there to protect the cluster and other
    Pods from misbehaving apps or simply restrict resource utilization. With the container
    reaching the specified memory limit, its process gets killed (and the container
    restarted), and when reaching allocated CPU units, the process gets throttled.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们了解到**资源请求**帮助 Kubernetes 做出更好的调度决策，而**资源限制**则是为了保护集群和其他 Pod 免受不良应用程序的影响，或者简单地限制资源利用。当容器达到指定的内存限制时，它的进程会被终止（并重新启动容器）；而当达到分配的
    CPU 单元时，进程会被限速。
- en: After, we explored ways to debug applications on K8s. Logs are one of the basic
    and most important tools in any problem analysis. Kubernetes follows a `stdout`
    and `stderr`, the solution is to run a **logging sidecar container** in the same
    pod as the application that will stream the logs.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们探讨了在 K8s 上调试应用程序的方法。日志是任何问题分析中最基础和最重要的工具之一。Kubernetes 遵循 `stdout` 和 `stderr`，解决方案是在与应用程序同一
    pod 中运行一个**日志 sidecar 容器**，以流式传输日志。
- en: Other practical ways to debug applications and get insights about what is happening
    in a cluster include Kubernetes events, port forwarding, and execution of arbitrary
    processes such as a `kubectl` `exec` command.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 调试应用程序并了解集群中发生的情况的其他实用方法包括 Kubernetes 事件、端口转发，以及执行任意进程，如 `kubectl` `exec` 命令。
- en: Coming next is the final chapter from the Kubernetes part, where we will learn
    some of the best practices and explore other operational aspects of K8s. If you
    would like to go deeper into the content described in this section, check out
    the *Further* *reading* section.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 Kubernetes 部分的最后一章，我们将在其中学习一些最佳实践并探索 K8s 的其他操作方面。如果你想更深入了解本节中描述的内容，请查看*进一步阅读*部分。
- en: Questions
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'As we conclude, here is a list of questions for you to test your knowledge
    regarding this chapter’s material. You will find the answers in the *Assessments*
    section of the *Appendix*:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结时，这里有一组问题供你测试自己对本章内容的理解。你可以在*附录*的*评估*部分找到答案：
- en: Which of the following stages are part of scheduling in Kubernetes (pick multiple)?
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪些阶段是 Kubernetes 调度的一部分（可以选择多个）？
- en: Spreading
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展
- en: Launching
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动
- en: Filtering
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤
- en: Scoring
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打分
- en: What happens if the Kubernetes scheduler cannot assign a pod to a node?
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 Kubernetes 调度器无法将一个 pod 分配到节点上，会发生什么？
- en: It will be stuck in a `CrashLoopBackOff` state
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将停留在 `CrashLoopBackOff` 状态
- en: It will be stuck in a `Pending` state
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将停留在 `Pending` 状态
- en: It will be stuck in a `NotScheduled` state
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将停留在 `NotScheduled` 状态
- en: It will be forcefully run on one of the control plane nodes
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将强制在某个控制平面节点上运行
- en: Which of the following scheduler instructions will not prevent a pod from being
    scheduled if a condition cannot be satisfied (soft affinity or soft anti-affinity)?
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪些调度器指令在条件无法满足（软亲和性或软反亲和性）时，不会阻止 pod 被调度？
- en: '`requiredDuringSchedulingIgnoredDuringExecution`'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`requiredDuringSchedulingIgnoredDuringExecution`'
- en: '`preferredDuringSchedulingIgnoredDuringExecution`'
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`preferredDuringSchedulingIgnoredDuringExecution`'
- en: '`neededDuringSchedulingIgnoredDuringExecution`'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`neededDuringSchedulingIgnoredDuringExecution`'
- en: '`softAffinity`'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`softAffinity`'
- en: Which Kubernetes scheduler feature should be used to control how Pods are spread
    across different failure domains (such as AZs, nodes, and so on)?
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用 Kubernetes 调度器的哪个功能来控制 Pods 如何在不同的故障域（如可用区、节点等）中分布？
- en: Pod failure domain constraints
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pod 故障域约束
- en: Pod topology spread constraints
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pod 拓扑分布约束
- en: '`podAntiAffinity`'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`podAntiAffinity`'
- en: '`nodeName`'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nodeName`'
- en: What is the purpose of `podAffinity` in Kubernetes?
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`podAffinity` 在 Kubernetes 中的作用是什么？'
- en: To schedule Pods to certain nodes in the cluster
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Pods 调度到集群中的特定节点
- en: To group two or more Pods together for better performance
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将两个或更多的 Pods 组合在一起以获得更好的性能
- en: To schedule Pods to nodes where other Pods already running
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Pods 调度到其他 Pods 已经运行的节点上
- en: To schedule Pods to different nodes where other Pods already running
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Pods 调度到其他 Pods 已经运行的不同节点上
- en: What is the purpose of resource requests in Kubernetes?
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes 中资源请求的目的是什么？
- en: They help to plan ahead the cluster extension
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们帮助规划集群扩展
- en: They define more important workloads in the cluster
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们定义了集群中更重要的工作负载
- en: They are needed to pick the right hardware in the cluster for the workload
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们帮助选择集群中适合工作负载的硬件
- en: They are needed for optimal pod placement in the cluster
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们有助于在集群中优化 Pod 的放置
- en: What happens if a container has a memory limit of `500Mi` set but tries to allocate
    `550Mi`?
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果容器的内存限制设置为 `500Mi`，但尝试分配 `550Mi`，会发生什么情况？
- en: '`550Mi` is within a 10% margin, so the container will allocate memory normally'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`550Mi` 在 10% 的误差范围内，因此容器将正常分配内存'
- en: Pods have higher limits than containers, so memory allocation will work
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pods 的限制比容器更高，因此内存分配将正常工作
- en: The container process will be killed with an OOM error
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容器进程将因为 OOM 错误被杀死
- en: The container process will be stuck when it gets over `500Mi`
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当容器进程超过 `500Mi` 时，将会卡住
- en: What will be the value of the container CPU request if the limit is set to `1.5`
    and there are no defaults for that namespace?
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果限制设置为 `1.5`，且该命名空间没有默认值，那么容器的 CPU 请求值是多少？
- en: '`0.0`'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`0.0`'
- en: '`0.75`'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`0.75`'
- en: '`1.5`'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`1.5`'
- en: '`1.0`'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`1.0`'
- en: What happens with a pod if its containers request a total of `10.0` CPU units,
    but the largest node in the cluster only has `8.0` CPUs?
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个 Pod 的容器请求总共为 `10.0` CPU 单位，但集群中最大的节点只有 `8.0` 个 CPU，会发生什么情况？
- en: Requests are not hard requirements; the pod gets scheduled
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求不是硬性要求；Pod 将被调度
- en: Requests are hard requirements; the pod will be stuck in a `Pending` state
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求是硬性要求；Pod 将处于 `Pending` 状态
- en: Because of the `preferredDuringScheduling` option, the pod gets scheduled anyway
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于 `preferredDuringScheduling` 选项，Pod 无论如何都会被调度
- en: The pod will be in a `CrashLoopBackOff` state due to a lack of resources in
    the cluster
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于集群资源不足，Pod 将处于 `CrashLoopBackOff` 状态
- en: Which logging level typically provides maximum verbosity for debugging purposes?
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常用于调试的最大详细级别是哪一个日志级别？
- en: '`INFO`'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`INFO`'
- en: '`ERROR`'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ERROR`'
- en: '`MAXIMUM`'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MAXIMUM`'
- en: '`DEBUG`'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DEBUG`'
- en: What does cluster-level logging mean for log storage in Kubernetes?
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群级别日志在 Kubernetes 中意味着什么样的日志存储？
- en: K8s aggregates all cluster logs on control plane nodes
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: K8s 将所有集群日志聚合到控制平面节点
- en: K8s needs separate log collection and aggregation systems
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: K8s 需要独立的日志收集和聚合系统
- en: K8s provides a complete log storage and aggregation solution
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: K8s 提供了一个完整的日志存储和聚合解决方案
- en: K8s provides storage only for the most important cluster health logs
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: K8s 仅为最重要的集群健康日志提供存储
- en: What do node logging agents in Kubernetes do (pick multiple)?
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes 中的节点日志代理做什么（选择多个）？
- en: Collect logs only from worker nodes
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅从工作节点收集日志
- en: Collect logs from all nodes
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集所有节点的日志
- en: Send logs from worker nodes to control plane nodes
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志从工作节点发送到控制平面节点
- en: Send logs for aggregation and storage to a logging backend
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日志发送到聚合和存储到日志后端
- en: What is the purpose of logging sidecar agents in Kubernetes?
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes 中日志侧车代理的作用是什么？
- en: To collect and stream logs from applications that cannot log to `stdout` and
    `stderr`
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从无法记录到 `stdout` 和 `stderr` 的应用程序收集并流式传输日志
- en: To provide a backup copy of logs in case of a node failure
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在节点故障时提供日志的备份副本
- en: To allow logging on verbosity levels such as `ERROR` and `DEBUG`
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 允许在 `ERROR` 和 `DEBUG` 等详细级别上记录日志
- en: To enable the `kubectl logs` command to work
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使 `kubectl logs` 命令正常工作的目的
- en: Which of the following `kubectl` commands allows us to run an arbitrary process
    in a container?
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪个 `kubectl` 命令允许我们在容器中运行一个任意进程？
- en: '`kubectl run`'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl run`'
- en: '`kubectl start`'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl start`'
- en: '`kubectl exec`'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl exec`'
- en: '`kubectl proc`'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl proc`'
- en: Which of the following commands will return logs from a pod container that has
    failed and restarted?
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪个命令将返回已失败并重新启动的 Pod 容器的日志？
- en: '`kubectl logs` `POD_NAME --previous`'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl logs` `POD_NAME --previous`'
- en: '`kubectl` `logs POD_NAME`'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl` `logs POD_NAME`'
- en: '`kubectl previous` `logs POD_NAME`'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl previous` `logs POD_NAME`'
- en: '`kubectl logs`'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl logs`'
- en: Which Kubernetes scheduler feature provides a simple way to constrain Pods to
    nodes with specific labels?
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个 Kubernetes 调度器功能提供了一种简单的方法，将 Pods 限制到具有特定标签的节点上？
- en: '`kubectl local`'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl local`'
- en: '`nodeConstrain`'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nodeConstrain`'
- en: '`nodeSelector`'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nodeSelector`'
- en: '`nodeName`'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nodeName`'
- en: Further reading
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解本章中涉及的更多主题，请查看以下资源：
- en: 'Scheduling and assigning Pods to nodes: [https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度和分配 Pods 到节点： [https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)
- en: 'Pod topology spread constraints: [https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 拓扑分布约束： [https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)
- en: 'Resource management in Kubernetes: [https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 中的资源管理： [https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
- en: 'Using port forwarding to access applications in a K8s cluster: [https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/](https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用端口转发访问 K8s 集群中的应用程序：[https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/](https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/)
- en: 'Getting a shell to a running container: [https://kubernetes.io/docs/tasks/debug/debug-application/get-shell-running-container/](https://kubernetes.io/docs/tasks/debug/debug-application/get-shell-running-container/)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '获取运行中容器的 Shell: [https://kubernetes.io/docs/tasks/debug/debug-application/get-shell-running-container/](https://kubernetes.io/docs/tasks/debug/debug-application/get-shell-running-container/)'
